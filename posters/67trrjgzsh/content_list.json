[{"type": "text", "text": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "$\\mathbf{Xu^{2}}$ ", "page_idx": 0}, {"type": "text", "text": "Xin Lu, Yanyan Zhao , Bing Qin , Liangyu Huo , Qing Yang , Dongliang Xu 1Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology $^{2}\\mathrm{Du}$ Xiaoman (Beijing) Science Technology Co., Ltd. 1{xlu, yyzhao, qinb}@ir.hit.edu.cn 2{huoliangyu, yangqing, xudongliang}@duxiaoman.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot learning. Unlike existing work focusing on the influence of scale on base capabilities, our work examines the influence of architecture on those. Specifically, our concern is: How does architecture influence the base capabilities of pre-trained language models? In this work, we attempt to explain and reverse the decline in base capabilities caused by the architecture of FFN-Wider Transformers, seeking to provide some insights. Through analysis, we found the contribution ratio of MultiHead Attention (a combination function) to pre-trained language modeling is a key factor affecting base capabilities. FFN-Wider Transformers reduce the contribution ratio of this combination function, leading to a decline in base capabilities. We confirmed this by experiments and proposed Combination Enhanced Architecture (CEA) to address the decline in base capabilities of such models. Significantly, we extended our explanation and CEA to Mixture of Experts (MoE) Transformers. We successfully achieved significant improvements in base capabilities on a 14B parameter MoE model, demonstrating the practical application value of our work. This also indicates that our analysis has a certain guiding significance for architecture analysis, architecture improvement and architecture design. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent research has discovered that pre-trained language models possess strong base capabilities [24, 7, 3, 22]. They can not only address in-distribution language modeling which is usually their pretraining objective, but also unexpectedly excel in out-of-distribution language modeling, transfer learning, few-shot learning, etc. This has attracted the attention of many researchers. ", "page_idx": 0}, {"type": "text", "text": "However, it has also been observed that the cost of pre-training a language model is substantial, with the trial-and-error approach based on empirical improvements proving to be very expensive. Consequently, there is a desire to gain insights into the final performance by analyzing factors like scale and architecture that directly determine the base capabilities of models. ", "page_idx": 0}, {"type": "text", "text": "In this process, much attention has been focused on analyzing scale, leading to the formulation of compelling scaling laws [14, 11] that drive the trend of enhancing base capability by increasing parameter numbers, data volume and training tokens. In contrast, the impact of architecture has not ", "page_idx": 0}, {"type": "image", "img_path": "67tRrjgzsh/tmp/8bdb6c236d405e76e62765973e1b444f74a3354d14449ceeb4fddfb9d3309818.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration showing that: 1) the synchronous improvement in model base capability as the contribution ratio of the Outer-FFN layer (a transformation function) decreases, that is, the contribution ratio of the MHA layer (a combination function) increases. This reveals a key factor affecting model\u2019s base capabilities. 2) Combination Enhanced Architecture (CEA) was designed based on this factor and applied to MoE models, resulting in an improvement in base capability. ", "page_idx": 1}, {"type": "text", "text": "received sufficient attention. According to the basic principle of inductive bias in machine learning, model architecture is also a crucial factor affecting base capabilities, and its impact could be equally decisive. ", "page_idx": 1}, {"type": "text", "text": "Some studies have already noted the significant influence of architecture on the base capabilities. For instance, [31] have observed considerable differences in base capabilities among various Transformer architecture variants. Some variants, though larger in scale and more powerful in pre-training performance than vanilla Transformers, exhibit significantly reduced performance in downstream tasks. This suggests different architecture models vary greatly in converting pre-training performance into base capabilities. Simply increasing scale does not resolve all issues, and exploring the impact of architecture on base capabilities is crucial. ", "page_idx": 1}, {"type": "text", "text": "However, despite these studies demonstrating the key influences of model architecture on base capabilities, the understanding of the underlying mechanisms of these influences remains limited. In this work, we attempt to explain the base capability change caused by a specific model architecture change and identify the underlying influencing factor, then design experiments to validate our explanation and influencing factor, and finally propose a generalizable enhancement method. ", "page_idx": 1}, {"type": "text", "text": "Specifically, we first focus on FFN-Wider Transformers. This architecture has wider FFN layers, but we found that the change leads to a significant decrease in base capabilities compared to vanilla Transformers. As shown in Figure 2, under similar pre-training performance, the FFN-Wider models exhibit a noticeable decline in base capabilities compared to the vanilla models. We believe such a simple change, leading to significant differences in base capabilities, makes for a good object of study in exploring how architecture impacts base capabilities. ", "page_idx": 1}, {"type": "text", "text": "Then, we attempted to explain this change in base capabilities. Through analysis, we concluded that the MHA (Multi-Head Attention) layer in the Transformer is a combination function, and the FFN (Feed-Forward Network) layer is a transformation function, with the former being a focused expression of the combinability of language. During their contribution to pre-trained language modeling, the actual contribution ratio of the MHA layer, as a combination function, is a key factor affecting the model\u2019s base capabilities. The FFN-Wider Transformer models directly enhance the FFN layer, indirectly reducing the combination function\u2019s actual contribution ratio to pre-trained language modeling, thereby leading to a significant decline in base capabilities. ", "page_idx": 1}, {"type": "text", "text": "To validate our explanation for it, we designed a Combination Adjustable Architecture (CAA), as depicted in Figure 1(a). This architecture bifurcates a wider FFN into two parts with adjustable width ratios: one remains in its original position as a transformation function, known as the Outer-FFN, and the other is relocated within the MHA layer, transformed through a special design into an Inner-FFN that solely enhances the combination function. We controlled the width ratio of the Outer-FFN, reducing it gradually from $100\\%$ to $0\\%$ . In Figure 1(b), we observed that the actual contribution ratio of the Outer-FFN progressively decreased, indicating a corresponding increase in the actual contribution ratio of the MHA layer. At the same time, we also observed a gradual improvement in base capabilities. These reveal a key phenomenon that confirms our explanation: as the actual ", "page_idx": 1}, {"type": "text", "text": "contribution ratio of the MHA layer (a combination function) increases, there is a general synchronous improvement in the model\u2019s base capabilities. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Subsequently, we identified the optimal width ratio of two parts of FFN for the Combination Adjustable Architecture (CAA) and established it as Combination Enhanced Architecture (CEA), and comparing this architecture with the original FFN-Wider Transformer. We conducted experiments on various scales of BERT and GPT models, with all results robustly supporting our explanation. ", "page_idx": 2}, {"type": "text", "text": "Importantly, we also noticed that existing work has observed a similar decline in base capabilities in the Mixture of Experts (MoE) Transformers. We applied our explanation and CEA to MoE Transformers as well. We pre-trained a 14B parameter GPT architecture MoE model and its improved version with CEA on 100B tokens (using CC and C4 from SlimPajama [29]). With the same number of parameters, computational load, and pre-training steps for both versions, the improved version with CEA showed significant improvements in out-of-distribution language modeling and fewshot learning. The performance of the out-of-distribution language modeling (average performance across all domains in SlimPajama except for CC and C4) is shown in Figure 1(c), which can prove the practical application value of our work. ", "page_idx": 2}, {"type": "text", "text": "Overall, the actual contribution ratio of the MHA layer (a combination function) is likely a universal factor affecting model\u2019s base capabilities, which can provide valuable insights for architecture analysis, improvement and design. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Subsequent sections will involve specific analyses and experiments of FFN-Wider and MoE transformers. Therefore, in this section, we introduce the relevant background of base capabilities, as well as the unified evaluation schemes and evaluation tasks. ", "page_idx": 2}, {"type": "text", "text": "2.1 Base Capabilities ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Pre-trained language models not only address in-distribution language modeling but also unexpectedly show strong base capabilities. In this work, we focus on the following base capabilities: ", "page_idx": 2}, {"type": "text", "text": "Out-of-Distribution Language Modeling This reflects the out-of-distribution generalization capability of pre-trained language models. Models that learn more essential language features often outperform others, which is a good measure of the base capabilities of pre-trained language models. ", "page_idx": 2}, {"type": "text", "text": "Transfer Learning This is a recognized base capability of pre-trained language models. The works proposing GPT [24] and BERT [7] have established the \"pre-training and fine-tuning\" transfer learning paradigm under the Transformer architecture. ", "page_idx": 2}, {"type": "text", "text": "Few-shot Learning This is also a recognized base capability. Some works [25, 3] found large-scale pre-trained language models could complete many NLP tasks with no or only a few demonstrations, revealing their remarkable few-shot learning capabilities. ", "page_idx": 2}, {"type": "text", "text": "2.2 Evaluation Schemes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we primarily explore how the architecture of a model influences its base capabilities in pre-trained language models, which requires designing a sound approach for quantitative analysis. ", "page_idx": 2}, {"type": "text", "text": "Comparing the base capabilities of one architecture model against another is straightforward: we simply compare their performance on a variety of representative tasks. However, changes in base capabilities cannot be directly attributed to changes in architecture alone, as model capabilities are also influenced by factors like scale. ", "page_idx": 2}, {"type": "text", "text": "This leads to a question: under what conditions can we compare the base capabilities of two different architecture models and be more confident that the differences are due to architecture variations? ", "page_idx": 2}, {"type": "text", "text": "Our scheme to this question: we compare the models when they use the same pre-training data and objectives, and have similar levels of pre-training performance (i.e., language modeling performance on an in-distribution development set). ", "page_idx": 2}, {"type": "image", "img_path": "67tRrjgzsh/tmp/a855baa5ed0ab1c2704be85e2480dd6af8ad98db53b2628b3a6b8a6411b5d638.jpg", "img_caption": ["Figure 2: Comparison of the base capabilities between FFN-Wider and Vanilla Transformers. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "The underlying rationale: two different architecture models, when trained on the same corpus and objectives, both gain base capabilities by reducing the loss in pre-trained language modeling. If one architecture model achieves greater base capabilities with the same reduction in language modeling loss, it is highly likely that this additional benefti stems from the inductive bias of its architecture. In other words, when pre-training performance is aligned, differences in base capabilities of models are likely reflecting the impact of architecture inductive biases. ", "page_idx": 3}, {"type": "text", "text": "This scheme is more appropriate for cross-architecture analysis compared to aligning pre-training steps, parameter numbers, or computational load, and we explain it in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Unless specified otherwise, all comparisons of base capabilities across architecture models in this work are conducted under the condition of aligned pre-training performance. ", "page_idx": 3}, {"type": "text", "text": "The experiment with MoE transformers is an exception, because we need to demonstrate practicality. Therefore, in the comparative experiments between the vanilla MoE model and the improved MoE model, not only is pre-training performance aligned, but pre-training steps, parameter numbers, and computational load are also kept consistent. ", "page_idx": 3}, {"type": "text", "text": "2.3 Evaluation Tasks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Due to subsequent experiments involving out-of-distribution language modeling evaluation, transfer learning evaluation and few-shot learning evaluation, it is necessary to clarify the selection of pretraining corpus and out-of-distribution test corpus, as well as the selection of downstream tasks. We provide a detailed introduction in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "3 FFN-Wider Transformers vs. Vanilla Transformers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This work focuses on the base capabilities of FFN-Wider Transformers. A typical Transformer model has Feed-Forward Network (FFN) layers. Assuming the hidden dimension is $d$ , the standard intermediate dimension of an FFN layer is $4d$ . However, an FFN-Wider Transformer model means the intermediate dimension of its FFN layer exceeds $4d$ . ", "page_idx": 3}, {"type": "text", "text": "We conducted experiments on various models with the intermediate dimension set to $32d$ , aligning pre-training performances with those of vanilla models. The results are shown in Figure 2. ", "page_idx": 3}, {"type": "text", "text": "We found that, at the same level of pre-training performance, the Transformer models with wider FFN layers exhibit a noticeable decline in performance on most downstream tasks, indicating a deterioration in their base capabilities. This presents us with a good research object. ", "page_idx": 3}, {"type": "text", "text": "4 Why FFN-Wider Transformers Have Worse Base Capabilities? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Combination and Transformation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Transformers consist of MHA and FFN layers. Considering a certain position in a sequence, the updated representation obtained after an MHA layer is a combination of the entire sequence context; whereas the updated representation obtained after an FFN layer is from a transformation of that position representation alone, context-insensitive. ", "page_idx": 3}, {"type": "text", "text": "When considering models composed of only one MHA layer or one FFN layer, we find that they can both be used for language modeling. If treated as black boxes focusing only on input and output, there is no difference. However, the way they accomplish language modeling is different, or rather, their inductive biases differ: the FFN layer directly maps the previous token to the target token, representing a one-to-one transformation function; while the MHA layer uses the entire sequence to calculate the target token, representing a many-to-one combination function. The latter aligns more closely with the combinability of language. ", "page_idx": 4}, {"type": "text", "text": "Further considering the multi-layer stacking of MHA and FFN layers in a Transformer model, we easily understand that although all layers contribute to the final language modeling objective, they do so with different inductive biases. Among these, the MHA layer might be the central embodiment of the model architecture\u2019s expression of the combinability of language. ", "page_idx": 4}, {"type": "text", "text": "With this understanding, when we revisit the scenarios where the FFN layer is widened or its capacity is increased, we have reason to suspect this will lead to a change in the actual contribution ratios of the transformation and combination function, thereby affecting the model\u2019s expression of the combinability of language, which is ultimately reflected in changes in basic capabilities. ", "page_idx": 4}, {"type": "text", "text": "Our hypothesis: the actual contribution ratio of the MHA layer (a combination function) is a key factor affecting the model\u2019s base capabilities. The FFN-Wider Transformers directly enhance the FFN layer, indirectly reducing the combination function\u2019s actual contribution ratio to pre-trained language modeling, thereby leading to a significant decline in base capabilities. ", "page_idx": 4}, {"type": "text", "text": "4.2 Contribution Ratio Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To verify our hypothesis, we first need to confirm whether the actual contribution ratio of MHA layers truly decreases, requiring quantitative analysis. ", "page_idx": 4}, {"type": "text", "text": "4.2.1 Mutual Information ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The first method is Mutual Information (MI). For positions in the sequence where a predicted token is to be output, we can focus on the representations of these positions after each layer and calculate the MI between these representations and the target tokens. Then, since we have MI before and after passing through any layer, we can calculate the contribution ratios of the MHA and FFN layers. ", "page_idx": 4}, {"type": "text", "text": "We adopted the mutual information estimate method proposed by [32], which mainly involves converting the representations into discrete variables through clustering, and then calculating the mutual information. For the definition of MI and more specific details, please refer to Appendix F. ", "page_idx": 4}, {"type": "text", "text": "We analyzed four small-scale models $({\\mathrm{H}}{=}128)$ ): a vanilla BERT, an FFN-Wider BERT, a vanilla GPT and an FFN-Wider GPT. The pre-training performances of the vanilla models and the FFNWider models are aligned. We plotted the MI results in Figure 3, where the blue, orange and grey lines represent the cumulative MI increment contributions of the MHA layer, FFN layer and Block, respectively. It can be seen that in the FFN-Wider models, the MI contribution of the FFN layer is significantly higher than that in the vanilla models, preliminarily validating our hypothesis. ", "page_idx": 4}, {"type": "text", "text": "4.2.2 Token Prediction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since the MI estimate requires clustering a large number of representations and is costly when the hidden dimension is high, we tried another more direct method called Token Prediction (TP). It involves predicting tokens directly from the hidden representations. The approach is as follows: ", "page_idx": 4}, {"type": "text", "text": "We still obtained representations from each layer, first dividing them into sets based on the corresponding output token. Then, we normalized all representations within a set and use their mean as the category vector for that token. During this process, we eliminated tokens where the set size was smaller than 50. For the representations of each layer, we then calculated the cosine similarity with the category vectors of tokens in that layer, selecting the token with the highest similarity as the prediction result for that representation. In this way, we could calculate a token prediction accuracy for each layer. The subsequent approach was identical to that of the MI method. ", "page_idx": 4}, {"type": "text", "text": "We analyzed four small-scale $\\scriptstyle(\\mathrm{H}=128)$ and four large-scale $({\\mathrm{H}}{=}768)$ ) models concurrently. We plotted the accuracy increment contribution ratio of the FFN layer for each model, as shown in Figure 4. It can be seen that the contribution of FFN layers in FFN-Wider models remains higher than that in vanilla models, reaffirming our hypothesis. ", "page_idx": 4}, {"type": "image", "img_path": "67tRrjgzsh/tmp/7491d96232e0bda5749b0d7d2589b19f2e407f7242b373af21f4ecce91a00c63.jpg", "img_caption": ["Figure 3: Contribution ratio analysis based on Mutual Information(MI) for various transformers. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "67tRrjgzsh/tmp/55c7665003a0ffbc904a24711586cdd2426d84e19ef89fbca70794a9069f76d7.jpg", "img_caption": ["Figure 4: Contribution ratio analysis based on Token Prediction (TP) for various transformers. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5 Combination Adjustable Architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although the FFN layer in FFN-Wider Transformers has a higher contribution ratio, we cannot assert this is the reason. This may merely be a correlation rather than a decisive factor in base capabilities. ", "page_idx": 5}, {"type": "text", "text": "Therefore, we designed an architecture that can directly intervene in the contribution ratios of the transformation and combination function, named Combination Adjustable Architecture (CAA). ", "page_idx": 5}, {"type": "text", "text": "5.1 Model Architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The new architecture, as shown in Figure5, differs from the FFN-Wider Transformer in two aspects: one is partially transferring the FFN into the MHA; the other is adding a direct pathway inside the MHA that bypasses the Inner-FFN, which we will introduce in detail below. ", "page_idx": 5}, {"type": "text", "text": "5.1.1 Inner-FFN and Outer-FFN ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A wider FFN would increase the contribution ratio of transformation function to pre-trained language modeling. A natural idea: is it possible to devise a variant FFN serves only combination function? ", "page_idx": 5}, {"type": "text", "text": "We found the primary reason that the FFN does not serve only combination function is due to the residual connections. The representation transformed by the FFN does not necessarily have to go through the MHA combination process; the FFN can bypass the MHA via residual connections and directly transmit the information in the representation to subsequent layers. ", "page_idx": 5}, {"type": "text", "text": "Therefore, we divided an FFN into two parts with adjustable width ratios: one remains in its original position as a transformation function, known as the Outer-FFN, and the other is relocated within the MHA layer, transformed an Inner-FFN. This design ensures that the representation transformed by the Inner-FFN must undergo the MHA combination process before proceeding, initially achieving the goal of the Inner-FFN serving only for the combination function. ", "page_idx": 5}, {"type": "text", "text": "5.1.2 Direct Pathway in MHA ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although we considered the impact of residual connections and designed an Inner-FFN, the InnerFFN still has a hidden pathway to bypass the combination function and directly transmit uncombined information to subsequent layers. ", "page_idx": 5}, {"type": "text", "text": "The attention mechanism involves weighted summation over values, and typically, the value at the current position also participates. This provides a possibility for the Inner-FFN to directly transmit information. Therefore, we designed a direct pathway in the MHA for current position computation to circumvent the Inner-FFN. Specifically, during the self-attention computation at the current position, the query, key and value from the current position use the input representation that has not been processed by the Inner-FFN. Only the context representations from non-current positions are transformed by the Inner-FFN. This design largely eliminates the possibility of the Inner-FFN bypassing the combination function. ", "page_idx": 5}, {"type": "image", "img_path": "67tRrjgzsh/tmp/a107a3b3cffe650d17e9fde548bf2e466a06ce0110bcf3dffa676bea6c9df367.jpg", "img_caption": ["Figure 5: Overview of our proposed Combination Adjustable Architecture (CAA). "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "67tRrjgzsh/tmp/2782150a6b8776251bf322335924e92cc665cd7d99425a12aa9bc782985084b1.jpg", "img_caption": ["Figure 6: Outer-FFN contribution ratio and base capability under different width ratios. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.2 Width Adjustment Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we control the width ratio of the Outer-FFN, reducing it gradually from $100\\%$ to $0\\%$ ,confirming our hypothesis by examining the trends of contribution ratio and base capability. ", "page_idx": 6}, {"type": "text", "text": "5.2.1 Trend of Contribution Ratio ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conducted experiments on large-scale $\\scriptstyle(\\mathrm{H}=768)$ ) BERT and GPT models. Initially, a series of models with different width ratios were pre-trained, ensuring their pre-training performance was aligned. Then, we calculated the contribution ratio of Outer-FFN using Token Prediction (TP) and plotted these trends, as shown in Figures 6(a) and 6(c). ", "page_idx": 6}, {"type": "text", "text": "It can be seen that the contribution ratio of Outer-FFN decreases as its width ratio is reduced. Conversely, the contribution ratio of the combination function increases with the reduction in the width ratio of Outer-FFN, demonstrating that controlling the width ratio indeed directly influences the contribution ratio of the combination function. ", "page_idx": 6}, {"type": "text", "text": "5.2.2 Trend of Base Capability ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We tested the performance of each model above in out-of-distribution (OOD) language modeling and plotted the trend in Figures 6(b) and 6(d). ", "page_idx": 6}, {"type": "text", "text": "For BERT, the OOD performance improves as the Outer-FFN width decreases. The performance of the optimal model has already surpassed the vanilla BERT. Most models not only outperform the FFN-Wider BERT in terms of performance but also require fewer pre-training steps. For GPT, only the model without Outer-FFN showed anomalous results, other models also follow this pattern. ", "page_idx": 6}, {"type": "text", "text": "After the analyses, a conclusion is drawn from the overall trend: the model\u2019s base capabilities generally improve as the contribution ratio of combination function increases. It holds over a wide range of width ratios and proves our hypothesis. ", "page_idx": 6}, {"type": "text", "text": "However, this overall trend does not mean the contribution of transformation function should be reduced to zero. A minor contribution from transformation function might still be crucial, as indicated by the sole anomalous result in GPT. BERT was unaffected, which is related to bidirectional attention still being able to leak few transformation contribution. More details are in Appendix G. ", "page_idx": 6}, {"type": "table", "img_path": "67tRrjgzsh/tmp/e5335575764450b7e067dc570eb5830a014e3eb4439f6f43411866cbf78d5c20.jpg", "table_caption": ["Table 1: The results of various BERT models. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "67tRrjgzsh/tmp/4dc37371e7ad77856f052924affad99e27f8389b9ca6ef8cb08fa11a2fba6e04.jpg", "table_caption": ["Table 2: The results of various GPT models. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Combination Enhanced Architecture ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we determine the Combination Enhanced Architecture (CEA) and pre-train more steps to verify whether it can reverse the decline in base capabilities of FFN-Wider Transformer. ", "page_idx": 7}, {"type": "text", "text": "6.1 Width Ratio Selection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We selected the optimal width ratio for Combination Enhanced Architecture (CEA). The Outer-FFN width ratio for FFN-Wider BERT w/ CEA is set to $0\\%$ . For FFN-Wider GPT w/ CEA, it is $12.5\\%$ . ", "page_idx": 7}, {"type": "text", "text": "6.2 Further Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conducted more pre-training steps on vanilla models, FFN-wider models, FFN-wider models w/ CEA, and aligned the pre-training performances. ", "page_idx": 7}, {"type": "text", "text": "For BERT, we tested fine-tuning performance on GLUE, SuperGLUE and other datasets. For GPT, we tested zero-shot and one-shot performance on multiple datasets. For both, we tested OOD language modeling on Pile. For small-scale models, we removed some experiments beyond their capabilities. The brief results are shown in Table 1 and 2, and the detailed results are in Appendix H. ", "page_idx": 7}, {"type": "text", "text": "The results show the FFN-wider models w/ CEA improve in base capabilities, not only surpassing the FFN-wider models in most aspects but also nearly reaching the level of vanilla models. In addition, the new architecture models w/o the direct pathway in MHA show a significant decline in base capabilities. These all confirm our explanation. ", "page_idx": 7}, {"type": "text", "text": "Additionally, we conducted similar experiments for other width ratios that can align pre-training performance and steps simultaneously, which can indisputably prove improvements come from architecture, as detailed in Appendix I. ", "page_idx": 7}, {"type": "text", "text": "7 From FFN-Wider Transformers to MoE Transformers ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The Mixture of Experts (MoE) Transformer [18, 8] is a practical architecture that introduces a mix of experts, which enables expanding the capacity with lower computation expense. ", "page_idx": 7}, {"type": "text", "text": "However, base capability decline also be observed in MoE Transformers. [8] found MoE models perform lower on SuperGLUE compared to vanilla models when achieving same pre-training level. Similar issue can also be analyzed from the results of [1]. ", "page_idx": 7}, {"type": "text", "text": "We found the MoE layer can be seen as an enhanced version of the FFN layer. Therefore, we believe the previous explanations could also apply to MoE Transformers. Consequently, we directly transplant our CEA to MoE Transformers, resulting in a new MoE architecture. ", "page_idx": 7}, {"type": "text", "text": "We selected a 1.3B GPT model as the backbone model, which incorporates Rotary Embedding [30] and RMSNorm [37] on the GPT3 [3]. Then, we added a MoE layer with 64 experts (top-1 activation) before the FFN layer, resulting in a 14B parameter MoE baseline model. Finally, our improved version with CEA involves transforming the MoE layer into an Inner-MoE layer within the MHA, while retaining the original FFN layer as the Outer-FFN layer. To accommodate FlashAttention-2 [6], we simplified the direct pathway. Instead of preventing transformation leakage by replacing the key and value at current position, we chose to directly mask the current position to achieve the same effect. The specifications, pre-training procedures and few-shot learning procedures are detailed in Appendix C and E. ", "page_idx": 7}, {"type": "table", "img_path": "67tRrjgzsh/tmp/ba05b8bb0bfcca8ae3b91654730a4b678fe8cfd6be5455f3d7ce111ed168c2ee.jpg", "table_caption": ["Table 3: The results of Vanilla GPT 1.3B, Vanilla MoE 14B and MoE 14B w/ CEA. \u2020 indicates the test set drawn from the same distribution as the pre-training data. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We conducted pre-training on CC and C4 subsets within SlimPajama [29], training all models from scratch with 100B tokens. Due to the de-duplication across subsets achieved by SlimPajama, we used the remaining subsets as out-of-distribution sets. We carried out comprehensive out-of-distribution language modeling and few-shot learning tests, with the results presented in Table 3. ", "page_idx": 8}, {"type": "text", "text": "From the results, it is evident that our method significantly enhances the base capabilities of the MoE model. This fully demonstrates the effectiveness of our analysis and improvement methods. ", "page_idx": 8}, {"type": "text", "text": "8 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work mainly analyzes the models that use language modeling as the pre-training objective, lacking experiments on models with other pre-training objectives. Hence, the conclusions are limited to pre-trained language models. Therefore, the current applicability of our findings is relatively narrow, and we consider conducting more experiments in the future. ", "page_idx": 8}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work explores how architecture affects the base capabilities of pre-trained language models. FFN-Wider Transformer is our research object and we try to explain and reverse the decline in base capabilities caused by its architecture. We found the contribution ratio of combination function is a key factor, while FFN-Wider Transformer reduces it, leading to a decline in base capabilities. We solved it by proposing CEA. In addition, we extended our conclusion to MoE Transformers, proving our work can offer guidance for architecture improvement. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Fundamental Research Funds for the Central Universities (project number: 2022FRFK060002), the National Key RD Program of China via grant 2021YFF0901602 and the National Natural Science Foundation of China (NSFC) via grant 62176078. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru, G. Anantharaman, X. Li, S. Chen, H. Akin, M. Baines, L. Martin, X. Zhou, P. S. Koura, B. O\u2019Horo, J. Wang, L. Zettlemoyer, M. Diab, Z. Kozareva, and V. Stoyanov. Efficient large scale language modeling with mixtures of experts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11699\u201311732, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics.   \n[2] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.   \n[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.   \n[4] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[5] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.   \n[6] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.   \n[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[8] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1\u201339, 2022.   \n[9] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. ", "page_idx": 9}, {"type": "text", "text": "[10] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021. ", "page_idx": 10}, {"type": "text", "text": "[11] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre. An empirical analysis of compute-optimal large language model training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 30016\u201330030. Curran Associates, Inc., 2022.   \n[12] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for NLP. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790\u20132799. PMLR, 09\u201315 Jun 2019.   \n[13] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.   \n[14] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models, 2020.   \n[15] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.   \n[16] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785\u2013794, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics.   \n[17] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020.   \n[18] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021.   \n[19] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.   \n[20] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381\u20132391, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics.   \n[21] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696, 2016.   \n[22] OpenAI. Gpt-4 technical report, 2023.   \n[23] D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fern\u00e1ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525\u20131534, Berlin, Germany, Aug. 2016. Association for Computational Linguistics.   \n[24] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n[25] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[26] M. Roemmele, C. A. Bejan, and A. S. Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.   \n[27] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[28] M. Sap, H. Rashkin, D. Chen, R. Le Bras, and Y. Choi. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463\u20134473, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.   \n[29] Z. Shen, T. Tao, L. Ma, W. Neiswanger, Z. Liu, H. Wang, B. Tan, J. Hestness, N. Vassilieva, D. Soboleva, and E. Xing. Slimpajama-dc: Understanding data combinations for llm training, 2024.   \n[30] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding, 2023.   \n[31] Y. Tay, M. Dehghani, S. Abnar, H. Chung, W. Fedus, J. Rao, S. Narang, V. Tran, D. Yogatama, and D. Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12342\u201312364, Singapore, Dec. 2023. Association for Computational Linguistics.   \n[32] E. Voita, R. Sennrich, and I. Titov. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4396\u20134406, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.   \n[33] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[34] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium, Nov. 2018. Association for Computational Linguistics.   \n[35] J. Welbl, N. F. Liu, and M. Gardner. Crowdsourcing multiple choice science questions, 2017.   \n[36] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791\u20134800, Florence, Italy, July 2019. Association for Computational Linguistics.   \n[37] B. Zhang and R. Sennrich. Root Mean Square Layer Normalization. In Advances in Neural Information Processing Systems 32, Vancouver, Canada, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Other Evaluation Schemes ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Aside from the pre-training performance alignment scheme we adopted, there are three other schemes.   \nHowever, none of these are suitable for our work. ", "page_idx": 12}, {"type": "text", "text": "Pre-training steps alignment Differences in model\u2019s base capabilities might be due to differences in model capacity, not solely from architecture changes. Especially in models with different architectures and parameter scales, models with more parameters have a capacity advantage, rapidly reducing pre-training loss to a low level, showing good metrics on various tasks, and appearing to have strong base capabilities. But this is more due to large model capacity, and not much related to architecture inductive biases. ", "page_idx": 12}, {"type": "text", "text": "Parameter numbers alignment It imposes restrictions on the model that make it inapplicable to any two different architecture models. More critically, this scheme has clear counterexamples, as it is not suitable for ALBERT models [17]. Under the same parameter numbers, ALBERT models exhibit strong base capabilities, but this is due to the large computational load, not much related to architecture inductive biases. ", "page_idx": 12}, {"type": "text", "text": "Computational load alignment It imposes restrictions on the model that make it inapplicable to any two different architecture models. More critically, this scheme also has clear counterexamples, as it is not suitable for MoE models [18, 8]. Under the same computational load, MoE models demonstrate strong base capabilities, but this is due to large parameter numbers and model capacity, not much related to architecture inductive biases. ", "page_idx": 12}, {"type": "text", "text": "Although these schemes have shortcomings, they also have reasonable aspects. To indisputably prove base capability improvements come from our architecture changes, we also conducted experiments that align pre-training performance, pre-training steps, parameter numbers and computational load simultaneously, as detailed in Appendix I. ", "page_idx": 12}, {"type": "text", "text": "In addition, the experiment with MoE transformers also adopts a stricter setting similar to the above. In the comparative experiments between the vanilla MoE model and the improved MoE model, not only is pre-training performance aligned, but pre-training steps, parameter numbers, and computational load are also kept consistent. ", "page_idx": 12}, {"type": "text", "text": "B Evaluation Tasks ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 FFN-Wider Transformers ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "All experiments with FFN-Wider transformers follow the unified settings described here: ", "page_idx": 12}, {"type": "text", "text": "The pre-training corpus is consistent with that of BERT [7], namely Wikipedia and BooksCorpus. We partition a portion of the corpus to serve as an in-distribution development set, and use it to align pre-training performance. ", "page_idx": 12}, {"type": "text", "text": "We conduct experiments on four specifications: BERT $({\\mathrm{H}}{=}128)$ ), BERT $\\scriptstyle(\\mathrm{H}=768)$ ), GPT $(\\mathrm{H}{=}128)$ ) and GPT $\\scriptstyle{\\mathrm{H}}=768)$ ). The specifications and pre-training procedures are detailed in Appendix C. ", "page_idx": 12}, {"type": "text", "text": "For out-of-distribution language modeling capability, we evaluate on the development set of Pile dataset [9]. ", "page_idx": 12}, {"type": "text", "text": "For transfer learning capability, we only evaluate BERT models on GLUE [34], SuperGLUE [33], HellaSwag [36], PIQA [2], OpenBookQA [20], ARC Easy & Challenge [5] and WinoGrande [27]. The experimental settings are detailed in Appendix D. ", "page_idx": 12}, {"type": "text", "text": "For few-shot learning capability, we only evaluate GPT models. Limited by the maximum sequence length (128) of our pre-trained models, we conduct 0-shot and 1-shot experiments on HellaSwag, PIQA, OpenBookQA, ARC Easy & Challenge, WinoGrande, Winograd [19], COPA [26] and StoryCloze [21]. The experimental settings are detailed in Appendix E. ", "page_idx": 12}, {"type": "text", "text": "B.2 MoE Transformers ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "All experiments with MoE transformers follow the unified settings described here: ", "page_idx": 12}, {"type": "text", "text": "The pre-training corpus is the CC and C4 subsets within the SlimPajama dataset [29]. The specifications and pre-training procedures are detailed in Appendix C. ", "page_idx": 13}, {"type": "text", "text": "For out-of-distribution language modeling capability, due to the de-duplication across subsets achieved by SlimPajama [29], we used the remaining subsets (Arxiv, Book, Github, Stack and Wiki) as out-of-distribution test sets. ", "page_idx": 13}, {"type": "text", "text": "For few-shot learning capability, we conduct 0-shot experiments on LAMBADA [23], and conduct 5-shot experiments on MMLU [10], OpenBookQA, ARC Easy & Challenge, BoolQ [4], RACE Middle & High [16], SIQA [28], SCIQ [35], HellaSwag, COPA, PIQA, StoryCloze, WinoGrande and Winograd. The experimental settings are detailed in Appendix E. ", "page_idx": 13}, {"type": "text", "text": "C Model Specifications and Pre-training Procedures ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 FFN-Wider Transformers ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "All experiments were conducted in English only. We concurrently pre-trained both BERT and GPT models, where the architecture design of the BERT model adheres to the work of [7], and the GPT model structure follows that established by [24]. Both models employ a post layer normalization scheme. ", "page_idx": 13}, {"type": "text", "text": "We trained small and large scales of both BERT and GPT. The small-scale models have a hidden dimension of 128, 12 layers, and 2 attention heads; the large-scale models possess a hidden dimension of 768, 12 layers, and 12 attention heads. In vanilla BERT and GPT, the FFN intermediate dimension is 4 times the hidden dimension, while the ratio of FFN-Wider models is 32 times. The small-scale vanilla models have $6.3\\mathrm{M}$ Parameters and the small-scale other models have $17.3\\mathrm{M}$ Parameters. The large-scale vanilla models have 110M Parameters and the large-scale other models have 506M Parameters. All models have a maximum sequence length of 128 and utilize the BERT vocabulary released by [7], comprising 30,522 tokens. We used PyTorch2 and transformers3 libraries. ", "page_idx": 13}, {"type": "text", "text": "The BERT model employs a masked language modeling task for pre-training, masking $15\\%$ of tokens in the sequence\u2014 $80\\%$ are replaced with [MASK], $10\\%$ with random tokens, and $10\\%$ remain unchanged. Differing from [7], we removed the next sentence prediction task, using long and continuous text for pre-training inputs and applying different masking schemes to the same input sequence in various epochs. The GPT model is pre-trained using a language modeling task without additional special settings. ", "page_idx": 13}, {"type": "text", "text": "The maximum epoch set for all model pre-training is 40, but in practice, it was not reached; midtraining checkpoints were used for alignment and experimentation. All models used the Adam optimizer [15] for pre-training, with a learning rate of 1e-4, $\\beta_{1}\\,=\\,0.9$ , $\\beta_{2}\\,=\\,0.999$ , L2 weight of 0.01, a warm-up over the first 10,000 steps, followed by linear decay. The small-scale models were pre-trained with a batch size of 512 on four Nvidia Tesla V100s, and total GPU days are approximately 55 days; the large-scale models with a batch size of 1024 on four Nvidia Tesla A100s, and total GPU days are approximately 87 days. ", "page_idx": 13}, {"type": "text", "text": "To fairly compare the base capabilities of different architecture models, all models were pre-trained from scratch. However, as mentioned earlier, due to limited computational resources, our pretraining steps generally fell short of those in the original papers [7, 24, 25, 3], which may result in discrepancies in downstream task performance compared to the original research. ", "page_idx": 13}, {"type": "text", "text": "C.2 MoE Transformers ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "All experiments were conducted in English only. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We selected the GPT model as the backbone model (Vanilla GPT 1.3B), which incorporates Rotary Embedding [30] and RMSNorm [37] on the GPT3 [3]. Then, we added a MoE layer before the FFN layer, resulting in our MoE baseline model (Vanilla MoE 14B). Finally, our improved version with CEA (MoE 14B w/ CEA) involves transforming the MoE layer into an Inner-MoE layer within the MHA, while retaining the original FFN layer as the Outer-FFN layer. To accommodate ", "page_idx": 13}, {"type": "text", "text": "FlashAttention-2 [6], we simplified the direct pathway. Instead of preventing transformation leakage by replacing the key and value at current position, we chose to directly mask the current position to achieve the same effect. ", "page_idx": 14}, {"type": "text", "text": "The Vanilla GPT 1.3B has a hidden dimension of 4,096, 6 layers and 32 attention heads; the Vanilla MoE 14B has a hidden dimension of 4,096, 6 layers, 32 attention heads and 6 MoE layers with 64 experts (top-1 activation, intermediate dimension is 4,096) before the corresponding FFN layers; the MoE 14B w/ CEA is similar to the Vanilla MoE 14B, just the position of the parameters has changed. All models have a maximum sequence length of 2,048 and utilize the Mixtral vocabulary released by [13], comprising 32,000 tokens. We used PyTorch, transformers and fastmoe4libraries. ", "page_idx": 14}, {"type": "text", "text": "These models are pre-trained using a language modeling task without additional special settings. We conducted pre-training on CC and C4 subsets within SlimPajama [29], training all models from scratch with 100B tokens. Due to the de-duplication across subsets achieved by SlimPajama, we used the remaining subsets as out-of-distribution sets. All models used the Adam optimizer [15] for pre-training, with a learning rate of 5e-4, $\\beta_{1}=0.9$ , $\\beta_{2}=0.95$ , L2 weight of 0.01, a warm-up over the first 2,500 steps, followed by linear decay. All models were pre-trained with a batch size of 256 on 8 Nvidia Tesla A100 80G cards, and total GPU days are approximately 280 days. ", "page_idx": 14}, {"type": "text", "text": "D Fine-tuning Settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We conducted fine-tuning experiments on BERT models across various datasets. For the small-scale $\\scriptstyle(\\mathrm{H}=128)$ ) models, we only conducted experiments on GLUE, while for the large-scale $\\scriptstyle{\\mathrm{H}}=768)$ ) models, we experimented with all datasets. ", "page_idx": 14}, {"type": "text", "text": "The maximum number of training epochs during fine-tuning was 10, with a batch size of 32. The optimizer was Adam [15], with a warmup ratio of 0.06, a linearly decaying learning rate, and a weight decay of 0.01. We reported the average performance of multiple runs. ", "page_idx": 14}, {"type": "text", "text": "For the small-scale $_{\\mathrm{H=128}}$ ) models, we observed models with a wider FFN might overfti when fully fine-tuned with all parameters. Thus, we performed both full parameter fine-tuning and efficient fine-tuning based on adapters [12] for all models, choosing the better result of the two for reporting. For full parameter fine-tuning, the learning rates were {1e-5, 2e-5, 5e-5}; for efficient fine-tuning, the adapter size was 128, with learning rates of {1e-4, 2e-4, 3e-4}. For the large-scale $\\scriptstyle(\\mathrm{H}=768)$ models, we directly conducted full parameter fine-tuning for all models, with learning rates of {1e-5, 2e-5, 5e-5}. ", "page_idx": 14}, {"type": "text", "text": "E Few-shot Learning Settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our study on GPT and MoE models, we conducted few-shot learning experiments on multiple datasets. Since small-scale $\\scriptstyle(\\mathrm{H}=128)$ ) models have limited capabilities and struggle with few-shot learning, we focused our experiments only on large-scale $\\scriptstyle{\\mathrm{H}}=768)$ ) models for 0-shot and 1-shot learning and MoE models for 0-shot and 5-shot learning. ", "page_idx": 14}, {"type": "text", "text": "We followed the method of [3], which involves transforming the classification into comparisons of probability magnitudes. ", "page_idx": 14}, {"type": "text", "text": "Specifically, the unified format of these datasets involves selecting one option from multiple choices, given a context. We concatenated the context with different options and compared their respective generation probabilities. This comparison could be based on either the probability of the option alone or the entire sequence text, with a choice of normalizing the probabilities by length or not. [3] mentioned a method of normalizing the probability of each option unconditionally, which we also included in our options. We then experimented with all possible combinations of these choices for each dataset, determined the best combination for each, and reported the results of all models under these optimal conditions. ", "page_idx": 14}, {"type": "text", "text": "For 1-shot experiment, we randomly selected one demonstration each time, repeated the experiment 10 times, and reported the average results. For 5-shot experiment, we randomly selected five demonstrations each time (for MMLU, randomly shuffilng the order of demonstrations), repeated the experiment 5 times, and reported the average results. ", "page_idx": 14}, {"type": "text", "text": "F Mutual Information Estimate ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The formal definition of mutual information for two discrete random variables is as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{I}(X;Y)=\\sum_{y\\in{\\mathcal{Y}}}\\sum_{x\\in{\\mathcal{X}}}P_{(X,Y)}(x,y)\\log\\!\\left({\\frac{P_{(X,Y)}(x,y)}{P_{X}(x)P_{Y}(y)}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $X$ and $Y$ are discrete random variables, $P_{(X,Y)}$ is the joint probability mass function of $X$ and $Y$ , and $P_{X}$ and $P_{Y}$ are the marginal probability mass functions of $X$ and $Y$ respectively. ", "page_idx": 15}, {"type": "text", "text": "For estimating the mutual information between intermediate representations and target tokens, we adopted the method proposed by [32], which primarily involves transforming the representations into discrete variables through clustering and then calculating mutual information. ", "page_idx": 15}, {"type": "text", "text": "We first randomly sampled 6.94 million input tokens and their corresponding output tokens from a pre-trained development set and collected all intermediate representations of the model at these locations. Then, for the GPT model, we performed clustering directly at each layer and calculated mutual information using the labels obtained from clustering with the output tokens. For the BERT model, we also conducted clustering at each layer; however, since masked language modeling only considers masked positions as the actual output, mutual information was calculated solely at these masked positions. Clustering was done using the mini-batch $\\mathbf{k}$ -means algorithm, initialized with the k-means $^{++}$ method, with a batch size of 1024 and a class number set to 2000. ", "page_idx": 15}, {"type": "text", "text": "G Additional Transformation Function Contribution in BERT ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "While it appears that BERT can reduce the actual contribution ratio of transformation function to zero, in reality, some contributions of transformation function remain uncovered by our method. ", "page_idx": 15}, {"type": "text", "text": "For BERT, although we have eliminated most of the pathways through which the Inner-FFN directly leaks independent transformation information to subsequent layers, there is still one way this can happen: due to the bidirectional nature of attention, in the MHA of the current block, the entire sequence context has already been inflitrated by information about the current position at lower levels. Therefore, the Inner-FFN can independently enhance this part of the information, which can then leak out again through the weighted summation over values. In contrast, the GPT models employ unidirectional attention, so the context definitely does not contain any information about the current position, hence there is truly no independent transformation without the Outer-FFN. ", "page_idx": 15}, {"type": "text", "text": "Therefore, while we are fairly certain that reducing the contribution ratio of independent transformation function is a general trend that can enhance base capabilities, completely eliminating the contribution of it could be harmful, as indicated by the trend in GPT; the absence of this phenomenon in BERT\u2019s trend is merely due to the likelihood that BERT still retains a small part of independent transformation in the Inner-FFN. ", "page_idx": 15}, {"type": "text", "text": "H Detailed Results (Main Setting: Pre-training Performance Alignment) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Section 6.2, we present the brief experimental results in Table 1 and 2, and here we provide the corresponding detailed results. ", "page_idx": 15}, {"type": "text", "text": "For BERT, out-of-distribution language modeling results on Pile are shown in Table 6, fine-tuning results on GLUE Benchmark are shown in Table 7, fine-tuning results on SuperGLUE Benchmark are shown in Table 8, fine-tuning results on multiple other tasks are shown in Table 9. ", "page_idx": 15}, {"type": "text", "text": "For GPT, out-of-distribution language modeling results on Pile are shown in Table 10, zero-shot results and one-shot results on multiple datasets are shown in Table 11. ", "page_idx": 15}, {"type": "text", "text": "I Detailed Results (Extra Setting: Pre-training Performance Alignment & Pre-training Steps Alignment) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The main experiments in this work are based on the pre-training performance alignment scheme. Although we have introduced the rationale behind this scheme, the difference in pre-training steps between original FFN-Wider models and new architecture models may still raise question: is the base capability improvement of new architecture models really due to architecture changes? ", "page_idx": 15}, {"type": "table", "img_path": "67tRrjgzsh/tmp/4be1958dd29c8b8b8c5213d18af5f9c97089d446e8dd23de79f0ca31bcbdb683.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "67tRrjgzsh/tmp/d7cebf51749f606b64c3c63c2376d07909c57c8a8605b50bbbc7198d4c90bfdc.jpg", "table_caption": ["Table 4: The results of BERT (Pre-training Steps Alignment & Pre-training Performance Alignment). ", "Table 5: The results of GPT (Pre-training Steps Alignment & Pre-training Performance Alignment). "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "To answer this question, we chose the Outer-FFN width ratios that allow CAA to align with FFNWider Transformer for both pre-training performance and pre-training steps, and conducted experiments similar to those in Section 6.2. Moreover, the parameter numbers and computational load of CAA are also aligned with the FFN-Wider Transformer. Therefore, we achieved alignment in four aspects, and under this setting, the improvements of base capabilities can be incontrovertibly attributed to our architectural changes. ", "page_idx": 16}, {"type": "text", "text": "The Outer-FFN width ratio for FFN-Wider BERT w/ CAA is set to $12.5\\%$ , and for FFN-Wider GPT w/ CAA, it is set to $37.5\\%$ . ", "page_idx": 16}, {"type": "text", "text": "The brief results are shown in Table 4 and 5, and the corresponding detailed results are as follows: ", "page_idx": 16}, {"type": "text", "text": "For BERT, out-of-distribution language modeling results on Pile are shown in Table 12, fine-tuning results on GLUE Benchmark are shown in Table 13, fine-tuning results on SuperGLUE Benchmark are shown in Table 14, fine-tuning results on multiple other tasks are shown in Table 15. ", "page_idx": 16}, {"type": "text", "text": "For GPT, out-of-distribution language modeling results on Pile are shown in Table 16, zero-shot results and one-shot results on multiple datasets are shown in Table 17. ", "page_idx": 16}, {"type": "text", "text": "The experimental results show that the base capabilities of new architecture models have still significantly improved, confirming the positive impact of our architecture modifications on base capabilities. ", "page_idx": 16}, {"type": "table", "img_path": "67tRrjgzsh/tmp/51ef91fd96d11ad8105fb15244806161d46c4bc921ed3217b77c7c082059be2d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "67tRrjgzsh/tmp/9e6aaacee81467856c1efb6f43f1d59ad640a601e3384880f32076e049cb93de.jpg", "table_caption": ["Table 6: Out-of-distribution language modeling results on the development set of Pile of various BERT models (Pre-training Performance Alignment). "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "67tRrjgzsh/tmp/e73251b1d2b127be2718c5c9901d70c4466062027d9d60feb071d7e7380876bf.jpg", "table_caption": ["Table 7: Fine-tuning results on the development set of GLUE Benchmark of various BERT models (Pre-training Performance Alignment). "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 8: Fine-tuning results on the development set of SuperGLUE Benchmark of various BERT models (Pre-training Performance Alignment). ", "page_idx": 17}, {"type": "table", "img_path": "67tRrjgzsh/tmp/e20f35dd4f736307a1c74334dcc996a93723fdbfa165054c7b262d8a6e9b00e3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 9: Fine-tuning results on multiple other tasks of various BERT models (Pre-training Performance Alignment). ", "page_idx": 17}, {"type": "table", "img_path": "67tRrjgzsh/tmp/6c5e0635a1a01c510d0499c4120e02e98283fbfb7d04fec5b80339161550d669.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "67tRrjgzsh/tmp/0c608b00bcd1292674a249b57925e26941464b172adab28ee41a68ed740b552b.jpg", "table_caption": ["Table 10: Out-of-distribution language modeling results on the development set of Pile of various GPT models (Pre-training Performance Alignment). "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 11: Zero-shot and one-shot results on multiple datasets of various GPT models (Pre-training Performance Alignment). ", "page_idx": 18}, {"type": "table", "img_path": "67tRrjgzsh/tmp/578a80bfa0fd8575203b57d789dcc80ed81c356e284ae3c9b97379cd1f4302eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "67tRrjgzsh/tmp/0ee4c25c3b651ee59e267b1026a828a0e54c033ec3ceb14de4245670aca1e209.jpg", "table_caption": ["Table 12: Out-of-distribution language modeling results on the development set of Pile of various BERT models (Pre-training Steps Alignment & Pre-training Performance Alignment). "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "67tRrjgzsh/tmp/e2b3de392b7e619dff70ab7b6135635a0a661334db774776502c60ff907875e6.jpg", "table_caption": ["Table 13: Fine-tuning results on the development set of GLUE Benchmark of various BERT models (Pre-training Steps Alignment & Pre-training Performance Alignment). "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 14: Fine-tuning results on the development set of SuperGLUE Benchmark of various BERT models (Pre-training Steps Alignment & Pre-training Performance Alignment). ", "page_idx": 19}, {"type": "table", "img_path": "67tRrjgzsh/tmp/04829180fe603505440bf4d7332958d4116ac6e1bc0efd506e07a88a7fcd1015.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 15: Fine-tuning results on multiple other tasks of various BERT models (Pre-training Steps Alignment & Pre-training Performance Alignment). ", "page_idx": 19}, {"type": "table", "img_path": "67tRrjgzsh/tmp/8cd2fa40e926f0b61917680e39283cec4bcf2b6a3bd4e264295eb91527828b3b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "67tRrjgzsh/tmp/a07e944fbc174d3765ebcc8c8539760711ae8dc2296f7e3c433e0ab512454976.jpg", "table_caption": ["Table 16: Out-of-distribution language modeling results on the development set of Pile of various GPT models (Pre-training Steps Alignment & Pre-training Performance Alignment). "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 17: Zero-shot and one-shot results on multiple datasets of various GPT models (Pre-training Steps Alignment & Pre-training Performance Alignment). ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In Abstract and Section 1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: In Section 8. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Not involving proof. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Appendix C, D, E and F. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: There are some restrictions that make it inconvenient to publish directly. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In Appendix C, D, E and F. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We use multiple rounds of randomization to ensure the stability of the results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In Appendix C. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This work is a foundational research. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 24}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not release models. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not release models. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: N/A. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: N/A. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: N/A. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]