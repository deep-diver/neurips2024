[{"figure_path": "67tRrjgzsh/tables/tables_7_1.jpg", "caption": "Table 1: The results of various BERT models.", "description": "This table presents the results of various BERT models, comparing the performance of vanilla BERT, FFN-Wider BERT (a model with wider feed-forward networks), and FFN-Wider BERT with Combination Enhanced Architecture (CEA) across different tasks, including out-of-distribution language modeling (Pile), transfer learning (GLUE, SuperGLUE), and other tasks.  The results are separated for models with hidden dimension (H) of 128 and 768, and pre-training performance is aligned across models to isolate the effects of architecture.", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_7_2.jpg", "caption": "Table 5: The results of various GPT models.", "description": "This table shows the results of various GPT models with different architectures (Vanilla, FFN-Wider, and FFN-Wider with CEA) evaluated on various downstream tasks.  The results are categorized by model size (H=128 and H=768), and include the Pile (out-of-distribution language modeling) performance, as well as 0-shot and 1-shot performance on 9 downstream tasks.", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_8_1.jpg", "caption": "Table 3: The results of Vanilla GPT 1.3B, Vanilla MoE 14B and MoE 14B w/ CEA. \u2020 indicates the test set drawn from the same distribution as the pre-training data.", "description": "This table presents the results of three different models: Vanilla GPT 1.3B, Vanilla MoE 14B, and MoE 14B with the Combination Enhanced Architecture (CEA).  It compares their performance across various metrics, including loss on different subsets of the SlimPajama dataset, perplexity and accuracy on LAMBADA, accuracy on MMLU, OpenBookQA, ARC, BoolQ, RACE, SIQA, SCIQ, HellaSwag, COPA, PIQA, StoryCloze, Winograd, and WinoGrande. The \u2020 symbol indicates that the test set for the SlimPajama dataset is from the same distribution as the pre-training data.", "section": "7 From FFN-Wider Transformers to MoE Transformers"}, {"figure_path": "67tRrjgzsh/tables/tables_16_1.jpg", "caption": "Table 1: The results of various BERT models.", "description": "This table presents the results of various BERT models, comparing the base capabilities of vanilla BERT, FFN-Wider BERT, and FFN-Wider BERT with Combination Enhanced Architecture (CEA).  The results are shown across multiple evaluation tasks, including out-of-distribution language modeling on Pile, and fine-tuning performance on GLUE and SuperGLUE benchmarks.  The table demonstrates the impact of the FFN-Wider architecture and the effectiveness of CEA in improving base capabilities.", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_16_2.jpg", "caption": "Table 5: The results of various GPT models.", "description": "This table presents the results of various GPT models, comparing the base capabilities of vanilla GPT models against FFN-Wider GPT models and their counterparts enhanced with the Combination Enhanced Architecture (CEA).  It shows the performance on Pile (an out-of-distribution language modeling benchmark) and other tasks with 0-shot and 1-shot learning settings.  The results illustrate the impact of the FFN-Wider architecture and the effectiveness of CEA in improving base capabilities.", "section": "Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_17_1.jpg", "caption": "Table 6: Out-of-distribution language modeling results on the development set of Pile of various BERT models (Pre-training Performance Alignment).", "description": "This table presents the out-of-distribution language modeling results on the Pile development set for various BERT models.  The results are broken down by model type (Vanilla BERT, FFN-Wider BERT, FFN-Wider BERT w/ CEA, FFN-Wider BERT w/o Direct Pathway in MHA) and hidden dimension size (H=128, H=768). Each row represents a specific dataset within Pile, and the values show the log-likelihood scores achieved by each model on that dataset.  The table highlights the impact of the FFN-Wider architecture and the proposed Combination Enhanced Architecture (CEA) on the models' ability to generalize to out-of-distribution data.", "section": "Detailed Results (Main Setting: Pre-training Performance Alignment)"}, {"figure_path": "67tRrjgzsh/tables/tables_17_2.jpg", "caption": "Table 1: The results of various BERT models.", "description": "This table presents the results of various BERT models on several downstream tasks, comparing the performance of Vanilla BERT, FFN-Wider BERT, and FFN-Wider BERT with CEA (Combination Enhanced Architecture).  The tasks include out-of-distribution language modeling on the Pile dataset, and fine-tuning on GLUE and SuperGLUE benchmarks.  The results demonstrate the impact of the FFN-Wider architecture and the effectiveness of the proposed CEA modification. ", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_17_3.jpg", "caption": "Table 8: Fine-tuning results on the development set of SuperGLUE Benchmark of various BERT models (Pre-training Performance Alignment).", "description": "This table presents the fine-tuning results on the SuperGLUE benchmark for various BERT models. The results are categorized by model type (Vanilla BERT, FFN-Wider BERT, and FFN-Wider BERT with CEA), and performance metrics are shown for eight different tasks: BoolQ, CB, COPA, MultiRC, WiC, ReCoRD, WSC, and RTE.  The pre-training performance was aligned across models for fair comparison. This comparison helps analyze how the FFN-Wider architecture affects performance and the improvement brought by the proposed CEA.", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_17_4.jpg", "caption": "Table 9: Fine-tuning results on multiple other tasks of various BERT models (Pre-training Performance Alignment).", "description": "This table presents the fine-tuning results on several downstream tasks for various BERT models.  The models were trained with the pre-training performance alignment scheme.  The results showcase the performance of Vanilla BERT, FFN-Wider BERT, and FFN-Wider BERT with Combination Enhanced Architecture (CEA). The tasks include HellaSwag, PIQA, WinoGrande, OpenBookQA, ARC Easy, and ARC Challenge, providing a comprehensive evaluation of model performance across diverse tasks.", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_18_1.jpg", "caption": "Table 10: Out-of-distribution language modeling results on the development set of Pile of various GPT models (Pre-training Performance Alignment).", "description": "This table shows the out-of-distribution language modeling performance on the Pile development set for various GPT models.  The models are categorized by size (H=128 and H=768), and include vanilla GPT, FFN-Wider GPT, and FFN-Wider GPT with the Combination Enhanced Architecture (CEA).  The results are presented to demonstrate the impact of the FFN-Wider architecture and the effectiveness of CEA in mitigating the negative effects on out-of-distribution performance. The pre-training performance is aligned for fair comparison.", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_18_2.jpg", "caption": "Table 3: The results of Vanilla GPT 1.3B, Vanilla MoE 14B and MoE 14B w/ CEA. \u2020 indicates the test set drawn from the same distribution as the pre-training data.", "description": "This table presents the results of three different models: Vanilla GPT 1.3B, Vanilla MoE 14B, and MoE 14B with the Combination Enhanced Architecture (CEA).  It compares their performance across various metrics, including loss on different subsets of the SlimPajama dataset, as well as few-shot learning performance on several downstream tasks like LAMBADA, MMLU, and OpenBookQA. The results demonstrate the improvement in base capabilities achieved by incorporating the CEA into the MoE model.", "section": "7 From FFN-Wider Transformers to MoE Transformers"}, {"figure_path": "67tRrjgzsh/tables/tables_19_1.jpg", "caption": "Table 12: Out-of-distribution language modeling results on the development set of Pile of various BERT models (Pre-training Steps Alignment & Pre-training Performance Alignment).", "description": "This table presents the out-of-distribution language modeling results on the Pile development set for various BERT models.  The results are broken down by model type (Vanilla BERT, FFN-Wider BERT, and FFN-Wider BERT with CEA), hidden dimension size (H=128 and H=768), and the specific subset of the Pile dataset being evaluated.  The table shows that the FFN-Wider BERT models with the Combination Enhanced Architecture (CEA) generally improve upon the performance of FFN-Wider BERT models without CEA, often approaching the performance of vanilla BERT models in some cases. The results highlight the positive impact of CEA on improving the base capabilities of FFN-Wider Transformer models, especially when both pre-training performance and steps are aligned.", "section": "I Detailed Results (Extra Setting: Pre-training Performance Alignment & Pre-training Steps Alignment)"}, {"figure_path": "67tRrjgzsh/tables/tables_19_2.jpg", "caption": "Table 1: The results of various BERT models.", "description": "This table presents the performance comparison of different BERT models across various tasks, including out-of-distribution (OOD) language modeling on the Pile dataset, and fine-tuning performance on GLUE and SuperGLUE benchmarks.  It compares the vanilla BERT model, the FFN-Wider BERT model (which has wider FFN layers), and the FFN-Wider BERT model with the proposed Combination Enhanced Architecture (CEA). The results show that CEA improves the FFN-Wider BERT's performance, often reaching or surpassing the vanilla BERT's scores.", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_19_3.jpg", "caption": "Table 8: Fine-tuning results on the development set of SuperGLUE Benchmark of various BERT models (Pre-training Performance Alignment).", "description": "This table presents the results of fine-tuning various BERT models on the SuperGLUE benchmark.  The models were pre-trained with performance alignment, meaning their pre-training performance was matched before the fine-tuning task.  The table shows the performance (accuracy) of each model on various subtasks of SuperGLUE, including BoolQ, CB, COPA, MultiRC, WiC, ReCoRD, WSC, and RTE.  The results highlight the impact of the FFN-Wider architecture and its improvement (CEA) on downstream task performance.", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_19_4.jpg", "caption": "Table 9: Fine-tuning results on multiple other tasks of various BERT models (Pre-training Performance Alignment).", "description": "This table presents the fine-tuning results of various BERT models on multiple downstream tasks.  The models are compared based on their performance after pre-training with the same level of performance on the language modeling task. The results include metrics such as accuracy or precision, and showcase the impact of the FFN-Wider architecture and the proposed Combination Enhanced Architecture (CEA) on model generalization ability across different tasks.", "section": "6 Combination Enhanced Architecture"}, {"figure_path": "67tRrjgzsh/tables/tables_20_1.jpg", "caption": "Table 10: Out-of-distribution language modeling results on the development set of Pile of various GPT models (Pre-training Performance Alignment).", "description": "This table shows the out-of-distribution language modeling performance on the Pile development set for various GPT models.  The models are grouped by size (H=128 and H=768), and further categorized by model type: Vanilla GPT, FFN-Wider GPT, and FFN-Wider GPT with the Combination Enhanced Architecture (CEA).  The results demonstrate the impact of the FFN-Wider architecture and the effectiveness of the CEA modification on improving out-of-distribution generalization.", "section": "Detailed Results (Main Setting: Pre-training Performance Alignment)"}, {"figure_path": "67tRrjgzsh/tables/tables_20_2.jpg", "caption": "Table 3: The results of Vanilla GPT 1.3B, Vanilla MoE 14B and MoE 14B w/ CEA. \u2020 indicates the test set drawn from the same distribution as the pre-training data.", "description": "This table presents the results of three different models: Vanilla GPT 1.3B, Vanilla MoE 14B, and MoE 14B with Combination Enhanced Architecture (CEA). It compares their performance across various metrics, including loss on different datasets (SlimPajama), perplexity and accuracy on LAMBADA, and accuracy on other benchmarks (MMLU, OpenBookQA, etc.).  The results highlight the improvement achieved by incorporating CEA into the MoE model, demonstrating its effectiveness.", "section": "7 From FFN-Wider Transformers to MoE Transformers"}]