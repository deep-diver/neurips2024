[{"figure_path": "67tRrjgzsh/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration showing that: 1) the synchronous improvement in model base capability as the contribution ratio of the Outer-FFN layer (a transformation function) decreases, that is, the contribution ratio of the MHA layer (a combination function) increases. This reveals a key factor affecting model's base capabilities. 2) Combination Enhanced Architecture (CEA) was designed based on this factor and applied to MoE models, resulting in an improvement in base capability.", "description": "This figure illustrates the relationship between the contribution ratio of the Outer-FFN and MHA layers in a transformer model and its base capabilities.  The left panel shows the Combination Adjustable Architecture (CAA), a modified transformer architecture used in the experiments.  The middle panel (b) shows that as the contribution ratio of the Outer-FFN layer decreases (meaning the contribution ratio of the MHA layer increases), there's a corresponding increase in model base capability, as measured by out-of-distribution language modeling performance.  The right panel (c) demonstrates the effectiveness of the Combination Enhanced Architecture (CEA) on a Mixture of Experts (MoE) Transformer model, showing improved base capabilities compared to the standard MoE model.", "section": "4 Why FFN-Wider Transformers Have Worse Base Capabilities?"}, {"figure_path": "67tRrjgzsh/figures/figures_3_1.jpg", "caption": "Figure 2: Comparison of the base capabilities between FFN-Wider and Vanilla Transformers.", "description": "This figure compares the base capabilities (out-of-distribution language modeling, transfer learning, and few-shot learning) of FFN-Wider Transformers and Vanilla Transformers.  The models were trained with similar pre-training performance (aligned pre-training loss). The results show that under similar pre-training performance, FFN-Wider models demonstrate a significant decline in base capabilities compared to Vanilla Transformers. This highlights the influence of architecture on the base capabilities of pre-trained language models, even when pre-training performance is similar.", "section": "3 FFN-Wider Transformers vs. Vanilla Transformers"}, {"figure_path": "67tRrjgzsh/figures/figures_5_1.jpg", "caption": "Figure 3: Contribution ratio analysis based on Mutual Information(MI) for various transformers.", "description": "This figure shows the contribution ratio analysis based on Mutual Information (MI) for various transformers.  It displays the cumulative MI increment contributions of the Block, MHA layer, and FFN layer for four different models: Vanilla BERT (H=128), FFN-Wider BERT (H=128), Vanilla GPT (H=128), and FFN-Wider GPT (H=128). The graphs visually represent how the contribution of each layer changes across different layers of the transformer network, highlighting the differences between vanilla and FFN-Wider models. This analysis helps in understanding the impact of the FFN-Wider architecture on the contribution ratio of the MHA (combination function) compared to the FFN (transformation function).", "section": "4.2 Contribution Ratio Analysis"}, {"figure_path": "67tRrjgzsh/figures/figures_5_2.jpg", "caption": "Figure 4: Contribution ratio analysis based on Token Prediction (TP) for various transformers.", "description": "This figure shows the contribution ratio of the FFN layer (Feed-Forward Network) to the overall model accuracy, as determined by the Token Prediction method.  The x-axis represents different models (BERT and GPT with different hidden dimensions, H=128 and H=768). The y-axis shows the percentage of the total accuracy increase attributed to the FFN layer. The bars are grouped in pairs: vanilla models (blue) and FFN-Wider models (orange).  The figure illustrates that the FFN-Wider models consistently have a higher FFN contribution ratio than their vanilla counterparts. This supports the hypothesis that widening the FFN layer disproportionately increases its contribution, potentially at the expense of the MHA (Multi-Head Attention) layer's contribution and the model's overall base capabilities.", "section": "4.2.2 Token Prediction"}, {"figure_path": "67tRrjgzsh/figures/figures_6_1.jpg", "caption": "Figure 5: Overview of our proposed Combination Adjustable Architecture (CAA).", "description": "This figure illustrates the Combination Adjustable Architecture (CAA), a novel architecture designed to investigate the impact of the contribution ratios of transformation and combination functions on the base capabilities of pre-trained language models.  The CAA modifies the FFN-Wider Transformer architecture by splitting the wider FFN layer into two parts: an Outer-FFN (transformation function) and an Inner-FFN (integrated into the MHA layer, serving as part of the combination function).  A direct pathway within the MHA bypasses the Inner-FFN, allowing for a controlled adjustment of the contribution ratio of each function, ultimately revealing how this balance impacts model performance.", "section": "5 Combination Adjustable Architecture"}, {"figure_path": "67tRrjgzsh/figures/figures_6_2.jpg", "caption": "Figure 1: Illustration showing that: 1) the synchronous improvement in model base capability as the contribution ratio of the Outer-FFN layer (a transformation function) decreases, that is, the contribution ratio of the MHA layer (a combination function) increases. This reveals a key factor affecting model's base capabilities. 2) Combination Enhanced Architecture (CEA) was designed based on this factor and applied to MoE models, resulting in an improvement in base capability.", "description": "This figure shows the relationship between the contribution ratio of different layers in Transformer models and their base capabilities.  Specifically, it demonstrates that increasing the contribution of the Multi-Head Attention (MHA) layer, a combination function, improves base capabilities, while increasing the contribution of the Feed-Forward Network (FFN) layer, a transformation function, reduces them. The Combination Enhanced Architecture (CEA) is introduced as a method to improve base capabilities by adjusting the contribution ratio of these layers, and its effectiveness is shown on both FFN-Wider and MoE Transformer models.", "section": "1 Introduction"}]