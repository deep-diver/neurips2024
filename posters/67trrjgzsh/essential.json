{"importance": "This paper is crucial because it identifies a previously overlooked architectural factor affecting the base capabilities of pre-trained language models.  It challenges the prevalent focus on scaling and offers a novel architectural enhancement (CEA) to improve model performance, **providing valuable insights for researchers to design more efficient and effective models.** Its application to MoE Transformers showcases its broader significance and opens new avenues for architectural improvement.", "summary": "Pre-trained language models' base capabilities are significantly influenced by architecture, not just scale; a novel Combination Enhanced Architecture (CEA) improves performance by addressing FFN-Wider Transformer limitations.", "takeaways": ["Architecture significantly impacts pre-trained language models' base capabilities, independent of scale.", "The contribution ratio of Multi-Head Attention (combination function) is a key factor influencing base capabilities.", "Combination Enhanced Architecture (CEA) successfully improves base capabilities in FFN-Wider and MoE Transformers."], "tldr": "Pre-trained language models (PLMs) have shown impressive capabilities, but their performance is not solely determined by scale.  **Existing research primarily focuses on scaling up models, ignoring the significant influence of architecture.**  A notable issue is the decline in base capabilities observed in certain architectures like FFN-Wider Transformers, despite comparable pre-training performance to standard models.  This highlights a gap in our understanding of how architecture impacts PLMs' effectiveness.\nThis paper investigates the architectural influence on PLMs' base capabilities.  **It introduces a novel Combination Enhanced Architecture (CEA) designed to address the performance drop in FFN-Wider Transformers.** By analyzing the contribution ratio of different layers, the researchers identified Multi-Head Attention (MHA) as a critical component affecting base capabilities.  CEA successfully improves the base capabilities by enhancing the MHA layer's contribution.  Moreover, the effectiveness of CEA is demonstrated by applying it to Mixture of Experts (MoE) transformers, showing substantial performance improvements. This work provides critical insights into architectural design for PLMs.", "affiliation": "Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "67tRrjgzsh/podcast.wav"}