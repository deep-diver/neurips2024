[{"figure_path": "6292sp7HiE/tables/tables_6_1.jpg", "caption": "Table 1: Accuracy of different one-shot FL methods over three datasets with ConvNet and ResNet-18. We vary the \u03b1 = {0.1, 0.3, 0.5} to simulate different levels of data heterogeneity for Tiny-ImageNet and Imagenette and use pre-defined splits for OpenImage. \"Central\" means that clients send all their local data to the server for centralized training, representing the upper bound of model performance.", "description": "This table presents the accuracy results of several one-shot federated learning (FL) methods on three image datasets (ImageNette, Tiny-ImageNet, and OpenImage) using two different model architectures (ConvNet and ResNet-18).  The level of data heterogeneity is varied (\u03b1 = 0.1, 0.3, 0.5), simulating different degrees of non-IID data distribution across clients.  The \"Central\" column provides a baseline representing the performance achievable with centralized training (all data on the server). The table compares FedAVG, F-DAFL, DENSE, Co-Boosting, and the proposed FedSD2C methods.", "section": "4.2 Evaluation Results"}, {"figure_path": "6292sp7HiE/tables/tables_7_1.jpg", "caption": "Table 2: Accuracy, PSNR and SSIM of FedSD2C combining different privacy-enhanced techniques. Laplace and Gaussian indicate adding corresponding noise into synthetic distillates without Fourier transform initialization. FedMix denotes averaging two real samples from Core-Set to synthesize data. \"-\" indicates no privacy-enhanced technique is combined.", "description": "This table presents a comparison of the performance (accuracy, PSNR, SSIM) of the FedSD2C model when different privacy-enhancing techniques are used. It shows how the model's accuracy, peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM) are affected by adding noise (Laplace and Gaussian) to the synthetic distillates, and by averaging real samples from the Core-Set (FedMix). The baseline is FedSD2C without any additional privacy-enhancing techniques.", "section": "4.3 Privacy Evaluation"}, {"figure_path": "6292sp7HiE/tables/tables_8_1.jpg", "caption": "Table 1: Accuracy of different one-shot FL methods over three datasets with ConvNet and ResNet-18. We vary the \u03b1 = {0.1, 0.3, 0.5} to simulate different levels of data heterogeneity for Tiny-ImageNet and Imagenette and use pre-defined splits for OpenImage. \"Central\" means that clients send all their local data to the server for centralized training, representing the upper bound of model performance.", "description": "This table presents the accuracy results of several one-shot federated learning methods on three image datasets (ImageNette, Tiny-ImageNet, and OpenImage) using two different model architectures (ConvNet and ResNet-18).  The accuracy is shown for different levels of data heterogeneity (\u03b1 = 0.1, 0.3, 0.5).  A \"Central\" training result is included as an upper performance bound, representing the ideal scenario where all client data is available to the server. The table allows for a comparison of the performance of different one-shot methods under varying conditions.", "section": "4.2 Evaluation Results"}, {"figure_path": "6292sp7HiE/tables/tables_9_1.jpg", "caption": "Table 4: Accuracy on Tiny-ImageNet with different client amounts.", "description": "This table shows the accuracy achieved by four different one-shot federated learning methods (F-DAFL, DENSE, Co-Boosting, and FedSD2C) on the Tiny-ImageNet dataset.  The results are broken down by the number of clients (20, 50, and 100) and the level of data heterogeneity (\u03b1 = 0.1, 0.3, 0.5).  Higher accuracy values indicate better performance of the model.", "section": "4.3.4 Impact of Client Scales"}, {"figure_path": "6292sp7HiE/tables/tables_15_1.jpg", "caption": "Table S1: Performance on CIFAR-10 with ResNet-18.", "description": "This table presents the performance comparison of different one-shot federated learning methods (DENSE, Co-Boosting, and FedSD2C) on the CIFAR-10 dataset using ResNet-18 as the model architecture.  The results are categorized by different levels of data heterogeneity (\u03b1 = 0.1, 0.3, 0.5) and include the communication cost (Comm.) for each method.  The table shows that FedSD2C with a larger number of images per class (ipc = 500) achieves better performance compared to other methods,  especially with lower heterogeneity levels.", "section": "C.1 Additional datasets"}, {"figure_path": "6292sp7HiE/tables/tables_15_2.jpg", "caption": "Table S2: Performance on COVID-FL datasets with ResNet-18.", "description": "This table presents the accuracy results of DENSE, Co-Boosting, and FedSD2C on the COVID-FL dataset using ResNet-18 as the model architecture.  The results are broken down by different levels of data heterogeneity (\u03b1 = 0.1, \u03b1 = 0.3, \u03b1 = 0.5).  This allows for a comparison of the methods' performance under varying levels of data non-IIDness.", "section": "C.2 Membership Inference Attack"}, {"figure_path": "6292sp7HiE/tables/tables_15_3.jpg", "caption": "Table 2: Accuracy, PSNR and SSIM of FedSD2C combining different privacy-enhanced techniques. Laplace and Gaussian indicate adding corresponding noise into synthetic distillates without Fourier transform initialization. FedMix denotes averaging two real samples from Core-Set to synthesize data. \"-\" indicates no privacy-enhanced technique is combined.", "description": "This table compares the performance (accuracy, PSNR, SSIM) of FedSD2C using various privacy-preserving techniques.  It shows that incorporating Fourier transform perturbation strikes a balance between privacy and performance, outperforming methods that add noise or average real samples.", "section": "4.3 Privacy Evaluation"}, {"figure_path": "6292sp7HiE/tables/tables_15_4.jpg", "caption": "Table 2: Accuracy, PSNR and SSIM of FedSD2C combining different privacy-enhanced techniques. Laplace and Gaussian indicate adding corresponding noise into synthetic distillates without Fourier transform initialization. FedMix denotes averaging two real samples from Core-Set to synthesize data. \"-\" indicates no privacy-enhanced technique is combined.", "description": "This table compares the performance (accuracy, PSNR, and SSIM) of FedSD2C using different privacy-preserving techniques. It shows that while methods like FedMix offer stronger privacy (lower PSNR and SSIM), they come at a cost of reduced accuracy.  The Fourier Transform perturbation method used in FedSD2C shows a good balance between privacy and performance.", "section": "4.3.1 Privacy Evaluation"}, {"figure_path": "6292sp7HiE/tables/tables_16_1.jpg", "caption": "Table 5: Performance of different selection strategy. Core-Set denotes that clients directly upload their local Core-Set, which leads to privacy issue. FedSD2C w/ random selection denotes replacing V-information-based Core-Set selection with random selection.", "description": "This table compares the performance of three different Core-Set selection methods: directly uploading the Core-Set, using random selection, and using the proposed V-information based selection method within the FedSD2C framework.  It highlights the impact of the proposed method on accuracy and the trade-off between accuracy and privacy/communication efficiency.", "section": "4.2 Evaluation Results"}, {"figure_path": "6292sp7HiE/tables/tables_16_2.jpg", "caption": "Table S6: Performance of integrating DP-SGD", "description": "This table presents the results of experiments evaluating the performance of FedSD2C when integrated with Differential Privacy using DP-SGD, demonstrating the trade-off between privacy and accuracy at different privacy levels.  The performance (accuracy) of the model is shown for various values of epsilon (\u03b5), representing the privacy budget.  A higher epsilon value indicates less privacy protection and potentially higher accuracy.  The table showcases how increasing the epsilon value corresponds to a higher accuracy, indicating the tradeoff inherent in balancing privacy with performance.", "section": "C.5 Integrating with Differential Privacy"}]