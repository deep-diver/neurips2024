[{"figure_path": "6292sp7HiE/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of issues in one-shot FL based on DFKD: (1) Information loss occurs during the transfer from local data to the model and from the model back to the inversed data. (2) t-SNE plots of feature distributions of data generated by DENSE(left \u25b2), Co-Boosting(middle), and our FedSD2C(right). We randomly select five different classes (indicated by different colors) of real and synthetic data from Tiny-ImageNet. Bad samples are data generated by the DFKD-based method that deviates from the distribution of local real data.", "description": "This figure illustrates two main challenges in one-shot federated learning (FL) using data-free knowledge distillation (DFKD).  The top half shows a two-tier information loss: information is lost when local data is used to train a local model and again when the model's knowledge is transferred to the server.  The bottom half uses t-SNE plots to visually compare the feature distributions of synthetic data generated by three different methods (DENSE, Co-Boosting, and FedSD2C) against real data.  It highlights how FedSD2C generates synthetic data that better matches the distribution of real data, unlike the other methods which produce some \"bad samples\" deviating significantly from the real data distribution.", "section": "1 Introduction"}, {"figure_path": "6292sp7HiE/figures/figures_3_1.jpg", "caption": "Figure 2: Framework of proposed FedSD2C.", "description": "The framework illustrates the proposed FedSD2C's process.  Each client independently selects a core-set from its local data. This core-set undergoes distillate synthesis, involving Fourier transform perturbation to enhance privacy and a pre-trained autoencoder to generate compact, informative distillates that minimize information loss. These distillates are then uploaded to the server, which uses the pre-trained autoencoder's decoder to reconstruct the information and train the global server model. The figure clearly shows the two-stage distillation process and the role of the pre-trained autoencoder as a distiller in improving efficiency and privacy.", "section": "3 Methodology"}, {"figure_path": "6292sp7HiE/figures/figures_8_1.jpg", "caption": "Figure 3: (a) Experiments on the medical image data domain. Adopting pre-trained Autoencoders on other data domains can reduce performance. However, this can be mitigated by increasing Tsyn. (b) Experiments of FedSD2C with randomly initialized downsampling and upsampling modules (blue line) compared to pre-trained Autoencoders (orange line) on ImageNette. Without pre-trained knowledge, FedSD2C requires a higher Tsyn for distillate synthesis but can still achieve comparable results. ResNet-18 is used for both experiments.", "description": "This figure shows the results of experiments conducted on a medical image dataset (COVID-FL) and ImageNette to evaluate the impact of pre-trained autoencoders on the performance of FedSD2C.  Figure 3a demonstrates that using pre-trained autoencoders on a different data domain (medical images) initially reduces performance but this can be improved by increasing the number of synthesis iterations (Tsyn). Figure 3b compares the performance of FedSD2C using pre-trained autoencoders versus randomly initialized downsampling/upsampling modules on ImageNette, illustrating that while pre-trained autoencoders accelerate convergence, FedSD2C can still achieve comparable results with randomly initialized modules by increasing Tsyn.", "section": "4.3.3 Impact of Pre-trained Autoencoder"}, {"figure_path": "6292sp7HiE/figures/figures_8_2.jpg", "caption": "Figure 3: (a) Experiments on the medical image data domain. Adopting pre-trained Autoencoders on other data domains can reduce performance. However, this can be mitigated by increasing Tsyn. (b) Experiments of FedSD2C with randomly initialized downsampling and upsampling modules (blue line) compared to pre-trained Autoencoders (orange line) on ImageNette. Without pre-trained knowledge, FedSD2C requires a higher Tsyn for distillate synthesis but can still achieve comparable results. ResNet-18 is used for both experiments.", "description": "This figure shows the results of experiments conducted on a medical image dataset (COVID-FL) and ImageNette to evaluate the impact of pre-trained autoencoders on the performance of FedSD2C.  The left subplot (a) demonstrates that using pre-trained autoencoders on a different data domain (medical images) initially reduces performance, but increasing the number of synthesis iterations (Tsyn) mitigates this. The right subplot (b) compares the performance of FedSD2C with pre-trained autoencoders versus using randomly initialized modules, revealing that while pre-trained autoencoders lead to faster convergence,  FedSD2C can still achieve comparable results with randomly initialized modules given sufficient iterations.", "section": "4.3.3 Impact of Pre-trained Autoencoder"}, {"figure_path": "6292sp7HiE/figures/figures_16_1.jpg", "caption": "Figure 1: Illustration of issues in one-shot FL based on DFKD: (1) Information loss occurs during the transfer from local data to the model and from the model back to the inversed data. (2) t-SNE plots of feature distributions of data generated by DENSE(left \u25b2), Co-Boosting(middle), and our FedSD2C(right). We randomly select five different classes (indicated by different colors) of real and synthetic data from Tiny-ImageNet. Bad samples are data generated by the DFKD-based method that deviates from the distribution of local real data.", "description": "This figure illustrates two key challenges in one-shot federated learning (FL) using data-free knowledge distillation (DFKD). The first part shows a two-tier information loss: data to model, then model to synthetic data.  The second part uses t-SNE plots to visualize the difference in feature distributions between synthetic data generated by three different methods (DENSE, Co-Boosting, and the proposed FedSD2C) and real data from Tiny-ImageNet.  It highlights how FedSD2C better generates synthetic data that closely matches the real data distribution, addressing the issue of low-quality synthetic data common in DFKD.", "section": "1 Introduction"}]