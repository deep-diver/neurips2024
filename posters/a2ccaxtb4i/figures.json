[{"figure_path": "a2ccaXTb4I/figures/figures_1_1.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure shows a qualitative comparison of the proposed method against the GarmentTracking method.  GarmentTracking uses the ground truth mesh as initialization. The figure displays the results for sequences of garment folding and flattening operations, illustrating the superior reconstruction accuracy and detail preservation of the proposed method compared to GarmentTracking across various manipulation stages. The top row showcases the input point clouds (green) overlaid on the ground truth meshes (gray).  The bottom row presents the reconstruction results from the proposed method.  The visual comparison highlights the ability of the novel approach to faithfully recover garment meshes from incomplete point cloud data, accurately capturing complex shape deformations and self-occlusions that are not well-represented by GarmentTracking.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_2_1.jpg", "caption": "Figure 2: Our framework. Given a point cloud, we first map it to UV space to obtain sparse UV maps M and panel masks \u00d5. We recover complete UV maps M and panel masks O from them using ISP and a deformation prior, enabling the reconstruction of the deformed garment's 3D mesh.", "description": "This figure illustrates the overall framework of the proposed method. It starts by mapping a partial point cloud of a garment to UV space using a trained UV mapper. This results in sparse UV maps and panel masks. Then, it recovers complete UV maps and panel masks by fitting the Implicit Sewing Patterns (ISP) model and incorporating a learned deformation prior (a diffusion model). Finally, it reconstructs the complete 3D mesh of the garment using the recovered UV maps and panel masks. The deformation prior is crucial for handling garments that are not worn and can assume arbitrary shapes.", "section": "3 Method"}, {"figure_path": "a2ccaXTb4I/figures/figures_5_1.jpg", "caption": "Figure 3: The projected sparse masks \u0150 and UV maps M of the point clouds with (a) the maximum volume and (b) the minimum volume. The point clouds are color coded by their 3D positions.", "description": "This figure shows the sparse masks and UV maps generated from point clouds with maximum and minimum volumes. The point clouds are color-coded to indicate their 3D positions, providing visual context for the sparsity and distribution of the projected data in UV space.  It illustrates the input data to the reconstruction pipeline, highlighting the challenges of working with incomplete point cloud data.", "section": "3 Method"}, {"figure_path": "a2ccaXTb4I/figures/figures_5_2.jpg", "caption": "Figure 2: Our framework. Given a point cloud, we first map it to UV space to obtain sparse UV maps M and panel masks \u00d5. We recover complete UV maps M and panel masks O from them using ISP and a deformation prior, enabling the reconstruction of the deformed garment's 3D mesh.", "description": "This figure illustrates the overall framework of the proposed method.  It starts with a point cloud of a garment. The point cloud is then mapped into UV space using a trained UV mapper, resulting in sparse UV maps and panel masks.  These sparse representations are then used as input to a reverse diffusion process guided by the learned deformation prior and Implicit Sewing Patterns (ISP) model. This process generates complete UV maps and panel masks. Finally, a 3D mesh of the garment is reconstructed using the ISP model.", "section": "3 Method"}, {"figure_path": "a2ccaXTb4I/figures/figures_7_1.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure shows a qualitative comparison of the proposed method's garment reconstruction results against those from GarmentTracking, a state-of-the-art method that leverages the ground truth garment mesh as initialization. The comparison is presented for both folding and flattening sequences across four garment categories: pants, tops, shirts, and skirts.  For each category and sequence type, input point clouds, ground truth meshes, results from the proposed method, and results from GarmentTracking are displayed. This allows for a visual assessment of the accuracy and detail captured by the different methods in handling various levels of deformation and self-occlusion. The visual comparison supplements the quantitative results presented in the paper, providing further insights into the method's strengths and limitations.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_8_1.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure shows a qualitative comparison between the results of the proposed method and GarmentTracking [2] (initialized with ground truth meshes).  The top two rows display results for the \"Folding\" sequence, and the bottom two rows display results for the \"Flattening\" sequence. Each row shows results for a specific garment type. The results of GarmentTracking, the proposed method, and the ground truth are shown for comparison.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_8_2.jpg", "caption": "Figure 7: Real-world evaluation. (a) The captured image and point cloud of the pants. (b) Our reconstructed results.", "description": "This figure shows the results of applying the proposed method to real-world data.  Part (a) displays the input: a captured image and the corresponding point cloud of a pair of pants in different folded configurations. Part (b) presents the 3D reconstruction results generated by the algorithm, demonstrating its ability to reconstruct the shape of real-world garments from captured point clouds. The results visualize the model's performance in reconstructing the geometry and folds of the pants, even with real-world challenges such as varying lighting and complex folds.", "section": "4.4 Evaluation on Real-World Data"}, {"figure_path": "a2ccaXTb4I/figures/figures_13_1.jpg", "caption": "Figure 12: The comparison of reconstructed results for the folding sequence of Skirt.", "description": "This figure displays a qualitative comparison of the 3D garment reconstruction results for a skirt undergoing a folding sequence.  It shows the ground truth model (GT), the results obtained using the proposed method (Ours), and the results obtained using the GarmentTracking method (GarmentTracking). By visually comparing the three sets of reconstructed skirt models, the differences in accuracy and detail between the methods can be observed. The figure illustrates the ability of the proposed approach to accurately reconstruct the complex shapes and folds of a manipulated garment compared to an existing method.", "section": "A.1 Qualitative Comparisons"}, {"figure_path": "a2ccaXTb4I/figures/figures_14_1.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure displays a qualitative comparison of the 3D garment reconstruction results obtained using three different methods: the proposed method, GarmentTracking (initialized with ground truth), and the ground truth itself.  The comparison is shown for both \"Folding\" and \"Flattening\" sequences of garment manipulations, visualizing the accuracy and realism of the different approaches in handling complex deformations and self-occlusions. The results demonstrate the superior performance of the proposed method in capturing accurate shapes and deformations compared to GarmentTracking.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_14_2.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure compares the 3D garment reconstruction results of the proposed method with those of the GarmentTracking method, using ground truth meshes as initialization.  The comparison is shown for both \"Folding\" and \"Flattening\" sequences of garment manipulation.  The top row shows the ground truth (GT) reconstructions, while the middle row presents the results from the proposed method, and the bottom row illustrates the results obtained using GarmentTracking.  By visually comparing the three rows for each sequence, one can assess the accuracy and detail preserved in the 3D garment reconstruction by each approach.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_14_3.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure displays a qualitative comparison of the proposed method's garment reconstruction results against the GarmentTracking method (using ground truth initialization) on the VR-Folding dataset, specifically focusing on the \"Folding\" and \"Flattening\" sequences.  The comparison showcases the differences in reconstructed garment shapes, demonstrating the accuracy and detail preserved by the proposed method in comparison to the baseline, highlighting the superior ability to handle complex deformations and occlusions.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_15_1.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure displays a qualitative comparison of the 3D garment reconstruction results obtained using the proposed method and the GarmentTracking method (initialized with ground truth meshes) on the VR-Folding dataset.  The comparison is shown for both folding and flattening sequences of the garments.  The figure aims to visually demonstrate the superior accuracy and detail preservation of the proposed method compared to the GarmentTracking method.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_15_2.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure displays a qualitative comparison between the proposed method and the GarmentTracking method.  The results for both folding and flattening sequences are shown.  GarmentTracking uses the ground truth mesh as initialization, while the proposed method is shown to recover garment meshes more accurately and faithfully reflect the actual shape and deformation, even in complex scenarios.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_15_3.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure compares the qualitative results of the proposed method against the GarmentTracking method for both folding and flattening sequences.  The top row shows the ground truth results. The middle row shows the results obtained using the proposed method, and the bottom row presents the results from the GarmentTracking method, which uses the ground truth mesh as initialization. The comparison highlights the superior accuracy and fidelity of the proposed method in reconstructing garment meshes in various states of deformation.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_16_1.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure provides a visual comparison of the 3D garment reconstruction results obtained using three different methods: the proposed method, GarmentTracking initialized with ground truth meshes, and ground truth.  It showcases multiple frames from both the folding and flattening sequences for several garments (pants, shirt, top, skirt). The visual comparison highlights the superior accuracy and detail preservation of the proposed method compared to GarmentTracking, particularly in capturing intricate folds and deformations.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_16_2.jpg", "caption": "Figure 7: Real-world evaluation. (a) The captured image and point cloud of the pants. (b) Our reconstructed results.", "description": "This figure shows the results of applying the proposed method to real-world data.  Part (a) displays the original captured image of a pair of pants and the corresponding point cloud generated from it. Part (b) presents the 3D reconstructions of the pants obtained using the proposed method, demonstrating its ability to handle real-world scenarios, including both flat and folded garments. ", "section": "4.4 Evaluation on Real-World Data"}, {"figure_path": "a2ccaXTb4I/figures/figures_17_1.jpg", "caption": "Figure 17: Quantitative results of (a) using different numbers of points as input and (b) under different noise levels with 4000 input points. Blue: the Correspondence Distance  Dcr. Red: the Chamfer Distance Def.", "description": "This figure shows the robustness of the proposed method.  (a) tests the influence of the number of input points on reconstruction accuracy, showing that even with fewer points, the accuracy remains relatively high. (b) evaluates the effect of adding Gaussian noise to the input points, demonstrating that the method remains relatively accurate even with considerable noise.", "section": "A.5 Robustness"}, {"figure_path": "a2ccaXTb4I/figures/figures_18_1.jpg", "caption": "Figure 6: Qualitative comparisons of our method to GarmentTracking (initialized with ground truth meshes) on VR-Folding dataset for the sequences of Folding and Flattening.", "description": "This figure presents a qualitative comparison of the 3D garment reconstruction results obtained using the proposed method and the GarmentTracking method (initialized with ground truth meshes) for both folding and flattening sequences.  The comparison highlights the superior accuracy and detail preservation of the proposed method in reconstructing the complex shapes and deformations of garments under various manipulation conditions.", "section": "4.3 Qualitative Results"}, {"figure_path": "a2ccaXTb4I/figures/figures_18_2.jpg", "caption": "Figure 19: The evaluation of the panel mask fitting results.", "description": "The figure shows the relationship between the volume of the input point cloud and the mean Intersection over Union (mIoU) between the ground truth panel masks and the masks fitted using equation 10 from the paper.  The x-axis represents the normalized volume of the point cloud, and the y-axis shows the mIoU.  It demonstrates that a larger point cloud volume generally leads to better fitting results, as indicated by higher mIoU values.", "section": "A.7 Evaluation of Panel Mask Fitting"}, {"figure_path": "a2ccaXTb4I/figures/figures_18_3.jpg", "caption": "Figure 2: Our framework. Given a point cloud, we first map it to UV space to obtain sparse UV maps M and panel masks \u00d5. We recover complete UV maps M and panel masks O from them using ISP and a deformation prior, enabling the reconstruction of the deformed garment's 3D mesh.", "description": "This figure illustrates the overall framework of the proposed method for 3D garment reconstruction.  It starts with a point cloud as input, maps the 3D points to the 2D UV space using a UV mapper to get sparse UV maps and panel masks. Then, it recovers complete UV maps and panel masks using ISP (Implicit Sewing Patterns) and a deformation prior. Finally, it reconstructs the 3D mesh of the deformed garment.", "section": "3 Method"}, {"figure_path": "a2ccaXTb4I/figures/figures_19_1.jpg", "caption": "Figure 2: Our framework. Given a point cloud, we first map it to UV space to obtain sparse UV maps M and panel masks \u00d5. We recover complete UV maps M and panel masks O from them using ISP and a deformation prior, enabling the reconstruction of the deformed garment's 3D mesh.", "description": "This figure illustrates the workflow of the proposed method for 3D garment reconstruction from a point cloud. The process starts by mapping the input point cloud to the UV space of the garment's panels using a UV mapper, resulting in sparse UV maps and panel masks. Then, using the Implicit Sewing Patterns (ISP) model and a learned deformation prior, the method recovers complete UV maps and panel masks. Finally, these complete maps are used to reconstruct the 3D mesh of the deformed garment.", "section": "3 Method"}]