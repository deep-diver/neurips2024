[{"figure_path": "XuWWq3gy7W/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of BitDelta. BitDelta applies 1-bit quantization to the weight delta between fine-tuned and base models. For each weight matrix, we quantize its delta as its sign bits and a trainable high-precision scale factor. The scale factor is initialized to achieve the best approximation error in L2 norm and further refined with a few distillation steps. BitDelta shows minimal degradation in model performance and reduces memory consumption in multi-tenancy serving by representing multiple fine-tuned models with a single high-precision base model and multiple 1-bit deltas.", "description": "This figure illustrates the core idea of BitDelta.  It shows how multiple fine-tuned models can be represented using a single high-precision base model and several 1-bit deltas. Each delta represents the difference between the fine-tuned model's weights and the base model's weights. By quantizing these deltas to only one bit, BitDelta significantly reduces memory usage and improves inference speed, especially in multi-tenant scenarios where many fine-tuned models need to be served concurrently.", "section": "1 Introduction"}, {"figure_path": "XuWWq3gy7W/figures/figures_1_2.jpg", "caption": "Figure 2: Cumulative Explained Variance (CEV) plot of a 4096 \u00d7 4096 weight delta between Llama 2-7B and Vicuna-7B v1.5. Deltas from full parameter fine-tuning are fairly high rank, making low-rank approximations difficult.", "description": "The figure shows the cumulative explained variance (CEV) plot for a 4096x4096 weight delta matrix between Llama 2-7B and Vicuna-7B v1.5.  The plot illustrates that the weight delta from full parameter fine-tuning has a high rank, making low-rank approximation techniques challenging for effective compression.  This observation supports the argument that fine-tuning adds relatively less new information, and emphasizes the potential for compressing the delta rather than the entire fine-tuned model.", "section": "1 Introduction"}, {"figure_path": "XuWWq3gy7W/figures/figures_7_1.jpg", "caption": "Figure 3: As the fidelity of \u0394 increases, the TruthfulQA scores of Llama 2-7B + \u0394 approaches that of Vicuna-7B v1.5.", "description": "This figure shows the result of an ablation study on the number of bits used to represent the weight delta (\u0394) in BitDelta.  The x-axis represents the number of bits used for quantization of the delta, ranging from 0 bits (no quantization of delta) to 7 bits.  The y-axis shows the TruthfulQA score, a metric measuring the model's performance on a question answering task. The plot shows that as the number of bits increases, the performance of the Llama 2-7B model with the quantized delta improves, approaching the performance of the fully fine-tuned Vicuna-7B v1.5 model. This demonstrates that even with a low-bit representation of the delta, the model can achieve comparable performance to a fully fine-tuned model, highlighting the effectiveness of BitDelta's quantization approach.", "section": "4.3 Latency Improvement"}, {"figure_path": "XuWWq3gy7W/figures/figures_8_1.jpg", "caption": "Figure 4: Decoding latency of a linear layer, as in Eqn. 6. Black: Shared base weight backbone Wbase X. Blue: Batched activation-product with B 1-bit deltas, as in BitDelta. Red: Batched activation-product with B low-rank deltas, as in S-LoRA. Left: Ablation over hidden size, assuming N = M and B = 1. Right: Ablation over batch size, assuming N = M = 4096.", "description": "This figure shows the decoding latency of a linear layer, comparing BitDelta and S-LORA against a naive method which computes the base weight backbone and deltas separately. The left panel shows ablation over hidden size (N=M, B=1), and the right panel shows ablation over batch size (N=M=4096).  BitDelta's performance is shown to scale efficiently with both hidden size and batch size, demonstrating its efficiency compared to the naive method. S-LORA shows a similar trend, but with some overhead at lower batch sizes.", "section": "4.3 Latency Improvement"}, {"figure_path": "XuWWq3gy7W/figures/figures_8_2.jpg", "caption": "Figure 5: Memory usage of Llama 2-7B, assuming each sequence in the batch has a length of 128. Blue: Memory usage of the naive method, separately storing B distinct fine-tuned models. Orange: Projected values for the naive method. Green: Memory usage of BitDelta. The naive forward pass succumbs to GPU memory issues at higher batch sizes.", "description": "This figure compares the GPU memory usage of three different approaches for serving multiple fine-tuned models with increasing batch size. The naive approach loads each model separately, quickly exceeding the GPU's capacity (OOM). BitDelta significantly reduces memory usage by employing a single base model and multiple compressed 1-bit deltas. The figure illustrates BitDelta's efficiency in handling large batch sizes where the naive approach fails due to memory limitations.", "section": "4.3 Latency Improvement"}, {"figure_path": "XuWWq3gy7W/figures/figures_8_3.jpg", "caption": "Figure 6: End-to-end decoding latency of Llama 2-7B. Blue: Naive forward pass with B distinct fine-tuned models. Orange: Projected values for the naive forward pass. Green: Batched forward pass with BitDelta. Gray: Batched forward pass with S-LORA. The naive forward pass succumbs to GPU memory issues at higher batch sizes.", "description": "The figure shows the end-to-end decoding latency of Llama 2-7B models under different methods (naive, BitDelta, and S-LoRA) and various batch sizes.  The naive approach, which processes each fine-tuned model separately, experiences significant latency increases and runs out of GPU memory at higher batch sizes. In contrast, BitDelta and S-LoRA, which share a common base model and process multiple deltas in batches, show significantly lower and more scalable latency.", "section": "4.3 Latency Improvement"}]