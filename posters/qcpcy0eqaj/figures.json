[{"figure_path": "qCpCy0EQAJ/figures/figures_2_1.jpg", "caption": "Figure 1: Schematics of proposed Dynamic Neural Regeneration (DNR) framework. Our framework utilizes a data-aware dynamic masking scheme to remove redundant connections and increase the network's capacity for further learning by incorporating random weight reinitialization. Thus, effectively improving the performance and generalization of deep neural networks on small datasets.", "description": "This figure illustrates the Dynamic Neural Regeneration (DNR) framework, which is an evolutionary training paradigm.  The framework consists of three stages: (a) Data-aware dynamic masking, where redundant connections are removed based on their importance; (b) Neuron deletion, where the removed connections are eliminated; and (c) Neuron creation, where new connections are randomly initialized. This process iteratively refines the network, improving performance and generalization on small datasets.", "section": "3 Methodology"}, {"figure_path": "qCpCy0EQAJ/figures/figures_6_1.jpg", "caption": "Figure 1: Schematics of proposed Dynamic Neural Regeneration (DNR) framework. Our framework utilizes a data-aware dynamic masking scheme to remove redundant connections and increase the network's capacity for further learning by incorporating random weight reinitialization. Thus, effectively improving the performance and generalization of deep neural networks on small datasets.", "description": "This figure illustrates the Dynamic Neural Regeneration (DNR) framework, which uses a data-aware dynamic masking scheme.  The scheme identifies and removes redundant connections, increasing the network's capacity for learning. This is achieved through random weight reinitialization during an iterative training process, ultimately enhancing the model's performance and generalization capabilities, especially on small datasets. The figure visually depicts the process, showing the evolution of the network across generations, highlighting the dynamic masking and neuron addition/deletion steps.", "section": "3 Methodology"}, {"figure_path": "qCpCy0EQAJ/figures/figures_7_1.jpg", "caption": "Figure 3: Convergence behavior: evaluating performance across generations in DNR and transfer learning with ResNet18 architecture trained on CUB dataset.", "description": "The figure compares the convergence behavior of Dynamic Neural Regeneration (DNR) and vanilla transfer learning using the ResNet18 architecture on the CUB dataset. The x-axis represents the generation number, while the y-axis shows the accuracy.  The plot illustrates that DNR achieves a higher accuracy with fewer generations, indicating faster convergence and potentially better generalization.", "section": "5.4 Comparison with Transfer Learning"}, {"figure_path": "qCpCy0EQAJ/figures/figures_12_1.jpg", "caption": "Figure 1: Schematics of proposed Dynamic Neural Regeneration (DNR) framework. Our framework utilizes a data-aware dynamic masking scheme to remove redundant connections and increase the network's capacity for further learning by incorporating random weight reinitialization. Thus, effectively improving the performance and generalization of deep neural networks on small datasets.", "description": "This figure illustrates the Dynamic Neural Regeneration (DNR) framework, which uses data-aware dynamic masking to remove unnecessary connections in a neural network.  By doing this, it increases the network's capacity for further learning, as weights are randomly reinitialized, leading to improved performance and generalization, particularly with small datasets. The figure shows the evolutionary training paradigm used, highlighting the dynamic masking, neuron deletion, and creation processes. ", "section": "3 Methodology"}, {"figure_path": "qCpCy0EQAJ/figures/figures_13_1.jpg", "caption": "Figure 5: Robustness to natural corruptions on CIFAR-10-C (37). DNR is more robust against the majority of corruptions compared to the baselines.", "description": "This figure displays the Mean Corruption Accuracy (MCA) for three different methods (LB, KE, and DNR) across various types of image corruptions (Gaussian Noise, Shot Noise, Impulse Noise, Speckle Noise, Defocus Blur, Gaussian Blur, Motion Blur, Zoom Blur, Brightness, Snow, Frost, Fog, Spatter, Contrast, Elastic, JPEG Compression, Pixelate, and Saturation).  Each corruption type has five levels of severity.  The heatmap visualizes the MCA for each method and corruption, demonstrating that DNR generally exhibits higher accuracy across multiple corruption types and severities, indicating superior robustness.", "section": "A.3 Robustness to Natural Corruptions"}, {"figure_path": "qCpCy0EQAJ/figures/figures_14_1.jpg", "caption": "Figure 6: Robustness to adversarial attacks", "description": "This figure displays the robustness of different models (LB, KE, and DNR) against adversarial attacks of varying strengths (epsilon values). The y-axis represents the adversarial accuracy, showing the percentage of correctly classified examples even under adversarial perturbations. The x-axis indicates the strength of the adversarial attack (epsilon). The figure shows how the adversarial accuracy decreases as the attack strength increases for all three models, but DNR demonstrates significantly better robustness compared to LB and KE, maintaining higher accuracy even under stronger attacks.", "section": "A.6 Robustness to Class Imbalance Dataset"}, {"figure_path": "qCpCy0EQAJ/figures/figures_14_2.jpg", "caption": "Figure 7: Robustness to class imbalance", "description": "The figure shows the class balanced accuracy for three different methods (LB, KE, and DNR) under class imbalanced conditions.  The results illustrate that DNR significantly outperforms the baselines (LB and KE) in handling class imbalance, suggesting its robustness and effectiveness in addressing this common challenge in real-world datasets.", "section": "A.6 Robustness to Class Imbalance Dataset"}]