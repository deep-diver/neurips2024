[{"type": "text", "text": "Bayesian Strategic Classification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lee Cohen1 Saeed Sharifi-Malvajerdi2 Kevin Stangl2 Ali Vakilian2 Juba Ziani3 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In strategic classification, agents modify their features, at a cost, to obtain a positive classification outcome from the learner\u2019s classifier, typically assuming agents have full knowledge of the deployed classifier. In contrast, we consider a Bayesian setting where agents have a common distributional prior on the classifier being used and agents manipulate their features to maximize their expected utility according to this prior. The learner can reveal truthful, yet not necessarily complete, information about the classifier to the agents, aiming to release just enough information to shape the agents\u2019 behavior and thus maximize accuracy. We show that partial information release can counter-intuitively benefti the learner\u2019s accuracy, allowing qualified agents to pass the classifier while preventing unqualified agents from doing so. Despite the intractability of computing the best response of an agent in the general case, we provide oracle-efficient algorithms for scenarios where the learner\u2019s hypothesis class consists of low-dimensional linear classifiers or when the agents\u2019 cost function satisfies a sub-modularity condition. Additionally, we address the learner\u2019s optimization problem, offering both positive and negative results on determining the optimal information release to maximize expected accuracy, particularly in settings where an agent\u2019s qualification can be represented by a real-valued number. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine Learning critically relies on the assumption that the training data is representative of the unseen instances a learner faces at test time. Yet, in many real-life situations, this assumption fails when individuals (agents) manipulate decision-making algorithms for personal advantage, by modifying their features at a cost. A typical example of such manipulations or strategic behavior is seen in loan applications or credit scoring: for example, an individual may open new credit card accounts to lower their credit utilization and increase their credit score artificially. In the context of job interviews, a candidate can spend time and effort to memorize solutions to common interview questions and potentially look more qualified than they are at the time of an interview. A student might cram to pass an exam this way without actually understanding or improving their knowledge of the subject. ", "page_idx": 0}, {"type": "text", "text": "The prevalence of such behaviors has led to the rise of an area of research known as strategic classification. Strategic classification, introduced by Hardt et al. [2016], aims to understand how a learner can optimally modify decision making algorithms to be robust to such strategic manipulations of agents, if and when possible. ", "page_idx": 0}, {"type": "text", "text": "Most of the strategic classification literature makes the assumption that the model deployed by the learner is fully observable by the agents, granting them the ability to optimally best respond to the learner using resources such as effort, time, and money. Yet, this full information assumption can be unrealistic in practice. There are several reasons for this: some machine learning models are proprietary and hide the details of the model to avoid leaking \u201ctrade secrets\u201d: e.g., this is the case for the credit scoring algorithms used by FICO, Experian, and Equifax.1 Some classifiers are simply too complex in the first place to be understood and interpreted completely by a human being with limited computational power, such as deep learning models. Other classifiers and models may be obfuscated for data privacy reasons, which are becoming an increasingly major concern with new European consumer protection laws such as GDPR [Regulation, 2018] and with the October 2023 Executive Order on responsible AI [Biden, 2023]. In turn, there is a need to study strategic classification when agents only have partial knowledge of the learner\u2019s model. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "There has been a relatively short line of work trying to understand the impact of incomplete information on strategic classification. Jagadeesan et al. [2021] and Bechavod et al. [2021] study the optimal classifiers in settings where agents can only gain partial or noisy information about the deployed model. Haghtalab et al. [2023] study calibrated Stackelberg games, a more general form of strategic classification; in their framework, the learner engages in repeated interactions with agents who base their actions on calibrated forecasts about the learner\u2019s classifier. They characterize the optimal utility of the learner for such games under some regularity assumptions. While we also model agents with prior knowledge of the learner\u2019s actions, Haghtalab et al. [2023] focus on an online learning setting and the selection of a strategy for the learner without incorporating any form of voluntary information release by the learner. ", "page_idx": 1}, {"type": "text", "text": "In contrast, we focus on this additional critical aspect of voluntary information release by the learner that these works do not study. Namely, we ask: ", "page_idx": 1}, {"type": "text", "text": "How to release partial and truthful information about the classifier to maximize accuracy? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This should give the reader pause: why should a learner release information about their deployed classifier since presumably such information only makes it easier for agents to manipulate their features and \u201ctrick\u201d the learner. In fact, Ghalme et al. [2021] showed that information revelation can help\u2014a learner may prefer to fully reveal their classifier as opposed to hiding it. While they consider either fully revealing the classifier or completely hiding it, our model considers a wider spectrum of information revelation that includes both \u201cfull-information-release\u201d and \u201cno-information-release\u201d. We show that there exist instances where it is optimal to reveal only partial information about the classifier, in a model where a learner is allowed to reveal a subset of the classifiers containing the true deployed classifier. For example, a tech firm might reveal to candidates that they will ask them about a new type of data structure during their job interviews. Lenders might reveal to clients that they do not consider factors like credit score [Lake, 2024]. This selective disclosure can discourage unfit individuals, ultimately saving time and energy for both sides. In the following, we summarize our contributions. ", "page_idx": 1}, {"type": "text", "text": "Summary of contributions: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 In Section 2, we propose a new model of interactions between strategic agents and a learner, under partial information. The two novel modeling elements compared to the standard strategic classification literature are: i) agents have partial knowledge about the learner\u2019s classifier in the form of a distributional prior over the hypothesis class, and ii) the learner can release partial information about their deployed classifier. Specifically, our model allows the learner to release a subset of the hypothesis class to narrow down the agents\u2019 priors. Given our model, we consider a (Stackelberg) game between agents with partial knowledge and a learner that can release partial information about its deployed model. On the one hand, the agents aim to manipulate their features, at a cost, to increase their likelihood of receiving a positive classification outcome. On the other hand, the learner can release partial information to maximize the expected accuracy of its model, after agent manipulations. \u2022 In Section 3, we study the agent\u2019s best response in our game. We show that while in general, it is intractable to compute the best response of the agents in our model, there exist oracle-efficient algorithms2 that can exactly solve the best response when the hypothesis class is the class of low-dimensional linear classifiers. We then move away from the linearity assumption and consider a natural condition on the agents\u2019 cost function for which we give an oracle-efficient approximation algorithm for the best response of the agents for any hypothesis class. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 In Section 4, we study the learner\u2019s optimal information release problem. We consider screening/classification settings where agents are represented to the learner by a real-valued number that measures their qualification level for a certain task. Prior work has focused on similar onedimensional settings in the context of strategic classification; see, e.g., [Beyhaghi et al., 2023, Braverman and Garg, 2020]. We first show that the learner\u2019s optimal information release problem is NP-hard when the agents\u2019 prior can be arbitrary. In light of this hardness result, we focus on uniform prior distributions and provide closed-form solutions for the case of continuous uniform priors, and an efficient algorithm to compute the optimal information release for discrete uniform priors.   \n\u2022 We finally consider alternative utility functions that are based on false positive (or negative) rates for the learner and provide insights as to what optimal information release should look under these utility functions, without restricting ourselves to uniform priors. ", "page_idx": 2}, {"type": "text", "text": "Related Work. Strategic classification was first formalized by Br\u00fcckner and Scheffer [2011], Hardt et al. [2016]. Hardt et al. [2016] is perhaps the most seminal work in the area of strategic classification: they provide the first computationally efficient algorithms (under assumptions on the agents\u2019 cost function) to efficiently learn a near-optimal classifier in strategic settings. Importantly, this work makes the assumption that the agents fully know the exact parameters of the classifier due to existing \u201cinformation leakage\u201d, even when the firm is obscuring their model. Hardt et al. [2016] also do not consider a learner that can release partial information about their model. ", "page_idx": 2}, {"type": "text", "text": "Closest to our work, Jagadeesan et al. [2021], Ghalme et al. [2021], Bechavod et al. [2022], and Haghtalab et al. [2023] relax the full information assumption and characterize the impact of opacity on the utility of the learner and agents. Jagadeesan et al. [2021] are the first to introduce a model of \u201cbiased\u201d information about the learner\u2019s classifier: instead of observing the learner\u2019s deployed classifier exactly, agents observe and best respond to a noisy version of this classifier; one that is randomly shifted (by an additive amount) from the true deployed classifier. ", "page_idx": 2}, {"type": "text", "text": "In contrast, Ghalme et al. [2021] and Bechavod et al. [2022] consider models of partial information on the classifiers, where agents can access samples in the form of historical (feature vector, learner\u2019s prediction) pairs. More precisely, Ghalme et al. [2021] study what they coin the \u201cprice of opacity\u201d in strategic classification, defined as the difference in prediction error when not releasing vs fully releasing the classifier. They are the first to show that this price can be positive (in the context of strategic classification), meaning that a learner can reduce their prediction error by fully releasing their classifier in strategic settings. Our work considers more general, intermediate forms of information release, instead of the all-or-nothing, binary approach of Ghalme et al. [2021]. ", "page_idx": 2}, {"type": "text", "text": "Bechavod et al. [2022] consider a strategic regression setting in which the learner does not release their regression rule, but agents have access to (feature, score) samples as described above. They study how disparity in sample access (e.g., agents may only access samples from people similar to them) about the classifier across different groups induce unfairness in classification outcomes across these groups. Haghtalab et al. [2023] consider agents with (calibrated) forecasts over the actions of the learner, but do not consider the learner\u2019s information release which is our focus. Additionally, in our model, we do not constrain the agent\u2019s prior distribution to be calibrated. ", "page_idx": 2}, {"type": "text", "text": "Beyond strategic classification, there are a few related lines of work where such partial information is considered. One is Bayesian Persuasion [Kamenica and Gentzkow, 2011]: in Bayesian persuasion, the state of the world is randomly drawn from the prior, and there is a mapping from the state of the world to signal distributions. This mapping, i.e. the \u201csignaling scheme\u201d, must be revealed to the agents in addition to the signal. In our setting, there is a fixed state of the world (the learner\u2019s classifier), and there is no need for the signaling scheme to be known, since the signal itself (the subset) reveals all the information needed for the agents. The agents only need to know that the learner is truthful, which is an assumption made in Bayesian persuasion too. ", "page_idx": 2}, {"type": "text", "text": "Relatedly, algorithmic recourse studies an \u201cintermediate\u201d information release problem where the learner publishes a recommended action or recourse for each agent to take, rather than a set of potential classifiers used by the learner; e.g., Harris et al. [2022]. In our model, we release the same signal or information to all agents based on the underlying distribution over these agents\u2019 features. ", "page_idx": 2}, {"type": "text", "text": "Our model consists of a population of agents and a learner. Each agent in our model is represented by a pair $(x,y)$ where $x\\in\\mathscr{X}$ is a feature vector, and $y\\in\\{0,1\\}$ is a binary label. Throughout, we call an agent with $y=0$ a \u201cnegative\u201d, and an agent with $y=1$ a \u201cpositive\u201d. We assume there exists a mapping $f:\\mathcal{X}\\to\\{0,1\\}$ that governs the relationship between $x$ and $y$ ; i.e., $y=f(x)$ for every agent $(x,y)$ . We will therefore use $x$ to denote agents from now on. We denote by $D$ the distribution over the space of agents $\\mathcal{X}$ . Agent manipulations are characterized by a cost function $c:\\mathcal{X}\\times\\mathcal{X}\\to[0,\\infty)$ where $c(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})$ denotes the cost that an agent incurs when changing their features from $x$ to $x^{\\prime}$ . We assume, similar to standard strategic classification settings, that manipulation does not change one\u2019s true label: manipulation is seen purely as \u201cgaming\u201d; it does not change the qualification of an agent. Let $\\mathcal{H}\\subseteq\\{0,1\\}^{\\dot{\\mathcal{X}}}$ denote our hypothesis class, and let $h\\in\\mathcal H$ be the fixed classifier that the learner is using for classification. ", "page_idx": 3}, {"type": "text", "text": "A Partial Knowledge Model for the Agents. We move away from the standard assumption that agents fully know $h$ and model agents as having a common (shared by all agents) prior distribution $\\pi$ over $\\mathcal{H}$ . This distribution captures their initial belief about which classifier is deployed by the learner. Formally, for every $h^{\\prime}\\in\\bar{\\mathcal{H}}$ , $\\pi(h^{\\prime})$ is the probability that the learner is going to deploy $h^{\\prime}$ for classification from the agents\u2019 perspective. We emphasize that the learner is committed to using a fixed classifier $h$ . The prior $\\pi$ captures the agents\u2019 belief about the deployed classifier and is known to the learner. ", "page_idx": 3}, {"type": "text", "text": "For example, job seekers may use Glassdoor to prepare for interviews. They may not know the exact hiring algorithm $(h)$ of a specific company but can observe patterns from other companies for similar roles. This forms their initial belief, represented by $\\pi$ , about the classifier a company might use. Thus, $\\pi$ captures the agents\u2019 probabilistic beliefs rather than assuming full knowledge of $h$ .3 ", "page_idx": 3}, {"type": "text", "text": "A Partial Information Release Model for the Learner. The learner has the ability to influence the agents\u2019 prior belief $\\pi$ about the deployed classifier $h$ by releasing partial information about $h$ . We model information release by releasing a subset $H\\subseteq\\mathcal{H}$ such that $h\\in H$ . We note that we reveal information truthfully, meaning that the deployed classifier is required to be in $H$ . ", "page_idx": 3}, {"type": "text", "text": "Note that this is a general form of information release because it allows the learner to release any subset of the hypothesis class, so long as it includes the deployed classifier $h$ . Below, we provide natural examples of information release that can be captured by our model. ", "page_idx": 3}, {"type": "text", "text": "Example 2.1 (Examples of Information Release via Subsets). Consider the class of linear halfspaces in $d$ dimensions: $\\mathcal{H}=\\left\\{h_{w,b}:w=[w^{1},w^{2},\\ldots,w^{d}]^{\\top}\\in\\mathbb{R}_{+}^{d},\\,b\\in\\mathbb{R}\\right\\}$ where $h_{w,b}(x)\\triangleq\\mathbb{1}[w^{\\top}x+$ $b\\geq0]$ and $\\boldsymbol{x}\\in\\mathcal{X}=\\mathbb{R}^{d}$ is the feature vector. Let $h=h_{w_{0},b_{0}}$ be the classifier deployed by the learner for some $w_{0},b_{0}$ . Under this setting, revealing the corresponding parameter of a feature, say $x^{j}$ , in $h$ corresponds to releasing $H_{1}=\\{h_{w,b}\\in\\mathcal{H}:w^{j}=w_{0}^{j}\\}$ (e.g., \u2018minimal GPA of 3.8 for grad school\u2019). Revealing the top $k$ features of $h$ (e.g., the most significant class grades are algorithms and calculus) corresponds to releasing $H_{2}\\,=\\,\\{\\bar{h_{w,b}}\\,\\in\\,\\mathcal{H}\\,:\\,w^{i_{1}^{-}},w^{i_{2}}\\,\\cdot\\,.\\,.\\,,w^{i_{k}}$ are the $k$ largest coordinates of $w\\}$ Let $I_{0}$ be such that $w_{0}^{i}\\neq0$ iff $i\\in I_{0}$ . Revealing the relevant features of $h$ , i.e. features with nonzero coefficients (e.g., sensitive attributes like race or gender will not be used in the decision) corresponds to releasing $\\bar{H_{3}^{-}}=\\{h_{w,b}\\in\\mathcal{H}:w^{i}\\neq0,\\,\\forall i\\in I_{0}\\}$ . This is a common form of information release in the real world4. ", "page_idx": 3}, {"type": "text", "text": "The Strategic Game with Partial Information Release. Once the partial information $H$ is released by the learner, agents best respond as follows: each agent first computes their posterior belief about the deployed classifier by projecting their prior $\\pi$ onto $H$ , which we denote by $\\pi|_{H}$ , and is formally defined by: $\\forall h^{\\prime}\\in\\mathcal{H}$ , $\\begin{array}{r}{\\pi|_{H}(h^{\\prime})\\triangleq\\frac{\\pi(h^{\\prime})}{\\pi(H)}\\mathbb{1}[h^{\\prime}\\in H]}\\end{array}$ . Given this posterior distribution, the agent then moves to a new point that maximizes their utility. The utility is quasi-linear and measured by the ", "page_idx": 3}, {"type": "text", "text": "Table 1: Hypothesis class $\\mathcal{H}$ in Example 2.3 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "probability (according to $\\pi|_{H})$ of receiving a positive outcome minus the manipulation cost. Formally, the utility of agent $x$ that manipulates to $x^{\\prime}$ , under the partial information $H$ released by the learner is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nu_{x}(x^{\\prime},H)\\triangleq\\operatorname*{Pr}_{h^{\\prime}\\sim\\pi|_{H}}\\left[h^{\\prime}(x^{\\prime})=1\\right]-c(x,x^{\\prime}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We let $\\mathsf{B R}(x,H)$ denote the best response of agent $x$ , i.e. a point $x^{\\prime}$ that maximizes $u_{x}(x^{\\prime},H)$ . 5 The goal of the learner is to release $H$ that includes its deployed classifier $h$ so as to maximize its utility which is measured by its expected strategic accuracy. ", "page_idx": 4}, {"type": "equation", "text": "$$\nU(H)\\triangleq\\operatorname*{Pr}_{x\\sim D}\\left[h(\\mathbf{BR}(x,H))=f(x)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Definition 2.2 (Strategic Game with Partial Information Release). The game, between the learner who is using $h\\in\\mathcal H$ for classification, and the agents who have a prior \u03c0 over $\\mathcal{H}$ , proceeds as follows: ", "page_idx": 4}, {"type": "text", "text": "1. The learner (knowing $f,\\,D,\\,c,\\,\\pi,$ ) publishes a subset of hypotheses $H\\subseteq{\\mathcal{H}}$ such that $h\\in H$ . ", "page_idx": 4}, {"type": "text", "text": "2. Every agent $x$ best responds by moving to a point $B R(x,H)$ that maximizes their utility: $B R(x,H)\\in\\operatorname{argmax}_{x^{\\prime}\\in\\mathcal{X}}u_{x}(x^{\\prime},H)$ . ", "page_idx": 4}, {"type": "text", "text": "The learner\u2019s goal is to find a subset $H^{\\star}\\subseteq\\;\\mathcal{H}$ with $h\\,\\in\\,H^{\\star}$ , that maximizes its utility6: $H^{\\star}\\in$ argmaxH\u2286H, $h\\!\\in\\!H^{\\mathit{U}}(H)$ ", "page_idx": 4}, {"type": "text", "text": "We note that similar to standard strategic classification, the game defined in Definition 2.2 can be seen as a Stackelberg game in which the learner, as the \u201cleader\u201d, commits to her strategy first and then the agents, as the \u201cfollowers\u201d, respond. The optimal strategy of the learner, $H^{\\star}$ , corresponds to the Stackelberg equilibrium of the game, assuming best response of the agents. ", "page_idx": 4}, {"type": "text", "text": "Contrasting with the Standard Setting of Strategic Classification. The game defined in Definition 2.2 not only captures both the partial knowledge of the agents and the leaner\u2019s partial information release, but can also be viewed as a generalization of the standard strategic classification game where the agents fully observe the classifier $h$ , which we refer to as the full information release game (e.g., see [Hardt et al., 2016]). This is because the learner can always choose $H=\\{h\\}$ . Next, we ask: ", "page_idx": 4}, {"type": "text", "text": "Can partial information release increase the learner\u2019s utility compared to full information release? ", "page_idx": 4}, {"type": "text", "text": "Observe that by definition, $U(H^{\\star})\\,\\geq\\,U(\\{h\\})$ , i.e., the learner can only gain utility when they optimally release partial information instead of fully revealing the classifier. In the following examples, we show that there exist instantiations of the problem where $U(H^{\\star})>U(\\{h\\})$ , even when $h$ is picked to be the optimal classifier in the full information release game, i.e., one that maximizes $U(\\{h\\})$ . In other words, we show that the learner can gain nonzero utility by releasing a subset that is not $\\{h\\}$ , even if the choice of $h$ is optimized for the full information release game. ", "page_idx": 4}, {"type": "text", "text": "Example 2.3 (Partial vs. Full Information Release). Suppose $\\mathcal{X}=\\{x_{1},x_{2}\\}$ , and that their probability weights under the distribution7 are given by $D(x_{1})=2/3,D(x_{2})=1/3$ , and their true labels are given by $f(x_{1})\\,=\\,1$ , $f(x_{2})\\,=\\,0$ . Suppose the cost function is given as follows: $c(x_{1},x_{2})\\;=\\;$ $2,c(x_{2},x_{1})=3/4$ . Let $\\mathcal{H}=\\{h_{1},h_{2},h_{3}\\}$ be given by table 1. One can show that under this setting, $h=h_{1}$ is the optimal classifier under full information release, i.e., it optimizes $U(\\{h\\})$ , and that for such $h$ , $U(\\{h\\})=2/3$ . However, suppose the prior distribution over $\\mathcal{H}$ is uniform. One can show that under this setting, and when $h=h_{1}$ is the deployed classifier, releasing $H^{\\star}=\\{h_{1},h_{2}\\}$ implies $U(H^{\\star})=1>U(\\{\\bar{h}\\})=2/3$ . In other words, the learner can exploit the agent\u2019s prior by releasing information in a way that increases its own utility by a significant amount. ", "page_idx": 4}, {"type": "text", "text": "In the next example, we consider the more natural setting of single-sided threshold functions in one dimension and show that the same phenomenon occurs: the optimal utility achieved by partial information release is strictly larger than the utility achieved by the full information release of $h$ , even after the choice of $h$ is optimized for full information release. ", "page_idx": 5}, {"type": "text", "text": "Example 2.4 (Partial vs. Full Information Release). Suppose $\\mathcal{X}=[0,2]$ , $D$ is the uniform distribution over $[0,2]$ , $f(x)\\,=\\,\\mathbb{1}\\left[x\\ge1.9\\right]$ , $\\mathcal{H}\\,=\\,\\{h_{t}\\,:\\,t\\,\\in\\,[0,2]\\}$ where $h_{t}(x)\\triangleq\\,\\mathbb{1}\\left[x\\geq t\\right]$ . Suppose the cost function is given by the distance $c(x,x^{\\prime})\\,=\\,|x\\,-\\,x^{\\prime}|$ . We have that under this setting, the optimal classifier in $\\mathcal{H}$ under full information release is $h=h_{2}$ , and that its corresponding utility is $U(\\{h\\})=1-\\operatorname*{Pr}_{x\\sim U n i f[0,2]}$ $[1\\leq x<1.9]=0.55$ . Now suppose the agents have the following prior over $\\mathcal{H}$ : $\\pi(h^{\\prime})=0.1\\cdot\\bar{1}[\\dot{h^{\\prime}}=h_{2}]+0.9\\cdot1[h^{\\prime}=h_{1.8}]$ . Under this setting, and when $h=h_{2}$ is deployed for classification, one can see that releasing $H^{\\star}=\\{h_{2},h_{1.8}\\}$ leads to perfect utility for the learner. We therefore have $U(H^{\\star})=1>U(\\{h\\})=0.55$ . ", "page_idx": 5}, {"type": "text", "text": "3 The Agents\u2019 Best Response Problem ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section we focus on the best response problem faced by the agents in our model, as described in Definition 2.2. We consider a natural optimization oracle for the cost function of the agents that can solve simple projections. We will formally define this oracle later on. Given access to such an oracle, we then study the oracle complexity8 of the agent\u2019s best response problem. First, we show that the best response problem is computationally hard even for a common family of $\\ell_{p}$ -norm cost functions. Next, we provide an oracle-efficient algorithm9 for solving the best response problem when the hypothesis class is the class of low-dimensional linear classifiers. In Appendix B, we consider submodular cost functions and show that for any hypothesis class, there exists an oracle-efficient algorithm that approximately solves the best response problem in this setting. ", "page_idx": 5}, {"type": "text", "text": "Recall that the agents\u2019 best response problem can be cast as the following: given an agent $x\\in\\mathscr{X}$ , and a distribution $P$ (e.g., $P=\\pi|_{H}$ where $\\pi$ is the prior and $H$ is the released information) over a set $\\{h_{1},\\ldots,h_{n}\\}\\subseteq{\\mathcal{H}}$ , we want to solve $\\begin{array}{r}{\\operatorname*{argmax}_{z\\in\\mathcal{X}}\\left\\{\\operatorname*{Pr}_{h^{\\prime}\\sim P}\\left[h^{\\prime}(z)=1\\right]-c(x,z)\\right\\}}\\end{array}$ . We consider an oracle that given any region $R\\subseteq\\mathcal{X}$ , specified by the intersection of positive (or negative) regions of $h_{i}$ \u2019s, returns the projection of the agent $x$ onto $R$ according to the cost function $c:$ $:\\operatorname{argmin}_{z\\in R}c(x,z)$ For example, when $\\mathcal{H}$ is the class of linear classifiers and $c(x,z)\\;=\\;\\|x\\,-\\,z\\|_{2}$ , the oracle can compute the $\\ell_{2}$ -projection of the agent $x$ onto the intersection of any subset of the linear halfspaces in $\\{h_{1},\\ldots,h_{n}\\}$ . We denote this oracle by Oracle $(c,{\\mathcal{H}})$ and formally define it in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Having access to such an oracle, and without further assumptions, the best response problem can be solved by exhaustively searching over all subsets of $\\{h_{1},\\ldots,h_{n}\\}$ because: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z\\in\\mathcal{X}}\\left\\{\\operatorname*{Pr}_{h^{\\prime}\\sim P}|h^{\\prime}(z)=1|-c(x,z)\\right\\}=\\operatorname*{max}_{S\\subseteq\\{h_{1},\\ldots,h_{n}\\}}\\left\\{\\sum_{h^{\\prime}\\in S}P(h^{\\prime})-\\operatorname*{min}_{z:h^{\\prime}(z)=1,\\forall h^{\\prime}\\in S}c(x,z)\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This algorithm is inefficient because it makes exponentially many oracle calls. In what follows, we consider natural instantiations of our model and examine if we can get algorithms that make only $p o l y(n)$ oracle calls. All missing proof of this sections are provided in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "$p$ -Norm Cost Functions. First, we consider Euclidean spaces and the common family of $p$ -norm functions for $p\\geq1$ and show that even under the assumption that the cost function of the agent belongs to this family, the problem of finding the best response is computationally hard. Formally, a $p$ -norm cost function is defined by: for every $x,x^{\\prime}\\in\\mathbb R^{d}$ , $\\dot{c}_{p}(x,x^{\\prime})=\\dot{\\|}x-x^{\\prime}\\|_{p}$ where $p\\geq1$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. $\\Omega(2^{n}/{\\sqrt{n}})$ calls to the oracle (Algorithm $^{\\,l}$ ) are required to compute the best response of an agent with $2/3$ probability of success, even when $\\mathcal{X}=\\mathbb{R}^{2}$ and the cost function is $c_{p}$ for $p\\geq1$ . ", "page_idx": 5}, {"type": "text", "text": "Input: agent $_x$ , cost function $c$ , arbitrary distribution $P$ over linear classifiers $\\{h_{1},\\ldots,h_{n}\\}$   \nStep 1. Compute the partitioning $\\left(R_{n}\\right)$ of the space given by $\\{h_{1},\\ldots,h_{n}\\}$ ;   \nInitialize $R_{1}\\leftarrow\\{\\{z:h_{1}(z)=1\\}$ , $\\{z:h_{1}(z)=0\\}\\}$ ;   \nfor $i=2,\\dots,n$ do $R_{i}\\gets R_{i-1}$ ; for $R\\in R_{i-1}$ do if $\\{z:h_{i}(z)=0\\}\\cap R\\neq\\emptyset$ then $R_{i}\\leftarrow R_{i}\\setminus R$ ; // Remove R Ri \u2190Ri \u222a{{z : hi(z) = 1} \u2229R, {z : hi(z) = 0} \u2229R} ; // Split R   \nStep 2. Given $R_{n}$ , compute the best response;   \nfor $R\\in R_{n}$ do Let $R=R^{+}\\cap R^{-}$ where $R^{+}=\\cap_{\\mathit{i}\\in I^{+}}\\left\\{z:h_{i}(z)=1\\right\\}$ and $R^{-}=\\cap_{\\mathit{i}\\in I^{-}}\\left\\{z:h_{i}(z)=0\\right\\}$ ; Call the oracle (Algorithm 1) to solve $z_{R}\\in\\mathrm{argmin}_{z\\in R}\\,c(x,z)$ ; Compute the utility of $z_{R}$ : utility $\\begin{array}{r}{\\dot{(z_{R})}=\\sum_{i\\in I^{+}}P(\\bar{h_{i}})-c(x,z_{R})}\\end{array}$ ;   \nOutput: $\\mathrm{argmax}_{z\\in Z}$ utility $(z)$ where $Z=\\{z_{R}:R\\in R_{n}\\}$ ", "page_idx": 6}, {"type": "text", "text": "Low-Dimensional Linear Classifiers. Next, we show that when $\\mathcal{X}=\\mathbb{R}^{d}$ for some $d$ , and when $\\mathcal{H}$ contains only linear classifiers, i.e., every $h\\in\\mathcal H$ can be written as $h(x)=\\mathbb{1}\\left[w^{\\top}x+b\\geq0\\right]$ for some $w\\in\\mathbb{R}^{d}$ and $b\\in\\mathbb{R}$ , then the best response of the agents can be computed with $O(n^{d})$ oracle calls when $d\\ll n$ . ", "page_idx": 6}, {"type": "text", "text": "The algorithm, which is described in Algorithm 2, first computes the partitioning $(R_{n})$ of the space $\\mathcal{X}$ given by the $n$ linear classifiers. For any element of the partition in $R_{n}$ , it then solves the best response when we restrict the agent to select its best response from that particular element. This gives us a set of points, one for each element of the partition. The algorithm finally outputs the point that has maximum utility for the agent. This point, by construction, is the best response of the agent. The oracle-efficiency of the algorithm follows from the observation that $n$ linear halfspaces in $d$ dimensions partition the space into at most $O(n^{d})$ elements when $d\\ll n$ . Formally, ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2. Suppose $\\mathcal{X}=\\mathbb{R}^{d}$ for some $d\\ll n$ , and $\\mathcal{H}$ contains only linear classifiers. Then for any agent $x$ , any cost function $c,$ , and any distribution $P$ over $\\{h_{1},\\ldots,h_{n}\\}\\subseteq{\\mathcal{H}},$ , Algorithm 2 returns the best response of the agent in time $O(n^{d+1})$ , while making $O(n^{d})$ calls to the oracle (Algorithm 1). ", "page_idx": 6}, {"type": "text", "text": "3.1 Generalizing to Arbitrary $P$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Theorem 3.2 we require the distribution $P$ be over $\\{h_{1},\\ldots h_{n}\\}\\subseteq{\\mathcal{H}}$ , e.g. to have finite support. ", "page_idx": 6}, {"type": "text", "text": "When this does not hold, ie. $P$ has infinite support size, we can ignore classifiers with sufficiently small probabilities (i.e., $p o l y(\\epsilon))$ ), as they do not affect the manipulation strategy when searching for an $(1+\\epsilon)$ -approximate solution. The number of classifiers in the support with probability at $p o l y(\\epsilon)$ for a fixed $\\epsilon>0$ is at most $1/p o l y(\\epsilon)$ which is a finite number. Therefore, to obtain a nearly optimal solution, it suffices to only consider probability distributions with finite support size. ", "page_idx": 6}, {"type": "text", "text": "4 The Learner\u2019s Optimal Information Release Problem ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section we focus on the learner\u2019s optimization problem as described in Definition 2.2. The learner is facing a population of agents with prior $\\pi$ and wants to release partial information $H\\subseteq{\\mathcal{H}}$ so as to maximize its utility $U(H)$ . We note that the learner\u2019s strategy space can be restricted to the support of the agents\u2019 prior $\\pi$ because including anything in $H$ that is outside of $\\pi$ \u2019s support does not impact $U(H)$ . Therefore, one naive algorithm to compute the utility maximizing solution for the learner is to evaluate the utility on all subsets $H\\subseteq{\\mathrm{support}}(\\pi)$ and output the one that maximizes the utility. However, this solution is inefficient; instead, can we have computationally efficient algorithms? We provide both positive and negative results for a natural instantiation of our model which is introduced below. ", "page_idx": 6}, {"type": "text", "text": "The Setup: Classification Based on Test Scores. We adopt the following setup for the learner\u2019s information release problem. We are motivated by screening problems such as school admissions and hiring where an individual\u2019s qualification level can be captured via a real-valued number, say, a test score. We therefore consider agents that live in the one dimensional Euclidean space: $\\mathcal{X}=[0,B]\\subseteq\\mathbb{R}$ for some $B$ . One can think of each $x$ as the corresponding qualification level or test score of an agent where larger values of $x$ correspond to higher qualification levels or higher test scores. As we are in a strategic setting, agents can modify their true feature $x$ and \u201cgame\u201d the learner by appearing more qualified than they actually are. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We let $f(x)\\;=\\;\\mathbb{1}\\left[x\\ge t\\right]$ for some $t$ : there exists some threshold $t$ that separates qualified and unqualified agents. We take the hypothesis class $\\mathcal{H}$ to be the class of all single-sided threshold classifiers: every $h^{\\prime}\\in\\mathcal{H}$ can be written as $h^{\\prime}(x)\\triangleq\\mathbb{1}\\left[x\\geq t^{\\prime}\\right]$ for some $t^{\\prime}$ . We further take the cost function of the agents to be the standard distance metric in $\\mathbb{R}$ : $c(x,x^{\\prime})=|x^{\\prime}-x|$ .10 ", "page_idx": 7}, {"type": "text", "text": "Remark 4.1. We emphasize that considering agents in the one-dimensional Euclidean space is only for simplicity of exposition. We basically assume, for an arbitrary space of agents $\\mathcal{X}$ , there exists a function $g:\\mathcal{X}\\to[0,B]$ such that $f(x)=\\mathbb{1}[g(x)\\geq t]$ for some $t$ , and that the cost function is given by $c(x,z)\\,=\\,|g(z)-g(x)|$ . Here, $g(x)$ captures the qualification level or test score of an agent $x$ . Now observe that we can reduce this setting to the introduced setup of this section: take ${\\bar{\\mathcal{X}}}^{\\prime}=\\{g(x):x\\in\\mathcal{X}\\}\\subseteq[0,B]$ , $f:\\mathcal{X}^{\\prime}\\rightarrow\\{0,1\\}$ is given by $f(x^{\\prime})=\\mathbb{1}[x^{\\prime}\\geq i]$ , and that the cost function $c:\\mathcal{X}^{\\prime}\\times\\mathcal{X}^{\\prime}\\rightarrow[0,\\infty)$ is given by $c(x^{\\prime},z^{\\prime})=|z^{\\prime}-x^{\\prime}|$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 4.2. Note that because every classifier $h^{\\prime}\\,\\in\\,{\\mathcal{H}}$ is uniquely specified by a real-valued threshold, for simplicity of our notations, we abuse notation and use $h^{\\prime}$ interchangeably as both a mapping (the classifier) and a real-valued number (the corresponding threshold) throughout this section. The same abuse of notation applies to $f$ as well. ", "page_idx": 7}, {"type": "text", "text": "The classifier deployed by the learner is some $h\\geq f$ . We note that it is natural to assume $h\\geq f$ because in our setup, higher values of $x$ are considered \u201cbetter\u201d. So given the strategic behavior of the agents, the learner only wants to make the classification task \u201charder\u201d compared to the ground truth $f$ \u2014 choosing $h<f$ will only hurt the learner\u2019s utility. Because we will extensively make use of the fact that $h\\geq f$ , we state it as a remark below. ", "page_idx": 7}, {"type": "text", "text": "Remark 4.3. The learner\u2019s adopted classifier is some $h\\in\\mathcal H$ such that $h\\geq f$ . ", "page_idx": 7}, {"type": "text", "text": "First, we show that under the introduced setup, the learner\u2019s optimization problem is NP-hard if the prior can be chosen arbitrarily. This is shown by a reduction from the NP-hard subset sum problem. The formal NP-hardness statement and its proof, as well as further useful facts about the agents\u2019 best response under this setup are in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "4.1 An Efficient Algorithm for Discrete Uniform Priors ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given the hardness of the learner\u2019s problem for arbitrary prior distributions, we focus on a specific family of priors, namely, uniform priors over a given set, and examine the existence of efficient algorithms for such priors. In Appendix D.2, we consider continuous uniform priors and provide closed form solutions for the learner\u2019s optimal partial information release problem. ", "page_idx": 7}, {"type": "text", "text": "In this section, we provide an efficient algorithm for computing the learner\u2019s optimal information release when the prior $\\pi$ is a discrete uniform distribution over a set $\\{h_{1},h_{2},\\ldots,h_{n}\\}\\subseteq{\\mathcal{H}}$ that includes the adopted classifier $h$ . Here, the objective of the learner is to release a subset $H\\subseteq$ $\\{h_{1},h_{2},\\ldots,h_{n}\\}$ such that $h\\,\\in\\,H$ . Throughout, we take $h=h_{k}$ where $1\\leq k\\leq n$ , and assume $h_{1}\\leq h_{2}\\leq\\ldots\\leq h_{n}$ . The complete exposition of this section, including all details, proofs, necessary lemmas, and the description of the proposed algorithm, can be found in Appendix $\\boldsymbol{\\mathrm E}$ . ", "page_idx": 7}, {"type": "text", "text": "We first characterize the utility of any subset $H$ released by the learner using a real-valued function of $H$ . Define, for any $H\\subseteq\\{h_{1},\\ldots,h_{n}\\}$ such that $h\\in H$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{H}\\triangleq\\operatorname*{inf}\\left\\{x:\\mathbf{BR}(x,H)\\geq h\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that $\\mathtt{B R}(x\\,=\\,h,H)\\,\\geq\\,h$ for any $H$ such that $h\\:\\in\\:H$ . Therefore, $\\{x:\\mathsf{B R}(x,H)\\geq h\\}$ is nonempty, and that $R_{H}\\leq h$ for any $H$ such that $h\\in H$ . In the following lemma, we show that $R_{H}$ characterizes the utility of $H$ for the learner, for any prior $\\pi$ over $\\{h_{1},\\ldots,h_{n}\\}$ . ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.4. Fix any prior $\\pi$ over $\\{h_{1},\\ldots,h_{n}\\}$ . We have that for any $H\\subseteq\\{h_{1},\\ldots,h_{n}\\}$ such that $h\\in H$ , the utility of the learner, given by Equation 2, can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\nU(H)=\\left\\{1-\\operatorname*{Pr}_{x\\sim D}\\left[R_{H}<x<f\\right]\\quad R_{H}<f\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Given such characterization of the learner\u2019s utility by $R_{H}$ , we show that when the agents\u2019 prior is uniform over $\\{h_{1},\\ldots,h_{n}\\}$ , there are only polynomially many possible values that $R_{H}$ can take, even though there are exponentially many $H\\ 's$ . We characterize the set of possible values for $R_{H}$ as well. For any possible value $R$ of $R_{H}$ , our algorithm efficiently finds a subset $H$ such that $R_{H}=R$ , if such $H$ exists, and finally outputs the one with maximal utility. ", "page_idx": 8}, {"type": "text", "text": "More formally, we consider the following partitioning of the space of subsets of $\\{h_{1},\\ldots,h_{n}\\}$ . For any $\\ell\\in\\{1,2,\\ldots,n\\}$ , and for any $i\\in\\{k,k+1,\\ldots,n\\}$ , define ", "page_idx": 8}, {"type": "equation", "text": "$$\nS_{i,\\ell}\\triangleq\\{H\\subseteq\\{h_{1},\\dots,h_{n}\\}:h\\in H,\\,|H|=\\ell,\\,\\mathbf{B}\\mathbf{R}(h,H)=h_{i}\\}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that $\\mathrm{BR}(h\\,\\equiv\\,h_{k},H)\\;\\in\\;\\{h_{i}\\;:\\;i\\;\\geq\\;k\\}$ for any $H$ . Therefore, $\\{S_{i,\\ell}\\}_{i,\\ell}$ gives us a proper partitioning of the space of subsets, which implies ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{H\\subseteq\\{h_{1},\\ldots,h_{n}\\},h\\in H}U(H)=\\operatorname*{max}_{i,\\ell}\\,\\operatorname*{max}_{H\\in S_{i,\\ell}}U(H)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Given this partitioning of the space, we show that $R_{H}$ can be characterized as follows. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.5. If the prior $\\pi$ is uniform over $\\{h_{1},\\ldots,h_{n}\\}$ , then for any $H\\in S_{i,l},\\,R_{H}=h_{i}-j/\\ell$ where $j=|\\{h^{\\prime}\\in H:h^{\\prime}\\in(R_{H},h_{i}]\\}|$ . ", "page_idx": 8}, {"type": "text", "text": "Given such characterization, our proposed algorithm (Algorithm 3), for any $i,\\ell$ , enumerates over all possible $j\\in\\{1,\\ldots,\\ell\\}$ and returns a $H$ such that $H\\in S_{i,\\ell}$ and $R_{H}=h_{i}-j/\\ell$ , if such $H$ exists. The algorithm then outputs the subset $H$ with maximal utility according to Equation 5. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.6. There exists an algorithm (Algorithm 3) that for any $n$ , any uniform prior over $\\{h_{1},...\\,,h_{n}\\}$ that includes $h$ , and any data distribution $D$ , returns $H^{\\star}\\,\\subseteq\\,\\{h_{1},\\ldots,h_{n}\\}$ in time $O(n^{3})$ such that $h\\in H^{\\star}$ , and that $U(H^{\\star})=\\operatorname*{max}_{H\\subseteq\\{h_{1},\\ldots,h_{n}\\},h\\in H}U(H).$ . ", "page_idx": 8}, {"type": "text", "text": "4.2 Minimizing False Positive (Negative) Rates for Arbitrary Priors ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While so far we worked with accuracy as the utility function of the learner, in this section, we consider other natural performance metrics and provide insights on the optimal information release for the proposed utility functions, without restricting ourselves to uniform priors. In particular, we consider utility functions that are based on False Negative Rate (FNR) and False Positive Rate (FPR) which are formally defined below. For any $H\\subseteq{\\mathcal{H}}$ such that $h\\in H$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{F P R}(H)\\triangleq1-F P R(H)\\triangleq1-\\operatorname*{Pr}_{x\\sim D}\\left[h(B R(x,H))=1|f(x)=0\\right]}\\\\ &{U_{F N R}(H)\\triangleq1-F N R(H)\\triangleq1-\\operatorname*{Pr}_{x\\sim D}\\left[h(B R(x,H))=0|f(x)=1\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In the following theorem, we establish that for any given prior $\\pi$ over a set $\\{h_{1},h_{2},\\ldots,h_{n}\\}\\subseteq{\\mathcal{H}}$ , if the learner aims to minimize the FPR, no-information-release is preferable to full-informationrelease.11 Additionally, we show that for minimizing the FNR, an optimal strategy for the learner is full-information-release. By \u201cno-information-release\u201d, we mean releasing any subset $H$ such that $H$ includes the support of the prior $\\pi$ : $H\\ \\supseteq\\ \\{h_{1},\\ldots,h_{n}\\}$ which results in $\\pi|_{H}\\;=\\;\\pi.\\mathbf{B}\\mathbf{y}$ \u201cfull-information-release\u201d, we mean revealing the classifier: $H=\\{h\\}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.7. Fix any $h\\,\\geq\\,f$ . For any prior $\\pi$ over $\\{h_{1},\\ldots,h_{n}\\}$ that includes $h_{i}$ , we have $^{\\,l}$ ) $F P R(\\mathcal{H})\\leq F P R(\\{h\\}).$ . 2) $F N R(\\{h\\})\\leq F N R(H)$ for every $H\\subseteq{\\mathcal{H}}$ such that $h\\in H$ . ", "page_idx": 8}, {"type": "text", "text": "The proof is provided in Appendix F. In Appendix $\\mathrm{G}$ , we show that minimizing FPR, unlike minimizing FNR, does not always have a clear optimal solution. We provide three instances such that full-information-release is optimal for the first, no-information-release is optimal for the second, and neither is optimal for the third. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We introduce Bayesian Strategic Classification, meaning strategic classification with partial knowledge (of the agents) and partial information release (of the learner). Our model relaxes the often unrealistic assumption that agents fully know the learner\u2019s deployed classifier. Instead, we model agents as having a distributional prior on which classifier the learner is using. Our results show the existence of previously unknown intriguing informational middle grounds; we also demonstrate the necessity of revisiting the fundamental modeling assumptions of strategic classification in order to provide effective recommendations to practitioners in high-stakes, real-world prediction tasks. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank Avrim Blum for helpful discussions in the early stages of this work. Special thanks to Roy Long for suggesting that our model might leak more information than intended, and to Odelia Lorch for identifying an instance where this occurs and providing a proof (located in Appendix H). ", "page_idx": 9}, {"type": "text", "text": "Lee Cohen is supported by the Simons Foundation Collaboration on the Theory of Algorithmic Fairness, the Sloan Foundation Grant 2020-13941, and the Simons Foundation investigators award 689988. Kevin Stangl was supported in part by the National Science Foundation under grants CCF2212968 and ECCS-2216899, by the Simons Foundation under the Simons Collaboration on the Theory of Algorithmic Fairness, and by the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003. The views expressed in this work do not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Y. Bechavod, K. Ligett, S. Wu, and J. Ziani. Gaming helps! learning from strategic interactions in natural dynamics. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1234\u20131242, 2021.   \nY. Bechavod, C. Podimata, S. Wu, and J. Ziani. Information discrepancy in strategic learning. In International Conference on Machine Learning (ICML), pages 1691\u20131715, 2022.   \nH. Beyhaghi, M. K. Camara, J. Hartline, A. Johnsen, and S. Long. Screening with Disadvantaged Agents. In 4th Symposium on Foundations of Responsible Computing (FORC), volume 256 of Leibniz International Proceedings in Informatics (LIPIcs), pages 6:1\u20136:20, 2023.   \nJ. R. Biden. Executive order on the safe, secure, and trustworthy development and use of artificial intelligence. 2023.   \nM. Braverman and S. Garg. The role of randomness and noise in strategic classification. In Foundations of Responsible Computing (FORC), volume 156 of LIPIcs, pages 9:1\u20139:20, 2020.   \nM. Br\u00fcckner and T. Scheffer. Stackelberg games for adversarial prediction problems. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 547\u2013555, 2011.   \nM. El Halabi and S. Jegelka. Optimal approximation for unconstrained non-submodular minimization. In International Conference on Machine Learning, pages 3961\u20133972. PMLR, 2020.   \nG. Ghalme, V. Nair, I. Eilat, I. Talgam-Cohen, and N. Rosenfeld. Strategic classification in the dark. In International Conference on Machine Learning, pages 3672\u20133681. PMLR, 2021.   \nN. Haghtalab, C. Podimata, and K. Yang. Calibrated stackelberg games: Learning optimal commitments against calibrated agents. Advances in Neural Information Processing Systems, 36, 2023.   \nM. Hardt, N. Megiddo, C. Papadimitriou, and M. Wootters. Strategic classification. In Proceedings of the 2016 ACM conference on innovations in theoretical computer science, pages 111\u2013122, 2016.   \nK. Harris, V. Chen, J. Kim, A. Talwalkar, H. Heidari, and S. Z. Wu. Bayesian persuasion for algorithmic recourse. Advances in Neural Information Processing Systems, 35:11131\u201311144, 2022.   \nL. Hu, N. Immorlica, and J. W. Vaughan. The disparate effects of strategic manipulation. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 259\u2013268, 2019.   \nM. Jagadeesan, C. Mendler-D\u00fcnner, and M. Hardt. Alternative microfoundations for strategic classification. In International Conference on Machine Learning, pages 4687\u20134697. PMLR, 2021.   \nE. Kamenica and M. Gentzkow. Bayesian persuasion. American Economic Review, 101(6):2590\u2013 2615, 2011.   \nR. Lake. How to get a personal loan without a credit check. Time, 2024. URL https://time.com/personal-finance/article/ how-to-get-a-personal-loan-without-a-credit-check/.   \nS. Milli, J. Miller, A. D. Dragan, and M. Hardt. The social cost of strategic classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 230\u2013239, 2019.   \nG. D. P. Regulation. General data protection regulation (gdpr). Intersoft Consulting, Accessed in October, 24(1), 2018.   \nA. C.-C. Yao. Probabilistic computations: Toward a unified measure of complexity. In 18th Annual Symposium on Foundations of Computer Science (sfcs 1977), pages 222\u2013227. IEEE Computer Society, 1977. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Limitations, and Broader Impacts, and Future Work ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Limitations Potential limitations of our model are the following: ", "page_idx": 11}, {"type": "text", "text": "\u2022 We assume the agents\u2019 prior is realizable, in the sense that the classifier deployed by the learner is in the support of the prior. This is a standard assumption in machine learning works, and it will be interesting to relax it in future works.   \n\u2022 The learner, in order to decide on the optimal level of information release, must know the agents\u2019 prior. This assumption, while common in related settings like Bayesian persuasion, may be unrealistic in practice; although, in real-life, the learner may have an imperfect idea of or be able to partially recover the agents\u2019 priors from previous interactions with them. Beyond this, we note that in practice, different agents may also have different beliefs and priors about the learner\u2019s model; this can affect the way the learner should release information, given that this information release may affect different users differently.   \n\u2022 A limitation of this model is that it assumes the learner must commit to a fixed classifier in advance. In real-life, classifiers are dynamically updated over time, using the additional information obtained from each decision. However, we note that changing the screening algorithm requires significant resources, and the rate at which the classifier is updated is generally slower than the rate at which decisions are made. In practice, this means that strategic agents effectively face a fixed model in each \u201cbatch\u201d between updates.   \n\u2022 If agents know the prior distribution $D$ and the mapping $f$ from feature vectors to labels, they might infer information regarding $h^{*}$ when $|H|>1$ . We show an example of such a case in Appendix H. ", "page_idx": 11}, {"type": "text", "text": "Broader Impacts On the plus side, our approach provides a deeper understanding of strategic behavior in machine learning settings, when strategic agents may not fully understand the deployed model. By doing so, we are taking the understanding of strategic classification one step closer to real life, providing useful insights on how much information a learner should provide about their model to prevent model manipulation and gaming. ", "page_idx": 11}, {"type": "text", "text": "One potential negative impact is that our approach takes the point of view of the learner who is solely interested in maximizing his own accuracy (or utility). It is well-known that this focus on accuracy can lead to unfairness and disparate harms across different populations; further, prior work studying fairness in the standard strategic classification setting [Hu et al., 2019, Milli et al., 2019] and in a related partial information setting [Bechavod et al., 2022] have shown that strategic classification can amplify these disparities. ", "page_idx": 11}, {"type": "text", "text": "Future Work [Hu et al., 2019, Milli et al., 2019] consider disparities across different groups due to differing cost functions. In our model of strategic classification, different population groups may not only have differing cost functions but also differing prior distributions: network homophily, social disparities, and stratification can cause population groups to have distinct priors, leading to further disparities across groups. In turn, it will be critical in future work to design fairness-aware information release strategies by a learner faced with strategic behavior. ", "page_idx": 11}, {"type": "text", "text": "B Oracle-Efficient Approximate Best Response for $V$ -Submodular Costs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this section we give a sufficient condition on the cost function under which we can give an approximation algorithm for the best response of the agents. In particular, for a given collection of classifiers $V\\subseteq\\mathcal{H}$ , we introduce the notion of $V$ -submodular cost functions which is a natural condition that can arise in many applications. Borrowing results from the literature on submodular optimization, we then show that for any distribution $P$ over a set $V\\,=\\,\\{h_{1},\\ldots,h_{n}\\}$ , if the cost function is $V$ -submodular, there exists an oracle-efficient approximation algorithm for the best response problem. Recall, from Equation (3), that the best response problem faced by agent $x$ can be written as $\\begin{array}{r}{\\operatorname*{max}_{S\\subseteq\\{h_{1},\\ldots,h_{n}\\}}g_{x}(S)\\triangleq\\sum_{h^{\\prime}\\in S}P(h^{\\prime})-c(x,S)}\\end{array}$ where, with slight abuse of notation, we define ", "page_idx": 11}, {"type": "equation", "text": "$$\nc(x,S)\\triangleq\\operatorname*{min}_{z:h^{\\prime}(z)=1,\\,\\forall h^{\\prime}\\in S}c(x,z)\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "For any $S\\subseteq\\{h_{1},\\ldots,h_{n}\\}$ , $c(x,S)$ is simply the minimum cost that the agent $x$ has to incur in order to pass all classifiers in $S$ , and can be computed via the oracle (Algorithm 1). We now state our main assumption on the cost function: ", "page_idx": 12}, {"type": "text", "text": "Definition B.1 $V$ -Submodularity). Let $V=\\{h_{1},\\ldots,h_{n}\\}$ be any collection of classifiers. We say $a$ cost function c is $V$ -submodular, if for all $x$ , the set function $c(\\boldsymbol{x},\\dot{\\cdot}):2^{V}\\rightarrow\\mathbb{R}$ defined in Equation $\\boldsymbol{\\vartheta}$ is submodular: for every $S,S^{\\prime}\\subseteq V$ such that $S\\subseteq S^{\\prime}$ and every $h^{\\prime}\\notin S^{\\prime}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\nc\\left(x,S\\cup\\{h^{\\prime}\\}\\right)-c\\left(x,S\\right)\\geq c\\left(x,S^{\\prime}\\cup\\{h^{\\prime}\\}\\right)-c\\left(x,S^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This condition asks that the marginal cost of passing the new classifier $h^{\\prime}$ is smaller when the new classifier is added to $S^{\\prime}$ versus $S$ , for any such $h^{\\prime},S,{\\bar{S^{\\prime}}}$ . Fix a collection of classifiers $V$ . Informally speaking, a cost function is $V$ -submodular if the agent\u2019s manipulation to pass a classifier only helps her (i.e., reduces her cost) to pass other classifiers: the more classifiers the agent passes, it becomes only easier for her to pass an additional classifier. This can happen in settings where some of the knowledge to pass a certain number of tests is transferable across tests. Some real-life examples include: 1) a student that is preparing for a series of math tests on topics like probability, statistics, and combinatorics. 2) a job applicant who is applying for multiple jobs within the same field and preparing for their interviews. ", "page_idx": 12}, {"type": "text", "text": "We give a formal example of a $V$ -submodular cost function below. In particular, we show that when $\\mathcal{X}=\\mathbb{R}$ , the cost function $c(x,x^{\\prime})=|x-x^{\\prime}|$ is $V$ -submodular where $V$ can be any set of single-sided threshold classifiers. ", "page_idx": 12}, {"type": "text", "text": "Claim B.2. Let $\\textstyle X=\\mathbb{R},$ , and $V=\\{h_{1},\\ldots,h_{n}\\}$ where every $h_{i}$ can be written as $h_{i}(x)=\\mathbb{1}[x\\geq t_{i}]$ for some $t_{i}\\in\\mathbb{R}$ . We have that the cost function $c(x,x^{\\prime})=|x-x^{\\prime}|$ is $V$ -submodular. ", "page_idx": 12}, {"type": "text", "text": "Proof of Claim B.2. We will abuse notation and use $h_{i}$ for the threshold $t_{i}$ $\\langle h_{i}\\equiv t_{i}\\in\\mathbb{R}_{}$ ). ", "page_idx": 12}, {"type": "text", "text": "Fix any $x$ . Consider $S\\subseteq S^{\\prime}\\subseteq\\{h_{1},\\ldots,h_{n}\\}$ and $h^{\\prime}\\in\\mathbb{R}$ such that $h^{\\prime}\\notin S^{\\prime}$ . Note that ", "page_idx": 12}, {"type": "equation", "text": "$$\nc(x,S)=\\operatorname*{max}\\left(\\operatorname*{max}(S)-x,0\\right),\\quad c\\left(x,S\\cup\\{h^{\\prime}\\}\\right)=\\operatorname*{max}\\left(\\operatorname*{max}(S\\cup\\{h^{\\prime}\\})-x,0\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\nc\\left(x,S^{\\prime}\\cup\\{h^{\\prime}\\}\\right)=\\operatorname*{max}\\left(\\operatorname*{max}\\left(S^{\\prime}\\cup\\{h^{\\prime}\\}\\right)-x,0\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\operatorname*{max}(F)$ is simply the largest threshold in $F$ , for any set $F$ . Note that $\\operatorname*{max}(S)\\leq\\operatorname*{max}(S^{\\prime})$ because $S\\subseteq S^{\\prime}$ . Suppose $x\\leq\\operatorname*{max}(S)$ . We have three cases ", "page_idx": 12}, {"type": "text", "text": "1. If $h^{\\prime}\\geq\\operatorname*{max}(S^{\\prime})$ , then ", "page_idx": 12}, {"type": "equation", "text": "$$\nc\\left(x,S\\cup\\{h^{\\prime}\\}\\right)-c\\left(x,S\\right)=h^{\\prime}-\\operatorname*{max}(S)\\geq h^{\\prime}-\\operatorname*{max}(S^{\\prime})=c\\left(x,S^{\\prime}\\cup\\{h^{\\prime}\\}\\right)-c\\left(x,S^{\\prime}\\right)\\in\\{0,\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "2. If $\\operatorname*{max}(S)\\leq h^{\\prime}\\leq\\operatorname*{max}(S^{\\prime})$ , then ", "page_idx": 12}, {"type": "equation", "text": "$$\nc\\left(x,S\\cup\\left\\{h^{\\prime}\\right\\}\\right)-c\\left(x,S\\right)=h^{\\prime}-\\operatorname*{max}(S)\\geq0=c\\left(x,S^{\\prime}\\cup\\left\\{h^{\\prime}\\right\\}\\right)-c\\left(x,S^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "3. If $h^{\\prime}\\leq\\operatorname*{max}(S)$ , then ", "page_idx": 12}, {"type": "equation", "text": "$$\nc\\left(x,S\\cup\\{h^{\\prime}\\}\\right)-c\\left(x,S\\right)=c\\left(x,S^{\\prime}\\cup\\{h^{\\prime}\\}\\right)-c\\left(x,S^{\\prime}\\right)=0\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "So we have shown that the cost function is submodular if $x\\leq\\operatorname*{max}(S)$ . We can similarly, using a case analysis, show that the cost function is submodular when $x>\\operatorname*{max}(S)$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "We now state the main result of this section. ", "page_idx": 12}, {"type": "text", "text": "Theorem B.3. Fix any $\\mathcal{H}$ and any distribution $P$ over some $V\\,=\\,\\{h_{1},\\ldots,h_{n}\\}\\,\\subseteq\\,{\\mathcal{H}}$ . If the cost function $c$ is $V$ -submodular, then there exists an algorithm that for every agent $x$ and every $\\epsilon\\mathrm{~>~0~}$ , makes $\\tilde{O}(n/\\epsilon^{2})$ calls to the oracle (Algorithm $^{\\,l}$ ) and outputs a set $\\hat{S}\\,\\subseteq\\,V$ such that $g_{x}(\\hat{S})\\geq\\operatorname*{max}_{S\\subseteq V}g_{x}(S)-\\epsilon.$ . ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem B.3. Note that when the cost function is $V$ -submodular, the objective function $g_{x}$ can be written as the difference of a monotone non-negative modular function12 and a monotone non-negative submodular function: $g_{x}:2^{V}\\to\\mathbb{R}$ , $\\begin{array}{r}{g_{x}\\bar{(}S\\rangle=\\sum_{h^{\\prime}\\in{\\cal{S}}}P(h^{\\prime})-c(x,S)}\\end{array}$ . The result then follows from [El Halabi and Jegelka, 2020] where the y provide an efficient algorithm for approximately maximizing set functions with such structure. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "C Missing Proofs of Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 3.1. $\\Omega(2^{n}/{\\sqrt{n}})$ calls to the oracle (Algorithm $^{\\,l}$ ) are required to compute the best response of an agent with $2/3$ probability of success, even when $\\mathcal{X}=\\mathbb{R}^{2}$ and the cost function is $c_{p}$ for $p\\geq1$ . ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3.1. To prove the claim, we reduce the following hidden-set detection problem with EQUALTO(\u00b7) oracle to our best response problem. In hidden-set detection problem, given two players, Alice and Bob, with Bob possessing a \u2018hidden\u2019 subset $S^{\\star}\\subseteq[n]$ of size $n/2$ , Alice aims to identify Bob\u2019s set $S^{\\star}$ using the minimum number of queries to Bob. She has query access to EQUALTO $(T)$ oracle that checks whether her set $T\\subset[n]$ matches Bob\u2019s set $(S^{\\star})$ . It is trivial that any randomized algorithm for the hidden-set detection problem with success probability of at least $1-O(1)$ requires $\\scriptstyle{\\binom{n}{n/2}}$ queries in the worst-case scenario. This is via a straightforward application of Yao\u2019s Min-Max principle Yao [1977]: consider a uniform distribution over all subsets of size $n/2$ from $[n]$ , as the Bob\u2019s set. Then, after querying half of the subsets of size $n/2$ , the failure probability of Alice in detecting Bob\u2019s set is at least $(1-1/n)(1-1/(n-1))\\cdot\\cdot(1-1/(n/2))>(1-2/n)^{n/2}>e^{-1}(1-2/n)>1/3$ for sufficiently large values of $n$ . ", "page_idx": 13}, {"type": "text", "text": "Next, corresponding to an instance of the hidden-set detection problem with $S^{\\star}$ , we create an instance of the agents\u2019 best response problem and show that any algorithm that computes the best response with success probability at least $2/3$ using $N$ oracle calls (Algorithm 1), detects the hidden set of the given instance of the hidden-set detection problem using at most $N$ calls of EQUALTO(\u00b7) with probability at least $2/3$ . Hence, co\u221amputing the best response problem with success probability at least $2/3$ requires $\\binom{\\dot{n}}{n/2}=\\Omega(2^{n}/\\sqrt{n})$ oracle calls. ", "page_idx": 13}, {"type": "text", "text": "Let $n=2k$ and $\\epsilon<1/n$ . Corresponding to every subset $S\\subset[n]$ of size $n/2-1$ , there is a distinct point $x_{S}$ at distance $1/2-\\epsilon$ from the origin, i.e., $\\|x_{S}\\|_{p}=1/2-\\epsilon$ . Corresponding to every subset $S\\subset[n]$ of size $n/2$ , there are two distinct points $x_{S,n}$ and $x_{S,f}$ at distances respectively $\\mathrm{\\dot{1}/2-\\epsilon}$ (near) and $1/2+\\dot{\\epsilon}$ (far) from the origin, i.e., $\\|x_{S,n}\\|_{p}=1/2-\\epsilon$ and $\\|x_{S,f}\\|_{p}=\\bar{1}/2+\\epsilon$ . ", "page_idx": 13}, {"type": "text", "text": "Now, we are ready to describe the instance $I_{S^{\\star}}$ of our best response problem corresponding to the given hidden-set detection problem with $S^{\\star}$ . We define $\\mathcal{H}=\\{\\bar{h}_{1},\\cdot\\cdot\\cdot,h_{n}\\}$ and distribution $P$ over $\\mathcal{H}$ such that, ", "page_idx": 13}, {"type": "text", "text": "\u2022 $P$ is a uniform distribution over all classifiers $\\mathcal{H}$ , i.e., $P(h_{i})=1/n$ for every $i\\in[n]$ . \u2022 For every subset $T\\subset[n]$ of size $n/2-1$ , we define $h_{i}(x_{T})=\\mathbb{1}[i\\in T]$ . \u2022 For every subset $T\\subset[n]$ of size $n/2$ , we define $h_{i}(x_{T,f})=\\mathbb{1}[i\\in T]$ . Moreover, if $T\\neq S^{\\star}$ , then $h_{i}(x_{T,n})=0$ for all $i\\in[n]$ . Otherwise, if $T=S^{\\star}$ , we define $\\bar{h}_{i}(x_{T,n})=\\mathbb{1}[i\\in T]$ . \u2022 Finally, for the remaining points in $\\mathcal{X}$ , i.e., $x^{\\prime}\\in\\mathbb{R}^{2}\\setminus\\left(\\{x_{T}:T\\subset\\,[n]\\}\\right.$ and $|T|\\,=\\,n/2\\,-\\,1\\}\\cup$ $\\{x_{T,n},x_{T,f}\\,:\\,T\\,\\subset\\,[n]$ and $\\left|T\\right|=n/2\\}$ ), we define $h_{i}(x^{\\prime})\\,=\\,0$ for all $i\\,\\in\\,[n]$ . In other words, points that do not correspond to subsets of size $n/2-1$ or $n/2$ are classified as negative examples by every classifier in $\\mathcal{H}$ . ", "page_idx": 13}, {"type": "text", "text": "In the constructed instance $I_{S^{\\star}}$ , no point is classified as positive by more than $n/2$ classifiers in $\\mathcal{H}$ , and the $p$ -norm distance from the origin for all points classified as positive by a subset of classifiers is at least $1/2-\\epsilon$ . Therefore, the best response for an agent located at the origin of the space is $x_{S^{\\star},n}$ , yielding a utility of $1/2-(1/2-\\epsilon)=\\bar{\\epsilon}>0$ . Hence, the computational task in computing the best response involves identifying the (hidden) subset $S^{\\star}$ . Refer to Figure 1 for a description of $I_{S^{*}}$ . ", "page_idx": 13}, {"type": "text", "text": "Although we described the construction of $I_{S^{\\star}}$ , what we need to show to get the exponential lower bound on the oracle complexity of the best response problem is constructing an oracle (i.e., an implementation of Algorithm 1), using the EQUALTO $(\\cdot)$ oracle, consistent with $I_{S^{\\star}}$ . To do so, given a subset of classifiers specified by $T\\subset[n]$ , the oracle returns as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 if $|T|>n/2$ : It returns an empty set.   \n\u2022 if $|T|<n/2$ : It returns $x_{T^{\\prime}}$ for an arbitrary set $T^{\\prime}\\supseteq T$ of size $n/2\\!-\\!1$ . Note that $\\Vert x_{T^{\\prime}}\\Vert_{p}=1/2\\!-\\!\\epsilon$ .   \n\u2022 if $|T|=n/2$ and EQUALTO $\\mathbf{(}T)=$ FALSE: It returns $x_{T,f}$ . Note that $\\|x_{T,f}\\|_{p}=1/2+\\epsilon.$ . ", "page_idx": 13}, {"type": "image", "img_path": "SadbRPoG2k/tmp/d527f16cbc9ffa643d57f40a681af9574209588822f213dded17880956dbc40b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 1: In this example, we consider $p=2$ , i.e., $c(x,x^{\\prime})=\\|x,x^{\\prime}\\|_{2}$ . The agent is located at the origin. Blue nodes correspond to a point in the intersection of the positive regions of subsets of classifiers of size ${\\frac{n}{2}}\\,-\\,1$ , each located at a Euclidean distance of $1/2-\\epsilon$ from the origin, where $\\epsilon$ is a small positive value. Moreover, points in the intersection of the positive regions of subsets classifiers of size $\\begin{array}{l}{{\\frac{n}{2}}}\\end{array}$ are indicated by red points, all except the one corresponding to $S^{\\star}$ are located at a Euclidean distance of $1/2+\\epsilon$ from the origin. The red point corresponding to $S^{\\star}$ is uniquely placed at a distance of $1/2-\\epsilon$ from the origin, similar to the blue nodes. Furthermore, all points, corresponding to different subsets, are located at distinct locations in the space. ", "page_idx": 14}, {"type": "text", "text": "\u2022 if $|T|=n/2$ and EQUALTO $\\mathbf{\\nabla}\\mathbf{\\Omega}(T)=\\mathbf{T}\\mathbf{R}\\mathbf{U}\\mathbf{E}$ : It returns $x_{T,n}$ . Note that $\\Vert x_{T,n}\\Vert_{p}=1/2-\\epsilon$ . ", "page_idx": 14}, {"type": "text", "text": "Remark C.1. As in each instance $I_{S^{\\star}}$ the only point with strictly positive utility is $x_{S^{*},n}$ , our proof for Theorem 3.1 essentially rules out the existence of any approxi\u221amation algorithm for the best response problem with success probability at least $2/3$ using $\\bar{o}(2^{n}/\\sqrt{n})$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem 3.2. Suppose $\\mathcal{X}=\\mathbb{R}^{d}$ for some $d\\ll n$ , and $\\mathcal{H}$ contains only linear classifiers. Then for any agent $x$ , any cost function $c$ , and any distribution $P$ over $\\{h_{1},\\ldots,h_{n}\\}\\subseteq{\\mathcal{H}}$ , Algorithm 2 returns the best response of the agent in time $O(n^{d+1})$ , while making $O(n^{d})$ calls to the oracle (Algorithm 1). ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 3.2. The fact that Algorithm 2 returns the best response follows from the construction of the algorithm. We first prove the oracle and the runtime complexity for $d\\,=\\,2$ and then generalize it to any $d$ . The oracle complexity of Algorithm 2 is $\\lvert R_{n}\\rvert$ . Note that $|R_{1}|=2$ , and for any $n\\geq2$ , $|R_{n}|\\leq|R_{n-1}|+n$ . This is because the line $\\{z:h_{n}(z)=0\\}$ intersects the lines formed by $\\{h_{1},\\ldots,h_{n-1}\\}$ in at most $n-1$ points, which will then partition $\\dot{\\{z:h_{n}(z)=0\\}}$ into at most $n$ segments. Each segment of the new line then splits a region in $R_{n-1}$ into two regions. So, there are at most $n$ new regions when $h_{n}$ is introduced. The recursive relationship implies that $\\begin{array}{r}{|R_{n}|\\,\\leq\\,1\\,+\\,\\frac{n(n+1)}{2}\\,=\\,\\bar{O(n^{2})}}\\end{array}$ . The runtime complexity of the algorithm is then given by $\\begin{array}{r}{O\\left(\\sum_{i=1}^{n}\\vert R_{i}\\vert\\right)=O(\\bar{n}^{3})}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Now consider any dimension $d$ and let $R(n,d)$ denote the number of partitions induced by the classifiers $\\{h_{1},\\ldots,h_{n}\\}$ . Note that in this case we have $R(n,d)\\leq R(n-1,d)+R(n-1,d-1)$ . The first term on the right hand side is the number of regions induced by $\\{h_{1},\\ldots,h_{n-1}\\}$ and the second term is the number of splits (dividing a region into two) when $h_{n}$ is introduced. Note that $\\{z:h_{n}(z)=0\\}$ is a $d-1$ -dimensional hyperplane and the number of splits induced by $h_{n}$ is simply the number of regions induced by $\\{h_{1},\\ldots,\\bar{h_{n-1}}\\}$ on $\\{z:h_{n}(z)=0\\}$ , which is $R(n-1,d-1)$ . The recursive relationship implies that $\\begin{array}{r}{|R_{n}|=R(n,d)\\leq\\sum_{j=0}^{d}{\\binom{n}{j}}=O(n^{d}).}\\end{array}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "D Missing Details of Section 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first state a remark about the tie-breaking of agents\u2019 best response problem: ", "page_idx": 14}, {"type": "text", "text": "Remark D.1. As mentioned in the model section, when there are several utility-maximizing solutions for the agents, we always break ties in favor of the lowest cost solution. Furthermore, each agent x in our setup manipulate only to larger values of $x$ $\\left\\langle x^{\\prime}\\geq x\\right\\rangle$ ); this is formally stated in the first part of ", "page_idx": 14}, {"type": "text", "text": "Lemma D.2. Therefore, the tie-breaking of agents is in favor of smaller values of $x^{\\prime}$ in our setup. In other words, given some released information $H$ , an agent $x$ chooses ", "page_idx": 15}, {"type": "equation", "text": "$$\nB R(x,H)=\\operatorname*{min}{\\left\\{\\operatorname{argmax}_{x^{\\prime}\\geq x}u_{x}(x^{\\prime},H)\\right\\}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the rest of this section, when we write ar $\\mathrm{gmax}_{x^{\\prime}\\geq x}\\,u_{x}(x^{\\prime},H).$ , we implicitly are taking the smallest $x^{\\prime}\\geq x$ that maximizes the utility of the agent $x$ . ", "page_idx": 15}, {"type": "text", "text": "Next, we state some useful facts about the agents\u2019 best response in our setup. ", "page_idx": 15}, {"type": "text", "text": "Lemma D.2. Fix any prior $\\pi$ and any points $x_{2}\\geq x_{1}$ . We have that, for any $H\\subseteq{\\mathcal{H}}$ , ", "page_idx": 15}, {"type": "text", "text": "1. $B R(x_{1},H)\\geq x_{1}$ .   \n2. $B R(x_{2},H)\\ge B R(x_{1},H)$ .   \n3. If $B R(x_{1},H)\\ge x_{2}$ , then $B R(x_{1},H)=B R(x_{2},H).$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma $D.2$ . Fix any $x$ , and any $H$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\mathbf{BR}(x,H)=\\mathop{\\operatorname{argmax}}_{x^{\\prime}}\\left\\{\\underset{h^{\\prime}\\sim\\pi\\mid H}{\\operatorname*{Pr}}[x^{\\prime}\\geq h^{\\prime}]-|x^{\\prime}-x|\\right\\}}\\\\ &{\\qquad\\qquad=\\mathop{\\operatorname{argmax}}_{x^{\\prime}\\geq x}\\left\\{\\underset{h^{\\prime}\\sim\\pi\\mid H}{\\operatorname*{Pr}}[x^{\\prime}\\geq h^{\\prime}]-(x^{\\prime}-x)\\right\\}}\\\\ &{\\qquad\\qquad=\\mathop{\\operatorname{argmax}}_{x^{\\prime}\\geq x}\\left\\{\\underset{h^{\\prime}\\sim\\pi\\mid H}{\\operatorname*{Pr}}[x^{\\prime}\\geq h^{\\prime}]-x^{\\prime}\\right\\}}\\\\ &{\\qquad\\qquad=\\mathop{\\operatorname{argmax}}_{x^{\\prime}\\geq x}g_{H}(x^{\\prime})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we take $g_{H}(x^{\\prime})\\triangleq\\operatorname*{Pr}_{h^{\\prime}\\sim P|_{H}}\\left[x^{\\prime}\\geq h^{\\prime}\\right]-x^{\\prime}$ . The first equality follows because agents do not gain any utility by moving to a point $x^{\\prime}<x$ , and that tie-breaking is in favor of lowest cost solution. The first and the second part of the lemma follows from this derivation. For the third part, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{BR}(x_{1},H)=\\operatorname*{argmax}_{x^{\\prime}\\geq x_{1}}g_{H}(x^{\\prime})=\\operatorname*{argmax}_{x^{\\prime}\\geq x_{2}}g_{H}(x^{\\prime})=\\mathrm{BR}(x_{2},H)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second equality follows because $\\mathrm{BR}(x_{1},H)\\geq x_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "D.1 NP-Hardness of Learner\u2019s Optimization Problem with Arbitrary Prior Distributions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we formally state the NP-hardness of the learner\u2019s optimization problem in the general setting. ", "page_idx": 15}, {"type": "text", "text": "Theorem D.3. Consider an arbitrary prior \u03c0 over a set of threshold classifiers $\\{h_{1},h_{2},\\ldots,h_{n}\\}\\subseteq{\\mathcal{H}}$ that includes h. The problem of finding $H\\subseteq\\{h_{1},h_{2},\\ldots,h_{n}\\}$ so that $h\\in H$ and the learner\u2019s utility $U(H)$ is maximized is NP-hard. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem D.3. The proof is via a reduction from the subset sum problem. In particular, we consider a variant of the subset sum problem in which we are given a set of $n$ positive numbers $a_{1},\\cdot\\cdot\\cdot\\,,a_{n}$ , and the goal is to decide whether a subset $S\\subset[\\bar{n}]$ such that $\\textstyle\\sum_{i\\in S}a_{i}=T:=$ $(1/2)\\sum_{i\\in[n]}a_{i}$ exists. ", "page_idx": 15}, {"type": "text", "text": "Given an instance of the subset sum problem with input $(\\{a_{1},\\cdot\\cdot\\cdot,a_{n}\\},T:=(1/2)\\textstyle\\sum_{i\\in[n]}a_{i})$ , we construct the following instance of our problem with one-dimensional threshold classifiers. Define $f(x)=\\mathbb{1}\\left[x\\geq0\\right]\\!,$ , $h(\\bar{x})=1\\,[x\\geq2/3]$ , and $h_{i}(x)=\\mathbb{1}\\left[x\\geq100+i\\right]$ for every $i\\in[n]$ . Moreover, suppose that the prior distribution of the agents $\\pi$ is given by: $\\pi(h)=1/2$ and for every $i\\,\\in\\,[n]$ , $\\pi(\\dot{h_{i}})\\,=\\,a_{i}/(4T)$ . Note that $\\begin{array}{r}{\\pi(h)+\\sum_{i}\\bar{\\pi}(h_{i})\\,=\\,1}\\end{array}$ . Let the data distribution $D$ be the uniform distribution over $[-1000,1000]$ . ", "page_idx": 15}, {"type": "text", "text": "Intuitively speaking, the inclusion of $h_{i}$ \u2019s in $H$ have no direct effect on the accuracy of the released subset $H$ , as they can only lead to a subset of the agents located at $x\\ge100$ to manipulate. However, their presence in $H$ will impact the probability mass of $h$ under the posterior $\\pi|_{H}$ , which is given by $\\pi|_{H}(h)=\\pi(h)/\\pi(H)\\triangleq\\rho_{H}$ . We will show that the learner can achieve perfect accuracy $i f$ and only $i f$ in the given instance subset sum problem there exists a subset which sums up to $T$ . To see this consider the following cases for the released information $H$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 Case 1: $\\rho_{H}\\,>\\,2/3$ . For any such $H$ , all agents at distance $\\rho_{H}$ from $2/3$ gain positive utility by manipulating to $x^{\\prime}\\,=\\,2/3$ . Hence, the utility of such solutions for the learner is given by $\\bar{1}^{\\bullet}-\\mathrm{Pr}_{x\\sim{\\cal D}}[x\\in[\\frac23-\\rho_{H},0)]<1$ .   \n\u2022 Case 2: $\\rho_{H}<2/3.$ . For any such $H$ , as all classifiers in $H\\setminus\\{h\\}$ are located at $t>100$ , no agent belonging to $[0,2/3-\\rho_{H})$ gain positive utility by manipulating to $x^{\\prime}=2/3$ . Hence, these points will be misclassified by $h$ , and consequently, the utility of such solutions for the learner is given by $1-\\mathrm{Pr}_{x\\sim D}[x\\in[0,\\frac{2}{3}\\stackrel{\\cdot}{-}\\dot{\\rho}_{H})]<1$ .   \n\u2022 Case 3: $\\rho_{H}=2/3\\mathrm{.}$ . By similar arguments to the previous cases, all agents belonging to $[0,2/3)$ manipulate to $x^{\\prime}=2/3$ and all points with negative labels $(x<0)$ ) stay at their location. Therefore, no one will be misclassified, and therefore, the utility of such solutions is 1. ", "page_idx": 16}, {"type": "text", "text": "Note that because \u03c1H = \u03c0\u03c0((Hh)) $\\begin{array}{r}{\\rho_{H}\\,=\\,\\frac{\\pi(h)}{\\pi(H)}\\,=\\,\\frac{1/2}{1/2+\\pi(H\\cap\\{h_{1},\\cdots,h_{n}\\})}}\\end{array}$ , we have that $\\rho_{H}\\,=\\,2/3$ if and only if $\\pi(H\\cap\\{h_{1},\\cdot\\cdot\\cdot\\,,h_{n}\\})=1/4$ . But $\\textstyle\\pi(\\dot{H}\\cap\\{h_{1},\\dot{\\cdot}\\cdot\\cdot,h_{n}\\})=1/(4T)\\sum_{h_{i}\\in H}a_{i}$ . We therefore have that $\\rho_{H}=2/3$ if and only if $\\sum_{h_{i}\\in H}a_{i}=T$ . Hence, deciding whether the learner\u2019s optimization problem has a solution with p erfect utility is equivalent to deciding whether in the given subset sum problem there exists a subset $S\\subset[n]$ such that $\\begin{array}{r}{\\sum_{i\\in S}a_{i}=T:=\\bar{(}1/2)\\sum_{i\\in[n]}a_{i}}\\end{array}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "D.2 A Closed-form Solution for Continuous Uniform Priors ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide closed-form solutions for continuous uniform priors. More concretely, we assume in this section that $\\pi$ is the uniform distribution over an interval $[a,b]\\subset\\mathbb{R}$ that includes $h$ . The information release of the learner will then be releasing an interval $H=[c,d]\\subseteq[a,b]$ such that $h\\in[c,d]$ . ", "page_idx": 16}, {"type": "text", "text": "For example, a student may know that a GPA of 3.5 or higher will guarantee admission to a certain college, but not the exact threshold. Similarly, a loan applicant might know that a credit score above 650 will likely suffice for securing a loan, but not the precise cutoff. These uncertainties are sometimes due to factors unknown to the agents, such as the financial situation of the lender. Therefore, agents treat the threshold as uniformly distributed within a known and reasonable range. ", "page_idx": 16}, {"type": "text", "text": "Theorem D.4. Fix any data distribution $D$ over $\\mathcal{X}$ . Suppose the prior $\\pi$ is uniform over an interval $[a,b]$ for some $a,b$ such that $h\\in[a,b]$ . Define $H_{c}\\triangleq[c,d]$ where $d\\triangleq\\operatorname*{min}\\left(b,\\operatorname*{max}\\left(h,f+1\\right)\\right)$ . ", "page_idx": 16}, {"type": "text", "text": "If $b-a<1$ , we have that $H^{\\star}=H_{c}$ is optimal for any $c\\in[a,h]$ , with corresponding utility ", "page_idx": 16}, {"type": "equation", "text": "$$\nU(H_{c})={\\binom{1-\\operatorname*{Pr}_{x\\sim D}\\left[d-1<x<f\\right]}{1-\\operatorname*{Pr}_{x\\sim D}\\left[f\\leq x\\leq d-1\\right]}}\\quad d-1<f\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $b-a\\geq1$ , we have that for any $c\\in(b-1,h]$ , the optimal solution $H^{\\star}$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\nH^{\\star}=\\left\\{H_{c}\\qquad U(H_{c})>U([a,b])\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $U(H_{c})$ is given above and $U([a,b])=1-\\operatorname*{Pr}_{x\\sim D}\\left[f\\leq x<h\\right]$ is the utility of releasing $[a,b]$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem D.4. Suppose $H=[c,d]\\subseteq[a,b]$ is the released information by the learner. The agents then project their uniform prior $\\pi$ over $[a,b]$ onto $H$ , which leads to the uniform distribution over $[c,d]$ for $\\pi|_{H}$ , and then best respond according to $\\pi|_{H}$ . Therefore, for any agent $x$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{BR}(x,H)=\\underset{x^{\\prime}\\geq x}{\\operatorname{argmax}}\\left\\lbrace\\underset{h^{\\prime}\\sim U n i f[c,d]}{\\operatorname*{Pr}}[x^{\\prime}\\geq h^{\\prime}]-(x^{\\prime}-x)\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "One can then show that if $d-c\\geq1$ , $\\mathrm{BR}(x,H)=x$ for all $x$ because for any manipulation $(x^{\\prime}>x)$ ), the marginal gain in the probability of receiving a positive classification is less than the marginal cost. ", "page_idx": 16}, {"type": "text", "text": "Furthermore, if $d-c<1$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{B R}(x,H)=\\left\\{\\!\\!\\begin{array}{l l}{d}&{d-1<x<d}\\\\ {x}&{\\mathrm{Otherwise}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, for any $H=[c,d]\\subseteq[a,b]$ , if $d-c\\geq1$ , we have $U(H)=1-\\operatorname*{Pr}_{x\\sim D}\\left[f\\leq x<h\\right]$ , and if $d-c<1$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nU(H)=\\left\\{1-\\operatorname*{Pr}_{x\\sim D}\\left[d-1<x<f\\right]\\quad d-1<f\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is because under $d-c\\geq1$ , no one manipulates, and thus, the error corresponds to the probability mass between $f$ and $h$ : the positives who cannot manipulate to pass $h$ . Under $d-c<1$ , because every agent $x>d-1$ can receive positive classification by manipulating, the error of $H$ corresponds to the probability mass between $d-1$ and $f$ : if $d-1<f$ , this corresponds to the negatives who can manipulate and receive positive classification, and if $d-1\\geq f$ , this corresponds to the positives who cannot manipulate to receive positive classification. ", "page_idx": 17}, {"type": "text", "text": "Now assume $b-a<1$ , which implies that $d-c<1$ . At a high level, to maximize $U(H)$ in this case, we want to pick $d$ such that $d-1$ is as close as possible to $f$ . More formally, our goal is to pick $[c,d]\\subseteq[a,b]$ such that $h\\in[c,d]$ and that the probability mass between $d-1$ and $f$ is minimized. In this case, one can see, via a case analysis, that $d=\\operatorname*{min}\\left(b,\\operatorname*{max}\\left(h,f+1\\right)\\right)$ is the optimal value, and that $c$ can be any point in $[a,h]$ . ", "page_idx": 17}, {"type": "text", "text": "If $b-a\\geq1$ , then both $d-c<1$ and $d-c\\geq1$ are possible. If $d-c<1$ , then the optimality of $[c,d]$ where $c$ is any point in $(b-1,h]$ , and $d=\\operatorname*{min}\\left(b,\\operatorname*{max}\\left(h,f+1\\right)\\right)$ can be established as above. Note that the choice of $c\\in(b-1,h]$ guarantees that $d-c<1$ . If $d-c\\geq1$ , then the utility of the learner doesn\u2019t change if $[c,d]=[a,b]$ simply because the agents do not manipulate for any $[c,d]$ such that $d-c\\geq1$ . Finally, the optimal interval is chosen based on which case $(d-c<1$ vs. $d-c\\geq1$ ) leads to higher utility. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "E The Complete Exposition of Section 4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section we will provide an efficient algorithm for computing the learner\u2019s optimal information release when the prior $\\pi$ is a discrete uniform distribution over a set $\\{h_{1},h_{2},\\bar{\\cdot}\\cdot\\cdot,h_{n}\\}\\subseteq\\mathcal{H}$ that includes the adopted classifier $h$ . The objective of the learner is to release a $H\\subseteq\\{h_{1},h_{2},\\ldots,h_{n}\\}$ such that $h\\in H$ . Throughout, we take $h=h_{k}$ where $1\\leq k\\leq n$ , and assume $h_{1}\\leq h_{2}\\leq\\ldots\\leq h_{n}$ . ", "page_idx": 17}, {"type": "text", "text": "We first state some facts about the agents\u2019 best response for any prior $\\pi$ over $\\{h_{1},h_{2},\\ldots,h_{n}\\}\\subseteq{\\mathcal{H}}$ . To start, we first show that the best response of any agent can be characterized as follows: ", "page_idx": 17}, {"type": "text", "text": "Lemma E.1. For any agent $x$ , and any prior $\\pi$ over $\\{h_{1},h_{2},\\ldots,h_{n}\\}\\subseteq{\\mathcal{H}}$ , we have $B R(x,H)\\in$ $\\{x\\}\\cup\\{h_{i}\\in H:h_{i}>x\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma E.1. Recall from Lemma D.2 that $\\mathrm{BR}(x,H)\\geq x$ . Note that the utility of the agent $x\\in\\mathscr{X}$ from manipulating to a point $x^{\\prime}\\geq x$ can be expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\nu_{x}(x^{\\prime},H)=\\sum_{i:h_{i}\\leq x^{\\prime}}\\pi|_{H}(h_{i})-(x^{\\prime}-x)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For any $x^{\\prime}\\geq x$ such $x^{\\prime}\\notin\\{x\\}\\cup\\{h_{i}\\in H:h_{i}>x\\}$ , it is easy to see that the agent can increase her utility by moving to a point in $\\{x\\}\\cup\\{h_{i}\\in H:h_{i}>x\\}$ , which proves the lemma. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "This lemma basically tells us that the best response of any agent $x$ is either to stay at its location, or to manipulate to $h_{i}\\in H$ such that $h_{i}>x$ . Given such characterization of the agents\u2019 best response in our setup, we now characterize, for any classifier $h_{i}$ in the support of $\\pi$ , the set of agents that will manipulate to $h_{i}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma E.2. Fix any prior $\\pi$ over $\\{h_{1},\\ldots,h_{n}\\}$ and any $H$ . If for any $i$ , $\\{x:B R(x,H)=h_{i}\\}\\neq\\emptyset$ , then for some $\\alpha$ , $\\{x:B R(x,H)={\\dot{h}}_{i}\\}=(\\alpha,h{\\dot{\\iota}}],$ , where $\\alpha$ satisfies $u_{\\alpha}(\\alpha,H)=u_{\\alpha}(h_{i},H)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma $E.2$ . Let \u03b1 = inf $\\{x:\\mathrm{BR}(x,H)=h_{i}\\}$ . Take any $x~\\in~(\\alpha,h_{i}]$ . We have, by the definition of $\\alpha$ , that there exists $x^{\\prime}\\ \\in\\ (\\alpha,x)$ such that $x^{\\prime}\\ \\in\\ \\{x:\\mathrm{BR}(x,H)=h_{i}\\}$ , implying $\\mathrm{BR}(x^{\\prime},H)\\;=\\;h_{i}$ . Therefore, $\\mathrm{BR}(x^{\\prime},H)\\;\\geq\\;x$ . The third part of Lemma D.2 implies that $h_{i}=\\mathbf{B}\\mathbf{R}(x^{\\prime},H)=\\mathbf{B}\\mathbf{R}(x,H)$ . This proves that ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\alpha,h_{i}]\\subseteq\\{x:\\mathbf{BR}(x,H)=h_{i}\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $x>h_{i}$ , then $\\mathsf{B R}(x,H)>h_{i}$ by the first part of Lemma D.2. Therefore $x\\notin\\{x:\\mathsf{B R}(x,H)=h_{i}\\}$ .   \nIf $x<\\alpha$ , then $\\mathrm{BR}(x,H)<h_{i}$ by the definition of $\\alpha$ , implying $x\\notin\\{x:\\mathsf{B R}(x,H)=h_{i}\\}$ . ", "page_idx": 18}, {"type": "text", "text": "If $x=\\alpha$ , we will show that $\\mathrm{BR}(x,H)=\\alpha$ . Note that $\\alpha\\leq\\mathrm{BR}(\\alpha,H)\\leq h_{i}$ by Lemma D.2. But if $\\mathrm{BR}(\\alpha,H)>\\alpha$ , then by the third part of Lemma D.2, $\\mathsf{B R}(\\alpha,H)=h_{i}$ . So $\\dot{\\bf B}\\dot{\\bf R}(\\alpha,H)\\in\\{\\alpha,h_{i}\\}$ . Suppose $\\mathsf{B R}(\\alpha,H)=h_{i}$ . Therefore, there exists $\\epsilon>0$ such that $u_{\\alpha}(\\alpha,H)+\\epsilon<u_{\\alpha}(h_{i},H)$ , by the definition of agents\u2019 best response and the fact that tie-breaking is in favor of smaller values (Remark D.1). Let $\\bar{\\epsilon}^{\\prime}\\in(0,\\epsilon/2]$ be such that $\\{h^{\\prime}\\in H:\\alpha-\\epsilon^{\\prime}\\leq h^{\\prime}<\\dot{\\alpha}\\}=\\emptyset$ . Consider $x^{\\prime}=\\alpha-\\epsilon^{\\prime}$ . We have, by Lemma D.2 and E.1, that ${\\tt B R}(x^{\\prime},H)\\in\\{x^{\\prime}\\}\\cup[\\alpha,h_{i}]$ . But because $\\mathsf{B R}(\\alpha,H)=h_{i}$ , Lemma D.2 implies that ${\\tt B R}(x^{\\prime},H)\\in\\{x^{\\prime},h_{i}\\}$ . Note that ", "page_idx": 18}, {"type": "equation", "text": "$$\nu_{x^{\\prime}}(x^{\\prime},H)\\leq u_{\\alpha}(\\alpha,H)<u_{\\alpha}(h_{i},H)-\\epsilon=u_{x^{\\prime}}(h_{i},H)-(\\epsilon-\\epsilon^{\\prime})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "implying that $\\mathrm{BR}(x^{\\prime},H)\\,=\\,h_{i}$ . But $x^{\\prime}\\,=\\,\\alpha\\,-\\,\\epsilon^{\\prime}$ and this contradicts with the definition of $\\alpha$ .   \nTherefore $\\mathrm{BR}(\\alpha,H)=\\alpha$ , and this completes the proof of the first part of the lemma. ", "page_idx": 18}, {"type": "text", "text": "We now focus on the second part of the lemma. Note that $u_{\\alpha}(\\alpha,H)\\,\\geq\\,u_{\\alpha}(h_{i},H)$ , because if $u_{\\alpha}(\\alpha,H)<u_{\\alpha}(h_{i},H)$ , then $\\alpha<\\mathrm{BR}(\\alpha,H)\\leq h_{i}$ . Together with the first part of this lemma, and Lemma D.2, this implies that $\\mathsf{B R}(\\alpha,H)=h_{i}$ which is a contradiction with the first part of the lemma. Next, we show that $u_{\\alpha}(\\alpha,H)\\leq u_{\\alpha}(h_{i},H)$ . Suppose $u_{\\alpha}(\\alpha,H)>u_{\\alpha}(h_{i},H)+\\dot{\\epsilon}$ for some $\\epsilon>0$ Consider $x=\\alpha+\\epsilon/2$ . We have that ", "page_idx": 18}, {"type": "equation", "text": "$$\nu_{x}(x,H)\\geq u_{\\alpha}(\\alpha,H)>u_{\\alpha}(h_{i},H)+\\epsilon=u_{x}(h_{i},H)+\\epsilon/2\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "implying that $\\mathsf{B R}(x,H)\\neq h_{i}$ . This is in contradiction with the first part of the lemma. Therefore, $u_{\\alpha}(\\alpha,H)=u_{\\alpha}(h_{i},H)$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Next, we characterize the utility of any subset $H$ released by the learner using a real-valued function of $H$ . Define, for any $H\\subseteq\\{h_{1},\\ldots,h_{n}\\}$ such that $h\\in H$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{H}\\triangleq\\operatorname*{inf}\\left\\{x:\\mathbf{BR}(x,H)\\geq h\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that $\\mathrm{BR}(x\\,=\\,h,H)\\;\\geq\\;h$ for any $H$ such that $h\\ \\in\\ H$ . Therefore, $\\{x:\\mathsf{B R}(x,H)\\geq h\\}$ is nonempty, and that $R_{H}\\ \\le\\ h$ for any $H$ such that $h\\:\\in\\:H$ . Our next lemma shows that $R_{H}$ characterizes the utility of $H$ for the learner, for any prior $\\pi$ over $\\{h_{1},\\ldots,h_{n}\\}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 4.4. Fix any prior $\\pi$ over $\\{h_{1},\\ldots,h_{n}\\}$ . We have that for any $H\\subseteq\\{h_{1},\\ldots,h_{n}\\}$ such that $h\\in H$ , the utility of the learner, given by Equation 2, can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\nU(H)={\\binom{1-\\operatorname*{Pr}_{x\\sim D}\\left[R_{H}<x<f\\right]}{1-\\operatorname*{Pr}_{x\\sim D}\\left[f\\leq x\\leq R_{H}\\right]}}\\quad R_{H}<f\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 4.4. Recall that $U(H)=\\operatorname*{Pr}_{x\\sim D}\\left[h(\\mathbf{BR}(x,H))=f(x)\\right]$ . The claim follows from the fact that $h(\\mathbf{BR}(x,H))=1$ if and only if $x>R_{H}$ . Note that if $x>R_{H}$ , then $\\mathtt{B R}(x,H)\\geq h$ (equivalently, $h(\\mathbf{B}\\mathbf{R}(x,H))=1)$ ) by the definition of $R_{H}$ and Lemma D.2. Further, if $\\operatorname{BR}(x,H)\\geq h$ then $x>R_{H}$ by the definition of $R_{H}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Given such characterization of the learner\u2019s utility, we will show that when the agents\u2019 prior is uniform over $\\{h_{1},\\ldots,h_{n}\\}$ , there are only polynomially many possible values that $R_{H}$ can take, even though there are exponentially many $H$ \u2019s. Our algorithm then for any possible value $R$ of $R_{H}$ , finds a subset $H$ such that $R_{H}=R$ , if such $H$ exists. The algorithm then outputs the $H$ with maximal utility according to Equation 5. More formally, we consider the following partitioning of the space of subsets of $\\{h_{1},\\ldots,h_{n}\\}$ . For any $\\ell\\in\\{1,2,\\ldots,n\\}$ , and for any $i\\in\\{k,\\overbar{k}+1,\\ldots,\\bar{n}\\}^{13}$ , define ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{i,\\ell}=\\{H\\subseteq\\{h_{1},\\dots,h_{n}\\}:h\\in H,\\,|H|=\\ell,\\,\\mathrm{BR}(h,H)=h_{i}\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that by Lemma E.1, $\\mathrm{{BR}}(h\\equiv h_{k},H)\\in\\{h_{i}:i\\geq k\\}$ for any $H$ . Therefore, $\\{S_{i,\\ell}\\}_{i,\\ell}$ gives us a proper partitioning of the space of subsets, which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{H\\subseteq\\{h_{1},\\ldots,h_{n}\\},h\\in H}U(H)=\\operatorname*{max}_{i,\\ell}\\,\\operatorname*{max}_{H\\in S_{i,\\ell}}U(H)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will show that when the prior is uniform, solving $\\operatorname*{max}_{H\\in S_{i,\\ell}}U(H)$ can be done efficiently, by showing a construction of the optimal $H\\in S_{i,\\ell}$ in our algorithm. To do so, we first show that $R_{H}$ (defined in Equation 10) can be characterized by $h_{i}$ , when we restrict ourselves to $H\\in S_{i,\\ell}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma E.3. Fix any prior $\\pi$ over $\\{h_{1},\\ldots,h_{n}\\}$ . If $H\\ \\in\\ S_{i,\\ell},$ , then $\\{x:B R(x,H)=h_{i}\\}\\;=$ $(R_{H},h_{i}]$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma E.3. We first show that $R_{H}=\\operatorname*{inf}\\left\\{x:\\mathrm{BR}(x,H)=h_{i}\\right\\}$ . Fix any $H\\in S_{i,\\ell}$ . Let $Q_{H}=\\operatorname*{inf}\\left\\{x:\\mathrm{BR}(x,H)=h_{i}\\right\\}$ . First note that because $H\\in S_{i,\\ell}$ , we have $\\mathrm{BR}(h,H)=h_{i}$ , and therefore $\\{x:\\mathrm{BR}(x,H)=h_{i}\\}\\neq\\emptyset$ , and that $Q_{H}\\leq h$ . Additionally, because ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\{x:\\mathrm{BR}(x,H)=h_{i}\\}\\subseteq\\{x:\\mathrm{BR}(x,H)\\geq h\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have that $Q_{H}\\geq R_{H}$ . So we have $R_{H}\\leq Q_{H}\\leq h$ . If $Q_{H}\\ne R_{H}$ , then there exists $R_{H}<x<$ $Q_{H}$ , such that $h\\leq\\tt B R(x,H)<h_{i}$ . But, for $x^{\\prime}=\\mathsf{B R}(x,H)$ , we have ${\\mathrm{BR}}(x^{\\prime},H)=h_{i}>x^{\\prime}=$ $\\mathsf{B R}(x,H)$ . This is in contradiction with the third part of Lemma D.2 (taking $x_{1}=x$ , and $x_{2}=x^{\\prime}$ ). Therefore, $Q_{H}=R_{H}$ , and this proves the first part of the lemma. The second part of the lemma is followed from part one and Lemma E.2. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "In particular, this Lemma implies that for $H\\in S_{i,\\ell}$ , we have $R_{H}=\\operatorname*{inf}\\left\\{x:\\mathrm{BR}(x,H)=h_{i}\\right\\}$ . Next, we demonstrate the possible values that $R_{H}$ can take for uniform priors. In particular, the following lemma establishes that $R_{H}$ can take only polynomially many values. ", "page_idx": 19}, {"type": "text", "text": "Lemma E.4. If the prior $\\pi$ is uniform over $\\{h_{1},\\ldots,h_{n}\\}$ , then for any $H\\in S_{i,l},$ , $R_{H}=h_{i}-j/\\ell$ where $j=|\\{h^{\\prime}\\in H:h^{\\prime}\\in(R_{H},h_{i}]\\}|$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma E.4. Fix any $H\\in S_{i,\\ell}$ . Note that Lemma E.3 and Lemma E.2 together imply that $u_{R_{H}}(R_{H},H)=u_{R_{H}}(h_{i},H)$ . This implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{h^{\\prime}\\sim\\pi\\vert_{H}}\\left[R_{H}\\geq h^{\\prime}\\right]=\\operatorname*{Pr}_{h^{\\prime}\\sim\\pi\\vert_{H}}\\left[h_{i}\\geq h^{\\prime}\\right]+\\left(h_{i}-R_{H}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "But $\\operatorname*{Pr}_{h^{\\prime}\\sim\\pi|_{H}}\\left[R_{H}\\geq h^{\\prime}\\right]=j_{1}/\\ell$ and $\\operatorname*{Pr}_{h^{\\prime}\\sim\\pi|_{H}}\\left[R_{H}\\geq h^{\\prime}\\right]=j_{2}/\\ell$ where $j_{1}$ and $j_{2}$ are the number of hypotheses in $H$ that are smaller (or equal to) $R_{H}$ , and smaller (or equal to) $h_{i}$ , respectively. In other words, ", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{j_{1}=\\left|\\left\\{h^{\\prime}\\in H:h^{\\prime}\\leq R_{H}\\right\\}\\right|,\\quad j_{2}=\\left|\\left\\{h^{\\prime}\\in H:h^{\\prime}\\leq h_{i}\\right\\}\\right|}}\\\\ {{\\,}}\\\\ {{R_{H}=h_{i}-\\displaystyle\\frac{j_{2}-j_{1}}{\\ell}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the proof. ", "page_idx": 19}, {"type": "text", "text": "Given such characterization, Algorithm 3, for any $i,\\ell$ , enumerates over all possible $j\\in\\{1,\\ldots,\\ell\\}$ and returns a $H$ such that $H\\in S_{i,\\ell}$ and $R_{H}=h_{i}-j/\\ell$ , if such $H$ exists. To elaborate, for any $i,\\ell,j$ , such H \u2261Hi,\u2113i s constructed by first picking the $j$ largest classifiers that are between $h_{i}-j/\\ell$ and $h_{i}$ (including both $h_{i}$ and $h$ ). If there are not at least $j$ classifiers between $h_{i}-j/\\ell$ and $h_{i}$ , then no such $H$ exists for $i,\\ell,j$ because of Lemma E.4. After picking the first $j$ elements as described, the remaining $\\ell-j$ classifiers are first chosen from all classifiers that are less than (or equal to) $h_{i}-j/\\ell$ , and once these classifiers are exhausted, the rest are taken from the classifiers that are larger than $h_{i}$ , starting from the largest possible classifier, and going down until $\\ell$ classifiers are picked. ", "page_idx": 19}, {"type": "text", "text": "Note that this construction of $H\\equiv H_{j}^{i,\\ell}$ guarantees that $\\mathsf{B R}(h_{i},H^{\\prime})$ is minimized among all $H^{\\prime}$ \u2019s with corresponding values of $(i,\\ell,j)$ . Therefore, if $\\mathtt{B R}(h_{i},H)\\,>\\,h_{i}$ , it is guaranteed that no $H$ exists for $(i,\\ell,j)$ . If $\\mathrm{BR}(h_{i},H)=h_{i}$ , then the construction of $H\\equiv H_{j}^{i,\\ell}$ guarantees that $R_{H}=$ inf $\\{x:\\mathrm{BR}(x,H)=h_{i}\\}$ is as small as possible. Therefore, if inf $\\{x:\\mathrm{BR}(x,H)=h_{i}\\}>h_{i}-j/\\ell,$ , then it is guaranteed that no such $H$ exists for $(i,\\ell,j)$ (note that inf $\\{x:\\mathrm{{BR}}(x,H)=h_{i}\\}\\geq h_{i}-j/\\ell$ by construction). The algorithm finally outputs, among all $H$ \u2019s found, the subset $H$ with maximum utility according to Equation 5. ", "page_idx": 19}, {"type": "text", "text": "This proves the following theorem. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 3: The Learner\u2019s Optimization Problem: Discrete Uniform Prior ", "page_idx": 20}, {"type": "image", "img_path": "SadbRPoG2k/tmp/d1c959f4426def7fe99bd1f6544a2a82b31751ef4788f0f4f3319bbf0e29fda2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Output: $H^{\\star}=H_{j^{\\star}}^{i^{\\star},\\ell^{\\star}}$ where $(i^{\\star},\\ell^{\\star},j^{\\star})\\in\\mathrm{argmax}_{(i,\\ell,j)}\\,U_{j}^{i,\\ell}$ . ", "page_idx": 20}, {"type": "text", "text": "Theorem 4.6. There exists an algorithm (Algorithm 3) that for any $n$ , any uniform prior over $\\{h_{1},\\ldots,h_{n}\\}$ that includes $h,$ , and any data distribution $D$ , returns $H^{\\star}\\,\\subseteq\\,\\{h_{1},\\ldots,h_{n}\\}$ in time $O(n^{3})$ such that $h\\in H^{\\star}$ , and that $U(H^{\\star})=\\operatorname*{max}_{H\\subseteq\\{h_{1},\\dots,h_{n}\\},h\\in H}U(H)$ . ", "page_idx": 20}, {"type": "text", "text": "F Missing Proof of Section 4.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem 4.7. Fix any $h\\,\\geq\\,f$ . For any prior $\\pi$ over $\\{h_{1},\\ldots,h_{n}\\}$ that includes $h$ , we have $^{\\,l}$ ) $F P R(\\mathcal{H})\\leq F P R(\\{h\\}).$ 2) $F N R(\\{h\\})\\leq F N R(H)$ for every $H\\subseteq{\\mathcal{H}}$ such that $h\\in H$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 4.7. We begin by showing that $F P R(\\mathcal{H})\\leq F P R(\\{h\\})$ . Let $x\\in\\mathscr{X}$ be such that $f(x)=0$ and $h(B R(x,\\mathcal{H}))=1$ . We will show that $h(B R(x,\\{h\\}))=1$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma E.1 together with $h\\,\\geq\\,f$ imply the existence of $h_{j}$ such that $B R(x,\\mathcal{H})\\,=\\,h_{j}\\,>\\,x$ (as $f(x)\\neq h(\\mathbf{BR}(x,\\mathcal{H}))$ . This further indicates that when $\\mathcal{H}$ is released, the utility of the agent is strictly higher when it manipulates to $h_{j}$ , compared to not moving: ", "page_idx": 20}, {"type": "equation", "text": "$$\nu_{x}(h_{j},\\mathcal{H})=\\sum_{i=1}^{j}\\pi(h_{i})-(h_{j}-x)>\\sum_{i:h_{i}\\leq x}\\pi(h_{i})=u_{x}(x,\\mathcal{H})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $h(B R(x,\\mathcal{H}))=1$ and $B R(x,\\mathcal{H})=h_{j}$ implies that $h_{j}\\geq h$ , and therefore: ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{x}(h,\\{h\\})=1-(h-x)\\geq\\sum_{i=1}^{j}\\pi(h_{i})-(h_{j}-x)>\\sum_{i:h_{i}\\leq x}\\pi(h_{k})=u_{x}(x,\\{h\\})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since Lemma E.1 implies that $B R(x,\\{h\\})\\,\\in\\,\\{x,h\\}$ , we derive from the above inequality that $h(B R(x,\\{h\\}))=1$ . This proves the first part of the theorem. ", "page_idx": 21}, {"type": "text", "text": "Next, we show that $F N R(\\{h\\})\\leq F N R(H)$ for every $H\\subseteq\\mathcal{H}$ . Let $x\\in\\mathscr{X}$ be such that $f(x)=1$ and $h(B R(x,\\{h\\}))=0$ , and let $H$ be any subset of $\\mathcal{H}$ . We will show that $h(B R(x,H))={\\mathrm{0}}$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma E.1 implies that $B R(x,\\{h\\})\\in\\{x,h\\}$ . Together with $h(B R(x,\\{h\\}))=0$ , we derive that $B R(x,\\{h\\})=x$ , and thus: ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{x}(h,\\{h\\})=1-(h-x)\\leq u_{x}(x,\\{h\\})=0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, for every $h_{j}$ such that $h_{j}\\geq h$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{x}(h_{j},H)=\\sum_{i=1}^{j}\\pi|_{H}(h_{i})-(h_{j}-x)\\leq1-(h-x)\\leq0\\leq\\sum_{i:h_{i}\\leq x}\\pi|_{H}(h_{i})=u_{x}(x,H).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As a result, when the learner releases $H$ , the utility of agent $x$ from remaining at $x$ is greater than (or equal to) any manipulation $h_{j}$ such that $h_{j}\\geq h$ . This implies that $h(B R(x,\\mathbf{\\bar{{H}}}))=0$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "G Optimal Information Release for Minimizing FPR ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We show that minimizing FPR, unlike minimizing FNR, does not always have a clear optimal solution for the learner, by providing three examples with very different optimal solutions. ", "page_idx": 21}, {"type": "text", "text": "Example G.1 (Full-information-release is optimal for FPR). Fix any $B>1$ and any $0\\leq t<B-1$ . Let $D$ be the uniform distribution over $\\mathcal{X}=[0,B]$ , and $f(x)=\\mathbb{1}\\left[x\\geq t\\right]$ . Let $\\mathcal{H}$ be the class of single-sided threshold classifiers and suppose the adopted classifier $h(\\dot{x})=\\dot{\\mathbb{1}}\\left[x\\geq t+1\\right]$ . Under any prior over $\\mathcal{H}$ , one can show that the full information release of $H=\\{h\\}$ achieves perfect FPR for this setting: $F P R(\\{h\\})=0$ . ", "page_idx": 21}, {"type": "text", "text": "Example G.2 (No-information-release is optimal for FPR). Under the same setup as in Example 2.4, one can show that releasing the support of the prior $H=\\{h_{1.8},h_{2}\\}$ achieves $F P R(H)=0$ , whereas full information release of the adopted classifier $h=h_{2}$ achieves $F P R(\\{h\\})=0.9/1.9\\approx0.47$ . Note that $H=\\{h_{1.8},h_{2}\\}$ is the support of the prior, so it constitutes as no-information-release. In other words, we have $F\\dot{P}R(H^{\\prime})=\\dot{F}P R(H)\\dot{=}\\,0$ for every $H^{\\prime}$ such that $H\\subseteq H^{\\prime}\\subseteq\\mathcal{H}$ . ", "page_idx": 21}, {"type": "text", "text": "Claim G.3. There exists an instance in which neither full-information-release nor no-informationrelease are optimal when the utility function of the learner is $U_{F P R}$ .14 ", "page_idx": 21}, {"type": "text", "text": "Proof of Claim G.3. We construct such an instance as follows. Suppose the domain is $\\mathcal{X}=\\{x_{1},x_{2}\\}$ with $x_{1}\\,=\\,0$ , $x_{2}\\,=\\,0.4$ and the distribution $D$ is given by $D(\\bar{x_{1}})\\,=\\,D(x_{2})\\,=\\,0.5$ . In addition, consider $f=0.3$ , and hypothesis class $\\mathcal{H}=\\{h_{1},h_{2},h_{3}\\}$ where $h_{1}=0.1,h_{2}=0.5,h_{3}=0.7,$ , and a prior distribution $\\pi$ such that $\\pi(h_{1})=0.2,\\pi(h_{2})=0.1,\\pi(h_{3})=0.7$ . ", "page_idx": 21}, {"type": "text", "text": "Observe that under full-information-release, $F P R(\\{h\\})\\,=\\,1$ for every $h\\ \\in\\ \\mathcal H$ . Now suppose $h=h_{2}$ is the adopted classifier. We have that $B R(x_{1},\\{h\\})=0.5$ implying $h(B R(x_{1},\\{h\\}))=1\\neq$ $f(x_{1})=0$ implying $x_{1}$ is a false positive under $\\{h\\}$ release. Additionally, $B R(x_{1},\\mathcal{H})=0.7$ implies that $h(B R(x_{1},\\mathcal{H}))=1\\neq f(x_{1})=0$ implying $x_{1}$ is a false positive under $\\mathcal{H}$ release. Further, it holds that $B R(x_{1},\\{h_{1},h_{2}\\})=0.1$ and so $h(B R(x_{1},\\{h_{1},h_{2}\\}))=0=f(x_{1})$ . Moreover, in this particular instance, releasing $\\{h_{1},h_{2}\\}$ achieves perfect utility as $B R(x_{2},\\{h_{1},h_{2}\\})\\,=\\,0.5$ which implies $h(B R(x_{2},\\{h_{1},h_{2}\\}))\\stackrel{}{=}1=\\dot{f}(x_{2})$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "H Possible Information Leakage Through Firm\u2019s Choice of $H$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "One limitation of our model is that if the agents have knowledge of the mapping $f$ from feature vectors to labels, they might gain information on $h^{*}$ in cases when $|H|>1$ . More specifically, knowing the mapping $D$ , the mapping $f$ , and that the firm is optimizing the choice of $H$ for accuracy, agents could deduce $h^{*}$ . In this case, the choice of $H$ leaks more information than intended. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "We proceed by showing an example with threshold classifiers for such a case. ", "page_idx": 22}, {"type": "text", "text": "Example H.1. Suppose a distribution $D$ over agents $x\\in\\mathcal{X}=[0,1]$ is uniform, $c(x,x^{\\prime})=|x-x^{\\prime}|,$ , and ${\\bar{f}}(x)\\;=\\;1\\;[x\\overset{.}{\\geq}0.15]$ . The set of classifiers available to the firm is $\\mathcal{H}\\;=\\;\\{h_{1},h_{2}\\}$ where $\\alpha_{1}=0.1,\\alpha_{2}=0.9$ is each classifier\u2019s respective threshold. ", "page_idx": 22}, {"type": "text", "text": "Following our Bayesian model, the firm chooses to release a subset $H\\subseteq{\\mathcal{H}}$ over which the agents have a uniform prior $\\pi|_{H}$ . The agents know that the firm is choosing $H$ to optimize accuracy, i.e. the function $U(H)=\\operatorname*{Pr}_{x\\sim D}[h^{*}(\\Delta_{H}(x))=y]$ (where $y=\\mathbb{1}[x\\geq T])$ . We will show that the firm can release a subset $H$ which is in arg $\\begin{array}{r}{\\operatorname*{max}_{H\\subseteq\\mathcal{H}}[U(H)|h^{*}=h_{1}]}\\end{array}$ but not in arg $\\mathrm{nax}_{H\\subseteq\\mathcal{H}}[U(H)|h^{*}=$ $h_{2}]$ , allowing the agent to reason that $h^{*}$ must be $h_{1}$ . ", "page_idx": 22}, {"type": "text", "text": "Proposition H.2. Consider Example H.1. If the agents know agents know that $f(x)=\\mathbb{1}\\left[x\\geq0.15\\right.$ ] and the prior $D$ , they can infer that $h^{*}=h_{1}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We first solve for arg $\\begin{array}{r}{\\operatorname*{max}_{H\\subseteq\\mathcal{H}}[U(H)|h^{*}=h_{1}]}\\end{array}$ . Suppose $H=\\{h_{1}\\}$ . Then all agents know $h^{*}=h_{1}$ . If $x\\leq\\alpha_{1}$ , the agent will manipulate to $\\alpha_{1}$ to get a positive outcome if $1-c(\\bar{x},\\alpha_{1})>0$ , which is always true. If $x\\geq\\alpha_{1}$ , agents will stay the same to get a positive outcome. So the set of misclassified agents is those with $x\\,\\in\\,[0,T]$ and $U(\\{h_{1}\\})\\stackrel{\\cdot}{=}1\\stackrel{\\cdot}{-}T\\,=\\,0.85$ . Now suppose $H\\,=\\,\\{h_{1},h_{2}\\}$ . Agents believe each classifier is $h^{*}$ with probability 0.5. If $x\\,\\geq\\,\\alpha_{2}$ , agents are guaranteed a positive outcome and stay the same. If $\\alpha_{1}\\,\\leq\\,x\\,\\leq\\,\\alpha_{2}$ , agents will manipulate to $\\alpha_{2}$ to get a positive outcome if $1\\,-\\,c(x,\\alpha_{2})\\,>\\,0.5$ , so all agents with $x\\ \\in\\ (0.4,0.9]$ will be classified correctly. The rest of the agents with $x\\in[\\alpha_{1},0.4]$ will stay the same to get a positive outcome with probability 0.5, and those with $x\\in[\\alpha_{1},T]$ will be misclassified. Lastly, if $x<\\alpha_{1}$ , agents will manipulate to $\\alpha_{2}$ to get a guaranteed positive outcome if $1-c(x,\\alpha_{2})>0.5-c(x,\\alpha_{1})$ (a.k.a. $1\\mathrm{~-~}0.9\\mathrm{~-~}x\\,>\\,0.5\\mathrm{~-~}0.1\\mathrm{~-~}x)$ , which is never true, and otherwise manipulate to $\\alpha_{1}$ to get a positive outcome with probability 0.5 if $0.5\\mathrm{~-~}c(x,\\alpha_{1})\\,>\\,0$ , which is always true. So the set of misclassified agents is those with $x\\in[0,T]$ and $U(\\{h_{1},h_{2}\\})=1-T=0.85$ . Therefore arg $\\begin{array}{r}{\\operatorname*{max}_{H\\subseteq\\mathcal{H}}[U(H)|\\bar{h}^{*}=h_{1}]=\\{\\{h_{1}\\},\\{h_{1},\\bar{h}_{2}\\}\\}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Now we consider arg $\\mathrm{max}_{H\\subseteq\\mathcal{H}}[U(H)|h^{*}=h_{2}]$ . Suppose $H=\\{h_{2}\\}$ . As before, all agents know $h^{*}=h_{2}$ . If $x\\leq\\alpha_{2}$ , the agent will manipulate to $\\alpha_{2}$ to get a positive outcome if $1-c(x,\\alpha_{2})>0$ , which is always true. If $x\\geq\\alpha_{2}$ , agents will stay the same to get a positive outcome. So the set of misclassified agents is those with $x\\,\\in\\,[0,T]$ and $U(\\{h_{2}\\})\\,\\,=\\,1\\,\\,-\\,T\\,=\\,0.85$ . Now suppose $H=\\{h_{1},h_{2}\\}$ . As before, agents believe each classifier is $h^{*}$ with probability 0.5. If $x\\geq\\alpha_{2}$ , agents are guaranteed a positive outcome and stay the same. If $\\alpha_{1}\\leq x\\leq\\alpha_{2}$ , agents will manipulate to $\\alpha_{2}$ to get a positive outcome if $1-c(x,\\alpha_{2})>0.5$ , so all agents with $x\\in(0.4,0.9]$ will be classified correctly. The rest of the agents with $x\\in[\\alpha_{1},0.4]$ will stay the same to get a positive outcome with probability 0.5, and will be misclassified. Lastly, if $x<\\alpha_{1}$ , agents will manipulate to $\\alpha_{1}$ to get a positive outcome with probability 0.5, and will be classified correctly. So the set of misclassified agents is those with $\\bar{x}\\in(0.4,0.9\\bar{]}$ and $U(\\left\\{h_{1},h_{2}\\right\\})=1-\\left\\lvert0.9-0.4\\right\\rvert=0.5$ , which is less than for $\\bar{U}(\\{h_{2}\\})$ . Therefore ar $\\begin{array}{r}{\\mathrm{g}\\operatorname*{max}_{H\\subseteq\\mathcal{H}}[U(H)|h^{*}=h_{2}]=\\{\\{h_{2}\\}\\}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "We have shown that if the firm chooses $H^{*}~=~\\{h_{1},h_{2}\\}$ , the agent can reason that $H^{*}\\quad\\in$ arg $\\begin{array}{r}{\\operatorname*{max}_{H\\subseteq\\mathcal{H}}[U(H)|h^{*}=h_{1}]}\\end{array}$ but $\\\"\\not\\in\\arg\\operatorname*{max}_{H\\subseteq\\mathcal{H}}[U(H)|h^{*}=h_{2}]$ , so $h^{*}$ must be $h_{1}$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our introduction provides a summary of contributions that accurately reflect the new model and the results of this paper, with pointers to specific sections. Our model is introduced in Section 2, and Sections 3, 4 contain all formal statements and their proofs. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See Section A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We describe our model in Section 2, and specific assumptions appear in relevant sections. For every theoretical result, we provide a complete (and correct) proof. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include experiments. All results are carefully proven, and do not require experiments to establish correctness. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This is a theoretical paper. All results are carefully proven, and do not require experiments to establish correctness. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Justification: This is a theoretical paper. All results are carefully proven, and do not require experiments to establish correctness. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Justification: This is a theoretical paper. All results are carefully proven, and do not require experiments to establish correctness. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 26}, {"type": "text", "text": "Justification: This is a theoretical paper. All results are carefully proven, and do not require experiments to establish correctness. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We have carefully read and comply with the NeurIPS code of ethics. This work is theoretical and we do not foresee harmful consequences. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We note that our work is in nature theoretical and we are not directly deploying new algorithms or technologies. However, our theory provides algorithms and insights that are motivated by and can be translated to practical applications such as loan decisions. As such, we discuss potential broader impacts in Section A, both positive and negative. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The work is entirely theoretical; we do not release new data or models. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not use existing assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]