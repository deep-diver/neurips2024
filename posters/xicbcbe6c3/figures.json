[{"figure_path": "XIcBCBe6C3/figures/figures_2_1.jpg", "caption": "Figure 1: Motivation study. (a)-(b) Both modulating and fine-tuning TTA have similar memory usage and remain memory-intensive, leading to out-of-memory issues during deployment on MPUs and MCUs. (c) TTA accuracy is highly reliant on batch sizes.", "description": "This figure shows the results of a motivation study that compares the memory usage and accuracy of modulating and fine-tuning test-time adaptation (TTA) methods on different hardware platforms (Raspberry Pi Zero 2W and STM32H747 MCU) and different batch sizes.  The results demonstrate that both modulating and fine-tuning TTA methods are memory-intensive, leading to out-of-memory issues on resource-constrained devices, especially at larger batch sizes. Also, the accuracy of TTA methods is highly dependent on the batch size, with smaller batch sizes resulting in lower accuracy. This highlights the challenges of applying TTA to resource-constrained devices.", "section": "1 Introduction"}, {"figure_path": "XIcBCBe6C3/figures/figures_3_1.jpg", "caption": "Figure 2: TinyTTA framework overview: TinyTTA (a) begins by analyzing and partitioning a pre-trained network into submodules based on memory usage similarity, named as self-ensemble network, (b) introduces early-exits for sample inference through specific submodules based on predicted confidence levels, (c) integrates WS normalization into each submodule to achieve batch-agnostic normalization, and (d) ultimately compiles and deploys the model on MCUs and MPUs for on-device TTA through the TinyTTA Engine.", "description": "This figure illustrates the TinyTTA framework, showcasing its four key modules: (a) self-ensemble network partitioning the pre-trained model into submodules for memory efficiency, (b) early exits to optimize inference based on confidence levels, (c) weight standardization (WS) for batch-agnostic normalization, and (d) the TinyTTA Engine for on-device deployment on MCUs and MPUs.  The diagram visually represents the flow of data and the functionality of each module within the TinyTTA framework.", "section": "3 Efficient TTA via Early-exit Ensembles and TinyTTA Engine"}, {"figure_path": "XIcBCBe6C3/figures/figures_4_1.jpg", "caption": "Figure 3: Memory usage of different layers during test-time adaptation. Note that the weight memory for modulation (bottom Figure) is < 10K, which is negligible. However, both methods show significant activation memory usage. All experiments were conducted using ResNet50 on the CIFAR-10 dataset with batch size 1.", "description": "This figure shows the memory usage (in MB) per layer during fine-tuning and modulation-based test-time adaptation (TTA) using ResNet50 on CIFAR-10 dataset with batch size of 1. It highlights that both methods show significant memory usage for activations, whereas the memory used for weights is minimal. This observation emphasizes the challenge in adapting models with limited memory on edge devices.", "section": "3 Efficient TTA via Early-exit Ensembles and TinyTTA Engine"}, {"figure_path": "XIcBCBe6C3/figures/figures_6_1.jpg", "caption": "Figure 4: The TinyTTA Engine operates in two phases: compile time and execution time. During compile time, (a) given a reprocessed model, it (b) fuses backbone operations to enhance efficiency, then enables backpropagation on TinyTTA exits. Subsequently, (c) it freezes and quantizes the backbone before integration with TinyTTA exits. Finally, the model is loaded onto the MCU for (d) on-device TTA during execution time.", "description": "This figure illustrates the TinyTTA Engine's two-phase operation: compile time and execution time.  During compile time, the pre-trained model is processed. Backbone operations are fused to improve efficiency and the backpropagation is enabled for TinyTTA exits. Then the backbone is frozen and quantized. During execution time, the optimized model is loaded onto the MCU to perform on-device TTA.", "section": "3.5 TinyTTA Engine"}, {"figure_path": "XIcBCBe6C3/figures/figures_7_1.jpg", "caption": "Figure 5: Comparison of model performance across four datasets, demonstrating the accuracy improvements when models are adapted using TinyTTA. Across all datasets, TinyTTA consistently boosts accuracy with respect to not adapting the model.", "description": "This figure compares the performance of four different models (MCUNet, EfficientNet, MobileNet, and RegNet) on four datasets (CIFAR10C, CIFAR100C, OfficeHome, and PACS) with and without TinyTTA.  It shows that TinyTTA consistently improves the accuracy of all models across all datasets, highlighting its effectiveness in adapting to various distribution shifts.", "section": "5 Results"}, {"figure_path": "XIcBCBe6C3/figures/figures_8_1.jpg", "caption": "Figure 6: TTA methods performance comparison of four edge models over four datasets. (a) TinyTTA (ours) is the only method capable of performing TTA under MCU constraints while maintaining high accuracy. (b)-(d) TinyTTA (ours) achieves the best performance using minimum memory. Results are tested on severity level 5.", "description": "This figure compares the performance of TinyTTA with other state-of-the-art Test Time Adaptation (TTA) methods across four different datasets (CIFAR10C, CIFAR100C, OfficeHome, PACS) and four different model architectures (MCUNet, EfficientNet, MobileNet, RegNet).  The key takeaway is that TinyTTA achieves the best accuracy while using significantly less memory, especially on resource-constrained MCUs.  Part (a) highlights TinyTTA's unique ability to perform TTA on MCUs. Parts (b-d) visually demonstrate TinyTTA's superior performance in terms of accuracy and memory efficiency for the other model architectures.", "section": "5.2 TinyTTA vs. SOTA baselines on MPUs"}, {"figure_path": "XIcBCBe6C3/figures/figures_9_1.jpg", "caption": "Figure 7: Ablation study. \u2605 represents TinyTTA using all components. represents without WS. \u25b2 represents without early-exits and WS, and \u2666 means without all components.", "description": "This ablation study shows the trade-offs between accuracy and memory usage for different configurations of TinyTTA across four datasets. The configuration using all components (self-ensembles, early-exits, and WS) achieves the best balance between accuracy and memory. Removing components individually results in decreased accuracy and/or increased memory usage, highlighting the importance of each component for optimal performance.", "section": "5.4 Ablation Study"}, {"figure_path": "XIcBCBe6C3/figures/figures_12_1.jpg", "caption": "Figure 8: Comparison between modulating and fine-tuning using online test-time entropy minimization. (a-b) For larger domain shifts (e.g., severity level 5), the modulating model is more prone to collapse and predict fewer classes. (c-d) For smaller domain shifts (e.g., severity level 3), modulating is more robust than fine-tuning. All experiments are conducted on ImageNet-C for fog noise and ResNet50. Refer to Section B for more details about severity levels.", "description": "This figure compares the performance of modulating and fine-tuning methods for test-time adaptation (TTA) under different levels of domain shift. It shows that for larger domain shifts (severity level 5), the modulating model tends to collapse and predict fewer classes, while for smaller domain shifts (severity level 3), the modulating method is more robust than fine-tuning.  The experiments use ImageNet-C with fog noise and the ResNet50 model.", "section": "Appendix/supplemental material"}, {"figure_path": "XIcBCBe6C3/figures/figures_14_1.jpg", "caption": "Figure 9: Comparison of accuracy between TinyEngine (using TENT) and TinyTTA (ours) on different datasets and model architectures.", "description": "This figure compares the accuracy of TinyEngine (using TENT) and TinyTTA on four different datasets (CIFAR10C, CIFAR100C, OfficeHome, and PACS) and four different model architectures (MCUNet, EfficientNet, MobileNet, and RegNet).  The results show that TinyTTA consistently outperforms TinyEngine, demonstrating the effectiveness of its dynamic early exit mechanism for adapting to varying data distributions.", "section": "C.4 TinyTTA vs. TinyEngine"}, {"figure_path": "XIcBCBe6C3/figures/figures_15_1.jpg", "caption": "Figure 1: Motivation study. (a)-(b) Both modulating and fine-tuning TTA have similar memory usage and remain memory-intensive, leading to out-of-memory issues during deployment on MPUs and MCUs. (c) TTA accuracy is highly reliant on batch sizes.", "description": "This figure presents a motivation study comparing different Test-Time Adaptation (TTA) methods. Subfigures (a) and (b) show the memory usage of modulating and fine-tuning TTA methods on Raspberry Pi Zero 2W and STM32H747 respectively, highlighting the memory-intensive nature of these methods and the resulting out-of-memory issues on resource-constrained devices. Subfigure (c) demonstrates the strong dependence of TTA accuracy on the batch size, suggesting that larger batch sizes are typically necessary for satisfactory performance but are often impractical on resource-limited devices.", "section": "1 Introduction"}, {"figure_path": "XIcBCBe6C3/figures/figures_16_1.jpg", "caption": "Figure 11: Ablation study at all levels (L) of distribution shift. Tests are performed on CIFAR10C.", "description": "This figure compares the performance of TinyTTA using Weight Standardization (WS) and Group Normalization (GN) across various levels of distribution shifts (L1 to L5) in the CIFAR-10C dataset.  The results show TinyTTA with WS consistently outperforming TinyTTA with GN across all levels, highlighting the effectiveness of WS in maintaining accuracy under distribution shifts.", "section": "5.4 Ablation Study"}, {"figure_path": "XIcBCBe6C3/figures/figures_17_1.jpg", "caption": "Figure 12: Performance comparison of MicroNets model on the Musan Keywords Spotting test dataset using different TTA methods.", "description": "This figure compares the performance of different Test-Time Adaptation (TTA) methods on the Musan Keywords Spotting test dataset using the MicroNets model.  The Musan dataset is a challenging, real-world dataset with various noisy audio conditions, making it a good test for the robustness of TTA techniques. TinyTTA significantly outperforms the other methods, demonstrating its resilience to noise and its ability to maintain high accuracy even with noisy or distorted data.  The results highlight the effectiveness of TinyTTA in real-world scenarios where adaptation to changing environments is critical.", "section": "5 Results"}, {"figure_path": "XIcBCBe6C3/figures/figures_17_2.jpg", "caption": "Figure 1: Motivation study. (a)-(b) Both modulating and fine-tuning TTA have similar memory usage and remain memory-intensive, leading to out-of-memory issues during deployment on MPUs and MCUs. (c) TTA accuracy is highly reliant on batch sizes.", "description": "This figure shows the results of a motivation study comparing different test-time adaptation (TTA) methods.  Subfigures (a) and (b) illustrate that both modulating and fine-tuning TTA methods consume a large amount of memory, resulting in out-of-memory errors when deployed on resource-constrained devices like microcontrollers (MCUs). Subfigure (c) demonstrates that the accuracy of TTA methods is highly dependent on batch size, indicating a trade-off between memory usage and performance.", "section": "1 Introduction"}, {"figure_path": "XIcBCBe6C3/figures/figures_18_1.jpg", "caption": "Figure 5: Comparison of model performance across four datasets, demonstrating the accuracy improvements when models are adapted using TinyTTA. Across all datasets, TinyTTA consistently boosts accuracy with respect to not adapting the model.", "description": "This figure compares the performance of four different model architectures (MCUNet, EfficientNet, MobileNet, and RegNet) on four corrupted datasets (CIFAR10C, CIFAR100C, OfficeHome, and PACS) with and without using TinyTTA for test-time adaptation.  The results show that TinyTTA consistently improves the accuracy of all models on all datasets, even when adapting to only a single batch of data.", "section": "5 Results"}, {"figure_path": "XIcBCBe6C3/figures/figures_18_2.jpg", "caption": "Figure 5: Comparison of model performance across four datasets, demonstrating the accuracy improvements when models are adapted using TinyTTA. Across all datasets, TinyTTA consistently boosts accuracy with respect to not adapting the model.", "description": "This figure presents a comparison of model performance with and without TinyTTA across four different datasets (CIFAR10C, CIFAR100C, OfficeHome, and PACS) using four different model architectures (MCUNet, EfficientNet, MobileNet, and RegNet). The bar chart visually represents the accuracy achieved by each model both with and without TinyTTA.  The results show that TinyTTA consistently improves the accuracy of all models across all datasets, highlighting its effectiveness in improving model adaptability and robustness.", "section": "5 Results"}]