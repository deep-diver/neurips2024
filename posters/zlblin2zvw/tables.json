[{"figure_path": "zLBlin2zvW/tables/tables_17_1.jpg", "caption": "Table 1: Cross-entropy losses for the original language model and after zero-ablating specified sub-layers of Pythia-2.8B and Gemma-7B.", "description": "This table presents the cross-entropy loss for the original language models (Pythia-2.8B and Gemma-7B) and the loss after zeroing out specific layers (MLP, Attention, Residual) within those models.  This helps quantify the impact of each layer on the model's overall performance and is used to contextualize and interpret the loss recovered metric used in other tables.", "section": "Further shrinkage plots"}, {"figure_path": "zLBlin2zvW/tables/tables_23_1.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results of training baseline sparse autoencoders (SAEs) on the Gemma-7B language model.  It shows the hyperparameters used during training, along with key performance metrics and characteristics of the resulting SAEs. Key metrics include reconstruction fidelity (Loss Recovered), sparsity (L0), and the relative reconstruction bias (Shrinkage), indicating whether the SAE's reconstructions systematically underestimate the true activation magnitudes. The table also specifies the model layer and site the SAEs were trained on, including MLP, attention, and residual stream activations. The Pareto optimal SAEs are italicized, representing a set of SAEs where no improvement can be achieved in reconstruction fidelity without sacrificing sparsity or vice versa. The data is used to compare the performance of baseline SAEs against Gated SAEs.", "section": "4.1 Benchmarking Gated SAEs"}, {"figure_path": "zLBlin2zvW/tables/tables_23_2.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results of training baseline sparse autoencoders (SAEs) on the Gemma-7B language model with 1024 sequence length.  For each layer and site (Residual stream, MLP, Attention), various sparsity levels (\u03bb) were tested, and the corresponding learning rate (LR), L0 norm (LO), percentage of loss recovered, clean cross-entropy loss, SAE cross-entropy loss, zero-ablation cross-entropy loss, width of the SAE, percentage of alive features, and shrinkage are reported.  The italicized rows represent the Pareto optimal SAEs, indicating the best trade-off between sparsity and reconstruction fidelity.", "section": "4.1 Benchmarking Gated SAEs"}, {"figure_path": "zLBlin2zvW/tables/tables_29_1.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results of training baseline sparse autoencoders (SAEs) on the Gemma-7B language model.  The table shows hyperparameters used during training, along with key performance metrics such as the percentage of loss recovered and the L0 sparsity.  The italicized entries highlight the Pareto optimal SAEs, representing the best trade-off between reconstruction fidelity and sparsity for each hyperparameter setting. Results are provided for multiple layers and sites within the model.", "section": "4.1 Benchmarking Gated SAEs"}, {"figure_path": "zLBlin2zvW/tables/tables_30_1.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results for Gemma-7B Baseline SAEs, with a sequence length of 1024.  It shows various hyperparameters used during training (\u03bb, LR), the resulting sparsity (LO), the percentage of loss recovered, the clean cross-entropy loss, the SAE cross-entropy loss, and the 0-ablation cross-entropy loss.  Additionally, the table indicates the width of the SAE, the percentage of alive features, the shrinkage factor (\u03b3), and the number of features.  The italicized entries indicate Pareto optimal SAEs, representing a trade-off between reconstruction accuracy and sparsity.", "section": "4.1 Benchmarking Gated SAEs"}, {"figure_path": "zLBlin2zvW/tables/tables_31_1.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results of training baseline sparse autoencoders (SAEs) on the Gemma-7B language model.  The table shows the performance of different SAEs with varying sparsity levels (Sparsity), achieved by adjusting the L1 regularization parameter (\u03bb).  For each SAE, several metrics are reported: the learning rate (LR) used during training, the number of active features (LO), the percentage of loss recovered relative to a zero-ablation baseline (% CE Recovered), the cross-entropy loss for the original language model (Clean CE Loss), the cross-entropy loss after applying the SAE reconstruction (SAE CE Loss), the cross-entropy loss after zeroing out the activations before the SAE (0 Abl. CE Loss), the width of the SAE's hidden layer (Width), the percentage of alive neurons (% Alive), the relative reconstruction bias (Shrinkage), and the total number of features used in the SAE dictionary (Features).  The italicized entries represent SAEs that achieve a Pareto optimal balance between sparsity and reconstruction fidelity.", "section": "4.1 Benchmarking Gated SAEs"}, {"figure_path": "zLBlin2zvW/tables/tables_32_1.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results of training baseline sparse autoencoders (SAEs) on the Gemma-7B language model.  It shows the performance of SAEs at different layers and sites within the model. The key metrics presented include sparsity (LO), reconstruction fidelity (% CE Recovered), and the relative reconstruction bias (Shrinkage).  The table helps in understanding how the performance of baseline SAEs varies across different layers and locations within the model. The italicized entries indicate the Pareto optimal SAEs, representing the best tradeoff between sparsity and reconstruction quality.", "section": "4.1 Benchmarking Gated SAEs"}, {"figure_path": "zLBlin2zvW/tables/tables_33_1.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results for Gemma-7B baseline SAEs with a sequence length of 1024.  It shows various hyperparameters used during training, including the L1 regularization strength (lambda), learning rate, and the resulting L0 sparsity (average number of active features).  Key performance metrics are also listed:  Loss Recovered (how well the model's performance is preserved after using the SAE's reconstruction), cross-entropy loss for the clean model, the SAE itself and for the zero-ablated (where the corresponding sub-layers activations are set to zero), the width of the SAE and how many features were actually alive (percentage of active features), and the relative reconstruction bias (shrinkage), which measures the extent of underestimation of feature activations. The italicized rows represent Pareto optimal SAEs; these represent trade-offs where improvement in one metric does not come at the cost of worsening another.  This table helps to understand the performance of the baseline SAEs and serves as a basis of comparison against Gated SAEs later in the paper.", "section": "4.1 Benchmarking Gated SAEs"}, {"figure_path": "zLBlin2zvW/tables/tables_34_1.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results of training baseline sparse autoencoders (SAEs) on the Gemma-7B language model.  The table shows the hyperparameters used for training, including the sparsity penalty (\u03bb), learning rate (LR), and the resulting L0 (sparsity) and loss recovered (reconstruction fidelity).  The table also shows the percentage of clean CE loss recovered, clean CE loss, SAE CE loss, 0 Abl. CE loss, the width of the SAE, the percentage of alive features, and the shrinkage observed.  The italicized rows indicate the Pareto optimal SAEs, representing the best trade-off between sparsity and reconstruction fidelity.", "section": "4.1 Benchmarking Gated SAEs"}, {"figure_path": "zLBlin2zvW/tables/tables_35_1.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results of training baseline sparse autoencoders (SAEs) on the Gemma-7B language model.  It shows the performance of SAEs at different layers (6, 13, 20, 27) and sites (residual stream, MLP, attention) within the model. The table includes hyperparameters (\u03bb, LR), sparsity metrics (LO), reconstruction fidelity (% CE Recovered), cross-entropy loss for clean activations, SAE reconstructions, and zero-ablated activations, the width of the SAE, percentage of alive features, and a shrinkage metric.  Italicized entries indicate Pareto optimal SAEs, representing the best trade-off between sparsity and reconstruction fidelity.", "section": "4.1 Benchmarking Gated SAEs"}, {"figure_path": "zLBlin2zvW/tables/tables_36_1.jpg", "caption": "Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs.", "description": "This table presents the results of training baseline sparse autoencoders (SAEs) on the Gemma-7B language model.  It shows the hyperparameters used for training, the resulting model performance metrics (loss recovered, sparsity (L0), etc.), and  whether the model was Pareto optimal.  The table focuses on different layers and sites within the model and helps compare performance of different configurations.", "section": "4.1 Benchmarking Gated SAEs"}]