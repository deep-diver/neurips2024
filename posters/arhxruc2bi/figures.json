[{"figure_path": "aRhxruC2bi/figures/figures_0_1.jpg", "caption": "Figure 1: Illustration of different approaches for open-vocabulary semantic segmentation. In contrast to existing methods utilizing (a) pixel-level semantic labels [1, 2, 3, 4, 5, 6] or (b) image-level semantic labels [7, 8, 9, 10, 11, 12], we leverage unlabeled masks as supervision, which can be freely generated from vision foundation models such as SAM [13] and DINO [14].", "description": "This figure illustrates three different approaches to open-vocabulary semantic segmentation. (a) shows the traditional method using pixel-level semantic labels, which requires extensive manual annotation. (b) demonstrates a less demanding approach utilizing image-level semantic labels (e.g., captions).  (c) presents the proposed method, PixelCLIP, which leverages unlabeled masks generated by vision foundation models (like SAM and DINO).  These masks, without explicit semantic labels, provide a novel form of supervision for training.", "section": "1 Introduction"}, {"figure_path": "aRhxruC2bi/figures/figures_1_1.jpg", "caption": "Figure 2: Visualization of masks from vision foundation models. We visualize the masks generated by SAM [13] and by clustering image features from DINO [14]. Although such models can freely generate fine-grained masks, the resulting masks can be too small or incomplete to have semantic meaning. To address this over-segmentation issue, we employ online clustering [18] of the masks into semantically meaningful groups defined globally for given images.", "description": "This figure shows the masks generated by two different vision foundation models, SAM and DINO.  The raw masks from each model are shown alongside versions where the masks have been clustered to improve semantic coherence.  The goal of the clustering is to group fine-grained masks into larger, more semantically meaningful regions.", "section": "3 Methodology"}, {"figure_path": "aRhxruC2bi/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of our overall framework. We provide illustration of PixelCLIP, utilizing unlabeled images and masks for fine-tuning the image encoder of CLIP, enabling open-vocabulary semantic segmentation. We note that the momentum image encoder and the mask decoder are only leveraged during training, and inference is only done with image and text encoders of CLIP.", "description": "This figure illustrates the PixelCLIP framework.  It shows how unlabeled images and masks from vision foundation models (like SAM and DINO) are used to guide the CLIP image encoder to perform open-vocabulary semantic segmentation. The process involves online clustering of masks using learnable class prompts, and a momentum encoder is used during training for stability.  Inference, however, only utilizes the image and text encoders of CLIP.", "section": "3 Methodology"}, {"figure_path": "aRhxruC2bi/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison between PixelCLIP and CLIP. We provide qualitative comparison on ADE-20K [51] dataset with PixelCLIP and CLIP. We demonstrate the dense visual recognition capabilities achieved from fine-tuning CLIP, whereas CLIP shows results with significant noise.", "description": "This figure compares the qualitative results of open-vocabulary semantic segmentation on the ADE-20K dataset between PixelCLIP and CLIP. PixelCLIP demonstrates superior performance in recognizing and segmenting objects within the images, while CLIP's results are noisy and less accurate.  The figure highlights the improved visual recognition capabilities of PixelCLIP by showing examples where PixelCLIP successfully segments objects, while CLIP struggles to do so.", "section": "4.3 Results"}, {"figure_path": "aRhxruC2bi/figures/figures_8_1.jpg", "caption": "Figure 5: Visualization of learned class prompts. We visualize the text features from our learned class prompts, as well as text features from classnames of COCO-Stuff with t-SNE visualization in (a-b). We also visualize images inferenced with the learned class prompts in (c-d).", "description": "This figure visualizes the learned class prompts' text features using t-SNE for dimensionality reduction.  (a) and (b) compare the learned prompts (purple) to COCO-Stuff class names (orange) for different numbers of clusters (k=64 and k=128). The visualization shows how well the learned prompts capture semantic concepts. (c) and (d) display images generated using the learned class prompts, demonstrating the model's ability to generate images based on the learned semantic representation.", "section": "4.5 Analysis"}, {"figure_path": "aRhxruC2bi/figures/figures_8_2.jpg", "caption": "Figure 6: Visualization of interpreting learned text prompt. We provide visualization on results for predicting with learned class prompts, then mapping the results to classes in the dataset with the highest similarity to the prompt.", "description": "This figure visualizes the results of using learned class prompts to predict semantic segmentation masks.  The learned prompts, represented by embeddings, are mapped to the nearest class names from the COCO-Stuff dataset. The images show the predicted masks overlaid on the original image, illustrating how well the learned prompts capture semantic concepts and how the predictions align with the ground truth classes. The comparison between results using 64 and 128 clusters highlights the impact of the number of clusters on the granularity of semantic understanding.", "section": "4.5 Analysis"}, {"figure_path": "aRhxruC2bi/figures/figures_16_1.jpg", "caption": "Figure 7: Visualization on COCO-Stuff with learned class prompts. We provide results with learned classes with different k up to 256.", "description": "This figure visualizes the results of applying PixelCLIP on the COCO-Stuff dataset with varying numbers of clusters (k). It shows the segmentation masks generated by the model for different values of k (64, 128, and 256). The images demonstrate how the model's segmentation performance changes as the number of clusters increases.  Specifically, it highlights the impact of the online clustering algorithm on the granularity of the segmentation.  With a lower k, larger semantic regions are clustered together; a higher k results in more refined segmentation that separates semantically similar regions.", "section": "4.3 Results"}, {"figure_path": "aRhxruC2bi/figures/figures_16_2.jpg", "caption": "Figure 4: Comparison between PixelCLIP and CLIP. We provide qualitative comparison on ADE-20K [51] dataset with PixelCLIP and CLIP. We demonstrate the dense visual recognition capabilities achieved from fine-tuning CLIP, whereas CLIP shows results with significant noise.", "description": "This figure shows a qualitative comparison of the results obtained by PixelCLIP and CLIP on the ADE-20K dataset. PixelCLIP demonstrates significantly improved dense visual recognition capabilities, producing clearer and more accurate segmentations compared to CLIP, which exhibits substantial noise and inaccuracies in its predictions.", "section": "4.3 Results"}, {"figure_path": "aRhxruC2bi/figures/figures_17_1.jpg", "caption": "Figure 4: Comparison between PixelCLIP and CLIP. We provide qualitative comparison on ADE-20K [51] dataset with PixelCLIP and CLIP. We demonstrate the dense visual recognition capabilities achieved from fine-tuning CLIP, whereas CLIP shows results with significant noise.", "description": "This figure shows a qualitative comparison of the results obtained by PixelCLIP and CLIP on the ADE20K dataset. PixelCLIP, by leveraging unlabeled masks and fine-tuning the CLIP image encoder, demonstrates significantly improved dense visual recognition compared to the standard CLIP model, which produces results with considerable noise.", "section": "4.3 Results"}]