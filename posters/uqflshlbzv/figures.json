[{"figure_path": "UQflshLbZv/figures/figures_0_1.jpg", "caption": "Figure 1: Our framework supports individual or collaborative editing of hairstyle and color, utilizing text, reference images, and stroke maps. With exceptional performance, particularly evident in editing multiple hair colors.", "description": "This figure showcases the results of the HairDiffusion model on various hair editing tasks.  It demonstrates the model's ability to edit both hairstyle and hair color individually or simultaneously using different input modalities (text descriptions, reference images, and stroke masks). The images highlight the model's effectiveness in handling multiple hair colors and maintaining high-quality results, even in complex scenarios.", "section": "Abstract"}, {"figure_path": "UQflshLbZv/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of HairDiffusion: (a) Using a hairstyle description Ts or reference image I as conditional input, coupled with the hair-agnostic mask Ma and source image Isrc, we can get the style proxy Ps. (b) Leveraging the color proxy and style proxy, along with the hair-agnostic mask Me and source image Isrc, enables individual or collaborative editing of hair color and hairstyle. (c) Given a series of conditions driven from the input image Ic, the hair color reference image I is used to obtain the color proxy Pe through a warping module. In the case of changing only the hairstyle while preserving the original hair color, I = Isrc. (d) The color proxy P\u00ba and the style proxy Ps are blended at different stages of the diffusion process.", "description": "This figure provides a visual overview of the HairDiffusion model, illustrating its pipeline for both hairstyle and hair color editing. It shows how the model uses different components: hairstyle editing leverages a hairstyle description or reference image along with a hair-agnostic mask and source image to generate a style proxy. Hair color editing utilizes both style and color proxies with a hair-agnostic mask and source image. A warping module aligns the color reference image with the source image to obtain a color proxy. Finally, a multi-stage hairstyle blend method combines the style and color proxies within the diffusion process for refined editing.", "section": "3 Method"}, {"figure_path": "UQflshLbZv/figures/figures_6_1.jpg", "caption": "Figure 3: Visual comparison with HairCLIPv2 [36], HairCLIP [35], TediGAN [38], Power-Paint (\"ControlNet\" version) [47], ControlNet-Inpainting [42], and DiffCLIP [18]. The simplified text descriptions (editing hairstyle, hair color, or both of them) are listed on the leftmost side. Our approach demonstrates better editing effects and irrelevant attribute preservation (e.g., identity, background).", "description": "This figure compares the results of HairDiffusion with several other state-of-the-art hair editing methods.  It showcases examples of various hair color and style edits, highlighting the superior performance of HairDiffusion in terms of image quality, detail preservation, and the ability to maintain the integrity of background and facial features.  The simplified text prompts used for each edit are also displayed for reference. ", "section": "4 Experiments"}, {"figure_path": "UQflshLbZv/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison with HairCLIPv2 [36] in detail. Our approach shows better preservation of irrelevant attributes.", "description": "This figure compares the results of the proposed HairDiffusion method with the HairCLIPv2 method on two examples of hair editing.  The left side shows a transformation from a short, dark haircut to a vibrant red afro. The right side shows a change from dark hair to blonde ringlets.  Both examples highlight HairDiffusion's improved ability to preserve facial details and other features such as eyebrows, earrings, and makeup, unlike HairCLIPv2, which shows noticeable artifacts and alterations to non-hair regions. The detailed comparison demonstrates the superior performance of HairDiffusion in maintaining the integrity of the original image while accomplishing the desired hair editing.", "section": "4.1 Quantitative and Qualitative Comparison"}, {"figure_path": "UQflshLbZv/figures/figures_8_1.jpg", "caption": "Figure 5: Visual comparison with HairCLIPv2 [36], HairCLIP [35], Barbershop [45], CtrlHair [11], MichiGAN [33] and HairFastGAN [26] on hair color transfer.", "description": "This figure compares the results of HairDiffusion with several state-of-the-art hair color transfer methods.  The input images are shown in the first column, followed by the results generated by each method.  The figure demonstrates HairDiffusion's ability to accurately transfer hair color while maintaining the integrity of other facial features.", "section": "4.2 Ablation Study"}, {"figure_path": "UQflshLbZv/figures/figures_9_1.jpg", "caption": "Figure 6: Ablation studies on text-guided hairstyle editing and reference image-guided hair color editing.", "description": "This figure demonstrates an ablation study on the HairDiffusion model. It shows the effects of different components of the model on the final output. The top row shows the results of text-guided hairstyle editing. The bottom row shows the results of reference image-guided hair color editing. Each column shows the results of adding one more component to the model. The first column shows the original image. The second column shows the results of adding a hair-agnostic mask. The third column shows the results of adding pose control. The fourth column shows the results of adding a color proxy (unwarped). The fifth column shows the results of adding a warping module. The sixth column shows the results of adding bilateral filtering.", "section": "Experiments"}, {"figure_path": "UQflshLbZv/figures/figures_9_2.jpg", "caption": "Figure 7: Visualizations of the ablation studies on the warping module and corresponding post-processing.", "description": "This figure shows the results of ablation studies on the warping module and its post-processing steps. It visually demonstrates the effect of each step (warping, patch match, bilateral filter) on hair color transfer and hairstyle generation.  Each row represents a different hair style and color transfer scenario. By comparing the results of each step, one can understand the contribution of each component to the final output and how these individual components work together.", "section": "3.4 Hair Color Aligning"}, {"figure_path": "UQflshLbZv/figures/figures_13_1.jpg", "caption": "Figure 2: Overview of HairDiffusion: (a) Using a hairstyle description T<sub>s</sub> or reference image I<sub>ref</sub> as conditional input, coupled with the hair-agnostic mask M<sub>a</sub> and source image I<sub>src</sub>, we can get the style proxy P<sub>s</sub>. (b) Leveraging the color proxy and style proxy, along with the hair-agnostic mask M<sub>c</sub> and source image I<sub>src</sub>, enables individual or collaborative editing of hair color and hairstyle. (c) Given a series of conditions driven from the input image I<sub>c</sub>, the hair color reference image I<sub>ref</sub> is used to obtain the color proxy P<sub>c</sub> through a warping module. In the case of changing only the hairstyle while preserving the original hair color, I<sub>ref</sub> = I<sub>src</sub>. (d) The color proxy P<sub>c</sub> and the style proxy P<sub>s</sub> are blended at different stages of the diffusion process.", "description": "This figure illustrates the HairDiffusion framework, showing its two main stages: hairstyle editing and hair color editing.  The hairstyle editing stage uses a text description or reference image along with a hair-agnostic mask to generate a style proxy, which is then used to modify the hairstyle in the input image. The hair color editing stage utilizes both a style proxy and a color proxy (obtained from a reference image via a warping module) to control both hairstyle and hair color simultaneously.  The multi-stage hairstyle blend (MHB) is also highlighted, demonstrating how the color and style proxies are integrated into the diffusion process.  Different masks (M<sub>a</sub> and M<sub>c</sub>) are employed for different stages to better separate the control of hair and facial features.", "section": "3 Method"}, {"figure_path": "UQflshLbZv/figures/figures_14_1.jpg", "caption": "Figure 2: Overview of HairDiffusion: (a) Using a hairstyle description Ts or reference image I as conditional input, coupled with the hair-agnostic mask Ma and source image Isrc, we can get the style proxy Ps. (b) Leveraging the color proxy and style proxy, along with the hair-agnostic mask Me and source image Isrc, enables individual or collaborative editing of hair color and hairstyle. (c) Given a series of conditions driven from the input image Ic, the hair color reference image I is used to obtain the color proxy Pc through a warping module. In the case of changing only the hairstyle while preserving the original hair color, I = Isrc. (d) The color proxy Pc and the style proxy Ps are blended at different stages of the diffusion process.", "description": "This figure provides a visual overview of the HairDiffusion model, illustrating the different stages involved in hairstyle and hair color editing.  Panel (a) shows the process of generating a 'style proxy' from a text description or reference image, combined with a hair-agnostic mask and the input image. Panel (b) demonstrates how the style proxy and a 'color proxy' (obtained from a reference color image via a warping module shown in (c)) are used together for collaborative editing. Panel (d) details the multi-stage hairstyle blend, showing how the color and style proxies are blended in different stages of the diffusion process to create the final edited image.", "section": "3 Method"}, {"figure_path": "UQflshLbZv/figures/figures_14_2.jpg", "caption": "Figure 2: Overview of HairDiffusion: (a) Using a hairstyle description T<sub>s</sub> or reference image I<sub>ref</sub> as conditional input, coupled with the hair-agnostic mask M<sub>a</sub> and source image I<sub>src</sub>, we can get the style proxy P<sub>s</sub>. (b) Leveraging the color proxy and style proxy, along with the hair-agnostic mask M<sub>c</sub> and source image I<sub>src</sub>, enables individual or collaborative editing of hair color and hairstyle. (c) Given a series of conditions driven from the input image I<sub>c</sub>, the hair color reference image I<sub>ref</sub> is used to obtain the color proxy P<sub>c</sub> through a warping module. In the case of changing only the hairstyle while preserving the original hair color, I<sub>ref</sub> = I<sub>src</sub>. (d) The color proxy P<sub>c</sub> and the style proxy P<sub>s</sub> are blended at different stages of the diffusion process.", "description": "This figure provides a visual overview of the HairDiffusion framework. Panel (a) illustrates hairstyle editing using text or image input and a hair-agnostic mask. Panel (b) shows how hair color and hairstyle can be edited collaboratively, leveraging both color and style proxies. Panel (c) details the hair warping module that aligns the hair color reference image with the input image.  Finally, Panel (d) demonstrates the multi-stage hairstyle blend (MHB) which integrates color and style information during different stages of the diffusion process.", "section": "3 Method"}, {"figure_path": "UQflshLbZv/figures/figures_15_1.jpg", "caption": "Figure 11: Visualization of the reconstruction comparison with StyleGAN Salon and HairFastGAN.", "description": "This figure provides a visual comparison of hair reconstruction results between the proposed HairDiffusion method and two other state-of-the-art methods: StyleGAN Salon and HairFastGAN.  It showcases the performance of each method on a variety of hairstyles and hair colors, highlighting the strengths and limitations of each approach in terms of accuracy, color preservation, and overall visual realism. The figure is crucial in demonstrating the efficacy of HairDiffusion in generating high-quality and realistic results.", "section": "A.5 Examples of Reconstruction"}, {"figure_path": "UQflshLbZv/figures/figures_16_1.jpg", "caption": "Figure 6: Ablation studies on text-guided hairstyle editing and reference image-guided hair color editing.", "description": "This figure demonstrates an ablation study on the HairDiffusion model, showing the effects of different components on hairstyle and hair color editing.  The top row showcases original images and the bottom row shows the results after the editing process. Each column presents a different ablation experiment by removing specific components of the model: hair-agnostic mask, pose control, color proxy (unwarped), warping module, and bilateral filtering. The results show the importance of each component in achieving high-quality results, highlighting how different combinations of factors influence the model's capacity to generate realistic and consistent hairstyle and hair color edits.", "section": "Experiments"}, {"figure_path": "UQflshLbZv/figures/figures_16_2.jpg", "caption": "Figure 13: By using a stroke map encoder in the latent space, it inevitably overlooks details of hair color, as shown in the second column. And it does not completely match the provided hair color areas, as shown in the fourth column.", "description": "This figure demonstrates the limitations of using stroke map encoders in latent space for hair color editing.  The second column shows that the method overlooks details in the hair color, failing to capture the nuances and variations.  The fourth column illustrates a mismatch between the generated and provided hair color areas, highlighting the model's inability to accurately replicate the specified color patterns.", "section": "A.7 Social Impact"}, {"figure_path": "UQflshLbZv/figures/figures_17_1.jpg", "caption": "Figure 2: Overview of HairDiffusion: (a) Using a hairstyle description T<sub>s</sub> or reference image I<sub>ref</sub> as conditional input, coupled with the hair-agnostic mask M<sub>a</sub> and source image I<sub>src</sub>, we can get the style proxy P<sub>s</sub>. (b) Leveraging the color proxy and style proxy, along with the hair-agnostic mask M<sub>c</sub> and source image I<sub>src</sub>, enables individual or collaborative editing of hair color and hairstyle. (c) Given a series of conditions driven from the input image I<sub>c</sub>, the hair color reference image I<sub>ref</sub> is used to obtain the color proxy P<sub>c</sub> through a warping module. In the case of changing only the hairstyle while preserving the original hair color, I<sub>ref</sub> = I<sub>src</sub>. (d) The color proxy P<sub>c</sub> and the style proxy P<sub>s</sub> are blended at different stages of the diffusion process.", "description": "This figure illustrates the HairDiffusion framework, showing the process of hairstyle and hair color editing. It consists of four parts:\n(a) Hairstyle editing using text or reference image as input and a hair-agnostic mask to generate a style proxy.\n(b) Hair color editing using both style and color proxies, along with a hair-agnostic mask, for combined or individual control.\n(c) Hair warping module that aligns the reference hair color image to the source image using facial keypoints and generates a color proxy.\n(d) Multi-stage hairstyle blend that combines color and style proxies at different diffusion stages for effective hair editing.", "section": "3 Method"}, {"figure_path": "UQflshLbZv/figures/figures_18_1.jpg", "caption": "Figure 2: Overview of HairDiffusion: (a) Using a hairstyle description T<sub>s</sub> or reference image I<sub>ref</sub> as conditional input, coupled with the hair-agnostic mask M<sub>a</sub> and source image I<sub>src</sub>, we can get the style proxy P<sub>s</sub>. (b) Leveraging the color proxy and style proxy, along with the hair-agnostic mask M<sub>c</sub> and source image I<sub>src</sub>, enables individual or collaborative editing of hair color and hairstyle. (c) Given a series of conditions driven from the input image I<sub>c</sub>, the hair color reference image I<sub>ref</sub> is used to obtain the color proxy P<sub>c</sub> through a warping module. In the case of changing only the hairstyle while preserving the original hair color, I<sub>ref</sub> = I<sub>src</sub>. (d) The color proxy P<sub>c</sub> and the style proxy P<sub>s</sub> are blended at different stages of the diffusion process.", "description": "This figure provides a visual overview of the HairDiffusion framework. It illustrates the process of hairstyle editing using text or image input (a), hair color editing using a color proxy and style proxy (b), the hair warping module that aligns the hair color from the reference image to the source image (c), and the multi-stage hairstyle blend that combines the style and color proxies for better editing results (d).", "section": "3 Method"}]