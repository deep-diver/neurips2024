[{"figure_path": "P6aJ7BqYlc/figures/figures_3_1.jpg", "caption": "Figure 1: An overview of our proposed GACL. (a) Labels of the exposed class and the unexposed class are extracted in each GCIL task (see definition in Section 3.2), respectively. (b) A frozen pre-trained ViT and a buffer layer are utilized to extract features from the inputs. (c) The key to the recursively updated formulation of the GACL contains two components. The \u0174(k) takes in the contribution of unexposed class data (see (11)). The other is contributed by the ECLG module (e.g., see (12)), which reflects the gain of exposed class data on the seen categories. The recursive formulation flows aided by the autocorrelation memory matrix R throughout the GCIL.", "description": "This figure provides a high-level overview of the Generalized Analytic Continual Learning (GACL) method.  It breaks down the process into three main stages: 1) Splitting the input data into exposed and unexposed classes; 2) Extracting features using a pre-trained ViT and buffer layer; and 3) Recursive weight updates using exposed and unexposed class information and an autocorrelation memory matrix.", "section": "3 The Proposed Method"}, {"figure_path": "P6aJ7BqYlc/figures/figures_7_1.jpg", "caption": "Figure 2: The task-wise accuracy Ak of the GACL with EFCIL methods (top) and replay-based methods (bottom) on benchmark datasets with K = 5.", "description": "This figure compares the performance of the proposed GACL method against other state-of-the-art EFCIL (exemplar-free class incremental learning) and replay-based methods across three benchmark datasets (CIFAR-100, ImageNet-R, Tiny-ImageNet). The plots show the task-wise accuracy (Ak) for each method over five consecutive tasks (K=5).  The top panel displays the results for EFCIL methods, while the bottom panel shows the results for replay-based methods.  The figure illustrates the consistent superior performance of GACL across all datasets and task settings compared to existing methods, particularly its ability to maintain high accuracy over multiple tasks (avoiding catastrophic forgetting).", "section": "4.2 Comparison with State-of-the-arts"}, {"figure_path": "P6aJ7BqYlc/figures/figures_15_1.jpg", "caption": "Figure 3: A configuration example of Si-Blurry setting.", "description": "The Si-Blurry setting satisfies all three properties of GCIL mentioned in Appendix B and can be treated as its good realization. As shown in Figure 3, for a K-task learning, the Si-Blurry first randomly partitions all classes into two groups: disjoint classes that cannot overlap between tasks and blurry classes that might reappear. The ratio of partition is controlled by the disjoint class ratio rD, which is defined as the ratio of the number of disjoint classes to the number of all classes. Then disjoint classes and blurry classes are randomly assigned to disjoint tasks (TD) and blurry tasks (TB) respectively. Next, each blurry task further conducts the blurry sample division by randomly extracting part of samples to assign to other blurry tasks based on blurry sample ratio r\u03b2, which is defined as the ratio of the extracted sample within samples in all blurry tasks. Finally, each Si-Blurry task TB+D with a stochastic blurry task boundary consists of a disjoint and blurry task. We adopt Si-Blurry with different combinations of rD and r\u03b2 for reliable empirical validations.", "section": "Si-Blurry Setting"}, {"figure_path": "P6aJ7BqYlc/figures/figures_16_1.jpg", "caption": "Figure 1: An overview of our proposed GACL. (a) Labels of the exposed class and the unexposed class are extracted in each GCIL task (see definition in Section 3.2), respectively. (b) A frozen pre-trained ViT and a buffer layer are utilized to extract features from the inputs. (c) The key to the recursively updated formulation of the GACL contains two components. The \u0174(k) takes in the contribution of unexposed class data (see (11)). The other is contributed by the ECLG module (e.g., see (12)), which reflects the gain of exposed class data on the seen categories. The recursive formulation flows aided by the autocorrelation memory matrix R throughout the GCIL.", "description": "This figure provides a high-level overview of the proposed Generalized Analytic Continual Learning (GACL) method. It shows the three main components: (a) the splitting of incoming data into exposed and unexposed classes, (b) feature extraction using a pre-trained Vision Transformer (ViT) and a buffer layer, and (c) the recursive weight update mechanism that leverages exposed class label gain (ECLG) and autocorrelation memory.", "section": "3 The Proposed Method"}, {"figure_path": "P6aJ7BqYlc/figures/figures_17_1.jpg", "caption": "Figure 5: Real-time accuracy of the GACL on all benchmark datasets with various values of the regularization term \u03b3.", "description": "The figure shows the real-time accuracy of the proposed GACL on CIFAR-100, ImageNet-R, and Tiny-ImageNet datasets with different regularization term (\u03b3) values.  The x-axis represents the number of training samples, and the y-axis represents the real-time accuracy. Different colored lines represent different \u03b3 values.  The figure demonstrates the robustness of the GACL across different datasets and regularization strengths, showing generally stable performance across a wide range of \u03b3 values, with some minor variations depending on the dataset.", "section": "4.3 Ablation Study on the ECLG Module"}, {"figure_path": "P6aJ7BqYlc/figures/figures_18_1.jpg", "caption": "Figure 1: An overview of our proposed GACL. (a) Labels of the exposed class and the unexposed class are extracted in each GCIL task (see definition in Section 3.2), respectively. (b) A frozen pre-trained ViT and a buffer layer are utilized to extract features from the inputs. (c) The key to the recursively updated formulation of the GACL contains two components. The \u0174(k) takes in the contribution of unexposed class data (see (11)). The other is contributed by the ECLG module (e.g., see (12)), which reflects the gain of exposed class data on the seen categories. The recursive formulation flows aided by the autocorrelation memory matrix R throughout the GCIL.", "description": "This figure provides a high-level overview of the proposed Generalized Analytic Continual Learning (GACL) method.  It's broken down into three parts:\n\n(a) **Exposed-Unexposed Class Split:** Shows how the input data for each task is divided into exposed (previously seen) and unexposed (new) classes.\n\n(b) **Buffered Embedding Extraction:** Illustrates how a pre-trained Vision Transformer (ViT) and a buffer layer extract features from the input images.\n\n(c) **Generalized Analytic Class Incremental Learning:** This depicts the core GACL algorithm, highlighting the recursive update process using exposed and unexposed class information and an autocorrelation memory matrix.  The key components are the weights updated for unexposed classes and an Exposed Class Label Gain (ECLG) module.", "section": "3 The Proposed Method"}]