[{"figure_path": "FZW7Ctyjm3/tables/tables_6_1.jpg", "caption": "Table 1: Performance of STIC compared with the original LVLM model across vision-language reasoning tasks. For LLaVA-v1.5 (Vicuna 7B), we directly report the values in the paper of POVID, and \"-\" indicates an unreported value.", "description": "This table presents the performance comparison between the original Large Vision Language Model (LLVM) and the same model enhanced with Self-Training on Image Comprehension (STIC) across seven different vision-language reasoning benchmarks.  The benchmarks cover diverse tasks such as scientific question answering, mathematical reasoning, optical character recognition, and various visual question answering scenarios.  The table includes results for different LLVM versions (7B and 13B parameters) and compares STIC's performance against baselines (InstructBLIP, mPLUG-OWL2, and POVID). The \"-\" symbol indicates that the baseline study did not report that specific metric.", "section": "5.2 Main Results"}, {"figure_path": "FZW7Ctyjm3/tables/tables_7_1.jpg", "caption": "Table 1: Performance of STIC compared with the original LVLM model across vision-language reasoning tasks. For LLaVA-v1.5 (Vicuna 7B), we directly report the values in the paper of POVID, and \"-\" indicates an unreported value.", "description": "This table presents the performance comparison between the original LLaVA models (versions 1.5 and 1.6, both with 7B parameters) and their corresponding versions fine-tuned using the STIC method across seven different vision-language benchmarks.  The benchmarks cover various tasks and domains, including ScienceQA (scientific reasoning), TextVQA (text-based VQA), ChartQA (chart-based reasoning), LLaVA-Bench (general VQA), MMBench (multimodal benchmark), MM-Vet (visual reasoning in veterinary medicine), and MathVista (mathematical reasoning). For LLaVA-v1.5, results from a concurrent work (POVID) are also included for comparison. The table highlights the improvement in accuracy achieved by STIC on each benchmark.", "section": "5.2 Main Results"}, {"figure_path": "FZW7Ctyjm3/tables/tables_8_1.jpg", "caption": "Table 3: Test performance of STIC if we remove negative examples and use positive ones to perform SFT in Stage 1.", "description": "This table presents the results of an ablation study on the impact of negative samples within the STIC (Self-Training on Image Comprehension) framework.  By comparing the performance of STIC using only positive samples (preferred responses) against the full STIC approach which utilizes both positive and negative samples, the table quantifies the contribution of negative samples to model improvement.  The results highlight the importance of negative samples (dispreferred responses) in achieving the performance gains reported by STIC.", "section": "6 Ablation Studies and Discussions"}, {"figure_path": "FZW7Ctyjm3/tables/tables_9_1.jpg", "caption": "Table 4: Performance of STIC on different stage-1 training images compared with the original LVLM model LLaVA-v.16 (Mistral 7B) across vision-language reasoning benchmarks.", "description": "This table presents the performance comparison between the original LLaVA-v1.6 (7B) model and the model enhanced with the STIC method using two different datasets for training: COCO and VFLAN.  The performance is measured across several vision-language reasoning tasks on the LLaVA-Bench, MM-Vet, and MMBench benchmarks. The table shows the accuracy scores for various sub-tasks within each benchmark, providing a comprehensive evaluation of the STIC method's effectiveness in improving image comprehension and overall model performance.", "section": "5.2 Main Results"}, {"figure_path": "FZW7Ctyjm3/tables/tables_9_2.jpg", "caption": "Table 5: Performance of STIC compared with the original LVLM model LLaVA-v1.6 (Vicuna 13B) across vision-language reasoning tasks. Image data used for 13B model remain the same as what we used for the 7B model.", "description": "This table presents the performance comparison between the original LLaVA-v1.6 model (using Vicuna 13B) and the same model enhanced with the STIC method.  It shows the accuracy scores on various vision-language reasoning tasks (LLaVA-Bench, MM-Vet, and MMBench). The improvement from STIC is also provided. Note that the same image data was used for both the 7B and 13B models.", "section": "5.2 Main Results"}, {"figure_path": "FZW7Ctyjm3/tables/tables_18_1.jpg", "caption": "Table 1: Performance of STIC compared with the original LVLM model across vision-language reasoning tasks. For LLaVA-v1.5 (Vicuna 7B), we directly report the values in the paper of POVID, and \"-\" indicates an unreported value.", "description": "This table presents a comparison of the performance of the Self-Training on Image Comprehension (STIC) method against the original Large Vision Language Models (LVLMs) across seven different vision-language reasoning tasks.  It shows accuracy improvements achieved by STIC on various benchmarks, including ScienceQA, TextVQA, ChartQA, LLaVA-Bench, MMBench, MM-Vet, and MathVista.  The table also includes a comparison with a concurrent baseline method, POVID, using LLaVA-v1.5 (Vicuna 7B).", "section": "5.2 Main Results"}, {"figure_path": "FZW7Ctyjm3/tables/tables_18_2.jpg", "caption": "Table 7: Detailed performance of STIC compared with the original VLM model on the MM-Bench dev set.", "description": "This table presents a detailed comparison of the performance of the original Large Vision Language Model (LVLM) and the LVLM enhanced with Self-Training on Image Comprehension (STIC) across six different sub-tasks within the MMBench dev set.  It shows the accuracy improvement achieved by STIC for each subtask, highlighting the overall effectiveness of the proposed method.", "section": "5.2 Main Results"}, {"figure_path": "FZW7Ctyjm3/tables/tables_18_3.jpg", "caption": "Table 8: Detailed performance of STIC compared with the original VLM model on the MM-Vet benchmark.", "description": "This table presents a detailed comparison of the performance of the original Vision Language Model (VLM) and the same VLM after applying the Self-Training on Image Comprehension (STIC) method. The comparison is done across six different sub-tasks within the MM-Vet benchmark: recognition, optical character recognition, knowledge, generation, spatial reasoning, and mathematical reasoning. For each subtask, the table shows the accuracy scores achieved by both the original VLM and the STIC-enhanced VLM, demonstrating the improvement gained by applying the STIC method.", "section": "5.2 Main Results"}, {"figure_path": "FZW7Ctyjm3/tables/tables_20_1.jpg", "caption": "Table 9: Test performance of llava-v1.6-mistral-7b using various prompts with DaR. We evaluate prompt quality using DaR as a prompting method. DaR=None represents the original LVLM model's performance. Normal prompt refers to the simple prompt we used for DaR in our paper. GPT-4's well-curated prompt refers to the prompt we used for preferred response generation, and we include Mistral 7B's curated prompt for additional comparison.", "description": "This table shows the impact of different prompt styles on the performance of the LLaVA-v1.6 (7B) model when using the describe-and-respond (DaR) method. It compares the performance using no DaR prompt, a normal prompt, a hallucination prompt, and two well-curated prompts (one from Llama-3 8B and one from GPT-4) across three benchmarks: LLaVA-Bench, MM-Vet, and MMBench.  The numbers in parentheses show the performance difference compared to the baseline (no DaR prompt).  The table highlights the significant positive effect of using well-crafted prompts and the detrimental effect of using hallucination prompts.", "section": "C.4 Investigation of Prompt Quality"}]