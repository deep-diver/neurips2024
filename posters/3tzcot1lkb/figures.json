[{"figure_path": "3Tzcot1LKb/figures/figures_1_1.jpg", "caption": "Figure 1: SimPO and DPO mainly differ in their reward formulation, as indicated in the shaded box. SimPO outperforms DPO significantly across a range of settings on AlpacaEval 2 and Arena-Hard.", "description": "This figure compares the performance of SimPO and DPO on AlpacaEval 2 and Arena-Hard benchmarks.  The shaded box highlights the key difference between the two methods: their reward formulation. SimPO uses the average log probability of a sequence as the implicit reward, while DPO uses the log ratio of the likelihood of a response between the current policy model and a reference model. The bar chart shows that SimPO consistently and significantly outperforms DPO across various settings, indicating the effectiveness of SimPO's simpler yet more effective reward formulation.", "section": "1 Introduction"}, {"figure_path": "3Tzcot1LKb/figures/figures_6_1.jpg", "caption": "Figure 1: SimPO and DPO mainly differ in their reward formulation, as indicated in the shaded box. SimPO outperforms DPO significantly across a range of settings on AlpacaEval 2 and Arena-Hard.", "description": "This figure compares the performance of SimPO and DPO on AlpacaEval 2 and Arena-Hard benchmarks.  The key difference highlighted is in their reward formulation.  SimPO shows a significant performance improvement over DPO across various settings. The shaded box emphasizes the core difference between the reward functions used by SimPO and DPO.", "section": "1 Introduction"}, {"figure_path": "3Tzcot1LKb/figures/figures_7_1.jpg", "caption": "Figure 3: Study of the margin \u03b3. (a) Reward accuracy and AlpacaEval2 LC win rate under different \u03b3 values. (b) Reward difference distribution under different \u03b3 values. (c) Log likelihood distribution on chosen responses under different \u03b3 values.", "description": "This figure analyzes the impact of the target reward margin (\u03b3) on SimPO's performance.  Subfigure (a) shows how reward accuracy and AlpacaEval2 LC win rate change with different \u03b3 values. Subfigure (b) displays the distribution of reward differences (r(x, yw) - r(x, y\u0131)) for various \u03b3 values, illustrating how the margin affects the separation between winning and losing responses. Subfigure (c) presents the distribution of average log probabilities of winning responses (p\u03b8(yw|x)) under different \u03b3 values, showing the effect of the margin on response likelihood.", "section": "4.3 The Impact of Target Reward Margin in SimPO"}, {"figure_path": "3Tzcot1LKb/figures/figures_8_1.jpg", "caption": "Figure 4: Comparison between SimPO and DPO, measured on UltraFeedback. (a) Spearman correlation between average log probability and response length for DPO. (b) Contingency table of rankings based on DPO rewards and the average log likelihood (measured on the training set). (c) Reward accuracy of DPO and SimPO.", "description": "This figure compares SimPO and DPO using data from UltraFeedback.  Panel (a) shows a scatter plot illustrating the Spearman correlation between the average log probability and response length for DPO, revealing a positive correlation and suggesting length bias. Panel (b) presents a contingency table contrasting DPO reward rankings against average log likelihood rankings for the training data, highlighting a significant mismatch (approximately 50%). Panel (c) displays a bar chart comparing the reward accuracy of SimPO and DPO in Mistral-Base and Mistral-Instruct settings, demonstrating SimPO's superior accuracy in aligning reward rankings with preference data.", "section": "4.4 In-Depth Analysis of DPO vs. SimPO"}, {"figure_path": "3Tzcot1LKb/figures/figures_8_2.jpg", "caption": "Figure 1: SimPO and DPO mainly differ in their reward formulation, as indicated in the shaded box. SimPO outperforms DPO significantly across a range of settings on AlpacaEval 2 and Arena-Hard.", "description": "This figure compares SimPO and DPO, highlighting their main difference in reward formulation and showing SimPO's superior performance on AlpacaEval 2 and Arena-Hard benchmarks.  The shaded box emphasizes the key difference in the reward calculation between the two methods. The bar chart visually demonstrates SimPO's consistent and significant performance improvement over DPO across different model settings.", "section": "1 Introduction"}, {"figure_path": "3Tzcot1LKb/figures/figures_18_1.jpg", "caption": "Figure 2: Effect of length normalization (LN). (a) Relationship between reward margin and length difference between winning and losing responses. (b) Spearman correlation between average log probability and response length for SimPO. (c) Spearman correlation for SimPO without LN.", "description": "This figure shows the effects of length normalization in SimPO. The first plot (a) demonstrates the relationship between the reward margin and the length difference between winning and losing responses. The second plot (b) shows the correlation between the average log probability and response length for SimPO, and the third plot (c) shows the same correlation for SimPO without length normalization.  These plots illustrate how length normalization helps mitigate bias towards longer responses.", "section": "4.2 Length Normalization (LN) Prevents Length Exploitation"}, {"figure_path": "3Tzcot1LKb/figures/figures_21_1.jpg", "caption": "Figure 1: SimPO and DPO mainly differ in their reward formulation, as indicated in the shaded box. SimPO outperforms DPO significantly across a range of settings on AlpacaEval 2 and Arena-Hard.", "description": "This figure compares SimPO and DPO, highlighting their differences in reward formulation and the resulting performance on AlpacaEval 2 and Arena-Hard benchmarks.  The shaded box shows the key difference in their reward functions: SimPO uses the average log probability of a response as the reward, while DPO uses a ratio of probabilities involving a reference model. The bar chart shows that SimPO consistently outperforms DPO across different model sizes and training conditions.", "section": "1 Introduction"}, {"figure_path": "3Tzcot1LKb/figures/figures_21_2.jpg", "caption": "Figure 1: SimPO and DPO mainly differ in their reward formulation, as indicated in the shaded box. SimPO outperforms DPO significantly across a range of settings on AlpacaEval 2 and Arena-Hard.", "description": "This figure compares the performance of SimPO and DPO on AlpacaEval 2 and Arena-Hard benchmark datasets.  The shaded box highlights the key difference between the two methods: their reward formulation.  SimPO uses the average log probability of a sequence as the reward, while DPO uses a more complex reward formulation based on the log ratio of likelihoods from the current policy and a reference model.  The bar charts show that SimPO consistently and significantly outperforms DPO across various settings.  The y-axis indicates the win rate increase and the x-axis shows different model sizes and training methods.", "section": "1 Introduction"}, {"figure_path": "3Tzcot1LKb/figures/figures_21_3.jpg", "caption": "Figure 1: SimPO and DPO mainly differ in their reward formulation, as indicated in the shaded box. SimPO outperforms DPO significantly across a range of settings on AlpacaEval 2 and Arena-Hard.", "description": "The figure compares the performance of SimPO and DPO on AlpacaEval 2 and Arena-Hard benchmarks.  It highlights that the key difference between SimPO and DPO lies in their reward formulation.  SimPO uses an average log probability as its implicit reward, while DPO uses a log ratio of likelihoods from the current policy and reference model. The bar chart visually demonstrates SimPO's superior performance across various settings, showing a significant improvement in both win rates.", "section": "1 Introduction"}]