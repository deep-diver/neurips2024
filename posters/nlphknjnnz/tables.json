[{"figure_path": "NlpHKNjNNZ/tables/tables_6_1.jpg", "caption": "Table 1: Detection performance comparison on nuScenes val set in terms of AP, mAP, and NDS. Based on CV-Voxel [5], we compare different placement methods, such as random [1], ground-based [12], and our placement. We also report the effect of using our pseudo-LiDAR samples.", "description": "This table presents a comparison of object detection performance on the nuScenes validation set using different data augmentation and object placement techniques.  It compares the performance of the baseline method (CV-Voxel) against three different augmentation strategies: GT-Aug (random placement), Real-Aug (ground-based placement), and the proposed method (ground+map-based placement).  Each strategy is evaluated both with and without the addition of pseudo-LiDAR samples. The results are presented in terms of Average Precision (AP), mean Average Precision (mAP), and NuScenes Detection Score (NDS) across ten object classes.", "section": "4 Experiments"}, {"figure_path": "NlpHKNjNNZ/tables/tables_6_2.jpg", "caption": "Table 1: Detection performance comparison on nuScenes val set in terms of AP, mAP, and NDS. Based on CV-Voxel [5], we compare different placement methods, such as random [1], ground-based [12], and our placement. We also report the effect of using our pseudo-LiDAR samples.", "description": "This table presents a comparison of object detection performance on the nuScenes validation set using different data augmentation and object placement strategies.  The results are measured using Average Precision (AP), mean Average Precision (mAP), and NuScenes Detection Score (NDS). The table compares the performance of the baseline CV-Voxel model against modifications including random object placement, ground-based placement, and the proposed ground+map-based placement. It also shows the impact of incorporating pseudo-LiDAR samples generated by the proposed method.", "section": "4 Experiments"}, {"figure_path": "NlpHKNjNNZ/tables/tables_7_1.jpg", "caption": "Table 1: Detection performance comparison on nuScenes val set in terms of AP, mAP, and NDS. Based on CV-Voxel [5], we compare different placement methods, such as random [1], ground-based [12], and our placement. We also report the effect of using our pseudo-LiDAR samples.", "description": "This table presents a comparison of object detection performance on the nuScenes validation set using different data augmentation and object placement techniques.  It compares the performance of a baseline model (CV-Voxel) with various augmentation methods (random placement, ground-based placement, and the proposed Ground+Map-based placement), both with and without the addition of pseudo-LiDAR samples generated by the proposed method.  The metrics used for comparison are Average Precision (AP), mean Average Precision (mAP), and NuScenes Detection Score (NDS).  The results highlight the effectiveness of the proposed method in improving detection performance, particularly for minority classes.", "section": "4 Experiments"}, {"figure_path": "NlpHKNjNNZ/tables/tables_7_2.jpg", "caption": "Table 4: Quality of Pseudo LiDAR Point Clouds. FID scores (squared Wasserstein distance between given samples and nuScenes samples, thus lower is better) comparison between variants of our models and public LiDAR datasets, Lyft [17] and A2D2 [18]. Abbr. G.S: Gaussian Splatting", "description": "This table presents the FID (Fr\u00e9chet Inception Distance) scores, comparing the quality of generated pseudo-LiDAR point clouds against real LiDAR data from the nuScenes dataset, as well as public Lyft and A2D2 datasets.  Lower FID scores indicate higher similarity to real data. The table analyzes the impact of different factors on the quality of the generated point clouds, such as the type of volumetric 3D rendering method used (Plenoxels or Gaussian Splatting), azimuth resolution, inclusion of RGB features, and the use of group intensity loss.  Results are broken down by object class (Bus, Construction Vehicle, etc.) and overall metrics (average FID, mAP, and NDS).", "section": "Ablation Studies"}, {"figure_path": "NlpHKNjNNZ/tables/tables_8_1.jpg", "caption": "Table 5: Mixing ratio between GT and PGT objects.", "description": "This table presents the results of an experiment to determine the optimal mixing ratio between ground truth (GT) objects and pseudo ground truth (PGT) objects in a 3D object detection task.  Different ratios of GT:PGT objects were tested (0:1, 1:3, 1:1, 3:1, 1:0), and the resulting mean Average Precision (mAP) and NuScenes Detection Score (NDS) are shown. The best performance is observed with a 1:1 ratio of GT:PGT objects.", "section": "Experiments"}, {"figure_path": "NlpHKNjNNZ/tables/tables_8_2.jpg", "caption": "Table 6: Continuously increasing pseudo-LiDAR data.", "description": "This table shows the mean Average Precision (mAP) and NuScenes Detection Score (NDS) on the nuScenes validation set when using different sizes of the pseudo-LiDAR object bank. The results demonstrate that continuously increasing the size of the pseudo-LiDAR object bank leads to improved performance.", "section": "4 Experiments"}, {"figure_path": "NlpHKNjNNZ/tables/tables_8_3.jpg", "caption": "Table 3: Comparisons between baseline and PGT-Aug for individual models on nuScenes val set. Abbr. C.V: Construction Vehicle, Ped: Pedestrian, T.C: Traffic Cone, M.C: Motorcycle, B.C: Bicycle. \u2020: our reproduction.", "description": "This table compares the performance of different 3D object detection models on the nuScenes validation set, with and without the proposed PGT-Aug data augmentation method.  It shows the mean Average Precision (mAP) and NDS scores for both majority and minority classes, across several base models. The results highlight the improvement achieved by integrating PGT-Aug, especially for the minority classes, showing its effectiveness in mitigating the class imbalance problem.", "section": "4 Experiments"}, {"figure_path": "NlpHKNjNNZ/tables/tables_9_1.jpg", "caption": "Table 8: PGT Performance on KITTI val set in terms of AP and mAP.", "description": "This table presents the performance of the PGT-Aug method on the KITTI validation set.  It shows Average Precision (AP) and mean Average Precision (mAP) scores for different object classes (cyclist, car, pedestrian) and difficulty levels (easy, moderate, hard). The results demonstrate the effectiveness of PGT-Aug in improving object detection accuracy, particularly for challenging cases.", "section": "4 Experiments"}, {"figure_path": "NlpHKNjNNZ/tables/tables_9_2.jpg", "caption": "Table 9: PGT Performance on Lyft val set. Abbr. E.V: Emergency Vehicle, O.V: Other Vehicle, M.C: Motorcycle, B.C: Bicycle. Ped: Pedestrian", "description": "This table presents the performance comparison of the proposed PGT-Aug method against the baseline (GT-Aug) on the Lyft dataset's validation set.  The results are broken down by object class (Truck, Bus, Other Vehicle, Motorcycle, Bicycle, Car, Pedestrian) and show the Average Precision (AP) for each class as well as the mean Average Precision (mAP) across all classes.  The table highlights the improvement achieved by PGT-Aug, especially for minority classes.", "section": "4 Experiments"}, {"figure_path": "NlpHKNjNNZ/tables/tables_16_1.jpg", "caption": "Table 10: Statistics for our data collection. (i) Videos capturing surround view of miniatures and (ii) publicly available videos of given minor-class objects. Abbr. C.V: Construction Vehicle, M.C: Motorcycle, B.C: Bicycle", "description": "This table shows the number of videos collected for each minority class from two sources: miniature videos and publicly available videos.  The miniature videos were collected by filming various miniature vehicles (trucks, trailers, buses, motorcycles, and construction vehicles), while the public videos were sourced from YouTube and other online video platforms. The table lists the number of videos collected for each object class from each source.", "section": "A.1 Dataset Details"}, {"figure_path": "NlpHKNjNNZ/tables/tables_21_1.jpg", "caption": "Table 11: Categories agreement among datasets for FID evaluation and 3D object detection.", "description": "This table shows the mapping between object categories across different datasets used for FID (Fr\u00e9chet Inception Distance) evaluation and 3D object detection.  It ensures that consistent class comparisons can be made between the nuScenes, Lyft, and A2D2 datasets when evaluating the quality of synthetically generated objects.", "section": "A.1 Dataset Details"}, {"figure_path": "NlpHKNjNNZ/tables/tables_22_1.jpg", "caption": "Table 12: Average processing time (per instance, in msec) and memory usage (MB) C.V: Construction Vehicle, Ped: Pedestrian, T.C: Traffic Cone, M.C: Motorcycle, B.C: Bicycle.", "description": "This table shows the average processing time and memory usage for each step in the pseudo LiDAR generation pipeline for different minority classes in the nuScenes dataset.  The steps include intensity estimation, view-dependent point sampling, and rigid body motion.  The memory usage is also provided for each class.", "section": "A.2 Implementation Details"}, {"figure_path": "NlpHKNjNNZ/tables/tables_23_1.jpg", "caption": "Table 13: FID score evaluation between SH coefficient 0 and ours. Abbr. C.V: Construction Vehicle, Ped: Pedestrian, T.C: Traffic Cone, M.C: Motorcycle, B.C: Bicycle.", "description": "This table presents a comparison of FID (Fr\u00e9chet Inception Distance) scores between two versions of the pseudo-LiDAR point cloud generation method.  One version uses only the first spherical harmonic coefficient (SH coefficient 0), while the other uses the authors' full approach. Lower FID scores indicate better similarity to real LiDAR data. The results show that the authors' method produces point clouds with significantly lower FID scores, suggesting that they are more realistic and closer to real-world LiDAR data than those generated using only the first SH coefficient.", "section": "A.1 Dataset Details"}, {"figure_path": "NlpHKNjNNZ/tables/tables_23_2.jpg", "caption": "Table 14: Ablation study in intensity, the number of images, data alignment on nuScenes val set. Abbr. C.V: Construction Vehicle, Ped: Pedestrian, T.C: Traffic Cone, M.C: Motorcycle, B.C: Bicycle. \u2020: our reproduction.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different components of the proposed PGT-Aug framework on the performance of 3D object detection.  It shows the effects of removing or modifying key aspects such as intensity generation, sampling methods, motion modeling, and the number of input images used in the 3D reconstruction process.  The results are presented in terms of average precision (AP) and NuScenes Detection Score (NDS) for both majority and minority classes, demonstrating the contribution of each component to the overall performance.", "section": "5 Ablation Studies"}, {"figure_path": "NlpHKNjNNZ/tables/tables_24_1.jpg", "caption": "Table 15: Performance comparison with other data augmentation approaches. \u2020: our reproduction", "description": "This table compares the performance of PGT-Aug with other data augmentation methods on the KITTI Car benchmark, using PointPillars as the baseline model.  It shows the Average Precision (AP) for the \"car\" class, broken down by difficulty level (Easy, Moderate, Hard), demonstrating the effectiveness of PGT-Aug in improving performance across all difficulty levels.", "section": "B.2 Effect of PGT-Aug on the majority of classes"}, {"figure_path": "NlpHKNjNNZ/tables/tables_25_1.jpg", "caption": "Table 16: Detection performance comparison for additional modules on nuScenes val set in terms of AP, mAP, and NDS. We use CP-Voxel [5] and Transfusion-L [55] as a baseline model.", "description": "This table presents the ablation study results on the nuScenes validation set using two baseline models, CP-Voxel and Transfusion-L. It shows the effect of adding several modules to the baseline model, including PGT-Aug, ray tracing, distance filtering, and Thompson sampling. Each module's impact on the mean Average Precision (mAP) and the NuScenes Detection Score (NDS) is evaluated and presented, providing insights into the effectiveness of each component and their combined impact on the overall performance.", "section": "4 Experiments"}]