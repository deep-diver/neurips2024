[{"type": "text", "text": "DeBaRA: Denoising-Based 3D Room Arrangement Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "L\u00e9opold Maillard1,2 Nicolas Sereyjol-Garros\u2217 Tom Durand2 Maks Ovsjanikov1 ", "page_idx": 0}, {"type": "text", "text": "1LIX, \u00c9cole Polytechnique, IP Paris 2Dassault Syst\u00e8mes {maillard,maks}@lix.polytechnique.fr {firsname.lastname}@3ds.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generating realistic and diverse layouts of furnished indoor 3D scenes unlocks multiple interactive applications impacting a wide range of industries. The inherent complexity of object interactions, the limited amount of available data and the requirement to fulflil spatial constraints all make generative modeling for 3D scene synthesis and arrangement challenging. Current methods address these challenges autoregressively or by using off-the-shelf diffusion objectives by simultaneously predicting all attributes without 3D reasoning considerations. In this paper, we introduce DeBaRA, a score-based model specifically tailored for precise, controllable and flexible arrangement generation in a bounded environment. We argue that the most critical component of a scene synthesis system is to accurately establish the size and position of various objects within a restricted area. Based on this insight, we propose a lightweight conditional score-based model designed with 3D spatial awareness at its core. We demonstrate that by focusing on spatial attributes of objects, a single trained DeBaRA model can be leveraged at test time to perform several downstream applications such as scene synthesis, completion and re-arrangement. Further, we introduce a novel Self Score Evaluation procedure so it can be optimally employed alongside external LLM models. We evaluate our approach through extensive experiments and demonstrate significant improvement upon state-of-the-art approaches in a range of scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Systems capable of generating realistic environments comprising multiple interacting objects would impact several industries including video games, robotics, augmented and virtual reality (AR/VR) and computer-aided interior design. As a result and in tandem with the growing availability of synthetic datasets of indoor layouts [10, 44, 42, 63, 7], which can be populated with high-quality 3D assets [11, 63, 1], data-driven approaches for automatically generating and arranging 3D scenes have been actively investigated by the computer vision community. Notably, the ongoing success of deep generative models for controllable content creation in the text and image domains has recently been extended to scene synthesis, allowing users to craft realistic indoor environments from a set of multimodal constraints [38, 37, 55, 54, 27, 35]. ", "page_idx": 0}, {"type": "text", "text": "Challenges associated with 3D indoor scene generation are numerous as the intricate nature of multiobject interactions is difficult to capture and model precisely. Items should be placed, potentially resized and oriented relative to one another, in a way that is both plausible and aligned with subjective and context-dependent priors such as style, as well as ergonomic and functional preferences. Additionally, objects should fit within a bounded, restricted area, and a subtle mismatch can break the perceived validity of the synthesized environment (e.g., overlapping, floating or out-of-bounds objects, inaccessible areas). Finally, the limited availability of high-quality data [10, 42] requires learning-based approaches to make careful design choices and trade-offs. ", "page_idx": 0}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/f1812f7a9d323ae1d7742f9fbdde38a63d0c5c715f8baaf5533e91ba011a9f2a.jpg", "img_caption": ["Figure 1: Application scenarios overview. Besides generating diverse and realistic 3D indoor layouts, a single trained DeBaRA model can be employed to execute several related tasks by tweaking the initial sampling noise level $\\sigma_{\\mathrm{max}}$ and/or performing object or attribute-level layout inpainting. Our novel SSE procedure enables 3D Scene Synthesis capabilities by efficiently selecting conditioning semantics from external sources using density estimates provided by the pretrained model. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Early data-driven approaches often rely on intermediate hand-crafted representations [43, 54, 36, 62] that are closely related to the considered dataset, which introduces significant biases. Concurrently, popular methods have been adopting autoregressive architectures that treat scene synthesis as a set generation task [55, 38, 27, 21, 37] by sequentially adding individual objects. More recently, score-based generative models (also known as denoising diffusion models) have shown promising capabilities in various 3D scene understanding applications [18, 58] including controllable scene synthesis [51, 62, 60] and re-arrangement [57]. In contrast to previous methods, denoising-based approaches enable a stable and scalable training phase and can output all scene attributes simultaneously. The iterative sampling framework brings an improved consideration for the conditioning information and an attractive balance between generation quality and variety. However, current methods leveraging score-based generative models try to model all attributes (both categorical and spatial) within a single framework, which, as we demonstrate below, is less data-efficient and leads to suboptimal solutions. ", "page_idx": 1}, {"type": "text", "text": "In this context, our work aims to establish principled and robust capabilities for generating accurate and diverse 3D layouts. Specifically, our key contributions are threefold: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a score-based conditional objective and architecture designed to effectively learn spatial attributes of interacting 3D objects in a constrained indoor environment. In contrast to previous approaches [51, 38], we disentangle the design space and reduce the model\u2019s prediction to a minimal representation consisting solely of oriented 3D bounding boxes, taking as conditioning input the room\u2019s floor plan and list of object semantic categories.   \n2. We propose a set of approaches which allows a model trained following our method to be flexibly employed at test time to perform several user-driven tasks enabling object or attribute-level control. In particular, we demonstrate strong capabilities on controllable scenarios such as scene re-arrangement or room completion, from a single trained network.   \n3. Finally, we introduce a novel Self Score Evaluation (SSE) procedure, which enables 3D scene synthesis by selecting the set of inputs provided by external sources, such as a LLM, that lead to the most realistic layouts. ", "page_idx": 1}, {"type": "text", "text": "We exhibit our model\u2019s capabilities across a wide range of experimental scenarios and report state-ofthe art 3D layout generation and scene synthesis performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Score-based Generative Models By smoothly perturbing training examples with noise, Diffusion Models map a complex data distribution to a known Gaussian prior from which they sample back via iterative denoising using a neural network trained over multiple noise levels. This family of generative models has been motivated by several theoretical foundations over the past years: DDPMs [16, 33] parameterize the diffusion process as a discrete-time Markov chain, as opposed to continuous-time approaches [49, 48]. The seminal EDM [19, 20] training and sampling settings later unified previous methods into an improved ideal framework defined by a set of interpretable parameters. Originally motivated by image generation, diffusion models have demonstrated impressive capabilities on various conditional tasks such as text-to-image synthesis [34, 45], image-to-image generation from various 2D input modalities [45, 64, 56], text-to-3D asset creation [40, 23] or environment-aware human motion synthesis [18, 24]. Relevant to our work, diffusion models have been applied to the generation of point clouds [31, 52] and other geometric representations involving 3D coordinates [59]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Lifting Pretrained Diffusion Models Knowledge of trained diffusion models can be leveraged in various settings including content inpainting[30, 18], score distillation [40], exact likelihood computation [49, 19] or teacher-student distillation [47, 32]. More relevant to our work, imagedomain diffusion priors have demonstrated compelling performance in discriminative tasks including zero-shot image classification [22, 6, 5] and segmentation [4]. More precisely, Diffusion Classifiers assign a label, from a finite set of possible classes $\\{c_{i}\\}_{i=1}^{N}$ to an observed sample $\\pmb{x}_{0}$ by computing class-conditional density estimates from a pretrained diffusion model under the assumption of a uniform prior $p\\left({{c}_{i}}\\right)=1/N$ . In practice, this is done by, for each class, iteratively adding noise to the observed sample $\\pmb{x}_{0}$ and computing a Monte Carlo estimate of the expected reconstruction loss using the class-conditioned model. ", "page_idx": 2}, {"type": "text", "text": "Controllable 3D Scene Synthesis Synthesizing indoor 3D layouts from a partial set of information or constraints has come in various settings depending on provided vs. predicted entities and enabled control granularity. A prolific line of research has been adopting intermediate 3D scene representations such as graphs [25, 43, 54, 36, 62, 13, 26], furniture matrices [65] or multi-view images [35]. Autoregressive furnishing approaches [55, 38] have been supplemented by object attribute-level conditioning [37, 27] and additional ergonomic constraints [21]. However, their one object at a time strategy does not comprehensively capture complex relationships between all the interacting elements and is known to easily fall into local minima in which new items fail to be accurately inserted to the current configuration. Lately, methods have unfolded LLMs double-edged capabilities in this area [9, 61] as they excel at generating sensible furniture descriptions while struggling in accurately arranging them in the 3D space, which [12] addresses by introducing a costly refinement stage. In the light of that, LLMs appear to be ideal candidates to supplement a specialized 3D layout generation model. ", "page_idx": 2}, {"type": "text", "text": "Denoising Indoor Scenes Previous methods have explored diffusion-based approaches in the context of 3D scene synthesis. Pioneering their usage, LEGO-Net [57] performs scene re-arrangement (i.e., recovering a clean object layout from a noisy one) in the 2D space using a transformer backbone that is not noise-conditioned, which we argue is the root cause of its main limitations. PhyScene[60] augment diffusion-based 3D scene synthesis with additional physic-based guidance to enable practical embodied agent applications. Most relevant to our work, DiffuScene [51] achieves 3D scene synthesis by fitting a DDPM [16] on stacked 3D object features, resulting in a high-dimensional composite distribution that is hard to learn and interpret. It does not enforce spatial configurations over other predicted features. More importantly, its generative process is not conditioned on the room\u2019s floor plan (i.e., bounds) that constrains objects to be placed within a restricted area. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 3D Scene Representation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our method is based on encoding the state of a 3D indoor scene $\\boldsymbol{S}$ that is defined by a floor plan (i.e., bounds) $\\mathcal{F}$ and an unordered set of $N$ objects $\\mathcal{O}=\\{o_{1},\\ldots,o_{N}\\}$ , each being modeled by its typed 3D bounding box $o_{i}=\\{\\pmb{x}_{i},\\pmb{c}_{i}\\}$ where $\\boldsymbol{c}_{i}\\in\\{0,1\\}^{k}$ is the one-hot encoding of the semantic category among $k$ classes and $\\pmb{x}_{i}\\,=\\,(\\pmb{p}_{i},\\pmb{r}_{i},\\pmb{d}_{i})\\,\\in\\,\\mathbb{R}^{8}$ comprises 3D spatial attributes. More specifically, $\\pmb{p}_{i}\\in\\bar{\\mathbb{R}}^{3}$ denotes the object\u2019s center coordinate position, $\\pmb{r}_{i}=(\\cos\\theta_{i},\\sin\\theta_{i})\\in\\mathbb{R}^{2}$ is a continuous encoding of the rotation of angle $\\theta_{i}$ around the scene\u2019s vertical axis [66] and $\\pmb{d}_{i}\\in\\mathbb{R}^{3}$ is the dimension. ", "page_idx": 2}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/6b014975125128c6557496c14bb7de5c45099f72f08394f9df9f5c81451bbcb5.jpg", "img_caption": ["Figure 2: DeBaRA architecture and training overview. At each iteration, 3D bounding boxes parameters $(p,r,d)$ of indoor scene\u2019s objects $\\scriptscriptstyle\\mathcal{O}$ are perturbed with Gaussian noise $\\sigma\\epsilon$ . The floor plan $\\mathcal{F}$ , noise level $\\sigma$ and resulting objects $O_{\\sigma}$ are processed by respective encoders to form an unordered set of representations $\\tau$ fed as input to a transformer encoder. Novel object embeddings $\\hat{\\mathcal{T}}_{o}$ are finally decoded back to their predicted clean spatial configuration $(\\hat{\\boldsymbol p},\\hat{\\boldsymbol r},\\hat{\\boldsymbol d})$ . Trainable modules are optimized by minimizing a semantic-aware Chamfer loss. Input object categories $^c$ are randomly dropped to model both the class-conditional and unconditional 3D layout distributions. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Diffusion Framework and Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We describe in this section our score-based layout generation framework, relevant design choices and network architecture, that are summarized in Figure 2. Remarkably, unlike previous approaches [51, 38, 37] that output a range of attributes lying in different spaces, we focus on accurately modeling 3D spatial layouts of bounded indoor scenes from a set of input object categories. ", "page_idx": 3}, {"type": "text", "text": "Learning 3D spatial configurations from object semantics We adopt a diffusion-based approach to yield a conditional generation model that outputs 3D object spatial features $\\{{\\pmb x}_{i}\\}_{i=1}^{N}$ from an input floor plan and set of semantic categories $\\pmb{y}\\,=\\,(\\mathcal{F},\\pmb{c})$ with $\\pmb{c}=\\{\\pmb{c}_{i}\\}_{i=1}^{N}$ . During training, 3D spatial attributes are perturbed with Gaussian noise $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ at various noise levels (i.e., magnitudes) $\\sigma$ . A trainable noise-conditioned denoiser model $D_{\\theta}(x_{\\sigma};y,\\sigma)$ maps noisy spatial attributes ${\\pmb x}_{\\sigma}={\\pmb x}+\\sigma{\\pmb\\epsilon}$ to their clean counterparts $\\hat{\\pmb{x}}\\approx\\pmb{x}\\in\\mathbb{R}^{N\\times8}$ . ", "page_idx": 3}, {"type": "text", "text": "We notice that each object spatial attribute has an individual real-world interpretation (e.g, $\\pmb{p}$ and $^d$ can be expressed in meters, $\\pmb{r}$ in degrees). To preserve their measurable nature at intermediate perturbed configurations $\\scriptstyle{x_{\\sigma}}$ , we want our diffusion parameterization to support a continuous range of noise levels, correlated to the scale of the input signal. This will be particularly convenient at test time (see Section 3.5). To guarantee both properties, we adapt the score-based EDM [19] framework. In our context, this formulation is more natural than the DDPM framework employed by previous work [51]. The latter is based on discretizing noise levels and does not offer a straightforward interpretability of the scene\u2019s state at arbitrary timesteps. Our parameterization is further detailed in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Estimating the unconditional layout density Inspired by classifier-free guidance [17] in the image domain, we model both the class-conditional density $p_{\\theta}\\bar{(}\\mathbf{x}|\\mathcal{F},c\\bar{)}$ and the unconditional density $p_{\\theta}(\\mathbf{\\boldsymbol{x}}|\\mathcal{F},\\emptyset)$ by a single network of parameters $\\theta$ . At each training iteration, we perform conditioning dropout on the set of semantic categories, by setting $\\boldsymbol c=\\emptyset$ with probability $p_{\\mathrm{drop}}$ else $\\{c_{i}\\}_{i=1}^{N}$ . We found that this mechanism helps to reduce overfitting of the training layouts $\\bar{p_{\\mathrm{data}}}(x)$ and enables novel capabilities that we introduce in Section 3.4. ", "page_idx": 3}, {"type": "text", "text": "Denoiser Network Architecture Our lightweight architecture is inspired by previous work [57] to which we make key changes. Similar to [38] and [51], we use a shared object encoder in order to obtain per-object token $\\mathcal{T}_{o_{i}}$ as a concatenation of the object $o_{i}$ attributes embedded by sinusoidal positional encoding and linear layers. Following [57], we uniformly sample $P$ points on the edges of the floor plan and feed them into a PointNet [41] model, resulting in a floor token $\\mathcal{T}_{\\mathcal{F}}$ . This choice of feature extractor backbone is natural as it allows to maintain all the input scene\u2019s spatial features in a common 3D space. Importantly, a noise token $\\mathcal{T}_{\\sigma}$ is computed from the current noise level $\\sigma$ , making our architecture noise-aware, i.e., able to denoise layouts $\\scriptstyle{x_{\\sigma}}$ at any perturbation magnitude. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "All the previously encoded tokens form a sequence $\\mathcal{T}=\\{\\mathcal{T}_{\\mathcal{F}},\\mathcal{T}_{\\sigma},\\mathcal{T}_{o_{i}},\\ldots,\\mathcal{T}_{o_{N}}\\}$ from which a global scene encoder $T_{\\theta}$ computes rich representations $\\hat{\\mathcal{T}}$ . We design the method without any token ordering and use padding mask for scene with fewer objects than the transformer capabilities. A final shared decoder MLP takes as input object tokens $\\{\\hat{T}_{o_{i}}\\}_{i=1}^{N}$ and returns denoised spatial attribute values $\\pmb{\\hat{x}}=\\{(\\hat{\\pmb{p}}_{i},\\hat{\\pmb{r}}_{i},\\hat{\\pmb{d}}_{i})\\}_{i=1}^{N}$ . We provide implementation details on the denoiser in Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "3.3 3D Spatial Objective ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our noise-conditioned model $D_{\\theta}$ is optimized towards a novel semantic-aware Chamfer Distance objective that does not penalize permutation of 3D bounding boxes sharing the same semantic category between the predicted scene objects layout $\\hat{O}$ and the ground truth one $\\scriptscriptstyle\\mathcal{O}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C D}(\\hat{\\mathcal{O}},\\mathcal{O})=\\frac{1}{2N}\\left(\\sum_{\\hat{\\alpha}\\in\\hat{\\mathcal{O}}}\\operatorname*{min}_{o\\in\\mathcal{O}}l(\\hat{o},o)+\\sum_{o\\in\\mathcal{O}}\\operatorname*{min}_{\\hat{\\alpha}\\in\\hat{\\mathcal{O}}}l(\\hat{o},o)\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\kappa$ is a large value so that a significant penalty is applied to objects that do not share the same semantic category $^c$ , preventing them to be returned by the min operator. ", "page_idx": 4}, {"type": "text", "text": "We can finally rewrite the usual score-based training objective [49, 19] as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{\\mathrm{data}}(\\boldsymbol{x}),\\epsilon,\\sigma}\\big[\\lambda(\\sigma)\\mathcal{L}_{C D}\\big(D_{\\theta}(\\boldsymbol{x}+\\sigma\\boldsymbol{\\epsilon};\\boldsymbol{y},\\sigma),\\boldsymbol{x}\\big)\\big]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda(\\sigma)$ is a noise-dependent loss weighting function. ", "page_idx": 4}, {"type": "text", "text": "3.4 Self Score Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While specifying complete conditioning information such as the set of object semantics $^c$ could be tedious, it can be provided by either a LLM or a separately trained sequence generation model. However, using independent models is inherently suboptimal since it does not guarantee that the generated conditioning input will be aligned with the score model knowledge. As a result, we propose a novel method to select conditioning inputs that are attuned with the model\u2019s capabilities. ", "page_idx": 4}, {"type": "text", "text": "More specifically, we evaluate a finite set of $C$ object semantic categories candidates, where each candidate is associated to a 3D spatial layout sampled from the learned conditional density, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{candidates}=\\left\\{\\left(c_{j},\\pmb{x}_{j}\\sim p_{\\theta}\\left(\\pmb{x}|\\mathcal{F},\\pmb{c}_{j}\\right)\\right)\\right\\}_{j=1}^{C}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, the optimal conditioning candidate $c^{*}$ is derived from a density estimate of its corresponding 3D spatial layout $x^{*}$ provided by the unconditional network: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{x}^{*}=\\arg\\operatorname*{min}_{\\pmb{x}_{i}}\\mathbb{E}_{\\epsilon,\\sigma}\\big[\\mathcal{L}_{C D}\\{D_{\\theta}(\\pmb{x}_{i}+\\sigma\\epsilon\\,;\\,\\mathcal{F},\\emptyset,\\sigma),\\pmb{x}_{i}\\}\\big]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Self Score Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Require: a diffusion prior $D_{\\theta}$ trained with conditioning dropout and by optimizing $\\mathcal{L}_{C D}$   \nInput: conditioning candidates $\\{c_{j}\\}_{j=1}^{C}$ , number of score evaluation trials $T_{\\mathrm{sse}}$   \n1: sample $\\pmb{x}_{j}\\sim p_{\\theta}(\\pmb{x}|\\mathcal{F},\\pmb{c}_{j})$ for each candidate $c_{j}$ using iterative sampling   \n2: initialize scores $[{\\pmb{c}}_{j}]=\\mathbf{\\bar{1}}\\mathbf{i}\\mathbf{s}\\mathbf{t}()$ for each $c_{j}$   \n3: for trial $t=1,\\ldots,T_{\\mathrm{sse}}$ do   \n4: sample $\\sigma\\sim\\mathcal{N}(0,\\sigma_{s});\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$   \n5: for candidate $c_{k}$ , sample $\\pmb{x}_{k}$ do   \n6: $\\begin{array}{r}{\\mathsf{s c o r e s}[c_{k}].\\mathsf{a p p e n d}\\!\\left(\\mathcal{L}_{C D}[D_{\\theta}(\\pmb{x}_{k}+\\sigma\\epsilon,;\\,\\mathcal{F},\\emptyset,\\sigma),\\pmb{x}_{k}]\\right)}\\end{array}$   \n7: end for   \n8: end for   \n9: return arg $\\operatorname*{min}_{c_{j}}$ mean(scores[cj]) ", "page_idx": 4}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/8d345087003a756d0c219ef9e0972882c8127c6bf5375a37571604c08a2abc06.jpg", "img_caption": ["Figure 3: We compare our method with established baselines for generating a 3D layout from a floor plan and set of object categories. DeBaRA produces less failure cases while consistently generating regular arrangements within the room\u2019s bounds. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In practice, we compute an unbiased Monte Carlo estimate of each candidate expectation using $T_{\\mathrm{sse}}$ fixed $(\\sigma,\\epsilon)$ pairs. Although similar in some aspects, SSE fundamentally differs from diffusion classifiers [22] as in our case, the uniform assumption over conditioning probabilities does not hold. Indeed, in our setting some input signals cannot lead to a plausible arrangement at all. As a result, density estimates of observed samples generated by the class-conditioned model are computed using the unconditional one, while diffusion classifiers compute density estimates of a single observed sample using the class-conditioned model. The SSE procedure is detailed in Algorithm 1. It is further illustrated and discussed in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "3.5 Application Scenarios ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As shown in Figure 1, a single trained DeBaRA model can be used at test time to perform multiple downstream interactive applications. Usual generation procedures, such as EDM $2^{\\mathrm{nd}}$ order stochastic sampler [19] can be applied using our trained denoiser to generate novel 3D layouts via $T$ -step iterative denoising at discretized noise levels $\\sigma_{0}=\\sigma_{\\mathrm{max}}>...>\\sigma_{T}=0$ . ", "page_idx": 5}, {"type": "text", "text": "In particular, several applications can be performed by inpainting [30], i.e., predicting missing spatial features from those specified (i.e., fixed) in the input layout $\\bar{\\mathbf{x}_{\\mathrm{~\\!~\\!~\\!~}}}$ . To do so, we introduce a binary mask $\\mathbf{m}\\in\\{0,\\^{\\mathbf{\\bar{1}}}\\}^{N\\times8}$ specifying values to retain from the input. The predicted layout at any sampling iteration $t$ can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{{x}}}_{\\sigma_{t}}=\\hat{\\mathbf{{x}}}_{\\sigma_{t}}\\odot\\left(1-\\mathbf{m}\\right)+\\mathbf{{x}}_{\\sigma_{t}}\\odot\\mathbf{m}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3D Layout Generation Novel and diverse 3D layouts can be generated from an input set of semantic categories $^c$ and a floor plan $\\mathcal{F}$ by sampling from a high initial noise level $\\sigma_{\\mathrm{max}}>>\\sigma_{\\mathrm{data}}$ , arbitrarily initialized 3D spatial features $\\mathbf{\\boldsymbol{x}}_{\\sigma_{0}}$ and with ${\\bf m}={\\bf0}_{N\\times8}$ . ", "page_idx": 5}, {"type": "text", "text": "3D Scene Synthesis DeBaRA can perform 3D scene synthesis via 3D layout generation from semantic categories provided by external sources such as a LLM [9]. Input conditioning candidates can further be optimally selected using the Self Score Evaluation procedure. ", "page_idx": 5}, {"type": "text", "text": "Scene Completion Additional objects $o_{a}$ can be inserted to an existing scene partially furnished with $k$ objects $O_{e}$ . To do so, their 3D spatial attributes $\\pmb{x}_{a}$ are inpainted from the existing ones $\\pmb{x}_{e}$ with $D_{\\theta}$ conditioned on the updated set of semantic categories $\\pmb{c}=\\pmb{c}_{e}\\|\\pmb{c}_{a}$ using $\\mathbf{m}(i,j)=\\mathbf{1}_{\\{i\\leq k\\}}$ . ", "page_idx": 5}, {"type": "text", "text": "Re-arrangement In the context of scene synthesis, re-arrangement [57] consists in recovering the closest clean spatial configuration of existing objects from a messy one, which has practical applications in robotics [2]. DeBaRA can perform re-arrangement by sampling from an initial noise $\\sigma_{\\mathrm{max}}$ that depends on the scene perturbation magnitude. During denoising, object positions and rotations $(\\dot{\\boldsymbol{p}_{\\u}},\\dot{\\boldsymbol{r}})\\in\\mathbb{R}^{N\\times5}$ are inpainted from the known object dimensions using $\\bar{\\mathbf{m}}(i,\\bar{j})=\\mathbf{1}_{\\{j>5\\}}$ . ", "page_idx": 5}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/a606be2cd63d9b5d28af7ad12a7c673cef910abbd541adf0dee9c7c1be88b67d.jpg", "img_caption": ["Figure 4: Qualitative results on scene re-arrangement (left) and completion (right). DeBaRA is able to recover a plausible layout from a messy one, and to finely take into account initial configurations. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Optimal Object Retrieval 3D scene synthesis systems depend on external 3D asset databases for furnishing rooms. For each object of semantic class $^c$ , a textured furniture is retrieved by minimizing the mismatch with the generated dimension $d_{\\sigma_{T}}$ . This is inherently suboptimal as the resulting scene quality is limited by the size of the external database. To overcome this issue, we introduce a post-retrieval refinement stage by performing additional re-arrangement steps starting from a noise level $\\sigma_{\\mathrm{max}}$ derived from the mismatch between generated and retrieved object dimensions. ", "page_idx": 6}, {"type": "text", "text": "Generation from Coarse Specifications We propose a time-dependent masking approach to synthesize layouts from rough input spatial features (i.e., instead of exact ones), that are adjusted in the late denoising iterations. To indicate approximate e.g., object dimensions, we set, at any sampling step $t$ , $\\mathbf{m}(i,j)=\\mathbf{1}_{\\{j>5\\}}$ and $t{<}T_{s}\\}$ . The denoising step $T_{s}$ from which $\\mathbf{m}$ is relaxed (i.e., set to 0) can be derived from its corresponding noise level $\\sigma_{T_{s}}$ and the precision of specified input features. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide a comprehensive experimental evaluation of DeBaRA that we compare with established baselines from different model families. We also demonstrate the capabilities of our approach in various practical scenarios, enabling a wide range of applications. ", "page_idx": 6}, {"type": "text", "text": "Datasets Our experiments are conducted on the 3D-FRONT [10] synthetic indoor layouts, furnished with assets from 3D-FUTURE [11] that we use as the object retrieval database. Out of the available room types in the dataset, we independently consider living rooms and dining rooms which are more densely furnished and feature complex floor plans. We follow the preprocessing from ATISS [38], leading respectively to 2338/587 and 2071/516 train/test splits. ", "page_idx": 6}, {"type": "text", "text": "Baselines We compare DeBaRA with ATISS [38] autoregressive transformer and DiffuScene [51] denoising network. To ensure a fair comparison with our method, we retrained both models with floor plan conditioning on each 3D-FRONT subset using their official implementations. To perform 3D arrangement generation with DiffuScene, we implemented DDPM inpainting [30] of object spatial features from their known semantic categories. Additionally, we report experimental results obtained by LayoutGPT [9] that we implemented with a Llama-3-8B backbone [8] that we also use to provide semantic categories in scene synthesis scenarios. Following the paper, we perform prompting with supporting examples: for each test scene, we retrieve top- $k$ samples from the training set that have the most similar floor plan and include their spatial configuration as few-shot exemplars. Note that LayoutGPT adopts a training-free approach and is therefore not directly comparable to our method. However, we show how it can be used alongside a specialized model such as DeBaRA. Full implementation details and LLM prompting strategies are reported in Appendix B.3. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics We follow previous work [43, 38, 51, 9, 55] and evaluate the realism and diversity of generated arrangements by reporting the $256^{2}$ Fr\u00e9chet Inception Distance (FID) [15], Kernel Inception Distance $(\\mathrm{KID}\\times1,000)$ [3] and Scene Classification Accuracy (SCA, values closer to $50\\%$ are better) computed on top-down orthographic renderings. Resulting projections feature the scene\u2019s floor plan and objects colored according to their semantic class [51]. The generation spatial validity is further assessed by reporting the cumulated out of bounds objects area (OBA, in $\\,\\!\\stackrel{-}{m}^{2}.$ ). Related indicators are provided and discussed in Appendix D.1. Metrics are computed across each test subset, for which we generate the same number of scenes as the number of real ones. ", "page_idx": 6}, {"type": "text", "text": "4.1 3D Layout Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The primary task of DeBaRA is to generate diverse and valid 3D layouts within a given floor plan and a list of object semantics. We showcase qualitative generation results and comparisons in Figure 3. As highlighted by previous work [57, 51], denoising-based methods better capture the interplay between interacting objects. We also observe that DeBaRA largely outperforms baselines at respecting the scene\u2019s bounds while consistently producing more natural arrangements. These observations are quantitatively verified in Table 1 and visualized in Figure 3. ", "page_idx": 7}, {"type": "table", "img_path": "rajRJ6WKj2/tmp/eabff329945d5a166968e5c3cf2a397f7e1284d22a9db5820f76158cdc301e90.jpg", "table_caption": ["Table 1: Quantitative experiment results on bounded 3D layout generation (providing a floor plan and a list of object semantic categories). We compare our method against other learning-based approaches and additionally indicate results obtained from a training-free LayoutGPT. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 3D Scene Synthesis and Self Score Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We demonstrate competitive or state-of-the-art capabilities on 3D scene synthesis against methods that have been specifically trained for this task. We consider several settings depending on the source of input object categories and report our results in Table 2. First, we observe that randomly picking input semantics from the training set (Dataset Random) or taking the set $^c$ generated by LayoutGPT [9] outperform baselines by a significant margin on the 3D-FRONT living rooms test set. Then, to measure the individual impact of SSE, we compare a setup in which input semantics are selected from a set of LLM-generated ones, either randomly (LLM) or by applying SSE. As LLMs often hallucinate or produce out-of-distribution sets, our procedure consistently improves realism and validity of the synthesized indoor scenes, which can also be qualitatively observed in Figure 5. These results further validate our choice to focus solely on 3D spatial features of objects. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Quantitative experiment results on 3D scene synthesis. DeBaRA is evaluated in various settings based on the source of object semantic categories $^c$ . Precise settings are detailed and discussed in Appendix B.4. DeBaRA outperforms established baselines on most evaluation metrics. ", "page_idx": 7}, {"type": "table", "img_path": "rajRJ6WKj2/tmp/426fed8fca57a47c544d43d4bf26c6198e099394fa0f9aba7943bef477b265d8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Other Application Scenarios ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present DeBaRA\u2019s capabilities at performing additional controllable tasks. Notably, we include quantitative (Table 3) and qualitative (Figure 7) experimental evaluations against LEGO-Net [57] on scene-rearrangement. Results highlight that our method is able to recover more realistic arrangements, while being closer to their initial, messy configurations. This is remarkable as the LEGO-Net baseline has been specifically trained to perform this task. ", "page_idx": 8}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/c2dad51ee401dcaebf813b36583f0605d21d35d1ec89c1e1df31c445ef0f542b.jpg", "img_caption": ["Figure 7: Qualitative comparison against LEGONet [57] on scene re-arrangement. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Quantitative evaluation on scene rearrangement. DeBaRA is able to recover more realistic arrangements, closer to their initial noisy configurations. ", "page_idx": 8}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/44c9a7f9adb32e94f983ca1db742a881f7d1b7508ad2dac7392bc56b2918b6f3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We also provide additional re-arrangement results and showcase DeBaRA\u2019s scene completion capabilities, by inserting objects from a list of additional semantics, in Figure 4. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate the individual contributions of some of our framework\u2019s key components on the base 3D layout generation task. Notably, results reported in Table 4 highlight the advantage of our novel objective (Section 3.3) over common formulations as well as the benefits of modeling both the unconditional and class-conditional densities of 3D layouts during training (Section 3.2). ", "page_idx": 8}, {"type": "text", "text": "Table 4: Ablation study on DeBaRA training setup. We evaluate the individual impact, on 3D layout generation, of different learning objectives $\\mathcal{L}$ and of applying conditioning dropout with rate $p_{\\mathrm{drop}}$ . Notably, the use of our novel Chamfer distance results in a significant performance increase. ", "page_idx": 8}, {"type": "table", "img_path": "rajRJ6WKj2/tmp/06dc685405d6d641f354eadf5f149e898a67d6039027d80e752f5856ac2e36c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Additional Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Complex Floor Plans We notice that the 3D-FRONT dataset mostly contains simple floor maps (i.e., single room, squared, rectangular) both for training and evaluation. As a result, we manually designed irregular floor shapes and report DeBaRA\u2019s generation in Figure 8, which further highlights the robustness of our method and its consideration for the conditioning input. ", "page_idx": 8}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/ef0602ee0ea25a5371a6ed32a0cb08bc5a702f9f55a83d9cbc59cb8eefed483c.jpg", "img_caption": ["Figure 8: Generated layouts from a given set of objects and complex floor plans, selected from the 3D-FRONT test set or handcrafted to irregular, out-of-distribution shapes. While challenging, DeBaRA is able to output plausible layouts in which objects are scattered across the input floor plans. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Iterative Sampling We provide a visualization of the iterative denoising process over time when generating a 3D layout from arbitrarily initialized object bounding boxes in Figure 9. ", "page_idx": 9}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/edbf3a0282e40b9dda9a66b1dddde749e3194b715dc1fe802780c99e408279b0.jpg", "img_caption": ["Figure 9: Visualization of intermediate layouts throughout the DeBaRA denoising process. Coarse object attributes (positions, rotations and dimensions) are determined in the early steps, and then refined in the late iterations. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Generation Variety and Validity We also perform scene completion by adding a bookshelf and a coffee table, repeat the experiment ten times and report in Figure 10 the denoising object trajectories, intermediate and final positions (colored and black dots respectively). This allows to observe the variety of predicted layouts. Notably, we can see that the bookshelf ends up in various different positions, always next to a wall, while the coffee table, which is functionally constrained by its surrounding objects, is placed at similar valid locations. ", "page_idx": 9}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/329af906c7b10b322296b6abfaa11b55d0e6ce45f84a924268a45e468f335572.jpg", "img_caption": ["Figure 10: Visualization of sampling trajectories and final positions of a pair of objects, inserted into a scene across ten trials. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "rajRJ6WKj2/tmp/27cb116ce8532e818d1514cd2b81c38ca6bec24489cdc88870dedcc934641e34.jpg", "table_caption": ["Table 5: Generation times are averaged on the 3DFRONT [10] living room test subset. DeBaRA is implemented with $T=50$ sampling steps and $T_{\\mathrm{sse}}=100$ Self Score Evaluation trials. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Network Efficiency Finally, we compare the number of parameters as well as the sampling (i.e., generation) time, measured on the 3D layout generation task, of our DeBaRA backbone with those of other recent data-driven approaches in Table 5. We can see that our lightweight architecture is bridging the gap with autoregressive methods in terms of inference efficiency. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion, Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we proposed DeBaRA, a novel score-based framework, which achieves state-of-theart results in 3D layout generation. Our approach is distinctive in its design choices, which both favor data-efficiency with enhanced spatial reasoning, while, at the same time, enabling a range of applications such as scene re-arrangement and completion. Furthermore, we introduce a novel Self Score Evaluation procedure, which allows us to leverage a trained model to select the conditioning signals, which lead to the most plausible results. Overall, our work is the first to unify the conditioning and prediction spaces of score-based models within the context of 3D generative layout. ", "page_idx": 9}, {"type": "text", "text": "While powerful, our method currently does not enforce physical constraints between interacting objects, which can lead to collisions. We also assume that object semantic classes are selected among a finite set of predefined categories. Finally, we do not enforce style consistency between objects, which can, nevertheless, be performed at retrieval time. ", "page_idx": 9}, {"type": "text", "text": "We believe that our approach can enhance other generative models (e.g., architectural layouts, images) by both evaluating the quality and by promoting more plausible 3D layout designs. Furthermore, it will be interesting to combine our approach with encoders from other modalities for a unified multi-modal layout generation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Despoina Paschalidou for the preprocessing code base and Jiapeng Tang for the mesh rendering pipeline. We also acknowledge the anonymous reviewers for their insightful suggestions. This work was supported by Dassault Syst\u00e8mes SE. The views and conclusions contained in the paper are those of the authors and should not be interpreted as representing official policies, either expressed or implied, of the company. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X Chang, and Matthias Nie\u00dfner. Scan2cad: Learning cad model alignment in rgb-d scans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2614\u20132623, 2019.   \n[2] Dhruv Batra, Angel X Chang, Sonia Chernova, Andrew J Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv:2011.01975, 2020.   \n[3] Miko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018.   \n[4] Ryan Burgert, Kanchana Ranasinghe, Xiang Li, and Michael S Ryoo. Peekaboo: Text to image diffusion models are zero-shot segmentors. arXiv preprint arXiv:2211.13224, 2022.   \n[5] Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai Hao, Xiao Yang, Hang Su, and Jun Zhu. Your diffusion model is secretly a certifiably robust classifier. arXiv preprint arXiv:2402.02316, 2024.   \n[6] Kevin Clark and Priyank Jaini. Text-to-image diffusion models are zero shot classifiers. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \n[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5828\u20135839, 2017.   \n[8] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \n[9] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \n[10] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10933\u201310942, 2021.   \n[11] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision (IJCV), pages 1\u201325, 2021.   \n[12] Rao Fu, Zehao Wen, Zichen Liu, and Srinath Sridhar. Anyhome: Open-vocabulary generation of structured and textured 3d homes. In European Conference on Computer Vision (ECCV), pages 52\u201370. Springer, 2024.   \n[13] Lin Gao, Jia-Mu Sun, Kaichun Mo, Yu-Kun Lai, Leonidas J Guibas, and Jie Yang. Scenehgn: Hierarchical graph networks for 3d indoor scene generation with fine-grained geometry. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 45(7):8902\u20138919, 2023.   \n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016.   \n[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017.   \n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems (NeurIPS), 33:6840\u20136851, 2020.   \n[17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[18] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-based generation, optimization, and planning in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16750\u201316761, 2023.   \n[19] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems (NeurIPS), 35:26565\u201326577, 2022.   \n[20] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 24174\u201324184, 2024.   \n[21] Kurt Leimer, Paul Guerrero, Tomer Weiss, and Przemyslaw Musialski. Layoutenhancer: Generating good indoor layouts from imperfect data. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20138, 2022.   \n[22] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly a zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2206\u20132217, 2023.   \n[23] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In The Twelfth International Conference on Learning Representations (ICLR), 2024.   \n[24] Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, and C. Karen Liu. Controllable human-object interaction synthesis. In European Conference on Computer Vision (ECCV), 2024.   \n[25] Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan, Ariel Shamir, Changhe Tu, Baoquan Chen, Daniel Cohen-Or, and Hao Zhang. Grains: Generative recursive autoencoders for indoor scenes. ACM Transactions on Graphics (TOG), 38(2):1\u201316, 2019.   \n[26] Chenguo Lin and MU Yadong. Instructscene: Instruction-driven 3d indoor scene synthesis with semantic graph prior. In The Twelfth International Conference on Learning Representations (ICLR), 2024.   \n[27] Jingyu Liu, Wenhan Xiong, Ian Jones, Yixin Nie, Anchit Gupta, and Barlas O\u02d8guz. Clip-layout: Styleconsistent indoor scene synthesis with semantic furniture embedding. arXiv preprint arXiv:2303.03565, 2023.   \n[28] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations (ICLR), 2017.   \n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2018.   \n[30] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11461\u201311471, 2022.   \n[31] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2837\u20132845, 2021.   \n[32] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14297\u201314306, 2023.   \n[33] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning (ICML), pages 8162\u20138171. PMLR, 2021.   \n[34] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning (ICML), pages 16784\u201316804. PMLR, 2022.   \n[35] Yinyu Nie, Angela Dai, Xiaoguang Han, and Matthias Nie\u00dfner. Learning 3d scene priors with 2d supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 792\u2013802, 2023.   \n[36] Wamiq Para, Paul Guerrero, Tom Kelly, Leonidas J Guibas, and Peter Wonka. Generative layout modeling using constraint graphs. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6690\u20136700, 2021.   \n[37] Wamiq Reyaz Para, Paul Guerrero, Niloy Mitra, and Peter Wonka. Cofs: Controllable furniture layout synthesis. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \n[38] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregressive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems (NeurIPS), 34:12013\u201312026, 2021.   \n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.   \n[40] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations (ICLR), 2022.   \n[41] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652\u2013660, 2017.   \n[42] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.   \n[43] Daniel Ritchie, Kai Wang, and Yu-an Lin. Fast and flexible indoor scene synthesis via deep convolutional generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6182\u20136190, 2019.   \n[44] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding. In International Conference on Computer Vision (ICCV), 2021.   \n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, 2022.   \n[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in Neural Information Processing Systems (NeurIPS), 35:36479\u201336494, 2022.   \n[47] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations (ICLR), 2022.   \n[48] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in Neural Information Processing Systems (NeurIPS), 34:1415\u20131428, 2021.   \n[49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations (ICLR), 2021.   \n[50] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818\u20132826, 2016.   \n[51] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nie\u00dfner. Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20507\u201320518, 2024.   \n[52] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, et al. Lion: Latent point diffusion models for 3d shape generation. Advances in Neural Information Processing Systems (NeurIPS), 35:10021\u201310039, 2022.   \n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017.   \n[54] Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X Chang, and Daniel Ritchie. Planit: Planning and instantiating indoor scenes with relation graph and spatial prior networks. ACM Transactions on Graphics (TOG), 38(4):1\u201315, 2019.   \n[55] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nie\u00dfner. Sceneformer: Indoor scene generation with transformers. In International Conference on 3D Vision (3DV), pages 106\u2013115. IEEE, 2021.   \n[56] Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. In The Eleventh International Conference on Learning Representations (ICLR), 2022.   \n[57] Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, and Leonidas Guibas. Lego-net: Learning regular rearrangements of objects in rooms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19037\u201319047, 2023.   \n[58] Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, et al. Blockfusion: Expandable 3d scene generation using latent tri-plane extrapolation. ACM Transactions on Graphics (TOG), 43(4):1\u201317, 2024.   \n[59] Xiang Xu, Joseph Lambourne, Pradeep Jayaraman, Zhengqing Wang, Karl Willis, and Yasutaka Furukawa. Brepgen: A b-rep generative diffusion model with structured latent geometry. ACM Transactions on Graphics (TOG), 43(4):1\u201314, 2024.   \n[60] Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: Physically interactable 3d scene synthesis for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16262\u201316272, 2024.   \n[61] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3d embodied ai environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16227\u201316237, 2024.   \n[62] Guangyao Zhai, Evin P\u0131nar \u00d6rnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, and Benjamin Busam. Commonscenes: Generating commonsense 3d indoor scenes with scene graphs. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \n[63] Genghao Zhang, Yuxi Wang, Chuanchen Luo, Shibiao Xu, Junran Peng, Zhaoxiang Zhang, and Man Zhang. Furniscene: A large-scale 3d room dataset with intricate furnishing scenes. arXiv preprint arXiv:2401.03470, 2024.   \n[64] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3836\u20133847, 2023.   \n[65] Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne Vouga, and Qixing Huang. Deep generative modeling for scene synthesis via hybrid representations. ACM Transactions on Graphics (TOG), 39(2):1\u201321, 2020.   \n[66] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5745\u20135753, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Score-based Framework ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we give additional details on the score-based parameterization that we adopt to learn the distribution of 3D layouts $p_{\\mathrm{data}}(x)$ and the sampling strategy used to generate new samples from the resulting trained denoiser $D_{\\theta}$ . ", "page_idx": 14}, {"type": "text", "text": "A.1 Training ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Score-based approaches model the score (i.e., the gradient of log-probability density w.r.t. the data) of marginal distributions $p_{\\sigma}(x)$ obtained by perturbing the data with Gaussian noise $\\pmb{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ at magnitudes $\\sigma$ . In practice, the score can be effectively approximated by a noise-conditioned denoiser that outputs clean samples from noisy ones, then, $\\nabla_{x}\\log p_{\\sigma}({\\pmb x})=(\\bar{D}_{\\theta}({\\pmb x};{\\pmb y},\\sigma)-{\\pmb x})/\\sigma^{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Parameterizing the denoiser to output $\\textbf{\\em x}$ from its corrupted version directly is not ideal as the input magnitude varies greatly depending on the current noise level. Instead, Karras et al. [19] propose in their EDM diffusion framework a preconditioning of the denoiser whose output is now derived from a trainable network $F_{\\theta}$ that either predicts the clean signal $\\textbf{\\em x}$ , the noise $\\epsilon$ or something in between, depending on the value of $\\sigma$ . More formally, it can be expressed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{\\theta}({\\pmb x};{\\pmb y},\\sigma)=c_{\\mathrm{skip}}(\\sigma)\\,{\\pmb x}+c_{\\mathrm{out}}(\\sigma)\\,F_{\\theta}\\bigl(c_{\\mathrm{in}}(\\sigma)\\,{\\pmb x};{\\pmb y},c_{\\mathrm{noise}}(\\sigma)\\bigr)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The preconditioning function $c_{\\mathrm{skip}}$ amplifies the network error as little as possible while $c_{\\mathrm{in}}$ and $c_{\\mathrm{out}}$ scale respectively the input and output to have unit variance. Following [19], we set: ", "page_idx": 14}, {"type": "equation", "text": "$$\nc_{\\mathrm{skip}}=\\frac{\\sigma_{\\mathrm{data}}^{2}}{\\sigma_{\\mathrm{data}}^{2}+\\sigma^{2}}~;~c_{\\mathrm{in}}=\\frac{1}{\\sqrt{\\sigma_{\\mathrm{data}}^{2}+\\sigma^{2}}}~;~c_{\\mathrm{out}}=\\frac{\\sigma\\cdot\\sigma_{\\mathrm{data}}}{\\sqrt{\\sigma_{\\mathrm{data}}^{2}+\\sigma^{2}}}~;~c_{\\mathrm{noise}}=\\frac{\\ln(\\sigma)}{4}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that in our case, the value of $\\sigma_{\\mathrm{data}}$ should preferably be computed channel-wise for each attribute of $\\ensuremath{\\mathbf{{x}}}=(p,r,d)\\in\\mathbb{R}^{8}$ , as object positions, rotations and dimensions typically have different standard deviations. In practice, we compute $\\sigma_{\\mathrm{data}}^{p}$ and $\\sigma_{\\mathrm{data}}^{d}$ from the training data and arbitrarily set $\\sigma_{\\mathrm{{data}}}^{r}\\,=\\,(0.5,0.5)$ . During training, noise values $\\sigma$ are drawn from a centered normal distribution of variance 0.25, which concentrates training on medium noise levels. Spatial values $\\pmb{p}$ and $^d$ of each training layout $\\textbf{\\em x}$ are normalized based on the maximum extent of the scene\u2019s floor plan $\\mathcal{F}$ , which ensures that all the network\u2019s inputs and outputs are scaled in $[-1,1]$ . To model both the class-conditional and unconditional layout densities, we perform conditioning dropout on object categories $^c$ with a rate $p_{\\mathrm{drop}}=0.2$ . ", "page_idx": 14}, {"type": "text", "text": "Finally, the training objective can be expressed by introducing our semantic-aware Chamfer reconstruction loss following Equation 3. As in EDM, we use $\\lambda(\\sigma)\\bar{=}1/c_{\\mathrm{out}}^{2}$ to get a uniform weighting across noise levels. ", "page_idx": 14}, {"type": "text", "text": "A.2 Sampling ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "At test time, the reverse SDE [19, 49] associated to the continuous-time diffusion process is used to generate novel samples from a standard normal distribution using numerical solvers. It depends on the score approximated during training: ", "page_idx": 14}, {"type": "equation", "text": "$$\nd\\pmb{x}=\\left[f(\\pmb{x},t)-g(t)^{2}\\nabla_{\\pmb{x}}\\log p_{t}(\\pmb{x})\\right]d t+g(t)d w\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $f(\\cdot,t):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ and $g(\\cdot):\\mathbb{R}\\rightarrow\\mathbb{R}$ are respectively the drift and diffusion coefficients and $w$ is the standard stochastic Wiener process. ", "page_idx": 14}, {"type": "text", "text": "In practice, we use EDM [19] $2^{\\mathrm{nd}}$ order Runge-Kutta stochastic sampler (see Algorithm 2), that resembles the predictor/corrector framework from Song et al. [49] and provides a good trade-off between generation quality and number of function evaluations (NFE). ", "page_idx": 14}, {"type": "text", "text": "It is based on a $T$ -step discretization of the reverse SDE [19, 49], with timesteps $t_{i\\in\\{0,...,T-1\\}}$ decreasing from $\\sigma_{\\mathrm{max}}$ $(i=0)$ ) to $\\sigma_{\\mathrm{min}}$ $(i=T-1)$ . Following [19], we use: ", "page_idx": 14}, {"type": "equation", "text": "$$\nt_{i<T}=\\left(\\sigma_{\\operatorname*{max}}^{\\frac{1}{\\rho}}+\\frac{i}{T-1}\\left(\\sigma_{\\operatorname*{min}}^{\\frac{1}{\\rho}}-\\sigma_{\\operatorname*{max}}^{\\frac{1}{\\rho}}\\right)\\right)^{\\rho},\\ \\ t_{T}=0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "1: pr $\\begin{array}{r}{\\mathbf{ocedure}\\;\\mathrm{LAYOUTSAMPLER}(D_{\\theta}(x;y,\\sigma),x_{0},t_{i\\in\\{0,\\ldots,T-1\\}},\\gamma_{i\\in\\{0,\\ldots,T-1\\}},S_{\\mathrm{noise}})}\\end{array}$   \n2: for $i\\in\\{0,\\ldots,T-1\\}$ do   \n3: sample $\\pmb{\\epsilon}_{i}\\sim\\mathcal{N}(\\mathbf{0},S_{\\mathrm{noise}}^{2}\\mathbf{I})$   \n4: t\u02c6i \u2190ti + \u03b3iti   \n5: \u02c6xi \u2190xi + t\u02c6i2 \u2212ti2 \u03f5i   \n6: $\\pmb{g}_{i}\\leftarrow\\left(\\hat{\\pmb{x}}_{i}-D_{\\theta}(\\hat{\\pmb{x}}_{i};\\pmb{y},\\hat{t}_{i})\\right)/\\hat{t}_{i}$   \n7: xi+1 \u2190x\u02c6i + (ti+1 \u2212t\u02c6i)gi   \n8: if ti+1 \u0338= 0 then   \n9: g\u2032i \u2190(xi+1 \u2212D\u03b8(xi+1; y, ti+1)) /ti+1   \n10: xi+1 \u2190\u02c6xi + 12(ti+1 \u2212t\u02c6i)(gi + g\u2032i)   \n11: end if   \n12: end for   \n13: end procedure ", "page_idx": 15}, {"type": "text", "text": "Note that $t$ and noise level $\\sigma$ can be used interchangeably. The $\\rho$ parameter is tuned to dedicate more steps of the denoising process to smaller or larger noise levels. The $\\sigma_{\\mathrm{min}}$ value should be small enough so that the model estimates the best approximation of the score and sample a precise layout. On the other hand, $\\sigma_{\\mathrm{max}}$ should be large enough to sample various layouts. The amount of fresh noise injected at the beginning of each denoising step is defined by $\\gamma_{i\\in\\{0,...,N-1\\}}$ . Similar to Wei et al. [57] and Karras et al. [19], we qualitatively observed that adding noise in the final timesteps, i.e., when the layout is close to its final configuration, leads to less precise results. As a result, an additional $S_{\\mathrm{min}}$ parameter is set so that $\\gamma_{i}=0$ when $t_{i}<S_{\\mathrm{min}}$ . ", "page_idx": 15}, {"type": "text", "text": "For 3D layout generation, we use $T=50$ timesteps and set $\\sigma_{\\mathrm{max}}=1.0$ , $\\sigma_{\\mathrm{min}}=0.005$ , $\\rho=7$ , and $S_{\\mathrm{min}}=0.005$ . Note that, although we perform conditioning dropout during training, we didn\u2019t find the need to amplify the strength of the input categories using any classifier-free guidance [17] scale at sampling time. ", "page_idx": 15}, {"type": "text", "text": "Additionally, Table 6 shows that sampling from DeBaRA using EDM [19] $2^{\\mathrm{nd}}$ order stochastic procedure (Algorithm 2) outperforms ancestral DDPM [16] sampling using a fraction of the denoising steps. This, combined with our lightweight architecture, enables real-time $(<\\!1\\mathrm{s})$ generation. ", "page_idx": 15}, {"type": "table", "img_path": "rajRJ6WKj2/tmp/4978d0b0b8440ce46e5f22240a0600ba75959f6a80b6db43b3316b4f357087be.jpg", "table_caption": ["Table 6: Ablation study on DeBaRA sampling strategy. Metrics are computed on the 3D layout generation task. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Implementation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide in this section additional implementation details on our model architecture and training configurations, illustrated in Figure 2. We also detail how baselines have been retrained and used at test time to ensure a fair and relevant comparison with our approach. ", "page_idx": 15}, {"type": "text", "text": "B.1 Network Architecture ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Shared Object Encoder The shared object encoder embeds each object $o_{i}$ from its input 3D spatial values $\\pmb{x}_{i}$ and semantic category $c_{i}$ . Triplets of scalar values of the object\u2019s position $\\pmb{p}_{i}$ and dimension $d_{i}$ are encoded with fixed sinusoidal positional encoding of 32 frequencies following [57]: ", "page_idx": 15}, {"type": "equation", "text": "$$\nP E(s)=\\left\\{\\sin(128^{j/31}s),\\cos(128^{j/31}s)\\right\\}_{j=0}^{31}\\in\\mathbb{R}^{64}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Applying this module projects $\\pmb{p}_{i}$ and $\\pmb{r}_{i}$ in $\\mathbb{R}^{192}$ . It is similarly applied to $\\pmb{r}_{i}=(\\cos(\\theta_{i}),\\sin(\\theta_{i}))$ to get a feature in $\\mathbb{R}^{128}$ , that is additionally fed to a linear layer to obtain a 192-dimensional attribute. ", "page_idx": 15}, {"type": "text", "text": "The object semantic class $c_{i}$ , represented as a one-hot vector among $k$ classes is encoded in $\\mathbb{R}^{192}$ by an MLP with 2 a hidden layer of 128 units and LeakyReLU activation. Respective object spatial and semantic encodings are then concatenated to form an object token $\\mathcal{T}_{o_{i}}$ of dimension $4\\times192=768$ . ", "page_idx": 16}, {"type": "text", "text": "Floor Encoder The scene\u2019s conditioning floor plan $\\mathcal{F}$ is embedded by a PointNet [41] module, similar to [57]. To do so, we first extract the floor\u2019s 2D polygon using the output of ATISS [38] preprocessing and sample $P=100$ evenly spaced points on its contour. The PointNet backbone2 produces a 1024-dimensional feature, that is further passed to a linear layer to get the appropriate floor token $\\mathcal{T}_{\\mathcal{F}}\\in\\mathbb{R}^{768}$ . ", "page_idx": 16}, {"type": "text", "text": "Noise Level Encoder We encode the noise level $\\sigma$ as a token $\\mathcal{T}_{\\sigma}\\in\\mathbb{R}^{768}$ obtained by subsequently applying $P E(\\sigma)$ and a linear layer with LeakyReLU activation. ", "page_idx": 16}, {"type": "text", "text": "Transformer Encoder Our transformer encoder that computes new representations $\\hat{\\mathcal{T}}$ is composed of multi-head self-attention and feedforward layers, following the original paper [53] and implementation from the PyTorch [39] API. Importantly, we don\u2019t enforce ordering of any input token and pass an additional padding mask to handle sequences of different lengths. We stack 3 encoder layers, each having 4 attention heads and a feedforward hidden dimension of 512. ", "page_idx": 16}, {"type": "text", "text": "Shared Object Decoder The final shared object decoder produces the network\u2019s predicted spatial values $\\hat{\\pmb{x}_{i}}\\in\\mathbb{R}^{8}$ for each of the $N$ objects from their respective $\\hat{\\mathcal{T}}_{o_{i}}$ embeddings. It is implemented as an MLP with three layers of 512, 128, and 8 units, using LeakyReLU activations and a dropout rate of 0.1. ", "page_idx": 16}, {"type": "text", "text": "B.2 Training Protocol ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "During training, the network is optimized towards our semantic-aware Chamfer loss, that can be efficiently implemented with appropriate broadcasting. We trained our models separately on the 3D-FRONT [10] living room and dining room subsets for 3000 epochs, with a batch size of 32 and monitor the validation loss to avoid overfitting of the training set in the late iterations. We use the AdamW [29] optimizer with its PyTorch default parameters and learning rate $\\eta=10^{-4}$ , scheduled with a linear warmup phase for the first 50 epochs, starting at $\\eta\\times0.01$ . Following this, a cosine annealing schedule [28] reduces $\\eta$ to a minimum of $10^{-8}$ over 2200 epochs. Finally, we randomly apply rotations to the training scenes as data augmentation. ", "page_idx": 16}, {"type": "text", "text": "B.3 Baselines ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "ATISS ATISS [38] is an autoregressive, permutation-invariant transformer that treats 3D scene synthesis as an unordered set generation task. The model is natively conditioned on the room\u2019s floor plan, from which it extracts features using a ResNet-18 [14] applied on a top-down binary projection. The model predicts the semantic class, location, rotation and dimension of the next object to be inserted to the current layout configuration. As our method, it also supports inserting objects from their semantic categories given as input, which is the setting that we used to report experimental results on the 3D layout generation task (see Table 1 and Figure 3). We retrained the model on each 3D-FRONT subset using the authors\u2019 implementation.3 ", "page_idx": 16}, {"type": "text", "text": "DiffuScene DiffuScene [51] employs a DDPM to perform 3D scene synthesis, by learning to denoise unordered sets of objects that are each represented by all their attributes, i.e., location, size, orientation, semantic category and shape code. Although the paper specifically mentions not being conditioned on the room\u2019s bounds, we found out the official implementation supports this feature that we enabled to retrain the model on the 3D-FRONT subsets, with other settings set to those of the authors. In practice and similar to ATISS [38], a ResNet-18 backbone is used to extract features from the floor plan\u2019s projection mask. The resulting encoding is passed to an MLP whose output is added to the diffusion timestep embedding, as in [46]. To assess the effectiveness of this conditioning mechanism and validate the relevance of this baseline, we report metrics obtained for the 3D layout generation task, using both a floor-conditioned and an unconditional (vanilla) trained DiffuScene model on the living room subset in Table 7. Note that, top-down view of scenes, that we use to evaluate e.g., FID and KID scores, feature the floor plan, which penalizes generated configurations that don\u2019t properly take it into account. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Table 7: Quantitative impact of DiffuScene [51] floor plan conditioning on 3D layout generation.   \nResults indicate that the additional input has successfully been learned, resulting in a solid baseline. ", "page_idx": 17}, {"type": "table", "img_path": "rajRJ6WKj2/tmp/28a90973268ae347c95cbb907756d01f620461dbcdeb51501735ddbbf3f17f49.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "To perform 3D layout generation from input semantic categories using DiffuScene (Table 1, Figure 3), we implemented DDPM inpainting of the object spatial features from their categories as an additional method within the official implementation4 and using default sampling settings with 1000 timesteps. ", "page_idx": 17}, {"type": "text", "text": "LayoutGPT LayoutGPT [9] is a training-free approach that utilizes Large Language Models to generate layouts both in the image and the 3D scene domains, demonstrating competitive performance with learning-based approaches on 3D scene synthesis. To do so, the method consists in prompting a LLM with specific instructions and by adding supporting examples from the training set, i.e., few-shot exemplars of expected, valid layouts. These examples are retrieved from the train set based on floor plan similarity computed from the binary masks with a test sample. We reimplemented LayoutGPT, using the official implementation5 for exact prompt and supporting examples retrieval, but using a Meta Llama- $\\ensuremath{\\cdot}3\\!-\\!8\\ensuremath{\\mathrm{B}}^{6}$ backbone instead of ChatGPT variants for local execution and better reproducibility. ", "page_idx": 17}, {"type": "text", "text": "To perform 3D layout generation, we include in the prompt the list of object semantic categories. Here is a typical LayoutGPT prompt for this task on the living room subset: ", "page_idx": 17}, {"type": "text", "text": "Instruction: synthesize the 3D layout of an indoor scene. The generated 3D layout should follow the CSS style, where each line starts with the furniture category and is followed by the 3D size, orientation and absolute position. Formally, each line should follow the template:   \nFURNITURE {length: ?m: width: ?m; height: ?m; left: ?m; top: ?m; depth: ?m; orientation: ? degrees;}   \nAll values are in meters but the orientation angle is in degrees. ", "page_idx": 17}, {"type": "text", "text": "Condition: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Room Type: livingroom Room Size: max length 1.00m, max width 1.53m ", "page_idx": 17}, {"type": "text", "text": "Furniture  categories:  console_table  multi_seat_sofa corner_side_table corner_side_table coffee_table tv_stand pendant_lamp ", "page_idx": 17}, {"type": "text", "text": "console_table {length: 0.28m; height: 0.31m; width: 0.14m; orientation: -90   \ndegrees; left: 0.93m; top: 0.41m; depth: 0.15m;}   \n[\u2026] ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "[OTHER SUPPORTING EXAMPLES] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Condition:   \nRoom Type: livingroom   \nRoom Size: max length 1.00m, max width 1.62m   \nFurniture categories: tv_stand corner_side_table coffee_table dining_table   \ndining_chair dining_chair dining_chair dining_chair armchair pendant_lamp   \npendant_lamp multi_seat_sofa ", "page_idx": 17}, {"type": "text", "text": "Layout: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For 3D scene synthesis, we follow the paper and include the training set\u2019s object frequencies in the prompt. We set the LLM sampling temperature to 0.7 and maximum output tokens to 1024. We report LayoutGPT performance on this task in Table 2, where we also indicate metrics obtained by DeBaRA when using as conditioning input the same semantic set as the one generated by LayoutGPT for the corresponding test scene. In this setup, we largely outperform the baseline and even report state-of-the-art FID, KID and OBA scores on 3D-FRONT living room. ", "page_idx": 17}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/ca176ae23553e7a8fea978c23029e27af29eb0bb425084c8e98effd6943a6e0a.jpg", "img_caption": ["Figure 11: Self Score Evaluation (SSE) Pipeline. The method allows to leverage the knowledge of a model trained following our method to select valid sets of object categories. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "LEGO-Net The LEGO-Net [57] model is specifically designed to perform 2D scene re-arrangement, i.e., recover a close clean layout configuration from a messy, perturbed one. It is trained using a regression loss on object position and rotation values, and proposes a Langevin dynamics-like iterative sampling procedure. In order to produce the results reported in Table 3, we used authors\u2019 implementation,7 in the grad without noise setting (which is the best performing in the original paper), on the 3D-FRONT living room test subset and with a scene perturbation level of 0.25. ", "page_idx": 18}, {"type": "text", "text": "B.4 LLM Prompting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As mentioned in the main submission, we perform 3D scene synthesis using DeBaRA conditioned on LLM-generated sets of object semantics, that we optionally select via Self Score Evaluation (Table 2, Figure 5). In practice, the generated categories have been obtained using Llama-3-8B [8] following the LayoutGPT [9] prompting strategy. We experienced asking the language model to generate solely lists of object semantics using a few supporting examples and providing the dataset statistics, but we noticed that it was more prone to hallucinate and drift towards inconsistent generations than when generating complete layout configurations (i.e., including object position and orientation values). ", "page_idx": 18}, {"type": "text", "text": "The DeBaRA LLM reported in Table 2 corresponds to the setup where we filter from the LLMgenerated sets of categories those having the same number of objects as the considered test scene and randomly select one to condition DeBaRA. The DeBaRA $L L M+S S E$ setting is similar, but instead of picking a set randomly, the selection is performed by applying the SSE procedure. ", "page_idx": 18}, {"type": "text", "text": "B.5 Computational Requirements ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All the training and evaluation experiments as well as the computation of generation times reported in Table 5 have been performed on a single NVIDIA RTX A6000 GPU. When comparing our number of network parameters and generation times with those of ATISS [38] and DiffuScene [51], we notice that DeBaRA is bridging the gap with autoregressive methods in terms of inference efficiency. This is made possible by our restricted output space that requires a more lightweight backbone as well as our choice of sampling procedure that leads to a favorable NFE / generation quality tradeoff. ", "page_idx": 18}, {"type": "text", "text": "C Self Score Evaluation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide additional content regarding our SSE procedure (Section 3.4), by illustrating it and by further assessing its expressive power. ", "page_idx": 19}, {"type": "text", "text": "C.1 Pipeline ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The SSE formulation is expressed by Equations 4 - 5. It follows the procedure outlined in Algorithm 1. We additionally illustrate the SSE pipeline in Figure 11. In our experiments, SSE is implemented using $T_{\\mathrm{sse}}=100$ trials, with noise levels $\\sigma$ drawn as in training. ", "page_idx": 19}, {"type": "text", "text": "C.2 Additional Evaluation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We additionally evaluate the expressive power of the SSE procedure in a toy binary classification task: for each scene of the test set, we create a corrupted version by replacing a proportion $p_{\\mathrm{rand}}$ of the scene\u2019s object categories by random ones. We report the binary classification score obtained by SSE when asked to discriminate the corrupted set of semantics from the ground truth one. We perform the experiments 10 times to account for the inherent stochasticity of the experimental setup and report the results for several values of $p_{\\mathrm{rand}}$ in Table 8. ", "page_idx": 19}, {"type": "table", "img_path": "", "table_caption": ["Table 8: SSE performance at discriminating ground truth sets of categories from perturbed ones. "], "table_footnote": [], "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Perturbation}\\qquad\\mathrm{None}\\qquad\\mathrm{Single}\\qquad p_{\\mathrm{rand}}=0.35\\quad p_{\\mathrm{rand}}=0.50\\quad p_{\\mathrm{rand}}=0.75\\quad\\qquad\\mathrm{All}}\\\\ &{\\mathbf{A}_{\\mathrm{ccuracy}}\\;(\\mathcal{G}_{e})\\quad\\;\\mathfrak{G}_{0.6}\\pm4.39\\quad71.7\\pm2.20\\quad73.8\\pm3.38\\quad78.5\\pm1.84\\quad80.9\\pm2.62\\quad84.0\\pm2.90}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We observe a significant gap in accuracy between the control experiment (none object is perturbed, meaning that the sets are equals) and the setting where only a single object has been swapped. It means that SSE is able to identify subtle misalignments between the network\u2019s knowledge and the provided conditioning candidates. ", "page_idx": 19}, {"type": "text", "text": "D Additional Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide supplementary quantitative indicators measuring the validity of generated layouts with respect to the conditioning floor plan. We also showcase additional qualitative results and comparisons in multiple application scenarios. ", "page_idx": 19}, {"type": "text", "text": "D.1 Bounding Metrics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In addition to the cumulated out of bounds objects area (OBA, in $m^{2}$ ) reported in Table 1, we indicate the rate of scenes having at least one object out of its bounds (OBR), and the cumulated number of out-of-bounds objects in the generated layouts (OBN) in Table 9. For OBR and OBN, we consider an object to be out-of-bounds if at least $20\\%$ of its 2D bounding box surface is outside the floor\u2019s limits. ", "page_idx": 19}, {"type": "table", "img_path": "rajRJ6WKj2/tmp/bbdbda6457ae18cf51db3ad0efdaa954dd6ef3b61287721e20978094c475cfc4.jpg", "table_caption": ["Table 9: Quantitative bounding metrics on the 3D layout generation task. We observe that DeBaRA is consistently better at respecting the indoor floor plan by a significant margin. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/f80ca2539d0d345dd934eb8e26dbcb2b6cf68bbdede63d1f4cc8a4f6cadd6be2.jpg", "img_caption": ["D.2 Qualitative Results ", "Figure 12: Additional 3D layout generation results. We compare our method by generating layouts from a list of object semantic categories and a floor plan. DeBaRA consistently produces more realistic arrangements while respecting the room\u2019s outline. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/38d539f5a8837a2219d4ef0a69098968e520c976c57745a77fcebb88e1fc7c13.jpg", "img_caption": ["Figure 13: Additional scene synthesis results. We compare DeBaRA with state-of-the-art approaches in various settings. Since object semantics are not part of our output space, they are randomly drawn from the training dataset (Dataset Random), generated by an external LLM (LayoutGPT) or selected from LLM-generated sets by our density estimate procedure (SSE), which helps to get more natural layouts for the considered floor configuration. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/8db50d03a839c348300922092f669a4c9547417d12845bbbd4b508b323eadb3c.jpg", "img_caption": ["Figure 14: Additional re-arrangement results. Our method can recover a plausible arrangement from a noisy one using iterative sampling with fixed dimension attributes and low initial noise level. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/771d41727b9993cdfcfeb50da397ad8f99554594b850aa8ce666eb768f2953ae.jpg", "img_caption": ["Figure 15: Additional scene completion results. From a list of additional objects semantics, DeBaRA is able to finely introduce the relevant items into an existing layout. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.3 Statistical Analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We can observe in our qualitative results, e.g., in Figure 3, that objects are most of the time parallel or perpendicular to walls, meaning that their rotation attributes $\\pmb{r}$ mainly take cardinal angle values, i.e., $0^{\\circ}$ , $90^{\\circ}$ , $180^{\\circ}$ or $270^{\\circ}$ . ", "page_idx": 22}, {"type": "text", "text": "Radial histograms of object rotation values reported in Figure 16 indicate that 3D-FRONT [10] subsets contain rooms in which objects, in their vast majority, exhibit this property and don\u2019t have much exotic angles, i.e., different from cardinal ones. These restricted rotation distributions are consequently learned and resembled by generative models at sampling time. ", "page_idx": 22}, {"type": "image", "img_path": "rajRJ6WKj2/tmp/6e8f794e450b883f887cb58c06691d4f527ab8cb792f2efed1d85be75c22b343.jpg", "img_caption": ["Figure 16: Distributions of object rotation values extracted from the dataset or generated following our method, reported for both test subsets. We can see that the original data predominantly features objects rotated by $0^{\\circ}$ , $90^{\\circ}$ , $180^{\\circ}$ or $270^{\\circ}$ around the scene\u2019s vertical axis, which, as a result, is also the tendency induced by the generative model. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Societal Impact ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We believe that our method will predominantly yield positive societal impact by enhancing accessibility, controllability and efficiency in 3D indoor design and creation, beneftiing both non-experts and professionals in several industries. ", "page_idx": 23}, {"type": "text", "text": "Nevertheless, it raises common concerns associated with deep generative models. Specifically, users should be warned about using the model for furnishing real-world rooms as generated layouts may result in unsafe designs. Additionally, privacy safeguards should systematically be implemented along our method in order to prevent unauthorized replication of personal spaces. ", "page_idx": 23}, {"type": "text", "text": "F Licenses ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we list licenses of datasets, open-source code artifacts and pretrained models that were used in the context of our experiments. ", "page_idx": 23}, {"type": "text", "text": "Datasets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 3D-FRONT [10]: CC BY-NC-SA 4.0 License \u2022 3D-FUTURE [11]: CC BY-NC-SA 4.0 License ", "page_idx": 23}, {"type": "text", "text": "Code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 ATISS [38]: NVIDIA Source Code License for ATISS8   \n\u2022 DiffuScene [51]: Sony Group Corporation License for DiffuScene9   \n\u2022 LayoutGPT [9]: MIT License   \n\u2022 LEGO-Net [57]: MIT License   \n\u2022 PointNet [41]: MIT License ", "page_idx": 23}, {"type": "text", "text": "Models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 Llama-3-8B [8]: Meta Llama 3 Community License Agreement10 \u2022 Inception-v3 [50]: Apache 2.0 License ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Claims made in the abstract are all developed throughout the paper, notably in the method Section 3. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the paper\u2019s main limitations in the last Section 5, along with the conclusion. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: For our main theoretical contribution, Self Score Evaluation (Section 3.4), we clearly state that it is made possible under the assumption that the denoiser models both the unconditional and the class-conditional data distributions. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We comprehensively detail our experimental setup, both for our method and the baselines in the main paper, Section 4, and in Appendix A and B. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: Unfortunately, the code for DeBaRA cannot be disclosed due to author affliiation. However, we comprehensively describe our settings and implementation details in Appendix A and B and the 3D-FRONT [10] dataset is openly available. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All the relevant details are given in the paper, notably in Section 4, as well as in Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: Unfortunately, we couldn\u2019t perform such analysis due to limited computational resources. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We indicate our GPU setting in Appendix B.5 and report network execution time in the main submission (Table 5). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Potential societal impacts, both positive or negative, are discussed in a dedicated section in Appendix E. We also discuss possible ways to mitigate potential negative societal impacts. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Creators or owners of assets are either credited via citation of their work (e.g., 3D-FRONT [10] and 3D-FUTURE [11] datasets) or direct link to their code or model (throughout Appendix B). A dedicated section in Appendix F explicitly mentions the relevant licenses, which have been properly respected to conduct our work. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]