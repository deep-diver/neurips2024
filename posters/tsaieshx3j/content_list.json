[{"type": "text", "text": "SGD vs GD: Rank Deficiency in Linear Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aditya Varre Margarita Sagitova EPFL EPFL aditya.varre@epfl.ch margarita.sagitova@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Nicolas Flammarion EPFL nicolas.flammarion@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this article, we study the behaviour of continuous-time gradient methods on a two-layer linear network with square loss. A dichotomy between SGD and GD is revealed: GD preserves the rank at initialization while (label noise) SGD diminishes the rank regardless of the initialization. We demonstrate this rank deficiency by studying the time evolution of the determinant of a matrix of parameters. To further understand this phenomenon, we derive the stochastic differential equation (SDE) governing the eigenvalues of the parameter matrix. This SDE unveils a replusive force between the eigenvalues: a key regularization mechanism which induces rank deficiency. Our results are well supported by experiments illustrating the phenomenon beyond linear networks and regression tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks have significantly advanced machine learning in recent decades. A key attribute of these models is their ability, despite being heavily overparameterized, to learn effective representations which generalizes well across different tasks. This capability has sparked substantial interest in understanding how neural networks learn internal representations for specific tasks [Bengio et al., 2013]. Gaining deeper insights into these mechanisms is crucial for enhancing model interpretability and refining training and application methodologies in real-world scenarios. ", "page_idx": 0}, {"type": "text", "text": "The success in learning these representations is often attributed to the gradient methods used in training. These methods navigate complex non-convex landscapes, finding solutions that not only minimize the training objective but also yield effective representations. They achieve this generalization while avoiding the spurious features that could potentially arise from the models\u2019 large number of parameters. Empirical studies have shown that the stochastic noise in gradient algorithms enhances generalization [Keskar et al., 2017] by favoring solutions with simpler structures that mitigate spurious features [Andriushchenko et al., 2022]. This paper address the overarching question: ", "page_idx": 0}, {"type": "text", "text": "How does stochasticity facilitate the discovery of solutions with simplified structures? ", "page_idx": 0}, {"type": "text", "text": "We explore this question using a simplified model: a single hidden-layer linear network. Despite lacking non-linearity, such networks capture some intricate phenomena of real-world deep networks and have been extensively studied to understand convergence [Arora et al., 2019a, Min et al., 2021], learning dynamics [Saxe et al., 2014], and the implicit bias of optimization algorithms [Gunasekar et al., 2017, Soudry et al., 2018]. Our work builds on this foundation by comparing stochastic algorithms with their deterministic counterparts, focusing on how these differences influence the learning of simpler structures. ", "page_idx": 0}, {"type": "text", "text": "Specifically, we analyze vector regression on two-layer linear networks trained with both gradient flow and stochastic gradient flow methods. Our contributions include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Section 4, we track the evolution of the determinant of the parameter matrix under gradient flow and stochastic gradient flow. We show that stochastic gradient flow drives the determinant towards zero, effectively removing irrelevant direction(s).   \n\u2022 In Section 5, we derive a stochastic differential equation that describes the behavior of the eigenvalues of the parameter matrix. This analysis reveals a repulsive force between eigenvalues that pushes them apart and a geometric Brownian motion that pulls them toward zero.   \n\u2022 In Section 6, we discuss the generalizability of our approach beyond square loss and various noise models, including discrete step sizes. Finally, we present experimental results in Section 7 that support our theoretical findings. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our work lies at the convergence of distinct research topics: ", "page_idx": 1}, {"type": "text", "text": "Effect of SGD on generalization. The relationship between the stochasticity of SGD and its generalization capabilities has been extensively examined [Mandt et al., 2016, Jastrzebski et al., 2018, He et al., 2019, Hoffer et al., 2017, Kleinberg et al., 2018]. Notably, SGD tends to yield models with superior generalization compared to gradient descent [Keskar et al., 2017, Jastrzebski et al., 2018, He et al., 2019]. Various explorations into this phenomenon have been conducted through various approaches: hypothesizing that SGD favors flatter minima linked to better generalization, as opposed to sharp minima associated with poor generalization [Hochreiter and Schmidhuber, 1997, Keskar et al., 2017, Andriushchenko et al., 2023], using a random walk on a random landscape model to understand the impact of stochasticity [Hoffer et al., 2017], proposing that the inherent noise in SGD smooths the loss landscape [Kleinberg et al., 2018], and exploring the implications of dynamical stability [Wu et al., 2018]. ", "page_idx": 1}, {"type": "text", "text": "Stochastic dynamics and Label Noise. Recent literature has explored label noise-driven Gradient Descent as an effective method to probe the beneficial impact of stochasticity on generalization, with two distinct perspectives emerging. Firstly, an asymptotic view on general model parametrization is considered, where Blanc et al. [2020], Damian et al. [2021] suggest that stochastic dynamics preferentially optimize a hidden objective linked to the curvature of the loss. In a related vein, Li et al. [2021] demonstrates appropriate limiting dynamics on the manifold of interpolators through time rescaling. Secondly, specifically for diagonal linear networks, HaoChen et al. [2021], Pillaud-Vivien et al. [2022] observe a similar collapsing effect due to label noise but with a finer characterization of the limiting process. Finally, in the absence of label noise, Pesme et al. [2021], Even et al. [2023] have characterized the outcomes of stochastic GF and GD for diagonal linear networks as the solutions to an implicit regularization problem that results in sparser solutions than without stochasticity. Recently, Ghosh et al. [2023] further exhibit a similar sparser features effect for single-neuron autoencoder. Chen et al. [2023] provides a condition under which an invariant set is attractive for SGD \u2014 characterizing the local behavior around these sets. The paper also studies linear networks in a teacher-student setup, however due to structured label-noise [Chen et al., 2023, A2 in p.30], the analysis falls short of capturing the repulsive force in the singular values. ", "page_idx": 1}, {"type": "text", "text": "Linear Networks. The study of two-layer linear networks has been explored extensively, particularly when optimized using gradient flow on the square loss, across various settings including zero-balance initialization and whitened data Fukumizu [1998], Saxe et al. [2014, 2019], Braun et al. [2022]. Early work by Saxe et al. [2014, 2019] elucidates the temporal changes in the singular values of the predictor, assuming decoupled dynamics and a specific data-dependent weight initialization. This condition is broadened by the analyses of Fukumizu [1998] and Braun et al. [2022], Tarmoun et al. [2021], who apply solutions from a matrix Riccati equation to characterize the weights dynamics under full-rank network initialization. Furthermore, Gidel et al. [2019] extends the existing framework by relaxing the whitened data assumption, conducting a perturbation analysis, and discussing the temporal evolution of the weight matrices\u2019 singular values. Additionally, Varre et al. [2024] eliminates the need for zero-balanced and full-rank initializations. Their study provides detailed formulas for weight evolution as a function of the initial scale , also studies a simple version of a stochastic flow without the drift. Wang and Jacot [2023] studied the implicit bias of SGD with $\\ell_{2}$ -regularization. ", "page_idx": 1}, {"type": "text", "text": "Matrix valued stochastic process and their eigenvalues. Stochastic process on the space of symmetric (or Hermitian) matrices and the evolution of their eigenvalues are well studied since Dyson [1962]. These techniques were further developed by Bru [1989, 1991] to study perturbations of principal component analysis and the eigenvalues of Wishart processes. Norris et al. [1986], Graczyk and Ma\u0142ecki [2013] applied SDE-based techniques to study the eigenvalues and eigenvectors of Brownian motion on ellipsoids. ", "page_idx": 2}, {"type": "text", "text": "3 Linear networks and continuous-time gradient method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation We use $\\langle.,.\\rangle$ to denote the inner product, i.e., $\\langle u,v\\rangle=u^{\\top}v$ for vectors, and $\\langle A,B\\rangle=$ $\\operatorname{Tr}\\left(A B^{\\top}\\right)$ for matrices. $\\mathrm{I}_{d}$ denotes the identity matrix of dimension $d$ and $0_{p\\times k}$ denote the matrix with all zero entries of dimension $p\\times k$ . ", "page_idx": 2}, {"type": "text", "text": "Vector regression. We study the vector regression problems with inputs $x_{1},\\ldots,x_{n}$ in $(\\mathbb{R}^{p})^{n}$ and outputs $y_{1},\\ldots,y_{n}$ in $(\\mathbb{R}^{k})^{n}$ . We consider the minimization of the square loss over a class of parametric models $\\mathcal{H}=\\{\\dot{f}_{\\theta}(\\cdot):\\mathbb{R}^{p}\\rightarrow\\mathbb{R}^{k}\\mid\\theta\\in\\mathbb{R}^{d}\\}$ specified in the next paragraph. The train loss therefore can be written as $\\begin{array}{r}{\\mathcal{L}\\left(\\theta\\right)=\\frac{1}{2n}\\sum_{i=1}^{n}\\left\\Vert y_{i}-f_{\\theta}(x_{i})\\right\\Vert^{2}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Parameterization with a linear network. We focus on two-layer linear neural networks of width $l\\in\\mathbb{N}^{*}$ . The model is described by the parameterization $\\boldsymbol{\\theta}=(\\mathbf{\\dot{W}}_{1},\\mathbf{W}_{2})$ , where $\\mathbf{W}_{1}\\in\\mathbb{R}^{p\\times l}$ and $\\mathbf{W}_{2}\\in\\mathbb{R}^{l\\times k}$ , and the function $f_{\\theta}(x)=\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}^{\\top}x$ . This model is linear with respect to the input $x$ . In terms of expressivity, it is comparable to the linear class of predictors, represented as $f_{\\beta}(x)=\\beta^{\\top}x$ , where $\\beta$ equals $\\mathbf{W}_{1}\\mathbf{W}_{2}$ . Throughout our analysis, we denote the equivalent linear predictor of the network as $\\beta$ . A key aspect of this parametrization is that the prediction function $f_{\\theta}$ is positive homogeneous of degree 2 with respect to $\\theta$ : specifically, for any $\\lambda\\in\\mathbb{R}$ , $f_{\\lambda\\theta}=\\lambda^{2}f_{\\theta}$ . This property mirrors that of two-layer ReLU networks and significantly influences the loss landscape navigated by the parameters $\\theta$ . It is important to note that this parameterization introduces some redundancy, a single linear predictor $\\beta$ can have multiple representations $\\mathbf{W}_{1},\\mathbf{W}_{2}$ such that $\\mathbf{W}_{1}\\mathbf{W}_{2}=\\beta$ . Some representations have a rich structure whereas other resemble random features. For example, consider the case of scalar regression $\\left.k=1\\right]$ ), for a vector $\\beta$ there exists rich parameterizations where all the neurons, i.e., columns of $\\mathbf{W}_{1}$ align with $\\beta$ and also some lazy structures where $\\mathbf{W}_{1}$ resembles a random matrix [Chizat et al., 2019, Varre et al., 2023]. ", "page_idx": 2}, {"type": "text", "text": "Train loss. By defining $X^{\\top}=[x_{1},\\ldots,x_{n}]$ and $\\boldsymbol{Y}^{\\top}=[y_{1},\\ldots,y_{n}]$ , the loss function is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left({{\\mathbf{W}}_{1}},{{\\mathbf{W}}_{2}}\\right)=\\frac{1}{2n}{{\\|{X\\mathbf{W}_{1}}{\\mathbf{W}}_{2}-{Y}\\|}^{2}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For simplicity, we adjust for the normalization factor $n$ by rescaling the data to $(X,Y)\\ \\leftarrow$ $\\left({X}/{\\sqrt{n}},{\\bar{Y}}/{\\sqrt{n}}\\right)$ , thereby implicitly considering it in the loss function without directly mentioning $n$ in the formula. Note that the loss is non-convex in $\\mathbf{W}_{1},\\mathbf{W}_{2}$ . ", "page_idx": 2}, {"type": "text", "text": "Gradient flow. The dynamics induced in parameter space by running GF on Equation (3.1) is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{W}_{1}=-\\nabla_{\\mathbf{W}_{1}}\\mathcal{L}\\left(\\mathbf{W}_{1},\\mathbf{W}_{2}\\right)\\mathrm{d}t=X^{\\top}(Y-X\\mathbf{W}_{1}\\mathbf{W}_{2})\\mathbf{W}_{2}^{\\top}\\mathrm{d}t,}\\\\ {\\mathrm{d}\\mathbf{W}_{2}=-\\nabla_{\\mathbf{W}_{2}}\\mathcal{L}\\left(\\mathbf{W}_{1},\\mathbf{W}_{2}\\right)\\mathrm{d}t=\\mathbf{W}_{1}^{\\top}X^{\\top}(Y-X\\mathbf{W}_{1}\\mathbf{W}_{2})\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Introducing the block matrix, $\\begin{array}{r}{\\Theta=\\left[\\mathbf{W}_{1}^{\\top}\\mid\\mathbf{W}_{2}\\right]\\in\\mathbb{R}^{l\\times(p+k)}}\\end{array}$ and denoting the residual matrix by $\\mathbf{R}=X^{\\top}(Y-X\\mathbf{W}_{1}\\mathbf{W}_{2})$ , the evolution of $\\Theta$ can be written as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{d}\\mathbf{\\Theta}=\\left[\\mathbf{d}\\mathbf{W}_{1}^{\\mathrm{\\top}}\\mid\\mathbf{d}\\mathbf{W}_{2}\\right]=\\left[\\mathbf{W}_{2}\\mathbf{R}^{\\top}\\mathbf{d}t\\mid\\mathbf{W}_{1}^{\\top}\\mathbf{R}\\mathbf{d}t\\right]=\\left[\\mathbf{W}_{1}^{\\top}\\mid\\mathbf{W}_{2}\\right]\\left[\\mathbf{\\Theta}_{\\mathbf{R}^{\\top}}^{\\!0_{p\\times p}}\\!\\!\\!\\!\\!\\!\\!\\mathbf{\\Theta}_{\\mathbf{\\Theta}_{0\\times\\mathbf{k}}}^{\\mathbf{\\mathbf{R}}}\\right]\\mathbf{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The gradient flow can therefore be compactly written as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{d}\\Theta=\\Theta\\mathbf{J}\\mathbf{d}t,\\quad\\mathrm{where}\\;\\mathbf{J}=\\left[\\!\\!\\begin{array}{c c}{0_{p\\times p}}&{\\mathbf{R}}\\\\ {\\mathbf{R}^{\\top}}&{0_{k\\times k}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The gradient flow (GF), when expressed in this form, reveals an inherent multiplicative structure with respect to $\\Theta$ in the gradient of the loss. As we see in subsequent sections, this representation of the gradient flow with block matrices proves to be very convenient. ", "page_idx": 2}, {"type": "text", "text": "Label noise gradient descent. Label noise gradient descent (LNGD) is a theoretically studied alternative to SGD that mirrors its practical behavior by sharing the geometric properties of the noise Blanc et al. [2020], Damian et al. [2021]. Let $\\boldsymbol{\\dot{\\varepsilon_{t}}}\\,\\in\\,\\mathbb{R}^{n\\times k}$ , where each entry of $\\varepsilon_{t}$ is an independent Gaussian random variable. At iteration $t$ , the labels are perturbed with this Gaussian noise at an intensity $\\delta$ , i.e., $\\widetilde{Y}=Y+\\sqrt{\\delta}\\varepsilon_{t}$ . The LNGD algorithm updates the iterates with a step size $\\eta$ in the direction of the gradient computed after the labels have been perturbed, as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{W}_{1}^{t+1}=\\mathbf{W}_{1}^{t}-\\eta\\nabla_{\\mathbf{W}_{1}}\\mathcal{L}\\left(\\widetilde{Y},\\mathbf{X},\\mathbf{W}_{1}^{t},\\mathbf{W}_{2}^{t}\\right);\\quad\\mathbf{W}_{2}^{t+1}=\\mathbf{W}_{2}^{t}-\\eta\\nabla_{\\mathbf{W}_{2}}\\mathcal{L}\\left(\\widetilde{Y},\\mathbf{X},\\mathbf{W}_{1}^{t},\\mathbf{W}_{2}^{t}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, by an abuse of notation, $\\mathcal{L}\\left(Y,\\mathrm{X},\\mathbf{W}_{1},\\mathbf{W}_{2}\\right)={1}/{2{\\left\\|X\\mathbf{W}_{1}\\mathbf{W}_{2}-Y\\right\\|}^{2}}$ . The iterates can then be restructured into a block matrix: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{\\Theta}\\Theta^{t+1}=\\mathbf{\\Theta}\\Theta^{t}-\\eta\\Theta^{t}\\mathbf{J}_{t}-\\eta\\sqrt{\\delta}\\Theta^{t}\\xi_{t},\\quad\\mathrm{where~}\\xi_{t}=\\left[\\!\\!\\begin{array}{c c}{0_{p\\times p}}&{X^{\\top}\\varepsilon_{t}}\\\\ {\\varepsilon_{t}^{\\top}X}&{0_{k\\times k}}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $J_{t}$ is defined as in Equation (3.4). ", "page_idx": 3}, {"type": "text", "text": "Stochastic gradient flow (SGF). We aim to model the aforementioned LNGD in continuous time using an appropriate SDE. Stochastic continuous-time counterparts of discrete stochastic gradient algorithms are favored for their enhanced amenability to theoretical analysis. We propose the following stochastic differential equation (SDE) to model LNGD in continuous time: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\Theta=\\Theta\\left[\\mathbf{J}\\mathrm{d}t+\\sqrt{\\eta\\delta}\\mathrm{d}\\xi\\right],\\mathrm{where~}\\mathrm{d}\\xi=\\left[\\begin{array}{c c}{0_{p\\times p}}&{X^{\\top}\\mathrm{d}\\mathbf{B}_{t}}\\\\ {\\mathrm{d}\\mathbf{B}_{t}^{\\top}X}&{0_{k\\times k},}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{B}_{t}$ denotes a matrix Brownian motion in $\\mathbb{R}^{n\\times k}$ . LNGD as defined in Equation (3.5), can be interpreted as the the Euler-Maryama discretization of the above SGF with a stepsize $\\eta$ . Although the inclusion of step size in the continuous-time modeling of an SDE may seem counter-intuitive, it is a necessary component [Li et al., 2019b]. As all the terms of the SDE in Equation (3.6) are polynomial in $\\Theta$ , both the drift and diffusion terms are locally Lipschitz continuous. Hence, the solution of the SDE is uniquely defined up to the explosion time $\\tau_{\\infty}$ [see, e.g., Khasminskii, 2012]. Furthermore, the explosion time can be proven to be infinite $\\tau_{\\infty}=\\infty$ almost surely), by using that the GF does not diverge and applying the techniques outlined by Pillaud-Vivien et al. [2022, Proposition 10]. ", "page_idx": 3}, {"type": "text", "text": "Initialization. The dynamics of gradient methods on homogeneous models are significantly influenced by initialization, which determines the regime they operate in\u2014specifically, the lazy regime for large initializations and the rich regime for small ones [Chizat et al., 2019, Woodworth et al., 2020]. Thus, the scale of initialization has garnered significant interest, particularly its impact on the training of linear and non-linear networks with GD [Woodworth et al., 2020, Boursier et al., 2022]. It is observed that stochastic methods eliminate the dependence on initialization [Pesme et al., 2021]. ", "page_idx": 3}, {"type": "text", "text": "Conserved quantities and balanceness. Gradient flows follow specific conservation laws along their trajectory [Marcotte et al., 2023], maintaining characteristics of the initial conditions. For linear networks, this conservation manifests as the balanceness property [Du et al., 2018], described by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta=\\mathbf{W}_{1}^{\\top}\\mathbf{W}_{1}-\\mathbf{W}_{2}\\mathbf{W}_{2}^{\\top}=\\mathbf{W}_{1}^{\\top}(0)\\mathbf{W}_{1}(0)-\\mathbf{W}_{2}(0)\\mathbf{W}_{2}^{\\top}(0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As a result, Saxe et al. [2014], Arora et al. [2018, 2019b] have adopted balanced initialization, where $\\mathbf{\\DeltaA}(0)\\,=\\,0$ , to ensure that weight matrices remain low rank throughout the trajectory. However, unbalanced initialization do not preserve these simple low-rank structures, as aspects of the initial conditions persist. ", "page_idx": 3}, {"type": "text", "text": "In contrast, stochastic methods do not adhere to these conservation laws [Ziyin et al., 2023] and the evolution of the imbalance $\\Delta$ for SGF is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\Delta=\\mathrm{d}\\big(\\mathbf{W}_{1}^{\\top}\\mathbf{W}_{1}-\\mathbf{W}_{2}\\mathbf{W}_{2}^{\\top}\\big)=\\mathrm{tr}\\left(X X^{\\top}\\right)\\,\\mathbf{W}_{2}\\mathbf{W}_{2}^{\\top}\\mathrm{d}t-k\\,\\mathbf{W}_{1}^{\\top}X^{\\top}X\\mathbf{W}_{1}\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While there is no diffusion term in the derivative, the matrices remain stochastic and no definitive conclusions can be drawn from this. However, in the case where $k=p$ and $X^{\\top}X=\\operatorname{I}_{p}$ , it can be shown that $\\mathbf{W}_{1}^{\\top}\\mathbf{W}_{1}-\\mathbf{W}_{2}\\mathbf{W}_{2}^{\\top}\\rightarrow0$ , indicating that the stochastic noise eliminates initial imbalance. ", "page_idx": 3}, {"type": "text", "text": "Conclusion. Understanding how stochastic methods mitigate dependency on initialization requires exploring beyond the evolution of the imbalance $\\Delta$ . To this end, we identify and discuss other conserved quantities, such as the determinant of the block matrix $\\Theta^{\\top}\\Theta$ in the following sections. ", "page_idx": 3}, {"type": "text", "text": "4 Separation between Gradient Flow through determinant ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we present our first separation result between GF and SGF. While the determinant of the parameters is preserved in GF, it is driven to zero by the stochasticity of SGF, leading to a simplistic low-rank structure. ", "page_idx": 4}, {"type": "text", "text": "4.1 Determinant evolution of the gradient flow ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The theorem below demonstrates that the determinant of the parameters is preserved in gradient flow. Theorem 4.1. For the gradient flow defined in Equation (3.4), the following property holds, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{d}\\bigl(\\operatorname*{det}\\left(\\Theta^{\\top}\\Theta\\right)\\bigr)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Hence, det $\\left(\\boldsymbol{\\Theta}(t)^{\\top}\\boldsymbol{\\Theta}(t)\\right)=\\operatorname*{det}\\left(\\boldsymbol{\\Theta}_{\\mathrm{0}}^{\\top}\\boldsymbol{\\Theta}_{\\mathrm{0}}\\right)$ , where $\\Theta_{0}=\\Theta(0)$ is the initialisation at time $t=0$ . ", "page_idx": 4}, {"type": "text", "text": "The proof presented in the App. B.1, is based on straightforward computations of the derivative of the determinant and the fact that the matrix $\\mathbf{J}$ has zero trace. We note that the simplicity of the proof arises from the strategically chosen block structure of $\\Theta$ . This result would have been less straightforward with different parametrizations, which likely explains why such a simple finding appears to be novel. The theorem implies that the determinant of M along the trajectory remains equal to the determinant at initialization. If $\\Theta_{\\mathrm{0}}^{\\top}\\Theta_{\\mathrm{0}}$ is full-rank initially, meaning the determinant is non-zero, the theorem ensures that the determinant of $\\mathbf{M}$ remains non-zero. Consequently, the rank of $\\Theta$ does not diminish along the trajectory. When $l\\geq p+k$ , i.e., the hidden layer has a large width and $\\mathbf{W}_{1},\\mathbf{W}_{2}$ are initialized randomly from a Gaussian distribution, $\\Theta_{\\mathrm{0}}^{\\top}\\Theta_{\\mathrm{0}}$ has full rank almost surely. The theore\u221am also reveals some implications regarding the impact of initialization scale. Note that $\\lambda_{m i n}(A)\\leq\\;\\sqrt[n]{\\operatorname*{det}A}$ , indicating that when the scale of initialization is very small, at least one singular value of $\\Theta$ is small. ", "page_idx": 4}, {"type": "text", "text": "4.2 Determinant evolution of the stochastic gradient flow ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In contrast, the theorem presented below demonstrates that the determinant of the parameters converges to zero in stochastic gradient flow. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. For the SDE, defined in the Equation (3.6), for $t\\leq\\tau_{\\infty}$ , the following property holds for the evolution of determinant ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\bigl(\\mathrm{det}\\,\\bigl(\\Theta^{\\top}\\Theta\\bigr)\\bigr)=-2\\eta\\delta k\\mathrm{tr}\\,\\bigl(X^{\\top}X\\bigr)\\mathrm{det}\\,\\bigl(\\Theta^{\\top}\\Theta\\bigr)\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Hence, $\\begin{array}{r}{\\operatorname*{det}\\left(\\boldsymbol{\\Theta}(t)^{\\top}\\boldsymbol{\\Theta}(t)\\right)=\\operatorname*{det}\\left(\\boldsymbol{\\Theta}_{0}^{\\top}\\boldsymbol{\\Theta}_{0}\\right)\\exp\\big\\{-2\\eta\\delta k\\mathrm{tr}\\left(X^{\\top}X\\right)t\\big\\},}\\end{array}$ where $\\mathbf{{\\Theta}}_{0}$ is the initialization. ", "page_idx": 4}, {"type": "text", "text": "Although the evolution of the parameters in SGF is random, the evolution of the determinant is deterministic. The theorem highlights a striking phenomenon: the noise in SGF diminishes the determinant along the trajectory, leading to a simplification of the network over time. The larger the noise and the stepsize, the faster the determinant vanishes. The vanishing of the determinant suggests that the rank of the parameters decreases by at least one, effectively eliminating some components. It holds for any initialization of $\\Theta_{\\mathrm{0}}$ and indicates how the SGF overrides some aspects of initialization. The proof uses the fact that stochastic Brownian term in the SDE, through It\u00f4\u2019s calculus, introduces a negative drift, ultimately driving the determinant to zero (refer to B.3 for the proof). ", "page_idx": 4}, {"type": "text", "text": "Limitations. Given the large width of the hidden layer, the determinant converging to zero does not fully reveal the complexity of the situation. It merely indicates that at least one singular value is approaching zero. Furthermore, the theorem provides limited insights when the determinant is already 0 at initialization, det $\\Theta_{0}=0$ which happens whenever $l<p+k$ . Next, we explore the mechanisms behind this low-rank phenomenon, suggesting that the repulsive forces induced by stochasticity drive the spurious singular values to zero as seen in the right plot of Figure 1. ", "page_idx": 4}, {"type": "text", "text": "5 Mechanism behind the low-rank phenomenon ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we investigate the evolution of singular values under stochastic training to gain deeper insights into the low-rank phenomenon. To simplify the discussion, throughout the section ", "page_idx": 4}, {"type": "text", "text": "we consider the case where $k\\,=\\,1$ and for notational convenience, we let $\\mathbf{W}_{1}\\,=\\,\\mathbf{W},\\mathbf{W}_{2}\\,=\\,\\mathbf{a}$ .   \nAdditionally, we assume that $l\\leq p$ , however the results can be extended to any $l$ . ", "page_idx": 5}, {"type": "text", "text": "Warm-up: Comparison with diagonal networks. Let $\\mathbf{W}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ be the singular value decomposition (assuming $l\\leq p$ ). The predictor $\\beta$ can be expressed as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{W}\\mathbf{a}=\\mathbf{U}{\\Sigma}\\mathbf{V}^{\\top}\\mathbf{a}=\\mathbf{U}\\left[\\sigma\\odot\\mathbf{c}\\right],\\mathrm{where~}\\mathbf{c}=\\mathbf{V}^{\\top}\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This expression reveals a Hadamard product between $\\pmb{\\sigma}$ and $\\mathbf{c}$ , reminiscent of diagonal networks which are widely studied to understand the nonconvex dynamics of gradient algorithms [Woodworth et al., 2020, Pesme et al., 2021, Pillaud-Vivien et al., 2022]. In the context of diagonal networks, SGD is known to provably induce sparsity in predictions. Similarly, for linear networks, SGF may induce sparsity in terms of the singular value $\\sigma$ . We next derive the SDE governing the evolution of the singular values $\\Sigma$ of the weight matrix to gain a clearer understanding of the low-rank phenomenon. ", "page_idx": 5}, {"type": "text", "text": "Scalar Regression. We assume that the data is isotropic, i.e., $X=\\ensuremath{\\mathrm{I}_{p}}$ . Under these conditions, the loss function for scalar regression can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(\\mathbf{W},\\mathbf{a}\\right)=\\frac{1}{2}{\\|y-\\mathbf{W}\\mathbf{a}\\|}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We train the above objective with SGF, formulated as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{d}\\mathbf{W}=(y-\\mathbf{W}\\mathbf{a})\\mathbf{a}^{\\top}\\mathbf{d}t+\\sqrt{\\eta\\delta}\\ \\mathbf{d}\\mathbf{B}_{t}\\mathbf{a}^{\\top};\\qquad\\mathbf{d}\\mathbf{a}=\\mathbf{W}^{\\top}(y-\\mathbf{W}\\mathbf{a})\\mathbf{d}t+\\sqrt{\\eta\\delta}\\ \\mathbf{W}^{\\top}\\mathbf{d}\\mathbf{B}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{B}_{t}$ is the standard Brownian motion in $\\mathbb{R}^{p}$ . For analytical convenience, we rescale the time $t\\rightarrow t/\\eta\\delta$ and use the process $\\mathrm{d}\\mathbf{X}=1/\\eta\\delta\\bigl(y-\\mathbf{W}\\mathbf{a}\\bigr)\\mathrm{d}t+\\mathrm{d}\\mathbf{B}_{t}$ . The SGF can then be rewritten as, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{W}=\\;\\mathrm{d}\\mathbf{X}\\mathbf{a}^{\\top};\\qquad\\mathrm{d}\\mathbf{a}=\\mathbf{W}^{\\top}\\mathrm{d}\\mathbf{X}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our focus is on understanding the evolution of the singular values of the matrix $\\mathbf{W}$ . This aim is facilitated by considering the symmetric matrix $\\mathbf{M}=\\mathbf{W}^{\\top}\\mathbf{W}$ , whose eigenvalues are the squares of the singular values of $\\mathbf{W}$ . Taking the derivative of $\\mathbf{M}$ , we find ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{M}=\\mathrm{d}\\mathbf{W}^{\\top}\\mathbf{W}+\\mathbf{W}^{\\top}\\mathrm{d}\\mathbf{W}+\\mathrm{d}\\mathbf{W}^{\\top}\\mathrm{d}\\mathbf{W}=\\mathbf{a}\\mathbf{d}\\mathbf{X}^{\\top}\\mathbf{W}+\\mathbf{W}^{\\top}\\mathrm{d}\\mathbf{X}\\mathbf{a}^{\\top}+p\\mathbf{a}\\mathbf{a}^{\\top}\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that dxdy represents $d[x,y]$ for any continuous semi-martingales $x,y$ [see, e.g., Ikeda and Watanabe, 1981, chapter 3 for reference]. ", "page_idx": 5}, {"type": "text", "text": "Eigenvalues of a matrix-valued stochastic process. We leverage tools from the study of eigenvalues of matrix-valued stochastic processes [Bru, 1989, Graczyk and Ma\u0142ecki, 2013] to derive the evolution of the eigenvalues of $\\mathbf{M}$ in the theorem that follows. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.1. Let $\\mathbf{s}_{1}\\;>\\;...\\;>\\;\\mathbf{s}_{l}$ be the order of the eigenvalues of the matrix M defined by Equation (5.4). Let the collision time for the eigenvalues be defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tau=\\{\\operatorname*{inf}t:\\mathbf{s}_{i}(t)=\\mathbf{s}_{j}(t)\\,f o r\\,1\\leq i\\neq j\\leq l\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For $t\\leq\\tau$ , the eigenvalues are semi-martingales given by the solution of the following SDE ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{d}(\\mathbf{s}_{i})=p\\mathbf{c}_{i}^{2}\\;\\mathbf{d}t+\\sum_{\\stackrel{j=1}{j\\neq i}}^{l}\\frac{\\mathbf{s}_{i}\\mathbf{c}_{j}^{2}+\\mathbf{s}_{j}\\mathbf{c}_{i}^{2}}{\\mathbf{s}_{i}-\\mathbf{s}_{j}}\\mathbf{d}t+2\\sqrt{\\mathbf{s}_{i}\\mathbf{c}_{i}^{2}}\\left(\\mathbf{d}\\tilde{\\mathbf{X}}\\right)_{i}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{c}=\\mathbf{V}^{\\top}\\mathbf{a}$ and $\\left(\\mathbf{d}\\tilde{\\mathbf{X}}\\right)_{i}=1/\\eta\\delta\\left(\\left\\langle\\mathbf{u}_{i},y\\right\\rangle-\\sqrt{\\mathbf{s}_{i}\\mathbf{c}_{i}^{2}}\\right)\\mathrm{d}t+\\mathrm{d}\\varepsilon_{i}$ with $\\mathbf{u}_{i}$ being the $i^{t h}$ column of U and $(\\varepsilon_{0},\\ldots,\\varepsilon_{l-1})$ is the standard Brownian motion in $\\mathbb{R}^{l}$ . The evolution of $\\mathbf{c}_{i}$ and $\\mathbf{U}$ are presented in the appendix B.5. ", "page_idx": 5}, {"type": "text", "text": "This theorem can be interpreted as the stochastic counterpart to the evolution of eigenvalues previously described for linear networks by Arora et al. [2019c], Varre et al. [2023]. The derivation of the eigenvalues is inspired by the work of Bru [1989]. ", "page_idx": 5}, {"type": "text", "text": "The evolution of the eigenvalues features a key term highlighted in Equation (5.6) consisting of the sum of skew-symmetric elements $\\mathbf{s}_{i}\\mathbf{c}_{j}^{2}\\!+\\!\\mathbf{s}_{j}\\mathbf{c}_{i}^{\\bar{2}}\\!\\big/\\mathbf{s}_{i}\\!-\\!\\mathbf{s}_{j}$ . For a pair of indices $(i_{0},j_{0})$ with $i_{0}<j_{0}$ and thus $\\mathbf{s}_{i_{0}}>\\mathbf{s}_{j_{0}}$ , the term $\\mathbf{s}_{i_{0}}\\mathbf{c}_{j_{0}}^{2}\\!+\\!\\mathbf{s}_{j_{0}}\\mathbf{c}_{i_{0}}^{2}\\!\\Big/\\mathbf{s}_{i_{0}}\\!-\\!\\mathbf{\\dot{s}}_{j_{0}}$ positively influences the evolution of the larger eigenvalue $\\mathbf{ds}_{i_{0}}$ and negatively affects the smaller eigenvalue $\\mathrm{d}\\mathbf{s}_{j_{0}}$ . Therefore, this force is repulsive, driving the eigenvalues apart and increasing their gap. Another factor influencing the dynamics is the presence of Geometric Brownian motion, where the singular value $\\sigma_{i}$ multiplicatively influences the Brownian motion as $\\sqrt{\\mathbf{s}_{i}\\mathbf{c}_{i}^{2}}\\left(\\mathrm{d}\\tilde{\\mathbf{X}}\\right)_{i}$ , similar to what is observed in diagonal linear networks (refer to the previous discussion for similarities). This effect tends to pull the singular values toward zero. Together with the fact that $({\\bf s}_{i},{\\bf c}_{i})=(0,0)$ represents a fixed point of the dynamics, these two forces collectively push redundant singular values toward zero. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "To further understand the interplay of repulsive forces and geometric Brownian motion, we consider the evolution of the smaller singular value $\\mathbf{s}_{p}$ for $l=p$ . Using the Ito chain rule, we analyze the evolution of $\\log\\mathbf{s}_{p}$ , expressed as, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{d}(\\log\\mathbf{s}_{p})=p\\frac{\\mathbf{c}_{p}^{2}}{\\mathbf{s}_{p}}\\,\\mathrm{d}t+\\frac{1}{\\mathbf{s}_{p}}\\sum_{j=1,\\atop j\\neq p}^{p}\\frac{\\mathbf{s}_{p}\\mathbf{c}_{j}^{2}+\\mathbf{s}_{j}\\mathbf{c}_{p}^{2}}{\\mathbf{s}_{p}-\\mathbf{s}_{j}}\\mathrm{d}t-2\\frac{\\mathbf{c}_{p}^{2}}{\\mathbf{s}_{p}}+2\\sqrt{\\frac{\\mathbf{c}_{p}^{2}}{\\mathbf{s}_{p}}}\\left(\\mathrm{d}\\tilde{\\mathbf{X}}\\right)_{p}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Using that $\\mathbf{s}_{p}\\mathbf{c}_{j}^{2}\\!+\\!\\mathbf{s}_{j}\\mathbf{c}_{p}^{2}\\!\\big/\\mathbf{s}_{p}\\!-\\!\\mathbf{s}_{j}\\mathbf{\\Delta}<\\mathbf{\\delta}\\!-\\!\\mathbf{c}_{p}^{2}.$ , for all indices $j$ , the repulsive force accumulates to $-(p\\mathrm{~-~}$ $1)(\\mathbf{c}_{p}^{2}/\\mathbf{s}_{p})$ and the Ito correction term from the logarithm contributes an additional $-2(\\mathbf{c}_{p}^{2}/\\mathbf{s}_{p})$ (the GBM component) thus offsetting the positive drift of $p(\\mathbf{c}_{p}^{2}/\\mathbf{s}_{p})$ . In the case of $l\\neq p$ , considering a polynomial $x^{\\alpha}$ with an appropriate $\\alpha$ would demonstrate similar behaviour. This discussion outlines the forces at play, yet a complete characterization of the solution of the SDE Equation (5.6) remains missing. Moreover, we have not established that the eigenvalues avoid a.s. collision, i.e., the explosion time $\\tau_{\\infty}=\\infty$ which is in itself a significant challenge [Bru, 1989, Graczyk and Ma\u0142ecki, 2014]. ", "page_idx": 6}, {"type": "text", "text": "A simplified two-vector problem. To enhance our understanding of the SDE governing the evolution of the eigenvalues detailed in Equation (5.6), we consider the large noise limit. In this scenario, the process described in Equation (5.3) simplifies to a purely noise-driven process without drift: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{W}=\\;\\mathrm{d}\\mathbf{B}_{t}\\mathbf{a}^{\\top};\\qquad\\mathrm{d}\\mathbf{a}=\\mathbf{W}^{\\top}\\mathrm{d}\\mathbf{B}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This SDE exhibits notable symmetry; allowing for an analysis using a matrix with sub-sampled columns. Let $S$ be any subset of $1,\\ldots,l$ , with $(\\mathbf{w}_{i})_{i=1}^{l}$ representing the columns of W. We define $\\mathbf{W}_{S}\\,\\in\\,\\mathbb{R}^{p\\times|S|}$ as the subsampled matrix obtained by selecting columns $\\mathbf{w}_{i}$ where $\\textit{i}\\in\\textit{S}$ , and similarly, we define a subsampled vector ${\\bf a}_{S}$ by selecting the corresponding coordinates. The SDE restricted to the set $S$ is structured as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}{\\mathbf{W}_{S}}=\\;\\mathrm{d}{\\mathbf{B}_{t}}{\\mathbf{a}_{S}^{\\top}};\\qquad\\mathrm{d}{\\mathbf{a}_{S}}={\\mathbf{W}_{S}^{\\top}}\\mathrm{d}{\\mathbf{B}_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To demonstrate that the columns of $\\mathbf{W}$ align, we leverage the symmetry of the SDE by examining the restricted problem on every pair of rows $S=\\{i,j\\}$ , and proving alignment within this subset. This approach leads us to consider the two vector problem $(l\\,=\\,2)$ , where $\\mathbf{W}\\,=\\,\\left[\\mathbf{w}_{1}\\vert\\mathbf{w}_{2}\\right]$ and $\\mathbf{w}_{1},\\mathbf{w}_{2}\\in\\mathbb{R}^{p}$ , $\\mathbf{a}\\in\\mathbb{R}^{2}$ . We describe the behavior of the eigenvalues for this two-vector problem in the theorem below. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2. In the large noise limit, let $\\mathbf{s}_{0}>\\mathbf{s}_{1}$ be the eigenvalues of W, the following properties hold, for $t\\leq\\tau$ defined by $\\tau=\\{\\operatorname*{inf}t:{\\mathbf s}_{0}(t)={\\mathbf s}_{1}(t)\\}$ , ", "page_idx": 6}, {"type": "text", "text": "(a) $\\mathbf{s}_{0},\\mathbf{s}_{1}$ are greater than zero almost surely, ", "page_idx": 6}, {"type": "text", "text": "This model for $l=2$ mirrors the dynamics of the Wishart process studied by Bru [1991], motivating the exploration of the evolution of an appropriately chosen exponent of $\\mathbf{s}_{0},\\mathbf{s}_{1}$ . The first part of the theorem arises from the fact that $\\mathbf{s}_{1}^{-\\alpha}\\mathbf{s}_{2}^{-\\alpha}$ is a local continuous martingale that cannot explode to infinity in finite time. The second part highlights a clear separation between the eigenvalues: one is a sub-martingale that consistently increases in expectation, while the other is a super-martingale that diminishes (note that the eigenvalues are raised to a negative power). This dynamic, coupled with the symmetry argument, suggests that for every pair of columns, there is a component that strengthens the alignment through its increases in expectation. Refer to App. B.6 for the proof. ", "page_idx": 6}, {"type": "text", "text": "Conclusion. In this section, we derive the SDE of eigenvalues for the matrix of parameters evolving under SGF. This derivation provides deeper insights into the mechanisms contributing to low-rank behavior. Specifically, repulsive forces drive the eigenvalues apart, while the geometric Brownian motion pulls them towards zero. These forces, unique to training with SGF, highlight the regularization effects of stochastic methods compared to gradient flow. However, fully characterizing the solution of this SDE remains a challenging open problem we let as future work. ", "page_idx": 6}, {"type": "text", "text": "6 Generalization to other settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we generalize our results beyond the square loss and the label noise gradient flow. We consider the general framework of a loss function over the weight product $\\mathbf{W}_{1}\\mathbf{W}_{2}$ defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(\\mathbf{W}_{1},\\mathbf{W}_{2}\\right)=\\widehat{\\mathcal{L}}(\\mathbf{W}_{1}\\mathbf{W}_{2})=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\left[\\ell(\\mathbf{W}_{1}\\mathbf{W}_{2};x,y)\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In this framework, the loss function $\\ell$ combines the prediction loss directly with the parametrized model $f_{\\theta}$ . This approach applies, for example, to classification problems using linear networks where $\\ell$ might represent any classification loss and $f_{\\theta}=\\mathbf{W}_{1}\\mathbf{W}_{2}$ . It also directly extends to more complex architectures where $f_{\\theta}=\\sigma(\\mathbf{W}_{1}\\mathbf{W}_{2})$ for an activation function $\\sigma$ , including settings like a self-attention layer with frozen value vectors. We denote the product by $\\beta=\\mathbf{W}_{1}\\mathbf{W}_{2}$ noting it solely controls the loss. We investigate the evolution of the weight matrix determinant for a general loss across various algorithms, from gradient flow to gradient descent, and demonstrate that a similar separation occurs due to stochasticity. ", "page_idx": 7}, {"type": "text", "text": "Warm-up: Gradient flow. The gradient flow on the loss $\\mathcal{L}$ can be written as the following, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{d}\\Theta=\\Theta\\mathbf{J}\\mathrm{d}t,\\qquad\\mathrm{where}\\;\\mathbf{J}=\\left[\\!\\!\\begin{array}{c c}{0_{p\\times p}}&{-\\nabla\\widehat{\\mathcal{L}}(\\beta)\\right].}\\\\ {-\\nabla\\widehat{\\mathcal{L}}(\\beta)^{\\top}}&{0_{k\\times k}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Following a similar proof as in Theorem 4.1, we obtain that $\\operatorname{d}\\bigl(\\operatorname*{det}\\left(\\Theta^{\\top}\\Theta\\right)\\bigr)\\,=\\,0$ . For separable classification problem, the gradient flow converges to infinity [Soudry et al., 2018, Ji and Telgarsky, 2019], hence, after appropriate rescaling, the layers are aligned, as shown by Ji and Telgarsky [2019]. Next, we contrast this result with the outcomes observed in stochastic and discrete algorithms. ", "page_idx": 7}, {"type": "text", "text": "Continuous modelling of SGD. We consider the SGD algorithm with a batch size $B$ . We denote the mini-batch version of the loss functions $\\mathcal{L}$ and $\\widehat{\\mathcal{L}}$ as $\\mathcal{L}_{B}$ and $\\widehat{\\mathcal{L}}_{B}$ , respectively. The SGD update with stepsize $\\eta$ can be represented with the followin g block struc ture, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathfrak{d}^{t+1}=\\Theta^{t}-\\eta\\Theta^{t}\\mathfrak{J}^{t}-\\eta\\Theta^{t}\\xi^{t},\\quad\\mathrm{where~}\\xi^{t}=\\left[\\begin{array}{c c}{0_{p\\times p}}&{-\\left(\\nabla\\widehat{\\mathcal{L}}(\\beta)-\\nabla\\widehat{\\mathcal{L}}_{B}(\\beta)\\right)^{\\top}}\\\\ {-\\left(\\nabla\\widehat{\\mathcal{L}}(\\beta)-\\nabla\\widehat{\\mathcal{L}}_{B}(\\beta)\\right)^{\\top}}&{0_{k\\times k}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We denote the SGD noise as $g_{t}\\ =\\ \\left(\\nabla{\\widehat{\\mathcal{L}}}(\\beta)-\\nabla{\\widehat{\\mathcal{L}}}_{B}(\\beta)\\right)$ and the noise covariance as $\\textstyle\\sum_{t}{\\mathrm{~}}=$ $\\mathbb{E}\\left[g^{t}\\left(g^{t}\\right)^{\\top}\\right]$ where the expectation is over all the minibatches. Following Li et al. [2019a], the SGD update can be modelled with the following SDE, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{d}\\Theta=-\\Theta\\mathbf{J}\\mathbf{d}t-\\sqrt{\\eta}\\mathbf{d}\\xi,\\mathrm{where~}\\mathbf{d}\\xi=\\left[{\\begin{array}{c c}{0_{p\\times p}}&{-\\Sigma_{t}^{1/2}\\mathbf{d}\\mathbf{B}_{t}}\\\\ {-\\left(\\Sigma_{t}^{1/2}\\mathbf{d}\\mathbf{B}_{t}\\right)^{\\top}}&{0_{k\\times k}}\\end{array}}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The main difference with SGF is that, in overparameterized problems, the noise covariance is timevarying and decreases to zero upon convergence. Using Theorem B.3, the evolution of the determinant of $\\dot{\\mathbf{M}}\\,\\bar{=}\\,\\mathbf{\\Theta}^{\\top}\\mathbf{\\Theta}$ is given by $\\mathrm{d}(\\bar{\\mathrm{det}}\\left(\\mathbf{M}\\right))=-\\eta\\mathrm{det}\\left(\\mathbf{M}\\bar{\\mathrm{)Tr}}\\left(\\boldsymbol{\\Sigma}(t)\\right)\\mathrm{d}t$ and can be explicitly solved as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{d}(\\operatorname*{det}\\left(\\mathbf{M}\\right)(t))=\\operatorname*{det}\\left(\\mathbf{M}(0)\\right)\\!\\exp\\{-\\eta\\int_{0}^{t}\\mathrm{Tr}\\left(\\Sigma(s)\\right)\\!\\mathrm{d}s\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Hence, the decay in the determinant is governed by the integral $\\int_{0}^{\\infty}\\mathrm{Tr}\\left(\\Sigma(t)\\right)\\mathrm{d}t$ which is a stochastic quantity. $\\operatorname{Tr}\\left(\\Sigma(t)\\right)$ represents the strength of the stochastic noise, which, in over-parameterized regression, is proportional to the loss, i.e., $\\mathrm{Tr}\\left(\\Sigma(t)\\right)\\propto\\mathcal{L}\\left(\\boldsymbol{\\Theta}\\right)$ [Pesme et al., 2021]. Therefore, the rate of decay in the determinant depends on $\\begin{array}{r}{\\int_{0}^{\\infty}\\mathcal{L}\\left((\\pmb{\\Theta}(t))\\right)\\mathrm{d}t,}\\end{array}$ , with slower convergence leading to a simpler model at convergence, as observed in the case of diagonal networks by Pesme et al. [2021]. The result above also holds for non-separable classification tasks where the noise of SGD drives the determinant to 0, a scenario not covered by the previous analysis of Ji and Telgarsky [2019]. ", "page_idx": 7}, {"type": "text", "text": "Discrete gradient algorithms. We can extend the previous results to discrete (possibly stochastic) gradient algorithm. Both algorithms can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{t+1}=\\Theta_{t}\\left(\\operatorname{I}_{p+k}+\\eta\\mathbf{J}_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for stepsize $\\eta$ and $\\mathbf{J}_{t}$ the possibly stochastic block gradient matrix defined in Equation (6.1). In the context of discrete algorithms, the determinant is controlled by the following lemma (refer to B.4 for the proof). ", "page_idx": 7}, {"type": "image", "img_path": "TSaieShX3j/tmp/c4aa45214215cce8465ebb2c3def78c76f08210e7940c7be6c200c7bbde7d4c7.jpg", "img_caption": ["Figure 1: Evolution of the model characteristics for gradient flow $\\overset{\\vartriangle}{\\boldsymbol{\\delta}}=\\boldsymbol{0}$ ) and stochastic gradient flow $\\mathit{\\Theta}^{'}\\delta=2$ ). Left: Determinant of M. Right: Top-5 singular values of $\\mathbf{W}_{1}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "TSaieShX3j/tmp/937a36258785e5415c4e29d2bef9f98c469489e06b68026965d46e4e96621b24.jpg", "img_caption": ["Figure 2: Evolution of the top-5 singular values of $\\mathbf{W}_{1}$ for SGD with small and large stepsizes $\\eta$ . Left: Regression with MSE loss, linear network. Middle: Classification with logistic loss, linear network. Right: Regression with MSE loss, 2-layer ReLU network. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Lemma 6.1. When $l=p+k$ and $\\eta^{2}\\big\\|\\mathbf J_{t}\\big\\|_{F}^{2}\\leq1,$ , the following property holds for the determinant, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\big\\vert\\operatorname*{det}\\Theta_{t+1}\\big\\vert\\leq\\exp\\biggl(-\\frac{\\eta^{2}}{2}\\big\\|\\mathbf{J}_{t}\\big\\|_{F}^{2}\\biggr)\\vert\\operatorname*{det}\\Theta_{t}\\vert.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "If the factor $\\eta^{2}\\big\\|\\mathbf{J}_{t}\\big\\|_{F}^{2}\\leq1$ at every iteration $t$ , the determinant is reduced by the discrete step size. However, there is a tradeoff: the sum $\\begin{array}{r}{S:=\\sum_{t=0}^{\\infty}\\eta^{2}\\big\\|\\mathbf{J}_{t}\\big\\|_{F}^{2}}\\end{array}$ can be finite, indicating that it does not completely drive the determinant to zero. Increasing $\\eta$ to increase $S$ might lead to instability and divergence. Furthermore, since $\\left\\|\\mathbf{J}_{t}\\right\\|_{F}^{2}\\propto\\mathcal{L}\\left(\\Theta_{t}\\right)$ , there is an additional tradeoff between convergence and the simplicity of the parameters. This illustrates how step sizes that produce non-convergent training loss patterns, such as the catapult effect [Lewkowycz et al., 2020] or the edge of stability mechanisms [Cohen et al., 2020], can simplify the network\u2019s parameters. ", "page_idx": 8}, {"type": "text", "text": "7 Experimental evidence ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We consider a regression problem on synthetic data with $n=1000$ samples of Gaussian data in $\\mathbb{R}^{5}$ $(p=5)$ with labels in $\\mathbb{R}^{2}$ $\\;k=2)$ ) generated by some ground truth $\\bar{\\boldsymbol{\\beta}}\\in\\mathbb{R}^{5\\times2}$ , the width of the network is $l=10$ . We use Gaussian initialization of the network parameters with entries from $\\mathcal{N}(0,1)$ . Experiments details can be found in the appendix C. In the left plot of Figure 1, we show the time evolution of the determinant of matrix M. As suggested by theorems 4.1 and 4.2, in the case without label noise, det $\\left(\\Theta^{\\top}\\Theta\\right)$ stays constant, while with the Label Noise of intensity $\\delta=2$ it goes to zero with time. In  the right plot of Figure 1, we demonstrate the time evolution of the top-5 singular values of the matrix $\\mathbf{W}_{1}$ . Note that in the case of Gradient Flow all except the first $k$ singular values $\\overline{{\\sigma_{0}}}$ and $\\sigma_{1}$ ) stay at the same scale, while adding Label Noise forces smallest $d+l-k$ singular values $(\\sigma_{2},\\sigma_{3}$ , and $\\sigma_{4}$ ) to tend toward zero. Further experiments illustrate in Figure 2 the evolution of singular values of parameter matrix $\\mathbf{W}_{1}$ when optimized with SGD, for classification tasks and with ReLU network. These results also confirm that the beneficial effects of stochasticity hold in these contexts. ", "page_idx": 8}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we demonstrate a distinct separation between GF and SGF when trained on linear networks. This separation is obtained by tracking the evolution of the determinant of the parameter matrix. However, while the determinant is a significant factor, it does not fully capture the implicit regularization effects. Notably, the determinant mirrors the imbalance $\\mathbf{u}^{2}-\\mathbf{v}^{2}$ in diagonal networks represented by u $\\odot\\,\\mathbf{v}$ , whose dynamics play a crucial role in attuning the implicit regularization across various algorithms [Woodworth et al., 2020, Pesme et al., 2021, Papazov et al., 2024]. Our analysis presents the initial step in deciphering implicit regularization for stochastic methods in linear networks, yet achieving a complete characterization remains a promising direction for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "AV is supported by Swiss data science fellowship. This work was supported by the Swiss National Science Foundation (grant number 212111). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "M. Andriushchenko, A. Varre, L. Pillaud-Vivien, and N. Flammarion. Sgd with large step sizes learns sparse features. arXiv preprint arXiv:2210.05337, 2022.   \nM. Andriushchenko, F. Croce, M. M\u00fcller, M. Hein, and N. Flammarion. A modern look at the relationship between sharpness and generalization. In International Conference on Machine Learning, pages 840\u2013902. PMLR, 2023.   \nS. Arora, N. Cohen, and E. Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In Proceedings of the 35th International Conference on Machine Learning, 2018.   \nS. Arora, N. Cohen, N. Golowich, and W. Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019a. URL https://openreview.net/forum?id $\\equiv$ SkMQg3C5K7.   \nS. Arora, N. Cohen, W. Hu, and Y. Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019b.   \nS. Arora, N. Cohen, W. Hu, and Y. Luo. Implicit regularization in deep matrix factorization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019c. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf.   \nY. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 2013.   \nG. Blanc, N. Gupta, G. Valiant, and P. Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In Conference on Learning Theory, COLT 2020, Proceedings of Machine Learning Research. PMLR, 2020.   \nE. Boursier, L. Pillaud-Vivien, and N. Flammarion. Gradient flow dynamics of shallow reLU networks for square loss and orthogonal inputs. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id $\\equiv$ L74c-iUxQ1I.   \nL. Braun, C. C. J. Domin\u00e9, J. E. Fitzgerald, and A. M. Saxe. Exact learning dynamics of deep linear networks with prior knowledge. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id $\\equiv$ lJx2vng-KiC.   \nM.-F. Bru. Diffusions of perturbed principal component analysis. Journal of Multivariate Analysis, 29(1):127\u2013136, 1989. ISSN 0047-259X. doi: https://doi.org/10.1016/0047-259X(89)90080-8. URL https://www.sciencedirect.com/science/article/pii/0047259X89900808.   \nM.-F. Bru. Wishart processes. Journal of Theoretical Probability, 4:725\u2013751, 1991.   \nF. Chen, D. Kunin, A. Yamamura, and S. Ganguli. Stochastic collapse: How gradient noise attracts SGD dynamics towards simpler subnetworks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\cdot^{=}$ iFxWrxDekd.   \nL. Chizat, E. Oyallon, and F. Bach. On Lazy Training in Differentiable Programming. Curran Associates Inc., Red Hook, NY, USA, 2019.   \nJ. Cohen, S. Kaur, Y. Li, J. Z. Kolter, and A. Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In International Conference on Learning Representations, 2020.   \nA. Damian, T. Ma, and J. D. Lee. Label noise sgd provably prefers flat global minimizers. Advances in Neural Information Processing Systems, 34:27449\u201327461, 2021.   \nS. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018.   \nF. J. Dyson. A Brownian-Motion Model for the Eigenvalues of a Random Matrix. Journal of Mathematical Physics, 3(6):1191\u20131198, 11 1962. ISSN 0022-2488. doi: 10.1063/1.1703862. URL https://doi.org/10.1063/1.1703862.   \nM. Even, S. Pesme, S. Gunasekar, and N. Flammarion. (s) gd over diagonal linear networks: Implicit regularisation, large stepsizes and edge of stability. Advances in Neural Information Processing Systems, 2023.   \nK. Fukumizu. Effect of batch learning in multilayer neural networks. Gen, 1(04):1E\u201303, 1998.   \nN. Ghosh, S. Frei, W. Ha, and B. Yu. The effect of sgd batch size on autoencoder learning: Sparsity, sharpness, and feature learning. arXiv preprint arXiv:2308.03215, 2023.   \nG. Gidel, F. Bach, and S. Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. Advances in Neural Information Processing Systems, 32, 2019.   \nP. Graczyk and J. Ma\u0142ecki. Multidimensional yamada-watanabe theorem and its applications to particle systems. Journal of Mathematical Physics, 54(2), 2013.   \nP. Graczyk and J. Ma\u0142ecki. Strong solutions of non-colliding particle systems. 2014.   \nS. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regularization in matrix factorization. Advances in Neural Information Processing Systems, 30, 2017.   \nJ. Z. HaoChen, C. Wei, J. D. Lee, and T. Ma. Shape matters: Understanding the implicit bias of the noise covariance. In M. Belkin and S. Kpotufe, editors, Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA, 2021.   \nC. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del R\u00edo, M. Wiebe, P. Peterson, P. G\u00e9rard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. Nature, 585(7825):357\u2013362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10. 1038/s41586-020-2649-2.   \nF. He, T. Liu, and D. Tao. Control batch size and learning rate to generalize well: Theoretical and empirical evidence. In Advances in Neural Information Processing Systems, volume 32, 2019.   \nS. Hochreiter and J. Schmidhuber. Flat minima. Neural Comput., 9(1):1\u201342, jan 1997.   \nE. Hoffer, I. Hubara, and D. Soudry. Train longer, generalize better: Closing the generalization gap in large batch training of neural networks. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 1729\u20131739, 2017.   \nN. Ikeda and S. Watanabe. Stochastic differential equations and diffusion processes, volume 24 of North-Holland Mathematical Library. North-Holland Publishing Co., Amsterdam, 1981. ISBN 0-444-86172-6.   \nS. Jastrzebski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, A. Storkey, and Y. Bengio. Three factors influencing minima in SGD. In International Conference on Learning Representations, 2018.   \nZ. Ji and M. Telgarsky. Gradient descent aligns the layers of deep linear networks. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= HJflg30qKX.   \nN. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2017.   \nR. Khasminskii. Stochastic Stability of Differential Equations, volume 66 of Stochastic Modelling and Applied Probability. Springer, Heidelberg, second edition, 2012.   \nB. Kleinberg, Y. Li, and Y. Yuan. An alternative view: When does SGD escape local minima? In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2698\u20132707. PMLR, 10\u201315 Jul 2018.   \nA. Lewkowycz, Y. Bahri, E. Dyer, J. Sohl-Dickstein, and G. Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218, 2020.   \nQ. Li, C. Tai, and W. E. Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations. Journal of Machine Learning Research, 20(40):1\u201347, 2019a.   \nQ. Li, C. Tai, and W. E. Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations. Journal of Machine Learning Research, 20(40):1\u201347, 2019b. URL http://jmlr.org/papers/v20/17-526.html.   \nZ. Li, T. Wang, and S. Arora. What happens after sgd reaches zero loss?\u2013a mathematical framework. In International Conference on Learning Representations, 2021.   \nS. Mandt, M. D. Hoffman, and D. M. Blei. A variational analysis of stochastic gradient algorithms. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, page 354\u2013363, 2016.   \nS. Marcotte, R. Gribonval, and G. Peyr\u00e9. Abide by the law and follow the flow: conservation laws for gradient flows. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $=$ kMueEV8Eyy.   \nE. Mayerhofer, O. Pfaffel, and R. Stelzer. On strong solutions for positive definite jump diffusions. Stochastic processes and their applications, 121(9):2072\u20132086, 2011.   \nH. Min, S. Tarmoun, R. Vidal, and E. Mallada. On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks. In Proceedings of the 38th International Conference on Machine Learning, 2021.   \nJ. R. Norris, L. C. G. Rogers, and D. Williams. Brownian motions of ellipsoids. Transactions of the American Mathematical Society, 294(2):757\u2013765, 1986. ISSN 00029947. URL http: //www.jstor.org/stable/2000214.   \nH. Papazov, S. Pesme, and N. Flammarion. Leveraging continuous time to understand momentum when training diagonal linear networks. In International Conference on Artificial Intelligence and Statistics, pages 3556\u20133564. PMLR, 2024.   \nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 8024\u20138035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.   \nS. Pesme, L. Pillaud-Vivien, and N. Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity. Advances in Neural Information Processing Systems, 34: 29218\u201329230, 2021.   \nL. Pillaud-Vivien, J. Reygner, and N. Flammarion. Label noise (stochastic) gradient descent implicitly solves the lasso for quadratic parametrisation. In Conference on Learning Theory, pages 2127\u20132159. PMLR, 2022.   \nA. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In 2nd International Conference on Learning Representations, ICLR 2014, 2014.   \nA. M. Saxe, J. L. McClelland, and S. Ganguli. A mathematical theory of semantic development in deep neural networks. Proceedings of the National Academy of Sciences, 116(23):11537\u201311546, 2019.   \nD. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70):1\u201357, 2018.   \nS. Tarmoun, G. Franca, B. D. Haeffele, and R. Vidal. Understanding the dynamics of gradient flow in overparameterized linear models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10153\u201310161. PMLR, 18\u201324 Jul 2021.   \nJ. Townsend. Differentiating the singular value decomposition. Technical report, Technical Report 2016, https://j-towns. github. io/papers/svd-derivative ..., 2016.   \nG. Van Rossum and F. L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA, 2009. ISBN 1441412697.   \nA. V. Varre, M.-L. Vladarean, L. Pillaud-Vivien, and N. Flammarion. On the spectral bias of two-layer linear networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $=$ FFdrXkm3Cz.   \nA. V. Varre, M.-L. Vladarean, L. Pillaud-Vivien, and N. Flammarion. On the spectral bias of two-layer linear networks. Advances in Neural Information Processing Systems, 36, 2024.   \nZ. Wang and A. Jacot. Implicit bias of sgd in $l_{2}$ -regularized linear dnns: One-way jumps from high to low rank, 2023.   \nB. Woodworth, S. Gunasekar, J. D. Lee, E. Moroshko, P. Savarese, I. Golan, D. Soudry, and N. Srebro. Kernel and rich regimes in overparametrized models. In J. Abernethy and S. Agarwal, editors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages 3635\u20133673. PMLR, 09\u201312 Jul 2020.   \nL. Wu, C. Ma, and W. E. How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective. In Advances in Neural Information Processing Systems, volume 31, 2018.   \nL. Ziyin, H. Li, and M. Ueda. Law of balance and stationary distribution of stochastic gradient descent. arXiv preprint arXiv:2308.06671, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Notation $S_{d},S_{d}^{+},S_{d}^{++}$ denote the set of symmetric, positive semi-definite and positive definite matrices in $R^{d\\times d}$ . We use $\\odot$ to denote the Hadamard product. ", "page_idx": 13}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem B.1. For the gradient flow defined in Equation (3.4), the following property holds, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{d}\\bigl(\\operatorname*{det}\\left(\\Theta^{\\top}\\Theta\\right)\\bigr)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, $\\operatorname*{det}\\left(\\boldsymbol{\\Theta}(t)^{\\top}\\boldsymbol{\\Theta}(t)\\right)=\\operatorname*{det}\\left(\\boldsymbol{\\Theta}_{\\mathrm{0}}^{\\top}\\boldsymbol{\\Theta}_{\\mathrm{0}}\\right)$ , where $\\Theta_{0}=\\Theta(0)$ is the initialisation at time $t=0$ . ", "page_idx": 13}, {"type": "text", "text": "First, we present a proof of this theorem, based on straightforward computations of the derivative of the determinant and the fact that the matrix $\\mathbf{J}$ has zero trace. ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $\\mathbf{M}=\\Theta^{\\top}\\Theta$ . The dynamics of $\\mathbf{M}$ are governed by the ODE, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{dM}=\\mathbf{d}\\mathbf{\\Theta}^{\\top}\\boldsymbol{\\Theta}+\\mathbf{\\Theta}^{\\top}\\mathbf{d}\\boldsymbol{\\Theta}=\\mathbf{\\Theta}^{\\top}\\boldsymbol{\\Theta}\\mathbf{J}\\mathbf{d}t+\\mathbf{J}\\mathbf{\\Theta}^{\\top}\\boldsymbol{\\Theta}\\mathbf{d}t=(\\mathbf{M}\\mathbf{J}+\\mathbf{J}\\mathbf{M})\\mathbf{d}t.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using the gradient of the determinant given in Proposition B.2, the determinant of $\\mathbf{M}$ evolves as follows, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}(\\operatorname*{det}{(\\mathbf{M})})=\\langle\\nabla\\mathrm{det}\\left(\\mathbf{M}\\right),\\mathrm{d}\\mathbf{M}\\rangle=\\operatorname*{det}\\left(\\mathbf{M}\\right)\\langle\\mathbf{M}^{-1},\\mathbf{M}\\mathbf{J}+\\mathbf{J}\\mathbf{M}\\rangle\\,\\mathrm{d}t,}\\\\ &{\\qquad\\qquad=\\operatorname*{det}\\left(\\mathbf{M}\\right)\\langle\\mathbf{M}^{-1},\\mathbf{M}\\mathbf{J}\\rangle+\\langle\\mathbf{M}^{-1},\\mathbf{J}\\mathbf{M}\\rangle=2\\mathrm{det}\\left(\\mathbf{M}\\right)\\langle\\mathbf{I}_{p+k},\\mathbf{J}\\rangle=2\\mathrm{det}\\left(\\mathbf{M}\\right)\\mathrm{Tr}\\left(\\mathbf{J}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Given that ${\\mathrm{Tr}}\\left(\\mathbf{J}\\right)=0$ , it follows that $\\mathbf{d}(\\operatorname*{det}\\left(\\mathbf{M}\\right))=0$ . ", "page_idx": 13}, {"type": "text", "text": "Proposition B.2. For any matrix M in $S_{d}^{++}$ , the first two derivatives of the determinant of M, denoted by det $(M)$ are the following ", "page_idx": 13}, {"type": "equation", "text": "$$\n(i)\\,\\,\\,\\nabla\\mathrm{det}\\left(M\\right)=\\mathrm{det}\\left(M\\right)M^{-1}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(ii) For $1\\leq a,b,k,l\\leq d,$ , the second order partial derivative is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\mathrm{det}\\left(M\\right)}{\\partial M_{a b}\\partial M_{k l}}=\\mathrm{det}\\left(M\\right)\\left[(M^{-1})_{b a}(M^{-1})_{l k}-(M^{-1})_{b k}(M^{-1})_{l a}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Theorem B.3. For a stochastic process given by the $S D E$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{d}\\Theta=\\Theta\\left[\\mathbf{J}\\mathrm{d}t+\\mathrm{d}\\xi\\right]\\,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with $\\operatorname{Tr}\\mathbf{J}=\\operatorname{Tr}\\xi=0$ , the determinant of the $\\mathbf{M}=\\Theta^{\\top}\\Theta$ evolves as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{d}(\\mathrm{det}\\left(\\mathbf{M}\\right))=-\\mathrm{det}\\left(\\mathbf{M}\\right)\\mathrm{Tr}\\left[\\mathrm{d}\\xi\\mathrm{d}\\xi\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. First, we compute the evolution of $\\mathbf{M}=\\Theta^{\\top}\\Theta$ using the Ito\u2019s product rule, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{dM}=\\mathbf{d}\\big(\\Theta^{\\top}\\Theta\\big)=\\mathbf{d}\\Theta^{\\top}\\Theta+\\Theta^{\\top}\\mathbf{d}\\Theta+\\mathbf{d}\\Theta^{\\top}\\mathbf{d}\\Theta\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The last term is interpreted as a derivative of the finite variation and it should be computed using d $\\dot{\\mathbf{\\rho}}_{\\cdot}\\left(\\mathbf{dB}_{t}\\right)_{i j}=0$ and $\\left(\\mathbf{\\bar{d}}\\mathbf{B}_{t}\\right)_{i j}.\\left(\\mathbf{d}\\mathbf{B}_{t}\\right)_{k l}=\\delta_{i=k\\wedge j=l}\\mathbf{d}t$ . Using Eq. (3.6), ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{M}=[\\mathbf{J}\\mathbf{d}t+\\mathbf{d}\\boldsymbol{\\xi}]\\boldsymbol{\\Theta}^{\\top}\\boldsymbol{\\Theta}+\\boldsymbol{\\Theta}^{\\top}\\boldsymbol{\\Theta}[\\mathbf{J}\\mathbf{d}t+\\mathbf{d}\\boldsymbol{\\xi}]+\\mathbf{d}\\boldsymbol{\\xi}\\boldsymbol{\\Theta}^{\\top}\\boldsymbol{\\Theta}\\mathrm{d}\\boldsymbol{\\xi},}\\\\ {=\\mathbf{J}\\mathbf{M}\\mathbf{d}t+\\mathbf{M}\\mathbf{J}\\mathbf{d}t+\\mathbf{d}\\boldsymbol{\\xi}\\mathbf{M}\\mathbf{d}\\boldsymbol{\\xi}+\\mathbf{d}\\boldsymbol{\\xi}\\mathbf{M}+\\mathbf{M}\\mathbf{d}\\boldsymbol{\\xi}.\\phantom{x x x x x x x x x x x x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using the Ito chain rule, we can compute the evolution of determinant as following, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{d}(\\operatorname*{det}\\left(\\mathbf{M}\\right))=\\left\\langle\\nabla\\mathrm{det}\\left(\\mathbf{M}\\right),\\mathrm{d}\\mathbf{M}\\right\\rangle+\\frac{1}{2}\\sum_{a,b,k,l}\\frac{\\partial^{2}\\mathrm{det}\\left(\\mathbf{M}\\right)}{\\partial\\mathbf{M}_{a b}\\partial\\mathbf{M}_{k l}}\\mathrm{d}\\mathbf{M}_{a b}\\mathrm{d}\\mathbf{M}_{k l},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The first term is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\nabla\\mathrm{det}\\left(\\mathbf{M}\\right),\\mathrm{d}\\mathbf{M}\\right\\rangle=\\operatorname*{det}\\left(\\mathbf{M}\\right)\\left\\langle\\mathbf{M}^{-1},\\mathbf{J}\\mathbf{M}\\mathrm{d}t+\\mathbf{M}\\mathbf{J}\\mathrm{d}t+\\mathrm{d}\\xi\\mathbf{M}\\mathrm{d}\\xi+\\mathrm{d}\\xi\\mathbf{M}+\\mathbf{M}\\mathrm{d}\\xi\\right\\rangle,}\\\\ &{\\qquad\\qquad\\qquad=2d e t(\\mathbf{M})\\left\\langle\\mathrm{I}_{p+k},\\mathbf{J}\\right\\rangle\\mathrm{d}t+2\\operatorname*{det}\\left(\\mathbf{M}\\right)\\left\\langle\\mathrm{I}_{p+k},\\mathrm{d}\\xi\\right\\rangle+\\operatorname*{det}\\left(\\mathbf{M}\\right)\\left\\langle\\mathbf{M}^{-1},\\mathrm{d}\\xi\\mathbf{M}\\xi\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the property that $\\operatorname{Tr}\\left(\\mathbf{J}\\right)=\\operatorname{Tr}\\left(\\mathbf{d}\\boldsymbol{\\xi}\\right)=0$ . We get that $\\left\\langle\\nabla\\operatorname*{det}\\left(\\mathbf{M}\\right),\\operatorname{d}\\!\\mathbf{M}\\right\\rangle=\\left\\langle\\mathbf{M}^{-1},\\operatorname{d}\\!\\xi\\mathbf{M}\\xi\\right\\rangle$ . For the second term ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{2}\\sum_{a,b,k,l}\\frac{\\partial^{2}\\operatorname*{det}{(\\mathbf{M})}}{\\partial{\\mathbf{M}_{a b}}\\partial{\\mathbf{M}_{k l}}}\\mathrm{d}{\\mathbf{M}_{a b}}\\mathrm{d}{\\mathbf{M}_{k l}}=\\frac{1}{2}\\sum_{a,b,k,l}\\operatorname*{det}{\\mathbf{M}\\left[(\\mathbf{M}^{-1})_{b a}(\\mathbf{M}^{-1})_{l k}-(\\mathbf{M}^{-1})_{b k}(\\mathbf{M}^{-1})_{l a}\\right]}\\,\\mathrm{d}{\\mathbf{M}_{a b}}\\mathrm{d}{\\mathbf{M}_{a b}}}\\\\ &{=\\frac{d e t(\\mathbf{M})}{2}\\sum_{a,b,k,l}\\left[(\\mathbf{M}^{-1})_{b a}(\\mathbf{M}^{-1})_{l k}\\right]\\mathrm{d}{\\mathbf{M}_{a b}}\\mathrm{d}{\\mathbf{M}_{k l}}}\\\\ &{\\qquad\\qquad-\\sum_{a,b,k,l}\\left[(\\mathbf{M}^{-1})_{b k}(\\mathbf{M}^{-1})_{l a}\\right]\\mathrm{d}{\\mathbf{M}_{a b}}\\mathrm{d}{\\mathbf{M}_{k l}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rearranging the terms in the summation, we get, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\mathbf{\\Phi},b,k,l}\\left[(\\mathbf{M}^{-1})_{b a}(\\mathbf{M}^{-1})_{l k}\\right]\\mathrm{d}\\mathbf{M}_{a b}\\mathrm{d}\\mathbf{M}_{k l}=\\displaystyle\\sum_{a,b,k,l}\\left[(\\mathbf{M}^{-1})_{b a}\\mathrm{d}\\mathbf{M}_{a b}\\right]\\left[(\\mathbf{M}^{-1})_{l k}\\mathrm{d}\\mathbf{M}_{k l}\\right],}\\\\ {\\displaystyle=\\sum_{b,l}\\left[\\sum_{a}(\\mathbf{M}^{-1})_{b a}\\mathrm{d}\\mathbf{M}_{a b}\\right]\\left[\\sum_{k}(\\mathbf{M}^{-1})_{l k}\\mathrm{d}\\mathbf{M}_{k l}\\right],}\\\\ {\\displaystyle}&{=\\displaystyle\\sum_{b,l}\\left(\\mathbf{M}^{-1}\\mathrm{d}\\mathbf{M}\\right)_{b b}\\left(\\mathbf{M}^{-1}\\mathrm{d}\\mathbf{M}\\right)_{l l}=\\displaystyle\\sum_{b}\\left(\\mathbf{M}^{-1}\\mathrm{d}\\mathbf{M}\\right)_{b b}\\sum_{l}\\left(\\mathbf{M}^{-1}\\mathrm{d}\\mathbf{M}\\right)_{b}}\\\\ {\\displaystyle}&{=\\mathrm{Tr}\\left(\\mathbf{M}^{-1}\\mathrm{d}\\mathbf{M}\\right)\\mathrm{Tr}\\left(\\mathbf{M}^{-1}\\mathrm{d}\\mathbf{M}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly for the other term, we get, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\substack{,b,k,l}}\\left[(\\mathbf{M}^{-1})_{b k}(\\mathbf{M}^{-1})_{l a}\\right]\\mathrm{d}\\mathbf{M}_{a b}\\mathbf{d}\\mathbf{M}_{k l}=\\displaystyle\\sum_{a,b,k,l}\\left[(\\mathbf{M}^{-1})_{b k}\\mathbf{d}\\mathbf{M}_{k l}\\right]\\left[(\\mathbf{M}^{-1})_{l a}\\mathbf{d}\\mathbf{M}_{a b}\\right],}\\\\ {\\displaystyle=\\sum_{b,l}\\left[\\sum_{a}(\\mathbf{M}^{-1})_{b a}\\mathbf{d}\\mathbf{M}_{a l}\\right]\\left[\\sum_{k}(\\mathbf{M}^{-1})_{b k}\\mathbf{d}\\mathbf{M}_{k l}\\right],}\\\\ {\\displaystyle}&{=\\displaystyle\\sum_{b}\\left[\\sum_{l}\\left(\\mathbf{M}^{-1}\\mathbf{d}\\mathbf{M}\\right)_{b l}\\left(\\mathbf{M}^{-1}\\mathbf{d}\\mathbf{M}\\right)_{l b}\\right]=\\sum_{b}\\left(\\mathbf{M}^{-1}\\mathbf{d}\\mathbf{M}\\mathbf{M}^{-1}\\right)\\mathbf{M}_{a b}\\left(\\mathbf{M}^{-1}\\mathbf{d}\\mathbf{M}\\right),}\\\\ {\\displaystyle}&{=\\mathrm{Tr}\\left[\\left(\\mathbf{M}^{-1}\\mathbf{d}\\mathbf{M}\\right)\\left(\\mathbf{M}^{-1}\\mathbf{d}\\mathbf{M}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that the diffusion part of $\\mathbf{M}^{-1}\\mathrm{d}\\mathbf{M}$ is $\\mathrm{d}\\xi+\\mathbf{M}^{-1}\\mathrm{d}\\xi\\mathbf{M}$ . Using this ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Tr}\\left(\\mathbf{M}^{-1}\\mathrm{d}\\mathbf{M}\\right)\\mathrm{Tr}\\left(\\mathbf{M}^{-1}\\mathrm{d}\\mathbf{M}\\right)=\\mathrm{Tr}\\left[\\mathrm{d}\\boldsymbol{\\xi}+\\mathbf{M}^{-1}\\mathrm{d}\\boldsymbol{\\xi}\\mathbf{M}\\right]\\mathrm{Tr}\\left[\\mathrm{d}\\boldsymbol{\\xi}+\\mathbf{M}^{-1}\\mathrm{d}\\boldsymbol{\\xi}\\mathbf{M}\\right]=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as $\\operatorname{Tr}\\mathrm{d}\\xi=0$ . For the other term, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}\\left[\\left(\\mathbf{M}^{-1}\\mathbf{dM}\\right)\\left(\\mathbf{M}^{-1}\\mathbf{dM}\\right)\\right]=\\mathrm{Tr}\\left[\\left(\\mathbf{d}\\boldsymbol{\\xi}+\\mathbf{M}^{-1}\\mathbf{d}\\boldsymbol{\\xi}\\mathbf{M}\\right)\\left(\\mathbf{d}\\boldsymbol{\\xi}+\\mathbf{M}^{-1}\\mathbf{d}\\boldsymbol{\\xi}\\mathbf{M}\\right)\\right],}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2\\mathrm{Tr}\\left[\\mathbf{d}\\boldsymbol{\\xi}\\mathbf{d}\\boldsymbol{\\xi}\\right]+2\\mathrm{Tr}\\left[\\mathbf{M}^{-1}\\mathbf{d}\\boldsymbol{\\xi}\\mathbf{M}\\mathbf{d}\\boldsymbol{\\xi}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Putting everything together, we get, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{a,b,k,l}\\frac{\\partial^{2}\\mathrm{det}\\left(\\mathbf{M}\\right)}{\\partial\\mathbf{M}_{a b}\\partial\\mathbf{M}_{k l}}=-\\mathrm{det}\\,\\mathbf{M}\\left(\\mathrm{Tr}\\left[\\mathbf{d}\\xi\\mathbf{d}\\xi\\right]+\\mathrm{Tr}\\left[\\mathbf{M}^{-1}\\mathbf{d}\\xi\\mathbf{M}\\mathbf{d}\\xi\\right]\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which gives us ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{d}(\\mathrm{det}\\left(\\mathbf{M}\\right))=-\\mathrm{det}\\left(\\mathbf{M}\\right)\\,\\mathrm{Tr}\\left[\\mathrm{d}\\xi\\mathrm{d}\\xi\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma B.4. When ${\\boldsymbol{l}}=p+{\\boldsymbol{k}}$ and $\\eta^{2}\\big\\|\\mathbf{J}_{t}\\big\\|_{F}^{2}\\leq1$ , the following property holds for the determinant, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\big\\vert\\operatorname*{det}\\Theta_{t+1}\\big\\vert\\leq\\exp\\biggl(-\\frac{\\eta^{2}}{2}\\big\\|\\mathbf{J}_{t}\\big\\|_{F}^{2}\\biggr)\\vert\\operatorname*{det}\\Theta_{t}\\vert.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Note that because of the block structure of the matrix $\\mathbf{J}_{t}$ , its nonzero eigenvalues come in $\\pm$ -pairs: $\\pm\\sigma_{1},\\ldots,\\pm\\sigma_{m}$ , moreover, since $\\mathbf{J}_{t}$ is symmetric, singular values of $\\bar{\\mathbf{J}}_{t}$ are the absolute values of eigenvalues, i.e. $\\sigma_{1},\\ldots,\\sigma_{m}$ . Then, the determinant of $\\Theta_{t+1}$ can be written as the following, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\Theta_{t+1}=\\operatorname*{det}\\Theta_{t}\\operatorname*{det}\\left(\\mathrm{I}_{p+k}+\\eta\\mathbf{J}_{t}\\right)=\\operatorname*{det}\\Theta_{t}\\prod_{i=1}^{m}(1-\\eta^{2}\\sigma_{i}^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using that $1-x^{2}\\leq e^{-x^{2}}$ for all $x$ , we can estimate ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{m}(1-\\eta^{2}\\sigma_{i}^{2})\\leq\\exp\\mathopen{}\\mathclose\\bgroup\\left(-\\eta^{2}\\sum_{i=1}^{m}\\sigma_{i}^{2}\\aftergroup\\egroup\\right)=\\exp\\mathopen{}\\mathclose\\bgroup\\left(-\\frac{\\eta^{2}}{2}\\big\\|\\mathbf{J}_{t}\\big\\|_{F}^{2}\\aftergroup\\egroup\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We obtain the required inequality by observing that $\\prod_{i=1}^{m}(1-\\eta^{2}\\sigma_{i}^{2})=\\left|\\prod_{i=1}^{m}(1-\\eta^{2}\\sigma_{i}^{2})\\right|$ since each term $1-\\eta^{2}\\sigma_{i}^{2}\\geq0$ when $\\eta^{2}\\left\\|\\mathbf{J}_{t}\\right\\|_{F}^{2}<1$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Theorem B.5. Let $\\mathbf{s}_{1}>...\\,.\\,\\mathbf{s}_{l}$ be the order of the eigenvalues of the matrix M defined by Equation (5.4). Let the collision time for the eigenvalues be defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tau=\\{\\operatorname*{inf}t:\\mathbf{s}_{i}(t)=\\mathbf{s}_{j}(t)\\,f o r\\,1\\leq i\\neq j\\leq l\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $t\\leq\\tau$ , the eigenvalues are semi-martingales given by the solution of the following SDE ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{d}(\\mathbf{s}_{i})=p\\mathbf{c}_{i}^{2}\\;\\mathbf{d}t+\\sum_{\\stackrel{j=1}{j\\neq i}}^{l}\\frac{\\mathbf{s}_{i}\\mathbf{c}_{j}^{2}+\\mathbf{s}_{j}\\mathbf{c}_{i}^{2}}{\\mathbf{s}_{i}-\\mathbf{s}_{j}}\\mathbf{d}t+2\\sqrt{\\mathbf{s}_{i}\\mathbf{c}_{i}^{2}}\\left(\\mathbf{d}\\tilde{\\mathbf{X}}\\right)_{i}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\left(\\mathbf{d}\\tilde{\\mathbf{X}}\\right)_{i}\\!\\!}&{{}=}&{\\!\\!1/\\eta\\delta\\left(\\langle\\mathbf{u}_{i},y\\rangle-\\sqrt{\\mathbf{s}_{i}\\mathbf{c}_{i}^{2}}\\right)\\mathrm{d}t\\,+\\,\\mathrm{d}\\varepsilon_{i}}\\end{array}$ with $\\mathbf{u}_{i}$ being the $i^{t h}$ column of $\\mathbf{U}$ and $\\left(\\varepsilon_{0},\\ldots,\\varepsilon_{l-1}\\right)$ is the standard Brownian motion in $\\mathbb{R}^{l}$ . The evolution of $\\mathbf{c}_{i}$ and $\\mathbf{U}$ are presented in the appendix. ", "page_idx": 15}, {"type": "text", "text": "Proof. The proof follows the approach of Bru [1989]. Let $\\mathbf{W}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ be the singularvalue decomposition (see Def.D.1 involved with $r=l$ and $l<p$ and it will be the rank). Our focus is on understanding the evolution of the singular values and singular vectors of the matrix W. To derive the evolution of $\\Sigma,\\mathbf{V}$ we can consider the eigenvalues and eigenvectors of the PSD matrix process M. Note that $\\mathbf{M}=\\mathbf{V}\\pmb{\\Sigma}^{2}\\mathbf{V}^{\\top}$ , let $\\mathbf{D}=\\pmb{\\Sigma}^{2}$ . ", "page_idx": 15}, {"type": "text", "text": "Evolution of $\\mathbf{D}$ and $\\mathbf{V}$ Taking the derivative of $\\mathbf{M}$ , we find ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{M}=\\mathrm{d}\\mathbf{W}^{\\top}\\mathbf{W}+\\mathbf{W}^{\\top}\\mathrm{d}\\mathbf{W}+\\mathrm{d}\\mathbf{W}^{\\top}\\mathrm{d}\\mathbf{W}=\\mathbf{a}\\mathbf{d}\\mathbf{X}^{\\top}\\mathbf{W}+\\mathbf{W}^{\\top}\\mathrm{d}\\mathbf{X}\\mathbf{a}^{\\top}+p\\mathbf{a}\\mathbf{a}^{\\top}\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We invoke the theorem D.2 we derived to give the eigenvalues of any matrix valued stochastic process.   \nNote that $\\mathbf{V}\\mathbf{V}^{\\top}=\\mathbf{I}_{l}$ , so some terms of the computation are not required. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{dD}=\\mathrm{I}\\odot\\widetilde{\\mathbf{N}}\\,\\mathbf{d}t+\\mathrm{I}\\odot\\widetilde{\\mathbf{dM}}\\,\\mathbf{d}t+\\mathrm{I}\\odot\\left(\\mathbf{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\widetilde{\\mathbf{dM}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the evolution of the eigenvectors, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{V}=\\mathbf{V}\\left(\\mathbf{Q}_{\\parallel}\\ \\mathrm{d}t+\\mathbf{S}\\odot(\\widetilde{\\mathbf{N}}\\mathrm{d}t+\\mathrm{d}\\widetilde{\\mathbf{M}})\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where you define, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{\\parallel}=\\frac{\\mathrm{I}\\odot\\left[\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\right]}{2}-\\mathbf{S}\\odot\\left[\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\left[\\mathrm{d}\\widetilde{\\mathbf{M}}\\odot\\mathrm{I}\\right]\\right]+\\mathbf{S}\\odot\\left(\\mathrm{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the matrix S is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf S_{i j}=\\left\\{\\boldsymbol{0}\\begin{array}{l l}{\\phantom{-}\\;}&{\\mathrm{if}\\;i=j,}\\\\ {(\\mathbf s_{j}-\\mathbf s_{i})^{-1}\\quad}&{\\mathrm{o.w.}}\\end{array}\\right.}\\\\ &{\\quad\\widetilde{\\mathbf N}=\\mathbf V^{\\top}(p\\mathbf a^{\\top})\\mathbf V=p\\mathbf c^{\\top}.}\\\\ &{\\quad\\mathbf d\\widetilde{\\mathbf M}=\\mathbf V^{\\top}\\left[\\mathbf a\\mathbf d\\mathbf X^{\\top}\\mathbf W+\\mathbf W^{\\top}\\mathbf d\\mathbf X\\mathbf a^{\\top}\\right]\\mathbf V,}\\\\ &{\\quad\\quad=\\mathbf c\\mathbf d\\mathbf X^{\\top}\\mathbf U\\boldsymbol\\Sigma+\\boldsymbol\\Sigma\\mathbf U^{\\top}\\mathbf d\\mathbf X\\mathbf c^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $\\Sigma=\\operatorname{diag}\\left(\\left(\\pmb{\\sigma}_{0},\\dots,\\pmb{\\sigma}_{l-1}\\right)\\right)$ where $\\pmb{\\sigma}_{0}\\,>\\,\\pmb{\\sigma}_{1}\\,.\\,.\\,.\\,>\\,\\pmb{\\sigma}_{l-1}$ . Let $\\mathbf{D}=\\pmb{\\Sigma}^{2}$ and denote the entires of $\\mathbf{D}$ as following, $\\mathbf{D}=\\mathrm{diag}\\left(\\left(\\mathbf{s}_{0},\\ldots,\\mathbf{s}_{p-1}\\right)\\right)$ . Note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{U}^{\\top}\\mathbf{d}\\mathbf{X}=\\mathbf{U}^{\\top}(\\frac{1}{\\eta\\delta}(y-\\mathbf{W}\\mathbf{a})\\mathbf{d}t+\\mathbf{d}\\mathbf{B}_{t}),}\\\\ &{\\qquad\\qquad=\\frac{1}{\\eta\\delta}\\left[\\mathbf{U}^{\\top}y-\\Sigma\\mathbf{c}\\right]\\mathbf{d}t+\\mathbf{U}^{\\top}\\mathbf{d}\\mathbf{B}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using Levy\u2019s characterization $\\mathbf{U}^{\\top}\\mathrm{d}\\mathbf{B}_{t}$ is a Brownian motion in $\\mathbb{R}^{l}$ , lets call that $\\mathrm{d}\\tilde{\\mathbf{B}}_{t}$ . The diffusion part of d M (say dF) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\mathbf{F}=\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{B}_{t}\\mathbf{c}^{\\top}+\\mathbf{c}\\mathbf{d}\\mathbf{B}_{t}^{\\top}\\mathbf{V}\\boldsymbol{\\Sigma},}\\\\ &{\\quad\\mathrm{~\\boldmath~\\Omega~}=\\left(\\pmb{\\sigma}\\odot\\mathrm{d}\\tilde{\\mathbf{B}}_{t}\\right)\\mathbf{c}^{\\top}+\\mathbf{c}\\left(\\pmb{\\sigma}\\odot\\mathrm{d}\\tilde{\\mathbf{B}}_{t}\\right)^{\\top}}\\\\ &{\\quad\\mathrm{~\\boldmath~\\Omega~}=\\mathrm{d}\\mathbf{m}_{t}\\mathbf{c}^{\\top}+\\mathbf{c}\\mathbf{d}\\mathbf{m}_{t}^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{dm}_{t}\\ \\stackrel{\\mathrm{def}}{=}\\left(\\pmb{\\sigma}\\odot\\mathbf{\\mathrm{d}}\\tilde{\\mathbf{B}}_{t}\\right)$ . We are required to compute $\\mathrm{d}\\mathbf{F}(\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F})$ to compute the evolution of eigenvalues. Using the lemma D.4, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{F}(\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F})=\\mathbf{c}\\mathbf{s}^{\\top}\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathrm{d}t-\\mathbf{D}\\mathrm{diag}\\left(\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathbf{c}\\right)\\mathrm{d}t+\\mathbf{D}\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathrm{d}t,}\\\\ {\\mathrm{I}\\odot\\left[\\mathrm{d}\\mathbf{F}(\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F})\\right]=\\mathrm{I}\\odot\\left[\\mathbf{c}\\mathbf{s}^{\\top}\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathrm{d}t-\\mathbf{D}\\mathrm{diag}\\left(\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathbf{c}\\right)\\mathrm{d}t\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The element wise computation of this term gives the required result for evolution of eigenvalues. ", "page_idx": 16}, {"type": "text", "text": "Evolution of c. Note that $c=\\mathbf{V}^{\\top}\\mathbf{a}$ . Computing the derivative using the Ito\u2019s product rule, we get, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,\\mathrm{d}\\mathbf{V}^{\\top}\\mathbf{a}=\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{a}+\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{a}+\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{a},}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{a}+\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{V}^{\\top}\\mathbf{a}+\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{a},}\\\\ &{\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{V}=\\left[\\left(\\mathbf{Q}_{\\parallel}^{\\top}\\mathbf{\\Phi}\\mathbf{\\Phi}\\mathbf{4}t-\\mathbf{S}\\odot\\mathbf{d}\\mathbf{X}\\right)\\right],}\\\\ &{\\quad\\quad\\quad\\quad\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{a}=\\mathbf{V}^{\\top}\\mathbf{W}^{\\top}\\mathbf{d}\\mathbf{B}_{t}+\\frac{1}{\\eta\\delta}\\left[\\mathbf{U}^{\\top}y-\\Sigma\\mathbf{c}\\right]\\mathbf{d}t=\\Sigma\\mathbf{d}\\Tilde{\\mathbf{B}}_{t}=\\mathbf{d}\\mathbf{m}_{t}+\\frac{1}{\\eta\\delta}\\left[\\mathbf{U}^{\\top}y-\\Sigma\\mathbf{c}\\right]\\mathbf{d}t,}\\\\ &{\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{a}=-(\\mathbf{S}\\odot\\mathbf{d}\\mathbf{F})\\mathbf{d}\\mathbf{m}_{t}.}\\\\ &{\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{a}=\\left[\\left(\\mathbf{Q}_{\\parallel}^{\\top}\\mathbf{\\Phi}\\mathbf{4}t-\\mathbf{S}\\odot\\left(\\widetilde{\\mathbf{N}}\\mathbf{d}t+\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)\\right)\\right]\\mathbf{c}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the lemma D.6, D.5, D.4 and computing the element wise summation, we get the following evolution for dc ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbf{d}\\mathbf{c}_{i}=-\\frac{1}{2}\\sum_{j=1}^{l}\\mathbf{S}_{i j}(\\mathbf{s}_{i}\\mathbf{c}_{j}^{2}+\\mathbf{s}_{j}\\mathbf{c}_{i}^{2})\\mathbf{d}t-\\mathbf{c}_{i}\\sum_{j=1}^{l}(\\mathbf{S}_{i j}\\mathbf{c}_{j}^{2})\\left(\\sum_{k\\neq i,j}\\mathbf{s}_{k i}\\mathbf{S}_{k i}\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad-\\left(p-2\\right)\\mathbf{c}_{i}\\sum_{j=1}^{l}\\mathbf{S}_{i j}\\mathbf{c}_{i}^{2}\\mathbf{d}t-\\sum_{j=1}^{l}\\mathbf{S}_{i j}\\mathbf{s}_{j}\\mathbf{d}t,}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\left.\\sigma_{i}(\\mathbf{U}^{\\top}\\mathbf{d}\\mathbf{X})_{i}(1-\\sum_{j=1}^{l}\\mathbf{S}_{i j}\\mathbf{c}_{j}^{2})-\\mathbf{c}_{i}\\sum_{j}\\mathbf{S}_{i j}\\sigma_{j}\\mathbf{c}_{j}(\\mathbf{U}^{\\top}\\mathbf{d}\\mathbf{X})_{j}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Evolution of U. To compute the evolution of $\\mathbf{U}$ , we invoke the theorem D.2 on the evolution of $\\mathbf{W}\\mathbf{W}^{\\top}=\\mathbf{U}\\mathbf{D}\\mathbf{U}^{\\top}$ . We ignore it here as it does not have much consequence on our results. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Theorem B.6. In the large noise limit, when $l=2$ , the following properties hold, for $t\\leq\\tau$ , ", "page_idx": 17}, {"type": "text", "text": "(a) $\\mathbf{s}_{0},\\mathbf{s}_{1}$ are greater than zero almost surely. ", "page_idx": 17}, {"type": "text", "text": "$(b)$ for $\\alpha=(p-3)/2,$ $\\mathbf{s}_{0}^{-\\alpha}$ is a super-martingale while $\\mathbf{s}_{1}^{-\\alpha}$ is a sub-martingale. ", "page_idx": 17}, {"type": "text", "text": "Proof. First, note that in the large noise limit with $l=2$ , the evolution of the eigenvalues is expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{d}(\\mathbf{s}_{0})=p\\mathbf{c}_{0}^{2}\\mathbf{d}t+\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\mathbf{d}t+2\\sqrt{\\mathbf{s}_{0}\\mathbf{c}_{0}^{2}}\\left(\\mathbf{d}\\tilde{\\mathbf{B}}_{t}\\right)_{0},}\\\\ {\\mathbf{d}(\\mathbf{s}_{1})=p\\mathbf{c}_{1}^{2}\\mathbf{d}t-\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\mathbf{d}t+2\\sqrt{\\mathbf{s}_{1}\\mathbf{c}_{1}^{2}}\\left(\\mathbf{d}\\tilde{\\mathbf{B}}_{t}\\right)_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the Ito chain rule, for the evolution of $\\mathbf{s}_{0}^{-\\alpha}$ we can write ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{d}\\big(\\mathbf{s}_{0}^{-\\alpha}\\big)=\\frac{\\partial\\big(\\mathbf{s}_{0}^{-\\alpha}\\big)}{\\partial\\mathbf{s}_{0}}\\left(p\\mathbf{c}_{0}^{2}\\mathbf{d}t+\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\mathbf{d}t+2\\sqrt{\\mathbf{s}_{0}\\mathbf{c}_{0}^{2}}\\left(\\mathbf{d}\\tilde{\\mathbf{B}}_{t}\\right)_{0}\\right)+\\frac{1}{2}\\frac{\\partial^{2}\\big(\\mathbf{s}_{0}^{-\\alpha}\\big)}{\\partial^{2}\\mathbf{s}_{0}}\\left(2\\sqrt{\\mathbf{s}_{0}\\mathbf{c}_{0}^{2}}\\right)^{2}\\mathbf{d}t}\\\\ {=-\\alpha\\mathbf{s}_{0}^{-\\alpha-1}\\left(p\\mathbf{c}_{0}^{2}\\mathbf{d}t+\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\mathbf{d}t-2(\\alpha+1)\\mathbf{c}_{0}^{2}\\mathbf{d}t+2\\sqrt{\\mathbf{s}_{0}\\mathbf{c}_{0}^{2}}\\left(\\tilde{\\mathbf{d}}\\tilde{\\mathbf{B}}_{t}\\right)_{0}\\right)}\\\\ {=-\\alpha\\mathbf{s}_{0}^{-\\alpha-1}\\left(\\mathbf{c}_{0}^{2}\\mathbf{d}t+\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\mathbf{d}t+2\\sqrt{\\mathbf{s}_{0}\\mathbf{c}_{0}^{2}}\\left(\\tilde{\\mathbf{d}}\\tilde{\\mathbf{B}}_{t}\\right)_{0}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "analogously ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\big(\\mathbf{s}_{1}^{-\\alpha}\\big)=-\\alpha\\mathbf{s}_{1}^{-\\alpha-1}\\left(\\mathbf{c}_{1}^{2}\\mathrm{d}t-\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\mathrm{d}t+2\\sqrt{\\mathbf{s}_{1}\\mathbf{c}_{1}^{2}}\\left(\\mathrm{d}\\tilde{\\mathbf{B}}_{t}\\right)_{1}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and finally for $\\mathbf{s}_{0}^{-\\alpha}\\mathbf{s}_{1}^{-\\alpha}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\big(\\mathbf{s}_{0}^{-\\alpha}\\mathbf{s}_{1}^{-\\alpha}\\big)=\\mathbf{d}\\big(\\mathbf{s}_{0}^{-\\alpha}\\big)\\mathbf{s}_{1}^{-\\alpha}+\\mathbf{s}_{0}^{-\\alpha}\\mathbf{d}\\big(\\mathbf{s}_{1}^{-\\alpha}\\big)+\\mathbf{d}\\big(\\mathbf{s}_{0}^{-\\alpha}\\big)\\mathbf{d}\\big(\\mathbf{s}_{1}^{-\\alpha}\\big)}\\\\ {=-\\alpha\\mathbf{s}_{0}^{-\\alpha-1}\\mathbf{s}_{1}^{-\\alpha}\\left(\\mathbf{c}_{0}^{2}\\mathbf{d}t+\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\mathbf{d}t+2\\sqrt{\\mathbf{s}_{0}\\mathbf{c}_{0}^{2}}\\left(\\mathbf{d}\\tilde{\\mathbf{B}}_{t}\\right)_{0}\\right)}\\\\ {-\\alpha\\mathbf{s}_{0}^{-\\alpha}\\mathbf{s}_{1}^{-\\alpha-1}\\left(\\mathbf{c}_{1}^{2}\\mathbf{d}t-\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\mathbf{d}t+2\\sqrt{\\mathbf{s}_{1}\\mathbf{c}_{1}^{2}}\\left(\\mathbf{d}\\tilde{\\mathbf{B}}_{t}\\right)_{1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, we can show that the drift term in the SDE that describes the dynamics of $\\mathbf{s}_{0}^{-\\alpha}\\mathbf{s}_{1}^{-\\alpha}$ is zero, which gives us the first part of the result by Mckean\u2019s argument [Mayerhofer et al., 2011], ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\alpha\\mathbf{s}_{0}^{-\\alpha-1}\\mathbf{s}_{1}^{-\\alpha-1}\\left(\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}+\\mathbf{s}_{1}\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}+\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{0}\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\right)}\\\\ {=-\\alpha\\mathbf{s}_{0}^{-\\alpha-1}\\mathbf{s}_{1}^{-\\alpha-1}\\left(\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}+\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\frac{\\mathbf{s}_{0}\\mathbf{s}_{1}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}^{2}\\mathbf{c}_{0}^{2}-\\mathbf{s}_{0}^{2}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{0}\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\right)}\\\\ {=-\\alpha\\mathbf{s}_{0}^{-\\alpha-1}\\mathbf{s}_{1}^{-\\alpha-1}\\left(\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}+\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\frac{\\left(\\mathbf{s}_{1}-\\mathbf{s}_{0}\\right)\\left(\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}\\right)}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}\\right)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second part is obtained by noticing that ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{c}{\\mathbf{c}_{0}^{2}+{\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}}={\\frac{\\mathbf{s}_{0}\\left(\\mathbf{c}_{1}^{2}+\\mathbf{c}_{0}^{2}\\right)}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}}\\geq0,}\\\\ {\\mathbf{c}_{1}^{2}-{\\frac{\\mathbf{s}_{0}\\mathbf{c}_{1}^{2}+\\mathbf{s}_{1}\\mathbf{c}_{0}^{2}}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}}=-{\\frac{\\mathbf{s}_{1}\\left(\\mathbf{c}_{1}^{2}+\\mathbf{c}_{0}^{2}\\right)}{\\mathbf{s}_{0}-\\mathbf{s}_{1}}}\\leq0,}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and hence the drift term of $\\mathbf{d}\\bigl(\\mathbf{s}_{0}^{-\\alpha}\\bigr)$ is not positive, while the drift term of $\\mathbf{d}\\bigl(\\mathbf{s}_{1}^{-\\alpha}\\bigr)$ is not negative. ", "page_idx": 17}, {"type": "text", "text": "C Experiment details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In all the graphs we plot the values averaged on 20 runs with different random seeds as well as the $95\\%$ confidence interval (lightly colored). To numerically emulate GF (Figure 1), we set a stepsize of $1e^{-6}$ in numerical simulation. ", "page_idx": 18}, {"type": "text", "text": "In the further experiments, we study the behaviour of the linear network for regression with the same synthetic data and same network initialization as in previous experiment. As seen in the left plot of the Figure 2, when the stepsize is large $(\\eta=0.1)$ , singular values exhibit behavior similar to the case of LNGF, while with the small stepsize $\\eta=0.005)$ the evolution of singular values is closer to GF case. Next, we examine the effect of SGD in the case of classification task with logistic loss, as illustrated in the middle plot of the Figure 2. We consider synthetic data with $n=1000$ samples of Gaussian data in $\\mathbb{R}^{5}$ $d=5$ ) constituting two clusters corresponding to two classes $\\left[k=1\\right]$ ). Note that larger stepsize $\\eta=0.5)$ in this case also forces the smallest singular value to tend to zero, however the effect is not so dramatic for the rest of singular values. Additionally, we study the 2-layer ReLu network optimized with SGD on the same regression task as before. As seen in the right plot of the Figure 2, the decrease of the last singular value $\\sigma_{4}$ is much slower than in the case of the linear network, however, the larger stepsize still facilitates divergence of $k$ largest ( $\\overline{{\\sigma_{0}}}$ and $\\sigma_{1}$ ) and $p-k$ smallest $(\\sigma_{2},\\sigma_{3}$ and $\\sigma_{4}$ ) singular values. ", "page_idx": 18}, {"type": "text", "text": "All experiments are implemented with Python 3 [Van Rossum and Drake, 2009] under PSF license, NumPy [Harris et al., 2020] under BSD license, and PyTorch [Paszke et al., 2019] under BSD-3- Clause license. ", "page_idx": 18}, {"type": "text", "text": "The experiments were run on a Intel i5-8250U, 8-GB RAM, with OS Ubuntu 20.04.6. ", "page_idx": 18}, {"type": "text", "text": "D Supplementary material ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Notations and preliminary definitions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Definition D.1 (Eigen decomposition and Singular Value decomposition). We discuss the eigen value decomposition for a symmetric square matrix, and the singular value decompostion for any matrix is defined as the following ", "page_idx": 18}, {"type": "text", "text": "(a) Eigen decomposition. For any rank $r$ matrix $\\mathbf{R}\\in S_{p}$ , $\\mathbf{R}=\\mathbf{V}\\mathbf{D}\\mathbf{V}^{\\top}$ is the eigen decomposition, where $\\mathbf{V}\\in\\mathbb{R}^{p\\times r}$ , $\\mathbf{D}\\in\\mathbb{R}^{r\\times r}$ , $\\mathbf{D}$ is a diagonal matrix and $\\mathbf{V}^{\\top}\\mathbf{V}=\\mathrm{I}_{r}$ , however, $\\mathbf{V}\\mathbf{V}^{\\top}$ is not necessarily an identity matrix unless $r=p$ . ", "page_idx": 18}, {"type": "text", "text": "(b) Singular Value Decomposition. For any rank r matrix $\\mathbf{W}\\,\\in\\,\\mathbb{R}^{p\\times l}$ , $\\mathbf{W}\\,=\\,\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ , where $\\mathbf{U}\\in\\mathbb{R}^{p\\times r}$ , $\\mathbf{V}\\in\\mathbb{R}^{l\\times r}$ , $\\bar{\\boldsymbol{\\Sigma}}\\in\\mathbb{R}^{r\\times r}$ , $\\Sigma$ is a diagonal matrix and $\\mathbf{U}^{\\top}\\mathbf{U}=\\mathbf{V}^{\\top}\\mathbf{V}=\\mathrm{I}_{r}$ , however the $\\mathbf{UU}^{\\top}$ and $\\mathbf{V}\\mathbf{V}^{\\top}$ are not necessarily identity unless $r=p$ or $r=l$ respectively. ", "page_idx": 18}, {"type": "text", "text": "D.2 Eigenvalues of matrix valued stochastic process ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem D.2. For a matrix-valued stochastic process on $S_{p+k}^{++}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}=\\mathbf{N}\\mathrm{d}t+\\mathrm{d}\\mathbf{M}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where dM is a local martingale process. Let $R={\\mathbf{V}}{\\mathbf{D}}{\\mathbf{V}}^{\\top}$ is the eigenvalue decomposition of the process, the evolution of eigenvalues satisfy the SDE for time $t$ less than the collision time, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{dD}=\\mathbf{I}\\odot\\widetilde{\\mathbf{N}}\\;\\mathbf{d}t+\\mathbf{I}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\;\\mathbf{d}t+\\mathbf{I}\\odot\\left(\\mathbf{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)\\right)+\\mathbf{D}^{-1}\\odot\\left(\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{S}$ is defined as per Eq. D.1 and $\\mathrm{d}\\widetilde{\\mathbf{M}}\\,=\\,\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{M}\\mathbf{V},\\widetilde{\\mathbf{N}}\\,=\\,\\mathbf{V}^{\\top}\\mathbf{N}\\mathbf{V}$ . The evolution of the eigenvectors, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{V}=\\mathbf{V}\\left(\\mathbf{Q}_{\\parallel}\\;\\mathrm{d}t+\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F}\\right)+\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\left(\\mathbf{Q}_{\\perp}\\;\\mathrm{d}t+\\mathrm{d}\\mathbf{R}\\;\\mathbf{V}\\mathbf{D}^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where you define, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Q}_{\\parallel}=\\frac{\\displaystyle\\left[\\left(\\mathbf{S}\\odot\\widetilde{\\mathbf{dM}}\\right)\\left(\\mathbf{S}\\odot\\widetilde{\\mathbf{dM}}\\right)\\right]}{\\displaystyle2}-\\frac{\\displaystyle\\mathrm{I}\\odot\\left[\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathbf{dR}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{dR}\\mathbf{V}\\mathbf{D}^{-1}\\right]}{\\displaystyle2}}\\\\ &{\\qquad\\qquad-\\,\\mathbf{S}\\odot\\left[\\left(\\mathbf{S}\\odot\\widetilde{\\mathbf{dM}}\\right)\\left[\\mathbf{d}\\widetilde{\\mathbf{M}}\\odot\\mathrm{I}\\right]\\right]+\\mathbf{S}\\odot\\left(\\mathbf{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\widetilde{\\mathbf{dM}}\\right)\\right)}\\\\ &{\\qquad\\qquad+\\,\\mathbf{S}\\odot\\left(\\mathbf{V}^{\\top}\\mathbf{dR}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{dR}\\mathbf{V}\\mathbf{D}^{-1}\\right),}\\\\ &{\\mathbf{Q}_{\\perp}=\\left[\\mathbf{dR}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\left[\\left[\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right]\\mathbf{D}-\\mathbf{d}\\widetilde{\\mathbf{M}}\\right]\\mathbf{D}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Evolution of eigenvalues for general matrix SDE ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Using the eigen decomposition, we have $\\mathbf{R}=\\mathbf{V}\\mathbf{D}\\mathbf{V}^{\\top}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}=\\mathbf{V}^{\\top}\\mathbf{R}\\mathbf{V},}\\\\ &{\\mathbf{d}\\mathbf{D}=\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\mathbf{V}+\\mathbf{V}^{\\top}\\mathbf{R}\\mathbf{d}\\mathbf{V}+\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{R}\\mathbf{V}+\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\mathbf{d}\\mathbf{V}+\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\mathbf{V}+\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{R}\\mathbf{d}\\mathbf{V},}\\\\ &{\\quad=\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\mathbf{V}+\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{V}+\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{D}+\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\mathbf{d}\\mathbf{V}+\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\mathbf{V}+\\left(\\mathbf{d}\\mathbf{V}^{\\top}\\mathbf{V}\\right)\\mathbf{D}\\left(\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{V}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The approach we follow is use the jacobian of the evolution of $\\mathbf{V}$ (see [Townsend, 2016] ) and solve the constrains equations to obtain the Ito correction term as done in Bru [1989]. Let $\\left(\\mathbf{s}_{1},\\mathbf{s}_{2},\\ldots,\\mathbf{s}_{r}\\right)$ denote the diagonal entries of $\\mathbf{D}$ . Furthermore, we define the matrix S, which plays a notable role in Jacobian w.r.t $\\mathbf{V}$ , as the following, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{S}_{i j}=\\left\\{0\\begin{array}{l l}{\\mathrm{~if~}i=j,}\\\\ {(\\mathbf{s}_{j}-\\mathbf{s}_{i})^{-1}}&{\\mathrm{~o.w.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the sake of brevity, we denote the evolution ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{dF}\\stackrel{\\mathrm{def}}{=}\\mathbf{V}^{\\top}\\mathbf{dR}\\mathbf{V}=\\mathbf{V}^{\\top}\\mathbf{N}\\mathbf{V}\\,\\mathbf{\\,d}t+\\mathbf{V}^{\\top}\\mathbf{dM}\\mathbf{V},}\\\\ {\\stackrel{\\mathrm{def}}{=}\\widetilde{\\mathbf{N}}\\,\\mathbf{\\,d}t+\\mathbf{\\,d}\\widetilde{\\mathbf{M}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The evolution of the eigenvectors, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{V}=\\mathbf{V}\\mathrm{d}\\Omega_{\\mathbf{V}}+(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top})\\mathrm{d}\\boldsymbol{\\Xi}_{\\mathbf{V}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the Jacobian of the eigen vectors, we write, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\Omega_{\\mathbf{V}}=\\mathbf{Q}_{\\parallel}\\,\\mathrm{d}t+\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F},}\\\\ &{\\mathrm{d}\\Xi_{\\mathbf{V}}=\\mathbf{Q}_{\\perp}\\,\\mathrm{d}t+\\mathrm{d}\\mathbf{R}\\,\\mathbf{V}\\mathbf{D}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $\\mathbf{V}^{\\top}\\mathbf{V}=\\mathbf{I}_{r}$ , using this we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\mathrm{d}\\big(\\mathbf{V}^{\\top}\\mathbf{V}\\big)=\\mathrm{d}\\mathbf{V}^{\\top}\\mathbf{V}+\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{V}+\\mathrm{d}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{V},}\\\\ &{\\qquad\\qquad\\quad=\\mathrm{d}\\boldsymbol{\\Omega}\\mathbf{V}^{\\top}+\\mathrm{d}\\boldsymbol{\\Omega}_{\\mathbf{V}}+\\mathrm{d}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{V}+\\mathrm{d}\\mathbf{V}^{\\top}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\mathbf{V},}\\\\ &{\\qquad\\qquad\\quad=\\mathrm{d}\\boldsymbol{\\Omega}_{\\mathbf{V}}^{\\top}+\\mathrm{d}\\boldsymbol{\\Omega}_{\\mathbf{V}}+\\mathrm{d}\\boldsymbol{\\Omega}_{\\mathbf{V}}^{\\top}\\mathrm{d}\\boldsymbol{\\Omega}_{\\mathbf{V}}+\\mathrm{d}\\boldsymbol{\\Xi}_{\\mathbf{V}}^{\\top}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\boldsymbol{\\Xi}_{\\mathbf{V}},}\\\\ &{\\qquad\\qquad\\quad=\\mathrm{d}\\boldsymbol{\\Omega}_{\\mathbf{V}}^{\\top}+\\mathrm{d}\\boldsymbol{\\Omega}_{\\mathbf{V}}-\\left(\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F}\\right)\\left(\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F}\\right)+\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using $\\mathrm{d}\\Omega_{\\mathbf{V}}^{\\top}=\\mathbf{Q}_{\\parallel}^{\\top}\\mathrm{d}t-\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F}$ , we have $\\mathrm{d}\\Omega_{\\mathbf{V}}^{\\top}+\\mathrm{d}\\Omega_{\\mathbf{V}}=\\left(\\mathbf{Q}_{\\parallel}^{\\top}+\\mathbf{Q}_{\\parallel}\\right)\\mathrm{d}t.$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\mathbf{Q}_{\\parallel}+\\mathbf{Q}_{\\parallel}^{\\top}\\right)\\mathbf{d}t=\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)-\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Coming back to the evolution of singular values, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\mathbf{\\left|D=V^{\\top}d R V+D V^{\\top}d V+d V^{\\top}V D+V^{\\top}d R d V+d V^{\\top}d R V+\\left(d V^{\\top}V\\right)\\mathbf{D}\\left(\\mathbf{V}^{\\top}\\mathbf{dV}\\right).}\\\\ &{\\qquad=\\mathbf{dF}+\\left(\\mathbf{DQ}_{\\parallel}+\\mathbf{Q}_{\\parallel}^{\\top}\\mathbf{D}\\right)\\mathbf{d}t+\\mathbf{D}\\left(\\mathbf{S}\\odot\\mathbf{dF}\\right)-\\left(\\mathbf{S}\\odot\\mathbf{dF}\\right)\\mathbf{D}+\\mathrm{d}\\Omega_{\\mathbf{V}}^{\\top}\\mathbf{D}\\mathrm{d}\\Omega_{\\mathbf{V}}}\\\\ &{\\qquad\\qquad\\qquad+\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}\\left[\\mathbf{V}\\mathbf{d}\\Omega_{\\mathbf{V}}+\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\Xi_{\\mathbf{V}}\\right]+\\left[\\mathrm{d}\\Omega_{\\mathbf{V}}\\mathbf{\\bar{\\Sigma}}\\mathbf{V}^{\\top}+\\mathrm{d}\\Xi_{\\mathbf{V}}^{\\top}\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\right]\\mathrm{d}\\mathbf{R}\\mathbf{V}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{dD}=\\mathbf{I}\\odot\\mathbf{dF}+\\left(\\mathbf{DQ}_{\\parallel}+\\mathbf{Q}_{\\parallel}^{\\top}\\mathbf{D}\\right)\\mathbf{d}t-\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)\\mathbf{D}\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)+\\mathbf{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)}\\\\ &{\\qquad\\qquad-\\left(\\mathbf{S}\\odot\\widetilde{\\mathbf{d}}\\widetilde{\\mathbf{M}}\\right)\\mathbf{d}\\widetilde{\\mathbf{M}}+\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}+\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that dD is diagonal, hence, $\\mathbf{I}\\odot\\mathbf{dD}=\\mathbf{dD}$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{I}\\odot\\mathrm{d}\\mathbf{D}=\\mathrm{I}\\odot\\mathrm{d}\\mathbf{F}+\\mathrm{I}\\odot\\left(\\mathbf{D}\\mathbf{Q}_{\\parallel}+\\mathbf{Q}_{\\parallel}^{\\top}\\mathbf{D}\\right)\\mathrm{d}t-\\mathrm{I}\\odot\\left[\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\mathbf{D}\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\;2\\mathrm{I}\\odot\\left(\\mathrm{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\right)+2\\mathrm{I}\\odot\\left(\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\mathbf{R}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\mathrm{I}\\odot(D M)=\\mathrm{I}\\odot(M D)=D\\odot M$ for any matrix $M$ and diagonal matrix $D$ , using this property, we can simplify the above expression as, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{dD}=\\mathbf{I}\\odot\\mathbf{dF}+\\mathbf{D}\\odot\\left(\\mathbf{Q}_{\\parallel}+\\mathbf{Q}_{\\parallel}^{\\top}\\right)\\mathbf{d}t-\\mathbf{I}\\odot\\left[\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)\\mathbf{D}\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;2\\mathbf{I}\\odot\\left(\\mathbf{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)\\right)+2\\mathbf{D}^{-1}\\odot\\left(\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using Eq. D.2, we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf D\\odot\\left(\\mathbf Q_{\\parallel}+\\mathbf Q_{\\parallel}^{\\top}\\right)\\mathrm d t=\\mathbf D\\odot\\left[\\left(\\mathbf S\\odot\\mathrm{d}\\widetilde{\\mathbf M}\\right)\\left(\\mathbf S\\odot\\mathrm{d}\\widetilde{\\mathbf M}\\right)-\\mathbf D^{-1}\\mathbf V^{\\top}\\mathrm d\\mathbf R\\left(\\mathrm I-\\mathbf V\\mathbf V^{\\top}\\right)\\mathrm d\\mathbf R\\mathbf V\\mathbf D^{-1}\\right],}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathrm{I}\\odot\\left[\\left(\\mathbf S\\odot\\mathrm{d}\\widetilde{\\mathbf M}\\right)\\left(\\mathbf S\\odot\\mathrm{d}\\widetilde{\\mathbf M}\\right)\\mathbf D\\right]-\\mathbf D^{-1}\\odot\\left(\\mathbf V^{\\top}\\mathrm d\\mathbf R\\left(\\mathrm I-\\mathbf V\\mathbf V^{\\top}\\right)\\mathrm d\\mathbf R\\mathbf V\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using this, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathrm{d}}{\\mathbf{D}}=\\mathbf{I}\\odot{\\mathrm{d}}{\\mathbf{F}}+\\mathbf{I}\\odot\\left[\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right)\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right){\\mathbf{D}}\\right]-\\mathbf{I}\\odot\\left[\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right){\\mathbf{D}}\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+2\\mathbf{I}\\odot\\left(\\mathbf{d}\\widetilde{{\\mathbf{M}}}\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right)\\right)+\\mathbf{D}^{-1}\\odot\\left(\\mathbf{V}^{\\top}{\\mathrm{d}}{\\mathbf{R}}\\left(\\mathbf{I}-\\mathbf{V}{\\mathbf{V}}^{\\top}\\right){\\mathrm{d}}{\\mathbf{R}}\\mathbf{V}\\right),}\\\\ &{\\qquad=\\mathbf{I}\\odot{\\mathrm{d}}{\\mathbf{F}}+\\mathbf{I}\\odot\\left[\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right)\\left[\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right){\\mathbf{D}}-\\mathbf{D}\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right)\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+2\\mathbf{I}\\odot\\left(\\mathbf{d}\\widetilde{{\\mathbf{M}}}\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right)\\right)+\\mathbf{D}^{-1}\\odot\\left(\\mathbf{V}^{\\top}{\\mathrm{d}}{\\mathbf{R}}\\left(\\mathbf{I}-\\mathbf{V}{\\mathbf{V}}^{\\top}\\right){\\mathrm{d}}{\\mathbf{R}}\\mathbf{V}\\right),}\\\\ &{\\qquad=\\mathbf{I}\\odot{\\mathrm{d}}{\\mathbf{F}}+\\mathbf{I}\\odot\\left[\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right){\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+2\\mathbf{I}\\odot\\left(\\mathbf{d}\\widetilde{{\\mathbf{M}}}\\left(\\mathbf{S}\\odot{\\mathrm{d}}\\widetilde{{\\mathbf{M}}}\\right)\\right)+\\mathbf{D}^{-1}\\odot\\left(\\mathbf{V}^{\\top}{\\mathrm{d}}{\\mathbf{R}}\\left(\\mathbf{I}-\\mathbf{V}{\\mathbf{V}}^{\\top}\\right){ \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Evolution of eigenvectors for general matrix SDE. Here, we derive the evolution of eigenvectors, Using Eq. D.2, we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(\\mathbf{Q}_{\\parallel}\\mathbf{D}+\\mathbf{Q}_{\\parallel}^{\\top}\\mathbf{D}\\right)\\mathrm{d}t=\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\mathbf{D}-\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\mathbf{R}\\mathbf{V}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now further using the constrain that dD needs to be diagonal we get, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbf{DQ}_{1}+\\mathbf{Q}_{\\|}^{\\top}\\mathbf{D}\\right)\\mathbf{d}t=\\mathbf{d}\\mathbf{D}-\\mathbf{I}\\odot\\mathbf{d}\\mathbf{F}+\\left(\\mathbf{S}\\odot\\mathbf{d}\\mathbf{M}\\right)\\mathbf{D}\\left(\\mathbf{S}\\odot\\mathbf{d}\\mathbf{M}\\right)-\\mathbf{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\mathbf{d}\\widetilde{\\mathbf{M}}\\right)+\\left(\\mathbf{S}\\odot\\boldsymbol{\\Phi}\\right)\\mathbf{\\Lambda}+\\left(\\mathbf{S}\\odot\\boldsymbol{\\Phi}\\right)\\mathbf{d}\\mathbf{M}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\times\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\left(\\mathbf{\\left[}-\\mathbf{V}\\mathbf{S}^{\\top}\\mathbf{\\Lambda}\\mathbf{]}^{\\top}\\mathbf{d}\\mathbf{R}\\mathbf{V}^{\\top}\\mathbf{\\Lambda}-\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\left(\\mathbf{\\left[}-\\mathbf{V}\\mathbf{V}^{\\top}\\mathbf{\\Lambda}\\mathbf{]}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\cdot\\mathbf{\\Lambda}}\\\\ &{\\quad\\big(\\mathbf{DQ}_{1}-\\mathbf{Q}_{\\|}\\mathbf{D}\\big)\\mathbf{d}t=\\mathbf{d}\\mathbf{D}-\\mathbf{I}\\odot\\mathbf{d}\\mathbf{F}-\\left(\\mathbf{S}\\odot\\boldsymbol{\\Phi}\\right)\\left[\\left(\\mathbf{S}\\odot\\boldsymbol{\\Phi}\\right)\\mathbf{D}-\\mathbf{D}\\left(\\mathbf{S}\\odot\\boldsymbol{\\Phi}\\right)\\right]-\\mathbf{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\boldsymbol{\\Phi}\\right)\\mathbf{d}\\mathbf{\\Lambda}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left(\\mathbf{S}\\odot\\boldsymbol{\\widetilde{\\Omega}}\\right)\\dim\\mathbf{\\left[}\\mathbf{\\Sigma}\\mathbf{d}\\mathbf{\\Lambda}-\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}\\left(\\mathbf{\\left[}-\\mathbf{V}\\mathbf{V}^{\\top}\\right]\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbf{d}\\mathbf{D}-\\mathbf{I}\\odot\\mathbf{d}\\mathbf{F}-\\left(\\mathbf{S}\\odot\\boldsymbol{\\widetilde{\\Omega}}\\right)\\left[\\mathbf{d}\\widetilde{\\mathbf{M}}\\odot\\mathbf{I}\\right]-\\mathbf{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\boldsymbol{\\Phi}\\right]\\mathbf{\\tilde{M}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad+\\left(\\mathbf{S}\\odot\\boldsymbol{\\widetilde{\\Omega}}\\right)\\dim\\mathbf{\\left[}\\mathbf{d \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{\\bar{\\mathrm{I}}\\odot\\mathbf{Q}_{[\\|})\\,\\mathrm{d}t=\\mathbf{S}\\odot\\left[-\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\left[\\mathrm{d}\\widetilde{\\mathbf{M}}\\odot\\mathbf{I}\\right]+\\mathrm{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)+\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right].}\\end{array}$ Combing these, we get the diagonal and off diagonal terms of $\\mathbf{Q}_{\\parallel}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathrm{I}\\odot\\mathbf{Q}_{\\parallel}\\right)\\mathrm{d}t=\\frac{1}{2}\\mathrm{~I}\\odot\\left(\\mathbf{Q}_{\\parallel}+\\mathbf{Q}_{\\parallel}^{\\top}\\right)\\mathrm{d}t,}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\mathrm{I}\\odot\\left[\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\left(\\mathbf{S}\\odot\\mathrm{d}\\widetilde{\\mathbf{M}}\\right)\\right]}{2}-\\frac{\\mathrm{I}\\odot\\left[\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Q}_{\\parallel}=\\frac{\\operatorname{I}\\odot\\left[\\left(\\mathbf{S}\\odot\\mathsf{d}\\widetilde{\\mathbf{M}}\\right)\\left(\\mathbf{S}\\odot\\mathsf{d}\\widetilde{\\mathbf{M}}\\right)\\right]}{2}-\\frac{\\operatorname{I}\\odot\\left[\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathsf{d}\\mathbf{R}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\,\\mathrm{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]}{2}}\\\\ &{\\qquad\\qquad-\\,\\mathbf{S}\\odot\\left[\\left(\\mathbf{S}\\odot\\mathsf{d}\\widetilde{\\mathbf{M}}\\right)\\left[\\mathsf{d}\\widetilde{\\mathbf{M}}\\odot\\mathrm{I}\\right]\\right]+\\mathbf{S}\\odot\\left(\\mathsf{d}\\widetilde{\\mathbf{M}}\\left(\\mathbf{S}\\odot\\mathsf{d}\\widetilde{\\mathbf{M}}\\right)\\right)}\\\\ &{\\qquad\\qquad+\\,\\mathbf{S}\\odot\\left(\\mathbf{V}^{\\top}\\mathsf{d}\\mathbf{R}\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathsf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Computing of $\\mathbf{Q}_{\\perp}$ . Recalling the evolution of the eigenvectors, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{V}=\\mathbf{V}\\mathrm{d}\\Omega_{\\mathbf{V}}+(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top})\\mathrm{d}\\boldsymbol{\\Xi}_{\\mathbf{V}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the Jacobian of the eigen vectors, we write, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\Omega_{\\mathbf{V}}=\\mathbf{Q}_{\\parallel}\\,\\mathrm{d}t+\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F},}\\\\ &{\\mathrm{d}\\Xi_{\\mathbf{V}}=\\mathbf{Q}_{\\perp}\\,\\mathrm{d}t+\\mathrm{d}\\mathbf{R}\\,\\mathbf{V}\\mathbf{D}^{-1},}\\\\ &{\\quad\\mathrm{d}\\mathbf{V}=\\mathbf{V}\\left[\\mathbf{Q}_{\\parallel}\\,\\mathrm{d}t+\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F}\\right]+\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\left[\\mathbf{Q}_{\\perp}\\,\\mathrm{d}t+\\mathrm{d}\\mathbf{R}\\,\\mathbf{V}\\mathbf{D}^{-1}\\right],}\\\\ &{\\mathrm{d}\\mathbf{V}^{\\top}=\\left[\\mathbf{Q}_{\\parallel}^{\\top}\\,\\mathrm{d}t-\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F}\\right]\\mathbf{V}^{\\top}+\\left[\\mathbf{Q}_{\\perp}^{\\top}\\mathrm{d}t+\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}\\right]\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the fact that $\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{R}=\\mathbf{0}$ and deriving it, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\boldmath~\\Gamma~}0=\\left({\\bf I}-{\\bf V}{\\bf V}^{\\top}\\right){\\bf R},}\\\\ &{\\mathrm{~\\boldmath~\\Gamma~}0=\\mathrm{d}\\big[\\big({\\bf I}-{\\bf V}{\\bf V}^{\\top}\\big){\\bf R}\\big],}\\\\ &{\\mathrm{{d}}{\\bf R}=\\mathrm{d}\\big({\\bf V}{\\bf V}^{\\top}{\\bf R}\\big),}\\\\ &{\\quad\\mathrm{~\\boldmath~\\Gamma~}=\\mathrm{d}{\\bf V}{\\bf V}^{\\top}{\\bf R}+{\\bf V}{\\bf d}{\\bf V}^{\\top}{\\bf R}+{\\bf V}{\\bf V}^{\\top}\\mathrm{d}{\\bf R}+\\mathrm{d}{\\bf V}{\\bf d}{\\bf V}^{\\top}{\\bf R}+\\mathrm{d}{\\bf V}{\\bf V}^{\\top}{\\bf d}{\\bf R}+{\\bf V}{\\bf d}{\\bf V}^{\\top}{\\bf d}{\\bf R},}\\\\ &{{\\bf i}{\\bf R}{\\bf V}=\\mathrm{d}{\\bf V}{\\bf D}+{\\bf V}{\\bf d}{\\bf V}^{\\top}{\\bf V}{\\bf D}+{\\bf V}{\\bf V}^{\\top}{\\bf d}{\\bf R}{\\bf V}+\\mathrm{d}{\\bf V}{\\bf d}{\\bf V}^{\\top}{\\bf V}{\\bf D}+\\mathrm{d}{\\bf V}{\\bf V}^{\\top}{\\bf d}{\\bf R}{\\bf V}+{\\bf V}{\\bf d}{\\bf V}^{\\top}{\\bf d}{\\bf R}{\\bf V},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\,dVD}=\\mathbf{V}\\left[\\mathbf{Q}_{\\parallel}\\mathbf{D}\\,\\mathrm{d}t+\\left(\\mathbf{S}\\odot\\mathrm{\\,dF}\\right)\\mathbf{D}\\right]+\\left(\\mathbf{I}-\\mathbf{VV}^{\\top}\\right)\\left[\\mathbf{Q}_{\\perp}\\mathbf{D}\\,\\mathrm{d}t+\\mathrm{d}\\mathbf{R}\\,\\mathbf{V}\\right],}\\\\ &{\\mathrm{\\,VdV}^{\\top}\\mathbf{V}\\mathbf{D}=\\mathbf{V}\\left[\\mathbf{Q}_{\\parallel}^{\\top}\\,\\mathrm{d}t-\\mathbf{S}\\odot\\mathrm{\\,dF}\\right]\\mathbf{D},}\\\\ &{\\mathrm{\\,dVdV}^{\\top}\\mathbf{V}\\mathbf{D}=-\\mathbf{V}\\left[\\mathbf{S}\\odot\\mathrm{\\,dF}\\right]\\left[\\mathbf{S}\\odot\\mathrm{\\,dF}\\right]\\mathbf{D}-\\left[\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\left[\\mathbf{S}\\odot\\mathrm{\\,dF}\\right]\\mathbf{D},}\\\\ &{\\mathrm{\\,dV}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}\\mathbf{V}=\\mathbf{V}\\left[\\mathbf{S}\\odot\\mathrm{\\,dF}\\right]\\mathrm{d}\\mathbf{F}+\\left[\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathrm{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\mathrm{d}\\mathbf{F},}\\\\ &{\\mathbf{V}\\mathrm{\\,d}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}\\mathbf{V}=-\\mathbf{V}\\left[\\mathbf{S}\\odot\\mathrm{\\,dF}\\right]\\mathrm{d}\\mathbf{F}+\\mathbf{V}\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathrm{d}\\mathbf{R}(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top})\\mathrm{d}\\mathbf{R}\\mathbf{V}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Adding the terms up we get, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{V}\\left[\\mathbf{Q}_{\\parallel}+\\mathbf{Q}_{\\parallel}^{\\top}\\right]\\mathbf{D}\\mathbf{d}t+\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{Q}_{\\perp}\\mathbf{D}\\mathbf{d}t}\\\\ &{-\\mathbf{V}\\left[\\mathbf{S}\\odot\\mathbf{d}\\mathbf{F}\\right]\\left[\\mathbf{S}\\odot\\mathbf{d}\\mathbf{F}\\right]\\mathbf{D}-\\left[\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\left[\\mathbf{S}\\odot\\mathbf{d}\\mathbf{F}\\right]\\mathbf{D}}\\\\ &{+\\left[\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\mathbf{d}\\mathbf{F}+\\mathbf{V}\\mathbf{D}^{-1}\\mathbf{V}^{\\top}\\mathbf{d}\\mathbf{R}(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top})\\mathbf{d}\\mathbf{R}\\mathbf{V}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{Q}_{\\perp}\\mathbf{D}\\mathbf{d}t-\\left[\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\left[\\mathbf{S}\\odot\\mathbf{d}\\mathbf{F}\\right]\\mathbf{D}+\\left[\\left(\\mathrm{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\mathbf{d}\\mathbf{F}=0.$ $\\begin{array}{r l}&{\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{Q}_{\\perp}\\mathbf{D}\\mathbf{d}t=\\left[\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\left[\\mathbf{S}\\odot\\mathbf{d}\\mathbf{F}\\right]\\mathbf{D}-\\left[\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\mathbf{d}\\mathbf{F},}\\\\ &{\\qquad\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{Q}_{\\perp}=\\left[\\left(\\mathbf{I}-\\mathbf{V}\\mathbf{V}^{\\top}\\right)\\mathbf{d}\\mathbf{R}\\mathbf{V}\\mathbf{D}^{-1}\\right]\\left[\\left[\\mathbf{S}\\odot\\mathbf{d}\\mathbf{F}\\right]\\mathbf{D}-\\mathbf{d}\\mathbf{F}\\right]\\mathbf{D}^{-1}}\\end{array}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{\\perp}=\\left[\\mathbf{dRVD}^{-1}\\right]\\left[\\left[\\mathbf{S}\\odot\\mathbf{dF}\\right]\\mathbf{D}-\\mathbf{dF}\\right]\\mathbf{D}^{-1}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This gives the expression for $\\mathbf{Q}_{\\perp}$ and this ends our computation. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.3. For any matrix $A\\in\\mathbb{R}^{n\\times m},B\\in\\mathbb{R}^{n\\times n}$ , $m\\times n$ -dimensional Brownian motion $\\mathbf{dB}_{t}$ , the following results hold on the covariance ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{B}_{t}\\boldsymbol{A}\\mathbf{d}\\mathbf{B}_{t}=\\boldsymbol{A}^{\\top}\\mathbf{d}t,}\\\\ {\\mathrm{d}\\mathbf{B}_{t}\\boldsymbol{B}\\mathbf{d}\\mathbf{B}_{t}^{\\top}=\\mathrm{tr}\\left(\\boldsymbol{B}\\right)\\mathbf{I}_{m}\\mathbf{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma D.4. With S defined in Equation (D.1), $\\mathbf{\\mathrm{d}}\\mathbf{F}\\;=\\;\\mathbf{\\mathrm{d}}\\mathbf{F}\\;=\\;\\mathbf{\\Sigma}\\mathbf{V}^{\\top}\\mathbf{\\mathrm{d}}\\mathbf{B}_{t}\\mathbf{c}^{\\top}\\;+\\;\\mathbf{c}\\mathbf{d}\\mathbf{B}_{t}^{\\top}\\mathbf{V}\\mathbf{\\Sigma}$ and $\\mathbf{dm}_{t}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\left(\\pmb{\\sigma}\\odot\\mathbf{\\mathrm{d}}{\\tilde{\\mathbf{B}}}_{t}\\right)$ . ", "page_idx": 22}, {"type": "text", "text": "$\\mathrm{d}\\mathbf{F}(\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F})=\\mathbf{c}\\mathbf{s}^{\\top}\\mathbf{S}\\mathrm{d}\\mathrm{i}\\mathrm{ag}\\left(\\mathbf{c}\\right)\\mathrm{d}t-\\mathbf{D}\\mathrm{d}\\mathrm{i}\\mathrm{ag}\\left(\\mathbf{S}\\mathrm{d}\\mathrm{i}\\mathrm{ag}\\left(\\mathbf{c}\\right)\\mathbf{c}\\right)\\mathrm{d}t+\\mathbf{D}\\mathrm{d}\\mathrm{i}\\mathrm{ag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathrm{d}\\mathrm{i}\\mathrm{ag}\\left(\\mathbf{c}\\right)\\mathrm{d}t.$ (D.6) Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{S}\\odot\\mathbf{dF}=\\left[\\operatorname{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathbf{diag}\\left(\\mathbf{dm}_{t}\\right)+\\operatorname{diag}\\left(\\mathbf{dm}_{t}\\right)\\mathbf{S}\\mathbf{diag}\\left(\\mathbf{c}\\right)\\right],\\quad\\quad}\\\\ {\\mathbf{dF}(\\mathbf{S}\\odot\\mathbf{dF})=\\left(\\mathbf{cdm}_{t}^{\\mathsf{T}}+\\mathbf{dm}_{t}\\mathbf{c}^{\\mathsf{T}}\\right)\\left[\\operatorname{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathbf{diag}\\left(\\mathbf{dm}_{t}\\right)+\\operatorname{diag}\\left(\\mathbf{dm}_{t}\\right)\\mathbf{S}\\mathbf{diag}\\left(\\mathbf{c}\\right)\\right],}\\\\ {=\\mathbf{cs}^{\\mathsf{T}}\\mathbf{S}\\mathbf{diag}\\left(\\mathbf{c}\\right)\\mathbf{d}t-\\mathbf{Ddiag}\\left(\\mathbf{S}\\mathbf{diag}\\left(\\mathbf{c}\\right)\\mathbf{c}\\right)\\mathbf{d}t+\\mathbf{D}\\mathbf{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathbf{diag}\\left(\\mathbf{c}\\right)\\mathbf{d}t.\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma D.5. With S defined in Equation (D.1), $\\mathbf{\\mathrm{d}}\\mathbf{F}\\;=\\;\\mathbf{\\mathrm{d}}\\mathbf{F}\\;=\\;\\mathbf{\\Sigma}\\mathbf{V}^{\\top}\\mathbf{\\mathrm{d}}\\mathbf{B}_{t}\\mathbf{c}^{\\top}\\;+\\;\\mathbf{c}\\mathbf{d}\\mathbf{B}_{t}^{\\top}\\mathbf{V}\\mathbf{\\Sigma}$ and $\\mathbf{dm}_{t}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\left(\\pmb{\\sigma}\\odot\\mathbf{\\mathrm{d}}{\\tilde{\\mathbf{B}}}_{t}\\right)$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathbf{S}\\odot\\operatorname{d}\\mathbf{F})(\\mathbf{S}\\odot\\operatorname{d}\\mathbf{F})=\\mathbf{D}\\operatorname{diag}\\left(\\mathbf{S}\\operatorname{diag}\\left(\\mathbf{c}\\right)^{2}\\mathbf{S}\\right)\\operatorname{d}t+\\operatorname{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathbf{D}\\mathbf{S}\\operatorname{diag}\\left(\\mathbf{c}\\right)\\operatorname{d}t.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F}\\right)=\\mathbf{S}\\odot\\left(\\mathbf{d}\\mathbf{m}_{t}\\mathbf{c}^{\\top}+\\mathbf{c}\\mathbf{d}\\mathbf{m}_{t}^{\\top}\\right),\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\Omega}}\\\\ {\\quad=\\operatorname{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{d}\\mathbf{m}_{t}\\right)+\\operatorname{diag}\\left(\\mathbf{d}\\mathbf{m}_{t}\\right)\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, computing the product, ", "page_idx": 22}, {"type": "text", "text": "( $\\begin{array}{r l}&{\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F})(\\mathbf{S}\\odot\\mathrm{d}\\mathbf{F})=\\left[\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{dm}_{t}\\right)+\\mathrm{diag}\\left(\\mathbf{dm}_{t}\\right)\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right)\\right]\\left[\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{dm}_{t}\\right)+\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right)\\right]\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{k}\\right)\\mathrm{d}\\mathbf{F}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{D}\\mathrm{diag}\\left(\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right)^{2}\\mathbf{S}\\right)\\mathrm{d}t+\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}\\mathbf{D}\\mathbf{S}\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathrm{d}t.}\\end{array}$ (dmt) Sdiag (c)] , ", "page_idx": 22}, {"type": "text", "text": "Lemma D.6. ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n(S\\odot\\mathrm{d}\\mathbf{F})\\mathrm{d}\\mathbf{m}_{t}\\mathbf{c}^{\\top}\\mathrm{d}\\mathbf{F}=\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n(S\\odot\\mathrm{d}\\mathbf{F})\\mathrm{d}\\mathbf{m}_{t}=\\left[\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathrm{Sdiag}\\left(\\mathbf{d}\\mathbf{m}_{t}\\right)+\\mathrm{diag}\\left(\\mathbf{d}\\mathbf{m}_{t}\\right)\\mathrm{Sdiag}\\left(\\mathbf{c}\\right)\\right]\\mathrm{d}\\mathbf{m}_{t}=\\mathrm{diag}\\left(\\mathbf{c}\\right)\\mathbf{S}(\\boldsymbol{\\sigma}\\odot\\boldsymbol{\\sigma})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The dichotomy between SGD and GD is revealed by theorems 4.1 and 4.2, the repulsive force between the eigenvalues of parameter matrix is discussed in theorems 5.1 and 5.2. Supporting experiments are discussed in section 7. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The limitations of the work are discussed in section 4 in a designated paragraph as well as in section 5 in the discussion of theorem 5.1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Proofs of the theorems that are not presented in the main body are presented in appendix B. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The data as well as the models and optimization algorithms used are discussed in the section 7 ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The code in the form of Jupyter notebook is provided and all the random seeds are fixed for the reproducibility purposes. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The setup is discussed in the section 7. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the graphs of the parameters evolution are accompanied with the $95\\%$ confidence interval calculated on the 20 runs with different random seeds. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All relevant information is stated in the appendix section C. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The research is theoretical and does not suggest any new model that can cause harm. All the data used is synthetic. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The work investigates the effects of well known algorithms on the simple models and doesn\u2019t suggest any new applications. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The work doesn\u2019t entail models or datasets releases. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: All relevant information is stated in the appendix section C. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]