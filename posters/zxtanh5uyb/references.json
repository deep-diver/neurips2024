{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report on GPT-4, a large language model that is relevant to the current work on continual learning in LLMs."}, {"fullname_first_author": "Dan Biderman", "paper_title": "Lora learns less and forgets less", "publication_date": "2024-05-09", "reason": "This paper proposes a novel parameter-efficient approach for continual learning in LLMs, which is directly related to the current work."}, {"fullname_first_author": "Aakanksha Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2023-00-00", "reason": "This paper introduces Pathways Language Model (PaLM), a large language model that is used as a baseline in the current work."}, {"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces LoRA, a low-rank adaptation technique that is used in the current work to improve the efficiency of continual learning."}, {"fullname_first_author": "Neil Houlsby", "paper_title": "Parameter-efficient transfer learning for nlp", "publication_date": "2019-00-00", "reason": "This paper provides a comprehensive overview of parameter-efficient transfer learning techniques for NLP, which are relevant to the current work."}]}