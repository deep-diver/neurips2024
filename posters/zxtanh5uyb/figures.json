[{"figure_path": "ZxtaNh5UYB/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison performance of the model after training task T2 with different layers replacement.", "description": "This figure shows the test accuracy on tasks T1 and T2 after training on T2 with different layer replacement strategies.  The x-axis represents the task order (T1 -> T2) and the replacement strategy applied. T1 represents either the Amazon Reviews or Yahoo Answers dataset, and T2 represents the other.  The y-axis shows the test accuracy.  It demonstrates the impact of transferring knowledge from a previously learned task (T1) to a new task (T2) by selectively replacing layers in T2's LoRA (Low-Rank Adaptation) parameters with those from T1. The results indicate that replacing certain layers from T1's LoRA parameters can improve performance on both tasks.", "section": "Focusing on knowledge transfer among tasks"}, {"figure_path": "ZxtaNh5UYB/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of our LB-CL framework. Starting with the pre-trained model including SVD weights of previous tasks, sensitivity metrics are calculated using a set of seed samples, facilitating the extraction of task-specific knowledge. Subsequently, the extracted layer triplets initialize SVD weights for the new task. Then, the new task is trained in an orthogonal subspace, employing orthogonal gradient projection to minimize forgetting.", "description": "This figure illustrates the LB-CL (Learn more but bother less Continual Learning) framework for continual learning in LLMs.  It starts with a pre-trained model.  Seed samples from the new task are used to calculate sensitivity metrics for SVD weights from previous tasks, extracting task-specific knowledge. This knowledge initializes the new task's SVD weights. The new task is then trained in an orthogonal subspace to the previous tasks, using orthogonal gradient projection to minimize forgetting.", "section": "2 Continual Learning Maestro: Learn More but Bother Less"}, {"figure_path": "ZxtaNh5UYB/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison of different initialization strategies across three orders of standard CL benchmark. The \"Avg\" value represents the average testing accuracy, illustrating how each strategy stabilizes learning performance.", "description": "This figure compares two different initialization strategies for the LB-CL model in a continual learning setting. The strategies are: (i) using the full low-rank matrix triplets (with \u03a3) from previous tasks, and (ii) using only the left and right singular vectors (without \u03a3) from the previous tasks' triplets. The comparison is made across three different task orders in a standard continual learning benchmark, and the average accuracy is presented. The figure shows that the initialization strategy without \u03a3 outperforms the other in average accuracy. The results demonstrate the impact of initialization strategies on model performance and stability during continual learning.", "section": "3.3 Discussions"}, {"figure_path": "ZxtaNh5UYB/figures/figures_7_2.jpg", "caption": "Figure 4: Impact analysis of seed sample quantity on the performance in LB-CL, evaluated across three orders of standard CL benchmark. This investigation highlights the influence of initial seed samples on model effectiveness.", "description": "This figure shows the impact of the number of seed samples used for sensitivity analysis on the performance of the LB-CL model.  The x-axis represents different numbers of seed samples (1, 2, 4, 8, 16, 32, 64), and the y-axis represents the average testing accuracy across three different task orders in the standard continual learning benchmark. Error bars represent the standard deviation. The results indicate that increasing the number of seed samples generally improves performance, but the gains diminish beyond a certain point, suggesting a diminishing return on increasing the number of samples. An optimal number of seed samples should be selected based on the balance between performance and computational cost.", "section": "3.3 Discussions"}, {"figure_path": "ZxtaNh5UYB/figures/figures_8_1.jpg", "caption": "Figure 5: Comparison of sensitivity scores and Fisher information of encoder and decoder Layers, and both results are the average results of three task orders in standard CL benchmark.", "description": "This figure compares the sensitivity scores and Fisher information across different layers (both encoder and decoder) of a model trained on a continual learning benchmark.  The data shown is averaged across three different task orders to showcase the general trends. The color intensity represents the magnitude of sensitivity or Fisher information, with darker colors indicating higher values. This visualization helps to identify which layers are most crucial for retaining knowledge from previous tasks (higher sensitivity/Fisher information) during continual learning, which can inform model optimization strategies.", "section": "3.3 Discussions"}, {"figure_path": "ZxtaNh5UYB/figures/figures_8_2.jpg", "caption": "Figure 5: Comparison of sensitivity scores and Fisher information of encoder and decoder Layers, and both results are the average results of three task orders in standard CL benchmark.", "description": "The figure shows a comparison of sensitivity scores and Fisher information for encoder and decoder layers across three different task orders in the standard continual learning benchmark.  The heatmaps illustrate the distribution of sensitivity and Fisher information across layers, offering insight into which layers are most sensitive to changes in input data.  This visualization aids in understanding which layers are most crucial for preserving previously learned knowledge during continual learning and effective knowledge transfer.", "section": "3.3 Discussions"}, {"figure_path": "ZxtaNh5UYB/figures/figures_15_1.jpg", "caption": "Figure 6: Order 1: Sensitivity scores and Fisher information of encoder and decoder Layers.", "description": "This figure shows the sensitivity scores and Fisher information for both encoder and decoder layers in the first task order.  It visually represents how sensitive different layers of the model are to changes in the parameters, indicating which layers are most important for learning in the first task. Darker colors represent higher sensitivity or Fisher information, indicating more importance.  The visualization helps in understanding the distribution of knowledge across different layers and informs strategies for parameter-efficient continual learning. ", "section": "3.3 Discussions"}, {"figure_path": "ZxtaNh5UYB/figures/figures_16_1.jpg", "caption": "Figure 6: Order 1: Sensitivity scores and Fisher information of encoder and decoder Layers.", "description": "This figure shows the sensitivity scores and Fisher information for each layer (both encoder and decoder) of the model for task order 1.  The color intensity represents the magnitude of the sensitivity or Fisher information, with darker colors indicating higher values.  This visualization helps to understand which layers are most important for learning in the continual learning setting, as indicated by their higher sensitivity scores.", "section": "3.3 Discussions"}, {"figure_path": "ZxtaNh5UYB/figures/figures_16_2.jpg", "caption": "Figure 6: Order 1: Sensitivity scores and Fisher information of encoder and decoder Layers.", "description": "This figure shows the sensitivity scores and Fisher information for each layer of both encoder and decoder in task order 1.  It visually represents the importance of different layers in the model for the first task in the continual learning sequence.  Darker colors indicate higher sensitivity or Fisher information, suggesting that these layers are more important for learning and retaining knowledge from that task.", "section": "A.4 Sensitivity Scores v.s. Fisher Information"}]