[{"figure_path": "ZxtaNh5UYB/tables/tables_1_1.jpg", "caption": "Table 1: Testing accuracy of T\u2081 and T\u2082 after training T\u2082 with different layer replacements, highlighting the best-performing strategy as shown in Fig. 1.", "description": "This table presents the results of an experiment designed to investigate the impact of replacing layers from a previously learned task's LoRA-based layers during the initialization of a new task's LoRA layers.  It shows the testing accuracy for both the old task (T\u2081) and the new task (T\u2082) under different layer replacement strategies. The 'no replace' column indicates the performance without any layer replacement. The other columns ('top 4', 'top 9', 'all') represent replacing the top 4, top 9, and all layers, respectively. The highest accuracy for both tasks is highlighted in bold, indicating the best replacement strategy.", "section": "1 Introduction"}, {"figure_path": "ZxtaNh5UYB/tables/tables_6_1.jpg", "caption": "Table 2: Testing performance on two standard CL benchmarks with T5-large.", "description": "This table presents the results of continual learning experiments using the T5-large model on two benchmarks: a standard benchmark with three task orders and a large-number-of-tasks benchmark with three task orders.  The table compares the performance of LB-CL against several baseline methods (SeqFT, SeqLoRA, IncLoRA, SeqSVD, Replay, EWC, LwF, L2P, LFPT5, L-CL, B-CL, NLNB-CL, O-LORA, ProgPrompt, PerTaskFT, and MTL).  Average Accuracy (AA) is reported, which is the mean accuracy across all tasks after training on the last task.", "section": "3.2 Main Results"}, {"figure_path": "ZxtaNh5UYB/tables/tables_7_1.jpg", "caption": "Table 3: Comparison of training computation cost between LB-CL and O-LORA.", "description": "This table compares the GPU memory usage and the number of training parameters per task for both the O-LORA and LB-CL methods. It shows that LB-CL uses slightly more GPU memory but has a comparable number of training parameters compared to O-LORA, suggesting similar computational efficiency.", "section": "3.3 Discussions"}, {"figure_path": "ZxtaNh5UYB/tables/tables_8_1.jpg", "caption": "Table 4: Comparisons of different rank r of low-rank matrix. This experiment is conducted based on T5-large in standard CL benchmark.", "description": "This table presents the results of an experiment comparing different ranks (r) of low-rank matrices in a continual learning setting using the T5-large model.  The experiment is performed on a standard continual learning benchmark. The table shows the average accuracy across three different task orders (Order 1, Order 2, Order 3) for each rank (r=2,4,8,16). The \"Std\" row shows the standard deviation across the three orders for each rank, providing a measure of the consistency of the performance.", "section": "3.2 Main Results"}, {"figure_path": "ZxtaNh5UYB/tables/tables_8_2.jpg", "caption": "Table 2: Testing performance on two standard CL benchmarks with T5-large.", "description": "This table presents the results of the continual learning experiments using the T5-large model on two benchmarks.  The table compares the average accuracy across three different task orders for the LB-CL method against several baseline methods, including SeqFT, SeqLoRA, IncLoRA, SeqSVD, Replay, EWC, LwF, L2P, LFPT5, L-CL, B-CL, NLNB-CL, O-LORA, and ProgPrompt.  PerTaskFT and MTL are also included as upper and lower performance bounds, respectively.  The results show the performance of LB-CL compared to the baselines across different task sequences.", "section": "3.2 Main Results"}, {"figure_path": "ZxtaNh5UYB/tables/tables_14_1.jpg", "caption": "Table 6: The details of 15 datasets used in our CL experiments. NLI denotes natural language inference, QA denotes questions and answers task. The first five tasks correspond to the standard CL benchmark, all other tasks are used in long-sequence experiments.", "description": "This table lists the 15 datasets used in the continual learning experiments of the paper.  The first five datasets are from the standard continual learning benchmark, while the remaining 10 are from GLUE, SuperGLUE, and IMDB.  Each dataset's category, task type, domain, and evaluation metric (Accuracy) are specified.", "section": "3 Experiments"}, {"figure_path": "ZxtaNh5UYB/tables/tables_14_2.jpg", "caption": "Table 7: Six different task sequence orders utilized in continual learning experiments. Orders 1-3 follow the standard continual learning benchmark as established by previous research, focusing on a more traditional task sequence. Orders 4-6 customized for long-sequence experimentation, encompass 15 tasks each and are structured according to the methodologies outlined in [35].", "description": "This table presents six different task sequences used in the continual learning experiments. The first three sequences are based on the standard continual learning benchmark, while the last three are designed for evaluating performance on a larger number of tasks. Each sequence specifies the order in which the 15 datasets (or a subset for the first three) are presented to the model during training.", "section": "A.3 Datasets"}, {"figure_path": "ZxtaNh5UYB/tables/tables_15_1.jpg", "caption": "Table 2: Testing performance on two standard CL benchmarks with T5-large.", "description": "This table presents the results of the proposed LB-CL method and several baseline methods on two continual learning benchmarks using the T5-large model.  The first benchmark is a standard continual learning benchmark with five text classification tasks, evaluated across three different task orders. The second benchmark features fifteen datasets from three different sources and is evaluated with three different orders.  The results show the average accuracy across all tasks after training on the final task for each method and task order.  The table demonstrates the superior performance of LB-CL compared to existing state-of-the-art methods in continual learning for LLMs.", "section": "3.2 Main Results"}, {"figure_path": "ZxtaNh5UYB/tables/tables_15_2.jpg", "caption": "Table 9: Comparison of ROUGE score between LB-CL and O-LORA", "description": "This table compares the average ROUGE-L scores (measuring the longest common subsequence between predicted and reference summaries) achieved by LB-CL and O-LORA across three different task orders in the standard Continual Learning benchmark.  ROUGE-L is used to assess the quality of summaries generated by each method. The table demonstrates the improved performance of LB-CL compared to O-LORA across various ordering of tasks.", "section": "A.5 Comparison of ROUGE score"}]