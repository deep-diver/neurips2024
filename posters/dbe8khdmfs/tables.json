[{"figure_path": "dBE8KHdMFs/tables/tables_4_1.jpg", "caption": "Table 1: Performance Comparison on the MNIST Model", "description": "This table compares the performance of four different neural ordinary differential equation (NODE) based models (NODE, ANODE, SONODE, and CSODE) on the MNIST image classification task.  The metrics used for comparison are test error rate, the number of parameters, floating-point operations per second (FLOPS), average batch processing time, and peak memory usage.  The results show that while CSODE has slightly better performance than other models (0.39% test error rate), the computational cost is comparable across all four models.", "section": "5 Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/tables/tables_7_1.jpg", "caption": "Table 2: The table categorizes NN models for time-series prediction in complex dynamical systems into three groups: Group A includes traditional models like MLP and RNN; Group B features ODE-based models including the NODE, ANODE, SONODE, and CSODE models; Group C presents CSODE-Adapt with convolutional layers. The best-performing models are highlighted in blue and the second-best in brown.", "description": "This table compares the performance of various neural network models (traditional MLP and RNN, ODE-based NODE, ANODE, SONODE, and CSODE, and CSODE-Adapt) on three complex time series prediction tasks (Hindmarsh-Rose neuron dynamics, Reaction-Diffusion system, and Shallow Water Equations).  The models are grouped into categories based on their architecture and the best-performing models in each task are highlighted.", "section": "6.4 Experimental Results and Analysis"}, {"figure_path": "dBE8KHdMFs/tables/tables_8_1.jpg", "caption": "Table 3: Performance comparison with observation-aware neural networks. Group A includes observation-aware baselines like ODE-RNN and Neural CDE; Group B contains our base model CSODE; Group C showcases CSODE-Adapt with enhanced observation mechanisms. Best performing models are marked in blue, second-best in brown.", "description": "This table compares the performance of CSODE and its variant (CSODE-Adapt) against other observation-aware neural network models (Neural CDE, ODE-RNN) across three different tasks: Character Trajectories prediction with varying levels of missing data, PhysioNet Sepsis Prediction with and without observation intensity, and Reaction-Diffusion system modeling.  The metrics used to evaluate performance vary depending on the task, and include test accuracy, AUC, MSE, MAE, and Chamfer Distance.  The best performing model in each category is highlighted in blue, and the second-best in brown.  The table shows that CSODE and its variant perform competitively with state-of-the-art methods, sometimes even surpassing them in specific tasks.", "section": "6.5 Comparison with Observation-aware Baselines"}, {"figure_path": "dBE8KHdMFs/tables/tables_16_1.jpg", "caption": "Table 1: Performance Comparison on the MNIST Model", "description": "This table presents a comparison of the performance of several neural ordinary differential equation (NODE) based models on the MNIST image classification dataset.  The models compared include NODE, Augmented Neural ODE (ANODE), Second Order Neural ODE (SONODE), and ControlSynth Neural ODE (CSODE).  The metrics used for comparison are test error rate, number of parameters, floating-point operations per second (FLOPS), average batch processing time, and peak memory usage. The results show that CSODE achieves a slightly better test error rate (0.39%) compared to other models, with comparable performance in terms of computational efficiency and memory usage.", "section": "5 Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/tables/tables_16_2.jpg", "caption": "Table 1: Performance Comparison on the MNIST Model", "description": "This table compares the performance of various Neural Ordinary Differential Equation (NODE) based models on the MNIST image classification task.  Metrics include test error rate, number of parameters, floating-point operations per second (FLOPS), average batch processing time, and peak memory usage. The models compared are NODE, Augmented Neural ODE (ANODE), Second Order Neural ODE (SONODE), and ControlSynth Neural ODE (CSODE).  The table shows that CSODE achieves a slightly lower test error rate than the other models while maintaining comparable computational efficiency.", "section": "5 Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/tables/tables_16_3.jpg", "caption": "Table 1: Performance Comparison on the MNIST Model", "description": "This table compares the performance of various neural ordinary differential equation (NODE) based models on the MNIST image classification task.  The models compared include NODE, Augmented Neural ODE (ANODE), Second Order Neural ODE (SONODE), and ControlSynth Neural ODE (CSODE).  For each model, the table reports the test error rate (%), the number of parameters (#Params), floating-point operations per second (FLOPS), average batch processing time (Time in seconds), and peak memory usage (MB). This allows for a direct comparison of the computational efficiency and resource requirements of each model.", "section": "5 Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/tables/tables_17_1.jpg", "caption": "Table 2: The table categorizes NN models for time-series prediction in complex dynamical systems into three groups: Group A includes traditional models like MLP and RNN; Group B features ODE-based models including the NODE, ANODE, SONODE, and CSODE models; Group C presents CSODE-Adapt with convolutional layers. The best-performing models are highlighted in blue and the second-best in brown.", "description": "This table compares the performance of various neural network models (traditional MLP and RNN, ODE-based NODE, ANODE, SONODE, and CSODE, and CSODE-Adapt) on three complex dynamical systems: Hindmarsh-Rose neuron, Reaction-Diffusion, and Shallow Water Equation.  The models are grouped by their architectural approach, and their performance is evaluated using MSE, MAE, R2 (for Hindmarsh-Rose only), and CD (for Reaction-Diffusion and Shallow Water only).  The best and second-best performing models in each group and task are highlighted.", "section": "Experimental Results and Analysis"}, {"figure_path": "dBE8KHdMFs/tables/tables_17_2.jpg", "caption": "Table 8: Performance comparison of Transformer, TLODE, and CSTLODE on Hindmarsh-Rose Model, Reaction-Diffusion Model, and Shallow Water Equations datasets. The best and second best results are highlighted in blue and brown, respectively.", "description": "This table compares the performance of three different models (Transformer, TLODE, and CSTODE) on three different datasets (Hindmarsh-Rose, Reaction-Diffusion, and Shallow Water Equations).  The performance is measured using MSE, MAE, R2 (for Hindmarsh-Rose only), and CD.  The best performing model for each metric and dataset is highlighted in blue, with the second-best highlighted in brown.  This table illustrates the relative effectiveness of integrating Neural ODEs with Transformer layers, and the benefit of adding a control term (as in CSTODE).", "section": "6.5 Comparison with Observation-aware Baselines"}, {"figure_path": "dBE8KHdMFs/tables/tables_19_1.jpg", "caption": "Table 9: MAE loss comparison for CSODE, NODE, and ANODE models with different widths in the Reaction-Diffusion task. Lower values indicate better performance.", "description": "This table presents a comparison of the Mean Absolute Error (MAE) achieved by three different neural ordinary differential equation (NODE) models \u2013 CSODE, NODE, and ANODE \u2013 across various network widths (128, 256, 512, 1024, and 2048) when applied to the Reaction-Diffusion task.  Lower MAE values indicate better model performance. The table helps illustrate how the models' accuracy changes with varying network capacity.", "section": "H.1 Comparison with Neural ODE and Augmented Neural ODE"}, {"figure_path": "dBE8KHdMFs/tables/tables_19_2.jpg", "caption": "Table 10: MSE loss comparison for CSODE, NODE, and ANODE models with different widths in the Reaction-Diffusion task. Lower values indicate better performance.", "description": "This table presents a comparison of the Mean Squared Error (MSE) achieved by three different neural ordinary differential equation (NODE) models: ControlSynth Neural ODEs (CSODEs), standard NODEs, and Augmented NODEs (ANODEs).  The comparison is made across varying network widths (128, 256, 512, 1024, and 2048) in the context of the Reaction-Diffusion task.  Lower MSE values indicate better model performance.  The table shows that CSODE consistently outperforms the other two models across all widths, demonstrating its superior performance and efficiency in this task.", "section": "H.1 Comparison with Neural ODE and Augmented Neural ODE"}, {"figure_path": "dBE8KHdMFs/tables/tables_21_1.jpg", "caption": "Table 11: Mean Absolute Error (MAE) Comparison between Euler and Dopri5 Solvers on the Reaction-Diffusion Task", "description": "This table compares the Mean Absolute Error (MAE) achieved by using two different numerical solvers, Euler and Dopri5, for three different Neural Ordinary Differential Equation (NODE) models: NODE, ANODE, and CSODE.  The comparison highlights the impact of solver choice on the accuracy of these models in solving the Reaction-Diffusion problem.  Lower MAE values indicate better performance.", "section": "I.1.2 Results and Discussion"}, {"figure_path": "dBE8KHdMFs/tables/tables_22_1.jpg", "caption": "Table 1: Performance Comparison on the MNIST Model", "description": "This table compares the performance of four different neural ordinary differential equation (NODE) models on the MNIST image classification task.  The models compared are NODE, ANODE, SONODE, and CSODE. The metrics used for comparison include test error rate, the number of parameters, floating-point operations per second (FLOPS), average batch processing time, and peak memory usage.  The table shows that CSODE achieves a slightly lower test error rate than the other models, while maintaining comparable computational performance in terms of FLOPS, time, and memory usage.", "section": "5 Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/tables/tables_23_1.jpg", "caption": "Table 1: Performance Comparison on the MNIST Model", "description": "This table presents a performance comparison of four different neural ordinary differential equation (NODE) based models on the MNIST image classification task.  The models compared are NODE, ANODE, SONODE, and CSODE.  The metrics used for comparison are test error rate, number of parameters, floating-point operations per second (FLOPS), average batch processing time, and peak memory usage. The results demonstrate that although CSODEs theoretically involves iterative computation of g(u), it does not significantly reduce computational efficiency in practice.", "section": "5 Preliminary Experiments"}]