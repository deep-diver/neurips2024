[{"figure_path": "dBE8KHdMFs/figures/figures_1_1.jpg", "caption": "Figure 1: Schematic of the CSODEs solver, showing integration via NNs at one time step. Using the forward Euler method as an example, it shows how ut and xt evolve through the update neural function h(\u00b7) and NNs g(\u00b7), A1f1(W1\u00b7), ..., \u0410\u043c\u0192\u043c (W\u043c.) to yield ut+\u25b3t and xt+\u25b3t as the next variables.", "description": "This figure illustrates the computational flow of the ControlSynth Neural ODEs (CSODEs) solver using the forward Euler method as an example. It shows how the control input (ut) and the state vector (xt) are updated at each time step (\u0394t) using a combination of neural networks (NNs) and matrix operations. Specifically, the control input is processed by the NN g(\u00b7), and the state vector is transformed by the NNs A1f1(W1\u00b7), ..., \u0410\u043c\u0192\u043c(W\u043c.) and the matrix A0. The outputs of these transformations are then combined and used to update the control input and the state vector for the next time step. The function h(\u00b7) represents the update neural function which handles the process of updating the variables using the results from the NNs and matrix operations.", "section": "ControlSynth Neural Ordinary Differential Equations"}, {"figure_path": "dBE8KHdMFs/figures/figures_4_1.jpg", "caption": "Figure 2: Comparison of Mean Absolute Error (MAE) loss reduction curves across 1500 training epochs between CSODE and NODE models until convergence.", "description": "This figure shows the mean absolute error (MAE) loss curves for both the CSODE and NODE models during training.  The x-axis represents the training epoch (iteration), and the y-axis shows the MAE loss. The CSODE model demonstrates faster convergence to a lower MAE loss compared to the NODE model, indicating improved learning efficiency and potentially better prediction accuracy.", "section": "5 Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/figures/figures_4_2.jpg", "caption": "Figure 3: Qualitative comparison of the CSODE (top) and the NODE (bottom) predictions against ground truth trajectories at 400, 600, and 800 training epochs.", "description": "This figure compares the performance of CSODE and NODE models in learning and predicting spiral trajectories.  The top row shows the results for CSODE, while the bottom row shows the results for NODE. Each column represents a different training epoch (400, 600, and 800). The orange dots represent the noisy sampled data points used for training, the red line represents the original trajectory, and the blue line represents the predicted trajectory from each model. The figure demonstrates that CSODE is able to learn the trajectory more accurately and generalize better than NODE.", "section": "5 Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparison of the CSODE (top) and the NODE (bottom) predictions against ground truth trajectories at 400, 600, and 800 training epochs.", "description": "This figure compares the prediction results of CSODE and NODE models against the actual ground truth trajectory at different training epochs (400, 600, and 800). It visually demonstrates that CSODE model achieves higher prediction accuracy and better aligns with the original trajectories compared to the NODE model, especially beyond the observation period.  This showcases the superior generalization and extrapolation capabilities of CSODEs for predicting future states.", "section": "Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/figures/figures_7_2.jpg", "caption": "Figure 3: Qualitative comparison of the CSODE (top) and the NODE (bottom) predictions against ground truth trajectories at 400, 600, and 800 training epochs.", "description": "This figure compares the performance of CSODE and NODE models in predicting trajectories.  It shows the predicted trajectories for both models alongside the true trajectory at three different training epochs (400, 600, and 800). This allows for a visual assessment of how well each model learns and generalizes over time, and highlights the superior performance of CSODE, which is much closer to the ground truth, especially at later epochs.  The sampled trajectory with noise illustrates the robustness of the models in the presence of noise.", "section": "Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/figures/figures_9_1.jpg", "caption": "Figure 6: Performance comparison of CSODE models with varying numbers of sub-networks. The scatter plot visualizes the performance trajectory of CSODE models during training, where each point represents the training loss (x-axis) and validation loss (y-axis) at a specific epoch. Models with 1\u20135 sub-networks are compared, each depicted in a distinct color while maintaining a fixed network width of 512 neurons. Points clustering near the bottom-left corner indicate superior model performance, while their distribution relative to the diagonal reveals the balance between training and validation performance.", "description": "This figure presents a performance comparison of CSODE models with varying numbers of sub-networks.  A heatmap shows the validation loss for different combinations of network width and number of subnetworks. A scatter plot displays the relationship between training and validation losses during training for models with 1 to 5 subnetworks, at a fixed network width of 512.  Models with lower losses (closer to the bottom-left corner) perform better. The scatter plot also illustrates the balance between training and validation performance.", "section": "Model Scaling Experiment"}, {"figure_path": "dBE8KHdMFs/figures/figures_18_1.jpg", "caption": "Figure 3: Qualitative comparison of the CSODE (top) and the NODE (bottom) predictions against ground truth trajectories at 400, 600, and 800 training epochs.", "description": "This figure compares the prediction results of CSODE and NODE models against the ground truth trajectory at three different training epochs (400, 600, and 800).  It visually demonstrates CSODE's superior performance in accurately learning and extrapolating the trajectory, even beyond the observed data points, showcasing its better understanding of underlying dynamics.", "section": "Preliminary Experiments"}, {"figure_path": "dBE8KHdMFs/figures/figures_18_2.jpg", "caption": "Figure 7: Visual comparison of neural network predictions on different dynamic systems.", "description": "This figure presents a qualitative comparison of the predictions generated by three different neural ordinary differential equation (NODE) models: NODE, Augmented NODE (ANODE), and ControlSynth NODE (CSODE), against the ground truth for three distinct dynamic systems.  These systems are the Hindmarsh-Rose neuron model, a Reaction-Diffusion system, and the Shallow Water Equations. The figure shows 3D plots visualizing the initial conditions, ground truth dynamics, and predictions from each model.  This allows for a visual assessment of the accuracy and ability of each model to capture complex and nonlinear dynamical behaviors.  Note that the CSODE demonstrates improved ability to capture the detail of the underlying dynamics.", "section": "Additional Qualitative Prediction Results"}, {"figure_path": "dBE8KHdMFs/figures/figures_18_3.jpg", "caption": "Figure 7: Visual comparison of neural network predictions on different dynamic systems.", "description": "This figure presents a qualitative comparison of the predictions made by three different neural ordinary differential equation (NODE) models: NODE, ANODE, and CSODE.  Each model's performance is shown alongside the ground truth for three different dynamical systems: the Hindmarsh-Rose neuron model (a), the Reaction-Diffusion system (b), and the Shallow Water Equations (c). The visualization helps illustrate how each model approximates the complex dynamics of these systems, highlighting CSODE's improved accuracy and ability to learn the time-dependent structures in the data.", "section": "Additional Qualitative Prediction Results"}, {"figure_path": "dBE8KHdMFs/figures/figures_20_1.jpg", "caption": "Figure 8: The left figure shows the loss curves for different sub-network numbers at a fixed width (512), and the right figure displays the loss curves for different widths with a fixed sub-network number (3), reflecting how changes in configuration impact model performance.", "description": "This figure shows two plots visualizing the training and validation loss curves for CSODEs under different configurations. The left plot shows the results of varying the number of sub-networks while keeping the network width fixed at 512. The right plot, on the other hand, varies the network width while keeping the number of sub-networks constant at 3.  Each plot demonstrates how changes in these architectural parameters affect the model's convergence and overall performance during training.  The plots illustrate the impact of different network configurations on the model's training and validation losses, providing insights into the trade-offs between model complexity and performance.", "section": "H.2 Convergence Study in ControlSynth Neural ODE Model Scaling"}]