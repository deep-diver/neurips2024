{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the transformer architecture, the foundation of the model this paper enhances."}, {"fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "publication_date": "2023-07-10", "reason": "This paper provides a benchmark suite for evaluating language models, which the current paper uses for its empirical evaluations."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper highlighted the few-shot learning capabilities of large language models, a concept relevant to the current paper's approach."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-06-01", "reason": "This paper established the multitask learning nature of language models, which the current paper builds upon."}, {"fullname_first_author": "Bowen Peng", "paper_title": "Yarn: Efficient context window extension of large language models", "publication_date": "2023-09-01", "reason": "This paper proposed a temperature scaling method for improving the context window of transformers, which is closely related to the current paper\u2019s approach."}]}