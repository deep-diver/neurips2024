[{"type": "text", "text": "Selective Attention: Enhancing Transformer through Principled Context Control ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuechen Zhang Xiangyu Chang Mingchen Li University of Michigan University of California, Riverside University of Michigan zxuechen@umich.edu cxian008@ucr.edu milii@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Amit Roy-Chowdhury University of California, Riverside amitrc@ece.ucr.edu ", "page_idx": 0}, {"type": "text", "text": "Jiasi Chen University of Michigan jiasi $@$ umich.edu ", "page_idx": 0}, {"type": "text", "text": "Samet Oymak University of Michigan oymak $@$ umich.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The attention mechanism within the transformer architecture enables the model to weigh and combine tokens based on their relevance to the query. While selfattention has enjoyed major success, it notably treats all queries $q$ in the same way by applying the mapping $V^{\\top}\\mathrm{softmax}(K q)$ , where $V,K$ are the value and key embeddings respectively. In this work, we argue that this uniform treatment hinders the ability to control contextual sparsity and relevance. As a solution, we introduce the \u201cSelective Self-Attention\u201d (SSA) layer that augments the softmax nonlinearity with a principled temperature scaling strategy. By controlling temperature, SSA adapts the contextual sparsity of the attention map to the query embedding and its position in the context window. Through theory and experiments, we demonstrate that this alleviates attention dilution, aids the optimization process, and enhances the model\u2019s ability to control softmax spikiness of individual queries. We also incorporate temperature scaling for value embeddings and show that it boosts the model\u2019s ability to suppress irrelevant/noisy tokens. Notably, SSA is a lightweight method which introduces less than $0.5\\%$ new parameters through a weight-sharing strategy and can be fine-tuned on existing LLMs. Extensive empirical evaluations demonstrate that SSA-equipped models achieve a noticeable and consistent accuracy improvement on language modeling benchmarks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Attention is a pivotal mechanism in modern machine learning that allows the model to focus on and retrieve different parts of the data, enhancing its ability to capture contextual relationships across time and space. While it was originally developed for NLP tasks through the transformer architecture, it has enjoyed widespread success in other domains such as computer vision, sequence modeling, and reinforcement learning [45, 35, 5, 9, 39]. ", "page_idx": 0}, {"type": "text", "text": "The canonical self-attention mechanism is a sequence-to-sequence map that outputs $X\\rightarrow\\mathbb{S}(Q K^{\\top})V$ where $\\mathbb{S}(\\cdot)$ denotes the row-wise softmax nonlinearity and $\\b{Q}$ , $\\pmb{K}$ , $V$ are the query, key, and value embeddings obtained through linear projections of the input sequence $X$ . Through this process, for each query, the model creates a query-dependent composition of the input context. Importantly, the model has to accomplish two objectives: namely, capturing semantic similarity between tokens and also adjusting the contextual sparsity. Here, semantic similarity can be quantified through the angle between key-query embeddings and the contextual sparsity through the spikiness of the attention map. While the importance of the former is clear, the latter is equally important given the fact that attention maps tend to be sparse in practice [8, 43, 37, 6]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we argue that these two objectives can be at odds and, as a result, the self-attention layer may struggle to achieve both objectives simultaneously due to its relatively inflexible parameterization. To address this issue, we propose the Selective Self-Attention (SSA) layer that aims to decouple semantic similarity from contextual sparsity. SSA relies on a principled application of temperaturescaling (TS) to query and value embeddings. For instance, given query embedding $\\pmb q$ , rather than computing $\\mathbb{S}(K\\pmb q)$ , SSA computes $\\mathbb{S}(\\tau(q)\\cdot K q)$ where $\\tau(\\pmb q)$ is the learnable inverse-temperature. Intuitively, this allows for better control of the context window because $\\tau(\\pmb q)$ can control contextual sparsity while the projection matrices $W_{k},W_{q}$ can fully focus on controlling semantic similarity. Figure 1 shows an example of the learned token temperatures when training the Pythia model with SSA. In summary, we make the following theoretical and empirical contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Query selectivity. We show that introducing TS to the query embeddings enhances the model\u2019s capability to express a target attention map with smaller parameter norms (Proposition 1). This is particularly so when attention maps exhibit large spikiness variations across different queries. Real experiments corroborate that TS leads to sparser attention maps with smaller norms. See Figure 3 as an illustration. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Value selectivity. We formalize the benefti of TS on value embeddings through a denoising perspective. Namely, we describe a denoising task where the linear value projection fails to fliter the noisy tokens, and demonstrate how nonlinear scaling can boost denoising capability. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Positional temperature. We incorporate a term that adjusts the query-temperature according to the position in the context window. We show that this term can mitigate the dilution of attention scores caused by the increasing context length. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Modularity and parameter-efficiency of SSA. Selective Attention is accomplished by introducing a parameter-efficient temperature module that can be easily integrated into existing attention models. In practice, this introduces $5\\%$ additional parameters to the model. We also introduce a weight sharing strategy that reduces the number of parameter overhead to less than $0.5\\%$ while maintaining the beneftis of SSA. We reuse the attention weights within the temperature module, which results in negligible inference/latency overhead since no additional matrix multiplication is required. These methods only involve vector dot-products (at the output layer of the temperature module) and elementwise scaling of matrices. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Empirical benefits. Our evaluations on the NLP benchmarks of Wikitext [27], Lambada [32], Piqa [4], Hella [50], Winogrande [38], Arc-E, and Arc-C [10] demonstrate that Selective Attention noticeably improves language modeling performance. These benefits are consistent across various models including GPT-2 [34], Pythia [3], Llama [44] and Llama3 [16], as well as during both fine-tuning and pre-training, as shown in Table 3. Additionally, evaluations on the passkey retrieval task [33, 29] reveal that SSA substantially enhances the retrieval capabilities of the transformer, shown in Table 4. ", "page_idx": 1}, {"type": "text", "text": "People think focus means saying yes to the thing you've got to focus on. But that 'snot what it means at all. It means saying no to the hundred other good ideas that there are.You have to pick carefully ", "page_idx": 1}, {"type": "text", "text": "Figure 1: A quotation by Steve Jobs. We highlight tokens according to their temperatures learned by the SSA layer. Darker colors correspond to lower temperatures and receive a sparser attention map. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Temperature Scaling (TS): TS is a fundamental method for controlling model behavior, influencing aspects such as stochasticity of generative LLMs, calibration and uncertainty, and imbalanced data, as highlighted in several studies [26, 22, 51]. Related to us, previous research [33, 49, 7] has also proposed utilizing a temperature term in the softmax function to enhance the length extrapolation capabilities of transformers. For instance, Yarn [33] scales the attention logits as a function of the sequence length and shows that this improves the perplexity when extending the context window. Our work provides a formal justification for the temperature scaling rule proposed in Yarn (see Proposition 2) and also highlights the value of adapting temperature to the individual positions. Importantly, our approach is differentiable and obviates the need for grid search required by prior works. Since we don\u2019t focus on length generalization, we have found that position-aware temperature has a much smaller benefit compared to token-aware temperature, which is our primary contribution. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Gating mechanisms and selectivity: Various strategies have been developed to mitigate the impact of uninformative inputs in model training and processing. Gating mechanisms, originally introduced through LSTMs [19], have been proposed to selectively filter or scale down the input sequence [48, 13, 14, 25, 40]. Very recent sequence models such as Mamba (a.k.a. selective state-space model) and Griffin also incorporate gating to boost language modeling [18, 46, 53, 14, 21]. These models leverage input-dependent gating to ensure parallellizable training and enjoyed noticeable success. These methodologies inspired our approach, which incorporates TS to augment the selection capabilities of the attention layer. Specifically, TS can be viewed as an instance of gating that selectively passes or suppresses tokens to provide better control of contextual sparsity and relevance. In this light, our work also provides a mechanistic understanding of how gating mechanism can aid self-attention to improve its expressive capabilities. ", "page_idx": 2}, {"type": "text", "text": "Mechanistic understanding of transformers: The importance of transformer-based models led to many research efforts on developing a stronger understanding of various aspects of transformer and attention [30, 47, 15]. While it is impossible to cover all of these works, it is evident that capability to select relevant features and promote contextual sparsity is crucial for the ability of language models to perform complex tasks such as reasoning [23, 1, 43, 52]. These have provided inspiration for us to pursue an enhanced modeling of attention\u2019s spikiness (e.g. as in Figure 3). The experiments in Figure 3 are inspired by the recent work [20] which characterizes the learnability of a ground-truth attention model via the next-token prediction objective in terms of the associated Markov transition matrix. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology: Selective Attention Layer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let us recap the self-attention mechanism in Transformer [45]. Canonical softmax attention admits an input sequence $X=[\\pmb{x}_{1}\\ .\\ .\\ .\\ \\pmb{x}_{L}]^{\\intercal}\\in\\mathbb{R}^{L\\times d}$ of length $L$ with embedding dimension $d$ . We then project $X$ to obtain key, query, and value embeddings $(\\pmb{K}=\\pmb{X}\\pmb{W}_{k}$ , $Q=X{\\pmb{W}}_{q}$ , $V=X{\\cal W}_{\\nu}.$ ) and compute the output of the dot-product attention as $\\begin{array}{r}{A t t(\\pmb{Q},\\pmb{K},\\pmb{V})=\\mathbb{S}(\\frac{\\pmb{Q}\\pmb{K}^{\\top}}{\\sqrt{d}})\\pmb{V}}\\end{array}$ . Here $\\S(\\cdot):\\mathbb{R}^{L}\\to\\mathbb{R}_{+}^{L}$ denotes the softmax nonlinearity that applies row-wise and $\\pmb{W_{q}},\\pmb{W_{k}},\\pmb{W_{\\nu}}\\in\\mathbb{R}^{d\\times d}$ are learnable weight matrices. In this paper, we mainly focus on casual language modeling where each token can only attend to previous tokens in the input. ", "page_idx": 2}, {"type": "text", "text": "The uniform treatment of all tokens through the same softmax map could hinder the ability to control contextual sparsity and relevance. For instance, it has been observed that current Transformer language models suffer from an attention dilution issue: the longer the input sequence, the flatter the attention distribution [49, 7]. A natural solution to the dispersed attention issue is to sharpen the self-attention distribution. Selective Attention aims to provide a general strategy to control spikiness of the softmax adaptive to the query and value embedding, as well as the position of the token. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Selective Self-Attention (SSA)). Let $X=[\\pmb{x}_{1}\\ .\\ .\\ .\\ \\pmb{x}_{L}]^{\\intercal}\\in\\mathbb{R}^{L\\times d}$ be an input sequence. $L e t\\,\\tau_{k/q/\\nu}(\\cdot):\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ be the inverse-temperature functions for keys, queries, and values, respectively. Then the embeddings for keys $(K)$ , queries $(Q)$ , and values $(V)$ are computed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\pmb K}=\\tau_{k}({\\pmb X})\\odot{\\pmb X}{\\pmb W}_{k},\\ {\\pmb Q}=\\tau_{q}({\\pmb X})\\odot{\\pmb X}{\\pmb W}_{q},\\ {\\pmb V}=\\tau_{\\nu}({\\pmb X})\\odot{\\pmb X}{\\pmb W}_{\\nu}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\odot$ denotes the elementwise product that assigns temperature to individual tokens. Selective Self-Attention (SSA) is then computed as $\\mathbb{S}(\\frac{Q K^{\\top}}{\\sqrt{d}})V$ . ", "page_idx": 2}, {"type": "text", "text": "In essence, SSA incorporates a temperature modulation mechanism into the attention framework to enhance selectivity and context control. The inverse-temperature function $\\tau(\\cdot)$ is data-dependent, allowing for dynamic adjustment of attention across different parts of the input sequence. In practice, we choose $\\tau_{k/q/\\nu}$ to be a scalar valued function as vector-valued temperature does not provide a significant advantage. It is also worth mentioning that we don\u2019t restrict $\\tau_{k/q/\\nu}$ to be non-negative. As a result, our temperature scaling strategy can be seen as an application of scalar gating on ${\\mathrm{K}}/{\\mathrm{O}}/{\\mathrm{V}}$ embeddings, and hence, the SSA layer could also be referred to as Scalar-Gated Attention (SGA) layer. The GitHub repo containing SSA implementation is provided in https://github.com/ umich-sota/selective_attention. Below, we discuss the design choices underlying SSA. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Temperature scaling for query and value tokens. In an attention mechanism, the concepts of keys $(K)$ , queries $(Q)$ , and values $(V)$ play distinct roles in determining how information is weighted and combined across a sequence. Temperature functions can be applied to all of those components, designated as Key-temperature $\\tau_{k}(\\cdot)$ , Query-temperature $\\tau_{q}(\\cdot)$ and Value-temperature $\\tau_{\\nu}(\\cdot)$ . We explore the advantages of each temperature function in in Appendix B.1. In practice, we employ Query-temperature $\\tau_{q}(\\cdot)$ and Value-temperature $\\tau_{\\nu}(\\cdot)$ but don\u2019t touch the original key embeddings. The query-temperature $\\tau_{q}$ adjusts the spikiness of the attention map associated with the query according to its embedding and position in the context window. The value-temperature $\\tau_{\\nu}$ enhances the model\u2019s ability to suppress irrelevant or noisy tokens, ensuring a refined aggregation of context window. In Section 4, we provide insights into theoretical and empirical benefits of incorporating these terms.While we keep the keys unmodified, guided by the intuition from word embeddings of [28] suggests that the similarity between a (key, query) pair should align with their cosine similarity. That is, $c o s(k e y_{1},q u e r y)>c o s(k e y_{2},q u e r y)$ should ideally imply that the query attends more to $k e y_{1}$ compared to $k e y_{2}$ . Assigning temperature/gating to scale the query vector does not change this order. However, if we assign distinct scalings to $k e y_{1}$ and $k e y_{2}$ , we will end up with scenarios where attention scores are filpped i.e. $\\tau_{1}*k e y_{1}^{\\top}q u e r y<\\tau_{2}*k e y_{2}^{\\top}$ query. In other words, our intuition is that assigning gating on keys will end up influencing their relative semantic similarities to queries (which could perhaps be better achieved via attention weights). This is in contrast to query-scaling which helps decouple the semantic similarity and contextual sparsity and the associated theoretical benefits (Section 4.1 and Proposition 1). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "\u2022 Token-aware and position-aware temperature scaling. The data-dependent inverse-temperature function is composed of two distinct components $\\tau({\\pmb x})\\stackrel{-}{=}\\tau^{t o k}({\\pmb x})+\\tau^{p o\\bar{s}}({\\pmb x})$ , $\\boldsymbol{x}$ is a token within the sequence $X$ : Token-aware Temperature Scaling $\\tau^{t o k}(\\cdot)$ and Position-aware Temperature Scaling $\\tau^{p o s}(\\cdot)$ . Token-aware Temperature Scaling $\\tau^{t o k}(\\cdot)$ is devised to modulate the influence of individual tokens within the sequence. The formula for this component is given by $\\pmb{\\tau}^{t o k}(\\pmb{x})=t a n h(f(\\pmb{x}))$ , where $f(\\cdot)$ represents a trainable function that adjusts the impact of the token $\\boldsymbol{x}$ . The activation function $t a n h(\\cdot)$ is used to enable the scaling function to output both positive and negative temperatures; for instance, if we want to have the option to fully-suppress a token $\\tau^{t o k}(\\pmb{x})$ can attain $\\approx\\,0$ . To address the issue of dispersed attention, where increasing length of the input sequence leads to a flatter attention distribution, we introduce Position-aware Temperature Scaling. This is defined by $\\tau^{p o s}(\\pmb{x})\\;=\\;1\\,+\\,\\sigma(\\alpha)l o g(n)$ , where $n$ denotes the position of the token $\\pmb{x}$ within the sequence $X=[\\pmb{x}_{1}\\ .\\ .\\ .\\ \\pmb{x}_{L}]^{\\intercal}\\in\\mathbb{R}^{L\\times d}$ , $n\\in[L]$ . We remark that $n$ reflects the token length when computing the temperature of token $x_{n}$ , aligning with our focus on causal attention where each token is restricted to attending only to previous tokens in the sequence. $\\alpha$ is a parameter designed to modify the scale of the factor. The non-linearity $\\sigma(\\cdot)$ is the sigmoid function, employed to control the range of $\\tau^{p o s}$ and ensure the stability of the training process. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Weight sharing. We introduce a weight sharing strategy to reduce the number of parameter overhead below $0.5\\%$ ( $10\\mathrm{x}$ fewer) while maintaining the benefits of SSA. Specifically, the Positionaware Temperature Scaling term, $\\tau^{p o s}(x)$ only includes a single parameter $\\alpha$ , whereas the Tokenaware Temperature Scaling term $\\pmb{\\tau}^{t o k}(\\pmb{x})\\,=\\,t a n h(f(\\pmb{x}))$ , relies on a trainable function $f(\\cdot)$ defined as $W_{t m p}\\mathrm{GeLU}(W_{t m p}^{\\prime}\\pmb{x})$ , involves separate trainable parameters $\\mathbf{W}_{t m p}$ and $\\mathbf{W}_{t m p}^{\\prime}$ , which increases parameter load. To improve efficiency, we (re)use the attention weights $W_{k/q/\\nu}$ for the temperature module by setting $f(\\pmb{x})=W_{t m p}\\mathrm{GeLU}(\\pmb{W}_{k/q/\\nu}\\pmb{x})$ . Here, SSA only adds the output layer of the MLP, a vector with few parameters. The approach only only stores 3 vectors (not matrices) per attention head. This also have negligible inference/latency overhead because we don\u2019t require additional matrix multiplication. These methods only require vector dot-products (at the output layer of the temperature module) and elementwise scaling of matrices. Other strategies can also be deployed to reduce the computational overhead. We describe feature-based approach which use simple token-level statistics, such as their frequencies in training corpus. Only constant parameters per head need to be stored that reduce the number of parameter overhead below $0.1\\%$ . The deatils are shown in B.2. ", "page_idx": 3}, {"type": "text", "text": "Finally, we discuss conceptual connections to sparse attention methods in Appendix D. ", "page_idx": 3}, {"type": "text", "text": "4 Theoretical Insights into Selective Attention ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Selective attention computes the query temperature based on the embedding and the position of the query. It also computes the value temperature based on the value embedding. In what follows, we discuss how these three components provably enhance expressivity of the attention mechanism. ", "page_idx": 3}, {"type": "text", "text": "4.1 The benefits of incorporating query embedding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Decoupling semantics from specificity. Consider two words: \u201cHinton\u201d and \u201cScientist\u201d. The former is a specific instance of the latter. As a result, while we expect token embeddings of these two words to have high cosine similarity, they might benefit from different attention maps. Specifically, \u201cHinton\u201d refers to a specific person and we expect it to have a more targeted attention to the context associated with it. We argue that query-temperature can aid optimization by retaining semantic similarity while allowing for distinct specificity. More formally, by specificity we are referring to the contextual sparsity level of a query. Denoting the combined key-query weights to be $\\pmb{W}=\\pmb{W_{q}}\\pmb{W}_{k}^{\\top}$ as a problem-agnostic measure of specificity, we will consider the magnitude of the query embedding. That is, given query token $\\pmb q$ , define $\\mathrm{spec}_{W}(q):=\\|W^{\\top}q\\|_{2}$ . It is well-established [43] that in order for attention map to be more sparse (hence higher specificity), the norm of the query embedding, or more generally the operator norm of $W$ , has to grow larger, justifying this definition. The following Lemma shows that, without TS, the attention weights within softmax have to be lower bounded by the ratio of specificity difference to semantic distance. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Let $\\pmb{W}=\\pmb{W_{q}}\\pmb{W_{k}}^{\\top}\\in\\mathbb{R}^{d\\times d}$ be the combined query-key matrix. Let $\\pmb{a},\\pmb{b}\\in\\mathbb{R}^{d}$ be unit norm token embeddings associated with the specific and general token respectively. Suppose we wish to achieve specificities $p e c_{W}({\\pmb a})\\geq L_{a}$ and $s p e c_{W}(\\pmb{b})\\leq L_{b}$ . Then, the associated $W$ obeys $\\begin{array}{r}{\\|\\pmb{W}\\|\\geq\\frac{L_{a}-L_{b}}{\\|\\pmb{a}-\\pmb{b}\\|_{2}}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Above follows from the triangle inequality $L_{a}-L_{b}$ is the specificity_difference whereas \u2225W\u2225\u22a4a(\u2212ab\u2212\u2225b2)\u22252 \u2265 \u2225W\u22a4a\u2225\u2225a22\u2212\u2212b\u2225\u2225W2\u22a4b\u22252 \u2265 \u2225Laa\u2212\u2212bL\u2225b2 . $||\\pmb{a}-\\pmb{b}||_{2}$ is the semantic distance. The proof ", "page_idx": 4}, {"type": "text", "text": "Comparison to Selective Attention. In SSA, the effective attention weight matrix for a query $\\pmb q$ is $\\pmb{W}=\\tau(\\pmb{q})\\cdot\\pmb{W}_{q}\\pmb{W}_{k}^{\\top}$ . To achieve the same specificity in Lemma 1 with SSA, we can set the temperatures as $\\tau(\\pmb{a})=L_{a}$ , $\\tau(\\tilde{\\pmb{b}})=L_{b}$ , and KQ-weights as $\\|\\pmb{W}\\|=1$ (e.g. via $\\boldsymbol{W}=\\boldsymbol{I}_{d},$ ). This achieves the desired specificities while maintaining that effective weights are upper bounded as $\\lVert W_{a}\\rVert,\\lVert W_{b}\\rVert\\leq\\operatorname*{max}(L_{a},L_{b})$ . In other words, the required norm growth is entirely decoupled from the semantic distance between the queries. ", "page_idx": 4}, {"type": "text", "text": "In essence, this highlights that without query-selectivity, the model weights have to grow excessively to assign different specificity to similar words. In practice, this is expected to create performance bottlenecks: (1) As the weights grow, optimization may slow down along certain directions due to vanishing softmax derivative and, (2) even if the optimization is successful, the final model could overfit or be overly sensitive to small perturbations in the context, hindering test accuracy. ", "page_idx": 4}, {"type": "text", "text": "This is also verified by our experiments. To study the norm growth of attention weights, we train Pythia from scratch, trainig with the SlimPajama dataset [41](our pretraining setting) and evaluate on Wikitext dataset. We examine the average norm of combined query-key matrix weight $\\|\\pmb{W}\\|$ from the average of all layers within the model. Additionally, we quantify the spikiness of the attention map computed as the ratio of the $l_{1}$ \u2013norm to the squared $l_{2}$ \u2013norm and normalized by the length, defined as \u2225s\u222521 , \u2225s\u2225L s where s is the softmax probability vector. It takes values from 0 to 1. A smaller value indicates a sparser vector. We compute the average of the first 1000 tokens of the Wikitext dataset. The results shown in Figure 2 align with the theory. The attention weights for selective atten", "page_idx": 4}, {"type": "image", "img_path": "QbqLcwMXfF/tmp/8494d884e99839dee4a9d97a646d633cfc811187794cd3ae8d87510fc10fa798.jpg", "img_caption": ["Figure 2: The operator norm of $W$ with and without Query-temperature scaling, scaled by $\\times10^{3}$ . The figure depicts the distribution across 1000 tokens. The dashed line is the average norm. Notably, the norm of the vanilla attention layer is approximately three times larger than that of SSA(dashed red line compare to green line). Furthermore, the vanilla attention layer exhibits a lower spikiness score (0.39) compared to SSA (0.26), where a lower value indicates higher spikiness. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "tion are smaller than the original ones, while the attention is sparser. Expressivity benefits of query-selectivity. A closely related consideration is whether query-selectivity can enhance expressivity. We expect that through query-temperature, the same attention head will have an easier time expressing sparse and dense attention maps associated with distinct queries. To formalize this, we investigate the ability of a single (selective) attention head to express a target attention map between all tokens in a discrete vocabulary. Let V = eiiK=1 be a vocabulary of $K$ tokens. To capture all $K^{2}$ pairwise interactions of these tokens, we first form the sequence $E=[\\pmb{e}_{1}\\,\\dots\\,\\pmb{e}_{K}]^{\\intercal}\\in\\mathbb{R}^{K\\times d}$ where each token appears uniquely and then study $K$ attention maps associated with individual queries, i.e., att $(E,e_{i})$ for $1\\le i\\le K$ . Stacking these together as rows, we study the $K\\times K$ attention matrix $\\mathsf{a t t}(E)$ . For standard attention with weights $W$ , this is given by at $\\mathbf{\\Psi}:(E,W)=\\mathbb{S}(E W E^{\\top})$ , whereas for query-selective attention, at $\\mathsf{t}(E,W)=\\mathbb{S}(\\tau(E)\\odot E W E^{\\top})$ . ", "page_idx": 4}, {"type": "image", "img_path": "QbqLcwMXfF/tmp/923800f4260fbe78743685d30aca90dc38e819b22c961ff0531d433a6409155f.jpg", "img_caption": ["Figure 3: We compare 1-layer SSA and 1-layer attention when solving next-token prediction on a small vocabulary of size 8. (a) is the graph associated to the token transition dynamics. (b) is the the pairwise token transition matrix of this vocabulary. Each row of $P_{\\star}$ represents an attention map where a particular token is the query and all tokens in the vocabulary serve as keys (see Sec 4.1 for details). The transition matrix $\\hat{P}$ estimated by SSA in (c) is sharper and more closely resembles the optimal $P_{\\star}$ . SSA achieves a smaller cross-entropy loss compared to vanilla attention, 0.009 vs 0.0126. The $\\ell_{1}$ approximation error of the attention map of SSA is also smaller than that of vanilla attention, 0.358 vs 0.543. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Thanks to the softmax nonlinearity, $\\mathsf{a t t}(E)$ is a stochastic matrix where rows add up to 1. This matrix can be viewed as a Markov chain transition between different tokens, which motivates a fundamental question: Can query-selective attention help express a larger class of stochastic matrices? Intuitively, we expect that if a stochastic matrix $P_{\\star}$ , which we wish to express via $\\mathsf{a t t}(E)$ , exhibits a lot of spikiness variation across its rows (i.e., different queries), selectivity can better capture these. ", "page_idx": 5}, {"type": "text", "text": "This can be verified with a token generation experiments. Recall that we expect \u201cbacteria\u201d to attend to more words compared to \u201csalmonella\u201d. We might expect more general words to have a larger number of neighbors in a graph. Accordingly, we abstract the vocabulary, which comprises words with various levels of specificity, into a simple undirected graph. This is depicted in Section 4.1. Additionally, the stochastic matrix $P_{\\star}$ can be derived from this graph, with the results displayed in Figure 3(a). To build the estimation of the stochastic matrix $P_{\\star}$ training, we conduct next token prediction experiments. ", "page_idx": 5}, {"type": "text", "text": "Token generation setting: Let $X\\in\\mathcal{V}^{L}$ be a sequence of length $L$ drawn from $_\\mathcal{V}$ . Suppose $X$ ends with $q:=x_{L}$ . The token $Y=x_{L+1}$ that follows $X$ will be drawn uniformly from $q$ or one of the neighbors of $q$ . This neighborhood is parameterized via the latent attention map $P_{\\star}$ which will govern the generation process. Let $\\pmb{E}=[\\pmb{e}_{1}\\,\\dots\\,\\pmb{e}_{N}]^{\\top}$ be the token embeddings associated with the vocabulary $_\\mathcal{V}$ . Assume elements of $E$ have unit $\\ell_{2}$ norm. In data generation, we simple sample input sequences containing each token in the vocabulary precisely once, and sample the next token according to the attention map $P_{\\star}$ , that is, the row of $P_{\\star}$ that corresponds to the final query token. We then fit a one-layer self-attention or SSA model $f(X)$ to approximate this latent dynamics. Concretely, we predict the next token $\\hat{Y}$ of $f(X)$ according to the distribution $g(X)=\\mathbb{S}(C f(\\mathbf{\\bar{X}}))\\in\\mathbb{R}^{N}$ . Here $C\\in\\dot{\\mathbb{R}}^{N\\times d}$ is the linear prediction head. As loss measure on how well we fit to the latent $P_{\\star}$ dynamics, use the cross entropy distance between $g(X)$ and the true label Y. Through this, we wish to formalize and visualize the intuitions on why \u201csalmonella\u201d deserves a lower temperature than \u201cbacteria\u201d. Further experimental details are described in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "In our experiments, besides smaller cross-entropy loss, we find that Selective Attention achieves a better approximation of $P_{\\star}$ as shown in Figure 3. To evaluate the similarity between the attention map $P_{\\star}$ and $\\hat{P}$ , we also define the $\\ell_{1}$ distance between the attention maps, namely, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathtt{e r r\\_m a p}=\\lVert\\hat{P}-P_{\\star}\\rVert_{1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We find that the err $\\tt m a p_{S S A}$ is also much lower than err_ $\\scriptstyle\\mathtt{m a p}_{\\mathrm{vanilla}}$ (0.358 vs 0.543). Additionally, SSA naturally assigns lower temperatures to tokens with fewer neighbors. This is in line with our expectations as fewer neighbors imply a sparser attention map. The results are shown in Table 1. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Temperature for each depth. Nodes with the same # of neighbors share the same temperature. ", "page_idx": 6}, {"type": "table", "img_path": "QbqLcwMXfF/tmp/3305f7436db4f4b0dc5b1fa838ae51e80ce36a66d350fc64dd65700f94f135a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "To further formalize this, we revisit Lemma 1 in terms of softmax map. Let $K\\,=\\,2$ and $P_{\\star}\\;=\\;$ $\\left[\\!\\!\\begin{array}{c c}{{1-\\gamma}}&{{\\gamma}}\\\\ {{0}}&{{1}}\\end{array}\\!\\!\\right]$ be the target pairwise attention map. Here second token is highly specific (only selects itself) whereas the first token is less specific when $0<\\gamma<1$ . The following proposition establishes a variation of Lemma 1 when approximating $P_{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. Suppose the embeddings $\\pmb{e}_{1},\\pmb{e}_{2}$ have unit $\\ell_{2}$ norm with correlation $\\boldsymbol\\rho=e_{1}^{\\top}\\boldsymbol e_{2}$ . Fix $0<\\varepsilon\\leq\\textstyle{\\frac{1}{2}}\\operatorname*{min}(\\gamma,1-\\gamma)$ and $\\begin{array}{r}{\\Gamma=\\bigg|\\log\\Big(\\frac{1-\\gamma}{\\gamma}\\Big)\\bigg|.}\\end{array}$ . For any $W$ obeying $||P_{\\star}-\\mathbb{S}(E W E^{\\top})||_{\\infty}\\leq\\varepsilon,$ , we have that $\\begin{array}{r}{\\|W\\|\\geq\\frac{\\|e_{1}-e_{2}\\|^{-1}}{\\sqrt{2-2\\rho^{2}}}\\left(\\log\\left(\\frac{1}{4\\varepsilon}\\right)-\\Gamma\\right)\\!.}\\end{array}$ . Conversely, Selective Attention can achieve this $\\varepsilon$ -approximation with weights bounded as \u03c4(e1,2) \u00b7 \u2225W\u2225\u2264\u2225e1 \u2212e2\u2225\u22121 max log \u03b51 ,\u221a1\u0393\u2212\u03c12 . ", "page_idx": 6}, {"type": "text", "text": "4.2 The benefits of incorporating query position ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The need for position-dependent scaling arises from the fact that, for a fixed weight matrix $W=$ $W_{q}W_{k}^{\\top}$ , the attention scores $\\pmb{s}^{L}=\\mathbb{S}(\\pmb{X}\\pmb{W}^{\\top}\\bar{\\pmb{x}}_{L})$ become diluted as sequence length $L$ grows. Specifically, for retrieval-type tasks, the model may want to concentrate softmax scores $\\mathbf{s}^{L}$ on a single token. However, assuming unit norm tokens, the top probability in $s^{L}$ is upper bounded via $\\|s^{L}\\|_{\\ell_{\\infty}}\\leq$ 1+(L\u221211)e\u22122\u2225W\u2225. This implies that, to enforce \u2225sL\u2225\u2113\u221eto be constant, we require the spectral norm lower growth rate of $\\left\\|W\\right\\|\\geq0.5\\log L+O(1)$ . This motivates our logarithmic scaling strategy which was also proposed by [33, 7]. ", "page_idx": 6}, {"type": "text", "text": "Here we provide a more formal justification on the optimal temperature scaling rule by describing a simple yet insightful task which is not solvable by a single attention head unless temperature scaling is employed. Specifically, we consider a setting where the sequence exhibits feature imbalances where frequent tokens start dominating the context and potentially overwhelm the less frequent but relevant tokens. ", "page_idx": 6}, {"type": "text", "text": "Imbalanced token setup: Suppose the input sequence $X=[\\pmb{x}_{1}\\,\\ldots\\,\\pmb{x}_{L}]^{\\top}$ is composed of a minority token $\\pmb{a}\\in\\mathbb{R}^{d}$ and a majority token $\\pmb{b}\\in\\mathbb{R}^{\\bar{d}}$ , that is, $\\pmb{x}_{i}\\in\\{\\pmb{a},\\pmb{b}\\}$ for all $i\\in[L]$ . For each position, we will simply ask the model to output a target mixture of $\\pmb{a}$ and $\\pmb{b}$ , namely, $\\mathbf{y}=\\alpha\\pmb{a}+(1-\\alpha)\\pmb{b}$ for some $\\alpha\\in(0,1)$ . Thus, using a 1-layer causal attention, we study the following objective by calculating the loss between target ${\\boldsymbol y}$ and each attention output: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\boldsymbol{W})=\\frac{1}{L}\\sum_{n=n_{0}}^{L}\\|\\boldsymbol{y}-\\boldsymbol{X}^{\\top}\\mathbb{S}_{\\leq n}(\\tau_{n}\\cdot\\boldsymbol{X}\\boldsymbol{W}^{\\top}\\boldsymbol{x}_{n})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Above, $\\tau_{n}$ is the inverse-temperature for the $n^{t h}$ position. Here, $n_{0}$ is a burn-in period to simplify our exposition: $n_{0}$ is the smallest number such that both $\\pmb{a}$ and $\\pmb{b}$ appear at least once within the first $n_{0}$ tokens1. Additionally, let $n_{a}$ be the number of tokens $\\boldsymbol{x}_{i}$ that are equal to $\\pmb{a}$ within $i\\in[n]$ . We have the following theorem. ", "page_idx": 6}, {"type": "text", "text": "Proposition 2. Assume $\\pmb{a},\\pmb{b}$ are unit Euclidean norm and linearly independent. Define the imbalance ratio $\\kappa_{n}=(n-n_{a})/n_{a}$ for $n\\in[L]$ . There is a $W_{\\star}$ such that, setting $\\begin{array}{r}{\\tau_{n}=\\log\\kappa_{n}+\\log\\frac{\\alpha}{1-\\alpha}}\\end{array}$ , $\\mathcal{L}(W_{\\star})$ minimizes the risk (1) to achieve $\\mathcal{L}(\\boldsymbol{W}_{\\star})=0$ . ", "page_idx": 6}, {"type": "text", "text": "Conversely, consider the problem instance with target mixture of $\\alpha=1/2$ , second-quadrant imbalance of $2\\geq\\kappa_{n}\\geq1$ for $L/4\\leq n\\leq L/2$ and fourth-quadrant imbalance of $\\dot{\\kappa}_{n}\\geq4$ for $n\\ge3L/4.$ . If we employ flat temperature $\\tau_{n}=1$ for all $n\\in[L],$ , for any choice of attention weights $W\\in\\mathbb{R}^{d\\times d}$ , we have the lower bound $\\mathcal{L}(W)>1/500$ . ", "page_idx": 6}, {"type": "table", "img_path": "QbqLcwMXfF/tmp/ae877fb7d1a84f13b5265d9483f6e23176716a404cb86fd95341ccaa8f5ef8df.jpg", "table_caption": ["Table 2: We apply normalization to attention output and compute the MSE risk "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Proposition 2 inspired our design of position-aware temperature scaling. Intuitively, as $n$ increases, the sequence may include less related tokens, leading to an increase in $K_{n}$ . When $\\kappa_{n}$ follows powerlaw $\\kappa_{n}\\,=\\,n^{\\mathrm{pow}}$ , we recover the logarithmic temperature scaling rule of $\\tau_{n}\\,=\\,{\\mathsf{c o n s t}}+{\\mathsf{p o w}}\\cdot\\log n$ . Consequently, our Position-aware Temperature Scaling function $\\tau_{n}$ is designed as $\\tau^{p o s}(\\pmb{x})\\,=\\,1\\,+$ $\\sigma(\\alpha)l o g(n)$ , $n$ is the position length, $\\alpha$ is the trainable parameter, $\\sigma$ is the non-linearity function sigmoid. The function is motivated by, other paper\u2019s rules [33, 26, 22, 51]. ", "page_idx": 7}, {"type": "text", "text": "4.3 The benefits of incorporating value embedding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Within attention, value embeddings $(V)$ are transformed using only a linear projection. Consequently, each token\u2019s contribution to the output is a weighted sum based on the attention scores, with these weights adjusted linearly. In sequences with many tokens, irrelevant or noisy tokens can negatively influence the attention mechanism. Because value embeddings are linearly projected, they may not be able to fully distinguish between relevant and irrelevant tokens. The value-temperature scaling acts as a nonlinear scalar weighting function. By adjusting the temperature, we aim to control the impact of each token, suppressing the influence of irrelevant or noisy tokens. This helps emphasize more relevant tokens, thereby improving the quality of the context representation. We motivate the potential benefits of TS on value embeddings through the following synthetic denoising task. ", "page_idx": 7}, {"type": "text", "text": "Denoising task Let $[K]$ be the token alphabet with embeddings $(\\pmb{e}_{i})_{i=1}^{K}$ . Assume $d=K$ and $\\pmb{e}_{i}$ \u2019s are standard basis. Consider the following data distribution $(X,y)\\sim{\\mathcal{D}}$ where $X=[\\pmb{x}_{1}\\ .\\ .\\ .\\ \\pmb{x}_{L}]^{\\intercal}\\in\\mathbb{R}^{L\\times d}$ is the input sequence and $\\mathbf{y}\\in\\mathbb{R}^{d}$ is the target label. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Draw $q\\sim{\\tt U n i f}([K])$ . Set ${\\boldsymbol{y}}={\\boldsymbol{e}}_{q}$ .   \n\u2022 Let $(z_{i})_{i=1}^{L}$ be IID noise vectors with $\\mathcal{N}(0,\\sigma^{2}I)$   \n\u2022 $\\pmb{x}_{L}=\\pmb{e}_{q}+\\pmb{z}_{L}$ . For $i\\in[L-1],x_{i}$ is determined by a Bernoulli distribution with a parameter of $\\alpha$ , selecting between $e_{q}+z_{i}$ and $z_{i}$ . Consequently, $\\alpha$ of the tokens are signal tokens $e_{q}+z_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "The denoising objective is minimizing the MSE risk ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)=\\mathbb{E}_{\\mathcal{D}}[||\\pmb{y}-\\mathrm{norm}(\\hat{\\pmb{y}})||_{2}^{2}]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathrm{norm}(\\hat{\\mathbf{y}})=\\hat{\\mathbf{y}}/\\|\\hat{\\mathbf{y}}\\|_{2},\\hat{\\mathbf{y}}$ is the output of model $f(\\cdot),{\\hat{\\mathbf{y}}}=f(X)$ . ", "page_idx": 7}, {"type": "text", "text": "To solve this task, the attention model $f(X)$ should intelligently combine the tokens within $X$ to approximate the denoised target $e_{q}$ . Importantly, the model will strictly benefti from eliminating the pure noise tokens, i.e., instances with $\\mathbf{x}_{i}=z_{i}$ . Note that the value projection of the attention matrix will not suffice to denoise the input sequence. The reason is that $q$ is uniform, and signal tokens span the whole space. Thus, we will benefit from a nonlinear denoising procedure. ", "page_idx": 7}, {"type": "text", "text": "To test this intuition, we use a 1-layer single-head attention model, denoted as different $f(\\cdot)$ to minimize the denoising objective. We compare the model with value-selectivity to the following baselines: ", "page_idx": 7}, {"type": "text", "text": "1. Vanilla Attention: The standard 1-layer single-head attention model, $\\hat{\\pmb{y}}_{a t t}=\\mathrm{Att}(\\pmb{X})$   \n2. Value-selective self-attention: 1-layer Selective Self-Attention (SSA). ${\\hat{\\pmb{y}}}_{S S A}\\;=\\;\\mathrm{SSA}(X)$ . Since this is a synthetic task, as a proxy for the token-aware temperature scaling, we use the selection function $\\operatorname*{max}_{j\\in[d]}x_{i j}\\geq1/2$ . Intuitively, when noise $\\sigma\\lesssim1/\\sqrt{\\log d}$ , thresholding with the largest entry will detect the signal tokens.   \n3. Naive averaging: Directly average the tokens, $\\begin{array}{r}{\\hat{\\pmb{y}}_{n a i\\nu e}=\\frac{1}{L}\\sum_{i=1}^{L}{\\pmb{x}}_{i}}\\end{array}$ .   \n4. Bayes optimal estimator: $\\begin{array}{r}{\\hat{\\pmb{y}}_{o p t}=\\frac{1}{|\\cal S|}\\sum_{i\\in{\\cal S}}{\\pmb x}_{i}}\\end{array}$ where $S\\subset[L]$ is the ground-truth set of signal tokens distributed as $e_{q}+z_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "The resulting MSE risks are displayed in Table 2. We set $d=k=8$ and $\\begin{array}{r}{\\alpha=\\frac{1}{4}}\\end{array}$ . With the addition of the value-selection function, the model achieved a loss comparable to the optimal estimator, indicating successful suppression of noisy tokens. In contrast, while vanilla softmax self-attention performs similarly to naive averaging, it fails to sufficiently denoise, resulting in a much larger loss compared to our value-selective attention. ", "page_idx": 7}, {"type": "table", "img_path": "QbqLcwMXfF/tmp/8a61254d717a4ed089008db88377dc43b31d1e9aab94304bccca11064afd0194.jpg", "table_caption": ["Table 3: Experiment results for model pretraining and finetuning. For perplexity (ppl), lower is better, and for accuracy (acc), higher is better. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5 Empirical Evaluations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1 Standard Benchmarks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Drawing on theoretical insights, we assess the performance of SSA on NLP tasks by integrating SSA into established models such as GPT-2 [34], Pythia [3], Llama [44] and Llama3 [16]. Our methodology includes both pre-training and fine-tuning to evaluate SSA\u2019s performance and efficiency. For the pre-training evaluation, we train the model from scratch on the SlimPajama dataset [41]. Subsequently, we evaluate the model on various downstream zero-shot tasks, including Wikitext [27], Lambada [32], Piqa [4], Hella [50], Winogrande [38], Arc-E, and Arc-C [10]. This approach is widely used for measuring the performance and generalization capabilities of pretrained large language models across diverse tasks [2, 3, 18]. For the fine-tuning evaluation, we start by loading the official pre-trained model and then fine-tune it on the downstream tasks. Unlike pre-training, where the downstream tasks are unseen during training, fine-tuning involves direct training on the tasks. This allows the model to better approximate the token distribution and understand the text domain. Details of the models are provided in Appendix A. ", "page_idx": 8}, {"type": "text", "text": "Our primary results are shown in Table 3. Based on the theoretical insights and ablation study results, we conduct both Token-aware and Position-aware Temperature Scaling on query $Q$ , and value $V$ . We observe that across various models and datasets, incorporating SSA consistently enhances performance. Notably, experiments with larger and more recent models, such as Llama3-8B and Pythia 410M, confirm that SSA improves accuracy across across model scales and architectures. We further introduce a weight sharing strategy that reduces the number of parameter overhead to less than $0.5\\%$ while preserving the benefits of SSA and still outperforming the standard transformer. This underscores the value of selectivity irrespective of its precise implementation. Thus, our improvements are not arising from an increase in the parameter count, but rather from the strategic integration of SSA. Additionally, we have also explored a feature-based method to further enhance SSA\u2019s parameter efficiency. In a nutshell, rather than training an MLP, we select the temperature as a function of token-level features, such as the frequency of a token in the training corpus, by fitting a single scalar parameter. This process requires only O(1) additional weights $(<\\!0.01\\%$ of total). Further details and results are provided in Appendix B.2. ", "page_idx": 8}, {"type": "text", "text": "For the ablation study, we fine-tuned the models on the Wikitext dataset to compare the influence of each component, using the same dataset and training configurations as those in the real experiments. The results are shown in Appendix B.1. Among the results, we observe that deploying both Token-aware and Position-aware Temperature Scaling on $\\b{Q}$ and $V$ independently could achieve significant improvement, aligning with our theoretical insights. Additionally, combining Key and Query temperatures can achieve additional improvement. Moreover, between token-aware and position-aware temperature scaling, the latter demonstrates a more consistent improvement across different scenarios, while combining them can achieve the best overall result. We also compare with more baselines including [26, 22, 51] and the results are shown in Appendix B.3. Our method consistently outperforms the baselines. ", "page_idx": 9}, {"type": "text", "text": "Additionally, SSA can accelerate the training process by achieving comparable performance with fewer tokens. This efficiency not only reduces the demand on computational resources but also shortens the time required to effectively train models. We illustrate this efficiency by plotting the training results when fine-tuning the Llama model on the Wikitext dataset, both with vanilla attention layer or SSA, in Figure 4. The results indicate that SSA can accelerate training, achieving similar performance with $1.45\\times$ reduction in pretraining steps. ", "page_idx": 9}, {"type": "image", "img_path": "QbqLcwMXfF/tmp/5377f70565ea24e96c90418217e8392d5bbdc6aea7af0783e0ac5aacb04e0ba7.jpg", "img_caption": ["Figure 4: Comparison of training curves. SSA provides reasonable benefits in terms of training speedup. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.2 Passkey Retrieval ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We also examines the perfromance on the passkey retrieval task as defined in [33, 29].This is a synthetic task to measure a model\u2019s ability to retrieve a simple passkey (i.e., a five-digit number) within a large amount of otherwise meaningless text. We performed 10 iterations of the passkey retrieval task with the passkey placed at a random location uniformly distributed across the evaluation context window. Intuitively, SSA could better solve this task by assigning different token-level temperatures to digits vs words. For our evaluation of the fine-tuned Pythia, SSA leads to substantial improvement (from $56.9\\%$ to $74.4\\%$ ), as seen in Table 4. ", "page_idx": 9}, {"type": "table", "img_path": "QbqLcwMXfF/tmp/f3740d96662d8bb450579c35260a258adf8e7868cfd232bda4cb24199e8e9900.jpg", "table_caption": ["Table 4: Passkey retrieval performance of various models. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusions, Limitations, and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have introduced the Selective Self-Attention layer, which augments the softmax nonlinearity with a principled temperature-scaling strategy. SSA shows consistent benefits and augments the performance of existing transformer-based models such as Pythia and Llama 2. We also provide theoretical insights into the benefits of query, value, and positional selectivity. ", "page_idx": 9}, {"type": "text", "text": "Future research. Based on SSA, there are several interesting research avenues to pursue. Firstly, our method can extend to linear attention strategies. While we can use the same method for value embeddings, for queries, we can train an additive bias term on attention similarities rather than using temperature scaling. Secondly, based on the visual benefits of SSA on Figure 3, it would be interesting to explore how SSA can help the interpretability and quality of the attention maps. Overall, SSA has the potential to assist in more principled use of transformers in language, vision, and other modalities. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Our work focuses on the canonical softmax-attention mechanism, which suffers from the quadratic computation bottleneck. As mentioned above, extending our method to linear attention can mitigate computational costs. Another direction to enhance efficiency is building stronger connections to sparsity and understanding how SSA can benefti and be integrated with sparse attention algorithms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the National Science Foundation grants CCF-2046816, CCF2403075, the Office of Naval Research award N000142412289, an Adobe Data Science Research award, and gifts by Open Philanthropy and Google Research. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Emmanuel Abbe, Samy Bengio, Aryo Lotf,i and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum. In International Conference on Machine Learning, pages 31\u201360. PMLR, 2023.   \n[2] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher R\u00e9. Simple linear attention language models balance the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024.   \n[3] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023.   \n[4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.   \n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[6] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00e9. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34:17413\u201317426, 2021.   \n[7] Ta-Chung Chi, Ting-Han Fan, and Alexander I Rudnicky. Attention alignment and flexible positional embeddings improve transformer length extrapolation. arXiv preprint arXiv:2311.00684, 2023.   \n[8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013 113, 2023.   \n[10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[11] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.   \n[12] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.   \n[13] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933\u2013941. PMLR, 2017.   \n[14] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024.   \n[15] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning, pages 2793\u20132803. PMLR, 2021.   \n[16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \n[17] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An $800\\mathrm{gb}$ dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[18] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[19] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.   \n[20] M Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, and Samet Oymak. From self-attention to markov models: Unveiling the dynamics of generative transformers. International Conference on Machine Learning, 2024.   \n[21] Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv preprint arXiv:2311.01927, 2023.   \n[22] Mingchen Li, Xuechen Zhang, Christos Thrampoulidis, Jiasi Chen, and Samet Oymak. Autobalance: Optimized loss functions for imbalanced data. Advances in Neural Information Processing Systems, 34:3163\u20133177, 2021.   \n[23] Bingbin Liu, Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing attention glitches with filp-flop language modeling. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[25] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In International Conference on Learning Representations, 2023.   \n[26] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. ICLR, 2021.   \n[27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.   \n[28] Tomas Mikolov. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.   \n[29] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023.   \n[30] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.   \n[31] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and Fran\u00e7ois Fleuret. Fast attention over long sequences with dynamic sparse flash attention. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[32] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.   \n[33] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.   \n[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[36] Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse modular activation for efficient sequence modeling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[37] Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling attention via convex duality: Analysis and interpretations of vision transformers. In International Conference on Machine Learning, pages 19050\u201319088. PMLR, 2022.   \n[38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[39] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.   \n[40] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.   \n[41] Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818, 2023.   \n[42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \n[43] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[46] Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander M Rush. Mambabyte: Token-free selective state space model. arXiv preprint arXiv:2401.13660, 2024.   \n[47] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \n[48] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.   \n[49] Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770\u20133785, Online, August 2021. Association for Computational Linguistics.   \n[50] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.   \n[51] Xuechen Zhang, Mingchen Li, Jiasi Chen, Christos Thrampoulidis, and Samet Oymak. Classattribute priors: Adapting optimization to heterogeneity and fairness objective. to appear at AAAI, 2024.   \n[52] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. arXiv preprint arXiv:2310.16028, 2023.   \n[53] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Table 5: Fine-tuning experiment results for language models on the Wikitext dataset, showcasing baseline and variations with different components $(Q,K,V)$ . ", "page_idx": 14}, {"type": "table", "img_path": "QbqLcwMXfF/tmp/0c5261a60c9163750e6395b248504c09d60c9db9a27b12cf21e0f229b2e8ae0e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the next token prediction experiment that substantiates the expressivity benefti of query-selectivity, as detailed in Figure 3 and Table 1, we employ the Adam optimizer to train a model. This model consists of a single-layer, single-head attention mechanism, accompanied by a tokenizer and a fully connected layer. The tokenizer embeds the discrete sequence to continuous embedding ${\\pmb E}$ . The fully connected layer is used as the classifier to predict the node index. We set the learning rate at $\\bar{1}e^{-4}$ . The training loss is the cross-entropy loss. In our experiments, $L=N=8$ . For SSA, we implement Token-aware Temperature Scaling for the query matrix $\\b{Q}$ . We assign a scaling parameter to each group of nodes that share the same number of neighbors. To have better visualization, we do normalization to plot the attention map $P_{\\star}$ and $\\hat{P}$ . For the experiments shown in Table 2, we also use the Adam optimizer and learning rate $1e^{-4}$ . But the objective is the MSE risk. ", "page_idx": 14}, {"type": "text", "text": "For our empirical evaluation, we utilize several models. We employ GPT-2, which has 124 million parameters, and use the official OpenAI GPT-2 checkpoints that were pre-trained on the WebText dataset [34] for our finetuning experiments. For Pythia, our experiments are conducted with a model size of 160 million and 410 million parameters, using the official checkpoint pre-trained on the Pile dataset [17]. Lastly, for Llama, we utilize the smallest variant available, with 7 billion parameters, and similarly fine-tune using the official pre-trained model. As the training configuration, we train with 3.5 million tokens for fine-tuning and 15B tokens for pre-training. We always use the AdamW optimizer [24], $\\beta_{1}\\,=\\,0.9$ and $\\beta_{2}\\,=\\,0.95$ . We set learning rate $1e^{-6}$ with no weight decay and no warmup. The pre-training takes about 2 hours using 4 A40 and fine-tuning takes about 2 days. We use FlashAttention [11] to accelerate the training. For weight sharing, each head shares the same funtion. ", "page_idx": 14}, {"type": "text", "text": "All the experiments are conducted with 4 or 8 A40 or L40S. We can directly reuse FlashAttention[12], which significantly improves model efficiency. ", "page_idx": 14}, {"type": "text", "text": "B Additional experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Ablation study ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the ablation study, we conducted fine-tuning on the Wikitext dataset to compare the influence of each component, using the same dataset and training configurations as those in the real experiments. The models are evaluated with perplexity (ppl). ", "page_idx": 14}, {"type": "text", "text": "B.1.1 Key-temperature, Query-temperature, and Value-temperature ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To evaluate the benefits of applying temperature scaling to $\\pmb{K},\\pmb{Q}$ , and $V$ , we conducted an ablation study, examining each component individually and in combination. For a fair comparison, both position-aware and token-aware temperature scaling were applied to all components. The results, detailed in table 5, indicate that modifying $\\b{Q}$ , and $V$ independently yields clear benefits, whereas alterations to $\\pmb{K}$ result in performance that is similar to, or even worse than, the baseline vanilla attention layer. The results align well with the theoretical analysis presented in section 4. However, when $\\b{Q}$ and $V$ are combined, we observe consistent improvements. These findings led us to develop our final algorithm, which applies temperature scaling to both $Q$ and $V$ . ", "page_idx": 14}, {"type": "table", "img_path": "QbqLcwMXfF/tmp/a14c6c4713e2f53ae2049056e321adbae2507bbd6a050d66dff550934108d17a.jpg", "table_caption": ["Table 6: Investigate the benefits of Token-aware Temperature Scaling, Position-aware Temperature Scaling. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "QbqLcwMXfF/tmp/c59c7fd169836fdf69a7daae09be070b5ee098a482c3c1eb8c9ccb02cb581399.jpg", "table_caption": ["Table 7: Comparing different SSA parameterizations "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.1.2 Token-aware Temperature Scaling, Position-aware Temperature Scaling ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We also conduct experiments to investigate the benefits of Token-aware and position-aware Temperature Scaling applied to Query $\\pmb q$ and value $\\pmb{\\nu}$ . The results are shown in table 6. Token-aware Temperature Scaling positively impacts both Query $\\pmb q$ and value $\\pmb{\\nu}$ , whereas Position-aware Temperature Scaling shows smaller improvement on value $\\pmb{\\nu}$ , aligning with our theoretical insights. Furthermore, when compared to GPT-2, Pythia\u2014which features a more advanced positional encoding[42] \u2014demonstrates fewer improvements. This suggests that while new strategies may mitigate the dispersed attention issue, our Selective Self-Attention (SSA) method still offers additional improvements. ", "page_idx": 15}, {"type": "text", "text": "B.2 Parameter-efficient SSA: Weight sharing and featurization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we also introduce a feature-based approach to improve parameter efficiency. In a nutshell, rather than training an MLP, we select the temperature as a function of token-level features, such as the frequency of a token in the training corpus, by fitting a single scalar parameter. This process requires only O(1) additional weights per attention head $(<\\!0.01\\%$ of total). This is inspired by the logit adjustment strategy of [26] which sets the cross-entropy temperature as a function of class frequencies. ", "page_idx": 15}, {"type": "text", "text": "Our evaluations on feature-based SSA are provided in Table 7. We find that, while the feature-based method is beneficial and highly parameter-efficient, it can be sensitive to feature selection and exhibits more variability across datasets. ", "page_idx": 15}, {"type": "text", "text": "B.3 Ablation of Different Parameterizations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In addition to the functions we propose for temperature design, we also explore alternative approaches. Instead of employing our Position-aware Temperature Scaling function, we use a constant parameter, as suggested in other studies [26, 22, 51]. We also compare with the temperature scaling method proposed by [33]. Furthermore, in place of our Token-aware Temperature Scaling, we adopt a simpler approach by directly utilizing token frequency and training only a scale parameter. These experiments were conducted using the Pythia model, fine-tuned on the Wikitext dataset. The outcomes of these comparative analyses are presented in Table 8. Among those baselines, we consistently outperform their results. ", "page_idx": 15}, {"type": "table", "img_path": "QbqLcwMXfF/tmp/cd613269b72c053b06e759801d712018358a88de649001bb8f26465b9821eb5a.jpg", "table_caption": ["Table 8: Conducting different functions. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Given embeddings $\\pmb{e}_{1},\\pmb{e}_{2}$ with unit $\\ell_{2}$ norm, their correlation is $\\boldsymbol\\rho=\\pmb{e}_{1}^{\\top}\\pmb{e}_{2}$ . First, consider the approximation error bound $||P_{\\star}-\\mathbb{S}(E W E^{\\top})||_{\\infty}\\leq\\varepsilon$ . To achieve this, the weight matrix $W$ must satisfy the inequality. ", "page_idx": 16}, {"type": "text", "text": "To derive a lower bound on $\\|\\pmb{W}\\|$ , observe that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|P_{\\star}-\\mathbb{S}(E W E^{\\top})\\|_{\\infty}\\leq\\varepsilon}\\\\ &{\\|P_{\\star}-\\mathbb{S}(E W E^{\\top})\\|_{\\infty}=\\left\\|\\displaystyle\\frac{1}{1+e^{-E W E^{\\top}}}-P_{\\star}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the fact that $\\pmb{e}_{1}$ and $e_{2}$ are unit vectors and $\\boldsymbol\\rho=\\pmb{e}_{1}^{\\top}\\pmb{e}_{2}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n||E W E^{\\top}||_{\\infty}\\geq\\frac{1}{4\\varepsilon}-\\Gamma.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, the norm $\\|\\pmb{W}\\|$ is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|W\\|\\geq\\frac{\\|e_{1}-e_{2}\\|^{-1}}{\\sqrt{2-2\\rho^{2}}}\\left(\\log\\left(\\frac{1}{4\\varepsilon}\\right)-\\Gamma\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Conversely, to achieve $\\varepsilon$ -approximation using Selective Attention, the weights need to be bounded such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tau(e_{1,2})\\cdot\\Vert W\\Vert\\leq\\Vert e_{1}-e_{2}\\Vert^{-1}\\operatorname*{max}\\left(\\log\\left(\\frac{1}{\\varepsilon}\\right),\\frac{\\Gamma}{\\sqrt{1-\\rho^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the selective attention avoids the $1/\\sqrt{1-\\rho^{2}}$ dependence on the $\\log(1/\\varepsilon)$ term, decoupling the high-specificity requirement (small $\\varepsilon$ ) from the semantic similarity of the tokens. ", "page_idx": 16}, {"type": "text", "text": "This completes the proof of Proposition 1. ", "page_idx": 16}, {"type": "text", "text": "C.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. We first show the success direction. Set $W$ such that $b^{\\top}\\pmb{W}=0$ and $\\pmb{a}^{\\top}\\pmb{W}\\pmb{a}=\\pmb{a}^{\\top}\\pmb{W}\\pmb{b}=1$ . Such $W$ exists thanks to the linear independence of $\\pmb{a},\\pmb{b}$ . Now plugging this $W$ into (1), for each position, regardless of whether $\\pmb{x}_{n}=\\pmb{a}$ or $\\pmb{x}_{n}=\\pmb{b}$ , we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\nX^{\\top}\\mathfrak{S}_{\\leq n}(\\tau_{n}\\cdot X W\\pmb{x}_{n})=\\frac{n_{a}e^{\\tau_{n}}}{n_{a}e^{\\tau_{n}}+(n-n_{a})}a+\\frac{(n-n_{a})e^{\\tau_{n}}}{n_{a}e^{\\tau_{n}}+(n-n_{a})}b.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We wish to ensure ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{n_{a}e^{\\tau_{n}}}{n_{a}e^{\\tau_{n}}+(n-n_{a})}=\\frac{1}{1+(n/n_{a}-1)e^{-\\tau_{n}}}=\\frac{1}{1+\\kappa_{n}e^{-\\tau_{n}}}=\\alpha.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This in turn implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\kappa_{n}e^{-\\tau_{n}}=\\frac{1-\\alpha}{\\alpha}\\iff\\tau_{n}=\\log\\kappa_{n}+\\log\\frac{\\alpha}{1-\\alpha}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we discuss the failure case of flat temperature. To do so, we will lower bound the loss over the queries $\\pmb{x}_{n}=\\pmb{b}$ . Set $M=e^{(b-a)^{\\top}W b}$ . Following same argument as above, for fixed temperature, $W$ will output a non-adaptive composition of the form ", "page_idx": 17}, {"type": "equation", "text": "$$\nX^{\\top}\\mathfrak{S}_{\\leq n}(X W b)=\\frac{1}{1+M\\kappa_{n}}\\pmb{a}+\\frac{M\\kappa_{n}}{1+M\\kappa_{n}}\\pmb{b}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, the loss function will be lower bounded by (accounting for the prediction error in $\\pmb{a},\\pmb{b}$ terms and their orthogonality) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}(W)\\geq\\operatorname*{min}_{M>0}\\sum_{n:x_{n}=b}(0.5-\\frac{1}{1+M\\kappa_{n}})^{2}+(0.5-\\frac{M\\kappa_{n}}{1+M\\kappa_{n}})^{2}=\\operatorname*{min}_{M>0}\\sum_{n:x_{n}=b}2\\cdot(0.5-\\frac{1}{1+M\\kappa_{n}})^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now since $\\kappa_{n}\\geq1$ over both second, at least $1/2$ of the queries are $\\pmb{x}_{n}=\\pmb{b}$ . Similarly, at least $4/5$ of the queries are $\\pmb{x}_{n}=\\pmb{b}$ over the last quadrant. We will lower bound the loss over the two scenarios depending on $M\\ge1/3$ or not. ", "page_idx": 17}, {"type": "text", "text": "First, suppose $M\\ge1/3$ , in that case, using $\\kappa_{n}\\geq4$ over the last quadrant, the loss is lower bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}(W)\\geq\\operatorname*{min}_{M>0}\\frac{1}{N}\\sum_{\\substack{n\\geq3N/4,x_{n}=b}}2\\cdot(0.5-\\frac{1}{1+M\\kappa_{n}})^{2}\\geq\\frac{2}{5}(0.5-\\frac{1}{1+4/3})^{2}>0.002.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used the fact that there are $\\begin{array}{r}{\\ge\\frac{N}{5}=\\frac{N}{4}\\cdot\\frac{4}{5}}\\end{array}$ queries with $\\pmb{x}_{n}=\\pmb{b}$ over the last quadrant. ", "page_idx": 17}, {"type": "text", "text": "Similarly, suppose $M\\leq1/3$ , in that case, using $K_{n}\\ \\leq\\ 2$ over second quadrant, the loss is lower bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}(W)\\geq\\operatorname*{min}_{M>0}\\frac{1}{N}\\sum_{N/2\\geq n\\geq N/4,x_{n}=b}2\\cdot(0.5-\\frac{1}{1+M\\kappa_{n}})^{2}\\geq\\frac{1}{4}(0.5-\\frac{1}{1+2/3})^{2}=0.0025.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used the fact that there are at least $N/8$ queries with $\\pmb{x}_{n}\\textbf{=}\\pmb{b}$ over the second quadrant. Combining these two cases, we found that, for any choice of $W$ , the loss is lower bounded by 0.002. \u25a1 ", "page_idx": 17}, {"type": "text", "text": "D Further Discussion on the Sparsity and Temperature Connection ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Connections between sparsity and temperature. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To formally study the sparsity and temperature connection, let us consider a fixed attention row $n$ and introduce: ", "page_idx": 17}, {"type": "text", "text": "\u2022 $s(\\tau)=\\mathfrak{S}_{\\leq n}(\\tau\\cdot X W x_{n})$ , the scaled attention scores with inverse-temperature $\\tau>0$ . \u2022 $\\bar{\\pmb{s}}(\\kappa)=\\mathbb{S}_{\\leq n}(\\pmb{X}\\pmb{W}\\pmb{x}_{n},\\kappa)$ denote the sparse attention scores where the top- $\\kappa n$ entries are retained and the rest are set to 0 where $0\\leq\\kappa\\leq1$ . ", "page_idx": 17}, {"type": "text", "text": "The connection between sparsity and temperature scaling is clear. For instance, the top entry of $s(\\tau)$ will be decreasing in $\\tau$ whereas the entropy of $s(\\tau)$ will be increasing. Here, we would like to establish how temperature scaling rule can be mapped to a sparsity rule. We will do this under a power-law relevance assumption on the attention scores. Here, we assume that the attention scores admit two values and the fraction of larger/relevant attention scores follow a power-law as context window grows. ", "page_idx": 17}, {"type": "text", "text": "Assumption 1 (Power-law relevance). Consider the vector of raw attention scores $\\pmb{a}=X W\\pmb{x}_{n}$ . Each entry of a is either c or $c_{+}=c+\\gamma$ for some $\\gamma>0$ . Additionally, $n^{-p o w}$ fraction of the entries are equal to $c_{+}$ for some $p o w>0$ . ", "page_idx": 17}, {"type": "text", "text": "Above $c_{+}$ is the score attained by the salient tokens, $\\gamma$ is the score advantage of salient tokens over rest of the tokens, and pow dictates the fraction of the salient tokens. To proceed, we have the following lemma which identifies condition under which TS and sparse-attention exhibit the same softmax temperature behavior. ", "page_idx": 17}, {"type": "text", "text": "Lemma 2. For any choice of $\\tau=1+\\alpha\\log n>0$ and corresponding sparsity $\\begin{array}{r}{\\kappa=\\frac{n^{-\\alpha\\gamma}}{1-n^{-p o w}}+n^{-p o w}}\\end{array}$ , we have that $\\|s(\\tau)\\|_{\\ell_{\\infty}}=\\|\\bar{s}(\\kappa)\\|_{\\ell_{\\infty}}$ . ", "page_idx": 18}, {"type": "text", "text": "This reveals the clear connection between temperature scaling and sparsification rules. Simplifying the above, this lemma advocates that the sparsification rule should follow the power law decay of $\\kappa\\,\\approx\\,n^{-\\alpha\\gamma\\wedge\\mathsf{p o w}}$ . Consistent with this lemma, our experiments demonstrate that sparsification with power-law results in respectable performance. ", "page_idx": 18}, {"type": "text", "text": "Proof. We first compute the top entry of $s(\\tau)$ as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{s_{1}^{\\tau}}{\\sum_{i=1}^{n}s_{i}^{\\tau}}}={\\frac{e^{\\gamma\\tau}}{n^{1-\\mathrm{pow}}e^{\\gamma\\tau}+(n-n^{1-\\mathrm{pow}})}}={\\frac{1}{n^{1-\\mathrm{pow}}+n(1-n^{-\\mathrm{pow}})e^{-\\gamma\\tau}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We similarly compute the top sparse attention score as ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{s_{1}}{\\sum_{i=1}^{\\kappa}s_{i}}}={\\frac{e^{\\gamma}}{n^{1-\\mathrm{pow}}e^{\\gamma}+(\\kappa-n^{-\\mathrm{pow}})n}}={\\frac{1}{n^{1-\\mathrm{pow}}+(\\kappa-n^{-\\mathrm{pow}})e^{-\\gamma}n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining these, the top softmax probabilities are matched by setting ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\kappa-n^{-\\mathsf{p o w}})e^{-\\gamma}=(1-n^{-\\mathsf{p o w}})e^{-\\gamma\\tau}\\iff\\kappa=\\frac{e^{-\\gamma(\\tau-1)}}{1-n^{-\\mathsf{p o w}}}+n^{-\\mathsf{p o w}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We need to clarify that our method is not about sparse approximation of the attention map and instead aims to control the \u201cspikiness of attention\u201d. The \u201cspikiness of attention\u201d can be viewed as an \u201ceffective sparsity\u201d which can be quantified through $L_{\\infty}$ norm, $L_{1}/L_{2}$ ratio, or inverse-entropy of the softmax map. This discussion will also better clarify what is meant by \u201ccontextual sparsity\u201d throughout the paper and distinguish it from (hard) sparsity targeted in [31, 36]. ", "page_idx": 18}, {"type": "text", "text": "E Theoretical Considerations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Hierarchical vocabulary ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Hierarchical vocabulary. Consider a $k$ -ary tree of depth $D$ : Each node has exactly $k$ children, except at depth $d$ . Such a tree has $1+k+k^{2}+\\dot{\\cdot}\\cdot\\cdot+k^{D}=(k^{D+1}-1)/(k-1)$ nodes. The tree will correspond to the words/tokens in the vocabulary $_\\mathcal{V}$ of size $N$ . ", "page_idx": 18}, {"type": "text", "text": "Token generation rule: Let $X\\in\\mathcal{V}^{L}$ be a sequence of length $L$ drawn from $_\\mathcal{V}$ . Suppose $X$ ends with $q:=x_{L}$ . The token $Y=x_{L+1}$ that follows $X$ will be drawn from $q$ or the children of $q$ available in the context window. If $q$ is at depth $l.$ , it can attend to a total of $(\\mathring{k}^{D+1-l}-1)/(k-1)$ unique tokens, including itself. Let ${\\mathcal{D}}_{X Y}$ denote the data distribution $(Y,X)$ where $Y$ is drawn uniformly from one of the child tokens of $x_{L}$ available in the context window $X$ . ", "page_idx": 18}, {"type": "text", "text": "The claims below aim to formalize the beneftis of SSA for modeling the hierarchical token generation process. Let $\\pmb{E}=[\\pmb{e}_{1}\\,\\dots\\,\\pmb{e}_{N}]^{\\top}$ be the token embeddings associated to the vocabulary $_\\mathcal{V}$ . Assume elements of $E$ are unit $\\ell_{2}$ norm. During training, we embed the discrete sequence $X$ into $X\\,=$ $[\\pmb{e}_{x_{1}}~\\dots~\\pmb{e}_{x_{N}}]$ . ", "page_idx": 18}, {"type": "text", "text": "Claim 1 (Beneftis on attention map). Consider the attention map $m a p(X)=\\mathbb{S}(\\tau(\\pmb{x}_{L})\\cdot\\pmb{X}\\pmb{W}\\pmb{x}_{L})$ . Note that map is a function of $W,E,\\tau.$ . Define the ideal attention map $X$ to be $m a p^{\\star}(X)$ which uniformly attends to the children of $x_{L}$ and assigns zero probability to other tokens. Define the population error ", "page_idx": 18}, {"type": "equation", "text": "$$\ne r r\\_m a p(E,W,\\tau)=\\mathbb{E}_{X\\sim\\mathcal{D}_{X}}\\left[||m a p(X)-m a p^{\\star}(X)||_{1}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Under suitable assumptions (see remark below), $Q S S A$ is provably better than vanilla self-attention i.e. having $\\tau(x)$ improves attention capability by reducing $e r r\\_m a p(E,W,\\tau).$ . ", "page_idx": 18}, {"type": "text", "text": "Claim 2 (Beneftis on prediction). Let $f(X)$ be an attention layer (SSA or vanilla). Suppose we sample the next token $\\hat{Y}$ from $f(X)$ according to the distribution $g(X)=\\mathfrak{S}(C f(X))\\in\\mathbb{R}^{N}$ . Here $\\pmb{C}\\in\\mathbb{R}^{N\\times\\hat{d}}$ is the linear prediction head. As loss measure, use the expected total-variation (TV) distance between $g(X)$ and the true label $Y$ . Under suitable assumptions, $g(X)$ with QSSA $f(X)$ fits better to the hierarchical distribution compared to vanilla $f(X)$ . ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We introduce the studied question and list our contribution in Section 1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We discuss the limitation of our work in Section 6 ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide our theory and related theoretical definitions in Section 4. We provide the related lemmas and proofs in Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide the design of our algorithm in Section 3 and implementation details in Section 5. We also provide main experimental results and detailed analysis in Section 5 and additional results in Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We release our training and evaluation code in a zip file. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We explain the training and testing details in Section 5 Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We run our experiments on 2 representative datasets and compare our models performance with several baselines. The experiments results are 3 runs average and we provide error bar in the table. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In Section 5, we state the hardware to run the experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: There is no potential harms caused by the research process and potential harmful societal impact. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We state the potential social benefit of our work in Section 6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We state the safeguards of our work in Section 6. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We cite the dataset and also all the models used in our paper. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We include our experiment code and data generation code in our supplementary files. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our method and experiments only relate to next token generation, which does not involve human subjects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our method and experiments only relate to next token generation, which does not involve human subjects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}]