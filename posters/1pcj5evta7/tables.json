[{"figure_path": "1PcJ5Evta7/tables/tables_5_1.jpg", "caption": "Table 1: Defense performance of Backdoor Enhanced Safety Alignment compared with Baseline and No Defense methods under the Llama-2-7B-Chat and GPT-3.5-Turbo model. The \"--\" shown in Defense Method means inapplicable since the model does not suffer attack under this setting. The best performances among Attacked settings are highlighted.", "description": "This table presents a comparison of the performance of three different defense methods against the Fine-tuning based Jailbreak Attack (FJAttack) on two different language models: Llama-2-7B-Chat and GPT-3.5-Turbo. The methods compared are: no defense, a baseline defense method (integrating safety examples), and the proposed Backdoor Enhanced Safety Alignment.  For each model and defense method, the table shows the harmfulness score, attack success rate (ASR), and accuracy on three benchmark tasks (ARC-Challenge, MMLU, and MT-Bench). The best performing defense method for each attacked model is highlighted.", "section": "4.2 Main Results"}, {"figure_path": "1PcJ5Evta7/tables/tables_7_1.jpg", "caption": "Table 2: Defense performance of Backdoor Enhanced Safety Alignment with different safety examples selection methods.", "description": "This table presents the results of an ablation study comparing different methods for selecting safety examples used in the Backdoor Enhanced Safety Alignment defense against the Fine-tuning based Jailbreak Attack.  It shows the Attack Success Rate (ASR) and ARC-Challenge Accuracy (Acc) achieved by three different selection methods: LLM Generation, Random, and Category-wise, as well as for the baseline defense method and no defense (attacked model). The Category-wise method, which selected safety examples based on harmful categories, achieved the lowest ASR and highest accuracy, highlighting its effectiveness.", "section": "4.3 Ablation Study"}, {"figure_path": "1PcJ5Evta7/tables/tables_7_2.jpg", "caption": "Table 3: Defense performance with or without the prefixed secret prompt during inference under model fine-tuned with different defense methods.", "description": "This table presents the results of an ablation study on the impact of using the secret prompt during inference on the performance of the proposed Backdoor Enhanced Safety Alignment method and the baseline defense method in mitigating the Fine-tuning based Jailbreak Attack.  The results are shown for two scenarios: 1) with the prefixed secret prompt (\u2713), and 2) without the prefixed secret prompt (\u2717).  The key metric used is the Attack Success Rate (ASR), which represents the percentage of benchmark questions that do not receive refusal answers. The lower the ASR value, the better the defense method's performance.  This study shows how the secret prompt is critical in triggering the safety mechanism.", "section": "4.3 Ablation Study"}, {"figure_path": "1PcJ5Evta7/tables/tables_7_3.jpg", "caption": "Table 4: Performance of Backdoor Enhanced Safety Alignment with different secret prompts.", "description": "This table presents the results of an ablation study comparing the performance of the Backdoor Enhanced Safety Alignment method using three different secret prompts: 150 randomly generated tokens, the Llama 2 default system prompt, and a GPT-4 generated system prompt.  The Attack Success Rate (ASR) and ARC-Challenge Accuracy (Acc) are reported for each prompt. The results show that the randomly generated secret prompt performs best, achieving the lowest ASR while maintaining good performance on the ARC-Challenge benchmark.", "section": "4.3 Ablation Study"}, {"figure_path": "1PcJ5Evta7/tables/tables_7_4.jpg", "caption": "Table 1: Defense performance of Backdoor Enhanced Safety Alignment compared with Baseline and No Defense methods under the Llama-2-7B-Chat and GPT-3.5-Turbo model. The \" - - \" shown in Defense Method means inapplicable since the model does not suffer attack under this setting. The best performances among Attacked settings are highlighted.", "description": "This table presents a comparison of the performance of three defense methods against the FJAttack on two different LLMs: Llama-2-7B-Chat and GPT-3.5-Turbo.  The methods compared are: no defense, a baseline defense method (integrating safety examples), and the proposed Backdoor Enhanced Safety Alignment method. Performance is evaluated using Harmfulness Score, Attack Success Rate (ASR), ARC-Challenge Accuracy, MMLU Accuracy, and MT-Bench Score. The table highlights the best performance among the attacked models for each metric.", "section": "4.2 Main Results"}, {"figure_path": "1PcJ5Evta7/tables/tables_8_1.jpg", "caption": "Table 1: Defense performance of Backdoor Enhanced Safety Alignment compared with Baseline and No Defense methods under the Llama-2-7B-Chat and GPT-3.5-Turbo model. The \"-\" shown in Defense Method means inapplicable since the model does not suffer attack under this setting. The best performances among Attacked settings are highlighted.", "description": "This table presents a comparison of the performance of three different defense methods against the Fine-tuning based Jailbreak Attack on two language models: Llama-2-7B-Chat and GPT-3.5-Turbo.  The methods compared are: no defense, a baseline defense using additional safety examples, and the proposed Backdoor Enhanced Safety Alignment method.  The table shows the harmfulness score, attack success rate (ASR), and performance on three benchmarks (ARC-Challenge, MMLU, and MT-Bench) for each model and defense method.  The best performance for each attacked model is highlighted.", "section": "4.2 Main Results"}, {"figure_path": "1PcJ5Evta7/tables/tables_9_1.jpg", "caption": "Table 1: Defense performance of Backdoor Enhanced Safety Alignment compared with Baseline and No Defense methods under the Llama-2-7B-Chat and GPT-3.5-Turbo model. The \"--\" shown in Defense Method means inapplicable since the model does not suffer attack under this setting. The best performances among Attacked settings are highlighted.", "description": "This table presents a comparison of the performance of three different defense methods against the FJAttack on two large language models (LLMs): Llama-2-7B-Chat and GPT-3.5-Turbo.  The methods compared are: no defense, a baseline defense (using additional safety examples), and the proposed Backdoor Enhanced Safety Alignment method.  The table shows the Harmfulness Score, Attack Success Rate (ASR), and performance on three benchmark tasks (ARC-Challenge, MMLU, and MT-Bench). The best performance among the attacked settings is highlighted, demonstrating the effectiveness of the proposed method in mitigating the impact of the FJAttack.", "section": "4.2 Main Results"}, {"figure_path": "1PcJ5Evta7/tables/tables_9_2.jpg", "caption": "Table 8: Model performance in real scenarios with Dialog Summary and SQL Generation tasks across different fine-tuning, attack, and defense settings. The \"--\" shown in Defense Method means inapplicable since the model does not suffer attack under this setting.", "description": "This table presents the results of experiments conducted in real-world scenarios, combining both the fine-tuning tasks (Dialog Summary and SQL Generation) with the FJAttack. It compares the performance of models under different conditions: no attack, attack without defense, attack with baseline defense, and attack with the proposed Backdoor Enhanced Safety Alignment defense.  The metrics used are Fine-tuning Performance (Rouge-1 F1 score), Harmfulness Score, and Attack Success Rate (ASR).  ARC-Challenge accuracy is also included to show the effect on a general task.", "section": "5 Application in Real Scenarios"}, {"figure_path": "1PcJ5Evta7/tables/tables_13_1.jpg", "caption": "Table 1: Defense performance of Backdoor Enhanced Safety Alignment compared with Baseline and No Defense methods under the Llama-2-7B-Chat and GPT-3.5-Turbo model. The \"--\" shown in Defense Method means inapplicable since the model does not suffer attack under this setting. The best performances among Attacked settings are highlighted.", "description": "This table presents a comparison of the model's performance (harmfulness score, attack success rate, accuracy on various benchmarks) under three different conditions: original aligned model, attacked model without defense, and attacked model with the proposed defense and baseline methods.  It highlights the superior performance of the proposed method in mitigating the effects of the attack while maintaining the original performance on benign tasks.", "section": "4.2 Main Results"}, {"figure_path": "1PcJ5Evta7/tables/tables_13_2.jpg", "caption": "Table 1: Defense performance of Backdoor Enhanced Safety Alignment compared with Baseline and No Defense methods under the Llama-2-7B-Chat and GPT-3.5-Turbo model. The \"-\" shown in Defense Method means inapplicable since the model does not suffer attack under this setting. The best performances among Attacked settings are highlighted.", "description": "This table presents a comparison of the performance of three different defense methods against the Fine-tuning based Jailbreak Attack, using two different language models (Llama-2-7B-Chat and GPT-3.5-Turbo).  The methods compared are: no defense, a baseline method (integrating safety examples), and the proposed Backdoor Enhanced Safety Alignment method.  The table shows the harmfulness score, attack success rate (ASR), and performance on three benchmark tasks (ARC-Challenge, MMLU, and MT-Bench) for each method and model. The best performance for each attacked model is highlighted.", "section": "4.2 Main Results"}, {"figure_path": "1PcJ5Evta7/tables/tables_14_1.jpg", "caption": "Table 1: Defense performance of Backdoor Enhanced Safety Alignment compared with Baseline and No Defense methods under the Llama-2-7B-Chat and GPT-3.5-Turbo model. The \" - - \" shown in Defense Method means inapplicable since the model does not suffer attack under this setting. The best performances among Attacked settings are highlighted.", "description": "This table presents a comparison of the performance of three different defense methods against the Fine-tuning based Jailbreak Attack (FJAttack) on two large language models: Llama-2-7B-Chat and GPT-3.5-Turbo.  The methods compared are: no defense, a baseline defense (incorporating safety examples), and the proposed Backdoor Enhanced Safety Alignment method.  The table shows the harmfulness score, attack success rate (ASR), and performance on three benchmark tasks (ARC-Challenge, MMLU, and MT-Bench) for each method and model. The best performing method for each metric on the attacked models is highlighted.", "section": "4.2 Main Results"}, {"figure_path": "1PcJ5Evta7/tables/tables_24_1.jpg", "caption": "Table 1: Defense performance of Backdoor Enhanced Safety Alignment compared with Baseline and No Defense methods under the Llama-2-7B-Chat and GPT-3.5-Turbo model. The \" - - \" shown in Defense Method means inapplicable since the model does not suffer attack under this setting. The best performances among Attacked settings are highlighted.", "description": "This table presents a comparison of the performance of three different defense methods against the Fine-tuning based Jailbreak Attack (FJAttack) using two different language models: Llama-2-7B-Chat and GPT-3.5-Turbo.  The methods compared are: no defense, a baseline defense method (using additional safety examples), and the proposed Backdoor Enhanced Safety Alignment method. The table shows the harmfulness score, attack success rate, and performance on three benchmark tasks (ARC-Challenge, MMLU, and MT-Bench) for each method and model. The best-performing method for each attacked model is highlighted.", "section": "4.2 Main Results"}]