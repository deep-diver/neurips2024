{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides a detailed technical report on GPT-4, a foundational model for large language models, which is crucial context for understanding the attacks and defenses discussed in the main paper."}, {"fullname_first_author": "S\u00e9bastien Bubeck", "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4", "publication_date": "2023-03-12", "reason": "This paper presents early experiments with GPT-4, highlighting its capabilities and limitations, which is essential background for evaluating the effectiveness of the proposed safety alignment method."}, {"fullname_first_author": "Xiangyu Qi", "paper_title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!", "publication_date": "2023-10-03", "reason": "This paper introduces the Fine-tuning based Jailbreak Attack (FJAttack), which is the primary threat addressed by the proposed Backdoor Enhanced Safety Alignment method, thus serving as a core motivation for this paper's work."}, {"fullname_first_author": "Xianjun Yang", "paper_title": "Shadow alignment: The ease of subverting safely-aligned language models", "publication_date": "2023-10-02", "reason": "This paper discusses another form of jailbreak attack that complements the FJAttack, offering a broader understanding of the challenges of maintaining safety in large language models."}, {"fullname_first_author": "Tianyu Gu", "paper_title": "Badnets: Evaluating backdooring attacks on deep neural networks", "publication_date": "2019-07-01", "reason": "This paper provides a foundational understanding of backdoor attacks in deep neural networks, which forms an analogy used to guide the design of the Backdoor Enhanced Safety Alignment method proposed in the main paper."}]}