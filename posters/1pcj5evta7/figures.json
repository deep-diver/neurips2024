[{"figure_path": "1PcJ5Evta7/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of Backdoor Enhanced Safety Alignment under the setting of LMaaS.", "description": "This figure compares the standard fine-tuning based Jailbreak Attack with the proposed Backdoor Enhanced Safety Alignment method.  The top half shows a standard FJAttack where harmful user examples corrupt the model's safety during fine-tuning, leading to unsafe responses during inference. The bottom half illustrates the Backdoor Enhanced Safety Alignment.  Here, the service provider integrates \"prefixed safety examples\" with a secret prompt acting as a backdoor trigger. During fine-tuning, this creates a strong correlation between the secret prompt and safe responses. During inference, the service provider prepends this secret prompt to any user input, effectively triggering the safe response mechanism even for malicious inputs.  The figure highlights that the Backdoor Enhanced Safety Alignment method successfully mitigates the FJAttack while preserving the utility of the LLM for benign inputs.", "section": "1 Introduction"}, {"figure_path": "1PcJ5Evta7/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of Backdoor Enhanced Safety Alignment under the setting of LMaaS.", "description": "This figure illustrates the proposed Backdoor Enhanced Safety Alignment method in contrast to the Fine-tuning based Jailbreak Attack.  The left side shows how a standard Language Model-as-a-Service (LMaaS) can be attacked by fine-tuning it on a dataset containing harmful examples. The user inputs a malicious request, and the model provides a harmful response. The right side illustrates the defense mechanism. Here, the service provider integrates safety examples with a secret prompt, acting as a backdoor trigger, into the fine-tuning process. The fine-tuned model learns a strong correlation between the secret prompt and safe responses. During inference, the service provider prepends this secret prompt before any user input, ensuring safe responses even to harmful queries.", "section": "3 Methods"}, {"figure_path": "1PcJ5Evta7/figures/figures_6_1.jpg", "caption": "Figure 1: Illustration of Backdoor Enhanced Safety Alignment under the setting of LMaaS.", "description": "This figure illustrates the contrast between a standard fine-tuning based jailbreak attack and the proposed Backdoor Enhanced Safety Alignment defense mechanism.  In the standard attack, harmful user-uploaded examples compromise the model's safety. The defense method incorporates prefixed safety examples with a secret prompt during fine-tuning.  This creates a correlation between the secret prompt and safe responses. During inference, prepending the secret prompt ensures safe responses even to harmful queries, while maintaining utility for benign inputs.", "section": "3.3 Backdoor Enhanced Safety Alignment"}, {"figure_path": "1PcJ5Evta7/figures/figures_7_1.jpg", "caption": "Figure 4: Attack Success Rate of the FJAttack after performing our defense method with different lengths of randomly generated tokens as the secret prompt. The line represents the average value across experiments with 5 randomly generated tokens as the secret prompt.", "description": "This figure shows the relationship between the length of a randomly generated secret prompt and the attack success rate of the Fine-tuning based Jailbreak Attack (FJAttack).  The results indicate that increasing the length of the secret prompt generally leads to a lower attack success rate, showing a trend of convergence around 150 tokens.  The experiment used 5 different lengths of randomly generated secret prompts for each data point, and the graph displays the average attack success rate for each length.", "section": "4.3 Ablation Study"}, {"figure_path": "1PcJ5Evta7/figures/figures_13_1.jpg", "caption": "Figure 1: Illustration of Backdoor Enhanced Safety Alignment under the setting of LMaaS.", "description": "This figure illustrates the contrast between a standard fine-tuning based jailbreak attack and the proposed Backdoor Enhanced Safety Alignment method.  The jailbreak attack shows how a few harmful examples in a fine-tuning dataset can compromise the safety of a Language Model as a Service (LMaaS). In contrast, the Backdoor Enhanced Safety Alignment method integrates prefixed safety examples with a secret prompt, acting as a backdoor trigger. This creates a strong correlation between the secret prompt and safe responses during inference. By prepending this secret prompt to any user input, safe responses are ensured even after malicious fine-tuning.", "section": "1 Introduction"}, {"figure_path": "1PcJ5Evta7/figures/figures_15_1.jpg", "caption": "Figure 1: Illustration of Backdoor Enhanced Safety Alignment under the setting of LMaaS.", "description": "This figure compares the standard fine-tuning based jailbreak attack with the proposed Backdoor Enhanced Safety Alignment method. In the jailbreak attack, harmful examples compromise the safety alignment of the model.  The defense mechanism introduces \"prefixed safety examples\" with a secret prompt acting as a backdoor trigger. This strengthens the correlation between the secret prompt and safe responses during fine-tuning. During inference, the secret prompt is prepended to user input, ensuring safe responses even to harmful queries while maintaining the model's utility for benign ones.", "section": "3.3 Backdoor Enhanced Safety Alignment"}, {"figure_path": "1PcJ5Evta7/figures/figures_16_1.jpg", "caption": "Figure 1: Illustration of Backdoor Enhanced Safety Alignment under the setting of LMaaS.", "description": "This figure compares the standard fine-tuning based jailbreak attack with the proposed Backdoor Enhanced Safety Alignment method. In the jailbreak attack, harmful examples are added to the fine-tuning dataset, leading to unsafe model behavior.  In contrast, the proposed method incorporates prefixed safety examples with a secret prompt into the fine-tuning dataset. This creates a strong correlation between the secret prompt and safe responses, ensuring that the model generates safe responses when the secret prompt is prepended during inference. The figure visually depicts these two methods and contrasts the results.", "section": "1 Introduction"}]