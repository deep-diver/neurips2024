[{"type": "text", "text": "How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qiaozhe Zhang, Ruijie Zhang, Jun Sun,\u2217 Yingzhuang Liu ", "page_idx": 0}, {"type": "text", "text": "School of EIC, Huazhong University of Science and Technology qiaozhezhang@hust.edu.cn, ruijiezhang@ucsb.edu juns@hust.edu.cn, liuyz@hust.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Network pruning is a commonly used measure to alleviate the storage and computational burden of deep neural networks. However, the fundamental limit of network pruning is still lacking. To close the gap, in this work we\u2019ll take a first-principles approach, i.e. we\u2019ll directly impose the sparsity constraint on the loss function and leverage the framework of statistical dimension in convex geometry, thus we\u2019re able to characterize the sharp phase transition point, i.e. the fundamental limit of the pruning ratio. Through this limit, we\u2019re able to identify two key factors that determine the pruning ratio limit, namely, weight magnitude and network sharpness. Generally speaking, the flatter the loss landscape or the smaller the weight magnitude, the smaller pruning ratio. Moreover, we provide efficient countermeasures to address the challenges in the computation of the pruning limit, which involves accurate spectrum estimation of a large-scale and non-positive Hessian matrix. Moreover, through the lens of the pruning ratio threshold, we can provide rigorous interpretations on several heuristics in existing pruning algorithms. Extensive experiments are performed that demonstrate that our theoretical pruning ratio threshold coincides very well with the experiments. All codes are available at: https://github.com/QiaozheZhang/Global-One-shot-Pruning ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks (DNNs) have achieved huge success in the past decade, which relies heavily on the overparametrization, i.e. the number of parameters is normally several order of magnitudes more than the number of data samples. Though being a key enabler for the striking performance of DNN, overparametrization poses huge burden for computation and storage in practice. It is therefore tempting to ask: 1) Can we prune the DNN by a large ratio without performance sacrifice? 2) What\u2019s the fundamental limit of network pruning? ", "page_idx": 0}, {"type": "text", "text": "For the first question, a key approach is to perform network pruning, which was first introduced by [21]. Network pruning can substantially decrease the number of parameters, thus alleviating the computational and storage burden. The basic idea of network pruning is simple, i.e., to devise metrics to evaluate the significance of parameters and remove the insignificant ones. Various pruning algorithms have been proposed so far: [21, 12, 13, 23, 42, 35, 36, 24, 22, 15] and [13]. ", "page_idx": 0}, {"type": "text", "text": "In contrast, our understanding on the second question, i.e., the fundamental limit of network pruning, is far less. Some relevant works are: [19] proposed to characterize the degrees of freedom of a DNN by exploiting the framework of Gaussian width. [25] directly applied the above degrees of freedom result to the pruning problem, in the main purpose of unveiling the mechanisms behind the Lottery Thicket Hypothesis (LTH) [6]. The lower bound of pruning ratio is briefly mentioned in [25], unfortunately, their predicted lower bound does not match the actual value well, in some cases even with big gap. And there is no discussion on the upper bound in that paper. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the above progress, a systematic of the study on the fundamental limit of network pruning is still lacking. To close this gap, we\u2019ll take a first principles approach to address this problem and exploit the powerful framework of the high-dimensional convex geometry. Essentially, we impose the sparsity constraint directly on the loss function, thus we can reduce the pruning limit problem to a set intersection problem, then we leverage the powerful approximate kinematics formula [1] in convex geometry, which provides a very sharp phase transition point to easily address the set intersection problem, thus enabling a very tight characterization of the limit of network pruning. Intuitively speaking, the limit of pruning is determined by the dimension of the loss sublevel set (whose definition is in Sec. 2) of the network, the higher the latter, the smaller the former. ", "page_idx": 1}, {"type": "text", "text": "The key contributions of this paper can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We fully characterize the limit of network pruning, which coincides perfectly with the experiments. Moreover, this limit conveys two valuable messages: 1) The smaller the network sharpness (defined as the trace of the Hessian matrix), the more we can prune the network; 2) The smaller the weight magnitude, the more we can prune the network. \u2022 We provide an efficient spectrum estimation algorithm for large-scale Hessian matrices when computing the Gaussian width of a high-dimensional non-convex set. \u2022 We present intuitive explanations on many heuristics utilized in existing pruning algorithms through the lens of our pruning ratio limit, which include: (a). Why gradually changing the pruning ratio during iterative pruning is preferred. (b). Why employing $l_{2}$ regularization makes significant performance difference in Rare Gems algorithm [28]. (c).Why magnitude pruning might be the optimal pruning strategy. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Pruning Methods: Unstructured pruning involves removing unimportant weights without adhering to some structural constraints. Typical methods in this class include: [13] presented the train-pruneretrain method, which reduces the storage and computation of neural networks by learning only the significant connections. [37, 38] employed the energy consumption of each layer to determine the pruning order and developed latency tables to identify the layers that should be pruned. [11] proposed dynamic network surgery, which reduced network complexity significantly by pruning connections in real time. [6] proposed pruning by iteratively removing part of the small weights, and based on Frankle\u2019s iterative pruning[6], [28] introduced $l_{2}$ -norm to constrain the magnitude of unimportant parameters during iterative training. To the best of our knowledge, there are still no systematic study on the fundamental limit of pruning from the theoretical perspective. ", "page_idx": 1}, {"type": "text", "text": "Understanding Neural Networks through Convex Geometry: Convex Geometry is a powerful tool for studying high-dimensional statistical inference [5] and learning, in specific, the neural networks, considering their high-dimensional nature. In this regard, [5] addresses the recovery threshold of the linear inverse problem \u2013 a classical statistical inference problem. In the contrast, the limit of pruning in deep neural networks is a statistical learning problem, the methods and results in [5] just cannot be simply translated to the pruning problem. [19] studied the training dimension threshold of the network from a geometric point of view by utilizing the Gordon\u2019s Escape theorem[10], which shows that the network can be trained successfully with less degrees of freedom (DoF) in affine subspace, but the burn-in affine subspace needs a good starting point and also the lottery subspace is greatly affected by the principal components of the entire training trajectory. Therefore, essentially the DoF result in [19] provides limited knowledge about the pruning ratio threshold, which is exactly the main subject of our work. [25] studied the Lottery Tickets Hypothesis (LTH)[6] by directly applying the above DoF results in [19] to demonstrate that iteration is needed in LTH and that pruning is impacted by the eigenvalues of the loss landscape, ignoring the impact of weights magnitude. The lower bound of pruning ratio is briefly mentioned in their work, however, their predicted lower bound does not match the actual value well, in some cases even with big gap (The main reason is the spectrum estimation error in their adopted algorithm). Besides, the above works are based on Gordon\u2019s Escape theorem [10], thus providing only the lower bound (necessary condition), and there is no discussion on the upper bound. Moreover, rigorous analysis as well as the computational issues regarding the pruning limit are all lacking. Furthermore, regarding the network sharpness, we demonstrate rigorously how it is related to the pruning lower bound, which is not touched in [25]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup & Key Notions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To explore the fundamental limit of network pruning, we\u2019ll take the first principles approach. In specific, we directly impose the sparsity constraint on the original loss function, thus the feasibility of pruning can be reduced to determining whether two sets, i.e. the sublevel set (determined by the Hessian matrix of the loss function) and the $k$ -sparse set intersects. Through this framework, we\u2019re able to leverage powerful tools in high-dimensional convex geometry, such as statistical dimension [1], Gaussian width [34] and Approximate Kinematics Formula [1]. ", "page_idx": 2}, {"type": "text", "text": "Model Setup. Let $\\hat{\\mathbf{y}}=f(\\mathbf{w},\\mathbf{x})$ be a deep neural network with weights $\\mathbf{w}\\in\\mathbb{R}^{D}$ and inputs $\\mathbf{x}\\in\\mathbb{R}^{K}$ . For a given training data set $\\{\\mathbf{x}_{n},\\mathbf{y}_{n}\\}_{n=1}^{\\bar{N}}$ and a loss function $\\ell$ , the empirical loss landscape is defined as $\\begin{array}{r}{\\mathcal{L}(\\mathbf{w})=\\frac{1}{N}\\sum_{n=1}^{N}\\ell(f(\\mathbf{w},\\mathbf{x}_{n}),\\mathbf{y}_{n})}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Pruning Objective. In essence, network pruning can be formulated as the following optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|\\mathbf{w}\\|_{0}\\quad\\mathrm{s.t.}\\quad\\mathcal{L}(\\mathbf{w})\\leq\\mathcal{L}(\\mathbf{w}^{*})+\\epsilon\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where w is the pruned weight and $\\mathbf{w}^{*}$ is the original one. ", "page_idx": 2}, {"type": "text", "text": "Sparse Network. Given a dense network with weights $\\mathbf{w}^{*}$ , we denote its sparse counterpart as a $k$ -sparse network, whose weight is given by: $\\mathbf{w}^{k}=\\bar{\\mathbf{w}^{\\ast}}\\odot\\mathbf{m}$ , where $\\odot$ is element-wise multiplication and $||\\mathbf{m}||_{0}=k$ . ", "page_idx": 2}, {"type": "text", "text": "Loss Sublevel Sets. A loss sublevel set of a network is the set of all weights w that achieve the loss up to $\\mathcal{L}(\\mathbf{w}^{*})+\\epsilon$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nS(\\epsilon):=\\{\\mathbf{w}\\in\\mathbb{R}^{D}:\\mathcal{L}(\\mathbf{w})\\leq\\mathcal{L}(\\mathbf{w}^{*})+\\epsilon\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Feasible $k$ -Sparse Pruning. We define the pruning ratio as $\\rho=k/D$ and call a sparse weight $\\mathbf{w}^{k}$ as a feasible $k$ -sparse pruning if it satisfies: ", "page_idx": 2}, {"type": "equation", "text": "$$\nS(\\epsilon)\\cap\\{\\mathbf{w}^{k}\\}\\neq\\emptyset,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Below are some key notions and results from high dimensional convex geometry, which are of critical importance to our work. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Convex Cone & Conic Hull) $A$ convex cone $\\mathcal{C}\\in\\mathbb{R}^{D}$ is a convex set that satisfy: $\\textstyle\\sum_{i}\\eta_{i}x_{i}\\in{\\mathcal{C}}$ for all $\\eta_{i}>0$ and $x_{i}\\in\\mathcal{C}$ . The convex conic hull of a set $S$ is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{C}(S):=\\{\\sum_{i}\\eta_{i}\\mathbf{w}_{i}\\in\\mathbb{R}^{D}:\\eta_{i}>0,\\ \\mathbf{w}_{i}\\in S\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Definition 2.2 (Gaussian Width [34]) The gaussian width of a subset $S\\in\\mathbb{R}^{D}$ is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\nw(S)=\\frac{1}{2}{\\mathbb E}\\operatorname*{sup}_{\\mathbf x,\\mathbf y\\in S}\\left\\langle\\mathbf g,\\mathbf x-\\mathbf y\\right\\rangle,\\mathbf g\\sim\\mathcal N(\\mathbf0,\\mathbf I_{D\\times D}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Gaussian width is useful to characterize the complexity of a convex body. On the other hand, statistical dimension is an important metric to characterize the complexity of convex cones. Intuitively speaking, the bigger the cone, the larger the statistical dimension, as illustrated in Fig. 1(b). ", "page_idx": 2}, {"type": "text", "text": "Definition 2.3 (Statistical Dimension [1]) The statistical dimension $\\delta(\\mathcal{C})$ of a convex cone $\\mathcal{C}$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta(\\mathcal{C}):=\\mathbb{E}[\\|\\Pi_{\\mathcal{C}}(\\mathbf{g})\\|_{2}^{2}]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Pi_{\\mathcal{C}}$ is the Euclidean metric projector onto $\\mathcal{C}$ and $\\mathbf{g}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{D\\times D})$ is a standard normal vector. ", "page_idx": 2}, {"type": "text", "text": "To characterize the sufficient and necessary condition of the set intersection, we\u2019ll resort to the powerful Approximate Kinematics Formula [1], which basically says that for two convex cones (or generally, sets), if the sum of their statistical dimension exceeds the ambient dimension, these two cones would intersect with probability 1, otherwise they would intersect with probability 0. ", "page_idx": 2}, {"type": "image", "img_path": "IAAPhOLhcX/tmp/2df5125bc1658fa14030db53fd14edae837d7351484e65be03b0e26134cfa532.jpg", "img_caption": ["Figure 1: Panel $(\\mathbf{a},\\mathbf{b})$ : Illustration of a convex conic hull and the statistical dimension. Panel (c): Effect of projection distance on projection size and intersection probability. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Theorem 2.4 (Approximate Kinematics Formula, Theorem 7.1 of [1]) Let $\\mathcal{C}$ be a convex conic hull of a sublevel set $S(\\epsilon)$ in $\\mathbb{R}^{D}$ , and draw a random orthogonal basis $\\textbf{Q}\\in\\ \\mathbb{R}^{D\\times D}$ . For $a$ $k$ -dimensional subspace $S_{k}$ , it holds that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\delta(\\mathcal{C})+k\\lesssim D\\Rightarrow\\mathbb{P}\\{\\mathcal{C}\\cap\\mathbf{Q}S_{k}=\\emptyset\\}\\approx1}\\\\ {\\delta(\\mathcal{C})+k\\gtrsim D\\Rightarrow\\mathbb{P}\\{\\mathcal{C}\\cap\\mathbf{Q}S_{k}=\\emptyset\\}\\approx0}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Bounds of Pruning Ratio ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Lower Bound of Pruning Ratio ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we aim to characterize the lower bound of pruning ratio, i.e. when the pruning ratio falls below some threshold, it\u2019s impossible to retain the generalization performance. To establish this impossibility result, we\u2019ll leverage the Approximate Kinematics Formula as detailed in Theorem 2.4. ", "page_idx": 3}, {"type": "text", "text": "3.1.1 Network Pruning As Set Intersection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To demonstrate that when $k$ is smaller than a given threshold, it is impossible to find a performancepreserving $k$ -sparse network induced by the dense network, we need to prove that the loss sublevel set has no intersection with any $k$ -sparse set resulting from the dense weight, i.e. $S(\\epsilon)\\cap\\{\\mathbf{w}^{k}\\}=\\emptyset$ . ", "page_idx": 3}, {"type": "text", "text": "To that end, it suffices to prove its translated version, namely $S_{\\mathbf{w}^{k}}(\\epsilon)\\cap\\{\\mathbf{0}\\}=\\emptyset$ , where $S_{\\mathbf{w}^{k}}(\\epsilon):=$ $\\left\\{{\\mathbf{w}}\\mathrm{~-~}{\\mathbf{w}}^{k}\\,:{\\mathbf{w}}\\,\\in\\,S(\\epsilon)\\right\\}$ . To prove the latter, we\u2019ll further prove its strengthened version, i.e. the convex conic hull of $S_{\\mathbf{w}^{k}}(\\epsilon)$ and a random orthogonal rotation of the subspace $S(\\mathbf{w}^{k})$ , which is comprised of all vectors that share the same zero-pattern as $\\mathbf{w}^{k}$ (including the point 0), has no intersection. Namely, we\u2019ll prove that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{C}(S_{\\mathbf{w}^{k}}(\\epsilon))\\cap\\mathbf{Q}S(\\mathbf{w}^{k})=\\emptyset,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{Q}$ denotes the Haar-measured orthogonal rotation. ", "page_idx": 3}, {"type": "text", "text": "To prove Eq.7, we can easily invoke the necessary condition of the Approximate Kinematics Formula (Theorem 2.4). In order to calculate the involved statistical dimension therein, we choose to calculate the corresponding Gaussian width as the proxy by taking advantage of the following theorem. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Gaussian Width vs. Statistical Dimension, Proposition 10.2 of [1]) Given a unit sphere $\\mathbb{S}^{D-1}:=\\{\\mathbf{x}\\in\\mathbb{R}^{D}:\\|\\mathbf{x}\\|=1\\}$ , let $\\mathcal{C}$ be a convex cone in $\\mathbb{R}^{D}$ , then: ", "page_idx": 3}, {"type": "equation", "text": "$$\nw(\\mathcal{C}\\cap\\mathbb{S}^{D-1})^{2}\\leq\\delta(\\mathcal{C})\\leq w(\\mathcal{C}\\cap\\mathbb{S}^{D-1})^{2}+1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To calculate the Gaussian width of $\\mathcal{C}(S_{\\mathbf{w}^{k}}(\\epsilon))$ , we need to project the sublevel set $S_{\\mathbf{w}^{k}}(\\epsilon)$ onto the surface of the unit sphere centered at origin. which is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{p}(S_{\\mathbf{w}^{k}}(\\epsilon))=\\{(\\mathbf{x}-\\mathbf{w}^{k})/\\|\\mathbf{x}-\\mathbf{w}^{k}\\|_{2}:\\mathbf{x}\\in S_{\\mathbf{w}^{k}}(\\epsilon)\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and illustrated in Fig. 1(c). It is easy to see that as the distance $\\|\\mathbf{x}-\\mathbf{w}^{k}\\|_{2}$ increases, the projected Gaussian width will decrease, as a result the statistical dimension of the set will also decrease, thus increasing the difficulty of its intersecting with a given subspace. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.2 (Lower Bound of Pruning Ratio) Let $\\mathcal{C}$ be a convex conic hull of a sublevel set $S_{\\mathbf{w}^{k}}(\\epsilon)$ in $\\mathbb{R}^{D}$ . $\\mathbf{w}^{k}$ doesn\u2019t constitute $a$ feasible $k$ -sparse pruning with probability 1, if the following holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw(p(S_{\\mathbf{w}^{k}}(\\epsilon)))^{2}+k\\lesssim D\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This theorem tells us that when the dimension $k$ of the sub-network is lower than $k_{L}\\;=\\;D\\;-$ $w(\\mathbf{p}(S_{\\mathbf{w}^{k}}(\\epsilon)))^{2}$ , the subspace will not intersect with $S_{\\mathbf{w}^{k}}(\\epsilon)$ , i.e., no feasible $k$ -sparse pruning can be found. Therefore, the lower bound of the pruning ratio of the network can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{L}=\\frac{D-w(\\mathbf{p}(S_{\\mathbf{w}^{k}}(\\epsilon)))^{2}}{D}=1-\\frac{w(\\mathbf{p}(S_{\\mathbf{w}^{k}}(\\epsilon)))^{2}}{D}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It\u2019s worth mentioning that this lower bound has been provided in [25] by utilizing the Gordon\u2019s Escape Theorem[10]. The main difference between our work and [10] lies in that Gordon\u2019s Escape Theorem is not strong enough to provide the upper bound (sufficient condition) of pruning ratio, while the Approximate Kinematic Formula we employ does. ", "page_idx": 4}, {"type": "text", "text": "Reformulation of the Sublevel Set. Consider a well-trained deep neural network model with weights $\\mathbf{w}^{*}$ and a loss function ${\\mathcal{L}}(\\mathbf{w})$ , where w lies in a small neighborhood of $\\mathbf{w}^{*}$ . By performing a Taylor expansion of ${\\mathcal{L}}(\\mathbf{w})$ at $\\mathbf{w}^{*}$ , using the fact that the first derivative is equal to 0 and ignoring the higher order terms, the loss sublevel set $S(\\epsilon)$ can be reformulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS(\\epsilon)=\\left\\{\\hat{\\mathbf{w}}\\in\\mathbb{R}^{D}:\\frac{1}{2}\\hat{\\mathbf{w}}^{T}\\mathbf{H}\\hat{\\mathbf{w}}\\leq\\epsilon\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\textbf{w}}=\\textbf{w}-\\textbf{w}^{*}$ and $\\mathbf{H}$ denote the Hessian matrix of ${\\mathcal{L}}({\\bf w})$ w.r.t. w. Due to the positivedefiniteness of $\\mathbf{H}$ , $S(\\epsilon)$ corresponds to an ellipsoid. The related proofs can be found in Appendix D.1. ", "page_idx": 4}, {"type": "text", "text": "3.1.2 Gaussain Width of the Ellipsoid ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We leverage tools in high-dimensional probability, especially the concentration of measure, which enables us to present a rather precise expression for the Gaussian width of a high-dimensional ellipsoid. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.3 For a ellipsoid $S(\\epsilon)$ defined by : $S(\\epsilon)\\,:=\\,\\{\\mathbf{w}\\,\\in\\,\\mathbb{R}^{D}\\,:\\,\\textstyle{\\frac{1}{2}}\\mathbf{w}^{T}\\mathbf{H}\\mathbf{w}\\,\\leq\\,\\epsilon\\}$ , where $\\mathbf{H}\\in$ $\\mathbb{R}^{D\\times D}$ is a positive definite matrix, its Gaussian width is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw(S(\\epsilon))\\approx(2\\epsilon\\mathrm{Tr}(\\mathbf{H}^{-1}))^{1/2}=(\\sum_{i=1}^{D}r_{i}^{2})^{1/2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $r_{i}=\\sqrt{2\\epsilon/\\lambda_{i}}$ is the radius of ellipsoidal body and $\\lambda_{i}$ is the $i$ -th eigenvalue of $\\mathbf{H}$ . ", "page_idx": 4}, {"type": "text", "text": "The proof of Lemma 3.3 is in Appendix D.1. The Gaussian width of an ellipsoid has been provided in [19] as in the interval $\\begin{array}{r}{\\big[(\\sqrt{\\frac{2}{\\pi}}(\\sum_{i=1}^{D}r_{i}^{2})^{1/2},(\\sum_{i=1}^{D}r_{i}^{2})^{1/2}\\big]}\\end{array}$ , in contrast we sharpen the estimation of Gaussian width to a point $(\\textstyle\\sum_{i=1}^{D}r_{i}^{2})^{1/2}$ . For the settings which involve projection, the squared radius $r_{i}^{2}$ should be modified to \u2225w\u2217\u2212wrik\u222522+ri2 [19]. Therefore, the Gaussian width of projected S(\u03f5) defined in Eq.(12) equals: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw(\\mathfrak{p}(S_{\\mathbf{w}^{k}}(\\epsilon)))\\approx(\\sum_{i=1}^{D}\\frac{r_{i}^{2}}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}+r_{i}^{2}})^{1/2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.1.3 Computable Lower Bound of Pruning Ratio ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Combining Eq.(11) and Eq.(14), we obtain the following computable lower bound of the pruning ratio: ", "page_idx": 4}, {"type": "text", "text": "Corollary 3.4 Given a well-trained deep neural network with trained weight $\\mathbf{w}^{*}\\in\\mathbb{R}^{D}$ and a loss function ${\\mathcal{L}}({\\bf w})$ , for a $k$ -sparse pruned weight $\\mathbf{w}^{k}$ , the lower bound of pruning ratio of model is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{L}=1-\\frac{1}{D}\\sum_{i=1}^{D}\\frac{r_{i}^{2}}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}+r_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $r_{i}=\\sqrt{2\\epsilon/\\lambda_{i}}$ and $\\lambda_{i}$ is the eigenvalue of the Hessian matrix of $\\mathcal{L}(\\mathbf{w})\\mathbf{\\Psi}w.r.t.$ w. ", "page_idx": 4}, {"type": "text", "text": "3.1.4 Pruning Ratio vs Magnitude & Sharpness ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "It is evident from Eq.(15) that for a given trained network (whose spectrum of Hessian matrix is fixed), to minimize the lower bound of pruning ratio, we just need to minimize $\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}$ , i.e. the sum of magnitudes of the pruned parameters. Therefore, the commonly-used magnitude-based pruning algorithms get justified. Moreover, it also inspires us to employ the one-shot magnitude pruning algorithm as detailed in Section 4, whose performance proves to be better than other existing algorithms, to the best of our knowledge. ", "page_idx": 5}, {"type": "text", "text": "Besides the above-discussed magnitude of the pruned sub-vector, we also identify another important factor that determines the pruning ratio, i.e. the network sharpness, which describes the sharpness of the loss landscape around the minima, as defined by the trace of Hessian matrix, namely $\\bar{\\mathrm{Tr}}(\\mathbf{H})$ ([26] and [9], network flatness is the opposite of network sharpness; as sharpness increases, flatness decreases, and vice versa.). ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.5 (Pruning Ratio vs. Sharpness) Given a well-trained neural network $f(\\mathbf{w})$ , where w is the parameters. The lower bound of the pruning ratio and the sharpness obeys: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho_{L}\\leq1-\\frac{2\\epsilon D}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}\\mathrm{Tr}(\\mathbf{H})+2\\epsilon D}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where H is the hessian matrix of the loss function w.r.t. w. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.5 is obtained by utilizing the Cauchy\u2013Schwarz Inequality, whose proof can be found in Appendix F. It can be seen from Lemma 3.5 that the lower bound of the network pruning ratio is heavily dependent on the sharpness of the network, i.e. flatter networks imply more sparsity. This can be a valuable guideline for both training and pruning the networks. Intuitively, a flatter loss landscape is less sensitive to weight perturbations, indicating greater tolerance to weight removal. ", "page_idx": 5}, {"type": "text", "text": "3.2 Upper Bound of Pruning Ratio ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to establish the upper bound of the pruning ratio, we need to prove that there exists an $k$ -sparse weight vector intersects with the loss sub-level set. ", "page_idx": 5}, {"type": "text", "text": "For a given trained weight $\\mathbf{w}^{*}$ , we split it into two parts, i.e. the unpruned subvector, $\\mathbf{w}^{1}\\,=$ $[\\mathbf{w}_{1}^{*},\\mathbf{w}_{2}^{*},\\ldots,\\mathbf{w}_{k}^{*}]$ and the pruned one $\\Bar{\\mathbf{w}^{2}}=[\\mathbf{w}_{k+1}^{*},\\mathbf{w}_{k+2}^{*^{*}},\\dots,\\mathbf{w}_{D}^{*}]$ . By fixing $\\mathbf{w}^{1}$ , the loss sublevel set can be reformulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS(\\mathbf{w}^{'},\\epsilon)=\\{\\mathbf{w}^{'}\\in\\mathbb{R}^{D-k}:\\mathcal{L}([\\mathbf{w}^{1},\\mathbf{w}^{'}])\\leq\\mathcal{L}(\\mathbf{w}^{*})+\\epsilon\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In order to prove the existence of a $k$ -sparse weight vector $\\mathbf{w}^{k}$ , we just need to show that the all-zero vector is in $S(\\mathbf{w}^{'},\\epsilon)$ . To that end, we\u2019ll take advantage of the sufficient condition of the approximate kinematics formula (Theorem 2.4) to show that it suffices to render the statistical dimension of the projected cone of $S(\\mathbf{w}^{'},\\epsilon)$ being full, i.e. $D-k$ . Thus we can obtain the upper bound of the number of unpruned parameters, i.e. $k$ . ", "page_idx": 5}, {"type": "text", "text": "Specifically, by invoking the sufficient part of Theorem 2.4, the upper bound of the pruning ratio by a given pruning strategy is as follows: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.6 (Upper Bound of Pruning Ratio) Given a sublevel set $S(\\mathbf{w}^{'},\\epsilon)$ in $\\mathbb{R}^{D-k}$ . To ensure that the all-zero vector $\\mathbf{0}\\in\\mathbb{R}^{D-k}$ contained in $S(\\mathbf{w}^{'},\\epsilon)$ , it suffices that: ", "page_idx": 5}, {"type": "equation", "text": "$$\nw(p(S(\\mathbf{w}^{'},\\epsilon)))^{2}\\gtrsim D-k.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The Gaussian width of projected $S(\\mathbf{w}^{'},\\epsilon)$ can be easily obtained by employing Lemma 3.3, i.e. $\\begin{array}{r}{w(\\mathfrak{p}(S(\\mathbf{w}^{'},\\epsilon)))^{2}=\\underset{\\cdot}{\\sum}_{i}^{D-k}\\,\\frac{\\widetilde{r}_{i}^{2}}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}+\\widetilde{r}_{i}^{2}}}\\end{array}$ , where $\\widetilde{r}_{i}=\\sqrt{2\\epsilon/\\widetilde{\\lambda}_{i}}$ $\\widetilde{\\lambda}_{i}$ eigenvalue of the hessian matrix of $\\mathcal{L}([\\mathbf{w}^{1},\\mathbf{w}^{'}])$ w.r.t. to w and the fact that $\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}=\\|\\mathbf{w}^{2}\\|_{2}$   \nthe upper bound of the pruning ratio can be expressed as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho_{U}\\approx1-\\frac{w(\\mathbf{p}(S(\\mathbf{w}^{'},\\epsilon)))^{2}}{D}=1-\\frac{1}{D}\\sum_{i=1}^{D-k}\\frac{\\widetilde{r}_{i}^{2}}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}+\\widetilde{r}_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3 Fundamental Limit of Pruning Ratios ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As demonstrated above, the pruning ratio can be bounded as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n1-\\frac{1}{D}\\sum_{i=1}^{D}\\frac{r_{i}^{2}}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}+r_{i}^{2}}\\le\\rho\\le1-\\frac{1}{D}\\sum_{i}^{D-k}\\frac{\\widetilde{r}_{i}^{2}}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}+\\widetilde{r}_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "It is easy to notice that the upper bound and lower bound are nearly identical in the form, in fact, as we\u2019ll elaborate in the following, they are also quite close in the value. In other words, we are able to obtain a sharp characterization of the fundamental limit of pruning ratio. Meanwhile, it is worthwhile noting that the pruning limit depends on the magnitude of the final weights, which is largely determined by the weight initialization. Therefore, we still need to explore whether the magnitude of final weights are dependent on the initialization values. In the appendix, we report that once the data, network architecture, and training method are fixed, the distribution of trained network weights remains nearly insensitive to the initializations. Thus we get an affirmative answer about the above question. ", "page_idx": 6}, {"type": "text", "text": "4 Achievable Scheme & Computational Issues ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Thus far we have established the lower bound and upper bound of the pruning ratio by leveraging the Approximate Kinematic Formula in convex geometry [1]. To proceed, we will demonstrate that our obtained bounds are tight in the sense that we can devise an achievable pruning algorithm whose corresponding upper bound is quite close to the lowest possible value of the lower bound. As argued in Corollary 3.4, the magnitude pruning, which removes all the smallest $D-k$ weights, will result in the lowest pruning ratio lower bound. Inspired by this result, we\u2019ll focus on the magnitude pruning methods in order to approach the lower bound in the sequel. ", "page_idx": 6}, {"type": "text", "text": "For the lower bound part, we need to address several challenges regarding the computation of the Gaussian width of a high-dimensional deformed ellipsoid, which involves tackling the nonpositiveness of the Hessian matrix as well as the spectrum estimation of a large-scale Hessian matrix. ", "page_idx": 6}, {"type": "text", "text": "For the upper bound part, we\u2019ll focus on a relaxed version of Eq. 1 by introducing the $l_{1}$ regularization term, for the sake of computational complexity. Then we\u2019ll employ the one-shot magnitude pruning to compress the network. ", "page_idx": 6}, {"type": "text", "text": "4.1 Computational Challenges & Countermeasures ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To compute the lower bound of the pruning ratio, we need to address the following two challenges: ", "page_idx": 6}, {"type": "text", "text": "Gaussian Width of the Deformed Ellipsoid. In practice, it is usually hard for the network to converge to the exact minima, thus leading to a non-positive definite Hessian matrix. In other words, the ideal ellipsoid gets deformed due to the existence of negative eigenvalues. Determining the Gaussian width of the deformed ellipsoid is a challenging task. To address this problem, we resort to convexifying (i.e. taking the convex hull of) the deformed ellipsoid and then calculating the Gaussian width of the latter instead, by proving that the convexifying procedure has no impact on the Gaussian width. (The proof is presented in Appendix D.2). ", "page_idx": 6}, {"type": "text", "text": "Improved Spectrum Estimation. Neural networks often exhibit a quite significant number of zero-valued or vanishingly small eigenvalues in their Hessian matrices. It\u2019s hard for the spectrum estimation algorithm SLQ (Stochastic Lanczos Quadrature) proposed by [39] to obtain accurate estimation of these small eigenvalues. To address this issue, we propose to enhance the existing large-scale spectrum estimation algorithms by a key modification, i,e, to estimate the number of these exceptionally small eigenvalues by employing the Hessian matrix sampling. See Algorithm 1 for the details of the improved spectrum Estimation algorithm. A comprehensive description of the algorithm and its experimental results are presented in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4.2 Achievable Scheme: $l_{1}$ Regularization & One-shot Magnitude Pruning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Inspired by the lower bound as well as upper bound of the pruning ratio, in which the magnitude of pruning parameters plays a key role, it\u2019s sensible to focus on the magnitude-based pruning methods. ", "page_idx": 6}, {"type": "text", "text": "Input: Hermitian matrix A of size $n\\times n$ , Lanczos iterations $m$ , ESD computation iterations l, gaussian kernel $f$ and variance $\\sigma^{2}$ , sampling nums and row $l_{1}$ norm threshold $\\epsilon$ . Output: The spectral distribution of matrix A ", "page_idx": 7}, {"type": "text", "text": "for $i=2,...,l$ do ", "page_idx": 7}, {"type": "text", "text": "1). Get the tridiagonal matrix $\\mathbf{T}$ of size $m\\times m$ through Lanczos algorithm[18];   \n2). Compute $m$ eigenpairs $(\\lambda_{k}^{(i)},v_{k}^{(i)})$ from $\\mathbf{T}$ ;   \n3). $\\begin{array}{r}{\\phi_{\\sigma}^{i}(t)=\\sum_{k=1}^{m}\\tau_{k}^{(i)}f(\\lambda_{k}^{(i)};t,\\sigma)}\\end{array}$ , where $\\tau_{k}^{(i)}=(v_{k}^{(i)}[1])^{2}$ is the first component of $v_{k}^{(i)}$ ; ", "page_idx": 7}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4). Random sample $s$ rows $\\mathbf{A}_{i}$ of matrix A and calculate every $l_{1}$ norm $\\|\\mathbf{A}_{i}\\|_{1}$ by take $i\\in[1,s]$ .   \n5). Compute the number $z$ of $\\|\\mathbf{A}_{i}\\|_{1}$ which satisfy $\\lVert\\mathbf{A}_{i}\\rVert_{1}\\leq\\epsilon$ . ", "page_idx": 7}, {"type": "text", "text": "6). Compute the integration of $\\textstyle{\\frac{1}{l}}\\sum_{i=1}^{l}\\phi_{\\sigma}^{i}(t)$ , termed as $c$ , $\\begin{array}{r}{\\phi(t)=\\frac{1}{l}\\sum_{i=1}^{l}\\phi_{\\sigma}^{i}(t)+\\frac{c z}{s-z}\\delta(1^{-30}).}\\end{array}$ Return: $\\phi(t)$ ", "page_idx": 7}, {"type": "text", "text": "On the other hand, to find exact solutions of our original problem for the best pruning in Eq. 1, it is obviously very hard due to the existence of $l_{0}$ norm. To make it feasible, it\u2019s natural to perform a convex relaxation of $l_{0}$ norm, namely, by employing $l_{1}$ regularization instead. Aside from the computational advantage of this relaxation, it is worthy noting that $l_{1}$ regularization provides two extra benefits: 1) A large portion of eigenvalues of the trained Hessian matrix are zero-valued or of quite small value, which renders the calculation of the pruning limit more accurately and fast. 2) A large portion of trained weights are of quite small value, thus making the lower bound and upper bound very close. Detailed statistics about the eigenvalues and magnitudes can be found in Figure 6. ", "page_idx": 7}, {"type": "text", "text": "Specifically, by utilizing the Lagrange formulation and convex relaxation of $l_{0}$ norm, the pruning objective in Eq.1 can be reformulated as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{\\omega}}\\mathcal{L}(\\mathbf{w})+\\lambda\\|\\mathbf{w}\\|_{1}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "After training with this relaxed objective, the network weights will be pruned based on magnitudes one time, rather than in an iterative way as in [13, 6, 28]. The performance of the above described pruning scheme (termed as $\"l_{1}$ regularized one-shot magnitude pruning\" and abbreviated as \"LOMP\") can be found in Table 10 in Appendix. The above stated \"zero-dominating\" property due to $l_{1}$ regularization gets supported in Figure 2(b), where it can be seen that the majority of weights are indeed extremely small. ", "page_idx": 7}, {"type": "image", "img_path": "IAAPhOLhcX/tmp/76199fae6830a0254271c6c83eec2d6a97ce633c66cee2ec5160082c69f8af9d.jpg", "img_caption": ["Figure 2: Effect of extremely small projection distance on projection size and intersection probability and statistics of ResNet50 on TinyImagenet. Statistics regarding all experiments can be found in Appendix G. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "The above \"zero-domination\" property turns out to be of critical value for our proposed pruning scheme to nearly achieve the limit (lower bound) of the pruning ratio. Fig. 2(c) illustrates the curve $\\|\\mathbf{w}^{2}\\|_{2}^{2}$ , i.e. the $l_{2}$ norm of the $D-k$ smallest weights, w.r.t. $k/D$ . The vertical line therein represents $\\rho_{L}$ , the lower bound of the pruning ratio predicted in Section 3.1. When $k=D\\rho_{L}$ , the curve and the line will intersect as shown in Figure 2(c). Mathematically, the upper bound for the pruning ratio can be approximated as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\rho_{U}=1-\\frac{1}{D}\\sum_{i=1}^{D-k}\\frac{\\widetilde{r}_{i}^{2}}{\\|\\mathbf{w}^{2}\\|_{2}^{2}+\\widetilde{r}_{i}^{2}}\\approx1-\\frac{1}{D}\\sum_{i=1}^{D-k}\\frac{\\widetilde{r}_{i}^{2}}{\\widetilde{r}_{i}^{2}}=\\frac{k}{D}=\\rho_{L}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It can be seen from the above demonstration that the upper bound corresponding to our proposed pruning scheme almost coincides with the minimal lower bound! In other words, we have established the fundamental limit of the pruning ratio. To provide further validation of the above claim, we performed the experiments five times across eight tasks and reported the differences between the upper bound and lower bound, denoted as $\\Delta$ , in Table 9. ", "page_idx": 8}, {"type": "text", "text": "Table 1: The Difference Between Lower Bound and Upper Bound of Pruning Ratio. ", "page_idx": 8}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/242705dcf1468e01d0c75499398bef3fea51d922ed3e5f13342b9f48f9cba105.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we validate our pruning method as well as the theoretical limit of the pruning ratio by experiments. ", "page_idx": 8}, {"type": "text", "text": "Tasks. We evaluate the pruning ratio threshold on: Full-Connect-5(FC5), Full-Connect-12(FC12), AlexNet [17] and VGG16 [27] on CIFAR10 [16], ResNet18 and ResNet50 [14] on CIFAR100 and TinyImageNet [20]. We employ the $l_{1}$ regularization during training, and execute a one-shot magnitude-based pruning and assess its performance with various sparsity ratios, in terms of the metrics of accuracy and loss. Detailed descriptions of datasets, networks, hyper-parameters, and eigenspectrum adjustment can be found in Section B of the Appendix. Moreover, the performance comparison between $l_{1}$ -regularization-based magnitude pruning and other pruning methods can be found in Table 10 in Appendix. ", "page_idx": 8}, {"type": "text", "text": "5.1 Validation of Pruning Lower Bound ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "After training with the $l_{1}$ regularization, we compute the eigenvalues and present the theoretical limit of pruning ratio. By pruning the trained network to various sparsity levels, we depict in Figure 3 both the line of theoretical lower bound and the sparsity-accuracy curve for the above-listed tasks. From the figures we can see that our theoretical result matches the numerical pruning ratio quite well. ", "page_idx": 8}, {"type": "image", "img_path": "IAAPhOLhcX/tmp/1293e54d65e1fd642a28ee001b9a80a95c82f05f67764490ee4561d95e717398.jpg", "img_caption": ["Figure 3: The impact of sparsity on loss and test accuracy are obtained on the test dataset, and we mark the theoretical pruning ratio limit with vertical lines. The loss values have been normalized and translated. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Prediction Performance ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present a more detailed comparison between our theoretical limit of pruning ratio, and the actual values by experiments in Table 2, which shows nearly perfect agreement between them. The difference between the theoretical value and the actual value is donated as $\\Delta$ . ", "page_idx": 8}, {"type": "text", "text": "6 Interpretation of Pruning Heuristics ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Equipped with the fundamental limit of network pruning, we\u2019re now able to provide rigorous interpretations of several heuristics employed by existing pruning algorithms. ", "page_idx": 8}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/9a1bf3ac959ba922f083396607b6e2e486385421f331f0c55791c7a56a01ee2b.jpg", "table_caption": ["Table 2: Comparison between Theoretical and Actual Values of Pruning Ratio "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "IAAPhOLhcX/tmp/6105d22151d05a44fc727626d90f8cf6dd3d23f017260711f18da25b08a59f1d.jpg", "img_caption": ["Figure 4: Top Row: From left to right, as the number of iterations increases, it leads to an increase in the theoretical pruning ratio threshold. The horizontal line represents the last pruning ratio. Bottom Row: The comparison of the pruning ratio threshold between using and not using $l_{2}$ -regularization. Sparse networks are obtained by magnitude-based pruning with fixed pruning ratios. The two plots on the left and the two plots on the right correspond to different fixed pruning ratios. Here, $\\mathbf{\\dot{\\boldsymbol{R}}}=\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}$ , which is the projection distance. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Pruning ratio adjustment is needed in IMP. For the IMP (Iterative Magnitude Pruning) algorithm [7], we determine the pruning ratio thresholds for various stages through calculations, as depicted in the top row of Figure 4. It is noteworthy that as the pruning depth gradually increases, the theoretical pruning ratio threshold also increases. Therefore, it is appropriate to prune smaller proportions of weights gradually during iterative pruning, Both [43] and [28] have employed pruning rate adjustment, which gradually prunes smaller proportions of the weights with the iteration of the algorithm. ", "page_idx": 9}, {"type": "text", "text": "$l_{2}$ -regularization enhances the performance of Rare Gems. For the Rare Gems algorithm [28], it is shown that $l_{2}$ regularization makes a significant difference in terms of the final performance, as shown in the bottom row of Figure 4. The main reason behind this phenomenon is: when $l_{2}$ -regularization is applied, the pruning ratio tends to be larger than the theoretical limit, however, the absence of $l_{2}$ -regularization would result in excessive pruning, which can be regarded as wrong pruning. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we explore the fundamental limit of pruning ratio of deep networks by taking the first principles approach and leveraging the framework of convex geometry, thus we can reduce the pruning limit problem to the sets intersection problem and by taking advantage of the powerful Approximate Kinematic Formula, we can sharply characterize the fundamental limit of network pruning ratio. This fundamental limit convey a key message as follows: the network pruning limit is mainly determined by the weight magnitude and the network sharpness. These two guidelines can provide intuitive explanations of several heuristics in existing pruning algorithms. Moreover, to address the challenges in computing the involved Gaussian width, we develop an improved spectrum estimation for large-scale and non-positive Hessian matrices. Experiments demonstrate the almost perfect agreement between our theoretical results and the experimental ones. ", "page_idx": 9}, {"type": "text", "text": "Limitations. We demonstrate that when the magnitude of the removed weights is small, the upper bound of the pruning ratio nearly coincides with the lower bound. However, not all training strategies result in small removed weights, and in such cases, the upper and lower bounds do not coincide. Due to the intrinsic challenge of empirical risk minimization (ERM), specifically, overftiting, some form of regularization is necessary. As a result, it becomes difficult to examine the pruning limits of the network in its purest form(without regularization). To make progress, we turn to the most direct form of regularization, aiming to find the sparsest solution, namely $l_{0}$ regularization and its convex relaxation, $l_{1}$ regularization. The remaining question is under what conditions $l_{1}$ regularization will yield the same pruning limits as $l_{0}$ regularization and is still under investigation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the National Key Research and Development Program of China under Grant 2024YFE0103800, in part by the National Natural Science Foundation of China under Grant 62071192. We thank the anonymous reviewers for their valuable and constructive feedback that helped in shaping the final manuscript. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dennis Amelunxen, Martin Lotz, Michael B McCoy, and Joel A Tropp. Living on the edge: Phase transitions in convex programs with random data. Information and Inference: A Journal of the IMA, 3(3):224\u2013294, 2014.   \n[2] Haim Avron and Sivan Toledo. Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix. Journal of the ACM (JACM), 58(2):1\u201334, 2011.   \n[3] Zhaojun Bai, Gark Fahey, and Gene Golub. Some large-scale matrix computation problems. Journal of Computational and Applied Mathematics, 74(1-2):71\u201389, 1996.   \n[4] Riade Benbaki, Wenyu Chen, Xiang Meng, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, and Rahul Mazumder. Fast as chita: Neural network pruning with combinatorial optimization. In International Conference on Machine Learning, pages 2031\u20132049. PMLR, 2023. [5] Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex geometry of linear inverse problems. Foundations of Computational mathematics, 12:805\u2013849, 2012.   \n[6] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.   \n[7] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pages 3259\u20133269. PMLR, 2020. [8] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Pruning neural networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576, 2020.   \n[9] Khashayar Gatmiry, Zhiyuan Li, Ching-Yao Chuang, Sashank Reddi, Tengyu Ma, and Stefanie Jegelka. The inductive bias of flatness regularization for deep matrix factorization. arXiv preprint arXiv:2306.13239, 2023.   \n[10] Yehoram Gordon. On milman\u2019s inequality and random subspaces which escape through a mesh in $\\mathbb{R}^{n}$ . In Geometric Aspects of Functional Analysis: Israel Seminar (GAFA) 1986\u201387, pages 84\u2013106. Springer, 1988.   \n[11] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. Advances in neural information processing systems, 29, 2016.   \n[12] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.   \n[13] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.   \n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[15] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4340\u20134349, 2019.   \n[16] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master\u2019s thesis, Department of Computer Science, University of Toronto, 2009.   \n[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84\u201390, 2017.   \n[18] Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. J. Res. Natl. Bur. Stand. B, 45:255\u2013282, 1950.   \n[19] Brett W Larsen, Stanislav Fort, Nic Becker, and Surya Ganguli. How many degrees of freedom do we need to train deep networks: a loss landscape perspective. arXiv preprint arXiv:2107.05802, 2021.   \n[20] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[21] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989.   \n[22] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.   \n[23] Jian-Hao Luo, Hao Zhang, Hong-Yu Zhou, Chen-Wei Xie, Jianxin Wu, and Weiyao Lin. Thinet: pruning cnn fliters for a thinner net. IEEE transactions on pattern analysis and machine intelligence, 41(10):2525\u20132538, 2018.   \n[24] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.   \n[25] Mansheej Paul, Feng Chen, Brett W Larsen, Jonathan Frankle, Surya Ganguli, and Gintare Karolina Dziugaite. Unmasking the lottery ticket hypothesis: What\u2019s encoded in a winning ticket\u2019s mask? arXiv preprint arXiv:2210.03044, 2022.   \n[26] Henning Petzka, Michael Kamp, Linara Adilova, Cristian Sminchisescu, and Mario Boley. Relative flatness and generalization. Advances in neural information processing systems, 34:18420\u2013 18432, 2021.   \n[27] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[28] Kartik Sreenivasan, Jy-yong Sohn, Liu Yang, Matthew Grinde, Alliot Nagle, Hongyi Wang, Eric Xing, Kangwook Lee, and Dimitris Papailiopoulos. Rare gems: Finding lottery tickets at initialization. Advances in Neural Information Processing Systems, 35:14529\u201314540, 2022.   \n[29] Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. Advances in neural information processing systems, 33:20390\u201320401, 2020.   \n[30] M. Talagrand. New concentration inequalities for product spaces. Inventionnes Math., 126:505\u2013 563, 1996.   \n[31] M. Talagrand. A New Look at Independence. Ann. Probab., 24:1\u201334, 1996.   \n[32] Michel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces. Publications Math\u00e9matiques de l\u2019Institut des Hautes Etudes Scientifiques, 81:73\u2013205, 1995.   \n[33] Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.   \n[34] Roman Vershynin. Estimation in high dimensions: a geometric perspective. In Sampling Theory, a Renaissance: Compressive Sensing and Other Developments, pages 3\u201366. Springer, 2015.   \n[35] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 409\u2013424, 2018.   \n[36] Qian Xiang, Xiaodan Wang, Yafei Song, Lei Lei, Rui Li, and Jie Lai. One-dimensional convolutional neural networks for high-resolution range profile recognition via adaptively feature recalibrating and automatically channel pruning. International Journal of Intelligent Systems, 36(1):332\u2013361, 2021.   \n[37] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5687\u20135695, 2017.   \n[38] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings of the European conference on computer vision (ECCV), pages 285\u2013300, 2018.   \n[39] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE international conference on big data (Big data), pages 581\u2013590. IEEE, 2020.   \n[40] Lei You and Hei Victor Cheng. Swap: Sparse entropic wasserstein regression for robust network pruning. In The Twelfth International Conference on Learning Representations, 2024.   \n[41] Xin Yu, Thiago Serra, Srikumar Ramalingam, and Shandian Zhe. The combinatorial brain surgeon: pruning weights that cancel one another in neural networks. In International Conference on Machine Learning, pages 25668\u201325683. PMLR, 2022.   \n[42] Hao Zhou, Jose M Alvarez, and Fatih Porikli. Less is more: Towards compact cnns. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 662\u2013677. Springer, 2016.   \n[43] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017.   \n[44] Liu Ziyin and Zihao Wang. spred: Solving l1 penalty with sgd. In International Conference on Machine Learning, pages 43407\u201343422. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Organization of Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendix is organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Sec. A: an overview of the organization of the appendix.   \n\u2022 Sec. B: detail descriptions of the datasets, models, hyper-parameter choices used in our experiments. Additionally, figures illustrating the theoretical pruning threshold are included.   \n\u2022 Sec. C: this section delves into the practical calculation of the Gaussian Width. It addresses the challenges associated with the non-positiveness of the Hessian matrix and the spectrum estimation of a large-scale Hessian matrix. Experimental results highlighting the \"important eigenvalues\" are also showcased.   \n\u2022 Sec. D: a comprehensive proof of the Gaussian Width for both the ellipsoid and the deformed ellipsoid is provided.   \n\u2022 Sec. E: this section presents the proof of the \"sub-sublevel set\" utilized in deriving the upper bound of the pruning ratio. Furthermore, a straightforward explanation of the relationship between the general lower bound and the upper bound is offered.   \n\u2022 Sec. F: omitted proof of the connection between the sharpness and the lower bound of the pruning ratio is detailed in this section.   \n\u2022 Sec. G: performance comparison between the $l_{1}$ regularized one-shot magnitude pruning, termed as \"LOMP\", and several prominent pruning strategies. Results from a hypothetical experiment verifying the importance of magnitude in pruning and comprehensive statistical results from Sec. 4.2 are also included.   \n\u2022 Sec. H: limitations of our assumptions and theoretical results.   \n\u2022 Sec. I: broader impacts statement of this research. ", "page_idx": 13}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we describe the datasets, models, hyper-parameter choices and eigenspectrum adjustment used in our experiments. All of our experiments are run using PyTorch 1.12.1 on Nvidia RTX3090s with ubuntu20.04-cuda11.3.1-cudnn8 docker. ", "page_idx": 14}, {"type": "text", "text": "B.1 Dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "CIFAR-10. CIFAR-10 consists of 60,000 color images, with each image belonging to one of ten different classes with size $32\\times32$ . The classes include common objects such as airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The CIFAR-10 dataset is divided into two subsets: a training set and a test set. The training set contains 50,000 images, while the test set contains 10,000 images [16]. For data processing, we follow the standard augmentation: normalize channel-wise, randomly horizontally flip, and random cropping. ", "page_idx": 14}, {"type": "text", "text": "CIFAR-100. The CIFAR-100 dataset consists of 60,000 color images, with each image belonging to one of 100 different fine-grained classes [16]. These classes are organized into 20 superclasses, each containing 5 fine-grained classes. Similar to CIFAR-10, the CIFAR-100 dataset is split into a training set and a test set. The training set contains 50,000 images, and the test set contains 10,000 images. Each image is of size $32\\mathtt{x}32$ pixels and is labeled with its corresponding fine-grained class. Augmentation includes normalize channel-wise, randomly horizontally flip, and random cropping. ", "page_idx": 14}, {"type": "text", "text": "TinyImageNet. TinyImageNet comprises 100,000 images distributed across 200 classes, with each class consisting of 500 images [20]. These images have been resized to $64\\times64$ pixels and are in full color. Each class encompasses 500 training images, 50 validation images, and 50 test images. Data augmentation techniques encompass normalization, random rotation, and random filpping. The dataset includes distinct train, validation, and test sets for experimentation. ", "page_idx": 14}, {"type": "text", "text": "B.2 Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In all experiments, pruning skips bias and batchnorm, which have little effect on the sparsity of the network. Non-affine batchnorm is applied in the network, and the initialization of the network is kaiming normal initialization. ", "page_idx": 14}, {"type": "text", "text": "Full Connect Network(FC-5, FC-12). We train a five-layer fully connected network (FC-5) and a twelve-layer fully connected network FC-12 on CIFAR-10, the network architecture details can be found in Table 3. ", "page_idx": 14}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/8e73bdca13b4823251d30978cd0d668b8a24db760aac4aa7822432c98784b36d.jpg", "table_caption": ["Table 3: FC-5 and FC-12 architecture used in our experiments. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "AlexNet [17]. We use the standard AlexNet architecture. In order to use CIFAR-10 to train AlexNet, we upsample each picture of CIFAR-10 to $3\\!\\times\\!224\\!\\times\\!224$ . The detailed network architecture parameters are shown in Table 4. ", "page_idx": 14}, {"type": "text", "text": "VGG-16 [27]. In the original VGG-16 network, there are 13 convolution layers and $3\\,\\,{\\mathrm{FC}}$ layers (including the last linear classification layer). We follow the VGG-16 architectures used in [7, 8] to remove the first two FC layers while keeping the last linear classification layer. This finally leads to a 14-layer architecture, but we still call it VGG-16 as it is modified from the original VGG-16 architectural design. Detailed architecture is shown in Table 5. VGG-16 is trained on CIFAR-10. ", "page_idx": 14}, {"type": "text", "text": "ResNet-18 and ResNet-50 [14]. We use the standard ResNet architecture for TinyImageNet and tune it for the CIFAR-100 dataset. The detailed network architecture parameters are shown in Table 6. ResNet-18 and ResNet-50 is trained on CIFAR-100 and TinyImageNet. ", "page_idx": 14}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/f226d5dcd0f7749a0501d9010f69c6d16c042f2c32ed0ac108a3124c3a7b4b16.jpg", "table_caption": ["Table 4: AlexNet architecture used in our experiments. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/7bb8885a542ea948444d032fc65150632ac3967bee0073253df42c8741003c09.jpg", "table_caption": ["Table 5: VGG-16 architecture used in our experiments. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/7aa217d140fee130fa56f59544b80f96c3e8f0aad2175db4aab7bea3eeffcea9.jpg", "table_caption": ["Table 6: ResNet architecture used in our experiments. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.3 Training Hyper-parameters Setup ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we will describe in detail the training hyper-parameters of the Global One-shot Pruning algorithm on multiple datasets and models. The various hyperparameters are detailed in Table 7. ", "page_idx": 16}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/5868c688163e93f3d9355c6e873db03da10a87fcc21e15c2b32f2eb5d76f6a9e.jpg", "table_caption": ["Table 7: Hyper Parameters used for different Datasets and Models. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.4 Sublevel Set Parameters Setup.", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given a dense well-trained neural network with weighted donated as $\\mathbf{w}^{*}$ , the loss sublevel set is defined as $\\left\\{\\hat{\\mathbf{w}}\\in\\mathbb{R}^{D}:\\frac{1}{2}\\hat{\\mathbf{w}}^{T}\\mathbf{H}\\hat{\\mathbf{w}}\\,\\le\\,\\epsilon\\right\\}$ where $\\hat{\\textbf{w}}\\mathbf{w}-\\mathbf{w}^{*}$ , under the assumption that the test data is often unavailable and we also generally assume that the training and test data share the same distribution, thus we use the training data to define the loss sublevel set. We compute the standard deviation of the network\u2019s loss across multiple batches on the training data set and denote it by $\\epsilon$ . ", "page_idx": 16}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/8770d50a292a88096b4e177eedaf9ca3f87eca2c3ab0e76467e7c5f8eeebdb75.jpg", "table_caption": ["Table 8: Hyper Parameters used in SLQ Algorithm. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.5 Theoretical Pruning Ratio ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Taking $\\mathbf{w}^{*}$ as the initial pruning point and calculating the corresponding value of $R=\\|\\mathbf{w}^{k}-\\mathbf{w}^{*}\\|_{2}$ for different pruning ratios $k/D$ . We then plot the corresponding curve of the theoretically predicted pruning ratio and the calculated $R$ in the same graph. The intersection point of these two curves is taken as the lower bound of the theoretically predicted pruning ratio. All results are shown in Figure 5. ", "page_idx": 16}, {"type": "image", "img_path": "IAAPhOLhcX/tmp/032c9aa48bdcd7c3b7a87d75950b1e2bf3e38c93a71526ea54d2fe8ba4b52389.jpg", "img_caption": ["Figure 5: The theoretically predicted pruning ratio in eight tasks. The first row, from left to right, corresponds to FC5, FC12, AlexNet, and VGG16 on CIFAR10. The second row, from left to right, corresponds to ResNet18 and ResNet50 on CIFAR100, as well as ResNet18 and ResNet50 on TinyImagenet. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Practical Calculation of Gaussian Width ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In practical experiments, determining the Gaussian width of the ellipsoid defined by the network loss function is a challenging task. There are two primary challenges encountered in this section: 1.) the computation of eigenvalues for high-dimensional matrices poses significant difficulty; 2.) the network fails to converge perfectly to the extremum, leading to a non-positive definite Hessian matrix for the loss function. In this section, we tackle these challenges through the utilization of a fast eigenspectrum estimation algorithm and an algorithm that approximates the Gaussian width of a deformed ellipsoid body. These approaches effectively address the aforementioned problems. ", "page_idx": 17}, {"type": "text", "text": "C.1 Improved SLQ (Stochastic Lanczos Quadrature) Spectrum Estimation ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "IAAPhOLhcX/tmp/f9d33089e1b46858d3ba640d3bcf66d34b0df4b3bc55f33f405a2229116bd65f.jpg", "img_caption": ["Figure 6: The statistical analysis of the L1 norm of the Hessian matrix in eight tasks. The first row, from left to right, corresponds to FC5, FC12, AlexNet, and VGG16. The second row, from left to right, corresponds to ResNet18 and ResNet50 on CIFAR100, as well as ResNet18 and ResNet50 on TinyImagenet. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Calculating the eigenvalues of large matrices has long been a challenging problem in numerical analysis. One widely used method for efficiently computing these eigenvalues is the Lanczos algorithm[18], which is presented in Algorithm 2. However, due to the huge amount of parameters of the deep neural network, it is still impractical to use this method to calculate the eigenspectrum of the Hessian matrix of a deep neural network. To tackle this problem, [39] proposed SLQ (Stochastic Lanczos Quadrature) Spectrum Estimation Algorithm, which estimates the overall eigenspectrum distribution based on a small number of eigenvalues obtained by Lanczos algorithm. This method enables the efficient computation of the full eigenvalues of large matrices. Algorithm 2 outlines the step-by-step procedure for the classic Lanczos algorithm, providing a comprehensive guide for its implementation. The algorithm requires the selection of the number of iterations, denoted as $m$ , which determines the size of the resulting triangular matrix T. ", "page_idx": 17}, {"type": "text", "text": "In general, the Lanczos algorithm is not capable of accurately computing zero eigenvalues, and this limitation becomes more pronounced when the SLQ algorithm has a small number of iterations. Similarly, vanishingly small eigenvalues are also ignored by Lanczos. However, in a well-trained large-scale deep neural network, the experiment found that the network loss function hessian matrix has a large number of zero eigenvalues and vanishingly small eigenvalues. In the Gaussian width of the ellipsoid, the zero eigenvalues and vanishingly small eigenvalues have the same effect on the width (insensitive to other parameters), and we collectively refer to these eigenvalues as the \u201cimportant\u201d eigenvalues. We divide the weight into 100 parts from small to large, calculate the second-order derivative (including partial derivative) of smallest weight in each part, and sum the absolute values of all second-order derivatives of the weight, which corresponds to $l_{1}$ -norm of a row in hessian matrix, and the row $l_{1}$ -norm is zero or a vanishingly small corresponds to an \u201cimportant\u201d eigenvalue, the experimental results can be seen in the Figure 6, from which the number of missing eigenvalues of the SLQ algorithm can be estimated, we then add the same number of 1e-30 as the missing eigenvalues in the Hessian matrix eigenspectrum. All the SLQ algorithm parameters are discribed in Table 8 and the statistical analysis of the $l_{1}$ norm of Hessian matrix rows for all experiments is presented in Figure 6. For details of the SLQ algorithm and the improved SLQ algorithm, see Algorithm 3 and Algorithm 1 ", "page_idx": 17}, {"type": "text", "text": "Input: a Hermitian matrix A of size $n\\times n$ , a number of iterations $m$   \nOutput: a tridiagonal real symmetric matrix $\\mathbf{T}$ of size $m\\times m$   \ninitialization:   \n1. Draw a random vector $\\mathbf{v_{1}}$ of size $n\\times1$ from $\\mathcal{N}(0,1)$ and normalize it;   \n2. $\\mathbf{w_{1}^{'}}=\\mathbf{Av_{1}}$ ; $\\alpha_{1}=<\\mathbf{w_{1}^{'},v_{1}}>$ ; $\\mathbf{w_{1}}=\\mathbf{w_{1}^{'}}-\\alpha_{1}\\mathbf{v_{1}}$ ;   \n3.   \nfor $j=2,...,m$ do 1). $\\beta_{j}=\\|\\mathbf{w_{j-1}}\\|$ ; 2). if $\\beta_{j}=0$ then stop else $\\mathbf{v_{j}}=\\mathbf{w_{j-1}}/\\beta_{j}$ end if 3). $\\mathbf{w_{j}^{'}}=\\mathbf{Av_{j}}$ ; 4). $\\alpha_{j}=<\\mathbf{w_{j}^{'}},\\mathbf{v_{j}}>$ ; 5). $\\mathbf{w_{j}}=\\mathbf{w_{j}^{'}}-\\alpha_{j}\\mathbf{v_{j}}-\\beta_{j}\\mathbf{v_{j-1}}$ ;   \nend for   \n4. ${\\bf T}(i,i)=\\alpha_{i},\\;i=1,\\ldots,m$ ; ${\\bf T}(i,i+1)={\\bf T}(i+1,i)=\\beta_{i},\\;i=1,\\ldots,m-1.$ .   \nReturn: T ", "page_idx": 18}, {"type": "text", "text": "Algorithm 3 SLQ(Stochastic Lanczos Quadrature) Spectrum Estimation Algorithm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Input: A hermitian matrix A of size $n\\times n$ , Lanczos iterations $m$ , ESD computation iterations $\\bar{\\boldsymbol{l}}$   \ngaussian kernel $f$ and variance $\\sigma^{2}$ .   \nOutput: The spectral distribution of matrix A   \nfor $i=2,...,l$ do 1). Get the tridiagonal matrix $\\mathbf{T}$ of size $m\\times m$ through Lanczos algorithm; 2). Compute $m$ eigenpairs $(\\lambda_{k}^{(i)},v_{k}^{(i)})$ from $\\mathbf{T}$ ; 3). $\\begin{array}{r}{\\phi_{\\sigma}^{i}(t)=\\sum_{k=1}^{m}\\tau_{k}^{(i)}f(\\lambda_{k}^{(i)};t,\\sigma)}\\end{array}$ , where $\\tau_{k}^{(i)}=(v_{k}^{(i)}[1])^{2}$ is the first component of $v_{k}^{(i)}$ .   \nend for   \n4). $\\begin{array}{r}{\\phi(t)=\\frac{1}{l}\\sum_{i=1}^{l}\\phi_{\\sigma}^{i}(t)}\\end{array}$   \nRe ", "page_idx": 18}, {"type": "text", "text": "C.2 Gaussian Width of the Deformed Ellipsoid ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "After effective training, it is generally assumed that a deep neural network will converge to the global minimum of its loss function. However, in practice, even after meticulous tuning, the network tends to oscillate around the minimum instead of converging to it. This leads to that the Hessian matrix of the loss function would be non-positive definite, and the resulting geometric body defined by this matrix would change from an expected ellipsoid to a hyperboloid, which is unfortunately nonconvex. To quantify the Gaussian width of the ellipsoid corresponding to the perfect minima, we propose to approximate it by convexifying the deformed ellipsoid through replacing the associated negative eigenvalues with its absolute value. This processing turns out to be very effective, as demonstrated by the experimental results. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.1 Consider a well-trained neural network with weights w, whose loss function defined by w has a Hessian matrix H. Due to the non-positive definiteness of $\\mathbf{H}$ , there exist negative eigenvalues. Let the eigenvalue decomposition of $\\mathbf{H}$ be $\\dot{\\mathbf H}=\\mathbf v^{T}\\dot{\\boldsymbol\\Sigma}\\mathbf v,$ , where $\\Sigma$ is a diagonal matrix of eigenvalues. Let $\\mathbf{D}=\\mathbf{\\check{v}}^{T}|\\mathbf{\\Sigma}\\Sigma|\\mathbf{v}$ , where $|\\cdot|$ means absolute operation. the geometric objects defined by $H$ and $D$ are $S(\\epsilon):=\\left\\{\\mathbf{w}\\in\\mathbb{R}^{D}:\\frac{1}{2}\\mathbf{w}^{T}\\mathbf{H}\\mathbf{w}\\leq\\epsilon\\right\\}$ and $\\hat{S}(\\epsilon):=\\{\\mathbf{w}\\in\\mathbb{R}^{D}:\\frac{1}{2}\\mathbf{w}^{T}\\mathbf{D}\\mathbf{w}\\leq\\epsilon\\}$ , then: ", "page_idx": 18}, {"type": "equation", "text": "$$\nw(S(\\epsilon))\\approx w(\\hat{S}(\\epsilon))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The proof of Lemma C.1 is in Appendix D.2. Lemma C.1 indicates that if the deep neural network converges to a vicinity of the global minimum of the loss function, the Gaussian width of the deformed ellipsoid body can be approximated by taking the convex hull of $S(\\epsilon)$ . ", "page_idx": 19}, {"type": "text", "text": "D Gaussian Width of the Sublevel Set ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide detailed proofs regarding the Gaussian Width of the sublevel sets of quadratic wells. ", "page_idx": 20}, {"type": "text", "text": "D.1 Gaussian Width of the Quadratic Well ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Gaussian width is an extremely useful tool to measure the complexity of a convex body. In our proof, we will use the following expression for its definition: ", "page_idx": 20}, {"type": "equation", "text": "$$\nw(S)=\\frac{1}{2}{\\mathbb E}\\operatorname*{sup}_{\\mathbf x,\\mathbf y\\in S}\\left\\langle\\mathbf g,\\mathbf x-\\mathbf y\\right\\rangle,\\mathbf g\\sim\\mathcal N(\\mathbf0,\\mathbf I_{D\\times D})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Concentration of measure is a universal phenomenon in high-dimensional probability. Basically, it says that a random variable which depends in a smooth way on many independent random variables (but not too much on any of them) is essentially constant.[30, 31, 32] ", "page_idx": 20}, {"type": "text", "text": "Theorem D.1 (Gaussian concentration) Consider a random vector $\\mathbf{x}\\,\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{n\\times n})$ and an $L$ - Lipschitz function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ (with respect to the Euclidean metric). Then for $t\\geq0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(|f(\\mathbf{x})-\\mathbb{E}f(\\mathbf{x})|\\ge t)\\le\\epsilon,\\quad\\epsilon=e^{-\\frac{t^{2}}{2L^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, if $\\epsilon$ is small, $f(\\mathbf{x})$ can be approximated as $f(\\mathbf{x})\\approx\\mathbb{E}f(\\mathbf{x})+{\\sqrt{-2L^{2}\\mathrm{ln}\\epsilon}}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma D.2 Given a random vector ${\\mathbf x}\\sim\\mathcal{N}(\\mathbf{0},{\\mathbf I}_{n\\times n})$ and the inverse of a positive definite Hessian matrix $\\mathbf{Q}=\\mathbf{H}^{-1}$ , where $\\mathbf{H}\\in\\mathbb{R}^{n\\times n}$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\sqrt{{\\bf x^{T}Q x}}\\approx\\sqrt{\\mathbb{E}{\\bf x^{T}Q x}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. 1.) Concentration of $\\mathbf{x}^{\\mathbf{T}}\\mathbf{Qx}$ ", "page_idx": 20}, {"type": "text", "text": "Define $f(\\mathbf{x})=\\mathbf{x}^{\\mathbf{T}}\\mathbf{Q}\\mathbf{x}$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{f(\\mathbf{x})=\\mathbf{x}^{\\mathbf{T}}\\mathbf{Q}\\mathbf{x}}}\\\\ &{}&{=\\mathbf{x}^{\\mathbf{T}}\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{U}^{\\mathbf{T}}\\mathbf{x}}\\\\ &{}&{=\\sum_{i=1}^{n}\\lambda_{i}x_{i}^{2}\\quad\\mathrm{w.~p.~almost~1.~}}\\end{array}\\qquad\\qquad\\mathrm{Eigenvalue~Decomposition~of~\\mathbf{Q}:~\\mathbf{Q}=U\\boldsymbol{\\Sigma}\\mathbf{U}^{\\mathbf{T}}\\mathbf{x}}}\\\\ &{}&{=\\sum_{i=1}^{n}\\lambda_{i}x_{i}^{2}\\quad\\mathrm{w.~p.~almost~1.~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\lambda_{i}$ is the eigenvalue of $\\mathbf{Q}$ . The lipschitz constant $L_{f}$ of function $f(\\mathbf{x})$ is : ", "page_idx": 20}, {"type": "equation", "text": "$$\nL_{f}=\\operatorname*{max}(|\\frac{\\partial f}{\\partial\\mathbf{x}}|)=\\operatorname*{max}(|2\\lambda_{i}x_{i}|)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $g(x_{i})=2\\lambda_{i}x_{i}$ , whose lipschitz constant is $L_{g}=|2\\lambda_{i}|$ . Invoking Theorem D.1, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(x_{i})\\approx\\mathbb{E}g(x_{i})+\\sqrt{-2(2\\lambda_{i})^{2}\\mathrm{ln}\\epsilon_{1}}}\\\\ &{\\qquad\\quad=\\sqrt{-8\\lambda_{i}^{2}\\mathrm{ln}\\epsilon_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, the lipschitz constant of $f(\\mathbf{x})$ can be approximated by: ", "page_idx": 20}, {"type": "equation", "text": "$$\nL_{f}=m a x(\\sqrt{-8\\lambda_{i}^{2}\\mathrm{ln}\\epsilon_{1}})=\\sqrt{-8\\mathrm{ln}\\epsilon_{1}}\\lambda_{m a x}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Invoking Theorem D.1 again, we establish the concentration of $f(\\mathbf{x})$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{f(\\mathbf{x})\\approx\\mathbb{E}f(\\mathbf{x})+\\sqrt{-2(L_{f})^{2}\\mathrm{ln}\\epsilon_{2}}}&{{}}&{}\\\\ {=\\mathbb{E}f(\\mathbf{x})+4\\sqrt{\\mathrm{ln}\\epsilon_{1}\\mathrm{ln}\\epsilon_{2}}\\lambda_{m a x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\sqrt{f(\\mathbf{x})}\\approx\\mathbb{E}\\sqrt{\\mathbb{E}f(\\mathbf{x})+4\\sqrt{\\mathrm{ln}\\epsilon_{1}\\mathrm{ln}\\epsilon_{2}}}\\lambda_{m a x}}\\\\ {\\approx\\sqrt{\\mathbb{E}f(\\mathbf{x})}+\\frac{2\\sqrt{\\mathrm{ln}\\epsilon_{1}\\mathrm{ln}\\epsilon_{2}}\\lambda_{m a x}}{\\sqrt{\\mathbb{E}f(\\mathbf{x})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, the Jensen ratio of $\\sqrt{f(\\mathbf{x})}$ can be approximated by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}\\sqrt{f(\\mathbf{x})}}{\\sqrt{\\mathbb{E}f(\\mathbf{x})}}\\approx1+2\\sqrt{\\mathrm{ln}\\epsilon_{1}\\mathrm{ln}\\epsilon_{2}}\\frac{\\lambda_{m a x}}{\\sum_{i=1}^{n}\\lambda_{i}}}\\\\ &{\\qquad\\qquad=1+\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If $\\mathbf{Q}$ is a Wishart matrix, i.e. ${\\mathbf{Q}}\\;=\\;{\\mathbf{A}}^{T}{\\mathbf{A}}$ , where $\\mathbf{A}$ is a random matrix whose elements are independently and identically distributed with unit variance, according to the Marchenko-Pastur law [33], the maximum eigenvalue of $\\mathbf{Q}$ is approximately $4n$ and the trace of $\\mathbf{Q}$ is approximately $n^{2}$ . Therefore, the above Jensen ratio approaches to 1 with decaying rate $\\textstyle{\\mathcal{O}}({\\frac{1}{n}})$ . ", "page_idx": 21}, {"type": "text", "text": "For the inverse of a positive definite Hessian matrix which is of our concern, we take $\\epsilon_{1}=\\epsilon_{2}=10^{-4}$ , numerical simulations show that when the dimension $n=10^{5}$ , the corresponding $\\delta$ in the above Jensen ratio is on the order of $10^{-3}$ , which is in good agreement with the theoretical value and is arguably negligible. Similar as the case of the above-discussed Wishart matrix, when the dimension $n$ increases, the value of $\\delta$ will further decrease. ", "page_idx": 21}, {"type": "text", "text": "Consequently, we can conclude that $\\mathbb{E}\\sqrt{f(\\mathbf{x})}\\approx\\sqrt{\\mathbb{E}f(\\mathbf{x})}$ , i.e. $\\mathbb{E}\\sqrt{\\mathbf{x}^{\\mathrm{T}}\\mathbf{Q}\\mathbf{x}}\\approx\\sqrt{\\mathbb{E}\\mathbf{x}^{\\mathrm{T}}\\mathbf{Q}\\mathbf{x}}.$ ", "page_idx": 21}, {"type": "text", "text": "Definition D.3 (Definition of ball) $A$ (closed) ball $B(c,r)\\,(i n\\,\\mathbb{R}^{D})$ centered at $c\\in\\mathbb{R}^{D}$ with radius $r$ is the set ", "page_idx": 21}, {"type": "equation", "text": "$$\nB(c,r):=\\{\\mathbf{x}\\in\\mathbb{R}^{D}:\\mathbf{x}^{T}\\mathbf{x}\\leq r^{2}\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The set $B(0,1)$ is called the unit ball. An ellipsoid is just an affine transformation of a ball. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.4 (Definition of ellipsoid) . An ellipsoid $S$ centered at the origin is the image $L(B(0,1))$ of the unit ball under an invertible linear transformation $L\\,:\\,\\mathbb{R}^{\\check{D}}\\,\\rightarrow\\,\\mathbb{R}^{D}$ . An ellipsoid centered at a general point $c\\in\\mathbb{R}^{D}$ is just the translate $c+S$ of some ellipsoid $S$ centered at the origin. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(B(0,1))=\\{\\mathbf{Lx}:\\mathbf{x}\\in B(0,1)\\}}\\\\ &{\\qquad\\qquad=\\{\\mathbf{y}:\\mathbf{L}^{-1}\\mathbf{y}\\in B(0,1)\\}}\\\\ &{\\qquad\\qquad=\\{\\mathbf{y}:(\\mathbf{L}^{-1}\\mathbf{y})^{T}\\mathbf{L}^{-1}\\mathbf{y}\\le1\\}}\\\\ &{\\qquad\\qquad=\\{\\mathbf{y}:\\mathbf{y}^{T}(\\mathbf{L}\\mathbf{L}^{T})^{-1}\\mathbf{y}\\le1\\}}\\\\ &{\\qquad\\qquad=\\{\\mathbf{y}:\\mathbf{y}^{T}\\mathbf{Q}^{-1}\\mathbf{y}\\le1\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbf{Q}=\\mathbf{LL}^{T}$ is positive definite. ", "page_idx": 21}, {"type": "text", "text": "The radius $r_{i}$ along principal axis $\\mathbf{e}_{i}$ obeys $\\begin{array}{r}{r_{i}^{2}={\\frac{1}{\\lambda_{i}}}}\\end{array}$ , where $\\lambda_{i}$ is the eigenvalue of $\\mathbf{Q}^{-1}$ and $\\mathbf{e}_{i}$ is eigen vector. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.5 (Gaussian width of ellipsoid) . Let $S$ be an ellipsoid in $\\mathbb{R}^{D}$ defined by the positive definite matrix $\\mathbf{H}\\in\\mathbb{R}^{D\\times D}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nS(\\epsilon):=\\{\\mathbf{w}\\in\\mathbb{R}^{D}:\\frac{1}{2}\\mathbf{w}^{T}\\mathbf{H}\\mathbf{w}\\leq\\epsilon\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then $w(S)^{2}$ or the squared Gaussian width of the ellipsoid satisfies: ", "page_idx": 21}, {"type": "equation", "text": "$$\nw(S)^{2}\\approx2\\epsilon\\mathrm{Tr}(\\mathbf{H}^{-1})=\\sum_{i}r_{i}^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $r_{i}=\\sqrt{2\\epsilon/\\lambda_{i}}$ with $\\lambda_{i}$ is $i$ -th eigenvalue of $\\mathbf{H}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Let $\\mathbf{g}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{D\\times D})$ and $\\mathbf{L}\\mathbf{L}^{T}=2\\epsilon\\mathbf{H}^{-1}$ . Then: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{w(L(B_{2}^{n}))=\\frac{1}{2}\\mathbb{E}\\,s u p_{\\mathbf{x},\\mathbf{y}\\in B(0,1)}<\\mathbf{g},\\mathbf{Lx}-\\mathbf{Ly}>}&{}\\\\ &{=\\frac{1}{2}\\mathbb{E}\\,s u p_{\\mathbf{x},\\mathbf{y}\\in B(0,1)}<\\mathbf{L^{T}}\\mathbf{g},\\mathbf{x}-\\mathbf{y}>}\\\\ &{=\\mathbb{E}\\|\\mathbf{L^{T}}\\mathbf{g}\\|_{2}}\\\\ &{=\\mathbb{E}\\sqrt{\\mathbf{g}^{\\mathbf{T}}\\mathbf{L}\\mathbf{L^{T}}\\mathbf{g}}}\\\\ &{=\\mathbb{E}\\sqrt{2\\epsilon\\mathbf{g}^{\\mathbf{T}}\\mathbf{H}^{-1}\\mathbf{g}}}\\\\ &{\\approx\\sqrt{2\\epsilon\\mathbb{E}[\\mathbf{g}^{\\mathbf{T}}\\mathbf{H}^{-1}\\mathbf{g}]}}\\\\ &{=\\sqrt{2\\epsilon\\mathbb{T}\\mathbf{(H}^{-1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D.2 Gaussian Width of the Deformed Ellipsoid ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Generally, it is assumed that the gradient descent algorithm will converge to a minimum point. However, in practice, even with small learning rates, the network may oscillate near the minimum point and not directly converge to it, but rather get very close to it. As a result, the actual Hessian matrix is often not positive definite and its eigenvalues may have negative values. ", "page_idx": 22}, {"type": "text", "text": "Lemma D.6 Let the Hessian matrix at the minimum point be denoted by $\\mathbf{H}$ with eigenvalue $\\lambda_{i}$ , and the Hessian matrix at an oscillation point be denoted by H\u02c6 with eigenvalue $\\hat{\\lambda}_{i}$ . The negative eigenvalues of $\\hat{\\bf H}$ have small magnitudes. ", "page_idx": 22}, {"type": "text", "text": "$P r o o f$ . Let the weights at the minimum point be denoted by ${\\bf w}_{0}$ and the Hessian matrix at an oscillation point be denoted by $\\hat{\\mathbf{w}}_{0}$ . Consider a loss function $\\mathcal{L}$ and a loss landscape defined by ${\\mathcal{L}}({\\bf w})$ , taking Taylor expansion of ${\\mathcal{L}}(\\mathbf{w})$ at ${\\bf w}_{0}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{w})=\\mathcal{L}(\\mathbf{w}_{0})+\\frac{1}{2}(\\mathbf{w}-\\mathbf{w}_{0})^{T}\\mathbf{H}(\\mathbf{w}-\\mathbf{w}_{0})+R(\\mathbf{w}_{0})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\hat{\\mathbf{w}}_{\\mathbf{0}}=\\mathbf{w}_{\\mathbf{0}}+\\mathbf{v}$ with $\\mathbf{v}$ is closed to 0: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathcal{L}}(\\hat{\\mathbf{w}}_{\\mathbf{0}})={\\mathcal{L}}(\\mathbf{w}_{0}+\\mathbf{v})}\\\\ {\\quad\\quad\\quad={\\mathcal{L}}(\\mathbf{w}_{0})+\\displaystyle{\\frac{1}{2}}\\mathbf{v}^{T}\\mathbf{H}\\mathbf{v}+R(\\mathbf{w}_{0}+\\mathbf{v})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the second order derivative of $\\mathcal{L}(\\hat{\\mathbf{w}}_{\\mathbf{0}})$ is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\mathcal{L}}^{\\prime\\prime}(\\mathbf{w})=\\boldsymbol{\\mathcal{L}}^{\\prime\\prime}(\\mathbf{w}_{0}+\\mathbf{v})}\\\\ &{\\qquad\\qquad=\\mathbf{H}+\\boldsymbol{R}^{\\prime\\prime}(\\mathbf{w}_{0}+\\mathbf{v})}\\\\ &{\\qquad\\qquad\\approx\\mathbf{H}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{L}}^{\\prime\\prime}(\\mathbf{w})=\\hat{\\mathbf{H}}$ , Let $\\mathbf{H}={\\hat{\\mathbf{H}}}+\\mathbf{H}_{0}$ with $\\mathbf{H}_{0}$ is closed to 0, considering the Weyl inequality: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda_{i}(\\mathbf{H})-\\hat{\\lambda}_{i}(\\hat{\\mathbf{H}})\\leq\\|\\mathbf{H}_{0}\\|_{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\lVert\\mathbf{H}_{0}\\rVert_{2}$ is small enough. So if $\\hat{\\lambda}_{i}(\\hat{\\bf H})$ is less than 0, since $\\hat{\\lambda}_{i}(\\hat{\\bf H})\\,\\ge\\,\\lambda_{i}({\\bf H})\\,-\\,\\|{\\bf H}_{0}\\|_{2}$ , its absolute value $|\\hat{\\lambda}_{i}(\\hat{\\mathbf{H}})|\\leq\\|\\mathbf{H}_{0}\\|_{2}-\\lambda_{i}(\\mathbf{H})\\leq\\|\\mathbf{H}_{0}\\|_{2}$ , which means that the negative eigenvalues of the Hessian matrix have small magnitudes. ", "page_idx": 22}, {"type": "text", "text": "Lemma D.7 For a sublevel set $S(\\epsilon)\\,:=\\,\\{\\mathbf{w}\\,:\\,\\mathbf{w}^{T}\\mathbf{H}\\mathbf{w}\\,\\leq\\,\\epsilon\\}$ defined by a matrix $\\mathbf{H}$ with small magnitude negative eigenvalues. The Gaussian width of $S(\\epsilon)$ can be estimated by obtaining the absolute values of the eigenvalues of the matrix $\\mathbf{H}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Assuming that the eigenvalue decomposition of $\\mathbf{H}$ is $\\mathbf{H}=\\mathbf{v}^{T}\\pmb{\\Sigma}\\mathbf{v}$ , where $\\Sigma$ is a diagonal matrix consisting of the eigenvalues of $\\mathbf{H}$ , let ${\\bf D}\\;=\\;{\\bf v}^{T}|{\\bf\\Sigma}{\\bf\\Sigma}|{\\bf v}$ be a positive definite matrix and $\\mathbf{M}=\\mathbf{H}-\\mathbf{D}=\\mathbf{\\bar{v}}^{T}(\\mathbf{\\Sigma}\\mathbf{\\Sigma}-|\\bar{\\boldsymbol{\\Sigma}}|)\\mathbf{v}$ be a negative definite matrix. Consider the definition of $S(\\epsilon)$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf w^{T}\\mathbf H\\mathbf w=\\mathbf w^{T}(\\mathbf H-\\mathbf D+\\mathbf D)\\mathbf w}\\\\ &{\\qquad\\qquad=\\mathbf w^{T}\\mathbf M\\mathbf w+\\mathbf w^{T}\\mathbf D\\mathbf w}\\\\ &{\\qquad\\qquad\\le\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, $S(\\epsilon)$ can be expressed as $\\mathbf{w}^{T}\\mathbf{D}\\mathbf{w}\\leq\\epsilon-\\mathbf{w}^{T}\\mathbf{M}\\mathbf{w}$ . Since the magnitudes of the negative eigenvalues of $\\mathbf{H}$ are very small, we can assume that $\\mathbf{w}^{T}\\mathbf{M}\\mathbf{w}$ is also small, and thus $\\mathbf{w}^{T}\\bar{\\mathbf{D}}\\bar{\\mathbf{w}}\\leq$ $\\epsilon\\stackrel{\\overline{{\\mathbf{\\Pi}}}}{-}\\mathbf{w}^{T}\\mathbf{M}\\mathbf{w}$ can be approximately equal to $\\mathbf{w}^{T}\\mathbf{D}\\mathbf{w}\\leq\\epsilon$ . As a result, we can estimate the Gaussian width of $S(\\epsilon)$ by approximating it with the absolute values of the eigenvalues of $\\mathbf{H}$ . ", "page_idx": 23}, {"type": "text", "text": "Corollary D.8 Consider a well-trained neural network with weights w, whose loss function defined by w has a Hessian matrix $\\mathbf{H}$ . Due to the non-positive definiteness of $\\mathbf{H}$ , there exist negative eigenvalues. Let the eigenvalue decomposition of $\\mathbf{H}$ be $\\mathbf{H}=\\mathbf{\\dot{v}}^{T}\\pmb{\\Sigma}\\mathbf{v}$ , where $\\Sigma$ is a diagonal matrix of eigenvalues. Let ${\\bf D}\\stackrel{<}{=}{\\bf v}^{T}|{\\bf\\Sigma}{\\bf\\Sigma}|{\\bf v}_{x}$ , where $\\big|\\cdot\\big|$ means absolute operation. the geometric objects defined by $H$ and $D$ are $S(\\epsilon):=\\left\\{\\mathbf{w}\\in\\mathbb{R}^{D}:\\textstyle{\\frac{1}{2}}\\mathbf{w}^{T}\\mathbf{H}\\mathbf{w}\\leq\\epsilon\\right\\}$ and $\\hat{S}(\\epsilon):=\\{\\mathbf{w}\\in\\mathbb{R}^{D}:\\frac{1}{2}\\mathbf{w}^{T}\\mathbf{D}\\mathbf{w}\\leq\\epsilon\\},$ , then the gaussian width of the two set satisfy: ", "page_idx": 23}, {"type": "equation", "text": "$$\nw(S(\\epsilon))\\approx w(\\hat{S}(\\epsilon))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E Comparison between the Upper and Lower Bound ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section provided the proofs used in the upper bound derivation and roughly analyzed how the lower bound changes when the upper bound varies. ", "page_idx": 24}, {"type": "text", "text": "E.1 $D-k$ Dimension Sublevel Set is Still an Ellipsoid ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In the derivation of the upper bound for the pruning ratio threshold, we employed a $D\\!-\\!k$ dimensional loss sublevel set: ", "page_idx": 24}, {"type": "equation", "text": "$$\nS(\\mathbf{w}^{'})=\\{\\mathbf{w}^{'}\\in\\mathbb{R}^{D-k}:\\mathcal{L}([\\mathbf{w}^{1},\\mathbf{w}^{'}])\\leq\\mathcal{L}(\\mathbf{w}^{*})+\\epsilon\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Perform Taylor expansion to $\\mathcal{L}([\\mathbf{w}^{1},\\mathbf{w}^{'}])$ with respect to $\\mathbf{w}^{'}$ , the sublevel set is represented as: ", "page_idx": 24}, {"type": "equation", "text": "$$\nS({\\mathbf{w}}^{'})=\\{{\\mathbf{w}}^{'}\\in\\mathbb{R}^{D-k}:\\frac{1}{2}({\\mathbf{w}}^{'})^{T}\\mathbf{H}^{'}\\mathbf{w}^{'}\\leq\\epsilon\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathbf{H}^{'}$ is the Hessian matrix of $\\mathcal{L}([\\mathbf{w}^{1},\\mathbf{w}^{'}])$ with respect to w. ", "page_idx": 24}, {"type": "text", "text": "Given that the full sublevel set $S(\\epsilon)\\,=\\,\\{\\mathbf{w}\\,\\in\\,\\mathbb{R}^{D}\\,:\\,\\textstyle{\\frac{1}{2}}\\mathbf{w}^{T}\\mathbf{H}\\mathbf{w}\\,\\le\\,\\epsilon\\}$ is an ellipsoid body, which implies that $\\mathbf{H}$ is a positive definite matrix, it is evident that $\\mathbf{H}^{'}$ is the principal submatrix of $\\mathbf{H}$ Consequently, $\\mathbf{H}^{'}$ is also a positive definite matrix, which implies that the sublevel set $S(\\mathbf{w}^{'})$ remains an ellipsoid. ", "page_idx": 24}, {"type": "text", "text": "E.2 Relationship between the Upper and Lower Bound ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Theorem E.1 (Eigenvalue Interlacing Theorem) Suppose $\\textbf{A}\\in\\ \\mathbb{R}^{n\\times n}$ is symmetric, Let $\\textbf{B}\\in$ $\\mathbb{R}^{m\\times m}$ with $m\\ <\\ n$ be a principal submatrix(obtained by deleting both $i$ -th row and $i$ -th column for some values of $i_{.}$ ). Suppose A has eigenvalues $\\lambda_{1}\\,\\leq\\,\\cdot\\,\\cdot\\,\\leq\\,\\lambda_{n}$ and $\\mathbf{B}$ has eigenvalues $\\beta_{1}\\leq\\cdots\\leq\\beta_{m}$ . Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lambda_{k}\\leq\\beta_{k}\\leq\\lambda_{k+n-m}\\quad f o r\\quad k=1,\\ldots,m\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "And if $m=n-1,$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lambda_{1}\\le\\beta_{1}\\le\\lambda_{2}\\le\\beta_{2}\\le\\cdot\\cdot\\cdot\\le\\beta_{n-1}\\le\\lambda_{n}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we provide an elucidation on the relationship between the upper bound and lower bound variations: ", "page_idx": 24}, {"type": "text", "text": "Lemma E.2 The direct and straightforward relationship between the upper bound and the lower bound can be articulated as follows: ", "page_idx": 24}, {"type": "text", "text": "1. When the eigenvalues change, the upper and lower bounds will change in the same direction; 2. When the weight magnitude changes, the upper bound will change in the same direction as the upper bound or do not change. ", "page_idx": 24}, {"type": "text", "text": "Proof. 1. When the eigenvalues change, the upper and lower bounds will change in the same direction; ", "page_idx": 24}, {"type": "text", "text": "By leveraging the Eigenvalue Interlacing Theorem (Theorem E.1), the eigenvalues of the principal submatrix in the upper bound is bounded by the eigenvalues in the lower bound. It\u2019s obvious that if the eigenvalues in the lower bound change, the eigenvalues in the upper bound will also change in the same direction, leading to the upper and lower bounds will change in the same direction. ", "page_idx": 24}, {"type": "text", "text": "2. When the weight magnitude changes, the upper bound will change in the same direction as the lower bound or do not change. ", "page_idx": 24}, {"type": "text", "text": "It\u2019s noted that the number of weights in the lower bound is more than the one of weights in the upper bound. These weights are used to calculate the projection distance. So it\u2019s clear that when the weight magnitude changes, the upper bound will change in the same direction as the lower bound or not change. ", "page_idx": 24}, {"type": "text", "text": "F Sharpness & Lower Bound ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we will provide the relationship between the lower bound of the pruning ratio and the sharpness of the loss landscape w.r.t the weights. We first introduce the definition of sharpness, which is similar to the sharpness definition in [26] and [9]: ", "page_idx": 25}, {"type": "text", "text": "Definition F.1 Given a second-order derivable function $f(\\mathbf{w})$ , where w is the parameters. Considering a hessian matrix H w.r.t. parameters w, the sharpness of $f(\\mathbf{w})$ w.r.t. parameters is defined as the trace of H, i.e. $\\mathrm{Tr}(\\mathbf{H})$ . ", "page_idx": 25}, {"type": "text", "text": "As defined in definition F.1, a smaller trace indicates a flatter function. Next, we will provide the connection between the sharpness and the lower bound: ", "page_idx": 25}, {"type": "text", "text": "Lemma F.2 Given a well-trained neural network $f(\\mathbf{w},\\mathbf{x})$ , where w is the parameters and the x is the input. The lower bound of pruning ratio $\\rho_{l}$ and the sharpness $\\mathrm{Tr}(\\mathbf{H})$ obeys: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\rho_{l}\\leq1-\\frac{2\\epsilon D}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}\\mathrm{Tr}(\\mathbf{H})+2\\epsilon D}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathbf{H}$ is the hessian matrix of $f(\\mathbf{w})\\ w.r.t.$ w. ", "page_idx": 25}, {"type": "text", "text": "Proof. ", "page_idx": 25}, {"type": "text", "text": "Cauchy\u2013Schwarz Inquality. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\rho_{l}=1-\\frac{1}{D}\\sum_{i=1}^{D}\\frac{r_{i}^{2}}{|\\mathbf{w}^{*}-\\mathbf{w}^{k}||_{2}^{2}+r_{i}^{2}}}\\\\ {\\displaystyle~=1-\\frac{2\\epsilon}{D}\\sum_{i=1}^{D}\\frac{1}{|\\mathbf{w}^{*}-\\mathbf{w}^{k}||_{2}^{2}\\lambda_{i}+2\\epsilon}}\\\\ {\\displaystyle~\\leq1-\\frac{2\\epsilon}{D}\\frac{D^{2}}{\\sum_{i=1}^{D}(|\\mathbf{w}^{*}-\\mathbf{w}^{k}||_{2}^{2}\\lambda_{i}+2\\epsilon)}}\\\\ {\\displaystyle~=1-\\frac{2\\epsilon D}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}\\sum_{i=1}^{D}\\lambda_{i}+2\\epsilon D}}\\\\ {\\displaystyle~=1-\\frac{2\\epsilon D}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}\\mathbb{T}(\\mathbf{H})+2\\epsilon D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It\u2019s obvious that if the trace of the hessian matrix becomes smaller, the lower bound will also decrease, indicating a higher sparsity level. Utilizing sharpness as the optimization objective for network pruning is both a rational and efficacious approach. ", "page_idx": 25}, {"type": "text", "text": "Corollary F.3 Given a well-trained neural network $f(\\mathbf{w},\\mathbf{x})$ , where w is the parameters and the x is the input. The pruning ratio lower bound and the sharpness obeys: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\rho_{l}\\leq1-\\frac{2\\epsilon D}{\\|\\mathbf{w}^{*}-\\mathbf{w}^{k}\\|_{2}^{2}\\mathrm{Tr}(\\mathbf{H})+2\\epsilon D}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where H is the hessian matrix of $f(\\mathbf{w})$ w.r.t. w. An informal version of this corollary can be stated as: sharpness controls the lower bound of the pruning ratio, specifically, a flatter neural network can be pruned more sparsely. ", "page_idx": 25}, {"type": "text", "text": "G Full Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here we present the full set of experiments performed for the results in the main text. ", "page_idx": 26}, {"type": "text", "text": "G.1 The Distance Between the Distribution of Weights ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we provide the total variation(TV) distance between the distribution of trained weights with independent initialization. ", "page_idx": 26}, {"type": "text", "text": "Table 9: The TV Distance Between the Distribution of Weights. ", "page_idx": 26}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/1f246f5552caa6158a3b8d9173e72fecb06f08f54e73ee1aa034009b74b6f47b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "G.2 Comparison of Pruning Algorithms ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "As discussed in Section 4, we adopt $l_{1}$ regularization during the training phase, and the hyperparameter for the $l_{1}$ regularization is selected empirically. After thorough training, we applied magnitude pruning to reduce the network to the lowest pruning ratio at which the network\u2019s performance is maintained, and we didn\u2019t apply fine-tuning after pruning. ", "page_idx": 26}, {"type": "text", "text": "We validated $l_{1}$ regularized one-shot magnitude pruning algorithm(LOMP) against four baselines: dense weight training and three pruning algorithms: (i) Rare Gems(RG) proposed by [28], (ii) Iterative Magnitude Pruning(IMP) donated by [7], (iii) Smart-Ratio (SR) which is the random pruning method proposed by [29]. Table 10 shows the pruning performance of the above algorithms, our pruning algorithm is better performing than other algorithms. ", "page_idx": 26}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/99a3c1ba8afb6a599188bea5401678127e971955c53a67aa6f43e6cf94a9c7cb.jpg", "table_caption": ["Table 10: Performance comparison of various pruning algorithms. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Remark. To demonstrate the effectiveness of $l_{1}$ regularization, the dense performance is obtained by training without any regularization. we obtained the dense performance by training without any regularization. According to our theoretical findings, LOMP is the optimal pruning strategy in the pruning-after-training regime, as the $l_{1}$ is the closest convex relaxation to the $l_{0}$ regularization, and the magnitude pruning can achieve the lowset pruning ratio. Consequently, we compared LOMP with several pruning algorithms that are not part of the pruning-after-training regime. Notably, LOMP outperforms these other pruning algorithms. ", "page_idx": 26}, {"type": "text", "text": "Discussion. The $l_{1}$ regularization metric is not new, and in practice, it becomes increasingly challenging to empirically select the hyper-parameter for $l_{1}$ regularization and train models with $l_{1}$ regularization as the model size grows. Nevertheless, some reparametrization techniques to address the $l_{1}$ regularization issue, as discussed in [44], may facilitate the use of $l_{1}$ regularization for pruning, making $l_{1}$ regularization one-shot pruning a very promising approach. ", "page_idx": 26}, {"type": "text", "text": "G.3 Comparison of Pruning as Optimization ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we compared our $l_{1}$ regularization-based one-shot magnitude pruning with those works treating pruning as optimization [4, 40, 41]. Our results are obtained by searching the hyperparameters of $l_{1}$ regularization, all the data and model settings are followed from [4]. The comparison results can be found in Table 11. As is shown in Table 11, in extremely high levels of pruning schemes, our proposed method performs better than other methods. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Table 11: The pruning performance (model accuracy) of various methods on MLPNet, ResNet20, and ResNet50. As to the performance of MP, WF, CBS, CHITA, and EWR, we adopt the results reported in [4]. We take five runs for our approaches and report the mean and standard error (in the brackets). The best accuracy values (significant) are highlighted in bold. Here sparsity denotes the fraction of zero weights in convolutional and dense layers. ", "page_idx": 27}, {"type": "table", "img_path": "IAAPhOLhcX/tmp/36045e440a5afaf1a7cf56c9adedfc9e2c176a7a945a643b5d6962e6179cf8d6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "G.4 Small Weights Benefits Pruning ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We verify that small sharpness is not equal to high sparsity through hypothetical experiments. Considering that the hessian matrix of network $A$ and network $B_{1},B_{2},B_{3},B_{4}$ share eigenvalues $\\{\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{n}\\}$ , the weight magnitude of network $B_{1},B_{2},B_{3},B_{4}$ is 2,3,4,5 times that of network $A$ , we take the eigenvalues and weights from a FC network trained without regularization. In this way, the gap between the curves will be more obvious. For other networks, the trend of the curve gap is consistent, the prediction of the network pruning ratio is shown in the Figure. 7. It is observed from Figure. 7 that as the magnitude of network weights increases, the capacity of the network to tolerate pruning decreases. The pruning ratio threshold is affected not only by loss sharpness but also the magnitude of weights. This finding, on the other hand, provides further evidence of the effectiveness of the $l_{1}$ -norm in pruning tasks. ", "page_idx": 27}, {"type": "text", "text": "G.5 Statistical Information of Weights in Various Experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The same plots as Fig. 2(b) and Fig. 2(c) are provided in Figure 8 ", "page_idx": 27}, {"type": "image", "img_path": "IAAPhOLhcX/tmp/cf6976d393d71a7d137d8c1082b5b351a960426846dc4c3cfa69e823eac4813c.jpg", "img_caption": ["Figure 7: Pruning ratio prediction on different weight magnitude. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "IAAPhOLhcX/tmp/53c8c390d55e8ad614c64b734910b5397fb2b20030001643fb4fa20d3ed1fd9b.jpg", "img_caption": ["Figure 8: The same plots as Fig. 2(b) and Fig. 2(c) on eight tasks. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "H Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this paper, we consider a well-trained neural network and argue that the weight magnitude and network sharpness with respect to the weights constitute the fundamental limits of a one-shot network pruning task. Although popular methods such as Iterative Magnitude Pruning [7] and the Lottery Ticket Hypothesis [6] involve multiple rounds of one-shot pruning, the fundamental limits of such multi-shot pruning remain unclear. Furthermore, we demonstrate that when the magnitude of the removed weights is small, the upper bound of the pruning ratio nearly coincides with the lower bound. However, not all training strategies result in small removed weights, and in such cases, the upper and lower bounds do not coincide. Nevertheless, weight magnitude and network sharpness with respect to the weights still represent the fundamental limits of a one-shot network pruning task. ", "page_idx": 29}, {"type": "text", "text": "I Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Our work aims to advance the theoretical understanding of network pruning, with the anticipation that theoretical insights can guide future designs of network pruning methods. There are no ethically related issues or negative societal consequences in our work. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See corollary 3.4, Lemma 3.5, Theorem 3.6, Section 4.1 and Algorithm 1. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have discussed the limitations of our assumptions and our theoretical results, a limitation section has been included in the appendix, see Section H. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In the main paper, we introduced all the assumptions and provided intuitive explanations. Comprehensive proofs are included in the appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: See Section 4, 5 in the main paper and Section B in the Appendix. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided open access to our code, the anonymized URL is included in the abstract. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All the details and settings are included in the appendix. See Section B in the Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We provided statistical results of our results under multiple independent runs.   \nSee Table 9 and Table 2. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have provided information of the computer resources in Section B and our code (see the URL in abstract). ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We confirm that our research adheres to the NeurIPS Code of Ethics in every respect and preserves anonymity. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: There is no societal impact of the work performed and we have included an impact statement in the appendix, see Section I. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our research didn\u2019t release any data or models. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We have cited the assets we used in our paper, See References. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our work introduces no new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]