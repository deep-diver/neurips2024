[{"figure_path": "IAAPhOLhcX/figures/figures_3_1.jpg", "caption": "Figure 1: Panel (a, b): Illustration of a convex conic hull and the statistical dimension. Panel (c): Effect of projection distance on projection size and intersection probability.", "description": "This figure illustrates three key concepts from convex geometry that are central to the paper's theoretical framework. Panel (a) shows a convex cone, which is a set of vectors that are closed under positive scalar multiplication.  Panel (b) demonstrates how the statistical dimension of a cone affects its size; a larger statistical dimension corresponds to a larger cone. Finally, panel (c) shows the effect of the distance between a subspace and a convex set on their intersection probability; the closer they are, the higher the probability of intersection. These concepts are used to analyze the fundamental limits of network pruning by relating it to the problem of set intersection in high-dimensional space.", "section": "Problem Setup & Key Notions"}, {"figure_path": "IAAPhOLhcX/figures/figures_7_1.jpg", "caption": "Figure 1: Panel (a, b): Illustration of a convex conic hull and the statistical dimension. Panel (c): Effect of projection distance on projection size and intersection probability.", "description": "This figure provides a visual representation of key concepts in high-dimensional convex geometry used in the paper. Panel (a) and (b) illustrate how the size of a convex cone influences its statistical dimension, a measure of complexity. Panel (c) shows how the projection distance affects the size of a projected set and its probability of intersecting with a given subspace, relating to the feasibility of network pruning.", "section": "Problem Setup & Key Notions"}, {"figure_path": "IAAPhOLhcX/figures/figures_8_1.jpg", "caption": "Figure 3: The impact of sparsity on loss and test accuracy are obtained on the test dataset, and we mark the theoretical pruning ratio limit with vertical lines. The loss values have been normalized and translated.", "description": "This figure shows the impact of different pruning ratios on the test loss and accuracy for various network architectures and datasets.  Each subfigure represents a specific model (e.g., FC5, ResNet18) trained on a particular dataset (e.g., CIFAR10, CIFAR100). The x-axis represents the pruning ratio (percentage of weights removed), and the y-axis represents the test loss and accuracy.  Vertical lines indicate the theoretical pruning ratio limit calculated by the authors' method.  The results demonstrate a close agreement between the theoretical limit and the experimental observations of loss and accuracy.", "section": "5.1 Validation of Pruning Lower Bound"}, {"figure_path": "IAAPhOLhcX/figures/figures_9_1.jpg", "caption": "Figure 4: Top Row: From left to right, as the number of iterations increases, it leads to an increase in the theoretical pruning ratio threshold. The horizontal line represents the last pruning ratio. Bottom Row: The comparison of the pruning ratio threshold between using and not using 12-regularization. Sparse networks are obtained by magnitude-based pruning with fixed pruning ratios. The two plots on the left and the two plots on the right correspond to different fixed pruning ratios. Here, R = ||w* - wk ||2, which is the projection distance.", "description": "This figure visualizes the impact of iterative pruning and l2 regularization on the pruning ratio. The top row demonstrates how the theoretical pruning ratio threshold increases with more iterations, while the bottom row compares the threshold with and without l2 regularization.  The plots show that iterative pruning gradually increases the pruning ratio, while l2 regularization enhances the performance of the Rare Gems algorithm by increasing the pruning threshold.", "section": "3.1.4 Pruning Ratio vs Magnitude & Sharpness"}, {"figure_path": "IAAPhOLhcX/figures/figures_16_1.jpg", "caption": "Figure 5: The theoretically predicted pruning ratio in eight tasks. The first row, from left to right, corresponds to FC5, FC12, AlexNet, and VGG16 on CIFAR10. The second row, from left to right, corresponds to ResNet18 and ResNet50 on CIFAR100, as well as ResNet18 and ResNet50 on TinyImagenet.", "description": "This figure shows a comparison of the theoretical pruning ratio predictions against the actual pruning ratios obtained using magnitude pruning for eight different model-dataset combinations.  The top row shows results for smaller, fully connected networks (FC5, FC12) and convolutional networks (AlexNet, VGG16) trained on CIFAR-10.  The bottom row displays results for larger ResNet models (ResNet18 and ResNet50) trained on CIFAR-100 and TinyImageNet datasets. For each task, the theoretical curve predicts the minimum pruning ratio that can be achieved without sacrificing performance. The actual pruning ratios (obtained by experiment) are plotted alongside these theoretical predictions to validate the theory.", "section": "B.5 Theoretical Pruning Ratio"}, {"figure_path": "IAAPhOLhcX/figures/figures_17_1.jpg", "caption": "Figure 6: The statistical analysis of the L1 norm of the Hessian matrix in eight tasks. The first row, from left to right, corresponds to FC5, FC12, AlexNet, and VGG16. The second row, from left to right, corresponds to ResNet18 and ResNet50 on CIFAR100, as well as ResNet18 and ResNet50 on TinyImagenet.", "description": "This figure shows the L1 norm of each row in the Hessian matrix for eight different network architectures trained on various datasets.  The x-axis represents the percentage of rows in the Hessian matrix, and the y-axis represents the L1 norm of those rows.  Each subplot displays the L1 norm distribution for a specific network and dataset combination, illustrating the heterogeneity in the Hessian matrix's row-wise magnitudes.  This analysis is important for understanding the network's structure and the relative significance of different parameters. The concentration of low L1 norm values is especially notable.", "section": "C Practical Calculation of Gaussian Width"}, {"figure_path": "IAAPhOLhcX/figures/figures_28_1.jpg", "caption": "Figure 7: Pruning ratio prediction on different weight magnitude.", "description": "This figure displays the relationship between pruning ratio and the magnitude of weights. It shows how the pruning ratio changes as the magnitude of weights increases. The 'Original' curve represents the pruning ratio without any modifications to the weight magnitudes. The other curves ('x2', 'x3', 'x4', 'x5') show the predicted pruning ratios when the magnitude of weights is multiplied by 2, 3, 4, and 5 respectively.  The green curve labeled 'Prediction' shows an approximation for the pruning ratio. The figure highlights that as the magnitude of weights increases, the achievable pruning ratio decreases.", "section": "G.4 Small Weights Benefits Pruning"}, {"figure_path": "IAAPhOLhcX/figures/figures_28_2.jpg", "caption": "Figure 8: The same plots as Fig. 2(b) and Fig. 2(c) on eight tasks.", "description": "This figure shows the distribution of weights and the relationship between the pruning ratio and the magnitude of weights for eight different tasks.  The top row shows the results for FC5, FC12, AlexNet, and VGG16 on CIFAR10.  The bottom row shows the results for ResNet18 and ResNet50 on CIFAR100 and TinyImageNet. For each task, there are two plots. The left plot shows the distribution of the weights (in log scale), and the right plot shows the relationship between the pruning ratio and the magnitude of the weights. The red curve in the right plot represents the theoretical prediction for the pruning ratio based on the paper's proposed model, and the blue curve represents the actual observed pruning ratio.", "section": "G Full Results"}]