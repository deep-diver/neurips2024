{"references": [{"fullname_first_author": "Dennis Amelunxen", "paper_title": "Living on the edge: Phase transitions in convex programs with random data", "publication_date": "2014-01-01", "reason": "This paper provides the theoretical foundation for the fundamental limit of network pruning by leveraging the framework of high-dimensional convex geometry."}, {"fullname_first_author": "Jonathan Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "publication_date": "2018-01-01", "reason": "This paper introduces the lottery ticket hypothesis, which is a key concept in network pruning that inspired the current research."}, {"fullname_first_author": "Brett W Larsen", "paper_title": "How many degrees of freedom do we need to train deep networks: a loss landscape perspective", "publication_date": "2021-01-01", "reason": "This paper provides valuable insights on the relationship between network capacity and the number of parameters, which directly relates to the fundamental limit of pruning."}, {"fullname_first_author": "Mansheej Paul", "paper_title": "Unmasking the lottery ticket hypothesis: What's encoded in a winning ticket's mask?", "publication_date": "2022-01-01", "reason": "This paper explores the lottery ticket hypothesis further, providing more context for the research on the fundamental limit of pruning."}, {"fullname_first_author": "Karen Simonyan", "paper_title": "Very deep convolutional networks for large-scale image recognition", "publication_date": "2014-01-01", "reason": "This paper is cited for the use of the VGG-16 network architecture in experiments, demonstrating the practical relevance of the theoretical findings."}]}