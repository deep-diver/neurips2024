[{"type": "text", "text": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiang Meng Operations Research Center Massachusetts Institute of Technology mengx@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Kayhan Behdin Operations Research Center Massachusetts Institute of Technology behdink@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Haoyue Wang Operations Research Center Massachusetts Institute of Technology haoyuew@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Rahul Mazumder Operations Research Center Massachusetts Institute of Technology rahulmaz@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce $A L P S$ , an optimizationbased framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. $A L P S$ outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the LLaMA3-8B model with $70\\%$ sparsity, ALPS achieves a $29\\%$ reduction in test perplexity on the WikiText dataset and a $8\\%$ improvement in zero-shot benchmark performance compared to existing methods. Our code is available at https://github.com/ mazumder-lab/ALPS. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) have revolutionized the field of natural language processing, demonstrating remarkable performance across a wide spectrum of tasks, from question answering and text generation to sentiment analysis and named entity recognition [Wei et al., 2022, Bubeck et al., 2023, Achiam et al., 2023]. The success of LLMs can in part be attributed to their massive scale\u2014state-ofthe-art models like OPT-175B [Zhang et al., 2022a] and LLaMA3 [Dubey et al., 2024] have hundreds of billions of parameters. However, this enormous size comes at a steep cost in terms of storage and computational resources. For instance, the OPT-175B model requires at least 320 GB of memory to store its parameters in half-precision (FP16) format, necessitating the use of multiple high-end GPUs for inference [Frantar and Alistarh, 2023]. To make LLMs more accessible and efficient, considerable efforts have been made to compress these models, with a particular emphasis on model quantization techniques [Lin et al., 2023, Behdin et al., 2023, Dettmers et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "Network pruning [LeCun et al., 1989, Hassibi and Stork, 1992, Han et al., 2015], a complementary approach to quantization, has received comparatively less attention in the realm of LLMs. Pruning aims to reduce the model size by identifying and removing redundant or less important weights, resulting in a sparser and more efficient network. Traditional pruning methods rely on iterative retraining to recover accuracy after each pruning stage [Han et al., 2015, Luo et al., 2017, Molchanov et al., 2016, Liu et al., 2018], which can be computationally expensive and time-consuming. To address this, recent research has focused on one-shot pruning methods [He et al., 2017, Singh and Alistarh, 2020] that compress a pre-trained model using only a small amount of data (e.g., a few thousand samples)\u2014the key idea here is to perform pruning while retaining model accuracy as much as possible without expensive fine-tuning/retraining on the entire dataset. Many prior works [Frantar and Alistarh, 2022, Yu et al., 2022, Benbaki et al., 2023] on one-shot pruning address such pruning-accuracy tradeoffs using optimization based approaches. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the progress made in one-shot pruning, the massive scale of LLMs poses additional challenges, as many one-shot pruning methods designed for vision models cannot be directly applied due to their large model sizes. To overcome this, existing LLM pruning methods often rely on heuristic approaches to prune instead of solving optimization problems. For instance, SparseGPT [Frantar and Alistarh, 2023] approximates the OBS [Hassibi and Stork, 1992] algorithm by employing partial weight updates and adaptive mask selection to reduce costly Hessian computation. Similarly, Wanda [Sun et al., 2023] prunes weights based on the product of their magnitudes and corresponding input activations. Zhang et al. [2023] propose to iteratively grow and prune the weight mask according to the change in reconstruction error achieved by each update. While these heuristics enable pruning at scale, they may lead to suboptimal compression (and hence, suboptimal compression-accuracy tradeoffs) compared to advanced optimization-based approaches, as we show in this paper. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose $A L P S^{1}$ , an optimization-based framework for one-shot LLM pruning. ALPS consists of two key components. First, it formulates pruning LLMs as an $\\ell_{0}$ -constrained optimization problem and solves it directly using the operator splitting technique (i.e., ADMM) [Boyd et al., 2011, Davis and Yin, 2016] without any simplification. The proposed algorithm simultaneously finds the support2 of the weights and updates them. After the support stabilizes, ALPS fixes the support and employs preconditioned conjugate gradient (PCG) [Nocedal and Wright, 1999, Section 5] to compute the optimal weights on the support. Our modified PCG leverages sparse matrix structure (arising from pruning) and GPU computation to solve large systems efficiently, providing a significant speed advantage over direct matrix inversion. An outline of our proposed optimizationbased framework $A L P S$ is given in Figure 1. Compared to previous heuristics, ALPS offers higher quality supports and weights, as demonstrated in Section 4.1. This improvement translates to a better performance of the pruned model compared to existing methods, particularly in the challenging high-sparsity regime. ", "page_idx": 1}, {"type": "image", "img_path": "0lBx844upd/tmp/6c98155c32a193c940100a7c19d3dd69836b3e73cbd2cd7680b6ca54e58b6d73.jpg", "img_caption": ["Figure 1: Overview of the proposed $A L P S$ algorithm. (Left) The pruning problem with a layerwise reconstruction objective and an $\\ell_{0}$ constraint on the weights (Section 3.1). (Middle) ADMM with a $\\rho$ -update scheme (Algorithm 1) is employed to determine high-quality support for the weight matrix W (Section 3.2). (Right) The optimization problem is restricted to the obtained support, and a modified PCG method (Algorithm 2) is used to solve for the optimal weight values within the support (Section 3.3). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Contributions. Our technical contributions are: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We introduce ALPS, a novel one-shot LLM pruning framework that formulates an $\\ell_{0}$ -constrained optimization problem with a layer-wise reconstruction objective. By extending the operator splitting technique (i.e., ADMM) to this non-convex, non-continuous problem, ALPS simultaneously finds a high-quality support and updates the weights on the support. This approach leads to improvements over state-of-the-art heuristics in terms of the pruning objective. Furthermore, we provide theoretical convergence guarantees for our proposed algorithm, which, to the best of our knowledge, is a novel convergence result for $\\ell_{0}$ -constrained problems. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2. We further enhance the performance of $A L P S$ through two techniques. First, we design a novel penalty parameter updating scheme that enables $A L P S$ to find better support and accelerates its convergence. Second, we propose a post-processing technique to further improve the performance of the pruned models\u2014we fix the support determined by ADMM and optimally solve the resulting quadratic problem using the PCG method. We utilize vectorization to solve the problem in a single pass and leverage GPU parallelism to further accelerate PCG. Our proposed method achieves a $20\\mathrm{x}{-}200\\mathrm{x}$ speedup compared to the vanilla backsolve approach. ", "page_idx": 2}, {"type": "text", "text": "3. ALPS substantially improves upon state-of-the-art methods for one-shot unstructured pruning of LLMs. For the LLaMA3-8B model with $70\\%$ sparsity, ALPS achieves a $29\\%$ reduction in test perplexity on the WikiText dataset and a $4\\%{-}13\\%$ improvement in performance on zero-shot benchmark evaluations. We also adapt $A L P S$ to the popular N:M sparsity format [Zhou et al., 2021] and observe a $3\\%{-10\\%}$ higher performance compared to existing methods. Our code is publicly available at: https://github.com/mazumder-lab/ALPS. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Network pruning. Network pruning is a well-established technique for reducing the complexity of deep neural networks by removing redundant weights [LeCun et al., 1989, Han et al., 2015]. Pruning methods can be classified based on the structure of the resulting sparse network and the training requirements. In terms of structure, pruning can be categorized into unstructured pruning, which removes individual weights [Han et al., 2015, Guo et al., 2016], and structured pruning, which removes entire structures such as channels, fliters, or attention heads [Lebedev and Lempitsky, 2016, Wen et al., 2016, Voita et al., 2019, El Halabi et al., 2022]. Unstructured pruning offers better flexibility and higher sparsity levels but requires specialized hardware for acceleration, while structured pruning is more hardware-friendly but may suffer from larger performance loss. Based on the training requirements, pruning methods can be classified into three categories: (i) one-shot pruning, which directly removes weights from a pre-trained model without further training [Gale et al., 2019, Frantar and Alistarh, 2022, Meng et al., 2024a,b], (ii) gradual pruning, which begins with a pre-trained model but alternates between pruning and fine-tuning via SGD to recover performance [Molchanov et al., 2016, Zhu and Gupta, 2017, Blalock et al., 2020, Kurtic et al., 2022], and (iii) training from scratch, where the model is trained from randomly initialized weights, and the sparse network structure is either determined before training or evolves during the training process, [Mocanu et al., 2018, Dettmers and Zettlemoyer, 2019, Evci et al., 2020, Kusupati et al., 2020, Chen et al., 2021]. In this paper, we focus on one-shot unstructured pruning. ", "page_idx": 2}, {"type": "text", "text": "Post-training unstructured pruning. Based on their pruning objectives, there are three types of post-training unstructured pruning methods: (i) Importance-based methods, which assign a score to each weight (e.g., its absolute value) to assess its significance and decide whether it should be eliminated [Han et al., 2015, Lee et al., 2018, Molchanov et al., 2019, Sun et al., 2023]. (ii) Second-order techniques, which consider a local quadratic approximation of the loss function around the pre-trained model and remove weights based on their influence on the loss [Hassibi and Stork, 1992, Singh and Alistarh, 2020, Yu et al., 2022, Benbaki et al., 2023]. These approaches employ the empirical Fisher information matrix to estimate the Hessian matrix efficiently. (iii) Layer-wise pruning algorithms, which adapt OBS [Hassibi and Stork, 1992] framework to the layer-wise reconstruction objective [Dong et al., 2017, Frantar and Alistarh, 2022, 2023]. These methods prune each layer separately to address the computational challenge of calculating the full Hessian required in OBS. This work considers layer-wise reconstruction error as the pruning objective. ", "page_idx": 2}, {"type": "text", "text": "Unstructured pruning in LLMs. While pruning algorithms designed for convolutional networks [Singh and Alistarh, 2020, Chen et al., 2020, Frantar and Alistarh, 2022] can be readily adapted to moderate-sized language models like BERT [Vaswani et al., 2017], pruning LLMs with billions of parameters presents distinct challenges. The immense model size and extensive datasets associated with LLMs render traditional pruning methods computationally infeasible [Ma et al., 2023]. SparseGPT [Frantar and Alistarh, 2023] utilizes partial weight updates and adaptive mask selection to mitigate the expensive Hessian computation, while Wanda [Sun et al., 2023] directly obtains a sparse LLM model using a criterion that considers the product of the absolute values of weights and their activations. DSnoT [Zhang et al., 2023] iteratively grow and prune the weight mask according to the change in reconstruction error achieved by each update. [Bo\u017ea, 2024] introduces an efficient approach to determine the optimal weights on a given support and extends this technique to develop heuristics for updating the support. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "ADMM in network pruning. The operator-splitting technique [Boyd et al., 2011, Davis and Yin, 2016] (also known as Alternating Direction Method of Multipliers, ADMM) is a well-known approach for solving composite optimization or optimization problems with coupled variables (or constraints), and has been used earlier in network pruning. Ye et al. [2018] applied ADMM to solve the original loss function under sparsity constraint, and Bo\u017ea [2024] used ADMM to solve a convex pruning problem with fixed support. Moreover, Zhang et al. [2018] employed ADMM to train deep neural networks under sparsity constraints, while Ye et al. [2019] utilized ADMM to perform concurrent adversarial training and weight pruning. Our proposed method differs significantly from previous methods in two key aspects: (i) ALPS solves the pruning problem with an $\\ell_{0}$ constraint at LLM scale, simultaneously optimizes over the weights and the sparsity pattern. (ii) We introduce a novel penalty parameter update scheme that ensures convergence both practically and theoretically. ", "page_idx": 3}, {"type": "text", "text": "3 ALPS: Effective LLM pruning in One-shot ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Problem formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A common approach in post-training unstructured pruning of LLMs is to decompose the full-model compression problem into layer-wise subproblems. The quality of the solution for each subproblem is assessed by measuring the $\\ell_{2}$ error between the output of the dense layer and that of the pruned one, given a set of input activations. ", "page_idx": 3}, {"type": "text", "text": "Formally, let $\\widehat{\\mathbf{W}}\\in\\mathbb{R}^{N_{i n}\\times N_{o u t}}$ denote the (dense) weight matrix of layer $\\ell$ , where $N_{i n}$ and $N_{o u t}$ denote the input and output dimension of the layer, respectively. Given a set of $N$ calibration samples, the input activations can be represented as $\\mathbf{X}\\in\\mathbb{R}^{N L\\times N_{i n}}$ , where $L$ is the sequence length. The goal of pruning is to find a sparse weight matrix W that minimizes the reconstruction error between the original and pruned layer outputs, while satisfying a target sparsity constraint. In addition, we add a ridge term that penalizes the distance between $\\mathbf{W}$ and $\\widehat{\\bf W}$ , preventing W from diverging too far from the original weights. This layer-wise pruning proble m can be formulated as an $\\ell_{0}$ -constrained optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{W}\\in\\mathbb{R}^{N_{i n}\\times N_{o u t}}}~\\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{W}\\|_{F}^{2}+\\lambda_{2}\\|\\widehat{\\mathbf{W}}-\\mathbf{W}\\|_{F}^{2}~~\\mathrm{s.t.}\\quad\\|\\mathbf{W}\\|_{0}\\leq k,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda_{2}\\geq0$ and $\\|\\cdot\\|_{0}$ denotes the $\\ell_{0}$ -(pseudo)norm, which counts the number of non-zero elements. ", "page_idx": 3}, {"type": "text", "text": "3.2 Operator-splitting for layer-wise pruning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Optimization of Problem (1) is quite challenging: we need to simultaneously find a support of W and a corresponding set of optimal weights (that minimize the objective restricted to the support). Notably, W may contain over 100 million parameters in the LLM setting, making (1) even more computationally demanding. To address this, we employ an operator-splitting technique [Boyd et al., 2011, Davis and Yin, 2016] (also known as ADMM), which decomposes the problem into two computationally \u2018friendlier\u2019 subproblems. Specifically, we reformulate problem (1) by introducing a copy $\\mathbf{D}$ of weight matrix W: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{W},\\mathbf{D}\\in\\mathbb{R}^{N_{i n}\\times N_{o u t}}}\\ \\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{W}\\|_{F}^{2}+\\lambda_{2}\\|\\widehat{\\mathbf{W}}-\\mathbf{W}\\|_{F}^{2}+\\infty\\cdot\\mathbf{1}_{\\|\\mathbf{D}\\|_{0}>k}\\quad\\mathrm{s.t.}\\quad\\mathbf{W}=\\mathbf{D},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the penalty function $\\mathbf{\\hat{\\Pi}}^{\\infty}\\cdot\\mathbf{1}_{\\|\\mathbf{D}\\|_{0}>k},\\mathbf{\\hat{\\Pi}}^{*}$ imposes the $\\ell_{0}$ constraint $\\|\\mathbf{D}\\|_{0}\\,\\leq\\,k$ by assigning a value of zero when this condition is met and infinity otherwise. This reformulation separates the objective function into two independent parts while coupling the variables $\\mathbf{W}$ and $\\mathbf{D}$ through the linear constraint $\\mathbf{W}=\\mathbf{D}$ . We consider the augmented Lagrangian function of this problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\therefore_{\\rho}(\\mathbf{W},\\mathbf{D},\\mathbf{V})=\\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{W}\\|_{F}^{2}+\\lambda_{2}\\|\\widehat{\\mathbf{W}}-\\mathbf{W}\\|_{F}^{2}+\\infty\\cdot\\mathbf{1}_{\\|\\mathbf{D}\\|_{0}>k}+\\langle\\mathbf{V},\\mathbf{W}-\\mathbf{D}\\rangle+\\frac{\\rho}{2}\\|\\mathbf{W}-\\mathbf{D}\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\rho>0$ is the quadratic penalty parameter. We minimize the augmented Lagrangian with respect to $\\mathbf{W}$ and $\\mathbf{D}$ alternatively, followed by a dual update. We get the following update at iteration $t$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}^{(t+1)}=\\operatorname*{arg\\,min}_{\\mathbf{W}}L_{\\rho}(\\mathbf{W},\\mathbf{D}^{(t)},\\mathbf{V}^{(t)})=(\\mathbf{H}+\\rho\\mathbf{I})^{-1}\\left(\\mathbf{H}\\widehat{\\mathbf{W}}-\\mathbf{V}^{(t)}+\\rho\\mathbf{D}^{(t)}\\right),}\\\\ &{\\mathbf{D}^{(t+1)}=\\operatorname*{arg\\,min}_{\\mathbf{D}}L_{\\rho}(\\mathbf{W}^{(t+1)},\\mathbf{D},\\mathbf{V}^{(t)})=P_{k}\\big(\\mathbf{W}^{(t+1)}+\\mathbf{V}^{(t)}/\\rho\\big),}\\\\ &{\\mathbf{V}^{(t+1)}=\\mathbf{V}^{(t)}+\\rho(\\mathbf{W}^{(t+1)}-\\mathbf{D}^{(t+1)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{H}=\\mathbf{X}^{\\top}\\mathbf{X}+\\lambda_{2}\\mathbf{I}.$ . Here, the W-update aims to minimize the objective by solving a system of equations, while the $\\mathbf{D}$ update enforces sparsity by using the projection operator $P_{k}(\\cdot)$ , which projects an input matrix onto the set of matrices with at most $k$ non-zero elements. The dual update on matrix $\\mathbf{V}$ ensures consistency between $\\mathbf{W}$ and $\\mathbf{D}$ . As the iterations (4) progress, our proposed method concurrently identifies the support of the weight matrix and updates the weights on the determined support. ", "page_idx": 4}, {"type": "text", "text": "$\\rho$ update scheme. In practice, we observe that the sequence of updates (4) and the resulting solution can be sensitive to the choice of the penalty parameter $\\rho$ . A small $\\rho$ leads to slow convergence due to large changes in the support of $\\mathbf{D}$ across iterations, while a large $\\rho$ may compromise solution quality though the support stabilizes early on. To balance support quality and convergence speed, we introduce a novel penalty parameter update scheme. Starting with a small $\\rho$ , we gradually increase it every few iterations, with the increase rate proportional to the change in the support of $\\mathbf{D}$ . The detailed $\\rho$ update scheme is provided in Appendix B.1. This scheme allows our algorithm to explore and find a good support when $\\rho$ is small and to converge rapidly as $\\rho$ grows, as demonstrated experimentally in Appendix B.2.1. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 outlines the proposed operator-splitting technique with the $\\rho$ update scheme. The convergence of Algorithm 1 is guaranteed by the following theorem, with its proof provided in Appendix A. We note that existing convergence results for operator-splitting type methods (e.g., ADMM) focus on convex or continuous problems [Hong et al., 2016, Wang et al., 2019]. However, our result guarantees the convergence on a non-convex, non-continuous $\\ell_{0}$ -constrained problem, which, to the best of our knowledge, is a novel convergence result for such a problem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $\\{\\mathbf{D}^{(t)}\\}_{t=0}^{\\infty}$ t=0 and W(t) t= be the sequences generated in Algorithm 1. Suppose the penalty parameter $\\{\\rho_{t}\\}_{t=1}^{\\infty}$ chosen in Algorithm $^{\\,l}$ satisfies $\\textstyle\\sum_{t=1}^{\\infty}1/\\rho_{t}<\\infty.$ . It then holds ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\|\\mathbf{D}^{(t+1)}-\\mathbf{D}^{(t)}\\|_{F},\\|\\mathbf{W}^{(t+1)}-\\mathbf{D}^{(t+1)}\\|_{F}\\right\\}\\leq C/\\rho_{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C$ is a constant depending on $\\mathbf{X}$ , $\\widehat{\\bf W}$ , $\\lambda_{2}$ , and $\\sum_{t=1}^{\\infty}1/\\rho_{t}$ . In particular, there exists a matrix $\\bar{\\bf D}$ such that $\\mathbf{D}^{(t)}\\rightarrow\\bar{\\mathbf{D}}$ and $\\mathbf{W}^{(t)}\\rightarrow\\bar{\\mathbf{D}}$ as $t\\to\\infty$ . ", "page_idx": 4}, {"type": "table", "img_path": "0lBx844upd/tmp/a16319fdbb95ea80baa8239fac9643efb55ac0880d3df3d50e1d30a6dcb3781a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Computational cost. The primary computational cost of Algorithm 1 arises from the W-update step in display (4), which involves solving a system of linear equations. In the update, the inverse of $\\mathbf{H}+\\rho\\mathbf{I}$ can be reused across iterations and needs to be updated when $\\rho$ changes. To avoid re-computing the inverse, we store the eigenvalue decomposition $\\mathbf{H}=\\mathbf{Q}\\mathbf{M}\\mathbf{Q}^{\\top}$ . For varying $\\rho$ values, the inverse can be efficiently calculated as $(\\mathbf{H}+\\rho\\mathbf{I})^{-1}=\\mathbf{Q}(\\mathbf{M}+\\rho\\mathbf{\\dot{I}})^{-1}\\mathbf{\\dot{Q}}^{\\top}$ , requiring only a single matrix-matrix multiplication. Additionally, the term $\\widehat{\\bf H}\\widehat{\\bf W}$ in W-update remains constant across iterations and can be pre-computed and stored. Thus, each iteration of update (4) requires at most two matrix-matrix multiplications, leading to a time complexity of $O(N_{i n}^{2}\\dot{N}_{o u t})$ . ", "page_idx": 4}, {"type": "text", "text": "Extension to other sparsity patterns. Algorithm 1 can be extended to support $N:M$ sparsity [Zhou et al., 2021, Hubara et al., 2021], a pattern in which a neural network has at most $N$ non-zero weights in each group of $M$ consecutive weights. This sparsity pattern enables inference time acceleration on specialized hardware like NVIDIA A100 GPUs. To accommodate $N:M$ sparsity, we modify the D-update step in (4) by replacing the projection operator $P_{k}(\\cdot)$ with a projection onto the set of matrices satisfying $N:M$ sparsity. This modification can be easily implemented by applying magnitude pruning [Zhou et al., 2021] to $\\mathbf{W}^{(t+1)}+\\mathbf{V}^{(t)}/\\rho$ . Our approach can be further generalized to handle other structured sparsity patterns, such as block sparsity [Gray et al., 2017] and row sparsity [Meng et al., 2024b]. Similar to the $N:M$ sparsity adaptation, this is achieved by modifying the $\\mathbf{D}$ -update step to project onto the set of matrices satisfying the desired sparsity pattern. Importantly, our convergence results and the refining procedure introduced in Section 3.3 remain applicable to these various sparsity patterns. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3 Efficiently refining weights after support stabilization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our proposed $\\rho$ -update technique enables Algorithm 1 to search for a high-quality support when $\\rho$ is small, and the support stabilizes quickly as $\\rho$ increases. However, once the support stabilizes, the convergence rate of Algorithm 1 becomes slow in practice. To accelerate the optimization process on the support, we employ a Preconditioned Conjugate Gradient (PCG) [Nocedal and Wright, 1999, Section 5] method with GPU parallelism and vectorization for efficient computation. ", "page_idx": 5}, {"type": "text", "text": "Formally, we introduce a post-processing technique that fixes the support $\\boldsymbol{S}$ of the current solution W and refines the solution within this support, leading to the following problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{W}\\in\\mathbb{R}^{N_{i n}\\times N_{o u t}}}\\ \\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{W}\\|_{F}^{2}+\\lambda_{2}\\|\\widehat{\\mathbf{W}}-\\mathbf{W}\\|_{F}^{2}\\quad\\mathrm{s.t.~}\\operatorname{Supp}(\\mathbf{W})\\subset\\mathcal{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(6) decomposes into separate least squares problems across the columns of W. However, as illustrated in Figure 1 (Middle), the supports of the columns of $\\mathbf{W}$ are different. Using direct matrix inversion (backsolve) to solve these problems would involve solving $N_{o u t}$ linear equations, each requiring the inversion of a submatrix of $\\mathbf{H}=\\mathbf{X}^{\\top}\\mathbf{X}+\\lambda_{2}\\mathbf{I}$ . Since the submatrices under consideration vary across different columns, parallelization is not straightforward, and we must solve $N_{o u t}$ different linear equations, each with size $O(N_{i n})$ . In LLMs, where $N_{o u t}$ and $N_{i n}$ are of the order $10^{4}$ , this would result in a significant computational expense. We present a workaround as discussed next. ", "page_idx": 5}, {"type": "text", "text": "To efficiently solve problem (6), we propose using the Preconditioned Conjugate Gradient (PCG) method, a high-performance numerical linear algebra technique for approximately solving systems of equations through repeated matrix-matrix multiplications. We further enhance PCG\u2019s performance by introducing two novel acceleration strategies. First, instead of solving for each column of W separately, we solve the entire problem in a single pass by directly solving the linear equation $\\mathbf{H}\\mathbf{W}=\\mathbf{H}\\widehat{\\mathbf{W}}$ using PCG and projecting W onto the given support $\\boldsymbol{S}$ in each iteration (Algorithm 2, line 8). We leverage vectorization to significantly enhance the speed. Second, we perform the matrix-matrix multiplications involved in PCG on the GPU, further utilizing GPU acceleration to expedite the computations. Algorithm 2 provides the detailed steps for the PCG method, and Figure1 offers an overview of our proposed ALPS method. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 PCG with vectorization for solving problem (6) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: Support $\\boldsymbol{S}$ , pre-conditioner ${\\mathbf M}=\\mathrm{Diag}({\\mathbf H})$ , initial solution $\\mathbf{W_{0}}$   \n1: Set $\\mathbf{R}_{0}:=\\mathbf{H}(\\widehat{\\mathbf{W}}-\\mathbf{W}_{0})$   \n2: Project $\\mathbf{R}_{0}$ onto the support $\\boldsymbol{S}$ by setting all elements outside the support to zero.   \n3: Set ${\\bf Z}_{0}={\\bf M}^{-1}{\\bf R}_{0}$ and $\\mathbf{P}_{0}=\\mathbf{Z}_{0}$   \n4: for $t=0,1,..\\,.$ . do   \n5: $\\begin{array}{r l}&{\\boldsymbol{\\alpha}_{t}=\\operatorname{Tr}(\\mathbf{R}_{t}^{\\top}\\bar{\\mathbf{Z}_{t}})/\\operatorname{Tr}(\\mathbf{P}_{t}^{\\top}\\mathbf{H}\\mathbf{P}_{t})}\\\\ &{\\mathbf{W}_{t+1}=\\mathbf{W}_{t}+\\boldsymbol{\\alpha}_{t}\\mathbf{P}_{t}}\\\\ &{\\mathbf{R}_{t+1}=\\mathbf{R}_{t}-\\boldsymbol{\\alpha}_{t}\\mathbf{H}\\mathbf{P}_{t}}\\end{array}$   \n6:   \n7:   \n8: Project $\\mathbf{R}_{t+1}$ onto the support $\\boldsymbol{S}$ by setting all elements outside the support to zero.   \n9: $\\mathbf{Z}_{t+1}=\\mathbf{M}^{-1}\\mathbf{R}_{t+1}$   \n10: if $\\mathbf{R}_{t+1}$ is sufficiently small then   \n11: break   \n12: end if   \n13: $\\begin{array}{r l}&{{\\boldsymbol{\\beta}}_{t}=\\mathrm{Tr}(\\mathbf{R}_{t+1}^{\\top}\\mathbf{Z}_{t+1})/\\operatorname{Tr}(\\mathbf{R}_{t}^{\\top}\\mathbf{Z}_{t})}\\\\ &{\\mathbf{P}_{t+1}:=\\mathbf{Z}_{t+1}+{\\boldsymbol{\\beta}}_{t}\\mathbf{P}_{t}}\\end{array}$   \n14:   \n15: end for ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section compares our proposed framework, $A L P S$ , with state-of-the-art unstructured pruning methods for LLMs. Detailed information on the experimental setup and reproducibility is provided in Appendix B.1, while additional results are presented in Appendix B.2. ", "page_idx": 6}, {"type": "text", "text": "Models and datasets. We evaluate the performance of ALPS on the OPT model family [Zhang et al., 2022b] with sizes ranging from 1.3 billion to 30 billion parameters, the LLaMA2 model family [Touvron et al., 2023] with 7 billion and 13 billion parameters, and the LLaMA3 model [Dubey et al., 2024] with 8 billion parameters. Following the approach of Frantar and Alistarh, 2023, we use 128 segments of 2048 tokens each, randomly selected from the first shard of the C4 dataset [Raffel et al., 2020], as calibration data. We assess the performance using perplexity and zero-shot evaluation benchmarks, with perplexity calculated according to the procedure described by HuggingFace [Per, 2022], using full stride. The test sets of raw-WikiText2 [Merity et al., 2017], PTB [Marcus et al., 1994], and a subset of the C4 validation data, which are popular benchmarks in LLM pruning literature [Yao et al., 2022, Xiao et al., 2023, Meng et al., 2024b], are used for evaluation. Additionally, we consider five zero-shot tasks: MMLU [Hendrycks et al., 2021], PIQA [Bisk et al., 2020], LAMBADA [Paperno et al., 2016], ARC-Easy and ARC-Challenge [Clark et al., 2018]. ", "page_idx": 6}, {"type": "text", "text": "Competing methods. We compare ALPS with several one-shot pruning methods for LLMs, including (i) Magnitude Pruning (MP, [Han et al., 2015]), (ii) SparseGPT [Frantar and Alistarh, 2023], (iii) Wanda Sun et al. [2023], and (iv) DSnoT [Zhang et al., 2023]. ", "page_idx": 6}, {"type": "text", "text": "4.1 Reconstruction error on a single layer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first evaluate the performance of our proposed $A L P S$ framework on a single layer. Specifically, we prune a linear layer in the OPT-13B model with input and output dimensions of 5120 to various sparsity levels and compute the relative reconstruction error of the pruned weight W using $\\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{W}\\|_{F}^{2}/\\|\\mathbf{X}\\widehat{\\mathbf{W}}\\|_{F}^{2}$ . The results are shown in Figure 2. As demonstrated, $A L P S$ achieves significantly lower reconstruction errors compared to other methods, especially at high sparsity levels. For instance, at a sparsity level of $0.8,A L P S$ yields a $7.6\\%$ relative reconstruction error, while SparseGPT shows a $12\\%$ error, and other methods exceed $20\\%$ . As demonstrated in Sections 4.2 and 4.3, our method\u2019s superior ability to approximate the dense model\u2019s output at each layer translates to much better performance in the pruned model. ", "page_idx": 6}, {"type": "text", "text": "We attribute the superior performance of ALPS in solving the reconstruction problem at each layer to two key aspects: (i) Algorithm 1 obtains a high-quality support by directly opti", "page_idx": 6}, {"type": "image", "img_path": "0lBx844upd/tmp/ca2d403b13523cfbf5a6dc8a35f825ed5d4a12db51ba638957395c28bfdad6d5.jpg", "img_caption": ["Figure 2: Performance analysis of pruning the \u201cself_attn.k_proj\u201d layer in the first block of the OPT-13B model at various sparsity levels. The plot shows the relative reconstruction error of pruned weights, comparing different pruning methods. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "mizing for an optimal subset of weights that contribute the most to recovering the dense model\u2019s output (ii) The PCG method in Algorithm 2 efficiently solves the reconstruction problem on a fixed support, further reducing the reconstruction error. To verify these claims, we conducted the following two ablation studies. ", "page_idx": 6}, {"type": "text", "text": "Firstly, we compare the quality of the support determined by various pruning methods. For each method, we prune the layer to different sparsity levels and fix the support of the weights matrix provided by the method. We then solve the post-processing problem (6) with this support to optimality and compute the relative reconstruction error of the resulting weights. This approach ensures that the reconstruction error depends solely on the quality of the support. Table 1 (left) presents the performance of each method. As shown, the support determined by $A L P S$ yields $\\bar{20\\%}\\sim40\\%$ lower reconstruction error compared to other methods, demonstrating its effectiveness in finding high-quality supports. ", "page_idx": 6}, {"type": "table", "img_path": "0lBx844upd/tmp/34389dac04d7ab790f741e54f1b5b765ea8690cb43a99c50845d5ebe81c5c9ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Performance analysis of pruning the \u201cself_attn.k_proj\u201d layer in the first block of the OPT-13B model at various sparsity levels. (Left) Relative reconstruction error $\\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{W}\\|_{F}^{2}/\\|\\mathbf{X}\\widehat{\\mathbf{W}}\\|_{F}^{2}$ of the optimal weights W constrained to the support determined by each pruning method. (Right) Comparison of time and reconstruction error for three scenarios, all using magnitude pruning to determine the support and then: (i) no post-processing (w/o pp.), (ii) refining the weights with $A L P S$ , and (iii) refining the weights optimally with backsolve. ", "page_idx": 7}, {"type": "text", "text": "We then evaluate the effectiveness of our proposed post-processing method in finding optimal weights on a given support. We first apply magnitude pruning (MP) to determine the support of the weights and then consider three scenarios: (i) without post-processing (w/o pp.), (ii) using Algorithm 2 to solve (6) to refine the weights on the support $(A L P S)$ , and (iii) using PyTorch\u2019s torch.linalg.solve function to solve (6) to optimality (Backsolve). We assessed both the relative reconstruction error and the computational time for each scenario, with results presented in Table 1 (right). The findings demonstrate that the post-processing procedure significantly lowers the reconstruction error. Notably, our PCG method achieves errors comparable to the optimal solution but is $20\\mathrm{x}{-}200\\mathrm{x}$ faster, underscoring the efficiency and effectiveness of our approach. ", "page_idx": 7}, {"type": "text", "text": "4.2 Pruning OPT and LLaMA models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section focuses on pruning OPT models and LLaMA models to various sparsity levels and evaluating the performance of the pruned models using perplexity and zero-shot benchmarks. The performance of the pruned LLaMA3-8B model at different sparsity levels on the WikiText2 and PIQA datasets is presented in Figure 3. Table 2 showcases the performance of OPT models with $70\\%$ sparsity on various datasets. Additional results on different models, sparsity levels, and datasets are provided in Appendix B.2.5. ", "page_idx": 7}, {"type": "table", "img_path": "0lBx844upd/tmp/7f1b024bb72a168dfbf6d8baa713af8439f8c63dae3cf48679d934fa3118b02d.jpg", "table_caption": [], "table_footnote": ["Table 2: Performance analysis for one-shot unstructured pruning of OPT models $1.3\\mathbf{B}\\sim30\\mathbf{B})$ ) at $70\\%$ sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, $\\downarrow$ denotes lower values corresponding to better performance, and $\\uparrow$ denotes higher values corresponding to better performance. "], "page_idx": 7}, {"type": "image", "img_path": "0lBx844upd/tmp/fd9ddc175b4e10c65616c6a72400969ebf2557f2782bfb5ea7c854ad96a330ed.jpg", "img_caption": ["Figure 3: Performance analysis for one-shot unstructured pruning of LLaMA3-8B model at various sparsity levels on two datasets: WikiText2 (Left) and PIQA (Right). We run each method five times and plot the shaded region as the area between the mean (solid line) and two standard deviations above and below the mean. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 3 demonstrates that ALPS outperforms other competitors when sparsity levels exceed $50\\%$ , and the performance gap between $A L P S$ and other methods widens as the sparsity level increases. For instance, ALPS achieves a $60\\%$ perplexity reduction on the WikiText2 dataset compared to other methods at $80\\%$ sparsity level. This observation aligns with our findings in Section 4.1, confirming that $A L P S$ \u2019s highly advanced optimization method in solving layer-wise reconstruction problems enables it to better preserve performance at medium-to-high sparsity levels compared to other methods. Table 2 further validates this fact, showing that ALPS outperforms other methods by a large margin across all models on all criteria. This suggests the superiority of $A L P S$ in pruning models at medium-to-high sparsity levels. ", "page_idx": 8}, {"type": "text", "text": "4.3 N:M sparsity ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further assess ALPS\u2019s performance on $N:M$ sparsity patterns, with Table 3 listing the results for pruning OPT-30B and LLaMA2-13B models at 2:4 and 4:8 sparse patterns (see Appendix B.2.5 for other models). $A L P S$ outperforms other methods on most datasets, achieving larger performance improvements in $N:M$ pruning compared to unstructured pruning at the same sparsity level. This is due to the higher complexity of the $N:M$ sparsity pruning problem, which $A L P S$ , as a highly advanced optimization algorithm, can handle more effectively than competing heuristics. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present ALPS, an efficient optimization-based framework for one-shot unstructured LLM pruning. ALPS employs the operator splitting technique to effectively solve the $\\ell_{0}$ -constrained layer-wise pruning problem. To enhance the performance of our algorithm, we introduce a novel penalty parameter updating scheme and a post-processing procedure using PCG with vectorization/GPU parallelism that takes into account problem-structure. We also establish novel convergence guarantees for our algorithm. ALPS can efficiently perform high-quality pruning of LLMs at scale. Our experiments confirm that ALPS outperforms existing pruning methods in terms of both the pruning objective and the performance of the pruned model. Future work will consider extending $A L P S$ to incorporate structured pruning constraints and quantization to get a better understanding of the strengths and scope of our optimization-based approach. ", "page_idx": 8}, {"type": "table", "img_path": "0lBx844upd/tmp/a22ea503f4a2984918401c6527d2fa0764da7e3f7cc5943f221fc422c7968dab.jpg", "table_caption": [], "table_footnote": ["Table 3: Performance analysis for one-shot pruning of OPT-30B and LLaMA2-13B at $2:4$ and $4:8$ sparsity patterns. We run each method five times and report the mean and standard deviation of each performance criterion. Here, $\\downarrow$ denotes lower values correspond to better performance, and $\\uparrow$ denotes higher values correspond to better performance. "], "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported in part by grants from ONR (N000142112841 and N000142212665). We acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC resources that have contributed to the research results reported within this paper. Additionally, we thank Google for providing us with Google Cloud Credits. We thank Shibal Ibrahim, Mehdi Makni, and Gabriel Afriat for their helpful discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Perplexity of fixed-length models, 2022. URL https://huggingface.co/docs/transformers/ perplexity.   \nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nKayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya Keerthi, and Rahul Mazumder. Quantease: Optimization-based quantization for language models\u2013an efficient and intuitive algorithm. arXiv preprint arXiv:2309.01885, 2023.   \nRiade Benbaki, Wenyu Chen, Xiang Meng, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, and Rahul Mazumder. Fast as chita: Neural network pruning with combinatorial optimization. arXiv preprint arXiv:2302.14623, 2023.   \nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.   \nDavis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural network pruning? Proceedings of machine learning and systems, 2:129\u2013146, 2020.   \nStephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends\u00ae in Machine learning, 3(1):1\u2013122, 2011.   \nVladim\u00edr Bo\u017ea. Fast and optimal weight update for pruned large language models. arXiv preprint arXiv:2401.02938, 2024.   \nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural information processing systems, 33:15834\u201315846, 2020.   \nTianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning framework. Advances in Neural Information Processing Systems, 34:19637\u201319651, 2021.   \nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \nDamek Davis and Wotao Yin. Convergence rate analysis of several splitting schemes. Splitting methods in communication, imaging, science, and engineering, pages 115\u2013163, 2016.   \nTim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019.   \nTim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023.   \nXin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in neural information processing systems, 30, 2017.   \nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \nMarwa El Halabi, Suraj Srinivas, and Simon Lacoste-Julien. Data-efficient structured pruning via submodular optimization. Advances in Neural Information Processing Systems, 35:36613\u201336626, 2022.   \nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International conference on machine learning, pages 2943\u20132952. PMLR, 2020.   \nElias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:4475\u20134488, 2022.   \nElias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 10323\u201310337. PMLR, 2023.   \nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.   \nScott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. arXiv preprint arXiv:1711.09224, 3(2):2, 2017.   \nYiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. Advances in neural information processing systems, 29, 2016.   \nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.   \nBabak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. Advances in neural information processing systems, 5, 1992.   \nY. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 1398\u20131406, Los Alamitos, CA, USA, oct 2017. IEEE Computer Society. doi: 10.1109/ICCV.2017.155. URL https: //doi.ieeecomputersociety.org/10.1109/ICCV.2017.155.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.   \nMingyi Hong, Zhi-Quan Luo, and Meisam Razaviyayn. Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems. SIAM Journal on Optimization, 26(1): 337\u2013364, 2016.   \nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nItay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel Soudry. Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks. Advances in neural information processing systems, 34:21099\u201321111, 2021.   \nEldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning for large language models. arXiv preprint arXiv:2203.07259, 2022.   \nAditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544\u20135555. PMLR, 2020.   \nVadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2554\u20132564, 2016.   \nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989.   \nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018.   \nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activationaware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.   \nZhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018.   \nJian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 5068\u20135076. IEEE Computer Society, 2017. doi: 10.1109/ICCV. 2017.541.   \nXinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:21702\u201321720, 2023.   \nMitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure. In Proceedings of the Workshop on Human Language Technology, HLT \u201994, page 114\u2013119, USA, 1994. Association for Computational Linguistics. ISBN 1558603573. doi: 10.3115/1075812. 1075835. URL https://doi.org/10.3115/1075812.1075835.   \nXiang Meng, Wenyu Chen, Riade Benbaki, and Rahul Mazumder. Falcon: Flop-aware combinatorial optimization for neural network pruning. In International Conference on Artificial Intelligence and Statistics, pages 4384\u20134392. PMLR, 2024a.   \nXiang Meng, Shibal Ibrahim, Kayhan Behdin, Hussein Hazimeh, Natalia Ponomareva, and Rahul Mazumder. Osscar: One-shot structured pruning in vision and language models with combinatorial optimization. arXiv preprint arXiv:2403.12983, 2024b.   \nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https: //openreview.net/forum?id=Byj72udxe.   \nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):2383, 2018.   \nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.   \nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11264\u201311272, 2019.   \nJorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.   \nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.   \nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.   \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.   \nPranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024.   \nSidak Pal Singh and Dan Alistarh. Woodfisher: Efficient second-order approximation for neural network compression. Advances in Neural Information Processing Systems, 33:18098\u201318109, 2020.   \nMingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019.   \nYu Wang, Wotao Yin, and Jinshan Zeng. Global convergence of admm in nonconvex nonsmooth optimization. Journal of Scientific Computing, 78:29\u201363, 2019.   \nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.   \nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016.   \nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate and efficient post-training quantization for large language models. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 27168\u201327183. Curran Associates, Inc., 2022.   \nShaokai Ye, Tianyun Zhang, Kaiqi Zhang, Jiayu Li, Kaidi Xu, Yunfei Yang, Fuxun Yu, Jian Tang, Makan Fardad, Sijia Liu, et al. Progressive weight pruning of deep neural networks using admm. arXiv preprint arXiv:1810.07378, 2018.   \nShaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, and Xue Lin. Adversarial robustness vs. model compression, or both? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 111\u2013120, 2019.   \nXin Yu, Thiago Serra, Srikumar Ramalingam, and Shandian Zhe. The combinatorial brain surgeon: pruning weights that cancel one another in neural networks. In International Conference on Machine Learning, pages 25668\u201325683. PMLR, 2022.   \nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022a.   \nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022b.   \nTianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, and Yanzhi Wang. A systematic dnn weight pruning framework using alternating direction method of multipliers. In Proceedings of the European conference on computer vision (ECCV), pages 184\u2013199, 2018.   \nYuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. Dynamic sparse no training: Training-free fine-tuning for sparse llms. arXiv preprint arXiv:2310.08915, 2023.   \nAojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning n: m fine-grained structured sparse neural networks from scratch. arXiv preprint arXiv:2102.04010, 2021.   \nMichael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proofs of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. For the sake of conciseness, throughout the proof, we denote $\\mathbf{H}=\\mathbf{X}^{\\top}\\mathbf{X}+\\lambda_{2}\\mathbf{I}$ and $\\mathbf{G}=$ $(\\mathbf{X}^{\\top}\\mathbf{X}+\\lambda_{2}\\mathbf{I})\\,\\widehat{\\mathbf{W}}$ . To establish the theorem, we first present the following two lemmas. The proofs of these two lemmas are given in Section A.1 and A.2, respectively. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. Let $\\left\\{{\\bf D}^{(t)}\\right\\}_{t=0}^{\\infty}$ and $\\left\\{\\mathbf{V}^{(t)}\\right\\}_{t=0}^{\\infty}$ be the sequence generated in Algorithm 1. Then for any $t\\geq0$ , it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{V}^{(t+1)}\\|_{F}\\leq\\|\\mathbf{G}-\\mathbf{H}\\mathbf{D}^{(t)}\\|_{F}+\\frac{\\|\\mathbf{H}\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{D}^{(t+1)}-\\mathbf{D}^{(t)}\\|_{F}\\leq\\frac{2}{\\rho_{t}}\\left(\\|\\mathbf{G}-\\mathbf{H}\\mathbf{D}^{(t)}\\|_{F}+\\frac{\\|\\mathbf{H}\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Let $\\left\\{\\mathbf{D}^{(t)}\\right\\}_{t=0}^{\\infty},\\,\\left\\{\\mathbf{W}^{(t)}\\right\\}_{t=0}^{\\infty}$ and $\\left\\{\\mathbf{V}^{(t)}\\right\\}_{t=0}^{\\infty}$ be the sequence generated in Algorithm $^{\\,l}$ . Suppose $\\{\\rho_{t}\\}_{t=0}^{\\infty}$ is non-decreasing. Then for any $t\\geq0$ , it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{D}^{(t)}\\|_{F}+\\frac{\\|\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}}\\le\\left[\\prod_{s=0}^{t-1}\\left(1+\\frac{3\\|\\mathbf{H}\\|_{2}}{\\rho_{s}}\\right)\\right]\\cdot\\left(\\|\\mathbf{D}^{(0)}\\|_{F}+\\frac{\\|\\mathbf{V}^{(0)}\\|_{F}}{\\rho_{0}}+\\sum_{s=0}^{t-1}\\frac{3\\|\\mathbf{G}\\|_{F}}{\\rho_{s}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Returning to the proof of the main theorem, combining Lemma 2 with the initialization of Algorithm 1 gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\mathbf{D}^{(t)}\\|_{F}+\\frac{\\|\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}}\\leq\\left[\\prod_{s=0}^{t-1}\\left(1+\\frac{3\\|\\mathbf{H}\\|_{2}}{\\rho_{s}}\\right)\\right]\\cdot\\left(\\|\\mathbf{D}^{(0)}\\|_{F}+\\frac{\\|\\mathbf{V}^{(0)}\\|_{F}}{\\rho_{0}}+\\sum_{s=0}^{t-1}\\frac{3\\|\\mathbf{G}\\|_{F}}{\\rho_{s}}\\right)}\\\\ {\\displaystyle\\leq\\exp\\left(3\\|\\mathbf{H}\\|_{2}\\sum_{s=0}^{\\infty}\\frac{1}{\\rho_{s}}\\right)\\cdot\\left(\\|\\mathbf{G}\\|_{F}+3\\|\\mathbf{G}\\|_{F}\\sum_{s=0}^{\\infty}\\frac{1}{\\rho_{s}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\mathbf{X},\\widehat{\\mathbf{W}},\\rho_{0},t_{u},\\widehat{\\tau}):=2\\|\\mathbf{G}\\|_{F}+2\\|\\mathbf{H}\\|_{2}\\left(\\exp\\left(3\\|\\mathbf{H}\\|_{2}\\sum_{s=0}^{\\infty}\\frac{1}{\\rho_{s}}\\right)\\cdot\\left(\\|\\mathbf{G}\\|_{F}+3\\|\\mathbf{G}\\|_{F}\\sum_{s=0}^{\\infty}\\frac{1}{\\rho_{s}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "be the constant depending on $\\mathbf{X},\\widehat{\\mathbf{W}}$ and $\\sum_{s=0}^{\\infty}1/\\rho_{s}$ . Lemma 1 together with (10) leads to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\mathbf{V}}^{(t+1)}\\|_{F}\\leq\\|{\\mathbf{G}}-{\\mathbf{H}}{\\mathbf{D}}^{(t)}\\|_{F}+\\frac{\\|{\\mathbf{H}}{\\mathbf{V}}^{(t)}\\|_{F}}{\\rho_{t}}}\\\\ &{\\qquad\\qquad\\leq\\|{\\mathbf{G}}\\|_{F}+\\|{\\mathbf{H}}\\|_{2}\\left(\\|{\\mathbf{D}}^{(t)}\\|_{F}+\\frac{\\|{\\mathbf{V}}^{(t)}\\|_{F}}{\\rho_{t}}\\right)\\leq\\frac{1}{2}C({\\mathbf{X}},\\widehat{\\mathbf{W}},\\rho_{0},t_{u},\\widehat{\\tau})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{D}^{(t+1)}-\\mathbf{D}^{(t)}\\|_{F}\\leq\\frac{2}{\\rho_{t}}\\left(\\|\\mathbf{G}-\\mathbf{H}\\mathbf{D}^{(t)}\\|_{F}+\\frac{\\|\\mathbf{H}\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}}\\right)\\leq\\frac{C(\\mathbf{X},\\widehat{\\mathbf{W}},\\rho_{0},t_{u},\\widehat{\\tau})}{\\rho_{t}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It then follows from $\\mathbf{W}^{(t+1)}-\\mathbf{D}^{(t+1)}=(\\mathbf{V}^{(t+1)}-\\mathbf{V}^{(t)})/\\rho_{t}$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}^{(t+1)}-\\mathbf{D}^{(t+1)}\\|_{F}\\leq\\frac{\\|\\mathbf{V}^{(t+1)}\\|_{F}+\\|\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}}\\leq\\frac{C(\\mathbf{X},\\widehat{\\mathbf{W}},\\rho_{0},t_{u},\\widehat{\\tau})}{\\rho_{t}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we prove the desired inequality. Since $\\textstyle\\sum_{s=0}^{\\infty}1/\\rho_{s}<\\infty,\\{\\mathbf D\\}_{t=0}^{\\infty}$ is a Cauchy sequence, and therefore there exists a matrix $\\bar{\\mathbf{D}}$ such that $\\mathbf{D}^{(t)}\\rightarrow\\bar{\\mathbf{D}}$ . It follows from $\\|\\mathbf{W}^{(t+1)}-\\mathbf{D}^{(t+1)}\\|_{F}\\to0$ that $\\mathbf{W}^{(t)}\\rightarrow\\bar{\\mathbf{D}}$ . The proof is completed. ", "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. According to the $\\mathbf{W}$ \u2212update rule in (4), it holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\mathbf{W}}^{(t+1)}-{\\mathbf{D}}^{(t)}+\\frac{{\\mathbf{V}}^{(t)}}{\\rho^{(t)}}=({\\mathbf{H}}+\\rho_{t}{\\mathbf{I}})^{-1}({\\mathbf{G}}-{\\mathbf{V}}^{(t)}+\\rho_{t}{\\mathbf{D}}^{(t)})-{\\mathbf{D}}^{(t)}+\\frac{{\\mathbf{V}}^{(t)}}{\\rho^{(t)}}}&{}\\\\ &{\\qquad\\qquad=\\left(({\\mathbf{H}}+\\rho_{t}{\\mathbf{I}})^{-1}\\rho_{t}-{\\mathbf{I}}\\right){\\mathbf{D}}^{(t)}+({\\mathbf{H}}+\\rho_{t}{\\mathbf{I}})^{-1}({\\mathbf{G}}-{\\mathbf{V}}^{(t)})+\\frac{{\\mathbf{V}}^{(t)}}{\\rho_{t}}}\\\\ &{\\qquad\\qquad=-\\frac{1}{\\rho_{t}}\\left({\\mathbf{I}}+\\frac{{\\mathbf{H}}}{\\rho_{t}}\\right)^{-1}{\\mathbf{H}}{\\mathbf{D}}^{(t)}+\\frac{1}{\\rho_{t}}\\left({\\mathbf{I}}+\\frac{{\\mathbf{H}}}{\\rho_{t}}\\right)^{-1}({\\mathbf{G}}-{\\mathbf{V}}^{(t)})+\\frac{{\\mathbf{V}}^{(t)}}{\\rho_{t}}}\\\\ &{\\qquad\\qquad=\\frac{1}{\\rho_{t}}\\left({\\mathbf{I}}+\\frac{{\\mathbf{H}}}{\\rho_{t}}\\right)^{-1}({\\mathbf{G}}-{\\mathbf{H}}{\\mathbf{D}}^{(t)})+\\frac{1}{\\rho_{t}}\\left[{\\mathbf{I}}-\\left({\\mathbf{I}}+\\frac{{\\mathbf{H}}}{\\rho_{t}}\\right)^{-1}\\right]{\\mathbf{V}}^{(t)}}\\\\ &{\\qquad\\qquad=\\frac{1}{\\rho_{t}}\\left({\\mathbf{I}}+\\frac{{\\mathbf{H}}}{\\rho_{t}}\\right)^{-1}\\left({\\mathbf{G}}-{\\mathbf{H}}{\\mathbf{D}}^{(t)}+\\frac{{\\mathbf{H}}{\\mathbf{V}}^{(t)}}{\\rho_{t}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|{\\mathbf{W}^{(t+1)}-\\mathbf{D}^{(t)}+\\frac{\\mathbf{V}^{(t)}}{\\rho^{(t)}}}\\right\\|_{F}\\le\\frac{1}{\\rho_{t}}\\left\\|\\left(\\mathbf{I}+\\frac{\\mathbf{H}}{\\rho_{t}}\\right)^{-1}\\right\\|_{2}\\left\\|\\mathbf{G}-\\mathbf{H}{\\mathbf{D}^{(t)}}+\\frac{\\mathbf{H}\\mathbf{V}^{(t)}}{\\rho_{t}}\\right\\|_{F}}&{}\\\\ {\\le\\displaystyle\\frac{1}{\\rho_{t}}\\left\\|\\mathbf{G}-\\mathbf{H}{\\mathbf{D}^{(t)}}+\\frac{\\mathbf{H}\\mathbf{V}^{(t)}}{\\rho_{t}}\\right\\|_{F}}&{}\\\\ {\\le\\displaystyle\\frac{1}{\\rho_{t}}\\left(\\|\\mathbf{G}-\\mathbf{H}{\\mathbf{D}^{(t)}}\\|_{F}+\\frac{\\|\\mathbf{H}\\mathbf{V}^{(t)}\\|}{\\rho_{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Denote $\\mathcal{\\widetilde{Z}}:=\\{(i,j)_{.}\\in_{.}[N_{i n}]\\times[N_{o u t}]\\ |\\ \\mathbf{D}_{i j}^{(t)}=0\\}$ . It follows from the ${\\bf D}-$ update rule and the definitio n of the projection operator that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{D}^{(t+1)}-\\mathbf{W}^{(t+1)}-\\frac{\\mathbf{V}^{(t)}}{\\rho_{t}}\\right\\|_{F}^{2}=\\underset{\\frac{T\\subset[N_{i}]\\times[N_{o u t}]}{\\mathbb{Z}\\subset[N_{i n}]\\times[N_{o u t}-k}(i,j)\\in\\mathbb{Z}}{\\operatorname*{min}}\\left(\\mathbf{W}^{(t+1)}+\\frac{\\mathbf{V}^{(t)}}{\\rho_{t}}\\right)_{i,j}^{2}}\\\\ &{\\leq\\underset{(i,j)\\in\\tilde{\\mathbb{Z}}}{\\sum}\\left(\\mathbf{W}^{(t+1)}+\\frac{\\mathbf{V}^{(t)}}{\\rho_{t}}\\right)_{i,j}^{2}=\\underset{(i,j)\\in\\tilde{\\mathbb{Z}}}{\\sum}\\left(\\mathbf{W}^{(t+1)}-\\mathbf{D}^{(t)}+\\frac{\\mathbf{V}^{(t)}}{\\rho_{t}}\\right)_{i,j}^{2}}\\\\ &{\\leq\\left\\|\\mathbf{W}^{(t+1)}-\\mathbf{D}^{(t)}+\\frac{\\mathbf{V}^{(t)}}{\\rho_{t}}\\right\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Together with (16), we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bigg\\|\\mathbf{D}^{(t+1)}-\\mathbf{W}^{(t+1)}-\\frac{\\mathbf{V}^{(t)}}{\\rho_{t}}\\bigg\\|_{F}\\leq\\frac{1}{\\rho_{t}}\\left(\\|\\mathbf{G}-\\mathbf{H}\\mathbf{D}^{(t)}\\|_{F}+\\frac{\\|\\mathbf{H}\\mathbf{V}^{(t)}\\|}{\\rho_{t}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It then follows from the $\\mathbf{V}$ \u2212update rule that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathbf{V}^{(t+1)}\\|_{F}}{\\rho_{t}}=\\left\\|\\mathbf{D}^{(t+1)}-\\mathbf{W}^{(t+1)}-\\frac{\\mathbf{V}^{(t)}}{\\rho_{t}}\\right\\|_{F}\\leq\\frac{1}{\\rho_{t}}\\left(\\|\\mathbf{G}-\\mathbf{H}\\mathbf{D}^{(t)}\\|_{F}+\\frac{\\|\\mathbf{H}\\mathbf{V}^{(t)}\\|}{\\rho_{t}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This establishes the inequality (7). Furthermore, by summing up (16) and (18) and applying the triangle inequality, we verify the inequality (8). \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. It follows from Lemma 1 that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\bf V}^{(t+1)}\\|_{F}\\leq\\|{\\bf G}-{\\bf H}{\\bf D}^{(t)}\\|_{F}+\\frac{\\|{\\bf H}{\\bf V}^{(t)}\\|_{F}}{\\rho_{t}}}\\\\ &{\\qquad\\qquad\\leq\\|{\\bf H}\\|_{2}\\|{\\bf D}^{(t)}\\|_{F}+\\|{\\bf G}\\|_{F}+\\frac{\\|{\\bf H}\\|_{2}\\|{\\bf V}^{(t)}\\|_{F}}{\\rho_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{D}^{(t+1)}-\\mathbf{D}^{(t)}\\|_{F}\\leq\\frac{2}{\\rho_{t}}\\left(\\|\\mathbf{G}-\\mathbf{H}\\mathbf{D}^{(t)}\\|_{F}+\\frac{\\|\\mathbf{H}\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{2}{\\rho_{t}}\\left(\\|\\mathbf{H}\\|_{2}\\|\\mathbf{D}^{(t)}\\|_{F}+\\|\\mathbf{G}\\|_{F}+\\frac{\\|\\mathbf{H}\\|_{2}\\|\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This further implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{D}^{(t+1)}\\|_{F}\\leq\\left(1+\\frac{2\\|\\mathbf{H}\\|_{2}}{\\rho_{t}}\\right)\\|\\mathbf{D}^{(t)}\\|_{F}+\\frac{2\\|\\mathbf{G}\\|_{F}}{\\rho_{t}}+\\frac{2\\|\\mathbf{H}\\|_{2}\\|\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining inequalities (20) and (22) yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\mathbf{D}^{(t+1)}\\|_{F}+\\frac{\\|\\mathbf{V}^{(t+1)}\\|_{F}}{\\rho_{t+1}}\\leq\\|\\mathbf{D}^{(t+1)}\\|_{F}+\\frac{\\|\\mathbf{V}^{(t+1)}\\|_{F}}{\\rho_{t}}}}\\\\ &{\\leq\\left(1+\\frac{3\\|\\mathbf{H}\\|_{2}}{\\rho_{t}}\\right)\\|\\mathbf{D}^{(t)}\\|_{F}+\\frac{3\\|\\mathbf{G}\\|_{F}}{\\rho_{t}}+\\frac{3\\|\\mathbf{H}\\|_{2}\\|\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}^{2}}}\\\\ &{\\leq\\left(1+\\frac{3\\|\\mathbf{H}\\|_{2}}{\\rho_{t}}\\right)\\left(\\|\\mathbf{D}^{(t)}\\|_{F}+\\frac{\\|\\mathbf{V}^{(t)}\\|_{F}}{\\rho_{t}}\\right)+\\frac{3\\|\\mathbf{G}\\|_{F}}{\\rho_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Denote $a_{t}:=\\|\\mathbf{D}^{(t)}\\|_{F}+\\|\\mathbf{V}^{(t)}\\|_{F}/\\rho_{t}$ , then the above inequality can be rewritten as ", "page_idx": 17}, {"type": "equation", "text": "$$\na_{t+1}\\leq\\left(1+\\frac{3\\|\\mathbf{H}\\|_{2}}{\\rho_{t}}\\right)a_{t}+\\frac{3\\|\\mathbf{G}\\|_{F}}{\\rho_{t}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{a_{t+1}}{\\prod_{s=0}^{t}(1+3\\|\\mathbf{H}\\|_{2}/\\rho_{k})}\\leq\\frac{a_{t}}{\\prod_{s=0}^{t-1}(1+3\\|\\mathbf{H}\\|_{2}/\\rho_{k})}+\\frac{3\\|\\mathbf{G}\\|_{F}}{\\rho_{t}\\prod_{s=0}^{t}(1+3\\|\\mathbf{H}\\|_{2}/\\rho_{k})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{a_{t}}{\\prod_{s=0}^{t-1}(1+3\\|\\mathbf{H}\\|_{2}/\\rho_{k})}+\\frac{3\\|\\mathbf{G}\\|_{F}}{\\rho_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It then follows from telescoping that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{a_{t}}{\\prod_{s=0}^{t-1}(1+3\\|\\mathbf{H}\\|_{2}/\\rho_{k})}\\le a_{0}+\\sum_{s=0}^{t-1}\\frac{3\\|\\mathbf{G}\\|_{F}}{\\rho_{t}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recalling the definition of $a_{t}$ completes the proof. ", "page_idx": 17}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Experimental setup ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We performed all experiments on a computing cluster using an Intel Xeon Gold 6248 machine with 20 CPUs and a single NVIDIA V100 GPU, which is equipped with 192GB of CPU RAM and 32GB of CUDA memory. The PyTorch library Paszke et al. [2017] was used to implement all language models and pruning methods for our experiments. ", "page_idx": 17}, {"type": "text", "text": "Pruning problem setup. For a given sparsity $s$ , we set the $\\ell_{0}$ constraint $k$ in the pruning problem (1) to $\\lfloor N_{i n}N_{o u t}s\\rfloor$ . Following the pruning framework proposed by Frantar and Alistarh [2023], Meng et al. [2024b], we solve the LLM pruning problem sequentially, layer by layer. For layer $\\ell$ , the input activation matrix $\\mathbf{X}$ in (1) is set as the output of the previous $\\ell-1$ pruned layers on $N$ calibration samples. ", "page_idx": 17}, {"type": "text", "text": "Data pre-processing. Let $\\mathbf{E}\\,=\\,\\mathrm{Diag}(\\mathbf{X}^{\\top}\\mathbf{X}+\\lambda_{2}\\mathbf{I})^{-1/2}$ . To achieve better scaling, we define $\\mathbf{W}^{\\prime}=\\mathbf{\\bar{E}}^{-\\bar{1}}\\mathbf{W}$ and reformulate Equation (1) into the following equivalent form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{W}^{\\prime}\\in\\mathbb{R}^{N_{i n}\\times N_{o u t}}}\\;\\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{E}\\mathbf{W}^{\\prime}\\|_{F}^{2}+\\lambda_{2}\\|\\widehat{\\mathbf{W}}-\\mathbf{E}\\mathbf{W}^{\\prime}\\|_{F}^{2}\\quad\\mathrm{s.t.~}\\|\\mathbf{W}^{\\prime}\\|_{0}\\leq k\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Practically, we apply Algorithm 1 to solve Equation (27) and recover the solution to the original problem by setting $\\mathbf{W}=\\mathbf{E}\\mathbf{W}^{\\prime}$ . It is important to note that this pre-processing step does not alter the procedure of Algorithm 1 or affect the convergence analysis. It only modifies the updates within Algorithm 1 and can lead to a better convergence rate in practice. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Hyperparamter choice. We choose $\\lambda_{2}=0.01\\,\\mathrm{{Tr}}(\\mathbf{X}^{\\top}\\mathbf{X})$ . In Algorithm 1, we set $\\rho_{0}=0.1$ . And we update $\\rho$ every 3 iteration based on a step function that depends on the current value of $\\rho_{t}$ and $\\mathbf{\\boldsymbol{s}}_{t}:=|\\operatorname{Supp}\\left(\\mathbf{D}^{(\\bar{t})}\\right)\\Delta\\operatorname{Supp}\\left(\\mathbf{D}^{(t-3)}\\right)|$ , which represents the number of elements in the symmetric difference between $\\operatorname{Supp}\\left(\\mathbf{D}^{(t)}\\right)$ and Supp $\\left(\\mathbf{D}^{(t-3)}\\right)$ . Specifically, we set ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\rho_{t+1}=\\left\\{\\begin{array}{l l}{1.3\\rho_{t}}&{\\mathrm{~if~}s_{t}\\geq0.1k,}\\\\ {1.2\\rho_{t}}&{\\mathrm{~if~}s_{t}\\geq0.005k,}\\\\ {1.1\\rho_{t}}&{\\mathrm{~if~}s_{t}\\geq1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $s_{t}\\,=\\,0$ , it indicates that $\\rho$ is sufficiently large and the support has stabilized. In this case, we terminate Algorithm 1, set $\\boldsymbol{S}$ as the support of $\\mathbf{W}$ , and apply Algorithm 2 with 10 iterations to solve problem (6) with support $\\boldsymbol{S}$ . ", "page_idx": 18}, {"type": "text", "text": "Implementation details. Below are the configuration and implementation details for the competing methods and our proposed framework ALPS. ", "page_idx": 18}, {"type": "text", "text": "\u2022 MP: For each layer in the LLM, we perform magnitude pruning by sorting the absolute values of all entries of the dense weightW  in descending order, keeping the top $k$ entries unchanged, and setting the remaining entries to zero.   \n\u2022 SparseGPT: We utilize the authors\u2019 implementation (codes available on GitHub) with default hyperparameter settings to perform one-shot unstructured LLM pruning.   \n\u2022 Wanda: We utilize the authors\u2019 implementation (codes available on GitHub) with default hyperparameter settings to perform one-shot unstructured LLM pruning.   \n\u2022 DSnoT: We utilize the authors\u2019 implementation (codes available on GitHub) with default hyperparameter settings to perform one-shot unstructured LLM pruning. ", "page_idx": 18}, {"type": "text", "text": "B.2 Additional experimental results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.2.1 The importance of the $\\rho$ update scheme ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our proposed $\\rho$ update scheme, theoretically supported by Theorem 1, ensures that $A L P S$ converges rapidly while finding high quality solutions. In contrast, ADMM with a fixed $\\rho$ may fail to converge when applied to $\\ell_{0}$ constrained least squares problems. To provide empirical evidence for this claim, we compare $A L P S$ with ADMM using fixed $\\rho$ values. We examined two key metrics: reconstruction loss (objective) and the rate of change of the support (of weights) between consecutive iterations (this measures the convergence speed of the algorithm).Tables 4 and 5 present the results for these metrics, respectively. Our findings show that ADMM with a large $\\rho(=3)$ converges quickly but yields poor solutions, while a small $\\rho(=0.3)$ fails to converge. $A L P S$ , utilizing our $\\rho$ update scheme, achieves both rapid convergence and high-quality solutions. ", "page_idx": 18}, {"type": "table", "img_path": "0lBx844upd/tmp/49c6c6e307db43c2340777fdb0e89732bc8bd8db01e51eac14c3e012d0d82cfa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 4: The relative reconstruction error $\\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{W}\\|_{F}^{2}/\\|\\mathbf{X}\\widehat{\\mathbf{W}}\\|_{F}^{2}$ over iterations, comparing ALPS with ADMM using a fixed penalty parameter $\\rho$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\begin{array}{r l}&{\\begin{array}{r l r l r l}&{\\mathrm{~Supp~change~/~Iter~}}\\end{array}}{\\frac{\\begin{array}{l l l l l}{5}&{10}&{20}&{30}&{50}&{100}\\\\ {4L P S}&{20.2\\%}&{17.0\\%}&{2.8\\%}&{0.0\\%}&{0.0\\%}&{0.0\\%}\\end{array}}}\\\\ &{\\frac{\\mathrm{~ADMM}(\\rho=0.3)}{\\mathrm{~ADMM}(\\rho=0.3)}\\begin{array}{l l l l l}{6.4\\%}&{7.0\\%}&{7.0\\%}&{7.0\\%}&{6.9\\%}&{6.9\\%}\\\\ {0.2\\%}&{<0.1\\%}&{<0.1\\%}&{<0.1\\%}&{<0.1\\%}&{<0.1\\%}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Table 5: The rate of change of the support (of weights) between consecutive iterations, comparing ALPS with ADMM using a fixed penalty parameter $\\rho$ . ", "page_idx": 18}, {"type": "table", "img_path": "0lBx844upd/tmp/ea49731f1c9867730a7f8a27d1c5c024dd691c70c051d43ac64e80b9a142311f.jpg", "table_caption": ["Table 6: Runtime (in seconds) comparison for one-shot unstructured pruning of OPT models and LLaMA models. Here, runtime includes input activation generation and model pruning. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.2.2 Runtime comparison ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We compare the runtime of ALPS with other methods in Table 6. ALPS employs an advanced optimization method to solve the layerwise reconstruction problem, which results in longer running times compared to other algorithms. However, it\u2019s important to note that ALPS \u2019s runtime is still negligible when compared to fine-tuning methods for LLMs, e.g., LoRA [Hu et al., 2021]. ", "page_idx": 19}, {"type": "text", "text": "B.2.3 Comparison of ALPS and ADMM-Grad ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We discuss the difference between ALPS and ADMM-Grad [Bo\u017ea, 2024], another interesting work using ADMM for LLM unstructured pruning. [Bo\u017ea, 2024] selects the sparsity mask via iterative magnitude pruning and applies ADMM to solve problem (6) with the selected sparsity mask. $A L P S$ , in contrast, is an end-to-end approach that directly targets the $\\ell_{0}$ constrained least squares problem (1). By simultaneously optimizing both weights and sparsity patterns, $A L P S$ achieves lower layerwise reconstruction loss compared to ADMM-Grad, as demonstrated in Table 7. ", "page_idx": 19}, {"type": "text", "text": "Additionally, since [Bo\u017ea, 2024] employs ADMM for solving problem (6), we compare it with our proposed PCG procedure. We tested both approaches for solving problem (6) with a given support. Results presented in Table 8 show that PCG outperforms ADMM-Grad in both computational time and objective value. The time advantage of PCG stems from its ability to backsolve without explicitly computing matrix inverses. ", "page_idx": 19}, {"type": "table", "img_path": "0lBx844upd/tmp/634499dcd2b4520312483da2b00b67e316a85f104d190f2e5bfffbc02622af99.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "0lBx844upd/tmp/082dca7ec3145765e5e89ba05aa91bfdbe75e673c40acae44f6506059ffd2b34.jpg", "table_caption": ["Table 7: Relative reconstruction error $\\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{W}\\|_{F}^{2}/\\|\\mathbf{X}\\widehat{\\mathbf{W}}\\|_{F}^{2}$ comparison between ALPS and ADMM-Grad across different sparsity levels. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 8: Comparison of PCG (Algorithm 2) and ADMM [Bo\u017ea, 2024] for solving problem (6). Results show runtime (seconds) and layerwise reconstruction loss $\\|\\mathbf{X}\\widehat{\\mathbf{W}}-\\mathbf{X}\\mathbf{W}\\|_{F}^{2}/\\|\\mathbf{X}\\widehat{\\mathbf{W}}\\|_{F}^{2}$ across supports with different sparsity levels. ", "page_idx": 19}, {"type": "text", "text": "B.2.4 Pruned model performance on MMLU benchmark ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We evaluated the one-shot unstructured pruning performance of MP, SparseGPT, Wanda, DSnoT, and ALPS on LLaMA3-8B using the MMLU benchmark to give a comprehensive assessment of each method\u2019s effectiveness. Table 1 shows the mean accuracy across all MMLU categories. The results demonstrate that ALPS outperforms other methods, further validating its effectiveness in producing high-performance pruned models. We also observe significant performance degradation on MMLU at high sparsity levels, suggesting that one-shot pruning should be combined with fine-tuning [Hu et al., 2021] or prompting [Sahoo et al., 2024] techniques to maintain model performance in practical applications. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "0lBx844upd/tmp/3326a6d4a17a23e8f814f432b5543a41ad9386a41971749ddc404db24f1f9766.jpg", "table_caption": ["Table 9: Performance analysis for one-shot unstructured pruning of LLaMA-3 8B models at various sparsity levels using MMLU benchmark. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.2.5 Comprehensive model performance across sparsity levels ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We compare the one-shot unstructured pruning performance of MP, SparseGPT, Wanda, DSnoT, and ALPS on OPT-1.3B-30B models and LLaMA2-7B, LLaMA2-13B, LLaMA3-8B models in Tables 10-17. The models are pruned at unstructured sparsity levels of $40\\%$ , $50\\%$ , $60\\%$ , $70\\%$ , $80\\%$ , and $90\\%$ , as well as at 2:4 and 4:8 sparsity patterns. We evaluate the perplexity of the pruned models on WikiText2, PTB, and C4 datasets. Additionally, we assess the accuracy of the pruned models on PIQA, LAMBADA, ARC-Easy, and ARC-Challenge. However, LAMBADA accuracy results for the LLaMA model have been omitted since LLaMA models have unsatisfactory performance on this dataset without further modifications. For each combination of model, sparsity, and pruning approach, we run each method five times and report the mean and standard deviation of each performance criterion. ", "page_idx": 20}, {"type": "table", "img_path": "0lBx844upd/tmp/377d1d4f3392993e54d7956a1e560b58cf2c4f04968c69494d86f52d951eaf34.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "0lBx844upd/tmp/2445648707f16cc8c941913ca7a1e5dc0261fdb6d24b4941a65fc1141dedc941.jpg", "table_caption": ["Table 10: Performance analysis for one-shot unstructured pruning of OPT models and LLaMA models at $40\\%$ sparsity. $(\\uparrow)$ : higher is better; (\u2193): lower is better. We omit LLaMA results on LAMBADA due to its poor performance without modifications. "], "table_footnote": ["Table 11: Performance analysis for one-shot unstructured pruning of OPT models and LLaMA models at $50\\%$ sparsity. (\u2191): higher is better; (\u2193): lower is better. We omit LLaMA results on LAMBADA due to its poor performance without modifications. "], "page_idx": 21}, {"type": "table", "img_path": "0lBx844upd/tmp/0be6c054758321294d21add2bc65e4a798effca74de677b353453bc9597ac037.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "0lBx844upd/tmp/c8d130c9eb2018db175eb3649efed71800da585439d1c58298d1d572bc09e64c.jpg", "table_caption": ["Table 12: Performance analysis for one-shot unstructured pruning of OPT models and LLaMA models at $60\\%$ sparsity. (\u2191): higher is better; (\u2193): lower is better. We omit LLaMA results on LAMBADA due to its poor performance without modifications. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 13: Performance analysis for one-shot unstructured pruning of OPT models and LLaMA models at $70\\%$ sparsity. $(\\uparrow)$ : higher is better; (\u2193): lower is better. We omit LLaMA results on LAMBADA due to its poor performance without modifications. ", "page_idx": 22}, {"type": "table", "img_path": "0lBx844upd/tmp/9b8579e14ce8ae98822cf7da0653d1820fd88cc8eb5e4ebe8000bcb3f413003b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "0lBx844upd/tmp/2075db8349f6c9cc360239b6a9fe482422400564873b95c51d858ae10905ba2c.jpg", "table_caption": ["Table 14: Performance analysis for one-shot unstructured pruning of OPT models and LLaMA models at $80\\%$ sparsity. (\u2191): higher is better; (\u2193): lower is better. We omit LLaMA results on LAMBADA due to its poor performance without modifications. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 15: Performance analysis for one-shot unstructured pruning of OPT models and LLaMA models at $90\\%$ sparsity. (\u2191): higher is better; (\u2193): lower is better. We omit LLaMA results on LAMBADA due to its poor performance without modifications. ", "page_idx": 23}, {"type": "table", "img_path": "0lBx844upd/tmp/3abcfc51b6e17c914d9ff8dfc5fbce582723db64bc88a4f31b91af0743d331e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "0lBx844upd/tmp/10f25f0710ab02073d10a90f7fa9618a01c9ff751a2f2f6603e498f61e93e46c.jpg", "table_caption": ["Table 16: Performance analysis for one-shot unstructured pruning of OPT models and LLaMA models at $2:4$ sparsity. $(\\uparrow)$ : higher is better; (\u2193): lower is better. We omit LLaMA results on LAMBADA due to its poor performance without modifications. "], "table_footnote": ["Table 17: Performance analysis for one-shot unstructured pruning of OPT models and LLaMA models at $4:8$ sparsity. (\u2191): higher is better; (\u2193): lower is better. We omit LLaMA results on LAMBADA due to its poor performance without modifications. "], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: At the end of the introduction, we include a paragraph that clearly states the contributions and scope of our paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We briefly discuss the limitation of our work in Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We clearly state all assumptions in Theorem 1 and provide a formal proof in Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We thoroughly describe our proposed pruning approach in Algorithms 1 and 2, and we include all other details necessary for reproducing the results in Appendix B.1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our code is available at https://github.com/mazumder-lab/ALPS. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide detailed settings of our proposed pruning framework and its competitors used in our experiments in Appendix B.1. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In our experiments, for each combination of model, pruning method, and sparsity level, we ran 5 different seeds and reported the mean and standard deviation of the results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide details of the computational resources used for our experiments in Appendix B.1. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and ensured that the research conducted in this paper fully complies with it. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: To the best of our knowledge, our work has no societal impact. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: To the best of our knowledge, our work poses no such risks. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: At the beginning of Section 4, we cite all the datasets and models used in our experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]