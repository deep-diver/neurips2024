[{"figure_path": "ZK1CZXKgG5/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table compares the performance of the proposed MemVLT model against other state-of-the-art methods on four benchmark datasets for vision-language tracking.  The metrics used for comparison are AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision).  The datasets are MGIT (which focuses on complex spatio-temporal relationships), TNL2K (which includes challenging scenarios like adversarial samples and modality switches), LaSOT (a large-scale dataset for long-term tracking), and LaSOText (an extension of LaSOT with language annotations). The table highlights the superior performance of MemVLT, particularly on MGIT and TNL2K, indicating its ability to handle complex and dynamic tracking challenges.", "section": "4 Experiments"}, {"figure_path": "ZK1CZXKgG5/tables/tables_7_1.jpg", "caption": "Table 3: Ablation study on important model components. The best results are shown in red.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different model components on the overall performance.  Specifically, it investigates the contribution of incorporating memory information (visual and textual) to generate adaptive prompts, a key feature of the MemVLT model. The table shows the AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision) metrics for four different configurations: (1) no adaptive prompts; (2) adaptive visual prompts only; (3) adaptive textual prompts only; and (4) both adaptive visual and textual prompts. The best performing configuration, with both types of adaptive prompts, is highlighted in red, demonstrating their combined effectiveness in improving the model's performance.", "section": "4.3 Ablation Study"}, {"figure_path": "ZK1CZXKgG5/tables/tables_7_2.jpg", "caption": "Table 4: Comparison of different long-term memory storage methods.", "description": "This table compares the performance of four different long-term memory storage methods: sliding window, top-L, section-L, and section-top.  The comparison is based on the AUC, PNorm, and P metrics on the TNL2K benchmark.  The results show that the section-top method outperforms the others, demonstrating the effectiveness of the proposed method in storing and utilizing memory information for accurate tracking.", "section": "4.3 Ablation Study"}, {"figure_path": "ZK1CZXKgG5/tables/tables_7_3.jpg", "caption": "Table 2: Results of efficiency analysis.", "description": "This table compares three vision-language tracking models (JointNLT, MMTrack, and MemVLT) in terms of their efficiency and performance on the TNL2K benchmark.  Efficiency is measured by the number of parameters (Params) and frames per second (Speed). Performance is measured by the area under the curve (AUC) and precision (P).  MemVLT shows improved performance with a comparable speed and parameter count to the others.", "section": "4 Experiments"}, {"figure_path": "ZK1CZXKgG5/tables/tables_8_1.jpg", "caption": "Table 3: Ablation study on important model components. The best results are shown in red.", "description": "This table presents the ablation study results focusing on the impact of different model components on the overall performance. It shows the performance variations when removing the adaptive visual prompts, the adaptive textual prompts, or both, from the MemVLT model. The results demonstrate the significance of adaptive prompts in improving the model's accuracy and precision.", "section": "4.3 Ablation Study"}, {"figure_path": "ZK1CZXKgG5/tables/tables_8_2.jpg", "caption": "Table 6: Generalizability analysis on SOT.", "description": "This table presents an ablation study evaluating the generalizability of the proposed MemVLT's memory mechanism to standard single object tracking (SOT) tasks.  It compares the performance of a naive SOT model against versions incorporating the Memory Interaction Module (MIM) and the Memory Storage Module (MSM). The results demonstrate the positive impact of MemVLT's memory components on SOT performance, suggesting that the adaptive prompting mechanism generalizes well beyond vision-language tracking.", "section": "4.3 Ablation Study"}, {"figure_path": "ZK1CZXKgG5/tables/tables_8_3.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table compares the performance of the proposed MemVLT model against other state-of-the-art methods on four popular vision-language tracking benchmarks: MGIT, TNL2K, LaSOT, and LaSOText.  The metrics used for comparison include AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision).  The best two results for each metric on each benchmark are highlighted in red and blue for easy comparison.  This provides a quantitative assessment of MemVLT's performance relative to existing techniques.", "section": "4 Experiments"}, {"figure_path": "ZK1CZXKgG5/tables/tables_20_1.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table compares the performance of the proposed MemVLT model against other state-of-the-art methods on four benchmark datasets for vision-language tracking (VLT).  The metrics used for comparison are AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision).  The table highlights the superior performance of MemVLT, especially on MGIT and TNL2K datasets, where it achieves significant improvements over existing methods.", "section": "4.2 Comparison with State-of-the-art"}, {"figure_path": "ZK1CZXKgG5/tables/tables_21_1.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table presents a comparison of the MemVLT model's performance against other state-of-the-art models on four benchmark datasets for vision-language tracking: MGIT, TNL2K, LaSOT, and LaSOText.  The metrics used for comparison are AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision).  The table highlights the superior performance of MemVLT, especially on MGIT and TNL2K, by showing improvements over existing best results.", "section": "4.2 Comparison with State-of-the-art"}, {"figure_path": "ZK1CZXKgG5/tables/tables_21_2.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table compares the performance of the proposed MemVLT model against other state-of-the-art vision-language tracking (VLT) models on four benchmark datasets: MGIT, TNL2K, LaSOT, and LaSOText.  The comparison is done using AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision) metrics.  The best two performing models for each metric on each dataset are highlighted in red and blue for easy identification. This table showcases MemVLT's improved accuracy over existing VLT models.", "section": "4 Experiments"}, {"figure_path": "ZK1CZXKgG5/tables/tables_22_1.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table compares the performance of the proposed MemVLT model against other state-of-the-art vision-language tracking models on four benchmark datasets: MGIT, TNL2K, LaSOT, and LaSOText.  It shows the AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision) scores for each method on each dataset. The best two performing models for each metric on each dataset are highlighted in red and blue for easy comparison and to demonstrate the effectiveness of MemVLT. The table shows that MemVLT achieves state-of-the-art performance on these datasets, outperforming the existing best results by a significant margin.", "section": "4 Experiments"}, {"figure_path": "ZK1CZXKgG5/tables/tables_22_2.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table presents a comparison of the MemVLT model's performance against other state-of-the-art models on four widely used vision-language tracking benchmarks: MGIT, TNL2K, LaSOT, and LaSOText.  For each benchmark and each metric (AUC, PNorm, P), the table shows the performance of various models. The best two performing models for each metric are highlighted in red and blue, respectively, clearly showing MemVLT's superior performance.", "section": "4.2 Comparison with State-of-the-art"}, {"figure_path": "ZK1CZXKgG5/tables/tables_22_3.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table compares the performance of the proposed MemVLT model against several state-of-the-art vision-language tracking (VLT) methods on four widely used benchmark datasets: MGIT, TNL2K, LaSOT, and LaSOText.  The metrics used for comparison are AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision).  The table highlights the superior performance of MemVLT, especially on MGIT and TNL2K, where it achieves significant improvements compared to the existing best results.", "section": "4 Experiments"}, {"figure_path": "ZK1CZXKgG5/tables/tables_23_1.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table compares the performance of MemVLT against other state-of-the-art (SOTA) methods on four benchmark datasets for vision-language tracking (VLT): MGIT, TNL2K, LaSOT, and LaSOText.  It presents results using several metrics, including AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision). The best two results for each metric on each dataset are highlighted to easily compare the performance of MemVLT with other approaches.", "section": "4 Experiments"}, {"figure_path": "ZK1CZXKgG5/tables/tables_23_2.jpg", "caption": "Table 3: Ablation study on important model components. The best results are shown in red.", "description": "This ablation study analyzes the impact of different model components on the performance of MemVLT, specifically focusing on whether the incorporation of memory information is used to generate adaptive visual or textual prompts.  It compares the model's performance when using memory information for only visual prompts, only textual prompts, both visual and textual prompts, and no adaptive prompts at all.  The results highlight the importance of the memory interaction module in achieving optimal performance.", "section": "4.3 Ablation Study"}, {"figure_path": "ZK1CZXKgG5/tables/tables_23_3.jpg", "caption": "Table 1: Comparison with state-of-the-arts on four popular benchmarks: MGIT [5], TNL2K [26], LaSOT [27] and LaSOText [28]. The best two results are highlighted in red and blue, respectively.", "description": "This table compares the performance of MemVLT against other state-of-the-art vision-language tracking methods on four benchmark datasets: MGIT, TNL2K, LaSOT, and LaSOText.  The metrics used are AUC (Area Under the Curve), PNorm (Normalized Precision), and P (Precision).  The table highlights MemVLT's superior performance, particularly on MGIT and TNL2K, demonstrating improvements over existing best results.", "section": "4.2 Comparison with State-of-the-art"}]