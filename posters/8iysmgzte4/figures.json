[{"figure_path": "8IysmgZte4/figures/figures_1_1.jpg", "caption": "Figure 1: The transfer setting for DiSPOs. Given an unlabeled offline dataset, DiSPOs model both \"what can happen?\" p(s) and \"how can we achieve a particular outcome?\" p(a|s, \u03c8). This is used for quick adaptation to new downstream tasks without test-time policy optimization.", "description": "This figure illustrates the two-stage process of DiSPOs.  First, an offline dataset is used to train the model to predict the distribution of possible long-term outcomes (successor features) in the environment, represented as p(\u03c8|s), and a policy \u03c0(a|s, \u03c8) that selects an action a to achieve a particular outcome \u03c8 given a current state s.  Second, this trained model is used to quickly adapt to new downstream tasks without requiring any further training or online policy optimization.  New tasks are simply specified by providing a new reward function or a set of (s, r) samples. ", "section": "1 Introduction"}, {"figure_path": "8IysmgZte4/figures/figures_4_1.jpg", "caption": "Figure 2: DiSPOs for a simple environment. Given a state feature function \"\", DiSPOs learn a distribution of all possible long-term outcomes (successor features \u03c8) in the dataset p(\u03c8|s), along with a readout policy \u03c0(a|s, \u03c8) that takes an action a to realise \u03c8 starting at state s.", "description": "This figure illustrates a simple environment with states and actions.  DiSPOs models the distribution of successor features p(\u03c8|s) for each state.  It also learns a policy \u03c0(a|s,\u03c8) that maps states and successor features to actions.  The goal is to predict and take actions to achieve specific desired long-term outcomes (successor features).", "section": "4.1 Learning Distributional Successor Features of the Behavior Policy"}, {"figure_path": "8IysmgZte4/figures/figures_5_1.jpg", "caption": "Figure 3: Zero-shot policy optimization with DiSPOs. Once a DiSPO is learned, the optimal action can be obtained by performing reward regression and searching for the optimal outcome under the dynamics to decode via the policy.", "description": "This figure illustrates the two-step process of zero-shot policy optimization using DiSPOs.  First, linear regression is performed to infer the task-specific reward weights from the observed rewards and features in the dataset. Then, planning is conducted by searching for the optimal outcome (successor feature) with the highest expected reward, satisfying a data support constraint. Finally, the readout policy is used to determine the optimal action corresponding to that outcome.", "section": "4.2 Zero-Shot Policy Optimization with Distributional Successor Features"}, {"figure_path": "8IysmgZte4/figures/figures_7_1.jpg", "caption": "Figure 1: The transfer setting for DiSPOs. Given an unlabeled offline dataset, DiSPOs model both \"what can happen?\" p(s) and \"how can we achieve a particular outcome?\" p(a|s, \u03c8). This is used for quick adaptation to new downstream tasks without test-time policy optimization.", "description": "The figure illustrates the offline training process for DiSPOs. DiSPOs takes an offline dataset and learns both the possible long-term outcomes and the policies to achieve those outcomes. During testing, this model allows for quick adaptation to new downstream tasks.", "section": "1 Introduction"}, {"figure_path": "8IysmgZte4/figures/figures_8_1.jpg", "caption": "Figure 5: Transfer across tasks with DiSPOs and COMBO [63] in medium antmaze. Each tile corresponds to a different task, with color of the tile indicating the normalized return. DiSPOs successfully transfer across a majority of tasks, while MBRL [63] struggles on tasks that are further away from the initial location.", "description": "This figure compares the performance of DiSPOs and COMBO on various tasks in the AntMaze environment. Each colored tile represents a different task, with the color intensity reflecting the normalized return achieved by each method. DiSPOs demonstrates better transferability across diverse tasks compared to COMBO, especially those far from the starting location.", "section": "6.3 Do DiSPOs enable zero-shot policy optimization across tasks?"}, {"figure_path": "8IysmgZte4/figures/figures_17_1.jpg", "caption": "Figure 2: DiSPOs for a simple environment. Given a state feature function \u03c6, DiSPOs learn a distribution of all possible long-term outcomes (successor features \u03c8) in the dataset p(\u03c8|s), along with a readout policy \u03c0(a|s, \u03c8) that takes an action a to realise \u03c8 starting at state s.", "description": "This figure illustrates the components of DiSPOs in a simple environment.  It shows how DiSPOs model the distribution of successor features (representing possible long-term outcomes) from a given state, alongside a policy that selects actions to achieve specific successor features.  This dual modeling allows DiSPOs to efficiently adapt to new tasks by selecting the best outcome and corresponding action, avoiding computationally expensive test-time policy optimization.", "section": "4.1 Learning Distributional Successor Features of the Behavior Policy"}, {"figure_path": "8IysmgZte4/figures/figures_19_1.jpg", "caption": "Figure 4: Evaluation domains: (1) D4RL Antmaze [15] (2) Franka Kitchen [15] (3) Hopper [9] (4) Preference-Based Antmaze with the goal of taking a particular path (5) Roboverse [46] robotic manipulation.", "description": "This figure showcases five different simulated robotics environments used to evaluate the DiSPOs model.  Each environment presents unique challenges for reinforcement learning agents, testing various aspects of robot control and decision-making. These range from navigation tasks (Antmaze, Preference Antmaze) to manipulation tasks (Franka Kitchen, Roboverse) and locomotion (Hopper).  The inclusion of a preference-based Antmaze highlights the model's ability to handle tasks beyond simple goal-reaching.", "section": "6.1 Problem Domains and Datasets"}]