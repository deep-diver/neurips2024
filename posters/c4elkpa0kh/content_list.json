[{"type": "text", "text": "Efficient $\\Phi$ -Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Brian Hu Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ioannis Anagnostides Carnegie Mellon University ianagnos@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Carnegie Mellon University bhzhang@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Gabriele Farina MIT gfarina@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Tuomas Sandholm   \nCarnegie Mellon University   \nStrategic Machine, Inc.   \nStrategy Robot, Inc.   \nOptimized Markets, Inc.   \nsandholm@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent breakthrough results by Dagan, Daskalakis, Fishelson and Golowich [2023] and Peng and Rubinstein [2023] established an efficient algorithm attaining at most $\\epsilon$ swap regret over extensive-form strategy spaces of dimension $N$ in $N^{\\tilde{O}(1/\\epsilon)}$ rounds. On the other extreme, Farina and Pipis [2023] developed an efficient algorithm for minimizing the weaker notion of linear-swap regret in $\\mathsf{p o l y}(N)/\\epsilon^{2}$ rounds. In this paper, we develop efficient parameterized algorithms for regimes between these two extremes. We introduce the set of $k$ -mediator deviations, which generalize the untimed communication deviations recently introduced by Zhang, Farina and Sandholm [2024] to the case of having multiple mediators, and we develop algorithms for minimizing the regret with respect to this set of deviations in $N^{O(k)}/\\epsilon^{2}$ rounds. Moreover, by relating $k$ -mediator deviations to low-degree polynomials, we show that regret minimization against degree- ${\\cdot k}$ polynomial swap deviations is achievable in $N^{O(k d)^{3}}/\\epsilon^{2}$ rounds, where $d$ is the depth of the game, assuming a constant branching factor. For a fixed degree $k$ , this is polynomial for Bayesian games and quasipolynomial more broadly when $d\\,=\\,{\\mathsf{p o l y l o g N}}.$ \u2014the usual balancedness assumption on the game tree. The first key ingredient in our approach is a relaxation of the usual notion of a fixed point required in the framework of Gordon, Greenwald and Marks [2008]. Namely, for a given deviation $\\phi$ , we show that it suffices to compute what we refer to as a fixed point in expectation; that is, a distribution $\\pi$ such that $\\mathbb{E}_{\\mathbf{x}\\sim\\pi}[\\phi(\\mathbf{x})-\\mathbf{\\alpha}\\mathbf{x}]\\,\\approx\\,0$ . Unlike the problem of computing an actual (approximate) fixed point $\\begin{array}{l}{{\\pmb x}~\\approx~\\phi({\\pmb x})}\\end{array}$ , which we show is PPAD-hard, there is a simple and efficient algorithm for finding a solution that satisfies our relaxed notion. As a byproduct, we provide, to our knowledge, the fastest algorithm for computing $\\epsilon$ -correlated equilibria in normal-form games in the medium-precision regime, obviating the need to solve a linear system in every round. Our second main contribution is a characterization of the set of low-degree deviations, made possible through a connection to low-depth decisions trees from Boolean analysis. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Correlated equilibrium $(C E)$ , introduced in a groundbreaking work by Aumann [1974], has emerged as one of the most influential solution concepts in game theory. Often contrasted with Nash equilibrium [Nash, 1950], it is regarded by many as more natural; in the words attributed to another Nobel laureate, Roger Myerson, \u201cif there is intelligent life on other planets, in a majority of them, they would have discovered correlated equilibrium before Nash equilibrium.\u201d Correlated equilibria also enjoy more favorable computational properties: unlike Nash equilibria, they can be expressed as solutions to a linear program, thereby enabling their computation in polynomial time, at least in normal-form games [Papadimitriou and Roughgarden, 2008, Jiang and Leyton-Brown, 2011]. Further, a correlated equilibrium arises through repeated play from natural no-regret learning dynamics [Hart and Mas-Colell, 2000, Foster and Vohra, 1997]. ", "page_idx": 1}, {"type": "text", "text": "However, many real-world strategic interactions feature sequential moves and imperfect information. In such scenarios, the so-called extensive form constitutes the canonical game representation [Kuhn, 1953, Shoham and Leyton-Brown, 2009]: a normal-form description of the game would be prohibitively large. It is startling to realize that 50 years after Aumann\u2019s original work, the complexity of computing correlated equilibria in extensive-form games\u2014sometimes referred to as normal-form correlated equilibria (NFCE) to disambiguate from other pertinent but weaker solution concepts\u2014remains an outstanding open problem [von Stengel and Forges, 2008, Papadimitriou and Roughgarden, 2008]. ", "page_idx": 1}, {"type": "text", "text": "The long-standing absence of efficient algorithms for computing an NFCE shifted the focus to natural relaxations thereof, which can be understood through the notion of $\\Phi$ -regret [Greenwald and Hall, 2003, Stoltz and Lugosi, 2007, Rakhlin et al., 2011]. In particular, $\\Phi$ represents a set of strategy deviations; the richer the set of deviations, the stronger the induced solution concept. When $\\Phi$ contains all possible transformations, one recovers the notion of NFCE\u2014corresponding to swap regret. At the other end of the spectrum, coarse correlated equilibria correspond to $\\Phi$ consisting solely of constant transformations (aka. external regret). Perhaps the most notable relaxation is the extensive-form correlated equilibrium (EFCE) [von Stengel and Forges, 2008], which can be computed exactly in time polynomial in the representation of the game tree [Huang and von Stengel, 2008]. Considerable interest in the literature has recently been on learning dynamics that minimize $\\Phi$ -regret (e.g., Morrill et al. [2021a,b], Bai et al. [2022], Bernasconi et al. [2023], Noarov et al. [2023], Dud\u00b4\u0131k and Gordon [2009], Gordon et al. [2008], Fujii [2023], Dann et al. [2023], Mansour et al. [2022]). A key reference point in this line of work is the recent construction of Farina and Pipis [2023], an efficient algorithm minimizing linear swap regret\u2014that is, the notion of $\\Phi$ -regret where $\\Phi$ contains all linear deviations. Such algorithms lead to an $\\epsilon$ -equilibrium in time polynomial in the game\u2019s description and $1/\\epsilon$ \u2014aka. a fully polynomial-time approximation scheme (FPTAS). ", "page_idx": 1}, {"type": "text", "text": "Yet, virtually nothing was known beyond those special cases until recent breakthrough results by Dagan et al. [2024] and Peng and Rubinstein [2024], who introduced a new approach for reducing swap regret to external regret; unlike earlier reductions [Gordon et al., 2008, Blum and Mansour, 2007, Stoltz and Lugosi, 2005], their algorithm can be implemented efficiently even in certain settings with an exponential number of pure strategies. For extensive-form games, their reduction implies a polynomial-time approximation scheme (PTAS) for computing an $\\epsilon$ -correlated equilibrium; their algorithm has complexity $N^{\\tilde{O}(1/\\epsilon)}$ for games of size $N$ , which is polynomial only when $\\epsilon$ is an absolute constant. Unfortunately, it was thereafter shown that in the usual regime of interest, where instead $\\epsilon\\,\\le\\,\\mathsf{p o l y}(1/N)$ , an exponential number of rounds is inevitable even against an oblivious adversary [Daskalakis et al., 2024]. In light of that lower bound, our focus here is on developing algorithms attaining a better complexity bound of pol $\\prime(N,1/\\epsilon)$ \u2014the typical guarantee one hopes for within the no-regret framework\u2014by considering a more structured but rich class of deviations $\\Phi$ . ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Before we proceed by giving an overview of our results and technical contributions, we first introduce some basic background on tree-form decisions problems and $\\Phi$ -regret minimization. ", "page_idx": 1}, {"type": "text", "text": "A tree-form decision problem describes a sequential interaction between a player and a (possibly adversarial) environment. There is a tree of nodes. The root is denoted $\\mathcal{Q}$ . We will use $s\\,\\in\\,S$ to denote a generic node, and $p_{s}$ (where $s\\neq\\emptyset$ ) to denote the parent of $s$ . Leaves are called terminal nodes; a generic terminal node is denoted $z\\in{\\mathcal{Z}}$ . Internal nodes can be one of three types: decision points, where the player plays an action, observation points, where the environment picks the next decision point. A generic decision point will be denoted $j$ , and the set of actions at $j$ will be denoted $\\mathbf{\\mathcal{A}}_{j}$ . The child node reached by following action $a\\in A_{j}$ is denoted $_{j a}$ . We will use $N$ to denote the number of terminal nodes. We will also assume without loss of generality that all decision points have branching factor at least 2, and that decision and observation points alternate. Thus, the total number of nodes in the tree is also $O(N)$ . The depth of a decision problem is the largest number of decision points in any root-to-terminal-node path. An example of a tree-form decision problem is depicted below in Figure 1. ", "page_idx": 2}, {"type": "image", "img_path": "c4ElkpA0kh/tmp/2451438a041076a46109724227f364013324d6f6161fb975850d7f0115d677af.jpg", "img_caption": ["Figure 1: An example of a tree-form decision problem. Decision points are black squares with white text labels; observataion points are white squares. Edges are labeled with action names, which are numbers. Pure strategies in this decision problem are identified with vectors $\\textbf{\\em x}=$ $(x_{1},x_{2},x_{3},x_{4},x_{5})\\in\\{0,1\\}^{5}$ satisfying $1-x_{1}=x_{2}+x_{3}=x_{4}+x_{5}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "A pure strategy consists of an assignment of one action $a_{j}\\ \\in\\mathcal A_{j}$ to each decision point $j$ . The tree-form representation of the pure strategy is the vector $\\pmb{x}\\in\\{0,1\\}^{N}$ where $\\pmb{x}[z]=1$ if and only if the player plays all the actions on the $\\mathcal{D}\\rightarrow z$ path. Although $\\textbf{\\em x}$ is a vector indexed only by terminal nodes, we also overload notation to write $\\pmb{x}[s]=1$ if and only if the player plays all actions on the $\\mathcal{D}\\rightarrow s$ path (In other words, $\\pmb{x}[s]=1$ if there exists some $z\\succeq s$ with $\\begin{array}{r}{\\mathbf{\\emx}[z]=1}\\end{array}$ ). Multiple pure strategies can have the same tree-form representation, but in this paper we will only concern ourselves with strategies in tree-form representation, and thus for our purposes such strategies will be treated as identical. We will use $\\dot{\\boldsymbol x^{\\prime}}\\subseteq\\,\\{\\boldsymbol0,\\boldsymbol1\\}^{N}$ to denote the set of tree-form strategies, and sometimes (when context is clear) we will also use $\\mathcal{X}$ to denote the tree-form decision problem itself. For a point in the convex hull of $\\mathcal{X}$ , conv $\\mathcal{X}$ , we also use the symbol $\\textbf{\\em x}\\in\\mathrm{\\conv}\\,\\mathcal X$ . For mixed strategies, we instead use $\\pi\\in\\Delta(\\mathcal{X})$ . When it is relevant, we assume that utilities are rational numbers representable with $\\mathsf{p o l y}(N)$ bits. ", "page_idx": 2}, {"type": "text", "text": "2.2 Regret minimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the framework of online learning, a learner interacts with an adversary over a sequence of rounds. In each round, the learner selects a strategy, whereupon the adversary constructs a utility function. Throughout this paper, we operate in the full feedback setting, wherein the learner gets to observe the entire utility function produced by the adversary after each round. We allow the adversary to be strongly adaptive, so that the (linear) utility function at the $t$ th round $u^{(t)}:\\mathcal{X}\\ni x\\mapsto\\langle{\\pmb u}^{(t)},{\\pmb x}\\rangle$ can depend on the strategy of the learner at that round; this is a standard assumption ( $c f.$ the notion of leaky forecasts in the context of calibration [Foster and Hart, 2018]) that will be used for our lower bound (Theorem 3.3). We assume that utilities belong to $\\mathcal{U}:=\\{\\pmb{u}:|\\langle\\pmb{u},\\pmb{x}\\rangle|\\leq1,\\forall\\pmb{x}\\in\\mathcal{X}\\}$ . It will be convenient to use $\\|\\pmb{x}\\|_{\\mathcal{X}}:=\\operatorname*{max}_{\\mathbf{u}\\in\\mathcal{U}}\\left\\langle\\mathbf{u},\\pmb{x}\\right\\rangle$ for the induced norm. ", "page_idx": 2}, {"type": "text", "text": "We measure the performance of an online learning algorithm as follows. Suppose that $\\Phi\\subseteq$ $(\\operatorname{conv}x)^{\\chi}$ is a set of deviations. If the learner outputs in each round a mixed strategy $\\pi^{(t)}\\in\\Delta(\\mathcal{X})$ , its (time-average) $\\Phi$ -regret [Greenwald and Hall, 2003, Stoltz and Lugosi, 2007] is defined as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{Reg}}}_{\\Phi}^{T}:=\\frac{1}{T}\\operatorname*{max}_{\\phi\\in\\Phi}\\sum_{t=1}^{T}\\left\\langle\\pmb{u}^{(t)},\\underset{\\pmb{x}^{(t)}\\sim\\pi^{(t)}}{\\mathbb{E}}[\\phi(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}]\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the special case where $\\Phi$ contains only constant transformations, one recovers the notion of $e x\\cdot$ - ternal regret. On the other extreme, swap regret corresponds to $\\Phi$ containing all functions $\\mathcal X\\rightarrow\\mathcal X$ . ", "page_idx": 3}, {"type": "text", "text": "It is sometimes assumed that the learner instead selects in each round a strategy $\\pmb{x}^{(t)}\\in\\mathrm{conv}\\,\\pmb{\\chi}$ . To translate (1) in that case, we introduce the extended mapping of a deviation $\\phi\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\mathrm{conv}\\,\\mathcal{X}$ as $\\phi^{\\delta}\\;:=\\;\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\delta({\\pmb x})}[\\phi({\\pmb x}^{\\prime})]$ , where $\\delta\\;:\\;\\operatorname{conv}\\lambda\\;\\to\\;\\Delta(\\chi)$ is a function that is consistent in the sense that $\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\delta({\\pmb x})}[{\\pmb x}^{\\prime}]={\\pmb x}$ . A canonical example of such a function $\\delta$ is the behavioral strategy map $\\beta:\\operatorname{conv}\\mathcal{X}\\to\\Delta(\\mathcal{X})$ , which returns the unique (ignoring actions at decision points reached with probability zero) mixed strategy whose actions at different decision points are independent and whose expectation is $\\textbf{\\em x}$ . We give another example of a consistent map later in Appendix C.2. Accordingly, we let $\\Phi^{\\delta}$ denote all extended mappings. In this context, $\\Phi^{\\delta}$ -regret is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{Reg}}}_{\\Phi^{\\delta}}^{T}:=\\frac{1}{T}\\operatorname*{max}_{\\phi^{\\delta}\\in\\Phi^{\\delta}}\\sum_{t=1}^{T}\\left\\langle\\pmb{u}^{(t)},\\phi^{\\delta}(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We are interested in algorithms whose regret is bounded by $\\epsilon$ after $T=\\mathsf{p o l y}(N,1/\\epsilon)$ rounds. We refer to such algorithms as fully polynomial no-regret learners. ", "page_idx": 3}, {"type": "text", "text": "Remark 2.1. We clarify that all the algorithms we consider in this paper are deterministic, even when we allow mixed strategies. The fact that (1) contains an expectation over $\\pmb{x}^{(t)}\\sim\\pi^{(t)}$ is simply how $\\Phi$ -regret is defined; at no point does the algorithm actually sample from $\\pi^{(t)}$ . Using deterministic algorithms is in line with most of the prior work in the full feedback setting. ", "page_idx": 3}, {"type": "text", "text": "3 Overview of our results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present an overview of our results on parameterized algorithms for minimizing $\\Phi$ -regret in extensive-form games. We shall first describe our results for the special case of Bayesian games with two actions per player, and we then treat general extensive-form games. ", "page_idx": 3}, {"type": "text", "text": "3.1 Bayesian games ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For now, we assume that each player\u2019s strategy space is a hypercube $\\{0,1\\}^{N}$ . Hypercubes are linear transformations of tree-form decision problems; in particular, for Bayesian games in which each player has exactly two actions, the strategy space of every player is, up to linear transformations, a hypercube. Since our results are particularly clean for the hypercube case, we start with that. ", "page_idx": 3}, {"type": "text", "text": "First, we introduce the set of depth- $k$ decision tree deviations $\\Phi_{\\mathrm{DT}}^{k}$ , which can be described as follows. For each of rounds, the deviator first elects a decision point and receives a recommendation, whereupon the deviator gets to decide which action to follow in that decision point. More formally, the set of deviations $\\Phi_{\\mathrm{DT}}^{k^{-}}$ is defined as follows: ", "page_idx": 3}, {"type": "text", "text": "1. The deviator observes an index $j_{0}\\in[N]$ .   \n2. For $i=1,\\ldots,k$ : the deviator selects an index $j_{i}\\in[N]$ , and observes $\\pmb{x}[j_{i}]$ .   \n3. The deviator selects $a_{0}\\in\\{0,1\\}$ . ", "page_idx": 3}, {"type": "text", "text": "We call attention to the order of operations. In particular, each query $j$ is allowed to depend on previously observed $\\mathbf{\\boldsymbol{x}}[j]\\mathbf{\\boldsymbol{s}}$ . We can assume (WLOG) that the deviator always chooses $k$ distinct indices $j$ . Now, the set of deviations $\\phi\\,:\\,\\{0,1\\}^{N}\\,\\rightarrow\\,[0,1]^{N}$ that can be expressed in the above manner is precisely the set of functions representable as (randomized) depth- $k$ decision trees on $N$ variables. To connect $\\Phi_{\\mathrm{DT}}^{k}$ with the concepts referred to earlier, we clarify that $k=1$ corresponds to linear-swap deviations, while $k=N$ captures all possible swap deviations. Our first result is a parameterized online algorithm minimizing regret with respect to deviations in $\\Phi_{\\mathrm{DT}}^{k}$ . (All our results are in the full feedback model under a strongly adaptive adversary.) ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. There is an online algorithm incurring (average) $\\Phi_{\\mathrm{DT}}^{k}$ -regret at most $\\epsilon$ in $N^{O(k)}/\\epsilon^{2}$ rounds with a per-round running time of $N^{O(k)}/\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "Next, we consider the set \u03a6pkoly consisting of all degree-k polynomials \u03d5 : {0, 1}N \u2192{0, 1}N. Our result for this class of deviations mirrors the one for $\\Phi_{\\mathrm{DT}}^{k}$ , but with a worse dependence on $k$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. There is an online algorithm incurring \u03a6pkoly-regret at most \u03f5 in N O(k3)/\u03f52 rounds with a per-round running time of $N^{O(k^{3})}/\\epsilon$ . ", "page_idx": 4}, {"type": "text", "text": "We find those results surprising; we originally surmised that even for quadratic polynomials $(k=2)$ ) the underlying online problem would be hard in the regime where $\\epsilon\\,\\,\\leq\\,\\,{\\sf p o l y}(1/N)$ . We will elaborate on our technical approach for establishing those results in Section 4 coming up. ", "page_idx": 4}, {"type": "text", "text": "Hardness in behavioral strategies A salient aspect of the previous results, which was intentionally blurred above, is that the learner is allowed to output a mixed strategy\u2014a probability distribution over $\\{0,1\\}^{N}$ . In stark contrast, and perhaps surprisingly, when the learner is constrained to output behavioral strategies, that is to say, points in $[\\bar{0},1]^{N}$ , we show that the problem immediately becomes PPAD-hard even for degree $k=2$ (Theorem 3.3)\u2014thereby being intractable under standard complexity assumptions. We are not aware of any such hardness results pertaining to a natural online learning problem, necessitating the use of mixed strategies. ", "page_idx": 4}, {"type": "text", "text": "The key connection behind our lower bound is an observation by Hazan and Kale [2007], which reveals that any $\\Phi^{\\beta}$ -regret minimizer is inadvertedly able to compute approximate fixed points of any deviation in $\\Phi^{\\beta}$ (Proposition B.1). Computing fixed points is in general a well-known (presumably) intractable problem, being PPAD-hard. In our context, the set $\\Phi^{\\beta}$ does not contain arbitrary (Lipschitz continuous) functions $[0,1]^{N}\\to[0,1]^{N}$ , but instead contains multilinear functions from $[0,\\dot{1}]^{N}$ to $[0,1]^{N}$ . To establish PPAD-hardness for our problem, we start with a generalized circuit (Definition I.3), and we show that all gates can be approximately simulated using exclusively gates involving multilinear operations (Proposition I.7); we defer the formal argument to Appendix I.1. As a result, we arrive at the following hardness result. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3\u221a. If a regret minimizer $\\mathcal{R}$ outputs strategies in $[0,1]^{N}$ , it is PPAD-hard to guarantee $\\overline{{\\mathrm{Reg}}}_{\\Phi^{\\beta}}\\leq\\epsilon/\\sqrt{N}$ , even with respect to low-degree deviations and an absolute constant $\\epsilon>0$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Extensive-form games ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We next expand our scope to arbitrary extensive-form games. We will assume here that the branching factor $b$ of the game is 2\u2014any game can be transformed as such by incurring a $\\log b$ factor overhead in the depth $d$ of the game tree. Generalizing $\\Phi_{\\mathrm{DT}}^{k}$ described above, we introduce the set of $k$ - mediator deviations $\\Phi_{\\mathrm{med}}^{k}$ . Informally, the player here has access to $k$ distinct mediators, which the player can query at any time; a formal definition is given in Section 4. Once again, the case $k=1$ corresponds to linear-swap deviations. Further, if $\\mathcal{X}$ denotes the set of pure strategies, we let $\\Phi_{\\mathrm{poly}}^{k}$ denote the set of all degree- $k$ deviations $\\mathcal X\\rightarrow\\mathcal X$ . We establish similar parameterized results in extensive-form games, but which may now also depend on the depth of the game tree $d$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. There is an online algorithm incurring at most an \u03f5 \u03a6pkoly regret in N O(kd)3/\u03f52 rounds with a per-round running time of $N^{O(k d)^{3}}/\\epsilon$ . For $\\Phi_{\\mathrm{med}}^{k}$ both bounds instead scale as $N^{O(k)}$ . ", "page_idx": 4}, {"type": "text", "text": "We recall that $N$ here denotes the dimension of the strategy space. We further clarify that parameter $k$ appearing in $\\Phi_{\\mathrm{poly}}^{k}$ is different than the $k$ in $\\Phi_{\\mathrm{med}}^{k}$ : the former refers to the degree of a polynomial, while the latter is the number of mediators. As all $k$ -mediator deviations are degree- $k$ polynomials (but not vice versa), it is to be expected that the bound in the theorem above concerning the former is worse. For a fixed degree $k$ and assuming that the game tree is balanced, in the sense that $d=\\mathrm{polylog}\\,N$ , Theorem 3.4 guarantees a quasipolynomial complexity with respect to $\\Phi_{\\mathrm{poly}}^{k}$ \u03a6poly, even when $\\epsilon$ is itself inversely quasipolynomial. The complexity we obtain for \u03a6kmed is more favorable, being polynomial for any extensive-form game.1 Finally, in light of the connection between noregret learning and convergence to correlated equilibria, our results imply parameterized tractability of the equilibrium concepts induced by $\\Phi_{\\mathrm{med}}^{k}$ or $\\Phi_{\\mathrm{poly}}^{k}$ (see Appendix F.1 for a formal treatment). ", "page_idx": 4}, {"type": "text", "text": "4 Technical contributions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From a technical standpoint, our starting point is the familiar template of Gordon et al. [2008] for minimizing $\\Phi$ -regret, which consists of two key components. Accordingly, we split our technical overview into two parts. ", "page_idx": 5}, {"type": "text", "text": "4.1 Circumventing fixed points ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The first key ingredient one requires in the framework of Gordon et al. [2008] is an algorithm for computing an approximate fixed point of any function within the set of deviations. In particular, if $\\mathcal{X}$ is the set of pure strategies and conv $\\mathcal{X}$ is the convex hull of $\\mathcal{X}$ , we now work with functions $\\Phi^{\\delta}\\,\\ni\\,\\phi^{\\delta}:\\,\\mathrm{conv}\\,\\lambda\\,\\to\\,\\mathrm{conv}\\,\\lambda$ , so that fixed points exist by virtue of Brouwer\u2019s theorem.2 As we discussed earlier, this fixed point computation is\u2014at least in some sense\u2014inherent: Hazan and Kale [2007] observed that minimizing $\\Phi^{\\delta}$ -regret is computationally equivalent to computing approximate fixed points of transformations in $\\Phi^{\\delta}$ . Specifically, an efficient algorithm minimizing $\\dot{\\Phi}^{\\delta}$ -regret\u2014 with respect to any sequence of utilities\u2014can be used to compute an approximate fixed point of any transformation in $\\dot{\\Phi}^{\\delta}$ (Proposition B.1 in Appendix B). Given that functions in $\\Phi^{\\delta}$ are generally nonlinear, this brings us to PPAD-hard territory (Theorem 3.3), seemingly contradicting the recent positive results of Dagan et al. [2024] and Peng and Rubinstein [2024]. ", "page_idx": 5}, {"type": "text", "text": "As we have alluded to, it turns out that there is a delicate precondition on the reduction of Hazan and Kale [2007] that makes all the difference: computing approximate fixed points is only necessary if the learner outputs points on conv $\\mathcal{X}$ . In stark contrast, a crucial observation that drives our approach is that a learner who selects a probability distribution over $\\mathcal{X}$ does not have to compute (approximate) fixed points of functions in $\\Phi$ . Instead, we show that it is enough to determine what we refer to as an approximate fixed point in expectation. More precisely, for a deviation $\\Phi\\ni\\phi:\\mathcal{X}\\to\\operatorname{conv}\\mathcal{X}$ with an efficient representation, it is enough to compute a distribution $\\pi\\in\\Delta(\\mathcal{X})$ such that $\\mathbb{E}_{{\\pmb x}\\sim\\pi}\\,\\phi({\\pmb x})\\approx$ $\\mathbb{E}_{\\mathbf{x}\\sim\\pi}\\,x$ . It is quite easy to compute an approximate fixed point in expectation: take any $\\mathbf{\\mathcal{x}_{1}}~\\in$ conv $\\mathcal{X}$ , and consider the sequence $x_{1},\\ldots,x_{L}\\ \\in\\ \\mathrm{conv}\\,\\chi$ such that $\\pmb{x}_{\\ell+1}\\ :=\\ \\mathbb{E}_{\\pmb{x}_{\\ell}^{\\prime}\\sim\\delta(\\pmb{x}_{\\ell})}\\,\\phi(\\pmb{x}_{\\ell}^{\\prime})$ for all $\\ell$ , where $\\delta\\;:\\;\\operatorname{conv}\\lambda\\;\\to\\;\\Delta(\\chi)$ is a mapping such that $\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\delta({\\pmb x})}[{\\pmb x}^{\\prime}]\\;=\\;{\\pmb x}$ .3 Then, for $\\pi:=\\mathbb{E}_{\\ell\\in[L]}[\\delta(\\pmb{x}_{\\ell})]$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\pi}{\\mathbb{E}}[\\phi(\\mathbf{x})-\\mathbf{x}]=\\frac{1}{L}\\sum_{\\ell=1}^{L}\\underset{x_{\\ell}^{\\prime}\\sim\\delta(x_{\\ell})}{\\mathbb{E}}[\\phi(\\pmb{x}_{\\ell}^{\\prime})-\\pmb{x}_{\\ell}^{\\prime}]=\\frac{1}{L}\\underset{x_{L}^{\\prime}\\sim\\delta(x_{L})}{\\mathbb{E}}[\\phi(\\pmb{x}_{L}^{\\prime})-\\pmb{x}_{1}]=O\\left(\\frac{1}{L}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This procedure can replace the fixed point oracle required by the template of Gordon et al. [2008], which is prohibitive when $\\Phi$ contains nonlinear functions, as we formalize in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Application to faster computation of correlated equilibria In fact, even in normal-form games where considering linear deviations suffices, computing a fixed point is relatively expensive, amounting to solving a linear system, dominating the per-iteration complexity. Leveraging instead our new reduction, we obtain the fastest algorithm for computing an approximate correlated equilibrium in the moderate-precision regime (Corollary 4.1). In particular, let us focus for simplicity on $n$ - player normal-form games with a succinct representation. Here, each player $i\\in[n]$ selects as strategy a probability distribution $\\pi_{i}\\in\\Delta(A_{i})$ , where we recall that $\\mathcal{A}_{i}$ is a finite set of available actions. The expected utility of player $i$ is given by $u_{i}(\\pi_{1},\\ldots,\\pi_{n})\\;:=\\;\\mathbb{E}_{a_{1}\\sim\\pi_{1},\\ldots,a_{n}\\sim\\pi_{n}}[u_{i}(a_{1},\\ldots,a_{n})],$ where $u_{i}:A_{1}\\times\\dots\\times A_{n}\\rightarrow[-1,1]$ . We assume that there is an expectation oracle that computes the vector ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left(u_{i}(a_{i},\\pi_{-i})\\right)_{i\\in[n],a_{i}\\in\\mathcal{A}_{i}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in time bounded by $\\mathsf E0(n,A)$ , where $A:=\\,\\operatorname*{max}_{i}|A_{i}|$ ; it is known that $\\mathsf E0(n,A)\\,\\leq\\,\\mathsf{p o l y}(n,A)$ for most interesting classes of succinct classes of games [Papadimitriou and Roughgarden, 2008]. Using our framework, we arrive at the following result. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.1. For any $n$ -player game in normal form, there is an algorithm that computes an $\\epsilon$ -correlated equilibrium and runs in time ", "page_idx": 6}, {"type": "equation", "text": "$$\nO\\left({\\frac{A\\log A}{\\epsilon^{2}}}\\left(\\mathsf{E O}(n,A)+n{\\frac{A^{2}}{\\epsilon}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assuming that the oracle call to (2) $(\\mathsf E\\mathsf{O}(n,A))$ does not dominate the per-iteration running time\u2014 which is indeed the case in, for example, polymatrix games\u2014Corollary 4.1 gives (to our knowledge) the fastest algorithm for computing $\\epsilon$ -correlated equilibria in the moderate-precision regime $1/\\!\\!\\!\\stackrel{\\cdot}{A}\\!\\!\\!^{\\frac{\\omega}{2}-1}\\,\\leq\\,\\epsilon\\,\\leq\\,\\bar{1}/\\log A$ , where $\\omega\\,\\approx\\,2.37$ is the exponent of matrix multiplication [Williams et al., 2024]; withou\u221at fast matrix multiplication, which is widely impractical, the lower bound instead reads $\\epsilon\\,\\geq\\,1/\\sqrt{A}$ . We provide a comparison with previous algorithms in Table 1 and defer the details to Appendix I.3. Finally, we stress that similar improvements can be obtained beyond normal-form games using our template; indeed, virtually all prior $\\Phi$ -regret minimizers rely on some fixed point operation. ", "page_idx": 6}, {"type": "table", "img_path": "c4ElkpA0kh/tmp/637d1e0168e438b8e67a2b5f103e5cfcd501bb6c00501829f22d23e6cb87896a.jpg", "table_caption": ["Table 1: Time complexity for computing $\\epsilon$ -correlated equilibria in $n$ -player normal-form games with $A$ actions per player. The second column suppresses absolute constants and polylogarithmic factors. For simplicity, issues related to bit complexity have been ignored (that is, we work in the RealRAM model of computation). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Before moving on, it is worth stressing that the discrepancy that has arisen between operating over $\\Delta(\\mathcal{X})$ versus conv $\\mathcal{X}$ is quite singular when it comes to regret minimization in extensive-form games and beyond. Kuhn\u2019s theorem [Kuhn, 1953] is often invoked to argue about their equivalence, but in our setting it is the nonlinear nature of deviations in $\\Phi$ that invalidates that equivalence.4 To tie up the loose ends, we adapt the reduction of Hazan and Kale [2007] to show that minimizing $\\Phi$ -regret over $\\Delta(\\mathcal X)$ necessitates computing approximate fixed points in expectation (Proposition C.3), and we observe that the reductions of Dagan et al. [2024] and Peng and Rubinstein [2024] are indeed compatible with computing approximate fixed points in expectation; the latter observation is made precise in Appendix F.3. ", "page_idx": 6}, {"type": "text", "text": "4.2 Regret minimization over the set of deviations $\\Phi$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The second ingredient prescribed by Gordon et al. [2008] is an algorithm minimizing external regret but with respect to the set of deviations $\\Phi$ . The crux in this second step lies in the fact that, even in normal-form games, $\\Phi$ contains at least an exponential number of deviations, so black-box reductions are of little use here. Instead, the problem boils down to appropriately leveraging the combinatorial structure of $\\Phi$ , as we explain below. ", "page_idx": 6}, {"type": "text", "text": "We will first describe our approach when $\\mathcal{X}=\\{0,1\\}^{N}$ , and we then proceed with the more technical generalization to extensive-form games. The key observation here is that regret minimization over $\\Phi_{\\mathrm{DT}}^{k}$ can be viewed as a tree-form decision problem of size $N^{O(k)}$ . Terminal nodes in this decision problem are identified by the original index $j_{0}~\\in~[N]$ , the queries $j_{1},\\ldots,j_{k}\\,\\in\\,[N]$ , their replies $a_{1},\\ldots,a_{k}\\in\\{0,1\\}$ , and finally the action $a_{0}\\in\\{0,1\\}$ that is played. Each tree-form strategy $\\pmb q$ in this decision problem defines a function $\\phi_{\\pmb q}:\\mathcal X\\rightarrow\\mathrm{conv}\\,\\mathcal X$ , which is computed by following the strategy $\\pmb q$ through the decision problem. Formally, we have ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\phi_{\\pmb q}(\\pmb x)[j_{0}]=\\sum_{j_{1},a_{1},\\dots,j_{k},a_{k}}\\pmb q[j_{0},j_{1},a_{1},\\dots,j_{k},a_{k},1]\\prod_{i=1}^{k}\\pmb x[j_{i},a_{i}]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\pmb{x}[j_{i},a_{i}]=\\pmb{x}[j_{i}]$ if $a_{i}=1$ , and $1-{\\pmb x}[j_{i}]$ if $a_{i}=0$ . Hence $\\phi_{q}$ is a degree- $k$ polynomial in $\\textbf{\\em x}$ . ", "page_idx": 7}, {"type": "text", "text": "Now, since ${\\pmb q}\\mapsto\\,\\phi_{{\\pmb q}}({\\pmb x})[i]$ is linear, it follows that ${\\pmb q}\\,\\mapsto\\,\\langle{\\pmb u},\\phi_{{\\pmb q}}({\\pmb x})\\rangle$ is also linear for any given $\\pmb{u}\\in\\mathbb{R}^{n}$ . Therefore, a regret minimizer on $\\Phi_{\\mathrm{DT}}^{k}$ can be constructed starting from any regret minimizer for tree-form decision problems; for example, counterfactual regret minimization [Zinkevich et al., 2007], or any of its modern variants. This enables us to rely on usual techniques for dealing with such problems, eventually leading to a complexity bound of $\\overset{\\cdot}{N}^{O(k)}$ , as we formalize in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "For the set of low-degree polynomials $\\Phi_{\\mathrm{poly}}^{k}$ , we leverage a result from Boolean analysis relating (randomized) low-depth decision trees with low-degree polynomials, stated below. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2 (Midrijanis, 2004). Every degree- $k$ polynomial $f:\\{0,1\\}^{N}\\rightarrow\\{0,1\\}$ can be written as a decision tree of depth at most $2k^{3}$ . ", "page_idx": 7}, {"type": "text", "text": "In particular, this implies that $\\Phi_{\\mathrm{poly}}^{k}\\subseteq\\Phi_{\\mathrm{DT}}^{2k^{3}}$ . Consequently, low-degree polynomials can be reduced to low-depth decision trees, albeit with an overhead in the exponent. ", "page_idx": 7}, {"type": "text", "text": "Turning to general extensive-form games, we follow a similar blueprint, although there are now additional technical challenges. In particular, in what follows, to describe the set of deviations it will be convenient to introduce a new formalism related to tree-form decision problems. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.3. The dual $\\bar{\\chi}$ of $\\mathcal{X}$ is the decision problem identical to $\\mathcal{X}$ , except that the decision points and observation points have been swapped. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.4. The interleaving $\\mathcal{X}\\otimes\\mathcal{Y}$ is the tree-form decision problem defined as follows. There is a state $\\pmb{\\mathscr{s}}=(\\mathscr{s}_{1},\\mathscr{s}_{2})\\in\\mathcal{S}_{1}\\times\\bar{S_{2}}$ . The root state is the tuple $(\\emptyset,\\emptyset)$ . The decision problem is defined by the player being able to interact with both decision problems, in the following manner. At each state $\\pmb{s}=(s_{1},s_{2})$ : ", "page_idx": 7}, {"type": "text", "text": "\u2022 If $s_{1}$ and $s_{2}$ are both terminal then so is $\\pmb{s}$ . Otherwise: ", "page_idx": 7}, {"type": "text", "text": "\u2022 If either of the $s_{i}\\mathbf{s}$ is an observation point, then so is $\\pmb{s}$ . The children are the states $(s_{i}^{\\prime},s_{-i})$ where $s_{i}^{\\prime}$ is a child of $s_{i}$ . (If both $s_{i}\\mathbf{s}$ are observation points, both children $s_{1}^{\\prime},s_{2}^{\\prime}$ are selected simultaneously. This can only happen at the root.)   \n\u2022 Otherwise, $\\pmb{s}$ is a decision point. The player selects an index $i\\in\\{1,2\\}$ at which to act, and a child $s_{i}^{\\prime}$ to transition to. The next state is $(s_{i}^{\\prime},s_{-i})$ . ", "page_idx": 7}, {"type": "text", "text": "In $\\mathcal{X}\\otimes\\mathcal{Y}$ , the same state $\\left(s_{1},s_{2}\\right)$ can be reachable through possibly exponentially many paths, because the learner may choose to interleave actions in $\\mathcal{X}$ with actions in $\\boldsymbol{\\wp}$ in any order. Thus, each state $\\left(s_{1},s_{2}\\right)$ corresponds to actually exponentially many histories in $\\mathcal{X}\\otimes\\mathcal{Y}$ . In the discussion below, we will therefore carefully distinguish between histories and states. In light of the above exponential gap between histories and states, it seems wasteful to represent $\\mathcal{X}\\otimes\\mathcal{Y}$ as a tree. Indeed, Zhang et al. [2023] recently studied $D A G$ -form decision problems, and showed that regret minimization on them is possible so long as the DAG obeys some natural properties. ", "page_idx": 7}, {"type": "text", "text": "Using the language we have now introduced, we can define the set of $k$ -mediator deviations $\\Phi_{\\mathrm{med}}^{k}$ as the set of reduced strategies in the decision problem $\\mathcal{X}\\otimes\\bar{\\mathcal{X}}^{\\otimes k}$ . That is, the player has access to not one but $k$ mediators, all holding strategy $\\textbf{\\em x}$ , which the player can query at any time. This is a significant advantage over having just one mediator since the player can send different queries to each of the $k$ mediators (who must all reply according to $\\textbf{\\em x}$ ), and therefore can learn more about the strategy $\\textbf{\\em x}$ than it could have otherwise. We will call the responses given by the mediator action recommendations. For a graphical illustration of such deviations, we refer to Figure 2 (in Appendix E). ", "page_idx": 7}, {"type": "text", "text": "Reduced strategies $q\\in\\pi(\\mathcal{X}\\otimes\\bar{\\mathcal{X}}^{\\otimes k})$ , once again, induce functions $\\phi_{\\pmb{q}}:\\mathcal{X}\\rightarrow\\mathrm{conv}\\,\\mathcal{X}$ given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\phi_{\\pmb q}(\\pmb x)[z]=\\sum_{z_{1},\\ldots,z_{k}}\\pmb q[z,z_{1},\\ldots,z_{k}]\\prod_{i=1}^{k}\\pmb x[z_{i}],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and in particular we have that $\\phi_{q}$ is a degree- $k$ polynomial. We define \u03a6kmed as the set of such deviations. For that set, we show that there is a reduction to a particular type of DAG-form decision problem of size $N^{O(k)}$ . As we explained, that formulation is more suitable than tree-form decision problems when the number of possible histories far exceeds the number of states, which is precisely the case when the player is gradually querying multiple mediators as the game progresses. ", "page_idx": 8}, {"type": "text", "text": "Finally, we establish a reduction from low-degree polynomials to having few mediators; namely, we show that \u03a6pkoly $\\Phi_{\\mathrm{poly}}^{k}\\subseteq\\Phi_{\\mathrm{med}}^{O(k d)^{3}}$ , where we recall that $d$ is the depth of the game tree. Our basic strategy is to again leverage the connection between low-depth decision trees and low-degree polynomials we described earlier (Theorem 4.2). To do so, we need to cast our problem in terms of functions $\\{0,1\\}^{N}\\to\\{0,1\\}^{N}$ instead of $\\mathcal X\\rightarrow\\mathcal X$ . To that end, we first show how to extend a degree- $k$ function $f:\\mathcal{X}\\to\\{0,1\\}$ to a degree- $k d$ function $\\bar{f}:\\{0,1\\}^{N}\\,\\rightarrow\\,\\{0,1\\}$ ; that is, $\\bar{f}$ coincides with $f$ on all points in $\\dot{\\mathcal{X}}\\subseteq\\dot{\\{0,1\\}}^{N}$ (Lemma E.7). This step is where the overhead factor $d$ comes from. The final technical piece is to show that if each component of $\\phi:\\mathcal{X}\\rightarrow\\mathcal{X}$ can be expressed using $K$ mediators, the same holds for $\\phi$ ; the naive argument here incurs another factor of $d$ , but we show that this is in fact not necessary. The details of the above argument are deferred to Appendix E. ", "page_idx": 8}, {"type": "text", "text": "5 Further related research ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A key reference point is the result of Blum and Mansour [2007], and a generalization due to Gordon et al. [2008], which reduces minimizing swap regret to minimizing external regret. Specifically, for the probability simplex $\\Delta(A)$ , it maintains a separate external-regret minimizer, one for each action $a\\in A$ . Both the per-iteration complexity and the number of iterations required is generally polynomial in $A:=|{\\mathcal{A}}|$ . Therefore, in settings where $A$ is exponentially large in the natural parameters of the problem (such as extensive-form games) it does not appear that the reduction of Blum and Mansour [2007] is of much use. It is tempting to instead rely on the reduction of Stoltz and Lugosi [2005] for minimizing internal regret, a weaker notion than swap regret, which is nonetheless sufficient for (asymptotic) convergence to correlated equilibria. However, one should be careful when relying on internal regret in settings where $A$ is exponentially large; as we point out in Remark A.1, internal regret can be smaller than swap regret by up to a factor of $A$ , so it is only meaningful when $\\epsilon\\,\\leq\\,1/A$ , a regime which is generally out of reach for regret minimization techniques when $A$ is exponentially large. ", "page_idx": 8}, {"type": "text", "text": "This gap motivated the new reduction by Dagan et al. [2024] and Peng and Rubinstein [2024], which we discussed earlier. Beyond extensive-form games, those reductions apply whenever it is possible to minimize external regret efficiently. The complexity of computing correlated equilibria beyond the regime where the precision parameter $\\epsilon$ is an absolute constant remains a major open problem, generally conjectured to be hard [von Stengel and Forges, 2008]; the recent online lower bound in the adversarial setting [Daskalakis et al., 2024] provides further evidence in support of that conjecture. ", "page_idx": 8}, {"type": "text", "text": "As a result, most prior work has focused on more permissive equilibrium concepts, understood through the framework of $\\Phi$ -regret [Morrill et al., 2021a,b, Farina et al., 2022, Bai et al., 2022, Bernasconi et al., 2023, Noarov et al., 2023, Dud\u00b4\u0131k and Gordon, 2009, Gordon et al., 2008, Fujii, 2023, Dann et al., 2023, Mansour et al., 2022, Sharma, 2024, Cai et al., 2024a]). In terms of the most recent developments, Farina and Pipis [2023] established efficient learning dynamics minimizing what is referred to as linear swap regret (cf. Dann et al. [2023], Fujii [2023] for related results in Bayesian games). The solution concept that arises from linear swap regret was later endowed with a natural mediator-based interpretation by Zhang et al. [2024], which can be viewed as a natural precursor to this work. Convergence to correlated equilibria has also attracted attention in the context of Markov (aka. stochastic) games (e.g., [Cai et al., 2024b, Jin et al., 2021, Erez et al., 2023, Liu and Zhang, 2023], and references therein). ", "page_idx": 8}, {"type": "text", "text": "Moreover, as we explained earlier, our approach also gives rise to a faster algorithm for computing approximate correlated equilibria in a certain regime. As we discuss further in Appendix I.3, improving the per-iteration complexity of Blum and Mansour [2007] has received interest in prior work [Ito, 2020, Greenwald et al., 2006, Yang and Mohri, 2017] (see also [Huang and Pan, 2023, Huang et al., 2023]). The main bottleneck lies in the (approximate) computation of a stationary distribution of a Markov chain, which can be phrased as a linear system. It is worth noting that solving linear systems faster than matrix multiplication even for a crude approximation is precluded, at least subject to fine-grained complexity assumptions [Bafna and Vyas, 2021]; we are not aware whether such hardness results are also known for computing the stationary distribution of a Markov chain. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Finally, although we have so far mostly directed our attention to the game-theoretic implication of minimizing swap (or indeed $\\Phi$ ) regret, namely the celebrated connection with correlated equilibria in repeated games, the notion of swap regret is a fundamental solution concept in its own right more broadly in online learning and learning theory. Compared to the more common notion of external regret, swap regret gives rise to a more appealing notion of hindsight rationality; as such, it is often adopted as a behavioral assumption to model learning agents (e.g., [Deng et al., 2019]). It is also fundamentally tied to the notion of calibration [Hu and Wu, 2024], and recently inspired work by Gopalan et al. [2023] in the context of multi-group fairness. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions and future research ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We provided a new family of parameterized algorithms for minimizing $\\Phi$ -regret in extensive-form games. Our results capture perhaps the most natural class of functions interpolating between linearswap and swap deviations, namely degree- ${\\cdot k}$ deviations. Along the way, we refined the usual template for minimizing $\\Phi$ -regret\u2014taught in many courses on algorithmic game theory and online learning\u2014which revolves around (approximate) fixed points [Gordon et al., 2008, Blum and Mansour, 2007, Stoltz and Lugosi, 2005]. Instead, we showed that it suffices to rely on a relaxation that we refer to as an approximate fixed point in expectation, which\u2014unlike actual fixed points\u2014can always be computed efficiently. Our refinement of the usual template for minimizing $\\Phi$ -regret is of independent interest beyond extensive-form games. For example, it can speed up the computation of approximate correlated equilibria even in normal-form games, as it obviates the need to solve a linear system in every round. As in the recent works by Dagan et al. [2024] and Peng and Rubinstein [2024], a crucial feature of our approach is to allow the learner to select a distribution over pure strategies, for otherwise we showed that regret minimization immediately becomes PPAD-hard (under a strongly adaptive adversary). ", "page_idx": 9}, {"type": "text", "text": "There are many interesting avenues for future research. First, the complexity of our algorithm pertaining to degree- $k$ deviations depends exponentially on the depth of the game tree. We suspect that such a dependency could be superfluous. To show this, it would be enough to refine Lemma E.7 by coming up with an extension whose degree does not depend on the depth of the game tree. It would also be interesting to devise parameterized algorithms for $k$ -mediator deviations that recover as a special case the PTAS of Peng and Rubinstein [2024] and Dagan et al. [2024], so as to smoothly interpolate between existing results for linear-swap regret [Farina and Pipis, 2023] and the aforementioned results for swap regret; is $k=\\tilde{O}(1/\\epsilon)$ enough to capture swap regret? ", "page_idx": 9}, {"type": "text", "text": "Finally, perhaps the most important question is to understand the computational complexity of computing $\\Phi$ -equilibria in extensive-form games. In particular, our results raise the interesting question of whether there is an algorithm (in the centralized model) for computing in polynomial time an exact correlated equilibrium induced by low-degree deviations. Extending the paradigm of Papadimitriou and Roughgarden [2008] in that setting presents several challenges, not least because computing fixed points\u2014which are crucial for implementing the separation oracle [Papadimitriou and Roughgarden, 2008]\u2014is now computationally hard. Relatedly, we suspect that there is an inherent connection between fixed points and correlated equilibria, in the spirit of the equivalence between $\\Phi$ -regret minimization and fixed points established by Hazan and Kale [2007]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are grateful to the anonymous reviewers at NeurIPS for their helpful feedback. This material is based on work supported by the Vannevar Bush Faculty Fellowship ONR N00014-23-1-2876, National Science Foundation grants RI-2312342 and RI-1901403, ARO award W911NF2210266, and NIH award A240108S001. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ioannis Anagnostides, Constantinos Daskalakis, Gabriele Farina, Maxwell Fishelson, Noah Golowich, and Tuomas Sandholm. Near-optimal no-regret learning for correlated equilibria in ", "page_idx": 9}, {"type": "text", "text": "multi-player general-sum games. In STOC \u201922: 54th Annual ACM SIGACT Symposium on Theory of Computing, 2022, pages 736\u2013749. ACM, 2022.   \nRobert Aumann. Subjectivity and correlation in randomized strategies. Journal of Mathematical Economics, 1:67\u201396, 1974.   \nYakov Babichenko, Christos H. Papadimitriou, and Aviad Rubinstein. Can almost everybody be almost happy? In Conference on Innovations in Theoretical Computer Science (ITCS), 2016.   \nMitali Bafna and Nikhil Vyas. Optimal fine-grained hardness of approximation of linear equations. In International Colloquium on Automata, Languages, and Programming, (ICALP), pages 20:1\u2013 20:19, 2021.   \nYu Bai, Chi Jin, Song Mei, Ziang Song, and Tiancheng Yu. Efficient phi-regret minimization in extensive-form games via online mirror descent. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2022.   \nMartino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Francesco Trovo\\`, and Nicola Gatti. Constrained phi-equilibria. In International Conference on Machine Learning (ICML), 2023.   \nAvrim Blum and Yishay Mansour. From external to internal regret. J. Mach. Learn. Res., 8:1307\u2013 1324, 2007.   \nYang Cai, Constantinos Daskalakis, Haipeng Luo, Chen-Yu Wei, and Weiqiang Zheng. Tractable local equilibria in non-concave games. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2024a.   \nYang Cai, Haipeng Luo, Chen-Yu Wei, and Weiqiang Zheng. Near-optimal policy optimization for correlated equilibrium in general-sum markov games. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2024b.   \nNicolo Cesa-Bianchi and Ga\u00b4bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \nXi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player Nash equilibria. Journal of the ACM, 2009.   \nXinyi Chen, Angelica Chen, Dean Foster, and Elad Hazan. Ai safety by debate via regret minimization, 2023.   \nMichael B. Cohen, Jonathan A. Kelner, John Peebles, Richard Peng, Anup B. Rao, Aaron Sidford, and Adrian Vladu. Almost-linear-time algorithms for markov chains and new spectral primitives for directed graphs. In Proceedings of the Annual Symposium on Theory of Computing (STOC), pages 410\u2013419. ACM, 2017.   \nYuval Dagan, Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. From external to swap regret 2.0: An efficient reduction and oblivious adversary for large action spaces. Proceedings of the Annual Symposium on Theory of Computing (STOC), 2024.   \nChristoph Dann, Yishay Mansour, Mehryar Mohri, Jon Schneider, and Balasubramanian Sivan. Pseudonorm approachability and applications to regret minimization. In International Conference on Algorithmic Learning Theory (ALT), 2023.   \nConstantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning in general games. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), pages 27604\u201327616, 2021.   \nConstantinos Daskalakis, Gabriele Farina, Noah Golowich, Tuomas Sandholm, and Brian Hu Zhang. Exponential lower bounds for minimizing swap regret in extensive-form games, 2024.   \nArgyrios Deligkas, John Fearnley, Alexandros Hollender, and Themistoklis Melissourgos. Purecircuit: Strong inapproximability for PPAD. In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS), pages 159\u2013170, 2022.   \nYuan Deng, Jon Schneider, and Balasubramanian Sivan. Strategizing against no-regret learners. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), pages 1577\u20131585, 2019.   \nMiroslav Dud\u00b4\u0131k and Geoffrey J. Gordon. A sampling-based approach to computing equilibria in succinct extensive-form games. In Jeff A. Bilmes and Andrew Y. Ng, editors, UAI 2009, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, Montreal, QC, Canada, June 18-21, 2009, pages 151\u2013160. AUAI Press, 2009.   \nLiad Erez, Tal Lancewicki, Uri Sherman, Tomer Koren, and Yishay Mansour. Regret minimization and convergence to equilibria in general-sum markov games. In International Conference on Machine Learning (ICML), 2023.   \nGabriele Farina and Charilaos Pipis. Polynomial-time linear-swap regret minimization in imperfectinformation sequential games. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2023.   \nGabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Simple uncoupled no-regret learning dynamics for extensive-form correlated equilibrium. Journal of the ACM, 69(6), 2022.   \nAris Filos-Ratsikas, Yiannis Giannakopoulos, Alexandros Hollender, Philip Lazos, and Diogo Po\u00b8cas. On the complexity of equilibrium computation in first-price auctions. SIAM Journal on Computing, 52(1):80\u2013131, 2023.   \nDean Foster and Rakesh Vohra. Calibrated learning and correlated equilibrium. Games and Economic Behavior, 21:40\u201355, 1997.   \nDean P. Foster and Sergiu Hart. Smooth calibration, leaky forecasts, finite recall, and nash dynamics. Games and Economic Behavior, 109:271\u2013293, 2018.   \nKaito Fujii. Bayes correlated equilibria and no-regret dynamics. CoRR, abs/2304.05005, 2023.   \nAnat Ganor and Karthik C. S. Communication complexity of correlated equilibrium with small support. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM), 2018.   \nPaul W. Goldberg and Aaron Roth. Bounds for the query complexity of approximate equilibria. ACM Trans. Economics and Comput., 4(4):24:1\u201324:25, 2016.   \nParikshit Gopalan, Michael P. Kim, and Omer Reingold. Swap agnostic learning, or characterizing omniprediction via multicalibration. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2023.   \nGeoffrey J Gordon, Amy Greenwald, and Casey Marks. No-regret learning in convex games. In International Conference on Machine Learning (ICML), 2008.   \nAmy Greenwald and Keith Hall. Correlated Q-learning. In International Conference on Machine Learning (ICML), 2003.   \nAmy Greenwald, Zheng Li, and Casey Marks. Bounds for regret-matching algorithms. In ISAIM, 2006.   \nAmy Greenwald, Zheng Li, and Warren Schudy. More efficient internal-regret-minimizing algorithms. In Conference on Learning Theory (COLT), pages 239\u2013250, 2008.   \nMartin Gro\u00a8tschel, La\u00b4szlo\u00b4 Lova\u00b4sz, and Alexander Schrijver. The ellipsoid method and its consequences in combinatorial optimization. Combinatorica, 1:169\u2013197, 1981.   \nSergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68:1127\u20131150, 2000.   \nElad Hazan and Satyen Kale. Computational equivalence of fixed points and no regret algorithms, and convergence to equilibria. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2007.   \nLunjia Hu and Yifan Wu. Predict to minimize swap regret for all payoff-bounded tasks. In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS), 2024.   \nWan Huang and Bernhard von Stengel. Computing an extensive-form correlated equilibrium in polynomial time. In International Workshop On Internet And Network Economics (WINE), 2008.   \nZhiming Huang and Jianping Pan. A near-optimal high-probability swap-regret upper bound for multi-agent bandits in unknown general-sum games. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), 2023.   \nZhiming Huang, Kaiyang Liu, and Jianping Pan. End-to-end congestion control as learning for unknown games with bandit feedback. In International Conference on Distributed Computing Systems (ICDCS), 2023.   \nShinji Ito. A tight lower bound and efficient reduction for swap regret. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2020.   \nAlbert Jiang and Kevin Leyton-Brown. Polynomial-time computation of exact correlated equilibrium in compact games. In Proceedings of the ACM Conference on Electronic Commerce (EC), 2011.   \nChi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning - A simple, efficient, decentralized algorithm for multiagent RL. CoRR, abs/2110.14555, 2021.   \nH. W. Kuhn. Extensive games and the problem of information. In H. W. Kuhn and A. W. Tucker, editors, Contributions to the Theory of Games, volume 2 of Annals of Mathematics Studies, 28, pages 193\u2013216. Princeton University Press, Princeton, NJ, 1953.   \nNicolas S. Lambert, Adrian Marple, and Yoav Shoham. On equilibria in games with imperfect recall. Games and Economic Behavior, 113:164\u2013185, 2019.   \nXiangyu Liu and Kaiqing Zhang. Partially observable multi-agent RL with (quasi-)efficiency: The blessing of information sharing. In International Conference on Machine Learning (ICML), pages 22370\u201322419, 2023.   \nYishay Mansour, Mehryar Mohri, Jon Schneider, and Balasubramanian Sivan. Strategizing against learners in bayesian games. In Conference on Learning Theory (COLT), 2022.   \nGatis Midrijanis. Exact quantum query complexity for total boolean functions. arXiv preprint quant-ph/0403168, 2004.   \nDustin Morrill, Ryan D\u2019Orazio, Marc Lanctot, James R. Wright, Michael Bowling, and Amy R. Greenwald. Efficient deviation types and learning for hindsight rationality in extensive-form games. In International Conference on Machine Learning (ICML), 2021a.   \nDustin Morrill, Ryan D\u2019Orazio, Reca Sarfati, Marc Lanctot, James R. Wright, Amy R. Greenwald, and Michael Bowling. Hindsight and sequential rationality of correlated play. In Conference on Artificial Intelligence (AAAI), 2021b.   \nJohn Nash. Equilibrium points in n-person games. Proceedings of the National Academy of Sciences, 36:48\u201349, 1950.   \nGeorgy Noarov, Ramya Ramalingam, Aaron Roth, and Stephan Xie. High-dimensional prediction for sequential decision making. CoRR, abs/2310.17651, 2023.   \nRyan O\u2019Donnell. Analysis of boolean functions. Cambridge University Press, 2014.   \nChristos H. Papadimitriou and Tim Roughgarden. Computing correlated equilibria in multi-player games. Journal of the ACM, 55(3):14:1\u201314:29, 2008.   \nBinghui Peng and Aviad Rubinstein. Fast swap regret minimization and applications to approximate correlated equilibria. Proceedings of the Annual Symposium on Theory of Computing (STOC), 2024.   \nMichele Piccione and Ariel Rubinstein. On the interpretation of decision problems with imperfect recall. Games and Economic Behavior, pages 3\u201324, 1997.   \nAlexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Beyond regret. In Conference on Learning Theory (COLT), 2011.   \nAviad Rubinstein. Settling the complexity of computing approximate two-player nash equilibria. In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS), 2016.   \nDravyansh Sharma. No internal regret with non-convex loss functions. In Conference on Artificial Intelligence (AAAI), 2024.   \nYoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Cambridge University Press, 2009.   \nGilles Stoltz and Ga\u00b4bor Lugosi. Internal regret in on-line portfolio selection. Machine Learning, 59 (1-2):125\u2013159, 2005.   \nGilles Stoltz and Ga\u00b4bor Lugosi. Learning correlated equilibria in games with compact sets of strategies. Games and Economic Behavior, 59(1):187\u2013208, 2007.   \nEmanuel Tewolde, Caspar Oesterheld, Vincent Conitzer, and Paul W. Goldberg. The computational complexity of single-player imperfect-recall games. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2023.   \nBernhard von Stengel and Fran\u00b8coise Forges. Extensive-form correlated equilibrium: Definition and computational complexity. Mathematics of Operations Research, 33(4):1002\u20131022, 2008.   \nVirginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New bounds for matrix multiplication: from alpha to omega. In Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2024.   \nScott Yang and Mehryar Mohri. Online learning with transductive regret. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 5214\u20135224, 2017.   \nBrian Hu Zhang, Gabriele Farina, and Tuomas Sandholm. Team belief DAG: generalizing the sequence form to team games for fast computation of correlated team max-min equilibria via regret minimization. In International Conference on Machine Learning (ICML), volume 202 of Proceedings of Machine Learning Research, pages 40996\u201341018. PMLR, 2023.   \nBrian Hu Zhang, Gabriele Farina, and Tuomas Sandholm. Mediator interpretation and faster learning algorithms for linear correlated equilibria in general extensive-form games. In International Conference on Learning Representations (ICLR), 2024.   \nMartin Zinkevich, Michael Bowling, Michael Johanson, and Carmelo Piccione. Regret minimization in games with incomplete information. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), 2007. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Further preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we introduce some further preliminaries. For additional background, we refer the interested reader to the excellent books of Cesa-Bianchi and Lugosi [2006] and Shoham and LeytonBrown [2009]. Before we describe more formally the construction of Gordon et al. [2008], we make a remark regarding minimizing internal regret in extensive-form games. ", "page_idx": 14}, {"type": "text", "text": "Remark A.1 (Swap versus internal regret). When it comes to defining correlated equilibria in normal-form games, there are two prevalent definitions appearing in the literature; one is based on internal regret, while the other on swap regret (e.g., [Ganor and Karthik C. S., 2018, Goldberg and Roth, 2016]). The key difference is that internal regret only contains deviations that swap a single action\u2014thereby being weaker. Nevertheless, it is not hard to see that swap regret can only be larger by a factor of $|{\\mathcal{X}}|$ [Blum and Mansour, 2007], where we recall that $\\mathcal{X}$ denotes the set of pure strategies. So, in normal-form games those two definitions are polynomially equivalent, and in most applications one can safely switch from one to the other. ", "page_idx": 14}, {"type": "text", "text": "However, this is certainly not the case in games with an exponentially large action space, such as extensive-form games. In fact, the definition of internal regret itself is problematic when the action set is exponentially large: the uniform distribution always attains an error of at most $1/|\\mathcal{X}|$ . Consequently, any guarantee for $\\epsilon\\,\\geq\\,1/|\\mathcal{X}|$ is vacuous. That is, if $|\\mathcal{X}|$ is exponentially large, an algorithm that requires a number of iterations polynomial in $1/\\epsilon$ \u2014which is what we expect to get from typical no-regret dynamics\u2014would need an exponential number of iterations to yield a nontrivial guarantee; this issue with internal regret was also observed by Fujii [2023]. Nevertheless, internal regret in the context of games with an exponentially large action set was used in a recent work by Chen et al. [2023], who provided oracle-efficient algorithms for minimizing internal regret. ", "page_idx": 14}, {"type": "text", "text": "A.1 The construction of Gordon et al. [2008] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Gordon et al. [2008], building on earlier work by Blum and Mansour [2007] and Stoltz and Lugosi [2005], came up with a general recipe for minimizing $\\Phi^{\\delta}$ -regret. That construction relies on a noregret learning algorithm on the set of deviations $\\Phi^{\\delta}$ , which we denote by $\\mathcal{R}_{\\Phi}$ . Then, a $\\Phi^{\\delta}$ -regret minimizer on conv $\\mathcal{X}$ can be constructed as follows: on each iteration $t\\,=\\,1,\\ldots,T$ , the learner performs the following steps. ", "page_idx": 14}, {"type": "text", "text": "The main guarantee regarding the above algorithm is summarized below. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.2 (Gordon et al., 2008). Suppose that $\\overline{{\\mathrm{Reg}}}^{T}$ is the external regret incurred by $\\mathcal{R}_{\\Phi}$ . After $T$ rounds of the above algorithm, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi^{\\delta}\\in\\Phi^{\\delta}}\\frac{1}{T}\\sum_{t=1}^{T}\\left\\langle\\pmb{u}^{(t)},\\phi^{\\delta}(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}\\right\\rangle\\leq\\overline{{\\mathrm{Reg}}}^{T}+\\epsilon.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In Appendix C.1, we will relax the requirement of needing (approximate) fixed points, while at the same time maintaining the guarantee of Theorem A.2. ", "page_idx": 14}, {"type": "text", "text": "B Hardness of minimizing $\\Phi$ -regret in behavioral strategies ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we show that if the learner is constrained to output in reach round a strategy in conv $\\mathcal{X}$ , then there is no efficient algorithm (under standard complexity assumptions) minimizing $\\Phi^{\\beta}$ -regret (Theorem 3.3); here, $\\beta:\\mathrm{conv}\\,\\mathcal{X}\\to\\Delta(\\mathcal{X})$ is the behavioral strategy mapping (introduced in the sequel as Definition C.5), the expression of which is not important for the purpose of this section. The key connection is a result by Hazan and Kale [2007], showing that any $\\Phi^{\\beta}$ -regret minimizer is able to compute approximate fixed points of any deviations in $\\Phi^{\\widetilde{\\beta}}$ . We then show that the set of induced deviations, even on the hypercube $\\mathcal{X}=\\{0,1\\}^{N}$ , is rich enough to approximate PPAD-hard fixed-point problems. ", "page_idx": 14}, {"type": "text", "text": "In this context, consider a transformation $\\Phi^{\\beta}\\;\\ni\\;\\phi^{\\beta}\\;:\\;[0,1]^{N}\\;\\to\\;[0,1]^{N}$ for which we want to compute an approximate fixed point $\\pmb{x}\\,\\in\\,\\mathrm{conv}\\,\\pmb{\\chi}$ ; that is, $\\|\\phi^{\\beta}(\\pmb{x})-\\pmb{x}\\|_{2}\\,\\leq\\,\\epsilon$ , for some precision parameter $\\epsilon>0$ . (It is convenient in the construction below to measure the fixed-point error with respect to $\\|\\cdot\\|_{2}.)$ Hazan and Kale [2007] observed that a $\\Phi^{\\beta}$ -regret minimizer can be readily turned into an algorithm for computing fixed points of any function in $\\Phi^{\\beta}$ , as stated formally below. Before we proceed, we remind that here and throughout we operate under a strongly adaptive adversary, which is quite crucial in the construction of Hazan and Kale [2007]. ", "page_idx": 15}, {"type": "text", "text": "Proposition B.1 (Hazan and Kale, 2007). Consider a regret minimizer $\\mathcal{R}$ operating over $[0,1]^{N}$ . If $\\mathcal{R}$ runs in time poly $(N,1/\\epsilon)$ and guarantees $\\overline{{\\mathrm{Reg}}}_{\\Phi^{\\beta}}^{T}\\leq\\epsilon$ for any sequence of utilities, then there is $a$ poly $(N,1/\\epsilon)$ algorithm for computing an $(\\epsilon\\sqrt{N})$ -fixed point of any $\\phi^{\\beta}\\in\\Phi^{\\beta}$ with respect to $\\|\\cdot\\|_{2}$ , assuming that $\\phi^{\\beta}$ can be evaluated in polynomial time. ", "page_idx": 15}, {"type": "text", "text": "Proposition B.1 significantly circumscribes the class of problems for which efficient $\\Phi^{\\beta}$ -regret minimization is possible, at least when operating in behavioral strategies. Indeed, computing fixed points is in general a well-known (presumably) intractable problem. In our context, the set $\\Phi^{\\beta}$ does not contain arbitrary (Lipschitz continuous) functions $[0,\\dot{1}]^{N}\\rightarrow[0,1]^{N}$ , but instead contains multilinear functions from $[0,1]^{N}$ to $[0,1]^{N}$ . Nonetheless, we show that PPAD-hardness persists in our setting. The basic idea is as follows. We start with a generalized circuit (Definition I.3), and we show that all gates can be approximately simulated using exclusively gates involving multilinear operations (Proposition I.7). The proof of that claim appears in Appendix I.1. As a result, we arrive at the main hardness result of this section, restated below. ", "page_idx": 15}, {"type": "text", "text": "Theorem 3.3\u221a. If a regret minimizer $\\mathcal{R}$ outputs strategies in $[0,1]^{N}$ , it is PPAD-hard to guarantee $\\overline{{\\mathrm{Reg}}}_{\\Phi^{\\beta}}\\leq\\epsilon/\\sqrt{N}$ , even with respect to low-degree deviations and an absolute constant $\\epsilon>0$ . ", "page_idx": 15}, {"type": "text", "text": "We also obtain a stronger hardness result under a stronger complexity assumption put forward by Babichenko et al. [2016] (Theorem I.9). At first glance, it may seem that the above results are at odds with the recent positive results of Dagan et al. [2024] and Peng and Rubinstein [2024], which seemingly obviate the need to compute approximate fixed points. As we have alluded to, the key restriction that drives Theorem 3.3 lies in constraining the learner to output behavioral strategies. In the coming section, we show that there is an interesting twist which justifies the discrepancy highlighted above. ", "page_idx": 15}, {"type": "text", "text": "C Circumventing fixed points ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The previous section, and in particular Theorem 3.3, seems to preclude the ability to minimize $\\Phi$ - regret efficiently when the set of (extended) deviations contains nonlinear functions.5In this section, we will show how to circumvent this issue via a relaxed notion of what constitutes a fixed point (Definition C.1). In the sequel, we will work with deviations $\\phi$ with domain $\\mathcal{X}$ instead of conv $\\mathcal{X}$ . ", "page_idx": 15}, {"type": "text", "text": "C.1 Approximate expected fixed points ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The key to our construction is to allow the learner to play distributions over $\\mathcal{X}$ , not merely points in conv $\\mathcal{X}$ , and to use a relaxed notion of a fixed point, formally introduced below. ", "page_idx": 15}, {"type": "text", "text": "Definition C.1. We say that a distribution $\\pi\\in\\Delta(\\mathcal{X})$ is an $\\epsilon$ -expected fixed point of $\\phi\\in(\\mathrm{conv}\\,\\lambda)^{\\chi}$ if $\\|\\mathbb{E}_{\\pmb{x}\\sim\\pi}[\\phi(\\pmb{x})-\\pmb{x}]\\|_{\\chi}\\leq\\epsilon$ . ", "page_idx": 15}, {"type": "text", "text": "The key now is to replace the fixed point oracle in the framework of Gordon et al. [2008] (recalled in Appendix A) with an oracle that instead returns an $\\epsilon$ -fixed point in expectation per Definition C.1. The learner otherwise proceeds as in the algorithm of Gordon et al. [2008] (our overall construction is spelled out as Algorithm 1 in Appendix I.2). It is easy to show, following the proof of Gordon et al. [2008], that a fixed point in expectation is still sufficient to minimize $\\Phi$ -regret. ", "page_idx": 15}, {"type": "text", "text": "Theorem C.2 ( $\\Phi$ -regret with $\\epsilon$ -expected fixed points). Suppose that the external regret of $\\mathcal{R}_{\\Phi}$ over $\\Phi$ after $T$ repetitions is at most $\\overline{{\\mathrm{Reg}}}^{\\bar{T}}$ . Then, the $\\Phi$ -regret of Algorithm 1 can be bounded as Reg + \u03f5. ", "page_idx": 15}, {"type": "text", "text": "Analogously to Proposition B.1, it turns out that there is a certain equivalence between minimizing $\\Phi$ in $\\bar{\\Delta(X)}$ and computing expected fixed points: ", "page_idx": 15}, {"type": "text", "text": "Proposition C.3. Consider a regret minimizer $\\mathcal{R}$ operating over $\\Delta(\\mathcal{X})$ . If $\\mathcal{R}$ runs in time pol $\\mathsf{y}(N,1/\\epsilon)$ and guarantees $\\overline{{\\mathrm{Reg}}}_{\\Phi}^{T}\\le\\epsilon$ for any sequence of utilities, then there is a poly $(N,1/\\epsilon)$ algorithm for computing $(\\epsilon D_{\\mathcal{X}})$ -expected fixed points of $\\phi\\,\\in\\,\\Phi$ , assuming that we can efficiently compute $\\mathbb{E}_{\\pmb{x}^{(t)}\\sim\\pi^{(t)}}[\\phi(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}]$ at any time $t$ . Here, $D_{\\mathcal{X}}$ is the diameter of $\\mathcal{X}$ with respect to $\\|\\cdot\\|_{2}$ . ", "page_idx": 16}, {"type": "text", "text": "The proof proceeds similarly to Proposition B.1, and so we include it in Appendix I.2. Next, we present a method for computing approximate expected fixed points of functions $\\phi\\;\\in\\;\\Phi$ without having to solve a PPAD-hard problem. ", "page_idx": 16}, {"type": "text", "text": "C.2 Extending deviation maps to conv $\\mathcal{X}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "First, since we will work both over conv $\\mathcal{X}$ and distributions in $\\Delta(\\mathcal{X})$ , we need efficient methods for passing between them. To that end, we introduce the following notion. ", "page_idx": 16}, {"type": "text", "text": "Definition C.4. A map $\\delta:\\mathrm{conv}\\,\\mathcal{X}\\to\\Delta(\\mathcal{X})$ is ", "page_idx": 16}, {"type": "text", "text": "\u2022 consistent if $\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\delta({\\pmb x})}\\,{\\pmb x}^{\\prime}={\\pmb x}$ , and   \n\u2022 efficient if, given some $\\phi\\ \\in\\ \\Phi$ and $\\textbf{\\textit{x}}\\in\\mathrm{\\conv}\\,\\mathcal{X}$ , it is easy to compute $\\phi^{\\delta}({\\pmb x})~:=$ $\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\delta({\\pmb x})}\\,\\phi({\\pmb x}^{\\prime})$ . ", "page_idx": 16}, {"type": "text", "text": "We will call the map $\\phi^{\\delta}:\\operatorname{conv}\\mathcal{X}\\to\\operatorname{conv}\\mathcal{X}$ the extended map of $\\phi$ . ", "page_idx": 16}, {"type": "text", "text": "One may ask why we use this indirect method of defining $\\phi^{\\delta}$ rather than simply directly using the representation of $\\phi$ (for example, as a polynomial) to extend $\\phi$ to conv $\\mathcal{X}$ . The answer is that, even assuming that $\\phi:\\mathcal{X}\\rightarrow\\mathcal{X}$ is represented as a multilinear polynomial (which is the representation assumed in the majority of this paper), naively extending that polynomial to domain conv $\\mathcal{X}$ will not necessarily result in a function $\\bar{\\phi}:\\mathrm{conv}\\,\\bar{\\chi}\\to\\mathrm{conv}\\,\\bar{\\chi}$ . For an example, consider the decision problem $\\mathcal{X}$ depicted in Figure 1, and consider the function $\\phi:\\mathcal{X}\\rightarrow\\mathcal{X}$ given by $\\phi(\\pmb{x})\\,=\\,(x_{1}\\pm$ $x_{3},x_{2}x_{4},x_{2}x_{5},x_{2},0)$ . One can easily check by hand that $\\phi$ is indeed a function $\\mathcal X\\rightarrow\\mathcal X$ , but also that, for the strategy $\\mathbf{\\dot{\\boldsymbol{x}}}=(1/2,1/2,\\bar{0},1/2,0)\\ \\in\\mathrm{conv}\\,\\boldsymbol{\\boldsymbol{\\chi}}$ , we have $\\phi(\\pmb{x})=(1/2,1/4,0,1/2,0)\\not\\in$ conv $\\mathcal{X}$ . Thus, we need a more robust way of extending functions $\\mathcal{X}\\,\\rightarrow\\,\\mathrm{conv}\\,\\mathcal{X}$ to functions conv $\\mathcal{X}\\rightarrow\\mathrm{conv}\\,\\mathcal{X}$ , ideally one that is dependent only the function $\\phi$ , not its representation. ", "page_idx": 16}, {"type": "text", "text": "We now give two methods of constructing consistent and efficient maps $\\delta:\\operatorname{conv}\\lambda\\to\\Delta(\\mathcal{X})$ for tree-form strategy sets $\\mathcal{X}$ . The first is the behavioral strategy map. ", "page_idx": 16}, {"type": "text", "text": "Definition C.5. The behavioral strategy map $\\beta:\\mathrm{conv}\\,\\mathcal{X}\\to\\Delta(\\mathcal{X})$ is defined as follows: $\\beta(x)$ is the distribution of pure strategies generated by sampling, at each decision point $j$ for which $\\boldsymbol{x}[j]>0$ , an action $a$ according to the probabilities $x[j a]/x[j]$ . Formally, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta(\\pmb{x})[\\pmb{y}]:=\\prod_{j a:\\pmb{x}[j]>0,\\pmb{y}[j a]=1}\\frac{\\pmb{x}[j a]}{\\pmb{x}[j]}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is possible for $\\phi^{\\beta}$ to be not a polynomial even when $\\phi$ is a polynomial, because $\\beta$ is itself not a polynomial. It is clear that $\\beta$ is consistent. For efficiency, we show the following claim. ", "page_idx": 16}, {"type": "text", "text": "Proposition C.6. Let $\\beta:\\mathrm{conv}\\,\\mathcal{X}\\to\\Delta(\\mathcal{X})$ be the behavioral strategy map. Let $\\phi:\\mathcal{X}\\to\\operatorname{conv}\\mathcal{X}$ be expressed as a polynomial of degree at most $k$ , in particular, as a sum of at most $O(N^{k})$ terms. Then there is an algorithm running in time $N^{O(k)}$ that, given $\\phi$ and $\\pmb{x}\\in\\mathrm{conv}\\,\\pmb{\\chi}$ , computes $\\phi^{\\beta}({\\pmb x})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. To compute $\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\beta({\\pmb x})}\\,\\phi({\\pmb x}^{\\prime})$ , since $\\phi$ is a polynomial, it suffices to compute $\\mathbb{E}_{\\pmb{x}^{\\prime}\\sim\\beta(\\pmb{x})}\\,m(\\pmb{x}^{\\prime})$ for multilinear monomials $m$ of degree at most $K$ , that is, functions of the form $m_{S}({\\pmb x})~:=$ $\\prod_{z\\in S}{\\mathbf x}[z]$ where $S\\subseteq{\\mathcal{Z}}$ has size at most $k$ . There are two cases. First, there are monomials that are clearly identically zero: in particular, if there are two nodes $g a,j a^{\\prime}\\preceq S$ for $a\\ne a^{\\prime}$ , then $m_{S}\\equiv0$ because a player cannot play two different actions at $j$ . For monomials that are not identically zero, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\underset{{\\pmb x}^{\\prime}\\sim\\beta({\\pmb x})}{\\mathbb{E}}\\prod_{j a\\in S}{\\pmb x}[j a]=\\underset{j a\\preceq S:{\\pmb x}[j]>0}{\\prod}\\frac{{\\pmb x}[j a]}{{\\pmb x}[j]},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is computable in time $O(k d)$ . Thus, the overall time complexity is $O(k d N^{k})\\leq N^{O(k)}$ . ", "page_idx": 16}, {"type": "text", "text": "The behavioral strategy map is in some sense the canonical strategy map: when one writes a treeform strategy $\\textbf{\\em x}\\in\\ \\operatorname{conv}\\chi$ without further elaboration on what distribution $\\Delta(\\mathcal{X})$ it is meant to represent, it is often implicitly or explicitly assumed to mean the behavioral strategy. ", "page_idx": 17}, {"type": "text", "text": "The behavioral strategy map has the unfortunate property that it usually outputs distributions of exponentially-large support; indeed, if $_{x\\in}$ relint conv $\\mathcal{X}$ then $\\beta(x)$ is full-support. ", "page_idx": 17}, {"type": "text", "text": "The second example we propose, which we call a Carath\u00b4eodory map, always outputs low-support distributions. In particular, for any $\\pmb{x}\\in\\mathrm{conv}\\,\\pmb{\\chi}$ , Carathe\u00b4odory\u2019s theorem on convex hulls guarantees that $\\textbf{\\em x}$ is a convex combination of $N$ pure strategies6 $x_{1},\\ldots,x_{N}\\;\\in\\;\\mathcal{X}$ . Gro\u00a8tschel et al. [1981, Theorem 3.9] moreover showed that there exists an efficient algorithm for computing the appropriate convex combination. Thus, fixing some efficient algorithm for this computational problem, we define a Carath\u00b4eodory map $\\gamma:\\mathrm{conv}\\,\\mathcal{X}\\to\\Delta(\\mathcal{X})$ to be any consistent map that returns a distribution of support at most $N$ . Given such a mapping, computing $\\phi^{\\gamma}({\\pmb x})$ is easy: one simply writes $\\textbf{\\em x}=$ $\\sum_{i}\\alpha_{i}\\mathbf{\\bar{x}}_{i}$ by computing $\\gamma(x)$ , and returns $\\begin{array}{r}{\\phi^{\\gamma}(\\pmb{x})=\\sum_{i}\\alpha_{i}\\phi(\\pmb{x}_{i})}\\end{array}$ . This only requires a $\\mathsf{p o l y}(N)$ -time computation of $\\gamma$ , and $N$ evaluations of the function $\\phi$ . As before, when $\\phi$ is a degree- $k$ polynomial, the time complexity of computing $\\phi^{\\gamma}$ is bounded by $\\overset{}{N}^{O(k)}$ . ", "page_idx": 17}, {"type": "text", "text": "C.3 Efficiently computing fixed points in expectation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Now let $\\delta:\\mathrm{conv}\\,\\mathcal{X}\\to\\Delta(\\mathcal{X})$ be consistent and efficient. Consider the following algorithm. Given $\\phi\\in\\Phi$ , select $\\pmb{x}_{1}\\in\\mathrm{conv}\\,\\pmb{\\chi}$ arbitrarily, and then for each $\\ell>1$ set $\\pmb{x}_{\\ell}:=\\phi^{\\delta}(\\pmb{x}_{\\ell-1})$ . Finally, select $\\pi:=\\mathbb{E}_{\\ell\\sim[L]}\\,\\delta(\\pmb{x}_{\\ell})\\in\\Delta(\\mathcal{X})$ as the output distribution. By a telescopic cancellation, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Big\\|_{x\\sim\\pi}\\mathbb{E}_{\\varnothing}[\\phi(x)-x]\\Big\\|_{X}=\\frac{1}{L}\\Big\\|\\sum_{\\ell=1}^{L}\\frac{\\mathbb{E}}{x\\sim\\delta(x_{\\ell})}[\\phi(x)-x]\\Big\\|_{X}\\leq\\frac{1}{L}\\Big\\|_{x\\sim\\delta(x_{L})}[\\phi(x)-x_{1}]\\Big\\|_{X}\\leq\\frac{2}{L},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as desired. As a result, applying Theorem C.2, we arrive at the following conclusion. ", "page_idx": 17}, {"type": "text", "text": "Theorem C.7. Let $\\mathcal{R}_{\\Phi}$ be an regret minimizer on $\\Phi$ whose external regret after $T$ iterations is $\\overline{{\\mathrm{Reg}}}^{T}$ and whose per-iteration runtime is $R_{1}$ , and assume that evaluating the extended map $\\phi^{\\delta}:$ conv $\\mathcal{X}\\rightarrow\\operatorname{conv}\\mathcal{X}$ takes time $R_{2}$ . Then, for every $\\epsilon>0$ , there is a learning algorithm on $\\mathcal{X}$ whose $\\Phi$ -regret after $T$ iterations is at most $\\overline{{\\mathrm{Reg}}}^{T}+\\epsilon$ and whose per-iteration runtime is $O(R_{1}+R_{2}/\\epsilon)$ . ", "page_idx": 17}, {"type": "text", "text": "The above result provides a full black-box reduction from $\\Phi$ -regret minimization to external regret minimization on $\\Phi$ , with no need for the possibly-expensive computation of a fixed point. We note that the iterates of the algorithm will depend on the choice of $\\delta$ \u2014for example, setting $\\delta\\,=\\,\\beta$ and setting $\\delta=\\gamma$ will produce different iterates. ", "page_idx": 17}, {"type": "text", "text": "D Low-degree regret on the hypercube ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we let $\\mathcal{X}$ be the hypercube $\\{0,1\\}^{N}$ . For the convenience of the reader, we first recall that the set of deviations $\\Phi_{\\mathrm{DT}}^{k}$ is defined as follows: ", "page_idx": 17}, {"type": "text", "text": "1. The deviator observes an index $j_{0}\\in[N]$ .   \n2. For $i=1,\\ldots,k$ : The deviator selects an index $j_{i}\\in[N]$ , and observes ${\\pmb x}[j_{i}]$ .   \n3. The deviator selects $a_{0}\\in\\{0,1\\}$ . ", "page_idx": 17}, {"type": "text", "text": "As we observed earlier, the above process describes a tree-form decision problem of size $N^{O(k)}$ . In particular, terminal nodes in this decision problem are identified by the original index $j_{0}\\in[N]$ , the queries $j_{1},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},j_{k}\\,\\in\\,[N]$ , their replies $a_{1},\\dotsc,a_{k}\\in\\{0,1\\}$ , and finally the action $a_{0}\\in\\{0,1\\}$ that is played. Each tree-form strategy $\\pmb q$ in this decision problem defines a function $\\phi_{\\pmb q}:\\mathcal X\\to\\mathrm{conv}\\,\\mathcal X$ , which is computed by following the strategy $\\pmb q$ through the decision problem. Namely, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi_{\\pmb q}(\\pmb x)[j_{0}]=\\sum_{j_{1},a_{1},\\dots,j_{k},a_{k}}\\pmb q[j_{0},j_{1},a_{1},\\dots,j_{k},a_{k},1]\\prod_{i=1}^{k}\\pmb x[j_{i},a_{i}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\pmb{x}[j_{i},a_{i}]=\\pmb{x}[j_{i}]$ if $a_{i}=1$ , and $1-{\\pmb x}[j_{i}]$ if $a_{i}=0$ . We see that $\\phi_{q}$ is a degree- $k$ polynomial in $\\textbf{\\em x}$ . ", "page_idx": 18}, {"type": "text", "text": "We define $\\Phi_{\\mathrm{DT}}^{k}$ as the set of such functions $\\phi_{q}$ . The \u201cDT\u201d in the name $\\Phi_{\\mathrm{DT}}^{k}$ stands for decision tree: the set of functions $\\phi:\\mathcal{X}\\to\\operatorname{conv}\\mathcal{X}$ that can be expressed in the above manner is precisely the set of functions representable as (randomized) depth- ${\\cdot k}$ decision trees on $N$ variables. ", "page_idx": 18}, {"type": "text", "text": "For intuition, we mention the following special cases: ", "page_idx": 18}, {"type": "text", "text": "\u2022 $\\Phi_{\\mathrm{DT}}^{0}$ is the set of external deviations.   \n\u2022 $\\Phi_{\\mathrm{DT}}^{1}$ is the set of all single-query deviations, which Fujii [2023] showed to be equivalent to the set of all linear deviations when $\\mathcal{X}$ is a hypercube. $\\Phi_{\\mathrm{DT}}^{N}$ is the set of all swap deviations. ", "page_idx": 18}, {"type": "text", "text": "Since ${\\pmb q}\\mapsto\\phi_{\\pmb q}({\\pmb x})[i]$ is linear, it follows that ${\\pmb q}\\mapsto\\langle{\\pmb u},\\phi_{{\\pmb q}}({\\pmb x})\\rangle$ is also linear for any given $\\pmb{u}\\in\\mathbb{R}^{n}$ . Therefore, a regret minimizer on $\\Phi_{\\mathrm{DT}}^{k}$ can be constructed starting from any regret minimizer for tree-form decision problems, such as counterfactual regret minimization [Zinkevich et al., 2007]. ", "page_idx": 18}, {"type": "text", "text": "Proposition D.1. There is a $N^{O(k)}$ -time-per-round regret minimizer on $\\Phi_{\\mathrm{DT}}^{k}$ whose external regret is at most $\\epsilon$ after $N^{O(k)}/\\epsilon^{2}$ rounds. ", "page_idx": 18}, {"type": "text", "text": "Thus, combining with Proposition C.6 and Theorem C.7, we immediately obtain a $\\Phi_{\\mathrm{DT}}^{k}$ -regret minimizer with the following complexity. ", "page_idx": 18}, {"type": "text", "text": "Corollary D.2. There is a $N^{O(k)}/\\epsilon$ -time-per-round regret minimizer on $\\mathcal{X}$ whose $\\Phi_{\\mathrm{DT}}^{k}$ -regret is at most \u03f5 after $N^{O(k)}/\\epsilon^{2}$ rounds. ", "page_idx": 18}, {"type": "text", "text": "Next, we relate depth- $k$ decision trees to low-degree polynomials. Let $\\Phi_{\\mathrm{poly}}^{k}$ be the set of degree- $k$ polynomials $\\phi:\\mathcal{X}\\rightarrow\\mathcal{X}$ . We appeal to a result from the literature on Boolean analysis, recalled below. ", "page_idx": 18}, {"type": "text", "text": "Theorem 4.2 (Midrijanis, 2004). Every degree- $k$ polynomial $f:\\{0,1\\}^{N}\\rightarrow\\{0,1\\}$ can be written as a decision tree of depth at most $2k^{3}$ . ", "page_idx": 18}, {"type": "text", "text": "In particular, $\\Phi_{\\mathrm{poly}}^{k}\\subseteq\\Phi_{\\mathrm{DT}}^{2k^{3}}$ . Corollary D.2 thus also implies a \u03a6pkoly-regret minimizer: ", "page_idx": 18}, {"type": "text", "text": "Corollary D.3. Let $\\mathcal{X}\\,=\\,\\{0,1\\}^{N}$ . There is an $N^{O(k^{3})}/\\epsilon$ -time-per-round regret minimizer on $\\mathcal{X}$ whose \u03a6pko $\\Phi_{\\mathrm{poly}}^{k}$ -regret is at most \u03f5 after $N^{O(k^{3})}/\\epsilon^{2}$ rounds. ", "page_idx": 18}, {"type": "text", "text": "It is reasonable to ask whether the above result generalizes to polynomials $\\phi\\;:\\;\\mathcal{X}\\;\\rightarrow\\;\\operatorname{conv}\\mathcal{X}$ . Indeed, when $k\\leq1$ or $k\\,=\\,N$ , every degree- $k$ polynomials $\\phi:\\mathcal{X}\\to\\mathrm{conv}\\,\\mathcal{X}$ can be written as a convex combination of degree- $k$ polynomials $\\phi:\\mathcal{X}\\rightarrow\\mathcal{X}$ , even for arbitrary tree-form decision problems.7 However, this is not generally true. A brute-force search shows that the polynomial $\\Dot{\\phi}:\\{0,1\\}^{4}\\rightarrow[0,1]^{4}$ given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi(x_{1},x_{2},x_{3},x_{4})=x_{1}-x_{1}x_{2}-\\frac{1}{2}x_{1}x_{3}+\\frac{1}{2}x_{2}x_{3}+\\frac{1}{2}x_{3}x_{4}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "is quadratic, but it is not a convex combination of quadratics whose range is $\\{0,1\\}^{4}$ . Perhaps more glaringly, if one could efficiently represent the set of quadratic functions $\\phi:\\{0,1\\}^{N}\\to\\mathbf{\\bar{\\alpha}}[0,1]^{N}$ , then one could in particular decide whether a given quadratic function $\\phi:\\{0,1\\}^{N}\\to\\mathbb{R}^{N}$ has range $[0,1]^{N}$ . But this is a coNP-complete problem. ", "page_idx": 18}, {"type": "text", "text": "E Extensive-form games ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The goal of this section is to extend the results in the previous section to the extensive-form setting, that is, to generalize them to all tree-form decision problems. ", "page_idx": 18}, {"type": "text", "text": "E.1 Interleaving decision problems ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first recall the operations of merging decision problems that will be very useful as notation in the subsequent discussion. In particular, given two decision problems $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ with node sets $S_{1}$ and $S_{2}$ respectively, we restate the following definitions previously introduced in Section 4. ", "page_idx": 19}, {"type": "text", "text": "Definition 4.3. The dual $\\bar{\\chi}$ of $\\mathcal{X}$ is the decision problem identical to $\\mathcal{X}$ , except that the decision points and observation points have been swapped. ", "page_idx": 19}, {"type": "text", "text": "Definition 4.4. The interleaving $\\mathcal{X}\\otimes\\mathcal{Y}$ is the tree-form decision problem defined as follows. There is a state $\\pmb{\\mathscr{s}}=(\\mathscr{s}_{1},\\mathscr{s}_{2})\\in\\mathcal{S}_{1}\\times\\bar{S_{2}}$ . The root state is the tuple $(\\emptyset,\\emptyset)$ . The decision problem is defined by the player being able to interact with both decision problems, in the following manner. At each state $\\pmb{s}=(s_{1},s_{2})$ : ", "page_idx": 19}, {"type": "text", "text": "\u2022 If $s_{1}$ and $s_{2}$ are both terminal then so is $\\pmb{s}$ . Otherwise:   \n\u2022 If either of the $s_{i}\\mathbf{s}$ is an observation point, then so is $\\pmb{s}$ . The children are the states $(s_{i}^{\\prime},s_{-i})$ where $s_{i}^{\\prime}$ is a child of $s_{i}$ . (If both $s_{i}\\mathbf{s}$ are observation points, both children $s_{1}^{\\prime},s_{2}^{\\prime}$ are selected simultaneously. This can only happen at the root.)   \n\u2022 Otherwise, $\\pmb{s}$ is a decision point. The player selects an index $i\\in\\{1,2\\}$ at which to act, and a child $s_{i}^{\\prime}$ to transition to. The next state is $(s_{i}^{\\prime},s_{-i})$ . ", "page_idx": 19}, {"type": "text", "text": "It follows immediately from definitions that $\\bar{\\bar{\\chi}}\\,=\\,\\chi$ , and $\\otimes$ is associative and commutative. The name and notation for the dual is inspired by the observation that $\\langle\\pmb{x},\\pmb{y}\\rangle\\,=\\,1$ for all $\\pmb{x}\\in\\mathcal{X}$ and $\\pmb{y}\\in\\bar{\\mathcal X}$ : indeed, the component-wise product ${\\pmb x}[z]{\\pmb y}[z]$ is exactly the probability that one reaches terminal node $z$ by following strategy $\\textbf{\\em x}$ at $\\mathcal{X}$ \u2019s decision points and $\\textit{\\textbf{y}}$ at $\\mathcal{X}$ \u2019s observation points. We also define the notation $\\mathcal{X}^{\\otimes\\bar{k}}:=\\mathcal{X}\\,\\bar{\\otimes}\\cdot\\cdot\\otimes\\mathcal{X}$ , where there are $k$ copies of $\\mathcal{X}$ . ", "page_idx": 19}, {"type": "text", "text": "It is important to observe that in $\\mathcal{X}\\otimes\\mathcal{Y}$ , the same state $\\left(s_{1},s_{2}\\right)$ can be reachable through possibly exponentially many paths. This is because the learner may choose to interleave actions in $\\mathcal{X}$ with actions in $\\boldsymbol{\\wp}$ in any order, which means that a state $\\left(s_{1},s_{2}\\right)$ corresponds to exponentially many histories in $\\mathcal{X}\\otimes\\mathcal{Y}$ . For that reason, we have to distinguish between histories and states. ", "page_idx": 19}, {"type": "text", "text": "In light of the above discussion, it is inefficient to represent $\\mathcal{X}\\otimes\\mathcal{Y}$ as a tree. Indeed, Zhang et al. [2023] studied $D A G$ -form decision problems, and showed that regret minimization on them is possible when the underlying DAG has some natural properties. We state here an immediate consequence of their analysis, which we will use as a black box. Intuitively, the below result states that, as long as utility vectors also only depend on the (terminal) state $\\pmb{s}$ that is reached, regret minimization on an arbitrary interleaving of decision problems $\\mathcal{X}_{1}\\otimes\\cdot\\cdot\\cdot\\otimes\\mathcal{X}_{k}$ is possible, and the complexity depends only on the number of states. ", "page_idx": 19}, {"type": "text", "text": "Theorem E.1 (Consequence of Zhang et al., 2023, Corollary A.4). Let $\\mathcal{X}:=\\mathcal{X}_{1}\\otimes\\cdot\\cdot\\otimes\\mathcal{X}_{k}$ , where $\\mathcal{X}_{i}$ has terminal node set $\\mathcal{Z}_{i}$ . Let $\\mathcal{Z}\\,:=\\,\\mathcal{Z}_{1}\\,\\times\\,\\cdot\\,\\cdot\\,\\cdot\\,\\times\\,\\mathcal{Z}_{k}$ be the set of terminal states for $\\mathcal{X}$ . For each such terminal state $z\\in{\\mathcal{Z}}$ , let $V(z)$ be the set of histories of $\\mathcal{X}$ whose state is $z$ . Define the projection $\\pi:\\mathcal{X}\\to\\mathbb{R}^{\\mathcal{Z}}$ by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi(\\pmb{x})[z]=\\sum_{\\pmb{v}\\in V(z)}\\pmb{x}[v].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then there exists an efficient regret minimizer on $\\pi(X):=\\{\\pi(\\pmb{x}):\\pmb{x}\\in\\mathcal{X}\\}\\subset\\mathbb{R}^{\\mathbb{Z}}$ : its per-round complexity is poly $(|\\mathcal{Z}|)$ , and its regret is \u03f5 after poly $(|\\mathcal{Z}|)/\\epsilon^{2}$ rounds. ", "page_idx": 19}, {"type": "text", "text": "Whenever we speak of regret minimizing on interleavings, it will always be the case that utility vectors depend only on the state, so we will always be able to apply the above result. We will call vectors in $\\pi(\\mathcal{X})$ reduced strategies. ", "page_idx": 19}, {"type": "text", "text": "Before proceeding, it is instructive to describe in more detail a result of Zhang et al. [2024], which we will also use later, in the language of this section. Let $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ be any two decision problems with terminal node sets $\\mathcal{Z}_{1}$ and ${\\mathcal{Z}}_{2}$ respectively. A reduced strategy $\\pmb q\\in\\pi(\\mathcal{X}\\otimes\\bar{\\mathcal{Y}})$ induces a linear map $\\phi_{\\pmb q}:\\mathcal{V}\\rightarrow\\mathrm{conv}\\,\\mathcal{X}$ , given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\phi_{\\pmb q}(\\pmb y)[z_{1}]=\\sum_{z_{2}\\in\\mathcal{Z}_{2}}\\pmb q[z_{1},z_{2}]\\pmb y[z_{2}].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "image", "img_path": "c4ElkpA0kh/tmp/270567d567f58485504440ece9e5f7ceaff7736b762f038d16c3fc71ebd0de6e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 2: A representation of the deviation $\\phi({\\pmb x})\\;=\\;(x_{1}\\,+\\,x_{3},x_{2}x_{4},x_{2}x_{5},x_{2},0)$ (discussed in Appendix C.2) in the decision problem $\\mathcal{X}$ in Figure 1, as a strategy in $x\\otimes{\\bar{x}}\\otimes{\\bar{x}}$ , i.e., with $k=2$ mediators. (For an example of a one-mediator deviation, see Zhang et al. [2024, Figure 1].) Again, black squares are decision nodes and white squares are observation nodes. Nodes are labeled with their state representations: the state in $\\mathcal{X}$ first (in blue), and the two mediator states after (in red). Similarly, blue edge labels indicate interactions with the decision problem (i.e., playing actions and receiving observations in $\\mathcal{X}$ ), and red edge labels indicate interactions with the mediators (i.e., querying and receiving action recommendations from the mediators). Redundant edges (such as those in which the decision problem in $\\mathcal{X}$ has terminated) are omitted. The deviation is shown in thick black lines. For example, $\\phi_{2}({\\pmb x})\\,=\\,x_{2}x_{4}$ because the only state in which the deviator plays action 2 is when the mediator state is (2,4). $\\phi_{1}({\\pmb x})\\,=\\,x_{1}\\,+\\,x_{3}$ because the deviator plays action 1 at mediator states (1,1) and (3,0), which would give the formula $\\phi_{1}({\\pmb x})\\,=\\,x_{1}^{2}+\\bar{x_{3}}\\bar{x_{0}}$ (where $x_{0}:=1-x_{1})$ , but one can easily check that $x_{1}^{2}+x_{3}\\dot{x}_{0}=x_{1}+x_{3}$ for all $x\\in\\mathscr{X}$ . ", "page_idx": 20}, {"type": "text", "text": "It is instructive to think, as Zhang et al. [2024] detailed extensively in their paper, about what strategies $\\pmb q\\,\\in\\,\\pi(\\mathcal{X}\\otimes\\bar{\\mathcal{Y}})$ represent, and why they induce the linear maps $\\phi_{q}$ . Decision points $j$ in $\\boldsymbol{\\wp}$ become observation points in $\\chi_{\\,\\otimes}\\,\\bar{y}$ \u2014at these observation points, the player should observe the action taken by strategy $\\textit{\\textbf{y}}$ at $j$ . The player in $\\mathcal{X}\\otimes\\bar{\\mathcal{Y}}$ is given the ability to query the strategy $\\textit{\\textbf{y}}$ by taking the role of the environment in $\\boldsymbol{\\wp}$ , while the environment, holding a strategy $\\pmb{y}\\in\\mathcal{V}$ , takes the role of the player and answers decision point queries with the actions that it plays. The player then uses these queries to inform how it plays in the true decision problem $\\mathcal{X}$ . This is the sense in which $\\pmb q$ induces a map $\\phi_{q}$ : the output $\\phi_{q}(\\pmb{y})$ is precisely the strategy that would be played if the environment in $\\mathcal{X}\\otimes\\bar{\\mathcal{Y}}$ answers the queries by consulting the strategy $\\textit{\\textbf{y}}$ . We will call a device that answers queries using strategy $\\textit{\\textbf{y}}$ a mediator holding strategy $\\textit{\\textbf{y}}$ . Zhang et al. [2024] then showed the following fact, which we will use critically and repeatedly in the rest of this paper. ", "page_idx": 20}, {"type": "text", "text": "Theorem E.2 (Zhang et al., 2024, Theorem A.2). Every linear map $\\phi:\\mathcal{Y}\\rightarrow\\mathrm{conv}\\,\\mathcal{X}$ is induced by some reduced strategy $\\pmb q\\in\\pi(\\mathcal{X}\\otimes\\bar{\\mathcal{Y}})$ . ", "page_idx": 20}, {"type": "text", "text": "E.2 Efficient low-degree swap-regret minimization in extensive-form games ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now proceed with generalizing the results of Appendix D to extensive-form games. ", "page_idx": 20}, {"type": "text", "text": "Let $\\mathcal{X}$ be any decision problem of dimension $N$ and depth $d$ . We will assume WLOG that every decision point in $\\mathcal{X}$ has branching factor exactly 2. This is without loss of generality, but it incurs a loss of $O(\\log b)$ , where $b$ is the original branching factor, in the depth. Thus, in the below bounds, when $d$ appears, it should be read as $O(d\\log b)$ . ", "page_idx": 20}, {"type": "text", "text": "Using the previous notation, the set of $k$ -mediator deviations $\\Phi_{\\mathrm{med}}^{k}$ is the set of reduced strategies in the decision problem $\\mathcal{X}\\otimes\\bar{\\mathcal{X}}^{\\otimes k}$ . In particular, we recall that reduced strategies $q\\in\\pi(\\mathcal{X}\\otimes\\bar{\\mathcal{X}}^{\\otimes k})$ induce functions $\\phi_{\\pmb{q}}:\\mathcal{X}\\rightarrow\\mathrm{conv}\\,\\mathcal{X}$ given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\phi_{\\pmb q}(\\pmb x)[z]=\\sum_{z_{1},\\ldots,z_{k}}\\pmb q[z,z_{1},\\ldots,z_{k}]\\prod_{i=1}^{k}\\pmb x[z_{i}].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we have that $\\phi_{q}$ is a degree- $k$ polynomial. $\\Phi_{\\mathrm{med}}^{k}$ is the set of such deviations. For intuition, we once again pose a few special cases: ", "page_idx": 21}, {"type": "text", "text": "\u2022 When the original decision problem\u2019s decision space is $\\Delta_{2}^{N}$ (i.e., the decision problem consists of a single root observation point with $N$ children, each of which is a decision   \npoint with two actions), we have $\\Phi_{\\mathrm{DT}}^{k}\\,=\\,\\Phi_{\\mathrm{med}}^{k}$ . Thus, the results in this section strictly generalize those in the previous section. \u03a60med and \u03a6mNe are, as before, the sets of external and swap deviations respectively.   \n\u2022 $\\Phi_{\\mathrm{med}}^{1}$ is, by Theorem E.2, the set of all linear deviations. ", "page_idx": 21}, {"type": "text", "text": "In this context, applying Theorem E.1 gives an efficient \u03a6kmed-regret minimizer: ", "page_idx": 21}, {"type": "text", "text": "Theorem E.3. There is an $N^{O(k)}$ -time-per-round regret minimizer on \u03a6kmed whose external regret is at most $\\epsilon$ after $N^{O(k)}/\\epsilon^{2}$ rounds. ", "page_idx": 21}, {"type": "text", "text": "Thus, once again Proposition C.6 and Theorem C.7 have the following consequence. ", "page_idx": 21}, {"type": "text", "text": "Corollary E.4. There is a $N^{O(k)}/\\epsilon$ -time-per-round regret minimizer on $\\mathcal{X}$ whose \u03a6kmed-regret is at most \u03f5 after $N^{O(k)}/\\epsilon^{2}$ rounds. ", "page_idx": 21}, {"type": "text", "text": "Next, we discuss extensions of our result to low-degree polynomials. Unfortunately, we cannot directly apply Theorem 4.2 to conclude the existence of a regret minimizer on $\\mathcal{X}$ with \u03a6poly-regret growing as $N^{O(k^{3})}/\\epsilon^{2}$ . There are two issues in attempting to do so. ", "page_idx": 21}, {"type": "text", "text": "First, when $\\mathcal{X}$ is not the hypercube, polynomials $f:\\mathcal{X}\\to\\{0,1\\}$ are not total functions. That is, it is not necessarily the case that degree- $k$ polynomials $f:\\mathcal{X}\\xrightarrow{}\\ '0,1\\}$ can be extended to degree$k$ polynomials $\\bar{f}:\\{0,1\\}^{N}\\,\\rightarrow\\,\\{0,\\bar{1}\\}$ , which is required in order to apply Theorem 4.2.8 For an example of this, consider $\\mathcal{X}\\,=\\,\\mathcal{D}_{4}$ where $\\mathcal{D}_{N}$ is the standard basis in $\\mathbf{\\dot{R}}^{N}$ , that is, $\\begin{array}{r}{\\mathcal{D}_{N}\\,=\\,\\{e_{i}\\,:}\\end{array}$ $i\\,\\in\\,[N]\\}$ where $e_{i}\\,\\in\\,\\mathbb{R}^{N}$ is the ith basis vector (in other words, $\\mathcal{D}_{N}$ is the set of vertices of the probability simplex $\\Delta(N))$ . Let $f:{\\mathcal{D}}_{4}\\to\\{0,1\\}$ given by $f(\\pmb{x})=\\pmb{x}_{1}+\\pmb{x}_{2}$ . Then $f$ is linear, but there is no linear ${\\bar{f}}:\\{0,1\\}^{4}\\rightarrow\\{0,1\\}$ extending $f$ . Indeed, there is a more general manifestation of this phenomenon: ", "page_idx": 21}, {"type": "text", "text": "Proposition E.5. For every $N$ , there exists a linear map $f:{\\mathcal{D}}_{N}\\to\\{0,1\\}$ such that any extension $\\bar{f}:\\bar{\\{0,1\\}}^{N}\\rightarrow\\{0,1\\}$ of $f$ must have degree at least $\\Omega(\\log N)$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Let $\\bar{f}:\\{0,1\\}^{N}\\to\\{0,1\\}$ be any degree- ${\\cdot k}$ function. By Theorem 3.4 of O\u2019Donnell [2014], $\\bar{f}$ is a $k2^{k}$ -junta, that is, $\\bar{f}({\\pmb x})$ depends on at most $k2^{k}$ entries of $\\textbf{\\em x}$ . Now consider the map $f:$ ${\\mathcal D}_{N}\\to\\{0,1\\}$ given by $\\textstyle f({\\pmb x})=\\sum_{i\\leq N/2}{\\pmb x}_{i}$ . Let $\\bar{f}:\\{0,1\\}^{N}\\rightarrow\\{0,1\\}$ be an extension of $f$ . Then $\\bar{f}$ depends on at least $N/2-1$ inputs: if $\\bar{f}({\\bf0})=0$ then $\\bar{f}$ depends on at least $\\pmb{x}_{1},\\dots,\\pmb{x}_{\\lfloor N/2\\rfloor}$ , and if $\\bar{f}(\\mathbf{0})=1$ then $\\bar{f}$ depends on at least ${\\bf}x_{\\lfloor N/2\\rfloor+1},\\ldots,{\\bf}x_{N}$ . Thus, we have $N/2-1\\leq k2^{k}$ , which upon rearraging gives $k\\geq\\Omega(\\log N)$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "The second issue is the following. Suppose that $K$ mediators were enough to represent a function $f:\\mathcal{X}\\to\\{0,1\\}$ . How does one then represent a function $\\phi:\\mathcal{X}\\rightarrow\\mathcal{X}?$ Each coordinate of $\\phi$ could be represented using $K$ mediators, but that need not mean the whole function can. In game-theoretic terms, representing a coordinate of $\\phi({\\pmb x})$ allows the player to play a single action, not necessarily the whole game. Naively, playing the whole game would seem to require $K d$ mediators: $K$ mediators for every level of the decision tree, to compute which action to take at each level. ", "page_idx": 21}, {"type": "text", "text": "We will show that it is possible to circumvent both of these issues: the first with a loss of $O(d)$ in the degree of the polynomial that is representable, and the second with no additional loss. In particular, we state our main result below. ", "page_idx": 22}, {"type": "text", "text": "Theorem E.6. $\\Phi_{\\mathrm{poly}}^{k}~\\subseteq~\\Phi_{\\mathrm{med}}^{O(k d)^{3}}$ . Therefore, for every $k$ , there is a $N^{O(k d)^{3}}/\\epsilon$ -time-per-round algorithm whose \u03a6pkoly-regret at most \u03f5 after N O(kd)3/\u03f5 rounds. ", "page_idx": 22}, {"type": "text", "text": "E.3 Proof of Theorem E.6 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We dedicate the rest of this section to the proof of Theorem E.6. We deal with the two aforementioned problems one by one. First, we show that every $f$ admits an extension at a loss of a factor of $d$ in the degree: ", "page_idx": 22}, {"type": "text", "text": "Lemma E.7. Every $f:\\mathcal{X}\\to\\{0,1\\}$ of degree $k$ admits a degree-kd extension $\\bar{f}:\\{0,1\\}^{N}\\rightarrow\\{0,1\\}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We claim first that the identity function id : $\\mathcal X\\rightarrow\\mathcal X$ admits a degree- $d$ extension, that is, there is a degree- $d$ function $\\bar{\\mathrm{id}}\\,:\\,\\{0,1\\}^{N}\\,\\rightarrow\\,\\chi$ that is the identity on $\\mathcal{X}$ . Indeed, consider the following function i\u00afd. Then we define $\\operatorname{id}({\\pmb x})[j a]$ recursively as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{i}\\bar{\\mathrm{d}}({\\pmb x})[j a]=\\left\\{\\!\\!\\!\\begin{array}{l l}{{\\pmb x}[j0]\\cdot\\mathrm{i}\\bar{\\mathrm{d}}({\\pmb x})[p_{j}]\\quad}&{\\mathrm{if}\\quad a=0,}\\\\ {(1-{\\pmb x}[j0])\\cdot\\mathrm{i}\\bar{\\mathrm{d}}({\\pmb x})[p_{j}]\\quad}&{\\mathrm{if}\\quad a=1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It is easy to check that $\\mathrm{i\\bar{d}}$ is indeed the identity on $\\mathcal{X}$ by definition of tree-form decision spaces, and that the degree of $\\mathrm{i\\bar{d}}$ is at most the depth of the tree, $d$ . But now, for any $f:\\mathcal{X}\\to\\{0,1\\}$ of degree $k$ , the map ${\\bar{f}}:=f\\circ{\\mathrm{i}}{\\bar{\\mathrm{d}}}:\\{0,1\\}^{N}\\rightarrow\\{0,1\\}$ has degree at most $k d$ and is an extension of $f$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Now we apply Theorem 4.2. That result tells us that a degree- $k d$ function $\\bar{f}:\\{0,1\\}^{N}\\,\\rightarrow\\,\\{0,1\\}$ can be evaluated using $K\\;=\\;O(k d)^{3}$ queries. One mediator is certainly more than enough to perform a single query, and therefore such a function can also be evaluated using $K$ mediators. It therefore remains only to address the second problem: namely, the ability to evaluate a function $\\bar{\\phi}:\\{0,1\\}^{N}\\rightarrow\\{0,1\\}$ , since the output is only binary, in principle only allows us to play a single action. But one needs to play $d$ actions to reach the end of the game. Naively, this would require losing another factor of $d$ , for a total of $O(K d)$ mediators. However, we now show that it is possible to completely circumvent this problem. The notation that we have built up will make this argument perhaps surprisingly short. ", "page_idx": 22}, {"type": "text", "text": "Let $\\phi:\\mathcal{X}\\rightarrow\\mathcal{X}$ be any function such that each component $\\phi_{z}:\\mathcal{X}\\to\\{0,1\\}$ , given by $\\pmb{x}\\mapsto\\phi(\\pmb{x})[z]$ , is expressible using $K$ mediators. By definition, $\\phi_{z}$ is expressible as a strategy9 $\\pmb q_{z}\\,\\in\\,\\pi(\\{0,1\\}\\otimes$ ${\\bar{\\mathcal{X}}}^{\\otimes K}$ ). By the argument in the previous section, $\\scriptstyle\\mathbf{q}_{z}$ induces a linear map $\\hat{\\phi}_{z}:\\overline{{{\\bar{X}}^{\\otimes k}}}\\rightarrow\\{0,1\\}$ . ", "page_idx": 22}, {"type": "text", "text": "Now let $\\hat{\\phi}:\\overline{{{\\bar{\\chi}}^{\\otimes K}}}\\rightarrow\\mathcal{X}$ be the function whose $z$ th coordinate is $\\hat{\\phi}_{z}$ . Every component of $\\phi$ is linear, so $\\hat{\\phi}$ is itself also linear. But then by Theorem E.2, there exists a strategy $\\pmb{q}\\in\\pi(\\mathcal{X}\\otimes\\bar{\\mathcal{X}}^{\\otimes K})$ , that is, a $K$ -mediator deviation, that represents $\\phi$ . This completes the proof. ", "page_idx": 22}, {"type": "text", "text": "F Discussion and applications ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we discuss various implications and make several remarks about the framework and results that we have introduced. ", "page_idx": 22}, {"type": "text", "text": "F.1 Convergence to correlated equilibria ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Notions of $\\Phi$ -regret correspond naturally to notions of correlated equilibria. Therefore, our results also have implications for no-regret learning algorithms that converge to correlated equilibria. Here, we formalize this connection. Consider an $n$ -player game in which player $i$ \u2019s strategy set is a treeform strategy set $\\mathcal{X}_{i}$ , and player $i$ \u2019s utility is given by a multilinear map $u_{i}\\;:\\;\\mathcal{X}_{1}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,\\mathcal{X}_{n}\\,\\rightarrow$ $[-1,1]$ . For each player $i$ , let $\\Phi_{i}\\ \\subseteq\\ (\\mathrm{conv}\\,\\lambda_{i})^{\\chi_{i}}$ be a set of deviations for player $i$ . Finally let $\\mathbf{{\\bar{\\Phi}}}=({\\mathbf{\\bar{\\Phi}}}_{1},\\dots,\\Phi_{n})$ . ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Definition F.1. A distribution $\\pi\\in\\Delta(\\mathcal{X}_{1}\\times\\cdots\\times\\mathcal{X}_{n})$ is called a correlated profile. A correlated profile $\\pi$ is an $\\epsilon$ - $\\Phi$ -equilibrium if no player $i$ can profit more than $\\epsilon$ via any of the deviations $\\phi_{i}\\in\\Phi_{i}$ to its strategy. That is, $\\mathbb{E}_{{\\pmb x}\\sim{\\pmb\\pi}}\\,u_{i}(\\phi_{i}({\\pmb\\alpha}_{i}),{\\pmb x}_{-i})\\leq\\bar{\\mathbb{E}}_{{\\pmb x}\\sim{\\pmb\\pi}}\\,u_{i}({\\pmb x}_{i},{\\pmb x}_{-i})+\\epsilon$ for all players $i$ and $\\phi_{i}\\in\\Phi_{i}$ . ", "page_idx": 23}, {"type": "text", "text": "For example, we can define $k$ -mediator equilibria and degree- $k$ swap equilibria by setting $\\Phi_{i}$ to \u03a6kmed and \u03a6pkoly, respectively. The following celebrated result follows immediately from the definitions of equilibrium and regret. ", "page_idx": 23}, {"type": "text", "text": "Proposition F.2. Suppose that every player i plays according to a regret minimizer whose $\\Phi_{i}$ -regret is at most \u03f5 after $T$ rounds. Let $\\pi_{i}^{(t)}\\in\\Delta(\\mathcal{X}_{i})$ be the distribution played by player $i$ at round $t$ . Let $\\pi^{(t)}\\in\\Delta(\\mathcal{X}_{1})\\times\\cdots\\times\\Delta(\\mathcal{X}_{n})$ be the product distribution whose marginal on $\\mathcal{X}_{i}$ is $\\pi_{i}^{(t)}$ . Then the average strategy profile, that is, the distribution $\\textstyle{\\frac{1}{T}}\\sum_{t\\in[T]}\\pi^{(t)}$ , is an $\\epsilon{-}\\Phi$ -equilibrium. ", "page_idx": 23}, {"type": "text", "text": "Thus, from Theorems E.3 and E.6 it follows, respectively, that, given a game $\\Gamma$ where the dimension of each player\u2019s decision problem is at most $N$ , we have the following results. ", "page_idx": 23}, {"type": "text", "text": "Corollary F.3. An \u03f5- $k$ -mediator equilibrium can be computed in time $N^{O(k)}/\\epsilon^{3}$ . ", "page_idx": 23}, {"type": "text", "text": "Corollary F.4. An $\\epsilon$ -degree- $k$ -swap equilibrium can be computed in time $N^{O(k d)^{3}}/\\epsilon^{3}$ . ", "page_idx": 23}, {"type": "text", "text": "The issue of representing the induced correlated distribution is discussed in Appendix G. ", "page_idx": 23}, {"type": "text", "text": "F.2 Strict hierarchy of equilibrium concepts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let $c\\in\\{\\mathrm{med},\\mathrm{poly}\\}$ . For every $k\\geq0$ , let $\\mathcal{E}_{c}^{k}(\\Gamma)$ be the set of $\\Phi_{c}^{k}$ -equilibria in $\\Gamma$ . It is clear from definitions that $\\mathcal{E}_{c}^{k}(\\Gamma)\\subseteq\\mathcal{E}_{c}^{k-1}(\\Gamma)$ . Further, even for normal-form games, it is known that coarsecorrelated equilibria are not generally equivalent to correlated equilibria, so at least one of these inclusions is strict in some games. We now show that all of these inclusions are strict, so that the deviations $\\Phi_{c}^{k}$ form a strict hierarchy of equilibria.10 ", "page_idx": 23}, {"type": "text", "text": "Proposition F.5. For every $k\\geq1$ , there exists a game \u0393 such that $\\mathcal{E}_{c}^{k}(\\Gamma)\\subsetneq\\mathcal{E}_{c}^{k-1}(\\Gamma)$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Consider the two-player game $\\Gamma$ defined as follows. ", "page_idx": 23}, {"type": "text", "text": "\u2022 P1\u2019s strategy space is $\\mathcal{X}=\\{-1,1\\}^{k}$ . Player 2\u2019s strategy space is simply ${\\boldsymbol{\\mathcal{V}}}=\\{-1,1\\}$ .11 \u2022 P1\u2019s utility function is $u_{1}(x,y)=x_{1}y$ . That is, P1 would like to set $x_{1}=y$ . P2 gets no utility. ", "page_idx": 23}, {"type": "text", "text": "Consider the correlated profile $\\pi$ defined as follows: $\\pi$ is uniform over the $2^{k}$ pure profiles $({\\pmb x},{\\boldsymbol y})\\in$ $\\mathcal{X}\\!\\times\\!\\mathcal{Y}$ such that $y=x_{1}x_{2}\\ldots x_{k}$ . P1\u2019s expected utility is clearly 0, and there is a swap $(i.e.,\\Phi_{c}^{k})$ ) deviation that yields a profit of 1, namely $\\pmb{x}\\mapsto(x_{1}x_{2}\\dots x_{k},\\dots)$ . (it does not matter what the swap deviation plays at coordinates other than the first one.) But, since all the $x_{i}\\mathbf{s}$ are independent, no function of degree less than $k$ can have positive correlation with $x_{1}x_{2}\\ldots x_{k}$ , and thus, there are no profitable deviations of degree less than $k$ . Thus, $\\pi$ is a $\\Phi_{c}^{k-1}$ -equilibrium, but not a $\\Phi_{c}^{k}$ -equilibrium. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "F.3 Characterization of recent low-swap-regret algorithms in our framework ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We have, throughout this paper, introduced and used a framework of $\\Phi$ -regret that involves fixed points in expectation. Proposition C.3 shows that the ability to compute fixed points in expectation is in some sense necessary for the ability to minimize $\\Phi$ -regret. It is instructive to briefly discuss how the recent swap-regret-minimizing algorithm of Dagan et al. [2024] and Peng and Rubinstein [2024] fits into this framework. Their algorithm makes no explicit reference to fixed-point computation, nor to the minimization of external regret over swap deviations $\\phi$ \u2014they do not explicitly invoke the framework we use in this paper, nor that of Gordon et al. [2008]. Where is the expected fixed point hidden, then? While we will not present their entire construction here, it suffices to state the following property of it. At every round $t$ , the learner outputs a distribution $\\pi^{(t)}\\,\\in\\,\\Delta(\\mathcal{X})$ that is uniform on $L$ strategies $\\pmb{x}^{(t,1)},\\ldots,\\pmb{x}^{(t,L)}$ . The way to map this into our framework is to consider $\\pi^{(t)}$ an approximate fixed point in expectation of the \u201cfunction\u201d12 $\\phi^{(t)}$ that maps $\\pmb{x}^{(t,\\ell)}\\mapsto\\pmb{x}^{(t,\\ell+1)}$ for each $\\ell=1,\\ldots,L-1$ . With this choice of $\\phi^{(t)}$ , their algorithm indeed fits into our framework. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "F.4 Revelation principles (or lack thereof) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Most notions of correlated equilibrium obey some form of revelation principle. Informally, one can treat a player attempting to deviate profitably from a correlated equilibrium as an interaction between a mediator (who sends useful information to the player) and the player (who tries to play optimally by using the mediator). When studying the regret of online algorithms, one assumes that the interaction with the mediator is canonical: the mediator holds with it some sampled strategy profile $(x_{1},\\ldots,x_{n})\\,\\sim\\,\\pi$ , and in equilibrium every player indeed plays $\\pmb{x}_{i}$ . We say that the revelation principle holds for a particular notion of equilibrium if allowing non-canonical equilibria would not expand the set of equilibria. In Appendix H, we give a rather general formalization of this notion, which is enough to encompass all the notions of correlated equilibrium discussed in the paper. We show that, in this formalism, the revelation principle does not hold for $k$ -mediator equilibria or degree- $k$ swap equilibria when $k>1$ , and indeed in both cases the set of outcomes that can be induced by non-canonical equilibria is the set of linear-swap outcomes (Theorems H.4 and H.5). ", "page_idx": 24}, {"type": "text", "text": "G Representation of strategies ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we discuss how strategies $\\pi\\;\\in\\;\\Delta(\\mathcal{X}_{1}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,\\mathcal{X}_{n})$ are represented for the purposes of all of the results in this paper, and in particular for Corollaries F.3 and F.4. In both cases, at each timestep, each player\u2019s strategy $\\pi_{i}^{(t)}$ is a uniform mixture of $L=O(1/\\epsilon)$ strategies $\\delta(\\pmb{x}_{i}^{(t,1)}),\\dots,\\delta(\\pmb{x}_{i}^{(t,L)})$ , and we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pi=\\frac{1}{T}\\sum_{t=1}^{T}\\bigotimes_{i=1}^{n}\\left(\\frac{1}{L}\\sum_{\\ell=1}^{L}\\delta(\\pmb{x}_{i}^{(t,\\ell)})\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "that is, $\\pi$ is a uniform mixture of products of mixtures of strategies that are themselves outputs of $\\delta$ . Thus, if the strategy map $\\delta$ is established by convention (for example, as mentioned before, it is often conventional to take $\\delta$ to be the behavioral strategy map), it suffices to output $\\pmb{x}_{i}^{(t,\\ell)}\\in\\mathrm{conv}\\,\\mathcal{X}_{i}$ for each $i,t,\\ell$ . ", "page_idx": 24}, {"type": "text", "text": "Suppose that we impose a slightly more stringent restriction on the output format, namely, we want $\\pi$ to be a uniform mixture of products of mixtures of pure strategies. In that case, we can take $\\delta$ to be the Carathe\u00b4odory map.13 Now, writing $\\begin{array}{r}{\\pmb{x}_{i}^{(t,\\ell)}=\\sum_{j\\in[N]}^{}\\alpha_{i}^{(t,\\ell,\\overline{{j}})}\\pmb{x}_{i}^{(t,\\ell,j)}}\\end{array}$ (t,\u2113,j)x(t,\u2113,j)for x(t i,\u2113,j)\u2208Xi, we set ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pi=\\frac{1}{T}\\sum_{t=1}^{T}\\bigotimes_{i=1}^{n}\\left(\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{j=1}^{N}\\alpha_{i}^{(t,\\ell,j)}x_{i}^{(t,\\ell,j)}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So, our output consists of the strategies $\\pmb{x}_{i}^{(t,\\ell,j)}\\in\\mathcal{X}_{i}$ and their coefficients $\\alpha_{i}^{(t,\\ell,j)}$ for each $i,t,\\ell,j$ ", "page_idx": 24}, {"type": "text", "text": "H On the revelation principle ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we give a formalization of the revelation principle which encompass all the notions of correlated equilibrium discussed in the paper. In this formalism, the revelation principle does not hold for $k$ -mediator equilibria or degree- $k$ swap equilibria when $k\\,>\\,1$ , and indeed in both cases we show that the set of outcomes that can be induced by non-canonical equilibria is the set of linear-swap outcomes (Theorems H.4 and H.5). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Let $\\mathcal{D}$ be the class of all finite tree-form decision problems. For each pair $\\mathcal{X},\\mathcal{Y}\\in\\mathcal{D}$ let $\\Phi_{\\mathcal{X},\\mathcal{Y}}\\subseteq$ $(\\operatorname{conv}x)^{y}$ be a subset of deviations. Finally let $\\begin{array}{r}{\\Phi=\\sqcup_{\\mathcal{X},\\mathcal{Y}\\in\\mathcal{D}}\\Phi_{\\mathcal{X},\\mathcal{Y}}}\\end{array}$ . As in the previous section, consider a game where player $i$ has strategy set $\\mathbf{\\mathcal{X}}_{i}$ and utility $u_{i}:\\mathcal{X}_{1}\\times\\dots\\times\\mathcal{X}_{n}\\rightarrow[-1,1]$ . ", "page_idx": 25}, {"type": "text", "text": "Definition H.1. Let $y_{1},\\ldots,y_{n}\\in{\\mathcal{D}}$ be arbitrary tree-form strategy spaces, let $\\pi\\in\\Delta(\\mathcal{D}_{1}\\times\\cdot\\cdot\\cdot\\times$ $\\textstyle\\mathcal{V}_{n}$ ), and let $\\phi_{i}\\,\\in\\,\\Phi_{\\mathcal{X},\\mathcal{Y}}$ for each player $i$ . We call the tuple $((\\mathcal{V}_{i},\\phi_{i})_{i=1}^{n},\\pi)$ a generalized profile. A generalized profile is a generalized $\\epsilon{-}\\Phi$ -equilibrium if no player $i$ can profit by switching to a different strategy mapping $\\phi^{\\prime}:\\mathcal{V}_{i}\\to\\mathrm{conv}\\,\\mathcal{X}_{i}$ . That is, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underset{y\\sim\\pi}{\\mathbb{E}}u_{i}(\\phi_{i}^{\\prime}(y_{i}),\\phi_{-i}(\\pmb{y}_{-i}))\\le\\underset{y\\sim\\pi}{\\mathbb{E}}u_{i}(\\phi_{i}(\\pmb{y}_{i}),\\phi_{-i}(\\pmb{y}_{-i}))+\\epsilon\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for all players $i$ and $\\phi_{i}^{\\prime}\\in\\Phi_{\\mathcal{X},\\mathcal{Y}}$ . We call a generalized profile canonical if $\\mathcal{V}_{i}=\\mathcal{X}_{i}$ and $\\phi_{i}:\\mathcal{X}_{i}\\to$ conv $\\mathcal{X}_{i}$ is the identity map for every $i$ . ", "page_idx": 25}, {"type": "text", "text": "In this language, the definitions of equilibrium in Appendix F.1 were definitions of canonical equilibria. Every generalized profile induces a canonical profile, namely, the distribution over strategy given by sampling $y\\sim\\pi$ and returning $(\\phi_{1}(\\pmb{x}_{1}),\\bar{.}\\,.\\,.\\,,\\phi_{n}(\\pmb{x}_{n}))$ . Call two generalized profiles equivalent if they induce the same canonical profile. We can now define the revelation principle as follows. ", "page_idx": 25}, {"type": "text", "text": "Definition H.2 (Revelation principle). The class of deviations $\\Phi$ satisfies the revelation principle if the induced canonical profile of every generalized $\\epsilon{-}\\Phi$ -equilibrium is also an $\\epsilon{-}\\Phi$ -equilibrium. ", "page_idx": 25}, {"type": "text", "text": "For an example, let $\\Phi$ be the set of all functions, so that the notion of equilibrium is the normal-form correlated equilibrium. Then one can think of the sample $y\\sim\\pi$ as a profile of signals (one signal per player) from a correlation device, and $\\phi_{i}$ as player $i$ \u2019s mapping from signals to strategies. Then the revelation principle states that, without loss of generality (up to utility equivalence), one can assume that signals are recommendations of strategies $(\\mathcal{Y}_{i}\\,=\\,\\mathcal{X}_{i})$ ) and players in equilibrium play their recommended strategies $(\\phi_{i}:\\mathcal{X}_{i}\\to\\mathrm{conv}\\,\\mathcal{X}_{i}$ is the identity map). ", "page_idx": 25}, {"type": "text", "text": "All notions of equilibrium that we have mentioned can be expressed in this language, and the revelation principle applies to all of them. ", "page_idx": 25}, {"type": "text", "text": "Proposition H.3 (Sufficient conditon for revelation principle). Let $\\delta$ be a consistent strategy map in the sense of Appendix C.2. Suppose that, for every $\\phi\\in\\Phi_{\\mathcal{X},\\mathcal{Y}}$ and $\\psi\\in\\Phi_{\\mathcal{X},\\mathcal{X}}$ , we have $\\psi^{\\delta}\\circ\\phi\\in$ $\\Phi_{\\mathcal{X},\\mathcal{Y}}$ . Then $\\Phi$ satisfies the revelation principle. ", "page_idx": 25}, {"type": "text", "text": "Proof. Given a generalized $\\epsilon{-}\\Phi.$ -equilibrium, $((\\mathcal{V}_{i},\\phi_{i})_{i=1}^{n},\\pi)$ , let $\\pi^{\\prime}$ be its induced canonical profile. We need to show that $\\pi^{\\prime}$ is also an $\\epsilon{-}\\Phi$ -equilibrium. Consider any hypothetical deviation $\\psi_{i}\\in\\Phi_{\\mathcal{X},\\mathcal{X}}$ . We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\pmb x}\\sim\\pi^{\\prime}}{\\mathbb{E}}u_{i}(\\psi_{i}({\\pmb x}_{i}),{\\pmb x}_{-i})=\\underset{{\\pmb y}\\sim\\pi}{\\mathbb{E}}u_{i}((\\psi_{i}^{\\delta}\\circ\\phi_{i})({\\pmb y}_{i}),\\phi_{-i}({\\pmb x}_{-i}))}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{{\\pmb\\phi}_{i}^{*}\\in\\Phi_{\\mathcal{X},\\mathscr{Y}}}{\\operatorname*{max}}\\underset{{\\pmb y}\\sim\\pi}{\\mathbb{E}}u_{i}(\\phi_{i}^{*}({\\pmb y}_{i}),\\phi_{-i}({\\pmb x}_{-i}))}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{{\\pmb y}\\sim\\pi}{\\mathbb{E}}u_{i}(\\phi_{i}({\\pmb y}_{i}),\\phi_{-i}({\\pmb x}_{-i}))+\\epsilon}\\\\ &{\\qquad\\qquad\\qquad=\\underset{{\\pmb x}\\sim\\pi^{\\prime}}{\\mathbb{E}}u_{i}({\\pmb x}_{i},{\\pmb x}_{-i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second line uses the assumed composition property. ", "page_idx": 25}, {"type": "text", "text": "The revelation principle has, of course, been shown for various special cases of equilibrium before us: for example, NFCE Aumann [1974], linear-swap equilibria Zhang et al. [2024], and so on. Our proposition above generalizes these proofs since each of those notions indeed satisfies the requisite compositional criterion: compositions of arbitrary functions are still arbitrary functions, and compositions of linear maps are linear. ", "page_idx": 25}, {"type": "text", "text": "The definition of $k$ -mediator functions and degree- $\\cdot k$ polynomials are both easy to generalize from the $\\mathcal X\\rightarrow\\mathcal X$ case to the $y\\rightarrow\\,x$ case. We can therefore define generalized $k$ -mediator equilibria and generalized degree- $k$ swap equilibria, and ask whether the revelation principle applies to these. Proposition H.3 does not apply, because compositions of $k$ -mediator and degree- ${\\cdot k}$ functions will usually require more mediators and a higher degree. Indeed, we now show that the revelation principle fails for these notions when $k>1$ . We will, in fact, show something quite strong. Recall Proposition F.5, which showed that both canonical $\\Phi_{\\mathrm{med}}^{k}$ and canonical \u03a6pkoly are strictly tightening notions of equilibrium as $k$ increases. Here we show that this is not the case for generalized equilibria. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Theorem H.4. For every $k\\geq1$ , every linear-swap equilibrium is equivalent to a (non-canonical) $k$ -mediator equilibrium. ", "page_idx": 26}, {"type": "text", "text": "Proof. Let $\\pi$ be any linear-swap equilibrium. We define a mediator of player $i$ . Given a strategy $\\textbf{\\em x}\\in\\mathcal{X}$ , the mediator will interact with the player as follows. First, for each decision point $j$ of player $i$ , and let $a_{j}\\in A_{j}$ be the action that is played by $\\textbf{\\em x}$ . (If $\\textbf{\\em x}$ defines no action at $j$ , the mediator selects one at random). The mediator samples integers aj , . . . , aj $a_{j}^{(1)},\\ldots,a_{j}^{(k)}\\,\\in\\,[b_{j}]$ uniformly at random under the constraint that $\\begin{array}{r}{\\sum_{\\ell=1}^{k}a_{j}^{(\\ell)}\\,=\\,a_{j}}\\end{array}$ (mod $b_{j}$ ). The important property that is derived from the above construction is simply that, in order to learn aj, the player must know all the a(j\u2113 )s, and without knowing all of them, the player learns no information whatsoever. ", "page_idx": 26}, {"type": "text", "text": "When the player arrives, the mediator has the following interaction with the player. First, the player must supply an integer $\\ell\\in[k]$ . We will call $\\ell$ the seed. Then, whenever the player sends a decision point $j$ , the mediator checks whether the decision point sent is consistent with the sequence of decision points previously sent by the player (i.e., if $j$ is a possible next-decision-point following the previous decision point sent). If not, the mediator terminates the interaction. If so, the mediator sends $a_{j}^{(\\ell)}$ to the player. ", "page_idx": 26}, {"type": "text", "text": "Now consider how a player can use $k$ copies of such a mediator. In order to learn any useful information at all about any decision point $j$ , the player must (1) supply a different seed to each of the $k$ mediators, and (2) query decision point $j$ with all $k$ mediators. Therefore, the player can essentially only use these mediators as if they were one, giving the same sequence of queries to every mediator and computing the true action recommendation by computing $\\textstyle\\sum_{\\ell}a_{j}^{(\\ell)}{\\pmod{b_{j}}}$ . Thus, the player can only implement deviations that it could with a single mediator, which by Theorem E.2 are the linear deviations. ", "page_idx": 26}, {"type": "text", "text": "What we have thus shown is the following. Let $\\mathcal{y}_{i}$ be a strategy space that represents the above interaction with the mediator, and let $\\phi_{i}$ be the $k$ -mediator deviation that acts according to the previous paragraph. Let $\\pi^{\\prime}\\in\\Delta(\\mathcal{V}_{1}\\times\\cdot\\cdot\\cdot\\times\\mathcal{V}_{n})$ be the distribution in which a strategy $x\\sim\\pi$ is sampled, the $a_{j}^{(\\ell)}\\mathrm{s}$ are sampled according to the first paragraph, and each mediator plays according to the strategy $\\pmb{y}_{i}$ that answers queries according to those $a_{j}^{(\\ell)}\\mathrm{s}$ . Then $((\\mathcal{V}_{i},\\phi_{i})_{i=1}^{n},\\pi^{\\prime})$ is a noncanonical $\\Phi_{\\mathrm{med}}^{k}$ -equilibrium. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Theorem H.5. Suppose that every player\u2019s strategy space in a given game $\\Gamma$ is the hypercube $\\mathcal{X}=\\{-1,1\\}^{N}$ . Then every linear-swap in $\\Gamma$ equilibrium is equivalent to a (non-canonical) degree$k$ -swap equilibrium. ", "page_idx": 26}, {"type": "text", "text": "Proof. The proof follows a similar idea to the previous one: we wish to construct a scenario such that, with any polynomial of degree less than $k$ , the deviator cannot learn anything about $\\textbf{\\em x}$ , and with a polynomial of degree $k$ the deviator can only implement linear functions on $\\textbf{\\em x}$ . Let $\\pi$ be any linearswap equilibrium. Let $\\mathcal{V}=\\{-1,1\\}^{N k}$ , and index the coordinates of $\\pmb{y}\\in\\mathcal{V}$ by pairs $(j,\\ell)$ where $j\\in\\bar{[N]}$ and $\\ell\\in[k]$ . Define $\\begin{array}{r}{\\phi(\\pmb{y})_{j}=\\prod_{\\ell\\in[k]}\\pmb{y}[j,\\ell]}\\end{array}$ . Finally, define the distribution $\\pi^{\\prime}\\in\\Delta(\\mathcal{V}^{n})$ as follows: sampling $\\pmb{y}\\sim\\pi^{\\prime}$ is done by sampling $x\\,\\sim\\,\\pi$ , and then, for each player $i$ and index $j$ , sampling $\\pmb{y}_{i}[j,\\cdot]\\in\\{-1,1\\}^{\\ell}$ uniformly at random under the constraint that $\\begin{array}{r}{\\hat{\\Pi}_{\\ell}\\,\\dot{\\pmb y}_{i}[j,\\ell]=\\pmb x_{i}[j]}\\end{array}$ . We will write $\\pi^{\\prime}|x$ for the conditional distribution of $\\pmb{y}\\sim\\pi^{\\prime}$ , conditioned on  sampling the given $x\\,\\sim\\,\\pi$ . Now consider the generalized profile $(\\mathfrak{V},\\phi,\\pi^{\\prime})$ (i.e., every player shares the same signal set $\\boldsymbol{\\wp}$ and equilibrium deviation function $\\phi:\\mathcal{Y}\\rightarrow\\mathcal{X})$ ). We see that it is equivalent to $(\\mathcal{X},\\mathrm{id},\\pi)$ by construction. We claim now that it is a degree- $k$ equilibrium which would complete the proof. Consider any degree- $k$ function $\\phi^{\\prime}:\\mathcal{V}\\rightarrow\\mathcal{X}$ , and define $\\psi:\\mathcal{X}\\rightarrow\\mathcal{X}$ by $\\psi(\\pmb{x})=\\mathbb{E}_{\\pmb{y}\\sim\\pi^{\\prime}|\\pmb{x}}\\,\\phi^{\\prime}(\\pmb{y})$ . It suffices to show that $\\psi$ is a linear map, since then deviating to $\\phi^{\\prime}$ would equate to applying the linear ", "page_idx": 26}, {"type": "text", "text": "deviation $\\psi$ to $\\textbf{\\em x}$ . Indeed, expressing ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\phi^{\\prime}(\\pmb{y})=\\sum_{\\pmb{S}\\subseteq[N]\\times[k],|\\pmb{S}|\\leq k}\\alpha_{\\pmb{S}}m_{\\pmb{S}}(\\pmb{y})\\quad\\mathrm{where}\\quad m_{\\pmb{S}}(\\pmb{y}):=\\prod_{(j,\\ell)\\in\\cal S}\\pmb{y}[i,s],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we see that $\\mathbb{E}_{\\pmb{y}\\sim\\pi^{\\prime}|\\boldsymbol{x}}\\,m_{S}(\\pmb{y})=0$ except when $S=S_{j}:=\\{(j,\\ell):\\ell\\in[k]\\}$ for some $j$ , in which case $\\mathbb{E}_{{\\pmb y}\\sim\\pi^{\\prime}|x}\\,m_{S}({\\pmb y})={\\pmb x}[j]$ . That is, the only monomials of nonzero expectation are those which multiply together all the $\\pmb{y}[j.$ , \u00b7]s, in which case the expectation is exactly $\\pmb{x}[j]$ . Thus, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\psi(\\pmb{x})=\\sum_{S}\\alpha_{S}\\,\\underset{\\pmb{y}\\sim\\pi^{\\prime}|\\pmb{x}}{\\mathbb{E}}[m_{S}(\\pmb{y})]=\\sum_{j}\\alpha_{S_{j}}\\,\\underset{\\pmb{y}\\sim\\pi^{\\prime}|\\pmb{x}}{\\mathbb{E}}\\left[m_{S_{j}}(\\pmb{y})\\right]=\\sum_{j}\\alpha_{S_{j}}\\pmb{x}[j]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which is indeed linear in $\\textbf{\\em x}$ . ", "page_idx": 27}, {"type": "text", "text": "One may wonder at this point about why the above two results do not contradict the fact that NFCE, which supposedly are the $\\Phi_{\\mathrm{med}}^{N}$ -equilibria, do satisfy the revelation principle and are certainly not equivalent to linear-swap equilibria. The answer is that the equivalence between swap deviations and $N$ -mediator deviations only holds for strategy spaces of size at most $N$ \u2014and the strategy spaces $\\mathcal{y}_{i}$ constructed as part of both proofs have size (at least) $N k$ . When $\\mathcal{y}_{i}$ has size more than $N$ , the set $\\Phi_{\\mathrm{med}}^{N}$ is no longer the set of all functions $y_{i}\\rightarrow\\operatorname{conv}\\mathcal{X}_{i}$ . In other words, the set of canonical $\\Phi_{\\mathrm{med}^{-}}^{N}$ equilibria is equivalent to the set of canonical NFCE, but that does not contradict the fact that there exist non-canonical $\\Phi_{\\mathrm{med}}^{N}$ -equilibria. ", "page_idx": 27}, {"type": "text", "text": "I Omitted proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide the proofs omitted from the previous sections. ", "page_idx": 27}, {"type": "text", "text": "I.1 Proofs from Section B ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We first provide the missing proofs from Appendix B. We start with Proposition B.1, the statement of which is recalled below. ", "page_idx": 27}, {"type": "text", "text": "Proposition B.1 (Hazan and Kale, 2007). Consider a regret minimizer $\\mathcal{R}$ operating over $[0,1]^{N}$ . If $\\mathcal{R}$ runs in time pol $\\mathsf{y}(N,1/\\epsilon)$ and guarantees $\\overline{{\\mathrm{Reg}}}_{\\Phi^{\\beta}}^{T}\\leq\\epsilon$ for any sequence of utilities, then there is $a$ po $\\mathsf{\\partial}_{\\mathsf{N}}(N,1/\\epsilon)$ algorithm for computing an $(\\epsilon\\sqrt{N})$ -fixed point of any $\\phi^{\\beta}\\in\\Phi^{\\beta}$ with respect to $\\|\\cdot\\|_{2}$ , assuming that $\\phi^{\\beta}$ can be evaluated in polynomial time. ", "page_idx": 27}, {"type": "text", "text": "Proof. Suppose that $\\mathcal{R}$ outputs a strategy $\\mathbf{\\boldsymbol{x}}^{(t)}\\in\\mathrm{conv}\\,\\boldsymbol{\\chi}$ at each time $t\\in[T]$ . The basic idea is to determ\u221aine whether $\\|\\phi^{\\beta}(\\pmb{x}^{(t)})\\!-\\!\\pmb{x}^{(t)}\\|_{2}\\leq\\epsilon\\sqrt{N}$ ; if so, the process can terminate as we have identified an $(\\epsilon\\sqrt{N})$ -fixed point of $\\phi^{\\beta}$ with respect to $\\|\\cdot\\|_{2}$ . Otherwise, we construct the utility function ", "page_idx": 27}, {"type": "equation", "text": "$$\nu^{(t)}:\\operatorname{conv}\\mathcal{X}\\ni x\\mapsto\\frac{1}{\\sqrt{N}}\\frac{1}{\\|\\phi^{\\beta}(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}\\|_{2}}\\langle\\phi^{\\beta}(\\pmb{x}^{(t)})-\\pmb{x}^{(t)},\\pmb{x}-\\pmb{x}^{(t)}\\rangle.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We note that, by Cauchy-Schwarz, $|u^{(t)}({\\pmb x})|\\ \\leq\\ 1$ since $\\|\\pmb{x}\\mathrm{~-~}\\pmb{x}^{(t)}\\|_{2}\\;\\le\\;\\sqrt{N}$ ; hence, the utility function adheres to our normalization constraint. Now, if at all rounds it was the case that $\\|\\dot{\\phi}^{\\beta}({\\pmb x}^{(t)})-{\\pmb x}^{(t)}\\|_{2}>\\epsilon\\sqrt{N}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{Reg}}}_{\\Phi^{\\beta}}^{T}\\geq\\frac{1}{T}\\sum_{t=1}^{T}u^{(t)}(\\phi^{\\beta}(\\pmb{x}^{(t)}))-\\frac{1}{T}\\sum_{t=1}^{T}u^{(t)}(\\pmb{x}^{(t)})>\\epsilon,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "since ${\\boldsymbol u}^{(t)}({\\boldsymbol x}^{(t)})=0$ and $\\begin{array}{r}{u^{(t)}(\\phi^{\\beta}(\\pmb{x}^{(t)}))=\\frac{1}{\\sqrt{N}}\\|\\phi^{\\beta}(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}\\|_{2}}\\end{array}$ for all $t\\in[T]$ . We conclude that (5) contradicts the assumption that $\\overline{{\\mathrm{Reg}}}_{\\Phi^{\\beta}}^{T}\\,\\le\\,\\epsilon$ for any\u221a sequence of utilities, in turn implying that there exists $t\\in[T]$ such that $\\|\\phi^{\\beta}(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}\\|_{2}\\leq\\epsilon\\sqrt{N}$ . Given that we can evaluate $\\phi^{\\beta}$ in time $\\mathsf{p o l y}(N)$ (by assumption), we can also compute each error $\\|\\phi^{\\beta}({\\pmb x}^{(t)})-{\\pmb x}^{(t)}\\|_{2}$ in polynomial time, concluding the proof. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "We next turn our attention to the proof of Theorem 3.3. We first observe that $\\Phi^{\\beta}$ contains functions of the following form. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left(\\sum_{S_{1}\\subseteq[N]}\\phi_{1}(S_{1})\\prod_{j\\in S_{1}}x[j]\\prod_{j\\in[N]\\setminus S_{1}}(1-x[j]),\\ldots,\\sum_{S_{N}\\subseteq[N]}\\phi_{N}(S_{N})\\prod_{j\\in S_{N}}x[j]\\prod_{j\\in[N]\\setminus S_{N}}(1-x[j])\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\phi=(\\phi_{1},\\ldots,\\phi_{N}):\\{0,1\\}^{N}\\rightarrow\\{0,1\\}^{N}$ . (The convention above is that a product with no terms is to be evaluated as 1.) ", "page_idx": 28}, {"type": "text", "text": "Above, we slightly abuse notation by interpreting $S_{j}~\\subseteq~[N]$ as a point in $\\{0,1\\}^{N}$ . Lemma I.1 is evident from the definition of the behavioral strategy map $\\beta$ : $\\phi^{\\beta}({\\pmb x})\\,=\\,\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\beta({\\pmb x})}\\,\\phi({\\pmb x}^{\\prime})$ , and expanding the expectation gives the expression of Lemma I.1\u2014the probability of a set $S$ is exactly $\\begin{array}{r}{\\prod_{j\\in S}{\\pmb x}[\\breve{j}]\\prod_{j\\in[N]\\backslash S}(1-\\pmb x[j])}\\end{array}$ . We will show that PPAD-hardness for computing fixed points persists under the set of functions contained in $\\Phi^{\\beta}$ (per Lemma I.1). ", "page_idx": 28}, {"type": "text", "text": "Our starting point is the usual generalized circuit problem (abbreviated as GCIRCUIT), introduced by Chen et al. [2009]; it is a generalization of a typical arithmetic circuit but with the twist that it may include cycles. (In what follows, we borrow some notation from the paper of Filos-Ratsikas et al. [2023].) ", "page_idx": 28}, {"type": "text", "text": "Definition I.2 (Chen et al., 2009). A generalized circuit with respect to a set of gate-types $\\mathcal{G}$ is a list of gates $g_{1},\\ldots,g_{\\sf M}$ . Every gate $g_{i}$ has a type $G_{i}\\in\\mathcal{G}$ . Depending on its type, $g_{i}$ may have zero, one or two input gates, which will be index by $j,k\\in[\\mathsf{M}]$ , with the restriction that $i,j,k$ are pairwise distinct. ", "page_idx": 28}, {"type": "text", "text": "We denote by $v:[\\mathsf{M}]\\to[0,1]$ a function that assigns an input gate to a value in $[0,1]$ . Each gate imposes a constraint induced by its corresponding type. For example, let us consider the following types. ", "page_idx": 28}, {"type": "text", "text": "\u2022 if $G_{i}=G_{+}$ , then $v(g_{i})=\\operatorname*{min}(1,v(g_{j})+v(g_{k}))$ ;   \n\u2022 if $G_{i}=G_{-}$ , then $v(g_{i})=\\operatorname*{max}(0,v(g_{j})-v(g_{k}))$ ;   \n\u2022 if $G_{i}=G_{1}$ , then $v(g_{i})=1$ ;   \n\u2022 if $G_{i}=G_{1-}$ , then $v(g_{i})=1-v(g_{j})$ ; and \u2022 if $G_{i}=G_{\\times}$ , then $v(g_{i})=v(g_{j})\\cdot v(g_{k})$ . ", "page_idx": 28}, {"type": "text", "text": "In accordance with the above types, we define $F:[0,1]^{\\mathsf{M}}\\to[0,1]^{\\mathsf{M}}$ to be the function mapping any initial evaluation of the gates to $(v(g_{1}),\\dotsc,v(g_{\\sf M}))$ . The main problem of interest can be now phrased as follows. ", "page_idx": 28}, {"type": "text", "text": "Definition I.3 ( $\\epsilon$ -GCIRCUIT). The problem $\\epsilon$ -GCIRCUIT asks for a fixed point of $F$ with respect to $\\|\\cdot\\|_{\\infty}$ ; that is, an assignment $v:[\\mathsf{M}]\\to[0,1]$ such that all gates are $\\epsilon_{}$ -satisfied. ", "page_idx": 28}, {"type": "text", "text": "A satisfying assignment always exists by Brouwer\u2019s theorem, but the associated computational problem is PPAD-hard. In fact, by virtue of a recent result of Filos-Ratsikas et al. [2023], PPAD-hardness persists even if one significantly restricts the type of gates. ", "page_idx": 28}, {"type": "text", "text": "Theorem I.4 (Filos-Ratsikas et al., 2023). Even when $\\mathcal{G}:=\\{G_{+},G_{1-}\\}$ , there is an absolute constant $\\epsilon>0$ such that computing an \u03f5-fixed point of $F$ with respect to $\\|\\cdot\\|_{\\infty}$ is PPAD-complete.14 ", "page_idx": 28}, {"type": "text", "text": "Yet, the addition gate $G_{+}$ does not induce a multilinear in the form of Lemma I.1. We will address this by showing that the gate $G_{+}$ can be approximately simulated via a small number of gates with type either $G_{1-}$ or $G_{\\times}$ . To provide better intuition, we first prove this claim under the assumption that the gates are satisfied exactly, and we then proceed with the more general statement. In the sequel, we sometimes use the shorthand notation $\\bar{t}=t^{\\prime}\\pm\\epsilon\\iff t\\in[t^{\\prime}-\\epsilon,t^{\\prime}+\\epsilon]$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma I.5. Any addition gate $G_{+}$ can be approximated with at most $\\epsilon\\mathrm{~\\ensuremath~{~>~}~0~}$ error using ${\\cal O}(\\log(1/\\epsilon)/\\epsilon)$ gates with type either $G_{1-}$ or $G_{\\times}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Let $t_{1}^{(1)},t_{2}^{(1)}\\in[0,1]$ . We define the mapping ", "page_idx": 29}, {"type": "equation", "text": "$$\nf:(t_{1},t_{2})\\mapsto(1-(1-t_{1})(1-t_{2}),t_{1}t_{2}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We consider the sequence t(1\u03c4+1), t(2\u03c4+1):= f(t(1\u03c4 ), t(2\u03c4 )) for \u03c4 = 1, 2, . . . . We will show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}(1,t_{1}^{(1)}+t_{2}^{(1)})=t_{1}^{(C)}\\pm\\epsilon,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$\\begin{array}{r}{C\\,\\ge\\,\\frac{\\log(1/\\epsilon)}{\\log\\left(\\frac{1}{1-\\epsilon}\\right)}+1\\,=\\,\\Theta(\\log(1/\\epsilon)/\\epsilon)}\\end{array}$ . We first observe that $t_{1}^{(\\tau+1)}\\,\\geq\\,t_{1}^{(\\tau)}$ and $t_{1}^{(\\tau)}+t_{2}^{(\\tau)}=$ $t_{1}^{(1)}+t_{2}^{(1)}$ . We consider two cases. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If $t_{1}^{(C)}>1-\\epsilon$ , then $t_{1}^{(1)}+t_{2}^{(1)}=t_{1}^{(C)}+t_{2}^{(C)}\\geq1-\\epsilon$ , in turn implying that $\\operatorname*{min}(1,t_{1}^{(1)}+$ $t_{2}^{(1)})\\geq1-\\epsilon$ . This clearly implies (6) as $t_{1}^{(C)}\\in[0,1]$ .   \n\u2022 In the contrary case, if $t_{1}^{(C)}\\leq1-\\epsilon$ , then $\\begin{array}{r}{t_{2}^{(C)}\\leq\\prod_{\\tau=1}^{C-1}t_{1}^{(\\tau)}\\leq(1-\\epsilon)^{C-1}\\leq\\epsilon}\\end{array}$ , where we   \nused the fact that $t_{1}^{(\\tau)}\\,\\le\\,t_{1}^{(C)}\\,\\le\\,1\\,-\\,\\epsilon$ . So, $\\operatorname*{min}(1,t_{1}^{(1)}+t_{2}^{(1)})=\\operatorname*{min}(1,t_{1}^{(C)}+t_{2}^{(C)})=$ $t_{1}^{(C)}+t_{2}^{(C)}\\leq t_{1}^{(C)}\\,\\overline{{+}}\\;\\epsilon.$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma I.6. Any addition gate $G_{+}$ can be approximated with at most $\\epsilon\\mathrm{~\\ensuremath~{~>~}~0~}$ error using ${\\cal O}(\\log(1/\\epsilon)/\\epsilon)$ gates with type either $G_{1-}$ or $G_{\\times}$ , each with error $\\epsilon^{\\prime}=O(\\epsilon/C)=O(\\epsilon^{2})$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Let $t_{1}^{(1)},t_{2}^{(1)}\\in[0,1]$ . We consider the sequence ", "page_idx": 29}, {"type": "equation", "text": "$$\n[0,1]^{2}\\ni(t_{1}^{(\\tau+1)},t_{2}^{(\\tau+1)}):=(1-(1-t_{1}^{(\\tau)})(1-t_{2}^{(\\tau)})\\pm5\\epsilon^{\\prime},t_{1}^{(\\tau)}t_{2}^{(\\tau)}\\pm\\epsilon^{\\prime})\\quad\\tau=1,2,\\ldots.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here, $(t_{1}^{(\\tau+1)},t_{2}^{(\\tau+1)})$ can be indeed obtained from $(t_{1}^{(\\tau)},t_{2}^{(\\tau)})$ using 5 gates with type either $G_{1-}$ or $G_{\\times}$ , each with error at most $\\epsilon^{\\prime}$ . We will show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}(1,t_{1}^{(1)}+t_{2}^{(1)})=t_{1}^{(C)}\\pm\\epsilon\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for $\\begin{array}{r}{C\\ge\\frac{\\log(6/\\epsilon)}{\\log\\left(\\frac{1}{1-\\epsilon/12}\\right)}+1=\\Theta(\\log(1/\\epsilon)/\\epsilon)}\\end{array}$ . Below, we will take $\\epsilon^{\\prime}:=\\epsilon/(12C)$ . We first observe that $t_{1}^{(\\tau+1)}+t_{2}^{(\\tau+1)}=t_{1}^{(\\tau)}+t_{2}^{(\\tau)}\\pm6\\epsilon^{\\prime}$ , thereby implying that $t_{1}^{(C)}+t_{2}^{(C)}=t_{1}^{(1)}+t_{2}^{(1)}\\pm6C\\epsilon^{\\prime}$ . Further, $t_{1}^{(\\tau)}\\le t_{1}^{(\\tau+1)}+5\\epsilon^{\\prime}$ t(1\u03c4+1)+ 5\u03f5\u2032. Hence, t(1\u03c4)\u2264t(1C)+ 5C\u03f5\u2032. We consider two cases. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If $t_{1}^{(C)}>1-\\epsilon/2$ , then $t_{1}^{(1)}+t_{2}^{(1)}\\ge t_{1}^{(C)}+t_{2}^{(C)}-6C\\epsilon^{\\prime}\\ge1-\\epsilon/2-6C\\epsilon^{\\prime}=1-\\epsilon$ since $\\epsilon^{\\prime}=\\epsilon/(12C)$ . This implies that $\\operatorname*{min}(1,t_{1}^{(1)}+t_{2}^{(1)})\\geq1-\\epsilon$ , and (7) follows. \u2022 In the contrary case, if $t_{1}^{(C)}\\,\\leq\\,1\\,-\\,\\epsilon/2$ , then $t_{2}^{(C)}\\,\\le\\,(1\\,-\\,\\epsilon/2+5C\\epsilon^{\\prime})t_{2}^{(C-1)}\\,+\\,\\epsilon^{\\prime}\\,=$ $(1-\\epsilon/12)t_{2}^{(C-1)}+\\epsilon^{\\prime}$ h. uIsn, $t_{2}^{(C)}\\le C\\epsilon^{\\prime}+(1-\\epsilon/12)^{C-1}\\le\\epsilon/4$ t. $\\begin{array}{r}{C\\geq\\frac{\\log(6/\\epsilon)}{\\log\\left(\\frac{1}{1-\\epsilon/12}\\right)}+1}\\end{array}$ $t_{1}^{(1)}+t_{2}^{(1)}\\le t_{1}^{(C)}+t_{2}^{(C)}+6C\\epsilon^{\\prime}\\le1+\\epsilon/4$ We conclude by observing that $t_{1}^{(C)}=t_{1}^{(1)}+t_{2}^{(1)}-t_{2}^{(C)}\\pm6C\\epsilon^{\\prime}=t_{1}^{(1)}+t_{2}^{(1)}\\pm3\\epsilon/4.$ ", "page_idx": 29}, {"type": "text", "text": "As a result, for any absolute constant $\\epsilon\\,>\\,0$ , we can reduce in polynomial time an $\\epsilon_{}$ -GCIRCUIT instance consisting of $\\mathsf{M}$ gates with a set of types $\\mathcal{G}\\,=\\,\\{G_{1-},\\bar{G}_{+}\\}$ to an $\\epsilon^{\\prime}$ -GCIRCUIT instance consisting of $\\Theta(\\mathsf{M})$ gates with a set of types $\\mathcal{G}^{\\prime}=\\{G_{1-},\\dot{G}_{\\times}\\}$ , so long as $\\epsilon^{\\prime}=O(\\epsilon^{2})$ is sufficiently small. Together with Theorem I.4, we arrive at the following conclusion. ", "page_idx": 29}, {"type": "text", "text": "Proposition I.7. Even when ${\\mathcal G}\\;:=\\;\\{G_{\\times},G_{1-}\\}$ , there is an absolute constant $\\epsilon\\mathrm{~>~0~}$ such that computing an $\\epsilon$ -fixed point of $F$ with respect to $\\|\\cdot\\|_{\\infty}$ is PPAD-complete. ", "page_idx": 29}, {"type": "text", "text": "Having established this reduction, a function $F$ arising from such circuits is clearly a multilinear that can be expressed in the form of Lemma I.1. Indeed, we simply observe that, for $S\\subseteq[N]$ and $\\bar{S}\\subseteq[N]$ with $\\dot{\\cal S}\\cap\\bar{\\cal S}=\\emptyset$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\prod_{j\\in S}x[j]\\prod_{j\\in\\bar{S}}(1-x[j])=\\prod_{j\\in[N]\\backslash S\\cup\\bar{S}}(x[j]+1-x[j])\\prod_{j\\in S}x[j]\\prod_{j\\in\\bar{S}}(1-x[j])}\\\\ &{\\displaystyle=\\sum_{S^{\\prime},S^{\\prime\\prime}}\\prod_{j\\in S\\cup S^{\\prime}}x[j]\\prod_{j\\in\\bar{S}\\cup S^{\\prime\\prime}}(1-x[j]),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the summation is over all partitions $S^{\\prime},S^{\\prime\\prime}$ of $[N]\\setminus(S\\cup{\\bar{S}})$ . So, combining with Proposition B.1, we arrive at Theorem 3.3, which is restated below for the convenience of the reader. ", "page_idx": 30}, {"type": "text", "text": "Theorem 3.3\u221a. If a regret minimizer $\\mathcal{R}$ outputs strategies in $[0,1]^{N}$ , it is PPAD-hard to guarantee $\\overline{{\\mathrm{Reg}}}_{\\Phi^{\\beta}}\\leq\\epsilon/\\sqrt{N}$ , even with respect to low-degree deviations and an absolute constant $\\epsilon>0$ . ", "page_idx": 30}, {"type": "text", "text": "In the above reduction, we used the inequality $\\|\\cdot\\|_{\\infty}\\,\\leq\\,\\|\\cdot\\|_{2}$ so as to translate the guarantee of Proposition B.1 in the language of Theorem I.4. That inequality can be loose, and so instead let us explain how one can obtain sharper hardness results by relying on a stronger complexity assumption which pertains to the so-called $(\\epsilon,\\delta)$ -GCIRCUIT problem. This relaxes the $\\epsilon_{}$ -GCIRCUIT problem by allowing at most a $\\delta$ -fraction of the gates to have an error larger than \u03f5. In this context, Babichenko et al. [2016] put forward the following conjecture. ", "page_idx": 30}, {"type": "text", "text": "Conjecture I.8 (Babichenko et al., 2016). There exist absolute constants $\\epsilon,\\delta>0$ such that solving the $(\\epsilon,\\delta)$ -GCIRCUIT problem with M gates requires $2^{\\tilde{\\Omega}(\\mathsf{M})}$ time. ", "page_idx": 30}, {"type": "text", "text": "In light of the simplifications observed by Filos-Ratsikas et al. [2023] (Theorem I.4) in conjunction with Lemma I.6, it is not hard to show that Conjecture I.8 can be equivalently phrased by restricting the gates to involve solely multilinear operations\u2014analogously to Proposition I.7. Namely, if the number of gates increases by a factor of $C\\,=\\,C(\\epsilon)$ , it suffices to take $\\epsilon^{\\prime}\\,=\\,\\Theta(\\epsilon/C)$ and $\\delta^{\\prime}\\,=$ $\\Theta(\\delta/C)$ . Now, the point is that Conjecture I.8 is more aligned with a guarantee in terms of $\\|\\cdot\\|_{2}$ . Indeed, if at least a $\\delta$ -fraction of the gates incur at least an $\\epsilon$ error, it follows that $\\|F({\\boldsymbol{\\mathbf{x}}})-{\\boldsymbol{\\mathbf{\\mathit{x}}}}\\|_{2}\\geq$ $\\epsilon\\sqrt{\\delta}\\sqrt{N}$ (we can assume here that $\\delta N$ is an integer). We can thus strengthen Theorem 3.3 as follows. ", "page_idx": 30}, {"type": "text", "text": "Theorem I.9. Suppose that Conjecture I.8 holds. If $\\mathcal{R}$ outputs strategies in $[0,1]^{N}$ , guaranteeing $\\overline{{\\mathrm{Reg}}}_{\\Phi^{\\beta}}\\leq\\epsilon$ requires time $2^{\\tilde{\\Omega}(N)}$ , even with respect to low-degree deviations and an absolute constant $\\epsilon>0$ . ", "page_idx": 30}, {"type": "text", "text": "I.2 Proofs from Section C ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We first spell out the construction that establishes Theorem C.2, which is a refinement of the algorithm of Gordon et al. [2008] described earlier in Appendix A. In particular, the one but crucial difference lies in using an $\\epsilon$ -expected fixed point (Line 3). In the context of Algorithm 1, we assume that the interface of a regret minimizer $\\mathcal{R}$ consists of two components: $\\mathcal{R}.\\mathrm{NEXTSTRATEGY}\\big(\\big)$ , which returns the next strategy of $\\mathcal{R}$ ; and $\\mathcal{R}$ .OBSERVEUTILITY $(\\cdot)$ , which provides to $\\mathcal{R}$ as feedback a utility function, whereupon $\\mathcal{R}$ may update its internal state accordingly. ", "page_idx": 30}, {"type": "table", "img_path": "c4ElkpA0kh/tmp/c62afd8b28c41edc97ffd7d35d38fc2ac7c748709e4fd84ce70273588d7454e2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "We next show that there is a certain equivalence between expected fixed points and $\\Phi.$ -regret minimization over $\\Delta(\\mathcal{X})$ , which mirrors the construction of Hazan and Kale [2007]. ", "page_idx": 30}, {"type": "text", "text": "Proposition C.3. Consider a regret minimizer $\\mathcal{R}$ operating over $\\Delta(\\mathcal{X})$ . If $\\mathcal{R}$ runs in time po $|\\mathsf{y}(N,1/\\epsilon)$ and guarantees $\\overline{{\\mathrm{Reg}}}_{\\Phi}^{T}\\le\\epsilon$ for any sequence of utilities, then there is a poly $(N,1/\\epsilon)$ algorithm for computing $(\\epsilon D_{\\mathcal{X}})$ -expected fixed points of $\\phi\\,\\in\\,\\Phi$ , assuming that we can efficiently compute $\\mathbb{E}_{\\pmb{x}^{(t)}\\sim\\pi^{(t)}}[\\phi(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}]$ at any time $t$ . Here, $D_{\\mathcal{X}}$ is the diameter of $\\mathcal{X}$ with respect to $\\|\\cdot\\|_{2}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Suppose that $\\mathcal{R}$ outputs a strategy $\\pi^{(t)}\\in\\Delta(\\mathcal{X})$ . At each time $t\\in[T]$ , we can terminate if $\\|\\mathbb{E}_{\\pmb{x}^{(t)}\\sim\\pi^{(t)}}[\\phi(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}]\\|_{2}\\le\\epsilon D_{X}$ ; that is, we have identified an $(\\epsilon D_{\\mathcal{X}})$ -expected fixed point. Otherwise, we construct the utility function ", "page_idx": 31}, {"type": "equation", "text": "$$\nu^{(t)}:\\mathcal{X}\\ni x\\mapsto\\frac{1}{D_{\\mathcal{X}}}\\frac{1}{\\|\\mathbb{E}_{\\mathbf{x}^{(t)}\\sim\\pi^{(t)}}[\\phi(\\mathbf{x}^{(t)})-\\mathbf{x}^{(t)}]\\|_{2}}\\left\\langle\\underset{\\mathbf{x}^{(t)}\\sim\\pi^{(t)}}{\\mathbb{E}}[\\phi(\\mathbf{x}^{(t)})-\\mathbf{x}^{(t)}],\\mathbf{x}-\\pi^{(t)}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which indeed satisfies the normalization constraint $|u^{(t)}({\\pmb x})|\\leq1$ . Now, if at all iterations it was the case that $\\|\\mathbb{E}_{\\pmb{x}^{(t)}\\sim\\pi^{(t)}}[\\phi(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}]\\|_{2}>\\epsilon D_{X}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{Reg}}}_{\\Phi}^{T}\\geq\\frac{1}{T}\\sum_{t=1}^{T}u^{(t)}\\left(\\underset{{\\pmb x}^{(t)}\\sim\\pi^{(t)}}{\\mathbb{E}}[\\phi({\\pmb x}^{(t)})]\\right)-\\frac{1}{T}\\sum_{t=1}^{T}u^{(t)}({\\pi^{(t)}})>\\epsilon\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "since $u^{(t)}(\\pi^{(t)})=0$ and ", "page_idx": 31}, {"type": "equation", "text": "$$\nu^{(t)}\\left(\\underset{{\\pmb x}^{(t)}\\sim{\\pmb\\pi}^{(t)}}{\\mathbb{E}}[\\phi({\\pmb x}^{(t)})]\\right)=\\frac{1}{D_{\\mathcal{X}}}\\left\\|\\underset{{\\pmb x}^{(t)}\\sim\\pi^{(t)}}{\\mathbb{E}}[\\phi({\\pmb x}^{(t)})-{\\pmb x}^{(t)}]\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for all $t\\,\\in\\,[T]$ . This contradicts the assumption that $\\overline{{\\mathrm{Reg}}}_{\\Phi}^{T}\\,\\le\\,\\epsilon$ for any sequence of utilities, in turn implying that there exists $t\\,\\in\\,[T]$ such that $\\pi^{(t)}$ is an $\\epsilon$ -expected fixed point. Given that, by assumption, we can compute $\\mathbb{E}_{\\pmb{x}^{(t)}\\sim\\pi^{(t)}}[\\phi(\\pmb{x}^{(t)})-\\pmb{x}^{(t)}]$ for any time $t$ , the claim follows. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "I.3 Proof of Corollary 4.1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this subsection, we discuss how expected fixed points (per Definition C.1) can be used to speed up equilibrium computation even in settings where actual fixed points can be computed in polynomial time. In particular, we recall the following result, which was stated earlier in the main body. ", "page_idx": 31}, {"type": "text", "text": "Corollary 4.1. For any $n$ -player game in normal form, there is an algorithm that computes an $\\epsilon$ -correlated equilibrium and runs in time ", "page_idx": 31}, {"type": "equation", "text": "$$\nO\\left({\\frac{A\\log A}{\\epsilon^{2}}}\\left(\\mathsf{E O}(n,A)+n{\\frac{A^{2}}{\\epsilon}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Indeed, using the algorithm of Blum and Mansour [2007] instantiated with MWU,15 one can guarantee (average) swap regret bounded as $O(\\sqrt{A\\log A/T})$ , where $T$ is the number of iterations; hence, taking $T\\,=\\,{\\cal O}(A\\log A/\\epsilon^{2})$ guarantees at most $\\epsilon$ swap regret for each player. Further, a function here can be represented as a stochastic matrix on $A$ states; as such, it can be evaluated at any point in time $\\bar{O}(A^{2})$ via a matrix-vector product. As a result, each iteration of Algorithm 1 can be implemented with a single oracle call to (2), along with $n$ updates\u2014one for each player\u2014each of which has complexity bounded as $O(A^{2}/\\epsilon)$ (Theorem C.7). This implies Corollary 4.1. ", "page_idx": 31}, {"type": "text", "text": "Let us compare the complexity of Corollary 4.1 with prior results. First, when $\\epsilon\\gg\\,0$ the best running time follows from the recent algorithms of Dagan et al. [2024] and Peng and Rubinstein [2024], but those quickly became superpolynomial even when $\\epsilon\\approx1/\\log A$ ; indeed, the number of iterations of their algorithm scales as $(\\log A)^{\\tilde{O}(1/\\epsilon)}$ . On the other end of the spectrum, one can compute an exact correlated equilibrium in polynomial time using the \u201cellispoid against hope\u201d algorithm [Papadimitriou and Roughgarden, 2008, Jiang and Leyton-Brown, 2011]. The original paper by Papadimitriou and Roughgarden [2008] did not specify the exact complexity of the algorithm, but the subsequent work of Jiang and Leyton-Brown [2011]\u2014which analyzed a slightly different version based on a derandomized separation oracle\u2014came up with a bound of $n^{6}A^{12}$ in terms of the number of iterations of the ellipsoid; the overall running time is higher than that. While the analysis of Jiang and Leyton-Brown [2011] can likely be significantly improved\u2014as acknowledged by the authors, we do not expect that algorithm to be competitive unless one is searching for very high-precision solutions. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "The original algorithm of Blum and Mansour [2007]\u2014instantiated as usual with MWU\u2014requires computing a stationary distribution of a Markov chain in every iteration. This amounts to solving a linear system, which in theory requires a running time of $O(A^{\\omega})$ , where $\\omega\\approx2.37$ is the exponent of matrix multiplication [Williams et al., 2024]. Without fast matrix multiplication\u2014which is currently widely impractical\u2014the running time would instead be $O(A^{3})$ . A strict improvement over that running time can be obtained using the optimistic counterpart of MWU (OMWU) [Daskalakis et al., 2021], which has been shown to reduce the number of iterations to $\\tilde{O}(A/\\epsilon)$ without essentially affecting the per-iteration complexity. Corollary 4.1 improves over that in the regime where $\\epsilon\\geq$ $1/A^{\\frac{\\omega}{2}-\\breve{1}}$ , where $\\omega\\approx2.37$ is the exponent of matrix multipli\u221acation [Williams et al., 2024]; without fast matrix multiplication, the lower bound is instead $\\epsilon\\geq1/\\sqrt{A}$ . ", "page_idx": 32}, {"type": "text", "text": "Another notable attempt at improving the complexity of Blum and Mansour [2007] was made by Greenwald et al. [2008] using power iteration. For a parameter $p\\geq1$ , their algorithm guarantees at most $O({\\sqrt{A p/T}})$ average internal regret with a per-iteration complexity of $\\Omega(A^{3}/p)$ (without fast matrix multiplication). Casting this guarantee in terms of swap regret, and observing that the overall running time is invariant on $p$ , we see that the resulting complexity is no better than that via OMWU, which we discussed earlier; the approach of Greenwald et al. [2008] is based on regret matching, which incurs an inferior regret bound compared to MWU, but is nevertheless known to perform well in practice. An improvement to the algorithm of Blum and Mansour [2007] was obtained by Yang and Mohri [2017], but requires a further assumption on the minimum probability given to every expert; it is unclear whether such an assumption can be guaranteed in general. ", "page_idx": 32}, {"type": "text", "text": "The above discussion concerns algorithms operating with full feedback. In the bandit feedback, Ito [2020] came up with a way to update a single column in each iteration, but it still requires computing a stationary distribution in every iteration. Ito [2020] claims that this can be achieved in time almost quadratic time through the work of Cohen et al. [2017]; however, the complexity of the algorithm of Cohen et al. [2017] depends on the condition number of the Markov chain, and it is unclear how to bound that in our setting. On the other hand, algorithms in the bandit setting do not require access to an expectation oracle. This can be beneficial in certain settings, but our discussion here focuses on the regime where the expectation oracle does not dominate the per-iteration complexity. We conjecture that Theorem C.7 can be generalized to the bandit setting, in which case our improvement will also manifest in the regime where the cost of the expectation oracle far outweighs the per-iteration complexity of Theorem C.7, but that is not within our scope in this paper. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All claims made in the abstract and introduction are proven in Appendices C to E and I. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: All limitations and assumptions are stated in Appendices C to E and I ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The full set of assumptions and proofs are given in Appendices C to E and I. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The contribution of the paper is theoretical, and conforms in every respect with the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The contribution of the paper is theoretical, and we do not foresee any societal impact. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 37}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]