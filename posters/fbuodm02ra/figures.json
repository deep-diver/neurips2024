[{"figure_path": "FbuODM02ra/figures/figures_0_1.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of noisy questions and noisy rationales used in the paper's experiments.  The left-hand side demonstrates examples where the provided context includes irrelevant information (base-10 arithmetic) that is not relevant to solving the actual base-9 math problem. The right-hand side displays examples where the rationales contain irrelevant or inaccurate reasoning steps, again interfering with the correct solution of the base-9 problem. Both illustrate the challenges posed by noisy data in chain-of-thought prompting.", "section": "1 Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_1_1.jpg", "caption": "Figure 2: Results of GPT-3.5 with 0-shot, 3-shot clean rationales, and 3-shot noisy rationales: Both inaccurate and irrelevant rationales degenerate performance significantly, while the proposed CD-CoT improves robustness against noisy rationales.", "description": "This figure displays the accuracy achieved by GPT-3.5 across three different prompting scenarios: zero-shot (no examples), three clean examples, and three examples with noisy rationales. The noisy examples were further divided into two categories: those containing irrelevant information and those with inaccurate information.  The results clearly demonstrate that noisy rationales significantly reduce the accuracy of GPT-3.5. The introduction of irrelevant information reduces accuracy by 1.4%-19.8% compared to the clean examples, while inaccurate information causes a much more significant drop, of 2.2%-40.4%.  In contrast, the proposed CD-CoT method shows significant improvement in robustness against noisy rationales.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_6_1.jpg", "caption": "Figure 3: Chain modeling of the noisy rationale problem: Recovering chain (3) from chain (1) with the guidance of chain (2). From question Xi to answer yi, the rationale of chain (3) includes clean thoughts T() and noisy thoughts (i).", "description": "This figure illustrates the chain-of-thought (CoT) process with noisy rationales.  It shows how a language model can recover a clean chain of reasoning (chain 1) from a noisy chain (chain 3) by using a clean chain (chain 2) as guidance.  Chain 3 contains both clean (T(i)) and noisy thoughts (f(i)). The model learns to separate clean and noisy reasoning steps from the examples to get the correct answers.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_18_1.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of the two types of noisy inputs used in the Noisy Rationales dataset. The left side demonstrates noisy questions that include irrelevant or inaccurate base-10 information, while the right side shows noisy rationales that contain irrelevant or inaccurate reasoning steps, both leading to incorrect solutions in a base-9 calculation problem.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_34_1.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of both noisy questions and noisy rationales.  The noisy questions contain irrelevant information (base-10 calculations) which is misleading when the test question is actually about base-9.  The noisy rationales include irrelevant or inaccurate reasoning steps within the otherwise correct examples. This highlights the core problem that the paper investigates: how robust are large language models when prompted with noisy rationales in a chain-of-thought prompting setting?", "section": "1 Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_34_2.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples from the NoRa dataset, highlighting the difference between inputs with clean rationales and inputs with noisy rationales. The noisy rationales contain irrelevant or inaccurate reasoning steps.  The examples illustrate the challenge of chain-of-thought prompting when dealing with noisy rationales.  Each input consists of three examples and a test question, focusing on base-9 calculation.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_35_1.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of noisy questions and rationales used in the paper's experiments.  The top half shows examples with noisy questions where irrelevant base-10 information is included alongside a base-9 calculation problem. The bottom half shows examples with noisy rationales, where irrelevant or inaccurate reasoning steps are presented within the example rationales, still leading to the correct answer. The goal is to evaluate how well large language models handle these types of noisy inputs when using chain-of-thought prompting.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_35_2.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of noisy questions and rationales used in the paper.  Each example includes three question-rationale-answer triplets followed by a test question. The noisy examples contain irrelevant or inaccurate reasoning steps (rationales) that are designed to mislead the language model. The test question, however, is similar in structure to the examples, allowing for an evaluation of the model's robustness to noisy rationales. The example illustrates a math problem involving base-9 arithmetic, yet the provided rationales mistakenly include calculations based on base-10, demonstrating the concept of noisy rationales.", "section": "1 Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_43_1.jpg", "caption": "Figure 2: Results of GPT-3.5 with 0-shot, 3-shot clean rationales, and 3-shot noisy rationales: Both inaccurate and irrelevant rationales degenerate performance significantly, while the proposed CD-CoT improves robustness against noisy rationales.", "description": "This figure shows the performance of GPT-3.5 language model on the NoRa dataset under different conditions.  The 0-shot results (no examples) show a baseline. The \"Clean\" bars show performance when the model is prompted with three examples with correct rationales.  \"Irrelevant\" and \"Inaccurate\" bars show performance when noisy rationales (irrelevant or inaccurate reasoning steps) are present in the examples.  The figure shows that both irrelevant and inaccurate rationales lead to lower accuracy.  The bars labeled \"Irrelevant (with CD-CoT)\" and \"Inaccurate (with CD-CoT)\" demonstrate the improvement in accuracy achieved by the proposed CD-CoT method, which is designed to improve robustness against noisy rationales.", "section": "1 Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_43_2.jpg", "caption": "Figure 2: Results of GPT-3.5 with 0-shot, 3-shot clean rationales, and 3-shot noisy rationales: Both inaccurate and irrelevant rationales degenerate performance significantly, while the proposed CD-CoT improves robustness against noisy rationales.", "description": "This figure shows the accuracy of GPT-3.5 on three different tasks from the NoRa dataset.  The accuracy is shown for three different conditions: 0-shot (no examples), 3-shot with clean rationales, and 3-shot with noisy rationales (both irrelevant and inaccurate).  The results show that the presence of noisy rationales significantly decreases the accuracy of GPT-3.5, while the proposed CD-CoT method improves the robustness of the model to noisy rationales.", "section": "1 Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_43_3.jpg", "caption": "Figure 2: Results of GPT-3.5 with 0-shot, 3-shot clean rationales, and 3-shot noisy rationales: Both inaccurate and irrelevant rationales degenerate performance significantly, while the proposed CD-CoT improves robustness against noisy rationales.", "description": "This figure displays the performance of GPT-3.5 under different prompting conditions.  The x-axis shows four different categories: 0-shot, clean rationales, irrelevant rationales and inaccurate rationales.  Each bar represents the average accuracy across multiple reasoning tasks. The results indicate a significant drop in accuracy when using noisy rationales compared to clean rationales.  The proposed CD-CoT method shows improved robustness to noisy rationales.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_61_1.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of noisy questions and rationales used in the paper's experiments.  The left side shows a standard prompt with clean questions and rationales while the right side shows a prompt with noisy rationales. Noisy rationales include irrelevant or factually incorrect information that is designed to mislead large language models (LLMs). The examples shown are addition problems in base 9; however, the rationales include unnecessary information from base 10 calculations that are designed to confuse the model.  This highlights the core research challenge of the paper; demonstrating the impact of noisy rationales on LLM performance.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_61_2.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of noisy questions and noisy rationales.  The left side displays examples with noisy questions containing irrelevant information about base-10 calculations, while the right side shows examples with noisy rationales.  Both types of noise are designed to mislead the model, while maintaining a correct final answer.  The aim is to test the robustness of large language models when faced with these types of noisy input.  Each example includes three prompting examples and one test question which requires base-9 calculation.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_61_3.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of noisy questions and noisy rationales used in the paper.  The examples are designed to highlight the challenge of robust reasoning in chain-of-thought prompting when the provided rationale contains irrelevant or inaccurate information. Each example consists of three question-rationale-answer triplets followed by a test question.  The test questions are all base-9 calculations, while the examples in the noisy questions and rationales include extraneous and misleading base-10 information, making it challenging for language models to correctly solve the test questions.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_62_1.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of both noisy questions and noisy rationales.  The top half demonstrates examples with noisy questions, where extra information about base-10 calculations is included, even though the task is to solve a problem in base-9. The bottom half shows examples with noisy rationales, where incorrect or irrelevant reasoning steps are provided within the solution.  In both scenarios, this extra information is meant to make it more difficult for a language model to determine the correct answer to the base-9 question.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_62_2.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of noisy questions and noisy rationales used in the paper's experiments.  The left side shows examples with noisy questions (containing irrelevant base-10 information, while the questions themselves are about base-9 calculations). The right side shows examples with noisy rationales (containing irrelevant or inaccurate reasoning steps). The purpose is to demonstrate the type of noisy data the language models are tested on.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_62_3.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of both noisy questions and noisy rationales.  The noisy questions contain irrelevant or inaccurate information that can mislead the model.  The noisy rationales also contain irrelevant or inaccurate reasoning steps in the chain of thought, which can also cause the model to give wrong answers.  The examples shown involve base-9 arithmetic, where the presence of base-10 information acts as noise. The figure highlights the problem of robust reasoning under noisy conditions, which is the core problem addressed in the paper.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_62_4.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of noisy questions and noisy rationales, which are the focus of the paper.  The top half displays examples where the input questions contain irrelevant base-10 information while the problem to be solved is actually in base-9. The bottom half shows examples where the rationales (reasoning steps) provided are noisy, including both irrelevant and inaccurate information, although they still lead to the correct answer.  This highlights the challenge the authors address: how to make language models robust to noisy rationales during chain-of-thought prompting.", "section": "Introduction"}, {"figure_path": "FbuODM02ra/figures/figures_62_5.jpg", "caption": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.", "description": "This figure shows examples of noisy questions and noisy rationales used in the paper's experiments.  It highlights the difference between inputs with clean questions/rationales and those with noisy ones, where irrelevant or incorrect information is added to the rationales.  This illustrates the core challenge the paper addresses: how well language models perform when the training examples (demonstrations) contain noisy rationales.", "section": "Introduction"}]