{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides a comprehensive technical report on GPT-4, a large language model, which is directly relevant to the current study."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the concept of in-context learning in large language models, a core concept relevant to the current research."}, {"fullname_first_author": "Takeshi Kojima", "paper_title": "Large language models are zero-shot reasoners", "publication_date": "2022-12-01", "reason": "This paper introduced the concept of zero-shot chain-of-thought prompting, a key technique directly related to the methods used in the paper."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduced the chain-of-thought prompting method, which is central to the focus of the current study."}, {"fullname_first_author": "Freda Shi", "paper_title": "Large language models can be easily distracted by irrelevant context", "publication_date": "2023-07-01", "reason": "This paper highlights the vulnerability of LLMs to irrelevant context, a problem directly addressed by the current research."}]}