[{"figure_path": "FbuODM02ra/tables/tables_3_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of different LLMs (GPT-3.5, and others) on the NoRa dataset.  It shows accuracy scores under three different conditions: clean rationales, irrelevant rationales, and inaccurate rationales for three difficulty levels (easy, medium, hard).  The results reveal how well each model performs in the presence of noisy rationales (irrelevant or inaccurate reasoning steps).  Base models and models with various robustness methods are compared for analysis. The bold numbers highlight the highest accuracies for each condition.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_3_2.jpg", "caption": "Table 2: Statistics of NoRa dataset.", "description": "This table presents the statistics of the NoRa dataset, categorized by task difficulty (Easy, Medium, Hard) and noise ratio (0.3, 0.5, 0.8). It shows the average number of total thoughts and noisy thoughts within the prompting rationales for each task (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense) and the number of questions in each category.", "section": "3 The NoRa Dataset"}, {"figure_path": "FbuODM02ra/tables/tables_4_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of different language models and robust methods on the NoRa dataset.  It compares the performance using 3-shot prompting examples with clean rationales, irrelevant rationales (noise), and inaccurate rationales (noise). The table highlights the best and second-best performance for each model and noise condition, and the base model's performance is shown in gray for comparison.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_5_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of several LLMs and reasoning methods on the NoRa dataset, broken down by task (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense) and noise type (clean, irrelevant, inaccurate).  The table shows the accuracy of each model under each condition (Base model, various robustness methods).  The best accuracy for each condition is bolded, and the second-best is underlined. The gray highlighting indicates baseline performance using the base model. This allows comparison of the impact of different noise types on model performance and the effectiveness of different robustness techniques.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_5_2.jpg", "caption": "Table 7: Performance (in accuracy%) on NoRa dataset under different few-shot shuffle configurations.", "description": "This table presents the performance of language models on the NoRa dataset under various few-shot shuffle configurations.  The configurations involve shuffling the questions (xi), rationales (Ti), or answers (yi) within the prompting examples. The table shows the accuracy (%) for each task (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense) under each shuffle configuration, along with the percentage change compared to the 'Few-shot (No Shuffle)' condition.  This allows for assessing the impact of shuffling different components of the prompting examples on model performance, revealing whether LLMs rely heavily on the exact mappings between questions, rationales, and answers.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_8_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of several LLMs and reasoning methods on the NoRa dataset.  The accuracy is evaluated under three different conditions: clean rationales, irrelevant rationales, and inaccurate rationales. For each condition, the accuracy is reported for different difficulty levels (easy, medium, hard) across three tasks: mathematical reasoning, symbolic reasoning and commonsense reasoning.  The best and second best results are highlighted for each task and condition.  The baseline performance of the base LLM (without additional methods) is also shown in gray.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_9_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy of several LLMs and reasoning methods on the NoRa dataset when using 3-shot prompting examples with clean, irrelevant, and inaccurate rationales.  The results show the performance of a baseline model and several methods aimed at improving robustness against noisy rationales, assessing their effectiveness across different types of noise and difficulty levels.  The bold numbers represent the best performance for each condition, and underlined numbers indicate the second-best.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_27_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of several LLMs on the NoRa dataset using different types of rationales (clean, irrelevant, and inaccurate) in 3-shot prompting experiments.  It compares the performance of a base LLM (GPT-3.5-turbo) with several other LLMs and methods designed to enhance robustness (ISC, SP, SM, SD, SC) against noisy rationales across three different tasks (Math, Symbolic, Commonsense). The accuracy is shown separately for easy, medium, and hard difficulty levels for each type of rationale, and the average accuracy across all difficulty levels is also provided. Bold numbers highlight the best performance among the compared methods.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_28_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy of various LLMs and reasoning methods on the NoRa dataset when using 3-shot prompting examples with different types of rationales (clean, irrelevant, and inaccurate). The results are categorized by task (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense), difficulty level (Easy, Medium, Hard), and method.  The best and second-best accuracy results for each setting are highlighted in bold and underlined, respectively, and the base model accuracy is highlighted in gray to facilitate comparison. The table allows for an assessment of the robustness of different LLMs and reasoning strategies to noisy rationales.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_29_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of various language models and reasoning methods on the NoRa dataset.  It shows the performance under three conditions: clean rationales, irrelevant rationales, and inaccurate rationales, across three difficulty levels (easy, medium, hard) for each of the three reasoning tasks (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense). The best-performing method for each condition is highlighted in bold, while the second-best is underlined. The results for the base model (without any additional method) are shown in gray for comparison.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_30_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of several LLMs and reasoning methods on the NoRa dataset. The accuracy is evaluated under three conditions: clean rationales, irrelevant rationales, and inaccurate rationales. The table shows the average accuracy across different difficulty levels (Easy, Medium, Hard) for each task type (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense). The best and second-best results for each condition are highlighted in bold and underlined, respectively. The baseline model results (Base) are shown in gray for comparison. This table helps to understand the robustness of LLMs and reasoning methods in dealing with noisy rationales.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_30_2.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy of different language models and reasoning methods on the NoRa dataset.  It shows the performance across three conditions: using clean rationales, irrelevant rationales, and inaccurate rationales.  The results are broken down by task (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense) and difficulty level (Easy, Medium, Hard).  The best accuracy for each condition is highlighted in bold. The table helps to assess the robustness of different LLMs and methods when presented with noisy rationales.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_31_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy of several LLMs and reasoning methods on the NoRa dataset, categorized by task (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense), method (Base, w/ ISC, w/ SP, w/ SM, w/ SD, w/ SC, w/SCO, w/BT, w/CC, w/ CD-CoT), and rationale type (clean, irrelevant, inaccurate).  The accuracy is shown for 'Easy', 'Medium', and 'Hard' difficulty levels for each task.  Bold numbers indicate the highest accuracy for a given task and rationale type, and underlined numbers show the second highest accuracy. The table highlights the significant drop in accuracy for LLMs when presented with irrelevant or inaccurate rationales compared to clean rationales.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_32_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the results of evaluating several language models and reasoning methods on the NoRa dataset.  It shows the accuracy of each method on three different types of rationales (clean, irrelevant, and inaccurate) across three difficulty levels (easy, medium, hard) for three different reasoning tasks (Math Base-9, Math Base-11, Symbolic Equal).  The gray rows highlight the baseline performance of the main LLM used in the study.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_33_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of several LLMs and reasoning methods on the NoRa dataset.  It compares the performance on three different types of rationales: clean, irrelevant, and inaccurate. The table helps to understand the impact of noisy rationales on the accuracy of different LLMs and the effectiveness of existing robust methods.  The results are shown for three difficulty levels (Easy, Medium, Hard) within each task type.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_41_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy of several LLMs and robust methods on the NoRa dataset, broken down by three types of rationales: clean, irrelevant, and inaccurate.  It shows the performance of the base model (GPT-3.5-turbo) and five different baseline methods (ISC, SP, SM, SD, SC) across different difficulty levels (Easy, Medium, Hard) for three reasoning tasks (Math Base-9, Math Base-11, Symbolic Equal). The best and second-best results for each condition are highlighted.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_44_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of various LLMs and reasoning methods on the NoRa dataset.  Different prompting example types are compared: clean, irrelevant, and inaccurate rationales.  The table shows the accuracy for each method across different difficulty levels (easy, medium, hard) for each task type (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense).  The best results for each task and difficulty are highlighted.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_45_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of different LLMs and robust methods on the NoRa dataset.  It shows the accuracy of each method under three conditions: clean rationales, irrelevant rationales, and inaccurate rationales.  The results are broken down by task (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, and Commonsense) and difficulty level (Easy, Medium, Hard).  The best performing method for each task and condition is highlighted in bold.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_45_2.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the results of evaluating several LLMs and robust methods on the NoRa dataset. It shows the accuracy of each model on three types of rationales (clean, irrelevant, and inaccurate) for three difficulty levels (easy, medium, hard) across three tasks. The bold numbers indicate the best performance for each scenario, and the underlined numbers indicate the second-best. The table highlights the significant performance drop of all models when using noisy rationales, demonstrating their vulnerability to noise.  The gray highlights show the baseline performance of the Base model.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_46_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy of different language models and reasoning methods on the NoRa dataset.  It shows the performance for each model under three conditions: clean rationales, irrelevant rationales, and inaccurate rationales.  The results are broken down by difficulty level (easy, medium, hard) and task type (math, symbolic, commonsense). The best results are highlighted in bold, and the second-best are underlined.  Base model results are shown in gray for comparison.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_47_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy of various LLMs and reasoning methods on the NoRa dataset when prompted with 3-shot examples. The results are categorized by task type (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense), rationale type (clean, irrelevant, inaccurate), and method (Base, w/ISC, w/SP, w/SM, w/SD, w/SC, w/SCO, w/BT, w/CC, w/CD-CoT). The boldface numbers indicate the best performance for each condition, while the underlined numbers indicate the second-best performance. The results for the Base model (without any additional methods) are highlighted in gray to facilitate comparison.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_47_2.jpg", "caption": "Table 35: Ablation study of different prompts in CD-CoT.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of using different prompts on the performance of the CD-CoT method. Three different prompts were tested: the original prompt, a shorter prompt, and a longer prompt. The results show that the performance of CD-CoT is only marginally influenced by the choice of prompt.  The table shows the NDA scores for both irrelevant and inaccurate medium-level noise.", "section": "F.3 The Superior Performance and Denoising Effectiveness"}, {"figure_path": "FbuODM02ra/tables/tables_49_1.jpg", "caption": "Table 8: Performance of denoising methods that require additional information for supervision.", "description": "This table presents the accuracy results of several denoising methods on the NoRa dataset.  It compares the performance of various methods across different reasoning tasks (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense) and difficulty levels (Easy, Medium, Hard).  Each method's performance is evaluated under three conditions: (1) clean rationales, (2) irrelevant rationales, and (3) inaccurate rationales. The table also shows the type of additional information required by each method, such as ground truth, noise position, or a clean demonstration. The aim is to compare the effectiveness of these methods in handling noisy rationales and highlight the advantages and disadvantages of each approach in terms of accuracy and the need for additional information.", "section": "Empirical Study"}, {"figure_path": "FbuODM02ra/tables/tables_51_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy of different language models and reasoning methods on the NoRa dataset.  It shows the accuracy for three types of rationales (clean, irrelevant, and inaccurate) across three difficulty levels (easy, medium, hard) for two mathematical tasks (Base-9, Base-11), and two symbolic tasks (Equal, Longer), and a commonsense task.  The baseline model's performance with clean rationales is presented for comparison. Bold numbers highlight the best performance among all methods for that task and rationale type, while underlined numbers show the second-best performance.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_51_2.jpg", "caption": "Table 7: Performance (in accuracy%) on NoRa dataset under different few-shot shuffle configurations.", "description": "This table presents the accuracy results of different language models on the NoRa dataset under three different few-shot shuffle configurations: shuffling questions, shuffling rationales, and shuffling answers.  The purpose is to investigate whether LLMs heavily rely on the exact mapping between questions, rationales, and answers in few-shot prompting scenarios. It is observed that shuffling the mappings degrades the reasoning performance but still performs better than without prompting. This suggests that LLMs may not rely heavily on the exact mapping but learn abstract information from demonstrations.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_52_1.jpg", "caption": "Table 41: Results partition of (0-shot, 1-shot, 3-shot).", "description": "This table presents the breakdown of results from experiments comparing 0-shot, 1-shot, and 3-shot prompting scenarios, categorizing outcomes based on whether the model answered correctly (C) or wrongly (W) at each stage.  It analyzes the transitions in model accuracy across different numbers of noisy examples.", "section": "F.6 The Number of Noisy Examples"}, {"figure_path": "FbuODM02ra/tables/tables_54_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of several LLMs and reasoning methods on the NoRa dataset.  Three different types of rationales are used: clean, irrelevant, and inaccurate. The accuracy for each model and type of rationale is shown for three difficulty levels (Easy, Medium, Hard).  The Base model's accuracy for clean rationales is highlighted in gray to provide a clear benchmark for comparison. The best and second-best results are indicated in bold and underlined, respectively.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_55_1.jpg", "caption": "Table 8: Performance of denoising methods that require additional information for supervision.", "description": "This table presents the accuracy of several denoising methods across different reasoning tasks and noise levels on the NoRa dataset. The methods include self-correction with oracle feedback (SCO), backtracking (BT), contrastive chain-of-thought (CC), and the proposed contrastive denoising with noisy chain-of-thought (CD-CoT).  The table shows the performance (accuracy) for each method on tasks with clean rationales, irrelevant noise, and inaccurate noise. The \"Additional Information\" column indicates the type of extra information each method needs.  It highlights the relative performance improvements of the proposed CD-CoT method compared to other methods.", "section": "5.2 Empirical Study"}, {"figure_path": "FbuODM02ra/tables/tables_56_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of various LLMs and robust methods on the NoRa dataset. It compares the performance of these models when using clean rationales, irrelevant rationales, and inaccurate rationales. The results are broken down by task (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense) and difficulty level (Easy, Medium, Hard). The table highlights the best and second-best performing models for each condition and indicates the base model's performance for comparison.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_56_2.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of different language models and reasoning methods on the NoRa dataset.  It shows the performance of each model when prompted with clean rationales, irrelevant rationales, and inaccurate rationales.  The table highlights the best-performing method for each condition and uses boldface and underlines to indicate the top two performers. The gray highlighting indicates baseline performance.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_58_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the results of evaluating several language models on the NoRa dataset, specifically focusing on their ability to perform robust reasoning when presented with noisy rationales (clean, irrelevant, inaccurate). The accuracy of each model is reported for three difficulty levels (Easy, Medium, Hard) across three reasoning tasks.  The use of bold and underlined font helps highlight the best and second-best performing models.  The gray highlighting draws attention to the base model's performance for comparison.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_59_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of different language models and reasoning methods on the NoRa dataset, categorized by task (Math-Base9, Math-Base11, Symbolic-Equal, Symbolic-Longer, Commonsense), method used (Base, w/ ISC, w/ SP, w/ SM, w/ SD, w/ SC), and type of rationale (clean, irrelevant, inaccurate).  The accuracy is given as a percentage for each task difficulty level (Easy, Medium, Hard).  The best and second-best results are highlighted, and the Base model results are shaded gray for easy comparison.", "section": "Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_60_1.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy results of several LLMs and robust methods on the NoRa dataset, categorized by task (Math-Base9, Math-Base11, Symbolic-Equal, Symbolic-Longer, Commonsense), method, and type of rationale (clean, irrelevant, inaccurate).  The accuracy is shown for easy, medium, and hard difficulty levels for each task and type of rationale, providing a comprehensive evaluation of LLM robustness against noisy rationales.  Boldface numbers indicate the best performance for each category, while underlined numbers show the second-best.", "section": "4 Evaluating Language Models on NoRa dataset"}, {"figure_path": "FbuODM02ra/tables/tables_60_2.jpg", "caption": "Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.", "description": "This table presents the accuracy of different language models and reasoning methods on the NoRa dataset when using 3-shot prompting examples with different types of rationales: clean, irrelevant, and inaccurate. The results are categorized by task (Math Base-9, Math Base-11, Symbolic Equal, Symbolic Longer, Commonsense) and difficulty level (Easy, Medium, Hard).  The table highlights the best performing model for each scenario and indicates the second-best results.  It also shows a comparison to a base model's performance. The gray highlights indicate the base model's results.", "section": "4 Evaluating Language Models on NoRa dataset"}]