[{"type": "text", "text": "Conformal Prediction for Class-wise Coverage via Augmented Label Rank Calibration ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuanjie Shi Subhankar Ghosh Taha Belkhouja Washington State University Washington State University Washington State University ", "page_idx": 0}, {"type": "text", "text": "Janardhan Rao Doppa Yan Yan Washington State University Washington State University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Conformal prediction (CP) is an emerging uncertainty quantification framework that allows us to construct a prediction set to cover the true label with a pre-specified marginal or conditional probability. Although the valid coverage guarantee has been extensively studied for classification problems, CP often produces large prediction sets which may not be practically useful. This issue is exacerbated for the setting of class-conditional coverage on classification tasks with many and/or imbalanced classes. This paper proposes the Rank Calibrated Class-conditional CP (RC3P) algorithm to reduce the prediction set sizes to achieve class-conditional coverage, where the valid coverage holds for each class. In contrast to the standard class-conditional CP (CCP) method that uniformly thresholds the class-wise conformity score for each class, the augmented label rank calibration step allows RC3P to selectively iterate this class-wise thresholding subroutine only for a subset of classes whose class-wise top- $k$ error is small. We prove that agnostic to the classifier and data distribution, RC3P achieves class-wise coverage. We also show that RC3P reduces the size of prediction sets compared to the CCP method. Comprehensive experiments on multiple real-world datasets demonstrate that RC3P achieves class-wise coverage and $\\bar{26}.25\\%\\downarrow$ reduction in prediction set sizes on average. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Safe deployment of machine learning (ML) models in high stakes applications such as medical diagnosis requires theoretically-sound uncertainty estimates. Conformal prediction (CP) [60] is an emerging uncertainty quantification framework that constructs a prediction set of candidate output values such that the true output is present with a pre-specified level (e.g., $\\ge90\\%$ ) of the marginal or conditional probability [65, 19]. ", "page_idx": 0}, {"type": "text", "text": "A promising property of CP is the model-agnostic and distribution-free coverage validity under certain notions [20]. For example, marginal coverage is the commonly studied validity notion [47, 1, 65], while conditional coverage is a stronger notion. There is a general taxonomy to group data (i.e., input-output pairs) into categories and to study the valid coverage for each group (i.e., the group-wise validity) [61, 60]. This paper focuses on the specific notion of class-conditional coverage that guarantees coverage for each class individually, which is important for classification tasks with many and/or imbalanced classes (e.g., medical applications) [39, 56, 38]. ", "page_idx": 0}, {"type": "text", "text": "In addition to the coverage validity, predictive efficiency is another important criterion for CP [20, 59], which refers to the size of the prediction sets. Both coverage validity and predictive efficiency are used together to measure the performance of CP methods [1, 45, 47, 15, 22, 18]. Since the two measures are competing [1], our goal is to guarantee the coverage validity with high predictive efficiency, i.e., small prediction sets [20, 47, 18]. Some studies improved the predictive efficiency under the marginal coverage setting using new conformity score function [1] and new calibration procedures [19, 18, 26, 21]. However, it is not known if these methods will benefit the predictive efficiency for the class-conditional coverage setting. A very recent work [15] proposed the cluster CP method to achieve approximate class-conditional coverage. It empirically improves predictive efficiency over the baseline class-wise CP method (i.e., each class is one cluster) [58], but the approximation guarantee for class-wise coverage is model-dependent (i.e., requires certain assumptions on the model). The main question of this paper is: how can we develop a model-agnostic CP algorithm that guarantees the class-wise coverage with improved predictive efficiency (i.e., small prediction sets)? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To answer this question, we propose a novel approach referred to as Rank Calibrated Class-conditional $C P\\left(R C3P\\right)$ that guarantees the class-wise coverage with small expected prediction sets. The classconditional coverage validity of RC3P is agnostic to the data distribution and the underlying ML model, while the improved predictive efficiency depends on very mild conditions of the given trained classifier. The main ingredient behind the RC3P method is the label rank calibration strategy augmented with the standard conformal score calibration from the class-wise CP (CCP) [58, 2]. ", "page_idx": 1}, {"type": "text", "text": "The CCP method finds the class-wise quantiles of non-conformity scores on calibration data. To produce the prediction set for a new test input $X_{\\mathrm{test}}$ , it pairs $X_{\\mathrm{test}}$ with each candidate class label $y$ and includes the label $y$ if the non-conformity score of the pair $(X_{\\mathrm{test}},y)$ is less than or equal to the corresponding class-wise quantile associated with $y$ . Thus, CCP constructs the prediction set by uniformly iterating over all candidate labels. In contrast, the label rank calibration allows RC3P to selectively iterate this class-wise thresholding subroutine only if the label $y$ is ranked by the classifier $f(X_{\\mathrm{test}})$ (e.g., $f(\\cdot)$ denotes the softmax prediction) in the top $k_{y}$ candidates, where the value of $k_{y}$ is calibrated for each label $y$ individually according to the class-wise top- $k_{y}$ error. In other words, given $X_{\\mathrm{test}}$ , RC3P enables standard class-wise conformal thresholding for the sufficiently certain class labels only (as opposed to all labels). Our theory shows that the class-wise coverage provided by RC3P is agnostic to the data distribution and the underlying ML model. Moreover, under a very mild condition, RC3P guarantees improved predictive efficiency over the baseline CCP method. ", "page_idx": 1}, {"type": "text", "text": "Contributions. The main contributions of this paper are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We design a novel algorithm RC3P that augments the label rank calibration strategy to the standard conformal score calibration step. To produce prediction sets for new inputs, it selectively performs class-wise conformal thresholding only on a subset of classes based on their corresponding calibrated label ranks.   \n\u2022 We develop theoretical analysis to show that RC3P guarantees class-wise coverage, which is agnostic to the data distribution and trained classifier. Moreover, it provably produces smaller average prediction sets over the baseline CCP method [58].   \n\u2022 We perform extensive experiments on multiple imbalanced classification datasets and show that RC3P achieves the class-wise coverage with significantly improved predictive efficiency over the existing class-conditional CP baselines $(26.25\\%$ reduction in the prediction size on average on all four datasets or $35\\%$ reduction excluding CIFAR-10). The code is available at https://github.com/YuanjieSh/RC3P. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Precise uncertainty quantification of machine learning based predictions is necessary in high-stakes decision-making applications. It is especially challenging for imbalanced classification tasks. Although many imbalanced classification learning algorithms [10, 25] are proposed, e.g., re-sampling [11, 42, 33, 54, 63] and re-weighting [28, 40], they do not provide uncertainty quantification with rigorous guarantees over predictions for each class. ", "page_idx": 1}, {"type": "text", "text": "Conformal prediction [62, 60] is a model-agnostic and distribution-free framework for uncertainty quantification by producing prediction sets that cover the true output with a pre-specified probability, which means CP could provide valid coverage guarantee with any underlying model and data distribution [32, 52, 16]. Many CP algorithms are proposed for regression [35, 46, 23, 17], classification [45, 1, 64, 37], structured prediction [6, 3, 13, 30], online learning [24, 7], and covariate shift [31, 53, 5] settings. Coverage validity and predictive efficiency are two common and competing desiderata for CP methods [1]. Thus, small prediction sets are favorable whenever the coverage validity is guaranteed [20, 47, 18], e.g., human and machine learning collaborative systems [39, 56, 38]. Recent work1 improved the predictive efficiency for marginal coverage by designing new conformity score [1] and calibration procedures [19, 18, 26, 21]. These methods can be combined with class-conditional CP methods including RC3P as we demonstrate in our experiments, but the effect on predictive efficiency is not clear. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In general, the methods designed for a specific coverage validity notion are not necessarily compatible with another notion of coverage, such as object-conditional coverage [58], class-conditional coverage [58], local coverage [36] which are introduced and studied in the prior CP literature [61, 60, 20, 15, 9]. The standard class-conditional CP method in [58, 49] guarantees the class-wise coverage, but does not particularly aim to reduce the size of prediction sets. The cluster CP method [15] which performs CP over clusters of labels achieves a cluster-conditional coverage that approximates the class-conditional guarantee, but requires some assumptions on the underlying clustering model. ", "page_idx": 2}, {"type": "text", "text": "Our goal is to develop a provable class-conditional CP algorithm with small prediction sets to guarantee the class-wise coverage that is agnostic to the underlying model. ", "page_idx": 2}, {"type": "text", "text": "3 Notations, Background, and Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. Suppose $(X,Y)$ is a data sample where $X\\ \\in\\ {\\mathcal{X}}\\end{array}$ is an input from the input space $\\mathcal{X}$ , and $Y\\;\\in\\;\\mathcal{Y}\\;=\\;\\{1,2,\\cdots\\,,K\\}$ is the ground-truth label with $K$ candidate classes. Assume $(X,Y)$ is randomly drawn from an underlying distribution $\\mathcal{P}$ defined on $\\mathcal X\\times\\mathcal X$ , where we denote $p_{y}=\\mathbb{P}_{X Y}[Y=y]$ . Let $f:\\mathcal{X}\\to\\Delta_{+}^{K}$ denote a soft classifier (e.g., a soft-max classifier) that produces prediction scores for all candidate classes on any given input $X$ , where $\\Delta_{+}^{K}$ denote the $K$ -dimensional probability simplex and $f(X)_{y}$ denotes the predicted confidence for class $y$ . We define the class-wise top- $k$ error for class $y$ from the trained classifier $f$ as $\\epsilon_{y}^{k}\\,=\\,\\mathbb{P}\\{r_{f}(X,Y)\\,>\\,k|Y\\,=\\,y\\}$ , where $\\begin{array}{r}{r_{f}(X,Y)=\\sum_{l=1}^{K}\\mathbb{1}[f(X)_{l}\\geq f(X)_{Y}]}\\end{array}$ er aerteu rpnrso vtihdee rda nwikt ho fa $Y$ aipnriendgi csteet yf $f(X)$ niinn ga  thdee sccleasnsdiifniegr $\\mathbb{I}[\\cdot]$ $\\mathcal{D}_{\\mathrm{tr}}$ $f$ , and a calibration set $D_{\\mathrm{cal}}=\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ for CP. Let $\\mathcal{T}_{y}=\\{i:Y_{i}=y$ , for all $({\\bar{X_{i}}},Y_{i})\\in{\\mathcal{D}}_{\\mathrm{cal}}\\}$ and $n_{y}=|{\\mathcal{T}}_{y}|$ denote the number of calibration examples for class $y$ . ", "page_idx": 2}, {"type": "text", "text": "Problem Setup of CP. Let $V:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ denote a non-conformity scoring function to measure how different a new example is from old ones [60]. It is employed to compare a given testing sample $(X_{\\mathrm{test}},Y_{\\mathrm{test}})$ with a set of calibration data $\\mathcal{D}_{\\mathrm{cal}}$ : if the non-conformity score is large, then $(\\bar{X_{\\mathrm{test}}},\\bar{Y_{\\mathrm{test}}})$ conforms less to calibration samples. Prior work has considered the design of good non-conformity scoring functions, e.g., [2, 50, 47]. In this paper, we focus on the scoring functions of Adaptive Prediction Sets (APS) proposed in [47] and Regularized APS (RAPS) proposed in [1] for classification based on the ordered probabilities of $f$ and true label rank $r_{f}(X,Y)$ . For the simplicity of notation, we denote the non-conformity score of the $i$ -th calibration example as $V_{i}=V(\\bar{X_{i}},Y_{i})$ . ", "page_idx": 2}, {"type": "text", "text": "Given a input $X$ , we sort the predicted probability for all classes $\\{1,\\cdot\\cdot\\cdot,K\\}$ of the classifier $f$ such that $1\\geq{\\bar{f}}(X)_{(1)}\\geq\\cdot\\cdot\\cdot\\geq{\\bar{f}}(X)_{(K)}\\geq0$ are ordered statistics, where $f(X)_{(k)}$ denotes the $k$ -th largest prediction. The APS [47] score for a sample $(X,Y)$ is computed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nV(X,Y)=\\sum_{l=1}^{r_{f}(X,Y)-1}f(X)_{(l)}+U\\cdot f(X)_{(r_{f}(X,Y))},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $U\\in[0,1]$ is a uniform random variable to break ties. We also consider its regularized variant RAPS [1], which additionally includes a rank-based regularization $\\lambda(r_{f}(X,Y)-k_{r e g}^{\\ \\ \\tilde{}})^{+}$ to the above equation, where $(\\cdot)^{+}=\\operatorname*{max}\\{0,\\cdot\\}$ denotes the hinge loss, $\\lambda$ and $k_{r e g}$ are two hyper-parameters. ", "page_idx": 2}, {"type": "text", "text": "For a target coverage $1-\\alpha$ , we find the corresponding empirical quantile on calibration data $\\mathcal{D}_{\\mathrm{cal}}$ defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{Q}_{1-\\alpha}=\\operatorname*{min}\\Bigl\\{t:\\sum_{i=1}^{n}\\frac{1}{n}\\cdot\\mathbb{1}[V_{i}\\leq t]\\geq1-\\alpha\\Bigr\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which can be determined by finding the $\\lceil(1\\!-\\!\\alpha)(1\\!+\\!n)\\rceil$ -th smallest value of $\\{V_{i}\\}_{i=1}^{n}$ . The prediction set of a testing input $X_{\\mathrm{test}}$ can be constructed by thresholding with $\\widehat{Q}_{1-\\alpha}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{C}}_{1-\\alpha}(X_{\\mathrm{test}})=\\{y\\in\\mathcal{y}:V(X_{\\mathrm{test}},y)\\leq\\widehat{Q}_{1-\\alpha}\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Therefore, $\\widehat{\\mathcal{C}}_{1-\\alpha}$ gives a marginal coverage guarantee [47, 1]: $\\mathbb{P}_{(X,Y)\\sim{\\mathcal{P}}}\\{Y\\in{\\widehat{\\mathcal{C}}}_{1-\\alpha}(X)\\}\\geq1-\\alpha$ . To achieve  the class-conditional coverage, standard CCP [58] uniformly iterates the class-wise thresholding subroutine with the class-wise quantiles $\\{\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)\\}_{y\\in\\mathcal{y}}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathcal{C}}_{1-\\alpha}^{\\mathrm{CCP}}(X_{\\mathrm{test}})=\\{y\\in\\mathcal{Y}:V(X_{\\mathrm{test}},y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)\\},}\\\\ &{\\qquad\\quad\\mathrm{~where~}\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)=\\operatorname*{min}\\Bigl\\{t:\\displaystyle\\sum_{i\\in\\mathbb{Z}_{y}}\\frac{1}{n_{y}}\\cdot\\mathbb{1}[V_{i}\\leq t]\\geq1-\\alpha\\Bigr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Specifically, CCP pairs $X_{\\mathrm{test}}$ with each candidate class label $y$ , and includes $y$ in the prediction set $\\widehat{\\mathcal{C}}_{1-\\alpha}^{\\mathrm{CCP}}(X_{\\mathrm{test}})$ if $V(X_{\\mathrm{test}},y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)$ holds. After going through all candidate class labels $y\\in\\mathcal{V}$ , it achieves the class-wise coverage for any $y\\in\\mathcal{V}$ [58, 2]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(X,Y)\\sim{\\mathcal{P}}}\\{Y\\in{\\widehat{\\mathcal{C}}}_{1-\\alpha}^{\\mathrm{CCP}}(X)|Y=y\\}\\geq1-\\alpha.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "CCP produces large prediction sets which are not useful in practice. Therefore, our goal is to develop a provable CP method that provides class-conditional coverage and constructs smaller prediction sets than those from CCP. We summarize all the notations in Table 3 of Appendix. ", "page_idx": 3}, {"type": "text", "text": "4 Rank Calibrated Class-Conditional CP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first explain the proposed Rank Calibrated Class-conditional Conformal Prediction $(R C3P)$ algorithm and present its model-agnostic coverage guarantee. Next, we provide the theoretical analysis for the provable improvement of predictive efficiency of RC3P over the CCP method. ", "page_idx": 3}, {"type": "text", "text": "4.1 Algorithm and Model-Agnostic Coverage Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We start with the motivating discussion about the potential drawback of the standard CCP method in terms of predictive efficiency. Equation (1) shows that, for a given test input $X_{\\mathrm{test}}$ , CCP likely contains some uncertain labels due to the uniform iteration over each class label $y\\in\\mathcal{V}$ to check if $y$ should be included into the prediction set or not. For example, given a class label $y$ and two test samples $X_{1},X_{2}$ , suppose their APS scores are $V(X_{1},y)=0.9$ , $V(X_{2},y)=0.8$ , with ranks $r_{f}(X_{1},y)=1,r_{f}(X_{2},y)=5$ . Furthermore, if $\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)=0.85$ , then by (1) for CCP, we know that $y\\not\\in{\\widehat{C}}_{1-\\alpha}^{\\mathrm{CCP}}(X_{1})$ and $y\\in{\\widehat{C}}_{1-\\alpha}^{\\mathrm{CCP}}(X_{2})$ , even though $f(X_{1})$ ranks $y$ at the #1 class label for $X_{1}$ with a very high confidence $f(X_{1})_{y}=0.9$ and CCP can still achieve the valid class-conditional coverage. We argue that, the principle of CCP to scans all $y\\in\\mathcal{V}$ uniformly can easily result in large prediction sets, which is detrimental to the effectiveness of human-ML collaborative systems [4, 51]. ", "page_idx": 3}, {"type": "text", "text": "Consequently, to improve the predictive efficiency of CCP (i.e., reduce prediction set sizes), it is reasonable to include label rank information in the calibration procedure to adjust the distribution of non-conformity scores for predictive efficiency. As mentioned in the previous sections, better scoring functions have been proposed to improve the predictive efficiency for marginal coverage, e.g., RAPS. However, directly applying RAPS for class-wise coverage presents challenges: 1) tuning its hyper-parameters for each class requires extra computational overhead, and 2) fixing its hyper-parameters for all classes overlooks the difference between distributions of different classes. Moreover, for the approximate class-conditional coverage achieved by cluster CP [15], it still requires some assumptions on the underlying model (i.e., it is not fully model-agnostic). ", "page_idx": 3}, {"type": "text", "text": "Therefore, the key idea of our proposed RC3P algorithm (outlined in Algorithm 1) is to refine the class-wise calibration procedure using a label rank calibration strategy augmented to the standard conformal score calibration, to enable adaptivity to various classes. Specifically, in contrast to CCP, RC3P selectively activates the class-wise thresholding subroutine in (1) according to their class-wise top- $k$ error $\\epsilon_{y}^{k}$ for class $y$ . RC3P produces the prediction set for a given test input $X_{\\mathrm{test}}$ with two calibration schemes (one for conformal score and another for label rank) as shown below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{C}}_{1-\\alpha}^{\\mathrm{RC3P}}(X_{\\mathrm{test}})=\\big\\{y\\in\\mathcal{Y}:\\underbrace{V(X_{\\mathrm{test}},y)\\leq\\widehat{Q}_{1-\\widehat{\\alpha}_{y}}^{\\mathrm{class}}(y)}_{\\mathrm{conformal~score~calibration}},\\ \\ r_{f}(X_{\\mathrm{test}},y)\\leq\\widehat{k}(y)\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\widehat{Q}_{1-\\widehat{\\alpha}_{y}}^{\\mathrm{class}}(y)$ and $\\widehat{k}(y)$ are score and label rank threshold for class $y$ , respectively. In particular, $\\widehat{k}(y)$ controls the class-wise uncertainty adaptive to each class $y$ based on the top- $k$ error \u03f5yk (y)of the ", "page_idx": 3}, {"type": "text", "text": "1: Input: Mis-coverage rate $\\alpha\\in(0,1)$ ., top- $k$ errors $\\epsilon_{u}^{k}$ for all classes and ranks $y,k\\in\\{1,\\cdots,K\\}$   \n2: Randomly split data into train $\\mathcal{D}_{\\mathrm{tr}}$ and calibration $\\mathring{D}_{\\mathrm{cal}}$ and train the classifier $f$ on $\\mathcal{D}_{\\mathrm{tr}}$   \n3: for $y\\in\\{1,\\cdots\\,,K\\}$ do   \n4: Compute $\\{V_{i}\\}_{i=1}^{n_{y}}$ for all $(X_{i},Y_{i})\\in\\mathcal{D}_{\\mathrm{cal}}$ such that $Y_{i}=y$   \n5: Configure calibrated label rank $\\widehat{k}(y)$ and nominal error $\\widehat{\\alpha}_{y}$ :   \n6: Option I (model-agnostic coverage):   \n$\\widehat{k}(y)\\in\\{k:\\epsilon_{y}^{k}<\\alpha\\},\\;\\;0\\leq\\widehat{\\alpha}_{y}\\leq\\alpha-\\epsilon_{y}^{\\widehat{k}(y)},$ , as per Eq (4)   \n7: Option II (model-agnostic covera ge $^+$ improved predictive efficiency):   \n$\\widehat{k}(y)=\\operatorname*{min}\\{k:\\epsilon_{y}^{k}<\\alpha\\},\\;\\;\\widehat{\\alpha}_{y}=\\alpha-\\epsilon_{y}^{\\widehat{k}(y)}$ , as per Eq (7)   \n8: $\\widehat{Q}_{1-\\widehat{\\alpha}_{y}}^{\\mathrm{class}}(y)\\gets\\lceil(1-\\widehat{\\alpha}_{y})(1+n_{y})\\rceil$ -t h smallest value in $\\{V_{i}\\}_{i=1}^{n_{y}}$ according to Eq (1)   \n9: end for   \n10: Construct $\\widehat{\\mathcal{C}}_{1-\\alpha}^{\\mathrm{RC3P}}(X_{\\mathrm{test}})$ with $\\widehat{Q}_{1-\\widehat{\\alpha}_{y}}^{\\mathrm{class}}(y)$ and $\\widehat{k}(y)$ for a test input $X_{\\mathrm{test}}$ using Eq (3) ", "page_idx": 4}, {"type": "text", "text": "classifier. By determining $\\widehat{k}(y)$ , the top $k$ predicted class labels of $f(X_{\\mathrm{test}})$ will more likely cover the true label $Y_{\\mathrm{test}}$ , making the augmented label rank calibration filter out the class labels $y$ that have a high rank (larger $r_{f}(X,y))$ . As a result, given all test input and label pairs $\\{(X_{\\mathrm{test}},y)\\}_{y\\in\\mathcal{y}}$ , RC3P performs score thresholding using class-wise quantiles only on a subset of reliable test pairs. ", "page_idx": 4}, {"type": "text", "text": "Determining $\\widehat{k}(y)$ and $\\widehat{\\alpha}_{y}$ for model-agnostic valid coverage. For class $y$ , intuitively, we would like a value for $\\widehat{k}(y)$ such that the corresponding top- $\\widehat{k}(y)$ error is smaller than $\\alpha$ , so that it is possible to guarantee valid coverage (recall ${\\mathbb{P}}\\{A,B\\}={\\mathbb{P}}\\{A\\}\\cdot{\\mathbb{P}}\\{B|A\\})$ . Since a larger $\\widehat{k}(y)$ gives a smaller $\\hat{\\epsilon}_{y}^{\\hat{k}(y)}$ untill $\\epsilon_{y}^{K}=0$ , it is guaranteed to find a value for $\\widehat{k}(y)$ , in which the corresponding $\\epsilon_{y}^{\\widehat{k}(y)}<\\alpha$ . As a result, given all test input and label pairs $\\{(X_{\\mathrm{test}},y)\\}_{y\\in\\mathcal{y}}$ , RC3P performs score thresholding using class-wise quantiles only on a subset of reliable test pairs and fliters out the class labels $y$ that have a high rank (larger $r_{f}(X,y))$ . The following result formally shows the principle to configure $\\widehat{k}(y)$ and $\\widehat{\\alpha}_{y}$ to guarantee the class-wise coverage that is agnostic to the underlying model. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. (Class-conditional coverage of $R C3P$ ) Suppose that selecting $\\widehat{k}(y)$ values result in the class-wise top- $k$ error $\\hat{\\epsilon}_{y}^{\\hat{k}(y)}$ for each class $y\\in\\mathcal{V}$ . For a target class-conditional coverage $1-\\alpha,\\,i f$ we set $\\widehat{\\alpha}_{y}$ and $\\widehat{k}(y)$ in RC3P (3) in the following ranges: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{k}(y)\\in\\{k:\\epsilon_{y}^{k}<\\alpha\\},\\quad0\\leq\\widehat{\\alpha}_{y}\\leq\\alpha-\\epsilon_{y}^{\\widehat{k}(y)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then RC3P can achieve the class-conditional coverage for every $y\\in\\mathcal{V}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(X,Y)\\sim{\\mathcal{P}}}\\{Y\\in{\\widehat{\\mathcal{C}}}_{1-\\alpha}^{R C3P}(X)|Y=y\\}\\geq1-\\alpha.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2 Analysis of Predictive Efficiency for RC3P ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We further analyze the predictive efficiency of RC3P: under what conditions RC3P can produce a smaller expected prediction set size compared to CCP, when both achieve the same $(1-\\alpha)$ -classconditional coverage. We investigate how to choose the value of $\\widehat{\\alpha}_{y}$ and $\\widehat{k}(y)$ from the feasible ranges in (4) to achieve the best predictive efficiency using RC3P. ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.2. (Trade-off condition for improved predictive efficiency of $R C3P_{.}$ ) Suppose $\\widehat{\\alpha}_{y}$ and $\\widehat{k}(y)$ satisfy (4) in Theorem 4.1. If the following inequality holds for any $y\\in\\mathcal{V}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}_{X_{t e s t}}\\big[V(X_{t e s t},y)\\leq\\widehat{Q}_{1-\\widehat{\\alpha}}^{c l a s s}(y),\\;r_{f}(X_{t e s t},y)\\leq\\widehat{k}(y)\\big]\\leq\\mathbb{P}_{X_{t e s t}}\\big[V(X_{t e s t},y)\\leq\\widehat{Q}_{1-\\alpha}^{c l a s s}(y)\\big],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then RC3P produces smaller expected prediction sets than CCP, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X_{t e s t}}[|\\widehat{\\mathcal{C}}_{1-\\widehat{\\alpha}}^{R C3P}(X_{t e s t})|]\\leq\\mathbb{E}_{X_{t e s t}}[|\\widehat{\\mathcal{C}}_{1-\\alpha}^{C C P}(X_{t e s t})|].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark. The above result demonstrates that when both RC3P and CCP achieve the target $1-\\alpha$ class-conditional coverage, under the condition of (5), RC3P produces smaller prediction sets than CCP. In fact, this condition implies that the combined (conformity score and label rank) calibration of RC3P tends to include less labels with high rank or low confidence from the classifier. In contrast, the CCP method tends to include relatively more uncertain labels into the prediction set, where their ranks are high and the confidence of the classifier is low. Now we can interpret the condition (5) by defining a condition number, termed as $\\sigma_{y}$ : ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sigma_{y}=\\frac{\\mathbb{P}_{X_{\\mathrm{test}}}\\Big[V(X_{\\mathrm{test}},y)\\leq\\widehat{Q}_{1-\\hat{\\alpha}}^{\\mathrm{class}}(y),\\;r_{f}(X_{\\mathrm{test}},y)\\leq\\widehat{k}(y)\\Big]}{\\mathbb{P}_{X_{\\mathrm{test}}}\\Big[V(X_{\\mathrm{test}},y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)\\Big]}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In other words, if we can verify that $\\sigma_{y}\\le1$ for all $y$ , then RC3P can improve the predictive efficiency over CCP. Furthermore, if $\\sigma_{y}$ is fairly small, then the efficiency improvement can be even more significant. To verify this condition, our comprehensive experiments (Section 5.2, Figure 3) show that $\\sigma_{y}$ values are much smaller than 1 on real-world data. These results demonstrate the practical utility of our theoretical analysis to produce small prediction sets using RC3P. Note that the reduction in prediction set size of RC3P over CCP is proportional to how small the $\\sigma_{y}$ values are. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3. (Conditions of improved predictive efficiency for RC3P) Define $D=\\mathbb{P}[r_{f}(X,y)\\leq$ $\\widehat{k}(y)|Y\\neq y]$ , and $\\begin{array}{r}{\\bar{r}_{f}(X,y)=\\lfloor\\frac{r_{f}(X,y)+1}{2}\\rfloor}\\end{array}$ . Denote $B=\\mathbb{P}[f(X)_{(\\bar{r}_{f}(X,y))}\\leq\\widehat{Q}_{1-\\alpha}^{c l a s s}(y)|Y\\neq y]\\;i f V$ is APS, or $\\begin{array}{r}{B=\\mathbb{P}[f(X)_{(\\bar{r}_{f}(X,y))}+\\lambda\\leq\\widehat{Q}_{1-\\alpha}^{c l a s s}(y)|Y\\neq y]\\,i f V\\,i s\\,R A P S.\\,I f B-D\\geq\\frac{p_{y}}{1-p_{y}}(\\alpha-\\epsilon_{y}^{\\widehat{k}(y)}),}\\end{array}$ then $\\sigma_{y}\\le1$ . ", "page_idx": 5}, {"type": "text", "text": "Remark. The above result further analyzes when the condition in Eq (5) of Lemma 4.2 (or equivalently, $\\sigma_{y}\\leq1\\,$ ) holds to guarantee the improved predictive efficiency. Specifically, the condition $\\begin{array}{r}{B-D\\geq\\frac{p_{y}}{1-p_{y}}(\\alpha-\\epsilon_{y}^{\\widehat{k}(y)})}\\end{array}$ of Theorem 4.3 can be realized in two ways: (i) making LHS $B-D$ as large as possible; (ii) making the RHS $\\begin{array}{r}{\\frac{p_{y}}{1-p_{y}}(\\alpha-\\epsilon_{y}^{\\widehat{k}(y)})}\\end{array}$ as small as possible. To this end, we can set Line 7 in Algorithm 1 in the following way: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{k}(y)=\\operatorname*{min}\\{k:\\epsilon_{y}^{k}<\\alpha\\},\\quad\\widehat{\\alpha}_{y}=\\alpha-\\epsilon_{y}^{\\widehat{k}(y)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Therefore, this setting ensures $\\sigma_{y}\\le1$ and as a result improves predictive efficiency. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments and Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present the empirical evaluation of the RC3P algorithm and demonstrate its effectiveness in achieving class-conditional coverage to produce small prediction sets. We conduct experiments using two baselines (CCP and Cluster-CP), four datasets (each with three imbalance types and five imbalance ratios), and two machine learning models (trained for 50 epochs and 200 epochs, with 200 epochs being our main experimental setting). Additionally, we use two scoring functions (APS and RAPS) and set three different $\\alpha$ values $\\mathit{\\dot{\\alpha}}\\alpha\\in0.1,0.05,0.01$ , with $\\alpha=0.1$ as our main setting). ", "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Classification datasets. We consider four datasets: CIFAR-10, CIFAR-100 [34], mini-ImageNet [57], and Food-101 [8] by using the standard training and validation split. We employ the same methodology as [41, 10, 14] to create an imbalanced long-tail setting for each dataset as a harder challenge: 1) We use the original training split as a training set for training $f$ with training samples $\\left(n_{t r}\\right.$ is defined as the number of training samples), and randomly split the original (balanced) validation set into calibration samples and testing samples. 2) We define an imbalance ratio $\\rho$ , the ratio between the sample size of the smallest and largest class: $\\begin{array}{r}{\\rho\\,=\\,\\frac{\\operatorname*{min}_{i}\\,\\{\\#\\mathrm{\\,samples\\;in\\;class\\;}i\\}}{\\operatorname*{max}_{i}\\,\\{\\#\\mathrm{\\,samples\\;in\\;class\\;}i\\}}}\\end{array}$ . 3) For each training set, we create three different imbalanced distributions using three decay types over the class indices $c\\in\\{1,\\cdots\\,,K\\}$ : (a) An exponential-based decay (EXP) with $\\frac{n_{t r}}{K}\\,\\times\\,\\Bar{\\rho}^{\\frac{c}{K}}$ examples in class c, (b) A polynomial-based decay (POLY) with nKtr \u00d7 $\\begin{array}{r}{\\frac{n_{t r}}{K}\\times\\frac{1}{\\sqrt{\\frac{c}{10\\rho}+1}}}\\end{array}$ examples in class $c$ , and (c) A majority-based decay (MAJ) with ${\\frac{n_{t r}}{K}}\\times\\rho$ examples in classes $c>1$ . We keep the calibration and test set balanced and unchanged. We provide an illustrative example of the three decay types in Appendix (Section C.3, Figure 4). Towards a more complete comparison, we also employ balanced datasets. Following Cluster- $C\\mathbf{P}^{2}$ , we employ CIFAR-100, Places365 [66], iNaturalist[55], and ImageNet[48]. ", "page_idx": 5}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/dd72a3f3982fb3a4d4c326974590ed3146e7118c6f6bdc1b228b39c6397dd55c.jpg", "table_caption": ["Table 1: Imalanced classification data experiment on CIFAR-10, CIFAR-100, mini-ImageNet, Food-101. APSS results comparing CCP, Cluster $-C\\mathbb{P}$ , and $_{\\mathrm{RC3P}}$ with ResNet-20 model trained with 200 epochs under different imbalance types and ratios when $\\alpha=0.1$ . For a fair comparison of APSS, we set UCR of RC3P the same as or smaller (more restrictive) than that of CCP and Cluster-CP under 0.16 on CIFAR-10 and 0.03 on other datasets. The specified UCR values are in Table 6 and 7 of Appendix C.4 and C.5. The APSS results show that $\\tt R C3P$ significantly outperforms CCP and Cluster-CP in terms of average prediction set size with $24.47\\%$ (four datasets) or $32.63\\bar{\\%}$ (excluding CIFAR-10) reduction over $\\operatorname*{min}\\{\\mathbb{C C P},\\mathbb{C l u s t e r\u2013C P}\\}$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Deep neural network models. We consider ResNet-20 [27] as the main architecture to train classifiers for imbalanced classification datasets. To handle imbalanced data, we employ the training algorithm \u201cLDAM\u201d proposed by [10] that assigns different margins to classes, where larger margins are assigned to minority classes in the loss function. We follow the training strategy in [10] where all models are trained with 200 epochs. The class-wise performance with three imbalance types and imbalance ratios $\\rho=0.5$ and $\\rho=0.1$ on four datasets are evaluated (see Appendix C.1). We also train models with 50 epochs and the corresponding APSS results are reported in Appendix C.8. ", "page_idx": 6}, {"type": "text", "text": "For balanced datsets, we follow the same settings from Cluster-CP, which uses IMAGENET1K_V2 as pre-trained weights from PyTorch [44] and then fine-tune models with ResNet-50 for all datasets except ImageNet. For ImageNet, we use SimCLR-v2 [12] as training models. ", "page_idx": 6}, {"type": "text", "text": "CP baselines. We consider three CP methods: 1) CCP which estimates class-wise score thresholds and produces prediction set using Equation (1); 2) Cluster-CP [15] that performs calibration over clusters to reduce prediction set sizes; and 3) RC3P that produces prediction set using Equation (3). All CP methods are built on the same classifier and non-conformity scoring function for a fair comparison. We employ the three common scoring functions: APS [47], RAPS [1], and HPS [49]. We set $\\alpha=0.1$ as our main experiment setting and also report other experiment results of different $\\alpha$ values (See Appendix C.7). Meanwhile, the hyper-parameters for each baseline are tuned according to their recommended ranges based on the same criterion (see Appendix C.2). We repeat experiments over 10 different random calibration-testing splits and report the mean and standard deviation. ", "page_idx": 6}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/7344ae1943bb58f2623b958bbac43f953642807e9475b628caa4ab4eb5f310ef.jpg", "table_caption": ["Table 2: Balanced experiment on CIFAR-100, Places365, iNaturalist, ImageNet. The models are pre-trained. UCR is controlled to $\\leq\\,0.05$ . RC3P significantly outperforms the best baseline with $32.826\\%$ reduction in APSS ( $\\downarrow$ better) on average over min{CCP, cluster-CP}. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Evaluation methodology. We use the target coverage $1-\\alpha=90\\%$ class-conditional coverage for CCP, Cluster-CP, and RC3P. We compute three evaluation metrics on the testing set: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Under Coverage Ratio (UCR). ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{UCR}:=\\sum_{c\\in[K]}\\mathbb{1}\\Big[\\frac{\\mathbb{E}_{X_{\\mathrm{tst}}}\\mathbb{1}[y\\in\\widehat{\\mathcal{C}}_{1-\\alpha}(x)\\mathrm{~s.t.~}y=c]}{\\mathbb{E}_{X_{\\mathrm{tst}}}\\mathbb{1}[y=c]}<1-\\alpha\\Big]/K.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "\u2022 Average Prediction Set Size (APSS). ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{APSS}:=\\sum_{c\\in[K]}\\frac{\\mathbb{E}_{X_{\\mathrm{test}}}\\mathbb{1}[y=c]\\cdot|\\widehat{\\mathcal{C}}_{1-\\alpha}(x)|}{\\mathbb{E}_{X_{\\mathrm{test}}}\\mathbb{1}[y=c]}/K.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that coverage and predictive efficiency are two competing metrics in CP [1], e.g., achieving better coverage (resp. predictive efficiency) degenerates predictive efficiency (resp. coverage). Therefore, following the same strategy in [20], we choose to control their UCR as the same level that is close to 0 for a fair comparison over three class-conditional CP algorithms in terms of APSS. Meanwhile, to address the gap between population values and empirical ones (e.g., quantiles with $\\tilde{O}(1/\\sqrt{n_{y}})$ error bound, common to all CP methods\u221a [58, 22, 2], or class-wise top- $k$ error $\\epsilon_{y}^{k}$ with $\\tilde{O}(1/\\sqrt{n_{y}})$ error bound [43]), we uniformly add $g/\\sqrt{n_{y}}$ (the same order with the standard concentration gap) to inflate the nominal coverage $1\\mathrm{~-~}\\alpha$ on each baseline and tune $g\\,\\in\\,\\{0.25,0.5,0.75,1\\}$ on the calibration dataset in terms of UCR. The detailed $g$ values of each method are displayed in Appendix C.2. In addition, the actual achieved UCR values are shown in the complete results (see Appendix C.4, C.5, and C.6). For a complete evaluation, we add the experiments without controlling coverage on imbalanced datasets under the same setting and use the total under coverage gap (UCG) metric: \u2022 Under Coverage Gap (UCG). ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{UCG}:=\\sum_{c\\in[K]}\\operatorname*{max}\\biggl\\{1-\\alpha-\\frac{\\mathbb{P}[Y\\in\\hat{\\mathcal{C}}(X),\\mathrm{s.t.~Y\\!=\\!c}]}{\\mathbb{P}[Y=c]},0\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Experiments with UCG metric evaluation are shown in the Appendix C.9. ", "page_idx": 7}, {"type": "text", "text": "We list empirical results in Table 1 for an overall comparison on four imbalanced datasets with $\\rho=0.5,0.1$ using all three training distributions (EXP, POLYand MAJ) based on the considered APS, RAPS and HPS scoring functions. Complete experiment results under more values of $\\rho$ are in Appendix C). Results with APS, RAPS, and HPS scoring functions on balanced datasets are also summarized in Table 2. We make the following two key observations: (i) CCP, Cluster-CP, and RC3P can guarantee the class-conditional coverage (their UCRs are all close to 0) for all settings; (ii) $\\mathsf{R C3P}$ significantly outperforms CCP and Cluster-CP in APSS on almost all imbalanced settings by reducing APSS with $24.47\\%$ on all four datasets and $32.63\\%$ on three datasets excluding CIFAR10 compared with $\\operatorname*{min}\\{\\mathrm{CCP},\\mathrm{C1uster\u2013CP}\\}$ on average, while for balanced settings, RC3P still significantly outperforms the best baselines in terms of APSS with $32.826\\%$ APSS reduction. ", "page_idx": 8}, {"type": "text", "text": "To investigate the challenge of imbalanced data and more importantly, how RC3P significantly improves the APSS, we further conduct three careful experiments on imbalanced datasets. First, we report the histograms of class-conditional coverage and the corresponding histograms of prediction set size. This experiment verifies that $\\mathsf{R C3P}$ derives significantly more class-conditional coverage above $1-\\alpha$ and thus reduces the prediction set size. Second, we visualize the normalized frequency of label rank included in prediction sets on testing datasets for all class-wise algorithms: CCP, Cluster-CP, and RC3P. The normalized frequency is defined as: P(k) :=  kKE=X1teEstX1t[esrtf1 ([rXft e(stX,yte)st,=yk),=yk\u2208,C y(\u2208xC) ](x)]. Finally, we empirically verify the trade-off condition number $\\{\\sigma_{y}\\}_{y=1}^{K}$ of Equation 6 on calibration dataset to itrmaibnailnagn cmedo ddealtsa reveal the underlying reason for $(\\mathrm{epoch}=200)$ )i. x WCe. 1a0l.s oA edvdailtiuoatnea $_{\\mathrm{RC3P}}$ producing smaller prediction sets over CCP with our standard $\\{\\sigma_{y}\\}_{y=1}^{K}$ ow ritehp eleast sa ltlr atihnreede  emxopdereilsm $(\\mathrm{epoch}=50)$ )c oend datasets (i.e., the histograms of class-conditional coverage and prediction set size, the normalized frequency of label rank included in prediction sets, and $\\{\\sigma_{y}\\}_{y=1}^{K})$ in Appendix C.11. Below we discuss our experimental results and findings in detail. ", "page_idx": 8}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/6c090180b151a73e4049c9033c6417764d8ea626264a502afe538a71b03b1fcf.jpg", "img_caption": ["Figure 1: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and $\\tt R C3P$ methods when $\\alpha=0.1$ and models are trained with 200 epochs on four imbalanced datasets with imbalance type EXP $\\rho=0.1$ . We clarify that $\\tt R C3P$ overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above 0.9 (the target $1-\\alpha$ class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "RC3P significantly outperforms CCP and Cluster-CP. First, it is clear from Table 6, 8, and 7, and 2 that RC3P, CCP, and Cluster-CP guarantee class-conditional coverage on all settings. This can also be observed by the first row of Fig 1, where the class-wise coverage bars of CCP and RC3P distribute on the right-hand side of the target probability $1-\\alpha$ (red dashed line). Second, RC3P outperforms CCP and Cluster-CP with $24.47\\%$ (four datasets) or $32.63\\%$ (excluding CIFAR-10) on imbalanced datasets and $32.63\\%$ on balanced datasets decrease in terms of average prediction set size for the same class-wise coverage. We also report the histograms of the corresponding prediction set sizes in the second row of Figure 1, which shows (i) RC3P has more concentrated class-wise coverage distribution than CCP and Cluster-CP; (ii) the distribution of prediction set sizes produced by RC3P is globally smaller than that produced by CCP and Cluster-CP, which is justified by a better trade-off number of $\\{\\sigma_{y}\\}_{y=1}^{K}$ as shown in Figure 3. Note that the class-wise coverage and the corresponding prediction set sizes RC3P overlap with CCP on CIFAR-10 in Figure 1. ", "page_idx": 8}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/8b817a444e09dae8d2b017d88befb90c4f1145aaf6f8befd0b670be987e2c039.jpg", "img_caption": ["Figure 2: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster $-C\\mathrm{P}$ , and RC3P with $\\rho=0.1$ for imbalance type EXP when $\\alpha=0.1$ and models are trained with 200 epochs. It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Visualization of normalized frequency. Figure 2 illustrates the normalized frequency distribution of label ranks included in the prediction sets across various testing datasets. It is evident that the distribution of label ranks in the prediction set generated by $\\mathsf{R C3P}$ tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the $_{\\mathrm{RC3P}}$ prediction set is notably shorter than that of other methods. This indicates that $_{\\mathrm{RC3P}}$ more effectively incorporates lower-ranked labels into prediction sets, as a result of its augmented rank calibration scheme. ", "page_idx": 9}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/59562c46d63e60df84486d9a46bcddb08dca2f475218db127899e3d90b2e8020.jpg", "img_caption": ["Figure 3: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{K}$ in Equation 6 with imbalance type EXP, $\\rho=0.1$ when $\\alpha=0.1$ and models are trained with 200 epochs. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that $\\mathsf{R C3P}$ produces smaller prediction sets than CCP using calibration on both non-conformity scores and label ranks. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Verification of $\\sigma_{y}$ . Figure 3 verifies the validity of Equation (6) on testing datasets and confirms the lcaobuelld  rbaen ke vlaelaudast etdo  osnm aclalleirb rparteiodinc tdiaotna sseettss .w iItt haolusto  tecsotninfgir mdsa ttahsaett st haen dc othnudist idoenc rneuasmebs etrh $\\{\\sigma_{y}\\}_{y=1}^{K}$ computation cost. We verify that $\\sigma_{y}\\le1$ for all settings and $\\sigma_{y}$ is much smaller than 1 on all datasets with large number of classes. ", "page_idx": 9}, {"type": "text", "text": "6 Summary ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper studies a provable conformal prediction (CP) algorithm that aims to provide classconditional coverage guarantee and to produce small prediction sets for classification tasks with many and/or imbalanced classes. Our proposed RC3P algorithm performs double-calibration, one over conformity score and one over label rank for each class separately, to achieve this goal. Our experiments clearly demonstrate the significant efficacy of RC3P over the baseline class-conditional CP algorithms on both balanced and imbalanced classification data settings. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. This research was supported in part by United States Department of Agriculture (USDA) NIFA award No. 2021-67021-35344 (AgAID AI Institute) and by NSF CNS-2312125 grant. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, and Michael I Jordan. Uncertainty sets for image classifiers using conformal prediction. arXiv preprint arXiv:2009.14193, 2020.   \n[2] Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021. [3] Anastasios N Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk control. arXiv preprint arXiv:2208.02814, 2022.   \n[4] Varun Babbar, Umang Bhatt, and Adrian Weller. On the utility of prediction sets in human-ai teams. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22), 2022.   \n[5] Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Conformal prediction beyond exchangeability. The Annals of Statistics, 51(2):816\u2013845, 2023.   \n[6] Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael Jordan. Distribution-free, risk-controlling prediction sets. Journal of the ACM (JACM), 68(6):1\u201334, 2021.   \n[7] Aadyot Bhatnagar, Huan Wang, Caiming Xiong, and Yu Bai. Improved online conformal prediction via strongly adaptive online learning. In International Conference on Machine Learning, pages 2337\u20132363. PMLR, 2023.   \n[8] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014.   \n[9] Henrik Bostr\u00f6m, Ulf Johansson, and Tuwe L\u00f6fstr\u00f6m. Mondrian conformal predictive distributions. In Conformal and Probabilistic Prediction and Applications, pages 24\u201338. PMLR, 2021.   \n[10] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. Advances in neural information processing systems, 32, 2019.   \n[11] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321\u2013357, 2002.   \n[12] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big selfsupervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:22243\u201322255, 2020.   \n[13] Kfir M Cohen, Sangwoo Park, Osvaldo Simeone, and Shlomo Shamai. Cross-validation conformal risk control. arXiv preprint arXiv:2401.11974, 2024.   \n[14] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9268\u20139277, 2019.   \n[15] Tiffany Ding, Anastasios Angelopoulos, Stephen Bates, Michael Jordan, and Ryan J Tibshirani. Classconditional conformal prediction with many classes. Advances in Neural Information Processing Systems, 36, 2024.   \n[16] Robin Dunn, Larry Wasserman, and Aaditya Ramdas. Distribution-free prediction sets with random effects. arXiv preprint arXiv:1809.07441, 2018.   \n[17] Shai Feldman, Stephen Bates, and Yaniv Romano. Calibrated multiple-output quantile regression with representation learning. Journal of Machine Learning Research, 24(24):1\u201348, 2023.   \n[18] Adam Fisch, Tal Schuster, Tommi Jaakkola, and Regina Barzilay. Efficient conformal prediction via cascaded inference with expanded admission. arXiv preprint arXiv:2007.03114, 2020.   \n[19] Adam Fisch, Tal Schuster, Tommi Jaakkola, and Regina Barzilay. Few-shot conformal prediction with auxiliary tasks. In International Conference on Machine Learning, pages 3329\u20133339. PMLR, 2021.   \n[20] Matteo Fontana, Gianluca Zeni, and Simone Vantini. Conformal prediction: a unified review of theory and new challenges. Bernoulli, 29(1):1\u201323, 2023.   \n[21] Subhankar Ghosh, Taha Belkhouja, Yan Yan, and Janardhan Rao Doppa. Improving uncertainty quantification of deep classifiers via neighborhood conformal prediction: Novel algorithm and theoretical analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 7722\u20137730, 2023.   \n[22] Subhankar Ghosh, Yuanjie Shi, Taha Belkhouja, Yan Yan, Jana Doppa, and Brian Jones. Probabilistically robust conformal prediction. In Uncertainty in Artificial Intelligence, pages 681\u2013690. PMLR, 2023.   \n[23] Isaac Gibbs and Emmanuel Candes. Adaptive conformal inference under distribution shift. Advances in Neural Information Processing Systems, 34:1660\u20131672, 2021.   \n[24] Isaac Gibbs and Emmanuel J Cand\u00e8s. Conformal inference for online prediction with arbitrary distribution shifts. Journal of Machine Learning Research, 25(162):1\u201336, 2024.   \n[25] Lee-Ad Gottlieb, Eran Kaufman, and Aryeh Kontorovich. Apportioned margin approach for cost sensitive large margin classifiers. Annals of Mathematics and Artificial Intelligence, 89(12):1215\u20131235, 2021.   \n[26] Leying Guan. Localized conformal prediction: A generalized inference framework for conformal prediction. Biometrika, 110(1):33\u201350, 2023.   \n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[28] Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Deep imbalanced learning for face recognition and attribute prediction. IEEE transactions on pattern analysis and machine intelligence, 42(11):2781\u20132794, 2019.   \n[29] Jianguo Huang, Huajun Xi, Linjun Zhang, Huaxiu Yao, Yue Qiu, and Hongxin Wei. Conformal prediction for deep classifier via label ranking, 2024.   \n[30] Kexin Huang, Ying Jin, Emmanuel Cand\u00e8s, and Jure Leskovec. Uncertainty quantification over graph with conformalized graph neural networks. In NeurIPS 2023.   \n[31] Ying Jin and Emmanuel J. Cand\u00e8s. Model-free selective inference under covariate shift via weighted conformal p-values.   \n[32] Lisa J\u00f6ckel, Michael Kl\u00e4s, Janek Gro\u00df, and Pascal Gerber. Conformal prediction and uncertainty wrapper: What statistical guarantees can you get for uncertainty quantification in machine learning? In International Conference on Computer Safety, Reliability, and Security, pages 314\u2013327. Springer, 2023.   \n[33] Bartosz Krawczyk, Micha\u0142 Koziarski, and Micha\u0142 Wo\u00b4zniak. Radial-based oversampling for multiclass imbalanced data classification. IEEE transactions on neural networks and learning systems, 31(8):2818\u2013 2831, 2019.   \n[34] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[35] Jing Lei, Max G\u2019Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523):1094\u20131111, 2018.   \n[36] Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression. Journal of the Royal Statistical Society Series B: Statistical Methodology, 76(1):71\u201396, 2014.   \n[37] Lihua Lei and Emmanuel J Cand\u00e8s. Conformal inference of counterfactuals and individual treatment effects. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(5):911\u2013938, 2021.   \n[38] Charles Lu, Anastasios N Angelopoulos, and Stuart Pomerantz. Improving trustworthiness of ai disease severity rating in medical imaging with ordinal conformal prediction sets. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 545\u2013554. Springer, 2022.   \n[39] Charles Lu, Andr\u00e9anne Lemay, Ken Chang, Katharina H\u00f6bel, and Jayashree Kalpathy-Cramer. Fair conformal predictors for applications in medical imaging. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 12008\u201312016, 2022.   \n[40] Harish Tayyar Madabushi, Elena Kochkina, and Michael Castelle. Cost-sensitive bert for generalisable sentence classification with imbalanced data. arXiv preprint arXiv:2003.11563, 2020.   \n[41] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314, 2020.   \n[42] Roweida Mohammed, Jumanah Rawashdeh, and Malak Abdullah. Machine learning with oversampling and undersampling techniques: overview study and experimental results. In 2020 11th international conference on information and communication systems (ICICS), pages 243\u2013248. IEEE, 2020.   \n[43] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.   \n[44] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[45] Yaniv Romano, Rina Foygel Barber, Chiara Sabatti, and Emmanuel Cand\u00e8s. With malice toward none: Assessing uncertainty via equalized coverage. Harvard Data Science Review, 2(2):4, 2020.   \n[46] Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. Advances in neural information processing systems, 32, 2019.   \n[47] Yaniv Romano, Matteo Sesia, and Emmanuel Candes. Classification with valid and adaptive coverage. Advances in Neural Information Processing Systems, 33:3581\u20133591, 2020.   \n[48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \n[49] Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with bounded error levels. Journal of the American Statistical Association, 114(525):223\u2013234, 2019.   \n[50] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research, 9(3), 2008.   \n[51] Eleni Straitouri, Lequn Wang, Nastaran Okati, and Manuel Gomez Rodriguez. Improving expert predictions with conformal prediction. In International Conference on Machine Learning (ICML), 2023.   \n[52] Jiankai Sun, Yiqi Jiang, Jianing Qiu, Parth Nobel, Mykel J Kochenderfer, and Mac Schwager. Conformal prediction for uncertainty-aware planning with diffusion dynamics model. Advances in Neural Information Processing Systems, 36, 2024.   \n[53] Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal prediction under covariate shift. Advances in neural information processing systems, 32, 2019.   \n[54] Chih-Fong Tsai, Wei-Chao Lin, Ya-Han Hu, and Guan-Ting Yao. Under-sampling class imbalanced datasets by combining clustering analysis and instance selection. Information Sciences, 477:47\u201354, 2019.   \n[55] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8769\u20138778, 2018.   \n[56] Janette Vazquez and Julio C Facelli. Conformal prediction in clinical medical sciences. Journal of Healthcare Informatics Research, 6(3):241\u2013252, 2022.   \n[57] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016.   \n[58] Vladimir Vovk. Conditional validity of inductive conformal predictors. In Asian conference on machine learning, pages 475\u2013490. PMLR, 2012.   \n[59] Vladimir Vovk, Valentina Fedorova, Ilia Nouretdinov, and Alexander Gammerman. Criteria of efficiency for conformal prediction. In Conformal and Probabilistic Prediction with Applications: 5th International Symposium, COPA 2016, Madrid, Spain, April 20-22, 2016, Proceedings 5, pages 23\u201339. Springer, 2016.   \n[60] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world. Springer Science & Business Media, 2005.   \n[61] Vladimir Vovk, David Lindsay, Ilia Nouretdinov, and Alex Gammerman. Mondrian confidence machine. Technical Report, 2003.   \n[62] Volodya Vovk, Alexander Gammerman, and Craig Saunders. Machine-learning applications of algorithmic randomness. 1999.   \n[63] Pattaramon Vuttipittayamongkol and Eyad Elyan. Neighbourhood-based undersampling approach for handling imbalanced and overlapped data. Information Sciences, 509:47\u201370, 2020.   \n[64] Yunpeng Xu, Wenge Guo, and Zhi Wei. Conformal risk control for ordinal classification. In Uncertainty in Artificial Intelligence, pages 2346\u20132355. PMLR, 2023.   \n[65] Margaux Zaffran, Aymeric Dieuleveut, Julie Josse, and Yaniv Romano. Conformal prediction with missing values. In International Conference on Machine Learning, pages 40578\u201340604. PMLR, 2023.   \n[66] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452\u20131464, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Mathematical Notations ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/ef3a908746ca41102a65882835e9d146300e8e0e501a80eb11b1e740e7f28b8b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Technical Proofs of Theoretical Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem B.1. (Theorem 4.1 restated, class-conditional coverage of $R C3P$ ) Suppose that selecting $\\widehat{k}(y)$ values result in the class-wise top- $k$ error $\\hat{\\epsilon}_{y}^{\\hat{k}(y)}$ for each class $y\\,\\in\\,\\mathcal{V}$ . For a target classconditional coverage $1-\\alpha,$ , if we set $\\widehat{\\alpha}_{y}$ and $\\widehat{k}(y)$ in RC3P (3) in the following ranges: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{k}(y)\\in\\{k:\\epsilon_{y}^{k}<\\alpha\\},\\quad0\\leq\\widehat{\\alpha}_{y}\\leq\\alpha-\\epsilon_{y}^{\\widehat{k}(y)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then RC3P can achieve the class-conditional coverage for every $y\\in\\mathcal{V}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(X,Y)\\sim{\\mathcal{P}}}\\{Y\\in{\\widehat{\\mathcal{C}}}_{1-\\alpha}^{R C3P}(X)|Y=y\\}\\geq1-\\alpha.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. (of Theorem 4.1) ", "page_idx": 14}, {"type": "text", "text": "Let $y\\in\\mathcal{V}$ denote any class label. In this proof, we omit the superscript $k$ in the top- $k$ error notation $\\epsilon_{y}^{k}$ for simplicity. ", "page_idx": 14}, {"type": "text", "text": "With the lower bound of the coverage on class $y$ (Theorem 1 in [47]), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad1-\\hat{\\alpha}\\leq\\mathbb{P}\\{Y_{\\mathrm{test}}\\in\\hat{C}_{1-\\hat{\\alpha}}^{\\mathrm{CP}}(X_{\\mathrm{test}})|Y=y\\}}\\\\ &{=\\!\\!\\mathbb{P}\\{V(X_{\\mathrm{test}},Y_{\\mathrm{test}})\\leq\\hat{Q}_{1}^{\\mathrm{test}}(y)|Y=y\\}}\\\\ &{=\\!\\!\\mathbb{P}\\{V(X_{\\mathrm{test}},Y_{\\mathrm{test}})\\leq\\hat{Q}_{1-\\hat{\\alpha}}^{\\mathrm{test}}(y),r_{f}(X_{\\mathrm{test}},Y_{\\mathrm{test}})\\leq\\hat{k}(y)|Y=y\\}}\\\\ &{\\quad+\\mathbb{P}\\{V(X_{\\mathrm{test}},Y_{\\mathrm{test}})\\leq\\hat{Q}_{1-\\hat{\\alpha}}^{\\mathrm{test}}(y),r_{f}(X_{\\mathrm{test}},Y_{\\mathrm{test}})>\\hat{k}(y)|Y=y\\}}\\\\ &{\\leq\\!\\!\\mathbb{P}\\{V(X_{\\mathrm{test}},Y_{\\mathrm{test}})\\leq\\hat{Q}_{1-\\hat{\\alpha}}^{\\mathrm{test}}(y),r_{f}(X_{\\mathrm{test}},Y_{\\mathrm{test}})\\leq\\hat{k}(y)|Y=y\\}}\\\\ &{\\quad+\\underbrace{\\mathbb{P}\\{r_{f}(X_{\\mathrm{test}},Y_{\\mathrm{test}})>\\hat{k}(y)\\}|Y=y\\}}\\\\ &{\\leq\\!\\!\\mathbb{P}\\{Y_{\\mathrm{est}}\\in\\hat{C}_{1-\\hat{\\alpha}}^{\\mathrm{ReSp}}(y)|Y=y\\}+\\hat{\\epsilon}_{y}^{\\hat{k}(y)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Re-arranging the above inequality, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\{Y_{\\mathrm{test}}\\in\\hat{\\mathcal{C}}_{1-\\widehat{\\alpha}}^{\\mathrm{RC3P}}(y)|Y=y\\}\\geq1-\\widehat{\\alpha}-\\epsilon_{y}^{\\widehat{k}(y)}\\geq1-\\alpha,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality is due to\u03b1y \u2264\u03b1 \u2212 \u03f5yk (y). This implies that RC3P guarantees the classconditional coverage on any class $y$ .  This completes the proof for Theorem 4.1. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Lemma 4.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem B.2. (Lemma 4.2 restated, improved predictive efficiency of $R C3P$ ) Let $\\widehat{\\alpha}_{y}$ and $\\widehat{k}(y)$ satisfy Theorem 4.1. If the following inequality holds for any $y\\in\\mathcal{V}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}_{X_{t e s t}}\\big[V(X_{t e s t},y)\\leq\\widehat{Q}_{1-\\widehat{\\alpha}}^{c l a s s}(y),\\;r_{f}(X_{t e s t},y)\\leq\\widehat{k}(y)\\big]\\leq\\mathbb{P}_{X_{t e s t}}\\big[V(X_{t e s t},y)\\leq\\widehat{Q}_{1-\\alpha}^{c l a s s}(y)\\big],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then RC3P produces smaller expected prediction sets than CCP, i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X_{t e s t}}[|\\widehat{\\mathcal{C}}_{1-\\widehat{\\alpha}}^{R C3P}(X_{t e s t})|]\\leq\\mathbb{E}_{X_{t e s t}}[|\\widehat{\\mathcal{C}}_{1-\\alpha}^{C C P}(X_{t e s t})|].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. (of Lemma 4.2) ", "page_idx": 15}, {"type": "text", "text": "The proof idea is to reduce the cardinality of the prediction set made by RC3P to that made by CCP in expectation. Let $\\begin{array}{r}{\\sigma_{y}=\\frac{\\mathbb{P}_{X_{\\mathrm{test}}}\\Big[V(X_{\\mathrm{test}},y)\\stackrel{\\cdot}{\\leq}\\widehat{Q}_{1-\\widehat{\\alpha}}^{\\mathrm{class}}(y),~r_{f}(X_{\\mathrm{test}},y)\\leq\\widehat{k}(y)\\Big]}{\\mathbb{P}_{X_{\\mathrm{test}}}\\Big[V(X_{\\mathrm{test}},y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)\\Big]}.}\\end{array}$ According to the assumption in (9), we know that $\\sigma_{y}\\le1$ , which will be used later. ", "page_idx": 15}, {"type": "text", "text": "We start with the expected prediction set size of RC3Pand then derive its upper bound. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X_{m}}[\\left|\\widehat{F}_{1-\\alpha}^{(X_{m},y)}(X_{m})\\right|]=\\mathbb{E}_{X_{m}}\\Bigg[\\sum_{\\psi\\in\\mathcal{Y}}\\left|\\left[{V}(X_{m*},y)\\le\\widehat{Q}_{1-\\alpha}^{\\mathrm{tass}}(y),\\;r_{f}(X_{m*},y)\\le\\widehat{k}(y)\\right|\\right]}\\\\ &{=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\mathbb{E}_{X_{m}}\\Big[1\\big|V(X_{m*},y)\\le\\widehat{Q}_{1-\\alpha}^{\\mathrm{tass}}(y),\\;r_{f}(X_{m*},y)\\le\\widehat{k}(y)\\Big|\\Big]}\\\\ &{=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\mathbb{P}_{X_{m}}\\Big[V(X_{m*},y)\\le\\widehat{Q}_{1-\\alpha}^{\\mathrm{tas}}(y),\\;r_{f}(X_{m*},y)\\le\\widehat{k}(y)\\Big]}\\\\ &{\\stackrel{(a)}{\\le}\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\mathbb{P}_{X_{m}}\\Big[V(X_{m*},y)\\le\\widehat{Q}_{1-\\alpha}^{\\mathrm{tas}}(y)\\Big]}\\\\ &{\\stackrel{(b)}{\\le}\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\mathbb{E}_{X_{m}}\\Big[1\\big|V(X_{m*},y)\\le\\widehat{Q}_{1-\\alpha}^{\\mathrm{tas}}(y)\\Big|}\\\\ &{\\stackrel{(b)}{\\le}\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\mathbb{E}_{X_{m}}\\Big[1\\big|V(X_{m*},y)\\le\\widehat{Q}_{1-\\alpha}^{\\mathrm{tas}}(y)\\Big|\\Big]}\\\\ &{=\\mathbb{E}_{X_{m}}\\Bigg[\\sum_{\\psi\\in\\mathcal{Y}}\\mathbb{I}\\big|V(X_{m*},y)\\le\\widehat{Q}_{1-\\alpha}^{\\mathrm{tas}}(y)\\Big]\\Bigg]=\\mathbb{E}_{X_{m}}\\big[\\widehat{C}_{1-\\alpha}^{\\mathrm{TC}}(X_{m*})\\big]\\Bigg|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the equality $(a)$ is due to the definitions of $\\sigma_{y}$ , and inequality $(b)$ is due to the assumption ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{y\\in\\mathcal{Y}}\\sigma_{y}\\cdot\\mathbb{P}_{X_{\\mathrm{test}}}\\Big[V(X_{\\mathrm{test}},y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)\\Big]\\leq\\sum_{y\\in\\mathcal{Y}}\\mathbb{P}_{X_{\\mathrm{test}}}\\Big[V(X_{\\mathrm{test}},y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)\\Big].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This shows that RC3P requires smaller prediction sets to guarantee the class-conditional coverage compared to CCP. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.3 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem B.3. (Theorem 4.3 restated, conditions of improved predictive efficiency for RC3P) Define $D=\\mathbb{P}[r_{f}(X,y)\\le\\widehat{k}(y)|Y\\ne y]$ , and $\\begin{array}{r}{\\bar{r}_{f}(X,y)=\\lfloor\\frac{r_{f}(X,y)+1}{2}\\rfloor}\\end{array}$ . Denote $B=\\mathbb{P}[f(X)_{(\\bar{r}_{f}(X,y))}\\leq$ $\\widehat{Q}_{1-\\alpha}^{c l a s s}(y)|Y\\ne y]$ if V is $A P S,$ , or $B=\\mathbb{P}[f(X)_{(\\bar{r}_{f}(X,y))}+\\lambda\\leq\\widehat{Q}_{1-\\alpha}^{c l a s s}(y)|Y\\neq y]$ if $V$ is RAPS. If $\\begin{array}{r}{B-D\\geq\\frac{p_{y}}{1-p_{y}}(\\alpha-\\epsilon_{y}^{\\widehat{k}(y)})}\\end{array}$ , then $\\sigma_{y}\\le1$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. (of Theorem 4.3) ", "page_idx": 15}, {"type": "text", "text": "Based on the different choices of scoring function, we first divide two scenarios: ", "page_idx": 15}, {"type": "text", "text": "(i): If $V(X,y)$ is the APS scoring function, since the APS score cumulatively sums the ordered prediction of $\\begin{array}{r}{f(X)\\colon V(X,y)=\\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)}}\\end{array}$ , it is easy to verify that $V(X,y)$ is concave in ", "page_idx": 15}, {"type": "text", "text": "terms of $l$ . As a result, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n^{\\prime}(X,y)=\\frac{r_{f}(X,y)}{r_{f}(X,y)}\\cdot\\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)}\\leq r_{f}(X,y)\\cdot f(X)_{(\\lfloor\\sum_{l=1}^{r_{f}(X,y)}l/r_{f}(X,y)\\rfloor)}=r_{f}(X,y)\\cdot f(X)_{(\\bar{r}_{f}(X,y)\\slash\\sum_{l=1}^{r_{f}(X,y)}l/r_{f}(X,y)\\rfloor)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we lower bound $\\mathbb{P}_{X}[V(X,y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)]$ as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{X}[V(X,y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{clas}}(y)]}\\\\ &{=\\underbrace{\\mathbb{P}_{X Y}[Y=y]}_{=p_{y}}\\cdot\\underbrace{\\mathbb{P}_{X}[V(X,y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{clas}}(y)|Y=y]}_{\\geq1-\\alpha}+\\underbrace{\\mathbb{P}_{X Y}[Y\\neq y]}_{=1-p_{y}}\\cdot\\underbrace{\\mathbb{P}_{X}[V(X,y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)|Y\\neq y]}_{\\geq B}}\\\\ &{\\geq p_{y}(1-\\alpha)+(1-p_{y})B+p_{y}(1-\\epsilon_{y}^{\\hat{k}(y)})+(1-p_{y})D-p_{y}(1-\\epsilon_{y}^{\\hat{k}(y)})-(1-p_{y})D}\\\\ &{\\geq\\mathbb{P}_{X}[r_{f}(X,y)\\leq\\widehat{k}(y)]-p_{y}(\\alpha-\\epsilon_{y}^{\\hat{k}(y)})+(1-p_{y})(B-D).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "According to the assumption B \u2212D \u22651\u2212pypy (\u03b1 \u2212\u03f5yk (y) ), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}_{X}[r_{f}(X,y)\\leq\\widehat{k}(y)]\\leq\\mathbb{P}_{X}[V(X,y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(ii): If $V(X,y)$ is the RAPS scoring function and $r_{f}(X,y)\\leq k_{r e g}$ , then the RAPS scoring function could be rewritten as: $\\begin{array}{r}{V(X,y)=\\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)}}\\end{array}$ . As a result, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle V(X,y)\\!=\\!\\frac{r_{f}(X,y)}{r_{f}(X,y)}\\cdot\\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)}}\\\\ {\\displaystyle\\le\\!r_{f}(X,y)\\cdot f(X)_{(\\lfloor\\sum_{l=1}^{r_{f}(X,y)}l/r_{f}(X,y)\\rfloor)}}\\\\ {\\displaystyle=\\!r_{f}(X,y)\\cdot f(X)_{(\\bar{r}_{f}(X,y))}}\\\\ {\\displaystyle\\le\\!r_{f}(X,y)\\cdot\\Big(f(X)_{(\\bar{r}_{f}(X,y))}+\\lambda\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $r_{f}(X,y)\\ >\\ k_{r e g}$ , then the RAPS scoring function could be rewritten as: $V(X,y)\\ =$ lrf= (1X,y)f(X)(l) + \u03bb(rf(X, y) \u2212kreg). As a result, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle V(X,y)=\\frac{r_{f}(X,y)}{r_{f}(X,y)}\\cdot\\Big(\\displaystyle\\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)}+\\lambda\\big(r_{f}(X,y)-k_{r e g}\\big)\\Big)}\\\\ {\\displaystyle\\leq r_{f}(X,y)\\cdot\\Big(f(X)_{(\\bar{r}_{f}(X,y))}+\\lambda\\big(1-\\frac{k_{r e g}}{r_{f}(X,y)}\\big)\\Big)}\\\\ {\\displaystyle\\leq r_{f}(X,y)\\cdot\\Big(f(X)_{(\\bar{r}_{f}(X,y))}+\\lambda\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, by applying the Inequality 12, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}_{X}[r_{f}(X,y)\\leq\\widehat{k}(y)]\\leq\\mathbb{P}_{X}[V(X,y)\\leq\\widehat{Q}_{1-\\alpha}^{\\mathrm{class}}(y)].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This completes the proof for Theorem 4.3. ", "page_idx": 16}, {"type": "text", "text": "Complete Experimental Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Training Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For CIFAR-10 and CIFAR-100, we train ResNet20 using LDAM loss function given in [10] with standard mini-batch stochastic gradient descent (SGD) using learning rate 0.1, momentum 0.9, and weight decay $2e-4$ for 200 epochs and 50 epochs. The batch size is 128. For experiments on mini-ImageNet, we use the same setting. For Food-101, the batch size is 256 and other parameters are kept the same. We reported our main results when models were trained in 200 epochs. Other results are reported in Appendix C.8 and Table 11. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "We also evaluate the top-1 accuracy over the majority, medium, and minority groups of classes as the class-wise performance when 200 epochs. To show the variation of class-wise performance, we divide some classes with the largest number of data samples into the majority group, and the number of these classes is a quarter $(25\\%)$ of the total number of classes. Similarly, we divide the classes with the smallest number of data into the minority group $(25\\%)$ ) and the remaining classes as the medium group $(50\\%)$ . In the above table, we show the accuracy of three groups with three imbalance types and two imbalance ratios $\\rho=0.1,\\rho=0.5$ on four datasets. ", "page_idx": 17}, {"type": "text", "text": "The results are summarized in Table 4. As can be seen, the group-wise performance can vary significantly from high to very low. The class-imbalance setting is the case where the classifier does not perform very well in some classes. ", "page_idx": 17}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/5209ff432fe38503abc12f3c0aaa706010510117c7da55cdfa5871e78dade586.jpg", "table_caption": ["Table 4: Top-1 accuracy of minority, medium, and majority groups with three imbalance types and two imbalance ratios $\\rho=0.1$ , $\\rho=0.5$ on four datasets. We could observe that the class-wise performance varies significantly over different classes. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.2 Calibration Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As mentioned in Section 5.1, we balanced split the validation set of CIFAR-10 and CIFAR-100, the number of calibration data is 5000. For mini-ImageNet, the number of calibration data is 15000. For Food-101, the total number is 12625. To compute the mean and standard deviation for the overall performance, we repeat calibration experiments for 10 times. In our main results, We set $\\alpha=0.1$ . We also report other experiment results of different $\\alpha$ values, $\\alpha=0.05$ and $\\alpha=0.01$ , in Appendix C.7, and Table 9 and 10. ", "page_idx": 17}, {"type": "text", "text": "The regularization parameter for RAPS scoring function is from the set $k_{r e g}\\in\\{3,5,7\\}$ and $\\lambda\\in$ $\\{0.001,0.01,0.1\\}$ based on the empirical setting in cluster-CP. We select the combination of $k_{r e g}$ and $\\lambda$ for each experiment with the same imbalanced type and imbalanced ratio on the same dataset, where most of the APSS values of all methods are minimum. ", "page_idx": 17}, {"type": "text", "text": "The hyper-parameter $g$ is selected from the set $\\{0.25,0.5,0.75,1.0\\}$ to find the minimal $g$ that CCP, Cluster-CP 3, and $\\mathsf{R C3P}$ achieve the target class-conditional coverage. We clarify that for each dataset and each class-conditional CP method, we use fixed $g$ values. The detailed $g$ values are displayed in Table 5. From Table 5, we could observe that the hyperparameter $g$ for $\\mathsf{R C3P}$ is always smaller than other methods, which means that comparing other class-wise CP algorithms, our algorithm needs the smallest inflation on $1-\\widehat{\\alpha}$ to achieve the target class-conditional coverage. This could also match the result of histograms of class-conditional coverage. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Table 5: Hyperparameter $g$ choices for each class-conditional CP methods CCP, Cluster-CP, and RC3P on four datasets CIFAR-10, CIFAR-100, mini-ImageNet, and Food101. We could observe that all $g$ values are in constant order to make a fair comparison. Meanwhile, the hyperparameter $g$ for $\\tt R C3P$ is always smaller than other methods. ", "page_idx": 18}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/4a0995aa8dbaa5079bff18f8e4534b6624464f9d832529e253ad999ae75bb240.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.3 Illustration of Imbalanced Data ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/eeda50f5d96f35a420c120edb2bc2696867337cab941589a604722afd41a2dfe.jpg", "img_caption": ["Figure 4: Illustrative examples of the different imbalanced distributions of the number of training examples per class index $c$ on CIFAR-100 "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.4 Comparison Experiments Using APS Score Function ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Based on the results in Table 6, we make the following observations: (i) CCP, Cluster-CP, and $\\mathsf{R C3P}$ can guarantee the class-conditional coverage; and (ii) RC3P significantly outperforms CCP and Cluster-CCP on three datasets by producing smaller prediction sets. ", "page_idx": 18}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/ad1aa641803048de351c059a27420c9852db0c5fc585f3a88ce4ce5b34613937.jpg", "table_caption": ["Table 6: Results comparing CCP, Cluster $-C\\mathrm{P}$ , and RC3P with ResNet-20 model and APS scoring function under different imbalance ratios $\\rho=0.5$ and $\\rho=0.1$ when $\\alpha=0.1$ . We set UCR of $\\mathsf{R C3P}$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. The APSS results show that RC3P significantly outperforms Cluster $-C\\mathbb{P}$ in terms of the average prediction set size over all settings on CIFAR-100, mini-ImageNet, and Food-101. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.5 Comparison Experiments Using RAPS Score Function ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "With the same model, evaluation metrics, and RAPS score function [1], we add the comparison experiments with CCP, and Cluster-CP on four datasets with different imbalanced types and imbalance ratio $\\rho=0.5$ and $\\rho=0.1$ . The regularization parameter for RAPS scoring function is from the set $k_{r e g}\\in\\{3,5,7\\}$ and $\\lambda\\in\\{0.001,0.01,0.1\\}$ . We select the combination of $k_{r e g}$ and $\\lambda$ for each experiment with the same imbalanced type and imbalanced ratio on the same dataset, where most of the $A P S S$ values of all methods are minimum. The overall performance is summarized in Table 7. We highlight that we also select the $g$ from the set $g\\,\\in\\,\\left\\lbrace0.25,0.5,0.75,1.0\\right\\rbrace$ to find the minimal $g$ that CCP, Cluster-CP, and $\\mathsf{R C3P}$ approximately achieves the target class conditional coverage. ", "page_idx": 20}, {"type": "text", "text": "Based on the results in Table 7, we make the following observations: (i) CCP, Cluster-CP, and $\\mathsf{R C3P}$ can guarantee the class-conditional coverage; and (ii) RC3P significantly outperforms CCP and Cluster-CP on three datasets by producing smaller prediction sets. ", "page_idx": 20}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/43af05aacfc2d49b8e60413d36a1e4de3a9743918d406d0dcac8db870c7f131c.jpg", "table_caption": ["Table 7: Results comparing CCP, Cluster-CP, and RC3P with ResNet-20 model and the RAPS scoring function under different imbalance ratios $\\rho=0.5$ and $\\rho=0.1$ when $\\alpha=0.1$ . The regularization parameter for RAPS scoring function is selected from the set [3, 5, 7] and [0.001, 0.01, 0.1]. We select the best results for each element in the table. We set UCR of $\\tt R C3P$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. The APSS results show that $\\tt R C3P$ significantly outperforms CCP and Cluster-CP in terms of average prediction set size over all settings on CIFAR-100, mini-ImageNet, and Food-101. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.6 Comparison Experiments Using HPS Score Function ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "With the same model, evaluation metrics, and HPS score function [1], we add the comparison experiments with CCP, and Cluster-CP on four datasets with different imbalanced types and imbalance ratio $\\rho\\,=\\,0.5$ and $\\rho\\,=\\,0.1$ . The overall performance is summarized in Table 8. We highlight that we also select the $g$ from the set $g\\in\\{0.25,0.5,0.75,1.0\\}$ to find the minimal $g$ that CCP, Cluster-CP, and RC3P approximately achieves the target class conditional coverage. ", "page_idx": 20}, {"type": "text", "text": "Based on the results in Table 8, we make the following observations: (i) CCP, Cluster-CP, and $\\mathsf{R C3P}$ can guarantee the class-conditional coverage; and (ii) RC3P significantly outperforms CCP and Cluster-CP on three datasets by producing smaller prediction sets. ", "page_idx": 21}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/491ad85cd592e08beb1ec60a9e0340ca9975b758009a1511bf6d3a6bb667f186.jpg", "table_caption": ["Table 8: Results comparing CCP, Cluster $-C\\mathrm{P}$ , and $\\tt R C3P$ with ResNet-20 model and the HPS scoring function under different imbalance ratios $\\rho=0.5$ and $\\rho=0.1$ when $\\alpha=0.1$ . We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. RC3P significantly outperforms CCP and Cluster-CP with $20.91\\%$ (four datasets) or $27.\\bar{8}8\\%$ (excluding CIFAR-10) reduction in APSS. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.7 Comparison Experiments with different $\\alpha$ values ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "With the same model, evaluation metrics, and scoring functions, we add the comparison experiments with CCP, and Cluster $-C D$ on four datasets with different imbalanced types and imbalance ratio $\\rho=0.5$ and $\\rho=0.1$ under the different $\\alpha$ values. The overall performance is summarized in Table 9 and 10, with $\\alpha=0.05$ and $\\alpha=0.01$ , respectively. We highlight that we also select the $g$ from the set $g\\in[0.15,0.75]$ with 0.05 range to find the minimal $g$ that CCP, Cluster-CP, and $_{\\mathrm{RC3P}}$ approximately achieves the target class conditional coverage. ", "page_idx": 21}, {"type": "text", "text": "Based on the results in Table 7, we make the following observations: (i) CCP, Cluster-CP, and $\\mathsf{R C3P}$ can guarantee the class-conditional coverage; and (ii) RC3P significantly outperforms CCP and Cluster-CP on three datasets by producing smaller prediction sets. ", "page_idx": 21}, {"type": "text", "text": "Table 9: APSS results comparing CCP, Cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ and $\\rho=0.1$ where $\\alpha=0.05$ . For a fair comparison of prediction set size, we set UCR of RC3P the same as or smaller (more restrictive) than that of CCP and Cluster-CP under 0.16 on CIFAR-10 and 0.03 on other datasets. The APSS results show that $_{\\mathrm{RC3P}}$ significantly outperforms CCP and Cluster-CP in terms of average prediction set size with $21.036\\%$ (four datasets) or $28.048\\%$ (excluding CIFAR-10) reduction in prediction size on average over $\\operatorname*{min}\\{\\mathbb{C C P},\\mathbb{C l u s t e r\u2013C P}\\}$ . ", "page_idx": 22}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/81331eef1be0ce4abb1715775a7220c46494b418c40d430b65abf65174d31bc1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 10: APSS results comparing CCP, Cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ and $\\rho=0.1$ where $\\alpha=0.01$ . For a fair comparison of prediction set size, we set UCR of RC3P the same as or smaller (more restrictive) than that of CCP and Cluster-CP under 0.16 on CIFAR-10 and 0.03 on other datasets. The APSS results show that $_{\\mathrm{RC3P}}$ significantly outperforms CCP and Cluster-CP in terms of average prediction set size with $16.911\\%$ (four datasets) or $22.549\\%$ (excluding CIFAR-10) reduction in prediction size on average over $\\operatorname*{min}\\{\\mathbb{C C P},\\mathbb{C l u s t e r\u2013C P}\\}$ . ", "page_idx": 23}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/120ac0f49c69a9e4856e985c34b2e2037b650f66598994efc568ef12ae4806a2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.8 Comparison Experiments when models are trained in different epochs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "With the same loss function, training criteria, evaluation metrics, and two scoring functions, we add the comparison experiments with CCP, and Cluster-CP on four datasets with different imbalanced types and imbalance ratio $\\rho=0.5$ and $\\rho=0.1$ and $\\alpha=0.1$ when models are trained with 50 epochs. The overall performance is summarized in Table 11. We highlight that we also select the $g$ from the set $g\\in\\{0.25,0.5,0.75,1.0\\}$ to find the minimal $g$ that CCP, Cluster-CP, and $\\mathsf{R C3P}$ approximately achieves the target class conditional coverage. ", "page_idx": 24}, {"type": "text", "text": "Based on the results in Table 7, we make the following observations: (i) CCP, Cluster-CP, and $\\mathsf{R C3P}$ can guarantee the class-conditional coverage; and (ii) RC3P significantly outperforms CCP and Cluster-CP on three datasets by producing smaller prediction sets. ", "page_idx": 24}, {"type": "text", "text": "Table 11: APSS results comparing CCP, Cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio $\\rho\\,=\\,0.5$ and $\\rho\\,=\\,0.1$ where $\\alpha\\,=\\,0.1$ and models are trained with 50 epochs. For a fair comparison of prediction set size, we set UCR of RC3P the same as or smaller (more restrictive) than that of CCP and Cluster-CP under 0.16 on CIFAR-10 and 0.03 on other datasets. The APSS results show that RC3P significantly outperforms CCP and Cluster-CP in terms of average prediction set size with $21.441\\%$ (four datasets) or $28.588\\%$ (excluding CIFAR-10) reduction in prediction size on average over $\\operatorname*{min}\\{\\mathrm{CCP,cluster\u2013CP}\\}$ . ", "page_idx": 24}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/ebf40b321661d37f1d13054be268fa44d1d26777c076512267c4e862cff73e45.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "C.9 Comparison Experiments with UCG metrics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We add the experiments without controlling coverage on imbalanced datasets under the same setting as the main paper. We then use the total under coverage gap (UCG, $\\downarrow$ better) between class conditional coverage and target coverage $1-\\alpha$ of all under covered classes. We choose UCG as the fine-grained metric to differentiate the coverage performance in our experiment setting. Conditioned on similar APSS of all methods, RC3P significantly outperforms the best baselines with $35.18\\%$ (four datasets) or $46.91\\%$ (excluding CIFAR-10)reduction in UCG on average. ", "page_idx": 24}, {"type": "text", "text": "Table 12: UCG and APSS results comparing CCP, Cluster-CP, and RC3P with ResNet-20 model trained with 200 epochs under different imbalance types with imbalance ratio $\\rho=0.1$ , where the coverage of each method are not aligned. The APSS results show that RC3P outperforms CCP and Cluster-CP in terms of average prediction set size with $1.64\\%$ (four datasets) or $2.19\\%$ (excluding CIFAR-10) reduction in prediction size on average over $\\operatorname*{min}\\{\\mathrm{CCP,cluster\u2013CP}\\}$ . The UCG results show that RC3Pachieve the similar class conditional coverage as CCP and Cluster-CP in terms of with $35.18\\%$ (four datasets) or $46.91\\%$ (excluding CIFAR-10) increment in the proportion of under coverage classes on average over $\\operatorname*{min}\\{\\mathrm{CCP,cluster\u2013CP}\\}$ . ", "page_idx": 25}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/8422aff6c769672b98f6262d920490f31d334cf6a84b4f99bbad43fce511bb52.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "C.10 Complete Experiment Results on Imbalanced Datasets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this subsection, we report complete experimental results over four imbalanced datasets, three decaying types, and five imbalance ratios when epoch $=200$ and $\\alpha=0.1$ . Specifically, Table 13, 14, 15 report results on CIFAR-10 with three decaying types. Table 16, 17, 18 report results on CIFAR-100 with three decaying types. Table 19, 20, 21 report results on mini-ImageNet with three decaying types. Table 22, 23, 24 report results on Food-101 with three decaying types. ", "page_idx": 26}, {"type": "text", "text": "Figure 5, Figure 6, Figure 7, Figure 8 and Figure 9 show the class-conditional coverage and the corresponding prediction set sizes on EXP $\\rho=0.5$ , POLY $\\rho=0.1$ , POLY $\\rho=0.5$ , MAJ $\\rho=0.1$ , MAJ $\\rho=0.5$ , respectively. This result on EXP $\\rho=0.1$ is in Figure 1. ", "page_idx": 26}, {"type": "text", "text": "Figure 10, Figure 11, Figure 12, Figure 13 and Figure 14 illustrates the normalized frequency distribution of label ranks included in the prediction sets on EXP $\\rho=0.5$ , POLY $\\rho=0.1$ , POLY $\\rho=0.5$ , MAJ $\\rho=0.1$ , MAJ $\\rho=0.5$ , respectively. This result on EXP $\\rho=0.1$ is in Figure 2. It is evident that the distribution of label ranks in the prediction set generated by $\\mathsf{R C3P}$ tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods. This indicates that $\\mathsf{R C3P}$ more effectively incorporates lower-ranked labels into prediction sets, as a result of its augmented rank calibration scheme. ", "page_idx": 26}, {"type": "text", "text": "Figure 15, Figure 16, Figure 17, Figure 18 and Figure 19 verify the condition numbers $\\sigma_{y}$ when models are fully trained $(\\mathrm{epoch}=200)$ ) on EXP $\\rho=0.5$ , POLY $\\rho=0.1$ , POLY $\\rho=0.\\dot{5}$ , MAJ $\\rho=0.1$ , MAJ $\\rho=0.5$ , respectively. This result on EXP $\\rho=0.1$ is in Figure Figure 3. We also evaluate the condition numbers $\\sigma_{y}$ when models are lessly trained $(\\mathrm{epoch}=50)$ ) and $\\alpha=0.1$ on EXP $\\rho=0.5$ , EXP $\\rho=0.1$ , POLY $\\rho=0.1$ , POLY $\\rho=0.5$ , MAJ $\\rho=0.1$ , MAJ $\\rho=0.5$ , respectively. These results are shown from Figure 21 to Figure 25. These results verify the validity of Lemma 4.2 and Equation 6 and confirm that the optimized trade-off between the coverage with inflated quantile and the constraint with calibrated rank leads to smaller prediction sets. They also show a stronger condition $\\langle\\sigma_{y}\\le1$ for all $y$ ) than the weighted aggregation condition in (5). They also confirm that the condition number $\\{\\sigma_{y}\\}_{y=1}^{C}$ could be evaluated on calibration datasets without testing datasets and thus decreases the computation cost. We notice that $\\mathsf{R C3P}$ degenerates to CCP on CIFAR-10, so $\\sigma_{y}\\,=1\\$ for all $y$ and there is no trade-off. On the other three datasets, we observe significant conditions for the optimized trade-off in RC3P. ", "page_idx": 26}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/131cfb24e254180846894b9dc7ecdf9b9bfdc9c254d488f613113d66bb5b6b70.jpg", "table_caption": ["Table 13: Results comparing CCP, cluster-CP, and $\\mathsf{R C3P}$ with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho=0.1$ with imbalance type EXP and two scoring functions, APS and RAPS, on dataset CIFAR-10. We set UCR of $\\mathsf{R C3P}$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/154f0bfd19c48d68a57e5304152c2e577cbd20f880748d3bc593aba68bac93dc.jpg", "table_caption": ["Table 14: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho=0.1$ with imbalance type POLY and two scoring functions, APS and RAPS, on dataset CIFAR-10. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/e06f5021ed86a38d6b9d761e329b43236d9048c263c8a20861289307e562f743.jpg", "table_caption": ["Table 15: Results comparing CCP, cluster-CP, and $\\mathsf{R C3P}$ with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho=0.1$ with imbalance type MAJ and two scoring functions, APS and RAPS, on dataset CIFAR-10. We set UCR of $\\mathsf{R C3P}$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/8840120788e2235eb3c264131c0282ac86a78bebf16bb8fa2ed2122704a9f051.jpg", "table_caption": ["Table 16: Results comparing CCP, cluster $-C\\mathrm{P}$ , and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho=0.1$ with imbalance type EXP and two scoring functions, APS and RAPS, on dataset CIFAR-100. We set UCR of $\\tt R C3P$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/c38acfa576cb226ed3a70a003b2de725d58323c159780621e0a7e48d8dea7413.jpg", "table_caption": ["Table 17: Results comparing CCP, cluster $-C\\mathrm{P}$ , and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho=0.1$ with imbalance type POLY and two scoring functions, APS and RAPS, on dataset CIFAR-100. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/1021e6dec9962d7ac20b4bb57a23c0a31bd0297d8b73f9757f3b6baf46ae9712.jpg", "table_caption": ["Table 18: Results comparing CCP, cluster $-C\\mathrm{P}$ , and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho=0.1$ with imbalance type MAJ and two scoring functions, APS and RAPS, on dataset CIFAR-100. We set UCR of $\\tt R C3P$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/6c9676f0244eb884a810cfbc2c9f4cc793fe241fc73a4301e5fc44951a620344.jpg", "table_caption": ["Table 19: Results comparing CCP, cluster $-C\\mathrm{P}$ , and RC3P with ResNet-20 model under different imbalance ratio $\\rho\\,=\\,0.5$ , $\\rho\\,=\\,0.4$ , $\\rho\\,=\\,0.2$ , and $\\rho\\,=\\,0.1$ with imbalance type EXP and two scoring function, APS and RAPS, on dataset mini-ImageNet. We set UCR of $\\tt R C3P$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/a66b0be590dd09aa2e62c564989a04dfb164fca6304cd7995426d185014aa87c.jpg", "table_caption": ["Table 20: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho\\,=\\,0.1$ with imbalance type POLY and two scoring function, APS and RAPS, on dataset mini-ImageNet. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/f0d6764b7fe8d0237aefbae7624971a2fd0d736c0e87d4bbd549e2946353dbcf.jpg", "table_caption": ["Table 21: Results comparing CCP, cluster-CP, and $\\mathsf{R C3P}$ with ResNet-20 model under different imbalance ratio $\\rho\\,=\\,0.5$ , $\\rho\\,=\\,0.4$ , $\\rho\\,=\\,0.2$ , and $\\rho\\,=\\,0.1$ with imbalance type MAJ and two scoring function, APS and RAPS, on dataset mini-ImageNet. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/efbcd9825f3c410fdd11a847e2d2f78730f1ca6016fdde9e6ce53520a00a3c78.jpg", "table_caption": ["Table 22: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho=0.1$ with imbalance type EXP and two scoring function, APS and RAPS, on dataset Food-101. We set UCR of $\\tt R C3P$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/200e8d0498cba78049291bb7bfef6414226bf8fd5dcc2e846441b3d2d49cf904.jpg", "table_caption": ["Table 23: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho=0.1$ with imbalance type POLY and two scoring function, APS and RAPS, on dataset Food-101. We set UCR of $\\tt R C3P$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "T7dS1Ghwwu/tmp/9e7b5d26a95918101e7d115ec84463f79171d5a6a06bd3bdece435a2b9c26423.jpg", "table_caption": ["Table 24: Results comparing CCP, cluster $-C\\mathrm{P}$ , and RC3P with ResNet-20 model under different imbalance ratio $\\rho=0.5$ , $\\rho=0.4$ , $\\rho=0.2$ , and $\\rho=0.1$ with imbalance type MAJ and two scoring function, APS and RAPS, on dataset Food-101. We set UCR of $\\tt R C3P$ the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size. "], "table_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/e72fe5b517a259d116fa180725595e3d3dd121e0aeb87c3e3cbf368535f23f70.jpg", "img_caption": ["Figure 5: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when $\\alpha\\,=\\,0.1$ on CIFAR-10, CIFAR-100, mini-ImageNet, and Food101 datasets with imbalance type EXP for imbalance ratio $\\rho=0.5$ . We clarify that RC3P overlaps with CCP on CIFAR-10. It is clear that $\\tt R C3P$ has more densely distributed class-conditional coverage above 0.9 (the target $1-\\alpha$ class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/66f7d9831827da83721eccfe9be7f76955c7e21db3eb02bde7c638b69f32033e.jpg", "img_caption": ["Figure 6: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when $\\alpha\\,=\\,0.1$ on CIFAR-10, CIFAR-100, mini-ImageNet, and Food101 datasets with imbalance type POLY for imbalance ratio $\\rho=0.1$ . We clarify that RC3P overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above 0.9 (the target $1-\\alpha$ class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/e94b4443e3ec365bf693ee7220a5a554f75978d9270b2f1ca53a34a9ac8b09c6.jpg", "img_caption": [], "img_footnote": ["Figure 7: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and $\\tt R C3P$ methods when $\\alpha\\,=\\,0.1$ on CIFAR-10, CIFAR-100, mini-ImageNet, and Food101 datasets with imbalance type POLY for imbalance ratio $\\rho=0.5$ . We clarify that $\\tt R C3P$ overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above 0.9 (the target $1-\\alpha$ class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101. "], "page_idx": 32}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/16d9fcd2d5c879034d0e3cdf0b7ae4c17a61b42c16a8d6591f84af7e294f9725.jpg", "img_caption": ["Figure 8: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when $\\alpha\\,=\\,0.1$ on CIFAR-10, CIFAR-100, mini-ImageNet, and Food101 datasets with imbalance type MAJ for imbalance ratio $\\rho=0.1$ . We clarify that $\\mathsf{R C3P}$ overlaps with CCP on CIFAR-10. It is clear that $\\tt R C3P$ has more densely distributed class-conditional coverage above 0.9 (the target $1-\\alpha$ class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/502e47f5848094b5d83ebc394b8a03eb5690a040c542efb4d72d079c19998f99.jpg", "img_caption": ["Figure 9: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when $\\alpha\\,=\\,0.1$ on CIFAR-10, CIFAR-100, mini-ImageNet, and Food101 datasets with imbalance type MAJ for imbalance ratio $\\rho=0.5$ . We clarify that $\\mathsf{R C3P}$ overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above 0.9 (the target $1-\\alpha$ class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/a60016f0df33300437847e08895ea57a6747a7d2dd1472c6abdc9fc7d366f155.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 10: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster $-C\\mathbb{P}$ , and RC3P with $\\rho=0.5$ EXP when $\\alpha=0.1$ . It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the $\\mathsf{R C3P}$ prediction set is notably shorter than that of other methods. ", "page_idx": 33}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/6a5ae4838368aa9292db2c309eb58b7a2cd9038546a5a1c668986fb6b11ec6e3.jpg", "img_caption": ["Figure 11: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with $\\rho\\,=\\,0.1$ POLY when $\\alpha\\,=\\,0.1$ . It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the $\\mathsf{R C3P}$ prediction set is notably shorter than that of other methods. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/7455582782666c84c293c443ba190d992ca29e626898764677e576000f61e0ea.jpg", "img_caption": ["Figure 12: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with $\\rho\\,=\\,0.5$ POLY when $\\alpha\\,=\\,0.1$ . It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/70477cd5abfdda4114f1b5346854ea00bcdd127e391d08dfde005b8435f66aff.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 13: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and $_{\\mathrm{RC3P}}$ with $\\rho=0.1$ MAJ when $\\alpha=0.1$ . It is clear that the distribution of normalized frequency generated by $\\mathsf{R C3P}$ tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods. ", "page_idx": 34}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/ad269ffd9b3455675a52e8fe6303c74ac8c5abe1286b48ef515ee146820e3170.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 14: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with $\\rho=0.5$ MAJ when $\\alpha=0.1$ . It is clear that the distribution of normalized frequency generated by $\\mathsf{R C3P}$ tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the $\\mathsf{R C3P}$ prediction set is notably shorter than that of other methods. ", "page_idx": 34}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/7951a4dd0876cb836cac67aaaa63a49213342708f26bddff661bdb79d2da5600.jpg", "img_caption": ["Figure 15: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=200$ and $\\alpha=0.1$ with $\\rho=0.5$ EXP. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/39be840e7b00addc37c48928b5a837991baaaf0aa883d7648cc8f69677bd9f21.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 16: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=200$ and $\\alpha=0.1$ with $\\rho=0.1$ POLY. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. ", "page_idx": 35}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/5069e1e0175745f379da70edc37c7e8cb5bf6ce046459323d39fbf750e0e4401.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 17: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=200$ and $\\alpha=0.1$ with $\\rho=0.5$ POLY. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. ", "page_idx": 35}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/a494b46181787b59be8aaf62cee472d04b2c9875675af0d816071ad5f3cc0017.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 18: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=200$ and $\\alpha=0.1$ with $\\rho=0.1$ MAJ. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. ", "page_idx": 35}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/7081267b323eb8721db26e73cdcf7bc2ea9bf40473297946539eab11b5bbf272.jpg", "img_caption": ["Figure 19: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=200$ and $\\alpha=0.1$ with $\\rho=0.5$ MAJ. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/0b5d69568738878c6f5b13b520d9844644860457493c002ada986e24dc7b99ef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 20: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=50$ and $\\alpha\\,=\\,0.1$ with $\\rho=0.1$ EXP. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. ", "page_idx": 36}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/cf4c466d18f68f17c5fc1caca1ff2a45aafadc775fd6d4ff5e30fa443fdbd323.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 21: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=50$ and $\\alpha\\,=\\,0.1$ with $\\rho=0.5$ EXP. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. ", "page_idx": 36}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/ddc9f1c398f6ace93ddac2a82a9270649fd37968ed7711ddc85e68cb75f44850.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 22: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=50$ and $\\alpha\\,=\\,0.1$ with $\\rho=0.1$ POLY. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. ", "page_idx": 36}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/5fcdf7f4313290a6a8f536f7700a2a161fb012a3e0a6bea2e222b3960d0fd8ef.jpg", "img_caption": ["Figure 23: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=50$ and $\\alpha\\,=\\,0.1$ with $\\rho=0.5$ POLY. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/7e1f6ed886d7863cfb6f794882ade768541da640cddc552231b9132cdbbb34c1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Figure 24: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=50$ and $\\alpha\\,=\\,0.1$ with $\\rho=0.1$ MAJ. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. ", "page_idx": 37}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/a05a74d1e790ef22826c733def59287d7a6d261f94a102fe89386f2ba9baeab9.jpg", "img_caption": ["Figure 25: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{C}$ of Equation 6 when epoch $=50$ and $\\alpha\\,=\\,0.1$ with $\\rho=0.5$ MAJ. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "C.11 Complete Experiment Results on Balanced Classification Datasets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this subsection, we report complete experimental results over four balanced datasets and $\\alpha=0.1$ . Specifically, Figure 26 shows the class-conditional coverage and the corresponding prediction set sizes. From the first row of $\\mathrm{Fig}\\,26$ , the class-wise coverage bars of CCP and $\\mathsf{R C3P}$ distribute on the right-hand side of the target probability $1-\\alpha$ (red dashed line). Second, RC3P outperforms CCP and Cluster-CP with $24.47\\%$ (on four datasets) or $32.63\\%$ (excluding CIFAR-10) on imbalanced datasets and $32.63\\%$ on balanced datasets decrease in terms of average prediction set size the same class-wise coverage. The second row of Figure 26 shows (i) RC3P has more concentrated classwise coverage distribution than CCP and Cluster-CP; (ii) the distribution of prediction set sizes produced by $\\mathsf{R C3P}$ is globally smaller than that produced by CCP and Cluster-CP, which is justified by a better trade-off number of $\\{\\sigma_{y}\\}_{y=1}^{K}$ as shown in Figure 3. ", "page_idx": 38}, {"type": "text", "text": "Figure 27 illustrates the normalized frequency distribution of label ranks included in the prediction sets on balanced datasets. It is evident that the distribution of label ranks in the prediction set generated by $_{\\mathrm{RC3P}}$ tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods. This indicates that $\\mathsf{R C3P}$ more effectively incorporates lower-ranked labels into prediction sets, as a result of its augmented rank calibration scheme. ", "page_idx": 38}, {"type": "text", "text": "Figure 28 verifies the condition numbers $\\sigma_{y}$ on balanced datasets. This result verifies the validity of Lemma 4.2 and Equation 6 and confirm that the optimized trade-off between the coverage with inflated quantile and the constraint with calibrated rank leads to smaller prediction sets. ", "page_idx": 38}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/e72492d2335d0ebbe08351f6b108ba019874c585a2dc545994e4b228069f2469.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 26: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when $\\alpha=0.1$ on four balanced datasets. It is clear that RC3P has more densely distributed class-conditional coverage above 0.9 (the target $1-\\alpha$ class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on all datasets. ", "page_idx": 38}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/1bebabed7e421e36a4ea95691ecdfb1930e52c62e72ff91ba121f48189cda3f0.jpg", "img_caption": ["Figure 27: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster $-C\\mathrm{P}$ , and RC3P with $\\rho=0.1$ on balanced datasets. It is clear that the distribution of normalized frequency generated by $\\mathsf{R C3P}$ tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "T7dS1Ghwwu/tmp/4f95d826669ed8d13e51f98ac9d912aa787d9189ad5a23d69847ab64b11bbcb5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 28: Verification of condition numbers $\\{\\sigma_{y}\\}_{y=1}^{K}$ in Equation 6 on balanced datasets. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than 1. This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3P produces smaller prediction sets than CCP using calibration on both non-conformity scores and label ranks. ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: The abstract and introduction has stated the contributions and important assumptions of our paper and match our theoretical and experimental results. We have summarize all claims at the end of introduction. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: A limitation of our paper is that we assume that $(X_{i},Y_{i})$ are exchangeable (for example, i.i.d.). This assumption is common and fundamental in CP works, so we do not discuss in our paper. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: In Section 4.1 and 4.2, we provide the the full set of assumptions for our theoretical result. Corresponding proofs are provided in Appendix B. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We have provided all all the information needed to reproduce the experiments, including experiments setting, evaluation metric and codes. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We have provided the codes in the supplementary material. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: We have provided the detail about dataset, training and calibration in Section 5.1 and Appendix C.1, C.2, and C.3. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We have provide the standard deviation of our main results. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [No] ", "page_idx": 43}, {"type": "text", "text": "Justification: We follow the training setting of previous papers, so we choose to not discuss the computer resources. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Our work strictly adheres to the NeurIPS Code of Ethics. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [No] ", "page_idx": 43}, {"type": "text", "text": "Justification: The methodological improvements gained in our paper can lead to improvements in safe deployment of classifiers in human-ML collaborative systems. We do not anticipate any negative ethical or societal impact. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 43}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our experiments are conducted on public and benchmark datasets. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: All the assets used in the paper are open-source and have been properly cited. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We have provided anonymized zip file in supplementary materiel. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]