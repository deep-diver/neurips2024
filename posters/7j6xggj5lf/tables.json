[{"figure_path": "7j6xgGj5lF/tables/tables_8_1.jpg", "caption": "Table 1: Direct evaluation performance on ImageNet-1K without any tuning after initialization.", "description": "This table presents the Top-1 accuracy (%) achieved by different variable-sized Vision Transformer (ViT) models on the ImageNet-1K dataset without any fine-tuning after initialization.  It compares the performance of models initialized using the proposed LeTs method against several baselines, including models trained from scratch and those initialized using other Learngene methods (IMwLM, Grad-LG, TLEG, SWS). The table shows that LeTs consistently outperforms the baselines, highlighting its effectiveness in improving initialization quality.", "section": "4.2 Main Results"}, {"figure_path": "7j6xgGj5lF/tables/tables_8_2.jpg", "caption": "Table 2: Performance comparisons on CIFAR-100 of variable-sized Des-Nets. Pre-Fin transfers the pretrained parameters (S-P(M)) to initialize, which totally requires about 758M for 12 Des-Nets. LeTs only needs to store 37.7M parameters (15.0M learngene) to initialize, which significantly reduces the parameters stored for initialization by 20\u00d7 (758M vs. 37.7M).", "description": "This table compares the performance of LeTs and Pre-Fin (pre-training and fine-tuning) on CIFAR-100 using variable-sized Des-Nets. It highlights LeTs' superior performance and significantly reduced parameter requirements for initialization compared to Pre-Fin.  The table demonstrates LeTs' efficiency by showing a 20x reduction in the number of parameters needed for initialization.", "section": "4.2 Main Results"}, {"figure_path": "7j6xgGj5lF/tables/tables_9_1.jpg", "caption": "Table 3: Performance on ImageNet-1K when using different selection strategies, selecting different learngene groups and initializing Des-Nets without certain components.", "description": "This table presents ablation studies on the LeTs model. It shows the impact of different parameter selection strategies (continuous, top-n L2-norm, bottom-n L2-norm, top-n L1-norm, bottom-n L1-norm) on the model's performance on ImageNet-1K.  It also shows the effect of various learngene group selections and the removal of certain components (MSA, MLP, LN, PE, Pos) on the final Top-1 accuracy.  The results highlight the importance of careful parameter selection and the contribution of each component to the model's performance. ", "section": "4.3 Ablation and Analysis"}, {"figure_path": "7j6xgGj5lF/tables/tables_9_2.jpg", "caption": "Table 4: Performance on ImageNet-1K when using depth and width expansion strategies proposed in LiGO [52] (named as LeTs(LiGO)), training smaller learngene module (named as LeTs(11.4M)), direct transforming learngene matrices to compose target ones (named as LeTs(DE)), not sharing weights between rows of G (named as LeTs(w/o ws)), training Aux-Net for 200 epochs (named as LeTs(200ep)) and not adopting distillation in the first stage (LeTs(w/o dis)).", "description": "This table presents ablation study results on ImageNet-1K, comparing the performance of LeTs against several variations.  It investigates the impact of different width and depth transformation methods (LiGO, direct transformation), reduced learngene size,  parameter sharing in the depth transformation matrix, extended training epochs for Aux-Net, and the effect of removing the distillation process. The results show the impact of each component on the overall performance.", "section": "4.3 Ablation and Analysis"}, {"figure_path": "7j6xgGj5lF/tables/tables_16_1.jpg", "caption": "Table 5: The numerical results for Fig.4 and Fig.5 in our original paper. The number of epochs is indicated in brackets within the \"Scratch\" column, with the default being 100 epochs when no brackets are present.", "description": "This table presents the performance comparison of different model sizes on ImageNet-1K.  It compares the Top-1 accuracy of models initialized using different methods (Scratch, TLEG, SWS, and LeTs) after varying numbers of epochs of fine-tuning. The table highlights the superior performance of LeTs, showing that it significantly outperforms other initialization methods, even after only one epoch of fine-tuning. The number of parameters (Params(M)) and floating-point operations (FLOPS(G)) are also included for each model.", "section": "4.2 Main Results"}, {"figure_path": "7j6xgGj5lF/tables/tables_17_1.jpg", "caption": "Table 6: The numerical results for Fig.6(a)-(h) in our original paper.", "description": "This table presents a comparison of the Top-1 accuracy (%) achieved by different model initialization methods (Pre-Fin, Scratch, Grad-LG, TLEG, SWS, and LeTs) on four different image classification datasets (CIFAR-100, CIFAR-10, Food-101, and Cars-196) using two different model sizes (Des-H6-L12 and Des-H12-L12).  The results show the performance of each method after a specified training period.", "section": "4.2 Main Results"}, {"figure_path": "7j6xgGj5lF/tables/tables_18_1.jpg", "caption": "Table 7: The numerical results for Fig.6(i)-(k) in our original paper.", "description": "This table presents the performance comparison on three semantic segmentation datasets: ADE20K, Pascal Context, and Cityscapes.  The results show the mean Intersection over Union (mIoU) scores achieved by three different methods: Pre-Fin (pre-training and fine-tuning), LeTs (0ep) (LeTs with 0 epochs of fine-tuning), and LeTs (5ep) (LeTs with 5 epochs of fine-tuning).  Two backbone models, Des-H6-L12 and Des-H12-L12, were used for each dataset. The table highlights the improved performance of LeTs, especially with a small amount of fine-tuning, compared to the traditional pre-training and fine-tuning approach.", "section": "4.2 Main Results"}]