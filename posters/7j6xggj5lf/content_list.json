[{"type": "text", "text": "Initializing Variable-sized Vision Transformers from Learngene with Learnable Transformation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shiyu Xia, Yuankun Zu, Xu Yang,\u2217 Xin Geng\u2217 ", "page_idx": 0}, {"type": "text", "text": "School of Computer Science and Engineering, Southeast University, Nanjing 210096, China Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China {shiyu_xia, zyk0418, xuyang_palm, xgeng}@seu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In practical scenarios, it is necessary to build variable-sized models to accommodate diverse resource constraints, where weight initialization serves as a crucial step preceding training. The recently introduced Learngene framework firstly learns one compact module, termed learngene, from a large well-trained model, and then transforms learngene to initialize variable-sized models. However, the existing Learngene methods provide limited guidance on transforming learngene, where transformation mechanisms are manually designed and generally lack a learnable component. Moreover, these methods only consider transforming learngene along depth dimension, thus constraining the flexibility of learngene. Motivated by these concerns, we propose a novel and effective Learngene approach termed LeTs (Learnable Transformation), where we transform the learngene module along both width and depth dimension with a set of learnable matrices for flexible variablesized model initialization. Specifically, we construct an auxiliary model comprising the compact learngene module and learnable transformation matrices, enabling both components to be trained. To meet the varying size requirements of target models, we select specific parameters from well-trained transformation matrices to adaptively transform the learngene, guided by strategies such as continuous selection and magnitude-wise selection. Extensive experiments on ImageNet-1K demonstrate that Des-Nets initialized via LeTs outperform those with 100-epoch from scratch training after only 1 epoch tuning. When transferring to downstream image classification tasks, LeTs achieves better results while outperforming from scratch training after about 10 epochs within a 300-epoch training schedule. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision Transformer (ViT) models have gained widespread attention due to their impressive performance on diverse vision tasks [1, 2, 3, 4, 5, 6]. In practice, models of various sizes are often deployed and trained under diverse resource constraints, ranging from edge devices with limited computational resources to computing clusters with sufficient resources, which may exhibit considerable diversity. Obviously, we could train each target model from scratch for specific tasks across different resource settings. However, such method underestimates the importance of weight initialization, which could significantly affect the training process and the final model quality [7, 8, 9, 10, 11, 12, 13, 14]. Moreover, the training and storage costs of such method grow linearly with the number of potential scenarios. Consequently, a fundamental research question arises: how to efficiently initialize variable-sized models while considering both the model performance and resource constraints. ", "page_idx": 0}, {"type": "image", "img_path": "7j6xgGj5lF/tmp/3afdd3ea155043b9c727bcf88efe0915e42016e0f0abd74afd42dce650fbfae5.jpg", "img_caption": ["Figure 1: (a) Learngene paradigm. (b) Manually-crafted and depth-only transformation. (c) Learnable transformation along both width and depth dimension. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Nowadays, pre-training on large-scale datasets provides an excellent initialization for fine-tuning models across a wide range of downstream tasks [15, 16, 17, 18, 19, 20, 21]. Nevertheless, such method generally requires transferring the entire pretrained model parameters repeatedly without considering the available resources of different downstream tasks. Take the pretrained model family SimMIM [22] as an example, even the smallest model 86M ViT-B [23] may be deemed excessively large for some resource-constrained environments. One direct solution involves firstly pre-training the target model on large-scale datasets under specific resource constraints before training it on target task. However, this process is not only time-consuming and computationally expensive, but also necessitates the access to the datasets used for pre-training and lacks the flexibility to initialize variable-sized models. ", "page_idx": 1}, {"type": "text", "text": "Recently, a novel learning paradigm termed as Learngene [24, 25] has been proposed, which firstly learns one compact module, termed learngene, from a large well-trained network termed as ancestry model (Ans-Net). Then learngene is transformed to initialize variable-sized descendant models (Des-Net), after which they are fine-tuned on diverse downstream tasks, as shown in Fig.1(a). GradLG [24] selects a few high-level layers as learngene, after which they are stacked with randomlyinitialized layers to construct Des-Nets. TLEG [26] linearly expands two integral learngene layers to initialize variable-depth Des-Nets. LearngenePool [27] distills multiple small models whose layers compose learngene instances and then stitches them to construct Des-Nets. However, there exist several limitations in previous studies. Firstly, the learngene learning process provides well-trained learngene parameters but lacks the structural knowledge required to transform the learngene for initializing Des-Nets. Secondly, existing transformation mechanisms are manually designed (e.g., stacking randomly-initialized layers over learngene layers) but lack a learnable component, as shown in Fig.1(b). Thirdly, existing studies typically focus on deepening well-trained learngene while overlooking the exploration of transforming learngene along another crucial dimension, i.e., the width dimension [28], let alone considering both width and depth dimension. ", "page_idx": 1}, {"type": "text", "text": "Motivated by the above limitations, we propose Learnable Transformation (LeTs), a novel and effective Learngene approach for efficient model initialization, as shown in Fig.1(c). LeTs enables the simultaneous training of a compact learngene module and a set of learnable transformation parameters, with the latter encoding structural knowledge for transforming the former. Specifically, we introduce and train an auxiliary model (Aux-Net) constructed from the compact learngene through a series of learnable transformation matrices $\\textbf{\\emph{T}}$ , comprising a set of width transformation matrices $\\pmb{F}$ and one depth transformation matrix $\\pmb{G}$ . For the width transformation matrices $\\pmb{F}$ , we utilize two series of matrices $\\pmb{F}^{i n}$ and $\\pmb{F}^{o u t}$ which perform in-dimension and out-dimension transformation on learngene matrices respectively. In the case of depth transformation matrix $\\pmb{G}$ , we divide the width-transformed learngene layers into multiple groups and utilize the entry of $\\pmb{G}$ as the coefficient to combine the layers within each group, thereby constructing new layers. During the training of Aux-Net, $\\pmb{F}$ and $\\pmb{G}$ learn to capture structural knowledge about adding neurons and layers respectively. To meet the varying size requirements of target models, we first select specific parameters from the well-trained transformation matrices $\\textbf{\\emph{T}}$ to construct target ones $T^{d e s}$ , guided by strategy such as continuous selection and magnitude-wise selection. Then we employ $T^{d e s}$ to transform learngene for initializing target Des-Nets, which are lastly fine-tuned on different downstream tasks. ", "page_idx": 1}, {"type": "text", "text": "With comprehensive experiments, we show the superiority of LeTs: (1) Compared to 100-epoch from scratch training (Scratch) on ImageNet-1K [29], Des-Nets initialized via LeTs performs better after only 1 epoch tuning. (2) Evaluation performance on ImageNet-1K without any tuning after initialization implies that LeTs significantly enhances initialization quality, e.g., $+3.5\\%$ with DesH12-L12 (86.6M) compared to TLEG [26]. (3) When transferring to downstream image classification tasks, LeTs presents better performance and training efficiency. For example, LeTs outperforms the pre-training and fine-tuning method (Pre-Fin) by $\\mathbf{1.5\\%}$ on CIFAR-100 with Des-H12-L12. Furthermore, within a 300-epoch training schedule on Food-101 [30], LeTs outperforms the final performance of Scratch after about 10 epochs. For semantic segmentation tasks, LeTs outperforms Pre-Fin by $\\mathbf{3.4\\%}$ on ADE20K [31] with Des-H6-L12. (4) Compared to Pre-Fin, LeTs performs better while reducing around $\\mathbf{20\\times}$ initialization parameters when initializing variable-sized models. ", "page_idx": 2}, {"type": "text", "text": "Our main contributions are summarized as follows: (1) We introduce a novel and effective Learngene approach, termed LeTs, for efficient ViT-based model initialization, which is the first to utilize learnable matrices to adaptively transform the compact learngene. (2) We propose to transform learngene along both depth and width dimension for initialization, to our knowledge, has not been explored in the Learngene literature. (3) Comprehensive experiments under various initialization settings demonstrate the efficiency of LeTs. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Parameter Initialization methods have been extensively developed, such as default initialization from Timm library [32], Xavier initialization [7] and Kaiming initialization [9]. A plenty of studies demonstrate that parameter initialization significantly affects the training process and the final model quality [7, 8, 10, 11, 12, 13, 14]. Appropriate initialization facilitates model convergence [33] while improper initialization may hinder the optimization process [10, 11]. Nowadays, the pre-training and fine-tuning method involves transferring model parameters pretrained on large-scale datasets for fine-tuning on specific downstream tasks, during which the model architecture is maintained [34, 35, 18, 36, 37]. However, such method requires transferring the entire pretrained model parameters repeatedly, without considering the varying resource availability of different downstream tasks. Moreover, when a pretrained model of the target size is unavailable, we may firstly pre-train the target model on large-scale datasets. This process is not only time-consuming and computationally expensive, but also requires access to the datasets used for pre-training and lacks the flexibility for initializing variable-sized models. Recently, [38, 39] have focused on transferring large pretrained model parameters to initialize small ones. Besides, Matformer [40] allows for training one universal model which can be used to extract many smaller sub-models. In contrast, we propose to transform one compact learngene with learnable transformation matrices for initialization. ", "page_idx": 2}, {"type": "text", "text": "Learngene is a two-stage framework [24, 25, 26, 27, 41, 42, 43, 44, 45, 46, 47] which firstly learns one compact module, termed learngene, from a large well-trained network called ancestry model (Ans-Net), and then transforms learngene to initialize variable-sized descendant models (DesNet), after which they are fine-tuned normally, as shown in Fig.1(a). Grad-LG [24] selects a few high-level layers as learngene based on the gradient information of Ans-Net, after which they are stacked with randomly-initialized layers to build Des-Nets. TLEG [26] linearly expands two integral learngene layers to initialize variable-depth Des-Nets. SWS [41] extracts learngene in a multi-stage weight sharing fashion and duplicates learngene in its stage during initialization. WAVE [45] trains multiple weight templates as learngenes, enabling efficient initialization for variable-sized models via Kronecker Product. Rather than manually designing learngene transformation strategies, we seek to use learnable matrices, which contain structural knowledge, to transform the compact learngene for initializing variable-sized models. Furthermore, LeTs enables learngene to be transformed along both depth and width dimension, significantly enhancing the transformation flexibility. ", "page_idx": 2}, {"type": "text", "text": "Network Expansion, a prevalent training acceleration framework pioneered by [48], involves incrementally increasing the size of neural networks [49, 50, 51, 52, 53]. Net2Net [48] employs function-preserving expansions to increase the width by copying neurons and the depth by introducing identity layers. Bert2Bert [54] extends this concept by proposing function-preserving width expansion specifically for Transformers. Expansion [53] introduces a width expansion strategy for convolutional neural networks using orthogonal filters, as well as a depth expansion strategy for Transformers through the corresponding exponential moving average model. LiGO [52] learns to linearly map the parameters of a smaller model to initialize a larger one. While LeTs is inspired by these studies [48, 52], it differs in at least two key aspects. In terms of objective, we focus on flexibly initializing variable-sized models from a single compact learngene. Methodologically, such as compared to LiGO [52], we first transform the learngene layers along width dimension, after which we divide the width-transformed learngene layers into multiple groups and linearly combine the layers within each group to construct new layers. Additionally, one crucial aspect of LeTs is that selecting specific parameters from well-trained transformation matrices for subsequent initialization. ", "page_idx": 2}, {"type": "image", "img_path": "7j6xgGj5lF/tmp/0503013fa3f71809e73c76b154e4cb200b55b131a8b2eb8d30a971e3cc8aa9f2.jpg", "img_caption": ["Figure 2: In stage 1, we construct and train an Aux-Net which is transformed from compact learngene layers using a series of learnable transformation matrices. During training, $\\pmb{F}$ and $\\pmb{G}$ learn to capture structural knowledge about how to add new neurons and layers into the compact learngene respectively. In stage 2, given the varying sizes of target Des-Nets, we select specific parameters from well-trained transformation matrices to transform learngene for initialization, which are fine-tuned lastly under different downstream scenarios. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Proposed Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Fig.2 depicts the overall framework of LeTs. In the first stage, we construct an auxiliary model (Aux-Net) comprising compact learngene and a series of learnable transformation matrices. Then we train it by distilling knowledge from a well-trained ancestry model (Ans-Net). In the second stage, given the varying size requirements of target models (e.g., layer numbers), we select specific parameters from well-trained transformation matrices guided by strategy such as continuous selection and magnitude-wise selection. Then we use these selected parameters to transform learngene for initializing Des-Nets, which are fine-tuned lastly. ", "page_idx": 3}, {"type": "text", "text": "3.1 Learnable Transformation for Learngene ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We denote the parameters of the learngene module with $L$ layers as $\\Theta^{l g}=[W_{1},...,W_{L}]^{\\top}$ , where $W_{l}\\,\\in\\,\\mathbb{R}^{d_{i n}\\times d_{o u t}}$ , $d_{i n}$ and $d_{o u t}$ denote the input and output dimension respectively. In the first stage, we design an auxiliary model (Aux-Net) whose parameters are transformed from the compact learngene parameters with a series of learnable transformation matrices $_T$ . We configure a set of width transformation matrices $\\pmb{F}$ and one depth transformation matrix $\\pmb{G}$ for composing $\\textbf{\\emph{T}}$ . Specifically, we enlarge the input and output dimension of learngene matrices with transformation matrices $\\pmb{F}$ containing ${\\pmb F}^{i n}$ and $\\pmb{F}^{o u t}$ . Afterwards, we divide these width-transformed learngene layers into multiple groups and utilize depth transformation matrix $\\boldsymbol{G}$ to combine the layers within each group to construct new layers. In the following, we detail the width transformation matrices $\\pmb{F}$ , depth transformation matrix $\\pmb{G}$ , the construction and training of Aux-Net. ", "page_idx": 3}, {"type": "text", "text": "Width transformation matrices. For each learngene layer $W_{l}$ , we introduce $F_{l}^{i n}$ and $F_{l}^{o u t}$ to perform in-dimension and out-dimension transformation respectively. In particular, we perform in-dimension transformation on $W_{l}$ by multiplying $F_{l}^{i n}$ , after which we insert the transformed part into the original learngene via ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{l}^{'}=C o n c a t(W_{l},F_{l}^{i n}W_{l}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{W}_{l}^{'}$ represents the in-dimension transformed learngene and $C$ oncat represents the concatenation operation. Similarly, we perform out-dimension transformation on $\\boldsymbol{W}_{l}^{'}$ by multiplying $F_{l}^{o u t}$ , ", "page_idx": 3}, {"type": "image", "img_path": "7j6xgGj5lF/tmp/1874266481ddf5b5ca735e584aa1ce1a492156982d657de72ccff1739370bb80.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: (a) Firstly, we perform in-dimension transformation on $W_{l}$ by multiplying $F_{l}^{i n}$ , after which we insert the transformed part into the original learngene. Afterwards, we perform out-dimension transformation on $\\boldsymbol{W}_{l}^{'}$ similarly. (b) After width transformation, we divide these width-transformed learngene layers into groups. Take the $j_{0}$ -th group with two width-transformed learngene layers $(\\boldsymbol{W}_{j_{0},1}^{w\\overline{{t}}}$ and $\\dot{W}_{j_{0},2}^{w t})$ as an example, we construct $\\dot{W_{1}}^{d t}$ via $G_{1,1}W_{j_{0},1}^{w t}+G_{1,2}W_{j_{0},2}^{w t}$ . For simplicity, we omit the original subscripts and superscripts for the entry of all matrices. ", "page_idx": 4}, {"type": "text", "text": "after which we insert the transformed part into $\\boldsymbol{W}_{l}^{'}$ via ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{l}^{w t}=C o n c a t(W_{l}^{'},W_{l}^{'}F_{l}^{o u t\\top}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${W_{l}^{w t}}$ represents the width-transformed learngene. Width transformation process is illustrated in Fig.3(a). We could also first perform out-dimension transformation followed by in-dimension transformation. Besides, we explore performing in-dimension and out-dimension transformation on $W_{l}$ by directly multiplying $\\dot{F}_{l}^{i n}$ and $F_{l}^{o u t}$ via $\\pmb{W}_{l}^{w t}=\\pmb{F}_{l}^{i n}\\pmb{W}_{l}\\pmb{F}_{l}^{o u t\\top}$ , referred to as LeTs(DE). Empirically, we first perform in-dimension and then out-dimension transformation in line with [52]. To keep the parameter efficiency, we share transformation matrices for different model components. ", "page_idx": 4}, {"type": "text", "text": "Depth transformation matrix. After the width transformation, we divide these width-transformed learngene layers into $M$ groups and set the number of width-transformed learngene layers in the $j$ -th group as $L_{j}$ where $j=1,...,M$ . We denote the parameter matrices of the $k$ -th width-transformed learngene layer in the $j$ -th group as W j,wkt where k = 1, ..., Lj. Then we utilize matrix G to combine the layers within the $j_{0}$ -th group to construct the $i$ -th layer of the target model via ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{i}^{d t}=\\sum_{k=1}^{L_{j_{0}}}G_{i,k}W_{j_{0},k}^{w t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${W}_{i}^{d t}$ represents the $i$ -th depth-transformed target layer and $G_{i,k}$ represents the $(i,k)$ -th entry. During the new layer constructions, we repeatedly select some learngene groups, as discussed in Sec.4.3. The process of depth transformation is illustrated in Fig.3(b). Moreover, we adopt parameter sharing strategy between rows of $\\boldsymbol{G}$ to provide guidance for initializing descendant models. ", "page_idx": 4}, {"type": "text", "text": "Construction and Training of Aux-Net. We construct the Aux-Net from the learngene module using width and depth transformation matrices. Then we train the Aux-Net via prediction-based distillation [55] for simplicity to distill knowledge from the Ans-Net. This involves minimizing the cross-entropy loss between the probability distributions of output predictions from both the Ans-Net and the Aux-Net similar to previous studies [26, 27]. Specifically, we introduce one distillation loss $\\mathcal{L}_{d i s t i l l}=C E(\\phi(\\pmb{r}_{s}/\\tau),\\phi(\\pmb{r}_{t}/\\tau))$ , where $\\pmb{r}_{s}$ and $\\pmb{r}_{t}$ represent the logits of the Aux-Net and those of the pretrained Ans-Net respectively, $C E(\\cdot,\\cdot)$ represents soft cross-entropy loss, $\\tau$ represents the temperature for distillation and $\\phi$ represents the softmax function. Additionally, we can seamlessly integrate advanced distillation techniques [56, 57] into our training. Besides, we also introduce one classification loss $\\mathcal{L}_{c l s}\\,=\\,C E(\\phi(r_{s}),y)$ , where $y$ represents the ground-truth label. Overall, our training loss is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a l l}=(1-\\lambda)\\mathcal{L}_{c l s}+\\lambda\\mathcal{L}_{d i s t i l l},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda$ represents the trade-off coefficient. Noteworthy, we train the Aux-Net to obtain well-trained learngene parameters and a set of transformation matrices. During training, the width transformation matrices and depth transformation matrix capture structural knowledge about how to add new neurons and layers respectively, preparing for the subsequent initialization. Next, we elaborate how to use these well-trained transformation matrices to adapt learngene for initializing variable-sized models. ", "page_idx": 4}, {"type": "text", "text": "3.2 Initialization from Learngene with Well-trained Transformation Matrices ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Different from previous manual and depth-only transformation, LeTs enables learngene to be deepened and widened with well-trained transformation matrices. To meet the diverse size requirements of Des-Nets (e.g., layer numbers), we select specific parameters from well-trained transformation matrices to construct target ones. Specifically, we design several parameter selection strategies including continuous selection, magnitude-wise selection and sequential selection. For width transformation matrices across different model components, we maintain the consistency of selection position to preserve the integrity of the learned neuron connections. After initialization, we fine-tune the descendant models on different downstream tasks without distillation. Next, we firstly introduce the selection strategy for width transformation matrices. ", "page_idx": 5}, {"type": "text", "text": "Continuous selection. For the initialization along width, we propose selecting continuous rows from $\\pmb{F}^{i n}$ and $\\pmb{F}^{o u t}$ to form the target transformation matrices. Notably, consistency is preserved throughout the row selection process for different model components to maintain connectivity between neurons. Empirically, we default to selecting the first n rows from F in and F out. ", "page_idx": 5}, {"type": "text", "text": "Magnitude-wise selection. Parameter magnitude serves as an effective metric for assessing importance in model pruning, where the significance of one weight is determined by its magnitude [58, 59, 60]. In our case, we adapt this metric for parameter selection from width transformation matrices. Take $\\pmb{F}^{i n}$ as an example, we propose selecting $n$ rows whose $L_{1}$ -norm or $L_{2}$ -norm ranks top- $^{\\cdot n}$ , abbreviated as top- $\\cdot n$ $L_{1}/L_{2}$ -norm selection. Intuitively, this selection strategy enables us to prioritize parameters that contribute most significantly to the model performance, ensuring that the most impactful connections are preserved. By concentrating on the most critical parameters, LeTs enhances the initialization quality of different Des-Nets. Empirically, we demonstrate its effectiveness by comparing it with bottom- $^{n}$ $L_{1}/L_{2}$ -norm selection which select $n$ rows whose $L_{1}$ -norm or $L_{2}$ -norm ranks bottom- $n$ , as discussed in Sec.4.3. ", "page_idx": 5}, {"type": "text", "text": "For the depth transformation matrix, we introduce the sequential selection strategy. ", "page_idx": 5}, {"type": "text", "text": "Sequential selection. We propose to sequentially select learngene groups in a predefined order, where different orders emphasize distinct groups. Each learngene group corresponds to several coefficient groups (rows of $\\boldsymbol{G}$ ). While selecting an approximately equal number of coefficient groups for each learngene group, we allocate more coefficient groups for shallower learngene group. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We perform our main experiments on ImageNet-1K [29], several downstream image classification datasets including CIFAR-10, CIFAR-100 [61], Food-101 [30] and Cars-196 [62], and several semantic segmentation datasets including ADE20K [31], Pascal Context [63] and Cityscapes [64]. We report Top-1 classification accuracy (Top- $1(\\%))$ for classification tasks, and Intersection over Union $(\\mathrm{mIoU}(\\%))$ ) averaged over all classes for segmentation tasks following [65]. Besides, we also report Params(M) and $\\mathrm{FLOPs}(\\mathrm{G})$ as the number of model parameters and indicators of theoretical complexity of model. In the first stage, we configure the learngene module with 6 heads (head dimension is 64) and 8 layers whose number of parameters is 15M, and transform it to construct Aux-Net with 12 heads and 16 layers based on DeiT [66]. Then we train the Aux-Net on ImageNet1K for 300 epochs to obtain learngene and transformation matrices. We choose Levit-384 [67] as the ancestry model. In the second stage, we set several variants of Des-Net where we change the layer and head numbers based on DeiT, e.g., we name the Des-Net with 12 heads and 12 layers as Des-H12-L12. Please see more details in A.3. ", "page_idx": 5}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Compared to training from scratch on ImageNet-1K, LeTs performs better while reducing large training costs. We compare LeTs with: (1) Scratch that training models of variable widths and depths from scratch for 100 epochs; (2) TLEG [26] that linearly expands learngene to initialize models and finetunes them for 40 epochs; (3) SWS [41] that duplicates learngene in its pre-defined stage to initialize models and finetunes them for 10 epochs. As shown in Fig.4 and Tab.5, compared to Scratch, LeTs performs better only after 1 epoch tuning, which reduces around $\\mathbf{7\\times}$ total training (m) Des-H10-L12 (60.3M)(n) Des-H10-L14 (70.2M) (o) Des-H10-L16 (80.0M) (p) Des-H12-L8 (58.2M) ", "page_idx": 5}, {"type": "image", "img_path": "7j6xgGj5lF/tmp/6f6810797a526a4bd9282e7a500261e21de9cc078c7c3f1dc2d02d999e157f52.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "7j6xgGj5lF/tmp/f96244325a8c410b6b674afa691d7dfb7e434191b254f5a0397e5c6924957298.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "7j6xgGj5lF/tmp/949048d3a5670db6cf0926c4f9a0a7a92b0387ee71ab90076b66fb69635dd1b0.jpg", "img_caption": ["(q) Des-H12-L10 (72.4M) (r) Des-H12-L12 (86.6M) (s) Des-H12-L14 (100.7M)(t) Des-H12-L16 (114.9M) "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Performance comparisons on ImageNet-1K. Number in bracket represents Params(M). ", "page_idx": 6}, {"type": "text", "text": "costs for 24 models. Compared to TLEG and SWS, LeTs also demonstrates superior performance after just 1 epoch tuning in most cases, which highlights the effectiveness of introducing learnable transformation matrices. Notably, the efficiency of LeTs becomes increasingly obvious as the number of Des-Nets grows, since we only need to train the learngene and transformation matrices once for most Des-Nets. Moreover, LeTs could flexibly initialize variable-sized models that are independent of the size of learngene and Aux-Net, as shown in Fig.5. Please see more detailed results in Tab.5. ", "page_idx": 6}, {"type": "text", "text": "LeTs exhibits better performance and training efficiency when transferring to downstream datasets. We compare LeTs with: (1) Pre-Fin that pre-training on ImageNet-1K and fine-tuning on downstream datasets; (2) Scratch; (3) Grad-LG [24]; (4) TLEG [26]; (5) SWS [41]. As shown in Fig.6(a)-(h), LeTs consistently outperforms these baselines on downstream image classification datasets, demonstrating the effectiveness of leveraging well-trained transformation matrices for model initialization. Take Des-H6-L12 as an example, LeTs outperforms Pre-Fin by $\\mathbf{2.3\\%}$ , $\\mathbf{1.9\\%}$ and $\\mathbf{3.5\\%}$ on CIFAR-100, Food-101 and Cars-196, respectively. Notably from Fig.7, we observe that LeTs outperforms the final performance of Scratch after about 10 epochs within a 300-epoch training ", "page_idx": 6}, {"type": "image", "img_path": "7j6xgGj5lF/tmp/af0d5986c95be338f3fd71634b8c20e6e789030192a42ab582d8769593cd9247.jpg", "img_caption": ["Figure 5: LeTs could flexibly initialize variable-sized models that are independent of the size of learngene and Aux-Net. Compared with Scratch and MatFormer [40], LeTs demonstrates more initialization efficiency. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "7j6xgGj5lF/tmp/b7d68478e3a13ce79ab9b5ac1a8deb75686658b0ed4280f532fb4ed03324fc68.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 6: Performance of (a)-(d): Des-H6-L12 and (e)-(h): Des-H12-L12 on downstream image classification datasets. We report the results of LeTs under the linear probing (LP) protocol in (l). Besides, we evaluate LeTs on semantic segmentation tasks in (i)- $\\langle\\mathbf{k}\\rangle$ , where we set two variants of LeTs as LeTs(0ep) and LeTs(5ep). LeTs(0ep) represents that initializing the backbone with learngene and LeTs(5ep) represents that further fine-tuning on ImageNet-1K for 5 epochs after initialization. ", "page_idx": 7}, {"type": "text", "text": "schedule on Food-101 [30], which is about $30\\times$ faster. We also report the results of LeTs under the linear probing protocol in Fig.6(l). Moreover, we report the results of LeTs on downstream semantic segmentation tasks. Specifically, we follow the training and model setting provided in [65], where we adopt Des-H6-L12 and Des-H12-L12 as the backbone and mask transformer as the decoder. As shown in Fig.6(i)-(k), LeTs(0ep) outperforms Pre-Fin by $\\mathbf{3.4\\%}$ , $\\mathbf{1.7\\%}$ and $\\mathbf{1.6\\%}$ on ADE20K, Pascal Context and Cityscapes respectively with Des-H6-L12 as the backbone. Please see more details in A.3. ", "page_idx": 7}, {"type": "text", "text": "Results of evaluation on ImageNet-1K without any tuning after initialization implies that LeTs greatly enhances initialization quality. To validate the initialization quality, we compare LeTs with (1) Scratch; (2) Grad-LG [24]; (3) TLEG [26]; (4) SWS [41]; (5) IMwLM [39]. In Tab.1, LeTs outperforms all baselines by a large margin in most cases. For example, LeTs outperforms TLEG by $\\mathbf{4.3\\%}$ , $3.0\\%$ and $\\mathbf{3.5\\%}$ on Des-H12-L8, Des-H12-L10 and Des-H12-L12. Importantly, LeTs can also achieve comparable performance with well-trained models (Scratch). For instance, LeTs outperforms Scratch by $\\mathbf{0.5\\%}$ and $\\mathbf{0.5\\%}$ on Des-H12-L10 and Des-H12-L12. The above results imply that LeTs provides effective initialization for variable-sized models. ", "page_idx": 7}, {"type": "table", "img_path": "7j6xgGj5lF/tmp/ab7f72bdf6f156aa79e8ebea2e699a54bbbb6e62270f9b96f9fa03ec422d9c12.jpg", "table_caption": [], "table_footnote": ["Table 1: Direct evaluation performance on ImageNet-1K without any tuning after initialization. "], "page_idx": 8}, {"type": "table", "img_path": "7j6xgGj5lF/tmp/6420cf545a59a9143ed87b64f9c02d43fb2222e950e72a5d683b75ecaab9ab7f.jpg", "table_caption": [], "table_footnote": ["Table 2: Performance comparisons on CIFAR-100 of variable-sized Des-Nets. Pre-Fin transfers the pretrained parameters (S-P(M)) to initialize, which totally requires about 758M for 12 Des-Nets. LeTs only needs to store $37.7\\mathbf{M}$ parameters (15.0M learngene) to initialize, which significantly reduces the parameters stored for initialization by $20\\times$ (758M vs. 37.7M). "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Compared to Pre-Fin, LeTs significantly reduces the pretraining efforts and initialization parameters when initializing variable-sized models for downstream tasks. As shown in Tab.2, LeTs demonstrates superior performance while reducing around $\\mathbf{3}\\times$ pre-training costs and decreasing around $\\mathbf{20\\times}$ (758M vs. 37.7M) initialization parameters, as compared to Pre-Fin. Furthermore, LeTs only needs to train learngene and transformation matrices once, whereas Pre-Fin requires individual pre-training for each Des-Net. Clearly, the efficiency gains of LeTs become more pronounced with an increasing number of Des-Nets for different downstream tasks. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation and Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Selection strategies. We evaluate the performance of LeTs (Des-H8-L12, 5 epochs tuning) using various selection strategies. As shown in Tab.3, we observe that continuous selection achieves slight better performance than magnitude-wise selection. Moreover, selecting $n$ rows whose $L_{1}$ -norm or $L_{2}$ -norm ranks top- $^{\\cdot n}$ (top- ${\\mathbf{\\nabla}}n$ $L_{1}/L_{2}$ -norm) is better than those whose $L_{1}$ -norm or $L_{2}$ -norm ranks bottom- ${\\cdot n}$ (bottom- ${\\cdot n}$ $L_{1}/L_{2}$ -norm). Besides, we observe that selecting different learngene groups for initialization achieves similar performance, where selecting more coefficient groups for shallower learngene groups performs better than else. ", "page_idx": 8}, {"type": "text", "text": "Width and depth transformation. We investigate the effectiveness of our proposed width and depth transformation by comparing LeTs with one state-of-the-art expansion method LiGO [52]. Specifically, we adopt the expansion strategy proposed in LiGO into our two-stage initialization pipeline and use our proposed selection strategy for initializing different Des-Nets, referred to as LeTs (LiGO). From Tab.4, we observe that LeTs outperforms LeTs (LiGO) in most cases, which indicates that our proposed transformation pipeline are more suitable for initializing variable-sized models. Moreover, we explore transforming the learngene matrices directly but not insert the transformed matrices into the learngene, referred to as (LeTs(DE)). In addition, we also explore the role of the parameter sharing strategy adopted on the rows of $\\pmb{G}$ (LeTs w/o ws). From Tab.4, we observe that LeTs(DE) and LeTs(w/o ws) still enhances the initialization efficiency but is inferior to our complete version. ", "page_idx": 8}, {"type": "table", "img_path": "7j6xgGj5lF/tmp/9cd2621e2ac4c5ab763c4bd5fe2938ab8a2c23c482d79c1e8ade7b1e5381525f.jpg", "table_caption": [], "table_footnote": ["Table 3: Performance on ImageNet-1K when using different selection strategies, selecting different learngene groups and initializing Des-Nets without certain components. "], "page_idx": 9}, {"type": "table", "img_path": "7j6xgGj5lF/tmp/455330adccd1976f6e711d455dc94d47af2a311311f4256de4009ab3ae7a0aca.jpg", "table_caption": [], "table_footnote": ["Table 4: Performance on ImageNet-1K when using depth and width expansion strategies proposed in LiGO [52] (named as LeTs(LiGO)), training smaller learngene module (named as LeTs(11.4M)), direct transforming learngene matrices to compose target ones (named as LeTs(DE)), not sharing weights between rows of $\\boldsymbol{G}$ (named as LeTs(w/o ws)), training Aux-Net for 200 epochs (named as LeTs(200ep)) and not adopting distillation in the first stage (LeTs(w/o dis)). "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Initialization components. We investigate the performance of LeTs (Des-H8-L12, 5 epochs tuning) by excluding one of the following components within ViT models: Patch Embedding (PE), Multihead Self-Attention (MSA), Multi-Layer Perception (MLP), Layer Normalization (LN) or Position Embedding (Pos). From Tab.3, we find that omitting MSA or MLP from initialization results in significant performance degradation and initializing all components is necessary. ", "page_idx": 9}, {"type": "text", "text": "Size and training setting of learngene and transformation matrices. We evaluate the performance of Des-Nets initialized from well-trained learngene modules under different training settings. Specifically, we train a smaller learngene module with 11.4M parameters (LeTs(11.4M)), shorten the training epochs of Aux-Net to 200 (LeTs(200ep)) or train the Aux-Net without distilling from the Ans-Net (LeTs(w/o dis)). From Tab.4, we observe that LeTs(11.4M), LeTs(200ep) and LeTs(w/o dis) still enhances the initialization effectiveness but is inferior to our complete version. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed a well-motivated and highly effective Learngene approach termed LeTs where we transform the learngene module with a set of learnable transformation matrices for variablesized model initialization, enabling adaptation to diverse resource constraints. LeTs is the first to adopt learnable matrices to transform the compact learngene along both the depth and width dimension, which significantly enhances the flexibility of learngene for model initialization. We demonstrated the efficiency of LeTs under various initialization settings empirically. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research is supported by the National Science Foundation of China (62125602, 62076063), Key Program of Jiangsu Science Foundation (BK20243012), the Fundamental Research Funds for the Central Universities(2242024k30035) and the Big Data Computing Center of Southeast University. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with hilo attention. Advances in Neural Information Processing Systems, 35:14541\u201314554, 2022.   \n[2] Changlong Jiang, Yang Xiao, Cunlin Wu, Mingyang Zhang, Jinghong Zheng, Zhiguo Cao, and Joey Tianyi Zhou. A2j-transformer: Anchor-to-joint transformer network for 3d interacting hand pose estimation from a single rgb image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8846\u20138855, 2023.   \n[3] Shaoru Wang, Jin Gao, Zeming Li, Xiaoqin Zhang, and Weiming Hu. A closer look at selfsupervised lightweight vision transformers. In International Conference on Machine Learning, pages 35624\u201335641. PMLR, 2023.   \n[4] Haoyu Xie, Changqi Wang, Jian Zhao, Yang Liu, Jun Dan, Chong Fu, and Baigui Sun. Prcl: Probabilistic representation contrastive learning for semi-supervised semantic segmentation. International Journal of Computer Vision, pages 1\u201319, 2024.   \n[5] Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng, Joey Tianyi Zhou, and Peng Hu. Noisycorrespondence learning for text-to-image person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27197\u201327206, 2024.   \n[6] Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, and Jianfei Cai. Generative region-language pretraining for open-ended object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13958\u201313968, 2024.   \n[7] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.   \n[8] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814, 2010.   \n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.   \n[10] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.   \n[11] Devansh Arpit, V\u00edctor Campos, and Yoshua Bengio. How to initialize your network? robust initialization for weightnorm & resnets. Advances in Neural Information Processing Systems, 32, 2019.   \n[12] Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs. Improving transformer optimization through better initialization. In International Conference on Machine Learning, pages 4475\u20134483. PMLR, 2020.   \n[13] Oscar Chang, Lampros Flokas, and Hod Lipson. Principled weight initialization for hypernetworks. arXiv preprint arXiv:2312.08399, 2023.   \n[14] Emily Dinan, Sho Yaida, and Susan Zhang. Effective theory of transformers at initialization. arXiv preprint arXiv:2304.02034, 2023.   \n[15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[17] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[18] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[19] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual parameter-efficient fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11825\u201311835, 2023.   \n[20] Yi-Kai Zhang, Shiyin Lu, Yang Li, Yanqing Ma, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, and Han-Jia Ye. Wings: Learning multimodal llms without text-only forgetting. arXiv preprint arXiv:2406.03496, 2024.   \n[21] Chao Yi, De-Chuan Zhan, and Han-Jia Ye. Bridge the modality and capacity gaps in visionlanguage model selection. arXiv preprint arXiv:2403.13797, 2024.   \n[22] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.   \n[24] Qiu-Feng Wang, Xin Geng, Shu-Xia Lin, Shi-Yu Xia, Lei Qi, and Ning Xu. Learngene: From open-world to your learning task. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8557\u20138565, 2022.   \n[25] Qiufeng Wang, Xu Yang, Shuxia Lin, and Xin Geng. Learngene: Inheriting condensed knowledge from the ancestry model to descendant models. arXiv preprint arXiv:2305.02279, 2023.   \n[26] Shiyu Xia, Miaosen Zhang, Xu Yang, Ruiming Chen, Haokun Chen, and Xin Geng. Transformer as linear expansion of learngene. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 16014\u201316022, 2024.   \n[27] Boyu Shi, Shiyu Xia, Xu Yang, Haokun Chen, Zhiqiang Kou, and Xin Geng. Building variable-sized models via learngene pool. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 14946\u201314954, 2024.   \n[28] Lorenzo Noci, Chuning Li, Mufan Li, Bobby He, Thomas Hofmann, Chris J Maddison, and Dan Roy. The shaped transformer: Attention models in the infinite depth-and-width limit. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[30] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014.   \n[31] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302\u2013321, 2019.   \n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Proceedings of the 33th Annual Conference on Neural Information Processing Systems, pages 8024\u20138035, 2019.   \n[33] Yann LeCun, L\u00e9on Bottou, Genevieve B Orr, and Klaus-Robert M\u00fcller. Efficient backprop. In Neural networks: Tricks of the trade, pages 9\u201350. Springer, 2002.   \n[34] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n[35] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.   \n[36] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220\u2013235, 2023.   \n[37] Hengcan Shi, Son Duy Dao, and Jianfei Cai. Llmformer: Large language model for openvocabulary semantic segmentation. International Journal of Computer Vision, pages 1\u201318, 2024.   \n[38] Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Fartash Faghri, Devang Naik, Oncel Tuzel, and Mohammad Rastegari. Weight subcloning: direct initialization of transformers using larger pretrained ones. arXiv preprint arXiv:2312.09299, 2023.   \n[39] Zhiqiu Xu, Yanjie Chen, Kirill Vishniakov, Yida Yin, Zhiqiang Shen, Trevor Darrell, Lingjie Liu, and Zhuang Liu. Initializing models with larger ones. In International Conference on Learning Representations (ICLR), 2024.   \n[40] Fnu Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit S Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham M Kakade, Ali Farhadi, et al. Matformer: Nested transformer for elastic inference. In Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023), 2023.   \n[41] Shi-Yu Xia, Wenxuan Zhu, Xu Yang, and Xin Geng. Exploring learngene via stage-wise weight sharing for initializing variable-sized models. arXiv preprint arXiv:2404.16897, 2024.   \n[42] Qiufeng Wang, Xu Yang, Haokun Chen, and Xin Geng. Vision transformers as probabilistic expansion from learngene. In Forty-first International Conference on Machine Learning, 2024.   \n[43] Fu Feng, Jing Wang, Congzhi Zhang, Wenqian Li, Xu Yang, and Xin Geng. Genes in intelligent agents. arXiv preprint arXiv:2306.10225, 2023.   \n[44] Fu Feng, Jing Wang, and Xin Geng. Transferring core knowledge via learngenes. arXiv preprint arXiv:2401.08139, 2024.   \n[45] Fu Feng, Yucheng Xie, Jing Wang, and Xin Geng. Wave: Weight template for adaptive initialization of variable-sized models. arXiv preprint arXiv:2406.17503, 2024.   \n[46] Yucheng Xie, Fu Feng, Jing Wang, Xin Geng, and Yong Rui. Kind: Knowledge integration and diversion in diffusion models. arXiv preprint arXiv:2408.07337, 2024.   \n[47] Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, and Xin Geng. Fine: Factorizing knowledge for initialization of variable-sized diffusion models. arXiv preprint arXiv:2409.19289, 2024.   \n[48] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015.   \n[49] Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. On the transformer growth for progressive bert training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5174\u20135180, 2021.   \n[50] Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for transformer language models. In International Conference on Machine Learning, pages 19893\u201319908. PMLR, 2022.   \n[51] Peihao Wang, Rameswar Panda, and Zhangyang Wang. Data efficient neural scaling law via model reusing. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 36193\u201336204. PMLR, 23\u201329 Jul 2023.   \n[52] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. International Conference on Learning Representations, 2023.   \n[53] Ning Ding, Yehui Tang, Kai Han, Chao Xu, and Yunhe Wang. Network expansion for practical training acceleration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20269\u201320279, 2023.   \n[54] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2134\u20132148, 2022.   \n[55] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[56] Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Minivit: Compressing vision transformers with weight multiplexing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12145\u201312154, 2022.   \n[57] Sucheng Ren, Fangyun Wei, Zheng Zhang, and Han Hu. Tinymim: An empirical study of distilling mim pre-trained models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3687\u20133697, 2023.   \n[58] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017.   \n[59] Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural network pruning. arXiv preprint arXiv:2003.02389, 2020.   \n[60] Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight importance. In International Conference on Machine Learning, pages 26809\u201326823. PMLR, 2022.   \n[61] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technique Report, 2009.   \n[62] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.   \n[63] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 891\u2013898, 2014.   \n[64] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213\u20133223, 2016.   \n[65] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7262\u20137272, 2021.   \n[66] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.   \n[67] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv\u00e9 J\u00e9gou, and Matthijs Douze. Levit: a vision transformer in convnet\u2019s clothing for faster inference. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12259\u2013 12269, 2021.   \n[68] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2018.   \n[69] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. EMNLP, 2020.   \n[70] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776\u20135788, 2020.   \n[71] Shiming Chen, Ziming Hong, Guo-Sen Xie, Wenhan Yang, Qinmu Peng, Kai Wang, Jian Zhao, and Xinge You. Msdn: Mutually semantic distillation network for zero-shot learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7612\u20137621, 2022.   \n[72] Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fast pretraining distillation for small vision transformers. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXI, pages 68\u201385. Springer, 2022.   \n[73] Yutong Bai, Zeyu Wang, Junfei Xiao, Chen Wei, Huiyu Wang, Alan L Yuille, Yuyin Zhou, and Cihang Xie. Masked autoencoders enable efficient knowledge distillers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24256\u201324265, 2023.   \n[74] Thanh Nguyen-Duc, Trung Le, He Zhao, Jianfei Cai, and Dinh Phung. Adversarial local distribution regularization for knowledge distillation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 4681\u20134690, 2023.   \n[75] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024.   \n[76] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the appendix, we present more details about the proposed LeTs for variable-sized ViT-based model initialization in this paper, including: ", "page_idx": 15}, {"type": "text", "text": "\u2022 In Subsection A.1, we present how to adopt weight sharing within width transformation matrices and depth transformation matrix for LeTs.   \n\u2022 In Subsection A.2, we present the comparison between LeTs and model expansion.   \n\u2022 In Subsection A.3, we provide more experimental details.   \n\u2022 In Subsection A.4, we analyze that LeTs can significantly enhance training efficiency when transferring to downstream datasets.   \n\u2022 In Subsection A.5, we discuss the comparison between LeTs and knowledge distillation.   \n\u2022 In Subsection A.6, we provide the comparison between LeTs and online models.   \n\u2022 In Subsection A.7, we give the comparison between LeTs and meta learning.   \n\u2022 In Subsection A.8, we discuss the limitations and future work of this paper.   \n\u2022 In Subsection A.9, we present the broader impacts of this paper. ", "page_idx": 15}, {"type": "text", "text": "A.1 Adopt Weight Sharing within Transformation Matrices for LeTs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To keep the parameter efficiency for transformation matrices, we propose to adopt weight sharing on width transformation matrices and depth transformation matrix inspired by [52]. However, with the \u201cfirst-width-then-depth\u201d transformation pipeline, we can maintain a more compact transferred part compared to [52], as we only need to add width transformation matrices to the shallower learngene modules, rather than to the deeper Aux-Net. We focus on leveraging LeTs for ViT models [66], which mainly contains the following weight components: patch embedding, layer norm (LN), multi-head self-attention (MHA) and multi-layer perception (MLP). ", "page_idx": 15}, {"type": "text", "text": "Width transformation matrices. For patch embedding, we adopt a learnable matrice $F^{o u t,e m b}$ . For MHA $\\mathbf{\\nabla}[W_{l}^{k}$ , $k\\,\\in\\,\\{Q,K,V,O\\})$ , we adopt learnable matrices $\\pmb{F}_{l}^{i n,k}$ and F out,k where $k\\ \\in$ {Q, K, V, O}. Here, we set F lin,O $\\pmb{F}_{l}^{i n,O}=\\pmb{F}_{l}^{o u t,V}$ , $F_{l}^{o u t,O}=F^{o u t,e m b}$ , and $F_{l}^{i n,k}=F^{o u t,e m b}$ where $k\\in\\{Q,K,V,O\\}$ . For LN, the learnable matrices are the associated out-dimension transformation matrices. For MLP $(\\boldsymbol{W_{l}^{f c1}},\\boldsymbol{W_{l}^{f c2}})$ , we adopt learnable matrices $\\boldsymbol{F}_{l}^{i n,k}$ and $F_{l}^{o u t,k}\\left(k\\in\\{f c1,f c2\\}\\right)$ for the first and second MLP layer. Similarly, we set $\\pmb{F}_{l}^{i n,f c2}=F_{l}^{o u t,f c1}$ , $F_{l}^{o u t,f c2}=F^{o u t,e m b}$ and $F_{l}^{i n,f c1}=F^{o u t,e m b}$ . Learnable matrices for bias are shared in the same way as those for weights. ", "page_idx": 15}, {"type": "text", "text": "Depth transformation matrix. For depth transformation matrix $\\pmb{G}$ , we propose to share the parameters within different rows of $\\pmb{G}$ . Such sharing strategy provides depth expansion guidance for transforming learngene along depth dimension. ", "page_idx": 15}, {"type": "text", "text": "A.2 Comparison between LeTs and Model Expansion ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Model Expansion, a widely adopted training acceleration framework pioneered by [48], involves the incremental enlargement of neural networks [49, 50, 52, 53]. Net2Net [48] utilizes functionpreserving transformation to expand width by duplicating neurons and depth by incorporating identity layers. Bert2Bert [54] builds on this concept by proposing function-preserving width expansion specifically for Transformers. Staged-training [50] achieves width expansion by doubling the dimensions of all matrices and depth expansion through zero-initializing normalization parameters. Expansion [53] introduces a width expansion strategy for convolutional neural networks using orthogonal filters, and a depth expansion strategy for Transformers based on the corresponding exponential moving average model. LiGO [52] learns to linearly map the parameters of a smaller model to initialize a larger one. ", "page_idx": 15}, {"type": "text", "text": "While LeTs is inspired by these studies, it differs in several key aspects: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Objective: Our goal is on flexibly initializing variable-sized models from a single compact learngene, rather than solely accelerating the training of models larger than learngene. ", "page_idx": 15}, {"type": "table", "img_path": "7j6xgGj5lF/tmp/ddd2663ce1d5e6b331b964743fb02c9ce2cb1b05df36715baea9f4006a8e1c76.jpg", "table_caption": ["Table 5: The numerical results for Fig.4 and Fig.5 in our original paper. The number of epochs is indicated in brackets within the \u201cScratch\u201d column, with the default being 100 epochs when no brackets are present. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "\u2022 Overall pipeline: Taking one state-of-the-art expansion method LiGO [52] as an example, LiGO firstly combines all layers of a smaller model to reconstruct each layer of a larger model along depth dimension, subsequently equipping each reconstructed layer with width expansion matrices. In contrast, we first transform the learngene layers along the width dimension and then partition the width-transformed layers into multiple groups, subsequently combining the layers within each group to construct new layers. With the \u201cfirst-width-thendepth\u201d transformation pipeline, we can maintain a compact transferred part, as we only need to add width transformation matrices to the shallower learngene modules, rather than to the deeper Aux-Net. In contrast, research on model expansion does not prioritize keeping the transferred part compact, as their objective is not to transfer fewer parameters for initializing models of varying sizes. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Parameter selection: Another crucial aspect of LeTs involves selecting specific parameters from well-trained transformation matrices for subsequent initialization. ", "page_idx": 16}, {"type": "text", "text": "A.3 Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we describe the experimental details of main results, present numerical results and model settings for Fig.4, Fig.5, Fig.6, Tab.1, Tab.2 and Tab.4 in our original paper. ", "page_idx": 16}, {"type": "text", "text": "Training Details for Aux-Net. In general, we follow the training setting and hyperparameters provided in DeiT [66]. We make several modifications on DeiT: (1) We remove the [class] token; (2) We attach the model with a global average pooling layer and a fully-connected layer for image classification. We train Aux-H12-L16 for 300 epochs with 5 warm-up epochs on ImageNet-1K. We choose LeViT-384 [67] as the ancestry model to employ distillation. Specifically, we use the AdamW [68] optimizer with weight decay 0.05 and a cosine scheduler, where batch size is set to 128 and $\\lambda$ is set to 1.0. All models are implemented by PyTorch [32], and trained on NVIDIA Tesla V100 ", "page_idx": 16}, {"type": "image", "img_path": "7j6xgGj5lF/tmp/0d1feb42bafa0eed7629ac7d0b64f868247ef413f67f634d917507ac0f0150db.jpg", "img_caption": ["Figure 7: Compared to Scratch, LeTs could significantly enhance training efficiency. Take Des-H12- L12 on Food-101 as an example, LeTs outperforms Scratch after 10 epoch tuning, which is about $30\\times$ faster. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "7j6xgGj5lF/tmp/4b64d134cf4fc8f9d153f8589b4b8fb0933b13a56904751de12c0de7ead3f9a1.jpg", "table_caption": ["Table 6: The numerical results for Fig.6(a)-(h) in our original paper. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "GPUs and NVIDIA GeForce RTX 3090 GPUs. And GPU hours are approximately measured based on Tesla V100 GPUs and RTX 3090 GPUs. ", "page_idx": 17}, {"type": "text", "text": "Experimental Details, Model Settings and Numerical Results for Fig.4 and Fig.5 in Our Original Paper. We report the numerical results of Fig.4 and Fig.5 in Tab.5. For Scratch, we train all the DesNets for 100 epochs (some for more than 100 epochs) with 5 warm-up epochs on ImageNet-1K [29] with timm default initialization [32], where we use the AdamW [68] optimizer with weight decay 0.05, batch size 128 and a cosine scheduler following [66]. For TLEG, we follow the experimental details and results of [26]. For SWS, we follow the experimental details and results of [41]. For Matformer, we refer to the results presented in Figure 4(a) of their original paper [40]. For LeTs, we initialize Des-Nets with continuous selection and fine-tune them for 5 epochs. We use the AdamW optimizer with weight decay 0.05, batch size 128 and a cosine scheduler for all Des-Nets. We also inherit the parameters of patch projection and classification head to initialize Des-Nets. In comparison to Scratch, we calculate GPU hours for training these 24 Des-Nets and find that LeTs significantly reduces total training GPU hours by approximately $\\mathbf{7\\times}$ (6K GPU hours for Scratch versus 0.8K GPU hours for LeTs). For Des-H16-L24, we construct and train one Aux-Net with 16 heads and 24 layers from Des-H12-L16 (initialized) for 5 epochs to obtain the well-trained transformation matrices, after which we use the Aux-Net to initialize the Des-H16-L24. When training on ImageNet-1K, Des-H16-L24 underperforms compared to Des-H12-L12, where the relatively small number of training samples in ImageNet-1K may be the cause. ", "page_idx": 17}, {"type": "table", "img_path": "7j6xgGj5lF/tmp/419851d94dafdcb94e03b6a753a23012b869c029e943c9416187b9d16cd42467.jpg", "table_caption": ["Table 7: The numerical results for Fig.6(i)- $(\\mathrm{k})$ in our original paper. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Experimental Details, Model Settings and Numerical Results for Fig.6 in Our Original Paper. We report the numerical results of Fig.6 in Tab.6 and Tab.7. For all methods and all downstream image classification datasets in Fig.6(a)-(h), we fine-tune the initialized Des-H6-L12 and Des-H12-L12 for 300 epochs. We use the AdamW [68] optimizer with batch size 128 or 256, and a cosine scheduler. For Cars-196 [62], Scratch fails to converge, where the relatively small number of training samples may be the cause. For baseline methods, we use the hyperparameter settings similar to those of LeTs. For all semantic segmentation datasets in Fig.6(i)- $\\left(\\mathbf{k}\\right)$ , we follow the training setting of [65]. ", "page_idx": 18}, {"type": "text", "text": "Experimental Details and Model Settings for Tab.1, Tab.2 and Tab.4 in Our Original Paper. For downstream transfer experiments, we acknowledge the necessity of retraining a specific task head when transferring well-trained parameters from task A to task $B$ . However, in Tab.1, both task A and task $B$ for all baselines and LeTs involve ImageNet-1K. Therefore, we also inherit the classification head parameters from either the first stage or the pre-training stage to initialize the Des-Nets for ImageNet-1K. For IMwLM [39], we use one 300ep-pretrained model with 12 heads and 18 layers (129.1M) to initialize models of different depths. Specifically, we use the consecutive selection strategy and select the first $N$ layers to initialize target models as introduced in IMwLM [39], where $N$ represents the layer number of target models. In Tab.2, for Pre-Fin, we use the models pretrained on ImageNet-1K to finetune on CIFAR-100. In Tab.4, about LeTs(LiGO), we firstly train a small model with 6 heads and 8 layers for 300 epochs. Then we use depth and width transformation strategy proposed in LiGO to construct and train the Aux-Net for 200 epochs, where the number of Aux-Net parameters is 60.2M. Lastly, we use the well-trained transformation matrices to initialize the Des-Net by our proposed parameter selection strategy. ", "page_idx": 18}, {"type": "text", "text": "A.4 Training Efficiency when Transferring to Downstream Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "From Fig.7, we observe that LeTs not only achieves better performance than Scratch, but also is significantly faster. Specifically, LeTs generally outperforms Scratch after about 10 epochs or 5 epochs tuning, which is about $30\\times$ or $60\\times$ faster on CIFAR-10, CIFAR-100 [61] and Food-101 [30]. In summary, LeTs provides a well initialization for variable-sized target models, which not only boosts the final model quality but also speeds up the model training. ", "page_idx": 18}, {"type": "text", "text": "A.5 Discussion about LeTs and Knowledge Distillation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Conceptual Comparison between LeTs and Knowledge Distillation. A considerable body of literature has emerged, focusing on exploring techniques for knowledge distillation [55, 69, 70, 66, 71, 72, 56, 57, 73, 74, 75]. DeiT [66] facilitates the training of ViT with guidance from a ConvNet teacher by introducing a distillation token. MiniViT [56] transfers knowledge from large-scale ConvNets to weight-multiplexed ViTs through weight distillation. DMAE [73] minimizes the discrepancy between the intermediate feature maps of the teacher and student models, alongside optimizing pixel reconstruction loss. The commonality among these methods is the requirement for multiple forward passes through a pretrained teacher during the training of new students, accommodating various resource constraints. Consequently, this entails additional resource consumption for the storage and computation of teacher models every time new models are trained for different scenarios. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Completely different from knowledge distillation, LeTs distills knowledge from the pretrained ancestry model to one auxiliary model once comprising one compact learngene module and a series of learnable transformation matrices. Subsequently, LeTs selects target transformation matrices from these well-trained ones given the different sizes of target model under the guidance of our proposed selection strategies. Lastly, LeTs can flexibly initialize variable-sized models from learngene with target transformation matrices, while simultaneously eliminating the need for the pretrained ancestry model. As can be seen, LeTs is also not a simple combination of knowledge distillation and transformation matrices. Obviously, if we simply combine them, we can obtain just only one target model, which is completely contrary to our research focus: transforming well-trained learngene to initialize variable-sized models. ", "page_idx": 19}, {"type": "text", "text": "A.6 Comparison between LeTs and Online Models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Despite the widespread availability of online models in various sizes, it is important to note that these models are pre-trained by other institutions, which also incurs substantial training costs. In contrast, LeTs offers a more effective and lightweight choice, which transforms one compact learngene with a series of learnable transformation matrices to initialize variable-sized models, eliminating the need for repetitive online model downloads. Furthermore, LeTs flexibly construct and initialize models with finer granularity, unconstrained by layer numbers and head numbers, unlike online models available on platforms such as HuggingFace [76] and Timm [32]. This obvious flexibility allows LeTs to offer greater customization for diverse downstream scenarios with varying resource constraints. ", "page_idx": 19}, {"type": "text", "text": "A.7 Comparison between LeTs and Meta Learning ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Regrading objective, Meta learning mainly focuses on parameter initialization of one target model, but rarely considers initializing variable-sized models to fti diverse resource constraints, the latter of which is our main focus in this paper. For methodology, LeTs highlights transforming one compact learngene with learnable transformation matrices for initialization. In contrast, Meta learning often uses techniques like learning to learn. ", "page_idx": 19}, {"type": "text", "text": "A.8 Limitations and Future Work ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This paper develops an effective approach termed LeTs to transform one compact learngene with a series of learnable transformation matrices for variable-sized model initialization. Despite LeTs achieves satisfactory performance under different initialization settings, we do not carefully select the hyperparameter settings for the construction and training process of Aux-Net, and the finetuning process of each initialized models with different sizes, such as the initialization order. Besides, we only adopt prediction-based distillation to train the compact learngene and transformation matrices, which may affect the quality of both of them. ", "page_idx": 19}, {"type": "text", "text": "In future work, we consider how to efficiently select specific hyperparameter settings and add more advanced distillation techniques to train the compact learngene and transformation matrices. Besides, we consider employing LeTs for language tasks and combine LeTs with other Learngene methods. ", "page_idx": 19}, {"type": "text", "text": "A.9 Broader lmpacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This paper focuses on the fundamental research question of initializing variable-sized models for accommodating diverse resource constraints. The goal is using learnable transformation matrices to transform one compact learngene for initialization. Generally, there are no negative societal impacts involved in this paper. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to the Introduction 1, which clearly states the paper\u2019s contributions and scope. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to the subsection A.8 of the Appendix ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The experimental results in this paper are reproducible. Please refer to experimental details in our original paper and the Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All source code for conducting the experiments will be made publicly available with a license that allows free usage for research purposes. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to experimental details in our original paper and the Appendix. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not have error bars to show. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to experimental details in our original paper and the Appendix. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics to make sure the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to the Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In our research, we ensured that all assets used comply with the corresponding license requirements and adhere to clear usage terms during use. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]