[{"figure_path": "pOXgdFEB7q/tables/tables_6_1.jpg", "caption": "Table 1: Higher resolution images reduce the tunnel effect. Pairwise statistical analysis between DNNs trained on (32 \u00d7 32) images vs. higher resolution images.", "description": "This table presents the results of pairwise statistical analyses comparing the effects of different image resolutions (32x32 vs 64x64, 32x32 vs 128x128, and 32x32 vs 224x24) on three key metrics related to the tunnel effect:  % OOD Performance Retained, Pearson Correlation, and ID/OOD Alignment.  For each comparison, the table shows the effect size (using Cliff's Delta) and the p-value from a Wilcoxon signed-rank test, indicating the statistical significance of the observed differences in performance between the lower resolution (32x32) and the higher resolution models. The effect sizes are categorized as negligible (N), small (S), medium (M), or large (L), providing a concise summary of the impact of image resolution on the tunnel effect.", "section": "4.1.3 Resolution Results"}, {"figure_path": "pOXgdFEB7q/tables/tables_9_1.jpg", "caption": "Table 2: Continual Learning Results. The tunnel is task-specific and impacts forgetting. Et and T1 denote the extractor and tunnel, respectively, for tasks t \u2208 {1, 2}. Orange indicates original task accuracy. Red and blue indicate degraded or maximally enhanced accuracy, respectively. Fine-tuning (FT) aims to recover 1st task performance (gray); therefore, the 2nd task columns are empty.", "description": "This table presents the results of continual learning experiments.  It shows the impact of swapping the \"tunnel\" (a set of layers in a neural network) and the \"extractor\" (another set of layers) trained on two sequential tasks.  The table demonstrates how the tunnel's presence or absence affects the ability of the model to retain knowledge from the first task when learning the second and whether fine-tuning just the extractor or the whole model (extractor + tunnel) is more effective at mitigating catastrophic forgetting. The results are shown for both 32x32 and 224x224 image resolutions.", "section": "4.3 Continual Learning Results"}, {"figure_path": "pOXgdFEB7q/tables/tables_9_2.jpg", "caption": "Table 4: OOD Accuracy. We report average results with 95% confidence intervals (CI). ImageNet-R is abbreviated as IN-R. \u03b3 denotes over-parameterization level.", "description": "This table presents the out-of-distribution (OOD) accuracy results for various deep neural network (DNN) models trained on different image datasets (ImageNet-100, CIFAR-10, CIFAR-100) with varied image resolutions (32x32, 224x224) and augmentation techniques.  The table shows the average OOD accuracy across eight different OOD datasets (NINCO, ImageNet-R, CUB, Aircrafts, Flowers, Pets, CIFAR-10, STL-10). The results are separated into models trained with and without augmentations.  It allows for comparison of OOD accuracy based on various model architectures, resolutions, and augmentation methods.", "section": "4.1.3 Resolution Results"}, {"figure_path": "pOXgdFEB7q/tables/tables_18_1.jpg", "caption": "Table 3: Augmentations used for training various large pre-trained models.", "description": "This table lists eight different large pre-trained models and the augmentations used for training each model.  The augmentations are described in detail, providing a comprehensive overview of the data augmentation strategies employed.", "section": "3.3 Datasets"}, {"figure_path": "pOXgdFEB7q/tables/tables_23_1.jpg", "caption": "Table 4: OOD Accuracy. We report average results with 95% confidence intervals (CI). ImageNet-R is abbreviated as IN-R. \u03b3 denotes over-parameterization level.", "description": "This table shows the out-of-distribution (OOD) accuracy of different models trained on ImageNet-100, CIFAR-10, and CIFAR-100 datasets.  The models were tested with different image resolutions and augmentations.  The table compares the OOD accuracy across nine different OOD datasets, highlighting the impact of model architecture, training data, image resolution, and augmentations on OOD performance.", "section": "C.2.1 OOD Accuracy"}, {"figure_path": "pOXgdFEB7q/tables/tables_24_1.jpg", "caption": "Table 5: % OOD Performance Retained. We report average results with 95% confidence intervals (CI). A higher % OOD performance retained indicates a lesser tunnel effect and better OOD generalization. ImageNet-R is abbreviated as IN-R. \u03b3 denotes over-parameterization level.", "description": "This table presents the percentage of out-of-distribution (OOD) performance retained across various models and experimental settings. The metrics are calculated for 8 different OOD datasets and are compared for various image resolutions (32x32 and 224x224), with and without augmentations. A higher percentage indicates a weaker tunnel effect and better OOD generalization. The over-parameterization level (\u03b3) is also included as it's a factor that might impact OOD generalization.", "section": "C.2.2 % OOD Performance Retained"}, {"figure_path": "pOXgdFEB7q/tables/tables_25_1.jpg", "caption": "Table 6: Pearson Correlation. We report average results with 95% confidence intervals (CI). A higher Pearson correlation indicates a lesser tunnel effect and better OOD generalization. ImageNet-R is abbreviated as IN-R.  \u03b3 denotes over-parameterization level.", "description": "This table presents the Pearson correlation values for various models trained on the ImageNet-100 dataset with different resolutions and augmentation strategies.  Higher Pearson correlation indicates a weaker tunnel effect and better out-of-distribution generalization. The table includes results with and without augmentation, allowing comparison of their impact on the tunnel effect.", "section": "C.2.3 Pearson Correlation"}, {"figure_path": "pOXgdFEB7q/tables/tables_26_1.jpg", "caption": "Table 7: ID/OOD Alignment. We report average results with 95% confidence intervals (CI). A higher ID/OOD alignment indicates a lesser tunnel effect and better OOD generalization. ImageNet-R is abbreviated as IN-R. \u03b3 denotes over-parameterization level.", "description": "This table presents the ID/OOD alignment scores for various DNN models trained with different settings (augmentation and resolution).  Higher scores indicate a weaker tunnel effect and better out-of-distribution generalization. The table includes results for models with and without augmentations and compares performance across various datasets.", "section": "C.2.4 ID/OOD Alignment"}, {"figure_path": "pOXgdFEB7q/tables/tables_26_2.jpg", "caption": "Table 8: OOD Accuracy of Large Pre-trained Models. We report average results with 95% confidence intervals (CI). ImageNet-R is abbreviated as IN-R. \u03b3 denotes over-parameterization level.", "description": "This table shows the out-of-distribution (OOD) accuracy of several large pre-trained models on various datasets.  The models were pre-trained using either self-supervised learning (SSL) or supervised learning (SL). The table shows the average OOD accuracy across nine different datasets, along with 95% confidence intervals.  The overparameterization level (\u03b3) is also included. This table helps to compare the generalization capabilities of models trained with different methods and architectures.", "section": "4.2 Analysis of Widely Used Pre-trained Backbones"}, {"figure_path": "pOXgdFEB7q/tables/tables_27_1.jpg", "caption": "Table 9: % OOD Performance Retained of Large Pre-trained Models. We report average results with 95% confidence intervals (CI). ImageNet-R is abbreviated as IN-R. \u03b3 denotes over-parameterization level.", "description": "This table shows the percentage of out-of-distribution (OOD) performance retained for eight widely used ImageNet-1K pre-trained CNN and ViT backbones trained with either supervised learning (SL) or self-supervised learning (SSL).  The results are broken down by model and OOD dataset, providing a comparison of performance across different models and datasets. A higher percentage indicates a lesser tunnel effect and better OOD generalization. The over-parameterization level (\u03b3) is also included.", "section": "4.2 Analysis of Widely Used Pre-trained Backbones"}, {"figure_path": "pOXgdFEB7q/tables/tables_35_1.jpg", "caption": "Table 8: OOD Accuracy of Large Pre-trained Models. We report average results with 95% confidence intervals (CI). ImageNet-R is abbreviated as IN-R. \u03b3 denotes over-parameterization level.", "description": "This table shows the out-of-distribution (OOD) accuracy of various large pre-trained models on 8 different OOD datasets. The models include those trained with self-supervised learning (SSL) and supervised learning (SL).  The results are presented as average accuracy with 95% confidence intervals.  The over-parameterization level (\u03b3) is also indicated.", "section": "4.2 Analysis of Widely Used Pre-trained Backbones"}, {"figure_path": "pOXgdFEB7q/tables/tables_36_1.jpg", "caption": "Table 11: ID performance of DNNs trained on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet Subsets. Reported is the best top-1 accuracy (%) for experiments with varied resolution and augmentation policies. The total number of DNN parameters is in Million and denoted by #P. For each ID dataset, we group results based on DNN architecture. The sum of rows indicates the total number of DNNs i.e., 64.", "description": "This table presents the in-distribution (ID) accuracy for 64 different deep neural networks (DNNs) trained on four different datasets: CIFAR-10, CIFAR-100, ImageNet-100, and subsets of ImageNet.  The table shows the top-1 accuracy achieved by each DNN on its respective training dataset.  The results are broken down by DNN architecture (ViT, ResNet, VGGm), resolution of input images (32x32, 64x64, 128x128, 224x224), and whether or not data augmentation was used during training. The number of parameters (#P) for each DNN model is also included. The table is structured to facilitate comparisons between DNN architectures and the effect of varying image resolution and augmentation.", "section": "A.5 ViT Experiments"}, {"figure_path": "pOXgdFEB7q/tables/tables_37_1.jpg", "caption": "Table 4: OOD Accuracy. We report average results with 95% confidence intervals (CI). ImageNet-R is abbreviated as IN-R. y denotes over-parameterization level.", "description": "This table presents the out-of-distribution (OOD) accuracy results for various deep neural networks (DNNs) trained on different image datasets.  The results are categorized by model architecture, image resolution (32x32 vs. 224x224), and whether data augmentation was used during training.  The table shows the average OOD accuracy across nine different OOD datasets for each DNN configuration, along with 95% confidence intervals to indicate the variability of the results.", "section": "C.2.1 OOD Accuracy"}, {"figure_path": "pOXgdFEB7q/tables/tables_38_1.jpg", "caption": "Table 12: Average Results. We report average results in 3 metrics with a 95% confidence interval (CI) for different variables and interventions.", "description": "This table summarizes the average results for three metrics (% OOD Performance Retained, Pearson Correlation, and ID/OOD Alignment) across different experimental settings.  These settings include variations in image resolution, the use of augmentations, different DNN architectures (VGGm, ResNet, ViT), levels of spatial reduction, the depth of the network, the overparameterization level, the number of classes in the training data (ID Class Count), and different training datasets (CIFAR-10, CIFAR-100, ImageNet-100). The table provides average values and 95% confidence intervals for each metric under each experimental condition.", "section": "C Additional Statistical Results"}]