[{"type": "text", "text": "What Variables Affect Out-of-Distribution Generalization in Pretrained Models? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Md Yousuf Harun1,\u2217 Kyungbok Lee2,\u2217 Jhair Gallardo1 Giri Krishnan3 Christopher Kanan2 1Rochester Institute of Technology 2University of Rochester 3Georgia Tech ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Embeddings produced by pre-trained deep neural networks (DNNs) are widely used; however, their efficacy for downstream tasks can vary widely. We study the factors influencing transferability and out-of-distribution (OOD) generalization of pre-trained DNN embeddings through the lens of the tunnel effect hypothesis, which is closely related to intermediate neural collapse. This hypothesis suggests that deeper DNN layers compress representations and hinder OOD generalization. Contrary to earlier work, our experiments show this is not a universal phenomenon. We comprehensively investigate the impact of DNN architecture, training data, image resolution, and augmentations on transferability. We identify that training with high-resolution datasets containing many classes greatly reduces representation compression and improves transferability. Our results emphasize the danger of generalizing findings from toy datasets to broader contexts. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding deep neural network (DNN) representations has been a central focus of the deep learning community. It is generally accepted that a DNN\u2019s initial layers learn transferable universal features, with deeper layers being task-specific [1\u20138]. At NeurIPS-2023, [9] challenged this view with evidence for the tunnel effect hypothesis: ", "page_idx": 0}, {"type": "text", "text": "The Tunnel Effect Hypothesis: An overparameterized $\\overline{{N}}$ -layer DNN forms two distinct groups: 1. The extractor consists of the first $K$ layers, creating linearly separable representations. 2. The tunnel comprises the remaining $N-K$ layers, compressing representations and hindering OOD generalization. ", "page_idx": 0}, {"type": "text", "text": "To test the tunnel effect hypothesis, they trained models on an in-distribution (ID) dataset and compared linear probes trained and evaluated on either ID or OOD datasets for embeddings produced by each DNN layer. They showed that ID accuracy increased monotonically, whereas OOD accuracy rapidly decreased after the extractor (Fig. 1). The likely cause of the tunnel effect is intermediate neural collapse [10]. Both [10] and [9] showed that collapsed/tunnel layers could be static without needing learning. If these results are universal, they suggest universal visual features learned in early layers and using embeddings from the penultimate layers of pre-trained DNNs, need to be rethought. However, both [9] and [10] limited their experiments to datasets with low-resolution images and relatively few categories (CIFAR-10, MNIST). Given the widespread use of embeddings from pre-trained DNNs for downstream OOD tasks, we aim to assess the universality of the tunnel effect and the variables that influence its strength . ", "page_idx": 0}, {"type": "text", "text": "If the tunnel effect is stronger in models trained on toy datasets like CIFAR-10 but diminishes for largescale datasets, this may explain why many algorithms evaluated only on toy datasets are ineffective on larger datasets such as ImageNet. This disparity has persisted across problem settings, including open set classification [11], active learning [12], OOD detection [13], uncertainty quantification [14], dataset distillation [15], and continual learning [16\u201318]. ", "page_idx": 1}, {"type": "text", "text": "Our paper makes the following contributions: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We define metrics to measure the strength of the tunnel effect and use a SHAP-based analysis to assess each variable\u2019s impact, e.g., image resolution, number of semantic classes, and DNN architecture. Using 64 pre-trained ID backbones and 8,604 linear probes, we identify conditions that exacerbate, reduce, and eliminate the tunnel effect.   \n2. Using our metrics, we find that widely used ImageNet-1K pre-trained CNN and ViT backbones do not exhibit tunnels, except for ResNet-50.   \n3. In contrast to [9], we find that the tunnel strongly impacts forgetting in continual learning. This suggests the generality of many continual learning systems depends on tunnel strength, which is heavily influenced by architectural and training dataset choices.   \n4. We establish a link between impaired OOD generalization and the characteristics of widely used toy datasets, with both resolution and a small number of classes exacerbating the tunnel effect.   \n5. We propose a revised tunnel effect hypothesis, in which the tunnel\u2019s strength is influenced by training data diversity. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 The Tunnel Effect ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Strong evidence for the tunnel effect was given in [9], but their experiments are limited. First, they only study MLPs and CNNs, whereas ViT models are now widely used. Second, their experiments only use $32\\,\\times\\,32$ images, and we hypothesize that higher resolution images could mitigate the tunnel effect by promoting learning hierarchical representations. Third, they do not control for the impact of data augmentation, where data augmentation is known to improve OOD generalization [19\u201324]. Lastly, they define tunnel as starting at the layer where a linear probe on the ID dataset achieves at least $95\\%$ of the final ID accuracy, ignoring OOD generalization. This is problematic since OOD generalization is central to their tunnel effect definition. Here, we measure tunnel effect strength using OOD performance. ", "page_idx": 1}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/830473d34d647628bb76fe51651d9ef82de126706d9691ed45ac42762fd5e49a.jpg", "img_caption": ["Figure 1: The tunnel effect. The tunnel impedes OOD generalization, which we study using linear probes trained on ID and OOD datasets for each layer. In this example, identical VGGm-17 architectures are trained on identical ID datasets, where only the resolution is changed. Probe accuracy on OOD datasets decreases once the tunnel is reached (denoted by $\\star$ ), where the model trained on low-resolution $(32\\times32)$ images creates a longer tunnel (layers 9-16) than the one (layers 13-16) trained on higher-resolution $\\mathrm{224\\times224)}$ images. The Y-axis shows the normalized accuracy. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2.2 Learning Embeddings that Generalize ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Pretrained DNNs are widely used to produce embeddings for downstream tasks, e.g., DNNs trained on ImageNet generalize effectively to many computer vision tasks [25\u201327]. Several studies [28\u201331] observe that ImageNet ID accuracy is highly predictive of OOD accuracy, while others [22, 32\u201334] find that observation does not always hold. Another example is CLIP [35], which has better OOD generalization due to larger and more diverse training data [36]. We disentangle the role of data quantity versus the level of semantic variability. ", "page_idx": 1}, {"type": "text", "text": "Many works suggest that representations learned in earlier layers are more universal across image datasets whereas later layers are more task-specific [1\u20138, 32]. This observation has guided the development of many transfer and continual learning methods. We revisit this phenomenon by studying the impact of DNN architecture on OOD generalization. ", "page_idx": 1}, {"type": "text", "text": "Previous studies have focused on independently analyzing variables that may impact OOD generalization [3, 4, 9, 22, 32, 36\u201340]. However, there is still a significant gap in our current understanding regarding each variable\u2019s relative importance. Our study bridges this gap. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Measuring the Tunnel Effect ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Following [9], we use linear probes for our tunnel effect analysis. Linear probes are widely used to evaluate the transferability of learned embeddings to OOD datasets [9, 41\u201352]. After supervised training of a DNN on an ID dataset, we train ID and OOD linear probes on embeddings produced by each layer. Embeddings for each layer are produced via global average pooling. Additional details are given in Appendix A.2. We use the linear probes to measure the strength of the tunnel effect. ", "page_idx": 2}, {"type": "text", "text": "In [9], authors showed that OOD accuracy decreased in the tunnel, whereas ID accuracy showed a monotonically increasing trend. However, they did not evaluate the strength of the tunnel and defined the start of the tunnel as when the ID probe reached either $95\\%$ or $98\\%$ of the final ID accuracy. Instead, we propose three metrics that enable us to measure the tunnel\u2019s strength, which are tied to OOD accuracy rather than ID accuracy. These metrics are computed for each OOD dataset. Our findings suggest that defining the start of the tunnel based on ID accuracy is ineffective. In Fig. 1, ID accuracy for $32\\times32$ images reaches $95\\%$ of the final accuracy at layer 14, while OOD accuracy degrades starting at layer 9. We use normalized accuracy curves in the main text to compare models across OOD datasets and resolutions, where each linear probe curve is divided by the highest value. ", "page_idx": 2}, {"type": "text", "text": "Metric 1: $\\%$ OOD Performance Retained. We measure the magnitude of the tunnel effect by evaluating the OOD performance through layer-wise linear probing. For a given OOD dataset, and a network with $N$ layers, we find the layer that achieves the highest OOD linear probe accuracy, denoted as $l_{m}$ . The linear probe accuracy of $l_{m}$ is denoted as $a_{m}$ . Then, we denote the OOD accuracy of the linear probe at the penultimate layer $l_{N-1}$ as $a_{p}$ . We assume that $l_{m}$ is the start of the tunnel if $l_{m}<l_{N-1}$ and $a_{m}>a_{p}$ . For an OOD dataset, the $\\%$ OOD performance retained, $r$ , w.r.t. the start of the tunnel is defined as $r\\,=\\,100\\times\\left(a_{p}/a_{m}\\right)$ . Higher $r$ means better OOD generalization, hence a weaker tunnel and vice-versa. When $a_{p}=a_{m}$ , there is no tunnel. ", "page_idx": 2}, {"type": "text", "text": "Metric 2: Pearson Correlation. Linear probe accuracy of both ID and OOD datasets should display similar trends (higher correlation) in the extractor layers. However, they should have a low correlation for the tunnel. To quantify this, we use Pearson correlation $(\\rho)$ between ID and OOD accuracy curves, where a higher correlation indicates less tunnel effect. Additional details are given in Appendix A.8. ", "page_idx": 2}, {"type": "text", "text": "Metric 3: ID/OOD Alignment. A strong model should have higher ID and OOD accuracy, whereas a weak model may show poor accuracy on ID or OOD or both. To capture the characteristic of a model in terms of how strongly it performs on both ID and OOD, we introduce a metric, called ID/OOD Alignment, denoted by $\\boldsymbol{\\mathcal{A}}$ . To formalize, we denote ID and OOD accuracy by $\\alpha_{i d}$ and $\\alpha_{o o d}$ , respectively. Chance accuracy (random guess) for ID and OOD datasets are denoted by $c_{i d}$ and $c_{o o d}$ , respectively. Finally, we define the metric as $A=(\\alpha_{i d}-c_{i d})\\times(\\alpha_{o o d}-c_{o o d})$ , where $\\boldsymbol{\\mathcal{A}}$ , $,\\,\\alpha_{i d},\\,\\alpha_{o o d}$ , $c_{i d}$ , $c_{o o d}\\in[0,1]$ . Higher $\\boldsymbol{\\mathcal{A}}$ means greater alignment between ID and OOD accuracy. ", "page_idx": 2}, {"type": "text", "text": "3.2 Variables Investigated for Their Role in OOD Generalization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study the role of image augmentation, training classes, training samples, image resolution, and DNN architecture on the tunnel effect, with the details for each given in the next paragraphs. ", "page_idx": 2}, {"type": "text", "text": "Augmentation. Prior work [22, 23] studied the impact of augmentation independently on OOD generalization whereas its impact on the tunnel effect has not been studied. To address this, we conducted 512 experiments, where half of them were trained with augmentations and half without. These are done for every combination of the other variables we study. We used random resized crop and random horizontal flip augmentations. ", "page_idx": 2}, {"type": "text", "text": "Number of Classes. In [9], the tunnel effect was shown to decrease as the number of classes and training samples increased. We aim to disentangle these two variables. We conducted 48 experiments with ImageNet-100, where we kept the training set fixed at 10,000 samples but varied the class counts: 10 (1000 images per class), 50 (200 images per class), and 100 (100 images per class). ", "page_idx": 2}, {"type": "text", "text": "Number of Samples. We conducted $64$ experiments using ImageNet-100 to assess the impact of the number of samples on the tunnel effect. We varied the number of training data per class from 100, 200, 500, and 700 while keeping the number of classes fixed at 100. ", "page_idx": 3}, {"type": "text", "text": "Resolution. Since [9] only studied $32\\times32$ image datasets, the impact of image resolution on the tunnel effect is unknown. We hypothesized that higher resolutions would result in more hierarchical features, resulting in reducing the tunnel effect. To test this, we trained models on ImageNet-100 with $32\\times32$ , $64\\times64$ , $128\\times128$ , and $224\\times224$ images, while keeping the number of parameters for each architecture constant. We conduct 48 experiments per resolution (192 total). ", "page_idx": 3}, {"type": "text", "text": "DNN Architecture Variables. We study the tunnel effect in eight DNN architectures drawn from three families: VGG [53], ResNet [54], and ViT [55]. We study the role of the size of the $k\\times k$ stem, which is the size of the first CNN filter or the ViT\u2019s patch size. Because over-parameterization is central to the tunnel effect hypothesis, we measure the over-parameterization level [56], $\\gamma=P/N$ , where $P$ is the number of DNN parameters and $N$ is the number of ID training samples. We conducted $4l6$ experiments to assess the impact of architecture type, depth, over-parameterization level, stem style, and spatial reduction. ", "page_idx": 3}, {"type": "text", "text": "We ensure that each DNN architecture uses the same number of parameters across image resolutions. To do this for the VGG family, we created VGGm, which replaces the two fully connected layers before the output layer with a ResNet-style global average pooling layer. Since the original VGG is designed for high-resolution images $(224\\times224)$ ), it includes the max-pool in all 5 stages to progressively reduce the spatial dimension, $s$ of the features $(s\\times s\\times c)$ across VGG layers. To capture this, we introduce a variable named spatial reduction by which a stem layer (first layer) reduces the spatial dimension of input images. Spatial reduction, $\\phi$ is defined as the ratio of the output spatial dimension $s_{o u t}$ to the input spatial dimension $s_{i n}$ , i.e., $\\phi=\\left.s_{o u t}\\right/s_{i n}$ . For instance, spatial reduction at a layer that reduces the spatial dimension from $32\\times32$ into $16\\times16$ becomes 0.5, whereas a DNN that did no spatial reduction would have $\\phi=1$ . We also created another variant, $\\mathrm{VGGm\\dagger}$ , to study the impact of spatial reduction on the tunnel effect. The difference between VGGm ( $\\phi=0.5)$ and $\\mathrm{VGGm\\dagger}$ $(\\phi=1)$ ) is that VGGm includes max-pool in all 5 stages whereas $\\mathrm{VGGm\\dagger}$ omits max-pool in the first 2 stages for $32\\times32$ input resolution. Compared to VGGm, $\\mathrm{VGGm\\dagger}$ achieved higher ID accuracy on ImageNet-100 (see Table 11). For ResNet, we use the original ResNet architecture [54]. To keep model size constant across resolutions for ViT models, we use a fixed patch size of $8\\times8$ , with the number of patches being larger for higher-resolution images. Following [57], we used 2D sin-cos position embeddings to encode spatial information. ", "page_idx": 3}, {"type": "text", "text": "3.3 Datasets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "ID Datasets. In our main experiments, we train DNNs on 3 ID datasets: 1) ImageNet-100 [58]\u2014a subset (100 classes) of ImageNet-1K, 2) CIFAR-10 [59], and 3) and CIFAR-100 [60]. For these experiments, 52 DNNs were trained on ImageNet-100, 8 on CIFAR-100, and 4 on CIFAR-10 (64 DNNs total), where resolution, augmentation, etc., were varied as described earlier. ID and OOD linear probes are trained and evaluated for each DNN layer. For our experiments on downloaded ImageNet-1K pre-trained DNNs, ID linear probes were trained on a training subset consisting of 50 images per class (50,000 images). Standard test sets are used for all ID datasets. ", "page_idx": 3}, {"type": "text", "text": "OOD Datasets. To assess OOD generalization with linear probes, we use 9 OOD datasets: NINCO [61], ImageNet- $R$ [62], CIFAR-100 [60], CIFAR-10 [59], Oxford 102 Flowers [63], CUB200 [64], Aircrafts [65], Oxford-IIIT Pets [66], and STL-10 [67] (see Appendix B). Eight OOD datasets are used with DNNs trained on each ID dataset, where CIFAR-10 is omitted for DNNs trained on ImageNet variants. When using CIFAR-10 or CIFAR-100 as the ID dataset, the other CIFAR dataset is used for OOD experiments since their classes do not overlap. ", "page_idx": 3}, {"type": "text", "text": "Resolution & ID Accuracy. All DNNs trained on CIFAR-10 or CIFAR-100 are trained and evaluated with $32\\times32$ images. For DNNs trained on ImageNet variants, all ID and OOD images were resized to the resolution with which the DNN was trained. See Table 11 for the resolution used for each DNN and their ID accuracy with and without augmentations. ", "page_idx": 3}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/7b9d0308c4cc2107daa366deef1c21425510ec2dcad742270f06e109173e47b0.jpg", "img_caption": ["Figure 2: SHAP Results. SHAP slope shows the individual contribution of variables to various targets. Positive values indicate enhanced OOD generalization, and vice-versa for negative values. ", "", ""], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.4 Statistical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For each of the 64 DNNs trained on an ID dataset in our main results, we compute our OOD generalization metrics on each OOD dataset, resulting in 512 values per metric. These values are derived from 8,604 linear probes (ID and OOD). We study the impact of each variable on OOD generalization in isolation using paired Wilcoxon signed-rank tests at $\\alpha\\,=\\,0.05$ , where pairs are constructed to control for the impact of other variables, and we use Cliff\u2019s Delta to measure effect sizes [68], which is appropriate for ratio data. For Cliff\u2019s Delta $(\\delta)$ , we follow the standard practice of defining a negligible effect for $|\\delta|<0.147$ , small effect for $0.147\\leq\\,|\\delta|<0.33$ , medium effect for $0.33\\leq|\\delta|<0.\\dot{4}7\\bar{4}$ , and large effect for $|\\delta|\\geq0.474$ . ", "page_idx": 4}, {"type": "text", "text": "To jointly analyze and rank the contribution of each variable, we use SHAP \u2020, which determines the contribution of each input variable to its output [69, 70]. Following [71], we train Gradient Boosting Regression models to predict three output targets: a) $\\%$ OOD performance retained, b) Pearson correlation, and c) ID/OOD alignment, from 8 input variables: 1) resolution, 2) augmentation, 3) ID class count, 4) spatial reduction, 5) stem, 6) CNN vs. ViT, 7) over-parametrization level, and 8) depth. We then obtain SHAP values for each variable, where using Gradient Boosting Regression facilitates controlling for variable interaction effects [71]. Because SHAP magnitude does not indicate the direction of a variable\u2019s impact, for each of the 3 models, we fit a linear regression model between each variable and its corresponding SHAP values to obtain its slope. Positive slopes indicate the variable improves the metric. We call this SHAP Slope. Details are given in Appendix A.9. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments & Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Main Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In [9], all architectures were trained with CIFAR-10 or CIFAR-100, and all experiments were conducted with $32\\times32$ images. Instead, most of our experiments use ImageNet-100 as the ID dataset, where image resolution is varied. We also include experiments on CIFAR datasets. In our main experiments, for each of our 64 DNNs, we produced 8 OOD linear probe curves and 1 ID linear probe curve (576 total), which required computing 8604 linear probes. From this, we obtain 512 values for each of our 3 metrics under various conditions. ", "page_idx": 4}, {"type": "text", "text": "4.1.1 Overall Findings & SHAP Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Results for our SHAP Slope analysis computed across all $512\\,\\mathrm{{OOD}}$ experiments are summarized in Fig. 2, which shows the impact of each variable on the $\\%$ OOD performance retained and ID/OOD alignment. The SHAP Slope figure for Pearson Correlation is given in Appendix C since it has nearly identical trends to $\\%$ OOD performance retained. The R for $\\%$ OOD performance retained, ID/OOD alignment predictions, and Pearson Correlation are 0.62, 0.44, and 0.73, respectively. Each variable\u2019s positive/negative impact is consistent across all 3 of our SHAP analyses. Our main findings are given below, with additional findings in Appendix C. ", "page_idx": 4}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/0fe4c5e1ea9a8164b3d3fffb5d355be2b2b64c788172414e9c4f456ccd02231a.jpg", "img_caption": ["Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a $95\\%$ confidence interval. \u2b50denotes the start of the tunnel. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Our metrics reveal that increasing the ID class count, higher resolutions, and using augmentations improve OOD generalization. For $\\%$ OOD performance retained and Pearson Correlation, increasing the number of ID classes had the greatest impact, whereas, for ID/OOD alignment, increasing resolution did. This is likely because ID/OOD alignment curves are not normalized using the best OOD accuracy, resulting in resolution\u2019s role being obscured for the other two metrics. Across metrics, using augmentations had the second greatest positive impact. These results indicate that increasing between-class diversity (more classes), greater within-class diversity (augmentations), and higher image resolutions improve OOD generalization. ", "page_idx": 5}, {"type": "text", "text": "While [9] argued that the primary source of the tunnel effect is over-parameterization, our results indicate it plays a minor role compared to other factors. Our results indicate that using a larger stem and excessive DNN depth somewhat impair generalization. For all metrics, the choice of ViT or CNN had the least impact on OOD generalization, consistent with the hypothesis that much of the reported benefits of ViTs for image classification are due to training with larger datasets, stronger augmentation policies, and other tricks [72]. ", "page_idx": 5}, {"type": "text", "text": "Using the average $\\%$ OOD performance retained across the 8 OOD datasets to analyze all 64 of our DNNs, 4 had negligible (non-existent) tunnels, 8 had weak tunnels, 13 had medium tunnels, and 39 had strong tunnels. We use intervals of $[100\\%$ , $95\\%]$ for negligible, $[90\\%$ , $95\\%)$ ) for weak, $[80\\%$ , $90\\%$ ) for medium, and $[0\\%,80\\%)$ for strong tunnel. This demonstrates that the tunnel effect is not universal and depends on variables. Next, we dive into the factors that influence tunnel effect strength. ", "page_idx": 5}, {"type": "text", "text": "4.1.2 Augmentation Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Fig. 3, example linear probe plots illustrate that augmentations play a major role in reducing the tunnel effect. To further analyze the impact of augmentation on OOD generalization, we compared all of our experiments in which augmentation was used or omitted with all other variables controlled using paired Wilcoxon Signed-Rank tests (256 paired experiments, 512 total). For $\\%$ OOD performance retained, augmentations significantly decreased the tunnel effect with $64.26\\%$ retained without augmentations and $78.41\\%$ with $p<0.001)$ , which had a medium effect size $\\langle\\lvert\\delta\\rvert=0.370)$ . For Pearson correlation, augmentations also had a significant effect where $\\rho$ increased from 0.77 to 0.86 $\\mathit{p}<0.001)$ , with a medium effect size $\\langle|\\delta|=0.374)$ . For ID/OOD alignment, augmentations increased alignment from 0.15 to 0.25 $(p<0.001)$ , with a medium effect size $\\langle|\\delta|=0.\\bar{3}57\\rangle$ . ", "page_idx": 5}, {"type": "text", "text": "4.1.3 Resolution Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Illustrative examples showing how increasing resolution improves OOD generalization are given in Fig. 1. To study resolution further, we conducted paired tests between models trained with $32\\times32$ images and those trained with $64\\times64$ , $128\\times128$ , or $224\\times224$ images (48 paired experiments per resolution comparison, 192 total). All models were trained on ImageNet-100. Mean effect sizes $(\\delta)$ and $p$ -values are given in Table 1. Average values for the 3 metrics computed for each resolution are given in Table 12. The findings are consistent with our SHAP analysis: training on higher-resolution images improves OOD generalization, whereas low-resolution datasets increase tunnel effect strength. Additional results are given in the Appendix C.13. ", "page_idx": 5}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/0536b4d2f598022e856846aa3d0a9446fbc35254e88a244774de4a4525ef59b6.jpg", "table_caption": ["Table 1: Higher resolution images reduce the tunnel effect. Pairwise statistical analysis between DNNs trained on $(32\\times32)$ images vs. higher resolution images. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/716669d92a20630e665ae38e97bf0023fffd8ff40a501d5ff6742b6a0895e6e4.jpg", "img_caption": ["Figure 4: High-resolution model does not exhibit representation compression. The t-SNE comparison between VGGm-11 models trained on low- (1st row) and high-resolution (2nd row) images of the same ID dataset (ImageNet-100) in an augmentation-free setting. Layer 8 marks the start of the tunnel in VGGm-11 trained on $32\\times32$ images whereas $224\\times224$ resolution does not create any tunnel. Layer 10 is the penultimate layer. The tunnel layers (layers 8-10) progressively compress representations for $32\\!\\times\\!32$ resolution whereas corresponding layers for $224\\!\\times\\!224$ resolution do not exhibit similar compression. For clarity, we show 5 classes from ImageNet-100 and indicate each class by a distinct color. The formation of distinct clusters in the $32\\times32$ model is indicative of representation compression and intermediate neural collapse [10], which impairs OOD generalization. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "A t-SNE analysis for various layers from VGGm-11 models trained on $32\\times32$ and $224\\times224$ images is given in Fig. 4. The low-resolution model exhibits much greater intermediate neural collapse [10] and representation compression than the high-resolution model. This is likely why many OOD detection algorithms that work well for CIFAR fail for higher-resolution datasets [11]. These results highlight the dangers of extrapolating findings from low-resolution datasets to all of deep learning. ", "page_idx": 6}, {"type": "text", "text": "4.1.4 DNN Architecture Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Spatial Reduction. Our SHAP analysis revealed that lower values for spatial reduction $\\left(\\phi\\right)$ hurt OOD generalization. To further study this, we conducted paired tests between VGGm-11 and VGGm-17, which both have $\\phi=0.5$ , and $\\mathrm{VGGm\\dag.}$ -11 and $\\mathrm{VGGm}\\mathrel{\\dagger}\\!-\\!1^{\\prime}$ 7 $\\langle\\phi=1.0\\rangle$ . All 4 DNNs are trained on ImageNet-100 at $32\\times32$ resolution, with and without augmentations, and each is evaluated on the 8 OOD sets (32 paired experiments, 64 total). In terms of $\\%$ OOD performance retained, the $\\mathrm{VGGm\\dagger}$ models retained $84.40\\%$ whereas the VGGm models retained $64.85\\%$ $(p\\,<\\,0.001)$ , with a large effect size $(|\\delta|=0.531)$ . Similarly, Pearson correlation significantly decreased from 0.92 to 0.72 $(p<0.001)$ , with a large effect size $(|\\delta|=0.536)$ , and ID/OOD alignment significantly decreased from 0.26 to 0.18 $\\mathit{p}_{\\mathit{\\Theta}}<\\mathit{0.001})$ , with a medium effect size $\\langle|\\delta|\\,=\\,0.\\bar{3}61)$ . Fig. 5 provides example normalized accuracy curves. VGGm-11 exhibits a strong tunnel spanning from layer 7 to 10 (Fig. 5a), whereas no tunnel is present in $\\mathrm{VGGm\\dagger}$ -11 (Fig. 5b). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/bc3136976e182a114ee806a58dbda1c50f8c873cbb93d73d01ffd63f0bab82ce.jpg", "img_caption": ["Figure 5: The tunnel effect is not universal. In (a), VGGm-11 consisting of max-pool in all 5 stages ( $\\phi=0.5)$ , creates a tunnel (layers 7-10, gray-shaded area). In (b), the same VGGm-11 without max-pool in the first 2 stages ( $\\mathit{\\Delta}(\\phi=1$ , called ${\\mathrm{VGGm}}^{\\dagger}.$ -11), eliminates the tunnel for all OOD datasets. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Stem. Our SHAP results indicate that increasing stem size hurts OOD generalization. To further study this, we conducted a paired test over 64 paired experiments between ResNet-18, which uses a $7\\times7$ stem, and VGGm-17, which uses a $3\\times3$ stem. Increasing the stem from 3 to 7 significantly decreased the $\\%$ OOD performance retained from $76.74\\%$ to $66.66\\%$ $\\mathit{\\Pi}_{p}\\,<\\,0.001)$ , with a small effect size $\\langle|\\delta|\\;=\\;0.30\\bar{6})$ . However, for Pearson correlation, there was no significant difference $(p=0.145)$ . For ID/OOD alignment, increasing the stem significantly reduced the score from 0.27 to 0.21 $p<0.001)$ , with a small effect size $\\dot{\\left|\\delta\\right|}\\,=\\,0.226)$ . Fig. 12 provides box plots of $\\%$ OOD performance retained and ID/OOD alignment for the three stem values. ", "page_idx": 7}, {"type": "text", "text": "Depth. Our SHAP analysis revealed that increasing depth impairs OOD generalization. As shown in Fig. 10, increasing depth impairs OOD performance retention for each architecture family. To study this further, we compared VGGm-11 and VGGm-17 using 48 paired experiments (96 total). Increasing depth significantly decreased $\\%$ OOD performance retained from $89.19\\%$ to $69.41\\%$ $(p\\,<\\,0.001)$ , with a large effect size $\\langle|\\delta|\\;=\\;0.53\\bar{9})$ ). Likewise, Pearson correlation significantly decreased from 0.94 to 0.80 $p<0.001)$ ), with a large effect size ( $\\left.\\vert\\delta\\right\\vert=0.497)$ . ID/OOD alignment also significantly decreased from 0.28 to 0.25 $\\mathit{\\Delta}{p}<0.001\\rangle$ with a small effect size $(|\\delta|=0.1\\bar{6}1\\$ ). ", "page_idx": 7}, {"type": "text", "text": "Over-parameterization Level. Our SHAP analysis showed that over-parameterization level negatively impacts OOD generalization. Fig. 11 shows how increasing the over-parameterization level decreases $\\%$ OOD performance retained. We conducted paired tests between VGGm-11 $(\\gamma=74.7)$ and ResNet-34 $\\langle\\gamma=168.4\\rangle$ to study this further (32 paired experiments). Increasing over-parameterization significantly reduced $\\%$ OOD performance retained from $87.22\\%$ to $62.78\\%$ $\\mathit{\\Delta}_{p}<\\mathrm{\\Delta}_{0.001})$ , with a large effect size $\\left\\langle\\left\\vert\\delta\\right\\vert=0.680\\right)$ . Likewise, the Pearson correlation was significantly reduced from 0.93 to 0.82 $p<0.001)$ , with a large effect size $(|\\delta|=0.570)$ . Lastly, ID/OOD alignment significantly dropped from 0.29 to 0.20 $p<0.001)$ , with a medium effect size $\\langle|\\delta|=0.340)$ . ", "page_idx": 7}, {"type": "text", "text": "4.1.5 ID Dataset Size vs. Total Classes ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our SHAP analysis shows that ID class count positively impacts OOD generalization. To further analyze this, we trained VGGm-11 on different subsets of ImageNet-100 with $32\\times32$ images where we kept the training dataset fixed at 10,000 samples but varied the class counts: 10 (1000 samples per class), 50 (200 samples per class), and 100 (100 samples per class). Experiments were done with and without augmentations. As shown in Fig. 6, increasing the number of classes greatly reduces the tunnel effect (Figs. 6a and 6b). To examine the role of the dataset size, we vary the number of samples per class from 100, 200, 500, and 700 while keeping the class count constant at 100, which has a relatively small impact on the tunnel effect (Figs. 6c and 6d). ", "page_idx": 7}, {"type": "text", "text": "4.2 Analysis of Widely Used Pre-trained Backbones ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We also studied OOD generalization for eight widely used ImageNet-1K pre-trained CNN and ViT backbones trained with either supervised learning (SL) or self-supervised learning (SSL). We studied ", "page_idx": 7}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/8f448ead72b0fb26d158a888292c6b15ad5aed17d39f3d8a1433331737723490.jpg", "img_caption": ["Figure 6: Training on more classes greatly reduces the tunnel effect, whereas increasing dataset size has less impact. (a) and (b) Results with a fixed number of samples but a varied number of classes. (c) and (d) Results with a fixed number of classes but a varied number of samples per class. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "ResNet-50 (1 SL and 1 SSL [73]), 4 ViT-B (1 SL and 3 SSL [50, 74, 75]), and ConvNeXt-B (1 SL and 1 SSL [76]) models. We trained linear probes on ImageNet-1K (ID) and our 8 OOD datasets (1980 linear probes total), resulting in 72 values per OOD metric. As shown in Fig. 17 and Table 9, the tunnel effect is absent in most models and is weakly present in both SL and SSL ResNet-50. Additional results are given in Appendix C.11. Appendix A.6 includes implementation details. ", "page_idx": 8}, {"type": "text", "text": "4.3 Continual Learning Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Catastrophic forgetting is a central focus in continual learning [17, 77]. Many methods assume forgetting occurs mostly in later layers with earlier layers serving as universal features [16, 78, 79]. The original tunnel effect paper challenged this [9]. They trained VGG-19 on two tasks sequentially, where the first task included half of the CIFAR-10 classes and the second task included the other half. After training on each task, the tunnel and extractor were saved. They found no forgetting when tunnels were swapped, indicating no learning occurred in the tunnel for task 2, and they found reduced forgetting when fine-tuning the extractor alone. These findings are worth assessing beyond CIFAR-10. We ask: What is the role of the tunnel in mitigating forgetting? ", "page_idx": 8}, {"type": "text", "text": "We replicated their general approach by training ResNet-18 on ImageNet-100, where the first task had the first 50 classes and the second had the rest. After learning each task $t$ , we saved the extractor $E_{t}$ , tunnel $T_{t}$ , and the classification head. We conducted this experiment with $32\\times32$ and $224\\times224$ images. Using the tunnel definition from [9], tunnels $T_{1}$ and $T_{2}$ correspond to layers 14-17 for $32\\!\\times\\!32$ images and 15-17 for $224\\times224$ images. See Appendix A.7 for additional details. ", "page_idx": 8}, {"type": "text", "text": "Results are given in Table 2. Unlike the findings in [9], when we swapped tunnels $T_{1}$ and $T_{2}$ , accuracy was greatly reduced for both resolutions, indicating that essential learning happened within the tunnel. Next, we replicated their fine-tuning experiments. If fine-tuning $E_{2}$ alone reduces forgetting more than fine-tuning $E_{2}$ with $T_{1}$ , it suggests the tunnel has a detrimental impact on mitigating forgetting. We found that fine-tuning $E_{2}$ alone improved task 1 accuracy less than fine-tuning $E_{2}+T_{1}$ , indicating $T_{1}$ helps reduce forgetting. Our findings, which contradict [9], suggest the \u201ctunnel\u201d plays an essential role in mitigating catastrophic forgetting, consistent with prior work [5]. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Continual Learning Results. The tunnel is task-specific and impacts forgetting. $E_{t}$ and $T_{t}$ denote the extractor and tunnel, respectively, for tasks $t\\in\\{1,2\\}$ . Orange indicates original task accuracy. Red and blue indicate degraded or maximally enhanced accuracy, respectively. Fine-tuning (FT) aims to recover 1st task performance (gray); therefore, the 2nd task columns are empty. ", "page_idx": 9}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/7807ce20b1b0c629b1021eac4002ce94ac1ccfbaa97bb775f6b51413e9339b27.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Discussion & Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conducted extensive experiments to investigate the generality of the tunnel effect in a wide range of transfer settings. Our study indicates that the best way to mitigate the tunnel effect, and thereby increase OOD generalization, is to increase diversity in the ID training dataset, especially by increasing the number of semantic classes, using augmentations, and higher-resolution images; hence, we revise the tunnel effect hypothesis as follows: ", "page_idx": 9}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/ee73f2c9e550fd01f538577a7150f66361ba86b60d592b35215629c0145418f2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Earlier works on the tunnel effect [9] and intermediate neural collapse [10] exclusively used $32\\times32$ images for ID training. We found that while DNNs trained on CIFAR always exhibited the tunnel effect, the tunnel effect was greatly reduced for ImageNet-100 at higher resolutions. This discrepancy helps explain why methods validated on CIFAR and similar datasets may not generalize in many scenarios [11\u201318]. We urge the community to use high-resolution datasets with 100 or more categories to improve the generality of findings, especially for studies related to representation learning, neural collapse, and OOD detection/generalization. ", "page_idx": 9}, {"type": "text", "text": "Limitations & Future Work. While our work validates the existence of tunnels, future research should focus on developing theoretical frameworks to help explain the tunnel effect. We rigorously studied OOD generalization on vision datasets with supervised learning. Future work could study non-vision, multi-modal [80], and biased datasets [81, 82], where the tunnel effect has not yet been studied. While we studied pre-trained SSL backbones, we could not do our SHAP analysis without having more SSL backbones. Valuable insights for SSL could be obtained by conducting carefully controlled paired experiments, as done in our main experiments. This would require probing at least four different SSL algorithms for each variable analyzed, where training each SSL DNN would require over $10\\times$ more time than our supervised DNNs. Additionally, SSL methods employ more advanced augmentation policies than ours, where we used random-resized crops and horizontal filps. Replicating our SHAP analysis with multiple augmentation policies could reveal whether the OOD generalization capabilities of SSL algorithms are due to their augmentation policies versus their objective functions. Lastly, identifying regularizers or other techniques that mitigate tunnel formation should be sought for continual learning methods that start from scratch using small initial sets. This could greatly improve forward transfer, leading to more efficient continual learning methods [83\u201386]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partly supported by NSF awards #2326491, #2125362, and #2317706. The views and conclusions contained herein are those of the authors and should not be interpreted as representing any sponsor\u2019s official policies or endorsements. We thank Junyu Chen for feedback on an early draft of this manuscript. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13, pages 818\u2013833. Springer, 2014.   \n[2] Nanxuan Zhao, Zhirong Wu, Rynson WH Lau, and Stephen Lin. What makes instance discrimination good for transfer learning? In The Ninth International Conference on Learning Representations (ICLR 2021), 2021.   \n[3] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? Advances in neural information processing systems, 27, 2014.   \n[4] Hattie Zhou, Ankit Vani, Hugo Larochelle, and Aaron Courville. Fortuitous forgetting in connectionist networks. In International Conference on Learning Representations, 2021.   \n[5] Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hidden representations and task semantics. In International Conference on Learning Representations, 2020.   \n[6] Gabriele Merlin, Vedant Nanda, Ruchit Rawal, and Mariya Toneva. What happens during finetuning of vision transformers: An invariance based investigation. In Conference on Lifelong Learning Agents, pages 601\u2013619. PMLR, 2023.   \n[7] Utku Evci, Vincent Dumoulin, Hugo Larochelle, and Michael C Mozer. Head2toe: Utilizing intermediate representations for better transfer learning. In International Conference on Machine Learning, pages 6009\u20136033. PMLR, 2022.   \n[8] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2021.   \n[9] Wojciech Masarczyk, Mateusz Ostaszewski, Ehsan Imani, Razvan Pascanu, Piotr Mi\u0142o\u00b4s, and Tomasz Trzcinski. The tunnel effect: Building data representations in deep neural networks. Advances in Neural Information Processing Systems, 36, 2023.   \n[10] Akshay Rangamani, Marius Lindegaard, Tomer Galanti, and Tomaso A Poggio. Feature learning in deep classifiers through intermediate neural collapse. In International Conference on Machine Learning, pages 28729\u201328745. PMLR, 2023.   \n[11] Ryne Roady, Tyler L Hayes, Ronald Kemker, Ayesha Gonzales, and Christopher Kanan. Are open set classification methods effective on large-scale datasets? Plos one, 15(9):e0238302, 2020.   \n[12] Zeyad Ali Sami Emam, Hong-Min Chu, Ping-Yeh Chiang, Wojciech Czaja, Richard Leapman, Micah Goldblum, and Tom Goldstein. Active learning at the imagenet scale. arXiv preprint arXiv:2111.12880, 2021.   \n[13] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8710\u20138719, 2021.   \n[14] B\u00e1lint Mucs\u00e1nyi, Michael Kirchhof, and Seong Joon Oh. Benchmarking uncertainty disentanglement: Specialized uncertainties for specialized tasks. arXiv preprint arXiv:2402.19460, 2024.   \n[15] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from a new perspective. Advances in Neural Information Processing Systems, 36, 2023.   \n[16] Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Deep classincremental learning: A survey. arXiv preprint arXiv:2302.03648, 2023.   \n[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366\u20133385, 2022. doi: 10.1109/ TPAMI.2021.3057446.   \n[18] Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet K Dokania, Philip HS Torr, Ser-Nam Lim, Bernard Ghanem, and Adel Bibi. Computationally budgeted continual learning: What does matter? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3698\u20133707, 2023.   \n[19] David Tellez, Geert Litjens, P\u00e9ter B\u00e1ndi, Wouter Bulten, John-Melle Bokhorst, Francesco Ciompi, and Jeroen Van Der Laak. Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology. Medical image analysis, 58:101544, 2019.   \n[20] Haoyue Bai, Rui Sun, Lanqing Hong, Fengwei Zhou, Nanyang Ye, Han-Jia Ye, S-H Gary Chan, and Zhenguo Li. Decaug: Out-of-distribution generalization via decomposed feature representation and semantic augmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6705\u2013 6713, 2021.   \n[21] Nathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi. Ssmba: Self-supervised manifold based data augmentation for improving out-of-domain robustness, 2020.   \n[22] Alex Fang, Simon Kornblith, and Ludwig Schmidt. Does progress on imagenet transfer to real-world datasets? Advances in Neural Information Processing Systems, 36, 2023.   \n[23] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. Advances in neural information processing systems, 31, 2018.   \n[24] Randall Balestriero, Ishan Misra, and Yann LeCun. A data-augmentation is worth a thousand samples: Exact quantification from analytical augmented sample moments. arXiv preprint arXiv:2202.08325, 2022.   \n[25] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pages 647\u2013655. PMLR, 2014.   \n[26] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 806\u2013813, 2014.   \n[27] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671, 2019.   \n[28] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389\u20135400. PMLR, 2019.   \n[29] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583\u201318599, 2020.   \n[30] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International conference on machine learning, pages 7721\u20137735. PMLR, 2021.   \n[31] Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D\u2019Amour, Dan Moldovan, et al. On robustness and transferability of convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16458\u201316468, 2021.   \n[32] Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why do better loss functions lead to less transferable features? Advances in Neural Information Processing Systems, 34:28648\u201328662, 2021.   \n[33] Florian Wenzel, Andrea Dittadi, Peter Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, et al. Assaying out-of-distribution generalization in transfer learning. Advances in Neural Information Processing Systems, 35:7181\u20137198, 2022.   \n[34] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. Id and ood performance are sometimes inversely correlated on real-world datasets. Advances in Neural Information Processing Systems, 36, 2023.   \n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[36] Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ali Farhadi, and Ludwig Schmidt. On the connection between pre-training data diversity and fine-tuning robustness. Advances in Neural Information Processing Systems, 36, 2023.   \n[37] Amir M Sarf,i Zahra Karimpour, Muawiz Chaudhary, Nasir M Khalid, Mirco Ravanelli, Sudhir Mudur, and Eugene Belilovsky. Simulated annealing in early layers leads to better generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20205\u201320214, 2023.   \n[38] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pages 491\u2013507. Springer, 2020.   \n[39] Kirill Vishniakov, Zhiqiang Shen, and Zhuang Liu. Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy, 2024.   \n[40] Chongzhi Zhang, Mingyuan Zhang, Shanghang Zhang, Daisheng Jin, Qiang Zhou, Zhongang Cai, Haiyu Zhao, Xianglong Liu, and Ziwei Liu. Delving deep into the generalization of vision transformers under distribution shifts. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 7277\u20137286, 2022.   \n[41] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.   \n[42] Zining Zhu, Soroosh Shahtalebi, and Frank Rudzicz. Ood-probe: A neural interpretation of out-of-domain generalization. arXiv preprint arXiv:2208.12352, 2022.   \n[43] Andreas Waldis, Yufang Hou, and Iryna Gurevych. Dive into the chasm: Probing the gap between in-and cross-topic generalization. arXiv preprint arXiv:2402.01375, 2024.   \n[44] Yue Cao, Zhenda Xie, Bin Liu, Yutong Lin, Zheng Zhang, and Han Hu. Parametric instance classification for unsupervised visual feature learning. Advances in neural information processing systems, 33:15614\u2013 15624, 2020.   \n[45] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.   \n[46] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 1691\u20131703. PMLR, 2020.   \n[47] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.   \n[48] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.   \n[49] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n[50] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[51] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9653\u20139663, 2022.   \n[52] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15619\u201315629, 2023.   \n[53] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.   \n[54] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[55] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \n[56] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. Advances in neural information processing systems, 31, 2018.   \n[57] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain vit baselines for imagenet-1k. arXiv preprint arXiv:2205.01580, 2022.   \n[58] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16, pages 776\u2013794. Springer, 2020.   \n[59] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[60] Alex Krizhevsky and Geoffrey Hinton. Cifar-100 (canadian institute for advanced research). Technical report, University of Toronto, 2014.   \n[61] Julian Bitterwolf, Maximilian M\u00fcller, and Matthias Hein. In or out? fixing imagenet out-of-distribution detection evaluation. ICML, 2023.   \n[62] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349, 2021.   \n[63] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722\u2013729. IEEE, 2008.   \n[64] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. California Institute of Technology, 2011.   \n[65] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n[66] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.   \n[67] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011.   \n[68] Norman Cliff. Dominance statistics: Ordinal analyses to answer ordinal questions. Psychological bulletin, 114(3):494, 1993.   \n[69] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30:4765\u20134774, 2017.   \n[70] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve G\u00fcrel, Bo Li, Ce Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the shapley value. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1167\u20131176. PMLR, 2019.   \n[71] Scott M Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to global understanding with explainable ai for trees. Nature machine intelligence, 2(1):56\u201367, 2020.   \n[72] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976\u201311986, 2022.   \n[73] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912\u20139924, 2020.   \n[74] Pan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim $\\mathrm{Ng}$ , and Shuicheng Yan. Mugs: A multi-granular self-supervised learning framework. arXiv preprint arXiv:2203.14415, 2022.   \n[75] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[76] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16133\u201316142, 2023.   \n[77] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 2019.   \n[78] Md Yousuf Harun, Jhair Gallardo, Tyler L. Hayes, Ronald Kemker, and Christopher Kanan. SIESTA: Efficient online continual learning with sleep. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id $=$ MqDVlBWRRV.   \n[79] Tyler L Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya, and Christopher Kanan. Remind your neural network to prevent catastrophic forgetting. In European Conference on Computer Vision, pages 466\u2013483. Springer, 2020.   \n[80] Manoj Acharya, Karan Jariwala, and Christopher Kanan. Vqd: Visual query detection in natural scenes. In NAACL, 2019.   \n[81] Robik Shrestha, Kushal Kafle, and Christopher Kanan. An investigation of critical issues in bias mitigation techniques. In WACV, 2022.   \n[82] Robik Shrestha, Kushal Kafle, and Christopher Kanan. Occamnets: Mitigating dataset bias by favoring simpler hypotheses. In ECCV, 2022.   \n[83] Md Yousuf Harun, Jhair Gallardo, Tyler L. Hayes, and Christopher Kanan. How efficient are today\u2019s continual learning algorithms? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 2431\u20132436, June 2023.   \n[84] Md Yousuf Harun, Jhair Gallardo, and Christopher Kanan. Grasp: A rehearsal policy for efficient online continual learning. In CoLLAs, 2024.   \n[85] Md Yousuf Harun and Christopher Kanan. Overcoming the stability gap in continual learning. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id= o2wEfwUOma.   \n[86] Eli Verwimp, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L Hayes, Eyke H\u00fcllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H Lampert, et al. Continual learning: Applications and the road forward. TMLR, 2024.   \n[87] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? Advances in Neural Information Processing Systems, 34:12116\u201312128, 2021.   \n[88] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.   \n[89] Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10809\u201310818, 2022.   \n[90] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.   \n[91] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najaf,i Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In International conference on machine learning, pages 6438\u20136447. PMLR, 2019.   \n[92] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.   \n[93] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \n[94] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In European Conference on Computer Vision, pages 631\u2013648. Springer, 2022.   \n[95] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[96] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. International Journal of Computer Vision, 132(1):208\u2013223, 2024.   \n[97] Amitis Shidani, Devon Hjelm, Jason Ramapuram, Russ Webb, Eeshan Gunesh Dhekane, and Dan Busbridge. Poly-view contrastive learning. arXiv preprint arXiv:2403.05490, 2024.   \n[98] Warren Morningstar, Alex Bijamov, Chris Duvarney, Luke Friedman, Neha Kalibhat, Luyang Liu, Philip Mansfield, Renan Rojas-Gomez, Karan Singhal, Bradley Green, et al. Augmentations vs algorithms: What works in self-supervised learning. arXiv preprint arXiv:2403.05726, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We organize additional implementation details and supporting results as follows: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Appendix A describes the implementation details and hardware used for training. It describes the DNN architectures (VGG, ResNet, and ViT), feature extraction for linear probing, training, and evaluation details of both pre-training and linear probing in various experiments. The implementation details of continual learning experiments and analysis of the widely used pre-trained models are also described. The implementation details of the SHAP slope analysis are also provided.   \n\u2022 Appendix B provides details on the datasets used in this paper. We used a total of 4 ID datasets and 9 OOD datasets.   \n\u2022 Appendix C presents additional supporting results for the Pearson correlation metric. It includes additional SHAP results. It reports the performance of various models in terms of OOD accuracy, $\\%$ OOD performance retained, Pearson correlation, and ID/OOD alignment in various settings. It presents analyses of how variables impact the tunnel effect and OOD generalization.   \n\u2022 Appendix D reports the in-distribution performance of various models (VGGm, VGGm\u2020, ResNet, ConvNeXt, and ViT) on various ID datasets (ImageNet-100, ImageNet-1K, CIFAR10, CIFAR-100).   \n\u2022 Appendix E reports additional statistical results for different variables and interventions.   \n\u2022 Appendix F includes the list of 100 classes in the imageNet-100 dataset and confirms there is no overlap between ID and OOD datasets. ", "page_idx": 16}, {"type": "text", "text": "A Implementation Details & Computational Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we use several acronyms such as WD $:$ Weight Decay, LR $:$ Learning Rate, GAP : Global Average Pooling, SSL : Self-Supervised Learning, SL : Supervised Learning, $\\mathbf{ID}:\\operatorname{In}-$ Distribution, and OOD : Out-Of-Distribution. We implemented our code in Python using PyTorch. ", "page_idx": 16}, {"type": "text", "text": "A.1 DNN Architectures ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To assess how the depth of DNN affects the tunnel effect, we create a deeper variant of each DNN architecture by increasing the number of layers (CNN) or blocks (ViT) while keeping other configurations identical. ", "page_idx": 16}, {"type": "text", "text": "VGGm. We modify the VGG-13 (or VGG-19) architecture to create our VGGm-11 (or VGGm-17). This is done by adding an adaptive average pooling layer (nn.AdaptiveAvgPool2d), which allows the network to accept any input size while keeping the output dimensions the same. Additionally, we include a single fully connected (FC) layer rather than three fully connected layers, resulting in the VGGm-11 (or VGGm-17) structure. In particular, VGGm has the final FC classifier layer without the additional two FC layers before the final layer. The number of parameters of VGGm-11 (or VGGm-17) is the same across all input resolutions. ", "page_idx": 16}, {"type": "text", "text": "ResNet. To maintain the same number of parameters across all input resolutions, we use the original and unmodified ResNet-18/34 [54]. Thus ResNet-18/34 consists of $7\\times7$ convolution with stride 2 in the stem layer (first layer) followed by a maxpool layer. ", "page_idx": 16}, {"type": "text", "text": "ViT. We select ViT-Tiny (5.61M parameters) for our main experiments with varying resolutions. To maintain exact same number of parameters across all resolutions, we use a fixed patch size of $8\\times8$ with a varied number of patches due to spatial dimensions. The number of patches or image tokens for $32\\times32$ and $224\\times224$ resolutions are 16 and 784 respectively. We use an embedding dimension of 192, a depth of 12 (i.e., 12 ViT blocks) and 3 heads. We also created another deeper variant with a depth of 18 (i.e., $18\\;\\mathrm{ViT}$ blocks), referred to as ViT-Tiny $^+$ (8.39M parameters). To maintain the same number of parameters across all input resolutions, following [57], we omit the learnable position embeddings and instead use the fixed 2D sin-cos position embeddings. Other details adhere to the original ViT paper [55]. ", "page_idx": 16}, {"type": "text", "text": "A.2 Feature Extraction For Linear Probing ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In experiments with CNNs, at each layer $l$ , for each sample, we extract features of dimension $H_{l}\\times W_{l}\\times C_{l}$ , where $H_{l},W_{l}$ , and $C_{l}$ denote the height, width and channel dimensions respectively. Next, following [37], we apply $2\\times2$ adaptive average pooling on each spatial tensor $(H_{l}\\times W_{l})$ . After average pooling, features of dimension $2\\times2\\times C_{l}$ are flattened and converted into a vector of dimension $4C_{l}$ . Finally, a linear probe is trained on the flattened vectors. In experiments with ViTs, following [87], we apply global average-pooling (GAP) to aggregate image tokens excluding the class token and train a linear probe on top of GAP tokens. ", "page_idx": 17}, {"type": "text", "text": "A.3 VGGm Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "VGGm ID Training: For training VGGm-11/17 on ImageNet-100, we employ the AdamW optimizer with an LR of $6\\times{10}^{-3}$ and a WD of $5\\times{10}^{-2}$ for batch size 512. The model is trained for 100 epochs using the Cosine Annealing LR scheduler with a linear warmup of 5 epochs. We use label smoothing of 0.1 with cross-entropy loss. For CIFAR-10 and CIFAR-100 datasets, we use an LR of 0.01 for batch size 512. We train VGGm-11 for 100 and 70 epochs in experiments without augmentations and with augmentations respectively. Whereas VGGm-17 is trained for 100 epochs in both settings without augmentations and with augmentations. ", "page_idx": 17}, {"type": "text", "text": "Varying Number of Classes and Sample Size: VGGm-11 uses LR of $8\\times{10}^{-3}$ and WD of $5\\times{10}^{-2}$ . VGGm-17 uses WD of $\\mathrm{1\\times10}^{-2}$ instead of $5\\times{10}^{-2}$ . For both with and without augmentation, We train VGGm-11 for 100 epochs and VGGm-17 for 200 epochs. ", "page_idx": 17}, {"type": "text", "text": "VGGm Linear Probing: We use the AdamW optimizer with a flat LR of $\\mathrm{1\\times10}^{-3}$ and a WD of 0 for batch size 128. The linear probes are trained for 30 epochs. We use label smoothing of 0.1 with the cross-entropy loss. ", "page_idx": 17}, {"type": "text", "text": "A.4 ResNet Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "ResNet ID Training: For training ResNet-18/34, we employ the AdamW optimizer with an LR of 0.01 and a WD of 0.05 for batch size 512. The model is trained using the Cosine Annealing LR scheduler with a linear warmup of 5 epochs. We use label smoothing of 0.1 with cross-entropy loss. We use 100 epochs to train ResNet-18 in experiments without augmentation. In experiments with augmentations, we train ResNet-18 for 80 epochs using random resized crop and random horizontal flip augmentations. The ResNet-34 models are trained for 100 epochs in experiments with and without augmentations. ", "page_idx": 17}, {"type": "text", "text": "ResNet Linear Probing: In the linear probing experiment, we use the AdamW optimizer with an LR of $\\mathrm{1\\times10}^{-3}$ and a WD of 0 for batch size 128. The linear probes are trained for 30 epochs. We use label smoothing of 0.1 with cross-entropy loss. ", "page_idx": 17}, {"type": "text", "text": "A.5 ViT Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For ViT training, we follow the codebase of DeiT [88] and A-ViT [89]. We also use timm library [90]. ", "page_idx": 17}, {"type": "text", "text": "ViT ID Training: For training ViT-Tiny/ViT-Tiny+, we employ the AdamW optimizer with an LR of $8\\times{10}^{-4}$ and a WD of 0.05 for batch size 96. We use the Cosine Annealing LR scheduler with a linear warm-up (5 epochs). We also use label smoothing of 0.1 with cross-entropy loss. We train the ViT-Tiny for 100 and 40 epochs in experiments without augmentations and with augmentations respectively. Whereas the ViT-Tiny+ is trained for 100 and 60 epochs in experiments without augmentations and with augmentations respectively. In the experiments with augmentations, we use random resized crop and random horizontal filp. Following [57, 87], we omit class token and instead use GAP token by global average-pooling image tokens for classification head. ", "page_idx": 17}, {"type": "text", "text": "ViT Linear Probing: We use the AdamW optimizer with LR of 0.01 and WD of $\\mathrm{1\\times10^{-4}}$ for batch size 512. The linear probes are trained for 30 epochs. We use label smoothing of 0.1 with cross-entropy loss. ", "page_idx": 17}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/feac0780d8aa695f5a52a490952516978d1ee51b27e8dd7a97ee80f474f9f641.jpg", "table_caption": ["Table 3: Augmentations used for training various large pre-trained models. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Note that, in all cases, we maintain the over-parameterization level where the DNN model size (number of parameters) exceeds the number of training samples $(\\gamma>1)$ . After ID training, we select the model or checkpoint that achieves the best top-1 accuracy $(\\%)$ on the validation dataset. ", "page_idx": 18}, {"type": "text", "text": "A.6 Pre-trained Model Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We describe the implementation details of large pre-trained model analyses that are presented in Sec. 4.2. We list the augmentations used by various pre-trained models in Table 3. ", "page_idx": 18}, {"type": "text", "text": "Datasets. Since it is computationally expensive to use the full ImageNet-1K (ID) dataset for layerwise linear probing experiments, we use a subset of ImageNet-1K as the training dataset where we randomly sampled 50 images per class from the ImageNet-1K dataset. During the evaluation, we used the original ImageNet-1K validation dataset (50 images per class). For OOD linear probe experiments, we used the same OOD datasets e.g., CIFAR-100, Flower-102, NINCO, Aircrafts-100, Oxford Pets-37, STL-10, CUB-200, and ImageNet-R. ", "page_idx": 18}, {"type": "text", "text": "ViT. For all ViT pre-trained models, we use ViT-Base (85.8M parameters) with depth of 12 and $16\\!\\times\\!16$ patch size. ", "page_idx": 18}, {"type": "text", "text": "Pre-trained Model Download Links. The links to download pre-trained models are given below: DINO V1 ViT-B : https://huggingface.co/timm/vit_base_patch16_224. dino, MAE ViT-B: https://huggingface.co/timm/vit_base_patch16_224.mae, MUGS ViT-B: https://huggingface.co/zhoupans/Mugs, SwAV ResNet-50 (800 epochs): https: //github.com/facebookresearch/swav, FCMAE ConvNeXt-B: https://huggingface. co/timm/convnextv2_base.fcmae_ft_in1k, ConvNeXt-B : https://huggingface.co/ facebook/convnext-base-224, ResNet-50 (IMAGENET1K_V2): https://pytorch.org/ vision/stable/models.html, and ViT-B (IMAGENET1K_V1): https://pytorch.org/ vision/main/models/generated/torchvision.models.vit_b_16.html. ", "page_idx": 18}, {"type": "text", "text": "Pre-trained Model Linear Probing. We attach a linear probe layer to each block of ViT models and each Conv2d layer of CNN models. In the ConvNeXt block, which has three layers, only the $7\\mathrm{x}7$ layer uses Conv2d. Therefore, using the Conv2d layer results in one layer per ConvNeXt block for linear probing. We use the AdamW optimizer with a WD of 0 and a batch size of 512. Linear probes are trained for 30 epochs using cross-entropy loss with label smoothing of 0.1. For all ViT models and ResNet-50 Supervised model, the LR is $5\\times{10}^{-4}$ . And, for the ResNet-50 SwAV model and both ConvNeXt models, the LR is $\\mathrm{3\\times10^{-5}}$ . ", "page_idx": 18}, {"type": "text", "text": "A.7 Continual Learning Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We describe the implementation details of continual learning experiments that are presented in Sec. 4.3. We adapt the experimental design from [9] for this study. We use the same linear probing approach and definition of the tunnel to identify tunnels in ResNet-18 models during continual learning experiments. We train the model on two subsequent tasks and each task contains non-overlapping 50 ImageNet classes. ", "page_idx": 18}, {"type": "text", "text": "Continual Learning. We employ the AdamW optimizer with LR of 0.01 and WD of 0.05 for batch size 512. The model is trained for 50 epochs using the Cosine Annealing LR scheduler with a linear warmup of 5 epochs (for the first task). We use label smoothing of 0.1 with cross-entropy loss. ", "page_idx": 18}, {"type": "text", "text": "Tunnel Layers. For $32\\times32$ resolution, tunnels $T_{1}$ and $T_{2}$ correspond to ResNet layer 14-17. For $224\\times224$ resolution, tunnels $T_{1}$ and $T_{2}$ correspond to ResNet layer 15-17. ", "page_idx": 19}, {"type": "text", "text": "Finetune. In this experiment, we use the AdamW optimizer with LR of 0.01 and WD of 5e-2 for batch size 512. The number of epochs is 50. We use label smoothing of 0.1 with cross-entropy loss. A Step LR scheduler is used. In particular, we reduce LR by a factor of 0.1 at predefined milestones, which are set to occur at the 20th and 40th epochs. ", "page_idx": 19}, {"type": "text", "text": "In all experiments, we use image augmentations namely random resized crop, random horizontal filp, mixup [91], and cutmix [92]. For mixup and cutmix augmentations, we use participation probability of 0.6 and 0.4 respectively. We set coefficient $\\beta=1.0$ for cutmix and coefficient $\\alpha=0.1$ for mixup. ", "page_idx": 19}, {"type": "text", "text": "We adapt image pre-processing steps from [89] for various image resolutions. We use the same number of network parameters, hyperparameters, and identical settings for all image resolutions. For a fair comparison, we choose hyperparameters to maximize the performance of all compared methods or models. ", "page_idx": 19}, {"type": "text", "text": "A.8 Evaluation Metrics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Accuracy. For accuracy in all experiments including ID training and ID or OOD linear probing, we use the best top-1 accuracy $(\\%)$ . ", "page_idx": 19}, {"type": "text", "text": "Pearson Correlation. We compute the Pearson correlation $\\rho$ between ID accuracy and OOD accuracy using the following equation, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho=\\frac{\\sum_{i=1}^{n}(x_{i}-\\overline{{x}})(y_{i}-\\overline{{y}})}{\\sqrt{\\sum_{i=1}^{n}(x_{i}-\\overline{{x}})^{2}\\sum_{i=1}^{n}(y_{i}-\\overline{{y}})^{2}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In this formula, $x_{i}$ represents the ID accuracy at the $i$ -th layer, and $y_{i}$ represents the OOD accuracy at the $i$ -th layer. $\\textstyle{\\overline{{x}}}$ and $\\overline{{y}}$ are the average of ID and OOD accuracy over layers, respectively. The index $i$ spans from the first layer (1) to the penultimate layer $(n)$ of models since ID and OOD accuracy are computed at each layer. $\\rho\\in\\left[-1,1\\right]$ , where $\\rho=1$ indicates a perfect positive linear relationship, and $\\rho=-1$ indicates a perfect negative linear relationship. ", "page_idx": 19}, {"type": "text", "text": "Wilcoxon Signed-Rank Test. To measure the significance of the difference between the two groups, we performed a Wilcoxon signed-rank test between the two groups. We compared $\\%$ OOD performance retained, Pearson correlation, and ID/OOD alignment metrics between each pair. We used the wilcoxon function from the spicy.stats python library. ", "page_idx": 19}, {"type": "text", "text": "Cliff\u2019s Delta. The effect size (Cliff\u2019s Delta) quantifies the statistical difference between the two sets. A bigger effect size denotes a bigger difference and the order is negligible $<$ small $<$ medium $<$ large. We computed Cliff\u2019s Delta for $\\%$ OOD performance retained, Pearson correlation, and ID/OOD alignment metrics to measure the effect size between the two groups. We used the cliffs_delta function from the cliffs_delta python library (https://github.com/neilernst/cliffsDelta). ", "page_idx": 19}, {"type": "text", "text": "Reasons for Multiple Metrics. The Pearson correlation metric gauges the correlation between ID and OOD accuracy, indicating tunnel behavior (low correlation), but doesn\u2019t quantify the tunnel effect\u2019s magnitude. The $\\%$ OOD performance retained metric assesses the tunnel effect\u2019s magnitude and identifies tunnel layers but solely compares OOD performance between the highest and penultimate layers, neglecting ID accuracy. The ID/OOD alignment metric addresses this by considering both ID and OOD accuracy. To ensure robustness, we employ all metrics to comprehensively capture ID and OOD performance and their relation to the tunnel effect. ", "page_idx": 19}, {"type": "text", "text": "A.9 SHAP Slope Analysis Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We describe the implementation details of SHAP slope analyses that are presented in Sec. 4.1.1. For SHAP analysis, categorical variables (e.g., CNN vs ViT) are treated as one-hot vectors, while noncategorical values are transformed into ordinal numbers. We calculate SHAP values on a Gradient Boosting Regression model, which is trained on our set of 512 experiments to predict a metric from the manipulated input variables already described in the main section of this work. We use the Huber loss to train the Gradient Boosting Regression model. ", "page_idx": 19}, {"type": "text", "text": "While SHAP values themselves can give us insights into the impact of each variable, their interpretation from commonly used plots like SHAP mean absolute bar plot (Fig. 7a) or SHAP Beeswarm plot (Fig. 7b) is limited. We want to show both the impact and the relationship direction of each variable with the predicted metric. To do this, for each variable, we fit a linear model on the obtained SHAP values and the normalized ordinal values of the variable. An example can be seen in Fig. 7c. In this way, the value of the slope tells us the impact of the variable on the predicted metric, while the sign of the slope tells us the direction. For each metric, we $L_{1}$ normalize the slope values. A positive slope means that increasing the variable\u2019s value increases the predicted metric, while a negative slope means that increasing the variable\u2019s value decreases the value of the predicted metric. A bigger value in any direction indicates that the predicted metric is more sensitive to the corresponding variable. We coined the plot containing the slopes for each variable as the \u201cSHAP Slope\u201d plot. ", "page_idx": 19}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/c70b50ed68b09d9f1598f989db51408e584542d2955750db10e242cf48df2008.jpg", "img_caption": ["Figure 7: Examples of SHAP analysis. SHAP analysis plots for the $\\%$ OOD performance retained metric and an example of the slope calculation for the depth. (a) Mean absolute SHAP bar plot, (b) Beeswarm plot, and (c) SHAP values vs normalized ordinal values of depth. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "A.10 Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We trained a total of 64 backbones and 10,652 linear probes to finalize our manuscript. We ran all experiments using four NVIDIA A5000 GPUs, including training backbones and linear probes. The aggregated compute time is $\\sim\\!1.161$ (wall clock) hours (48 days). ", "page_idx": 20}, {"type": "text", "text": "B Datasets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "All datasets are widely used and publicly available. We provide a link to the license if it exists. Despite their widespread use, we were unable to identify the license for CIFAR-10, CIFAR-100, ImageNet-1K, CUB-200, Aircrafts-100, Flowers-102, and STL-10 datasets. ", "page_idx": 20}, {"type": "text", "text": "We used 11 datasets in total in our paper and they are ImageNet-1K, ImageNet-100, ImageNet-R, CIFAR-10, CIFAR-100, NINCO, CUB-200, Aircrafts, Oxford Pets, Flowers-102, and STL-10. The dataset details are given below: ", "page_idx": 20}, {"type": "text", "text": "CIFAR-10. CIFAR-10 [59] dataset contains $32\\times32$ color images with 10 classes. It comprises a total of 60,000 images and each class has 6,000 images. The dataset is split into 50,000 training images and 10,000 test images. CIFAR-10 is a public dataset but does not specify any particular license (https://www.cs.toronto.edu/\\~kriz/cifar.html). ", "page_idx": 20}, {"type": "text", "text": "CIFAR-100. CIFAR-100 [60] dataset is similar to CIFAR-10 but with 100 classes. And, each class has 600 images. It comprises a total of 60,000 images. The training and test sets contain 50,000 and 10,000 images respectively. Note that the classes in CIFAR-100 are mutually exclusive with those in CIFAR-10. CIFAR-100 is a public dataset but does not specify any particular license (https://www.cs.toronto.edu/\\~kriz/cifar.html). ", "page_idx": 20}, {"type": "text", "text": "ImageNet-1K. Imagenet-1K [93] is a widely-used large-scale dataset with 1000 object categories and over 1.2 million training images $(224\\times224)$ . The test set contains 50000 images. ImageNet-1K dataset is available for free to researchers for non-commercial use. There is no particular license specified by creators (https://image-net.org/challenges/LSVRC/). ", "page_idx": 20}, {"type": "text", "text": "ImageNet-100. ImageNet-100 [58] is a subset of ImageNet-1K and contains 100 ImageNet classes. It consists of 126689 training images ( $224\\times224)$ and 5000 test images. The dataset is available to use for research purposes under a BSD 2-Clause License: https://github.com/HobbitLong/ CMC/blob/master/LICENSE. The class names of ImageNet-100 dataset are given in Sec. F. ", "page_idx": 21}, {"type": "text", "text": "NINCO (No ImageNet Class Objects). NINCO [61] is a dataset with 64 classes. The dataset is curated to use as an out-of-distribution dataset for ImageNet-1K in-distribution dataset. NINCO dataset has 5878 samples and the classes do not overlap with ImageNet classes. We split 5878 samples into 4702 samples for training and 1176 samples for evaluation. We do not have a fixed number of samples per class for training and evaluation datasets. The dataset is available to use for research purposes under an MIT license: https://github.com/j-cb/NINCO/blob/main/LICENSE. ", "page_idx": 21}, {"type": "text", "text": "ImageNet-Rendition (ImageNet-R). ImageNet-R incorporates distribution shifts using different artistic renditions of object classes from the original ImageNet dataset [62]. We use a variant of ImageNet-R dataset from [94]. ImageNet-R is a challenging benchmark for continual learning, transfer learning, and OOD detection. It consists of classes with different styles and intra-class diversity and thereby poses significant distribution shifts for ImageNet-1K pre-trained models [94]. It contains 200 classes, 24000 training images, and 6000 test images. The dataset is available to use for research purposes under an MIT license: https://github.com/hendrycks/imagenet-r/blob/ master/LICENSE. ", "page_idx": 21}, {"type": "text", "text": "CUB-200. CUB-200 is composed of 200 different bird species [64]. The CUB-200 dataset comprises a total of 11,788 images, with 5,994 images allocated for training and 5,794 images for testing. CUB-200 is a public dataset but does not specify any particular license (http: //www.vision.caltech.edu/datasets/cub_200_2011/). ", "page_idx": 21}, {"type": "text", "text": "Aircrafts-100. The Aircrafts or FGVCAircrafts dataset [65] consists of 100 different aircraft categories and 10000 high-resolution images with 100 images per category. The training and test sets contain 6667 and 3333 images respectively. Aircrafts dataset is available for non-commercial research purposes only and does not mention a particular license (http://www.robots.ox.ac.uk/ \\~vgg/data/fgvc-aircraft/). ", "page_idx": 21}, {"type": "text", "text": "Oxford Pets-37. The Oxford Pets dataset includes a total of 37 various pet categories, with an approximately equal number of images for dogs and cats, totaling around 200 images for each category [66]. The dataset is available to use for commercial/research purposes under a Creative Commons Attribution-ShareAlike 4.0 International License: https://www.robots.ox.ac.uk/ \\~vgg/data/pets/. ", "page_idx": 21}, {"type": "text", "text": "Flowers-102. The Flowers-102 dataset contains 102 flower categories that can be easily found in the UK. Each category of the dataset contains 40 to 258 images [63]. Flowers-102 is a public dataset but does not mention a particular license (https://www.robots.ox.ac.uk/\\~vgg/data/ flowers/102/). ", "page_idx": 21}, {"type": "text", "text": "STL-10. STL-10 has 10 classes with 500 training images and 800 test images per class [67]. STL10 is a public dataset but does not specify any particular license (https://cs.stanford.edu/ \\~acoates/stl10/). ", "page_idx": 21}, {"type": "text", "text": "C Results & Insights ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Additional SHAP Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We calculate the Pearson correlation metric over all 512 experiments, using the formula described in Sec. A.8. Fig. 8 shows the SHAP Slope plot for the Pearson correlation metric. The gradient-boosting regression model got a $R^{2}$ of 0.73 when predicting this metric. The variables follow a similar trend as seen in Sec. 4.1.1 for $\\%$ OOD performance retained metric, where ID class count, augmentation, spatial reduction, and resolution have a positive impact on Pearson correlation. Again, ID class count stands out as the most influential variable for Pearson correlation, suggesting that input diversity enhances OOD generalization. In contrast, depth, over-parameterization level, stem, and CNN vs ", "page_idx": 21}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/6bd33f0f71dc71fd113cba41fab72671d30866f8f85723962450dc44bd88ac99.jpg", "img_caption": ["Figure 8: SHAP results for Pearson correlation. SHAP slope shows the individual impact of variables on the Pearson correlation target. A positive value indicates enhanced OOD transferability and reduced tunnel effect. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "ViT variables negatively impact Pearson correlation. Depth shows the most negative impact among others. ", "page_idx": 22}, {"type": "text", "text": "C.2 Assessing The Tunnel Effect ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We conduct 224 experiments on ImageNet-100, exploring various model architectures, and resolutions $\\mathrm{32\\times32}$ and $224\\times224)$ ), with and without augmentations, and evaluate them on 8 different OOD datasets mentioned in Sec. 3.3. Additionally, to examine resolution in a more fine-grained manner, we conduct 96 experiments with resolutions of $64\\times64$ and $128\\times128$ using VGGm-17, ResNet-18, and $\\mathrm{ViT-T+}$ , also on ImageNet-100 and the same 8 OOD datasets. We conduct 96 more experiments on ImageNet-100, varying class numbers while keeping image numbers constant, and vice versa. To compare the effect of the ID dataset, we perform 64 experiments on CIFAR-10 and CIFAR-100 as ID datasets with their native resolution $(32\\times32)$ , exploring VGGm-11 and VGGm-17 models, with and without augmentations, and evaluate them on all OOD datasets. We do the same for $\\mathrm{VGGm\\dagger}$ -11 and VGGm\u2020-17 on CIFAR-100, resulting in another 32 experiments. This totals 512 OOD experiments. ", "page_idx": 22}, {"type": "text", "text": "In this section, we summarize results in terms of OOD accuracy and three metrics e.g., $\\%$ OOD performance retained, Pearson correlation, and ID/OOD alignment. This helps analyze the strength of the tunnel effect and OOD generalization in various transfer settings. The following subsections present results in terms of different metrics and discuss the findings. ", "page_idx": 22}, {"type": "text", "text": "C.2.1 OOD Accuracy ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Table 4, we compare OOD accuracy among models trained on the ImageNet-100 ID dataset with varied resolutions. The insights derived from three metrics regarding the influence of variables on the tunnel effect are consistent with observations of OOD accuracy. Models exhibiting weaker or negligible tunnel effects tend to attain higher OOD accuracy compared to those with stronger tunnel effects. ", "page_idx": 22}, {"type": "text", "text": "C.2.2 $\\%$ OOD Performance Retained ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Table 5, we compare $\\%$ OOD performance retained among models trained on the ImageNet-100 ID dataset with varied resolutions. Models exhibiting weaker or no tunnel effect tend to achieve higher $\\%$ OOD performance retained compared to those displaying a stronger tunnel effect. ", "page_idx": 22}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/c3208c26fd9966c93acc23ad1234531823d1970b8b63976ebba2c1f3bc27d2ee.jpg", "table_caption": ["Table 4: OOD Accuracy. We report average results with $95\\%$ confidence intervals (CI). ImageNet-R is abbreviated as IN-R. $\\gamma$ denotes over-parameterization level. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.2.3 Pearson Correlation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Table 6, we compare the Pearson correlation among models trained on the ImageNet-100 ID dataset with varied resolutions. We observe that the Pearson correlation adeptly captures the tunnel effect across different models and settings. ", "page_idx": 23}, {"type": "text", "text": "C.2.4 ID/OOD Alignment ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Table 7, we compare ID/OOD Alignment among models trained on the ImageNet-100 ID dataset with varied resolutions. More performant models obtain higher ID/OOD alignment scores across various experiments compared to less performant models. The impact of different variables and interventions on the ID/OOD alignment is shown in Fig. 19 and Fig. 20. ", "page_idx": 23}, {"type": "text", "text": "C.3 DNN Architecture ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we analyze how DNN architecture impacts $\\%$ OOD performance retained across transfer settings. To investigate this, We create a boxplot (Fig. 9) that shows $\\%$ OOD performance retained of each model trained on ImageNet-100 ID dataset with resolutions $32\\times32$ and $224\\times224$ . We find that DNN architecture greatly impacts the OOD generalization. Among CNNs, VGGm-11 shows superior performance. In terms of CNN vs. ViT, ViTs perform better than CNNs when augmentations are used. Augmentations benefti both CNNs and ViTs, but ViTs seem to benefti more from augmentations. ", "page_idx": 23}, {"type": "text", "text": "C.4 DNN Depth ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we analyze how DNN depth impacts the tunnel effect. Fig. 10 shows that smaller models have higher $\\%$ OOD performance retained than larger (deeper) models. Therefore, the depth of DNN has an adverse impact on OOD generalization and the tunnel effect. ", "page_idx": 23}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/d5e48b4ff577ad0e64bf685087a1ecc7bf77dfcf4211e088aca12feb44f7edbf.jpg", "table_caption": ["Table 5: $\\%$ OOD Performance Retained. We report average results with $95\\%$ confidence intervals (CI). A higher $\\%$ OOD performance retained indicates a lesser tunnel effect and better OOD generalization. ImageNet-R is abbreviated as IN-R. $\\gamma$ denotes over-parameterization level. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "C.5 Overparameterization Level ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we investigate the relationship between overparameterization level and $\\%$ OOD performance retained. We use experimental results from models trained on ImageNet-100 ID dataset with resolutions $32\\times32$ and $224\\times224$ for this analysis. To do this, we compute average $\\%$ OOD performance retained across overparameterization levels and create a boxplot using this data. Fig. 11 shows that $\\%$ OOD performance retained decreases as over-parameterization level increases. Therefore, overparameterization level has an adverse impact on OOD generalization. ", "page_idx": 24}, {"type": "text", "text": "C.6 Stem ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section analyzes how the stem affects tunnel formation. We use experimental results of models trained on ImageNet-100 ID dataset with resolutions $32\\times32$ and $224\\times224$ . For CNNs, the stem represents kernel size ( $\\mathrm{3\\times3}$ and $7\\times7$ ) at the stem layer whereas, for ViTs, it is patch size $(8\\times8)$ . Fig. 12 reveals that increasing stem hurts $\\%$ OOD performance retained. In particular, when comparing CNNs (VGGm\u2019s $3\\times3$ and ResNet\u2019s $7\\times7$ ), increasing stem from 3 to 7 decreases average $\\%$ OOD performance retention. However, ViTs $(8\\times8)$ perform better than CNNs. On the other hand, stem significantly impacts ID/OOD alignment. Across all compared models, increasing stem shows a significant negative impact on ID/OOD alignment. ", "page_idx": 24}, {"type": "text", "text": "C.7 Augmentation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we study how augmentation affects the tunnel effect. We used all 512 experiments to assess the impact of augmentation. 256 experiments used augmentations and another 256 experiments did not use any augmentation. In experiments with augmentation, we used random resized crop and random horizontal flip. Fig. 13 exhibits that increasing augmentation greatly improves $\\%$ OOD performance retained. ", "page_idx": 24}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/af1dcdf8c0f5718cd64ccdb7ab282ac771758316c3f3dd2deac3b41e26641bfd.jpg", "table_caption": ["Table 6: Pearson Correlation. We report average results with $95\\%$ confidence intervals (CI). A higher Pearson correlation indicates a lesser tunnel effect and better OOD generalization. ImageNet-R is abbreviated as IN-R. $\\gamma$ denotes over-parameterization level. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "C.8 ID Class Count ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This section explores the relationship between the ID class count and the tunnel effect. We use experimental results from the VGGm-11 models trained with ImageNet-100, CIFAR-10, and CIFAR100 using $32\\!\\times\\!32$ resolution and image augmentations. Fig. 14 displays how the number of ID classes affects the tunnel effect and OOD performance. We see that more ID classes reduce the strength of the tunnel effect and enhance OOD generalization. ", "page_idx": 25}, {"type": "text", "text": "For the statistical analysis, we consider 16 paired experiments (32 total) comparing CIFAR-10 and CIFAR-100 datasets (ID). The compared models are VGGm-11, VGGm-17, $\\mathrm{VGGm}\\dagger-1$ 1, and $\\mathrm{VGGm\\dagger}$ -17. Increasing the number of CIFAR classes from 10 to 100 increased $\\%$ OOD performance retained from $35.81\\%$ to $73.42\\%$ $p<0.001)$ , with a large effect size $(|\\delta|=0.758)$ . Similarly, the Pearson correlation was improved from 0.54 to 0.88 $(p<0.001)$ , with a large effect size $\\left.\\left\\langle{\\delta}\\right|=0.742\\right\\rangle$ . And, ID/OOD alignment reached 0.22 from 0.12 $p=0.013)$ ), with a large effect size $\\langle|\\delta|=0.531\\rangle$ ). ", "page_idx": 25}, {"type": "text", "text": "C.9 Number of ID Training Classes vs. ID Dataset Size ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Fig. 15 shows OOD performance for various dataset configurations with varied classes and samples per class. Increasing class counts significantly impacts OOD performance, whereas, increasing the number of training samples has less impact. ", "page_idx": 25}, {"type": "text", "text": "C.10 The Tunnel Effect Is Not Specific To CIFAR-10 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In Fig. 16 and Fig. 14, we observe that the tunnel effect is present in various ID datasets e.g., ImageNet-100, CIFAR-10, and CIFAR-100. Therefore, the tunnel effect is not a characteristic of a particular ID dataset such as CIFAR-10. ", "page_idx": 25}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/47f699f594ad4e5d264441b61ac9b68ce8ae870cbd8814de78ebaa7f1578156e.jpg", "table_caption": ["Table 7: ID/OOD Alignment. We report average results with $95\\%$ confidence intervals (CI). A higher ID/OOD alignment indicates a lesser tunnel effect and better OOD generalization. ImageNet-R is abbreviated as IN-R. $\\gamma$ denotes over-parameterization level. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/9abb8d3de681328782ca61fcc65dd4750d50e1416afcbb2735edd2a884b65095.jpg", "table_caption": ["Table 8: OOD Accuracy of Large Pre-trained Models. We report average results with $95\\%$ confidence intervals (CI). ImageNet-R is abbreviated as IN-R. $\\gamma$ denotes over-parameterization level. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "C.11 Large Pre-trained Models Do Not Exhibit The Tunnel Effect ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "With the prevalence of large pre-trained models, many research and applications are driven by these models. They show impressive transferability, especially those trained with SSL methods. Our goal is to explain this phenomenon using the tunnel effect hypothesis. Although the tunnel effect is hypothesized to be present in all over-parameterized DNNs, in [9], only supervised DNNs were studied. Using pre-trained models, we study both SSL and SL models. For SSL pre-trained models, we study the following: ", "page_idx": 26}, {"type": "text", "text": "\u2022 ResNet-50 pre-trained with SwAV [73], \u2022 ViT-B pre-trained with DINO V1 [75], \u2022 ViT-B pre-trained with MAE [50], \u2022 ViT-B pre-trained with MUGS [74], and ", "page_idx": 26}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/ebd66b17d05108752b8db0ee1e4dd7873a12de8bf3afba3096d200edcae7c6db.jpg", "img_caption": ["Figure 9: Box-plots of $\\%$ OOD Performance Retained. The figure shows the $\\%$ OOD performance retained, computed across models trained on ImageNet-100 at resolutions $32\\times32$ and $224\\times224$ , both with and without augmentation. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/708315d090449063a81af61e0c0bee0bae15a91a4e0c43dcf965dd62995a26cf.jpg", "table_caption": ["Table 9: $\\%$ OOD Performance Retained of Large Pre-trained Models. We report average results with $95\\%$ confidence intervals (CI). ImageNet-R is abbreviated as IN-R. $\\gamma$ denotes overparameterization level. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "\u2022 ConvNeXt-B V2 pre-trained with FCMAE [76]. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For SL pre-trained models, we study ResNet-50 [54], ConvNext-B V1 [72], and ViT-B [55] which are pre-trained with SL loss and data labels. We carry out 72 experiments in total of which 27 are for 3 SL models and 45 are for 5 SSL models. All SL and SSL models are pre-trained and fine-tuned (SSL only) on ImageNet-1K dataset. In total, we trained 1980 linear probes for ID and OOD datasets. Appendix A.6 gives additional details and model configurations. ", "page_idx": 27}, {"type": "text", "text": "Due to limited computational resources, we could not pre-train our own SSL models to do a rigorous paired analysis. SSL training is approximately $16\\times$ more expensive than SL, considering multiplicative factors, e.g., $\\sim8\\times$ more epochs and $\\sim2\\times$ more forward passes. Thus, we could only evaluate publicly available pre-trained models released by their creators. We find that SHAP analysis appears to be less informative due to the small sample size and because most of the pre-trained models do not exhibit the tunnel effect, resulting in less variability. Therefore, we focus on OOD linear probe analysis. ", "page_idx": 27}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/f49d1717fbcf1a81bf0d9021a70fcc6eeddceb9f1c127c4a849a7311741b6686.jpg", "img_caption": ["Figure 10: Impact of Depth. The figure shows how depth impacts the tunnel effect in terms of $\\%$ OOD performance retained. Increasing depth decreases the OOD performance retention and intensifies the tunnel effect. The Y-axis shows the average with a $95\\%$ confidence interval. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/c5167bd8e14e188575853c54fd90b26312a856704cba622720a8185e3aaa1650.jpg", "img_caption": ["Figure 11: Over-parameterization level. This figure exhibits $\\%$ OOD performance retained computed across over-parameterization levels. This is based on models trained on ImagerNet-100 at resolutions $32\\times32$ and $224\\times224$ . Increasing the over-parameterization level reduces the $\\%$ OOD performance retained and intensifies the tunnel effect. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "In Fig. 17 and Table 9, we observe that the tunnel is not present in most cases except for the ResNet-50 models. As shown in Table 8, the SSL method, MUGS achieves the highest average OOD accuracy among all methods. In terms of DNN architecture, ViT-B exhibits better OOD performance than ResNet-50. Interestingly, among CNNs, ConvNeXt-B, which adapts design choices from Swin", "page_idx": 28}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/114a01e3deb84eac13de49985d4272d8509f896b443afc67c1dc29b6f551e68c.jpg", "img_caption": ["(a) $\\%$ OOD Performance Retained "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/2513895cfca5a524aa2f71854232373340d587b9e3138ee243c8a92a8170d30a.jpg", "img_caption": ["(b) ID/OOD Alignment "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 12: Stem. This figure exhibits $\\%$ OOD performance retained and ID/OOD alignment computed across stems and averaged across OOD datasets. This is based on models trained on ImagerNet-100 (ID) with $32\\times32$ and $224\\times224$ resolutions. When comparing CNNs (VGGm\u2019s $3\\times3$ and ResNet\u2019s $7\\times7$ , increasing stem exhibits a negative impact on $\\%$ OOD performance retained. Across all models, increasing stem negatively impacts ID/OOD alignment. ", "page_idx": 29}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/ce931674e10b4d91a481f7591882455f7c79a5b3333dea47528e3fd6a3ba8cdf.jpg", "img_caption": ["Figure 13: Augmentation. This figure exhibits $\\%$ OOD performance retained to assess the impact of augmentation. The comparison is made between 256 experiments without augmentation and 256 experiments with augmentations. Augmentation greatly enhances the $\\%$ OOD performance retained and reduces the tunnel effect. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "transformer [95], shows better OOD performance than ResNet-50. They also rival ViTs. This corroborates our argument that DNN architecture matters for OOD performance. ", "page_idx": 29}, {"type": "text", "text": "The comparison between SSL and SL models is not straightforward as there are multiple factors involved, e.g., pretext tasks, training epochs, augmentations, and so on. Except for MAE, other SSL methods exhibit strong OOD performance compared to SL counterparts (Table 8). Previous works [52, 96] also found that MAE performs poorly in linear evaluation. When we compared SL vs. SSL models in the same family, in terms of average OOD accuracy, SSL $(71.42\\%)$ ) outperforms SL $(68.83\\%)$ . As illustrated in Fig. 18, SSL models perform better than SL models in terms of $\\%$ OOD performance retained. We report the ID accuracy of these models in Table 10. Given that SSL methods employ more augmentations than their SL counterparts (see Table 3), the performance improvements of SSL methods over SL ones are likely, in part, due to stronger augmentations, which is also argued in prior studies [97, 98]. While complimenting earlier works, our work confirms that large pre-trained models do not form tunnels, adding valuable perspectives to current knowledge. ", "page_idx": 29}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/8fe2e0984348983b66d899a5d686181d31bdab35b88b6c30b715353b1527360f.jpg", "img_caption": ["Figure 14: ID class count. This figure compares the OOD and ID linear probe accuracy for VGGm-11 models across three ID datasets $\\mathrm{32\\times32}$ resolution): CIFAR-10, CIFAR-100, and ImageNet-100. The OOD curve is the average of 8 OOD datasets and the shaded area denotes standard deviation. The strength of the tunnel is weaker for ImageNet-100 and CIFAR-100 compared to CIFAR-10. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/2bf4841f56914839e1805ca96adb7f452a1451058b8dbccb736d6dd4cbb9b82e.jpg", "img_caption": ["Figure 15: Class Count vs. Data Quantity. The figure shows the trend that models trained with more classes and more samples have better OOD accuracy $(\\%)$ . We use C and S to denote the number of classes and number of samples per class respectively, in the $\\Chi$ -axis of the figure. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "C.12 Examining Actual Accuracy ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We also examine the impact of different variables and interventions on the ID performance, OOD performance, and ID/OOD alignment by analyzing the actual accuracy (no normalization). We show these results in Fig. 19 and Fig. 20. We observe that the actual accuracy trends are consistent with our previous findings where variables that improve (degrade) OOD generalization also show higher (lower) ID/OOD alignment. ", "page_idx": 30}, {"type": "text", "text": "C.13 Additional Resolution Results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Here we present additional resolution results. The image resolution shows a significant impact on the tunnel effect and OOD transferability. As shown in Fig. 19a, higher resolution images achieve higher performance in terms of ID accuracy, OOD accuracy, and ID/OOD alignment. ", "page_idx": 30}, {"type": "text", "text": "CNN. Fig. 21 and Fig. 22 show the results for VGGm-17 models trained on ImageNet-100 with varied image resolutions in experiments with and without augmentations. The tunnel effect decreases with the increase in image resolution in all experiments. ", "page_idx": 30}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/c0cf8c99710978e7fff9f5e5be166019940e1585ea95e4223ee40c271bcfa524.jpg", "img_caption": ["Figure 16: The tunnel effect is not specific to a particular ID dataset. The OOD linear probe accuracy for VGGm models trained on various ID datasets: CIFAR-10, CIFAR-100, and ImageNet100 with low resolution $(32\\times32)$ images in augmentation-free settings. The gray shaded area denotes the tunnel. The tunnel effect is prominent in all low-resolution settings. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/3011a5a8099c5e3d4fe71fbd4dacbe7a0c38f6d5b9704f631d3ac83fff860443.jpg", "img_caption": ["Figure 17: OOD performance of large pre-trained models. We show normalized linear probe accuracy on the ID (ImageNet-1K) and OOD datasets. The OOD curve is the average of 8 OOD datasets. Shaded area denotes the standard deviation. Most models do not show any tunnel effect. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "ViT. Fig. 23 and Fig. 24 show the results for $\\mathrm{ViT-T+}$ models trained on ImageNet-100 with varied image resolutions in experiments with and without augmentations. When augmentation is omitted, low-resolution inputs show a weaker tunnel effect than high-resolution inputs. However, both lowand high-resolution inputs significantly mitigate the tunnel effect when augmentations are used. ", "page_idx": 31}, {"type": "text", "text": "Rank Analysis. We examine the numerical rank of the representations evaluated on the ID dataset, following prior work [9]. In Fig. 25, we observe that a model trained on high-resolution images maintains a higher rank than a model trained on low-resolution images, corroborating our previous findings (Sec. 4.1.3). The representations rank for resolution $32\\times32$ plummets after the extractor, exhibiting the neural collapse phenomenon [10] whereas resolution $224\\times224$ retains a much higher rank in the corresponding layers. The degradation in OOD accuracy and ID representation rank is more pronounced for low-resolution images than for high-resolution images. As a result, a model trained on low-resolution images exhibits a stronger tunnel effect than one trained on high-resolution images. ", "page_idx": 31}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/cbd91532b59bbd011095eddd8c245a14320efdaf081725361d4c4859510494ec.jpg", "img_caption": ["Figure 18: Summary plots for large pre-trained models. The left figure exhibits $\\%$ OOD performance retained computed across training methods (SL and SSL). The right figure displays $\\%$ OOD performance retained computed across distinct architectures. SSL models show higher $\\%$ OOD performance retained than SL models. ", ""], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/cbf77e0200772a6f813772b6bb3223431985096ed943deee3c1486a24307ea43.jpg", "img_caption": ["(b) Comparison among pre-trained models "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/08da518d7d33c2f8936d86263aceb486f0bec23ecf70022559c00b02de539c27.jpg", "img_caption": ["Figure 19: Dataset variables. Impact of resolution, augmentation, and ID class count on ID accuracy, OOD accuracy, and ID/OOD alignment. This analysis is based on actual accuracy (no normalization) averaged over models and datasets. The accuracy and alignment score range from 0 to 1. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/a3380c6fff7463a04b860a3aa63214d1fed408907cb29555660f8ae2a29665e6.jpg", "img_caption": ["Figure 20: DNN architecture variables. Impact of depth (D), overparameterization level $(\\gamma)$ , spatial reduction $\\left(\\phi\\right)$ , and stem on ID accuracy, OOD accuracy, and ID/OOD alignment. This analysis is based on actual accuracy (no normalization) averaged over models and datasets. The accuracy and alignment score range from 0 to 1. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/2904a3f9a65022fb32f0eec1c922e261c0ccf8fe62c2964f60bc9f075622f558.jpg", "img_caption": ["(c) Resolution 128 \u00d7 128 ", "(d) Resolution $224\\times224$ "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 21: The OOD linear probe accuracy for similar sized VGGm-17 models trained on ImageNet100 dataset with varied image resolutions in augmentation-free settings. The OOD degradation reduces with the increase in image resolution. ", "page_idx": 33}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/0cf539baa3927037fba8a16fa6d6f60da3b8579c2f9872b1681ffa15e6e12fa1.jpg", "img_caption": ["(c) Resolution 128 \u00d7 128 "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "", "img_caption": ["(d) Resolution $224\\times224$ "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 22: The OOD linear probe accuracy for similar sized VGGm-17 models trained on ImageNet100 dataset with varied image resolutions while using image augmentations. The OOD degradation reduces with the increase in image resolution. ", "page_idx": 33}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/735ff0c6aef6e0dcb9f20e9d02fc6e0fcff8b2638b8fdc5d1681c2e7ae6ff28e.jpg", "img_caption": ["(c) Resolution 128 \u00d7 128 ", "(d) Resolution 224 \u00d7 224 "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 23: The OOD linear probe accuracy for similar sized ViT- $\\mathbf{T+}$ models trained on ImageNet-100 dataset with varied image resolutions in augmentation-free settings. The OOD degradation reduces with the increase in image resolution. ", "page_idx": 34}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/ec5a1d19484e5ed4d64fc9e5d8e3763be6605480f21d2db5c4778e60ae09a045.jpg", "img_caption": ["(c) Resolution 128 \u00d7 128 ", "(d) Resolution $224\\times224$ "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 24: The OOD linear probe accuracy for similar sized $\\mathbf{{V}i T-T+}$ models trained on ImageNet-100 dataset with varied image resolutions while using image augmentations. The OOD degradation reduces with the increase in image resolution. ", "page_idx": 34}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/aa1947f1e369d52386c8ada29aa88eebde68ebd48712e76ca9d800ee3aed884b.jpg", "img_caption": ["(a) Resolution $32\\times32$ (strong tunnel effect) "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "pOXgdFEB7q/tmp/d648a65ad4d4b5131af73cfe8dd6e8b68d3fbc85e4d0ff7afe03cb824b8a0e83.jpg", "img_caption": ["(b) Resolution $224\\times224$ (weak tunnel effect) "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 25: High-resolution model maintains a high representation rank. This figure corresponds to Fig. 1 in the main text. In this setting, identical VGGm-17 architectures are trained on identical ID datasets (100 ImageNet classes), where only the resolution is changed. The Y-axis shows the actual accuracy (no normalization) of linear probes trained on ID and OOD datasets. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading. The Y-axis also shows the numerical rank evaluated on the ID dataset. Here the OOD accuracy and ID representation rank align with the dynamics of the tunnel effect: the stronger the tunnel effect, the lower the OOD accuracy and ID representation rank. ", "page_idx": 35}, {"type": "text", "text": "D In-Distribution Performance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, we present the ID performance of all pre-trained DNNs. Table 10 shows the ID performance of 8 large-scale off-the-shelf models pre-tained on ImageNet-1K dataset. ", "page_idx": 35}, {"type": "text", "text": "Table 10: ID performance of large pre-trained models. Reported is the best top-1 accuracy $(\\%)$ on ImageNet-1K dataset $(224\\times224)$ . ", "page_idx": 35}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/67d63a5ceea821fec4f5328c6d28c0ccf02bab7b9fe0acd702bf7eafff88b372.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "Table 11 presents the results for each of the 64 DNN models studied in our main results. For each model and ID dataset combination, it gives the ID accuracy for the resolution/augmentation combination. ", "page_idx": 35}, {"type": "text", "text": "On ImageNet-100, for ViT- $\\cdot\\mathrm{T}+$ , ResNet-18, and VGGm-17, 8 models were trained per DNN architecture (4 resolution $\\textbf{x}2$ augmentation policies). The total is 24 DNNs. For the same ID dataset, 4 models were trained per DNN architecture, namely, ViT-T, ResNet-34, and VGGm-11 (2 resolution $\\times$ 2 augmentation policies). The total is 12 DNNs. An additional 2 models were trained per architecture for $\\mathrm{VGGm\\dag.}$ -11 and $\\mathrm{VGGm\\dagger}$ -17 (1 resolution $\\times\\ 2$ augmentation policies). The total is 4 DNNs. Overall, 40 DNNs were trained for ImageNet-100. ", "page_idx": 35}, {"type": "text", "text": "On CIFAR-100, for VGGm-11, $\\mathrm{VGGm\\dagger}$ -11, VGGm17, and VGGm\u2020-17, 2 models were trained per architecture (1 resolution $\\times\\,2$ augmentation policies). The total is 8 DNNs. ", "page_idx": 35}, {"type": "text", "text": "On CIFAR-10, for VGGm-11, VGGm-17, 2 models were trained per architecture (1 resolution $\\times\\,2$ augmentation polices). The total is 4 DNNs. ", "page_idx": 35}, {"type": "text", "text": "On ImageNet Subsets, for VGGm-11, 12 models were trained (1 resolution times 6 dataset configurations $\\times\\,2$ augmentation policies). The total is 12 DNNs. ", "page_idx": 35}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/4eef0226316dac1804683d9bd789c6a392fbad115e8641a804a4abf37cf3ba1a.jpg", "table_caption": ["Table 11: ID performance of DNNs trained on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet Subsets. Reported is the best top-1 accuracy $(\\%)$ for experiments with varied resolution and augmentation policies. The total number of DNN parameters is in Million and denoted by $\\#P$ . For each ID dataset, we group results based on DNN architecture. The sum of rows indicates the total number of DNNs i.e., 64. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "E Additional Statistical Results ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we report additional statistical results for different variables and interventions. Table 12 includes average results with confidence interval. Table 13 reports effect size (Cliff\u2019s Delta) and $p$ -value (Wilcoxon signed-rank test) for various pair comparisons where $\\#E$ denotes the number of paired experiments (total number of experiments is $2\\times E$ ). ", "page_idx": 36}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/33a4c7950904647ae92a679a46f963b6bc3172ea031d98919fb9237ec9e85e26.jpg", "table_caption": ["Table 12: Average Results. We report average results in 3 metrics with a $95\\%$ confidence interval (CI) for different variables and interventions. "], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "pOXgdFEB7q/tmp/4f152c7b66d8a49bd3b596e961e90c538a02762304c9a678a501531b7d3ba950.jpg", "table_caption": ["Table 13: Effect Size & $\\pmb{p}$ -value. A bigger effect size, $|\\delta|$ indicates a bigger statistical difference between each pair and the order is negligible $(N)<$ small $\\left(S\\right)<$ medium $(M)<$ large $(L)$ . "], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "F Classes of ImageNet-100 ID Dataset ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We list the 100 classes present in the ID dataset, ImageNet-100 [58]. This list can also be found at: https://github.com/HobbitLong/CMC/blob/master/imagenet100.txt ", "page_idx": 39}, {"type": "text", "text": "Rocking chair, pirate, computer keyboard, Rottweiler, Great Dane, tile roof, harmonica, langur, Gila monster, hognose snake, vacuum, Doberman, laptop, gasmask, mixing bowl, robin, throne, chime, bonnet, komondor, jean, moped, tub, rotisserie, African hunting dog, kuvasz, stretcher, garden spider, theater curtain, honeycomb, garter snake, wild boar, pedestal, bassinet, pickup, American lobster, sarong, mousetrap, coyote, hard disc, chocolate sauce, slide rule, wing, cauliflower, American Staffordshire terrier, meerkat, Chihuahua, lorikeet, bannister, tripod, head cabbage, stinkhorn, rock crab, papillon, park bench, reel, toy terrier, obelisk, walking stick, cocktail shaker, standard poodle, cinema, carbonara, red fox, little blue heron, gyromitra, Dutch oven, hare, dung beetle, iron, bottlecap, lampshade, mortarboard, purse, boathouse, ambulance, milk can, Mexican hairless, goose, boxer, gibbon, football helmet, car wheel, Shih-Tzu, Saluki, window screen, English foxhound, American coot, Walker hound, modem, vizsla, green mamba, pineapple, safety pin, borzoi, tabby, fiddler crab, leafhopper, Chesapeake Bay retriever, and ski mask. ", "page_idx": 39}, {"type": "text", "text": "Is there any semantic class overlap between ID and OOD datasets? There is no semantic class overlap between ImageNet-100 (ID dataset) and 8 other OOD datasets e.g., CIFAR-10, CIFAR-100, NINCO-64, CUB-200, Aircrafts-100, Oxford Pets-37, Flowers-102, and STL-10. ", "page_idx": 39}, {"type": "text", "text": "Only ImageNet-R (consisting of 200 classes) has 19 classes that overlap with ImageNet-100. This is expected and we know that ImageNet-R includes classes from ImageNet-1K dataset but incorporates significant distribution shifts using artistic renditions. The overlapping classes are: Gasmask, American lobster, Standard poodle, Red fox, Head cabbage, Harmonica, Ambulance, Gibbon, Pineapple, Chihuahua, Tabby, Pirate, Rottweiler, Lorikeet, Boxer, Pickup, Goose, Shih-Tzu, and Meerkat. ", "page_idx": 39}, {"type": "text", "text": "Also, there is no overlap between CIFAR-10 and CIFAR-100 so using one as ID and the other as OOD retains OOD challenges. It is evident that OOD evaluations in all our experiments are substantially robust due to dissimilar classes and significant distribution shifts between ID and OOD datasets. ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our claims in our introduction and abstract are directly supported by our Main Experiments. We also provided supporting experiments, which were not mentioned in the introduction or abstract. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Sec. 5, where we point out that we only study vision datasets and primarily supervised learning approaches. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our paper is empirical in nature and does not include mathematical proofs. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We described the implementation details in Appendix A. We included a link to our project website and code on the front page to enhance reproducibility. We used public datasets to facilitate reproduction. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We described dataset details in Appendix B. The datasets used in our work are publicly available. We included a link to our project website and code on the front page to enhance reproducibility. We intend to release the dataset of results and our SHAP Slope analysis code on our project website, where the code will have an open-source license. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: Besides the main paper, we described the training and test details as well as additional implementation details in Appendix A. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We used multiple metrics such as Pearson correlation, hypothesis testing, SHAP analysis, boxplots, etc. to provide statistical rigor in our results. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We specified compute resources in Appendix A.10, where we provided the total time in aggregate for running all experiments (48 days) and our hardware, which is one machine with four NVIDIA A5000 GPUs shared among lab members. Since we trained over 64 DNN backbones and almost 10,000 linear probes, it is not feasible to provide this information for each backbone and linear probe. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The research described in this paper adheres to the NeurIPS Code of Ethics. It does not involve human subjects, all datasets are public and widely used, and our work does not have the potential for harmful societal consequences. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: This work aims to understand the variables that impact embedding generalization and rigorously assess the tunnel effect hypothesis. It does not have any adverse societal impacts. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our work involves analysis of existing DNNs and does not have the potential for a high-risk of misuse or negative societal consequences. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We have included proper citations for the datasets and models used in our work. All the datasets and models are publicly available. Dataset information is given in Appendix B. Where possible, we provide the licenses for datasets; however, some widely used datasets lack license information (e.g., CIFAR-10 and CIFAR-100). ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We document our code for replicating our experiments and our analysis, and the SHAP analysis code is provided with an MIT license. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: We did not perform any research with human subjects. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: We did not perform any research with human subjects. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]