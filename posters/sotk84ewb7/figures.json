[{"figure_path": "SoTK84ewb7/figures/figures_1_1.jpg", "caption": "Figure 1: An illustration of our work. We assemble diverse deep priors from large models with frozen parameters for scene reconstruction from single images in a zero-shot manner.", "description": "This figure illustrates the overall workflow of the proposed Deep Prior Assembly method.  A single image is input to the system, where several large pre-trained models are used to extract different aspects of the scene. Grounded-SAM segments the image into individual objects. Stable Diffusion enhances and inpaints the segmented objects. Open-CLIP filters out poor results from Stable Diffusion, ensuring that only high-quality results proceed. Omnidata estimates depth information, providing geometric context. Finally, Shap-E generates 3D models of the objects. The result is a complete 3D scene reconstruction assembled from these individual components.", "section": "1 Introduction"}, {"figure_path": "SoTK84ewb7/figures/figures_2_1.jpg", "caption": "Figure 2: The overview of deep prior assembly. Given a single image of a 3D scene, we detect the instances and segment them with Grounded-SAM. After normalizing the size and center for the instances, we attempt to amend the quality of the instance images by enhancing and inpainting them. Here, we take a sofa in the image for example. Leveraging the Stable-Diffusion model, we generate a set of candidate images through image-to-image generation, with additional guidance from a text prompt of the instance category predicted by Grounded-SAM. We then filter out the bad generation samples with Open-CLIP by evaluating the cosine similarity between the generated instances and original one. After that, we generate multiple 3D model proposals for this instance with Shap-E from the Top-K generated instance images. Additionally, we estimate the depth of the origin input image with Omnidata as a 3D scene geometry prior. To estimate the layout, we propose an approach to optimize the location, orientation and scale for each 3D proposal by matching it with the estimated segmentation masks and the depths (the * for the example sofa). Finally, we choose the 3D model proposal with minimal matching error as the final prediction of this instance, and the final scene is generated by combining the generated 3D models for all detected instances.", "description": "This figure illustrates the Deep Prior Assembly pipeline.  It starts with a single image as input and uses several large pre-trained models sequentially to generate a 3D scene reconstruction.  Grounded-SAM segments objects, Stable Diffusion enhances images, OpenCLIP filters results, Shap-E generates 3D models, and Omnidata provides depth information for layout optimization.  The process iteratively refines the reconstruction until the optimal 3D scene is produced.", "section": "3 Method"}, {"figure_path": "SoTK84ewb7/figures/figures_4_1.jpg", "caption": "Figure 3: Examples on the effect of our pipeline. For the corrupted 2D instant segmented from the scene image, we leverage Stable-Diffusion to produce 6 amended generations. We then adopt Open-CLIP to filter out bad samples by judging the similarities and producing confidence scores (2) for the generations, and keep the Top-3 generated images. The shape generations with Shap-E from the amended images are significantly more complete and accurate than the one produced by the original corrupted image.", "description": "This figure demonstrates the pipeline's ability to enhance and inpaint corrupted 2D instances before 3D model generation.  It highlights the use of Stable Diffusion for generating multiple versions of an image, OpenCLIP to select the most suitable versions based on similarity to the original, and finally Shap-E to generate the 3D models. The results show that using this pipeline significantly improves the quality and completeness of the final 3D model compared to using the original, corrupted instance.", "section": "3.2 Assembling Large Models"}, {"figure_path": "SoTK84ewb7/figures/figures_4_2.jpg", "caption": "Figure 4: Illustration of the depth transform. The estimated depth maps from Omnidata is not scale-aware, resulting in scale inaccuracies and distortion in the back-projected depth point clouds. We achieve the accurate depth point cloud by first transforming the depth maps with the pre-solved scale and shift before back-projecting.", "description": "The figure shows the depth map estimation and back projection results from Omnidata.  The leftmost image shows the ground truth depth. The middle image shows the depth map estimated by Omnidata, which is not scale-aware and produces distorted results after direct back projection (rightmost image).  However, after applying a scale and shift transformation to correct for this, the back-projected point cloud (middle image) accurately matches the ground truth.", "section": "3.3 Recovering Scene Layout"}, {"figure_path": "SoTK84ewb7/figures/figures_5_1.jpg", "caption": "Figure 5: Effect of the 2D Matching. An example of optimizing the pose and scale for a chair. We visualize the optimization in 2D space. The red 2D points indicate the dense 2D point cloud sampled in the mask, which is the target. And the green 2D points donate the 2D projection of transformed 3D point clouds sampled from the generated shape of this chair instance. More robust registration is achieved with the proposed 2D matching constraint. The total 1,000 iterations take 9.2 seconds on a single 3090 GPU.", "description": "This figure demonstrates the impact of using 2D matching in addition to 3D matching during the pose and scale optimization step for 3D object placement in a scene.  The top row shows the optimization process without 2D matching, revealing instability and drift. The bottom row, with 2D matching added, illustrates a more robust and accurate registration process, leading to better alignment between the generated 3D model and the 2D mask from the input image. The visualization clearly shows how the green (projected 3D points) and red (sampled 2D points) converge more effectively with 2D matching, resulting in more accurate placement of the 3D chair model in the final scene. ", "section": "3.3 Recovering Scene Layout"}, {"figure_path": "SoTK84ewb7/figures/figures_6_1.jpg", "caption": "Figure 6: Comparisons on scene reconstruction from single images under the 3D-Front dataset.", "description": "This figure compares the results of scene reconstruction from single images using four different methods: Mesh R-CNN, Total3D, PanoRecon, and the authors' proposed method.  The ground truth (GT) is also shown for comparison. Each row represents a different input image, and the corresponding reconstructions generated by each method are displayed side-by-side. The figure demonstrates the superior performance of the authors' method in terms of accuracy and completeness of the generated scene geometries. The colored version of the authors' results is also presented to showcase the model's ability to generate textured 3D objects.", "section": "4 Experiments"}, {"figure_path": "SoTK84ewb7/figures/figures_7_1.jpg", "caption": "Figure 6: Comparisons on scene reconstruction from single images under the 3D-Front dataset.", "description": "This figure compares the scene reconstruction results from four different methods: Mesh R-CNN, Total3D, PanoRecon, and the proposed \"Ours\" method.  Each column shows the input image, the reconstruction from each method, and the ground truth. The \"Ours (Colored)\" column shows a colored version of the proposed method's reconstruction. The figure demonstrates the superior performance of the proposed method in generating accurate and detailed 3D scene reconstructions from single images, especially when compared to the other methods which tend to produce incomplete or less accurate results.", "section": "4 Experiments"}, {"figure_path": "SoTK84ewb7/figures/figures_8_1.jpg", "caption": "Figure 6: Comparisons on scene reconstruction from single images under the 3D-Front dataset.", "description": "This figure compares the scene reconstruction results from different methods (Mesh R-CNN, Total3D, PanoRecon, and the proposed Deep Prior Assembly) on the 3D-Front dataset.  Each row shows the input image, followed by the 3D reconstructions generated by each method, and finally the ground truth 3D model.  The figure visually demonstrates the superior performance of the proposed method in accurately reconstructing the scene, especially in terms of detail and completeness.", "section": "4 Experiments"}, {"figure_path": "SoTK84ewb7/figures/figures_14_1.jpg", "caption": "Figure 3: Examples on the effect of our pipeline. For the corrupted 2D instant segmented from the scene image, we leverage Stable-Diffusion to produce 6 amended generations. We then adopt Open-CLIP to filter out bad samples by judging the similarities and producing confidence scores (2) for the generations, and keep the Top-3 generated images. The shape generations with Shap-E from the amended images are significantly more complete and accurate than the one produced by the original corrupted image.", "description": "This figure shows an example of how the proposed pipeline improves the quality of 2D instance images before generating 3D models.  A corrupted 2D instance is first enhanced and inpainted using Stable Diffusion, generating multiple versions. OpenCLIP then filters these, selecting the top three most similar to the original. Finally, Shap-E generates 3D models from these improved images, resulting in significantly better and more complete models than those generated from the original, low-quality image.", "section": "3.2 Assembling Large Models"}, {"figure_path": "SoTK84ewb7/figures/figures_15_1.jpg", "caption": "Figure 10: The ablation study on the instance scale. We select one instance for each input image and show the amended instance images. The generations obtained with Shap-E under different instance scales are visualized on the right.", "description": "This figure shows an ablation study on how the scale of an instance in an image affects the quality of 3D model generation using the Shap-E model.  Two examples are shown: a coffee table and a sofa. For each, the original image segment is shown, along with the results of enhancing/inpainting that segment, and then multiple 3D generations using different scales for the input instance image. This demonstrates the model's sensitivity to input scale and highlights the optimal scaling used in the paper.", "section": "3.2 Assembling Large Models"}, {"figure_path": "SoTK84ewb7/figures/figures_15_2.jpg", "caption": "Figure 11: The ablation study on the confidence threshold.", "description": "The figure shows the ablation study on the confidence threshold (\u03c3) used in the \"Detect and Segment 2D instances\" step of the deep prior assembly framework.  The x-axis represents different confidence thresholds, and the y-axis shows the corresponding Chamfer-L1 distance, a metric used to evaluate the accuracy of 3D scene reconstruction.  The plot indicates that selecting an appropriate threshold is crucial for optimal performance.  A threshold that is too high may discard many instances, leading to incomplete scene reconstructions, while a threshold that is too low might include inaccurate instances, causing errors in the final reconstruction.", "section": "4.5 Ablation Study"}, {"figure_path": "SoTK84ewb7/figures/figures_15_3.jpg", "caption": "Figure 12: Comparisons on scene reconstruction from single images under 3D-Front and ScanNet datasets.", "description": "This figure compares the results of scene reconstruction from single images using different methods including PanoRecon, BUOL, Uni-3D, and the proposed method.  The top row shows the results on the 3D-Front dataset, and the bottom row shows the results on the ScanNet dataset. For each dataset, it shows the input image, and then reconstruction results from the three comparison methods, followed by the results of the proposed method (colored). The comparison highlights the superior performance of the proposed method in terms of accuracy and visual quality.", "section": "4.3 Scene Generation of Open-World Images"}, {"figure_path": "SoTK84ewb7/figures/figures_16_1.jpg", "caption": "Figure 13: Visual comparisons with ScenePrior under ScanNet dataset.", "description": "This figure compares the results of scene reconstruction using the proposed deep prior assembly method and the ScenePrior method on the ScanNet dataset.  It showcases example images from the dataset, along with the 3D reconstructions generated by each approach.  The visual comparison allows for an assessment of the relative accuracy and detail achieved by each method.  The \"Ours (Colored)\" column shows textured 3D models generated by the proposed method.", "section": "4.4 Scene Reconstruction from Real Images"}, {"figure_path": "SoTK84ewb7/figures/figures_16_2.jpg", "caption": "Figure 14: Scene reconstructions with backgrounds.", "description": "This figure shows two examples of scene reconstruction results by the proposed deep prior assembly method. The figure contains two subfigures: (a) BlendSwap and (b) Replica.  Each subfigure shows the input image, and the corresponding reconstructed 3D scene with backgrounds (e.g., walls and floors).  The reconstructions demonstrate the method's ability to recover not just the main objects in the scene but also the background geometry, creating more complete and realistic 3D models.", "section": "4.3 Scene Generation of Open-World Images"}, {"figure_path": "SoTK84ewb7/figures/figures_16_3.jpg", "caption": "Figure 15: Our scene reconstructions produced by deep prior assembly.", "description": "This figure showcases the results of the proposed Deep Prior Assembly method on several outdoor scenes containing complex objects and animals.  The figure is divided into two rows, each showing an input image followed by the corresponding reconstruction in grayscale and then color.  The results demonstrate the method's ability to reconstruct diverse outdoor scenes, including a street scene with cars and buildings, a park scene with a bench and trees, and a scene with penguins and a dog sitting on a turtle.", "section": "D Outdoor Scene Reconstruction"}]