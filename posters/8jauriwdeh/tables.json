[{"figure_path": "8JauriwDeH/tables/tables_2_1.jpg", "caption": "Table 1: Sample complexity bounds (for converging to an \u03b5 approximate solution) of various algorithms for SCO under heavy tailed stochastic gradients. Results are instantiated for smooth and strongly convex losses, and for the case where the gradient noise has bounded covariance equal to the Identity matrix. D\u2081 is the distance of the initial iterate from the optimal solution. For readability, we ignore the dependence of rates on the condition number. Observe all prior works have d log \u03b4\u207b\u00b9 dependence in the sample complexity.", "description": "This table compares the sample complexities of various algorithms for solving stochastic convex optimization (SCO) problems with heavy-tailed stochastic gradients.  The complexities are shown for smooth and strongly convex loss functions, assuming the gradient noise has bounded covariance. The table highlights the dependence on the dimension (d), the desired accuracy (\u03b5), the initial distance from the optimum (D\u2081), and the failure probability (\u03b4).  It shows that the proposed Clipped SGD algorithm achieves a significantly improved sample complexity bound compared to existing methods.", "section": "1.2 Related Work"}]