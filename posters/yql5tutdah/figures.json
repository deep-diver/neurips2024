[{"figure_path": "yQL5tutdaH/figures/figures_1_1.jpg", "caption": "Figure 1: The evaluation results of LVLMs on four instruction sets using the CHAIR with the average-based framework. Length refers to the average length of generated image descriptions. Each instruction set consists of six distinct instructions, and there is no overlap between instructions in different sets. All instructions prompt LVLMs to describe the image.", "description": "The figure displays the evaluation results of several Large Vision-Language Models (LVLMs) using the CHAIR metric under four different instruction sets.  Each instruction set contains six unique instructions designed to elicit image descriptions.  The key observation is that the average length of generated image descriptions varies significantly across the instruction sets, highlighting the impact of instruction length on the consistency of LVLMs' evaluation.", "section": "3.1 Experimental Settings"}, {"figure_path": "yQL5tutdaH/figures/figures_3_1.jpg", "caption": "Figure 1: The evaluation results of LVLMs on four instruction sets using the CHAIR with the average-based framework. Length refers to the average length of generated image descriptions. Each instruction set consists of six distinct instructions, and there is no overlap between instructions in different sets. All instructions prompt LVLMs to describe the image.", "description": "This figure displays the evaluation results of several Large Vision-Language Models (LVLMs) using the CHAIR metric under four different instruction sets.  It highlights the inconsistency of the average-based evaluation framework when dealing with instruction sets that produce image descriptions of significantly different lengths. Each bar represents the hallucination rate for a specific LVLM and instruction set, illustrating how the average description length affects the consistency of the evaluation.", "section": "1 Introduction"}, {"figure_path": "yQL5tutdaH/figures/figures_4_1.jpg", "caption": "Figure 1: The evaluation results of LVLMs on four instruction sets using the CHAIR with the average-based framework. Length refers to the average length of generated image descriptions. Each instruction set consists of six distinct instructions, and there is no overlap between instructions in different sets. All instructions prompt LVLMs to describe the image.", "description": "The figure displays the evaluation results of several Large Vision-Language Models (LVLMs) using the CHAIR metric, which measures object hallucination.  The results are grouped into four sets, each comprising six unique instructions that prompt the models to describe an image.  A key observation is the inconsistent evaluation across instruction sets which produce image descriptions of varying lengths, highlighting the limitation of the average-based evaluation framework.", "section": "1 Introduction"}, {"figure_path": "yQL5tutdaH/figures/figures_5_1.jpg", "caption": "Figure 4: Illustrations of the average-based evaluation framework (ABF) and our LeHaCE framework. The left figure presents the object hallucination evaluation of LLaVa on two instruction sets. The right figure presents the object hallucination evaluation of LLaVa and mPLUG-Owl on the same set of instructions.", "description": "This figure compares the average-based framework (ABF) and the proposed LeHaCE framework for evaluating object hallucination in large vision-language models (LVLMs). The left panel shows how ABF's evaluation of LLaVA varies significantly depending on the instruction set used, because different instruction sets lead to image descriptions with varying lengths. In contrast, LeHaCE provides more stable evaluations across different instruction sets by fitting a length-hallucination curve and evaluating at a consistent description length. The right panel illustrates the unfairness of ABF when comparing different LVLMs (LLaVA and mPLUG-Owl). ABF's comparison is influenced by varying description lengths, while LeHaCE provides a fair comparison by evaluating at the same description length.", "section": "4 Length-Hallucination Curve Based Hallucination Evaluation Framework"}, {"figure_path": "yQL5tutdaH/figures/figures_9_1.jpg", "caption": "Figure 5: Average RSD of CHAIR with the LeHaCE framework at different lengths, lower is better. ABF refers to the average-based evaluation framework.", "description": "This figure displays the relative standard deviation (RSD) of CHAIR scores for both the average-based framework (ABF) and the proposed LeHaCE framework across various average description lengths.  The lower the RSD, the more stable the evaluation method. The plot shows the RSD for four different numbers of instructions (5, 6, 7, and 8) for two different LVLMs (LLaVA and Qwen-VL). It demonstrates LeHaCE's improved stability in hallucination evaluation compared to ABF across a range of description lengths.", "section": "4.4 Stability of LeHaCE"}, {"figure_path": "yQL5tutdaH/figures/figures_15_1.jpg", "caption": "Figure 6: Left: Comparison of the hallucination rates between image descriptions containing hallucinogenic words and image descriptions without hallucinogenic words. Right: Proportion of hallucinogenic words in image descriptions containing hallucinations under different instructions.", "description": "The figure on the left compares hallucination rates in image descriptions with and without what the authors call \"hallucinogenic words.\"  These are words that seem to trigger more hallucinations, like \"in addition,\" \"also,\" and \"as well.\"  The bar chart shows that the rate of hallucinations is higher when these words are present. The right-hand figure shows the percentage of these hallucinogenic words in image descriptions containing hallucinations, revealing a positive correlation between the frequency of these words and hallucination rate.", "section": "7.2 Further Exploration"}, {"figure_path": "yQL5tutdaH/figures/figures_17_1.jpg", "caption": "Figure 1: The evaluation results of LVLMs on four instruction sets using the CHAIR with the average-based framework. Length refers to the average length of generated image descriptions. Each instruction set consists of six distinct instructions, and there is no overlap between instructions in different sets. All instructions prompt LVLMs to describe the image.", "description": "This figure displays the evaluation results of several large vision-language models (LVLMs) using the CHAIR metric, focusing on the impact of instruction set length on object hallucination.  Four different instruction sets were used, each set prompting the models to describe the same image but with varying instruction lengths. The graph shows that the average-based framework, which averages results across instructions, yields inconsistent evaluations depending on the length of the image descriptions produced by the instructions.", "section": "3.1 Experimental Settings"}]