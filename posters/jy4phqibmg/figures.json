[{"figure_path": "jY4PhQibmg/figures/figures_4_1.jpg", "caption": "Figure 1: The recurrent representation of GSA. means taking \u00e6t as input.", "description": "This figure shows the recurrent representation of the Gated Slot Attention (GSA) model.  It illustrates how the model updates its hidden states (K and V) at each time step (t), incorporating a gating mechanism (\u03b1t) to control information flow. The model receives input (xt), updates its memory using the input and previous states, then outputs (ot).  The gating mechanism helps to selectively retain and forget information, improving efficiency and performance.", "section": "3 Method"}, {"figure_path": "jY4PhQibmg/figures/figures_4_2.jpg", "caption": "Figure 2: The backbone of our proposed GSA models.", "description": "This figure shows the architecture of the proposed Gated Slot Attention (GSA) model.  It consists of L GSA blocks stacked together. Each GSA block includes a GSA token mixing layer followed by a Gated Linear Unit (GLU) channel mixing layer. The GSA token mixing layer uses a multi-head attention mechanism. The GSA layer includes two passes of gated linear attention, and it is illustrated in Figure 1.", "section": "3.3 Neural Architecture"}, {"figure_path": "jY4PhQibmg/figures/figures_5_1.jpg", "caption": "Figure 3: The recurrent representation of GSA.  means taking \u00e6t as input.", "description": "This figure shows the recurrent representation of the Gated Slot Attention (GSA) model.  It illustrates how the model's recurrent state is updated at each time step using the input token (xt) and gate values. The figure visually represents the flow of information and the calculations involved in the model's operation. The key components of GSA, including the forget gate (at) and update values, are also depicted. This diagram helps in understanding how GSA uses a two-pass gated linear attention mechanism for more efficient training and inference.", "section": "3.3 Neural Architecture"}, {"figure_path": "jY4PhQibmg/figures/figures_7_1.jpg", "caption": "Figure 4: (a) Training throughput of various 1.3B models on a single H800 GPU, with a fixed batch size containing 16K tokens. \u201cGSA w/o recomp.\u201d indicates the use of the GSA kernel without hidden state recomputation during the backward pass. (b) Memory footprint (in GiB) of each 1.3B model during training with a batch size containing 16K tokens. (c) Inference latency (in seconds) of each 1.3B model on a single H800 GPU with 2K prefix tokens and a batch size of 1.", "description": "This figure compares the training throughput, memory footprint, and inference latency of different 1.3B parameter models (Xfmr++, Mamba, RetNet, GLA, and GSA) on a single H800 GPU.  It shows the impact of batch size and sequence length on training speed, memory usage, and inference time.  The 'GSA w/o recomp.' line in (a) demonstrates the effect of recomputing hidden states during backpropagation to reduce memory consumption.  The results indicate GSA's relative efficiency and small memory footprint compared to other models.", "section": "4.1.4 Efficiency"}, {"figure_path": "jY4PhQibmg/figures/figures_18_1.jpg", "caption": "Figure 1: The recurrent representation of GSA.  means taking \u00e6t as input.", "description": "This figure illustrates the recurrent structure of the Gated Slot Attention (GSA) model.  It shows how the input at each time step (xt) is processed through a series of operations involving linear transformations, gating mechanisms, and softmax to update the recurrent states (Kt, Vt) and generate the output (ot). The dashed lines represent the flow of information between different time steps, emphasizing the recurrent nature of the model.", "section": "3 Method"}, {"figure_path": "jY4PhQibmg/figures/figures_19_1.jpg", "caption": "Figure 1: The recurrent representation of GSA.  means taking \u00e6t as input.", "description": "This figure shows a detailed illustration of the recurrent representation of the Gated Slot Attention (GSA) model. It visually depicts how the input at time step t (xt) is processed through the model's components, including the forget gate (at), to update the hidden state and produce the output (ot).  It highlights the recursive nature of the GSA architecture and how information from previous time steps is used to inform the current output.", "section": "3.3 Neural Architecture"}]