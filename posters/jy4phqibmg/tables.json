[{"figure_path": "jY4PhQibmg/tables/tables_5_1.jpg", "caption": "Table 1: The zero-shot results of 1.3B and 2.7B models evaluated by lm-evaluation-harness [21]. L denotes number of layer while d denotes the model dimension.", "description": "This table presents the zero-shot performance of 1.3B and 2.7B parameter models on various commonsense reasoning and world knowledge tasks.  The performance is measured using perplexity (ppl) and accuracy (acc) and compared across different models (Xfmr++, Mamba, RetNet, GLA, HGRN2, and GSA) with varying hidden state sizes.  The table highlights the comparative performance of GSA against other state-of-the-art models in this zero-shot setting.", "section": "4.1 Language Modeling"}, {"figure_path": "jY4PhQibmg/tables/tables_5_2.jpg", "caption": "Table 3: Performance comparison across various 7B models. \u2020 denotes models using softmax-attention.", "description": "This table compares the performance of different 7B parameter models (including those trained from scratch and those fine-tuned from Mistral 7B) on various tasks.  The tasks assess performance in commonsense reasoning, world knowledge, and aggregated benchmarks.  The table highlights the relative performance of GSA compared to other models, focusing on its effectiveness in different model sizes and training data amounts.", "section": "4.2 Finetuning Pretrained Transformers to RNNs"}, {"figure_path": "jY4PhQibmg/tables/tables_6_1.jpg", "caption": "Table 2: Ablation study results for 340M models trained on 10B Slimpajama tokens.", "description": "This table presents the results of ablation studies conducted on a 340M parameter model trained using 10B tokens from the Slimpajama corpus.  It shows the impact of different design choices on the model's performance, measured by perplexity (PPL).  Specifically, it examines the effects of removing the gating mechanism (comparing to ABC), using a data-independent decay instead of a data-dependent one,  testing different non-linearities (softmax, Swish, ReLU, and ReLU squared), and varying the number of memory slots (32, 64, and 128). Lower perplexity indicates better performance.", "section": "4.1.3 Ablation"}, {"figure_path": "jY4PhQibmg/tables/tables_7_1.jpg", "caption": "Table 1: The zero-shot results of 1.3B and 2.7B models evaluated by lm-evaluation-harness [21]. L denotes number of layer while d denotes the model dimension.", "description": "This table presents the zero-shot performance of 1.3B and 2.7B parameter models on various commonsense reasoning and knowledge tasks.  The results are compared against several other models, showing GSA's competitive performance, particularly with smaller state sizes.  Metrics include perplexity (ppl), accuracy (acc) and are averaged across multiple tasks.", "section": "4.1 Language Modeling"}, {"figure_path": "jY4PhQibmg/tables/tables_8_1.jpg", "caption": "Table 4: Long-context performance comparison.", "description": "This table presents a comparison of the performance of various language models on long-context tasks.  The models are evaluated on four tasks: Qasper, NarrativeQA, QUALITY, and QMSum. The input for each task is truncated to 16K tokens, which is 8 times the training length.  The table shows that GSA consistently outperforms other subquadratic models, and even outperforms RWKV6 and Mamba (which were trained from scratch on >1T tokens) when finetuned from Mistral 7B on only 20B tokens.", "section": "4.1 Language Modeling"}]