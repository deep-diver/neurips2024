{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-03", "reason": "This paper introduced the Transformer architecture, which is foundational to the field of large language models and heavily influences the current work."}, {"fullname_first_author": "Aidan N. Gomez", "paper_title": "Attention is all you need", "publication_date": "2017-12-03", "reason": "This paper introduced the Transformer architecture, which is foundational to the field of large language models and heavily influences the current work."}, {"fullname_first_author": "Sainbayar Sukhbaatar", "paper_title": "End-to-End Memory Networks", "publication_date": "2015-12-07", "reason": "This paper introduced the concept of memory networks, which are directly relevant to the development of linear attention mechanisms that have bounded memory."}, {"fullname_first_author": "Felix A. Gers", "paper_title": "Learning to forget: continual prediction with LSTM", "publication_date": "1999-07-01", "reason": "This paper introduced the Long Short-Term Memory (LSTM) network, which is foundational to the field of recurrent neural networks and the use of gating mechanisms."}, {"fullname_first_author": "H. Peng", "paper_title": "ABC: Attention with bounded-memory control", "publication_date": "2022-05-01", "reason": "This paper introduced the ABC model which directly inspired the design of the GSA model presented in the current paper."}]}