[{"figure_path": "mpDbWjLzfT/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of our setting to the existing adaptation settings. Our proposed setting meets all the criteria that are expected in a comprehensive adaptation framework.", "description": "The table compares the characteristics of different domain adaptation settings, including UDA, source-free UDA, and TTA.  It highlights the unique aspects of the proposed CONTRAST approach, which addresses dynamic data distributions and the absence of source data, aspects not fully covered by existing methods. The comparison is based on the availability of source data for adaptation, whether adaptation happens during testing (on the fly), the presence of dynamic target distributions, and the use of multiple source models. CONTRAST is shown to be the only method that covers all these criteria.", "section": "2 Related Works"}, {"figure_path": "mpDbWjLzfT/tables/tables_8_1.jpg", "caption": "Table 2: Results on CIFAR-100C. We take four source models trained on Clear, Snow, Fog, and Frost. We employ these models for adaptation on 15 sequential test domains. This table illustrates that even in the dynamic environment X+CONTRAST performs better than X-Best, which is the direct consequence of optimal aggregation of source models as well as better preservation of source knowledge. (Results in error rate \u2193 (in %))", "description": "This table presents the results of experiments conducted on the CIFAR-100C dataset, which involves adapting four source models (trained on different weather conditions) to 15 sequential test domains with varying weather conditions.  The table compares the performance of the proposed CONTRAST method against three baseline methods (Tent, EaTA, and COTTA), illustrating that CONTRAST consistently outperforms the best-performing single-source adaptation method (X-Best) across all test domains. The results highlight CONTRAST's ability to optimally combine multiple source models and retain source knowledge effectively over time.", "section": "4 Experiments"}, {"figure_path": "mpDbWjLzfT/tables/tables_8_2.jpg", "caption": "Table 3: Results on Office-Home. We train three source models using three domains in this dataset and use them for testing on the remaining domain under the TTA setting. Our results demonstrate that X+CONTRAST consistently outperforms all of the baselines (X) (% error).", "description": "This table presents the results of the experiment conducted on the Office-Home dataset. The experiment is set up in a way that three source models are trained using three domains from the Office-Home dataset, and they are used for testing on the remaining domain. This is a test-time adaptation (TTA) setting. The table compares the performance of the proposed method (X+CONTRAST) with several baseline methods (X). The results show that the proposed method consistently outperforms all baselines in terms of error rate. The table highlights the effectiveness of the proposed method in adapting to the target domain even without access to the source data during the test-time adaptation.", "section": "4 Experiments"}, {"figure_path": "mpDbWjLzfT/tables/tables_16_1.jpg", "caption": "Table 4: Results on Digits dataset. We train the source models using four digit datasets to perform inference on the remaining dataset. The column abbreviations correspond to the datasets as follows: \u2018MM\u2019 for MNIST-M, \u2018MT\u2019 for MNIST, \u2018UP\u2019 for USPS, \u2018SV\u2019 for SVHN, and \u2018SY\u2019 for Synthetic Digits.. The table (reporting % error rate(\u2193)) shows that X+CONTRAST outperforms all of the baselines (X-Best) consistently.", "description": "This table presents the results of the digit classification experiment.  Four source models are trained on four different digit datasets (MNIST-M, MNIST, USPS, SVHN, and Synthetic Digits). Each column represents a target dataset (one of the five datasets is held out as a target for each test). The table shows that using the CONTRAST method with different test-time adaptation methods (Tent, EaTA, CoTTA) consistently outperforms the best individual source model for each adaptation setting.  The table reports the error rate (percentage of misclassified samples) for each method and each dataset. ", "section": "B Results on Digits"}, {"figure_path": "mpDbWjLzfT/tables/tables_16_2.jpg", "caption": "Table 2: Results on CIFAR-100C. We take four source models trained on Clear, Snow, Fog, and Frost. We employ these models for adaptation on 15 sequential test domains. This table illustrates that even in the dynamic environment X+CONTRAST performs better than X-Best, which is the direct consequence of optimal aggregation of source models as well as better preservation of source knowledge. (Results in error rate \u2193 (in %))", "description": "This table presents the results of applying CONTRAST and several baseline methods on the CIFAR-100C dataset.  Four source models (trained on 'Clear', 'Snow', 'Fog', and 'Frost' conditions) are used for continual adaptation across 15 noisy test domains.  The table shows error rates (%) for each method across the domains, demonstrating that CONTRAST consistently outperforms baseline single-source methods by effectively combining multiple source models while mitigating catastrophic forgetting.", "section": "4 Experiments"}, {"figure_path": "mpDbWjLzfT/tables/tables_17_1.jpg", "caption": "Table 6: Effect of initialization and step size choice. Error rate on Office-Home under different choices of initialization and step sizes.", "description": "This table presents the error rate results on the Office-Home dataset under the same experimental setting as Table 3 (Appendix) with Tent as the adaptation method, but with different initialization and learning rate choices for solving the optimization in (4). It is evident from the table that our chosen initialization and adaptive learning rate result in the highest accuracy gain.", "section": "D Ablation Study"}, {"figure_path": "mpDbWjLzfT/tables/tables_17_2.jpg", "caption": "Table 7: Initialization based on Entropy. The table shows the results of entropy based initialization.", "description": "This ablation study in Section D.1 of the paper compares the performance of CONTRAST using two different weight initialization methods: one based on entropy and another based on KL divergence.  The results show that initializing weights based on KL divergence, which favors the most correlated source model, significantly outperforms entropy-based initialization, demonstrating the superiority of the proposed initialization strategy in achieving optimal performance.", "section": "D Ablation Study"}, {"figure_path": "mpDbWjLzfT/tables/tables_18_1.jpg", "caption": "Table 8: Choice of model update (CONTRAST+CoTTA). In our experiments using CoTTA as the model update method on CIFAR100-C, we tested four scenarios: updating all models, updating only the least correlated model, updating subset of model, and updating only the most correlated model. Our results indicate that our model selection approach produces the most favorable outcome. (Results in error rate \u2193 (in %))", "description": "This table presents an ablation study on the model update policy within the CONTRAST framework using CoTTA as the underlying single-source adaptation method. It compares the performance of four different strategies: updating all models, updating only the least correlated model, updating a subset of models based on weights, and updating only the most correlated model. The results, presented as error rates, show that updating only the most correlated model yields the best performance, highlighting the effectiveness of this approach in preventing catastrophic forgetting.", "section": "D Ablation Study"}, {"figure_path": "mpDbWjLzfT/tables/tables_18_2.jpg", "caption": "Table 8: Choice of model update (CONTRAST+CoTTA). In our experiments using CoTTA as the model update method on CIFAR100-C, we tested four scenarios: updating all models, updating only the least correlated model, updating subset of model, and updating only the most correlated model. Our results indicate that our model selection approach produces the most favorable outcome. (Results in error rate \u2193 (in %))", "description": "This table presents the ablation study on the model update policy of CONTRAST using CoTTA as the single-source adaptation method. It compares four different update strategies: updating all models, updating only the least correlated model, updating a subset of models, and updating only the most correlated model. The results, presented as error rates, show that updating only the most correlated model yields the best performance, highlighting the effectiveness of the proposed approach in mitigating catastrophic forgetting.", "section": "D Ablation Study"}, {"figure_path": "mpDbWjLzfT/tables/tables_18_3.jpg", "caption": "Table 2: Results on CIFAR-100C. We take four source models trained on Clear, Snow, Fog, and Frost. We employ these models for adaptation on 15 sequential test domains. This table illustrates that even in the dynamic environment X+CONTRAST performs better than X-Best, which is the direct consequence of optimal aggregation of source models as well as better preservation of source knowledge. (Results in error rate \u2193 (in %))", "description": "This table presents the results of applying CONTRAST and several baseline methods (Tent, COTTA, EaTA) on the CIFAR-100C dataset with 15 different noise types.  Four source models (trained on clean images and three different noise types) are used for multi-source adaptation. The table shows the error rate for each method across all noise types, demonstrating the superiority of CONTRAST in achieving lower error rates while mitigating catastrophic forgetting.  The \"X-Best\" and \"X+CONTRAST\" columns represent the best single-source performance and the performance when integrating each single-source method with CONTRAST, respectively.", "section": "4 Experiments"}, {"figure_path": "mpDbWjLzfT/tables/tables_19_1.jpg", "caption": "Table 11: Comparison with MSDA. The table compares the performance of our method with MSDA approach DECISION. (Results in error-rate % \u2193)", "description": "This table compares the performance of CONTRAST+Tent against the DECISION method from a prior work on multi-source domain adaptation.  The comparison highlights the superior performance of CONTRAST+Tent, particularly in online adaptation settings where data arrives in batches.  The results are presented as error rates (percentage) across various noise conditions (GN to JPEG) in the CIFAR-100C dataset. The lower the error rate, the better the performance.", "section": "D.4 Comparison with MSDA"}, {"figure_path": "mpDbWjLzfT/tables/tables_19_2.jpg", "caption": "Table 2: Results on CIFAR-100C. We take four source models trained on Clear, Snow, Fog, and Frost. We employ these models for adaptation on 15 sequential test domains. This table illustrates that even in the dynamic environment X+CONTRAST performs better than X-Best, which is the direct consequence of optimal aggregation of source models as well as better preservation of source knowledge. (Results in error rate \u2193 (in %))", "description": "This table presents the results of applying the CONTRAST method and several baselines on the CIFAR-100C dataset.  Four source models (trained on 'Clear', 'Snow', 'Fog', and 'Frost' conditions) are used, and their performance is evaluated across 15 different noisy test domains. The table compares the error rates (lower is better) of CONTRAST against several single-source adaptation methods (Tent, EaTA, COTTA), showing the best and worst performance achieved by each method across the 15 test domains.  The results demonstrate that CONTRAST consistently outperforms even the best-performing single-source method in a dynamic setting, highlighting its effectiveness in combining source models and mitigating forgetting.", "section": "4 Experiments"}, {"figure_path": "mpDbWjLzfT/tables/tables_20_1.jpg", "caption": "Table 13: Result on Cityscape to ACDC: In this experiment, we test our method on the test data from individual weather conditions (static test distribution) of ACDC. The source models are trained on the train set of Cityscapes and its noisy variants. Our method clearly outperforms baseline adaptation method. (Results in % mIoU)", "description": "This table presents the results of semantic segmentation on the ACDC dataset using models trained on Cityscapes and its noisy variants.  The experiment uses a static test distribution with various weather conditions. The table shows that the proposed CONTRAST method outperforms the baseline Tent method in terms of mean Intersection over Union (mIoU).", "section": "F Semantic Segmentation"}, {"figure_path": "mpDbWjLzfT/tables/tables_21_1.jpg", "caption": "Table 2: Results on CIFAR-100C. We take four source models trained on Clear, Snow, Fog, and Frost. We employ these models for adaptation on 15 sequential test domains. This table illustrates that even in the dynamic environment X+CONTRAST performs better than X-Best, which is the direct consequence of optimal aggregation of source models as well as better preservation of source knowledge. (Results in error rate \u2193 (in %))", "description": "This table presents the results of the proposed CONTRAST method on the CIFAR-100C dataset, which involves 15 test domains with varying levels of noise (Snow, Fog, Frost).  The table compares the performance of CONTRAST against several baseline methods (Tent, EaTA, and COTTA). Each baseline is tested both individually (X-Best and X-Worst) and integrated into CONTRAST (X+CONTRAST). The error rates (in %) are presented for each method, demonstrating that CONTRAST consistently outperforms the best individual baseline across all test domains.", "section": "4 Experiments"}]