[{"type": "text", "text": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhichao Hou1 Weizhi Gao1 Yuchen Shen2 Feiyi Wang3 Xiaorui Liu1 \u2217 ", "page_idx": 0}, {"type": "text", "text": "1North Carolina State University, 2Carnegie Mellon University, 3Oak Ridge National Laboratory ", "page_idx": 0}, {"type": "text", "text": "{zhou4,wgao23,xliu96}@ncsu.edu yuchens3@cs.cmu.edu fwang2@ornl.gov ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer-based architectures have dominated various areas of machine learning in recent years. In this paper, we introduce a novel robust attention mechanism designed to enhance the resilience of transformer-based architectures. Crucially, this technique can be integrated into existing transformers as a plug-and-play layer, improving their robustness without the need for additional training or fine-tuning. Through comprehensive experiments and ablation studies, we demonstrate that our ProTransformer significantly enhances the robustness of transformer models across a variety of prediction tasks, attack mechanisms, backbone architectures, and data domains. Notably, without further fine-tuning, the ProTransformer consistently improves the performance of vanilla transformers by $19.5\\%$ , $28.3\\%$ , $16.1\\%$ , and $11.4\\%$ for BERT, ALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler attack. Furthermore, ProTransformer shows promising resilience in large language models (LLMs) against prompting-based attacks, improving the performance of T5 and LLaMA by $24.8\\%$ and $17.8\\%$ , respectively, and enhancing Vicuna by an average of $10.4\\%$ against the Jailbreaking attack. Beyond the language domain, ProTransformer also demonstrates outstanding robustness in both vision and graph domains. Our code is available at https://github.com/chrishzc/ProTransformer. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, attention mechanisms and transformer-based architectures have drawn significant attention across many domains in machine learning, such as natural language processing (NLP) [1, 2], computer vision [3, 4], and graph learning [5, 6]. In particular, transformers have demonstrated superior capabilities to learn and model complex relations in data through powerful and universal attention mechanisms, and they have dominated many popular NLP tasks such as topic classification, sentiment analysis, textual entailment, machine translation, dialogue generation, etc [2]. Despite their success in NLP and beyond, many recent studies have demonstrated that transformers are highly vulnerable to adversarial attacks such that even small modifications to the input can easily fool the model [7, 8, 9]. However, most research on transformer architectures focuses on accuracy and efficiency, largely ignoring their security and robustness [10, 11]. ", "page_idx": 0}, {"type": "text", "text": "With the increasing popularity of Large Language Models (LLMs) [12, 13], the robustness and security concerns of transformer architectures become particularly of interest. It has been shown that malicious attackers can invade the language models through various approaches as shown in Figure 1. The attacker can modify the input content in text attacks [14] or the prompt template in prompt attacks to mislead the model predictions [15]. Moreover, by adding adversarial suffixes, the jailbreaking attack [16] can prompt a LLM to generate toxic and illegal content which could lead to catastrophic legal and ethical impacts such as malicious speech or privacy leaks. Given the broad applications of transformers and their vulnerabilities under attacks, it is imperative to design a universal and effective strategy to enhance the robustness of transformers. ", "page_idx": 0}, {"type": "image", "img_path": "UkauUrTbxx/tmp/6dc5aa928dd02ecfa481e7d1044eb38b46d3b58c19d0f495d32a6e364d1a94bb.jpg", "img_caption": ["Figure 1: Various attack mechanisms on language models. Classic text attacks modify the input content using typos or synonyms; Prompt attacks perturb the prompt template within the input; and Jailbreaks append adversarial, non-semantic suffixes to manipulate the model into producing malicious outputs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Existing research attempting to improve the robustness of transformers can be roughly divided into empirical defenses [17, 18, 19, 20, 21] and certifiable defenses [22, 23, 24, 25]. Nevertheless, these defenses require excessive computation costs for training, inference, or both. In addition to these architecture-agnostic defenses, there are also several works proposing to enhance the robustness of transformers architecture [26, 27, 28, 29]. However, these approaches either require substantial computations or rely on specific domain knowledge, which hinders their extensions to larger models or broader application domains. ", "page_idx": 1}, {"type": "text", "text": "In this paper, given the limitations of existing works and the enormous training cost of transformers, we aim to robustify transformer architectures via a plugand-play paradigm without additional training or finetuning. Our proposed ProAttention (Algorithm 1) can be readily plugged into the given transformers to convert them to ProTransformer (as shown in Figure 2) with significantly stronger robustness. Specifically, our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "image", "img_path": "UkauUrTbxx/tmp/c8a805f0d8b97dce5f003d65713ae3dbcd9168b69f4f30b3434f4e2cbfb03991.jpg", "img_caption": ["Figure 2: Overview of ProTransformer. ProAttention can be plugged into pretrained transformers without additional training. The ProTransformer is versatile and can be applied across various domains, including language, image, and graph. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "\u2022 We establish a novel connection between the attention mechanism in transformers and the weighted least square estimator. We provide interpretation and numerical simulation to reveal its vulnerability against potential adversarial attacks. ", "page_idx": 1}, {"type": "text", "text": "\u2022 From our new perspective, we propose robust token estimators to improve the resilience of token aggregation against adversarial attacks. We also propose an efficient Newton-IRLS algorithm to approximate the non-convex and non-smooth robust token estimator with convergence guarantees. The derived algorithm can be plugged into the given transformer as a plug-and-play layer to enhance its robustness against attacks even without additional training or fine-tuning. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Our comprehensive experiments and ablation studies demonstrate that the proposed ProTransformer is effective, efficient, and generalizable. It significantly improves the robustness of transformers across various machine learning tasks, attack mechanisms, backbone architectures, and data domains such as language, vision, and graphs. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we mainly summarize related works on the attacks and defenses of transformers focusing on language domains since this is the focus of this paper. ", "page_idx": 2}, {"type": "text", "text": "Attacks. Compared to the attack mechanisms in vision domain [30, 31], the text attacks in the language domain are highly complicated due to the natural irregularity of data structure. According to the perturbation units, text attacks can be classified into character-level [7, 32], word-level [33, 34, 35, 36, 9, 14, 37], sentence-level [38], and multi-level [39, 40, 8]. These classic text attacks typically generate adversarial examples through misspellings, synonym replacement, etc. In the era of LLMs, several new types of attacks have emerged, such as jailbreak attacks [16, 41, 42, 43] and prompt injection [44, 45, 46]. These prompting-based attacks aim to trick models into generating unsafe outputs using adversarially crafted prompts. ", "page_idx": 2}, {"type": "text", "text": "Defenses. There have been some works proposed to defend against adversarial text attacks from various perspectives. Empirical defenses, such as data augmentation [17] and adversarial training [47, 18, 19, 20, 21], attempt to robustify models by exposing them to a wider range of adversaries during training. On the other hand, several certifiable defenses [24, 25, 22, 23] have been proposed to guarantee the model robustness regardless of the attacks. However, these defenses require excessive computation costs for training, inference, or both, which limits their application in large-scale problems such as LLMs. Besides, all these methods are typically architecture-agnostic, which are orthogonal to and can be combined with our proposed defenses on the transformer architecture to further enhance the robustness. ", "page_idx": 2}, {"type": "text", "text": "To safeguard the transformers, several endeavors have been made from the transformer architecture perspective. [26] modifies the attention mechanism and position embedding to robustify text-to-speech transformers. In the crisis detection and recognition task, [27] proposes an end-to-end attention-based classifier to enhance robustness. For tabular data, TableFormer [28] adopts structural-aware table-text encodings that are more robust to row and column order perturbations. However, these architectures are tailored for specific tasks, which require specific domain knowledge and can not be generalized across tasks. [29] proposes a general framework for self-attention modules via robust kernel density estimation (RKDE). However, this method introduces excess computation cost and shows relatively limited robustness improvement. Generally speaking, existing approaches either require substantial computations or rely on specific domain knowledge, which hinders their extensions to larger models or broader application domains. ", "page_idx": 2}, {"type": "text", "text": "3 ProTransformer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The main goal of this paper is to design robust self-attention mechanisms that are more resilient to adversarial attacks so they can be applied to robustify Transformer architectures. In this section, we first provide a new interpretation of the self-attention mechanism in Transformer architecture as the weighted least-square token estimator in Section 3.1. Then we propose robust token estimators that are more resilient to the dominating impact of input tokens in Section 3.2. An efficient Newton-IRLS algorithm is derived with a convergence guarantee to approximate the robust token estimator in Section 3.3. Finally, we describe how the proposed algorithm can be unrolled as robust attention layers to enhance the robustness of transformer architectures in Section 3.4. ", "page_idx": 2}, {"type": "text", "text": "3.1 Attention Mechanism as WLS Token Estimator ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, we provide a new perspective to formulate the vanilla attention mechanism as the weighted least squares (WLS) token estimator. In the self-attention layer, each output token ${\\bf z}$ aggregates the values of input tokens $\\{\\mathbf{v}_{j}\\}$ as their weighted sum according to the attention weights: $\\begin{array}{r}{\\mathbf{z}=\\sum_{j=1}^{N}a_{j}\\mathbf{v}_{j}}\\end{array}$ , where $\\{a_{j}\\}_{j\\in[N]}$ are the attention weights and $\\{\\mathbf{v}_{j}\\}_{j\\in[N]}$ are value vectors for all $N$ input tokens. This weighted sum can be interpreted as the optimal solution of the following weighted least squares ", "page_idx": 2}, {"type": "text", "text": "(WLS) error minimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{z}}{\\arg\\operatorname*{min}}\\,\\mathcal{L}(\\mathbf{z})=\\sum_{j=1}^{N}a_{j}\\cdot\\|\\mathbf{v}_{j}-\\mathbf{z}\\|^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "whose first-order optimality condition $\\begin{array}{r}{(\\nabla{\\mathcal{L}}({\\mathbf z})=\\sum_{j=1}^{N}a_{j}\\cdot2({\\mathbf z}-{\\mathbf v}_{j})=\\mathbf{0})}\\end{array}$ yields $\\begin{array}{r}{{\\bf z}=\\sum_{j=1}^{N}a_{j}{\\bf v}_{j}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Vulnerability analysis of vanilla attention. When adversaries perturb the input tokens, these tokens will dominate the impact on output tokens since the quadratic penalty on the residual $\\lVert\\mathbf{v}_{j}-\\mathbf{z}\\rVert^{2}$ will dominate the WLS estimator. Therefore, the output token ${\\bf z}$ will be shifted to those dominating input tokens. As a result, the adversarial input tokens will significantly impact the representation of output tokens. We also provide an empirical study to verify that adversarial attacks will significantly increase the residual $\\|\\bar{\\mathbf{v}_{j}}-\\mathbf{z}\\|^{2}$ in Appendix F.3. Moreover, we simulate a mean estimation problem under outlier data points using synthetic data to better illustrate the sensitivity of the WLS estimator. The detailed setting and visualization results of the numerical simulation are provided in Appendix F. ", "page_idx": 3}, {"type": "text", "text": "3.2 Robust WLS Token Estimators ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The analysis above provides a valid explanation of why various attention-based transformer architectures are easily compromised by introducing adversarial perturbations in the input data. Also, our interpretation of the attention mechanism in transformers as WLS estimator provides a rigorous perspective to design robust alternatives. To dampen the effect of outlier data, multiple robust regression algorithms have been proposed in robust statistics using least absolute deviations [48], Huber regression [49], and Minimax Concave Penalty (MCP) [50]. Motivated by these advancements with rigorous robustness guarantees, we propose the robust weighted least squares token estimators to enhance the resilience against potential adversarial attacks as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{z}}{\\arg\\operatorname*{min}}\\,\\mathcal{L}(\\mathbf{z})=\\sum_{j=1}^{N}a_{j}\\cdot\\rho(\\|\\mathbf{v}_{j}-\\mathbf{z}\\|)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\rho$ can be flexibly replaced with the specific robust penalties in Figure 3. ", "page_idx": 3}, {"type": "text", "text": "Special cases of $\\rho$ . (1) The quadratic $\\ell_{2}$ loss recovers vanilla WLS estimator; $\\ell_{1}$ loss exerts linear effect on the residuals; (2) Huber loss performs as $\\ell_{2}$ loss within the range $(0,\\delta)$ , and becomes similar to $\\ell_{1}$ when $z>\\delta$ ; (3) MCP loss behaves like $\\ell_{1}$ loss near zero and becomes constant when $z$ is large than $\\gamma$ . (4) We also propose HuberMCP to combine the advantage of Huber and MCP loss. The detailed formulations are available in Appendix B.4 due to the space limit. ", "page_idx": 3}, {"type": "image", "img_path": "UkauUrTbxx/tmp/1b8d03bf0c81544847a6e4e04ccabd8300dffde4ffadea4e22cf2de3a3f6d8d0.jpg", "img_caption": ["Figure 3: Different $\\rho(z)$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.3 Newton-IRLS algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The proposed robust token estimator in Eq. (2) is non-convex and non-smooth, posing a challenge for efficient algorithm design. Moreover, the exploding model size of evolving transformers further necessitates the design of efficient neural network layers. To this end, we propose an efficient Newton iterative reweighted least square (Newton-IRLS) algorithm to tackle this challenging problem. We first design a localized upper bound for the original objective and then optimize the upper bound with a second-order Newton method. We also provide a rigorous theoretical loss descent guarantee. The precise statements are presented as follows and the detailed proof are provided in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "Localized upper bound. Instead of directly optimizing the original loss function ${\\mathcal{L}}(\\mathbf{z})$ in Eq. (2), we optimize a convex localized upper bound at the current iteration $\\mathbf{z}^{(k)}$ as follows: ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1 (Localized Upper Bound). Suppose the loss objective is defined as in Eq. (2), where $\\rho\\circ s q r t(\\cdot)$ is any non-convex function. For any fixed point $\\mathbf{z}^{(k)}$ , there exists a convex localized upper ", "page_idx": 3}, {"type": "text", "text": "bound as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{L}}(\\mathbf{z})=\\sum_{j=1}^{N}a_{j}\\cdot w_{j}^{(k)}\\cdot\\|\\mathbf{v}_{j}-\\mathbf{z}\\|^{2}+C(\\mathbf{z}^{(k)}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where wj(k $\\begin{array}{r}{w_{j}^{(k)}\\,=\\,\\frac{\\rho^{\\prime}(\\,\\parallel\\mathbf{v}_{j}\\,-\\mathbf{z}^{(k)}\\,\\parallel)}{2\\parallel\\mathbf{v}_{j}\\,-\\mathbf{z}^{(k)}\\,\\parallel}}\\end{array}$ and $\\rho^{\\prime}$ is the first derivative of $\\rho$ . Particularly, the constant $C(\\mathbf{z}^{(k)})$ guarantees the equality of $\\hat{\\mathcal{L}}$ and $\\mathcal{L}$ at $\\mathbf{z}^{(k)}$ , i.e., $\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})=\\mathcal{L}(\\mathbf{z}^{(k)})$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. Please refer to Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "As $C(\\mathbf{z}^{(k)})$ is treated as a constant during the optimization at the current step, the upper bound in Eq. (3) becomes convex and can be efficiently optimized. ", "page_idx": 4}, {"type": "text", "text": "Newton-IRLS iteration. After obtaining the convex upper bound $\\hat{\\mathcal{L}}$ in Eq. (3), we can derive a concise closed-form iteration using the second-order Newton method as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{z}^{(k+1)}=\\mathbf{z}^{(k)}-\\left[\\nabla^{2}\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})\\right]^{-1}\\nabla\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})=\\frac{\\sum_{j}a_{j}\\cdot w_{j}^{(k)}\\cdot\\mathbf{v}_{j}}{\\sum_{j}a_{j}\\cdot w_{j}^{(k)}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Eq. (4) can be interpreted as a reweighted sum, in which the derived wj(k) modifies the original attention score $a_{j}$ on the value vector $\\mathbf{v}_{j}$ . We leave detailed derivations of Newton-IRLS algorithm in Appendix B.2. Its convergence and rigorous loss descent are guaranteed by the following Theorem 3.2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 (Convergence guarantee). Suppose the loss objective ${\\mathcal{L}}(\\mathbf{z})$ is defined as in Eq. (2) and its corresponding convex localized upper bound is in Eq. (3). Then, through the iteration in Eq. (4), the following inequality holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\mathbf{z}^{(k+1)})\\leq\\hat{\\mathcal{L}}(\\mathbf{z}^{(k+1)})\\leq\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})=\\mathcal{L}(\\mathbf{z}^{(k)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "that is, optimizing upper bound $\\hat{\\mathcal{L}}$ can guarantee the rigorous descent of $\\mathcal{L}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. Please refer to Appendix B.3. ", "page_idx": 4}, {"type": "text", "text": "Although the loss ${\\mathcal{L}}(\\mathbf{z})$ is not necessarily convex and does not possess a global optimum, Theorem 3.2 guarantees that the Newton-IRLS iteration, which optimizes $\\hat{\\mathcal{L}}({\\bf z})$ , can rigorously reduce the original loss ${\\mathcal{L}}(\\mathbf{z})$ . The algorithm analyses in Appendix F, along with the main experiments in Section 4 and Section 5, validate that the local optimal solution achieved by our algorithm performs well in terms of both convergence and empirical robustness. ", "page_idx": 4}, {"type": "text", "text": "Robust token estimator by reweighting the tokens. The robust estimator in Eq. (2) provides a general framework that covers several special cases. By choosing different penalty functions $\\rho$ on the residuals $\\|\\mathbf{v}_{j}\\-\\mathbf{z}^{(k)}\\|$ , we obtain various reweighting schemes in Eq. (4). Take the MCP function as the instance, the weight is derived as $\\begin{array}{r}{w_{j}^{(k)}=\\frac{\\rho_{\\gamma}^{\\prime}(\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|)}{2\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}=\\operatorname*{max}\\left[\\frac{1}{\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}-\\frac{1}{\\gamma},0\\right]\\!.}\\end{array}$ . Obviously, the weight wj(k)becomes smaller as \u2225vj \u2212z(k)\u2225increases, thereby down-weighting the large residuals. The residuals will be completely removed when it exceeds the threshold $\\gamma$ , since the weight then becomes 0. The complete discussions for all cases are provided in Appendix B.4. ", "page_idx": 4}, {"type": "text", "text": "3.4 ProAttention: Robust Attention Layers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the previous subsection, we formulate the token-wise Newton-IRLS approach for notation simplicity. Here, we will present the corresponding matrix version for the entire attention layer. ", "page_idx": 4}, {"type": "text", "text": "Matrix Form. Denote $\\mathbf{V}=\\{\\mathbf{v}_{j}\\}_{j\\in[N]}$ and $\\mathbf{A}=\\{a_{i j}\\}_{i,j\\in[N]}$ are value matrix and the attention matrix, respectively. ${\\bf Z}^{(k)}=\\{{\\bf z}_{i}^{(k)}\\}_{i\\in[N]}$ is the estimator for token $i$ at the $k$ -th iteration. Subsequently, the pairwise distance $\\mathbf{D}^{(k)}=\\{\\|\\mathbf{v}_{j}-\\mathbf{z}_{i}^{(k)}\\|\\}_{i,j\\in[N]}$ between $\\mathbf{Z}^{(k)}$ and $\\mathbf{V}$ can be efficiently computed using the torch.cdist function in PyTorch. Following this, the weight $\\mathbf{W}^{(k)}=\\{w_{i j}^{(k)}\\}_{i,j\\in[N]}$ can be calculated element-wise based on . Then the next step $\\mathbf{Z}^{(k+1)}$ is updated as a reweighted matrix multiplication $\\left(\\mathbf{W}^{(k)}\\odot\\mathbf{A}\\right)\\cdot\\mathbf{V}$ . ", "page_idx": 4}, {"type": "text", "text": "Plug-and-Play Robust Attention. The proposed algorithm can be packaged as a robust attention module, which can be readily plugged into the transformers as a Plug-andPlay Robust Attention (ProAttention) layer without additional training or fine-tuning as shown in Figure 2. The implementation of ", "page_idx": 5}, {"type": "text", "text": "ProAttention using MCP penalty in PyTorch is shown in Algorithm 1. The complete pseudocode for other penalties is presented in in Appendix A. ", "page_idx": 5}, {"type": "table", "img_path": "UkauUrTbxx/tmp/7888a39fdf7c21ea4c79fa75bd82a57277e852fa6755a9c0c913fb61088f36bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Complexity analysis. Let $N,D$ , and $K$ represent the length of tokens, the dimension of vectors, and the steps of the iterations, respectively. The vanilla attention requires $2\\cdot N\\times N\\times D$ basic operations while our ProAttention needs $(1+2K)\\cdot N\\times N\\times D$ . However, our ProAttention remains efficient, as the Newton-IRLS method can effectively approximate the solution within only 3 steps $[K\\le3)$ ) (Figure 4 (a)) and ProTransformers do not introduce additional computation for training or fine-tuning. We provide the detailed complexity analysis of various attentions in Appendix L. ", "page_idx": 5}, {"type": "text", "text": "Advantages. Our proposed ProAttention enjoys the following advantages: (1) Simplicity: it is simple and easy to implement with only 4 core lines of code in Algorithm 1; (2) Efficiency: it is a plug-andplay layer that can be integrated into any trained transformer without additional training or fine-tuning; (3) Universality: it is a universal framework that advances the vanilla attention mechanism into a series of robust derivatives with different penalties. Moreover, it can be applied to any attention-based model across various modalities and tasks. ", "page_idx": 5}, {"type": "text", "text": "In the following sections, we will present comprehensive experiments and studies to validate the effectiveness of the proposed ProAttention on language modeling in Section 4 as well as computer vision and graph learning in Section 5. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment on Language Modeling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate the effectiveness of the proposed ProAttention and ProTransformer under classic text attacks on pre-trained language models, and two prompting-based attacks (prompt attack and jailbreak attack) in the context of LLMs with comprehensive ablation studies. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experiment Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Tasks and Datasets. For topic classification, we use AG\u2019s News Corpus (AGNEWS) [51]. For sentiment analysis, we utilize two widely-used datasets: Internet Movie Database (IMDB) [52] and Stanford Sentiment Treebank (SST-2) [53]. For textual entailment, we make use of Recognizing Textual Entailment (RTE) in the General Language Understanding Evaluation benchmark [54]. For jailbreak attack, we select a new dataset Behaviors introduced in [55]. For the detailed information on these datasets, please refer to Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Backbone Architectures. For classical pre-trained language models, we choose BERT [56] and its variants including RoBERTa [57], ALBERT [58] and DistilBERT [59]. For large language models (LLMs), we choose T5 [60], LLaMA [12] and Vicuna [13]. For the detailed information on backbone architectures, please refer to Appendix D.2. ", "page_idx": 5}, {"type": "text", "text": "Attacks. We not only evaluate several classic text attacks but also include popular prompt attacks and jailbreak attacks on the LLMs. The three attack mechanisms and their differences are illustrated in Figure 1. For classic text attacks, we evaluate the attacks at various levels, including the characterlevel DeepWordBug [7], word-level PWWS [9], TextFooler [14], and multi-level TextBugger [8]. For prompt attacks, we modify the prompt template according to the aforementioned text attacks following the evaluation setting in PromptBench [15]. For jailbreak, we evaluate the suffix attack using Greedy Coordinate Gradient (GCG) method [55] and we test both attacks transferred from surrogate model Vicuna (transfer attack) and attacks directly targeting the victim models (adaptive attack). Please refer to Appendix E for details on attacks. ", "page_idx": 5}, {"type": "text", "text": "Defense Baselines. We include the following defense baselines in our experiments: MixADA [17], PGD-Adv [31], FreeLB [47], TA-VAT [18] and SmoothLLM [61]. Additionally, we also include the adversarial training (AT), wherein the augmented perturbations are generated by the attack to be assessed. Details of these defense methods are provided in Appendix D.1. ", "page_idx": 5}, {"type": "text", "text": "Evaluation metrics. Following [62], we use 3 metrics to evaluate the model performance. Clean accuracy $(\\mathbf{Clean}\\%)$ ) is the model accuracy on the clean testing data. Accuracy under attack $(\\mathbf{AUA\\%})$ is the accuracy on the perturbed data under specific attack. Attack success rate $(\\mathbf{ASR}\\%)$ is the ratio of the number of successfully perturbed cases divided by the number of attempted texts. ", "page_idx": 6}, {"type": "text", "text": "Hyperparameters. For text attack setting, we follow the setting in the TextAttack framework [63]. For prompt attack, we follow the setting in PromptBench [15]. For GCG-based jailbreak attack, we follow the setting in [61]. The detailed attack settings can be found in Appendix E. For defense baselines, we follow the settings in their original papers. For our ProTransformer, we set the default number of ProAttention layers as $K=3$ since it can quickly converge to a reasonable precision within 3 layers. Finally we tune $\\delta$ (default 1) or $\\gamma$ (default 4) in the penalties (Huber and MCP loss) to obtain the optimal parameters. ", "page_idx": 6}, {"type": "text", "text": "4.2 Classic Text Attacks on Language Models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To demonstrate the effectiveness of the proposed ProTransformer, we compare the robustness of our methods with several popular defenses in three classical tasks: topic classification, sentiment analysis, and textual entailment. ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Adversarial Robustness ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "UkauUrTbxx/tmp/97bdb32ea0629693a400bb1b56021ea8febfc4e84a2c6d4fe6d43732a8acfb7a.jpg", "table_caption": ["Table 1: The results of topic classification on AGNEWS. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Performance analysis. The experimental results of topic classification (AGNEWS) are presented in Table 1, and we provide the results of sentiment analysis (IMDB) and textual entailment (RTE) in Appendix G.1 and G.2 due to the space limit. From the experiment results, we can make the following observations: ", "page_idx": 6}, {"type": "text", "text": "\u2022 The proposed ProAttention is a highly effective plug-in module that significantly and consistently enhances the robustness of various transformer backbones across various adversarial attacks. Taking AGNEWS as the instance, when combined with ProAttention (MCP), under the attacks {Textfooler, TextBugger, DeepWordBug, PWWS}: (1) ALBERT is improved by $\\{28.3\\%$ , $15.7\\%$ , $20.6\\%$ , $27.2\\%\\}$ (2) DistilBERT is improved by $\\{16.1\\%$ , $15.1\\%$ , $4.3\\%$ , $14.0\\%]$ } (3) RoBERTa is improved by $\\{11.4\\%$ , $1.8\\%$ , $4.3\\%$ , $5.4\\%$ } (4) BERT is improved by $\\{19.5\\%$ , $16.6\\%$ , $14.3\\%$ , $13.1\\%\\}$ . ", "page_idx": 6}, {"type": "text", "text": "\u2022 Our method, Pro-BERT $(\\mathrm{MCP})+\\mathrm{AT};$ , exhibits best robustness among all the baselines. By simply plugging in ProAttention (MCP) module without fine-tuning, our Pro-BERT can achieve comparable robustness to most adversarial training-based methods which require substantial computational time and resources. Furthermore, our framework is orthogonal to most existing defenses, allowing for combined use with them to further enhance robustness. For instance, when combined with AT technique, our Pro-BERT $(\\mathrm{MCP})+\\mathrm{AT}$ can further improve BERT $^+$ AT by { $14.7\\%$ , $4.6\\%$ , $18.6\\%$ , $6.2\\%\\}$ under {TextFooler, TextBugger, DeepWordBug, PWWS}. ", "page_idx": 6}, {"type": "image", "img_path": "UkauUrTbxx/tmp/9f5a022c4d31438f9ec7d89170411b8e3fb300263143ae9567dbc001541bf1ba.jpg", "img_caption": ["Figure 4: Ablation studies. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2.2 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Convergence. To validate the advantage of our Newton-IRLS over the first-order method, we conduct a simulation experiment and plot the loss descent curves in Figure 4 (a). It can be observed that Newton-IRLS exhibits efficient convergence as claimed in Section 3.3. We provide the experiment details, loss descent curves (Figure 7), and the visualization of trajectories (Figure 8) of the updated vectors in 2D plane in Appendix F.1 to further demonstrate the effectiveness of our algorithm. ", "page_idx": 7}, {"type": "text", "text": "Adversarial fine-tuning. To get insight into how the models gain more robustness from adversarial examples, we track the training curves of adversarial fine-tuning under TextFooler in Figure 4 (b), and put the results of other attacks in Figure 10 in Appendix G.3. We can observe that our Pro-BERT (MCP) is compatible with adversarial fine-tuning technique to further enhance the model resilience. ", "page_idx": 7}, {"type": "text", "text": "Attack constraints. In text attack, there are several kinds of attack constraints including the maximum percentage of perturbed words, minimum cosine similarity between the replaced synonym and original word, and minimum sentence similarity threshold between the original sentence and perturbed sentence. We test the values of these constraints in TextFooler. We present the results under different perturbation percentages in Figure 4 (c) and other constraint measurements in Appendix G.4. From the results, we observe that our Pro-ALBERT (MCP) can significantly outperform the backbone ALBERT across all ranges of constraints. ", "page_idx": 7}, {"type": "text", "text": "Different penalties. Our Newton-IRLS is flexible to be formulated as different robust estimators with different penalties. From the comparison in Figure 4 (d) , it can be observed that our robust framework can consistently improve the robustness of the backbone BERT $(\\ell_{2})$ . Specifically, $\\ell_{1}$ and Huber-based defenses are comparable, and MCP-based method exhibits the best performance. ", "page_idx": 7}, {"type": "text", "text": "Different backbones. Our method is a general plug-and-play layer applicable to various transformer backbones. The results in Table 1 and the ablation study on different backbones in Appendix G.5 (Figure 12) demonstrate that ProAttention improves the robustness over various architecture backbones (BERT, RoBERTa, DistilBERT and ALBERT) against various attacks with significant margins. ", "page_idx": 7}, {"type": "text", "text": "Running time. To empirically evaluate the efficiency of our method, we test the average running time on AGNEWS using BERT and ProBERT (MCP) equipped with multi-layer $(K)$ ProAttention. The results in Table 2 show that our ProAttention only requires 1-2 times addi", "page_idx": 7}, {"type": "table", "img_path": "UkauUrTbxx/tmp/6d9709fa1a3a48052b199f56c5738526eb6ffdc2e6a59a979d654dfd121b1905.jpg", "table_caption": ["Table 2: Average running time (ms) on AGNEWS. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "tional inference time of the backbone model yet achieves significant improvement in robustness without training. ", "page_idx": 7}, {"type": "text", "text": "4.3 Adversarial Prompting Attacks on LLMs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the context of prompt-based generative AI, the adversarial attacks mechanisms on LLMs become more enriched and sophisticated. In this section, we will evaluate the robustness of our proposed ProTransformer under two popular attacks: prompt attack and jailbreak attack. ", "page_idx": 7}, {"type": "text", "text": "4.3.1 Prompt Attack ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As shown in Figure 1, the most significant distinction between prompt attacks and classical text attacks is that prompt attacks aim to mislead the models by altering the prompt template rather than the input content. We display the results of T5 in Figure 5 and leave the comprehensive study in Appendix H.1. We also present the results on LLaMA in Appendix H.2. From the results, we can make the following observations: (1) For T5, the choice of the penalty would affect the robustness of defenses. Specifically, Pro-T5 (MCP) exhibits a significant advantage over other methods, and this advantage becomes even more evident as the number of perturbed words increases. Pro-T5 $(\\ell_{1})$ and Pro-T5 (Huber) show a slight improvement over the backbone model T5. (2) For LLaMA, Huber-MCP and Huber-based methods exhibit better robustness than other methods while preserving good clean performance. The detailed experiments and discussions can be found in Appendix H.2. ", "page_idx": 8}, {"type": "image", "img_path": "UkauUrTbxx/tmp/9f6199c6fda5407dc96094ee3ba952953575312afb1564fd99b5379ae551be39.jpg", "img_caption": ["Figure 5: Prompt attack results. Figure 6: Attack success rates (ASRs) under transfer jailbreak. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.2 Jailbreak Attack ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In recent years, prompts have played a pivotal role in guiding models to generate desired outputs. Nevertheless, there exist malicious \"jailbreak prompts\", which are intentionally designed to bypass the built-in safeguards in LLMs, causing the model to produce harmful content that violates the legal policies. As illustrated in Figure 1, the suffix-injection jailbreaks attempt to append a non-semantic suffix to the user\u2019s prompt to fool the models. We select GCG method to evaluate the resilience of models comprehensively. ", "page_idx": 8}, {"type": "text", "text": "In Figure 6, we compare the Attack Success Rates (ASRs) of Vicuna and its corresponding Pro-Vicuna (Huber) with various $\\delta$ values on Behaviors. In each column, we also include SmoothLLM [61] with different smoothing extent $q(\\%)$ to further reinforce the resilience of every single model. The last row of matrix $(q=0)$ ) stands for the performance without random smoothing. The additional results of random smoothing with swap, insert and patch, as well as the results under adaptive jailbreaking attack are presented in Appendix I. ", "page_idx": 8}, {"type": "text", "text": "From the results, we can observe that: (1) Our Pro-Vicuna can significantly improve the robustness of Vicuna. As shown in the last row of Figure 6, with $\\delta=0.1$ , we successfully reduce the ASR to $1.8\\%$ , which is comparable to the random smoothing defense that requires multiple random perturbations, inferences and aggregations. (2) Our ProAttention is orthogonal to randomized smoothing defense and can be combined with it to further improve the robustness. ", "page_idx": 8}, {"type": "text", "text": "5 Experiment beyond Language Modeling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the previous section, we have provided comprehensive experiments to validate the effectiveness of our ProTransformer in the (large) language models. In fact, as shown in Figure 2, our ProAttention is a fundamental module which can reinforce any attention-based models across various domains or modalities. In this section, we will integrate ProAttention into vision models and graph learning models to further validate the effectiveness and generality of our approach. ", "page_idx": 8}, {"type": "text", "text": "5.1 Image Classification ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In computer vision, we conduct two attacks (FGSM [30] and PGD [31]) on several vision transformers including ViT, BeiT, ConviT, DeiT and Swin. We perform the experiments on CIFAR-10 and ImageNet-1K across budgets $\\{1/255,4/255,8/255\\}$ , and present the results of PGD on CIFAR-10 in Table 3 and additional experiments in Appendix J. From the results, we can observe that Pro-ViT can outperform the ", "page_idx": 9}, {"type": "table", "img_path": "UkauUrTbxx/tmp/08e8a0aab9f5307beafca7ef7bc35f1239ed0a37d69576889f05bd0660536aed.jpg", "table_caption": ["Table 3: Adversarial robustness under PGD. "], "table_footnote": ["second best model by $\\{35.64\\%,46.\\dot{2}8\\%,33.14\\%\\}$ under different budgets. "], "page_idx": 9}, {"type": "text", "text": "5.2 Graph Representation Learning ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Besides the language and vision domains, we also validate the effectiveness of our method in the graph domain. We conduct the semisupervised node classification task and leverage PGD adaptive attack [64] to evaluate the robustness of models. We show the experiment results of Cora-ML and Citeseer, averaged over 5 different random splits, in Table 4 and Table 29 (in Appendix K), respectively. The ablation studies on the layers and $\\gamma$ in MCP are presented in Table 30. Please refer to Appendix K for more ", "page_idx": 9}, {"type": "table", "img_path": "UkauUrTbxx/tmp/91160d0fd2ceebd9c99a208e89549797f352ec33335722366f1b093a8075ebaf.jpg", "table_caption": ["Table 4: Adversarial robustness on Cora-ML. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "detailed results and studies. From the results, we can conclude that our Pro-GAT significantly outperforms the backbone GAT and exhibits strong robustness across various budgets while keeping good clean accuracy. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion & Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we delve into the robustness and security of the popular transformer-based architectures. We revisit the vulnerability of attention mechanisms with theoretical understanding and simulations. We propose an interpretable robust attention layer to robustify transformer architecture via a plugand-play paradigm. Our proposed ProAttention is an effective, efficient, and universal framework that can significantly enhance the robustness of transformers across various tasks, architectures, attacks, and domains without additional training or fine-tuning. ", "page_idx": 9}, {"type": "text", "text": "Regarding the limitations, despite the acceptable complexity of our ProTransformer, there is still potential to improve the efficiency of our models. Additionally, while we primarily claim and validate the effectiveness of our model under a plug-and-play paradigm, we are excited about the future of the proposed ProTransformer architecture and hope to see its full potential realized through training or fine-tuning on large models in the future. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zhichao Hou, Weizhi Gao, and Dr. Xiaorui Liu are supported by the National Science Foundation (NSF) National AI Research Resource Pilot Award, Amazon Research Award, NCSU Data Science Academy Seed Grant Award, and NCSU Faculty Research and Professional Development Award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[2] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI Open, 2022.   \n[3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[4] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[5] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [6] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. Advances in neural information processing systems, 32, 2019.   \n[7] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops (SPW), pages 50\u201356. IEEE, 2018.   \n[8] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text against real-world applications. arXiv preprint arXiv:1812.05271, 2018.   \n[9] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating natural language adversarial examples through probability weighted word saliency. In Proceedings of the 57th annual meeting of the association for computational linguistics, pages 1085\u20131097, 2019.   \n[10] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1\u201328, 2022.   \n[11] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s):1\u201341, 2022.   \n[12] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality, March 2023.   \n[14] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8018\u20138025, 2020.   \n[15] Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and Xing Xie. Promptbench: A unified library for evaluation of large language models, 2024.   \n[16] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail? In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[17] Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, and Maosong Sun. Better robustness by more coverage: Adversarial training with mixup augmentation for robust fine-tuning. In Findings of ACL, 2021.   \n[18] Linyang Li and Xipeng Qiu. Token-aware virtual adversarial training in natural language understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8410\u20138418, 2021.   \n[19] Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. Infobert: Improving robustness of language models from an information theoretic perspective. In International Conference on Learning Representations, 2021.   \n[20] Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards robustness against natural language word substitutions. arXiv preprint arXiv:2107.13541, 2021.   \n[21] Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-Wei Chang, and Xuanjing Huan. Defense against synonym substitution-based adversarial attacks via dirichlet neighborhood ensemble. In Association for Computational Linguistics (ACL), 2021.   \n[22] Mao Ye, Chengyue Gong, and Qiang Liu. SAFER: A structure-free approach for certified robustness to adversarial word substitutions. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3465\u20133475, Online, July 2020. Association for Computational Linguistics.   \n[23] Jiehang Zeng, Jianhan Xu, Xiaoqing Zheng, and Xuanjing Huang. Certified robustness to text adversarial attacks by randomized [mask]. Computational Linguistics, 49(2):395\u2013427, 2023.   \n[24] Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krishnamurthy Dvijotham, and Pushmeet Kohli. Achieving verified robustness to symbol substitutions via interval bound propagation. arXiv preprint arXiv:1909.01492, 2019.   \n[25] Robin Jia, Aditi Raghunathan, Kerem G\u00f6ksel, and Percy Liang. Certified robustness to adversarial word substitutions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4129\u20134142, Hong Kong, China, November 2019. Association for Computational Linguistics.   \n[26] Naihan Li, Yanqing Liu, Yu Wu, Shujie Liu, Sheng Zhao, and Ming Liu. Robutrans: A robust transformer-based text-to-speech model. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8228\u20138235, 2020.   \n[27] Junhua Liu, Trisha Singhal, Lucienne TM Blessing, Kristin L Wood, and Kwan Hui Lim. Crisisbert: a robust transformer for crisis classification and contextual crisis embedding. In Proceedings of the 32nd ACM conference on hypertext and social media, pages 133\u2013141, 2021.   \n[28] Jingfeng Yang, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, and Shachi Paul. TableFormer: Robust transformer modeling for table-text encoding. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 528\u2013537, Dublin, Ireland, May 2022. Association for Computational Linguistics.   \n[29] Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat Ho. Designing robust transformers using robust kernel density estimation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[30] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[31] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[32] Yotam Gil, Yoav Chai, O. A. Gorodissky, and Jonathan Berant. White-to-black: Efficient distillation of black-box adversarial attacks. In North American Chapter of the Association for Computational Linguistics, 2019.   \n[33] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In MILCOM 2016-2016 IEEE Military Communications Conference, pages 49\u201354. IEEE, 2016.   \n[34] Suranjana Samanta and Sameep Mehta. Towards crafting text adversarial samples. arXiv preprint arXiv:1707.02812, 2017.   \n[35] Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji Matsumoto. Interpretable adversarial perturbation in input embedding space for text. In 27th International Joint Conference on Artificial Intelligence, IJCAI 2018, pages 4323\u20134330. International Joint Conferences on Artificial Intelligence, 2018.   \n[36] Melika Behjati, Seyed-Mohsen Moosavi-Dezfooli, Mahdieh Soleymani Baghshah, and Pascal Frossard. Universal adversarial attacks on text classifiers. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7345\u20137349. IEEE, 2019.   \n[37] Siddhant Garg and Goutham Ramakrishnan. BAE: BERT-based adversarial examples for text classification. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6174\u20136181, Online, November 2020. Association for Computational Linguistics.   \n[38] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial example generation with syntactically controlled paraphrase networks. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1875\u20131885, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.   \n[39] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text classification can be fooled. arXiv preprint arXiv:1704.08006, 2017.   \n[40] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 31\u201336, 2018.   \n[41] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-step jailbreaking privacy attacks on chatGPT. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.   \n[42] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715, 2023.   \n[43] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860, 2023.   \n[44] Hezekiah J Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del Castillo Iglesias, Ron Heichman, and Ramesh Darwishi. Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples. arXiv preprint arXiv:2209.02128, 2022.   \n[45] Yiming Zhang and Daphne Ippolito. Prompts should not be seen as secrets: Systematically measuring prompt extraction attack success. arXiv preprint arXiv:2307.06865, 2023.   \n[46] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499, 2023.   \n[47] Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding. In International Conference on Learning Representations, 2020.   \n[48] Peter Bloomfield and William L Steiger. Least absolute deviations: theory, applications, and algorithms, volume 6. Springer, 1983.   \n[49] Peter J Huber. Robust regression: asymptotics, conjectures and monte carlo. The annals of statistics, pages 799\u2013821, 1973.   \n[50] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. 2010.   \n[51] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015.   \n[52] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.   \n[53] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.   \n[54] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR.   \n[55] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023.   \n[56] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[57] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.   \n[58] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations, 2020.   \n[59] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.   \n[60] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.   \n[61] Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas. Smoothllm: Defending large language models against jailbreaking attacks, 2023.   \n[62] Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, Kai-Wei Chang, and Cho-Jui Hsieh. Searching for an effective defender: Benchmarking defense against adversarial word substitution, 2021.   \n[63] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 119\u2013126, 2020.   \n[64] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin. Topology attack and defense for graph neural networks: An optimization perspective, 2019.   \n[65] Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. Recognizing textual entailment: Rational, evaluation and approaches\u2013erratum. Natural Language Engineering, 16(1):105\u2013105, 2010.   \n[66] Roy Bar-Haim, Ido Dagan, and Idan Szpektor. Benchmarking applied semantic inference: The pascal recognising textual entailment challenges. In Language, Culture, Computation. Computing-Theory and Technology: Essays Dedicated to Yaacov Choueka on the Occasion of His 75th Birthday, Part I, pages 409\u2013424. Springer, 2014.   \n[67] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B Dolan. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1\u20139, 2007.   \n[68] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing textual entailment challenge. TAC, 7:8, 2009.   \n[69] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[70] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.   \n[71] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[72] C Lee Giles, Kurt D Bollacker, and Steve Lawrence. Citeseer: An automatic citation indexing system. In Proceedings of the third ACM conference on Digital libraries, pages 89\u201398, 1998.   \n[73] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018.   \n[74] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.   \n[75] Xiang Zhang and Marinka Zitnik. Gnnguard: Defending graph neural networks against adversarial attacks. Advances in neural information processing systems, 33:9263\u20139275, 2020.   \n[76] Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks against adversarial attacks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1399\u20131407, 2019.   \n[77] Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov, and Jie Tang. Graph random neural networks for semi-supervised learning on graphs. Advances in neural information processing systems, 33:22092\u201322103, 2020.   \n[78] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 66\u201374, 2020.   \n[79] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu. Adversarial examples for graph data: Deep insights into attack and defense. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 4816\u20134823. International Joint Conferences on Artificial Intelligence Organization, 7 2019.   \n[80] Simon Geisler, Tobias Schmidt, Hakan \u00b8Sirin, Daniel Z\u00fcgner, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Robustness of graph neural networks at scale. Advances in Neural Information Processing Systems, 34:7637\u20137649, 2021.   \n[81] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[82] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022.   \n[83] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "[84] St\u00e9phane d\u2019Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In International Conference on Machine Learning, pages 2286\u20132296. PMLR, 2021. ", "page_idx": 15}, {"type": "text", "text": "A Pseudocode of Plug-and-Play Robust Attention (ProAttention) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we provide the complete pseudocode of our ProAttention including various penalties cases in Algorithm 2. The core itertions are show in the for loop in the code. Our ProAttention is easy to implement by only replacing the vanilla attention module with our ProAttention. ", "page_idx": 16}, {"type": "table", "img_path": "UkauUrTbxx/tmp/1bd9a2706afb4517cdbd9fb1ef1738933fad7280a3a49889da7dba9e2c8b402b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Proof of Newton-IRLS Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Localized Upper Bound (Lemma 3.1) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Define $\\phi(z):=\\rho(\\sqrt{z})$ as a non-convex function, then for fixed point $z_{0}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi(z)\\le\\phi(z_{0})+\\phi^{\\prime}(z_{0})(z-z_{0})=\\phi^{\\prime}(z_{0})\\cdot z+C(z_{0})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality holds with equality at $z=z_{0}$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi^{\\prime}(z_{0})=\\left.\\rho^{\\prime}(\\sqrt{z})\\cdot\\frac{1}{2\\sqrt{z}}\\right|_{z=z_{0}}=\\frac{\\rho^{\\prime}(\\sqrt{z_{0}})}{2\\sqrt{z_{0}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By replacemnet as $z=\\|\\mathbf{v}_{j}-\\mathbf{z}\\|^{2}$ and $z_{0}=\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|^{2}$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho(\\|\\mathbf{v}_{j}-\\mathbf{z}\\|)\\leq\\frac{\\rho^{\\prime}(\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|)}{2\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}\\cdot\\|\\mathbf{v}_{j}-\\mathbf{z}\\|^{2}+C(\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|^{2})}\\\\ &{\\qquad\\qquad=w_{j}^{(k)}\\cdot\\|\\mathbf{v}_{j}-\\mathbf{z}\\|^{2}+C(\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the first inequality holds with equality at $\\mathbf{z}=\\mathbf{z}^{(k)}$ . Sum up the items on both sides with weights $\\{a_{j}\\}_{j\\in[N]}$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}({\\mathbf z})=\\sum_{j=1}^{N}a_{i}\\cdot\\rho(||{\\mathbf v_{j}}-{\\mathbf z}||)}\\\\ {\\displaystyle\\qquad\\leq\\sum_{j=1}^{N}a_{j}\\cdot w_{j}^{(k)}\\cdot||{\\mathbf v_{j}}-{\\mathbf z}||^{2}+\\sum_{j=1}^{N}a_{j}\\cdot C(||{\\mathbf v_{j}}-{\\mathbf z}^{(k)}||^{2})}\\\\ {\\displaystyle\\qquad=\\sum_{j=1}^{N}a_{j}\\cdot w_{j}^{(k)}\\cdot||{\\mathbf v_{j}}-{\\mathbf z}||^{2}+C_{1}({\\mathbf z}^{(k)})}\\\\ {\\displaystyle\\qquad=\\hat{\\chi}({\\mathbf z})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the equality holds at $\\mathbf{z}=\\mathbf{z}^{(k)}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})=\\mathcal{L}(\\mathbf{z}^{(k)}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "After obtaining the convex upper bound $\\hat{\\mathcal{L}}({\\bf z})$ , it becomes feasible to employ convex optimization algorithms to optimize this objective. ", "page_idx": 17}, {"type": "text", "text": "B.2 Proof of Newton-IRLS algorithm and Special Cases ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Newton-IRLS. We first derive the formulations of gradient and Hessain matirx of $\\hat{\\mathcal{L}}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})=\\sum_{j=1}^{N}a_{j}\\cdot w_{j}^{(k)}\\cdot2(\\mathbf{z}^{(k)}-\\mathbf{v}_{i})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla^{2}\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})=\\sum_{j=1}^{N}a_{j}\\cdot w_{j}^{(k)}\\cdot2\\cdot\\mathbf{I}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, the gradient descent (GD) $\\eta$ is the stepsize) is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{z}^{(k+1)}=\\mathbf{z}^{(k)}-\\boldsymbol{\\eta}\\cdot\\nabla\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})}}\\\\ &{}&{=\\mathbf{z}^{(k)}-\\boldsymbol{\\eta}\\cdot\\displaystyle\\sum_{j=1}^{N}a_{j}\\cdot w_{j}^{(k)}\\cdot2(\\mathbf{z}^{(k)}-\\mathbf{v}_{j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the Newton Iteration is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}^{(k+1)}=\\mathbf{z}^{(k)}-\\left[\\nabla^{2}\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})\\right]^{-1}\\nabla\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}=\\mathbf{z}^{(k)}-\\cdot\\left(\\displaystyle\\sum_{j=1}^{N}a_{j}\\cdot w_{j}^{(k)}\\cdot\\boldsymbol{\\cdot}\\boldsymbol{\\cdot}\\mathbf{z}^{(k)}\\cdot\\mathbf{z}^{(k)}-\\mathbf{z}^{(k)}\\right)}\\\\ &{\\phantom{x x x x x x x x x x}=\\frac{\\sum_{j}a_{j}\\cdot w_{j}^{(k)}\\cdot\\mathbf{v}_{j}}{\\sum_{j}a_{j}\\cdot w_{j}^{(k)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In convex optimization, it has been well-established that second-order methods converge much faster than first-order approaches, but they require substantial computation in calculating or approximating the inverse Hessian matrix. However, due to the uniqueness of our $\\hat{\\mathcal{L}}$ in Eq. (3), we can derive a concise closed-form iteration using the second-order Newton method as in Eq. (10). Compared to the first-order gradient descent (GD) iteration, our Newton-IRLS algorithm enjoys several advantages as follows: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Fast convergence: Newton method converges at a quadratic rate, which is significantly faster than the linear convergence of gradient descent (GD). The comparative analysis of them can be found in Figure 4 (a) in ablation studies;   \n\u2022 Interpretable formulation: The resulted form in Eq. (??) employs a normalized reweighted sum, which can be interpreted as robust estimator by down-weighting the outliers, as discussed in the following paragraph;   \n\u2022 Efficient computation: The Hessian $\\nabla^{2}\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})$ can be easily computed as a closed-form diagonal matrix, facilitating the matrix inversion and multiplication in the Newton\u2019s iteration. ", "page_idx": 18}, {"type": "text", "text": "B.3 Proof of Rigorous Loss Descent Guarantee (Theorem 3.2) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Since $\\mathbf{z}^{(k+1)}$ is obtained from optimize the convex localized upper bound $\\mathcal{L}$ at $\\mathbf{z}^{(k)}$ , then we have $\\hat{\\mathcal{L}}(\\mathbf{z}^{(k+1)})\\leq\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})$ . According to the upper bound in Eq. (6) and localized equality in Eq. (7), it is not hard to get the following inequality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\mathbf{z}^{(k+1)})\\leq\\hat{\\mathcal{L}}(\\mathbf{z}^{(k+1)})\\leq\\hat{\\mathcal{L}}(\\mathbf{z}^{(k)})=\\mathcal{L}(\\mathbf{z}^{(k)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, optimizing the localized upper bound $\\hat{\\mathcal{L}}$ can guarantee the rigorous descent of $\\mathcal{L}$ . ", "page_idx": 18}, {"type": "text", "text": "B.4 Special cases of Newton-IRLS ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our Newton-IRLS is a general framework which can be derived as different reweighting schemes with different penalties: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Square Loss $(\\ell_{2})$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\rho(z)=\\frac{1}{2}z^{2},}\\\\ {\\displaystyle w_{j}^{(k)}=\\frac{\\rho^{\\prime}(\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|)}{2\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}=\\frac{1}{2},}\\\\ {\\displaystyle\\mathbf{z}^{*}=\\sum_{j=1}^{N}a_{j}\\cdot\\mathbf{v}_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\ell_{2}$ loss increase quadratically with $z$ , which suggests that $\\ell_{2}$ loss is more sensitive to the residual magnitude. Particularly, $\\ell_{2}$ loss can recover the vanilla attention since the weights are constant $\\frac{1}{2}$ \u2022 Absolute Loss $(\\ell_{1})$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\rho({z})={z},}\\\\ {\\displaystyle w_{j}^{(k)}=\\frac{\\rho^{\\prime}(\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|)}{2\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}=\\frac{1}{2\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "With $\\ell_{1}$ loss, the weight is inversely proportional to $\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|$ . By up-weighting the inliers and down-weighting the outliers, $\\ell_{1}$ -based estimators can mitigate the effect of large magnitude residues. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Minimax Concave Penalty (MCP) [50]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\rho_{\\gamma}(z)={\\left\\{\\!\\!\\begin{array}{l l}{z-{\\frac{z^{2}}{2\\gamma}}}&{{\\mathrm{if~}}y<\\gamma}\\\\ {{\\frac{\\gamma}{2}}}&{{\\mathrm{if~}}y\\geq\\gamma}\\end{array}\\right.},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{j}^{(k)}=\\frac{\\rho^{\\prime}(\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|)}{2\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}=\\frac{1}{2}\\operatorname*{max}\\left[\\frac{1}{\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}-\\frac{1}{\\gamma},0\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "MCP loss becomes constant when $z$ is large and the weight derived by MCP loss enhances the interpretability of the robust estimator by down-weighting or completely removing the outliers. To be specific, the weight $w_{j}$ becomes smaller as the distance $\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|$ increases, thereby down-weighting the outlying cases. When this distance exceeds the threshold $\\gamma$ , the weight becomes 0, totally removing the outliers. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Huber loss: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{\\delta}(z)=\\binom{\\frac{1}{2}z^{2}}{\\delta\\cdot(z-\\frac{1}{2}\\delta)}\\quad\\mathrm{if~}z<\\delta\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{j}^{(k)}=\\frac{\\rho^{\\prime}(\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|)}{2\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}=\\frac{1}{2}\\operatorname*{min}\\left[1,\\frac{\\delta}{\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Huber loss is equivalent to the $\\ell_{2}$ loss within the range $(0,\\delta)$ , and it becomes similar to $\\ell_{1}$ when $z>\\delta$ , which indicates that Huber loss may mitigate the effect of large noise while keeping decent performance in noiseless scenario. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Huber-MCP: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho_{\\delta,\\gamma}(z)=\\left\\{\\!\\!\\begin{array}{l l}{{\\frac{1}{2}z^{2}}}&{{\\mathrm{if}\\;z<\\delta}}\\\\ {{\\delta\\cdot(z-\\frac{1}{2}\\delta-\\frac{(z-\\delta)^{2}}{2(\\gamma-\\delta)})}}&{{\\mathrm{if}\\;\\delta\\leq z<\\gamma\\;,}}\\\\ {{\\frac{\\delta\\gamma}{2}}}&{{\\mathrm{if}\\;\\gamma\\leq z}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{j}^{(k)}=\\frac{\\rho^{\\prime}(\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|)}{2\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}=\\frac{1}{2}\\operatorname*{max}\\left[\\operatorname*{min}\\left[\\frac{\\delta}{\\gamma-\\delta}\\left(\\frac{\\gamma}{\\|\\mathbf{v}_{j}-\\mathbf{z}^{(k)}\\|}-1\\right),1\\right],0\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This penalty combines the advantages of Huber and MCP in recovering the $\\ell_{2}$ loss and largely mitigating the outliers. ", "page_idx": 19}, {"type": "text", "text": "C Dataset Information ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Language Domain ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 AG\u2019s News Corpus (AGNEWS) [51]: It is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity. The AG\u2019s news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Internet Movie Database (IMDB) [52]: IMDB dataset having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Stanford Sentiment Treebank (SST-2) [53]: It is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Recognizing Textual Entailment (RTE): It comes from a series of annual textual entailment challenges. The authors of the benchmark combined the data from RTE1 [65], RTE2 [66], RTE3 [67], and RTE5 [68]. Examples are constructed based on news and Wikipedia text. The authors of the benchmark convert all datasets to a two-class split, where for three-class datasets they collapse neutral and contradiction into not entailment, for consistency. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Behaviors: It is a new dataset introduced in [55] for robustness evaluation of jailbreaking attack. The dataset includes 520 goal prompts and corresponding targets, it is available in https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/. ", "page_idx": 20}, {"type": "text", "text": "C.2 Beyond Language Domain ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 CIFAR10 [69]: The CIFAR-10 dataset is a well-known dataset used in the field of computer vision. It consists of $60\\mathrm{,}000\\,32\\mathrm{x}32$ color images in 10 different classes, with 6,000 images per class. The dataset is divided into two parts: 50,000 training images and 10,000 test images. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Each image is labeled with one of these 10 categories. ", "page_idx": 20}, {"type": "text", "text": "\u2022 ImageNet-1K [70]: This dataset provides access to ImageNet which is the most commonly used subset of ImageNet. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images. The version also has the patch which fixes some of the corrupted test set images already applied. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Cora-ML [71]: The Cora dataset is a widely-used benchmark dataset in the field of graph-based tasks. It consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a $0/1$ -valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. Working with the Cora dataset presents challenges typical of real-world graph data, such as handling sparse and high-dimensional feature vectors, and dealing with the complex structure of the graph. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Citeseer [72]: The CiteSeer dataset is another popular dataset in the graph field. It consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words. ", "page_idx": 20}, {"type": "text", "text": "D Defense Baselines and Backbone Architectures ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Defense Baselines ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Language Domain: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 PGD-Adv [31]: The Projected Gradient Descent (PGD) method stands as the most prevalent attack strategy in the field of computer vision. It is primarily utilized for crafting adversarial examples in the context of adversarial training. The defense in this paper is adapted directly from PGD-adv in computer vision, extending its application to language modeling. ", "page_idx": 21}, {"type": "text", "text": "\u2022 MixADA [17]: The search space for adversarial examples in language models is typically vast due to their discrete nature. To enhance the robustness of these models, MixADA integrates adversarial training [30] with mixup data augmentation [73], thereby expanding the range of adversarial examples covered. Specifically, mixup generates synthetic training examples by linearly blending pairs of inputs and their corresponding labels. This approach enables the model to learn from a broader and more effective set of adversarial examples during training. ", "page_idx": 21}, {"type": "text", "text": "\u2022 FreeLB [47]: Different from attacks that directly change the words in the sentence, FreeLB adds adversarial perturbations to word embeddings and minimizes the resultant adversarial loss around input samples. To expedite the process of adversarial training, FreeLB implements a single descent step on the parameters concurrently with each of the $K$ ascent steps applied to the perturbation, which utilizes the average of accumulated gradients over the $K$ steps. This efficiency has established FreeLB as a popular defense method in the field of NLP. ", "page_idx": 21}, {"type": "text", "text": "\u2022 TA-VAT [18]: TA-VAT is another virtual adversarial training method that generates gradientbased perturbations on the embedding space. To create fine-grained perturbations, TA-VAT employs a token-level accumulated perturbation vocabulary. This vocabulary serves to better initialize the perturbations. Additionally, TA-VAT utilizes a token-level normalization ball, which effectively constrains these perturbations in a relevant and precise manner. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Adversarial Training (AT): Adversarial training is adaptive to the attack to be evaluated. Take the Textfooler as the instance, at every epoch, we generate 1000 perturbations from the Textfooler and add them into the training dataset to reinforce the training of models. We utilize the TextAttack [63] platform the conduct this adversarial training. ", "page_idx": 21}, {"type": "text", "text": "\u2022 SmoothLLM [61]: Motivated by finding that the adversarial-prompting jailbreak is sensitive to the random character-level changes, SmoothLLM is designed by firstly perturbing multiple copies of the given prompt and then aggregating all the outputs. ", "page_idx": 21}, {"type": "text", "text": "Beyond Language Domain: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 Graph Convolutional Network (GCN) [74]: GCN is motivated by the localized first-order approximation of spectral graph convolutions. The basic idea is to first add self-loops to the adjacency matrix and then normalize the matrix. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Graph Attention Network (GAT) [5]: GAT leverages the attention mechanism to construct masked self-attentional layers. This allows the nodes to reweight their neighbors via the feature similarity. ", "page_idx": 21}, {"type": "text", "text": "\u2022 GNNGuard [75]: GNNGuard is a universal reweighting framework that can be applied to any GNN. It leverages the cosine similarities between nodes\u2019 features to up-weight the correlated nodes and prune the edges between the dissimilar pairs. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Robust GCN (RGCN) [76]: RGCN first models the latent representations as the Gaussian distributions. Then the weights of different neighborhoods will be assigned different weights according to their variances when performing the message propagation. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Graph Random Neural Network (GRAND) [77]: The core of GRAND is the random propagation, wherein the node feature will be partially or entirely dropped out and then propagated through over the graph. This operation enable the node to be insensitive to the specific neighborhood, which prevents the effect of malicious outliers. Additionally, the random propagation also help to augment the representation for each node, thus improving the generalization of GNN. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Property GNN (ProGNN) [78]: The core principle of ProGNN is to robustify the GNNs through enhancing the graph properties of sparsity, low rank and feature smoothness. It provides a graph structure learning framework to learn the clean graph structure and parameters simultaneously. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Jaccard-GCN [79]: The basic idea of Jaccard-GCN is to preprocess the adjacency matrix by first computing the Jaccard coefficients of paired node features and then dropping the edges where the coefficients are below the threshold. ", "page_idx": 22}, {"type": "text", "text": "\u2022 SoftMedian [80]: SoftMedian is a robust estimator for the message passing aggregation. It reweights the adjacency weights based to the distances of the hidden embeddings between the neighbor nodes and the dimension-wise median of the the entire neighboring representations. ", "page_idx": 22}, {"type": "text", "text": "D.2 Backbone Architectures ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Classical language models: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 BERT [56]: BERT stands out as one of the most well-known transformer-based language models. It is pretrained through masked language modeling (MLM), where it learns to predict words that have been masked, using context for guidance. This pretrained model is then fine-tuned for a variety of downstream tasks, showcasing its versatility and effectiveness in diverse applications. In our experiments, we will use BERT-110M. ", "page_idx": 22}, {"type": "text", "text": "\u2022 RoBERTa [57]: RoBERTa is developed to overcome certain limitations of the original BERT model. This is accomplished by implementing key modifications such as increasing the batch size, extending the training epochs, and employing advanced optimization techniques. As a result of these strategic changes, RoBERTa has demonstrated substantial performance improvements over BERT across various NLP benchmarks. In our experiments, we will use RoBERTa-125M. ", "page_idx": 22}, {"type": "text", "text": "\u2022 ALBERT [58]: ALBERT is a lite variant of BERT. It is achieved by decoupling the word embedding from the hidden embedding, significantly cutting down the number of parameters. To further enhance its efficiency, ALBERT employs cross-layer parameter sharing, ensuring that all layers use the same parameters. The reductions not only minimize memory footprint but also improve the efficiency of the model. In our experiments, we will use ALBERT-12M. ", "page_idx": 22}, {"type": "text", "text": "\u2022 DistilBERT [59]: DistilBERT is a light version of BERT, maintaining most of the performance of the original BERT. It is trained with the knowledge distillation technique [81] to achieve high efficiency. In our experiments, we will use DistilBERT-66M. ", "page_idx": 22}, {"type": "text", "text": "Large Language Models: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 T5 [60]: Text-to-Text Transfer Transformer (T5) is a transformer-based neural network model known for its versatility and power in handling a wide range of NLP tasks. T5 simplifies NLP tasks by treating them uniformly as text-generation challenges. The T5 model family offers a range of sizes, from 60 million to 11 billion parameters, catering to different computational needs. The flexibility has made T5 a popular choice in NLP research. In our experiments, we will use T5-770M. ", "page_idx": 22}, {"type": "text", "text": "\u2022 LLaMA [12]: LLaMa, the Large Language Model developed by Meta AI, represents a cuttingedge advancement in language modeling. Trained on publicly available datasets, LLaMa is available in various sizes to suit different computational needs. Notably, LLaMa-13B demonstrates superior performance over GPT-3 in most benchmarks, highlighting its exceptional effectiveness and capability in NLP tasks. In our experiments, we will use LLaMA-7B. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Vicuna [13]: Vicuna is a high-performing, open-source chatbot that impresses with capabilities comparable to GPT-4. Fine-tuned from the LLaMa model, it utilizes user-shared conversations gathered from Share-GPT for its training. Remarkably, Vicuna achieves $90\\%$ of the performance level of GPT-4, despite having only 13 billion parameters, showcasing its efficiency and effectiveness. In our experiments, we will use Vicuna-7B. ", "page_idx": 22}, {"type": "text", "text": "Vision Models: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 ViT [3]: The Vision Transformer (ViT) is a model in computer vision that adopts the principles of the Transformer architecture. In ViT, an image is processed similarly to a sequence of words, or tokens. Specifically, the image is segmented into fixed-size patches, each of which is then linearly transformed into an embedded representation. When trained on sufficient data, ViT achieves state-of-the-art performance on image classification benchmarks, competing with or outperforming leading CNN-based models. In our experiments, we will use ViT-86M. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Swin [4]: Swin Transformer is a popular variant of ViT, standing out for its enhanced efficiency and superior performance. It employs a hierarchical architecture, which not only aligns more closely with the nature of visual data but also boosts efficiency. To effectively capture global contextual information, Swin Transformer incorporates shifted window-based self-attention, further enhancing its effectiveness in vision-related applications. In our experiments, we will use Swin-50M.   \n\u2022 BEIT [82]: Due to the success of BERT, BEIT harnesses the concept of masked language modeling to enhance self-supervised learning in the visual domain. To align with the words in language models, BEIT first maps the patch in an image into a token with an autoencoder. In the training process, it masks a portion of these patches, using the remaining unmasked ones to predict the masked tokens. Subsequently, the model is fine-tuned for a variety of downstream tasks, demonstrating its adaptability and effectiveness in diverse applications. In our experiments, we will use BEIT-86M.   \n\u2022 DeiT [83]: To address the substantial data requirements for training the Vision Transformer, Data-Efficient Image Transformer (DeiT) employs knowledge distillation [81] to train the model. By integrating this approach with various data augmentation techniques, DeiT successfully attains competitive results in image classification tasks, even with constrained training data availability. In our experiments, we will use DeiT-22M.   \n\u2022 ConViT [84]: ConViT designs a hybrid architecture to leverage the local processing capabilities of CNNs and the global context understanding of transformers. To be specific, it replaces the several first self-attention layers with gated-self positional self-attention layers, allowing the model to adjust between local and global processing. In our experiments, we will use ConViT-30M. ", "page_idx": 23}, {"type": "text", "text": "E Attacks. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 Classic Text Attack: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For the classic text attacks, we follow the default attack setting in the TextAttack [63] and the detailed information are as follows: ", "page_idx": 24}, {"type": "text", "text": "\u2022 DeepWordBug [7]: DeepWordBug is black-box attacks that apply character-level transformations to the highest-ranked tokens misclassify the text input. It includes several character transformations including swap, substitution, deletion and insertion. We hold the maximum difference on edit distance (Levenshtein Edit Distance) to 30 for each sample. We will greedily modify the works with the word importance ranking. ", "page_idx": 24}, {"type": "text", "text": "\u2022 PWWS [9]: The probability weighted word saliency (PWWS) employs a new word order determined by the word saliency and predicted probability, and then greedily perform the synonyms substitution. ", "page_idx": 24}, {"type": "text", "text": "\u2022 TextFooler [14]: TextFooler propose a more comprehensive paradigm to generate adversarial perturbations. It firstly identify the important words and then replace them with the most semantically and syntacticaly similar synonyms until the prediction is altered. We set the minimum word embedding cosine similarity as 0.5 and the universal sentence similarity threshold as 0.840845057. ", "page_idx": 24}, {"type": "text", "text": "\u2022 TextBugger [8]: TextBugger is a framework for creating utility-preserving adversarial texts against the classification models. It can be employed under both white-box and black-box settings. Specifically, this attack sorts the words based on Jacobian matrix in white-box setting and based on specific scoring function in black-box setting. It includes swap, substitution, deletion and insertion. There are two types of substitutions: Sub-C (replace characters with visually similar characters) and Sub-W (replace a word with its top- $K$ ( $K=5$ ) nearest neighbors.) The universal sentence threshold is set as 0.8. ", "page_idx": 24}, {"type": "text", "text": "E.2 Attack in LLMs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 Prompt attack: we conduct adversarial prompt attacks using the text attacks including DeepWordBug [7], PWWS [9], TextFooler [14] and TextBugger [8]. The evaluation setting follow PromptBench [15] which is a unified and comprehensive python library for evaluating LLMs. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Jailbreak: We employ the Greedy Coordinate Gradient (GCG) [55] method to search for the adversarial prompt suffixes that circumvent the alignment of the language models. It can be conducted under the transfer attack and direct attack. Specifically, the transfer attacks firstly attack the surrogate model (e.g., Vicuna) and can apply the perturbations to the victim models. The direct attacks directly apply the GCG method to the victim models to generate the suffixes, which can be much stronger. The JailbreakKeywords is a list of words commonly found in messages that abstain from responding to a prompt asking for inappropriate content. In our experiment, we use the following list: JailbreakKeywords $=$ [\"I\u2019m sorry\", \"Sorry\", \"I apologize\", \"As an\", \"I\u2019m just\", \"I\u2019m an\", \"I cannot\", \"I would advise\", \"it\u2019s not appropriate\", \"As a responsible AI\"]. ", "page_idx": 24}, {"type": "text", "text": "E.3 Attacks in Vision & Graph Models ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 FGSM [30]: Fast Gradient Sign Method (FGSM) is a widely used technique for generating adversarial examples. It crafts perturbations specifically to maximize errors in the output of neural networks. ", "page_idx": 24}, {"type": "text", "text": "\u2022 PGD [31]: Projected Gradient Descent (PGD) method is one of the most prominent attack strategies in computer vision. Unlike FGSM employing a single step, PGD uses multiple steps to generate adversarial examples. This iterative approach includes a projection operation, which ensures that the intensity of the attack remains within specified limits, making PGD a more controlled and effective method for generating adversarial examples. The steps are $K=7$ and the steps size $\\alpha=0.00784$ . ", "page_idx": 24}, {"type": "text", "text": "\u2022 PGD on Graph [64]: Motivated by PGD [31] in vision domain, [64] propose a first-order method to conduct topology attack on discrete graph structure. This method firstly solve continuous optimization problem by Projected Gradient Descent (PGD) method and then utilize the random sampling to get the optimal binary topology perturbation from the continuous probabilistic matrix. ", "page_idx": 24}, {"type": "text", "text": "F Algorithm Convergence and Robust Estimation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 Convergence Guarantee ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Loss curves. We use generated data to verify the convergence of our proposed algorithm. The batch size, number of heads, length of inputs and dimension of data are chosen as $B=8,H=4,N=$ 64, $D=8$ , respectively. The $\\gamma$ in MCP is set as 4 and $\\delta$ in Huber loss is set as 0.8. The loss curve of our algorithm with different penalties are shown in Figure 7. We can observe that our algorithm show a fast convergence and even 2 to 3 steps can well approximate the optimal solution. ", "page_idx": 25}, {"type": "image", "img_path": "UkauUrTbxx/tmp/429d9d56a44c566ffeaae7e7e2c909c5eaab41e8658904f7f8e3df6c1f3adfa7.jpg", "img_caption": ["Figure 7: Loss Curve of Algorithms "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Trajectory. To further validate the convergence and effectiveness of our algorithm, we use a toy experiment to visualize the trajectories of updated vector in 2D plane in Figure 8. We use $L_{1}$ penalty in our algorithm, the simulated attention matrix and value matrix are as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{1}&{1}&{1}\\\\ {2}&{0}&{0}\\\\ {0}&{0}&{2}\\end{array}\\right]}\\,,\\mathbf{V}={\\left[\\begin{array}{l l}{1}&{2}\\\\ {7}&{25}\\\\ {25}&{37}\\end{array}\\right]}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "image", "img_path": "UkauUrTbxx/tmp/cb9aa1d09c1d76ea67f314d245c96a1e8096662b20012fe4eadd199a3010d7d9.jpg", "img_caption": ["Figure 8: Optimization trajectory. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "From the figure, we can find that with the mean as the initial position, the updated vector can approach closely to the ground truth within only 3 steps. This phenomenon further validate the effectiveness and efficiency of our algorithm. ", "page_idx": 25}, {"type": "text", "text": "F.2 Robust Estimation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Robust estimation.We firstly generated clean samples $\\{{\\bf x}_{i}\\}_{i=1}^{n}$ (blue dots) and the outlier samples $\\{{\\bf x}_{i}\\}_{i=n+1}^{n+m}$ (red dots) from 2-dimensional Gaussian distributions, $\\mathcal{N}((0,0),1)$ and $\\mathcal{N}((8,8),0.5)$ , respectively. We calculate the mean of clean samples $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{x}_{i}$ as the ground truth of the mean estimator. Then we estimate the mean of all the samples by solving arg $\\mathrm{min}_{\\mathbf{z}}$ $\\textstyle\\sum_{i=1}^{n+m}\\rho(\\mathbf{z}-\\mathbf{x}_{i})$ using the our method, where $\\rho(\\cdot)$ can take different penalties such as $\\ell_{2}$ penalty $\\|\\cdot\\|_{2}^{2}$ and $\\ell_{1}$ penalty $\\Vert\\cdot\\Vert_{2}$ . In Figure 9, we visualize the generated clean samples and outliers, as well as the ground truth means and the mean estimators with $\\eta(\\cdot)=\\|\\cdot\\|_{2}^{2}$ or $\\|\\cdot\\|_{2}$ under different outlier ratios $15\\%$ and $45\\%$ . The results show that, with the existence of outliers, the $\\ell_{2}$ -based estimator deviates far from the true mean, while the $\\ell_{1}$ -based estimator is more resistant to outliers and MCP-based estimator is the most robust. ", "page_idx": 25}, {"type": "image", "img_path": "UkauUrTbxx/tmp/c8b3076044378e469f90d2bb42ea3dd75e74ccc73eb250da7cb05c5d6f2e2cbb.jpg", "img_caption": ["Figure 9: Different estimators in simulations. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "F.3 Vulnerability Analysis of Vanilla Attention (WLS estimator) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To gain insight into the vulnerability of vanilla attention (WLS estimator), we attempt to analyze the attention can be formalized as a WLS estimator: effect of perturbed tokens on the output embedding of attention modules. Specifically, since vanilla $\\begin{array}{r}{\\mathbf{z}^{*}=\\arg\\operatorname*{min}_{\\mathbf{z}}\\mathcal{L}(\\mathbf{z})=\\sum_{j=1}^{N}a_{j}\\cdot\\|\\mathbf{v}_{j}-\\mathbf{z}\\|^{2}}\\end{array}$ , we quantify and compare $\\|\\mathbf{v}_{j}^{\\prime}-\\mathbf{z}^{*}\\|^{2}$ and $\\|\\mathbf{v}_{j}-\\mathbf{z}^{*}\\|^{2}$ , where $\\mathbf{v}_{j}$ and $\\mathbf{v}_{j}^{\\prime}$ are the original and perturbed tokens, and $\\mathbf{z}^{\\ast}$ is the ground truth output embedding. We present the numerical results across every attention module in the pretrained BERT on IMDB dataset in Table 5. The results demonstrate that attackers tend to introduce larger residuals $\\lVert\\mathbf{v}_{j}-\\mathbf{z}\\rVert$ by modifying the important tokens $\\mathbf{v}_{j}$ when perturbing the text. ", "page_idx": 26}, {"type": "table", "img_path": "UkauUrTbxx/tmp/52307a2e542ec78d3d71e24a9716b0233d29fdb34dec686ba2d2c7dda4aaccc3.jpg", "table_caption": ["Table 5: Residual magnitude of original and perturbed tokens. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "G Additional Experiments of Text Attacks ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "G.1 Sentiment Analysis: IMDB ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We present the results of sentiment analysis on IMDB dataset under various attacks in Table 6. We can conlude from the results that our methods improve the robustness of the backbones significantly by simply plugging the ProAttention into the models without additional fintuning or training. Moreover, our method can be combined with the existing defenses such as Adversarial Training (AT) to further improve the performance. ", "page_idx": 27}, {"type": "table", "img_path": "UkauUrTbxx/tmp/7bc3778881df0fc62eb20cd7e2e3bbac25ed03ec1272a1b9c241c2f389276485.jpg", "table_caption": ["Table 6: The results of sentiment analysis on IMDB. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "G.2 Textual Entailment: RTE ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In Table 7, we display the results of textual entailment on RTE across different cosine similarities constraints in TextFooler attack. We select DistilBERT as the backbone model and construct several MCP-based architectures with different $\\gamma$ . We can observe that our method can improve the robustness acorss different cosine similarities. The performance improvement is more evident under the smaller cosine similarities, which is equivalent to larger budgets. ", "page_idx": 27}, {"type": "text", "text": "In Table 8, we present the results of textual entailment on RTE across various attacks. The results exhibit the consistent improvement of our methods over the backbone model. ", "page_idx": 27}, {"type": "table", "img_path": "UkauUrTbxx/tmp/4d8b60fe25d316f22dec104eb50f5cb782ae581ed7725f4ff25282d9a69bc597.jpg", "table_caption": ["Table 7: The results of textual entailment on RTE across different cosine similarities in TextFooler "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "UkauUrTbxx/tmp/6b0882f0570d475563e522fe625180e706ce2f21f1033ed7fdf76824752d147d.jpg", "table_caption": ["Table 8: The results of textual entailment on RTE across different attacks. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "G.3 Adversarial Fine-tuning on Topic Classification: AGNEWS ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Adversarial training techniques are highly effective to enhance the robustness of models via adding the adversarial examples into the training set. To better capture the robustness enhancement of adversarial training, we track the adversarial fine-tuning curves and present the detailed results on AG\u2019s News in Table 9 and Figure 10. In the beginning of every epoch, we generate 1000 perturbed examples using the specific attack and then put them to the original training dataset. From the results, we can make the following observations: (1) the models show even better robustness during the process of adversarial training. (2) our method can be combined with adversarial training to further improve the resilience of the models. ", "page_idx": 28}, {"type": "table", "img_path": "UkauUrTbxx/tmp/eb51b782c6786f0b2821502cfec857d744fb7a2896fd4987e41f8db80ba75730.jpg", "table_caption": ["Table 9: Adversarial fine-tuning on AGNEWS. "], "table_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "UkauUrTbxx/tmp/86a86e31bcab84cf8147efeb73d2f8ad7b2229fab43dbefc3a99f6a49e1541bd.jpg", "img_caption": ["Figure 10: Adversarial fine-tuning on AGNEWS. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "G.4 Ablation Study on Attack constraints ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We present the ablation study on the maximum perturbation percentage, minimum cosines similarity and sentence similarity threshold in Figure 11, Table 10, Table 11 and Table 12, respectively. The experiments are performed on AGNEWS under TextFooler with the ALBERT as the backbone. The default values are as follows: sentence similarity threshold is 0.840845057, maximum perturbation percentage is 1.0, synonym cosine similarity is 0.5. The results show the consistent improvement of our method over the backbone models. ", "page_idx": 29}, {"type": "image", "img_path": "UkauUrTbxx/tmp/3c272fd56b3084946eaaafd45d3581c042786095ecbfa8c2fd6fed92e9b51ee1.jpg", "img_caption": ["Figure 11: Ablation studies on attack constraints. "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "UkauUrTbxx/tmp/bcd011d08ef2a113660b09f3c72f31c9aa00a120efd9ed1e8bb947c679168807.jpg", "table_caption": ["Table 10: Ablation study on max perturbation percentage. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "UkauUrTbxx/tmp/d052da11974bff366a8f4269266d97f8aaa635d9cbdd831eb7c76ee1de90e599.jpg", "table_caption": ["Table 11: Ablation study on minimum synonym cosine similarity. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "UkauUrTbxx/tmp/0e4ce8d3a2c250417a6e9ffb096d926aaaf18c602216e97ab90beab5f9498f41.jpg", "table_caption": ["Table 12: Ablation study on universal sentence similarity threshold. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "G.5 Ablation Study on Backbone Models ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Our proposed ProAttention is a universal framework which can be applied to various attention-based models. To verify the universality of our methods, we integrate our robust attention module into various backbones and present the results in Figure 12 and Table 13. As seen in the results, our ProAttention can consistently enhance the robustness over any backbone under various attacks. ", "page_idx": 30}, {"type": "image", "img_path": "UkauUrTbxx/tmp/44e2ef2cce4e6db18766e99e1217cceddc8ab2d8544185a54fd127150e80ff5f.jpg", "img_caption": ["Figure 12: Accuracy under attack of different backbones. "], "img_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "UkauUrTbxx/tmp/162187e3ce7a9785e72b97bebc0ee78b1e4f6e45c6f5bcfa8126b782e11833a1.jpg", "table_caption": ["Table 13: The results of different backbones on AGNEWS "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "G.6 Ablation Study on Hyperparameters of Penalties ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "G.6.1 Ablation Study on Huber ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For the Huber-based model, we present the ablation study on the $\\delta$ and layers $K$ of Huber loss under TextFooler in Table 14 and Figure 13. The default setting are as follows: $\\delta=0.9$ and $K=3$ , and we vary the two parameters separately to capture the trend. We can find that the performance is insensitive to $\\delta$ or layers within an appropriate range. ", "page_idx": 31}, {"type": "table", "img_path": "UkauUrTbxx/tmp/541c2368440bfad9beff04d90b1a9496394d01d4e0cb7e66bc83b4378bad5041.jpg", "table_caption": ["Table 14: Ablation study on Huber on AGNEWS. "], "table_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "UkauUrTbxx/tmp/acb8d4e9fb62f945bf3b58044e00289a0599dc26c93a39ff3262ef468fe36fe2.jpg", "img_caption": ["Figure 13: Ablation study on Huber "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "G.6.2 Ablation Study on MCP ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We present the ablation study of the $\\gamma$ and layers $K$ in MCP in Table 15 & Figure 14 and Table 16 & Figure 15 . From the figures, it can be concluded that appropriate $\\gamma$ is needed to get the best robustness. Besides, more layers can get a more precise solution for the robust objective, but we need to consider a good balance between precision and efficiency. ", "page_idx": 32}, {"type": "table", "img_path": "UkauUrTbxx/tmp/75aa9c17eeb762d728a804af29f449f83a4bb86b47b274117d282952dbfb7181.jpg", "table_caption": ["Table 15: Ablation on MCP on AGNEWS. "], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "UkauUrTbxx/tmp/6ed5acc9a9484d6cec01c9858f522d3c5d950be9b077457714d6e53e2fa583fe.jpg", "img_caption": ["Figure 14: Ablation study on MCP on AGNEWS. "], "img_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "UkauUrTbxx/tmp/ead2679c3d1a5f3914e83a72f158fceff56882aa18cb87097a35cab1113eb8c1.jpg", "table_caption": ["Table 16: Ablation in MCP on IMDB. "], "table_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "UkauUrTbxx/tmp/ed98496bcaf3de7faa4d1938e2e25989d1e3547adcac80a60577207092c5b4dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 15: Ablation study on MCP on IMDB. ", "page_idx": 33}, {"type": "text", "text": "H Additional Experiments on LLMs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "H.1 Experiments on T5 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "H.1.1 main result ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We compare the backbone model T5 with its robust version based on $\\ell_{1}$ , MCP and Huber loss in Figure 16 and Table 17. The experiments are conducted on SST2 under TextFooler. As shown in the figure, Pro-T5 (Huber or $\\ell_{1}$ ) can slightly improve the robustness of backbone T5. Moreover, Pro-T5 (MCP) significantly outperform other baselines, especially under the large attack budgets. ", "page_idx": 34}, {"type": "image", "img_path": "UkauUrTbxx/tmp/4227922da37f83b29ffffde671232ca6e5c93d23415754c3c8979487e219560b.jpg", "img_caption": ["Figure 16: Main results on T5. "], "img_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "UkauUrTbxx/tmp/7220d0e6dc900f704d94ef8332abce6e5d8b39143fc5642dcc65afe8c7714b82.jpg", "table_caption": ["Table 17: Accuary $(\\%)$ under prompt attack on SST2 (TextFooler, T5) "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "H.1.2 Ablation on Huber ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We present the ablation study on $\\delta$ in Huber on SST2 in Figure 17 and Table 18. We fix the number of layers as 3 and vary the values of $\\delta$ from 3.0 to 9.0. The Huber-based model with $\\delta=4.0$ perform best among all the selected parameters. ", "page_idx": 35}, {"type": "image", "img_path": "UkauUrTbxx/tmp/63ef772ae8becbbf19556e475e8d5b51ae4e344a8935e4a640d26087de4d3c68.jpg", "img_caption": ["Figure 17: Ablation study on Huber on T5 "], "img_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "UkauUrTbxx/tmp/dea8076265489d5fbed5c725568aabd57d7dbaf43df2fd63ff31feeb14e0f026.jpg", "table_caption": ["Table 18: Ablation study in Huber on SST2. "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "H.1.3 Ablation on MCP of T5 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We present the ablation studies in MCP on SST2 in Figure 18 and Table 19. The default settings are as follows: $K=3$ and $\\gamma=3.0$ . We can make the observations as follows: (1) For $\\gamma$ , the optimal value falls into the range of (3,5). (2) For number of layers, the best value can be chosen as 4 or 5. ", "page_idx": 36}, {"type": "image", "img_path": "UkauUrTbxx/tmp/f5aa57dd2b2747874621ab616deca7102bb02de4c97f38ddf2afe3769fb1d9e9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "UkauUrTbxx/tmp/ce7f281b5d5647ae5adbede686516f53797dc49a7f29a5052fd9e2dcdf4a2815.jpg", "table_caption": ["Table 19: Ablation study on MCP on SST2 "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "H.1.4 Ablation on Huber-MCP ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We present the ablation study on Huber-MCP on SST2 in Figure 19 and Table 20. The results show that Pro-T5 (Huber-MCP) is insensitive to $\\delta$ and get better robustness at $\\gamma=14$ or 15. ", "page_idx": 37}, {"type": "image", "img_path": "UkauUrTbxx/tmp/579cdccfa570c7ae9e6941cf397f198089d3f69c0742ea0a6992e6a40b6ba09d.jpg", "img_caption": ["Figure 19: Ablation studies on Huber-MCP "], "img_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "UkauUrTbxx/tmp/6ffa3fc6d7dcff97d8c632a835d5233c8e249727321c7c9d0a8214752555ca82.jpg", "table_caption": ["Table 20: Ablation study on Huber-MCP on SST2 "], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "H.2 Experiments on LLaMA ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "For LLaMA, we can observe an intriguing phenomenon that differs from the T5 case: the $\\ell_{1}$ and MCP-based methods sacrifice too much accuracy while Huber method can keep decent performance under small budgets. This reason is that in the small range region, $\\ell_{1}$ and MCP utilize a linear or concave functions while Huber can recover the $\\ell_{2}$ function. Inspired by the characteristics of these functions, we combine the properties of Huber and MCP, and construct a new function which we refer to Huber-MCP2 . The detailed formulation and derived robust attention layers are available in Appendix B. As indicated by the following curves, Huber-MCP and Huber-based models exhibits better robustness than other methods while preserving the good clean performance. ", "page_idx": 38}, {"type": "text", "text": "H.2.1 Textfooler ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We present the results of textual entailment on SST2 under TextFooler in Figure 20. We can observe that $\\ell_{1}$ and MCP-based methods sacrifice the performance because of the estimation bias. Pro-LLaMA (Huber-MCP) shows slight improvement over other models. ", "page_idx": 38}, {"type": "image", "img_path": "UkauUrTbxx/tmp/094d109cb293f387900f8a5eaa90a3454d17edf7011942cff9f4dd64c328debf.jpg", "img_caption": ["(c) Ablation on $\\gamma$ in Huber-MCP $\\emph{K}=({\\sf d})$ Ablation on $\\gamma$ in Huber-MCP $\\smash{\\bigcup_{s=1}^{\\infty}}$ $1,\\delta=9]$ ) $3,\\delta=9.$ ) ", "Figure 20: LLaMA (Textfooler) "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "H.2.2 TextBugger ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We present the results of textual entailment on SST2 under TextBugger in Figure 21. In this case, $\\ell_{1}$ -based model show a catastrophic performance drop while Pro-LLaMA (Huber) outperforms other baselines with a sinificant margin. ", "page_idx": 39}, {"type": "image", "img_path": "UkauUrTbxx/tmp/c590a15a015e55ee0a26144e20dd12098f5534c9deff68221fb9dd84f310ce77.jpg", "img_caption": ["(a) Main results on SST2 (TextBugger) (b) Ablation study on $\\delta$ in Huber $K=3$ ) "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "UkauUrTbxx/tmp/e2ac56691c945d1b10b3d7d05141711096a3e4ee9c5428c3bc27a32b4c33cc88.jpg", "img_caption": ["(c) Ablation study on $\\gamma$ in Huber-MCP ( $K=$ $3,\\delta=9]$ ) "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 21: LLaMA (TextBugger) ", "page_idx": 39}, {"type": "text", "text": "H.2.3 DeepWordBug ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We present the results of textual entailment on SST2 under DeepWordBug in Figure 22. The experiment shows the similar phenomenon that $\\ell_{1}$ and MCP-based models sacrifice too much performance. Additionally, Pro-LLaMA (Huber-MCP) significantly outperforms other methods. ", "page_idx": 40}, {"type": "image", "img_path": "UkauUrTbxx/tmp/556d6fa42324b4d98adcf9e04697658da2094fc0f1a01206e6b992dd407dd448.jpg", "img_caption": ["(a) Main results on SST2 (DeepWordBug) (b) Ablation study on $\\delta$ in Huber $K=3$ ) "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "UkauUrTbxx/tmp/eb076b36bab086a217800cf891959d114ca64c97ff350761897f8efd8779ab3d.jpg", "img_caption": ["(c) Ablation study on $\\gamma$ in Huber-MCP ( $\\overset{\\cdot}{\\theta}=$ $9,L=3]$ ) "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Figure 22: LLaMA (DeepWordBug) ", "page_idx": 40}, {"type": "text", "text": "I Additional Experiments on Jailbreak ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "I.1 Transfer Jailbreak ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We provide the results of transfer jailbreak in Table 21, Table 22, and Table 23. As SmoothLLM exhibits excellent performance to defend the jaibreaking attack, we also combine the random smoothing with the backbone models and our methods to further validate the effectiveness of our method. As shown in the results, when $q=0$ (without random smoothing), by simply plugging our ProAttention into the Vicuna, the robustness can be improved by a significant margin. Our plug-and-play method can even be comparable to the randomly smoothed models which require multiple operations including random perturbations, votings and aggregations. ", "page_idx": 41}, {"type": "table", "img_path": "UkauUrTbxx/tmp/aee746db3fdcc3e88c1c516c79e2ff424c5fb780e0649bf665ea92863d8d478a.jpg", "table_caption": ["Table 21: Vicuna: ASRs of JailBreak with Swap random smoothing on Behaviours. "], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "Table 22: Vicuna: ASRs of JailBreak with Insert random smoothing on Behaviours. ", "page_idx": 41}, {"type": "table", "img_path": "UkauUrTbxx/tmp/12c419289cbf4e8c6ba65680e82259c776079402f72dfeeeae238bb41afc756e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "Table 23: Vicuna: ASRs of JailBreak with Patch random smoothing on Behaviours. ", "page_idx": 41}, {"type": "table", "img_path": "UkauUrTbxx/tmp/d7e720129fe87dc8c8c81549e9ff4934483e53892f52c9a8454d8bcf9806111d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "I.2 Adaptive Jailbreak ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Although our Pro-Vicuna has demonstrated significant effectiveness under transfer attack (black-box), it is still unclear whether our method can be resilient under white-box attacks which adaptively target the specific victim models. The comparison of our Pro-Vicuna and backbone Vicuna under adaptive jailbreak is presented in Table 24 and Figure 23. Our Pro-Vicuna can improve Vicuna by an average of $10.4\\%$ across various numbers of attack queries. We don\u2019t include SmoothLLM in adaptive attacks since it introduces non-differentiable operators that preclude the gradient-based GCG attack on it. ", "page_idx": 42}, {"type": "table", "img_path": "UkauUrTbxx/tmp/269123d0946073cae0eb8c9fe2df0c677cb62fdf4d507756c33efb966522890e.jpg", "table_caption": ["Table 24: Vicuna: ASRs of Adaptive JailBreak on Behaviours "], "table_footnote": [], "page_idx": 42}, {"type": "image", "img_path": "UkauUrTbxx/tmp/5c6b609e7d5fea6c1b6541e1eac3df189aff4c5048698d35f8daf728dd623949.jpg", "img_caption": [], "img_footnote": ["Figure 23: Adaptive JailBreak "], "page_idx": 42}, {"type": "text", "text": "J Additional Experiments on ViT ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "J.1 Main Results ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "The main results of FGSM on ViT are presented in Table 25. We can conclude that our Pro-ViT (MCP) outperforms other methods across various budgets. ", "page_idx": 43}, {"type": "table", "img_path": "UkauUrTbxx/tmp/1f3cf65c09135fd9d1b07130644ecd780d70b2964d883091182018c93c76ffea.jpg", "table_caption": ["Table 25: Adversarial robustness on CIFAR-10 (FGSM). "], "table_footnote": [], "page_idx": 43}, {"type": "table", "img_path": "UkauUrTbxx/tmp/9e7b144a46a3aad06197352b34f92674d39d1842f4a220c5bbefcbbead9ef54a.jpg", "table_caption": ["Table 26: Adversarial Robsutness on Imagenet-1k (PGD). "], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "J.2 Ablation study. ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "The ablation study of PGD and FGSM are provided in Table 27, Table 28 and Figure 24. As shown in the results, the optimal $\\gamma$ of MCP fall into the range of (3,4). The robust estimators with more layers show the better robustness while slightly sacrifice the clean performance. ", "page_idx": 43}, {"type": "table", "img_path": "UkauUrTbxx/tmp/f8abcfbaec16150602ae623c2304bff93fd52cceca71756a9fd3353c13f03e37.jpg", "table_caption": ["Table 27: Ablation: CIFAR-10 (PGD) "], "table_footnote": [], "page_idx": 43}, {"type": "table", "img_path": "UkauUrTbxx/tmp/a9422d7e89ae641e44a2190cbaebc829b82b039f2870be618f873507c53195c0.jpg", "table_caption": ["Table 28: Ablation: CIFAR-10 (FGSM) "], "table_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "UkauUrTbxx/tmp/ea86cd8c773e380fcc5f9efad606ab1df0c16461ee788e9e6fb1d7899707f5f2.jpg", "img_caption": ["(c) Ablation study on $\\gamma$ in MCP (FGSM) (d) Ablation study on $L$ in MCP (FGSM) "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "K Additional Experiments in GAT ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "The main results on Citeseer and the ablation study on Cora-ML are presented in Table 29 and Table 30. From the results, we can make the following conclusions: (1) Our Pro-GAT outperform other methods significantly. Under the larger budgets, the outliers introduced by the adversarial attack will enlarge the bias effect of the estimation. In this scenario, our MCP function can mitigate or even remove the effect of outlying values in the large-value region. (2) The parameter $\\gamma$ provide an implication for the robustness. For small budget, the models with large $\\gamma$ perform well since it is more similar the original attention. While for large budget, the models with small $\\gamma$ offer better robustness cause it can mitigate the bias introduced by the outliers. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "table", "img_path": "UkauUrTbxx/tmp/a061e4325c5bbbcc878781dc6046b23f1e67dd268a5d5b70b631e7402e6cd7da.jpg", "table_caption": ["Table 29: Results on Citeseer. "], "table_footnote": [], "page_idx": 45}, {"type": "table", "img_path": "UkauUrTbxx/tmp/63fe79e433815f1a8a601f038280c629eb14de82c26e0336793a497b452978f2.jpg", "table_caption": ["Table 30: Ablation study on Cora-ML. "], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "L Complexity Analysis. ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Here we will provide a complexity analysis of the vanilla attention, our robust attention, KDE-based attention and RKDE-based attention. The related notations are Q, K, V \u2208RN\u00d7D and A \u2208RN\u00d7N. The major difference in these methods is how to derive the attention matrix, therefore we will only count the complexity of attention matrix derivation and context matrix computation. ", "page_idx": 46}, {"type": "text", "text": "\u2022 Vanilla Attention. The vanilla attention matrix in can be formulated as ${\\bf A}\\quad=$ softmax Q\u221aKD\u22a4 , which costs about N \u00d7 N \u00d7 D. The context matrix computation requires $N\\times N\\times D$ , so the total cost is $2\\cdot(N\\times N\\times D)$   \n\u2022 Our ProAttention. For our robust attention, we need to firstly compute the original matrix A $(N\\times N\\times D)$ . Then we need to compute weight $\\mathbf{W}^{(k)}$ based on the pairwise distance between the $\\mathbf{V}$ and current estimator $\\bar{\\mathbf{Z}^{(k)}}$ $(N\\times N\\times D)$ . Finally we need to update the estimator by $\\mathbf{Z}^{(k+1)}=\\bigl(\\mathbf{A}\\odot\\mathbf{W}^{(k)}\\bigr)\\mathbf{V}\\mathbf{\\Lambda}(N\\times N\\times$ . The context matrix is calculated through the iterations. Therefore, the total cost will be $(1+2K)\\cdot N\\times N\\times D$ . As stated in the ablation study in Section 4.2.2, our Newton-IRLS in ProAttention can converge efficiently within 3 steps $(K\\leq3)$ . Therefore, our ProAttention is still effeicient. Kernel Density Estimation (KDE) Attention. For KDE-based attention, the attention matrix is computed based on the pairwise ditance between the $\\mathbf{K}$ and $\\mathbf{Q}$ , which cost $N\\times N\\times D$ . The context matrix computation requires $N\\times N\\times D$ , so the total cost is $2\\cdot(N\\times N\\times D)$ .   \n\u2022 Robust Kernel Density Estimation (RKDE) Attention. For the RKDE-based attention, we need to perform the following operations. Firstly, we need to compute the basic matrix ad KDE attention matrix A $(N\\times N\\times D)$ . Then we need to compute pairwise distance ${\\bf D}_{\\bf K}^{(k)}$ for all the key . pSaiirms $(N\\times N\\times D)$ caanlcd uulaptdea tthe et hpea irwweiisgeh t $\\mathbf{W}_{\\mathbf{K}}^{(k)}$ eb ased o na $\\mathbf{W}_{\\mathbf{K}}^{(k-1)}$ e atnhde ${\\bf D}_{\\bf K}^{(k)}$ $(N\\times N\\times N)$ ilarly, we need  distanc $\\mathbf{D}_{\\mathbf{K}\\mathbf{V}}^{(k)}$ KV nd updat weight $\\mathbf{W}_{\\mathbf{K}\\mathbf{V}}^{(k)}$ for concatenated key and value, which costs $N\\times N\\times2D$ and $N\\times N\\times N$ , repectively. The context matrix computation requires $N\\times N\\times D$ . Therefore, the total cost will be $(2+3K)\\cdot N\\times N\\times D+2K\\cdot N\\times N\\times N$ . ", "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our abstract and introduction (Section 1) closely follow the contribution (at the end of Section 1) in the paper. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We discuss the limitations of the work in the Section 6. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide the theory in Section 3.3 and the detailed proof in the Appendix B. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We provide the detailed implementation in Section 3.4 and experimental setting in Section 4.1. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 48}, {"type": "text", "text": "Answer: [No] ", "page_idx": 48}, {"type": "text", "text": "Justification: We provide the detailed implementation in Appendix A and the dataset information in Section 4.1. We will organize the data and code access after the submission. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https:// nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We provide the detailed experimental setting in Section 4.1. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [No] ", "page_idx": 49}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 49}, {"type": "text", "text": "\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We include the theoretical computation analysis in Appendix 3.4 and the running time in Section 4.2.2. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper closely follows NeurIPS Code of Ethics ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety fliters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our paper does not use existing assets. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 51}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA]   \nJustification: Our paper does not release new assets.   \nGuidelines: \u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 52}]