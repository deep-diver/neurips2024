{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, which is the foundation for many of the models discussed and used in the current paper."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This paper adapted the Transformer architecture for computer vision, a domain explored in the current work\u2019s experiments."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-00-00", "reason": "This paper introduced the LLaMA large language model (LLM), one of the models used in the current work\u2019s experiments."}, {"fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality", "publication_date": "2023-00-00", "reason": "This paper introduced the Vicuna LLM, another model used in the current work\u2019s experiments."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2023-00-00", "reason": "This paper introduced the T5 model, a model used in the current work\u2019s experiments."}]}