[{"type": "text", "text": "Segment Any Change ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhuo Zheng1, Yanfei Zhong2\u2217, Liangpei Zhang2, Stefano Ermon1\u2217 1Stanford University 2Wuhan University zhuozheng@cs.stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem. In this paper, we propose the segment any change models (AnyChange), a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions. AnyChange is built on the segment anything model (SAM) via our training-free adaptation method, bitemporal latent matching. By revealing and exploiting intra-image and inter-image semantic similarities in SAM\u2019s latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way. We also propose a point query mechanism to enable AnyChange\u2019s zero-shot object-centric change detection capability. We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection. AnyChange sets a new record on the SECOND benchmark for unsupervised change detection, exceeding the previous SOTA by up to $4.4\\%$ $\\mathrm{F_{1}}$ score, and achieving comparable accuracy with negligible manual annotations (1 pixel per image) for supervised change detection. Code is available at https://github.com/Z-Zheng/pytorch-change-models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Earth\u2019s surface undergoes constant changes over time due to natural processes and human activities. Some of the dynamic processes driving these changes (e.g., natural disasters, deforestation, and urbanization) have huge impact on climate, environment, and human life (Zhu et al., 2022). Capturing these global changes via remote sensing and machine learning is a crucial step in many sustainability disciplines (Yeh et al., 2020; Burke et al., 2021). ", "page_idx": 0}, {"type": "text", "text": "Deep change detection models have yielded impressive results via large-scale pre-training (Manas et al., 2021; Wang et al., 2022; Mall et al., 2023; Zheng et al., 2023) and architecture improvements (Chen et al., 2021b; Zheng et al., 2022). However, their capabilities depend on training data and are limited to specific application scenarios. These models cannot generalize to new change types and data distributions (e.g., new geographic areas) beyond those seen during training. ", "page_idx": 0}, {"type": "text", "text": "This desired level of generalization on unseen change types and data distributions requires change detection models with zero-shot prediction capabilities. However, the concept of zero-shot change detection has not been explored so far in the literature. While we are in the era of \u201cfoundation models\u201d (Bommasani et al., 2021) and have witnessed the emergence of large language models (LLMs) and vision foundation models (VFMs) (e.g., CLIP (Radford et al., 2021) and Segment Anything Model (SAM) (Kirillov et al., 2023)) with strong zero-shot prediction and generalization capabilities via prompt engineering, zero-shot change detection is still an open problem. ", "page_idx": 0}, {"type": "text", "text": "To close this gap, we present Segment Any Change, the first change detection model with zero-shot generalization on unseen change types and data distributions. Our approach builds on SAM, the first promptable image segmentation model, which has shown extraordinary zero-shot generalization on object types and data distributions. While SAM is extremely capable, it is non-trivial to adapt SAM to change detection and maintain its zero-shot generalization and promptability due to the extreme data collection cost of large-scale change detection labels that would be required to enable promptable training as in the original SAM. ", "page_idx": 0}, {"type": "image", "img_path": "D7X9Grmd7L/tmp/370299ce438009d97b19bb4078a92ddeb7ba47eb5156c397a3746b7f641b2e43.jpg", "img_caption": ["Figure 1: Zero-Shot Change Detection with AnyChange on a wide range of application scenarios in geoscience. Each subfigure presents the pre-event image, the post-event image, and their change instance masks in order. The boundary of each change instance mask is rendered by cyan, and meanwhile, these change masks are also drawn on pre/post-event images to show more clearly where the change occurred. The color of each change mask is used to distinguish between different instances. ", "De-agriculturalization or Deforestation ", "Natural Resource Monitoring "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To resolve this problem, we propose a training-free method, bitemporal latent matching, that enables SAM to segment changes in bitemporal remote sensing images while inheriting these important properties of SAM (i.e., promptability, zero-shot generalization). This is achieved by leveraging intra-image and inter-image semantic similarities that we empirically discovered in the latent space of SAM when applying SAM\u2019s encoder on unseen multi-temporal remote sensing images. The resulting models, AnyChange, are capable of segmenting any semantic change. ", "page_idx": 1}, {"type": "text", "text": "AnyChange can yield class-agnostic change masks, however, in some real-world application scenarios, e.g., disaster damage assessment, there is a need for object-centric changes, e.g. to detect how many buildings are destroyed. To enable this capability we propose a point query mechanism for AnyChange, leveraging SAM\u2019s point prompt mechanism and our bitemporal latent matching for filtering desired object changes. The user only needs a single click on a desired object, AnyChange with the point query can yield change masks centered on this object\u2019s semantics, i.e., from this object class to others and vice versa, thus achieving object-centric change detection. ", "page_idx": 1}, {"type": "text", "text": "We demonstrate the zero-shot prediction capabilities of AnyChange on several change detection datasets, including LEVIR-CD (Chen & Shi, 2020), S2Looking (Shen et al., 2021), xView2 (Gupta et al., 2019), and SECOND (Yang et al., 2021). Due to the absence of published algorithms for zero-shot change detection, we also build baselines from the perspectives of zero-shot change proposal, zero-shot object-centric change detection, and unsupervised change detection. AnyChange outperforms other zero-shot baselines implemented by DINOv2 (Oquab et al., 2023) and SAM with different matching methods in terms of zero-shot change proposal and detection. From the unsupervised change detection perspective, AnyChange beats the previous state-of-the-art model, I3PE (Chen et al., 2023), setting a new record of $48.2\\%$ $\\mathrm{F_{1}}$ on SECOND. We show some qualitative results in Fig. 1, demonstrating the zero-shot prediction capabilities of AnyChange on a wide range of application scenarios (i.e., urbanization, disaster damage assessment, de-agriculturalization, deforestation, and natural resource monitoring). The contributions of this paper are summarized as follows: ", "page_idx": 1}, {"type": "image", "img_path": "D7X9Grmd7L/tmp/bec28a4306717f865c201051524839a4f911b7f9a2a714c3c80705acf2b605ff.jpg", "img_caption": ["Figure 2: Segment Any Change Models, AnyChange. SAM forward: given grid points $\\{{\\bf p}_{i}\\}$ as prompts and input images, SAM produces object masks $\\{\\mathbf{m}_{t,i}\\}$ and image embedding $\\mathbf{z_{t}}$ on the image at time $t$ . Bitemporal Latent Matching does a bidirectional matching to compute the change confidence score for each change proposal, and then top- $\\cdot\\mathbf{k}$ sorting or thresholding is applied for zero-shot change proposal and detection. Point Query allows users to click some points (the case of two points in this subfigure) with the same category to fliter class-agnostic change masks via semantic similarity for object-centric change detection. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 AnyChange, the first zero-shot change detection model, enables us to obtain both instance-level and pixel-level change masks either in a fully automatic mode or interactively with simple clicks. \u2022 Bitemporal Latent Matching, a training-free adaptation method, empowers SAM with zero-shot change detection by leveraging intra-image and inter-image semantic similarities of images in SAM\u2019s latent space. \u2022 Zero-Shot Change Detection is explored for the first time. We demonstrate the effectiveness of AnyChange from four perspectives, i.e., zero-shot change proposal, zero-shot object-centric change detection, unsupervised change detection, and label-efficient supervised change detection, achieving better results over strong baselines or previous SOTA methods. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Segment Anything Model (Kirillov et al., 2023) is the first foundation model for promptable image segmentation, possessing strong zero-shot generalization on unseen object types, data distributions, and tasks. The training objective of SAM is to minimize a class-agnostic segmentation loss given a series of geometric prompts. Based on compositions of geometric prompts, i.e., prompt engineering, SAM can generalize to unseen single-image tasks in a zero-shot way, including edge detection, object proposal, and instance segmentation. Our work extends SAM with zero-shot change detection for bitemporal remote sensing images via a training-free adaptation method, extending the use of SAM beyond single-image tasks. ", "page_idx": 2}, {"type": "text", "text": "Segment Anything Model for Change Detection. SAM has been used for change detection via a \u201cparameter-efficient fine-tuning\u201d (PEFT) paradigm (Mangrulkar et al., 2022), such as SAM-CD (Ding et al., 2023) that used Fast-SAM (Zhao et al., 2023) as a frozen visual encoder and fine-tuned adapter networks and the change decoder on change detection datasets in a fully supervised way. This model does not inherit the most two important properties of SAM, i.e., promptability and zero-shot generalization. Fine-tuning in a promptable way with large-scale training change data may achieve these two properties, however, collecting large-scale bitemporal image pairs with class-agnostic change annotations is non-trivial (Tewkesbury et al., 2015; Zheng et al., 2023), thus no such method exists in the current literature. Our work introduces a new and economic adaptation method for SAM, i.e., training-free adaptation, guaranteeing these two properties with zero additional cost, making zero-shot change detection feasible for the first time. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Unsupervised Change Detection. The most similar task to zero-shot change detection is unsupervised change detection (Coppin & Bauer, 1996), however zero-shot change detection is a more challenging task. They both require models to find class-agnostic change regions, and the main difference is that zero-shot change detection also requires models to generalize to unseen data distributions. From early model-free change vector analysis (CVA) (Bruzzone & Prieto, 2000; Bovolo & Bruzzone, 2006) to advanced deep CVA (Saha et al., 2019) and I3PE (Chen et al., 2023), unsupervised change detection methods have undergone a revolution enabled by deep visual representation learning. These model-based unsupervised change detection methods need to re-train their models on new data distributions. Our proposed model is training-free and can achieve comparable or even better performance for unsupervised change detection. ", "page_idx": 3}, {"type": "text", "text": "3 Segment Any Change Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This paper introduces Segment Any Change to resolve the long-standing open problem of zero-shot change detection (see Sec. 3.1). As illustrated in Fig. 2, we propose a new type of change detection models that support zero-shot prediction capabilities and generalization on unseen change types and data distributions, allowing two output structures (instance-level and pixel-level), and three application modes (fully automatic, semi-automatic with a custom threshold, and interactive with simple clicks). AnyChange achieves the above capabilities building on SAM in a training-free way. ", "page_idx": 3}, {"type": "text", "text": "3.1 Background ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Preliminary: Segment Anything Model (SAM) is a promptable image segmentation model with an image encoder of ViT (Dosovitskiy et al., 2021), a prompt encoder, and a mask decoder based on transformer decoders. As Fig. 2(a) shows, given as input a single image at time $t$ , the image embedding $\\mathbf{z}_{t}$ is extracted from its image encoder. Object masks $\\{\\mathbf{m}_{t,i}\\}$ can be obtained by its mask decoder given $\\mathbf{z}_{t}$ and dense point prompts obtained by feeding grid points $\\{{\\bf p}_{i}\\}$ into its prompt encoder. ", "page_idx": 3}, {"type": "text", "text": "Problem Formulation: Zero-Shot Change Detection is formulated at the pixel and instance levels. Our definition of \u201czero-shot\u201d is similar to that of SAM, i.e., the model without training on change detection tasks can transfer zero-shot to change detection tasks and new image distributions. $\\mathcal{U}=\\overline{{\\{0,1\\}}}$ denotes a set of classes of non-change and change. A pixel position or instance area belongs to the change class if the corresponding semantic categories at different times are different. This means that the model should generalize to unseen change types, even though all change types are merged into a class of 1. The input is a bitemporal image pair $\\mathbf{I}_{t},\\mathbf{I}_{t+1}\\in\\mathbb{R}^{h\\times w\\times\\ast}$ , where the image size is $(h,w)$ and $^*$ denotes the channel dimensionality of each image. For pixel level, the model is expected to yield a pixel-level change mask ${\\bf C}\\in\\mathcal{U}^{h\\times w}$ . For instance level, the model is expected to yield an arbitrary-sized set of change masks $\\{\\mathbf{m}_{i}\\}$ , where each instance ${\\bf m}_{i}$ is a polygon. ", "page_idx": 3}, {"type": "text", "text": "3.2 Exploring the Latent Space of SAM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivated by the experiments in Kirillov et al. (2023) on probing the latent space of SAM, we known there are potential semantic similarities between mask embeddings in the same natural image. We further explore the latent space for satellite imagery from both intra-image and inter-image perspectives, thus answering the following two questions: ", "page_idx": 3}, {"type": "text", "text": "Q1: Do semantic similarities exist on the same satellite image? Empirically, we find they do. We show this semantic similarity in two ways, i.e., visualizing the first components of principal components analysis (PCA) (Oquab et al., 2023) and probing the latent space (Kirillov et al., 2023), as Fig. 3 (a) shows. Observing the first three PCA components, geospatial objects with the same category have a similar appearance in this low-dimensional subspace. This suggests that this satellite image embedding from SAM encodes the semantics of geospatial objects reasonably well. Furthermore, we do the same latent space probing experiment as Kirillov et al. (2023) did, but on satellite images, and present the results in Fig. 3 (a) (bottom). We compute the mask embedding of object proposals and manually select three proposals with different categories (water, building, and bareland) as queries. ", "page_idx": 3}, {"type": "image", "img_path": "D7X9Grmd7L/tmp/74196512c3268b41fdbd56d727d918f8ad5fcdb5e3fd47692c7c27ac65ce81a9.jpg", "img_caption": ["(a) intra-image semantic similarity "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "D7X9Grmd7L/tmp/153dab9679a6c30c3edb3aa951f4c1919f1999547f82f42dab182f119abd1e9f.jpg", "img_caption": ["(b) inter-image semantic similarity "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Empirical evidence on semantic similarity on (a) the same satellite image and (b) the satellite images at different times. (a) The visualization of the first three PCA components indicates the objects with the same category are well matched with each other in SAM\u2019s latent space; Three red point queries confirm the existence of semantic similarity on the same satellite image. (b) The object proposals (indicated by red points) from the satellite image at $t_{1}$ as queries are used to match all proposals from the satellite image at $t_{2}$ . (best viewed with zoom, especially for the point query) By matching the query embedding with other mask embeddings, we obtain the most similar object proposals with the query proposal. We find that the most similar object proposals mostly belong to the same category as the query. ", "page_idx": 4}, {"type": "text", "text": "$Q2$ : Do semantic similarities exist on satellite images of the same location collected at different times? Empirically, we find they do. To verify this, we introduce a new satellite image from the same geographic area but at a different time $t_{1}$ . The above satellite image is captured at time $t_{2}$ . Different from the above latent space probing experiment, we use three object proposals with different spatial positions from the image at $t_{1}$ , as queries. These three queries have the same category, i.e., building. By matching with all proposals from the image at $t_{2}$ , we obtain three basically consistent results $(\\mathrm{F_{1}}$ of $68.1\\%{\\pm}0.67\\%$ and recall of $96.2\\%{\\pm}0.66\\%$ , as shown in Fig. 3 (b). This suggests that this semantic similarity exists on the satellite images at different times, even though the images have different imaging conditions because they were taken at different times. ", "page_idx": 4}, {"type": "text", "text": "From the above empirical study, we find that there are intra-image and inter-image semantic similarities in the latent space of SAM for unseen satellite images. These two properties are the foundation of our training-free adaptation method. ", "page_idx": 4}, {"type": "text", "text": "3.3 Bitemporal Latent Matching ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Based on our findings, we propose a training-free adaptation method, namely bitemporal latent matching, which bridges the gap between SAM and remote sensing change detection without requiring for training or architecture modifications. ", "page_idx": 4}, {"type": "text", "text": "The main idea is to leverage the semantic similarities in latent space to identify changes in bitemporal satellite images. Given the image embedding $\\mathbf{z}_{t}$ and object proposals $\\bar{\\mathcal{M}_{t}}=\\,\\{\\mathbf{\\bar{m}}_{t,i}\\}_{i\\in[1,2,...,N_{t}]}$ generated from SAM on the satellite image at time $t$ , each object proposal $\\mathbf{m}_{t,i}\\,\\in\\,\\mathbb{R}^{h\\times w}$ is a binary mask. We can compute the mask embedding $\\mathbf{x}_{t,i}\\,=\\,\\mathbf{z}_{t}[\\mathbf{m}_{t,i}]\\,\\in\\,R^{d_{m}}$ by averaging the image embedding $\\mathbf{z}_{t}$ over all non-zero positions indicated by the object proposal $\\mathbf{m}_{t,i}$ . Next, we introduce a similarity metric to measure the semantic similarity. To do this, we need to consider the statistical properties of the image embeddings from SAM. In particular, SAM\u2019s image encoder uses layer normalization (Ba et al., 2016). This means that the mask embedding has zero mean and unit variance if we drop the affine transformation in the last layer normalization, i.e., the vari\u221aance $\\begin{array}{r}{D(\\mathbf{x}_{t,i})=d_{m}^{-1}\\sum_{j}(\\bar{\\mathbf{x}_{t,i}^{'}}[j])^{2}=1}\\end{array}$ , thus we have the mask embedding\u2019s $\\ell_{2}$ norm $\\left\\|\\mathbf{x}_{t,i}\\right\\|_{2}=\\sqrt{d_{m}}$ , which is a constant since $d_{m}$ is the channel dimensionality. Given this, cosine similarity is a suitable choice to me\u221aasure similarity between two mask embeddings since they are on a hypersphere with a radius of $\\sqrt{d_{m}}$ , and differences are encoded by their directions. Therefore, we propose to use negative cosine similarity as the change confidence score $c(\\mathbf{x}_{i},\\mathbf{x}_{j})$ for mask embeddings $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nc(\\mathbf{x}_{i},\\mathbf{x}_{j})=-\\frac{\\mathbf{x}_{i}\\cdot\\mathbf{x}_{j}}{d_{m}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The next question is which two mask embeddings to use to compute the change confidence score. The real-world change is defined at the same geographic location from time $t$ to $t+1$ . This means that it is comparable only if two mask embeddings cover the approximate same geographic region. Therefore, we additionally compute the mask embedding $\\hat{\\mathbf{x}}_{t+1,i}^{\\phantom{\\mathrm{~\\,~}}}=\\mathbf{z}_{t+1}[\\mathbf{m}_{t,i}]\\in\\mathbb{R}^{d_{m}}$ on the image embedding $\\mathbf{z}_{t+1}$ using the same object proposal $\\mathbf{m}_{t,i}$ . We then compute the change confidence score $c(\\mathbf{x}_{t,i},\\hat{\\mathbf{x}}_{t+1,i})$ for the change at $\\mathbf{m}_{t,i}$ from $t$ to $t+1$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Since we need to guarantee the temporal symmetry (Zheng et al., 2021, 2022) of class-agnostic change, we propose to match the object proposals bidirectionally. To this end, the change confidence score $c(\\mathbf{x}_{t+1,i},\\hat{\\mathbf{x}}_{t,i})$ for the change at $\\mathbf{m}_{t+1,i}$ from $t+1$ to $t$ is also computed, where $\\hat{\\mathbf{x}}_{t,i}=\\mathbf{z}_{t}[\\mathbf{m}_{t+1,i}]\\in$ $\\mathbb{R}^{d_{m}}$ is computed on the image embedding $\\mathbf{z}_{t}$ with the same object proposal $\\mathbf{m}_{t+1,i}$ . Afterwards, we can match object proposals $\\mathcal{M}_{t}$ and $\\mathcal{M}_{t+1}$ bidirectionally, and $(N_{t}+N_{t+1})$ change proposals with their confidence score are obtained in total. We propose to finally obtain change detection predictions by sorting by confidence scores and selecting the top- $\\cdot\\mathbf{k}$ elements or by angle thresholding. The pseudo-code of Bitemporal Latent Matching is in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "3.4 Point Query Mechanism ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To empower AnyChange with interactive change detection with semantics, we combine our bitemporal latent matching with the point prompt mechanism of SAM, thus yielding the point query mechanism. Given as input a set of single-temporal point $\\{\\mathbf{p}_{t,i}\\}=\\left(x_{t,i},y_{t,i}\\right)$ with the same category, $t$ denote that this point belongs to the image at time $t$ , and $(x,y)$ indicates the spatial coordinate of image domain. The object proposals $\\left\\{\\mathbf{m_{p}}_{t,i}\\right\\}$ can be obtained via SAM\u2019s point prompts. Following our bitemporal latent matching, we then compute their average mask embedding $\\begin{array}{r}{\\bar{\\mathbf{x}}_{\\mathbf{p}_{t}}=n^{-1}\\sum_{1}^{n}\\mathbf{x}_{\\mathbf{p}_{t,i}}}\\end{array}$ and match it with all proposals $\\{\\mathcal{M}_{t},\\mathcal{M}_{t+1}\\}$ via cosine similarity. In this way, the object-centric change detection results can be obtained via a custom angle threshold. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we demonstrate the two most basic applications of AnyChange, i.e., (i) zero-shot change proposal and detection and (ii) change data engine. We conduct experiments from these two perspectives to evaluate our method. ", "page_idx": 5}, {"type": "text", "text": "4.1 Zero-Shot Object Change Proposals ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets: We use four commonly used change detection datasets to evaluate AnyChange. The first three, i.e., LEVIR-CD (Chen & Shi, 2020), S2Looking (Shen et al., 2021), and xView2 (Gupta et al., 2019), are building-centric change detection datasets. SECOND (Yang et al., 2021) is a multi-class (up to 36 change types) urban change detection dataset with full annotation. For zero-shot object proposal evaluation, we convert their labels into binary if the dataset has multi-class change labels. ", "page_idx": 5}, {"type": "text", "text": "Metrics: Conventional change detection mainly focuses on pixel-level evaluation using $\\mathrm{F_{1}}$ , precision, and recall. More and more real-world applications have started to focus on instance-level change, also called \u201cchange parcel\u201d. Based on this requirement and AnyChange\u2019s capability of predicting instance change, we adapt the evaluation protocol of the zero-shot object proposal (Kirillov et al., 2023) for the zero-shot change proposal since they have the same output structure. The metric is mask $\\operatorname{AR}@1000$ (Lin et al., 2014). Note that change proposals are class-agnostic, therefore, for the first three building-centric datasets, we cannot obtain accurate $\\mathrm{F_{1}}$ and precision due to incomplete \u201cany change\u201d annotations. These two metrics are only used for reference and to see whether the model is close to the naive baseline (predict all as \u201cchange\u201d class, and vice versa). Here we mainly focus on recall for both pixel- and instance-levels. ", "page_idx": 5}, {"type": "text", "text": "Baselines: AnyChange is based on SAM, however, there is no SAM or other $\\mathrm{VFM}^{2}$ -based zero-shot change detection model that can be used for comparison in the current literature. For a fair comparison, we build three strong baselines based on DINOv2 (Oquab et al., 2023) (a state-of-the-art VFM) or SAM. The simple baseline is CVA (Bruzzone & Prieto, 2000), a model-free unsupervised change detection method based on $\\ell_{2}$ norm as dissimilarity and thresholding. We build \u201cDINOv2+CVA\u201d, an improved version with DINOv2 using an idea similar to DCVA (Saha et al., 2019). We build \u201cSAM $^+$ Mask Match\u201d, which follows the macro framework of AnyChange and replaces the latent match with the mask match that adopts the IoU of masks as the similarity. \u201cSAM $^+$ CVA Match\u201d follows the same idea of AnyChange but adopts the negative $\\ell_{2}$ norm of feature difference as the similarity to compute pixel-level change map via SAM feature-based CVA first. The instance-level voting is then adopted to obtain change proposals. We also build each \u201cOracle\u201d version of AnyChange as an upper bound, where we fine-tune SAM with LoRA $\\boldsymbol{r}=32$ ) (Hu et al., 2022) and train a change ", "page_idx": 5}, {"type": "table", "img_path": "D7X9Grmd7L/tmp/4b6e2f18a26eba4e3a10876ed34f606ffa784204174286f503fddfb13bd5977f.jpg", "table_caption": ["Table 1: Zero-shot Object Change Proposals. The metrics include pixel-based $\\mathrm{F_{1}}$ , Precision (Prec.), and Recall (Rec.) and instance-based mask AR $@1000$ . Note that the metric names rendered with gray represent inaccurate estimations due to the absence of ground truth of \u201cany change\u201d, but reflect whether their predictions approximate the naive baseline (predict all as \u201cchange\u201d class). "], "table_footnote": ["confidence score network on each dataset. More implementation details can be seen in Appendix C.1. "], "page_idx": 6}, {"type": "text", "text": "Results: We compare the change recall of AnyChange with other zero-shot baselines in Table 1. For pixel-level change recall, AnyChange achieves better recalls than the other two SAM baselines, especially when using a small ViT-B. This is because bitemporal latent matching better measures semantic similarity, i.e., using the angle between two embeddings. We can observe that the gap between AnyChange and SAM baselines gradually reduces as the backbone becomes larger since visual representation capabilities generally become stronger. AnyChange still has better average performance on four datasets, although a stronger representation can close the performance gap to some extent. This highlights the importance of finding the essential semantic difference. Besides, our AnyChange with ViT-H (636M parameters) achieves comparable recalls to CVA with DINOv2 (ViT-G, 1,100M parameters) on four datasets in fewer parameters. On the SECOND dataset, all variants of AnyChange achieve better zero-shot change detection performance than the strong baseline, $\\mathrm{DINOv}2\\mathrm{+CVA},$ , and the margin is up to $3.2\\%$ $\\mathrm{F_{1}}$ . For instance-level change recall, AnyChange outperforms the other two SAM baselines by a significant margin. This further confirms the effectiveness and superiority of bitemporal latent matching. We observe that the Oracles obtained via supervised learning have superior precision since they learn semantics and dataset bias explicitly, however, this comes with the cost of recall on pixel or instance levels. ", "page_idx": 6}, {"type": "text", "text": "Ablation: Matching Strategy. \u201cMask Match\u201d performs geometry-based matching, while \u201cCVA Match\u201d and bitemporal latent matching perform latent-based matching. In Table 1, we see that latentbased matching is much more promising than geometry-based matching from multiple perspectives of pixel-level and instance-level change recall and zero-shot change detection performance. Compared with \u201cCVA Match\u201d, AnyChange outperforms \u201cSAM+CVA Match\u201d on instance-level object change proposals by large margins and has comparable recalls on pixel-level change recalls. In principle, the similarity of \u201cCVA Match\u201d is linearly related to our bitemporal latent matching since the magnitude of the embeddings is a constant. Therefore, the performance difference lies in the computational unit (pixel or instance). \u201cCVA Match\u201d is pixel-wise, while bitemporal latent matching is instance-wise. This suggests that it is more robust to compute mask embedding averaged over all involved pixel embeddings for matching. ", "page_idx": 6}, {"type": "text", "text": "Ablation: Matching Direction. As presented in Table 2, we can find that the performance of single-directional matching is sensitive to temporal order, e.g., mask AR of two single-directional matching on LEVIR-CD are $1.3\\%$ and $35.9\\%$ , respectively. This is because the class-agnostic change is naturally temporal symmetric which is exactly the motivation of our bidirectional design. This result also confirms generally higher and more robust zero-shot change proposal capability. ", "page_idx": 6}, {"type": "text", "text": "Ablation: Robustness to radiation variation. We used ViT-B as the backbone for fast experiments. The results are presented in Table 3. The performance jitter of mask AR is less than $2\\%$ $(-1.9\\%,+0.1\\%$ , - $.1.9\\%$ , $-1.4\\%$ ) on these four datasets. We believe this sensitivity to radiation variation is acceptable for most applications. ", "page_idx": 6}, {"type": "table", "img_path": "D7X9Grmd7L/tmp/c7341e123a84c19bf492786ec9c22d8f08ff04e26c8d66282da591c84c188879.jpg", "table_caption": ["Table 2: Ablation: Matching Direction. The backbones are ViT-B. Single-directional matching is sensitive to temporal order, while bidirectional matching is more stable due to its guaranteed temporal symmetry. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "D7X9Grmd7L/tmp/4cd12e3732689a8582f200b938b17d961f586ae18d21112c50ed979cdf6c5485.jpg", "table_caption": ["Table 3: Ablation: Robustness to radiation variation. The backbones are ViT-B. Radiation variation was simulated by applying random color jitter independently to the pre- and post-event images. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Zero-shot Object-centric Change Detection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Zero-shot object change proposal yields class-agnostic change masks. The point query mechanism can convert the class-agnostic change into object-centric change via simple clicks of the user on a single-temporal image, thus providing an interactive mode for AnyChange. This step is typically easy for humans. Here we evaluate the point query on three building-centric change datasets. ", "page_idx": 7}, {"type": "text", "text": "Results: We demonstrate the effect of the point query in Fig. 4. Without the point query, AnyChange yields class-agnostic change masks, including building changes, vegetation changes, etc. With a single-point query on a building, we can observe that change masks unrelated to the building are filtered out. Further clicking two more buildings to improve the stability of mask embedding, we find the building changes previously missed are successfully recovered. Table 4 quantitatively reflects this mechanism. With a single-point query, the zeroshot performances on three datasets significantly gain $\\sim$ $15\\%$ $\\mathrm{F_{1}}$ score. This improvement hurts recall as a cost, however it achieves a better trade-off between precision and recall. ", "page_idx": 7}, {"type": "text", "text": "After increasing to three-point queries, the recalls of the model on three datasets get back to some extent, and the model has comparable precision with the single-point query. The zero-shot performances on three datasets gain $\\sim\\,3\\%$ $\\mathrm{F_{1}}$ further. These results confirm the effectiveness of the point query as a plugin for AnyChange to provide an interactive mode. ", "page_idx": 7}, {"type": "table", "img_path": "D7X9Grmd7L/tmp/81278c263ebf6ce952adec712bbe1f65249f9a67e8b1794e516356415313f199.jpg", "table_caption": ["Table 4: Zero-shot Object-centric Change Detection. All results of introducing semantics from the point query are accurate estimations since the detected changes are object-centric. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 AnyChange as Change Data Engine ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "AnyChange can provide pseudo-labels for unlabeled bitemporal image pairs with zero or few annotation costs. To evaluate this capability, we conduct experiments on two typical tracks for remote sensing change detection: supervised object change detection and unsupervised change detection. ", "page_idx": 7}, {"type": "text", "text": "Training recipe: On S2Looking, we train the model on its training set with pure pseudo-labels of AnyChange with ViT-H with a single-point query. All training details follow Zheng et al. (2023) except the loss function. On SECOND, we train the model on its training set with pure pseudo-labels of AnyChange with ViT-H via fully automatic mode, and other details follow Zheng et al. (2022). For both these two cases, the loss function is BCE loss with a label smoothing of 0.4. See Appendix C.2 for more details. ", "page_idx": 7}, {"type": "image", "img_path": "D7X9Grmd7L/tmp/945758ceac7a9c00dc66fc19f156feba13f47fb0427390b7107c3473293ccb4a.jpg", "img_caption": ["Figure 4: Examples of Point Query Mechanism. The effects of w/o point query, one-point query, and three-point queries are shown in sequence from left to right. (best viewed digitally with zoom, especially for the red points) "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "D7X9Grmd7L/tmp/72bcd701cdc5f4e294bd2048de1e52cc1949e7d072a3a3945eeec1df57ca35b5.jpg", "table_caption": ["Table 5: Supervised Object Change Detection. Comparison with the state-of-the-art change detectors on the S2Looking test. \u201cR- $18^{\\circ}$ : ResNet-18. The amount of Flops was computed with a float32 tensor of shape [2,512,512,3] as input. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Supervised Object Change Detection Results. Table 5 presents standard benchmark results on the S2Looking dataset, which is the one of most challenging building change detection datasets. From a data-centric perspective, we use the same architecture with different fine-tuned data. The model architecture adopts ResNet18-based ChangeStar $(1\\!\\times\\!96)$ (Zheng et al., 2021) due to its simplicity and good performance. We set the fine-tuning on $100\\%$ ground truth as the upper bound, which achieved $66.3\\%$ $\\mathrm{F_{1}}$ . We can observe that fin-tuning on the pseudo-labels of AnyChange with ViT-H yields $40.2\\%\\,\\mathrm{F_{1}}$ with 3,500-pixel annotations3. Leveraging AnyChange, we achieve $61\\%$ of the upper bound at a negligible cost $\\zeta\\sim10^{-5}\\%$ full annotations). We also compare it to the model trained with fewer annotations ( ${1\\%}$ and $0.1\\%$ ). We find that their performances are reduced by a significant amount, and are inferior to the model trained with pseudo-labels from AnyChange. This confirms the potential of AnyChange as a change data engine for supervised object change detection. ", "page_idx": 8}, {"type": "text", "text": "Unsupervised Change Detection Results. AnyChange\u2019s class-agnostic change masks are natural pseudo-labels for unsupervised change detection. We also compare our AnyChange with unsupervised change detection methods. In Table 6, we find that AnyChange with ViT-B in a zero-shot setting improves over the previous state-of-the-art method, I3PE (Chen et al., 2023). To learn the biases on the SECOND dataset, we trained a ChangeStar $(1\\!\\times\\!256)$ model with pseudo-labels of AnyChange on the SECOND training set. The setting follows I3PE ", "page_idx": 8}, {"type": "text", "text": "The results show that two variants of unsu Notably, based on pseudo-labels of AnyChange with ViT-B, our model set a new record of $48.2\\%$ $\\mathrm{F_{1}}$ on the SECOND dataset for unsupervised change detection. Besides, we obtain some useful insights for unsupervised change detection: (i) deep features from VFMs significantly assist unsupervised change detection models since $\\mathrm{DINOv}2\\mathrm{+CVA}$ beats all advanced competitors except I3PE. Before our strong baseline, DI$\\mathrm{NOv}2\\mathrm{+}\\mathrm{CV}\\mathrm{A}$ , CVA has been always regarded as a simple and ineffective baseline for unsupervised change detection. (ii) dataset biases are helpful for in-domain unsupervised change detection since our model trained with pseudolabels achieves higher performance than these pseudo-labels. This indicates the model learns some biases on the SECOND dataset, which may be change types and style. ", "page_idx": 9}, {"type": "text", "text": "one and use ResNet50 for ChangeStar $(1\\!\\times\\!256)$ . ervised ChangeStar $(1\\!\\times\\!256)$ outperform I3PE. Table 6: Unsupervised Change Detection. Comparison with the state-of-the-art unsupervised change detectors on the SECOND test. \u201c\\*\u201d indicates this change detection model is trained with pseudo labels predicted by AnyChange. ", "page_idx": 9}, {"type": "table", "img_path": "D7X9Grmd7L/tmp/9eb307e4b868a5a5cf1fb8f33833bb81ebe91bac03d3f94e9ac52718c7ce93d0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present the segment any change models (AnyChange), a new type of change detection model for zero-shot change detection, allowing fully automatic, semi-automatic with custom threshold, and interactive mode with simple clicks. The foundation of all these capabilities is the intra-image and inter-image semantic similarities in SAM\u2019s latent space we identified on multi-temporal remote sensing images. Apart from zero-shot change detection, we also demonstrated the potential of AnyChange as the change data engine and demonstrated its superiority in unsupervised and supervised change detection. AnyChange is an out-of-the-box zero-shot change detection model, and a step forward towards a \u201cfoundation model\u201d for the Earth vision community. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), the CZ Biohub, and the National Natural Science Foundation of China under Grant No. 42325105. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. ", "page_idx": 9}, {"type": "text", "text": "Luca Bergamasco, Sudipan Saha, Francesca Bovolo, and Lorenzo Bruzzone. Unsupervised change detection using convolutional-autoencoder multiresolution features. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201319, 2022. ", "page_idx": 9}, {"type": "text", "text": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. ", "page_idx": 9}, {"type": "text", "text": "Francesca Bovolo and Lorenzo Bruzzone. A theoretical framework for unsupervised change detection based on change vector analysis in the polar domain. IEEE Transactions on Geoscience and Remote Sensing, 45(1): 218\u2013236, 2006. ", "page_idx": 9}, {"type": "text", "text": "Lorenzo Bruzzone and Diego F Prieto. Automatic analysis of the difference image for unsupervised change detection. IEEE Transactions on Geoscience and Remote sensing, 38(3):1171\u20131182, 2000. ", "page_idx": 9}, {"type": "text", "text": "Marshall Burke, Anne Driscoll, David B Lobell, and Stefano Ermon. Using satellite imagery to understand and promote sustainable development. Science, 371(6535):eabe8628, 2021. ", "page_idx": 9}, {"type": "text", "text": "Hao Chen and Zhenwei Shi. A spatial-temporal attention-based method and a new dataset for remote sensing image change detection. Remote Sensing, 12(10):1662, 2020. ", "page_idx": 9}, {"type": "text", "text": "Hao Chen, Wenyuan Li, and Zhenwei Shi. Adversarial instance augmentation for building change detection in remote sensing images. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201316, 2021a.   \nHao Chen, Zipeng Qi, and Zhenwei Shi. Remote sensing image change detection with transformers. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201314, 2021b.   \nHongruixuan Chen, Jian Song, Chen Wu, Bo Du, and Naoto Yokoya. Exchange means change: An unsupervised single-temporal change detection framework based on intra-and inter-image patch exchange. ISPRS Journal of Photogrammetry and Remote Sensing, 206:87\u2013105, 2023.   \nPol R Coppin and Marvin E Bauer. Digital change detection in forest ecosystems with remote sensing imagery. Remote sensing reviews, 13(3-4):207\u2013234, 1996.   \nRodrigo Caye Daudt, Bertr Le Saux, and Alexandre Boulch. Fully convolutional siamese networks for change detection. In ICIP, pp. 4063\u20134067. IEEE, 2018.   \nLei Ding, Kun Zhu, Daifeng Peng, Hao Tang, and Haitao Guo. Adapting segment anything model for change detection in hr remote sensing images. arXiv preprint arXiv:2309.01429, 2023.   \nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.   \nBo Du, Lixiang Ru, Chen Wu, and Liangpei Zhang. Unsupervised deep slow feature analysis for change detection in multi-temporal remote sensing images. IEEE Transactions on Geoscience and Remote Sensing, 57(12):9976\u20139992, 2019.   \nRitwik Gupta, Richard Hosfelt, Sandra Sajeev, Nirav Patel, Bryce Goodman, Jigar Doshi, Eric Heim, Howie Choset, and Matthew Gaston. xbd: A dataset for assessing building damage from satellite imagery. arXiv preprint arXiv:1911.09296, 2019.   \nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.   \nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment Anything. In ICCV, pp. 4015\u20134026, October 2023.   \nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pp. 740\u2013755. Springer, 2014.   \nUtkarsh Mall, Bharath Hariharan, and Kavita Bala. Change-aware sampling and contrastive learning for satellite images. In CVPR, pp. 5261\u20135270, 2023.   \nOscar Manas, Alexandre Lacoste, Xavier Gir\u00f3-i Nieto, David Vazquez, and Pau Rodriguez. Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data. In ICCV, pp. 9414\u20139423, 2021.   \nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft, 2022.   \nAllan Aasbjerg Nielsen. The regularized iteratively reweighted mad method for change detection in multi-and hyperspectral data. IEEE Transactions on Image processing, 16(2):463\u2013478, 2007.   \nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \nNobuyuki Otsu. A threshold selection method from gray-level histograms. IEEE transactions on systems, man, and cybernetics, 9(1):62\u201366, 1979.   \nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8748\u20138763. PMLR, 18\u201324 Jul 2021.   \nSudipan Saha, Francesca Bovolo, and Lorenzo Bruzzone. Unsupervised deep change vector analysis for multiplechange detection in vhr images. IEEE Transactions on Geoscience and Remote Sensing, 57(6):3677\u20133693, 2019.   \nLi Shen, Yao Lu, Hao Chen, Hao Wei, Donghai Xie, Jiabao Yue, Rui Chen, Shouye Lv, and Bitao Jiang. S2looking: A satellite side-looking dataset for building change detection. Remote Sensing, 13(24):5094, 2021.   \nAndrew P Tewkesbury, Alexis J Comber, Nicholas J Tate, Alistair Lamb, and Peter F Fisher. A critical synthesis of remotely sensed optical image change detection techniques. Remote Sensing of Environment, 160:1\u201314, 2015.   \nDi Wang, Jing Zhang, Bo Du, Gui-Song Xia, and Dacheng Tao. An empirical study of remote sensing pretraining. IEEE Transactions on Geoscience and Remote Sensing, 2022.   \nChen Wu, Bo Du, and Liangpei Zhang. Slow feature analysis for change detection in multispectral imagery. IEEE Transactions on Geoscience and Remote Sensing, 52(5):2858\u20132874, 2013.   \nChen Wu, Hongruixuan Chen, Bo Du, and Liangpei Zhang. Unsupervised change detection in multitemporal vhr images based on deep kernel pca convolutional mapping network. IEEE Transactions on Cybernetics, 52 (11):12084\u201312098, 2021.   \nPengfeng Xiao, Xueliang Zhang, Dongguang Wang, Min Yuan, Xuezhi Feng, and Maggi Kelly. Change detection of built-up land: A framework of combining pixel-based detection and object-based recognition. ISPRS Journal of Photogrammetry and Remote Sensing, 119:402\u2013414, 2016.   \nKunping Yang, Gui-Song Xia, Zicheng Liu, Bo Du, Wen Yang, Marcello Pelillo, and Liangpei Zhang. Asymmetric siamese networks for semantic change detection in aerial images. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201318, 2021.   \nChristopher Yeh, Anthony Perez, Anne Driscoll, George Azzari, Zhongyi Tang, David Lobell, Stefano Ermon, and Marshall Burke. Using publicly available satellite imagery and deep learning to understand economic well-being in africa. Nature communications, 11(1):2583, 2020.   \nXu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment anything. arXiv preprint arXiv:2306.12156, 2023.   \nZhuo Zheng, Ailong Ma, Liangpei Zhang, and Yanfei Zhong. Change is Everywhere: Single-temporal supervised object change detection in remote sensing imagery. In ICCV, pp. 15193\u201315202, 2021.   \nZhuo Zheng, Yanfei Zhong, Shiqi Tian, Ailong Ma, and Liangpei Zhang. ChangeMask: Deep multi-task encoder-transformer-decoder architecture for semantic change detection. ISPRS Journal of Photogrammetry and Remote Sensing, 183:228\u2013239, 2022.   \nZhuo Zheng, Shiqi Tian, Ailong Ma, Liangpei Zhang, and Yanfei Zhong. Scalable multi-temporal remote sensing change data generation via simulating stochastic change process. In ICCV, pp. 21818\u201321827, 2023.   \nZhe Zhu, Shi Qiu, and Su Ye. Remote sensing of land change: A multifaceted perspective. Remote Sensing of Environment, 282:113266, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "B Pseudocode of Bitemporal Latent Matching ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Algorithm 1 Bitemporal Latent Matching (top-k)   \nRequire: $\\mathbf{z}_{t},\\mathbf{z}_{t+1}$ : image embeddings; $\\overline{{\\mathcal{M}_{t},\\mathcal{M}_{t+1}}}$ : object proposals; $k$ : number of change proposals   \n1: for $i=1$ to $N_{t}$ do   \n2: $\\mathbf{m}_{t,i}\\gets\\mathcal{M}_{t}[i]$   \n3: $\\mathbf{x}_{t,i}\\gets\\mathbf{z}_{t}[\\mathbf{m}_{t,i}]$   \n4: $\\hat{\\mathbf{x}}_{t+1,i}\\gets\\mathbf{z}_{t+1}[\\mathbf{m}_{t,i}]$   \n5: $c_{t,i}\\gets-d_{m}^{-1}\\mathbf{x}_{t,i}\\cdot\\hat{\\mathbf{x}}_{t+1,i}$ $\\triangleright$ compute change confidence score via embedding dissimilarity   \n6: for $j=1$ to $N_{t+1}$ do   \n7: $\\mathbf{m}_{t+1,j}\\leftarrow\\mathcal{M}_{t+1}[j]$   \n8: $\\mathbf{x}_{t+1,j}\\gets\\mathbf{z}_{t+1}[\\mathbf{m}_{t+1,j}]$   \n9: $\\hat{\\mathbf{x}}_{t,j}\\gets\\mathbf{z}_{t}[\\mathbf{m}_{t+1,j}]$   \n10: $c_{t+1,j}\\gets-d_{m}^{-1}\\mathbf{x}_{t+1,j}\\cdot\\hat{\\mathbf{x}}_{t,j}$ $\\triangleright$ compute change confidence score via embedding dissimilarity   \n11: $\\{\\mathbf{m}_{i}\\}_{1}^{k}\\gets\\mathsf{s o r t}(\\mathcal{M}_{t}\\cup\\mathcal{M}_{t+1}$ , key=lambda t, $\\mathfrak{i}:\\{c_{t,i}\\}\\cup\\{c_{t+1,j}\\}[\\mathbf{t}\\,,\\mathfrak{i}])[:k]$ $\\triangleright$ Python-style top-k sorting   \n12: return $\\{\\mathbf{m}_{i}\\}_{1}^{k}$ ", "page_idx": 12}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Baselines for zero-shot change proposal and detection ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "SAM forward: For object proposal generation, we adopt a point per side of 64, an NMS threshold of 0.7, a predicted IoU threshold of 0.5, and a stability score threshold is 0.8 for LEVIR-CD, S2\u221aLooking, SECOND, and 0.95 for xView2. To obtain the image embedding with a constant $\\ell_{2}$ norm $(\\sqrt{d_{m}})$ , we demodulate the output of the image encoder $f$ of SAM with the affine transformation $(\\mathbf{w};\\mathbf{b})$ of the last layer normalization, i.e., $\\mathbf{z}=\\mathbf{\\dot{w}}^{-1}(f(\\mathbf{x})-\\mathbf{b})$ , where $\\mathbf{x}$ is the single input image. The forward computation was conducted on 8 NVIDIA A4000 GPUs. ", "page_idx": 12}, {"type": "text", "text": "$\\mathbf{DINOv}2+\\mathbf{CVA}$ : The implementation is straightforward where the image embedding is first extracted from DINOv2 and then upsampled into the image size with bilinear interpolation. The change intensity map is computed by the $\\ell_{2}$ norm of the difference between bitemporal image embeddings. The optimal threshold is obtained by an adaptive threshold selection method, OTSU (Otsu, 1979). We also conduct a linear search for its optimal threshold on a small validation set, and the searched results have comparable performance with OTSU. Considering optimal peak performance and ease of use, we choose OTSU\u2019s threshold as the default for this baseline. ", "page_idx": 12}, {"type": "text", "text": "$\\mathbf{SAM+Mask}$ Match: The main idea of this baseline is to use the geometric difference to measure an object\u2019s change. In general, if an object disappears at the next time image, the object mask generated by SAM is empty or has a different geometric shape, and vice versa. Therefore, using their geometric shape difference to measure if the change occurred is reasonable. To this end, we compute pairwise mask IoU between pre-event object masks and post-event object masks to match a potentially identical object at another time for each object. We recognize the object region belongs to non-change when successfully matched $(\\mathrm{IoU}{>}0.5)$ ), otherwise this region belongs to a change. ", "page_idx": 12}, {"type": "text", "text": "$\\mathbf{SAM+CVA}$ Match: This baseline follows the same idea of AnyChange, i.e., latent-based matching, but adopts the negative $\\ell_{2}$ norm of feature difference as the similarity. This method first computes pixel-level change map via SAM feature-based CVA, the procedure of which is similar to $\\phantom{-}\\mathrm{^{\\leftarrow}D I N O v}2+$ CVA\u201d. The instance-level voting with a threshold of 0.5 is then adopted to obtain change proposals, which means that each region is considered as a change when more than half of the pixels are identified as changes. ", "page_idx": 12}, {"type": "text", "text": "AnyChange: For a fair comparison and automatic benchmark evaluation, we use fully automatic mode for AnyChange in Table 1. The change angle threshold $(155^{\\circ})$ is obtained by OTSU and a linear search on the small validation set sampled from the SECOND training set. This threshold is directly used for the other three datasets without any adjustment. ", "page_idx": 12}, {"type": "text", "text": "AnyChange (Oracle): We only train a LoRA ( $\\textit{r}\\!=32$ , $a l p h a=320$ , dropout = 0.1) for the SAM model on each dataset due to unaffordable full-parameter training. Besides, we attach a change confidence score network on the image encoder of SAM to predict an accurate semantic similarity instead of our negative cosine similarity. This network architecture is composed of an MLP block (Linear-LayerNorm-GELU-Linear-LayerNorm), an upsampling block (ConvTranspose-LayerNormGELU-ConvTranspose-Linear-LayerNorm-Linear-LayerNorm) for $4\\times$ upsampling, and a linear layer for predicting change confidence score. The loss function is a compound of binary cross-entropy loss and soft dice loss. The inference pipeline exactly follows AnyChange, where it first generates object masks and computes mask embeddings, and the change confidence score is obtained from the trained score network. The training iterations are 200 epochs with a batch size of 16, AdamW optimizer with a weight decay of 0.01. The learning rate schedule is \u201cpoly\u201d $(\\gamma=0.9)$ decay with an initial learning rate of 6e-5 The training data augmentation adopts random rotation, flip, scale jitter, and cropping. The crop size is 512 for LEVIR-CD and S2Looking and 256 for xView2 and SECOND, respectively. ", "page_idx": 13}, {"type": "text", "text": "C.2 Pseudo-label Training ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We have the training step of 20k, a batch size of 16, SGD with a momentum of 0.9, and a weight decay of 1e-3 for optimization on the SECOND training set with pseudo-labels of AnyChange in fully automatic mode. For regularization, we adopt the label smoothing of 0.4 and strong data augmentations which include random crop to $256\\!\\times\\!256$ , filp, rotation, scale jitter, and temporal-wise adjustment of brightness and contrast. ", "page_idx": 13}, {"type": "text", "text": "D Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Zero-shot change detection is an open problem in remote sensing and computer vision communities. There is little reference to point out any potential effective roadmap before our work. Our work is the first to define this problem suitably and provide a simple yet effective model and evaluation protocol. The problem formulation and evaluation protocol themselves bring some potential limitations (i.e., scenario coverage, the robustness to objects of different geometries) since there is little mature infrastructure, e.g., a concept-complete class-agnostic change detection dataset. ", "page_idx": 13}, {"type": "text", "text": "E Broader Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The proposed method can detect class-agnostic changes in Earth\u2019s surface, however, its actual effect is impacted by SAM\u2019s latent space. The model may produce some impossible changes due to SAM\u2019s biases. These issues warrant further research and consideration when building upon this work for real-world applications. ", "page_idx": 13}, {"type": "text", "text": "F More visualization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Nature Image Domain. Our method can be also used in the natural image domain to detect more general object changes, as shown in Fig. 5. ", "page_idx": 13}, {"type": "text", "text": "More demonstration of point query mechanism. We provide an additional example to supplement Fig. 4, which includes unchanged and changed buildings simultaneously. It is more clear to show that AnyChange can more accurately detect building changes with the help of the point query. ", "page_idx": 13}, {"type": "text", "text": "Effectiveness of AnyChange in detecting tiny object changes. Fig. 7 demonstrates a case of tiny/minor changes, such as small vehicle changes. We observe that directly applying AnyChange to the original image overlooks these subtle changes (see the first row). After we bilinearly upsampled the red box region by $2\\times$ and then applied AnyChange to it, we find some tiny/minor changes could be detected (see the second row). This observation shows that our method has the ability on tiny object change detection, although it is not yet optimal. Future work could use our approach as a strong baseline to further improve the detection of subtle changes. ", "page_idx": 13}, {"type": "image", "img_path": "D7X9Grmd7L/tmp/3220ae4302a8941f1652fbf23145d1dd0d6c305ad3d7b52a2dcb691041e9b384.jpg", "img_caption": ["Figure 5: AnyChange on Natural Image Domain. Best viewed digitally with zoom. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "D7X9Grmd7L/tmp/80a522a88155d8316fc59b03067e87f1e1462ea776132f9cefb50f0d60932a16.jpg", "img_caption": ["Figure 6: Examples of Point Query Mechanism. The effects of w/o point query, one-point query, and three-point queries are shown in sequence from left to right. (best viewed digitally with zoom, especially for the red points) "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "D7X9Grmd7L/tmp/bd5fe1f96a2861f092b330894abbd66f1166da197ad5bb7111cba2debdfb01b6.jpg", "img_caption": ["Figure 7: Illustration of the effectiveness of AnyChange in detecting tiny or minor object changes. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We have made claims in the abstract and introduction. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We have discussed the limitations in Appendix D ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: We have no theoretical result. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have fully described technical details and pseudo-code for our method. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We provide source code and pseudo-code for our method. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have reported all training and test details. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our algorithm is training-free and deterministic. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have provided this information. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: conformed. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have discussed broader impacts in Appendix E ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 18}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our paper poses no such risks ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have cited their publications. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not release new assets. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]