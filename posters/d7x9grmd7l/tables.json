[{"figure_path": "D7X9Grmd7L/tables/tables_6_1.jpg", "caption": "Table 1: Zero-shot Object Change Proposals. The metrics include pixel-based F1, Precision (Prec.), and Recall (Rec.) and instance-based mask AR@1000. Note that the metric names rendered with gray represent inaccurate estimations due to the absence of ground truth of \"any change\", but reflect whether their predictions approximate the naive baseline (predict all as \u201cchange\u201d class).", "description": "This table presents the results of zero-shot object change proposal experiments on four datasets (LEVIR-CD, S2Looking, xView2, SECOND).  It compares the performance of AnyChange against several baseline methods (CVA, DINOV2+CVA, SAM+Mask Match, SAM+CVA Match) across different backbone models (ViT-B, ViT-L, ViT-H). The evaluation metrics include pixel-level F1 score, precision, recall, and instance-level mask Average Recall at 1000 (AR@1000).  Metrics shaded gray indicate less reliable estimates due to the lack of ground truth for \"any change\" but help assess whether model predictions are close to the naive approach of labelling everything as changed or unchanged.", "section": "4.1 Zero-Shot Object Change Proposals"}, {"figure_path": "D7X9Grmd7L/tables/tables_7_1.jpg", "caption": "Table 2: Ablation: Matching Direction. The backbones are ViT-B. Single-directional matching is sensitive to temporal order, while bidirectional matching is more stable due to its guaranteed temporal symmetry.", "description": "This table presents the ablation study of matching direction in the AnyChange model. It compares the performance of bidirectional matching against two single-directional matching strategies (from t to t+1 and from t+1 to t). The results show that bidirectional matching is more robust and achieves better performance across all four datasets (LEVIR-CD, S2Looking, xView2, and SECOND) and metrics (F1, Precision, Recall, and mask AR@1000). The superior performance of bidirectional matching is attributed to its inherent temporal symmetry, which is crucial for effectively capturing change events in bitemporal remote sensing images.", "section": "4.1 Zero-Shot Object Change Proposals"}, {"figure_path": "D7X9Grmd7L/tables/tables_7_2.jpg", "caption": "Table 3: Ablation: Robustness to radiation variation. The backbones are ViT-B. Radiation variation was simulated by applying random color jitter independently to the pre- and post-event images.", "description": "This ablation study demonstrates the robustness of the AnyChange model to variations in radiation.  The table presents the performance metrics (F1 score, precision, recall, and mask AR@1000) across four different datasets (LEVIR-CD, S2Looking (binary), xView2 (binary), and SECOND (binary)).  The \"baseline\" row shows the results without any added radiation variation. The \"w/ color jitter\" row shows the results with random color jitter applied to both pre- and post-event images, simulating radiation variations.  The relatively small change in performance metrics indicates that AnyChange is robust to these variations.", "section": "4.1 Zero-Shot Object Change Proposals"}, {"figure_path": "D7X9Grmd7L/tables/tables_7_3.jpg", "caption": "Table 4: Zero-shot Object-centric Change Detection. All results of introducing semantics from the point query are accurate estimations since the detected changes are object-centric.", "description": "This table presents the results of zero-shot object-centric change detection experiments. It compares the performance of the AnyChange model with and without the point query mechanism. The metrics used are F1 score, precision, and recall for LEVIR-CD, S2Looking (binary), and xView2 (binary) datasets.  The improvement in performance is shown when using one and three points as queries for object-centric change detection. The results show a significant gain in F1 score with the point query, but a trade-off between precision and recall when using a single point. Adding more points improves the recall and maintains the precision improvement.", "section": "4.2 Zero-shot Object-centric Change Detection"}, {"figure_path": "D7X9Grmd7L/tables/tables_8_1.jpg", "caption": "Table 5: Supervised Object Change Detection. Comparison with the state-of-the-art change detectors on the S2Looking test. \u201cR-18\u201d: ResNet-18. The amount of Flops was computed with a float32 tensor of shape [2,512,512,3] as input.", "description": "This table compares the performance of AnyChange with other state-of-the-art change detection methods on the S2Looking dataset for supervised object change detection.  It shows the F1 score, precision, recall, number of parameters, and floating point operations (FLOPs) for each method, along with the backbone network used (ResNet-18 or MiT-B1) and the amount of labeled pixels used for fine-tuning (100%, 1%, or 0.1%). AnyChange achieves comparable performance to models trained on 100% of the labeled data, while only using a very small fraction (3500 pixels).", "section": "4.3 Zero-shot Object Change Proposals"}, {"figure_path": "D7X9Grmd7L/tables/tables_9_1.jpg", "caption": "Table 6: Unsupervised Change Detection. Comparison with the state-of-the-art unsupervised change detectors on the SECOND test. \u201c*\u201d indicates this change detection model is trained with pseudo labels predicted by AnyChange.", "description": "This table compares the performance of AnyChange with other state-of-the-art unsupervised change detection methods on the SECOND dataset.  It shows the F1 score, precision, and recall for each method, highlighting AnyChange's superior performance, especially when combined with ChangeStar.  The table demonstrates AnyChange's effectiveness as a zero-shot approach and also showcases the performance boost achievable by leveraging pseudo-labels generated by AnyChange for supervised training.", "section": "4.3 AnyChange as Change Data Engine"}]