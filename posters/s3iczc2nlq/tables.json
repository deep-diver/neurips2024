[{"figure_path": "s3icZC2NLq/tables/tables_2_1.jpg", "caption": "Table 1: A comparison of existing algorithms in terms of regret and switching cost for linear MDP and general function class with bounded eluder dimension and Bellman completeness. The results hold for in-homogeneous episodic RL with horizon length H, number of episodes K where the total reward obtained in an episode is not larger than 1. For regret, we only present the leading term when K is large enough compared to other variables and hide poly-logarithmic factors in K, d or dim, H and the constant. For linear MDPs, d is the dimension of the feature vectors. For general function class, dim is a shorthand of the eluder dimension of the underlying function class, N is the covering number of the value function class, and Ns, A is the covering number of the state-action space.", "description": "This table compares various reinforcement learning algorithms across several key metrics: regret (the difference between the cumulative reward of the optimal policy and the algorithm's policy), switching cost (how often the algorithm changes its policy), and the type of model class they are applicable to (linear MDPs or general function classes with bounded eluder dimension and Bellman completeness). The regret bounds presented are simplified, showing only the leading terms and ignoring polylogarithmic factors.  For general function classes, the table uses the eluder dimension (dim(F)), covering numbers (N, Ns,A) and other parameters to quantify the regret and switching cost.", "section": "3 Algorithm and Key Techniques"}]