{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-06", "reason": "This paper introduced the Transformer architecture, a fundamental building block for many modern sequence-to-sequence models, including those used in time series forecasting."}, {"fullname_first_author": "Haoyi Zhou", "paper_title": "Informer: Beyond efficient transformer for long sequence time-series forecasting", "publication_date": "2021-01-01", "reason": "This paper proposed the Informer model, a highly efficient Transformer-based model specifically designed for long sequence time series forecasting, addressing a key challenge in the field."}, {"fullname_first_author": "Haixu Wu", "paper_title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting", "publication_date": "2021-01-01", "reason": "This paper introduced Autoformer, another efficient Transformer-based architecture optimized for long-term time series forecasting by effectively using autocorrelation."}, {"fullname_first_author": "Minhao Liu", "paper_title": "Scinet: Time series modeling and forecasting with sample convolution and interaction", "publication_date": "2022-01-01", "reason": "This paper introduced the SCINet model, a CNN-based approach that effectively captures both temporal and spatial dependencies in time series data, offering an alternative to Transformer-based methods."}, {"fullname_first_author": "Yong Liu", "paper_title": "iTransformer: Inverted transformers are effective for time series forecasting", "publication_date": "2023-10-26", "reason": "This paper presented iTransformer, a novel Transformer architecture tailored for time series forecasting, improving performance by using inverted attention mechanisms and other design choices."}]}