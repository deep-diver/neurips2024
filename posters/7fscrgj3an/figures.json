[{"figure_path": "7fScrgJ3An/figures/figures_1_1.jpg", "caption": "Figure 1: DistillNeRF is a generalizable model for 3D scene representation, self-supervised by natural sensor streams along with distillation from offline NeRFs and vision foundation models. It supports rendering RGB, depth, and foundation feature images, without test-time per-scene optimization, and enables zero-shot 3D semantic occupancy prediction and open-vocabulary text queries.", "description": "This figure is a schematic overview of the DistillNeRF model.  It shows the training and inference processes, highlighting the key components: the two-stage encoder and lifting process that converts 2D images into 3D representations, the sparse hierarchical voxel structure used for efficient scene representation, the distillation process that integrates information from pre-trained NeRFs and 2D foundation models, and the differentiable rendering process that generates RGB, depth, and feature images. The diagram also illustrates how these capabilities support downstream tasks like open-vocabulary text queries and zero-shot 3D semantic occupancy prediction.", "section": "1 Introduction"}, {"figure_path": "7fScrgJ3An/figures/figures_3_1.jpg", "caption": "Figure 2: DistillNeRF model architecture. (left) single-view encoding with two-stage probabilistic depth prediction; (center) multi-view pooling into a sparse hierarchical voxel representation using sparse quantization and convolution; (right) volumetric rendering from sparse hierarchical voxels.", "description": "This figure illustrates the DistillNeRF model architecture, broken down into three stages: single-view encoding, multi-view pooling, and rendering. The single-view encoding stage uses a two-stage probabilistic depth prediction to lift 2D image features into 3D.  The multi-view pooling stage then combines these 3D features into a sparse hierarchical voxel representation using sparse quantization and convolution. Finally, the rendering stage uses the sparse hierarchical voxels to generate RGB images, depth maps, and feature images.  The figure shows the flow of data and the key components of each stage.", "section": "3 Method"}, {"figure_path": "7fScrgJ3An/figures/figures_4_1.jpg", "caption": "Figure 3: DistillNeRF Capabilities - Given single-frame multi-view cameras as input and without test-time per-scene optimization, DistillNeRF can reconstruct RGB images (row 2), estimate depth (row 3), render foundation model features (rows 4, 5) which enables open-vocabulary text queries (rows 6, 7, 8), and predict binary and semantic occupancy in zero shot (rows 9, 10).", "description": "This figure demonstrates the capabilities of the DistillNeRF model.  Given a single frame of multi-view camera images, without any per-scene optimization at test time, the model can perform several tasks. These include reconstructing RGB images, estimating depth maps, rendering features from foundation models (CLIP and DINOv2), enabling open vocabulary text queries, and predicting both binary and semantic occupancy maps (zero-shot). Each row shows a different capability of the model.", "section": "3. Experiments"}, {"figure_path": "7fScrgJ3An/figures/figures_5_1.jpg", "caption": "Figure 4: DistillNeRF Generalizability - Trained on the nuScenes dataset, our model demonstrates strong zero-shot transfer performance on the unseen Waymo NOTR dataset, achieving decent reconstruction quality (row 2). This quality can be further enhanced by applying simple color alterations to account for camera-specific coloring discrepancies (row 3). After fine-tuning (row 4), our model surpasses the offline per-scene optimized EmerNeRF, achieving higher PSNR (29.84 vs. 28.87) and SSIM (0.911 vs. 0.814). See Tab 3 for quantitative results.", "description": "This figure demonstrates the generalizability of the DistillNeRF model.  The model, trained on the nuScenes dataset, is tested on the unseen Waymo NOTR dataset.  The results show zero-shot transfer capabilities with decent reconstruction quality.  Applying simple color adjustments improves the results further.  Finally, fine-tuning surpasses the performance of the offline, per-scene optimized EmerNeRF model, as measured by PSNR and SSIM.", "section": "Experiments"}, {"figure_path": "7fScrgJ3An/figures/figures_14_1.jpg", "caption": "Figure 5: Scene-level reconstruction in autonomous driving poses different challenges compared to object-level reconstruction. 1) Typical object-level indoor NeRF involves an \"inward\" multi-view setup, where numerous cameras are positioned around the object from various angles. This setup creates extensive view overlap and simplifies geometry learning. In contrast, the outdoor driving task uses an \"outward\" sparse-view setup, with only 6 cameras facing different directions from the car. The limited overlap between cameras significantly aggravates the ambiguity in depth/geometry learning. 2) In the images captured from unbounded driving scenes, nearby objects occupy significantly more pixels than those far away, even if their physical sizes are identical. This introduces the difficulty in processing and coordinating distant/nearby objects.", "description": "This figure compares object-level reconstruction (left) with scene-level reconstruction in autonomous driving (right).  Object-level reconstruction uses many cameras with overlapping views of a single object, simplifying the task.  Autonomous driving, however, uses sparsely distributed cameras with limited overlap, creating challenges for depth estimation and handling of objects at varying distances.", "section": "A.1 Discussion on Challenges in Driving Scenes"}, {"figure_path": "7fScrgJ3An/figures/figures_19_1.jpg", "caption": "Figure 6: Qualitative comparison on RGB image and depth image reconstruction. Our generalizable DistillNeRF is on par with SOTA offline per-scene optimized NeRF method (EmerNeRF), and significantly outperforms SOTA generalizable methods (UniPAD and SelfOcc).", "description": "This figure compares the RGB and depth image reconstruction results of DistillNeRF against several state-of-the-art methods, including EmerNeRF, UniPAD, and SelfOcc.  It visually demonstrates that DistillNeRF achieves comparable performance to the per-scene optimized EmerNeRF method while significantly outperforming the generalizable methods, UniPAD and SelfOcc, in terms of both RGB image and depth reconstruction quality.", "section": "4.1 Rendering Images and Foundation Features"}, {"figure_path": "7fScrgJ3An/figures/figures_20_1.jpg", "caption": "Figure 7: Qualitative ablation studies of our model on the RGB image and depth image reconstruction. Without depth distillation, we see inconsistent depth predictions between low and high regions of the image. Without parameterized space, the model can only predict depth in a limited depth range, while with parameterized space we can generate reasonable unbounded depth.", "description": "This figure shows the results of ablation studies on DistillNeRF, demonstrating the impact of key components on depth and image reconstruction.  The top row shows the ground truth (GT) images and depth maps.  Subsequent rows show results with different components removed, illustrating the effect of depth distillation and parameterized space on the quality and range of the depth prediction.  The results highlight the importance of both components for accurate, unbounded depth estimation.", "section": "4.2 Depth Estimation"}]