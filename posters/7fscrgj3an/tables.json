[{"figure_path": "7fScrgJ3An/tables/tables_7_1.jpg", "caption": "Table 1: Reconstruction and novel-view synthesis on nuScenes validation set. DistillNeRF is on par with the per-scene optimized NeRFs, both in RGB and foundation feature rendering, and significantly outperforms SOTA generalizable NeRF methods. In the DistillNeRF variants, we denote 'Depth' as the depth distillation from offline NeRFs, 'Param.' as the parameterized space, and 'Virt.' as the distillation from virtual cameras in offline NeRFs. See Fig. 6 and Fig. 7 for qualitative results.", "description": "This table presents a comparison of reconstruction and novel-view synthesis performance on the nuScenes validation set.  It compares DistillNeRF against several state-of-the-art (SOTA) methods, including a per-scene optimized NeRF (EmerNeRF) and two generalizable NeRF methods (SelfOcc and UniPad). The table shows that DistillNeRF achieves performance comparable to the per-scene optimized method, while significantly outperforming the generalizable methods. It also analyzes different DistillNeRF variants that employ depth distillation, parameterized space, and virtual camera distillation, demonstrating their impact on performance.", "section": "4.1 Rendering Images and Foundation Features"}, {"figure_path": "7fScrgJ3An/tables/tables_7_2.jpg", "caption": "Table 2: Depth estimation results on the nuScenes validation set. Depth targets are defined by (a) sparse LiDAR scans or (b) dense depth images rendered from EmerNeRF. We use highlighting across comparable methods with rendering support and no test-time optimization. DistillNeRF outperforms comparable generalizable NeRF methods, especially on dense depth targets.", "description": "This table presents the depth estimation results on the nuScenes validation set, comparing DistillNeRF against other methods.  It shows the performance using two different depth targets: sparse LiDAR and dense depth images rendered from EmerNeRF.  The metrics used for comparison include Absolute Relative Error, Squared Relative Error, Root Mean Squared Error, RMSE log, and the percentage of points with depth error less than 1.25, 1.25^2, and 1.25^3.  The results highlight DistillNeRF's superior performance, particularly when using dense depth targets.", "section": "4.1 Rendering Images and Foundation Features"}, {"figure_path": "7fScrgJ3An/tables/tables_8_1.jpg", "caption": "Table 3: Trained on the nuScenes dataset, DistillNeRF shows strong generalizability to the unseen Waymo NOTR dataset.", "description": "This table presents the results of evaluating DistillNeRF's generalizability on the Waymo NOTR dataset after training it on the nuScenes dataset. It demonstrates the model's ability to perform zero-shot transfer.  The table shows that even without fine-tuning, DistillNeRF achieves reasonable performance (Zero-Shot Transfer). Applying a simple color correction further improves the results (Zero-Shot Transfer + Recolor).  Finally, fine-tuning on the Waymo NOTR dataset leads to performance surpassing that of the offline per-scene optimized EmerNeRF baseline.", "section": "4 Experiments"}, {"figure_path": "7fScrgJ3An/tables/tables_9_1.jpg", "caption": "Table 4: Unsupervised 3D occupancy prediction on the Occ3D-nuScenes [5] dataset. Our method learns meaningful geometry and reasonable semantics compared to alternative unsupervised methods. F-mIoU, mIoU and G-IoU denote the IoU for foreground-object classes, IoU for all classes, and geometric IoU ignoring the classes.", "description": "This table presents the results of unsupervised 3D occupancy prediction on the Occ3D-nuScenes dataset.  It compares the performance of DistillNeRF against several other unsupervised methods, evaluating the Intersection over Union (IoU) metric across various semantic classes (e.g., cars, pedestrians, bicycles).  The results are broken down into three IoU variations: F-mIoU (foreground classes), mIoU (all classes), and G-IoU (geometric IoU, ignoring classes). The table highlights DistillNeRF's ability to learn meaningful 3D geometry and semantics, even without explicit supervision.", "section": "4.3 3D Semantic Occupancy Prediction"}, {"figure_path": "7fScrgJ3An/tables/tables_15_1.jpg", "caption": "Table 5: Ablation studies on key components in our model.", "description": "This table presents the ablation study results for DistillNeRF, showing the impact of removing key components, such as density complement, decoder, pretrained 2D encoder, two-stage LSS, and two-depth distillation on the overall performance (PSNR and SSIM). The results demonstrate the importance of each component for achieving the best results.", "section": "4.1 Rendering Images and Foundation Features"}, {"figure_path": "7fScrgJ3An/tables/tables_16_1.jpg", "caption": "Table 1: Reconstruction and novel-view synthesis on nuScenes validation set. DistillNeRF is on par with the per-scene optimized NeRFs, both in RGB and foundation feature rendering, and significantly outperforms SOTA generalizable NeRF methods. In the DistillNeRF variants, we denote 'Depth' as the depth distillation from offline NeRFs, 'Param.' as the parameterized space, and 'Virt.' as the distillation from virtual cameras in offline NeRFs. See Fig. 6 and Fig. 7 for qualitative results.", "description": "This table presents a comparison of the reconstruction and novel-view synthesis performance on the nuScenes validation set, comparing DistillNeRF with various other methods, including per-scene optimized NeRFs and other state-of-the-art generalizable NeRF methods.  The results demonstrate that DistillNeRF achieves comparable performance to per-scene optimized NeRFs while significantly outperforming other generalizable methods, highlighting its effectiveness. Different variants of DistillNeRF are evaluated to analyze the impact of specific components (depth distillation, parameterized space, virtual camera distillation).", "section": "4.1 Rendering Images and Foundation Features"}, {"figure_path": "7fScrgJ3An/tables/tables_18_1.jpg", "caption": "Table 7: Inference time comparison with SOTA methods, and a breakdown on each component in our model.", "description": "This table compares the inference time of DistillNeRF with two state-of-the-art generalizable NeRF methods (SelfOcc and UniPAD).  It also breaks down the inference time of DistillNeRF into its main components: encoder, single-view encoding, multi-view fusion, voxel convolution, renderer, projection + ray march, and decoder. The PSNR for reconstruction is also provided for SelfOcc, UniPAD, and DistillNeRF, demonstrating that DistillNeRF achieves superior reconstruction quality while maintaining a reasonable inference time.", "section": "4.1 Rendering Images and Foundation Features"}]