[{"figure_path": "haUnEiXgQ7/tables/tables_6_1.jpg", "caption": "Table 1: On each dataset, we compare the Precision (%) and Recall (%) of DEFT with CLIP label-match and small-loss to evaluate the clean sample selection performance. \u0394 is the difference between the performance of DEFT and small-loss.", "description": "This table presents the performance comparison of three methods (DEFT, CLIP label-match, and small-loss) for clean sample selection on various datasets.  Precision and recall are used as evaluation metrics, showing the effectiveness of DEFT in identifying clean samples.  The \u0394 column highlights the performance improvement of DEFT compared to the small-loss method.", "section": "5.2 Performance for Noisy Label Detection"}, {"figure_path": "haUnEiXgQ7/tables/tables_7_1.jpg", "caption": "Table 2: Test accuracy (%) on synthetic datasets with symmetric and instance-dependent label noise.", "description": "This table presents the results of image classification experiments conducted on four synthetic datasets (CIFAR-100, Tiny-ImageNet, Stanford-Cars, CUB-200-2011) with varying levels of symmetric and instance-dependent label noise.  The results are broken down by noise type and ratio, showing the performance of different methods including CE (Cross-Entropy loss), ELR (Early-Learning Regularization), SCE (Symmetric Cross-Entropy loss), GMM (Gaussian Mixture Model), and DEFT (Denoising Fine-Tuning).  The table allows for a comparison of the effectiveness of these different methods in handling various types and levels of noisy labels.", "section": "5.2 Performance for Noisy Label Detection"}, {"figure_path": "haUnEiXgQ7/tables/tables_7_2.jpg", "caption": "Table 3: Test accuracy (%) on datasets with real-world label noise.", "description": "This table presents the test accuracy achieved by different methods (CE, ELR, SCE, GMM, RoLT, UNICON, LongReMix, ProMix, and DEFT) on three real-world datasets with noisy labels: CIFAR-100N, Clothing1M, and WebVision.  The results show the effectiveness of each method in handling real-world label noise and highlight DEFT's superior performance.", "section": "5.1 Experimental Settings"}, {"figure_path": "haUnEiXgQ7/tables/tables_8_1.jpg", "caption": "Table 4: Test accuracy (%) using various pre-trained models on Clothing1M. Partial results are sourced from [1]. The best results across all methods are highlighted in bold, with the second-best results indicated by underscores.", "description": "This table shows the test accuracy achieved by using different pre-trained models on the Clothing1M dataset.  The models tested include ResNet-50, MAE-ViT-B, ViT-B/16, and ConvNeXt-T.  The accuracy is reported for each model using cross-entropy loss (CE), generalized cross-entropy loss (GCE), early learning regularization (ELR), TURN, and the proposed DEFT method. The best performing model for each architecture is highlighted in bold, and the second-best is underlined.", "section": "5.4 Further Analyses"}, {"figure_path": "haUnEiXgQ7/tables/tables_13_1.jpg", "caption": "Table 1: On each dataset, we compare the Precision (%) and Recall (%) of DEFT with CLIP label-match and small-loss to evaluate the clean sample selection performance. \u0394 is the difference between the performance of DEFT and small-loss.", "description": "This table compares the performance of three methods (DEFT, CLIP label-match, and small-loss) in terms of precision and recall for clean sample selection on seven different datasets (CIFAR-100, Tiny-ImageNet, Stanford Cars, CUB-200-2011, with symmetric and instance-dependent noise at different ratios).  The \u0394 column shows the improvement of DEFT over the small-loss method.", "section": "5.2 Performance for Noisy Label Detection"}, {"figure_path": "haUnEiXgQ7/tables/tables_14_1.jpg", "caption": "Table 1: On each dataset, we compare the Precision (%) and Recall (%) of DEFT with CLIP label-match and small-loss to evaluate the clean sample selection performance. \u0394 is the difference between the performance of DEFT and small-loss.", "description": "This table compares the performance of three methods (DEFT, CLIP label-match, and small-loss) in terms of precision and recall for clean sample selection on seven datasets (CIFAR-100, Tiny-ImageNet, Stanford Cars, CUB-200-2011, with symmetric noise at 0.2, 0.4, and 0.6 ratios and instance-dependent noise at 0.2, 0.3, and 0.4 ratios). It demonstrates DEFT's superior performance in identifying clean samples compared to the other methods.", "section": "5.2 Performance for Noisy Label Detection"}, {"figure_path": "haUnEiXgQ7/tables/tables_14_2.jpg", "caption": "Table 2: Test accuracy (%) on synthetic datasets with symmetric and instance-dependent label noise.", "description": "This table presents the test accuracy achieved by different methods on four synthetic datasets with varying levels of symmetric and instance-dependent label noise.  The methods compared include Cross-Entropy (CE), Early-Learning Regularization (ELR), Symmetric Cross-Entropy (SCE), Gaussian Mixture Model (GMM), and the proposed DEFT method.  The results are shown for different noise ratios (0.2, 0.4, 0.6 for symmetric noise and 0.2, 0.3, 0.4 for instance-dependent noise).  The table allows for a comparison of the robustness of different methods against various types and intensities of label noise across different datasets.", "section": "5. Experimental Settings"}, {"figure_path": "haUnEiXgQ7/tables/tables_14_3.jpg", "caption": "Table 8: Noise detection results on CIFAR-100N.", "description": "This table presents the performance of different noisy label detection methods on the CIFAR-100N dataset.  The methods compared include Label-match, Small-loss, GMM, RoLT, UNICON, LongReMix, ProMix, and DEFT (the authors' method). The table shows the precision, recall, and F1-score for each method, providing a comprehensive comparison of their performance in identifying noisy labels in a real-world dataset.  DEFT achieves the highest F1-score, demonstrating its effectiveness in noisy label detection.", "section": "5.2 Performance for Noisy Label Detection"}, {"figure_path": "haUnEiXgQ7/tables/tables_14_4.jpg", "caption": "Table 1: On each dataset, we compare the Precision (%) and Recall (%) of DEFT with CLIP label-match and small-loss to evaluate the clean sample selection performance. \u0394 is the difference between the performance of DEFT and small-loss.", "description": "This table presents the performance comparison of three methods for clean sample selection: DEFT, CLIP label-match, and small-loss.  The comparison is done across multiple datasets (CIFAR-100, Tiny-ImageNet, Stanford Cars, CUB-200-2011) with varying levels of symmetric and instance-dependent noise.  Precision and Recall are reported for each method on each dataset, showing DEFT's superiority in selecting clean samples and highlighting the improvement achieved over the small-loss baseline. ", "section": "5.2 Performance for Noisy Label Detection"}, {"figure_path": "haUnEiXgQ7/tables/tables_15_1.jpg", "caption": "Table 1: On each dataset, we compare the Precision (%) and Recall (%) of DEFT with CLIP label-match and small-loss to evaluate the clean sample selection performance. \u0394 is the difference between the performance of DEFT and small-loss.", "description": "This table compares the precision and recall of three methods (DEFT, CLIP label-match, and small-loss) for selecting clean samples from noisy datasets.  The results are shown for various datasets with different levels of symmetric and instance-dependent noise.  The \u0394 column shows the difference in performance between DEFT and the small-loss method.", "section": "5.2 Performance for Noisy Label Detection"}, {"figure_path": "haUnEiXgQ7/tables/tables_16_1.jpg", "caption": "Table 11: Configurations of different pre-trained models.", "description": "This table shows the hyperparameters used for training different pre-trained models in the DEFT framework.  These hyperparameters include the optimizer used (SGD or AdamW), the learning rate, and the weight decay.  The table lists the specific values used for each hyperparameter for four different models: ViT-B/16, ResNet-50, ConvNeXt-T, and MAE-ViT-B.", "section": "A.3 Additional Implementation Details"}]