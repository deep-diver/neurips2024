[{"figure_path": "haUnEiXgQ7/figures/figures_3_1.jpg", "caption": "Figure 1: Comparison of different fine-tuning methods under (a) various ratios of noisy labels and (b) clean datasets.", "description": "This figure compares the performance of three different fine-tuning methods for CLIP models: FFT (full fine-tuning), VPT (visual prompt tuning), and VLPT (vision-language prompt tuning).  The comparison is done across various datasets with different noise ratios in (a) and clean datasets in (b).  The results show that VPT is the most robust method in the presence of noisy labels, while FFT performs best on clean datasets. VLPT shows a good performance with noisy data but is inferior to VPT and FFT on clean datasets.", "section": "3 Preliminary and Initial Findings"}, {"figure_path": "haUnEiXgQ7/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of the proposed DEFT framework. Left: We identify noisy labels with learnable dual textual prompts and improve image-text alignment by optimizing PEFT modules. Right: Adapt pre-trained models using FFT on selected clean samples.", "description": "The figure illustrates the DEFT framework, which consists of two phases. In the first phase (noisy label detection), dual textual prompts (positive and negative) are learned to identify noisy labels by comparing the similarity between image and text embeddings. The visual encoder is adapted using parameter-efficient fine-tuning (PEFT) methods to improve image-text alignment.  In the second phase (model adaptation), the pre-trained model is further adapted using full fine-tuning (FFT) on the selected clean samples to boost visual recognition performance.  The left side shows the noisy label detection phase, while the right side illustrates the model adaptation phase using the clean subset of the data.  Arrows indicate data flow and the different network components (learnable, frozen).", "section": "4 The Denoising Fine-tuning Framework"}, {"figure_path": "haUnEiXgQ7/figures/figures_8_1.jpg", "caption": "Figure 3: Ablation studies. We report the test accuracy across varying noise ratios for the following variants: 1) w/o adap.: DEFT without the model adaptation phase, 2) PEFT: use PEFT for model adaptation phase, and 3) FFT: use FFT for model adaptation phase.", "description": "This figure presents the results of ablation studies conducted to evaluate the impact of different model adaptation techniques on the performance of DEFT.  Four datasets (CIFAR-100, Tiny-ImageNet, Stanford-Cars, CUB-200-2011) were used, each with varying levels of synthetic label noise. The bars show the test accuracy achieved under three conditions: \n1. DEFT without the model adaptation phase (w/o adap.)\n2. DEFT using Parameter-Efficient Fine-Tuning (PEFT) for the adaptation phase.\n3. DEFT using Full Fine-Tuning (FFT) for the adaptation phase.\nThe results demonstrate the effectiveness of the model adaptation phase and show that FFT generally outperforms PEFT when the data is clean but can be negatively affected by noise.", "section": "5.2 Performance for Noisy Label Detection"}, {"figure_path": "haUnEiXgQ7/figures/figures_15_1.jpg", "caption": "Figure 3: Ablation studies. We report the test accuracy across varying noise ratios for the following variants: 1) w/o adap.: DEFT without the model adaptation phase, 2) PEFT: use PEFT for model adaptation phase, and 3) FFT: use FFT for model adaptation phase.", "description": "This figure shows the results of ablation studies conducted to evaluate the impact of different components of the DEFT framework on its performance.  Four different scenarios are compared:  DEFT without model adaptation, DEFT using parameter-efficient fine-tuning (PEFT), DEFT using full fine-tuning (FFT), and a baseline without any adaptation.  The results are shown across different levels of noise in the training data, indicating how well each version of DEFT handles varying amounts of noisy labels during model training. The x-axis represents the noise ratio (from 0 to 0.8), and the y-axis shows the test accuracy (percentage). Each subplot displays the accuracy for one of the four scenarios on a specific dataset (CIFAR-100, Tiny-ImageNet, Stanford-Cars, CUB-200-2011). This allows a visual comparison of the effects of different adaptation strategies on various datasets.", "section": "5 Experiment"}]