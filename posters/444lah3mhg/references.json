{"references": [{"fullname_first_author": "M. Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-10-27", "reason": "This paper introduces the DINO framework, which is used in the current paper for unsupervised pre-training of the DeiT-Small model, a crucial step in the BiLAF framework."}, {"fullname_first_author": "K. He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2021-11-06", "reason": "This paper provides a strong foundation for the use of masked autoencoders in the field of self-supervised learning, which indirectly supports BiLAF's approach to feature representation learning in the absence of labels."}, {"fullname_first_author": "O. Sener", "paper_title": "Active learning for convolutional neural networks: A core-set approach", "publication_date": "2017-08-01", "reason": "This paper introduces the core-set approach for active learning, which BiLAF adapts for its core sample selection stage, a fundamental component of the BiLAF framework."}, {"fullname_first_author": "S. Sinha", "paper_title": "Variational adversarial active learning", "publication_date": "2019-10-27", "reason": "This paper provides the VAAL framework which is one of the baselines used for comparison with BiLAF, providing insights into the performance of different active learning strategies in the context of image classification."}, {"fullname_first_author": "Y. Xie", "paper_title": "Active finetuning: Exploiting annotation budget in the pretraining-finetuning paradigm", "publication_date": "2023-06-20", "reason": "This paper introduces the concept of Active Finetuning and proposes the ActiveFT method, which is directly compared with BiLAF in the experiments and used as a core component within BiLAF."}]}