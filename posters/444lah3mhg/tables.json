[{"figure_path": "444LAH3MhG/tables/tables_6_1.jpg", "caption": "Table 1: Benchmark Results. Experiments are conducted on three popular datasets with different annotation ratios. We report the mean and standard deviation over three trials. Traditional active learning methods require random initial data to start, thus we use \"-\" to represent. BiLAF has shown a significant competitive advantage across the majority of scenarios, affirming its effectiveness.", "description": "This table presents the benchmark results of the BiLAF method and several other baselines on three popular datasets (CIFAR10, CIFAR100, and ImageNet) with varying annotation ratios (1%, 2%, 5%, and 10%).  The results demonstrate BiLAF's superior performance across different scenarios, highlighting its effectiveness compared to traditional active learning methods and other state-of-the-art active finetuning techniques.", "section": "4.1 Experiment Setup"}, {"figure_path": "444LAH3MhG/tables/tables_7_1.jpg", "caption": "Table 4: Ablation on CIFAR100. In the denoising process, \u201cDG\u201d, \u201cDB\u201d and \u201cIDC\u201d indicate the basic distance-guide method, density-based method and iterative density-based clustering method. In the selection criterion, \u201cBD\u201d, \u201cBS\u201d indicate the basic distance metric and the boundary score metric. In the selection process, \u201cOS\u201d, \u201cISR\u201d, \u201cOP\u201d indicate selecting the top samples in one shot, iterative selection and removal and whether to use opponent penalty. BiLAF represents the complete implementation and we explore the influence of three designs separately.", "description": "This ablation study on CIFAR100 dataset investigates the impact of three key components of the BiLAF framework: the denoising process, the selection criterion, and the selection process.  It compares the performance of BiLAF against variants where one or more of these components are modified or removed, providing insights into the contribution of each component to the overall accuracy.  The table displays the results for different annotation budgets (1%, 2%, 5%, 10%).", "section": "4.4 Ablation Study"}, {"figure_path": "444LAH3MhG/tables/tables_7_2.jpg", "caption": "Table 3: Time complexity. Our method exhibits significant advantages beyond conventional active learning approaches and comparable speed to ActiveFT.", "description": "This table compares the time complexity of different active learning methods, including the proposed BiLAF method, for selecting varying proportions of data from the CIFAR100 dataset.  The results highlight BiLAF's efficiency compared to other methods, demonstrating its advantage in terms of time taken for data selection.", "section": "4.3 Analysis"}, {"figure_path": "444LAH3MhG/tables/tables_8_1.jpg", "caption": "Table 5: Ablation for Core Samples Numbers.", "description": "This table shows the ablation study on the number of core samples used in the BiLAF method.  It demonstrates the impact of varying the number of core samples on the accuracy across different annotation budgets, specifically in the CIFAR100 dataset. The results illustrate that insufficient core samples lead to suboptimal performance due to inadequate representation of all categories, whereas an excessive number does not result in significant performance gains, indicating an optimal ratio of core samples to boundary samples exists.", "section": "4.4 Ablation Study"}, {"figure_path": "444LAH3MhG/tables/tables_9_1.jpg", "caption": "Table 7: Threshold of Core Numbers. Distance and Rate of Return on CIFAR10 and CIFAR100.", "description": "This table presents an ablation study on the impact of varying the number of core samples on the performance of the BiLAF method. It shows the average Euclidean distance from each sample to its nearest selected sample (Distance) and the rate of return (incremental benefit per core sample) for different numbers of core samples on CIFAR10 and CIFAR100 datasets. The rate of return decreases as the number of core samples increases, indicating that the benefits of adding more core samples diminish after a certain point.  The table helps to determine an optimal threshold for the number of core samples.", "section": "4.4 Ablation Study"}, {"figure_path": "444LAH3MhG/tables/tables_9_2.jpg", "caption": "Table 8: Performance on Different Pretraining Frameworks and Models on CIFAR10.", "description": "This table compares the performance of different active learning methods on CIFAR10 using two different pre-trained models (DeiT-S with iBOT and ResNet50 with DINO) with varying annotation budgets (1% and 2%).  It demonstrates the generalizability and robustness of the proposed BiLAF method across different model architectures and pre-training frameworks.", "section": "4.2 Overall Peformance Comparison"}, {"figure_path": "444LAH3MhG/tables/tables_17_1.jpg", "caption": "Table 9: CUB-200-2011 Dataset. Comparison of Random, ActiveFT, and BiLAF on Different Selection Budget.", "description": "This table presents the performance comparison of three different sample selection methods (Random, ActiveFT, and BiLAF) on the CUB-200-2011 dataset for fine-grained image classification.  It shows the Top-1 accuracy achieved by each method under varying annotation budgets (20%, 30%, 40%, and 50%).  The results demonstrate the superior performance of BiLAF in selecting informative samples, leading to higher accuracy compared to the baselines, especially as the budget increases.", "section": "F.4 Fine-Grained Classification"}, {"figure_path": "444LAH3MhG/tables/tables_18_1.jpg", "caption": "Table 10: Results on CIFAR10 using KNN, Linear Probing, and Full Fine-Tuning across different budgets.", "description": "This table presents the performance comparison of three different finetuning methods (K-Nearest Neighbors, Linear Probing, and Full Fine-Tuning) using three different sample selection methods (Random, ActiveFT, and BiLAF) on the CIFAR10 dataset.  The results are shown for various annotation budgets (B = 0.5%, 1%, 2%, 5%).  It demonstrates how the choice of finetuning method and sample selection strategy impacts the final accuracy of the model, particularly at different annotation budget scales.", "section": "G Extra Finetuning Methods: Linear Probing and K-Nearest Neighbors"}, {"figure_path": "444LAH3MhG/tables/tables_19_1.jpg", "caption": "Table 1: Benchmark Results. Experiments are conducted on three popular datasets with different annotation ratios. We report the mean and standard deviation over three trials. Traditional active learning methods require random initial data to start, thus we use \"-\" to represent. BiLAF has shown a significant competitive advantage across the majority of scenarios, affirming its effectiveness.", "description": "This table presents a comparison of the BiLAF model's performance against several baselines and existing active learning methods on three widely-used image classification datasets (CIFAR10, CIFAR100, and ImageNet).  The results are shown for different annotation ratios (1%, 2%, 5%, and 10%), demonstrating the model's performance under varying data constraints. The table highlights BiLAF's consistent superior performance compared to alternative methods, particularly when the annotation budget is limited.", "section": "4.1 Experiment Setup"}, {"figure_path": "444LAH3MhG/tables/tables_19_2.jpg", "caption": "Table 1: Benchmark Results. Experiments are conducted on three popular datasets with different annotation ratios. We report the mean and standard deviation over three trials. Traditional active learning methods require random initial data to start, thus we use \"-\" to represent. BiLAF has shown a significant competitive advantage across the majority of scenarios, affirming its effectiveness.", "description": "This table presents the benchmark results of the BiLAF method compared to various baselines on three popular datasets (CIFAR10, CIFAR100, and ImageNet) with different annotation ratios.  The results, averaged over three trials, show BiLAF's superior performance in most scenarios, highlighting its effectiveness in the active finetuning task.", "section": "4.1 Experiment Setup"}, {"figure_path": "444LAH3MhG/tables/tables_19_3.jpg", "caption": "Table 1: Benchmark Results. Experiments are conducted on three popular datasets with different annotation ratios. We report the mean and standard deviation over three trials. Traditional active learning methods require random initial data to start, thus we use \"-\" to represent. BiLAF has shown a significant competitive advantage across the majority of scenarios, affirming its effectiveness.", "description": "This table presents a comparison of the BiLAF method's performance against various baselines and traditional active learning methods across three datasets (CIFAR10, CIFAR100, and ImageNet) under different annotation ratios (1%, 2%, and 5%). The results highlight BiLAF's superior performance in most scenarios.", "section": "4.1 Experiment Setup"}, {"figure_path": "444LAH3MhG/tables/tables_19_4.jpg", "caption": "Table 1: Benchmark Results. Experiments are conducted on three popular datasets with different annotation ratios. We report the mean and standard deviation over three trials. Traditional active learning methods require random initial data to start, thus we use \"-\" to represent. BiLAF has shown a significant competitive advantage across the majority of scenarios, affirming its effectiveness.", "description": "This table presents the benchmark results of the BiLAF method and several baselines on three popular image classification datasets (CIFAR10, CIFAR100, and ImageNet) using different annotation ratios (1%, 2%, 5%, and 10%).  For each dataset and annotation ratio, the table shows the mean and standard deviation of the Top-1 accuracy achieved by each method over three independent trials.  The results demonstrate BiLAF's superior performance across various settings.", "section": "4.1 Experiment Setup"}]