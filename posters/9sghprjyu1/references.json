{"references": [{"fullname_first_author": "Ying Jin", "paper_title": "Is pessimism provably efficient for offline rl?", "publication_date": "2021-07-11", "reason": "This paper is foundational for the study of pessimism in offline RL, a key concept for distributionally robust offline RL."}, {"fullname_first_author": "Wei Xiong", "paper_title": "Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game", "publication_date": "2023-00-00", "reason": "This paper provides state-of-the-art results on offline RL with linear function approximation, establishing a benchmark for comparison in the distributionally robust setting."}, {"fullname_first_author": "Jose Blanchet", "paper_title": "Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage", "publication_date": "2023-05-22", "reason": "This paper introduces the concept of double pessimism, a crucial idea for addressing challenges in robust offline RL, that the current paper builds upon."}, {"fullname_first_author": "Laixi Shi", "paper_title": "Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity", "publication_date": "2024-00-00", "reason": "This paper addresses the model-based offline RL setting in a distributionally robust way, providing insights relevant to the model-free setting that the current paper focuses on."}, {"fullname_first_author": "Aviv Tamar", "paper_title": "Scaling up robust mdps using function approximation", "publication_date": "2014-00-00", "reason": "This paper is a foundational work on the use of function approximation in robust RL, which the current paper extends to the offline setting."}]}