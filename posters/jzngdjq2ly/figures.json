[{"figure_path": "jzngdJQ2lY/figures/figures_5_1.jpg", "caption": "Figure 1: Summary of the RC-PPO algorithm. In phase one, the original dynamic system is transformed into the augmented dynamic system defined in (7). Then RL is used to optimize value function V and learn a stochastic policy \u03c0. In phase two, we fine-tune V on a deterministic version of \u03c0 and compute the optimal upper-bound z* to obtain the optimal deterministic policy \u03c0*. ", "description": "This figure summarizes the two phases of the RC-PPO algorithm.  Phase one transforms the original system into an augmented system using RL to optimize the value function and learn a stochastic policy. Phase two fine-tunes the value function on a deterministic policy, calculates the optimal upper bound, and then produces the optimal deterministic policy. ", "section": "4 Solving with Reinforcement Learning"}, {"figure_path": "jzngdJQ2lY/figures/figures_7_1.jpg", "caption": "Figure 2: Illustrations of the benchmark tasks. In each picture, red denotes the unsafe region to be avoided, while green denotes the goal region to be reached.", "description": "This figure shows six different benchmark tasks used to evaluate the performance of the proposed RC-PPO algorithm. Each task involves reaching a green goal region while avoiding a red unsafe region.  The tasks vary in complexity and dynamics, representing a diverse set of challenges for minimum-cost reach-avoid problems.", "section": "5 Experiments"}, {"figure_path": "jzngdJQ2lY/figures/figures_7_2.jpg", "caption": "Figure 3: Reach rates under the sparse reward setting. RC-PPO consistently achieves the highest reach rates in all benchmark tasks. Error bars denote the standard error.", "description": "This figure displays the reach rates achieved by RC-PPO and several baseline reinforcement learning algorithms across six different benchmark tasks under a sparse reward setting.  The results clearly show that RC-PPO consistently outperforms all other algorithms, achieving the highest reach rate in every task.  Error bars indicate the standard error of the mean, providing a measure of the uncertainty associated with the reach rate measurements.", "section": "5 Experiments"}, {"figure_path": "jzngdJQ2lY/figures/figures_8_1.jpg", "caption": "Figure 7: Cumulative cost and reach rates of the final converged policies.", "description": "This figure compares the performance of different reinforcement learning algorithms on two tasks, FixedWing and Safety Hopper, in terms of both cumulative cost and reach rate.  RC-PPO consistently achieves significantly lower cumulative costs while maintaining comparable reach rates to other methods, demonstrating its efficiency and effectiveness in solving minimum-cost reach-avoid problems.", "section": "H.1 Additional Cumulative Cost and Reach Rates"}, {"figure_path": "jzngdJQ2lY/figures/figures_8_2.jpg", "caption": "Figure 5: Trajectory comparisons. On Pendulum, RC-PPO learns to perform an extensive energy pumping strategy to reach the goal upright position (green line), resulting in vastly lower cumulative energy. On WindField, RC-PPO takes advantage instead of fighting against the wind field, resulting in a faster trajectory to the goal region (green box) that uses lower cumulative energy. The start of the trajectory is marked by", "description": "This figure compares the trajectories of RC-PPO and other baselines on the Pendulum and WindField environments. In Pendulum, RC-PPO uses energy pumping to reach the goal more slowly but with lower cumulative cost. In WindField, RC-PPO leverages the wind to reach the goal faster and with less energy consumption.", "section": "5.2 Comparison under Reward Shaping"}, {"figure_path": "jzngdJQ2lY/figures/figures_9_1.jpg", "caption": "Figure 6: Pareto front of PPO across different reward coefficients. RC-PPO outperforms the entire Pareto front of what can be achieved by varying the reward function coefficients of the surrogate CMDP problem when solved using PPO.", "description": "This figure shows the Pareto front achieved by varying the reward function coefficients of the surrogate CMDP problem solved using PPO.  The Pareto front represents the tradeoff between reach rate and additional cumulative cost.  RC-PPO's performance is plotted as a single point, demonstrating that it outperforms all points on the Pareto front, achieving both a high reach rate and a low cumulative cost.", "section": "5.4 Robustness to Noise"}, {"figure_path": "jzngdJQ2lY/figures/figures_23_1.jpg", "caption": "Figure 7: Cumulative cost and reach rates of the final converged policies.", "description": "This figure compares the performance of different reinforcement learning algorithms on two benchmark tasks, FixedWing and Safety Hopper.  The left half of the figure shows the cumulative cost, while the right half shows the reach rate. RC-PPO consistently outperforms other algorithms across both metrics.  The chart presents a visual comparison of the algorithm performances, highlighting the superiority of RC-PPO in minimizing cumulative cost while maintaining a high reach rate.", "section": "Additional Experiment Results"}, {"figure_path": "jzngdJQ2lY/figures/figures_23_2.jpg", "caption": "Figure 8: Learned RC-PPO policy for different z on Pendulum. For a smaller cost lower-bound z, cost minimization is prioritized at the expense of not reaching the goal. For a larger cost lower-bound z, the goal is reached using a large cumulative cost. Performing rootfinding to solve for the optimal zopt automatically finds the policy that minimizes cumulative costs while still reaching the goal.", "description": "This figure visualizes the learned RC-PPO policy for different cost upper bounds (z) on the Pendulum environment.  It shows how the policy changes depending on the value of z. For low z values, the policy prioritizes cost minimization, even at the cost of not reaching the goal. For high z values, the policy prioritizes reaching the goal, even if it leads to higher costs.  The optimal z (zopt), found through root-finding, balances these two objectives, achieving the lowest cumulative cost while still reaching the goal. The visualizations are contour plots showing the learned policy and line graphs showing the cumulative cost over time for various z values.", "section": "H.2 Visualization of learned policy for different z"}, {"figure_path": "jzngdJQ2lY/figures/figures_25_1.jpg", "caption": "Figure 9: Minimum-cost reach-avoid example to illustrate the limitation of CMDP-based formulation.", "description": "This figure presents a minimum-cost reach-avoid problem to show the limitations of CMDP-based methods.  It's a graph showing two initial states (A and B), each with two possible actions leading to different goal states (G1, G2, G3) with associated costs (C) or an absorbing state (I). The probabilities of choosing each action are denoted by pA and pB. The example demonstrates that optimizing a weighted combination of reward and cost, or using a threshold on the cost, can lead to suboptimal solutions for the original minimum-cost reach-avoid problem. The optimal solution requires considering both reaching the goal and minimizing the cost simultaneously, a challenge not directly addressed by CMDP formulations.", "section": "I Discussion on Limitation of CMDP-Based Algorithms"}]