[{"figure_path": "0KvYLaTBTE/tables/tables_7_1.jpg", "caption": "Table 1: Evaluation results of offline OpenAI Gym MuJoCo tasks. We provide results for data specification with step-wise reward (left) and final return (right). Bold highlighting indicates top scores. LPT outperforms all final-return baselines and most step-wise-reward baselines.", "description": "This table presents the performance comparison of the proposed Latent Plan Transformer (LPT) model against several baseline methods (CQL, DT, QDT) on various OpenAI Gym MuJoCo tasks.  The results are categorized by two data specifications: one with step-wise rewards and another with only the final return. The table shows the final return achieved by each method across different tasks, highlighting the superior performance of LPT, especially when only the final return is provided as input.", "section": "6.2 Credit assignment"}, {"figure_path": "0KvYLaTBTE/tables/tables_8_1.jpg", "caption": "Table 2: Evaluation results of Maze2D tasks. Bold highlighting indicates top scores.", "description": "This table presents the performance comparison of different reinforcement learning algorithms (CQL, DT, QDT, LPT, and LPT-EI) on three different Maze2D tasks with varying complexities (umaze, medium, and large). The results are expressed as mean \u00b1 standard deviation and show the average scores achieved by each algorithm on the tasks.  Bold highlights indicate the best-performing algorithm for each task. The table demonstrates LPT's success in achieving superior performance compared to the other algorithms, particularly when combined with Exploitation-inclined Inference (LPT-EI).", "section": "6.3 Trajectory stitching"}, {"figure_path": "0KvYLaTBTE/tables/tables_8_2.jpg", "caption": "Table 3: Evaluation results of Antmaze tasks. Bold highlighting indicates top scores.", "description": "This table presents the performance comparison of different reinforcement learning algorithms (CQL, DT, LPT, and LPT-EI) on two Antmaze tasks: Antmaze-umaze and Antmaze-umaze-diverse.  The results show the average success rate (percentage) and standard deviation across multiple trials for each algorithm.  LPT and LPT-EI consistently outperform the baselines, indicating their effectiveness in solving these sparse-reward tasks.", "section": "6.3 Trajectory stitching"}, {"figure_path": "0KvYLaTBTE/tables/tables_9_1.jpg", "caption": "Table 4: Evaluation results on Connect Four. Bold highlighting indicates top scores.", "description": "This table presents the performance comparison of different algorithms (CQL, DT, ESPER, and LPT) on the Connect Four game. The results are shown as mean \u00b1 standard deviation over 5 runs.  The bold values highlight the best-performing algorithm for each metric, demonstrating LPT's superior performance in this specific environment.", "section": "6.4 Environment contingencies"}, {"figure_path": "0KvYLaTBTE/tables/tables_13_1.jpg", "caption": "Table 5: Gym-Mujoco Environments LPT Model Parameters", "description": "This table lists the hyperparameters used for training the Latent Plan Transformer (LPT) model on various Gym-Mujoco locomotion tasks.  The parameters shown include the number of layers, attention heads, embedding dimension, context length, learning rate, Langevin step size, and the nonlinearity function used in the model architecture.  These settings are task-specific, reflecting the differing complexities of the HalfCheetah, Walker2D, Hopper, and AntMaze environments.", "section": "6. Experiments"}, {"figure_path": "0KvYLaTBTE/tables/tables_13_2.jpg", "caption": "Table 6: Maze2D Environments LPT Model Parameters", "description": "This table shows the hyperparameters used for training the Latent Plan Transformer (LPT) model on three different Maze2D environments: Umaze, Medium, and Large.  Each environment has varying complexity, and these settings were adjusted to optimize performance for each environment's unique characteristics. The parameters shown include the number of layers in the neural network, the number of attention heads used in the Transformer architecture, the embedding dimension, context length, learning rate, Langevin step size, and nonlinearity function.", "section": "6 Experiments"}, {"figure_path": "0KvYLaTBTE/tables/tables_13_3.jpg", "caption": "Table 7: Franka Kitchen Environments LPT Model Parameters", "description": "This table shows the hyperparameters used for training the Latent Plan Transformer (LPT) model on the Franka Kitchen dataset.  It lists the number of layers, number of attention heads, embedding dimension, context length, learning rate, Langevin step size, and nonlinearity function used for both the 'mixed' and 'partial' datasets. The 'mixed' dataset contains both task-directed and non-task-directed demonstrations, while the 'partial' dataset contains primarily task-directed demonstrations. These parameters were adjusted to optimize the LPT model's performance on the Franka Kitchen task.", "section": "6 Experiments"}, {"figure_path": "0KvYLaTBTE/tables/tables_13_4.jpg", "caption": "Table 8: Connect 4 LPT Model Parameters", "description": "This table shows the hyperparameters used for the Connect 4 environment in the Latent Plan Transformer (LPT) model.  It includes the number of layers, attention heads, embedding dimension, context length, learning rate, Langevin step size, and the nonlinearity function used.", "section": "6.1 Overview"}, {"figure_path": "0KvYLaTBTE/tables/tables_15_1.jpg", "caption": "Table 9: Ablation study results on Gym-Mujoco tasks and Connect Four.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of the expressive prior, specifically the UNet component, in the Latent Plan Transformer (LPT) model. By removing the UNet, the study assesses the model's performance across three Gym-Mujoco tasks (halfcheetah-medium-replay, hopper-medium-replay, walker2d-medium-replay) and the Connect Four game. The results demonstrate a clear performance decrease across all tasks when the UNet is removed.  The comparison includes scores for the original LPT model and a Decision Transformer (DT) baseline without latent variables.", "section": "A.4 Ablation study"}, {"figure_path": "0KvYLaTBTE/tables/tables_15_2.jpg", "caption": "Table 10: Effect of different UNet configurations on LPT performance.", "description": "This ablation study investigates the impact of different UNet configurations on the LPT model's performance. The table shows the normalized scores on the walker2d-medium-replay task using various UNet architectures, demonstrating that reducing the UNet's capacity or expressiveness consistently degrades performance, highlighting the importance of a sufficiently expressive prior for enhanced model performance.", "section": "6.2 Credit assignment"}, {"figure_path": "0KvYLaTBTE/tables/tables_16_1.jpg", "caption": "Table 11: Evaluation results of online OpenAI Gym MuJoCo and Antmaze tasks. ODT baselines are sourced from Zheng et al. (2022). Our results are reported over 5 seeds.", "description": "This table compares the performance of the proposed Latent Plan Transformer (LPT) model against the Decision Transformer (ODT) baseline on several online reinforcement learning tasks.  The results show the average scores and standard deviations for both step-wise reward and final return settings, highlighting the improvement achieved by LPT in most tasks.", "section": "6 Experiments"}]