{"references": [{"fullname_first_author": "Lili Chen", "paper_title": "Decision Transformer: Reinforcement Learning via Sequence Modeling", "publication_date": "2021-12-01", "reason": "This paper introduces the Decision Transformer (DT), a foundational model for the generative modeling approach to decision-making that the current paper builds upon and improves."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-Learning for Offline Reinforcement Learning", "publication_date": "2020-12-01", "reason": "This paper introduces Conservative Q-Learning (CQL), a key baseline algorithm for offline reinforcement learning that is compared against in the current paper's experiments."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning", "publication_date": "2020-04-07", "reason": "This paper introduces the D4RL benchmark, a collection of datasets used for offline reinforcement learning, which are heavily utilized in the experiments of the current paper."}, {"fullname_first_author": "Ian Osband", "paper_title": "(More) Efficient Reinforcement Learning via Posterior Sampling", "publication_date": "2013-12-01", "reason": "This paper introduces the theoretical foundations for posterior sampling, a crucial technique used in the current paper for efficient inference and planning."}, {"fullname_first_author": "Taku Yamagata", "paper_title": "Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modeling in Offline RL", "publication_date": "2023-07-01", "reason": "This paper proposes Q-learning Decision Transformer (QDT), a significant improvement over DT which is used as a key comparative baseline in the current paper."}]}