[{"figure_path": "IM4LtYRWdE/figures/figures_2_1.jpg", "caption": "Figure 1: SDE-ODE Duality of diffusion-based models. The forward (noising) SDE defining the DBM (left) gives rise to a sequence of marginal probability densities whose temporal evolution is described by a Fokker-Planck equation (FPE, middle). But this correspondence is not unique: the probability flow ODE (pfODE, right) gives rise to the same FPE. That is, while both the SDE and the pfODE possess the same marginals, the former is noisy and mixing while the latter is deterministic and neighborhood-preserving. Both models require knowledge of the score function \u2207x log pt (x), which can learned by training either model.", "description": "This figure illustrates the duality between stochastic differential equations (SDEs) and probability flow ordinary differential equations (pfODEs) in diffusion-based models.  It shows how both an SDE (noisy, mixing process) and a pfODE (deterministic, neighborhood-preserving process) can lead to the same sequence of marginal probability densities, described by a Fokker-Planck equation.  The key is that both models require knowledge of the score function of the data distribution.", "section": "2 Three views of diffusion-based models"}, {"figure_path": "IM4LtYRWdE/figures/figures_4_1.jpg", "caption": "Figure 2: Dimension-preserving flows for toy datasets. Numerical simulations of dimension-preserving flows for five sample toy datasets. Left sequences of sub-panels show results for integrating the pfODE forward in time (inflation); right sub-panels show results of integrating the same system backwards in time (generation) (Appendix B.3). Simulations were conducted with score approximations obtained from neural networks trained on each respective toy dataset (Appendix B.4.1).", "description": "This figure shows the results of numerical simulations for five toy datasets using dimension-preserving flows.  The left side shows the 'inflation' process, where the probability flow ODE (pfODE) is integrated forward in time, gradually transforming data into a Gaussian distribution. The right side demonstrates the 'generation' process, where the pfODE is integrated backward in time, creating data samples from the Gaussian distribution. Each row represents a different toy dataset, and the score function is learned from a neural network trained on the specific dataset.  The figure illustrates that dimension-preserving flows map data while maintaining its intrinsic dimensionality.", "section": "3 Inflationary flows"}, {"figure_path": "IM4LtYRWdE/figures/figures_5_1.jpg", "caption": "Figure 2: Dimension-preserving flows for toy datasets. Numerical simulations of dimension-preserving flows for five sample toy datasets. Left sequences of sub-panels show results for integrating the pfODE forward in time (inflation); right sub-panels show results of integrating the same system backwards in time (generation) (Appendix B.3). Simulations were conducted with score approximations obtained from neural networks trained on each respective toy dataset (Appendix B.4.1).", "description": "This figure shows numerical simulations of dimension-preserving flows on five toy datasets.  Each dataset is visualized with a sequence of sub-panels showing how the model's probability flow ODE transforms the data (inflation, integrating forward in time) and generates samples from it (generation, integrating backward). The left sub-panels show the forward process, and the right sub-panels illustrate the generation process. The color scheme represents the density of the data points. Each panel shows a distinct stage in the transformation, demonstrating how the method preserves the data's intrinsic dimensionality during these transitions.", "section": "3 Inflationary flows"}, {"figure_path": "IM4LtYRWdE/figures/figures_6_1.jpg", "caption": "Figure 4: Calibration experiments. To assess error in our posterior model estimates, we used Hamiltonian Monte Carlo (HMC) to perform inference in one of our toy datasets (2D circles). Drawing samples from a 3-component Gaussian Mixture Model (GMM) prior, we integrated the generative process backward in time to obtain corresponding data space samples (A, components shown in orange, blue, and purple). We then used HMC to obtain posterior samples from the posterior distribution over the weights of the GMM components. (B, C) Kernel density estimates from the joint posterior samples over the mixture distribution weights in the dimension-preserving and dimension-reducing cases. Dashed vertical and horizontal lines indicate posterior means for each component. Reference ground-truth weights were w = [0.5, 0.25, 0.25].", "description": "This figure demonstrates the calibration experiments performed to assess the error in the posterior model estimates.  It uses Hamiltonian Monte Carlo (HMC) for inference on a 2D circles toy dataset.  The figure shows the generative process, where samples are drawn from a 3-component Gaussian Mixture Model (GMM) prior and then integrated backward in time.  The resulting data space samples are displayed.  Kernel density estimates of the joint posterior samples are shown for dimension-preserving and dimension-reducing cases.  The dashed lines indicate the posterior means for each component, with reference ground-truth weights provided.", "section": "4 Calibration experiments"}, {"figure_path": "IM4LtYRWdE/figures/figures_8_1.jpg", "caption": "Figure 5: Generation and round-trip experiments for AFHQv2 at IG=1.02 and varying number of preserved dimensions. Top row: Generated samples for select flow schedules (PR-Preserving (PRP), PR-Reducing to 2D (\u2248 0.07%), 30D(\u2248 1%), and 307D(\u2248 10%), at 1.02 IG. Bottom row: Results for round-trip experiments under same schedules. Leftmost columns are original samples, middle columns are samples mapped to Gaussian latent spaces, and rightmost columns are recovered samples.", "description": "This figure shows the results of generation and round-trip experiments on the AFHQv2 dataset with different numbers of preserved dimensions using inflationary flows.  The top row displays generated samples for four different flow schedules, while the bottom row shows the corresponding reconstructed samples after a round-trip process of mapping to and from a lower-dimensional latent space.  The four schedules vary in how much compression is applied, demonstrating the models ability to generate high-quality images even with significant dimensionality reduction.", "section": "Experiments"}, {"figure_path": "IM4LtYRWdE/figures/figures_29_1.jpg", "caption": "Figure 5: Generation and round-trip experiments for AFHQv2 at IG=1.02 and varying number of preserved dimensions. Top row: Generated samples for select flow schedules (PR-Preserving (PRP), PR-Reducing to 2D (\u2248 0.07%), 30D(\u2248 1%), and 307D(\u2248 10%), at 1.02 IG. Bottom row: Results for round-trip experiments under same schedules. Leftmost columns are original samples, middle columns are samples mapped to Gaussian latent spaces, and rightmost columns are recovered samples.", "description": "This figure displays results for generation and round-trip experiments performed on the AFHQv2 dataset using inflationary flows with different numbers of preserved dimensions. The top row shows generated samples obtained with four different flow schedules (PR-Preserving, PR-Reducing to 2D, 30D, and 307D) at an inflation gap of 1.02.  The bottom row presents the corresponding round-trip results, showcasing how well the original samples are recovered after being mapped to a Gaussian latent space and then back to data space.  The leftmost columns show the original images. The middle columns represent their corresponding mappings to the latent Gaussian spaces, and the rightmost columns depict the final recovered images.", "section": "Experiments"}, {"figure_path": "IM4LtYRWdE/figures/figures_29_2.jpg", "caption": "Figure 5: Generation and round-trip experiments for AFHQv2 at IG=1.02 and varying number of preserved dimensions. Top row: Generated samples for select flow schedules (PR-Preserving (PRP), PR-Reducing to 2D (\u2248 0.07%), 30D(\u2248 1%), and 307D(\u2248 10%), at 1.02 IG. Bottom row: Results for round-trip experiments under same schedules. Leftmost columns are original samples, middle columns are samples mapped to Gaussian latent spaces, and rightmost columns are recovered samples.", "description": "This figure shows the results of generative and roundtrip experiments on the AFHQv2 dataset with different numbers of preserved dimensions.  The top row displays generated samples for different flow schedules (PR-Preserving, PR-Reducing to various dimensions) while the bottom row shows the results of the reverse process, demonstrating reconstruction quality after going through the generative flow.  Leftmost columns show the original samples; middle columns show the latent space representation after applying the forward flow; and rightmost columns show the reconstruction after applying the backward flow. This figure highlights the ability of the model to compress high-dimensional data into a lower-dimensional latent space while preserving information needed for generating high quality images.", "section": "Experiments"}, {"figure_path": "IM4LtYRWdE/figures/figures_31_1.jpg", "caption": "Figure 2: Dimension-preserving flows for toy datasets. Numerical simulations of dimension-preserving flows for five sample toy datasets. Left sequences of sub-panels show results for integrating the pfODE forward in time (inflation); right sub-panels show results of integrating the same system backwards in time (generation) (Appendix B.3). Simulations were conducted with score approximations obtained from neural networks trained on each respective toy dataset (Appendix B.4.1).", "description": "This figure shows numerical simulations for five toy datasets using dimension-preserving flows.  The left side shows the forward integration of the probability flow ordinary differential equation (pfODE) over time (inflation), while the right side shows the reverse integration (generation).  The results demonstrate the ability of these flows to maintain the intrinsic dimensionality of the data.", "section": "3 Inflationary flows"}, {"figure_path": "IM4LtYRWdE/figures/figures_32_1.jpg", "caption": "Figure 2: Dimension-preserving flows for toy datasets. Numerical simulations of dimension-preserving flows for five sample toy datasets. Left sequences of sub-panels show results for integrating the pfODE forward in time (inflation); right sub-panels show results of integrating the same system backwards in time (generation) (Appendix B.3). Simulations were conducted with score approximations obtained from neural networks trained on each respective toy dataset (Appendix B.4.1).", "description": "This figure shows numerical simulations of dimension-preserving flows on five toy datasets. Each dataset is visualized in a sequence of sub-panels, with the left side showing the forward integration of the probability flow ordinary differential equation (pfODE) (inflation), and the right side showing backward integration (generation). The score functions used in these simulations are approximations learned from neural networks trained on the corresponding toy datasets. This visualization aims to demonstrate the behavior of dimension-preserving flows in transforming the data distribution.", "section": "3 Inflationary flows"}, {"figure_path": "IM4LtYRWdE/figures/figures_32_2.jpg", "caption": "Figure 10: Mesh/Alpha-Shape Calibration experiments. For select toy datasets, we numerically assessed coverage during the inflation and generation procedures using (3D) meshes and (2D) alpha-shapes. (A) We constructed fixed coverage sets by sampling data points at fixed Mahalanobis radii from the centers of each distribution and creating alpha shapes (2D) or meshes (3D). (B-C) We then quantified the change in coverage fraction for each of these sets at the end of either \u201cinflation\u201d or \"generation\" procedures. Lines represent means and shaded regions \u00b12 standard deviations across three sets of random seeds. (D) Illustration of the effect of flows on set geometry. While both types of flows distort the shapes of initial sets, they do preserve local neighborhoods, even when one dimension is compressed by five orders of magnitude.", "description": "This figure presents a numerical experiment to evaluate the performance of the proposed inflationary flows model in preserving local neighborhood structure. It uses two toy datasets (2D circles and 3D S-curve) and compares the dimension-preserving and dimension-reducing flows. The experiment involves creating sets of points with known coverage, running the flows (inflation and generation), and measuring the changes in coverage probability. The results show that both types of flows preserve local structures even with significant dimensionality reduction, supporting the model's ability to perform calibrated Bayesian inference.", "section": "C.2.1 Toy Alpha-Shape/Mesh Coverage Experiments"}, {"figure_path": "IM4LtYRWdE/figures/figures_33_1.jpg", "caption": "Figure 11: Additional PR-Preserving experiments for 2D data embedded in 3D space. Here we integrate our PR-Preserving pfODEs forwards in time (i.e., inflation) for 2 different toy datasets, constructed by embedding the 2D Circles data in 3 dimensional space as either a flat (top rows) or a curved (bottom rows) manifold. We present results for such simulations both without any added noise (1st and 3rd rows) and with some small added noise (0.2 and 0.5 o for flat and curved cases, respectively - 2nd and 4th rows).", "description": "This figure shows the results of additional experiments using PR-Preserving inflationary flows on 2D toy datasets embedded in 3D space. The datasets are represented as either a flat or curved manifold. The simulations are run both with and without added noise. The figure displays front and side views of the data distributions at the start and end of the inflation process. The results demonstrate that the inflationary flows preserve the local neighborhood structure, even when small amounts of noise are added.", "section": "C.2.2 Toy Experiments on Datasets with Lower Intrinsic Dimensionality"}, {"figure_path": "IM4LtYRWdE/figures/figures_34_1.jpg", "caption": "Figure 2: Dimension-preserving flows for toy datasets. Numerical simulations of dimension-preserving flows for five sample toy datasets. Left sequences of sub-panels show results for integrating the pfODE forward in time (inflation); right sub-panels show results of integrating the same system backwards in time (generation) (Appendix B.3). Simulations were conducted with score approximations obtained from neural networks trained on each respective toy dataset (Appendix B.4.1).", "description": "This figure shows numerical simulations on five toy datasets for dimension-preserving flows.  The left side shows the forward integration of the probability flow ordinary differential equation (pfODE), which is called \"inflation.\" The right side shows the reverse integration, which is called \"generation.\" Both \"inflation\" and \"generation\" utilize score approximations obtained from neural networks trained for each dataset.", "section": "3 Inflationary flows"}, {"figure_path": "IM4LtYRWdE/figures/figures_35_1.jpg", "caption": "Figure 2: Dimension-preserving flows for toy datasets. Numerical simulations of dimension-preserving flows for five sample toy datasets. Left sequences of sub-panels show results for integrating the pfODE forward in time (inflation); right sub-panels show results of integrating the same system backwards in time (generation) (Appendix B.3). Simulations were conducted with score approximations obtained from neural networks trained on each respective toy dataset (Appendix B.4.1).", "description": "This figure shows numerical simulations of dimension-preserving flows on five different toy datasets.  Each dataset is represented by a set of sub-panels showing the forward (inflation) and reverse (generation) flows. The left side shows the data being transformed into a lower-dimensional Gaussian distribution, and the right side shows the reverse process of generating new samples from the Gaussian distribution. Each simulation uses score approximations from neural networks trained on each dataset.", "section": "3 Inflationary flows"}]