[{"type": "text", "text": "Achieving Near-Optimal Convergence for Distributed Minimax Optimization with Adaptive Stepsizes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yan Huang College of Control Science and Engineering Zhejiang University, China huangyan5616@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Xiang Li Department of Computer Science ETH Zurich, Switzerland xiang.li@inf.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Yipeng Shen College of Control Science and Engineering Zhejiang University, China 22332074@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Niao He Department of Computer Science ETH Zurich, Switzerland niao.he@inf.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Jinming Xu College of Control Science and Engineering Zhejiang University, China jimmyxu@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we show that applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes. To address this challenge, we propose D-AdaST, a Distributed Adaptive minimax method with Stepsize Tracking. The key strategy is to employ an adaptive stepsize tracking protocol involving the transmission of two extra (scalar) variables. This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state error due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence. For nonconvex-strongly-concave distributed minimax problems, we characterize the specific transient times that ensure timescale separation of stepsizes and quasi-independence of networks, leading to a near-optimal convergence rate of $\\tilde{\\mathcal{O}}\\left(\\epsilon^{-(4+\\bar{\\delta})}\\right)$ for any_ small $\\delta\\;>\\;0$ .matching that of the centralized counterpart. To our best knowledge, D-AdaST is the first distributed adaptive method achieving near-optimal convergence without knowing any problem-dependent parameters for nonconvex minimax problems. Extensive experiments are conducted to validate our theoretical results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distributed optimization has seen significant research progress over the last decade, resulting in numerous algorithms (Nedic and Ozdaglar, 2009; Yuan et al., 2016; Lian et al., 2017; Pu and Nedic, 2021). However, the traditional focus of distributed optimization has primarily been on minimization tasks. With the rapid growth of machine learning research, various applications have emerged that go beyond simple minimization, such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Gulrajani et al., 2017), robust optimization (Mohri et al., 2019; Sinha et al., 2017), adversary training of neural networks (Wang et al., 2021), fair machine learning (Madras et al., 2018), and just to name a few. These tasks typically involve a minimax structure as follows: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in{\\mathcal{X}}}\\operatorname*{max}_{y\\in{\\mathcal{Y}}}f\\left(x,y\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{X}\\subseteq\\mathbb{R}^{p}$ \uff0c $\\mathcal{V}\\subseteq\\mathbb{R}^{d}$ , and $x,y$ are the primal and dual variables to be learned, respectively. One of the simplest yet effective methods for solving the above minimax problem is Gradient Descent Ascent (GDA) (Dem'yanov and Pevnyi, 1972; Nemirovski et al., 2009) which alternately performs stochastic gradient descent for the primal variable and stochastic gradient ascent for the dual variable. This approach has demonstrated its effectiveness in solving minimax problems, especially for convexconcave objectives (Hsieh et al., 2021; Daskalakis et al., 2021; Antonakopoulos et al., 2021), i.e., the function $f{\\bar{(\\cdot,y)}}$ isconvexfor any $y\\in\\mathcal{V}$ , and $f(x,\\cdot)$ isconcavefor any $x\\in\\mathscr{X}$ ", "page_idx": 1}, {"type": "text", "text": "Adaptive gradient methods, such as AdaGrad (Duchi et al., 2011), Adam (Kingma and Ba, 2014), and AMSGrad (Reddi et al., 2018), are often integrated with GDA to effectively solve minimax problems with theoretical guarantees in convex-concave settings (Diakonikolas, 2020; Antonakopoulos et al., 2021; Ene and L\u00e9 Nguyen, 2022). These adaptive methods are capable of adjusting stepsizes based on historical gradient information, making it robust to hyper-parameters tuning and can converge without requiring to know problem-dependent parameters (a characteristic often referred to as being \"parameter-agnostic\"). However, in the nonconvex regime, it has been shown by Lin et al. (2020); Yang et al. (2022b) that it is necessary to have a time-scale separation in stepsizes between the minimization and maximization processes to ensure the convergence of GDA and GDA-based adaptive algorithms. In particular, the stepsize ratio between primal and dual variables needs to be smaller than a threshold depending on the properties of the problem such as the smoothness and strong-concavity parameters (Li et al., 2022; Guo et al., 2021; Huang et al., 2021), which are often unknown or difficult to estimate in real-world tasks, such as training deep neural networks. ", "page_idx": 1}, {"type": "text", "text": "Applying GDA-based adaptive methods into decentralized settings poses additional challenges due to the presence of inconsistency in locally computed adaptive stepsizes. In particular, it has been shown that the inconsistency of stepsizes can result in non-convergence in federated learning with heterogeneous computation speeds (Wang et al., 2020; Sharma et al., 2023). This is mainly due to the lack of a central node coordinating the stepsizes of nodes in distributed settings, making it dificult to converge, as observed in minimization problems (Liggett, 2022; Chen et al., 2023b). As a result, the following question arises naturally: ", "page_idx": 1}, {"type": "text", "text": "\"Can we design an adaptive minimax method that ensures the time-scale separation and consistency of stepsizes with provable convergencein fully distributed settings?\" ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this paper, we aim to propose a distributed adaptive method for efficiently solving nonconvex-strongly-concave (NC-SC) minimax problems. The contributions are threefold: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We construct counterexamples showing that directly applying adaptive methods designed for centralized problems will lead to inconsistencies in locally computed adaptive stepsizes, resulting in non-convergence in distributed settings. To tackle this issue, we propose the first distributed adaptive minimax method, named D-AdaST, that incorporates an efficient stepsize tracking mechanism to maintain consistency across local stepsizes, which involves transmission of merely two extra (scalar) variables. The proposed algorithm exhibits timescale separation in stepsizes and parameter-agnostic capability in fully distributed settings. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Theoretically, we prove that D-AdaST is able to achieve a near-optimal convergence rate of $\\tilde{\\mathcal{O}}\\left(\\epsilon^{-(4+\\delta)}\\right)$ with arbitrarily small $\\delta>0$ to find an $\\epsilon_{}$ stationary point for distributed NC-SC minimax problems. In contrast, we also prove the existence of a constant steady-state error in both the lower and upper bounds for GDA-based distributed minimax algorithms when being directly integrated with the adaptive stepsize rule without the stepsize tracking mechanism. Moreover, we explicitly characterize the transient times that ensure time-scale separation and quasi-independence of network, respectively. \u00b7 We conduct extensive experiments on real-world datasets to verify our theoretical findings and the effectiveness of D-AdaST on a variety of tasks, including robust training of neural networks and optimizing Wasserstein GANs. In all tasks, we demonstrate the superiority of D-AdaST over several vanilla distributed adaptive methods across various graphs, initial stepsizes and data distributions (see also additional experiments in Appendix A). ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Distributed nonconvex minimax methods. In the realm of federated learning, Deng and Mahdavi (2021) introduce Local SGDA algorithm combining FedAvg/Local SGD with stochastic GDA and showan $\\tilde{O}\\left(\\epsilon^{-6}\\right)$ sample complexity for NC-SC objective functions. Sharma et al. (2022) provide improved complexity result of $\\Tilde{\\mathcal{O}}\\left(\\epsilon^{-4}\\right)$ matching that of the lower bound of first-order algorithms for both NC-SC and nonconvex-Polyak-Lojasiewicz (NC-PL) settings (Li et al., 2021; Zhang et al., 2021a) . Yang et al. (2022a) integrate Local SGDA with stochastic gradient estimators to eliminate the data heterogeneity. More recently, Zhang et al. (2023) adopt compressed momentum methods with Local SGD to increase the communication efficiency of the algorithm. For decentralized nonconvex minimax problems, Liu et al. (2020) study the training of GANs using decentralized optimistic stochastic gradient and provide non-asymptotic convergence with fixed stepsizes. Tsaknakis et al. (2020) propose a double-loop decentralized SGDA algorithm with gradient tracking techniques (Pu and Nedic, 2021) and achieve $\\tilde{\\mathcal{O}}\\left(\\epsilon^{-4}\\right)$ sample complexity. With a stronger assumption of average smoothness, some studies employ variance reduction techniques to accelerate convergence (Zhang et al., 2021b; Chen et al., 2022; Xian et al., 2021; Tarzanagh et al., 2022; Wu et al., 2023; Chen et al., 2024; Zhang et al., 2024), which require more memory and computational resources due to the need for larger batch-sizes or full gradient evaluations. However, all the above-mentioned methods use a fixed or uniformly decaying stepsize, requiring the prior knowledge of smoothness and concavity. ", "page_idx": 2}, {"type": "text", "text": "(Distributed) adaptive minimax methods. For centralized nonconvex minimax problems, Yang et al. (2022b) show that, even in deterministic settings, GDA-based methods necessitate the timescale separation of the stepsizes for primal and dual updates. Many attempts have been made for ensuring the time-scale separation requirement (Lin et al., 2020; Yang et al., 2022c; Bot and Bohm, 2023; Huang et al., 2023). However, these methods typically come with the prerequisite of having knowledge about problem-dependent parameters, which can be a significant drawback in practical scenarios. To this end, Yang et al. (2022b) introduce a nested adaptive algorithm named NeAda that achieves parameter-agnosticism by incorporating an inner loop to effectively maximize the dual variable, which can obtain an optimal sample complexity of $\\tilde{\\mathcal{O}}\\left(\\epsilon^{-4}\\right)$ when the strong-concavity parameter is known. More recently, Li et al. (2023) introduce TiAda, a single-loop parameter-agnostic adaptive algorithm for nonconvex minimax optimization which employs separated exponential factors on the adaptive primal and dual stepsizes, improving upon NeAda on the noise-adaptivity. There has been few works dedicated to adaptive minimax optimization in federated learning settings. For instance, Huang et al. (2024) introduces a federated adaptive algorithm that integrates the stepsize rule of Adam with full-client participation, resembling the centralized counterpart. Ju et al. (2023) study a federated Adam algorithm for fair federated learning where the objective function is properly weighted to account for heterogeneous updates among nodes. To the best of our knowledge, it is still unknown how one can design an adaptive minimax method capable of fulfilling the time-scale separation requirement and being parameter-agnostic in fully distributed settings. ", "page_idx": 2}, {"type": "text", "text": "Notations. Throughout this paper, we denote by $\\mathbb{E}\\left[\\cdot\\right]$ the expectation of a random variable, $\\lVert\\cdot\\rVert$ the Frobenius norm, $\\langle\\cdot,\\cdot\\rangle$ the inner product of two vectors, $\\odot$ the Hadamard product (entry wise), $\\otimes$ the Kronecker product. We denote by 1 the all-ones vector, I the identity matrix and ${\\dot{\\mathbf{J}}}=\\mathbf{11}^{T}/n$ the averaging matrix with $n$ dimension. For a vector or matrix $A$ and constant $\\alpha$ , we denote $A^{\\alpha}$ the entry-wise exponential operations. We denote $\\phi\\left(x\\right):=f\\left(x,y^{*}\\left(x\\right)\\right)$ as the primal function where $y^{*}\\left({\\overset{.}{x}}\\right)=\\operatorname{argmax}_{y\\in\\mathcal{Y}}f\\left(x,y\\right)$ , and $\\mathcal{P}_{\\mathcal{Y}}\\left(\\cdot\\right)$ as the projection operation onto set $\\boldsymbol{\\wp}$ ", "page_idx": 2}, {"type": "text", "text": "2   Distributed Adaptive Minimax Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the distributed minimax problem collaboratively solved by a set of agents over a network. The overall objective of the agents is to solve the following finite-sum problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}\\ \\operatorname*{max}_{y\\in\\mathcal{Y}}f\\left(x,y\\right)=\\frac{1}{n}\\sum_{i=1}^{n}\\underbrace{\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\left[F_{i}\\left(x,y;\\xi_{i}\\right)\\right]}_{:=f_{i}\\left(x,y\\right)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f_{i}\\,:\\,\\mathbb{R}^{p+d}\\,\\rightarrow\\,\\mathbb{R}$ is the local private loss function accessible only by the associated node $i\\in\\mathcal{N}=\\{1,2,\\cdots\\,,n\\}$ $\\mathcal{V}\\subset\\mathbb{R}^{d}$ is closed and convex, and $\\xi_{i}\\sim\\mathcal{D}_{i}$ denotes the data sample locally stored at node $i\\in\\mathcal N$ with distribution $\\mathcal{D}_{i}$ . We consider a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ , here, $\\mathcal{V}=\\{\\bar{1},2,...,n\\}$ represents the set of agents, and $\\mathcal{E}\\subseteq\\mathcal{V}\\times\\mathcal{V}$ denotes the set of edges consisting of ordered pairs $(i,j)$ ", "page_idx": 2}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/ef626e68aee5d5993793236a83dfd260960b57b177823c577507ef60c281397d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Comparison among D-SGDA, D-TiAda and D-AdaST for NC-SC quadratic objective function (6) with $n\\,=\\,2$ nodes and $\\gamma_{x}=\\gamma_{y}$ . In (a), it shows the trajectories of primal and dual variables of the algorithms, the points on the black dash line are stationary points of $f$ . In (b), it shows the convergence of $\\|\\nabla_{x}f\\left(x_{k},y_{k}\\right)\\|^{2}$ over the iterations. In (c), it shows the convergence of the inconsistency of stepsizes, $\\zeta_{v}^{2}$ defined in (8), over the iterations. Notably, $\\zeta_{v}^{2}$ fails to converge for D-TiAda and $\\bar{\\zeta_{v}^{2}}=0$ for non-adaptive D-SGDA. ", "page_idx": 3}, {"type": "text", "text": "representing the communication link from node $j$ tonode $i$ .Fornode $i$ wedefine $\\mathcal{N}_{i}=\\{j\\mid(i,j)\\in$ $\\mathcal E\\bar{\\}}$ as the set of its neighboring nodes. Before proceeding to the discussion of distributed algorithms, we first introduce the following notations for brevity: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{k}:=\\left[x_{1,k},x_{2,k},\\cdot\\cdot\\cdot\\,,x_{n,k}\\right]^{T}\\in\\mathbb{R}^{n\\times p},\\ \\mathbf{y}_{k}:=\\left[y_{1,k},y_{2,k},\\cdot\\cdot\\,.\\,,y_{n,k}\\right]^{T}\\in\\mathbb{R}^{n\\times d},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{i,k}\\in\\mathbb{R}^{p},y_{i,k}\\in\\mathcal{Y}$ denote the primal and dual variable of node $i$ at each iteration $k$ , and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right):=\\left[\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\nabla_{x}F_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{x}\\right),\\cdot\\cdot\\cdot\\right]^{T},}\\\\ &{\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right):=\\left[\\cdot\\cdot\\mathbf{\\nabla},\\nabla_{y}F_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{y}\\right),\\cdot\\cdot\\cdot\\right]^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "are the coresponding partial stochastic gradients with ii.d. samples $\\xi_{k}^{x},\\xi_{k}^{y}$ in a compact form. ", "page_idx": 3}, {"type": "text", "text": "Next, we will first explain the pitfalls of directly applying centralized adaptive stepsize rules to decentralized settings, and then introduce our newly proposed solution to address the challenge. ", "page_idx": 3}, {"type": "text", "text": "2.1  Non-Convergence of Direct Extensions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For the distributed minimax optimization problem as depicted in (1) involving NC-SC objective functions, we will show shortly that the Distributed Stochastic Gradient Descent Ascent (D-SGDA) method may not converge due to the inability of time-scale separation with constant stepsizes (c.f., Figure 1), which is also observed in centralized settings (Lin et al., 2020; Yang et al., 2022b). To address this issue, one can adopt the adaptive stepsize rule used in centralized TiAda (Li et al., 2023) for each individual node, which is renowned for its ability to adaptively fulfill the time-scale separation requirements. As a result, we arrive at the following Distributed TiAda (D-TiAda) algorithm. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{k+1}=W\\left(\\mathbf{x}_{k}-\\gamma_{x}V_{k+1}^{-\\alpha}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right),}\\\\ &{\\mathbf{y}_{k+1}=\\mathcal{P}_{\\mathcal{V}}\\left(W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\gamma_{x}$ and $\\gamma_{y}$ are the stepsizes, $W$ is a doubly-stochastic weight matrix induced by graph $\\mathcal{G}$ (Xiao et al., 2006) (c.f., Assumption 4), and ", "page_idx": 3}, {"type": "equation", "text": "$$\nV_{k+1}^{-\\alpha}=\\mathrm{diag}\\left\\{v_{i,k+1}^{-\\alpha}\\right\\}_{i=1}^{n},\\quad U_{k+1}^{-\\beta}=\\mathrm{diag}\\left\\{u_{i,k+1}^{-\\beta}\\right\\}_{i=1}^{n},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $v_{i,k+1}=\\operatorname*{max}\\left\\{m_{i,k+1}^{x},m_{i,k+1}^{y}\\right\\},u_{i,k+1}=m_{i,k+1}^{y}$ , and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m_{i,k+1}^{x}=m_{i,k}^{x}+\\left\\|\\nabla_{x}F_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{x}\\right)\\right\\|^{2},\\ m_{i,k+1}^{y}=m_{i,k}^{y}+\\left\\|\\nabla_{y}F_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{y}\\right)\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "are the local accumulated gradient norm. Note that we impose a maximum operator in the preconditioner $v_{i,k}$ , and employ different stepsize decaying rates, i.e., $0<\\beta<\\alpha<1$ , for the primal and dual variables, respectively. Such design allows to balance the updates of $x$ and $y$ , and achieves the desired time-scale separation without requiring any knowledge of parameters (Li et al., 2023). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "However, in the distributed setting, such direct extension may fail to converge to a stationary point because $v_{i,k}$ and $u_{i,k}$ can be inconsistent due to the difference of local objective functions $f_{i}$ ,In particular, we can rewrite the above vanilla distributed optimization algorithm (2) in the sense of average system of primal variables as below, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{x}_{k+1}=\\bar{x}_{k}-\\gamma_{x}\\bar{v}_{k}^{-\\alpha}\\frac{{\\mathbf{1}}^{T}}{n}\\nabla_{x}F\\left({\\mathbf{x}}_{k},{\\mathbf{y}}_{k};{\\xi}_{k}^{x}\\right)-\\gamma_{x}\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n}\\nabla_{x}F\\left({\\mathbf{x}}_{k},{\\mathbf{y}}_{k};{\\xi}_{k}^{x}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$\\begin{array}{r}{\\left(\\tilde{v}_{k}^{-\\alpha}\\right)^{T}:=\\left[\\,\\cdot\\cdot\\,,\\,v_{i,k}^{-\\alpha}-\\bar{v}_{k}^{-\\alpha},\\cdot\\,\\cdot\\,\\right],\\bar{x}_{k}:=\\mathbf{1}^{T}\\mathbf{x}_{k}/n\\mathrm{~and~}\\bar{v}_{k}:=1/n\\sum_{i=1}^{n}v_{i,k}.}\\end{array}$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is evident that, in comparison to centralized adaptive methods, an unexpected term (i.e., $\\tilde{v}_{k}$ on the right-hand side (RHS) arises due to inconsistencies. This term introduces inaccuracies in the directions of gradient descent, degrading the optimization performance. The theorem presented below reveals a gap near the stationary points in a properly designed counterexample, indicating the non-convergence of D-TiAda. The proof is available in Appendix B.3. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. There exists a distributed minimax problem in the form of Problem $(I)$ and certain initialization such that afterrunning $D$ TiAda with any $0<\\beta<0.5<\\alpha<1$ and $\\gamma_{x},\\gamma_{y}>0$ it holds that for any $t=0,1,2,\\ldots$ , we have, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\parallel\\nabla_{x}f(x_{t},y_{t})\\parallel=\\parallel\\nabla_{x}f(x_{0},y_{0})\\parallel,\\quad\\parallel\\nabla_{y}f(x_{t},y_{t})\\parallel=\\parallel\\nabla_{y}f(x_{0},y_{0})\\parallel,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "here $\\|\\nabla_{x}f(x_{0},y_{0})\\|$ and $\\|\\nabla_{y}f(x_{0},y_{0})\\|$ can be arbitrarily large depending on the initialization. ", "page_idx": 4}, {"type": "text", "text": "Remark 1. The counterexample we constructed consists of three nodes, forming a complete graph. Without the stepsize tracking, $D$ -TiAdawill remainstationary,and theiterateswill not progress $i f$ initiated along a specific line. In this counterexample, the only stationary point is at $(0,0)$ ,but initial points along the line $\\left(c.f\\right)$ , Eq. (72)) can be positioned arbitrarily far away from this stationary point, implying the non-convergence off $D$ -TiAda with certain initialization. ", "page_idx": 4}, {"type": "text", "text": "Apart from the counterexample discussed in Theorem 1, we also experimentally observe the divergence of of D-SGDA and D-TiAda even in a simple scenario involving only two connected agents. This phenomenon is illustrated in Figure 1 and the functions are depicted as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{1}\\left(x,y\\right)=-\\displaystyle\\frac{9}{20}y^{2}+\\displaystyle\\frac{3}{5}y-x+x y-\\displaystyle\\frac{1}{2}x^{2},}}\\\\ {{f_{2}\\left(x,y\\right)=-\\displaystyle\\frac{9}{20}y^{2}+\\displaystyle\\frac{3}{5}y-x+2x y-2x^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is not difficult to verify that the points on the line $3y=5x+2$ are stationary points of $f\\left(x,y\\right)=$ $1/2\\left(f_{1}\\left(x,y\\right)+f_{2}\\left(x,y\\right)\\right)$ . It follows from Figure 1(a) and 1(b) that D-SGDA does not converge to a stationary point because of the lack of time-scale separation, and D-TiAda also fails to converge due to stepsize inconsistency, as shown in Figure 1(c). In contrast, the utilization of the stepsize tracking protocol in D-AdaST ensures convergence to a stationary point, with the inconsistency in stepsizes gradually diminishing (c.f., Lemma 9). These two motivating examples effectively highlight the challenges associated with applying adaptive minimax algorithms to distributed settings. ", "page_idx": 4}, {"type": "text", "text": "2.2  The Proposed D-AdaST Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To address the issue of stepsize inconsistency across different nodes, we propose the following Distributed Adaptive minimax optimization algorithm with Stepsize Tracking protocol, termed DAdaST, which allows us to asymptotically eliminate the stepsize inconsistency in a decentralized manner over networks. The pseudo-code for the algorithm is summarized in Algorithm 1, and can be rewritten in a compact form as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{m}_{k+1}^{x}=W\\left(\\mathbf{m}_{k}^{x}+\\mathbf{h}_{k}^{x}\\right),}\\\\ &{\\mathbf{m}_{k+1}^{y}=W\\left(\\mathbf{m}_{k}^{y}+\\mathbf{h}_{k}^{y}\\right),}\\\\ &{\\mathbf{x}_{k+1}=W\\left(\\mathbf{x}_{k}-\\gamma_{x}V_{k+1}^{-\\alpha}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right),}\\\\ &{\\mathbf{y}_{k+1}=\\mathcal{P}_{\\mathcal{V}}\\left(W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Distributed Adaptive Minimax Method with Stepsize Tracking (D-AdaST) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Initialization: Ti,.o E RP, y.o E V, buffers m2.0 $m_{i,0}^{x}\\;=\\;m_{i,0}^{y}\\;=\\;c\\;>\\;0$ , stepsizes $\\gamma_{x},\\gamma_{y}~>~0$ exponential factors $0<\\beta<\\alpha<1$ and weight matrix $W$ ", "page_idx": 5}, {"type": "text", "text": "1: for iteration $k=0,1,\\cdot\\cdot\\cdot$ , each node $i\\in[n]$ ,do ", "page_idx": 5}, {"type": "text", "text": "2: Sample i.d. $g_{i,k}^{x}=\\nabla_{x}F_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{x}\\right)$ and $g_{i,k}^{y}=\\nabla_{y}F_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{y}\\right)$ ", "page_idx": 5}, {"type": "text", "text": "3: Accumulate the gradient norm: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m_{i,k+1}^{x}=m_{i,k}^{x}+\\|g_{i,k}^{x}\\|^{2},\\ m_{i,k+1}^{y}=m_{i,k}^{y}+\\|g_{i,k}^{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4: Compute the ratio: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{i,k+1}=({m}_{i,k+1}^{x})^{\\alpha}/\\operatorname*{max}\\left\\{({m}_{i,k+1}^{x})^{\\alpha},({m}_{i,k+1}^{y})^{\\alpha}\\right\\}\\leqslant1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5: Update primal and dual variables locally: ", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{i,k+1}=x_{i,k}-\\gamma_{x}\\psi_{i,k+1}\\left(m_{i,k+1}^{x}\\right)^{-\\alpha}g_{i,k}^{x},\\ y_{i,k+1}=y_{i,k}+\\gamma_{y}(m_{i,k+1}^{y})^{-\\beta}g_{i,k}^{y}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "6:  Communicate adaptive stepsizes and decision variables with neighbors: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Big\\{m_{i,k+1}^{x},m_{i,k+1}^{y},\\boldsymbol{x}_{i,k+1},\\boldsymbol{y}_{i,k+1}\\Big\\}\\gets\\sum_{j\\in\\mathcal{N}_{i}}W_{i,j}\\,\\Big\\{m_{j,k+1}^{x},m_{j,k+1}^{y},\\boldsymbol{x}_{j,k+1},\\boldsymbol{y}_{j,k+1}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "7:  Projection of dual variable on the set $\\boldsymbol{\\wp}$ $\\mathcal{V}\\!:\\,y_{i,k+1}\\leftarrow\\mathcal{P}_{\\mathcal{V}}\\left(y_{i,k+1}\\right)$ ", "page_idx": 5}, {"type": "text", "text": "8: end for ", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{m}_{k}^{x}=[\\cdot\\cdot\\cdot,m_{i,k}^{x},\\cdot\\cdot\\cdot]^{T},\\mathbf{m}_{k}^{y}=[\\cdot\\cdot\\cdot,m_{i,k}^{y},\\cdot\\cdot\\cdot]^{T}$ denote the tracking variables for the accumulated global gradient norm, i.e., for $z\\in\\{x,y\\}$ \uff0c ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{1}^{T}}{n}\\mathbf{m}_{k+1}^{z}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\sum_{t=0}^{k}{\\left\\|g_{i,t}^{z}\\right\\|^{2}}+m_{i,0}^{z}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "While $\\begin{array}{r}{\\pmb{h}_{k}^{z}=[\\cdot\\cdot\\cdot\\cdot,\\|\\ g_{i,k}^{z}\\ \\|^{2},\\cdot\\cdot\\cdot]^{T}}\\end{array}$ ,and $V_{k},U_{k}$ are diagonal matrices with $v_{i,k}=\\operatorname*{max}\\left\\{m_{i,k}^{x},m_{i,k}^{y}\\right\\}$ and $u_{i,k}=m_{i,k}^{x}$ . Note that we also provide a variant of D-AdaST with coordinate-wise adaptive stepsizes in Algorithm 2, along with its convergence analysis in Appendix B.5. ", "page_idx": 5}, {"type": "text", "text": "3  Convergence Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present the main convergence results for the proposed D-AdaST algorithm and compare it with D-TiAda to show the effectiveness of the proposed stepsize tracking protocol. To this end, letting $\\begin{array}{r}{\\bar{u}_{k}:=1/n\\sum_{i=1}^{n}u_{i,k}}\\end{array}$ , we define the following metrics to evaluate the level of inconsistency of stepsizes among nodes, which are ensured to be bounded by Assumption 3. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\zeta_{v}^{2}:=\\operatorname*{sup}_{i\\in[n],k>0}\\left\\{\\left(v_{i,k}^{-\\alpha}-\\bar{v}_{k}^{-\\alpha}\\right)^{2}/\\left(\\bar{v}_{k}^{-\\alpha}\\right)^{2}\\right\\},\\ \\zeta_{u}^{2}:=\\operatorname*{sup}_{i\\in[n],k>0}\\left\\{\\left(u_{i,k}^{-\\beta}-\\bar{u}_{k}^{-\\beta}\\right)^{2}/\\left(\\bar{u}_{k}^{-\\beta}\\right)^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.1 Assumptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We consider the NC-SC setting of Problem (1) with the following assumptions that are commonly used in the existing works (c.f., Remark 2 and Remark 3). Notably, for the function and algorithm class determined by the assumptions of this work, Li et al. (2021) derived a lower complexity bound Oof $\\varOmega\\left(\\epsilon^{-4}\\right)$ and proved that such a dependency on $\\epsilon$ is optimal (c.f., Remark 2). ", "page_idx": 5}, {"type": "text", "text": "Assumption 1 $\\lvert\\mu\\rvert$ -strong concavity in $y$ ). Each objective function $f_{i}\\left(x,y\\right)$ is $\\mu$ -strongly concave in $y$ i.e., $\\forall x\\in\\mathbb{R}^{p}$ $\\forall y,y^{\\prime}\\in\\mathcal{y}$ and $\\mu>0$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{i}\\left(x,y\\right)-f_{i}\\left(x,y^{\\prime}\\right)\\geqslant\\left\\langle\\nabla_{y}f_{i}\\left(x,y\\right),y-y^{\\prime}\\right\\rangle+\\frac{\\mu}{2}\\left\\Vert\\mathbf{\\Sigma}y-y^{\\prime}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumption 2 (Joint smoothness). Each objective function $f_{i}\\left(x,y\\right)$ is $L$ -smoothin $x$ and $y$ , i.e., $\\forall x,x^{\\prime}\\in\\mathbb{R}^{p}$ and $\\forall y,y^{\\prime}\\in\\mathcal{y}$ thereexistsaconstant $L$ such that for $z\\in\\{x,y\\}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla_{z}f_{i}\\left(x,y\\right)-\\nabla_{z}f_{i}\\left(x^{\\prime},y^{\\prime}\\right)\\right\\|^{2}\\leqslant L^{2}\\left(\\left\\|x-x^{\\prime}\\right\\|^{2}+\\left\\|y-y^{\\prime}\\right\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Furthermore, $f_{i}$ is second-order Lipschitz continuous for $y$ i.e.,for $z\\in\\{x,y\\}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\Vert\\nabla_{z y}^{2}f_{i}\\left(x,y\\right)-\\nabla_{z y}^{2}f_{i}\\left(x^{\\prime},y^{\\prime}\\right)\\right\\Vert^{2}\\leqslant L^{2}\\left(\\left\\Vert x-x^{\\prime}\\right\\Vert^{2}+\\left\\Vert y-y^{\\prime}\\right\\Vert^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 2. Assumption $^{\\,l}$ does not require the convexity in $x$ and the objective function thus can be nonconvex. Assumption $^{\\,l}$ and 2ensure that $y^{\\ast}(\\cdot)$ is smooth $(c.f.$ ,Lemma 2),whichis essential for achieving (near) optimal convergence rate (Chen et al., 2021; $L i$ et al., 2023). Besides, it can be verified that the constructed\u2018hard' examples for obtaining the lower complexity bound in Li et al. (2021) satisfy the above second-order Lipschitz continuity $(I I)$ on $y$ ,implying that theachievable optimal complexity for the function and algorithm class considered in this work is $\\mathcal{O}\\left(\\epsilon^{-4}\\right)$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 3 (Stochastic gradient). For i.i.d. sample $\\xi_{i}$ the stochastic gradient of each i is unbiased, i.e., $\\forall x\\in\\mathbb{R}^{p},y\\in\\mathcal{Y}$ $\\mathbb{E}_{\\xi_{i}}\\bar{[}\\nabla_{z}F_{i}\\left(x,y;\\xi_{i}\\right)]=\\bar{\\nabla_{z}}f_{i}\\left(x,y\\right)$ ,for $z\\in\\{x,y\\}$ , and there is a constant $C>0$ such that $\\|\\nabla_{z}F_{i}\\left(x,y;\\xi_{i}\\right)\\|\\leqslant C$ ", "page_idx": 6}, {"type": "text", "text": "Remark 3. Assumption 3 on unbiased stochastic gradient is widely used for establishing convergence rates of both minimization and minimax optimization methods with AdaGrad (Kavis et al., 2022; $L i$ et al., 2023) or Adam (Zou et al., 2019; Chen et al., $2023a$ : Huang et al., 2024) adaptive stepsize. We note that under Assumption 2, this assumption can be easily satisfied in many real-world tasks by imposing constraints on the compact domain of $f$ e.g., neural networks with rectified activation (Dinh et al., 2017) and GANs with projections on the critic (Gulrajani et al., 2017). ", "page_idx": 6}, {"type": "text", "text": "Next, we make the following assumption on the underlying graph to ensure its connectivity. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4 (Graph connectivity). The weight matrix $W$ induced by graph $\\mathcal{G}$ is doubly stochastic, i.e, $W\\mathbf{1}=\\mathbf{1},\\mathbf{1}^{T}W=\\mathbf{1}^{T}$ and $\\rho_{W}:=\\left\\|W-\\mathbf{J}\\right\\|_{2}^{2}<1$ ", "page_idx": 6}, {"type": "text", "text": "Note that one can always find a proper weight matrix $W$ compliant to the graph that satisfies Assumption 4 once the underlying graph is undirected and connected. For instance, the weight matrix can be easily determined based on the Metropolis-Hastings protocol (Xiao et al., 2006). Moreover, this assumption is more general than that in Lian et al. (2017); Borodich et al. (2021) in the sense that $W$ is not required to be symmetric, implying that certain directed graphs can be included in this assumption, e.g., directed ring and exponential graphs (Ying et al., 2021). ", "page_idx": 6}, {"type": "text", "text": "3.2  Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We are now ready to present the key convergence results in terms of the primal function $\\varPhi\\left(x\\right):=$ $f\\left(x,y^{*}\\left(x\\right)\\right)$ With $y^{*}\\left(x\\right)=\\operatorname{argmax}_{y\\in\\mathcal{Y}}f\\left(x,y\\right)$ , whose proofs can be found in Appendix B.4. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Suppose Assumption 1-4 hold. Let $0<\\beta<\\alpha<1$ and the total iteration $K$ satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Omega\\left(\\operatorname*{max}\\left\\{\\left(\\frac{\\gamma_{x}^{2}\\kappa^{4}}{\\gamma_{y}^{2}}\\right)^{\\frac{1}{\\alpha-\\beta}},\\ \\left(\\frac{1}{\\left(1-\\rho_{W}\\right)^{2}}\\right)^{\\operatorname*{max}\\left\\{\\frac{1}{\\alpha},\\frac{1}{\\beta}\\right\\}}\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With $\\kappa:=L/\\mu$ to ensure time-scale separation and quasi-independence of the network. For $D$ AdaST, wehave ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\lVert\\nabla\\Phi\\left(\\bar{x}_{k}\\right)\\right\\rVert^{2}\\right]=\\tilde{\\mathcal{O}}\\left(\\frac{1}{K^{1-\\alpha}}+\\frac{1}{\\left(1-\\rho_{W}\\right)^{\\alpha}K^{\\alpha}}\\right)+\\tilde{\\mathcal{O}}\\left(\\frac{1}{K^{1-\\beta}}+\\frac{1}{\\left(1-\\rho_{W}\\right)K^{\\beta}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 4 (Near-optimal convergence). Theorem 2 implies that if the total number of iterations satisfies the conditions (12), the proposed $D$ -AdaST algorithm converges to a stationary point exactly forProblem $(I)$ with an $\\tilde{\\mathcal{O}}\\left(\\epsilon^{-(4+\\delta)}\\right)$ sample complexity for arbitrarily small $\\delta>0$ e.g, letting $\\alpha=0.5+\\delta/\\left(8+2\\delta\\right)$ and $\\beta=0.5-\\delta/\\left(8+2\\delta\\right)$ .Itisworthnotingthatthisrateisnear-optimal compared to the existing lower bound of $\\varOmega\\left(\\epsilon^{-4}\\right)$ (Li et al.,2021) for a class of smooth NC-SC functions. Moreover, this result recovers the centralized TiAda algorithm (Li et al., 2023) as a special case, i.e., setting $\\rho_{W}=0$ withoutassumingtheexistenceofinterioroptimalpoint( $c f.$ ,Assumption $3.3~L i$ et al. (2023)). To the best of our knowledge, there is no existing fully parameter-agnostic method that achieves a convergence rate of $\\Tilde{\\mathcal{O}}\\left(\\epsilon^{-\\bar{4}}\\right)$ , even in a centralized setting. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Remark 5 (Parameter-agnostic property and transient times). The above results show that $D$ -AdaST converges without requiring to know any problem-dependent parameters,i.e., $L,$ $\\mu$ and $\\rho_{W}$ ,or tuning the initial stepsize $\\gamma_{x}$ and $\\gamma_{y}$ and is thus parameter-agnostic. Moreover, we explicitly characterize the transient times (c.f., Eq. (12)) that ensure time-scale separation and quasi-independence of the network, respectively.Indeed,we can see that if \u03b1 and $\\beta$ areclosetoeachother,thetimerequiredfor time-scale separation to occur increases significantly, which has been observed in (Li et al., 2023). On the other hand, if \u03b1 and $\\beta$ are relatively large, then $\\tilde{\\mathcal{O}}\\left(1/K^{1-\\alpha}+1/K^{1-\\beta}\\right)$ dominates the other terms, indicating independence on the network. These observations highlight the trade-offs between the convergence rate and the required duration of the transition phase. ", "page_idx": 7}, {"type": "text", "text": "For proper comparison, we also derive an upper bound for D-TiAda as follows. Together with the lower bound in Theorem 1, we demonstrate that without the stepsize tracking mechanism, the inconsistency among local stepsizes prevents D-TiAda from converging in the distributed setting. ", "page_idx": 7}, {"type": "text", "text": "Corollary 1. Under the same conditions of Theorem 2. For the proposed $D$ -TiAda,wehave ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla\\phi\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]\\,=\\tilde{\\mathcal{O}}\\left(\\frac{1}{K^{1-\\alpha}}+\\frac{1}{\\left(1-\\rho_{W}\\right)^{\\alpha}K^{\\alpha}}\\right)}&{}\\\\ {\\displaystyle\\,+\\,\\tilde{\\mathcal{O}}\\left(\\frac{1}{K^{1-\\beta}}+\\frac{1}{\\left(1-\\rho_{W}\\right)K^{\\beta}}\\right)+\\tilde{\\mathcal{O}}\\left(\\left(\\zeta_{v}^{2}+\\kappa^{2}\\zeta_{u}^{2}\\right)C^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we conduct experiments to validate the theoretical findings and demonstrate the effectiveness of the proposed algorithm on real-world machine learning tasks. We compare the proposed D-AdaST with the distributed variants of AdaGrad (Duchi et al., 2011), TiAda (Li et al., 2023) and NeAda (Yang et al., 2022b), namely D-AdaGrad, D-TiAda and D-NeAda, respectively. These experiments run across multiple nodes with different networks, and we consider heterogeneous distributions of local objective functions/datasets. For example, each node can only access samples with a subset of labels on MNIST and CIFAR-10 datasets, which is a common scenario in decentralized and federated learning tasks (Sharma et al., 2023; Huang et al., 2022). The experiments cover three main tasks: synthetic function, robust training of the neural network, and training of Wasserstein GANs (Heusel et al., 2017). For the exponential factors of stepsize, we set $\\alpha=0.6$ and $\\beta=0.4$ for both D-TiAda and D-AdaST. More detailed settings and additional experiments with different initial stepsizes, data distributions and choices of $\\alpha$ and $\\beta$ can be found in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "Synthetic example. We consider a distributed minimax problem with the following NC-SC local objective functions over exponential networks with $n=50$ $\\rho_{W}=0.71)$ and $n=100$ $\\rho_{W}=0.75$ ", "page_idx": 7}, {"type": "equation", "text": "$$\nf_{i}\\left(x,y\\right)=-\\frac{1}{2}y^{2}+L_{i}x y-\\frac{L_{i}^{2}}{2}x^{2}-2L_{i}x+L_{i}y,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $L_{i}\\sim\\mathcal{U}\\left(1.5,2.5\\right)$ . The local gradient of each node is computed with an additive $\\mathcal{N}\\left(0,0.1\\right)$ Gaussian noise. It follows from Figure 2 (a) and 2 (b) that the proposed D-AdaST algorithm outperforms other distributed adaptive methods for both initial stepsize settings, especially in cases with a favorable initial stepsize ratio, as illustrated in plots (b) and (d) where $\\bar{\\gamma}_{x}/\\gamma_{y}=0.\\bar{2}$ Similar observation can be found in Figure 2 (c) and 2 (d), demonstrating the effectiveness of D-AdaST. ", "page_idx": 7}, {"type": "text", "text": "Robust training of neural networks. Next, we consider the task of robust training of neural networks, in the presence of adversarial perturbations on data samples (Sharma et al., 2022; Deng and Mahdavi, 2021). The problem can be formulated as $\\begin{array}{r}{\\underset{\\boldsymbol{x}}{\\operatorname*{min}}\\,\\underset{\\boldsymbol{y}}{\\operatorname*{max}}\\,1/n\\sum_{i=1}^{n}f_{i}\\left(\\boldsymbol{x};\\xi_{i}+\\boldsymbol{y}\\right)-\\eta\\left\\|\\boldsymbol{y}\\right\\|^{2}}\\end{array}$ where $x$ denotes the parameters of the model, $y$ denotes the perturbation and $\\xi_{i}$ denotes the data sample of node $i$ . Note that if $\\eta$ is large enough, the problem is NC-SC. We conduct experiments on ", "page_idx": 7}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/fbf4380da2367ea87c0e076c27b16b10861019986e08a853985c9806ef4d5fcd.jpg", "img_caption": ["Figure 2: Performance comparison of algorithms on quadratic functions over exponential graphs with nodecounts $n=\\{50,100\\}$ and different initial stepsizes $(\\gamma_{y}=0.1)$ "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/404e53ff05d225f374b1f475b75d3e399fa5b1608762493a82e034597fb80051.jpg", "img_caption": ["Figure 3: Comparison of the algorithms on training robust CNN on MNIST dataset. The first row shows the results of AdaGrad-like stepsize, and the second row is for Adam-like stepsize. For the first three columns, we compare the algorithms on different graphs with $n=20$ .For thelast column, we show the scalability of D-AdaST in terms of number of nodes. Initial stepsizes are set as $\\gamma_{x}=0.01,\\gamma_{y}=0.1$ for AdaGrad-like stepsize, and $\\gamma_{x}=0.1,\\gamma_{y}=0.1$ for Adam-like stepsize. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "MNIST dataset over different networks, e.g., ring graph, exponential (exp.) graph (Ying et al., 2021) and dense graph with $n/2$ edges for each node. We consider a heterogeneous scenario in which each node possesses only two distinct classes of labeled samples, resulting in heterogeneity among the local datasets across nodes, while the data is i.i.d within each node. ", "page_idx": 8}, {"type": "text", "text": "In Figure 3, we compare D-AdaST with D-AdaGrad, D-TiAda and D-NeAda, using adaptive stepsizes in AdaGrad (first row) and Adam (second row, name suffixed with Adam) respectively, it can be observed from the first three columns that the proposed D-AdaST outperforms the others on three different graphs and it is not very sensitive to the graph connectivity (i.e., $\\rho_{W}.$ D,demonstrating the quasi-independence of network as indicated in Theorem 2. It should be noted that Adam-like algorithms exhibit more fuctuations in the later stages of optimization as the gradient norm vanishes, leading to an inevitable increase in the Adam stepsize as the optimization process converges (Kingma and Ba, 2014). In plots (d) and (h), we further demonstrate that D-AdaST can scale efficiently with respect to the number of nodes, while keeping a constant batch-size of 64 for each node. This showcases the algorithm's ability to handle large-scale distributed scenarios effectively. ", "page_idx": 8}, {"type": "text", "text": "Generative Adversarial Networks. We further illustrate the effectiveness of D-AdaST on another popular task of training GANs, which has a generator and a discriminator used to generate and distinguish samples respectively (Goodfellow et al., 2014). In this experiment, we train Wasserstein GANs (Gulrajani et al., 2017) on CIFAR-10 dataset in a decentralized setting where each discriminator is 1-Lipschitz and has access to only two classes of samples. We compare the inception score of D-AdaST with D-Adam and D-TiAda adopting Adam-like stepsizes in Figure 4. It can be observed from the figure that D-AdaST achieves higher inception scores in three cases with different initial stepsizes, and has a small score loss as the initial step size changes. We believe that this example shows the great potential of D-AdaST in solving real-world problems. ", "page_idx": 8}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/a92200adbc61dc6df2bf3a86a35929d4d5dcc6ff53c28194b3a20548019972ac.jpg", "img_caption": ["Figure 4: Training GANs on CIFAR-10 dataset over exponential graphs with $n=10$ nodes. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced a new distributed adaptive minimax method, D-AdaST, designed to tackle the issue of non-convergence in nonconvex-strongly-concave minimax problems caused by the inconsistencies among locally computed adaptive stepsizes. Vanilla distributed adaptive methods could suffer from such inconsistencies, as highlighted by the carefully designed counterexamples for demonstrating their potential non-convergence. In contrast, our proposed method employs an efficient adaptive stepsize tracking protocol that not only ensures the time-scale separation, but also guarantees stepsize consistency among nodes and thus effectively eliminates steady-state errors. Theoretically, we showed that D-AdaST can achieve a near-optimal convergence rate of $\\tilde{\\mathcal{O}}\\left(\\epsilon^{-(4+\\delta)}\\right)$ with any arbitrarily small $\\delta>0$ . Extensive experiments on both real-world and synthetic datasets have been conducted to validate our theoretical findings across various scenarios. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of Huang, Shen and Xu has been supported by the National Key R&D Program of China under Grant No. 2022YFB3102100, and in parts by National Natural Science Foundation of China under Grants 62373323, 62088101. The work of Li and He has been supported by the ETH research grant and Swiss National Science Foundation (SNSF) Starting Grant. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Antonakopoulos, K., Belmega, V. E., and Mertikopoulos, P. (2021). Adaptive extra-gradient methods for min-max optimization and games. In ICLR 2021-9th International Conference on Learning Representations, pages 1-28.   \nBorodich, E., Beznosikov, A., Sadiev, A., Sushko, V., Savelyev, N., Takac, M., and Gasnikov, A. (2021). Decentralized personalized federated min-max problems. arXiv preprint arXiv:2106.07289.   \nBot, R. I. and Bohm, A. (2023). Alternating proximal-gradient steps for (stochastic) nonconvexconcave minimax problems. SIAM Journal on Optimization, 33(3):1884-1913.   \nChen, C., Shen, L., Liu, W., and Luo, Z.-Q. (2023a). Effcient-adam: Communication-efficient distributed adam. IEEE Transactions on Signal Processing.   \nChen, L., Ye, H., and Luo, L. (2022). A simple and effcient stochastic algorithm for decentralized nonconvex-strongly-concave minimax optimization. arXiv preprint arXiv:2212.02387.   \nChen, L., Ye, H., and Luo, L. (2024). An efficient stochastic algorithm for decentralized nonconvexstrongly-concave minimax optimization. In International Conference on Artificial Intelligence and Statistics, pages 1990-1998. PMLR.   \nChen, T., Sun, Y., and Yin, W. (2021). Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. Advances in Neural Information Processing Systems, 34:25294-25307.   \nChen, X., Karimi, B., Zhao, W., and Li, P. (2023b). On the convergence of decentralized adaptive gradient methods. In Asian Conference on Machine Learning, pages 217-232. PMLR.   \nDaskalakis, C., Skoulakis, S., and Zampetakis, M. (2021). The complexity of constrained minmax optimization. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 1466-1478.   \nDem'yanov, V. F. and Pevnyi, A. B. (1972). Numerical methods for finding saddle points. USSR Computational Mathematics and Mathematical Physics, 12(5):11-52.   \nDeng, Y. and Mahdavi, M. (2021). Local stochastic gradient descent ascent: Convergence analysis and communication efficiency. In International Conference on Artijficial Intelligence and Statistics, pages 1387-1395. PMLR.   \nDiakonikolas, J. (2020). Halpern iteration for near-optimal and parameter-free monotone inclusion and strong solutions to variational inequalities. In Conference on Learning Theory, pages 1428- 1451. PMLR.   \nDinh, L., Pascanu, R., Bengio, S., and Bengio, Y. (2017). Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pages 1019-1028. PMLR.   \nDuchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7).   \nEne, A. and Le Nguyen, H. (2022). Adaptive and universal algorithms for variational inequalities with optimal convergence. In Proceedings of the AAAl Conference onArtificial Intelligence, volume 36, pages 6559-6567.   \nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27.   \nGulrajani, I, Ahmed, F, Arjovsky, M., Dumoulin, V., and Courville, A. C. (2017). Improved training of wasserstein gans. Advances in neural information processing systems, 30.   \nGuo, Z., Xu, Y., Yin, W., Jin, R., and Yang, T. (2021). A novel convergence analysis for algorithms of the adam family. arXiv preprint arXiv:2112.03459.   \nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30.   \nHsieh, Y-P, Mertikopoulos, P, and Cevhr, V 2021). Th limits of min-max optimization algorithms: Convergence to spurious non-critical sets. In International Conference on Machine Learning, pages 4337-4348. PMLR.   \nHuang, F., Wang, X., Li, J., and Chen, S. (2024). Adaptive federated minimax optimization with lowercomlexities IIntrational Conference onArtifcial Intellgence and Statistics pages 4663-4671. PMLR.   \nHuang, F., Wu, X., and Hu, Z. (2023). Adagda: Faster adaptive gradient descent ascent methods for minimaxoptimization In International Conference on Arifcial Intellgence and Statistics, pages 2365-2389. PMLR.   \nHuang, F., Wu, X., and Huang, H. (2021). Efficient mirror descent ascent methods for nonsmooth minimax problems. Advances in Neural Information Processing Systems, 34:10431-10443.   \nHuang, Y., Sun, Y., Zhu, Z., Yan, C., and Xu, J. (2022). Tackling data heterogeneity: A new unified framework for decentralized SGD with sample-induced topology. In Proceedings of the 39th International ConferenceonMachineLearning,volume 162ofProceedings of MachineLearning Research, pages 9310-9345. PMLR.   \nJu, L., Zhang, T., Toor, S., and Hellander, A. (2023). Accelerating fair federated learning: Adaptive federated adam. arXiv preprint arXiv:2301.09357.   \nKavis, A., Levy, K. Y., and Cevher, V. (2022). High probability bounds for a class of nonconvex algorithms with adagrad stepsize. In International Conference on Learning Representations.   \nKingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv: 1412.6980.   \nLi, H., Farnia, F, Das, S., and Jadbabaie, A. (2022). On convergence of gradient descent ascent: Atight local analysis. In International Conference on Machine Learning, pages 12717-12740. PMLR.   \nLi, H., Tian, Y., Zhang, J., and Jadbabaie, A. (2021). Complexity lower bounds for nonconvexstrongly-concave min-max optimization. Advances in Neural Information Processing Systems 34:1792-1804.   \nLi, X., YANG, J., and He, N. (2023). Tiada: A time-scale adaptive algorithm for nonconvex minimax optimization. In The Eleventh International Conference on Learning Representations.   \nLian, X., Zhang, C., Zhang, H., Hsieh, C.J., Zhang, W, and Liu, J. (2017). Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. Advances in Neural Information Processing Systems, 30.   \nLiggett, B. (2022). Distributed learning with automated stepsizes.   \nLin, T, Jin, C., and Jordan, M. (2020). On gradient descent ascent for nonconvex-concave minimax problems. In International Conference on Machine Learning, pages 6083-6093. PMLR.   \nLiu, M., Zhang, W., Mroueh, Y., Cui, X., Ross, J, Yang, T., and Das, P. (2020). A decentralized parallel algorithm for training generative adversarial nets. Advances in Neural Information Processing Systems, 33: 11056-11070.   \nMadras, D., Creager, E., Pitassi, T, and Zemel, R. 2018). Learning adversarially fair and transferable representations. In International Conference on Machine Learning, pages 384-3393. PMLR.   \nMohri, M., Sivek, G., and Suresh, A. T. (2019). Agnostic federated learning. In International Conference on Machine Learning, pages 4615-4625. PMLR.   \nNedic, A. and Ozdaglar, A. (2009). Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48-61.   \nNedic, A., Ozdaglar, A., and Parrilo, P. A. (2010). Constrained consensus and optimization in multi-agent networks. IEEE Transactions on Automatic Control, 55(4):922-938.   \nNemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. (2009). Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-1609.   \nPu, S. and Nedi, A. (2021). Distributed stochastic gradient tracking methods. Mathematical Programming, 187(1):409-457.   \nReddi, S. J., Kale, S., and Kumar, S. (2018). On the convergence of adam and beyond. In International Conference on Learning Representations.   \nSharma, P., Panda, R., and Joshi, G. (2023). Federated minimax optimization with client heterogeneity. arXiv preprint arXiv:2302.04249.   \nSharma, P., Panda, R., Joshi, G., and Varshney, P. (2022). Federated minimax optimization: Improved convergence analyses and algorithms. In International Conference on Machine Learning, pages 19683-19730. PMLR.   \nSinha, A., Namkoong, H., Volpi, R., and Duchi, J. (2017). Certifying some distributional robustness with principled adversarial training. arXiv preprint arXiv: 1710.10571.   \nTarzanagh, D. A., Li, M., Thrampoulidis, C., and Oymak, S. (2022). Fednest: Federated bilevel, minimax, and compositional optimization. In International Conference on Machine Learning, pages 21146-21179. PMLR.   \nTsaknakis, I., Hong, M., and Liu, S. (2020). Decentralized min-max optimization: Formulations, algorithms and applications in network poisoning attack. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5755-5759. IEEE.   \nWang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H. V. (2020). Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems, 33:7611-7623.   \nWang, J., Zhang, T., Liu, S., Chen, P-Y, Xu, J., Fardad, M., and Li, B. (2021). Adversarial attack generation empowered by min-max optimization. Advances in Neural Information Processing Systems, 34:16020-16033.   \nWu, X., Sun, J., Hu, Z., Zhang, A., and Huang, H. (2023). Solving a class of non-convex minimax optimization in federated learning. In Thirty-seventh Conference on Neural Information Processing Systems.   \nXian, W., Huang, F., Zhang, Y, and Huang, H. (2021). A faster decentralized algorithm for nonconvex minimax problems. Advances in Neural Information Processing Systems, 34:25865-25877.   \nXiao, L., Boyd, S., and Lall, S. (2006). Distributed average consensus with time-varying metropolis weights. Automatica, 1:1-4.   \nYang, H., Liu, Z., Zhang, X., and Liu, J. (2022a). Sagda: Achieving $\\mathcal{O}\\left(\\varepsilon^{-2}\\right)$ communication complexity in federated min-max learning. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information Processing Systems, volume 35, pages 7142-7154. Curran Associates, Inc.   \nYang, J., Li, X., and He, N. (2022b). Nest your adaptive algorithm for parameter-agnostic nonconvex minimax optimization. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K., editors, Advances in Neural Information Processing Systems.   \nYang, J., Orvieto, A., Lucchi, A., and He, N. (2022c). Faster single-loop algorithms for minimax optimizationwithoutstrongcncavityInntenationalConferenceonAtifcialIntelligenceand Statistics, pages 5485-5517. PMLR.   \nYing, B., Yuan, K., Chen, Y., Hu, H., Pan, P, and Yin, W. (2021). Exponential graph is provably efcient for decentralized deep training. Advances in Neural Information Processing Systems, 34:13975-13987.   \nYuan, K., Ling, Q., and Yin, W. (2016). On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835-1854.   \nZhang, S., Choudhury, S., Stich, S. U., and Loizou, N. (2023). Communication-efficient gradient descent-accent methods for distributed variational inequalities: Unified analysis and local updates. arXiv preprint arXiv:2306.05100.   \nZhang, S., Yang, J, Guman, C., Kiyavash, N., and He, N. 2021a). The complexity of nonconvexstrongly-concave minimax optimization. In Uncertainty in Artificial Intelligence, pages 482-492. PMLR.   \nZhang, X., Liu, Z., Liu, J., Zhu, Z., and Lu, S. (2021b). Taming communication and sample complexities in decentralized policy evaluation for cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34:18825-18838.   \nZhang, X., Mancino-Ball, G., Aybat, N. S., and Xu, Y. (2024). Jointly improving the sample and communication complexities in decentralized stochastic minimax optimization. In Proceedings of the AAAI Conference on Artijficial Intelligence, volume 38, pages 20865-20873.   \nZhou, D., Chen, J., Cao, Y., Tang, Y., Yang, Z., and Gu, Q. (2018). On the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv: 1808.05671.   \nZou, F, Shen, L., Jie, Z., Zhang, W., and Liu, W.(2019). A sufficient condition for convergences of adam andrmsprop.InProceedings of theIEEE/CVF Conferenceoncomputervision and pattern recognition, pages 11127-11135. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A  Additional Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide detailed experimental settings and perform additional experiments on the task of training robust neural networks with different choices of hyper-parameters. All experiments are deployed in a server with Intel Xeon E5-2680 v4 CPU $\\textcircled{a}\\ 2.40\\mathrm{GHz}$ and 8 Nvidia RTX 3090 GPUs, and implemented using distributed communication package torch.distributed in PyTorch 2.0, where each process serves as a node, and we use inter-process communication to mimic communication between nodes. For the AdaGrad-like algorithms considered in the experiments of training neural networks, similar to the Adam-like stepsize, we adopt a coordinate-wise adaptive stepsize rule as commonly used in existing centralized adaptive methods (Yang et al., 2022b; Li et al., 2023). Moreover, since we attempt to develop a parameter-agnostic algorithm that does not need much effort in tuning hyper-parameters, we set $\\alpha=0.6$ and $\\beta=0.4$ for all tasks in the main text, and evaluate the effect of the choices of $\\alpha$ and $\\beta$ on the performance of D-AdaST individually in an additional experiment on the synthetic objective function as shown in Appendix A.4. ", "page_idx": 13}, {"type": "text", "text": "A.1 Experimental details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Communication topology. For the experiments in the main text, we utilize three commonly used communication topologies: indirect ring, exponential graph and dense graph. An indirect ring is a sparse graph in which each node is sequentially connected to form a ring, with only two neighbors per node. Exponential graph (Ying et al., 2021) is a directed graph where each node is connected to nodes at distances of $2^{\\tilde{0}},2^{\\tilde{1}}...,2^{\\log n}$ . Exponential graphs achieve a good balance between the degree and connectivity of the graph. A dense graph is an indirect graph where each node is connected to nodes at distances of $1,2,4,...,n$ We also consider directed ring and fully connected graphs, which are more sparsely and densely connected, respectively, in the additional experiments. ", "page_idx": 13}, {"type": "text", "text": "Robust training of neural network. In this task, we train CNNs with three convolutional layers and one fully connected layer on MNIST dataset containing images of 10 classes. Each layer adopts batch normalization and ELU activation. The total batch-size is 1280, and the batch-size of each node during training is $1280/n$ . For Adam-like algorithms, we set the first and second moment parameters as $\\beta_{1}=0.9,\\beta_{2}=0.999$ respectively. Since NeAda is a double-loop algorithm, for fair comparison, we imply D-AdaGrad and D-Adam using 15 iterations of inner loop in this task. ", "page_idx": 13}, {"type": "text", "text": "Generative Adversarial Networks. In this task, we train Wasserstein GANs on CIFAR-10 dataset, where the model used for discriminator is a four layer CNNs, and for generator is a four layer CNNs with transpose convolution layers. The total batch-size is 1280, and the batch-size of each node during training is 128 with 10 nodes. For Adam-like algorithms, we use $\\beta_{1}=0.5,\\beta_{2}=0.9$ .To obtain the inception score, we use 8000 artificially generated samples to feed the previously trained inceptionnetwork. ", "page_idx": 13}, {"type": "text", "text": "A.2  Additional experiments on robust training of neural network. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this part, we conduct additional experiments on robust training of CNNs on MNIST dataset considering a variety of settings. We compare the convergence performance of D-AdaST with D-AdaGrad, D-TiAda and D-NeAda using adaptive stepsizes of AdaGrad and Adam. Unless otherwise specified, the total batch-size is set to 1280; the initial stepsizes for $x$ and $y$ are assigned as $\\gamma_{x}=0.01$ \uff0c $\\gamma_{y}=0.1$ for AdaGrad-like algorithms, and $\\gamma_{x}=\\gamma_{y}=0.1$ for Adam-like algorithms. Specifically, we consider two extra graphs that are more sparse and more dense, respectively in Figure 5, e.g., directed ring and fully-connected (fc) graphs. We consider more initial stepsizes settings for $x$ and $y$ respectively in Figure 6. Further, we also consider different data distributions where each node has samples from 4 of the 10 classes in Figure 7. Finally, we perform a comparison experiment with 40 nodes in Figure 8. Under all settings, the proposed D-AdaST outperforms the others, demonstrating the superiority of D-AdaST. ", "page_idx": 13}, {"type": "text", "text": "A.3  Additional experiments on training GANs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide additional experiments of training GANs on a more complicated dataset CIFAR-100 to further illustrate the effectiveness of the proposed D-AdaST, as shown in Figure 9. We use the entire training set of CIFAR-100 with coarse labels (20 classes) to train GANs over networks, where each node is assigned with four distinct classes of labeled samples. Under the same settings as in ", "page_idx": 13}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/60226f8aed4937b7e0abb862b67343f27054eaa4c148b536e762dacb849ee928.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 5: Performance comparison of training CNN on MNIST with $n=20$ nodes over directed ring and fully connected graphs. ", "page_idx": 14}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/b2ff04118ba86ad84fb62089242c4ce5e3e73293ec4301864a4fa854133dc504.jpg", "img_caption": ["Figure 6: Performance comparison of training CNN on MNIST with $n=20$ nodes with different initial stepsizes x and y\u00b7 "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/2aadc9fae93ae9a20d721577b2ea0b2611ea301a8a67f15f01c1ec78ae7a4e53.jpg", "img_caption": ["Figure 7: Performance comparison of training CNN on MNIST with $n=20$ nodes over exponential and dense graphs where each node has 4 sample classes. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/74f4d81ce8fd96b5e3bd6033392f085d3ad32f4f6de04662b6328ae838fa3a4e.jpg", "img_caption": ["Figure 8: Performance comparison of training CNN on MNIST with $n=40$ nodes over exponential and dense graphs. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 4 (a), it can be observed that D-AdaST outperforms the others in terms of the inception score. Together with other experimental results in the main text, we believe that we have demonstrated the effectiveness of the proposed D-AdaST method and its potential for further real-world applications. ", "page_idx": 14}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/a1200e02a38aad7f13013e55bfbf497ded848719cc3907260c593b6bfdbe589e.jpg", "img_caption": ["Figure 9: Performance comparison of D-AdaST with D-Adam and D-TiAda adopting Adam-like stepsizes for training GANs on CIFAR-100 with coarse labels over the exponential graph consisting Oof $n=10$ nodes under initial stepsizes $\\gamma_{x}=\\gamma_{y}=0.001$ "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "O7IN4nsaIO/tmp/398eed01a2ec20c56b0b4b17b0dd882558b5c90608bc15fbd6b3f53df5f1fee2.jpg", "img_caption": ["Figure 10: Performance comparison of D-AdaST on quadratic functions over an exponential graph of $n=50$ nodes with different choices of $\\alpha$ and $\\beta$ "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Additional experiments with different choices of $\\alpha$ and $\\beta$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this part, we evaluate the effect of the choices of $\\alpha$ and $\\beta$ on the performance of D-AdaST. In particular, we provide an additional experimental result on the synthetic quadratic objective functions (15) with a larger ratio of initial stepsizes, i.e., $\\gamma_{x}/\\gamma_{y}\\,=\\,20\\$ (indicating faster minimization and slower maximization processes at the beginning). As shown in Figure 10, it can be observed that the transient time (iteration before the inflection point) becomes longer as $\\alpha-\\beta$ decreases, while the convergence rate is relatively faster, which is consistent with Theorem 2 and the result in the centralized TiAda algorithm (c.f., Figure 5, Li et al., 2023). ", "page_idx": 15}, {"type": "text", "text": "B Proof of the main results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We recall here some definitions used in the main text. The averaged variables and the inconsistency are defined as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{x}_{k}:=\\displaystyle\\frac{\\mathbf{1}^{T}}{n}\\mathbf{x}_{k},\\quad\\bar{v}_{k}:=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}v_{i,k},\\quad\\left(\\tilde{v}_{k}^{-\\alpha}\\right)^{T}:=\\left[\\cdot\\cdot\\cdot,v_{i,k}^{-\\alpha}-\\bar{v}_{k}^{-\\alpha},\\cdot\\cdot\\right],}\\\\ &{\\bar{y}_{k}:=\\displaystyle\\frac{\\mathbf{1}^{T}}{n}\\mathbf{y}_{k},\\quad\\bar{u}_{k}:=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}u_{i,k},\\quad\\left(\\tilde{u}_{k}^{-\\beta}\\right)^{T}:=\\left[\\cdot\\cdot\\cdot,u_{i,k}^{-\\beta}-\\bar{u}_{k}^{-\\beta},\\cdot\\cdot\\cdot\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The inconsistency of stepsizes of the primal and dual variables is defined as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\zeta_{v}^{2}:=\\operatorname*{sup}_{i\\in[n],k>0}\\left\\{\\left(v_{i,k}^{-\\alpha}-\\bar{v}_{k}^{-\\alpha}\\right)^{2}/\\left(\\bar{v}_{k}^{-\\alpha}\\right)^{2}\\right\\},\\ \\zeta_{u}^{2}:=\\operatorname*{sup}_{i\\in[n],k>0}\\left\\{\\left(u_{i,k}^{-\\beta}-\\bar{u}_{k}^{-\\beta}\\right)^{2}/\\left(\\bar{u}_{k}^{-\\beta}\\right)^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof Sketch. The convergence analysis of the main results in Theorem 2 is mainly based on carefully analyzing the average system as shown in (5), and the diference between the distributed system and the averaged system. In general, under Assumption 1-4, we first give a telescoped descent lemma from0to $K-1$ iterations in Lemma 3, which is upper bounded by the following key error terms: ", "page_idx": 16}, {"type": "text", "text": "\u00b7 $\\begin{array}{r}{S_{1}:=\\frac{1}{n K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\end{array}$ The asymtallyeayingy adopting adaptive stepsize;   \n\u00b7 $\\begin{array}{r}{S_{2}:=\\frac{1}{n K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{k}-\\mathbf{1}\\bar{x}_{k}\\right\\|^{2}+\\left\\|\\mathbf{y}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}\\right]}\\end{array}$ The consensus error of $x$ and $y$ between the distributed system and the average system;   \n\u00b7 $\\begin{array}{r}{S_{3}:=\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[f\\left(\\bar{x}_{k},y^{*}\\left(\\bar{x}_{k}\\right)\\right)-f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right]}\\end{array}$ The optimaltygapin dual variable $y$   \n$\\begin{array}{r}{\\bullet\\,\\,S_{4}:=\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]:}\\end{array}$ The inconsistency of stepsize of $x$ ", "page_idx": 16}, {"type": "text", "text": "Next, we prove the contraction properties of these terms in Lemma 4-8 and Lemma 9 respectively. Finally, these results are integrated into the descent lemma to complete the proof. We note that the proof is not trivial in the sense that these terms are coupled and therefore are needed to be carefully analyzed. This proof can also be adapted to analyze the coordinate-wise adaptive stepsize variant of D-AdaST as explained in Appendix B.5, which is of independent interest. ", "page_idx": 16}, {"type": "text", "text": "B.1  Supporting lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this part, we provide several supporting lemmas that have been shown in the existing literature, which are essential to the subsequent convergence analysis. ", "page_idx": 16}, {"type": "text", "text": "Lemma 1 (Lemma A.2 in Yang et al. (2022b)). Let $\\{x_{t}\\}_{t=0}^{T-1}$ be a sequence of non-negative real numbers, $x_{0}>0$ and $\\alpha\\in(0,1)$ . Then we have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\sum_{t=0}^{T-1}x_{t}\\right)^{1-\\alpha}\\leqslant\\sum_{t=0}^{T-1}\\frac{x_{t}}{\\left(\\sum_{k=0}^{t}x_{k}\\right)^{\\alpha}}\\leqslant\\frac{1}{1-\\alpha}\\left(\\sum_{t=0}^{T-1}x_{t}\\right)^{1-\\alpha}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When $\\alpha=0$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\frac{x_{t}}{\\left(\\sum_{k=0}^{t}x_{k}\\right)^{\\alpha}}\\leqslant1+\\log\\left(\\frac{\\sum_{t=0}^{T-1}x_{t}}{x_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 2. Suppose Assumption $^{\\,l}$ and 2 hold. Define $\\phi\\left(x\\right):=f\\left(x,y^{*}\\left(x\\right)\\right)$ as the envelope function and $y^{*}\\left(x\\right)=\\operatorname{argmax}_{y\\in\\mathcal{Y}}f\\left(x,y\\right)$ . Then, we have, ", "page_idx": 16}, {"type": "text", "text": "$\\varPhi\\left(\\cdot\\right)$ is $L_{\\varPhi}$ -smoothwith ${L}_{\\mathcal{P}}={L}\\left(1+\\kappa\\right)$ ,and $\\nabla\\varPhi\\left(x\\right)=\\nabla_{x}f\\left(x,y^{*}\\left(x\\right)\\right)$ $\\left.c.f.\\right.$ ,Lemma 4.3 in Lin et al. (2020)); \u00b7 $y^{\\ast}\\left(\\cdot\\right)$ .s $\\kappa$ -Lipschitz and $\\hat{L}$ smooth with $\\hat{L}=\\kappa\\,\\big(1+\\kappa\\big)^{2}(c.f.,$ Lemma 2 in Chen et al. (2021). ", "page_idx": 16}, {"type": "text", "text": "B.2  Key Lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this subsection, we give the key lemmas to help the analysis of the main results. For simplicity, wedefine $\\Delta_{k}:=\\|\\mathbf{x}_{k}-\\mathbf{1}\\bar{x}_{k}\\|^{2}+\\bar{\\|}\\mathbf{y}_{k}-\\mathbf{1}\\bar{y}_{k}\\|^{2}$ as the consensus eror for primal and dual variables. Then, we have the following lemmas. ", "page_idx": 16}, {"type": "text", "text": "Lemma 3 (Descent lemma). Suppose Assumption 1-4 hold. Then, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla\\phi\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leqslant\\frac{8C^{2\\alpha}(\\phi^{\\mathrm{max}}-\\phi^{*})}{\\gamma_{\\mathrm{z}}K^{1-\\alpha}}-\\frac{4}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla_{x}f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right\\|^{2}\\right]}\\\\ &{\\displaystyle+8\\gamma_{\\mathrm{z}}L_{\\phi}\\left(1+\\zeta_{\\mathrm{v}}^{2}\\right)\\frac{1}{\\underbrace{N K}}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{*}\\right)\\right\\|^{2}\\right]+8L\\underbrace{2}_{\\underbrace{N K}}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\varDelta_{k}\\right]}\\\\ &{\\displaystyle+8\\kappa L\\underbrace{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[f\\left(\\bar{x}_{k},y^{*}(\\bar{x}_{k})\\right)-f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right]}_{S_{3}}+16\\underbrace{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\left(\\bar{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{*}\\right)\\right\\|^{2}\\right]}_{\\mathbb{E}_{\\mathrm{\\tam}}^{-\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\kappa:=L/\\mu$ is the condition number of thefunction in $y$ $\\cdot\\Phi^{\\mathrm{max}}=\\operatorname*{max}_{x}\\,\\phi\\left(x\\right),\\Phi^{*}=\\operatorname*{min}_{x}\\,\\phi\\left(x\\right)\\!.$ ", "page_idx": 17}, {"type": "text", "text": "Proof. By the smoothness of $\\varPhi$ given in Lemma 2, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi\\left(\\bar{x}_{k+1}\\right)-\\phi\\left(\\bar{x}_{k}\\right)\\leqslant\\left\\langle\\nabla\\varPhi\\left(\\bar{x}_{k}\\right),\\bar{x}_{k+1}-\\bar{x}_{k}\\right\\rangle+\\frac{L_{\\phi}}{2}\\left\\Vert\\bar{x}_{k+1}-\\bar{x}_{k}\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and noticing that the scalar $\\bar{v}_{k},\\bar{u}_{k}$ are random variables, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\frac{\\displaystyle\\Phi\\left(\\bar{x}_{k+1}\\right)-\\varPhi\\left(\\bar{x}_{k}\\right)}{\\gamma_{x}\\bar{v}_{k+1}^{-\\alpha}}\\right]}\\\\ &{\\leqslant-\\mathbb{E}\\left[\\left\\langle\\nabla\\phi\\left(\\bar{x}_{k}\\right),\\frac{\\displaystyle\\mathbf{1}^{T}}{n}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)\\right\\rangle\\right]-\\mathbb{E}\\left[\\left\\langle\\nabla\\phi\\left(\\bar{x}_{k}\\right),\\frac{\\left(\\bar{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\rangle\\right]}\\\\ &{\\quad+\\frac{\\gamma_{x}L_{\\phi}}{2}\\mathbb{E}\\left[\\frac{1}{\\bar{v}_{k+1}^{-\\alpha}}\\left\\|\\left(\\frac{\\bar{v}_{k+1}^{-\\alpha}\\mathbf{1}^{T}}{n}+\\frac{\\left(\\bar{v}_{k+1}^{-\\alpha}\\right)^{T}}{n}\\right)\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have used the definition of $\\bar{x}_{k+1}$ as presented in (5). Then, we bound the inner-product terms on the RHS. Firstly, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{E}\\left[\\left\\langle\\nabla\\phi\\left(\\bar{x}_{k}\\right),\\frac{1^{T}}{n}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\rangle\\right]}\\\\ &{=-\\mathbb{E}\\left[\\left\\langle\\nabla\\phi\\left(\\bar{x}_{k}\\right),\\frac{1^{T}}{n}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right)-\\frac{\\mathbf{1}^{T}}{n}\\nabla_{x}F\\left(\\mathbf{1}\\bar{x}_{k},\\mathbf{1}\\bar{y}_{k}\\right)+\\frac{\\mathbf{1}^{T}}{n}\\nabla_{x}F\\left(\\mathbf{1}\\bar{x}_{k},\\mathbf{1}\\bar{y}_{k}\\right)\\right\\rangle\\right]}\\\\ &{\\leqslant\\frac{1}{4}\\mathbb{E}\\left[\\left\\|\\nabla\\phi\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]+\\mathbb{E}\\left[\\left\\|\\frac{\\mathbf{1}^{T}}{n}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right)-\\frac{\\mathbf{1}^{T}}{n}\\nabla_{x}F\\left(\\mathbf{1}\\bar{x}_{k},\\mathbf{1}\\bar{y}_{k}\\right)\\right\\|^{2}\\right]}\\\\ &{+\\frac{1}{2}\\left(\\mathbb{E}\\left[\\left\\|\\nabla\\phi\\left(\\bar{x}_{k}\\right)-\\nabla_{x}f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right\\|^{2}\\right]-\\mathbb{E}\\left[\\left\\|\\nabla\\phi\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]-\\mathbb{E}\\left[\\left\\|\\nabla_{x}f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right\\|^{2}\\right]\\right)}\\\\ &{\\leqslant-\\frac{1}{4}\\mathbb{E}\\left[\\left\\|\\nabla\\phi\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]+\\frac{L^{2}}{n}\\mathbb{E}\\left[\\left\\|\\bar{y}_{k}\\right\\|_{+}+\\frac{L^{2}}{2}\\mathbb{E}\\left[\\left\\|\\bar{y}_{k}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]-\\frac{1}{2}\\mathbb{E}\\left[\\left\\|\\nabla_{x}f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "wherein the last inequality we have used the smoothness of the objective functions. Then, for the second inner-product in (19), using Young's inequality we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\,\\mathbb{E}\\left[\\left\\langle\\nabla\\phi\\left(\\bar{x}_{k}\\right),\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\rangle\\right]}\\\\ &{\\leqslant\\frac{1}{8}\\mathbb{E}\\left[\\|\\nabla\\phi\\left(\\bar{x}_{k}\\right)\\|^{2}\\right]+2\\mathbb{E}\\left[\\left\\|\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, for the last term on the RHS of (18), recalling the definition of stepsize inconsistency in (8), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\gamma_{x}L_{\\phi}}{2}\\mathbb{E}\\left[\\frac{1}{\\overline{{v}}_{k+1}^{-\\alpha}}\\left\\|\\left(\\frac{\\bar{v}_{k+1}^{-\\alpha}\\mathbf{1}^{T}}{n}+\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n}\\right)\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leqslant\\frac{\\gamma_{x}L_{\\phi}}{n}\\frac{\\left(1+\\zeta_{v}^{2}\\right)}{n}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging the obtained inequalities into (18) and telescoping the terms, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k=0}{\\overset{K-1}{\\sum}}\\mathbb{E}\\left[\\|\\nabla\\phi(\\bar{x}_{k})\\|^{2}\\right]}\\\\ &{\\overset{K=}{\\lesssim}8\\sum_{k=0}^{-1}\\mathbb{E}\\left[\\frac{\\phi\\left(\\bar{x}_{k}\\right)-\\phi\\left(\\bar{x}_{k+1}\\right)}{\\gamma_{k}\\bar{x}_{k}^{-\\alpha}}\\right]-4\\underset{k=0}{\\overset{K-1}{\\sum}}\\mathbb{E}\\left[\\|\\nabla_{x}f(\\bar{x}_{k},\\bar{y}_{k})\\|^{2}\\right]}\\\\ &{+4L_{\\mathcal{X}}^{2}\\underset{k=0}{\\overset{K-1}{\\sum}}\\mathbb{E}\\left[\\|\\bar{y}_{k}-\\bar{y}^{*}\\|^{2}\\right]+\\frac{8L_{\\mathcal{X}}^{2}\\mathcal{K}-1}{n}\\mathbb{E}\\left[\\Delta_{k}\\right]}\\\\ &{+\\frac{8\\gamma_{k}L_{\\phi}\\left(1+\\zeta_{0}^{2}\\right)}{n}\\underset{k=0}{\\overset{K-1}{\\sum}}\\mathbb{E}\\left[\\bar{\\nu}_{k}^{-\\alpha}\\|\\nabla_{x}f(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{\\mathbb{K}})\\|^{2}\\right]}\\\\ &{+16\\underset{k=0}{\\overset{K-1}{\\sum}}\\mathbb{E}\\left[\\left\\|\\frac{\\left(\\frac{\\bar{x}_{k}^{-\\alpha}}{\\bar{x}_{k+1}^{-\\alpha}}\\right)^{T}}{n\\bar{x}_{k+1}^{-\\alpha}}\\nabla_{x}F(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{\\mathbb{K}})\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now it remains to bound the first term on the RHS of the above inequality. With the help of Assumption 3, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\frac{\\phi\\left(\\bar{x}_{k}\\right)-\\phi\\left(\\bar{x}_{k+1}\\right)}{\\gamma_{x}\\bar{v}_{k+1}^{-\\alpha}}\\right]}\\\\ &{\\displaystyle=\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\frac{\\phi\\left(\\bar{x}_{k}\\right)}{\\gamma_{x}\\bar{v}_{k}^{-\\alpha}}-\\frac{\\phi\\left(\\bar{x}_{k+1}\\right)}{\\gamma_{x}\\bar{v}_{k+1}^{-\\alpha}}+\\phi\\left(\\bar{x}_{k}\\right)\\left(\\frac{1}{\\gamma_{x}\\bar{v}_{k+1}^{-\\alpha}}-\\frac{1}{\\gamma_{x}\\bar{v}_{k}^{-\\alpha}}\\right)\\right]}\\\\ &{\\displaystyle\\leqslant\\mathbb{E}\\left[\\frac{\\phi_{\\operatorname*{max}}}{\\gamma_{x}\\bar{v}_{0}^{-\\alpha}}-\\frac{\\phi^{*}}{\\gamma_{x}\\bar{v}_{k}^{-\\alpha}}\\right]+\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\phi_{\\operatorname*{max}}\\left(\\frac{1}{\\gamma_{x}\\bar{v}_{k+1}^{-\\alpha}}-\\frac{1}{\\gamma_{x}\\bar{v}_{k}^{-\\alpha}}\\right)\\right]}\\\\ &{\\displaystyle\\leqslant\\frac{\\left(\\phi_{\\operatorname*{max}}-\\phi^{*}\\right)}{\\gamma_{x}}\\mathbb{E}\\left[\\bar{v}_{k}^{-\\alpha}\\right]}\\\\ &{\\displaystyle\\leqslant\\frac{\\left(\\phi_{\\operatorname*{max}}-\\phi^{*}\\right)\\left(K C^{2}\\right)^{\\alpha}}{\\gamma_{x}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Noticing that $\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\bar{y}_{k}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]\\leqslant\\frac{2}{\\mu}\\mathbb{E}\\left[f\\left(\\bar{x}_{k},y^{*}\\left(\\bar{x}_{k}\\right)\\right)-f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right]}\\end{array}$ we thus complete the proof. ", "page_idx": 18}, {"type": "text", "text": "Next, we need to bound the last four terms. $S_{1}{-}S_{4}$ in (18) respectively. For $S_{1}$ , we have the asymptotic convergence for both primal and dual variables in the following lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma 4. Suppose Assumption 1-4 hold. Then, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{n K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]\\leqslant\\frac{C^{2-2\\alpha}}{\\left(1-\\alpha\\right)K^{\\alpha}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{n K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\bar{u}_{k+1}^{-\\beta}\\left\\|\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\|^{2}\\right]\\leqslant\\frac{C^{2-2\\beta}}{\\left(1-\\beta\\right)K^{\\beta}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. With the help of Lemma 1 and Assumption 3, taking the primal variable $x$ as an example, and noticing that $v_{i,0}>0,i\\in[n]$ wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\overline{{v}}_{k+1}^{-\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{=\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\displaystyle\\left\\|\\nabla_{x}F_{i}\\left(x_{i},b_{i};\\xi_{i,k}^{x}\\right)\\right\\|^{2}}{\\bar{v}_{k+1}^{\\alpha}}}\\\\ &{\\leqslant\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\displaystyle\\left\\|\\nabla_{x}F_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{x}\\right)\\right\\|^{2}}{\\displaystyle\\left(\\sum_{k=0}^{K-1}\\frac{1}{n}\\sum_{j=1}^{n}\\left\\|\\nabla_{x}F_{j}\\left(x_{j,t},y_{j,t};\\xi_{j,k}^{x}\\right)\\right\\|^{2}\\right)^{\\alpha}}}\\\\ &{\\leqslant\\displaystyle\\frac{1}{1-\\alpha}\\frac{1}{K}\\left(\\sum_{k=0}^{K-1}\\frac{1}{n+1}\\|\\nabla_{x}F_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{x}\\right)\\|^{2}\\right)^{1-\\alpha}\\leqslant\\frac{C^{2-2\\alpha}}{(1-\\alpha)K^{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The similar result can be obtained for dual variable $y$ and we thus complete the proof. ", "page_idx": 19}, {"type": "text", "text": "Next, we bound the the consensus error term $S_{2}$ in the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma 5. Suppose Assumption 1-4 hold. Then, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\varDelta_{k}\\right]\\leqslant\\frac{2\\mathbb{E}\\left[\\varDelta_{0}\\right]}{\\left(1-\\rho_{W}\\right)K}}\\\\ &{\\displaystyle+\\,\\frac{8n\\rho_{W}\\gamma_{x}^{2}\\left(1+\\zeta_{v}^{2}\\right)}{\\left(1-\\rho_{W}\\right)^{2}}\\left(\\frac{C^{2-4\\alpha}}{\\left(1-2\\alpha\\right)K^{2\\alpha}}\\mathbb{I}_{\\alpha<1/2}+\\frac{1+\\log v_{K}-\\log v_{1}}{K\\bar{v}_{1}^{2\\alpha-1}}\\mathbb{I}_{\\alpha\\geqslant1/2}\\right)}\\\\ &{\\displaystyle+\\,\\frac{8n\\rho_{W}\\gamma_{y}^{2}\\left(1+\\zeta_{u}^{2}\\right)}{\\left(1-\\rho_{W}\\right)^{2}}\\left(\\frac{C^{2-4\\beta}}{\\left(1-2\\beta\\right)K^{2\\beta}}\\mathbb{I}_{\\beta<1/2}+\\frac{1+\\log u_{K}-\\log u_{1}}{K\\bar{u}_{1}^{2\\beta-1}}\\mathbb{I}_{\\beta\\geqslant1/2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbb{I}_{[\\cdot]}\\in\\{0,1\\}$ is the indicator for specific condition, and the initial consensus error $\\varDelta_{0}$ canbe set toOwith proper initialization. ", "page_idx": 19}, {"type": "text", "text": "Proof. By the updating rule of the primal variable, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{k+1}-\\mathbf{1}\\bar{x}_{k+1}\\right\\|^{2}\\right]}\\\\ &{\\mathrm{~}=\\mathbb{E}\\left[\\left\\|{W}\\left(\\mathbf{x}_{k}-\\gamma_{x}V_{k+1}^{-\\alpha}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right)-\\mathbf{J}\\left(\\mathbf{x}_{k}-\\gamma_{x}V_{k+1}^{-\\alpha}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right)\\right\\|^{2}\\right]}\\\\ &{\\mathrm{~}\\leqslant\\frac{1+\\rho_{W}}{2}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{k}-\\mathbf{1}\\bar{x}_{k}\\right\\|^{2}\\right]+\\frac{2\\gamma_{x}^{2}\\left(1+\\rho_{W}\\right)\\rho_{W}}{1-\\rho_{W}}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-2\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\mathrm{~}+\\frac{2\\gamma_{x}^{2}\\left(1+\\rho_{W}\\right)\\rho_{W}}{1-\\rho_{W}}\\mathbb{E}\\left[\\left\\|\\left(V_{k+1}^{-\\alpha}-\\bar{v}_{k+1}^{-\\alpha}\\mathbf{I}\\right)\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we have used Young's inequality. Then, by the definition of $\\zeta_{v}$ in (8), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\left(V_{k+1}^{-\\alpha}-\\bar{v}_{k+1}^{-\\alpha}\\mathbf{I}\\right)\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]\\leqslant\\zeta_{v}^{2}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-2\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{k+1}-\\mathbf{1}\\bar{x}_{k+1}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leqslant\\frac{2}{1-\\rho_{W}}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{k}-\\mathbf{1}\\bar{x}_{k}\\right\\|^{2}\\right]+\\frac{8\\gamma_{x}^{2}\\rho_{W}}{\\left(1-\\rho_{W}\\right)^{2}}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-2\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we bound the last term on the RHS of the above inequality by Lemma 4. For the case $\\alpha<1/2$ by Assumption 3we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-2\\alpha}\\left\\lVert\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\boldsymbol{\\xi}_{k}^{x}\\right)\\right\\rVert^{2}\\right]}\\\\ &{\\displaystyle=\\sum_{k=0}^{K-1}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{\\left\\lVert\\nabla_{x}F_{i}\\left(x_{i,k},y_{i,k};\\boldsymbol{\\xi}_{i,k}^{x}\\right)\\right\\rVert^{2}}{\\bar{v}_{k+1}^{2\\alpha}}\\right]\\leqslant\\frac{n\\left(K C^{2}\\right)^{1-2\\alpha}}{\\left(1-2\\alpha\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the case $\\alpha\\geqslant1/2$ , with the help of Lemma 1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-2\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\displaystyle=\\sum_{k=0}^{K-1}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{\\left\\|\\nabla_{x}F_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{x}\\right)\\right\\|^{2}}{\\bar{v}_{k+1}\\cdot\\bar{v}_{k+1}^{2\\alpha-1}}\\right]\\leqslant\\frac{n\\left(1+\\log{v_{T}}-\\log{v_{1}}\\right)}{\\bar{v}_{1}^{2\\alpha-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the dual variable, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{y}_{k+1}=\\mathcal{P}_{\\mathcal{Y}}\\left(W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right)\\right)}\\\\ {=W\\mathbf{y}_{k}+\\gamma_{y}\\nabla_{y}\\hat{G}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{y}\\hat{G}=\\frac{1}{\\gamma_{y}}\\left(\\mathcal{P}_{y}\\left(W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right)\\right)-W\\mathbf{y}_{k}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, using Young's inequality with parameter $\\lambda$ ,we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\mathbf{y}_{k+1}-\\mathbf{1}\\bar{y}_{k+1}\\right\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\bigg\\Vert W\\mathbf{y}_{k}+\\gamma_{y}\\nabla_{y}\\hat{G}-\\mathbf{J}\\left(W\\mathbf{y}_{k}+\\gamma_{y}\\nabla_{y}\\hat{G}\\right)\\bigg\\Vert^{2}\\right]}\\\\ &{\\leqslant(1+\\lambda)\\rho_{W}\\mathbb{E}\\left[\\left\\Vert\\mathbf{y}_{k}-\\mathbf{J}\\mathbf{y}_{k}\\right\\Vert^{2}\\right]}\\\\ &{+\\left(1+\\frac{1}{\\lambda}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathcal{P}_{y}\\left(W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right)\\right)-W\\mathbf{y}_{k}\\right\\Vert^{2}\\right]}\\\\ &{\\leqslant\\frac{1+\\rho_{W}}{2}\\mathbb{E}\\left[\\left\\Vert\\mathbf{y}_{k}-\\mathbf{J}\\mathbf{y}_{k}\\right\\Vert^{2}\\right]}\\\\ &{+\\frac{1+\\rho_{W}}{1-\\rho_{W}}\\mathbb{E}\\left[\\left\\Vert\\mathcal{P}_{y}\\left(W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right)\\right)-W\\mathbf{y}_{k}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Noticing that $W\\mathbf{y}_{k}=\\mathcal{P}_{\\mathcal{Y}}\\left(W\\mathbf{y}_{k}\\right)$ holds for convex set $\\boldsymbol{\\wp}$ , we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\mathbf{y}_{k+1}-\\mathbf{1}\\bar{y}_{k+1}\\right\\Vert^{2}\\right]}\\\\ &{\\leqslant\\frac{1+\\rho_{W}}{2}\\mathbb{E}\\left[\\left\\Vert\\mathbf{y}_{k}-\\mathbf{J}\\mathbf{y}_{k}\\right\\Vert^{2}\\right]}\\\\ &{+\\frac{1+\\rho_{W}}{1-\\rho_{W}}\\mathbb{E}\\left[\\left(\\left\\Vert\\mathcal{P}_{\\mathcal{P}}\\left(W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right)\\right)-\\mathcal{P}_{\\mathcal{P}}\\left(W\\mathbf{y}_{k}\\right)\\right\\Vert\\right)^{2}\\right]}\\\\ &{\\leqslant\\frac{1+\\rho_{W}}{2}\\mathbb{E}\\left[\\left\\Vert\\mathbf{y}_{k}-\\mathbf{J}\\mathbf{y}_{k}\\right\\Vert^{2}\\right]+\\frac{1+\\rho_{W}}{1-\\rho_{W}}\\mathbb{E}\\left[\\left\\Vert\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\leqslant\\frac{1+\\rho_{W}}{2}\\mathbb{E}\\left[\\left\\Vert\\mathbf{y}_{k}-\\mathbf{J}\\mathbf{y}_{k}\\right\\Vert^{2}\\right]+\\frac{4\\gamma_{y}^{2}}{\\left(1-\\rho_{W}\\right)}\\left(1+\\zeta_{u}^{2}\\right)\\mathbb{E}\\left[\\bar{u}_{k+1}^{-2\\beta}\\left\\Vert\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\Vert^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have used the non-expansiveness of projection operator. Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\mathbf{y}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leqslant\\frac{2}{1-\\rho_{W}}\\mathbb{E}\\left[\\left\\|\\mathbf{y}_{0}-\\mathbf{J}\\mathbf{y}_{0}\\right\\|^{2}\\right]+\\frac{8\\gamma_{y}^{2}\\left(1+\\zeta_{u}^{2}\\right)}{\\left(1-\\rho_{W}\\right)^{2}}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\bar{u}_{k+1}^{-2\\beta}\\left\\|\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\|^{2}\\right]\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similar to the primal variable, we can bound the last term above, which completes the proof. ", "page_idx": 21}, {"type": "text", "text": "Next, we need to bound the term $S_{3}$ i.e., the optimality gap in dual variable. The intuition of the proof relies on the adaptive two time-scale protocol, that is, for given $\\alpha$ and $\\beta$ , we try to find the threshold of the iterations $k_{0}$ , after which the inner sub-problem can be well solved (faster) to ensure that the computation of outer sub-problem can be solved accurately (slower). In specific, we suppose that there is a constant $G$ such that $\\bar{u}_{k}\\leqslant G$ hold for $k=0,1,\\cdot\\cdot\\cdot,k_{0}-1$ , then the analysis is divided into two phases. ", "page_idx": 21}, {"type": "text", "text": "Lemma 6 (First phase). Suppose Assumption 1-4 hold. If $\\bar{u}_{k}\\leqslant G,k=0,1,\\cdots,k_{0}-1$ thenwe have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{k_{0}-1}\\mathbb{E}\\left[f\\left(\\bar{x}_{k},y^{*}\\left(\\bar{x}_{k}\\right)\\right)-f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right]}\\\\ &{\\displaystyle\\leqslant\\sum_{k=0}^{k_{0}-1}\\mathbb{E}\\left[E_{1,k}\\right]+\\frac{\\gamma_{x}^{2}\\kappa^{2}\\left(1+\\zeta_{y}^{2}\\right)G^{2\\beta}}{n\\mu\\gamma_{y}^{2}}\\sum_{k=0}^{k_{0}-1}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-2\\alpha}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{\\cdot}\\right)\\right\\|^{2}\\right]}\\\\ &{+\\displaystyle\\frac{\\gamma_{y}}{n}\\left(1+\\zeta_{u}^{2}\\right)\\sum_{k=0}^{k_{0}-1}\\mathbb{E}\\left[\\bar{u}_{k+1}^{-\\beta}\\left\\|\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)\\right\\|^{2}\\right]+\\frac{4\\kappa L}{n}\\sum_{k=0}^{k_{0}-1}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{k}-\\mathbf{1}\\bar{x}_{k}\\right\\|^{2}\\right]}\\\\ &{+\\displaystyle\\frac{4}{\\mu}\\sum_{k=0}^{k_{0}-1}\\mathbb{E}\\left[\\bigg\\|\\frac{\\bar{u}_{k+1}^{-\\beta}}{n\\bar{u}_{k+1}^{-\\beta}}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{\\prime}\\right)\\bigg\\|^{2}\\right]+C\\displaystyle\\sum_{k=0}^{k_{0}-1}\\mathbb{E}\\left[\\sqrt{\\frac{1}{n}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\nE_{1,k}:=\\frac{1-3\\mu\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}/4}{2\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}n}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}-\\frac{\\left\\|\\mathbf{y}_{k+1}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k+1}\\right)\\right\\|^{2}}{\\left(2+\\mu\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}\\right)\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}n}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Using Young's inequality with parameter $\\lambda_{k}$ ,we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{n}\\left\\|\\mathbf{y}_{k+1}-\\mathbf{1}\\bar{y}^{*}\\left(\\bar{x}_{k+1}\\right)\\right\\|^{2}}\\\\ {\\displaystyle\\leqslant\\frac{\\left(1+\\lambda_{k}\\right)}{n}\\left\\|\\mathbf{y}_{k+1}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}+\\left(1+\\frac{1}{\\lambda_{k}}\\right)\\left\\|y^{*}\\left(\\bar{x}_{k}\\right)-y^{*}\\left(\\bar{x}_{k+1}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recalling that $\\mathbf{y}_{k+1}={\\mathcal{P}}_{\\mathcal{V}}\\left(W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};{\\boldsymbol{\\xi}}_{k}^{y}\\right)\\right)\\right)$ , we further define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{y}}_{k+1}=W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, for the first term on the RHS of (35), by the non-expansiveness property of projection operator $\\mathcal{P}_{\\mathcal{Y}}(\\cdot)$ (c.f., Lemma 1 in (Nedic et al., 2010)), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\left\\|\\mathbf{y}_{k+1}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}}\\\\ &{\\displaystyle\\leqslant\\frac{1}{n}\\left\\|\\hat{\\mathbf{y}}_{k+1}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}-\\frac{1}{n}\\left\\|\\mathbf{y}_{k+1}-\\hat{\\mathbf{y}}_{k+1}\\right\\|^{2}}\\\\ &{\\displaystyle\\leqslant\\frac{1}{n}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}+\\frac{\\gamma_{y}^{2}}{n}\\left\\|U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\|^{2}}\\\\ &{\\displaystyle-\\frac{1}{n}\\sum_{i=1}^{n}2\\left\\langle\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}g_{i,k}^{y},y_{i,k}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\rangle-\\frac{1}{n}\\sum_{i=1}^{n}2\\left\\langle\\gamma_{y}\\left(u_{i,k+1}^{-\\beta}-\\bar{u}_{k+1}^{-\\beta}\\right)g_{i,k}^{y},y_{i,k}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\rangle_{\\Omega_{c,1}^{-\\beta}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "wherein tlastequaltywehaeud t $\\|\\boldsymbol{W}\\|_{2}^{2}\\leqslant1$ Then, multiplying by $1/\\left(\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}\\right)$ on both sides of (35) we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\gamma\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\left\\|\\mathbf{y}_{k+1}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}}\\\\ &{\\leqslant\\frac{1+\\lambda_{k}}{\\lambda_{k}\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\left\\|\\bar{y}^{*}\\left(\\bar{x}_{k}\\right)-\\bar{y}^{*}\\left(\\bar{x}_{k+1}\\right)\\right\\|^{2}}\\\\ &{+\\left(1+\\lambda_{k}\\right)\\left(\\frac{1}{n\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}+\\frac{\\gamma_{y}}{n\\bar{u}_{k+1}^{-\\beta}}\\left\\|U_{k+1}^{-\\beta}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{\\prime}\\right)\\right\\|^{2}\\right)}\\\\ &{-\\left(1+\\lambda_{k}\\right)\\left(\\frac{1}{n}\\sum_{i=1}^{n}2\\left\\langle g_{i,k}^{\\prime},y_{i,k}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\rangle-\\frac{1}{n}\\sum_{i=1}^{n}2\\left\\langle\\left(\\frac{u_{i,k+1}^{-\\beta}-\\bar{u}_{k+1}^{-\\beta}}{\\bar{u}_{k+1}^{-\\beta}}\\right)g_{i,k}^{\\prime},y_{i,k}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\rangle\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the inner-product terms on the RHS, taking expectation on both sides, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[-2\\left(\\hat{\\eta}_{i}^{n},\\hat{\\eta}_{i},\\boldsymbol{k}_{i}-\\hat{\\eta}^{*}(\\boldsymbol{k}_{i})\\right)\\right]}\\\\ &{=\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[-2\\left(\\hat{\\eta}_{i}^{n},\\hat{\\eta}_{i}^{*}\\right),\\hat{\\eta}_{i},\\boldsymbol{k}_{i}-\\hat{\\eta}^{*}(\\boldsymbol{k}_{i})\\right]}\\\\ &{+\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[-2\\left(\\hat{\\eta}_{i}^{n},f_{i}\\left(\\hat{\\tau}_{i},\\hat{\\eta}_{i},\\hat{\\eta}_{i}\\right)-\\nabla_{\\mathbf{\\eta}}f_{i}\\left(\\hat{\\tau}_{i},\\hat{\\eta}_{i},\\hat{\\eta}_{i},\\hat{\\eta}_{i}-\\hat{\\eta}^{*}(\\boldsymbol{k}_{i})\\right)\\right]}\\\\ &{\\leq\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[-2\\left(\\hat{\\eta}_{i}^{n},f_{i}\\left(\\hat{\\tau}_{i},\\hat{\\eta}_{i}^{*}\\right)-f_{i}\\left(\\hat{\\tau}_{i},\\hat{\\eta}_{i},\\hat{\\eta}_{i}\\right)-\\mu\\left[\\hat{\\eta}_{i},\\hat{\\eta}_{i}-\\hat{\\eta}^{*}(\\boldsymbol{k}_{i})\\right]^{2}\\right)}\\\\ &{+\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{3}{n}\\nabla_{\\mathbf{\\eta}}f_{i}\\left(\\hat{\\tau}_{i},\\hat{\\eta}_{i},\\hat{\\eta}_{i}\\right)-\\nabla_{\\mathbf{\\eta}}f_{i}\\left(\\hat{\\tau}_{i},\\hat{\\eta}_{i},\\hat{\\eta}_{i}\\right)\\right]+\\frac{\\beta}{n}\\left[\\|\\hat{\\eta}_{i},\\hat{\\eta}_{i}-\\hat{\\eta}^{*}(\\boldsymbol{k}_{i})\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[-2\\left(\\hat{\\eta}_{i}^{n},\\hat{\\eta}_{i}^{*}\\right)+f_{i}\\left(\\hat{\\tau}_{i},\\hat{\\eta}_\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we have used Young's inequality and strong-concavity of $f_{i}$ ,and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[-2\\left<\\left(\\frac{u_{i,k+1}^{-\\beta}-\\bar{u}_{k+1}^{-\\beta}}{\\bar{u}_{k+1}^{-\\beta}}\\right)g_{i,k}^{y},y_{i,k}-y^{*}\\left(\\bar{x}_{k}\\right)\\right>\\right]}\\\\ &{\\leqslant\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{8}{\\mu}\\left\\|\\left(\\frac{u_{i,k+1}^{-\\beta}-\\bar{u}_{k+1}^{-\\beta}}{\\bar{u}_{k+1}^{-\\beta}}\\right)g_{i,k}^{y}\\right\\|^{2}+\\frac{\\mu}{8}\\left\\|y_{i,k}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the consensus error of dual variable on the objective function, using strong-concavity of $f_{i}$ and Jensen's inequality, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}-2\\left(f_{i}\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)-f_{i}\\left(\\bar{x}_{k},y_{i,k}\\right)\\right)}\\\\ {\\displaystyle}\\\\ &{\\displaystyle\\leqslant\\frac{1}{n}\\sum_{i=1}^{n}2\\left\\langle\\nabla_{y}f_{i}\\left(\\bar{x}_{k},\\bar{y}_{k}\\right),y_{i,k}-\\bar{y}_{k}\\right\\rangle-\\displaystyle\\frac{\\mu}{n}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}}\\\\ {\\displaystyle}\\\\ &{\\displaystyle\\leqslant2C\\frac{1}{n}\\sum_{i=1}^{n}\\|y_{i,k}-\\bar{y}_{k}\\|\\leqslant2C\\sqrt{\\frac{1}{n}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Letting $\\lambda_{k}=\\mu\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}/2$ , we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\bar{x}_{k},\\bar{y}^{*}(\\bar{x}_{k})\\right)-f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right]}\\\\ &{\\leqslant\\mathbb{E}\\left[\\frac{1-3\\mu\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}/4}{2\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\left\\|y_{k}-1y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}-\\frac{\\left\\|y_{k+1}-1y^{*}\\left(\\bar{x}_{k+1}\\right)\\right\\|^{2}}{\\left(2+\\mu\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}\\right)\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}n}\\right]}\\\\ &{+\\frac{\\gamma_{x}^{2}\\kappa^{2}\\left(1+\\zeta_{y}^{2}\\right)G^{2\\beta}}{n\\mu\\gamma_{y}^{\\prime}}\\mathbb{E}\\left[\\bar{v}_{k+1}^{-2\\alpha}\\left\\|\\nabla_{x}F\\left({\\mathbf x}_{k},{\\mathbf y}_{k};\\xi_{k}^{\\varepsilon}\\right)\\right\\|^{2}\\right]}\\\\ &{+\\frac{\\gamma_{y}}{n}\\left(1+\\zeta_{u}^{2}\\right)\\underset{i=1}{^{\\varepsilon}}\\mathbb{E}\\left[\\bar{u}_{k+1}^{-\\beta}\\left\\|\\nabla_{y}F\\left({\\mathbf x}_{k},{\\mathbf y}_{k};\\xi_{k}\\right)\\right\\|^{2}\\right]+\\frac{4\\kappa L}{n}\\mathbb{E}\\left[\\left\\|{\\mathbf x}_{k}-1\\bar{y}_{k}\\right\\|^{2}\\right]}\\\\ &{+\\frac{4}{\\mu}\\mathbb{E}\\left[\\left\\|\\frac{\\bar{u}_{k+1}^{-\\beta}}{n\\bar{u}_{k+1}^{-\\beta}}\\nabla_{y}F\\left({\\mathbf x}_{k},{\\mathbf y}_{k};\\xi_{k}^{\\varepsilon}\\right)\\right\\|^{2}\\right]+C\\mathbb{E}\\left[\\sqrt{\\frac{1}{n}\\left\\|{\\mathbf y}_{k}-1\\bar{y}_{k}\\right\\|^{2}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By the $\\kappa$ -smoothness of $y^{\\ast}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|y^{*}\\left(\\bar{x}_{k+1}\\right)-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}}\\\\ &{\\leqslant\\kappa^{2}\\left\\|\\bar{x}_{k+1}-\\bar{x}_{k}\\right\\|^{2}}\\\\ &{=\\kappa^{2}\\left\\|\\gamma_{x}\\bar{v}_{k+1}^{-\\alpha}\\displaystyle\\frac{\\mathbf{1}^{T}}{n}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)-\\gamma_{x}\\displaystyle\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}}\\\\ &{\\leqslant\\frac{2\\gamma_{x}^{2}\\kappa^{2}\\left(1+\\zeta_{v}^{2}\\right)\\bar{v}_{k+1}^{-2\\alpha}}{n}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Telescoping the obtained terms from 0 to $k_{0}-1$ and noticing that $\\bar{u}_{k}\\,\\leqslant\\,G$ for $k\\,\\leqslant\\,k_{0}\\,-\\,1$ we complete the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "For the second phase, i.e., $k\\geqslant k_{0}$ , we have the following lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma 7 (Second phase). Suppose Assumption 1-4 hold. If $\\bar{u}_{k}\\leqslant G,k=0,1,\\cdots,k_{0}-1$ thenwe have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t=0}{\\overset{k-1}{\\sum}}\\mathbb{E}\\left[f\\left(\\bar{x}_{t},y^{*}(\\bar{x}_{t})\\right)-f\\left(\\bar{x}_{t},\\bar{y}_{t}\\right)\\right]}\\\\ &{\\underset{t=0}{\\overset{k-1}{\\sum}}\\mathbb{E}\\left[f\\left(\\bar{x}_{t},\\bar{y}^{*}(\\bar{x}_{t})\\right)-\\frac{\\mathbb{E}\\left(\\bar{x}_{t}^{2}\\right)}{\\mu\\gamma_{t}\\sigma_{t}^{2}\\sigma^{2}\\sigma^{2}\\sigma^{2}}\\right]\\underset{\\underset{t=0}{\\overset{k-1}{\\sum}}}{\\sum}\\int_{\\mathbb{D}_{\\theta}}f\\left(\\bar{x}_{t},\\bar{y}_{t}\\right)\\|^{2}}\\\\ &{+\\left(\\frac{\\mathbb{E}\\gamma_{t}^{2}\\sigma^{2}\\bar{L}^{2}\\left(1+\\zeta_{t}^{2}\\right)}{\\mu\\gamma_{t}\\sigma_{t}^{2}\\sigma^{2}\\sigma^{2}\\sigma^{2}}+\\frac{4\\zeta_{t}}{\\mu\\gamma}\\right)\\underset{t=0}{\\overset{k-1}{\\sum}}\\mathbb{E}\\left[\\Delta_{i}\\right]}\\\\ &{+\\frac{\\gamma_{t}\\left(1+\\zeta_{t}^{2}\\right)}{\\mu}\\mathbb{E}\\left[\\bar{u}_{t}^{-3}\\left|\\nabla_{y}F\\left(x_{t},y_{t};\\bar{x}_{t}\\right)\\right|^{2}\\right]+C\\underset{t=\\lambda_{0}}{\\overset{k-1}{\\sum}}\\mathbb{E}\\left[\\sqrt{\\frac{1}{n}\\|y_{t}-1\\bar{y}_{t}\\|^{2}}\\right]}\\\\ &{+\\frac{\\gamma_{t}^{2}\\left(1+\\zeta_{t}^{2}\\right)}{\\gamma_{t}\\sigma_{t}^{2}\\sigma^{2}}\\left(\\kappa^{2}+\\frac{2\\gamma_{t}^{2}\\sigma^{2}\\left(1+\\zeta_{t}^{2}\\right)C^{2}\\bar{L}^{2}}{\\mu\\gamma_{t}\\sigma_{t}^{2}\\sigma^{2}}\\right)\\underset{t=0}{\\overset{k-1}{\\sum}}\\Bigg[\\frac{\\bar{x}_{t}^{-3}}{n}\\bigg\\|\\nabla_{x}F\\left(x_{t},y_{t};\\bar{x}_{t}\\right)\\bigg\\|^{2}\\Bigg]}\\\\ &{+\\frac{ \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Firstly, by the non-expansiveness of projection operator, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|y_{i,k+1}-y^{*}\\left(\\bar{x}_{k+1}\\right)\\right\\|^{2}}\\\\ &{\\leqslant\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k+1}\\right)\\right\\|^{2}-\\left\\|y_{i,k+1}-\\hat{y}_{i,k+1}\\right\\|^{2}}\\\\ &{=\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}+\\left\\|y^{*}\\left(\\bar{x}_{k+1}\\right)-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}}\\\\ &{-\\;2\\left\\langle\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right),y^{*}\\left(\\bar{x}_{k+1}\\right)-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\rangle}\\\\ &{=\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}+\\left\\|y^{*}\\left(\\bar{x}_{k+1}\\right)-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}}\\\\ &{-\\;2\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\bar{x}_{k+1}-\\bar{x}_{k}\\right)^{T}}\\\\ &{-\\;2\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\left(y^{*}\\left(\\bar{x}_{k+1}\\right)-y^{*}\\left(\\bar{x}_{k}\\right)-\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\bar{x}_{k+1}-\\bar{x}_{k}\\right)^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, for the frst inner-product term on the RHS, letting $\\nabla_{x}\\tilde{F}_{k}=\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)-\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right),$ weget ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;2\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\bar{x}_{k+1}-\\bar{x}_{k}\\right)^{T}}\\\\ &{=2\\gamma_{x}\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\nabla_{x}F\\left({\\mathbf x}_{k},{\\mathbf y}_{k}\\right)\\right)^{T}\\left(\\frac{1\\bar{v}_{k+1}^{-\\alpha}}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n}\\right)}\\\\ &{+\\;2\\gamma_{x}\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\nabla_{x}\\tilde{F}_{k}\\right)^{T}\\left(\\frac{1\\bar{v}_{k+1}^{-\\alpha}}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n}\\right)}\\\\ &{\\leqslant2\\gamma_{x}\\kappa\\left\\Vert\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\Vert\\left\\Vert\\left(\\nabla_{x}F\\left({\\mathbf x}_{k},{\\mathbf y}_{k}\\right)\\right)^{T}\\left(\\frac{1\\bar{v}_{k+1}^{-\\alpha}}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n}\\right)\\right\\Vert}\\\\ &{+\\;2\\gamma_{x}\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\nabla_{x}\\tilde{F}_{k}\\right)^{T}\\left(\\frac{1\\bar{v}_{k+1}^{-\\alpha}}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "wherein the last inequality we have used the fact that $y^{\\ast}$ is $\\kappa$ -Lipschitz. Then, using Young's inequality with parameter $\\lambda_{k}$ ,weget ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\left.2\\left(\\widehat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\bar{x}_{k+1}-\\bar{x}_{k}\\right)^{T}}\\\\ &{\\leqslant\\lambda_{k}\\left\\|\\widehat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}}\\\\ &{+\\left.\\frac{2\\gamma_{x}^{2}\\bar{v}_{k+1}^{-2\\alpha}\\kappa^{2}}{\\lambda_{k}}\\left(\\left\\|\\frac{\\mathbf{1}^{T}}{n}\\nabla_{x}F\\left({\\mathbf x}_{k},{\\mathbf y}_{k}\\right)\\right\\|^{2}+\\left\\|\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left({\\mathbf x}_{k},{\\mathbf y}_{k}\\right)\\right\\|^{2}\\right)}\\\\ &{+\\left.2\\gamma_{x}\\left(\\widehat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\nabla_{x}\\tilde{F}_{k}\\right)^{T}\\left(\\frac{\\mathbf{1}\\bar{v}_{k+1}^{-\\alpha}}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the second inner-product term on the RHS, noticing that $y^{\\ast}$ is $\\hat{L}=\\kappa\\left(1+\\kappa\\right)^{2}$ smooth given in Lemma 2, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\left(y^{*}\\left(\\bar{x}_{k}\\right)-y^{*}\\left(\\bar{x}_{k+1}\\right)+\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\bar{x}_{k+1}-\\bar{x}_{k}\\right)^{T}\\right)}\\\\ &{\\leqslant2\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|\\left\\|y^{*}\\left(\\bar{x}_{k}\\right)-y^{*}\\left(\\bar{x}_{k+1}\\right)+\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\bar{x}_{k+1}-\\bar{x}_{k}\\right)\\right\\|^{2}}\\\\ &{\\leqslant2\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|\\frac{\\hat{Z}}{2}\\left\\|\\bar{x}_{k+1}-\\bar{x}_{k}\\right\\|^{2}}\\\\ &{\\leqslant\\gamma_{x}^{2}\\hat{L}\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|\\left\\|\\left(\\frac{\\bar{v}_{k+1}^{-\\alpha}-1}{n}\\frac{\\left(\\bar{v}_{k+1}^{-\\alpha}\\right)^{T}}{n}\\right)\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}}\\\\ &{\\leqslant\\gamma_{x}^{2}\\hat{L}\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|\\frac{2\\bar{v}_{k+1}^{-2\\alpha}}{n}\\left(1+\\zeta_{v}^{2}\\right)C\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|}\\\\ &{\\leqslant\\gamma_{x}^{2}\\bar{\\hat{L}}\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|\\frac{2\\bar{v}_{k+1}^{-2\\alpha}}{n}\\left(1+\\zeta_{v}^{2}\\right)^{C}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|}\\\\ &{\\leqslant\\tau\\gamma_{x}^{2}\\bar{v}_ \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "wherein the last inequality we have used Young's inequality with parameter $\\tau$ . Plugging the obtained inequalities into (44), we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|y_{i,k+1}-y^{*}(\\bar{x}_{k+1})\\right\\|^{2}}\\\\ &{\\leqslant\\left(1+\\lambda_{k}+\\tau\\gamma_{x}^{2}\\bar{v}_{k+1}^{-2\\alpha}\\left(1+\\zeta_{v}^{2}\\right)C^{2}\\hat{L}\\right)\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}}\\\\ &{+\\frac{\\gamma_{x}^{2}\\bar{v}_{k+1}^{-2\\alpha}\\left(1+\\zeta_{v}^{2}\\right)}{n}\\left(2\\kappa^{2}+\\displaystyle\\frac{\\hat{L}}{\\tau}\\right)\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)\\right\\|^{2}}\\\\ &{+\\frac{2\\gamma_{x}^{2}\\bar{v}_{k+1}^{-2\\alpha}\\kappa^{2}}{\\lambda_{k}}\\left(\\left\\|\\frac{1^{T}}{n}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right)\\right\\|^{2}+\\left\\|\\frac{\\left(\\bar{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right)\\right\\|^{2}\\right)}\\\\ &{+\\left.2\\gamma_{x}\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\nabla_{x}\\tilde{F}\\right)^{T}\\left(\\frac{1\\bar{v}_{k+1}^{-\\alpha}}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Setting the parameters for Young's inequalities we used as follows, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lambda_{k}=\\frac{\\mu\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}{4},\\quad\\tau=\\frac{\\mu\\gamma_{y}\\bar{v}_{0}^{2\\alpha-\\beta}}{4\\gamma_{x}^{2}\\left(1+\\zeta_{v}^{2}\\right)C^{2}\\hat{L}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|y_{i,k+1}-y^{*}\\left(\\bar{x}_{k+1}\\right)\\right\\|^{2}}\\\\ &{\\leqslant\\left(1+\\frac{\\mu\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}{2}\\right)\\left\\|\\bar{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}}\\\\ &{+\\frac{\\gamma_{x}^{2}\\left(1+\\zeta_{\\sigma}^{2}\\right)}{n}\\left(2\\kappa^{2}+\\frac{4\\gamma_{x}^{2}}{\\mu\\gamma_{y}\\bar{v}_{0}^{2}-\\beta}\\right)\\bar{v}_{k+1}^{-2\\alpha}\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)\\|^{2}}\\\\ &{+\\frac{8\\gamma_{x}^{2}\\bar{v}_{k+1}^{-2\\alpha}\\kappa^{2}}{\\mu\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\left(\\left\\|\\frac{1}{m}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right)\\right\\|^{2}+\\left\\|\\frac{\\left(\\bar{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right)\\right\\|^{2}\\right)}\\\\ &{+\\left.2\\gamma_{x}\\left(\\bar{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\nabla_{x}\\bar{F}_{k}\\right)^{T}\\left(\\frac{1\\bar{v}_{k+1}^{-\\alpha}}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Recalling that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{1}{\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\left\\|\\hat{y}_{i,k+1}-\\bar{y}^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leqslant\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{1-3\\mu\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}/4}{\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\left\\|y_{i,k}-\\bar{y}^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]+\\frac{8\\kappa L}{n}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle+\\frac{2\\gamma_{y}}{n}\\left(1+\\zeta_{u}^{2}\\right)\\mathbb{E}\\left[\\frac{\\left\\|\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)\\right\\|^{2}}{\\bar{u}_{k+1}^{-\\beta}}-\\mathbb{E}\\left[2\\left(f\\left(\\bar{x}_{k},\\bar{y}^{*}\\left(\\bar{x}_{k}\\right)\\right)-f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right)\\right]}\\\\ &{\\displaystyle+\\frac{8}{\\mu}\\mathbb{E}\\left[\\left\\|\\frac{\\bar{u}_{k+1}^{-\\beta}}{n\\bar{u}_{k+1}^{-\\beta}}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{\\nu}\\right)\\right\\|^{2}\\right]+2C\\mathbb{E}\\left[\\sqrt{\\displaystyle\\frac{1}{n}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and multiplying by (2+\u03bcryu)yu on both sides of (50), we obtain that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\bar{x}_{k},\\bar{y}^{*}\\left(\\bar{x}_{k}\\right)\\right)-f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right]}\\\\ &{\\leqslant\\mathbb{E}\\left[E_{1,k}\\right]+\\frac{\\gamma_{y}\\left(1+\\zeta_{u}^{2}\\right)}{n}\\mathbb{E}\\left[\\bar{u}_{k+1}^{-\\beta}\\left\\|\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)\\right\\|^{2}\\right]+\\frac{4\\kappa L}{n}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}\\right]}\\\\ &{+\\frac{4}{\\mu}\\mathbb{E}\\left[\\left\\|\\frac{\\bar{u}_{k+1}^{-\\beta}}{n\\bar{u}_{k+1}^{-\\beta}}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\|^{2}\\right]+C\\mathbb{E}\\left[\\sqrt{\\frac{1}{n}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}}\\right]}\\\\ &{+\\underbrace{\\mathbb{E}\\left[\\frac{4\\gamma_{x}^{2}\\bar{v}_{k+1}^{-2\\alpha}\\kappa^{2}}{\\mu\\gamma_{y}^{2}\\bar{u}_{k+1}^{-2\\beta}}\\left(\\left\\|\\frac{\\mathbf{1}^{T}}{n}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right)\\right\\|^{2}+\\left\\|\\frac{\\left(\\bar{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right)\\right\\|^{2}\\right)\\right]}_{\\mu\\gamma_{y}^{2}\\bar{u}_{k+1}^{-2\\beta}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n+\\left.\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{\\gamma_{x}}{\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\nabla_{x}\\tilde{F}_{k}\\right)^{T}\\left(\\frac{\\mathbf{1}\\bar{v}_{k+1}^{-\\alpha}}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n}\\right)\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Telescoping the terms from $t_{0}$ to $K-1$ , we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[f\\left(\\bar{x}_{k},\\bar{y}^{*}\\left(\\bar{x}_{k}\\right)\\right)-f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right]}\\\\ &{\\displaystyle\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[E_{1,k}\\right]+\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[E_{2,k}\\right]+\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[E_{3,k}\\right]+\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[E_{4,k}\\right]}\\\\ &{\\displaystyle+\\frac{\\gamma_{y}}{n}\\left(1+\\zeta_{u}^{2}\\right)\\mathbb{E}\\left[\\bar{u}_{k+1}^{-\\beta}\\left\\|\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)\\right\\|^{2}\\right]+\\frac{4\\kappa L}{n}\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle+\\frac{4}{\\mu}\\mathbb{E}\\left[\\left\\|\\frac{\\bar{u}_{k+1}^{-\\beta}}{n\\bar{u}_{k+1}^{-\\beta}}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{\\nu}\\right)\\right\\|^{2}\\right]+{\\cal C}\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[\\sqrt{\\frac{1}{n}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}\\bar{y}_{k}\\right\\|^{2}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Next we need to further bound the running sums of $\\mathbb{E}\\left[E_{2,k}\\right],\\mathbb{E}\\left[E_{3,k}\\right]$ and E $\\left[E_{4,k}\\right]$ respectively.For $\\mathbb{E}\\left[E_{2,k}\\right]$ , with the help of Assumption 2 and noticing that $\\bar{u}_{k}\\leqslant G,k=0,1,\\cdots,k_{0}-1$ ,we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[E_{2,k}\\right]}\\\\ &{\\lesssim\\displaystyle\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[\\frac{4\\gamma_{x}^{2}\\bar{v}_{k+1}^{-2\\alpha}\\kappa^{2}}{\\mu\\gamma_{y}^{2}\\bar{u}_{k+1}^{-2\\beta}}\\left(\\left\\|\\frac{{\\bf1}^{T}}{n}\\nabla_{x}F\\left({\\bf x}_{k},{\\bf y}_{k}\\right)\\right\\|^{2}+\\left\\|\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left({\\bf x}_{k},{\\bf y}_{k}\\right)\\right\\|^{2}\\right)\\right]}\\\\ &{\\lesssim\\displaystyle\\frac{8\\gamma_{x}^{2}\\kappa^{2}\\left(1+\\zeta_{v}^{2}\\right)}{\\mu\\gamma_{y}^{2}G^{2\\alpha-2\\beta}}\\displaystyle\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla_{x}f\\left(\\bar{x}_{k},\\bar{y}_{k}\\right)\\right\\|^{2}+\\displaystyle\\frac{L^{2}}{n}\\varDelta_{k}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, for the term $\\mathbb{E}\\left[E_{3,k}\\right]$ , noticing that $\\bar{u}_{k+1}\\leqslant\\bar{v}_{k+1}$ and $\\bar{v}_{k+1}\\geqslant\\bar{v}_{1}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[E_{3,k}\\right]}\\\\ &{\\lesssim\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[\\frac{\\gamma_{x}^{2}\\left(1+\\zeta_{v}^{2}\\right)}{n\\gamma_{y}}\\left(\\kappa^{2}+\\frac{2\\gamma_{x}^{2}\\left(1+\\zeta_{v}^{2}\\right)C^{2}\\hat{L}^{2}}{\\mu\\gamma_{y}\\bar{\\nu}_{1}^{2\\alpha-\\beta}}\\right)\\frac{\\bar{v}_{k+1}^{-2\\alpha}}{\\bar{u}_{k+1}^{-\\beta}}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leqslant\\frac{\\gamma_{x}^{2}\\left(1+\\zeta_{v}^{2}\\right)}{\\gamma_{y}\\bar{v}_{1}^{-\\beta}}\\left(\\kappa^{2}+\\frac{2\\gamma_{x}^{2}\\left(1+\\zeta_{v}^{2}\\right)C^{2}\\hat{L}^{2}}{\\mu\\gamma_{y}\\bar{v}_{1}^{2\\alpha-\\beta}}\\right)\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[\\frac{\\bar{v}_{k+1}^{-\\alpha}}{n}\\left\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the term $E_{4,k}$ , we denote ", "page_idx": 27}, {"type": "equation", "text": "$$\ne_{k}:=\\frac{\\gamma_{x}}{\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right)^{T}\\right)\\nabla y^{*}\\left(\\bar{x}_{k}\\right)\\left(\\nabla_{x}\\tilde{F}_{k}\\right)^{T}\\left(\\frac{1}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n\\bar{v}_{k+1}^{-\\alpha}}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left.e_{k}\\right|\\leqslant\\frac{\\gamma_{x}\\kappa}{\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\hat{y}_{i,k+1}-y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|\\left\\|\\left(\\nabla_{x}\\tilde{F}_{k}\\right)^{T}\\left(\\frac{1}{n}+\\frac{\\tilde{v}_{k+1}^{-\\alpha}}{n\\bar{v}_{k+1}^{-\\alpha}}\\right)\\right\\|}\\\\ &{\\quad\\leqslant\\frac{\\gamma_{x}\\kappa\\left(1+\\zeta_{v}\\right)}{\\gamma_{y}\\sqrt{n}\\bar{u}_{k+1}^{-\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\mu}\\left\\|\\nabla_{y}f\\left(\\bar{x}_{k},\\hat{y}_{i,k+1}\\right)-\\nabla_{y}f\\left(\\bar{x}_{k},y^{*}\\right)\\right\\|\\right)\\left\\|\\nabla_{x}\\tilde{F}\\right\\|}\\\\ &{\\quad\\leqslant\\underbrace{\\frac{2\\gamma_{x}\\kappa\\left(1+\\zeta_{v}\\right)C^{2}\\bar{u}_{K}^{\\beta}}{\\mu\\gamma_{y}}}_{M},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we have used the Lipschitz continuity of $y^{\\ast}$ given in Lemma 2 and Assumption 3. Then, noticing that $\\mathbb{E}\\left[\\nabla_{x}\\tilde{F}_{k}\\right]=0$ we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[E_{4,k}\\right]=\\displaystyle\\sum_{k=k_{0}}^{K-1}\\mathbb{E}\\left[e_{k}\\bar{v}_{k+1}^{-\\alpha}\\right]}\\\\ &{\\phantom{\\sum_{k=1}^{K-1}}=\\mathbb{E}\\left[e_{k}\\bar{v}_{k+1}^{-\\alpha}\\right]+\\displaystyle\\sum_{k\\leq k_{0}+1}^{K-1}\\mathbb{E}\\left[e_{k}\\bar{v}_{k}^{-\\alpha}\\right]+\\sum_{k=k_{0}+1}^{K-1}\\mathbb{E}\\left[-e_{k}\\Big(\\bar{v}_{k}^{-\\alpha}-\\bar{v}_{k+1}^{-\\alpha}\\Big)\\right]}\\\\ &{\\phantom{\\sum_{k=1}^{K-1}}\\leqslant\\mathbb{E}\\left[M\\bar{v}_{k+1}^{-\\alpha}\\right]+\\displaystyle\\sum_{k=k_{0}+1}^{K-1}\\mathbb{E}\\left[M\\left(\\bar{v}_{k}^{-\\alpha}-\\bar{v}_{k+1}^{-\\alpha}\\right)\\right]}\\\\ &{\\leqslant2\\mathbb{E}\\left[M\\bar{v}_{k+1}^{-\\alpha}\\right]\\leqslant\\displaystyle\\frac{4\\gamma\\kappa\\kappa(1+\\zeta_{v})C^{2}}{\\mu\\gamma_{v}\\bar{v}_{k}^{\\alpha}}\\mathbb{E}\\left[\\bar{u}_{k}^{\\beta}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, combining the obtained inequalities, we complete the proof. ", "page_idx": 27}, {"type": "text", "text": "Now, it remains to bound the term $\\ensuremath{{\\cal E}}_{1,k}$ ", "page_idx": 27}, {"type": "text", "text": "Lemma 8. Suppose Assumption $^{\\,l}$ -4 hold. Then, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K-1}\\mathbb{E}\\left[E_{1,k}\\right]\\leqslant\\frac{1}{2\\gamma_{y}\\bar{u}_{1}^{-\\beta}n}\\left\\|\\mathbf{y}_{0}-\\mathbf{1}y^{*}\\left(\\bar{x}_{0}\\right)\\right\\|^{2}+\\frac{2\\left(4\\beta C^{2}\\right)^{2+\\frac{1}{1-\\beta}}}{\\mu^{3+\\frac{1}{1-\\beta}}\\gamma_{y}^{2+\\frac{1}{1-\\beta}}\\bar{u}_{1}^{2-2\\beta}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Recalling the definition of $\\ensuremath{{\\cal E}}_{1,k}$ as given in (34), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{K-1}{\\sum_{j\\in\\bar{j}}\\mathbb{Z}}\\left[\\frac{1-3\\mu_{\\gamma}\\eta_{\\gamma}\\bar{\\alpha}_{\\gamma+1,j}^{-1}\\bar{\\alpha}_{1}^{-1}}{2\\gamma_{j}\\bar{\\alpha}_{1}^{-1}\\bar{\\alpha}_{1}^{-1}}|\\mathbf{y}_{t}-\\mathbf{1}y^{*}(\\bar{x}_{t})|^{2}-\\frac{\\|\\mathbf{y}_{t+1}-\\mathbf{1}y^{*}(\\bar{x}_{t+1})\\|^{2}}{\\left(2+\\mu_{\\gamma}\\eta_{\\gamma}\\bar{\\alpha}_{1}^{-1}\\right)\\gamma_{0}\\bar{\\alpha}_{1}^{-1}\\bar{\\alpha}_{1}^{-1}}\\right]}\\\\ &{\\leqslant\\frac{1-3\\mu_{\\gamma}\\eta_{\\gamma}\\bar{\\alpha}_{1}^{-1}\\bar{\\alpha}_{1}^{-2}\\|\\mathbf{y}_{0}-\\mathbf{1}y^{*}(\\bar{x}_{0})\\|^{2}}{2\\gamma_{0}\\bar{\\alpha}_{1}^{-1}\\bar{\\alpha}_{1}^{-1}}}\\\\ &{+\\displaystyle\\sum_{k=1}^{K-1}\\mathbb{E}\\left[\\left(\\frac{1-3\\mu_{\\gamma}\\bar{\\eta}_{k+1}^{-3}/4}{2\\gamma_{0}\\bar{\\alpha}_{1}^{-1}\\bar{\\alpha}_{1}^{-2}}-\\frac{1}{2\\alpha_{\\gamma}\\eta_{k}\\bar{\\alpha}_{1}^{-3}}\\left(2+\\mu_{\\gamma}\\eta_{k}\\bar{\\alpha}_{1}^{-3}\\right)\\right)\\|\\mathbf{y}_{k}-\\mathbf{1}y^{*}(\\bar{x}_{k})\\|^{2}\\right]}\\\\ &{\\leqslant\\frac{1-3\\mu_{\\gamma}\\eta_{\\gamma}\\bar{\\alpha}_{1}^{-3}\\bar{\\alpha}_{1}^{-2}\\|\\mathbf{y}_{0}-\\mathbf{1}y^{*}(\\bar{x}_{0})\\|^{2}}{2\\gamma_{0}\\bar{\\alpha}_{1}^{-1}\\bar{\\alpha}_{1}^{-1}}|\\mathbf{y}_{0}-\\mathbf{1}y^{*}(\\bar{x}_{0})|^{2}}\\\\ &{+\\displaystyle\\sum_{k=1}^{K-1}\\mathbb{E}\\left[\\left\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Next, we show that the term $\\frac{1}{2\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}-\\frac{1}{2\\gamma_{y}\\bar{u}_{k}^{-\\beta}}-\\frac{\\mu}{8}$ If the term is positive at iteration $k$ , then we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0<\\frac{\\bar{u}_{k+1}^{\\beta}}{2\\gamma_{y}}-\\frac{\\bar{u}_{k}^{\\beta}}{2\\gamma_{y}}-\\frac{\\mu}{8}}\\\\ &{\\leqslant\\bar{u}_{k}^{\\beta}\\frac{\\left(1+\\|\\nabla_{y}F({\\mathbf x}_{k},{\\mathbf y}_{k};\\xi_{k}^{y})\\|^{2}/n\\bar{u}_{k}^{\\beta}\\right)^{\\beta}}{2\\gamma_{y}}-\\frac{\\bar{u}_{k}^{\\beta}}{8}-\\frac{\\mu}{8}}\\\\ &{\\leqslant\\bar{u}_{k}^{\\beta}\\frac{\\left(1+\\beta\\|\\nabla_{y}F({\\mathbf x}_{k},{\\mathbf y}_{k};\\xi_{k}^{y})\\|^{2}/n\\bar{u}_{k}\\right)}{2\\gamma_{y}}-\\frac{\\bar{u}_{k}^{\\beta}}{2\\gamma_{y}}-\\frac{\\mu}{8}}\\\\ &{=\\frac{\\beta\\left\\Vert\\nabla_{y}F({\\mathbf x}_{k},{\\mathbf y}_{k};\\xi_{k}^{y})\\right\\Vert^{2}}{2\\gamma_{y}n\\bar{u}_{k}^{-\\beta}}-\\frac{\\mu}{8},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "wherein the last inequality we used Bernoulli's inequality. Then we have the following two conditions, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\frac{1}{n}\\,\\|\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}\\right)\\|^{2}\\geqslant\\frac{\\gamma_{y}\\bar{u}_{k+1}^{1-\\beta}}{4\\beta}\\geqslant\\frac{\\gamma_{y}\\bar{u}_{1}^{1-\\beta}}{4\\beta},}\\\\ {\\frac{4\\beta G^{2}}{\\mu\\gamma_{y}}\\geqslant\\frac{4\\beta\\left\\|\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\|^{2}}{\\mu\\gamma_{y}n}\\geqslant\\bar{u}_{k+1}^{1-\\beta},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies that we have at most ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\frac{4\\beta C^{2}}{\\mu\\gamma_{y}}\\right)^{\\frac{1}{1-\\beta}}\\frac{4\\beta}{\\mu\\gamma_{y}\\bar{u}_{1}^{1-\\beta}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "constant number of iterations when the term is positive. Furthermore, when the term is positive, by the inequality (59), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\frac{1}{2\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}-\\frac{1}{2\\gamma_{y}\\bar{u}_{k}^{-\\beta}}-\\frac{\\mu}{8}\\right)\\frac{1}{n}\\left\\Vert\\mathbf{y}_{k}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\Vert^{2}}\\\\ &{\\leqslant\\frac{\\beta\\left\\Vert\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{\\prime}\\right)\\right\\Vert^{2}}{2\\gamma_{y}n\\bar{u}_{1}^{-\\beta}}\\frac{1}{n}\\left\\Vert\\mathbf{y}_{k}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\Vert^{2}}\\\\ &{\\leqslant\\frac{\\beta C^{2}}{2\\mu^{2}\\gamma_{y}\\bar{u}_{1}^{1-\\beta}}\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert\\nabla_{y}f_{i}\\left(\\bar{x}_{k},y_{i,k}\\right)-\\nabla_{y}f_{i}\\left(\\bar{x}_{k},y^{*}\\right)\\right\\Vert^{2}}\\\\ &{\\leqslant\\frac{2\\beta C^{4}}{\\mu^{2}\\gamma_{y}\\bar{u}_{1}^{1-\\beta}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we have used the concavity of $f_{i}$ in $y$ and Assumption 3. Then, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K-1}\\mathbb{E}\\left[\\left(\\frac{1}{2\\gamma_{y}\\bar{u}_{k+1}^{-\\beta}}-\\frac{1}{2\\gamma_{y}\\bar{u}_{k}^{-\\beta}}-\\frac{\\mu}{8}\\right)\\frac{1}{n}\\left\\|\\mathbf{y}_{k}-\\mathbf{1}y^{*}\\left(\\bar{x}_{k}\\right)\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leqslant\\frac{2\\beta C^{4}}{\\mu^{2}\\gamma_{y}\\bar{u}_{1}^{-\\beta}}\\left(\\frac{4\\beta C^{2}}{\\mu\\gamma_{y}}\\right)^{\\frac{1}{1-\\beta}}\\frac{4\\beta}{\\mu\\gamma_{y}\\bar{u}_{1}^{1-\\beta}}}\\\\ &{\\displaystyle\\leqslant\\frac{2\\left(4\\beta C^{2}\\right)^{2+\\frac{1}{1-\\beta}}}{\\mu^{3+\\frac{1}{1-\\beta}}\\gamma_{y}^{2+\\frac{1}{1-\\beta}}\\bar{u}_{1}^{2-2\\beta}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which completes the proof. ", "page_idx": 29}, {"type": "text", "text": "Next, we show in the following lemma that the inconsistency terms, as described in (5), exhibit asymptotic convergence for the proposed D-AdaST algorithm. ", "page_idx": 29}, {"type": "text", "text": "Lemma 9 (Convergence of inconsistency terms). Suppose Assumption 1-4 hold. For the proposed D-AdaST in Algorithm 1, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left({\\mathbf x}_{k},{\\mathbf y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]\\leqslant\\sqrt{\\frac{1}{n^{1-\\alpha}}\\left(\\frac{4\\rho_{W}}{\\left(1-\\rho_{W}\\right)^{2}}\\right)^{\\alpha}}\\frac{\\left(1+\\zeta_{v}\\right)\\zeta_{v}C^{2-\\alpha}}{\\left(1-\\alpha\\right)K^{\\alpha}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(64) ", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\left(\\tilde{u}_{k+1}^{-\\beta}\\right)^{T}}{n\\bar{u}_{k+1}^{-\\beta}}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\|^{2}\\right]\\leqslant\\sqrt{\\frac{1}{n^{1-\\beta}}\\left(\\frac{4\\rho_{W}}{\\left(1-\\rho_{W}\\right)^{2}}\\right)^{\\beta}}\\frac{\\left(1+\\zeta_{u}\\right)\\zeta_{u}C^{2-\\beta}}{\\left(1-\\beta\\right)K^{\\beta}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. By the definition of $v_{i,k}$ in (3), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leqslant\\mathbb{E}\\left[\\displaystyle\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\left(\\bar{v}_{k+1}^{\\alpha}-v_{i,k+1}^{\\alpha}\\right)^{2}\\frac{\\left\\|g_{i,k}^{x}\\right\\|^{2}}{v_{i,k+1}^{2\\alpha}}\\right]}\\\\ &{\\leqslant\\mathbb{E}\\left[\\displaystyle\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\left(\\bar{v}_{k+1}^{\\alpha}-v_{i,k+1}^{\\alpha}\\right)^{2}\\frac{\\bar{v}_{k+1}^{\\alpha}}{v_{i,k+1}^{2\\alpha}}\\frac{\\left\\|g_{i,k}^{x}\\right\\|^{2}}{\\bar{v}_{k+1}^{\\alpha}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Noticing that ++1 , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\frac{(\\mathbf{S}_{t}^{\\alpha})-\\mathbf{S}_{t}^{\\alpha}}{n\\hbar\\mathbf{\\Phi}_{t}+\\mathbf{S}_{t}}\\nabla_{\\alpha}\\mathbf{F}(\\mathbf{x}_{t},\\mathbf{y}_{t};\\xi)\\right\\|^{2}\\right]}\\\\ &{\\leqslant\\mathbb{E}\\left[\\frac{1}{n^{2}}\\frac{\\mathbf{\\Phi}_{t}}{m_{t}^{2}}\\left(\\kappa_{t+1}^{2}-\\kappa_{t+1}^{2}\\right)^{2}\\left(\\frac{U_{t+1}^{2}-\\mathbf{\\Phi}_{t}^{\\alpha}+\\mathbf{\\Phi}_{t+1}^{\\alpha}+}{v_{t}^{2}\\Delta_{t+1}^{2}}+\\frac{1}{v_{t+1}^{2}}\\right)\\frac{\\left\\|U_{t}^{\\alpha}\\right\\|^{2}}{v_{t+1}^{2}}\\right]}\\\\ &{\\leqslant\\mathbb{E}\\left[\\frac{1}{n^{2}}\\frac{\\mathbf{\\Phi}_{t}}{m_{t}^{2}}\\frac{\\left(U_{t+1}^{2}-\\mathbf{\\Phi}_{t+1}^{\\alpha}\\right)^{2}}{v_{t+1}^{2}}\\left|v_{t+1}^{2}-v_{t+1}^{2}\\right|\\frac{\\left\\|U_{t}^{\\alpha}\\right\\|^{2}}{v_{t+1}^{2}}\\right]}\\\\ &{+\\mathbb{E}\\left[\\frac{1}{n^{2}}\\frac{\\left\\|\\frac{U_{t+1}^{\\alpha}}{\\Delta_{t+1}^{\\alpha}}-v_{t+1}^{2}\\right\\|}{v_{t+1}^{2}}\\left|v_{t+1}^{2}-v_{t+1}^{2}\\right|\\frac{\\left\\|U_{t}^{\\alpha}\\right\\|^{2}}{v_{t+1}^{2}}\\right]}\\\\ &{\\leqslant(1+\\zeta)\\zeta\\mathbb{E}\\left[\\frac{1}{n}\\sum_{k=1}^{n}|\\overline{{u_{t+1}^{\\alpha}-v_{t+1}^{\\alpha}}}|\\frac{1}{n}\\frac{\\zeta}{\\lambda_{t+1}^{\\alpha}}\\right|^{2}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma 4, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\left(\\frac{\\widetilde{v}_{k+1}^{\\,\\alpha}}{n\\widetilde{v}_{k+1}^{\\,\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{\\,\\varepsilon}\\right)\\right\\|^{2}\\right]\\right.}\\\\ &{\\displaystyle\\leqslant\\left.\\left(1+\\zeta_{v}\\right)\\zeta_{v}\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\Big|\\bar{v}_{k+1}^{\\,\\alpha}-v_{i,k+1}^{\\,\\alpha}\\Big|\\frac{1}{K}\\sum_{k=0}^{K-1}\\frac{1}{v_{k+1}^{\\,\\alpha}}\\Big|\\mathcal{G}_{v_{k+1}^{\\,k}}^{\\,\\alpha}\\Big]\\right\\|^{2}}\\\\ &{\\displaystyle\\leqslant\\left(1+\\zeta_{v}\\right)\\zeta_{v}\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\Big|\\bar{v}_{k+1}^{\\,\\alpha}-v_{i,k+1}^{\\,\\alpha}\\Big|\\right]\\frac{C^{2-2\\alpha}}{\\left(1-\\alpha\\right)K^{\\alpha}}}\\\\ &{\\displaystyle\\leqslant\\left(1+\\zeta_{v}\\right)\\zeta_{v}\\sqrt{\\frac{1}{n}\\mathbb{E}\\left[\\|v_{k+1}-1\\bar{v}_{k+1}\\|^{2\\alpha}\\right]}\\frac{C^{2-2\\alpha}}{\\left(1-\\alpha\\right)K^{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Next, for the term of inconsistency of the stepsize $\\left\\|\\pmb{v}_{k}-\\mathbf{1}\\bar{v}_{k}\\right\\|^{2}$ , we consider two cases due to the max operator we used. At iteration $k$ , for the case $\\mathbf{m}_{k}^{x}\\geqslant\\mathbf{m}_{k}^{y}$ With $\\left\\|\\mathbf{m}_{0}^{x}-\\mathbf{1}\\bar{m}_{0}^{x}\\right\\|^{2}=0$ wehave ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert v_{k+1}-1\\bar{v}_{k+1}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert\\mathbf{m}_{k+1}^{x}-1\\bar{m}_{k+1}^{x}\\right\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\Vert(W-\\mathbf{J})\\left(\\mathbf{m}_{k}^{x}-\\mathbf{1}\\bar{m}_{k}^{x}\\right)+\\eta_{k}\\left(W-\\mathbf{J}\\right)h_{k}^{x}\\right\\Vert^{2}\\right]}\\\\ &{\\leqslant\\frac{1+\\rho_{W}}{2}\\mathbb{E}\\left[\\left\\Vert\\mathbf{m}_{k}^{x}-\\mathbf{1}\\bar{m}_{k}^{x}\\right\\Vert^{2}\\right]+\\frac{\\left(1+\\rho_{W}\\right)\\rho_{W}}{1-\\rho_{W}}\\mathbb{E}\\left[\\left\\Vert h_{k}^{x}\\right\\Vert^{2}\\right]}\\\\ &{\\leqslant\\left(\\frac{1+\\rho_{W}}{2}\\right)^{k}\\mathbb{E}\\left[\\left\\Vert\\mathbf{m}_{0}^{x}-\\mathbf{1}\\bar{m}_{0}^{x}\\right\\Vert^{2}\\right]+\\frac{n C^{2}\\left(1+\\rho_{W}\\right)\\rho_{W}}{1-\\rho_{W}}\\sum_{t=0}^{k}\\left(\\frac{1+\\rho_{W}}{2}\\right)^{k-t}}\\\\ &{\\leqslant\\frac{2n C^{2}\\left(1+\\rho_{W}\\right)\\rho_{W}}{\\left(1-\\rho_{W}\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the case ${\\bf m}_{k}^{x}<{\\bf m}_{k}^{y}$ , with $\\left\\|\\mathbf{m}_{0}^{y}-\\mathbf{1}\\bar{m}_{0}^{y}\\right\\|^{2}=0$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert v_{k+1}-\\mathbf{1}\\bar{v}_{k+1}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert\\mathbf{m}_{k+1}^{y}-\\mathbf{1}\\bar{m}_{k+1}^{y}\\right\\Vert^{2}\\right]\\leqslant\\frac{2n C^{2}\\left(1+\\rho_{W}\\right)\\rho_{W}}{\\left(1-\\rho_{W}\\right)^{2}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining these two cases, and using Lemma 4 and the fact $\\|v_{k}^{\\alpha}-\\mathbf{1}\\bar{v}_{k}^{\\alpha}\\|^{2}\\leqslant\\|v_{k}-\\mathbf{1}\\bar{v}_{k}\\|^{2\\alpha}$ for $\\alpha\\in(0,1)$ , we obtain the result for primal decision variable. Following the same proof, we can also derive the result for dual decision variable. We thus complete the proof. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "We further give the following lemma to show that the inconsistency of stepsize remains uniformly bounded for the vanilla D-TiAda algorithm as given in (2). ", "page_idx": 31}, {"type": "text", "text": "Lemma 10 (Inconsistency for D-TiAda). Suppose Assumption 1-4 hold. Then, for $D$ TiAda,wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\left(\\tilde{v}_{k+1}^{-\\alpha}\\right)^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]\\leqslant\\zeta_{v}^{2}C^{2},}\\\\ &{\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\left(\\tilde{u}_{k+1}^{-\\beta}\\right)^{T}}{n\\bar{u}_{k+1}^{-\\beta}}\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\|^{2}\\right]\\leqslant\\zeta_{u}^{2}C^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. By the definition of inconsistency of stepsizes in (8) and Assumption 3 on bounded gradient, we immediately get the result. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "B.3Proof of Theorem 1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Theorem 1. Consider a complete graph with 3 nodes where the functions corresponding to the nodes are as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f_{1}(x,y)=-\\displaystyle\\frac{1}{2}y^{2}+x y-\\displaystyle\\frac{1}{2}x^{2},}}\\\\ {{f_{2}(x,y)=f_{3}(x,y)=-\\displaystyle\\frac{1}{2}y^{2}-(1+\\displaystyle\\frac{1}{a}+\\displaystyle\\frac{1}{b})x y-\\displaystyle\\frac{1}{2}x^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $a=2^{\\frac{-1}{2\\alpha-1}}$ and $b=2^{\\frac{-1}{2\\beta-1}}$ ", "page_idx": 31}, {"type": "text", "text": "Notice that the only stationary point of $f(x,y)=(f_{1}(x,y)+f_{2}(x,y)+f_{3}(x,y))/3$ is $(0,0)$ We denote $g_{i,k}^{x}=\\nabla_{x}f_{i}(x_{k},y_{k})$ and $g_{i,k}^{y}=\\nabla_{y}f_{i}(x_{k},y_{k})$ ", "page_idx": 31}, {"type": "text", "text": "Now we consider points initialized in line ", "page_idx": 31}, {"type": "equation", "text": "$$\ny=-{\\frac{1+a}{a+{\\frac{a}{b}}}}x,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{g_{1,0}^{x}=y_{0}-x_{0}=-\\displaystyle\\frac{2a b+a+b}{a b+a}x_{0}}}\\\\ {{g_{2,0}^{x}=g_{3,0}^{x}=-\\left(1+\\displaystyle\\frac{1}{b}+\\displaystyle\\frac{1}{a}\\right)y_{0}-x_{0}=\\displaystyle\\frac{2a b+a+b}{a^{2}(b+1)}x_{0}}}\\\\ {{g_{1,0}^{y}=x_{0}-y_{0}=\\displaystyle\\frac{2a b+a+b}{a b+a}x_{0}}}\\\\ {{g_{2,0}^{y}=g_{2,0}^{y}=-\\displaystyle\\frac{2a b+a+b}{a b(b+1)}x_{0}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that by our assumptions of the range of $\\alpha$ and $\\beta$ ,wehave $a<b$ . Thus, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n|g_{1,0}^{x}|=|g_{1,0}^{y}|\\quad\\mathrm{and}\\quad|g_{2,0}^{x}|>|g_{2,0}^{y}|,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which means $g_{2,0}^{x}$ would be chosen in the maximum operator in the denominator of TiAda stepsize for $x$ . Therefore, after one step, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{1}=x_{0}-\\eta^{x}\\underbrace{\\left(\\frac{g_{1,0}^{x}}{\\left(\\left|g_{1,0}^{x}\\right|^{2}\\right)^{\\alpha}}+\\frac{g_{2,0}^{x}}{\\left(\\left|g_{2,0}^{x}\\right|^{2}\\right)^{\\alpha}}+\\frac{g_{3,0}^{x}}{\\left(\\left|g_{3,0}^{x}\\right|^{2}\\right)^{\\alpha}}\\right)}_{=0}}\\\\ &{y_{1}=y_{0}-\\eta^{y}\\underbrace{\\left(\\frac{g_{1,0}^{y}}{\\left(\\left|g_{1,0}^{y}\\right|^{2}\\right)^{\\beta}}+\\frac{g_{2,0}^{y}}{\\left(\\left|g_{2,0}^{y}\\right|^{2}\\right)^{\\beta}}+\\frac{g_{3,0}^{y}}{\\left(\\left|g_{3,0}^{y}\\right|^{2}\\right)^{\\beta}}\\right)}_{=0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Next, we will use induction to show that $x$ and $y$ will stay in $x_{0}$ and $y_{0}$ for any iteration. Assuming for all iterations $k$ in $1,\\ldots,t,x_{k}=x_{0}$ and $y_{k}=y_{0}$ , then we have in next step ", "page_idx": 32}, {"type": "equation", "text": "$$\nx_{t+1}=x_{t}-\\eta^{x}\\left(\\frac{g_{1,0}^{x}}{\\left(t\\cdot|g_{1,0}^{x}|^{2}\\right)^{\\alpha}}+\\frac{g_{2,0}^{x}}{\\left(t\\cdot|g_{2,0}^{x}|^{2}\\right)^{\\alpha}}+\\frac{g_{3,0}^{x}}{\\left(t\\cdot|g_{3,0}^{x}|^{2}\\right)^{\\alpha}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that $g_{1,0}^{x}=-a\\cdot g_{2,0}^{x}$ . Then, we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t+1}=x_{t}-\\eta^{x}\\left(\\frac{-p\\cdot g_{2,0}^{x}}{t^{\\alpha}\\cdot a^{2\\alpha}\\cdot|g_{2,0}^{x}|^{2\\alpha}}+\\frac{2g_{2,0}^{x}}{t^{\\alpha}\\cdot|g_{2,0}^{x}|^{2\\alpha}}\\right)}\\\\ &{\\qquad=x_{t}-\\frac{g_{2,0}^{x}}{t^{\\alpha}\\cdot|g_{2,0}^{x}|^{2\\alpha}}\\underbrace{\\left(2-a^{1-2\\alpha}\\right)}_{=0\\mathrm{~(by~definition~of~}a)}}\\\\ &{\\qquad=x_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly, we can show that $y_{t+1}=y_{t}$ . Therefore all iterates will stay at $\\left(x_{0},y_{0}\\right)$ if initialized at line $\\begin{array}{r}{y=-\\frac{a b+b}{a b+a}x}\\end{array}$ whichmpliesthat theinitial gradient orm canbe arbitrarily large by picking $x_{0}$ to be large. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "B.4 Proof of Theorem 2 and Corollary 1 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof of Theorem 2. Combining the results obtained in Lemma 6, 7 and 8, we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sinh^{2}}{n(n-1)}\\ensuremath{\\mathbb{E}\\left[(f(x_{k})\\cdot f(x_{k}))-f(x_{k},y_{k})\\right]}}\\\\ &{=\\frac{\\sinh^{2}}{n(n-1)}\\ensuremath{\\mathbb{E}\\left[(f(x_{k})\\cdot f^{\\prime}(x_{k}))-f(x_{k},y_{k})\\right]}+\\sum_{k=1}^{n-1}\\ensuremath{\\mathbb{E}\\left[f(x_{k},y^{*}(x_{k}))-f(x_{k},y_{k})\\right]}}\\\\ &{\\leq\\frac{1}{2n\\sqrt{n}\\sqrt{n}}\\ensuremath{\\mathbb{E}\\left[(\\ensuremath{\\mathbb{R}}_{n}-\\ensuremath{\\mathbb{R}}^{2}(x_{k}))\\right]}+\\frac{\\sum_{k=1}^{n-1}\\ensuremath{\\mathbb{E}\\left[(f(x_{k})\\cdot f^{\\prime}(x_{k}))-f^{\\prime}(x_{k},y_{k})\\right]}}{\\sqrt{n}^{2}\\sqrt{n}\\sqrt{n}\\sqrt{n}\\omega_{k}^{2}}}\\\\ &{+\\frac{2\\sqrt{n}\\sqrt{n}\\sqrt{n}\\sqrt{n}\\sqrt{n}}{n\\sqrt{n}}\\ensuremath{\\mathbb{E}\\left[(\\ensuremath{\\mathbb{R}}_{n}-\\ensuremath{\\mathbb{R}}^{2}(x_{k}))\\right]}+\\frac{2\\sqrt{n}\\sqrt{n}\\sqrt{n}\\sqrt{n}\\omega_{k}}{n\\sqrt{n}\\sqrt{n}\\sqrt{n}\\omega_{k}}}\\\\ &{+\\frac{2\\sqrt{n}(1+\\sqrt{n})^{2}}{n}\\ensuremath{\\mathbb{E}\\left[(\\ensuremath{\\mathbb{R}}_{n}^{2}+\\ensuremath{\\mathbb{R}}_{n}^{2}(x_{k}))\\right]}}\\\\ &{+\\frac{\\rho_{0}}{n\\sqrt{n}}\\ensuremath{\\mathbb{E}\\left[(\\ensuremath{\\mathbb{R}}_{n}^{2}+\\ensuremath{\\mathbb{R}}_{n}^{2}(x_{k}))\\right]}+\\frac{\\rho_{0}}{n}\\ensuremath{\\mathbb{E}\\left[(\\ensuremath{\\mathbb{R}}_{n}^{2}+\\ensuremath{\\mathbb{R}}_{n}^{2}(x_{k}))\\right]}+\\sum_{k=1}^{n-1}\\ensuremath{\\mathbb{E}\\left[\\frac{1}{n}|\\ensuremath{\\mathbb{R}}_{n}-\\ensuremath{\\mathbb{R}}_{n}|\\right]}}\\\\ &{+\\frac{\\rho_{0}}{n\\sqrt{n}}\\ensuremath{\\mathbb \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Letting the separation point between the two phases discussed in Lemma 6 and 7 satisfy ", "page_idx": 33}, {"type": "equation", "text": "$$\nG=\\left(\\frac{16\\left(1+\\zeta_{v}^{2}\\right)\\gamma_{x}^{2}\\kappa^{4}}{\\gamma_{y}^{2}}\\right)^{\\frac{1}{2\\alpha-2\\beta}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "then, plugging above inequality into (18), with the help of Lemma 4-8 and Lemma 9, we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\{\\nabla\\phi(\\boldsymbol{\\hat{x}}_{k})\\right\\}^{\\mathrm{if}}\\right]}\\\\ &{~~\\times\\int_{0}^{\\infty}\\!\\!\\!\\!\\!K\\,\\rho\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{0}:=\\frac{4\\kappa L}{K\\gamma_{y}\\bar{u}_{1}^{-\\beta}n}\\mathbb{E}\\left[\\|\\mathbf{y}_{0}-\\mathbf{1}y^{*}(\\bar{x}_{0})\\|^{2}\\right]+\\frac{16\\kappa^{2}\\left(4\\beta C^{2}\\right)^{2+1}\\frac{1}{1-\\beta}}{K\\mu^{2+\\frac{1}{1-\\beta}}\\gamma_{y}^{2+\\frac{1}{1-\\beta}}\\bar{u}_{1}^{2-2\\beta}},}\\\\ &{E_{G}:=\\frac{16\\gamma_{x}^{2}\\kappa^{4}\\left(1+\\zeta_{v}^{2}\\right)G^{2\\beta}}{\\gamma_{y}^{2}}\\left(\\frac{C^{2-4\\alpha}}{(1-2\\alpha)K^{2\\alpha}}\\frac{\\mathbb{E}(\\mathbf{1}_{\\alpha<1/2}+\\frac{1}{K}+\\log v_{1}-\\log v_{1}}{K\\bar{v}_{1}^{2\\alpha-1}}\\right),}\\\\ &{E_{W}:=\\frac{32\\left(8\\kappa L+3L^{2}\\right)\\rho_{W}\\gamma_{x}^{2}\\left(1+\\zeta_{v}^{2}\\right)}{(1-\\rho_{W})^{2}}\\left(\\frac{C^{2-4\\alpha}}{(1-2\\alpha)K^{2\\alpha}}\\frac{\\mathbb{E}\\alpha_{<1/2}+\\frac{1+\\log v_{K}-\\log v_{1}}{K\\bar{v}_{1}^{2\\alpha-1}}\\mathbb{I}_{\\alpha\\geq1/2}}{(1-2\\alpha)K^{2\\alpha}}\\right)}\\\\ &{\\quad\\quad+\\frac{32\\left(8\\kappa L+3L^{2}\\right)\\rho_{W}\\gamma_{y}^{2}\\left(1+\\zeta_{u}^{2}\\right)}{(1-\\rho_{W})^{2}}\\left(\\frac{C^{2-4\\beta}}{(1-2\\beta)K^{2\\beta}}\\mathbb{I}_{\\beta<1/2}+\\frac{1+\\log u_{K}-\\log v_{1}}{K\\bar{u}_{1}^{2\\beta-1}}\\mathbb{I}_{\\beta\\geq1/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Letting the total iteration $K$ satisfy the conditions given in (12) such that the terms $E_{0},E_{G}$ and $E_{W}$ are dominated by the others, we thus complete the proof. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Proof ofCorollary $^{\\,I}$ . With the help of Lemma 10, we can directly adapt the proof of Theorem 2 to get the result in (14). ", "page_idx": 33}, {"type": "text", "text": "B.5   Extend the proof to coordinate-wise stepsize ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this subsection, we show how to extend our convergence analysis of D-AdaST to the coordinatewise adaptive stepsize (Zhou et al., 2018) variant. We first present this variant in Algorithm 2, which can be rewritten in a compact form with the Hadamard product denoted by $\\odot$ ", "page_idx": 33}, {"type": "text", "text": "Algorithm 2 D-AdaST with coordinate-wise adaptive stepsize ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Initialization: $x_{i,0}\\,\\in\\,\\mathbb{R}^{p}$ $y_{i,0}\\in\\mathscr{Y}$ , buffers $m_{i,0}^{x},m_{i,0}^{y}>0$ stepsizes $\\gamma_{x},\\gamma_{y}\\,>\\,0$ and $0<\\beta<$ $\\alpha<1$   \n1: for iteration $k=0,1,\\cdot\\cdot\\cdot$ , each node $i\\in[n]$ do   \n2:  Sample i.d $\\xi_{i,k}^{x}$ and Si.k, compute: ", "page_idx": 34}, {"type": "equation", "text": "$$\ng_{i,k}^{x}=\\nabla_{x}f_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{x}\\right),\\ g_{i,k}^{y}=\\nabla_{y}f_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{y}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "3: Accumulate the gradient with Hadamard product: ", "page_idx": 34}, {"type": "equation", "text": "$$\nm_{i,k+1}^{x}=m_{i,k}^{x}+g_{i,k}^{x}\\odot g_{i,k}^{x},\\ m_{i,k+1}^{y}=m_{i,k}^{y}+g_{i,k}^{y}\\odot g_{i,k}^{y}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "4: Compute the ratio: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\psi_{i,k+1}=\\left\\|m_{i,k+1}^{x}\\right\\|^{2\\alpha}/\\operatorname*{max}\\left\\{\\left\\|m_{i,k+1}^{x}\\right\\|^{2\\alpha},\\left\\|m_{i,k+1}^{y}\\right\\|^{2\\alpha}\\right\\}\\leqslant1.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "5: Update primal and dual variables locally: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i,k+1}=x_{i,k}-\\gamma_{x}\\psi_{i,k+1}\\left(m_{i,k+1}^{x}\\right)^{-\\alpha}\\odot g_{i,k}^{x},}\\\\ &{y_{i,k+1}=y_{i,k}+\\gamma_{y}\\left(m_{i,k+1}^{y}\\right)^{-\\beta}\\odot g_{i,k}^{y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "6:  Communicate parameters with neighbors: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Big\\{m_{i,k+1}^{x},m_{i,k+1}^{y},\\boldsymbol{x}_{i,k+1},\\boldsymbol{y}_{i,k+1}\\Big\\}\\gets\\sum_{j\\in\\mathcal{N}_{i}}W_{i,j}\\,\\Big\\{m_{j,k+1}^{x},m_{j,k+1}^{y},\\boldsymbol{x}_{j,k+1},\\boldsymbol{y}_{j,k+1}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "7:   Projection of dual variable on to set $\\boldsymbol{\\wp}$ $?\\!:y_{i,k+1}\\leftarrow\\mathcal{P}_{\\mathcal{Y}}\\left(y_{i,k+1}\\right)$ ", "page_idx": 34}, {"type": "text", "text": "8: end for ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{m}_{k+1}^{x}=W\\left(\\mathbf{m}_{k}^{x}+\\mathbf{h}_{k}^{x}\\right),}\\\\ &{\\mathbf{m}_{k+1}^{y}=W\\left(\\mathbf{m}_{k}^{y}+\\mathbf{h}_{k}^{y}\\right),}\\\\ &{\\mathbf{x}_{k+1}=W\\left(\\mathbf{x}_{k}-\\gamma_{x}V_{k+1}^{-\\alpha}\\odot\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right),}\\\\ &{\\mathbf{y}_{k+1}=\\mathcal{P}_{\\mathcal{V}}\\left(W\\left(\\mathbf{y}_{k}+\\gamma_{y}U_{k+1}^{-\\beta}\\odot\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{k}^{x}=\\left[\\cdots,g_{i,k}^{x}\\odot g_{i,k}^{x},\\cdots\\right]^{T}\\in\\mathbb{R}^{n\\times p},\\;\\pmb{h}_{k}^{y}=\\left[\\cdots,g_{i,k}^{y}\\odot g_{i,k}^{y},\\cdots\\right]^{T}\\in\\mathbb{R}^{n\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and the matrices $U_{k}^{\\alpha}$ and $V_{k}^{\\beta}$ are edefine a follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{k}^{-\\alpha}=\\left[\\cdot\\cdot\\cdot\\,,v_{i,k}^{-\\alpha},\\cdot\\cdot\\,\\cdot\\right]^{T},\\ [v_{i,k}]_{j}=\\operatorname*{max}\\left\\{\\left[m_{i,k}^{x}\\right]_{j},\\,\\left[m_{i,k}^{y}\\right]_{j}\\right\\},\\ j\\in[p]\\,,}\\\\ &{U_{k}^{-\\beta}=\\left[\\cdot\\cdot\\,,u_{i,k}^{-\\beta},\\cdot\\cdot\\,\\cdot\\right]^{T},\\ [u_{i,k}]_{j}=\\left[m_{i,k}^{y}\\right]_{j},\\ j\\in[d]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $[\\cdot]_{j}$ denotes the $j$ -th element of a vector. ", "page_idx": 34}, {"type": "text", "text": "Recalling the definitions of inconsistency of stepsize in (8), we give the following notations: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{V}_{k}=V_{k}-\\bar{\\nu}_{k}{\\bf1}{\\bf1}_{p}^{T},\\;\\bar{\\boldsymbol{v}}_{k}=\\frac{1}{n p}\\sum_{i=1}^{n}\\sum_{j}^{p}V_{i j},\\;\\bar{v}_{i,k}=\\frac{1}{p}\\sum_{j}^{p}V_{i j},\\;\\bar{v}_{j,k}=\\frac{1}{n}\\sum_{i=1}^{n}V_{i j},}}\\\\ {{\\displaystyle\\tilde{U}_{k}=U_{k}-\\bar{u}_{k}{\\bf1}{\\bf1}_{p}^{T},\\;\\bar{u}_{k}=\\frac{1}{n d}\\sum_{i=1}^{n}\\sum_{j}^{d}U_{i j},\\;\\bar{u}_{i,k}=\\frac{1}{d}\\sum_{j}^{d}U_{i j},\\;\\bar{u}_{j,k}=\\frac{1}{n}\\sum_{i=1}^{n}U_{i j},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta_{V}^{2}=\\operatorname*{sup}_{k\\geqslant0}\\left\\{\\frac{\\left\\|V_{k}^{-\\alpha}-\\bar{v}_{k}^{-\\alpha}\\mathbf{1}\\mathbf{1}_{p}^{T}\\right\\|^{2}}{n p\\left(\\bar{v}_{k}^{-\\alpha}\\right)^{2}}\\right\\},\\;\\hat{\\zeta}_{v}^{2}=\\operatorname*{sup}_{k\\geqslant0}\\left\\{\\frac{\\left\\|V_{k}^{-\\alpha}-(V_{k}\\mathbf{J}_{p})^{-\\alpha}\\right\\|^{2}}{n p\\left(\\bar{v}_{k}^{-\\alpha}\\right)^{2}}\\right\\},}\\\\ &{\\zeta_{U}^{2}=\\operatorname*{sup}_{k\\geqslant0}\\left\\{\\frac{\\left\\|U_{k}^{-\\beta}-\\bar{u}_{k}^{-\\beta}\\mathbf{1}\\mathbf{1}_{d}^{T}\\right\\|^{2}}{n d\\left(\\bar{u}_{k}^{-\\beta}\\right)^{2}}\\right\\},\\;\\hat{\\zeta}_{u}^{2}=\\operatorname*{sup}_{k\\geqslant0}\\left\\{\\frac{\\left\\|U_{k}^{-\\beta}-(U_{k}\\mathbf{J}_{d})^{-\\beta}\\right\\|^{2}}{n d\\left(\\bar{u}_{k}^{-\\beta}\\right)^{2}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Building upon the established definitions of coordinate-wise stepsize inconsistency, the subsequent lemma is presented to show the non-convergence of the inconsistency term compared to Lemma 9. ", "page_idx": 35}, {"type": "text", "text": "Lemma 11 (Inconsistency, coordinate-wise). Suppose Assumption 1-4 hold. For the proposed $D$ -AdaST algorithm,wehave ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\mathbf{1}^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\tilde{V}_{k+1}^{-\\alpha}\\odot\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leqslant2\\left(1+\\zeta_{v}\\right)\\zeta_{v}\\sqrt{\\frac{1}{n^{1-\\alpha}}\\left(\\frac{4C^{2}\\rho_{W}}{\\left(1-\\rho_{W}\\right)^{2}}\\right)^{\\alpha}}\\frac{C^{2-2\\alpha}}{\\left(1-\\alpha\\right)K^{\\alpha}}+2n p\\hat{\\zeta}_{v}^{2}C^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\mathbf{1}^{T}}{n\\bar{u}_{k+1}^{-\\beta}}\\tilde{U}_{k+1}^{-\\beta}\\odot\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leqslant2\\left(1+\\zeta_{u}\\right)\\zeta_{u}\\sqrt{\\frac{1}{n^{1-\\beta}}\\left(\\frac{4C^{2}\\rho_{W}}{\\left(1-\\rho_{W}\\right)^{2}}\\right)^{\\beta}}\\frac{C^{2-2\\beta}}{\\left(1-\\beta\\right)K^{\\beta}}+2n d\\hat{\\zeta}_{u}^{2}C^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "In contrast,for $D$ -TiAda,we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\mathbf{1}^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\tilde{V}_{k+1}^{-\\alpha}\\odot\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]\\leqslant p\\zeta_{V}^{2}C^{2},}\\\\ &{\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\frac{\\mathbf{1}^{T}}{n\\bar{u}_{k+1}^{-\\beta}}\\tilde{U}_{k+1}^{-\\beta}\\odot\\nabla_{y}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{y}\\right)\\right\\|^{2}\\right]\\leqslant d\\zeta_{U}^{2}C^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. For the coordinate-wise adaptive stepsize, with the definitions of Frobenius norm and Hadamard product, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\frac{{\\bf1}^{T}}{n\\hat{\\bf v}_{k+1}^{-\\alpha}}\\bar{V}_{k+1}^{-\\alpha}\\odot\\nabla_{x}F\\left({\\bf x}_{k},{\\bf y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|\\frac{{\\bf1}^{T}}{n\\hat{v}_{k+1}^{-\\alpha}}\\left(V_{k+1}^{-\\alpha}-(V_{k+1}{\\bf J})^{-\\alpha}+(V_{k+1}{\\bf J})^{-\\alpha}-\\bar{v}_{k+1}^{-\\alpha}{\\bf1}_{p}^{T}\\right)\\odot\\nabla_{x}F\\left({\\bf x}_{k},{\\bf y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leqslant2\\mathbb{E}\\left[\\left\\|\\frac{{\\bf1}^{T}}{n\\hat{v}_{k+1}^{-\\alpha}}\\left((V_{k+1}{\\bf J})^{-\\alpha}-\\bar{v}_{k+1}^{-\\alpha}{\\bf1}_{p}^{1T}\\right)\\odot\\nabla_{x}F\\left({\\bf x}_{k},{\\bf y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{+\\,2\\mathbb{E}\\left[\\left\\|\\frac{{\\bf1}^{T}}{n\\hat{v}_{k+1}^{-\\alpha}}\\left(V_{k+1}^{-\\alpha}-(V_{k+1}{\\bf J})^{-\\alpha}\\right)\\odot\\nabla_{x}F\\left({\\bf x}_{k},{\\bf y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For the first term on the RHS, according to the definitions given in (78), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\frac{\\mathbf{1}^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\left((V_{k+1}\\mathbf{J})^{-\\alpha}-\\bar{v}_{k+1}^{-\\alpha}\\mathbf{1}\\mathbf{1}_{p}^{T}\\right)\\odot\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leqslant\\mathbb{E}\\left[\\frac{1}{n^{2}\\bar{v}_{k+1}^{-2\\alpha}}\\sum_{i=1}^{n}\\left(\\bar{v}_{i,k+1}^{\\alpha}-\\bar{v}_{k+1}^{\\alpha}\\right)^{2}\\left\\|\\nabla_{x}f_{i}\\left(x_{i,k},y_{i,k};\\xi_{i,k}^{x}\\right)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, for the second part, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\frac{{\\bf1}^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\left(V_{k+1}^{-\\alpha}-(V_{k+1}{\\bf J})^{-\\alpha}\\right)\\odot\\nabla_{x}F\\left({\\bf x}_{k},{\\bf y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leqslant\\frac{1}{n}\\mathbb{E}\\left[\\left\\|\\frac{V_{k+1}^{-\\alpha}-\\left(V_{k+1}{\\bf J}\\right)^{-\\alpha}}{\\bar{v}_{k+1}^{-\\alpha}}\\right\\|^{2}\\left\\|\\nabla_{x}F\\left({\\bf x}_{k},{\\bf y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leqslant p\\hat{\\zeta}_{v}^{2}\\mathbb{E}\\left[\\|\\nabla_{x}F\\left({\\bf x}_{k},{\\bf y}_{k};\\xi_{k}^{x}\\right)\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the term $\\hat{\\zeta}_{v}^{2}$ is not guaranteed to be convergent because the stepsizes between the different dimensions of each node are not consistent. Then, similar to the proof of Lemma 9, we can obtain the result presented in (79). ", "page_idx": 36}, {"type": "text", "text": "Next, noticing that for D-TiAda, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\frac{\\mathbf{1}^{T}}{n\\bar{v}_{k+1}^{-\\alpha}}\\tilde{V}_{k+1}^{-\\alpha}\\odot_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\right\\|^{2}\\right]\\leqslant\\frac{1}{n}\\mathbb{E}\\left[\\left\\|\\frac{\\tilde{V}_{k+1}^{-\\alpha}}{\\bar{v}_{k+1}^{-\\alpha}}\\right\\|^{2}\\|\\nabla_{x}F\\left(\\mathbf{x}_{k},\\mathbf{y}_{k};\\xi_{k}^{x}\\right)\\|^{2}\\right]\\leqslant p\\zeta_{V}^{2}C^{2},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and using Lemma 9, we complete the proof. ", "page_idx": 36}, {"type": "text", "text": "Theorem 3. Suppose Assumption 1-4 hold. Let $0<\\beta<\\alpha<1$ and the total iteration satisfy ", "page_idx": 36}, {"type": "equation", "text": "$$\nK=\\Omega\\left(\\operatorname*{max}\\left\\{\\left(\\frac{\\gamma_{x}^{2}\\kappa^{4}}{\\gamma_{y}^{2}}\\right)^{\\frac{1}{\\alpha-\\beta}},\\quad\\left(\\frac{1}{\\left(1-\\rho_{W}\\right)^{2}}\\right)^{\\operatorname*{max}\\left\\{\\frac{1}{\\alpha},\\frac{1}{\\beta}\\right\\}}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "to ensure time-scale separation and quasi-independence of network. For $D$ -AdaSTwithcoordinatewise adaptive stepsize, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\Vert\\nabla\\phi\\left(\\bar{x}_{k}\\right)\\right\\Vert^{2}\\right]}\\\\ &{=\\tilde{\\mathcal{O}}\\left(\\frac{1}{K^{1-\\alpha}}+\\frac{1}{\\left(1-\\rho_{W}\\right)^{\\alpha}K^{\\alpha}}+\\frac{1}{K^{1-\\beta}}+\\frac{1}{\\left(1-\\rho_{W}\\right)K^{\\beta}}\\right)+\\mathcal{O}\\left(n\\left(p\\hat{\\zeta}_{v}^{2}+\\kappa^{2}d\\hat{\\zeta}_{u}^{2}\\right)C^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. With the help of Lemma 11 and the obtained result (75) in the proof of Theorem 2, we can derive the convergence results for D-AdaST with coordinate-wise adaptive stepsize. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "Remark 6. In Theorem 3, we show that the coordinate-wise variant of D-AdaST exhibits a steadystate error in its upper bound. This error depends on the number of nodes and the dimension of the problem, which stems from the stepsize inconsistency in each dimension of the local decision variablesforeachnode( $\\left.c.f.\\right.$ ,Line3ofAlgorithm2). ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The contributions and scope of this work have been accurately discussed. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] , ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We have carefully discussed the limitations of this work in terms of assumptions and main results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have provided a full set of assumptions and complete proof for the theoretical results. See Section 3 and Appendix B. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have provided detailed experimental settings and reproducibility information for the experiments of this work. See Appendix A. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The code of this work is included in the supplementary. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips. cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have provided detailed experimental settings in Appendix A. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Multiple runs with averaging are used to produce the experimental curves in this work. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We have provided sufficient information on the computer resources in AppendixA. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: This work conforms with the NeurIPS Code of Ethics. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [No] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel are negative. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: NA ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 41}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: The license/copyright information of the code and dataset in this paper is clear. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The code of this paper is included in the supplementary. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: NA ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with humansubjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: NA ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]