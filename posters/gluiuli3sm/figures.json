[{"figure_path": "GLUIuli3Sm/figures/figures_8_1.jpg", "caption": "Figure 1: Convergence in terms of average cross-entropy progressive loss of random sampling, loss-based sampling based on the absolute error loss, and our proposed algorithm (loss-based sampling with stochastic Polyak's step size). Our proposed algorithm outperforms the baselines in most cases.", "description": "This figure compares the convergence speed of three different active learning sampling methods: random sampling, loss-based sampling (using absolute error), and the proposed Adaptive-Weight Sampling (AWS) algorithm.  The y-axis represents the average cross-entropy loss (a measure of model error), and the x-axis represents the number of iterations.  The results visually demonstrate that the AWS algorithm converges faster and achieves lower loss than the other two methods across various datasets.", "section": "4 Numerical Results"}, {"figure_path": "GLUIuli3Sm/figures/figures_9_1.jpg", "caption": "Figure 1: Convergence in terms of average cross-entropy progressive loss of random sampling, loss-based sampling based on the absolute error loss, and our proposed algorithm (loss-based sampling with stochastic Polyak's step size). Our proposed algorithm outperforms the baselines in most cases.", "description": "This figure compares the performance of three different sampling methods: random sampling, loss-based sampling using absolute error, and the proposed Adaptive-Weight Sampling (AWS) algorithm, in terms of average cross-entropy progressive loss.  Progressive loss is calculated sequentially on each datapoint. The x-axis represents the number of iterations, and the y-axis represents the average cross-entropy loss. The AWS method consistently shows lower loss than the other two, demonstrating its superior performance.", "section": "4 Numerical Results"}, {"figure_path": "GLUIuli3Sm/figures/figures_19_1.jpg", "caption": "Figure 3: Sampling probability function for the family of generalized smooth hinge loss functions.", "description": "This figure shows the sampling probability function \u03c0* for different values of the parameter a in the generalized smooth hinge loss function. The parameter a controls the smoothness of the hinge loss function, with larger values of a resulting in smoother functions that approach the standard hinge loss function as a goes to infinity.  The plot demonstrates that as the value of a increases, the sampling probability function \u03c0* becomes increasingly concave, approaching the behavior of the standard hinge loss function as a approaches infinity.", "section": "A.5 Linear Classifiers: Generalized Smooth Hinge Loss Function"}, {"figure_path": "GLUIuli3Sm/figures/figures_27_1.jpg", "caption": "Figure 1: Convergence in terms of average cross-entropy progressive loss of random sampling, loss-based sampling based on the absolute error loss, and our proposed algorithm (loss-based sampling with stochastic Polyak's step size). Our proposed algorithm outperforms the baselines in most cases.", "description": "This figure compares the performance of three different sampling methods for active learning: random sampling, loss-based sampling (using the absolute error loss), and the proposed Adaptive-Weight Sampling (AWS) algorithm. The y-axis represents the average cross-entropy progressive loss, while the x-axis shows the number of iterations.  The results demonstrate that the AWS algorithm consistently achieves lower loss than the other two methods across various datasets, showcasing its efficiency in active learning.", "section": "4 Numerical Results"}, {"figure_path": "GLUIuli3Sm/figures/figures_31_1.jpg", "caption": "Figure 5: Average cross entropy loss as a function of labeling cost for different sampling methods.", "description": "This figure demonstrates the efficiency of different sampling methods in active learning. The x-axis represents the labeling cost (number of labeled instances used for training), and the y-axis represents the average cross-entropy loss. Three sampling strategies are compared: random sampling, loss-based sampling (proportional to absolute error loss), and the proposed Adaptive-Weight Sampling (AWS-PA) algorithm.  The results show that AWS-PA outperforms both random and loss-based sampling, achieving lower cross-entropy loss at the same labeling cost. This indicates that AWS-PA effectively selects more informative samples for training, leading to faster convergence and higher model accuracy.", "section": "Numerical Results"}, {"figure_path": "GLUIuli3Sm/figures/figures_31_2.jpg", "caption": "Figure 1: Convergence in terms of average cross-entropy progressive loss of random sampling, loss-based sampling based on the absolute error loss, and our proposed algorithm (loss-based sampling with stochastic Polyak's step size). Our proposed algorithm outperforms the baselines in most cases.", "description": "This figure compares the convergence rate of three different active learning algorithms: random sampling, loss-based sampling (using absolute error), and the proposed Adaptive-Weight Sampling (AWS) algorithm.  The y-axis shows the average cross-entropy progressive loss, which is a measure of how well the model is learning over time.  The x-axis represents the number of iterations.  The AWS algorithm consistently shows faster convergence than the other methods, indicating its improved efficiency in active learning.", "section": "4 Numerical Results"}, {"figure_path": "GLUIuli3Sm/figures/figures_31_3.jpg", "caption": "Figure 1: Convergence in terms of average cross-entropy progressive loss of random sampling, loss-based sampling based on the absolute error loss, and our proposed algorithm (loss-based sampling with stochastic Polyak's step size). Our proposed algorithm outperforms the baselines in most cases.", "description": "The figure compares the performance of three different sampling methods: random sampling, loss-based sampling (absloss), and the proposed Adaptive-Weight Sampling (AWS-PA) algorithm. The y-axis represents the average cross-entropy progressive loss, which measures the model's error during training. The x-axis shows the number of iterations or data points processed.  The plot shows that AWS-PA consistently achieves lower loss than the other two methods, indicating its superior efficiency in learning from a limited set of labeled data.", "section": "4 Numerical Results"}, {"figure_path": "GLUIuli3Sm/figures/figures_32_1.jpg", "caption": "Figure 1: Convergence in terms of average cross-entropy progressive loss of random sampling, loss-based sampling based on the absolute error loss, and our proposed algorithm (loss-based sampling with stochastic Polyak's step size). Our proposed algorithm outperforms the baselines in most cases.", "description": "This figure compares the performance of three different sampling methods for active learning: random sampling, loss-based sampling (using absolute error), and the proposed Adaptive-Weight Sampling (AWS) algorithm.  The y-axis represents the average cross-entropy loss, and the x-axis represents the number of iterations.  The results show that AWS consistently achieves lower loss values than the other two methods across six different datasets, demonstrating its improved efficiency.", "section": "4 Numerical Results"}, {"figure_path": "GLUIuli3Sm/figures/figures_33_1.jpg", "caption": "Figure 2: Active learning sampling based on an estimator of the absolute error loss performs on par with the sampling based on the ground truth value of absolute error loss.", "description": "This figure compares the performance of active learning using two different methods for estimating the absolute error loss: one using the ground truth value and the other using an estimator.  The results show that the estimator performs similarly to using the ground truth loss values across six different datasets. The graph plots the average cross-entropy loss against the number of iterations, visualizing how both methods converge over time.  This suggests that using an estimator for the loss does not significantly impede the performance of the active learning algorithm.", "section": "4 Numerical Results"}, {"figure_path": "GLUIuli3Sm/figures/figures_33_2.jpg", "caption": "Figure 10: Average cross-entropy progressive loss of Polyak's step size compared to SGD with constant step size, for 1% and 10% sampling from the mushrooms data.", "description": "This figure compares the convergence speed of two different optimization methods: SGD with a constant step size and SGD with Polyak's step size.  The experiment is conducted on a subset of the mushrooms dataset, with only 1% and 10% of the data being sampled at each iteration. The graph shows the average cross-entropy loss over iterations. The results demonstrate that using Polyak's step size leads to faster convergence compared to using a constant step size, even when the amount of data used for training is considerably limited.", "section": "B.4 Additional experiments"}, {"figure_path": "GLUIuli3Sm/figures/figures_34_1.jpg", "caption": "Figure 11: Robustness of the proposed sampling approach with adaptive Polyak\u2019s step size for different variance var[labs] = labs(1 \u2212 labs)/(a + labs) noise levels of absolute error loss estimator: (low) a = 100, (medium) a = 2.5, and (high) a = 1.", "description": "This figure shows the robustness of the proposed Adaptive-Weight Sampling algorithm against the noise in the absolute error loss estimator. The y-axis represents the average cross-entropy loss, and the x-axis represents the number of iterations. Four lines are plotted representing different noise levels in the estimator, controlled by parameter 'a': high noise (a=1), medium noise (a=2.5), low noise (a=100), and no noise (a=\u221e, the baseline). The results demonstrate that the algorithm remains effective even with considerable noise in the loss estimation.", "section": "4 Numerical Results"}]