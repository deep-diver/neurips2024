{"references": [{"fullname_first_author": "Nicol\u00f3 Cesa-Bianchi", "paper_title": "Worst-case analysis of selective sampling for linear classification", "publication_date": "2006-01-01", "reason": "This paper provides a worst-case analysis of selective sampling for linear classification, which is a fundamental problem in active learning and directly relevant to the paper's theoretical analysis."}, {"fullname_first_author": "Sanjoy Dasgupta", "paper_title": "Analysis of perceptron-based active learning", "publication_date": "2009-01-01", "reason": "This paper analyzes the perceptron algorithm in active learning, providing theoretical guarantees for convergence rates, which is foundational to the paper's theoretical framework."}, {"fullname_first_author": "Andreas Kirsch", "paper_title": "Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning", "publication_date": "2019-01-01", "reason": "This paper introduces a novel batch acquisition method for active learning, addressing the practical challenge of selecting multiple informative data points simultaneously, which is a relevant area of active learning."}, {"fullname_first_author": "Nicolas Loizou", "paper_title": "Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence", "publication_date": "2021-01-01", "reason": "This paper proposes an adaptive step-size SGD algorithm, which is the basis for the paper's novel algorithm, Adaptive-Weight Sampling (AWS), that combines an adaptive step-size update with a sampling strategy."}, {"fullname_first_author": "Anant Raj", "paper_title": "Convergence of uncertainty sampling for active learning", "publication_date": "2022-07-17", "reason": "This paper establishes convergence guarantees for uncertainty-based active learning algorithms, which is relevant to the paper's investigation of both uncertainty and loss-based active learning."}]}