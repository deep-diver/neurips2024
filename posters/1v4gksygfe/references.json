{"references": [{"fullname_first_author": "Ananya Kumar", "paper_title": "Fine-tuning can distort pretrained features and underperform out-of-distribution", "publication_date": "2022-00-00", "reason": "This paper introduces the LP-FT method, which is the central focus of the current paper's analysis."}, {"fullname_first_author": "Arthur Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-00-00", "reason": "This paper introduces the neural tangent kernel (NTK) theory, which forms the theoretical foundation for the analysis in the current paper."}, {"fullname_first_author": "Jaehoon Lee", "paper_title": "Wide neural networks of any depth evolve as linear models under gradient descent", "publication_date": "2019-00-00", "reason": "This paper provides theoretical support for applying NTK theory to deep neural networks, which is crucial for the current paper's analysis of Transformers."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-00-00", "reason": "This paper introduces the LoRA method, which is also analyzed within the context of NTK theory in the current paper."}, {"fullname_first_author": "Sadhika Malladi", "paper_title": "A kernel-based view of language model fine-tuning", "publication_date": "2023-00-00", "reason": "This paper provides a recent analysis of fine-tuning dynamics using the NTK, offering a comparison and contrast to the current work."}]}