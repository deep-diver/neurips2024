[{"figure_path": "1v4gKsyGfe/figures/figures_4_1.jpg", "caption": "Figure 1: Increase in classifier weight norms during training on the RTE dataset. (a) and (b) show the increase of the both accuracy and classifier weight norms with training. (c) shows classifier weights norms after training.", "description": "This figure shows the increase in classifier weight norms during training for three different methods: LP, FT, and LORA.  The plots show how the norm of the classifier weights and training accuracy change over the number of iterations (LP) or epochs (FT).  The third subplot (c) shows a comparison of the classifier weight norms after training, highlighting the significant increase in norm achieved by LP compared to FT and LORA.", "section": "5 Numerical evaluation with transformer models"}, {"figure_path": "1v4gKsyGfe/figures/figures_7_1.jpg", "caption": "Figure 2: Singular value distribution normalized by the maximum value on the CB dataset, showing the common pre-train-effective component (Pre-train E) and the FT-effective components for each training option.", "description": "This figure displays the distribution of singular values of the NTK matrix for different training methods on the CB dataset. Each line represents a different training method (Pre-train E, FT, LP-FT, LORA, LP-LORA). The singular values are normalized by the maximum singular value to allow for easier comparison across methods. The plot shows the distribution of singular values for the pre-train-effective and FT-effective components of the NTK matrix. The pre-train-effective component is similar across all methods, while the FT-effective component differs depending on the method.", "section": "5 Numerical evaluation with transformer models"}, {"figure_path": "1v4gKsyGfe/figures/figures_8_1.jpg", "caption": "Figure 3: Feature differences on SST-5 (OOD). Solid lines show mean values; shaded areas represent standard errors. Dashed vertical lines indicate the classifier weight norm after training. This figure validates our analysis that larger classifier weight norms reduce feature changes.", "description": "This figure shows the impact of classifier weight norm on feature changes during fine-tuning, specifically focusing on the SST-5 dataset (out-of-distribution). The x-axis represents the norm of the classifier weight after training, while the y-axis represents the norm of the feature difference (the change in features).  Two lines are plotted: one for fine-tuning (FT) alone and one for linear probing then fine-tuning (LP-FT). Shaded regions represent standard errors, highlighting the uncertainty in the measurements. The plot demonstrates that as the classifier weight norm increases, the norm of the feature difference decreases, which is more pronounced in LP-FT compared to FT. This supports the paper's claim that LP-FT leads to reduced feature changes due to the increase in the classifier weight norm during linear probing.", "section": "5.4 Analysis of classifier weight norms and temperature scaling"}, {"figure_path": "1v4gKsyGfe/figures/figures_21_1.jpg", "caption": "Figure 1: Increase in classifier weight norms during training on the RTE dataset. (a) and (b) show the increase of the both accuracy and classifier weight norms with training. (c) shows classifier weights norms after training.", "description": "This figure shows the increase in classifier weight norms during the training process on the RTE dataset for three different fine-tuning methods: Linear Probing (LP), Fine-tuning (FT), and LP-FT (Linear Probing then Fine-tuning).  Subfigure (a) displays the training dynamics of LP showing the increase in both accuracy and classifier weight norm. Subfigure (b) shows the same for FT. Finally, subfigure (c) compares the final classifier weight norms after training for all three methods, highlighting the significantly larger norm achieved by LP-FT.", "section": "5 Numerical evaluation with transformer models"}, {"figure_path": "1v4gKsyGfe/figures/figures_21_2.jpg", "caption": "Figure 5: Small changes in feature and large changes in classifier weight during LP-FT. We visualize the t-SNE plot of the penultimate layer features and the classifier row vector of the model trained on the CB dataset. (a) The features after FT are clearly separated by class, while the classifier row vectors are plotted nearly the same place as the pre-trained model. (b) The features after LP-FT keep the structure of the pre-trained model, while the classifier row vectors are changed from the initialization.", "description": "This figure uses t-SNE to visualize the feature vectors (penultimate layer) and classifier weights of a model trained on the CB dataset using two different methods: standard fine-tuning (FT) and linear probing then fine-tuning (LP-FT).  In (a), standard FT shows clearly separated feature clusters by class, but classifier weights remain close to the initial pretrained weights. In (b), LP-FT shows that feature clusters maintain a similar structure to the pretrained model, while classifier weights have notably shifted from their initial state. This visualization supports the paper's claim that LP-FT causes smaller changes to features during training than standard FT while significantly altering classifier weights.", "section": "5.2 Small feature changes during LP-FT and significant norm increase during LP"}, {"figure_path": "1v4gKsyGfe/figures/figures_22_1.jpg", "caption": "Figure 5: Small changes in feature and large changes in classifier weight during LP-FT. We visualize the t-SNE plot of the penultimate layer features and the classifier row vector of the model trained on the CB dataset. (a) The features after FT are clearly separated by class, while the classifier row vectors are plotted nearly the same place as the pre-trained model. (b) The features after LP-FT keep the structure of the pre-trained model, while the classifier row vectors are changed from the initialization.", "description": "This figure uses t-SNE to visualize the changes in features and classifier weights after fine-tuning (FT) and linear probing then fine-tuning (LP-FT).  Panel (a) shows that standard FT leads to clear separation of features by class, but the classifier weights remain close to their pre-trained values.  Panel (b) demonstrates that LP-FT preserves the structure of the pre-trained features while substantially altering the classifier weights. This visually supports the paper's claim that LP-FT minimizes feature changes while effectively optimizing the linear head.", "section": "5.2 Small feature changes during LP-FT and significant norm increase during LP"}, {"figure_path": "1v4gKsyGfe/figures/figures_22_2.jpg", "caption": "Figure 7: Singular value distribution normalized by the maximum singular value on the RTE, BoolQ, and WiC datasets. Pre-train E denotes the pre-train-effective component, and other plots denote the FT-effective component of NTK matrix with each training option.", "description": "This figure shows the distribution of singular values for the NTK matrices obtained through different fine-tuning methods (FT, LP-FT, LoRA, LP-LORA) and the pre-trained model on three different datasets (RTE, BoolQ, WiC). The singular values are normalized by the maximum singular value. The plot visually illustrates how the singular value distribution changes with different fine-tuning strategies and datasets, providing insights into the training dynamics and feature extraction.", "section": "5.3 Kernel analysis"}, {"figure_path": "1v4gKsyGfe/figures/figures_23_1.jpg", "caption": "Figure 1: Increase in classifier weight norms during training on the RTE dataset. (a) and (b) show the increase of the both accuracy and classifier weight norms with training. (c) shows classifier weights norms after training.", "description": "This figure visualizes the increase in classifier weight norms during the training process on the RTE dataset using three different methods: LP (linear probing), FT (fine-tuning), and LP-FT (linear probing then fine-tuning).  The plots show that the classifier weight norms increase over the training iterations/epochs for all methods, but significantly more for LP-FT than other methods.  This indicates how the  LP-FT strategy optimizes the linear head during the LP stage, resulting in a large increase in the norm at the beginning of the FT stage. This phenomenon is linked to the reduction of feature changes, and is considered one of the reasons for LP-FT's high performance.", "section": "5 Numerical evaluation with transformer models"}, {"figure_path": "1v4gKsyGfe/figures/figures_26_1.jpg", "caption": "Figure 1: Increase in classifier weight norms during training on the RTE dataset. (a) and (b) show the increase of the both accuracy and classifier weight norms with training. (c) shows classifier weights norms after training.", "description": "This figure shows the increase in classifier weight norms during training on the RTE dataset.  The left panel (a) displays the increase in training accuracy and classifier weight norm during linear probing (LP). The middle panel (b) shows the same during fine-tuning (FT). Finally, the right panel (c) shows the final classifier weight norms after the training is complete. The figure demonstrates that the classifier weight norm increases during both LP and FT stages.", "section": "5 Numerical evaluation with transformer models"}, {"figure_path": "1v4gKsyGfe/figures/figures_27_1.jpg", "caption": "Figure 1: Increase in classifier weight norms during training on the RTE dataset. (a) and (b) show the increase of the both accuracy and classifier weight norms with training. (c) shows classifier weights norms after training.", "description": "This figure shows how the classifier weights' norms change during the training process of different fine-tuning methods on the RTE dataset.  It visually represents three key observations:\n\n1. **Increase in Norm:** The norm of the classifier weights increases notably during training for LP, FT, and LP-FT. This highlights the impact of training on the model's linear head.\n2. **Accuracy Correlation:** The increase in classifier weight norm is correlated with improvements in training accuracy for each method.  This shows the linear head's optimization plays a key role in performance.\n3. **Post-Training Norms:** The final classifier weight norms after training (panel (c)) reveal that LP-FT results in classifier weights with higher norms compared to FT alone. This suggests that LP-FT better preserves pre-trained features while achieving improved performance.", "section": "5 Numerical evaluation with transformer models"}, {"figure_path": "1v4gKsyGfe/figures/figures_27_2.jpg", "caption": "Figure 3: Feature differences on SST-5 (OOD). Solid lines show mean values; shaded areas represent standard errors. Dashed vertical lines indicate the classifier weight norm after training. This figure validates our analysis that larger classifier weight norms reduce feature changes.", "description": "This figure shows the impact of classifier weight norm on feature changes during fine-tuning. The SST-5 dataset (out-of-distribution) was used for this experiment. The y-axis represents the norm of feature difference and x-axis represents the norm of classifier weight. The solid lines show average values and shaded area represents standard errors. As the norm of classifier weight increases, the norm of feature difference decreases, supporting the paper's analysis that a higher classifier weight norm reduces feature distortion.", "section": "5.4 Analysis of classifier weight norms and temperature scaling"}]