[{"figure_path": "nExI4FuKWD/figures/figures_2_1.jpg", "caption": "Figure 1: Overall architecture of FineCLIP. For simplicity, the diagram omits unused visual dense features of regions extracted by ViT and textual dense features generated by BERT. By integrating multi-grained contrastive learning as well as a real-time self-distillation scheme, FineCLIP aligns visual global embedding, regional dense features, and textual global embedding into a unified space, acquiring rich coarse and fine-grained knowledge from image-text and generated region-text pairs.", "description": "This figure illustrates the overall architecture of FineCLIP, highlighting its three main components: global contrastive learning, regional contrastive learning, and real-time self-distillation.  It shows how FineCLIP processes image and text data to align visual and textual embeddings at both global and regional levels, leveraging a large vision-language model (LVLM) to generate semantically rich region-text pairs. The diagram simplifies the representation by omitting unused visual and textual dense features for clarity.", "section": "2.2 FineCLIP"}, {"figure_path": "nExI4FuKWD/figures/figures_6_1.jpg", "caption": "Figure 2: Zero-shot comparisons of models pre-trained on datasets in three different scales.", "description": "This figure displays the performance of various models (CLIP, RegionCLIP, CLIPSelf, and FineCLIP) on three different tasks: COCO box classification, COCO R@1 image-to-text retrieval, and COCO R@1 text-to-image retrieval.  The models were pre-trained on datasets of three different sizes (100K, 500K, and 2.5M samples). The graphs clearly show FineCLIP's superior performance and superior scaling ability compared to other methods across all three tasks and dataset scales.", "section": "3.3 Comparisons on Scaled Trainset"}, {"figure_path": "nExI4FuKWD/figures/figures_7_1.jpg", "caption": "Figure 1: Overall architecture of FineCLIP. For simplicity, the diagram omits unused visual dense features of regions extracted by ViT and textual dense features generated by BERT. By integrating multi-grained contrastive learning as well as a real-time self-distillation scheme, FineCLIP aligns visual global embedding, regional dense features, and textual global embedding into a unified space, acquiring rich coarse and fine-grained knowledge from image-text and generated region-text pairs.", "description": "This figure illustrates the overall architecture of the proposed FineCLIP model.  It shows how FineCLIP combines global and regional contrastive learning with a real-time self-distillation mechanism.  The diagram shows the processing of images and texts through Vision and Language encoders, including the extraction of global and regional visual features.  It highlights the generation of region-text pairs using a Large Vision-Language Model (LVLM), emphasizing the model's ability to learn from both coarse-grained (global) and fine-grained (regional) image-text information.", "section": "2.2 FineCLIP"}, {"figure_path": "nExI4FuKWD/figures/figures_15_1.jpg", "caption": "Figure 4: K-Means visualization of the dense features of CLIP ViT. We show the raw images, the K-means results of the pre-trained CLIP ViT, and those of our FineCLIP ViT.", "description": "This figure visualizes the dense features extracted by CLIP and FineCLIP using k-means clustering.  It shows the original images alongside the k-means clustering results for both CLIP and FineCLIP. The visualization helps illustrate how FineCLIP produces more focused and semantically consistent dense features compared to CLIP.", "section": "A.3 Comparisons with CLIPSelf Training Setting"}, {"figure_path": "nExI4FuKWD/figures/figures_16_1.jpg", "caption": "Figure 1: Overall architecture of FineCLIP. For simplicity, the diagram omits unused visual dense features of regions extracted by ViT and textual dense features generated by BERT. By integrating multi-grained contrastive learning as well as a real-time self-distillation scheme, FineCLIP aligns visual global embedding, regional dense features, and textual global embedding into a unified space, acquiring rich coarse and fine-grained knowledge from image-text and generated region-text pairs.", "description": "This figure illustrates the architecture of FineCLIP, highlighting its key components: global contrastive learning, regional contrastive learning, and real-time self-distillation.  FineCLIP processes image-text pairs and generated region-text pairs to align visual global embeddings, regional dense features, and textual global embeddings in a unified space, thereby capturing both coarse-grained and fine-grained information.", "section": "2.2 FineCLIP"}]