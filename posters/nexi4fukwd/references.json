{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, the foundational model that FineCLIP builds upon, significantly impacting vision-language representation learning."}, {"fullname_first_author": "Liunian Harold Li", "paper_title": "Grounded language-image pre-training", "publication_date": "2021-06-14", "reason": "This paper is highly relevant as it directly addresses fine-grained understanding in vision-language models, a key focus of FineCLIP."}, {"fullname_first_author": "Jiahui Yu", "paper_title": "Coca: Contrastive captioners are image-text foundation models", "publication_date": "2022-07-18", "reason": "This paper provides a strong baseline for image-text retrieval, which FineCLIP aims to improve upon, particularly in fine-grained aspects."}, {"fullname_first_author": "Size Wu", "paper_title": "CLIPSelf: Vision transformer distills itself for open-vocabulary dense prediction", "publication_date": "2024-01-01", "reason": "This is a very closely related work that also focuses on improving CLIP's fine-grained understanding; FineCLIP builds upon and improves upon the self-distillation technique presented here."}, {"fullname_first_author": "Yiwu Zhong", "paper_title": "RegionCLIP: Region-based language-image pre-training", "publication_date": "2022-06-14", "reason": "This paper proposes a method to improve CLIP's performance on fine-grained tasks; FineCLIP is an advancement on the ideas presented here."}]}