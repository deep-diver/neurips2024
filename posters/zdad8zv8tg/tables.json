[{"figure_path": "zDaD8zv8tG/tables/tables_6_1.jpg", "caption": "Table 1: For each positive concept-text pair, we randomly selected 100 other texts to replace the original text, forming negative pairs. We then calculated the cosine similarity between the mean of the clinical concept list and the text, with embeddings generated by various models. By ranking these cosine similarities from highest to lowest, we identified the rank of each positive pair among its negative pairs and computed the mean rank, mean reverse rank, and Top-10 accuracy (Top10@Acc). Improvements over the corresponding teacher models are indicated by \"\u2193\" for mean rank and \"\u2191\" for mean reverse rank and Top10@Acc.", "description": "This table presents the results of a cosine similarity experiment designed to evaluate the alignment of clinical text embeddings with their corresponding clinical concept embeddings.  It compares various models (including BioBERT, Clinical BioBERT, SapBERT, PubMedBERT, CODER, BGE, GPT-4, and two versions of the LINE model) by calculating their mean rank, mean reverse rank, and Top-10 accuracy in correctly aligning positive text-concept pairs against randomly selected negative pairs.  Lower mean rank and higher mean reverse rank and Top10@Acc indicate better alignment.", "section": "3.2 Validation on alignment objective"}, {"figure_path": "zDaD8zv8tG/tables/tables_7_1.jpg", "caption": "Table 2: AUCs for detecting related pairs versus randomly selected pairs under various models. The classes of clinical concepts include parent-child hierarchy, siblings hierarchy, may treat or may prevent, classifies as, differential diagnosis, method of, and causative of. The last row lists the detailed number of positive pairs in each class. Improvements over the corresponding teacher models are indicated by \u201c\u2191\u201d.", "description": "This table presents the Area Under the Curve (AUC) for various models in detecting related pairs of clinical concepts compared to randomly selected pairs.  The clinical concepts are categorized into seven relationship types (parent-child, sibling, may treat/prevent, classifies as, differential diagnosis, method of, causative of).  The AUC is calculated for each relationship type and model, showing the model's ability to distinguish true relationships from random associations. Improvements achieved by the LINE model over its constituent teacher models (BGE and CODER or GPT-4 and CODER) are highlighted.", "section": "3.3.1 Clinical concept similarity"}, {"figure_path": "zDaD8zv8tG/tables/tables_7_2.jpg", "caption": "Table 3: The mean of the F1 scores for different models over five random initializations on two i2b2 datasets. The results for BioBERT, Clinical BioBERT and UmlsBERT were directly copied from [16] for comparison. Here, the LINE projection is applied to the token-level BGE embeddings. The best result under each metric is highlighted in bold font.", "description": "This table presents the mean F1 scores achieved by different models on two i2b2 datasets (i2b2 2006 and i2b2 2014).  The models compared include BioBERT, Clinical BioBERT, UmlsBERT, CODER, BGE, and the proposed LINE model. The LINE model uses token-level BGE embeddings and the results are averaged over five runs with different random initializations. The best F1 scores for each dataset are highlighted in bold, allowing for easy comparison of model performance.", "section": "3.3 Validation on downstream tasks"}, {"figure_path": "zDaD8zv8tG/tables/tables_8_1.jpg", "caption": "Table 4: Performance metrics of sentence embeddings and their proxy embeddings generated from concept lists for various models, averaged over five-fold cross-validation. Improvements over the corresponding teacher models are indicated by \u201c\u2191\u201d.", "description": "This table presents the performance of different models in generating sentence embeddings and proxy embeddings (from concept lists) using five-fold cross-validation.  Metrics include precision, recall, F1-score, and accuracy for both concept and sentence embeddings.  The improvement of LINE over its base models is highlighted.", "section": "3.3 Validation on downstream tasks"}, {"figure_path": "zDaD8zv8tG/tables/tables_8_2.jpg", "caption": "Table 5: Difference in performance metrics between sentence embeddings and their proxy embeddings generated from concept lists, calculated using results in Table 4. Reductions in difference, which indicate improved alignment, are marked by \"\u2193\"", "description": "This table presents the differences in precision, recall, F1-score, and accuracy between sentence embeddings and their proxy embeddings generated from concept lists. The results are calculated based on the data from Table 4. A reduction in the difference indicates that the alignment between the two types of embeddings has improved.  The table shows the differences for two model configurations: BGE+CODER and GPT-4+CODER.", "section": "3.3 Validation on downstream tasks"}, {"figure_path": "zDaD8zv8tG/tables/tables_13_1.jpg", "caption": "Table 1: For each positive concept-text pair, we randomly selected 100 other texts to replace the original text, forming negative pairs. We then calculated the cosine similarity between the mean of the clinical concept list and the text, with embeddings generated by various models. By ranking these cosine similarities from highest to lowest, we identified the rank of each positive pair among its negative pairs and computed the mean rank, mean reverse rank, and Top-10 accuracy (Top10@Acc). Improvements over the corresponding teacher models are indicated by \"\u2193\" for mean rank and \"\u2191\" for mean reverse rank and Top10@Acc.", "description": "This table presents the results of an experiment to evaluate the alignment of clinical text and concept embeddings generated by different models.  The experiment uses a contrastive approach, comparing the similarity of positive pairs (clinical text with its associated concepts) against negative pairs (clinical text with randomly selected concepts). The table shows the mean rank, mean reverse rank, and top-10 accuracy for each model, highlighting improvements achieved by the LINE framework.", "section": "3 Training and validation"}, {"figure_path": "zDaD8zv8tG/tables/tables_14_1.jpg", "caption": "Table A2: Further results for Table 1.", "description": "This table provides additional results for the alignment objective evaluation, comparing different models' performance in aligning the embedding of clinical texts with their associated concept embeddings. The metrics used are Mean Rank, Mean Reverse Rank, and Top10@Acc, which measure the degree of alignment between positive and negative pairs. The table compares the performance of CODER\u2192BGE, BGE\u2192CODER, and the proposed LINE framework (BGE+CODER).", "section": "A.4 Further experiment results"}, {"figure_path": "zDaD8zv8tG/tables/tables_14_2.jpg", "caption": "Table 4: Performance metrics of sentence embeddings and their proxy embeddings generated from concept lists for various models, averaged over five-fold cross-validation. Improvements over the corresponding teacher models are indicated by \u201c\u2191\u201d.", "description": "This table presents the performance comparison of different models in generating sentence embeddings and their proxy embeddings from concept lists.  The models are evaluated using precision, recall, F1-score, and accuracy metrics across five-fold cross-validation.  Improvements achieved by the LINE model over its constituent teacher models are highlighted.", "section": "3.3 Validation on downstream tasks"}, {"figure_path": "zDaD8zv8tG/tables/tables_15_1.jpg", "caption": "Table 3: The mean of the F1 scores for different models over five random initializations on two i2b2 datasets. The results for BioBERT, Clinical BioBERT and UmlsBERT were directly copied from [16] for comparison. Here, the LINE projection is applied to the token-level BGE embeddings. The best result under each metric is highlighted in bold font.", "description": "This table compares the performance of different models on two i2b2 datasets (i2b2 2006 and i2b2 2014) for a named entity recognition task.  The models compared include BioBERT, Clinical BioBERT, UmlsBERT, CODER, BGE, and the proposed LINE model.  The F1 score, a common metric for evaluating the performance of classification models, is reported for both the validation and test sets of each dataset.  The LINE model uses token-level BGE embeddings, which are projected using the LINE module.  The best F1 scores for each dataset and set (validation/test) are highlighted in bold.", "section": "3.3 Validation on downstream tasks"}]