[{"type": "text", "text": "Grammar-Aligned Decoding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kanghee Park1\u2217 Jiayu Wang1\u2217 Taylor Berg-Kirkpatrick2 Nadia Polikarpova Loris D\u2019Antoni1 ", "page_idx": 0}, {"type": "text", "text": "1University of Wisconsin-Madison 2University of California San Diego {kpark247, jwang2782, ldantoni}@wisc.edu, {tberg, npolikarpova}@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM\u2019s output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM\u2019s distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM\u2019s distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM\u2019s distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints. 2 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite their remarkable success, pre-trained Large Language Models (LLMs) often struggle with generating highly structured outputs, such as program code, configuration files, or mathematical formulas. A na\u00efve approach to enforcing structure is rejection sampling, which repeatedly samples strings from the LLM and checks them against a validity oracle, typically in the form of a contextfree grammar (CFG). Rejection sampling is highly inefficient or simply intractable for restrictive grammars and long output sequences\u2014i.e., most generated strings will not be in the target grammar. ", "page_idx": 0}, {"type": "text", "text": "Constrained decoding addresses the inefficiency of rejection sampling by greedily \u201cforcing\u201d the LLM output to satisfy the given constraint. Specifically, when the constraint is given as a grammar, grammar-constrained decoding (GCD) [7, 27, 28], can build automata that allow for on-the-fly masking of tokens that will provably lead to outputs outside of the grammar during decoding. ", "page_idx": 0}, {"type": "text", "text": "While GCD does not incur the overhead of rejection sampling\u2014i.e., the generated output is always in the language of the grammar\u2014we show that GCD and in general all forms of structured decoding introduce a new problem: structured decoding distorts the LLM\u2019s learned language distribution, effectively hindering the LLM\u2019s capabilities. ", "page_idx": 0}, {"type": "text", "text": "This paper introduces and formalizes grammar-aligned decoding (GAD), the problem of sampling from an LLM so that the outputs (1) are guaranteed to adhere to a given grammar, and (2) are unbiased wrt. the LLM\u2019s distribution. Although exact GAD is intractable in general (similar to rejection sampling), we propose a new adaptive decoding algorithm for approximate GAD, which starts off as GCD and gradually converges to the LLM\u2019s distribution, and thus allows trading off between efficiency and accuracy. The algorithm, which we dub Adaptive Sampling with Approximate Expected Futures (ASAp), is built \u201con top\u201d of existing constrained decoding algorithms. Whereas GCD approaches simply mask out tokens that lead to non-grammatical sequences for a given prefix, ASAp remembers for all sampled prefixes the probability associated with masked-out tokens and uses it to upper bound the probability of grammaticality. By updating this bound when more samples are observed, the decoding algorithm converges to the desired probability distribution\u2014i.e., it samples outputs from the LLM-induced probability conditioned on the outputs being accepted by the grammar. The idea works for any structured decoding approach and not just for GCD, but in this paper we focus our evaluation on constraints expressed via grammars. ", "page_idx": 1}, {"type": "text", "text": "We evaluate ASAp on two structured prediction tasks: formal program synthesis and constituency parsing. Our experiments on program synthesis and NLP tasks show that GCD techniques generate outputs that are grammatical but unlikely according to the LLM, while with ASAp, the likelihood of the generated outputs improves over time, converging to the target constrained LLM\u2014i.e., GAD better respects the LLM while still enforcing the constraints. ", "page_idx": 1}, {"type": "text", "text": "2 Grammar-Aligned Decoding ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we formalize the problem of grammar-aligned decoding (GAD) as decoding from an autoregressive language model while enforcing the output sequence to be accepted by a given context-free grammar. We also demonstrate the limitations of existing approaches to this problem. ", "page_idx": 1}, {"type": "text", "text": "Language Models An (autoregressive) language model defines a probability distribution $P$ on the set of all strings $w\\in\\Sigma^{*}$ over a vocabulary of tokens $\\Sigma$ via a product of left-to-right next-token conditional distributions $P(w_{1}\\ldots w_{n})=\\Pi_{i=1}^{n}P(w_{i}\\mid w_{1:i-1})$ . ", "page_idx": 1}, {"type": "text", "text": "Context-Free Grammars A context-free grammar   \n(CFG) is a quadruple $\\mathcal{G}\\;=\\;(\\Sigma,\\mathcal{N},S,\\mathcal{R})$ , where $\\Sigma$ is $\\begin{array}{r l}{S::=}&{{}\\;060\\Theta\\Theta\\mid1A_{2}}\\\\ {A_{i}:=}&{{}\\;0A_{i+1}\\mid1A_{i+1},\\operatorname{for}i=2,3,4}\\\\ {A_{5}:=}&{{}\\;\\Theta\\mid1}\\end{array}$ a vocabulary of tokens (also called terminal symbols), $\\mathcal{N}$   \nis a finite set of non-terminal symbols, $S\\in N$ is the start  \ning non-terminal, and $\\mathcal{R}$ is the set of production rules. An   \nexample CFG is shown in Fig. 1. A grammar $\\mathcal{G}$ defines Figure 1: CFG $\\mathcal{G}_{s k}$ over tokens $\\Sigma=\\{0,1\\}$ , a single-step derivation relation on sequences of symbols written in Backus-Naur form (BNF) notation. $\\alpha,\\beta,\\gamma\\,\\in\\,({\\mathcal{N}}\\cup\\Sigma)^{*}$ : $\\alpha A\\gamma\\,\\Rightarrow\\,\\alpha\\beta\\gamma$ if $A\\,\\rightarrow\\,\\beta\\,\\in\\,\\mathcal{R}$ . This grammar accepts the string 00000 and all The reflexive transitive closure of this relation is called length-5 strings that start with a 1.   \nderivation and written $\\Rightarrow^{*}$ . A sequence of tokens $w$ is a   \nsentence if it is derivable from $S$ ; the set of all sentences is called the language of the grammar $\\mathcal{G}$ , that is, $\\mathcal{L}(\\mathcal{G})=\\{w\\in\\Sigma^{*}\\mid S\\Rightarrow^{*}w\\}$ . The following example illustrates these definitions. ", "page_idx": 1}, {"type": "text", "text": "Example 1 (CFG Derivations). Given the CFG $\\mathcal{G}_{s k}$ shown in Fig. 1, the string 00000 belongs to the language $\\mathcal{L}(\\mathcal{G}_{s k})$ because it can be derived using the derivation $S\\Rightarrow\\theta\\theta\\theta\\theta\\theta$ . The string 10101 is also in $\\mathcal{L}(\\mathcal{G}_{s k})$ and can be derived as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\nS\\Rightarrow1A_{2}\\Rightarrow1\\theta A_{3}\\Rightarrow1\\theta1A_{4}\\Rightarrow1\\theta1\\theta A_{5}\\Rightarrow1\\theta1\\theta1\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Each step replaces a nonterminal symbol using a production rule in $\\mathcal{G}_{s k}...\\boldsymbol{e}.\\boldsymbol{g}.$ ., in the string $2\\theta A_{3}$ , the nonterminal $A_{3}$ is rewritten as $\\imath A_{4}$ by applying the rule $A_{3}\\rightarrow1A_{4}$ , resulting in the string $\\hphantom{0}\\dphantom{0}A_{4}$ . ", "page_idx": 1}, {"type": "text", "text": "In addition, we define the prefix language of $\\mathcal{G}$ as the set of all prefixes of sentences in ${\\mathcal{L}}({\\mathcal{G}})$ : $\\mathcal{L}_{\\mathrm{prefix}}(\\mathcal{G})=\\{w\\in\\Sigma^{*}\\mid w\\bar{v}\\in\\mathcal{L}(\\mathcal{G})\\}$ . ", "page_idx": 1}, {"type": "text", "text": "Grammar-Aligned Decoding Given a model distribution $P$ and a CFG $\\mathcal{G}$ , grammar-aligned decoding $(G A D)$ is the task of sampling from the distribution $Q^{P,\\mathcal{G}}$ that is proportional to $P$ but restricted to sentences in $\\mathcal{G}$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\nQ^{P,\\mathcal{G}}(w)=\\frac{\\mathbb{1}[w\\in\\mathcal{L}(\\mathcal{G})]\\cdot P(w)}{\\sum_{w^{\\prime}}\\mathbb{1}[w^{\\prime}\\in\\mathcal{L}(\\mathcal{G})]\\cdot P(w^{\\prime})}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "When $P$ and $\\mathcal{G}$ are clear from context, we will write $Q(w)$ instead of $Q^{P,\\mathcal{G}}(w)$ . ", "page_idx": 2}, {"type": "text", "text": "Example 2 (GAD). Consider the distribution $P$ that arises from prompting an LLM to \u201cgenerate a binary string that ends with a 1\u201d. We expect $P$ to assign high probability to strings of the form $(\\theta\\mid\\l^{\\l}\\l^{\\l})^{*}\\r{\\r{\\r}}^{\\l}$ \u2014i.e. those that satisfy the prompt (Mixtral-8x7B-Instruct-v0.1 (temperature $^{-l}$ ) generates binary strings that end with a 1 around $90\\%$ of the time.) $A$ snippet of a possible distribution $P$ is depicted in Fig. 2. ", "page_idx": 2}, {"type": "text", "text": "Suppose we constrain the model\u2019s output to the language of the grammar $\\mathcal{G}_{s k}$ in Fig. 1, which only accepts strings of length 5. Moreover, $\\mathcal{G}_{s k}$ only accepts one string that starts with 0, i.e., 00000, which does not end with 1. In Fig. 2, the grayed out parts of the trie are tokens that lead to sequences outside of the grammar $\\mathcal{G}_{s k}$ . According to the definition of GAD, the target sampling distribution $Q^{P,\\mathcal{G}_{s k}}$ should assign: (i) high probability to all eight strings of the form $2w_{2}w_{3}w_{4}\\mathbb{1}$ \u2014which conform both to the grammar and the prompt; (ii) low probability to the string 00000\u2014which conforms to the grammar but not the prompt; and (iii) zero probability to all other strings. ", "page_idx": 2}, {"type": "text", "text": "Exact GAD Can one exactly sample from $Q^{P,\\mathcal{G}}$ ? Rejection sampling, which repeatedly draws from $P$ until a sample lands in $\\mathcal{L}(\\mathcal{G})$ , provably yields exact samples according to $\\stackrel{\\triangledown}{Q}^{P,\\boldsymbol{g}}$ , but if $P$ assigns most of its mass outside of ${\\mathcal{L}}({\\mathcal{G}})$ , it is intractably slow, especially if the prompt is not including information about the grammar (see [27]). For Ex. 2, rejection sampling would be highly inefficient because the model would generate many strings that are not of length five. ", "page_idx": 2}, {"type": "text", "text": "In contrast, exact sampling from $P$ is efficient because its joint distribution is represented by a product of easily computed left-to-right conditionals, enabling ancestral sampling (i.e., generating tokens left to right, conditioned on already generated tokens). Can we similarly factor $Q$ into a product of left-to-right conditionals $Q^{P,\\bar{\\mathcal{G}}}(w_{i}|w_{1:i-1})$ , to enable ancestral sampling? ", "page_idx": 2}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/9e582155f394b34e695f0abb8887b3cf462415966f0fa8b64249efca0186efcf.jpg", "img_caption": ["Figure 2: Fragment of the conditional model distribution $P$ for Ex. 2 depicted as a trie. Each node corresponds to a prefix $w_{1:i-1}$ , and each edge is annotated with the next token $w_{i}$ and its conditional probability $P(w_{i}\\mid w_{1:i-1})$ . Filled nodes are complete strings. Grayed out parts of the trie are outside of the grammar $\\mathcal{G}_{s k}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "For simplicity, let us assume that $P$ is a distribution over sequences of exactly length $n$ (although, in practice, language models can produce \u2018stop\u2019 tokens which allow for a valid distribution on sequences of all lengths). The exact conditionals of $\\bar{Q}^{P,\\mathcal{G}}$ are given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Q^{P,\\mathcal{G}}(w_{i}\\mid w_{1:i-1})}&{\\propto}&{\\sum_{w_{i+1:n}}\\left[\\mathbb{1}\\lbrack w\\in\\mathcal{L}(\\mathcal{G})\\rbrack\\cdot\\Pi_{j=i}^{n}P(w_{j}\\mid w_{1:j-1})\\right]}\\\\ &{\\propto}&{P(w_{i}\\mid w_{1:i-1})\\cdot\\mathbb{E}_{P(w_{i+1:n}\\mid w_{1:i})}[\\mathbb{1}\\lbrack w\\in\\mathcal{L}(\\mathcal{G})\\rbrack]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Thus, exact left-to-right sampling from $Q^{P,\\mathcal{G}}$ consists of sampling from model conditionals $P(w_{i}\\mid$ $w_{1:i-1})$ , with an additional weighting term $c(w_{1:i})=\\mathbb{E}_{P(w_{i+1:n}|w_{1:i})}[\\mathbb{1}[w\\in{\\mathcal{L}}({\\mathcal{G}})]]$ that considers the grammar. ", "page_idx": 2}, {"type": "text", "text": "We refer to $c(w_{1:i})$ as expected future grammaticality (EFG), i.e. the probability that a continuation of $w_{1:i}$ sampled from $P$ lands in $\\mathcal{L}(\\mathcal{G})$ . Using this notation, we can write the exact left-to-right sampling conditional explicitly as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ^{P,\\mathcal{G}}(w_{i}\\mid w_{1:i-1})=\\frac{P(w_{i}\\mid w_{1:i-1})\\cdot c(w_{1:i})}{\\sum_{w_{i}^{\\prime}}P(w_{i}^{\\prime}\\mid w_{1:i-1})\\cdot c(w_{1:i-1},w_{i}^{\\prime})}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "To see why computing this conditional is intractable, consider using dynamic programming to compute $c(w_{1:i})$ by marginalizing over a product of potential functions: the set of model conditionals and an indicator potential for the grammar. While the indicator potential can be factorized across rules in the grammar, the model\u2019s contribution generally does not factorize: in practice, the final conditional probability $P(w_{n}\\mid w_{1:n-1})$ is a global potential function, defined by a non-linear neural network touching every variable. Thus, the main goal of this paper is to develop effective approximations to the EFG $c(w_{1:i})$ , which would enable us to compute the left-to-right conditionals of $Q$ . ", "page_idx": 2}, {"type": "text", "text": "Limitations of Grammar-Constrained Decoding Existing work [27, 28] has proposed grammarconstrained decoding (GCD) as a way to efficiently sample from an autoregressive language model subject to grammar constraints. Although the exact details of these techniques vary depending on class of grammars they support, the common thread is that they rely on an incremental parser, which can efficiently check whether a given string $w$ is a prefix of a sentence in the grammar, i.e., $w\\in{\\mathcal{L}}_{\\mathrm{prefix}}({\\mathcal{G}})$ . When given a sentence $w_{1:i-1}$ , GCD techniques use this parser during decoding to mask out any next token $w_{i}$ that results in a prefix $w_{1:i}$ for which no completion will produce a sequence in the grammar. Using the trie in Fig. 2 as an example, one can think of GCD as sampling a path through the trie by selecting only among the black outgoing edges from every node, proportional to their conditional probabilities in the diagram (e.g. the first token is 0 or 1 with equal probability). ", "page_idx": 3}, {"type": "text", "text": "In terms of the GAD problem, we can view GCD as approximating the exact left-to-right conditionals $Q^{P,\\mathcal{G}}(w_{i}\\mid w_{1:i-1})$ by the conditional distribution $\\tilde{Q}_{\\mathrm{GCD}}^{-}(w_{i}\\mid w_{1:i-1})$ , defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{Q}_{\\mathrm{GCD}}(w_{i}\\mid w_{1:i-1})=\\frac{P(w_{i}\\mid w_{1:i-1})\\cdot\\mathbb{1}[w_{1:i}\\in\\mathcal{L}_{\\mathrm{prefix}}(\\mathcal{G})]}{\\sum_{w_{i}^{\\prime}}P(w_{i}^{\\prime}\\mid w_{1:i-1})\\cdot\\mathbb{1}[w_{1:i-1},w_{i}^{\\prime}\\in\\mathcal{L}_{\\mathrm{prefix}}(\\mathcal{G})]}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Though not originally formulated in this way, we can view recent work on GCD [27, 28] as forming a binary approximation $\\mathbb{1}[w_{1:i}\\in\\mathcal{L}_{\\mathrm{prefix}}(\\mathcal{G})]$ to the EFG $c(w_{1:i})$ . In other words, while GCD considers the possibility of future grammaticality, it makes no attempt to integrate the model\u2019s likelihood to estimate expected future grammaticality, which can lead to substantial bias in the sampling distribution\u2014i.e., every EFG such that $c(w_{1:i})>0$ will simply be approximated via the value 1. ", "page_idx": 3}, {"type": "text", "text": "Example 3 (GCD). Consider again the GAD problem from Ex. 2, where our target sampling distribution $Q^{P,\\mathcal{G}_{s k}}$ assigns high probability to strings that both start and end with a 1 and a low probability to the string 00000. However, we observe that GCD [8] generates strings ending with a 1 only $30\\%$ of the time\u2014i.e., GCD has effectively ruined the LLM\u2019s ability to follow the prompt by biasing sampling towards 00000, an incorrect output. ", "page_idx": 3}, {"type": "text", "text": "When generating the first token (0 or 1), the GCD algorithm does not know how many grammatical strings can start with each character and, more importantly, how likely these strings are under $P$ . Since both tokens 0 and 1 have the possibility of leading to a grammatical string, GCD will estimate their expected future grammaticality as 1, and choose each of them roughly half of the time (since $P(\\boldsymbol{\\theta})\\approx P(\\boldsymbol{\\imath}),$ . Once GCD has chosen 0, however, it becomes \u201ctrapped\u201d in the part of the search space where the only grammatical string is the low-probability sequence 00000. ", "page_idx": 3}, {"type": "text", "text": "Ex. 3 illustrates how existing GCD approaches can hinder the language model\u2019s abilities to explore the space of possible outputs according to the learned distribution, thus highlighting the importance of designing a better approximation to the EFG $c(w_{1:i})$ ; this is addressed in the next section. ", "page_idx": 3}, {"type": "text", "text": "3 Adaptive Sampling with Approximate Expected Futures (ASAp) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we propose an adaptive sampling algorithm that iteratively builds better approximations of the future grammaticality of a sequence. Our procedure operates by sampling repeatedly, each time bounding lost probability mass to provably ungrammatical areas of the search space in order to better guide the next sampling iteration. As a result, our algorithm converges over many iterations to exact samples from the constrained LLM distribution, allowing for a flexible trade-off between efficiency and accuracy. ", "page_idx": 3}, {"type": "text", "text": "Overview of the Algorithm GCD approaches poorly approximate the desired distribution because they greedily sample prefixes without worrying about the EFG. When sampling the first token in Ex. 3, GCD simply uses the likelihood for tokens 0 and 1 assigned by the LLM without considering the probability that these next tokens would result in grammatical completions if sampling were unconstrained\u2014i.e. without incorporating the critical EFG re-weighting terms that are necessary for unbiased sampling from the constrained LLM distribution. However, if GCD ends up sampling 0 as the first token for Ex. 3, it will necessarily sample the string 00000 since no other sequences starting with 0 are allowed by the grammar. We can \u201clearn\u201d from this result: the true probability mass assigned to all grammatical sequences starting with a 0 is not 0.45 as the LLM\u2019s next token probability would have us believe; instead, the total grammatical mass in this section of the search space is the joint probability of the single string 00000, which is the much lower value of $0.45^{5}*10^{-8}$ as depicted in Fig. 3. In other words, simply by sampling 00000, we can better approximate (in this case, exactly) the EFG of tokens along this path. ", "page_idx": 3}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/5a5b8a990779b0046c7e489cecd0564fd56af4a1079e90404144441246adac16.jpg", "img_caption": ["Figure 3: Illustration of the trie built by ASAp after sampling 00000 as the first string (left) and after sampling 11111 as the second string (right). EFG updates after each iteration are shown in red. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The key insight behind our algorithm, which we call ASAp, is that we can iterate this process of discovering lost grammatical probability mass by repeatedly sampling and revising transition weights after each sample is produced. More formally, we can think of this procedure as starting with GCD\u2019s over-approximation to each EFG $c(w_{1:i})$ term, and then, through repeated sampling and discovery of mass assigned to non-grammatical completions, reducing each overapproximation to make it more accurate. In the limit, the approximations converge to exact EFG estimates and unbiased sampling. ", "page_idx": 4}, {"type": "text", "text": "Two possible first iterations of the ASAp algorithm are depicted in Fig. 3. In the first iteration (left of Fig. 3), after sampling the sequence 00000, the algorithm directly addresses the issue that arose in Ex. 3 by attempting to better approximate the probability mass of potential grammatical completions of each prefix of 00000 (red quantities). For example, the expected future grammaticality of the prefix 0000 it is now $0.45*10^{-8}$ \u2014i.e., the algorithm effectively \u201clooks ahead\u201d to determine that only one valid (but low probability) string $\\Theta\\Phi$ that can follow 0000. The ideas developed in GCD allow us to efficiently compute, for a given string, the likelihood of the next tokens that will immediately result in non-grammaticality. ", "page_idx": 4}, {"type": "text", "text": "If we only sample one string from the LLM, we cannot hope to do better than GCD in terms of sampling faithfully in a grammar-aligned way. However, if we were to now sample once more, we could now better direct our sampling strategy. In the second iteration (right of Fig. 3), the string 11111 is sampled and the expected future grammaticality is updated (red quantities). Note that at this point the probabilites assigned to the string 00000 from the earlier iteration have already been updated. ", "page_idx": 4}, {"type": "text", "text": "By repeating the above approach multiple times (i.e., by producing more samples), the ASAp algorithm produces precise approximations of the expected future grammaticalities and thus better samples from the constrained LLM. ", "page_idx": 4}, {"type": "text", "text": "Algorithm Formalization The key quantity that the algorithm approximates based on past samples is the expected future grammaticality (EFG) $c(w_{1:i})=\\overset{\\cdot\\;}{\\mathbb{E}}_{p(w_{i+1:n}|w_{1:i})}[\\mathbb{1}[w\\in\\mathcal{L}(\\mathcal{G})]]$ . At iteration $m+1$ , our algorithm uses the set of samples $S\\,=\\,\\{s_{1},\\ldots,s_{m}\\}$ observed so far to compute an overapproximation $\\tilde{c}_{S}(w_{1:i})$ of $c_{S}(w_{1:i})$ for every possible string $w_{1:i}$ . The overapproximation is inductively defined: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\tilde{c}_{S}(w_{1:i})=\\mathbb{1}[w_{1:i}\\in\\mathcal{L}_{\\mathrm{prefix}}(\\mathcal{G})]}&{\\mathrm{no~string~in~}S\\mathrm{~starts~with~}w_{1:i}}\\\\ &{\\tilde{c}_{S}(w_{1:i})=\\sum_{w_{i+1}}P(w_{i+1}\\mid w_{1:i})\\cdot\\tilde{c}_{S}(w_{1:i+1})}&{\\mathrm{otherwise}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Intuitively, if no samples in $S$ start with the prefix $w_{1:i}$ , then $\\tilde{c}_{S}(w_{1:i})$ , the overapproximation of EFG is simply whether the string is or is not a valid prefix in the grammar\u2014i.e. the same overapproximation used by GCD. If, on the other hand, we have encountered the prefix $w_{1:i}$ before in previous samples in $S$ , the overapproximation uses the next token likelihoods that were computed during the previous sampling runs of the algorithm to compute a better estimate of EFG. ", "page_idx": 4}, {"type": "text", "text": "For example, in Fig. 3, once we have sampled the sequences 00000 and 11111, we have that $\\tilde{c}_{S}(\\Theta\\Theta\\Theta)=$ $0.45*10^{\\dot{-}8}$ and $\\tilde{c}_{S}(110)=1$ (i.e., we have not seen a sample with the prefix 110 yet). ", "page_idx": 4}, {"type": "text", "text": "Proof. To see that $\\tilde{c}_{S}(w_{1:i})$ is indeed an upperbound on $c(w_{1:i})$ , consider two cases: First, suppose $w_{1:i}$ is not a prefix of any string in $S$ . In this case, $\\tilde{c}_{S}(w_{1:i})\\ =\\ \\mathbb{1}[w_{1:i}\\ \\in\\ \\mathcal{L}_{\\mathrm{prefix}}(\\mathcal{G})]$ and, like GCD, provides a trivial upper bound. When $\\mathbb{1}[w_{1:i}\\ \\in\\ \\mathcal{L}_{\\mathrm{prefix}}(\\mathcal{G})]\\ =\\ 0$ , there is no possibility of grammaticality along this path and the EFG is therefore also zero. When $\\mathbb{1}[\\bar{w_{1:i}}\\,\\in\\,\\mathcal{L}_{\\mathrm{prefix}}(\\bar{\\mathcal{G}})]\\,=\\,1$ it trivially bounds EFG, which is a probability. Second, we need to prove that $\\dot{\\forall}w_{1:i}\\,\\in\\,\\mathrm{prefix}(S),\\tilde{c}_{S}(w_{1:i})\\,\\geq\\,c(w_{1:i})$ . where $\\mathrm{prefix}(S)$ is the set of (finitely many) prefixes of string in $S$ . We proceed by induction, where the base case is when $w_{1:i}$ is in prefix $(S)$ but no $w_{1:i+1}$ is in prefix $(S)$ for any possible next token $w_{i+1}$ . Consequently, every $w_{1:i+1}$ falls under the first case, leading us to the following inequality: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{c}_{S}(w_{1:i})=\\sum_{w_{i+1}}P(w_{i+1}\\mid w_{1:i})\\cdot\\tilde{c}_{S}(w_{1:i+1})\\ge\\sum_{w_{i+1}}P(w_{i+1}\\mid w_{1:i})\\cdot c(w_{1:i+1})=c(w_{1:i})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next, we move on to the inductive step where $w_{1:i}$ is in prefix $(S)$ and for any $w_{i+1}$ , the string $w_{1:i+1}$ can either be a node that is not a prefix of $S$ , which falls under the first case, or it can in prefix $(S)$ , for which the property holds by induction. Therefore, the reasoning used in Eq. 4 works for the inductive case as well. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "The sampling procedure itself proceeds autoregressively like GCD, but using the iteratively updated EFG estimates we have just defined, $\\tilde{c}_{S}$ . Specifically, the left-to-right sampling conditional for our procedure, $\\tilde{Q}_{S}(w_{i}|w_{1:i-1})$ , after having previously sampled the strings in $S$ , is defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{Q}_{S}(w_{i}|w_{1:i-1})=\\frac{P(w_{i}\\mid w_{1:i-1})\\cdot\\tilde{c}_{S}(w_{1:i})}{\\sum_{w_{i}^{\\prime}}P(w_{i}^{\\prime}\\mid w_{1:i-1})\\cdot\\tilde{c}_{S}(w_{1:i-1},w_{i}^{\\prime})}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our overall algorithm, which is presented in Algorithm 1, then proceeds iteratively, using past samples to improve subsequent samples. Whenever the sample set $S$ is updated with a new sample $w_{1:n}$ , the overapproximation $\\tilde{c}$ is updated for the prefixes of $w_{1:n}$ . The update begins at the end of the sequence and proceeds backward toward the start, by the recursive definition in Eq. 3. In the listing, we assume that we are only interested in the final sample, but in our evaluation we will analyze whether the algorithm induces the desired distribution. ", "page_idx": 5}, {"type": "table", "img_path": "5G7ve8E1Lu/tmp/4f21d251f2c47a80af6e33dc93b75f2ad321b0419da0d7b670385e4f8cd350f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Next we provide a proof that this algorithm ", "page_idx": 5}, {"type": "text", "text": "converges to exact estimates of EFG in the limit of infinite iterations, and therefore to exact samples from the constrained LLM distribution. The theorem assumes almost sure termination of ancestral sampling in the unconstrained LLM distribution $P$ \u2014i.e., the LLM eventually terminates. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Assume that as $L\\rightarrow\\infty$ , the distribution $P$ assigns vanishingly small probability mass to sequences longer than length $L$ . Now, let $S_{m}=\\{s_{1},\\ldots,s_{m}\\}$ be the set of recorded samples up to the mth iteration of $A S A p$ . Then, $\\forall w_{1:i}\\in\\mathcal{L}_{\\mathrm{prefix}}(\\mathcal{G}),\\tilde{c}_{S_{m}}(w_{1:i})\\overset{p}{\\rightarrow}c(w_{1:i})$ as $m\\rightarrow\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. Let $w_{1:i}$ be an arbitrary sequence in ${\\mathcal{L}}_{\\mathrm{prefix}}({\\mathcal{G}})$ . The approximation gap after $m$ iterations of sampling with ASAp, $\\epsilon_{m}=\\tilde{c}_{S_{m}}(w_{1:i})-c(\\dot{w_{1:i}})$ , is equal to the marginal probability under $P$ of all ungrammatical continuations of $w_{1:i}$ that have not yet been encountered in the first $m$ samples, $S_{m}$ . Now consider an arbitrarily small $\\epsilon>0$ . By assumption, there exists an $L$ such that the probability mass $P$ places on sequences longer than $L$ is less than $\\epsilon$ . Further, ASAp samples according to $P$ , but re-weighted by an upper bound on the true EFG (Theorem 1). Thus, the probability of encountering a previously unseen ungrammatical continuation of $w_{1:i}$ no longer than $L$ on any given iteration is at least as high as the probability of encountering the same continuation when sampling directly from $P$ . Because the number of sequences no longer than $L$ is finite, this implies that the probability mass under $P$ of ungrammatical continuations of $w_{1:i}$ that are no longer than $L$ and that are not yet encountered in $S_{m}$ becomes vanishingly small as as $m\\rightarrow\\infty$ . The remaining unencountered ungrammatical continuations of $w_{1:i}$ are longer than $L$ , and thus their total mass is bounded by $\\epsilon$ Therefore $P(\\epsilon_{m}>\\epsilon)\\rightarrow0$ as $m\\rightarrow\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "; Determines what terms can appear   \n(set-logic SLIA)   \n; The function to synthesize   \n(synth-fun f ((name String)) String ; The grammar for f to be synthesized in ((Start String (S)) (S String (name \" \" \".\" (str. $++\\textsf{S S}$ ) (str.at S I) (str.replace S S S) (str.substr S I I))) (I Int (0 1 2 (+ I I) (- I I) (str.len S) (str.indexof S S I)))))   \n; Specifications to satisfy   \n(constraint $(=$ (f \"Nancy FreeHafer\") \"N.F.\"))   \n(constraint $(=$ (f \"Andrew Cencici\") \"A.C.\"))   \n(constraint $(=$ (f \"Jan Kotas\") \"J.K.\"))   \n(constraint $(=$ (f \"Mariya Sergienko\") \"M.S.\")) (a) SLIA/initials-small   \nSta $\\begin{array}{r l}{\\mathsf{\\Pi}_{\\mathsf{\\tau}}+\\boldsymbol{:}=}&{\\mathsf{\\Delta}5}\\\\ {\\mathsf{\\Delta}\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\simeq}&{\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\simeq\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\simeq\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\simeq\\mathsf{\\Pi}_{\\mathsf{\\tau}}}\\\\ {\\mathsf{\\Pi}}&{\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\simeq\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\simeq\\mathsf{\\Delta}\\mathsf{S}\\index{\\mathsf{S}\\index{\\tau}}\\ s t r\\_{\\mathsf{\\Delta}}\\ s\\ \\textrm{I}}\\\\ {\\big|}&{\\mathsf{\\Delta}_{\\mathsf{\\tau}}\\mathsf{t r}\\mathbin{\\lrcorner}\\mathsf{r e p l a c e~\\mathsf{S}\\index{\\tau}}\\ s\\ \\mathsf{S}}\\\\ {\\mathsf{\\Pi}}&{\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\simeq\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\wedge\\mathsf{t r}\\mathbin{\\lrcorner}\\mathsf{S}\\ \\mathsf{I}\\ \\mathsf{I}}\\\\ {\\mathsf{\\Pi}_{\\mathsf{\\tau}}:=}&{\\Theta\\left|\\ 1\\ \\vert2\\big|+\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\textup{I}\\right|\\ \\cdot\\textup{I I}}\\\\ {\\mathsf{\\Pi}}&{\\mathsf{\\Pi}_{\\mathsf{\\tau}}\\wedge\\mathsf{t r}\\mathbin{\\lrcorner}\\mathsf{S}\\ \\big|\\ s\\ t\\ r\\mathbin{\\lrcorner}\\mathrm{indexof~\\mathsf{S}\\ s\\ \\Pi}}\\\\ &{\\mathsf{\\Pi}(\\mathrm{\\Delta}_{\\mathsf{\\tau}})\\mathrm{\\&~}\\mathrm{\\Pi}(\\mathrm{\\Delta}_{\\mathsf{\\tau}})\\mathrm{\\Pi}}\\end{array}$ ", "page_idx": 6}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/dd53161d4ceabe9297b5ced66e3574b910af537cdff420d31cb06b2415a682a3.jpg", "img_caption": ["Figure 4: (a) A SLIA problem in which the grammar for the target function is explicitly defined. (b) INV-BV problem in which the grammar for the target function inv is implicitly defined. (c) The explicitly defined grammar for $^\\dagger$ written in BNF notation. (d) The implicitly defined grammar for inv written in BNF notation. The grammar is implicitly defined by primitives of BV logic and parameters of inv. The goal of each problem is to find an implementation for synth-fun functions that satisfies all the constraints within a specified grammar\u2014 i.e., to find implementation of $^\\dagger$ in the grammar (c) and inv in the grammar (d). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We implemented the ASAp algorithm as an extension of the Transformers-CFG implementation of GCD [8]. When the LLM generates a sequence $w_{1:n}$ , the ASAp algorithm keeps track of the original LLM\u2019s probability $P(w_{i}\\mid w_{1:i-1})$ for $1\\leq i\\leq n$ and the set of allowed next tokens $\\{w_{i}\\mid$ $w_{1:i-1}$ , $w_{i}^{\\prime}\\in\\mathcal{L}(\\bar{\\mathcal{G}})\\}$ determined by the incremental parser in the Transformers-CFG library. After the LLM finishes generating a sequence, our implementation of ASAp updates the overapproximation $\\tilde{c}_{S}$ from the end of sequence by back-propagating the quantity 1 minus probability of the tokens that will for sure lead to non-grammatical sequences. The implementation of ASAp updates $\\tilde{c}_{S}(w_{1:n-1},w_{n}^{\\prime})$ for all possible tokens $w_{n}^{\\prime}$ , and then moves on to update $\\tilde{c}_{S}(w_{1:n-2},w_{n-1}^{\\prime})\\;.\\;.\\;,\\tilde{c}_{S}(w_{1},w_{2}^{\\prime}),\\tilde{c}_{S}(w_{1}^{\\prime})$ using Equation (3). ", "page_idx": 6}, {"type": "text", "text": "Datasets and Models. We consider the benchmark from Example 3 and three structured-decoding tasks. Two of our tasks involve solving Syntax-Guided Synthesis Problems (SyGuS) [2]. SyGuS is a standardized format where one provides a logical specification and a context free grammar of first-order terms and the goal is to synthesize a term in the grammar that satisfies the specification. SyGuS is a natural fit for GAD and we consider two tasks from the standard SyGuS benchmarks where grammars vary from benchmark to benchmark: strings with linear integer arithmetic (SLIA) and loop invariant generation with bit-vector arithmetic (INV-BV). In the former, the grammar is used to restrict what constant strings one can use when building string-manipulating programs and in the latter the grammar is used to restrict constant bit-vectors and operations used to build invariants. Fig. 4 provides examples of SLIA and INV-BV problems. For both families of benchmarks, our prompts consist of 3 in-context examples of the form (specification, solution) and the grammar is then provided as a constraint for GAD. Our third task is the constituency parsing (CP) task already used in prior GCD work [7] where the grammar is used to help the model produce well-parenthesized parse trees for English sentences. ", "page_idx": 6}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/4d5ea47e9b557ad28d0c876224cda35cca0f72ca880596174116f7bc07c67875.jpg", "img_caption": ["Figure 6: Expectations of $\\tilde{Q}_{A S A p}$ , $\\tilde{Q}_{G C D}$ , and $P$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Due to constrained resources and needing to run inference multiple times to measure whether the distribution $\\tilde{Q}$ is faithful to $Q$ , we randomly select 15 SLIA problems, 15 INV-BV problems, and 6 CP problems. We select the open-source Mistral-7B [12] for evaluation due to its superior reasoning and code generation capabilities. ", "page_idx": 7}, {"type": "text", "text": "Measures. We run both algorithms for 2,000 iterations/sample on each benchmark. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To assess converge to the target distribution, we measure the Kullback\u2013Leibler (KL) divergence between the distributions of GCD and ASAp from the target distribution $Q$ for a given number of samples. Because the ideal GAD distribution $Q_{P,\\mathcal{G}}$ is proportional to the original LLM\u2019s distribution $P$ for sequences allowed by a grammar $\\mathcal{G}$ , we can use the LLM\u2019s distribution $P$ on all observed samples as an estimate $Q_{P,\\mathcal{G}}$ . The quantity $K L(Q||P)$ only differs by a constant from the KL divergence between empirical distributions and the ideal GAD distribution: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{\\Sigma}\\chi L(\\tilde{Q}||P)=\\mathbb{E}_{\\tilde{Q}}\\left[\\log\\frac{\\tilde{Q}}{P}\\right]=\\mathbb{E}_{\\tilde{Q}}\\left[\\log\\frac{\\tilde{Q}}{C\\cdot Q_{P,\\mathcal{G}}}\\right]=\\mathbb{E}_{\\tilde{Q}}\\left[\\log\\frac{\\tilde{Q}}{Q_{P,\\mathcal{G}}}\\right]-\\log C=K L(\\tilde{Q}||Q_{P,\\mathcal{G}})-\\log C\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{C=\\sum_{w}\\mathbb{1}[w\\in\\mathcal{L}(\\mathcal{G})]P(w)}\\end{array}$ . Thus, $K L(\\tilde{Q}||P)$ can be used to quantify the alignment between the empiric al distributions of GCD and ASAp with the ideal GAD distribution. ", "page_idx": 7}, {"type": "text", "text": "For example, Fig. 5a shows convergence results for the first 75 iterations on the illustrative Ex. 3\u2014i.e., the KL divergence for $\\tilde{Q}_{A S A p}$ quickly converges to 0 whereas the one for $\\tilde{Q}_{G C D}$ doesn\u2019t. ", "page_idx": 7}, {"type": "text", "text": "We also compare the empirical expectations of the variables $\\tilde{Q}_{G C D}$ , $\\tilde{Q}_{A S A p}$ , and $P$ . For example, Fig. 6a shows convergence results for the first 75 iterations on the illustrative Ex. 3\u2014i.e., $\\tilde{Q}_{A S A p}$ converges to the right expectation. ", "page_idx": 7}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/edfe8bbd4ef1ff9621966a181247dc7b6cd3abe8a00e45e7bca25659fb3a7d85.jpg", "img_caption": ["Figure 7: Scatter plots of $\\tilde{Q}_{A S A p}$ (\u2022) and $\\tilde{Q}_{G C D}$ $(\\times)$ vs. expectations of $P$ after 2,000 samples. Proximity to the diagonal indicates proximity to the actual expectation\u2014e.g., a $\\bullet$ at (0.45,0.4) indicates a benchmark where the empirical expectation of $P$ was 0.45 and $\\Tilde{Q}_{A S A p}$ had converged to an expectation of 0.4 after 2,000 iterations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results. Fig. 5b and Fig. 6b illustrate a benchmark in which our ASAp algorithm quickly converges to the target distribution. Fig. 5 depicts the KL divergence of a sliding window of size 500 (e.g., the points at ${\\bf x}{=}800$ denote the KL divergence of the samples 800-1300). Fig. 6 depicts all the samples from the experiment, as well as how the expectations converges (a point at ${\\bf{x}}{=}i$ denotes the empirical expectation on the first $i$ samples. For this case the expecation for GCD stays very close to 0. ", "page_idx": 8}, {"type": "text", "text": "Similarly, Fig. 5c and Fig. 6c illustrate a benchmark in which our ASAp algorithm converges slowly. In this case, bot ASAp and GCD are far from the target expectation (Fig. 6c), but because GCD happens to be biased towards the most likely outcome, it exhibits better KL divergence. The complete set of plots is shown in Sec. E.1. ", "page_idx": 8}, {"type": "text", "text": "To better understand how the algorithms respectively converge, Fig. 7 plot for each benchmark category the expectations for each benchmark computed by GCD and ASAp against the target expectation of $P$ after 2,000 iterations. The sum of least square difference between expectations computed by GCD and the expectations of $P$ are 2.259 (SLIA), 1.852 (INV-BV4), and 0.109 (CP). The sum of least square difference between expectations computed by ASAp and the expectation and those of $P$ are 1.242 (SLIA), 0.802 (INV-BV4), and 0.159 (CP). While we have too few points for CP to draw conclusions, the expectations computed by ASAp are much closer to the ones computed by GCD across our experiments. ", "page_idx": 8}, {"type": "text", "text": "While our work is interested in the theoretical convergence of the ASAp algorithm, we also report how the GCD and ASAp differ for solving the SLIA and INV-BV4 tasks\u2014i.e., how many of the sampled programs are correct solutions to the given problem. GCD and ASAp solve approximately the same set of problems (there is just one SLIA benchmark for which ASAp returns a valid solution on one sample and GCD never does so). ASAp produces correct samples $38\\%$ more often than GCD (geomean), whereas for SLIA benchmarks that both tools can solve, ASAp produces correct samples $73\\%$ less often than GCD (geomean). Detailed results can be found in Sec. E.2. These results are in line with the fact ASAp shows faster convergence on INV-BV4 benchmarks. For example, for the benchmark illustrated in Fig. 5b, ASAp returns the correct solution for 1588 samples, whereas GCD only returns the correct solution 12 times, whereas for the benchmark in Fig. 5c, ASAp returns the correct solution 69 times and GCD 363 times. ", "page_idx": 8}, {"type": "text", "text": "Discussion and Limitations. As predicted by our theorems, on most benchmarks the ASAp algorithm converges to the desired distribution $P$ whereas GCD does not improve over time (i.e., it exhibits the bias described in this paper). ", "page_idx": 8}, {"type": "text", "text": "While ASAp has no strong effect on solving downstream tasks, we observe that on instances where the convergence is prominent, ASAp ends up sampling correct solutions more often than GCD, which is what we expect when the LLM has \u201clearned\u201d how to solve the given task. ", "page_idx": 8}, {"type": "text", "text": "The key limitation of our work is the current slow convergence of the ASAp algorithm. In some benchmarks, even after 2,000 iterations the KL divergence barely improves and even though the expectation of $\\tilde{Q}_{A S A p}$ is improving, it converges very slowly. ", "page_idx": 8}, {"type": "text", "text": "We highlight that the contributions of this paper are discovering and formalizing the bias of existing constrained decoding approaches and proposing the first converging algorithm to address this problem. ", "page_idx": 8}, {"type": "text", "text": "Now that we have identified the problem, there are many \u201clow-hanging fruits\u201d to improve our sampling strategy, which are great targets for future work\u2014e.g., using forms of targeted beam search to bootstrap our sample set to better explore grammar paths and avoid sampling similar strings. ", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Constrained Decoding Past work has extensively explored constrained decoding algorithms, which modify the original decoding process of LLMs to ensure the output adheres to a user-specified regular [18, 28] or context-free language [5, 6, 7, 19, 23, 24, 25, 26] in a discrete space. Other works enforce hard output constraints using dynamic monitoring and verification methods [1, 15, 27] or by modifying beam search techniques to impose lexical constraints, which require specific keywords to appear in the generated text [4, 9, 10, 16, 17, 20]. At a high level, these methods involve running the LLM decode in parallel with a monitoring scheme (e.g., parsing algorithms for CFGs) to identify which next tokens or beams can produce valid output sequences that meet the constraints. The decoder then masks out any tokens that would lead to invalid sequences, sampling only from the permissible ones. ", "page_idx": 9}, {"type": "text", "text": "To incorporate sequence-level soft semantic or contextual constraints, Amini et al. [3], Kumar et al. [13], Li et al. [14], Qin et al. [21] have applied gradient-based sampling techniques that relax those constraints to differentiable ones, used them as classifiers to further guide the decoding process. While these works guarantee that the decoded output meets the specified constraints (whether in the form of grammar, monitoring schemes, or differentiable functions), they often operate greedily and introduce bias into the output distribution in the way that has been discussed in this paper. Depending on the application one considers, this problem may or may not affect downstream tasks, but as we have argued in this paper, the bias can be quite prominent and sometimes affect downstream performance. Our adaptive decoding algorithm improves decoding over time by analyzing how previous samples led to nongrammaticaility. ", "page_idx": 9}, {"type": "text", "text": "Constraint-Aligned Decoding This paper formally defines the problem of aligning the output distribution of an LLM in the presence of a constraint. We focus our attention on constraints expressed as grammars, but our definitions and algorithm apply to any constraint for which possible satisfaction (in our case grammaticality) can be evaluated in a left-to-right manner. ", "page_idx": 9}, {"type": "text", "text": "In some settings, one is interested in generating multiple outcomes with an LLM to approximate a distribution of interest [11, 22]\u2014e.g., to generate a random number or a set of good test cases for a program. As we have shown, constrained decoding can heavily skew the LLMs distribution and result in biasing the model towards certain constraint-matching sequences. While our work is at this point theoretical, now that the problem of aligning an LLM\u2019s distribution with constraints has been defined, we expect advances in how sampling is performed to quickly converge to better distributions faster (e.g., using beam search to quickly explore possible paths instead of just sampling). ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have introduced a new analysis of the ideal target for constrained sampling from an LLM using a grammar, which we call grammar-aligned decoding (GAD). We proposed a new algorithm for GAD which we call ASAp that iteratively builds better approximations to the critical re-weighting term required for GAD: the expected future grammaticality. We analyzed the convergence of our proposed algorithm and demonstrated its effectiveness in relation to existing grammar-constrained decoding techniques on a set of benchmark code generation tasks. We analyzed and evaluated our approach using constraints enforced by a context-free grammar; however, extensions of our approach might be applied to more general classes of constraints for LLM decoding. ", "page_idx": 9}, {"type": "text", "text": "While the primary goals of this work are to formalize the likelihood misalignment problem of existing grammar-constrained decoding approaches and to provide an initial solution with provable asymptotic guarantees, future work may explore faster-converging approaches, such as sampling multiple tokens simultaneously, to improve efficiency further. We hope this work lays a solid foundation for generating structured outputs from LLMs without distorting the original distribution, advancing the field toward more efficient, trustworthy, and constraint-aligned approaches in LLM-driven generation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank NeurIPS anonymous reviewers for their insightful feedback and helpful discussions. The authors thank Gurindar S. Sohi, Shivaram Venkataraman, Ming Liu, and Yixuan Li for the support of computing resources. This work is supported, in part, by NSF under grants CCF-1918211, CCF-1955457, CCF-2023222, CCF-2200333, CCF-2211968, CCF-2402833, CCF-2422214, and CCF-2446711. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors, and do not necessarily reflect the views of the sponsoring entities. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Lakshya A Agrawal, Aditya Kanade, Navin Goyal, Shuvendu K. Lahiri, and Sriram K. Rajamani. 2023. Monitor-guided decoding of code lms with static analysis of repository context. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.   \n[2] Rajeev Alur, Dana Fisman, Saswat Padhi, Rishabh Singh, and Abhishek Udupa. 2019. Syguscomp 2018: Results and analysis.   \n[3] Afra Amini, Li Du, and Ryan Cotterell. 2024. Structured voronoi sampling. Advances in Neural Information Processing Systems, 36.   \n[4] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. Guided open vocabulary image captioning with constrained beam search. arXiv preprint arXiv:1612.00576.   \n[5] Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. 2023. Prompting is programming: A query language for large language models. Proc. ACM Program. Lang., 7(PLDI).   \n[6] Yihong Dong, Xue Jiang, Yuchen Liu, Ge Li, and Zhi Jin. 2022. Codepad: Sequence-based code generation with pushdown automaton. arXiv preprint arXiv:2211.00818.   \n[7] Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. 2023. Grammar-constrained decoding for structured NLP tasks without finetuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore. Association for Computational Linguistics.   \n[8] Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. 2023. Transformers-CFG. https://github.com/epfl-dlab/transformers-CFG. [9] Chris Hokamp and Qun Liu. 2017. Lexically constrained decoding for sequence generation using grid beam search. arXiv preprint arXiv:1704.07138.   \n[10] J Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. 2019. Improved lexically constrained decoding for translation and monolingual rewriting. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 839\u2013850.   \n[11] Linghan Huang, Peizhou Zhao, Huaming Chen, and Lei Ma. 2024. Large language models based fuzzing techniques: A survey.   \n[12] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.   \n[13] Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. 2022. Gradient-based constrained sampling from language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2251\u20132277.   \n[14] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. 2022. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343.   \n[15] Yixuan Li, Julian Parsert, and Elizabeth Polgreen. 2024. Guiding enumerative program synthesis with large language models. In International Conference on Computer Aided Verification, pages 280\u2013301. Springer.   \n[16] Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. 2022. NeuroLogic a\\*esque decoding: Constrained text generation with lookahead heuristics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 780\u2013799, Seattle, United States. Association for Computational Linguistics.   \n[17] Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4288\u20134299, Online. Association for Computational Linguistics.   \n[18] Daniel Melcer, Nathan Fulton, Sanjay Krishna Gouda, and Haifeng Qian. 2024. Constrained decoding for code language models via efficient left and right quotienting of context-sensitive grammars. arXiv preprint arXiv:2402.17988.   \n[19] Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained language models. arXiv preprint arXiv:2201.11227.   \n[20] Matt Post and David Vilar. 2018. Fast lexically constrained decoding with dynamic beam allocation for neural machine translation. arXiv preprint arXiv:1804.06609.   \n[21] Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. 2022. Cold decoding: Energybased constrained text generation with langevin dynamics. In Advances in Neural Information Processing Systems.   \n[22] Alex Renda, Aspen K. Hopkins, and Michael Carbin. 2023. Can LLMs generate random numbers? evaluating LLM sampling in controlled domains. In ICML 2023 Workshop: Sampling and Optimization in Discrete Space.   \n[23] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9895\u20139901, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.   \n[24] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768.   \n[25] Elias Stengel-Eskin, Kyle Rawlins, and Benjamin Van Durme. 2023. Zero and few-shot semantic parsing with ambiguous inputs. arXiv preprint arXiv:2306.00824.   \n[26] Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh. 2024. Improving llm code generation with grammar augmentation. arXiv preprint arXiv:2403.01632.   \n[27] Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A. Saurous, and Yoon Kim. 2023. Grammar prompting for domain-specific language generation with large language models.   \n[28] Brandon T Willard and R\u00e9mi Louf. 2023. Efficient guided generation for large language models. arXiv e-prints, pages arXiv\u20132307. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Hardware and Software ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our experiments are conducted on 4 NVIDIA RTX A6000 GPUs and 4 NVIDIA A100 GPUs. Our implementation is based on Python 3.10 and PyTorch 2.1.2. ", "page_idx": 12}, {"type": "text", "text": "B Hyperparameters ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The hyperparameters discussed in this paper pertain to the decoding strategy of language models. As we aim to investigate the LM\u2019s original distribution, we set Top-P at 1.0, Temperature at 1.0, and Top-K at 0 to consider the complete token vocabulary. ", "page_idx": 12}, {"type": "text", "text": "C Model Checkpoint ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We use the Mistral-7B model checkpoint provided by Hugging Face: https://huggingface.co/ mistralai/Mistral-7B-Instruct-v0.2. ", "page_idx": 12}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "D.1 SLIA and INV-BV ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Prompt Construction For both families of benchmarks, our prompts adopt standard in-context learning format which consist of 3 in-context examples of the form (specification, solution) and ask the model to provide the solution for the last example. A concrete example would be ", "page_idx": 12}, {"type": "text", "text": "You are an expert in program synthesis.   \nYou are tasked with solving a Syntax-Guided Synthesis (SyGuS) problem. Your goal is to output a function that should produce outputs that satisfy a series of constraints when given specific inputs. ", "page_idx": 12}, {"type": "text", "text": "Question: (set-logic BV) ", "page_idx": 12}, {"type": "text", "text": "(synth-fun inv ((s (BitVec 4)) (t (BitVec 4))) (BitVec 4)) ", "page_idx": 12}, {"type": "text", "text": "(declare-var s (BitVec 4))   \n(declare-var t (BitVec 4))   \n(define-fun udivtotal ((a (BitVec 4)) (b (BitVec 4))) (BitVec 4) (ite (= b #x0) #xF (bvudiv a b)))   \n(define-fun uremtotal ((a (BitVec 4)) (b (BitVec 4))) (BitVec 4) (ite (= b #x0) a (bvurem a b)))   \n(define-fun min () (BitVec 4) (bvnot (bvlshr (bvnot #x0) #x1)))   \n(define-fun max () (BitVec 4) (bvnot min))   \n(define-fun l ((s (BitVec 4)) (t (BitVec 4))) Bool (bvsle (bvlshr s (inv s t)) t))   \n(define-fun SC ((s (BitVec 4)) (t (BitVec 4))) Bool (or (bvult t min) (bvsge t s)))   \n(constraint $\\mathrel{\\mathop{=}}\\!>$ (SC s t) (l s t)))   \n(check-synth)   \nSolution:   \n(define-fun inv ((s (BitVec 4)) (t (BitVec 4))) (BitVec 4) (bvnot (bvor s #b0111))) ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "... (2 more examples) ", "page_idx": 12}, {"type": "text", "text": "Question: (set-logic BV) ", "page_idx": 13}, {"type": "text", "text": "(synth-fun inv ((s (BitVec 4)) (t (BitVec 4))) (BitVec 4)) ", "page_idx": 13}, {"type": "text", "text": "(declare-var s (BitVec 4))   \n(declare-var t (BitVec 4))   \n(define-fun udivtotal ((a (BitVec 4)) (b (BitVec 4))) (BitVec 4) (ite ( $=$ b #x0) #xF (bvudiv a b)))   \n(define-fun uremtotal ((a (BitVec 4)) (b (BitVec 4))) (BitVec 4) (ite (= b #x0) a (bvurem a b)))   \n(define-fun min () (BitVec 4) (bvnot (bvlshr (bvnot #x0) #x1)))   \n(define-fun max () (BitVec 4) (bvnot min))   \n(define-fun l ((s (BitVec 4)) (t (BitVec 4))) Bool (bvsgt (bvnot (inv s t)) t))   \n(define-fun SC ((s (BitVec 4)) (t (BitVec 4))) Bool (distinct t max))   \n(constraint $\\mathrel{\\mathop{=}}\\!>$ (SC s t) (l s t)))   \n(check-synth)   \nSolution: ", "page_idx": 13}, {"type": "text", "text": "Grammar Constraint While most SYGUS problems contain grammar constraints, some problems have grammars implicitly defined by the theory. We explicitly converted the grammar constraint of the problem into EBNF format for constrained-decoding. The example for the last example would be ", "page_idx": 13}, {"type": "text", "text": "root ::= \"(define-fun inv ((s (BitVec 4)) (t (BitVec 4))) (BitVec 4) \" Start \")\"   \nStart :: $=$ \"s\" | \"t\" | \"#x0\" | \"#x8\" | \"#x7\" | \"(\" \"bvneg\" \" \" Start \")\" | \"(\" \"bvnot\" \" \" Start \")\" | \"(\" \"bvadd\" \" \" Start \" \" Start \")\" | \"(\" \"bvsub\" \" \" Start \" \" Start \")\" | \"(\" \"bvand\" \" \" Start \" \" Start \")\" | \"(\" \"bvlshr\" \" \" Start \" \" Start \")\" | \"(\" \"bvor\" \" \" Start \" \" Start \")\" | \"(\" \"bvshl\" \" \" Start \" \" Start \")\" ", "page_idx": 13}, {"type": "text", "text": "D.2 Constituency Parsing ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For Constituency parsing task, our prompts consist of 8 in-context examples of the form. A concrete example would be ", "page_idx": 13}, {"type": "text", "text": "Perform constituency parsing on the provided sentences in accordance with the Penn TreeBank annotation guidelines. Fill in the last mapping. ", "page_idx": 13}, {"type": "text", "text": "Ad Notes   \n->   \n[ ( NP-HLN ( NN Ad ) ( NNS Notes ) ) ]   \nThe market crumbled   \n->   \n[ ( S ( NP-SBJ ( DT The ) ( NN market ) ) ( VP ( VBD crumbled ) ) ) ]   \nI felt betrayed he later said   \n->   \n[ ( S ( S-TPC-1 ( NP-SBJ ( PRP I ) ) ( VP ( VBD felt ) ( ADJP-PRD ( VBN betrayed ) ) ) )   \n( NP-SBJ ( PRP he ) ) ( ADVP-TMP ( RB later ) ) ( VP ( VBD said ) ) ) ]   \nFriday October 13 1989   \n->   \n[ ( NP ( NNP Friday ) ( NNP October ) ( CD 13 ) ( CD 1989 ) ) ] ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "The Arabs had merely oil ", "page_idx": 14}, {"type": "text", "text": "> [ ( S ( NP-SBJ ( DT The ) ( NNPS Arabs ) ) ( VP ( VBD had ) ( NP ( RB merely ) ( NN oil ) ) ) ) ] ", "page_idx": 14}, {"type": "text", "text": "Energy   \n->   \n[ ( NP-HLN ( NN Energy ) ) ]   \nSome U.S. entrepreneurs operate on a smaller scale   \n->   \n[ ( S ( NP-SBJ ( DT Some ) ( NNP U.S. ) ( NNS entrepreneurs ) ) ( VP ( VBP operate )   \n( PP-MNR ( IN on ) ( NP ( DT a ) ( JJR smaller ) ( NN scale ) ) ) ) ) ] ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Knowledgeware Inc. ", "page_idx": 14}, {"type": "text", "text": "Grammar Constraint For the constituency parsing (CP) task we used the grammar provided in prior GCD work [7]. The grammar is too large to attach, but it is used to help the model produce wellparenthesized parse trees and ensure that all words in a given English sentence appear in left-to-right order. ", "page_idx": 14}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/4798bda6836153dd1a643a73c03522079f9acd5da4f13df4fb245311932d7ed8.jpg", "img_caption": ["Figure 8: SLIA/dr-name "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/01e820dae5bdceac3a2072ef96b9221b1a0bd5b70cdbc4152b189de650dfc413.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/fefe680c0f0b569a03df776857e5b7f6b4e2ac0ed98b290ecb03aae6e8e632bb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/1503ddeaca01d353d13fffb2f89e200a206b3e0d8effd164adf7644484d190a0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 9: SLIA/firstname_small ", "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/121f9d9e541cff3cd77e93f0d5ccd303ae32e7547152516875d4d7754afff6a7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/326affdba1d6c2e3e7331305962529e55bdf5010213c58e7d41f47b969bc6749.jpg", "img_caption": ["Figure 10: SLIA/firstname ", "(b) Expectations "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/9dd28410dd8da889eaf203aa70829a810fafaade164c9fe0ce65390b763809bd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/f6743c2100717f60990fc3debff2379aa5cc4b8ac1f50f835b02c949bfe663e9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 11: SLIA/initials_small ", "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/9fae19a233072c3445860c65de4a70aba8ea4c6d007a6ba1abc4a2d7a24708ed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/9dd0d8bab99024865a5c0bc29b989b036efe9dbd9d54d5433ca12537b59b63b7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 12: SLIA/initials-long-repeat (a) KL divergences (b) Expectations Figure 15: SLIA/name-combine-2-long-repeat ", "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/75c2a79de7d1a426bbfb20d64d47425e0bd25f88224e3edb8fa2b1efcfcbfa13.jpg", "img_caption": ["(a) KL divergences (b) Expectations Figure 13: SLIA/lastname "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/25017779b351d72401dc867861b5acc74ebfdab9433e5e66c3356118e3270ea1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/af6c00f14d77dd371312e8e207371411af031024a042a42c727c6c1efdaa23a9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/6041479a6139264eb0e3f2169a134d8293edcac4072c61ada08a1bcfb2fde1e0.jpg", "img_caption": ["(a) KL divergences (b) Expectations ", "Figure 14: SLIA/name-combine-2_short "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/f1034284c4e63c6e8418bbc5b92c277e06fb996564e17fc53f49de3363cd3f95.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/966d7f2034f97fecdfd77985d35f767cfd03aea99e9ac2644dc8362ed590e562.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/d7186090dfe50fac0182ae8e4d303fb23789b7ce40c2f21d54fa5c30981f6d49.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/94f0185e73a17f3369f1b882a757854466edc248b1d51d36d0fc367f358d10dc.jpg", "img_caption": ["Figure 16: SLIA/name-combine-4_short "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(b) Expectations ", "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/c767bd2b0fb847bd90d359cedc98387fafb29e4af8c720c386def00e44d1f20e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/195487b6c1f8a55733a2f73163bec544c410519f8519df29cf800240d6296443.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 17: SLIA/name-combine-4-long ", "page_idx": 15}, {"type": "text", "text": "E Detailed Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide additional plots and experimental data. ", "page_idx": 15}, {"type": "text", "text": "E.1 Plots ", "page_idx": 15}, {"type": "text", "text": "Figures 8\u201322 provide the KL divergence and expectation results for the SLIA benchmarks. Figures 23\u2013 37 provide the KL divergence and expectation results for the INV-BV benchmarks. Figures 38\u201343 provide the KL divergence and expectation results for the INV-BV benchmarks. ", "page_idx": 15}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/899c4859a79ff84f1a83d04bab561bc337c04d82db1358368f69f9ce44154cba.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/d008bab8b99583746da5f35c7b529190d338286451edd9ec318ef8eea8c7796b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 18: SLIA/phone-3-long ", "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/5909e98ce524e4c108d7e1815a18aacbbd7a14d361e2889758f2dc6ba9eb03a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/2f1e350a224f37758bdfe4bd6dab8e4908576fd92f0043c8445b54c0c0235355.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 19: SLIA/reverse-name-long ", "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/2cffb0042582ea227ca8fac7f58c5677fa2519fd04744ef2a24292f387595eb5.jpg", "img_caption": ["Figure 20: SLIA/univ_1_short"], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/2aabbd83a791ffef04c104dec88a1ab44d50fa911ce3551ef8b6cd192c991ac0.jpg", "img_caption": ["(b) Expectations "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/88d95ce6bc7041db72b36a0d4b9b212e2a5bfe8eeb70494ec6935857b83952a7.jpg", "img_caption": ["(a) KL divergences (b) Expectations Figure 21: SLIA/univ_1 "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/7f98cbc05a7119bc9c948d3e69b6eb1474a40365b081a63fb0e02616a3e4bb63.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/1d670136603c1cd83c9b2dcadbaea074bd6487aa6b3bf76bd54237dca1a15931.jpg", "img_caption": ["Figure 22: SLIA/univ_2_short "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/634d200fa804181af2823163f38e0f24805cf3a6044c108160ee8c790b236969.jpg", "img_caption": ["(b) Expectations "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/1d777be51cb2957d839a535de56bd3d48cf4fe1b06ac7c3937d16d181ad0fe0e.jpg", "img_caption": ["(a) KL divergences (b) Expectations Figure 23: INV-BV/find_inv_bvsge_bvlshr1_4bit "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/275f3ff3551bdd212d9928cf4550a8e02087a68b226e1af7421feb234725f2a2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/c2023f969d8e9b0483050d8495692a1e0a8acab97385452f8de8073bd295be7c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/90153283d9e444ee4e9f686a037c1967d5dd7049197072a2e79560a7f2580b8d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 24: INV-BV/find_inv_bvsge_bvneg_4bit ", "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/ee71676b29819e964f9ee0facb2100fafa9efc76ba94c233e18b66dc46748d33.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/1855553588305cf3cd9d07c133cf8579700669af53501a1e0e2827feafeb8012.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 25: INV-BV/find_inv_bvsge_bvnot_4bit ", "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/2acef278115303d656d9b502075d0d4f5653bee4bc06c02cc4ca0803c6d5e17e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/4da4daf05c6cc1f48037f880682c1b5ff34f140c62b9edba8d14ff53732e3e6d.jpg", "img_caption": ["(b) Expectations "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 26: INV-BV/find_inv_bvsgt_bvor_4bit ", "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/de21fd43acd4a519fe5f8a0d8127dcce6d346e4e8889c1c066adde5af07f416d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/6b7586660ea2794a94d758e9c66ee6ff4519e7e6b792d99733050dd6c93fb5e6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 27: INV-BV/find_inv_bvugt_bvashr0_4bit ", "page_idx": 16}, {"type": "text", "text": "E.2 Correctness Results for SYGUS Tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 1 shows how many samples (out of 2000) yielded correct solutions for each benchmark (bold is better). The task initials_long-repeat was only solved using ASAp. ", "page_idx": 16}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/4869d66d2e59b5c1ed7f8119e18e578ba3904f0a71cc7c001cc133bbfadc4829.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/2561aa9db83ef10cd8b0fc682635e14ca46d0ec5460e133788b52a80ffcf1119.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 28: INV-BV/find_inv_bvugt_bvneg_4bit ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/72aaca8eb92ef3d238ee66b5cd1fafd76d44f72542681b270500af8d4e41bbb9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/b4eda8b6ba7b5d0e9231cac0f192a4f7c059a9ffc66e51f70f9184c7d6e94982.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 29: INV-BV/find_inv_bvule_bvurem0_4bit ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/0aee6d465dbcbd827189deedc4b33e3bd52abe8d9dfa7472deb6af6f8a81c5dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/ea183e3f568b6a9460b0a73064e3eefaa996cea58140e1a680ddf4fbc5360acf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 30: INV-BV/find_inv_bvule_bvurem1_4bit ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/552c55daeae3b043e8626e777738794a4a0260ddb48c691820022781736f3765.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/666a9fca6af1f9b8dfe559c5f32c4b33f288202039384ffe64072ffe1a60cbef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 31: INV-BV/find_inv_eq_bvand_4bit ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/c2724a5dc975258d96a51232074251ff4bde8e641ebc5779e3438b458e90971d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/7739e02b4137924e2cf416983cd38cc9c5b030ac28d96886dd9c4b88eb52f0aa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 32: INV-BV/find_inv_eq_bvlshr0_4bit ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/5ae441388a06e4ec8c278090c9bc45cb927889631a7b4e87a3ae0b91937a995e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/50916ee6ce059a0b8e26ffee04d3f4d2c08471618b6ea9e62ee930391f15ec69.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 33: INV-BV/find_inv_ne_bvneg_4bit ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/a54fcf94ded698acbf6843e64340d0e927e09d42f094592d87e7127635db6e07.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/05cba1367f95f407b0273b0b2b71a56eb6bbbb28181abaa92913ab3da47e8ba9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 34: INV-BV/find_inv_ne_bvudiv0_4bit ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/36275e974064927d8240c3ff8ef9166bcb2f5072378707da094106bcf52e6540.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/d731b4a54f2e5659519bc74cb5cf20302f6d6a524c09fbff05ac872ed6fb19b1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 35: INV-BV/find_inv_ne_bvudiv1_4bit ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/a647a4097de9ed32a9e40e27788e38e19bde0116d9550e5aadabcb6322676b21.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/04dd5790454fbb57280cf7f0ee24e6fe51d43c92941de97e54cf037e9f9f7487.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(b) Expectations ", "page_idx": 17}, {"type": "text", "text": "Figure 36: INV-BV/find_inv_ne_bvurem0_4bit ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/2e03c76a9b79d9d4db8620d7afb90feccc5e407a07a2986e1588fca0a3fed560.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/213dafe1b4fb080673d40cf21d03344dede6f9f4f49524b22ddb8b85446fa897.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(a) KL divergences (b) Expectations Figure 37: INV-BV/find_inv_ne_bvurem1_4bit ", "page_idx": 17}, {"type": "text", "text": "F Will ASAp still be more aligned than GCD after fine-tuning? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Experimental setup for fine-tuning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We adhere to the established QLoRA finetuning pipeline and create task-specific datasets of INV-BV4 and CP for instruction tuning. In line with our paper\u2019s methodology, we incorporate in-context examples in the instruction tuning dataset to enhance the models\u2019 performance in in-context learning. For each task, we independently finetune Mistral-7B, resulting in two versions of the model (for ", "page_idx": 17}, {"type": "image", "img_path": "5G7ve8E1Lu/tmp/729c3bfb9dddf32e2a193d371e0220237b762d3ccbb651170587aa06ec22bd94.jpg", "img_caption": ["INV-BV4 and CP). We employ a standard train-validation-test split of $70{-}10{-}20\\%$ . Instruction tuning is conducted on the training set, and model selection is based on the lowest validation loss. Key hyperparameters include a learning rate of 2e-4, a warmup ratio of 0.03, a maximum sequence length of 2048, LoRA alpha of 32, LoRA dropout of 0.05, and LoRA rank of 64. The best checkpoints were at 328 and 536 steps for INV-BV and CP, respectively. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "F.2 Additional Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "No significant differences in convergence rates post fine-tuning. In Section 4, we evaluate ASAp and GCD on the base model Mistral-7B. A natural extension of this evaluation is determining whether ASAp retains its advantages over GCD after fine-tuning the base model on task-specific datasets, which optimizes the LLM for higher grammatical accuracy from the start. ", "page_idx": 18}, {"type": "text", "text": "In our fine-tuning step, we want to teach the LLM to assign higher probabilities to grammatical outputs for the specific task DSL. We randomly selected two INV-BV problems (find_inv_bvsge_bvneg_4bit and find_inv_bvsgt_bvor_4bit for INV-BV) and four CP problems (CP_re_ptb_215, CP_re_ptb_434, CP_re_ptb_1627 and CP_re_ptb_1643 for CP) from the test set, and instrction tuned input-output pairs of prompt and output programs in the training set for the base model Mistral-7B. We obtained two fine-tuned models, one for INV-BV and one for CP. ", "page_idx": 18}, {"type": "text", "text": "We tested GCD and ASAp on the finetuned Mistral-7B on the randomly left-out problems and checked the convergence rates of the KL-divergence. The results from finetuned Mistral-7B did not show significant differences in terms of convergence compared to the base Mistral-7B. As done in Section 4, we computed the expectation for each benchmark obtained via GCD and ASAp after 2,000 iterations and compared it against the target expectation $Q^{P,\\mathcal{G}}$ of GAD. The sum of least squares difference between expectations computed by GCD and the expectations of $Q^{P,\\mathcal{G}}$ are 0.677 (INV-BV4), 0.278 (CP), while ASAp achieved lower errors: 0.051 (INV-BV4), 0.201 (CP), indicating that ASAp more closely aligned with the exact GAD expectations. We did not include SLIA as we did not have sufficient data for further fine-tuning. ", "page_idx": 18}, {"type": "table", "img_path": "5G7ve8E1Lu/tmp/cc829f241cba847f7bc054127ed9b0c5ab9860c914746d569bc29763859e4318.jpg", "table_caption": ["Table 1: Correctness of solutions for different algorithms. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims are well-supported by the algorithm formalization and theoretical results in Section 3, and comprehensive experiments in Section 4, and further results in the Appendix. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide detailed limitations in Section 4. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All theorems are accompanied by formal proofs. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide all the hyperparameters we use in Appendix B, model checkpoints in Appendix C and experimental details in Appendix D to support the reproducibility of our experiments. The codebase and datasets with detailed instructions will also be released. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We will attach datasets we covered in this paper during submission and the code will be released with detailed instructions. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Hyperparameters are included in Appendix B. All experimental details are included in Appendix D to facilitate a better understanding of our setting. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our experiments are run on 2,000 sampling iterations for which we show plots for convergence. Given the cost of the experiments we cannot run on multiple seeds, but a high number of iterations mitigates the problem. Therefore, error bars are not applicable. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] Justification: We provide hardware and software details in Appendix A. ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The authors have read the NeurIPS Code of Ethics and made sure the paper follows the NeurIPS Code of Ethics in every aspect. ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper simply improves sampling. Existing LLM approaches will benefti from it, but the work will not directly lead to specific broader impacts. ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?   \nAnswer: [NA]   \nJustification: The paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper properly cites the original paper or sources whenever an asset is used. URL of the model checkpoint is included in Appendix C. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We will attach the datasets as part of our submission and the code will be released with well-documented instructions. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}]