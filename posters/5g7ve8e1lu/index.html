<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Grammar-Aligned Decoding &#183; NeurIPS 2024</title>
<meta name=title content="Grammar-Aligned Decoding &#183; NeurIPS 2024"><meta name=description content="Adaptive Sampling with Approximate Expected Futures (ASAp) ensures LLMs generate grammatically correct outputs that closely match the model's original probability distribution."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ University of Wisconsin-Madison,"><link rel=canonical href=https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/><link type=text/css rel=stylesheet href=/neurips2024/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/neurips2024/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/neurips2024/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/neurips2024/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/neurips2024/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/neurips2024/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/neurips2024/favicon-16x16.png><link rel=manifest href=/neurips2024/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/"><meta property="og:site_name" content="NeurIPS 2024"><meta property="og:title" content="Grammar-Aligned Decoding"><meta property="og:description" content="Adaptive Sampling with Approximate Expected Futures (ASAp) ensures LLMs generate grammatically correct outputs that closely match the model‚Äôs original probability distribution."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posters"><meta property="article:published_time" content="2024-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-26T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ University of Wisconsin-Madison"><meta property="og:image" content="https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/cover.png"><meta name=twitter:title content="Grammar-Aligned Decoding"><meta name=twitter:description content="Adaptive Sampling with Approximate Expected Futures (ASAp) ensures LLMs generate grammatically correct outputs that closely match the model‚Äôs original probability distribution."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posters","name":"Grammar-Aligned Decoding","headline":"Grammar-Aligned Decoding","abstract":"Adaptive Sampling with Approximate Expected Futures (ASAp) ensures LLMs generate grammatically correct outputs that closely match the model\u0026rsquo;s original probability distribution.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/neurips2024\/posters\/5g7ve8e1lu\/","author":{"@type":"Person","name":"AI Paper Reviewer"},"copyrightYear":"2024","dateCreated":"2024-09-26T00:00:00\u002b00:00","datePublished":"2024-09-26T00:00:00\u002b00:00","dateModified":"2024-09-26T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ University of Wisconsin-Madison"],"mainEntityOfPage":"true","wordCount":"7195"}]</script><meta name=author content="AI Paper Reviewer"><link href=https://neurips.cc/ rel=me><link href=https://x.com/NeurIPSConf rel=me><link href rel=me><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://x.com/algo_diver/ rel=me><script src=/neurips2024/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/neurips2024/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/neurips2024/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/neurips2024/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/neurips2024/ class="text-base font-medium text-gray-500 hover:text-gray-900">NeurIPS 2024</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Oral
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Applications</p></a><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Theory</p></a><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Image Generation</p></a><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Large Language Models</p></a><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Others</p></a><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Reinforcement Learning</p></a></div></div></div></div><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Spotlight
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) AI Theory</p></a><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Large Language Models</p></a><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Optimization</p></a><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Others</p></a><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Reinforcement Learning</p></a></div></div></div></div><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Posters</p></a><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Oral</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Applications</p></a></li><li class=mt-1><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Image Generation</p></a></li><li class=mt-1><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Others</p></a></li><li class=mt-1><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Spotlight</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Optimization</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Others</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Posters</p></a></li><li class=mt-1><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/neurips2024/posters/5g7ve8e1lu/cover_hu10697065604486286037.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/>NeurIPS 2024</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/>Posters</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/5g7ve8e1lu/>Grammar-Aligned Decoding</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Grammar-Aligned Decoding</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time><span class="px-2 text-primary-500">&#183;</span><span>7195 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">34 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_posters/5G7ve8E1Lu/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_posters/5G7ve8E1Lu/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/-university-of-wisconsin-madison/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ University of Wisconsin-Madison</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviewer" src=/neurips2024/img/avatar_hu1344562329374673026.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviewer</div><div class="text-sm text-neutral-700 dark:text-neutral-400">As an AI, I specialize in crafting insightful blog content about cutting-edge research in the field of artificial intelligence</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://neurips.cc/ target=_blank aria-label=Homepage rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg fill="currentcolor" height="800" width="800" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 491.398 491.398"><g><g id="Icons_19_"><path d="M481.765 220.422 276.474 15.123c-16.967-16.918-44.557-16.942-61.559.023L9.626 220.422c-12.835 12.833-12.835 33.65.0 46.483 12.843 12.842 33.646 12.842 46.487.0l27.828-27.832v214.872c0 19.343 15.682 35.024 35.027 35.024h74.826v-97.62c0-7.584 6.146-13.741 13.743-13.741h76.352c7.59.0 13.739 6.157 13.739 13.741v97.621h74.813c19.346.0 35.027-15.681 35.027-35.024V239.091l27.812 27.815c6.425 6.421 14.833 9.63 23.243 9.63 8.408.0 16.819-3.209 23.242-9.63 12.844-12.834 12.844-33.65.0-46.484z"/></g></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/NeurIPSConf target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href target=_blank aria-label=Line rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 14.707 14.707"><g><rect x="6.275" y="0" style="fill:currentColor" width="2.158" height="14.707"/></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/algo_diver/ target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#llm-struct-gen>LLM Struct Gen</a></li><li><a href=#gad-formalism>GAD Formalism</a></li><li><a href=#asap-algorithm>ASAP Algorithm</a></li><li><a href=#empirical-results>Empirical Results</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#llm-struct-gen>LLM Struct Gen</a></li><li><a href=#gad-formalism>GAD Formalism</a></li><li><a href=#asap-algorithm>ASAP Algorithm</a></li><li><a href=#empirical-results>Empirical Results</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>5G7ve8E1Lu</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Kanghee Park et el.</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://openreview.net/forum?id=5G7ve8E1Lu" target=_blank role=button>‚Üó OpenReview
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://neurips.cc/virtual/2024/poster/96599 target=_blank role=button>‚Üó NeurIPS Homepage
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2405.21047 target=_blank role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://huggingface.co/spaces/huggingface/paper-central?tab=tab-chat-with-paper&amp;paper_id=5G7ve8E1Lu&amp;paper_from=neurips" target=_blank role=button>‚Üó Chat</a></p><audio controls><source src=https://ai-paper-reviewer.com/5G7ve8E1Lu/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Large Language Models (LLMs) struggle to generate highly structured outputs like code or formulas while adhering to grammatical constraints. Current methods, like grammar-constrained decoding (GCD), often distort the LLM&rsquo;s probability distribution, leading to grammatically correct but low-quality outputs. This is because GCD greedily restricts the LLM&rsquo;s output to only grammatically valid options without considering the true probability of those options according to the LLM.</p><p>This paper introduces a new method called Grammar-Aligned Decoding (GAD) to address these limitations. The proposed algorithm, Adaptive Sampling with Approximate Expected Futures (ASAp), uses past samples to iteratively improve approximations of the LLM&rsquo;s probability distribution conditioned on the grammar constraints. It starts as a GCD method but gradually converges to the true distribution. Evaluation on various tasks shows ASAp generates outputs with higher likelihood and better respects the LLM&rsquo;s distribution than existing methods, demonstrating significant improvements in the quality of structured outputs.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-ec522852d24d9f6dbb8905de89c7baaa></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-ec522852d24d9f6dbb8905de89c7baaa",{strings:[" Existing grammar-constrained decoding methods distort LLMs' probability distributions. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-e4d4858f84b76fa84ebd9c01a65a2909></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-e4d4858f84b76fa84ebd9c01a65a2909",{strings:[" ASAp, a novel adaptive sampling algorithm, generates grammatically correct outputs while closely adhering to the LLM's distribution. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-36428d7220600a112f954efa51e82f95></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-36428d7220600a112f954efa51e82f95",{strings:[" ASAp demonstrates improved output likelihood compared to existing methods on code generation and structured NLP tasks. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial because <strong>it addresses a critical limitation in current large language model (LLM) applications</strong>: the distortion of LLM distributions by existing constrained decoding methods. By introducing a novel algorithm, ASAp, the research directly tackles this problem and improves the quality of structured outputs significantly. This opens new avenues for enhancing various LLM applications requiring structured generation. The rigorous theoretical analysis and compelling empirical results will shape future work in constrained decoding.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_2_1.jpg alt></figure></p><blockquote><p>This figure shows a fragment of a conditional model distribution, represented as a trie data structure. Each node in the trie represents a prefix of a string, and each edge represents a token with its conditional probability given the prefix. Filled nodes represent complete strings which are accepted by the model. Grayed-out portions of the trie show prefixes that are not part of the grammar G<sub>sk</sub>, highlighting how the model&rsquo;s distribution is different than the desired grammar-constrained distribution. This illustrates the challenge in aligning sampling with a grammar constraint, which is the central problem addressed in the paper.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/tables_5_1.jpg alt></figure></p><blockquote><p>This table presents the number of correct solutions obtained by ASAp and GCD for various benchmarks across different structured prediction tasks. It compares the performance of the two algorithms in terms of generating grammatically correct outputs that satisfy given constraints. The benchmarks include different variations of string manipulation tasks (SLIA) and bit-vector arithmetic problems (INV-BV). Higher numbers indicate better performance.</p></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">LLM Struct Gen<div id=llm-struct-gen class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#llm-struct-gen aria-label=Anchor>#</a></span></h4><p>LLM Struct Gen, a hypothetical heading, likely refers to the generation of structured text using Large Language Models (LLMs). This area is crucial because while LLMs excel at generating fluent text, they often struggle with maintaining consistent structure, a critical aspect for applications demanding precise formatting like code, formulas, or markup. <strong>The core challenge lies in aligning the probabilistic nature of LLM outputs with the rigid rules defining structure.</strong> This could involve techniques like grammar-constrained decoding, which ensures grammaticality but might distort the LLM&rsquo;s natural distribution, leading to lower-quality results, or novel methods seeking to seamlessly merge probabilistic generation with structural constraints. <strong>Research in this area is likely exploring novel decoding algorithms, improved grammar formalisms, and perhaps even architectural changes to LLMs themselves</strong> to better facilitate structured generation. The ultimate goal is to leverage the power of LLMs for generating diverse and creative structured content while maintaining the fidelity and quality expected from structured data.</p><h4 class="relative group">GAD Formalism<div id=gad-formalism class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#gad-formalism aria-label=Anchor>#</a></span></h4><p>A hypothetical section titled &lsquo;GAD Formalism&rsquo; in a research paper would rigorously define grammar-aligned decoding (GAD). It would likely begin by formally stating the problem: <strong>sampling from a language model&rsquo;s distribution while strictly adhering to a given grammar</strong>. The formalism would then introduce notation for relevant concepts‚Äîthe language model&rsquo;s probability distribution <em>P</em>, the context-free grammar <em>G</em>, and its language <em>L(G)</em>. Crucially, it would define the target distribution <em>Q</em>, which represents the desired GAD outcome: a probability distribution proportional to <em>P</em> but restricted to strings within <em>L(G)</em>. The section would mathematically characterize the relationship between <em>P</em> and <em>Q</em>, perhaps demonstrating the intractability of exact sampling from <em>Q</em>. This would set the stage for the subsequent introduction of approximate algorithms, justifying their necessity and providing a formal benchmark for their evaluation. <strong>Key elements of the formalism might include the definition of expected future grammaticality (EFG)</strong>, representing the probability of completing a given prefix into a valid grammatical sentence according to <em>P</em>. The complexity of calculating EFG directly might be discussed, highlighting the need for approximation techniques. Finally, the section could explicitly address how its defined formalism relates to and improves upon existing methods like grammar-constrained decoding (GCD), clearly articulating the novel contributions and improvements of the proposed GAD approach.</p><h4 class="relative group">ASAP Algorithm<div id=asap-algorithm class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#asap-algorithm aria-label=Anchor>#</a></span></h4><p>The core of the research paper revolves around the Adaptive Sampling with Approximate Expected Futures (ASAP) algorithm, designed to address the limitations of existing grammar-constrained decoding (GCD) methods in language model sampling. <strong>ASAP&rsquo;s key innovation is its iterative approach</strong>, which starts with a GCD-like strategy but progressively refines its sampling distribution by incorporating information from previously generated samples. This iterative refinement, based on the concept of <em>approximate expected futures</em>, <strong>helps to reduce bias in sampling</strong>, ensuring outputs closely align with the original language model&rsquo;s distribution while still adhering to grammatical constraints. The algorithm leverages learned approximations of the probability of future grammaticality, dynamically updating these probabilities as more samples become available. <strong>This adaptive mechanism is crucial for mitigating the distortion effects</strong> often observed in GCD, where grammaticality constraints can significantly skew the sampling process, leading to outputs that are grammatically correct but improbable under the original model. The theoretical analysis of the algorithm&rsquo;s convergence behavior and empirical evaluations on code generation and natural language processing tasks demonstrates <strong>ASAP&rsquo;s superior ability to produce high-likelihood, grammatically-correct outputs</strong> compared to GCD, highlighting its significance in addressing the challenges of generating highly structured outputs from large language models.</p><h4 class="relative group">Empirical Results<div id=empirical-results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#empirical-results aria-label=Anchor>#</a></span></h4><p>An Empirical Results section would ideally present a detailed analysis of experimental findings, comparing the proposed ASAp algorithm against existing GCD techniques. Key aspects to cover include <strong>quantitative metrics</strong> such as KL divergence to measure the alignment between the sampled distribution and the true LLM distribution, conditioned on the grammar. The results should show that ASAp converges towards the target distribution, better respecting the LLM&rsquo;s probabilities while maintaining grammatical constraints. Visualizations like plots of KL divergence over iterations or expectation comparisons would strengthen the analysis, showcasing the algorithm&rsquo;s convergence behavior and effectiveness. The discussion should explain any unexpected results or discrepancies, acknowledge limitations, and potentially discuss the impact of hyperparameter choices or dataset characteristics on performance. <strong>Qualitative analysis</strong> of generated outputs, perhaps including examples, would provide further insight into the quality and diversity of samples produced by each algorithm. Finally, a thorough comparison across different tasks (code generation, NLP tasks) would demonstrate the algorithm&rsquo;s generalizability and robustness. <strong>Statistical significance</strong> testing, where applicable, is crucial for ensuring that observed differences are not due to random chance. Overall, a strong Empirical Results section would provide compelling evidence of ASAp&rsquo;s superiority over GCD, establishing its value as a superior algorithm for grammar-aligned decoding.</p><h4 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h4><p>The &ldquo;Future Work&rdquo; section of this research paper presents exciting avenues for enhancing grammar-aligned decoding (GAD). <strong>Improving the convergence speed of the ASAp algorithm</strong> is paramount; its current slow convergence limits practical applications. Exploring <strong>more efficient approximation techniques for expected future grammaticality (EFG)</strong> could significantly accelerate convergence. The authors suggest investigating <strong>targeted beam search strategies</strong> to effectively explore grammar paths and avoid redundant sampling. Furthermore, extending the framework beyond context-free grammars to handle more complex grammatical structures, like those found in programming languages, is a significant challenge and opportunity. <strong>Investigating the impact of different language models and their inherent biases</strong> on GAD performance is also crucial for a broader understanding of the algorithm‚Äôs capabilities and limitations. Finally, a key area for future research is <strong>the rigorous evaluation of GAD on a wider array of tasks and benchmarks</strong>, such as machine translation and program synthesis, to assess its general applicability and compare its performance against existing methods.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_4_1.jpg alt></figure></p><blockquote><p>This figure shows two tries representing the probability distribution learned by the ASAp algorithm after two sampling iterations. The left trie shows the state after sampling the string &lsquo;00000&rsquo;. The right trie shows the state after subsequently sampling the string &lsquo;11111&rsquo;. The nodes represent prefixes of strings, edges represent the next token in the sequence, and edge weights represent conditional probabilities. The red numbers highlight updates to the approximate expected future grammaticality (EFG) after each sampling iteration, showing how ASAp refines probability estimates based on newly observed data to better align sampling with the target distribution while respecting grammar constraints.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_6_1.jpg alt></figure></p><blockquote><p>This figure illustrates the trie data structure used by the ASAp algorithm. The left side shows the trie after sampling &lsquo;00000&rsquo; as the first string, demonstrating how the algorithm updates the expected future grammaticality (EFG) values (shown in red). The right side shows the same trie after sampling &lsquo;11111&rsquo; as the second string, again illustrating the updated EFGs. The figure demonstrates how ASAp iteratively refines its approximation of the grammaticality probabilities through repeated sampling and updating.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_7_1.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions obtained using ASAp (QASAp) and GCD (QGCD) against the original LLM distribution (P) for three different benchmarks: an illustrative example, SLIA/name-combine-4-long, and SLIA/initials-small. The plots show how the KL divergence changes over a certain number of iterations or samples. This illustrates the convergence of ASAp towards the true distribution, which represents the ability of ASAp to align sampling with the grammar constraint without distorting the LLM&rsquo;s distribution, unlike GCD.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_8_1.jpg alt></figure></p><blockquote><p>This figure displays scatter plots comparing the empirical expectations of QASAp and QGCD against the true expectation of P (the original LLM distribution) after 2000 samples for three different tasks: SLIA, INV-BV4, and CP. Each point represents a benchmark, with its x-coordinate being the expectation from the LLM distribution and its y-coordinate representing the expectation from either QASAp (blue circles) or QGCD (red crosses). Points close to the diagonal line indicate that the algorithm&rsquo;s output distribution aligns well with the original LLM distribution, suggesting unbiased sampling.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_1.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions produced by ASAp and GCD, denoted as QASAp and QGCD respectively, and the original LLM distribution P. The KL divergence is a measure of how different two probability distributions are. Lower KL divergence indicates better alignment with the original distribution. The figure shows KL divergence over a series of iterations for three different benchmarks: (a) Illustrative Ex. 3, (b) SLIA/name-combine-4-long, and (c) SLIA/initials-small. This allows for a visual comparison of how well each algorithm maintains the original LLM distribution while respecting the grammar constraints.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_2.jpg alt></figure></p><blockquote><p>This figure shows the expectations of QASAP, QGCD, and the original LLM distribution P for three different benchmarks: an illustrative example (a), SLIA/name-combine-4-long (b), and SLIA/initials-small (c). Each plot displays the expectation values over time as the number of samples increases, allowing one to observe how the expectations from ASAp and GCD approach or deviate from the true LLM distribution (P). This visual representation helps understand the algorithms&rsquo; convergence properties toward the ideal GAD distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_3.jpg alt></figure></p><blockquote><p>This figure compares the Kullback-Leibler (KL) divergence between the distributions produced by ASAp (QASAp) and GCD (QGCD) and the true distribution P of the language model for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The KL divergence is a measure of how different two probability distributions are. A smaller KL divergence indicates a better alignment between the two distributions. The plots show how the KL divergence changes over the number of samples generated. It shows that ASAp&rsquo;s distribution generally converges more closely to P than GCD&rsquo;s distribution does.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_4.jpg alt></figure></p><blockquote><p>This figure presents a comparison of the Kullback-Leibler (KL) divergence between the distributions generated by the ASAp and GCD algorithms and the original language model (LLM) distribution (P). The KL divergence measures how different a probability distribution is from a target distribution. A lower KL divergence indicates better alignment. The plots demonstrate the convergence of ASAp and GCD over time. Each subfigure represents a different benchmark task, illustrating the varying convergence speeds and levels of alignment for each method across different tasks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_5.jpg alt></figure></p><blockquote><p>This figure presents a comparison of the Kullback-Leibler (KL) divergence between the distributions generated by the ASAp and GCD algorithms (QASAp and QGCD, respectively) and the original LLM distribution (P) across multiple benchmarks. The KL divergence serves as a measure of how well the algorithm-generated distributions align with the original LLM distribution, with lower KL values indicating better alignment. The figure visually shows the KL divergence over iterations, allowing for an assessment of convergence and the relative performance of ASAp and GCD. Three subfigures ((a), (b), (c)) show the KL-divergence for different benchmarks: (a) an illustrative example, (b) SLIA/name-combine-4-long benchmark, and (c) SLIA/initials-small benchmark.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_6.jpg alt></figure></p><blockquote><p>This figure displays the empirical expectations of QASAP, QGCD, and the original language model (P) over 2000 samples for three different benchmarks: an illustrative example (Ex. 3), SLIA/name-combine-4-long, and SLIA/initials-small. The x-axis represents the number of samples, and the y-axis represents the expectation. The plots show how the expectations of QASAP and QGCD converge to the target expectation of P over time. The convergence rate and closeness to the target vary across benchmarks. This is a key visual representation of how ASAp improves over GCD in aligning the sampled distribution with the language model&rsquo;s distribution, while still respecting the grammatical constraints.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_7.jpg alt></figure></p><blockquote><p>This figure presents a comparison of the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD algorithms, respectively, and the target LLM distribution (P). The KL divergence measures how different the sampled distribution (Q) is from the true distribution of the language model. Lower KL divergence values indicate that the generated distribution is closer to the true distribution of the LLM. The plots show the KL divergence over time (number of samples) for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long and SLIA/initials-small. It visually shows how ASAp converges faster to the true distribution in all benchmarks, demonstrating that ASAp better aligns with the LLM distribution than GCD.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_8.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, and the original LLM distribution (P) across iterations. The plots show the KL divergence over time for different tasks. Lower KL divergence indicates better alignment with the original LLM distribution while respecting the grammatical constraints. The plots show that ASAp often converges closer to the target distribution than GCD.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_9.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD and the original LLM distribution (P) for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The x-axis represents the number of samples, and the y-axis represents the KL divergence. The plots show how the KL divergence changes as more samples are generated by each algorithm. This helps to visualize the convergence of ASAp and GCD towards the target distribution, illustrating ASAp&rsquo;s faster convergence and closer alignment with the original LLM distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_10.jpg alt></figure></p><blockquote><p>This figure shows scatter plots comparing the empirical expectations of QASAP and QGCD against the expectations of P after 2000 samples for three different tasks: SLIA, INV-BV4, and CP. Each point represents a benchmark, with the x-coordinate being the expectation under P, and the y-coordinate being the expectation under either QASAP or QGCD. Points closer to the diagonal indicate better alignment between the two distributions, suggesting that the sampling methods are accurately reflecting the LLM&rsquo;s distribution when constrained by the grammar. The plot visually demonstrates the extent to which ASAp better aligns with the expected distribution than GCD, especially for the SLIA and INV-BV4 tasks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_11.jpg alt></figure></p><blockquote><p>This figure shows the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD and the target distribution P for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The KL divergence is a measure of how different two probability distributions are. A lower KL divergence indicates that the two distributions are more similar. In this figure, we can see that ASAp generally achieves a lower KL divergence than GCD, which means that ASAp&rsquo;s generated distribution is closer to the target distribution than GCD&rsquo;s. This suggests that ASAp is a better algorithm for grammar-aligned decoding than GCD. The x-axis shows the number of samples and the y-axis shows the KL divergence.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_12.jpg alt></figure></p><blockquote><p>This figure shows the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, and the target distribution P, for three different benchmarks: an illustrative example from the paper, and two real-world tasks from the SLIA benchmark (name-combine-4-long and initials-small). The x-axis represents the number of iterations, and the y-axis represents the KL divergence. The plots demonstrate how the KL divergence of ASAp converges faster to the ideal distribution P than GCD in the illustrative example and SLIA/name-combine-4-long, while in the SLIA/initials-small benchmark, the convergence is slower but still performs better than GCD.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_13.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by the ASAp and GCD algorithms and the original LLM distribution (P). The KL divergence measures how different the distributions are. Lower KL divergence indicates better alignment with the original LLM distribution. The figure shows this divergence over multiple iterations (samples) for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. This helps to visualize the convergence of ASAp towards the LLM&rsquo;s distribution while adhering to grammatical constraints, whereas GCD shows a larger and more persistent divergence.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_14.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD (QASAp and QGCD) and the target distribution P. It shows the KL divergence over a series of iterations for three different benchmarks: an illustrative example (a), SLIA/name-combine-4-long (b), and SLIA/initials-small (c). The KL divergence measures how different the generated distributions are from the target LLM distribution, conditioned on the grammar constraints. Lower KL divergence indicates better alignment with the target distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_15.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp (approximate grammar-aligned decoding) and GCD (grammar-constrained decoding) and the true distribution P (the original LLM distribution) for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The x-axis represents the number of samples, and the y-axis represents the KL divergence. The plots show how quickly the KL divergence between the generated distributions and the true distribution decreases over time. For all the benchmarks, the KL divergence of ASAp decreases faster than that of GCD. This demonstrates that ASAp converges to the LLM distribution much faster than GCD and therefore better preserves the LLM&rsquo;s distribution when sampling strings that satisfy the constraints of a context-free grammar.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_16.jpg alt></figure></p><blockquote><p>This figure displays the expectation values, or averages, of the probability distributions produced by ASAp, GCD, and the original Language Model (LLM) over the course of 2000 iterations. Each subplot represents a different benchmark task from the paper. The plots visually show how well the sampling distributions of ASAp and GCD align with the LLM&rsquo;s distribution. The closer the lines are to each other and to the LLM, the better the algorithm is at preserving the LLM&rsquo;s original distribution while enforcing grammatical constraints.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_17.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, and the target distribution P, across multiple benchmarks. The plots show how quickly the KL divergence of ASAp converges towards zero, which represents alignment with the target distribution. In contrast, GCD‚Äôs KL divergence remains higher, indicating a significant distribution mismatch. Three benchmarks, Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small, are shown, highlighting the varying convergence rates across different tasks. This visual representation effectively demonstrates ASAp&rsquo;s superior performance in producing outputs aligned with the original LLM&rsquo;s distribution compared to GCD.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_18.jpg alt></figure></p><blockquote><p>This figure shows the Kullback-Leibler (KL) divergence between the distributions of ASAp and GCD, and the target distribution P for several benchmarks across different numbers of samples. Lower KL divergence indicates better alignment with the target distribution. The plots show that for some benchmarks (e.g., SLIA/name-combine-4-long), ASAp quickly converges to P while GCD does not. For other benchmarks (e.g., SLIA/initials-small), both methods may not converge as quickly, highlighting variations in convergence speeds depending on the task.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_19.jpg alt></figure></p><blockquote><p>This figure presents KL divergence plots, comparing the distributions generated by ASAp and GCD against the original LLM distribution (P) across various benchmarks. Lower KL divergence indicates a better alignment with the target distribution. It shows how the KL divergence of ASAp decreases over time towards 0, demonstrating its convergence to the LLM&rsquo;s distribution while respecting grammatical constraints. In contrast, GCD shows less convergence or higher KL divergence, reflecting a greater distortion of the LLM&rsquo;s distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_15_20.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by the ASAp and GCD algorithms and the target LLM distribution (P). The plots show how the KL divergence changes over the number of samples (iterations) for three different tasks: an illustrative example and two tasks from the SLIA benchmark. Lower KL divergence indicates better alignment with the target distribution. The figure shows that ASAp generally converges to the target distribution better than GCD, particularly in the illustrative example and the SLIA/name-combine-4-long benchmark.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_1.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions produced by the ASAp and GCD algorithms and the original LLM distribution (P) over 2000 samples. The KL divergence is a measure of how different two probability distributions are. A lower KL divergence indicates that the algorithm&rsquo;s distribution is closer to the original LLM&rsquo;s distribution. The figure shows multiple subplots, each representing a different benchmark task. The plots show the KL divergence over time (iterations). Lower values indicate better alignment with the original LLM&rsquo;s distribution, suggesting higher quality outputs.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_2.jpg alt></figure></p><blockquote><p>This figure presents a comparison of the performance of ASAp and GCD in matching the expected distribution of the Language Model (LM). Each point represents a benchmark task. The x-coordinate shows the expectation from the LM&rsquo;s distribution (P), while the y-coordinate shows the empirical expectation obtained using either ASAp or GCD after 2000 sampling iterations. Points close to the diagonal line indicate that the algorithm accurately reflects the LM&rsquo;s distribution (a good result). The plots reveal that ASAp generally shows better convergence towards the LM&rsquo;s expectation than GCD, demonstrating ASAp&rsquo;s effectiveness in aligning the sampling distribution with the desired distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_3.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions produced by ASAp (QASAp) and GCD (QGCD) and the original LLM distribution (P). The KL divergence measures how different two probability distributions are. A lower KL divergence indicates that the two distributions are more similar. The plots show the KL divergence over the number of samples generated by the algorithms for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The plots illustrate how ASAp converges to a distribution that is closer to the LLM&rsquo;s original distribution than GCD for different structured prediction tasks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_4.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD (QASAp and QGCD, respectively) and the original LLM distribution (P). The KL divergence measures how different two probability distributions are. A lower KL divergence indicates a closer match between the distributions. The plots show how these divergences change over 2000 iterations/samples for three different benchmarks: an illustrative example and two benchmarks from SLIA (Syntax-Guided Synthesis). The figure helps to visualize the convergence of ASAp&rsquo;s distribution towards the original LLM distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_5.jpg alt></figure></p><blockquote><p>This figure shows the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, and the target distribution P. The KL divergence measures how different two probability distributions are. A lower KL divergence indicates that the two distributions are more similar. In this figure, the KL divergence is plotted against the number of samples, allowing us to observe how the algorithms converge to the target distribution over time. The different subfigures correspond to various experiments, each showcasing the convergence behavior of the two algorithms in different structured prediction tasks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_6.jpg alt></figure></p><blockquote><p>This figure presents the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, denoted as QASAp and QGCD respectively, and the original LLM distribution P. It shows how the KL divergence changes over the number of samples for three different tasks: an illustrative example (5a), SLIA/name-combine-4-long (5b), and SLIA/initials-small (5c). Lower KL divergence indicates better alignment with the LLM&rsquo;s original distribution. The plot illustrates that ASAp typically converges to a lower KL divergence than GCD, demonstrating that it better preserves the LLM&rsquo;s distribution while still enforcing grammatical constraints.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_7.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp (QASAp) and GCD (QGCD), and the original LLM distribution (P). The KL divergence is a measure of how different two probability distributions are. Lower KL divergence indicates better alignment between the generated distributions and the original LLM distribution. The figure shows KL divergence over a series of iterations (samples) for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The plots illustrate how the KL divergence changes as the number of samples increases for ASAp and GCD on each benchmark, showcasing the convergence properties of each algorithm.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_8.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, and the original LLM distribution (P). It shows the convergence of KL divergence over 2000 iterations for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. Lower KL divergence indicates better alignment with the original LLM distribution. The plots reveal that ASAp, in most cases, converges faster and closer to the LLM&rsquo;s distribution than GCD, indicating that ASAp effectively reduces the distributional distortion caused by constrained decoding.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_9.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD and the target distribution P, plotted against the number of iterations. It shows how ASAp&rsquo;s KL divergence converges to 0, indicating that it aligns better with the target distribution than GCD. The plots showcase this for three different benchmarks: an illustrative example and two benchmarks from the SLIA dataset.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_10.jpg alt></figure></p><blockquote><p>This figure presents scatter plots comparing the empirical expectations obtained using ASAp and GCD against the true expectations from the LLM distribution P, after 2000 sampling iterations. Each point represents a benchmark task. Points closer to the diagonal line (y=x) indicate better alignment between the method&rsquo;s estimated expectation and the true LLM expectation. The figure visually demonstrates how ASAp, in most cases, achieves a closer alignment to the true LLM expectations compared to GCD.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_11.jpg alt></figure></p><blockquote><p>This figure presents a comparison of the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, respectively, against the target distribution P (the original LLM distribution). The x-axis represents the number of samples, and the y-axis represents the KL divergence. Multiple subfigures show results for different benchmarks, illustrating the convergence behavior of each algorithm in approximating P. Lower KL divergence indicates that the generated distribution is closer to the target distribution, showing better alignment.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_12.jpg alt></figure></p><blockquote><p>This figure presents a comparison of the empirical expectations obtained using ASAp and GCD against the true expectation from the LLM distribution (P) after 2000 samples. Each point represents a benchmark, with the x-coordinate showing the expectation from P and the y-coordinate showing the expectation from either ASAp or GCD. Points close to the diagonal indicate that ASAp or GCD accurately estimates the LLM&rsquo;s expectation. The plot helps visualize how close ASAp and GCD get to the true distribution of the LLM, demonstrating ASAp&rsquo;s superiority in aligning with the target distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_13.jpg alt></figure></p><blockquote><p>This figure shows the Kullback-Leibler (KL) divergence between the distributions produced by ASAp and GCD, and the original LLM distribution (P). KL divergence is a measure of how different two probability distributions are. Lower KL divergence indicates better alignment with the original LLM distribution. The figure displays KL divergence over a certain number of iterations (samples) for three different benchmarks (Illustrative Ex. 3, SLIA/name-combine-4-long, SLIA/initials-small). It illustrates how ASAp&rsquo;s distribution aligns more closely with P than GCD&rsquo;s distribution, especially over more iterations.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_14.jpg alt></figure></p><blockquote><p>This figure compares the empirical expectations of the variables computed by QASAP and QGCD against the target expectation P for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The plots show how the expectations of QASAP and QGCD converge towards the true expectation P over time. In the first benchmark (Illustrative Ex. 3), QASAP quickly converges to P, whereas QGCD does not. The second and third benchmark show varying convergence rates of QASAP towards P and highlight that while QASAP converges towards the correct expectation for all three benchmarks, the rate of convergence varies.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_15.jpg alt></figure></p><blockquote><p>This figure shows the Kullback-Leibler (KL) divergence between the distributions produced by the ASAp and GCD algorithms and the original LLM distribution (P). KL divergence measures how different two probability distributions are; lower values indicate closer agreement. The figure presents results across multiple experiments, showing that ASAp (blue, dashed line) generally demonstrates lower KL divergence than GCD (red, solid line), indicating that ASAp&rsquo;s output distribution more closely aligns with the LLM&rsquo;s original distribution while adhering to the grammar constraints.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_16.jpg alt></figure></p><blockquote><p>This figure displays the KL divergence between the distributions produced by ASAp and GCD, and the original LLM distribution (P). It visually represents how well the sampling methods align with the true language model distribution, conditioned on the grammatical constraints. Lower KL divergence indicates better alignment. The plots show KL divergence over time (number of samples) for various tasks (Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small), enabling a comparison of ASAp and GCD in terms of their distribution bias.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_17.jpg alt></figure></p><blockquote><p>This figure displays KL divergence plots, measuring the difference between the distributions produced by ASAp and GCD and the target LLM distribution (P). It shows the convergence of ASAp and GCD to the LLM distribution over 2000 iterations across various benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The plots illustrate how ASAp, in many cases, converges more closely to the LLM&rsquo;s distribution compared to GCD, indicating that ASAp better preserves the original LLM&rsquo;s distribution while maintaining grammatical correctness. Note that this is not always the case, depending on the data, as shown in (c).</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_18.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, denoted as QASAp and QGCD respectively, and the original LLM distribution P. The KL divergence measures how different the sampled distribution is from the target distribution. Lower KL divergence indicates that the sampled distribution is closer to the target distribution. The figure shows this divergence over 2000 iterations for three different benchmarks. This allows for a visual comparison of the convergence speed and accuracy of both methods in approximating the desired distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_19.jpg alt></figure></p><blockquote><p>This figure presents a comparison of the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, and the target distribution P, across multiple benchmarks. Lower KL divergence indicates better alignment with the target distribution. The figure shows that for some benchmarks (e.g., Illustrative Ex. 3 and SLIA/name-combine-4-long), ASAp converges to a lower KL divergence than GCD, demonstrating that it better aligns with the original LLM&rsquo;s distribution while maintaining grammatical constraints. For other benchmarks (e.g., SLIA/initials-small), the divergence remains high for both algorithms, suggesting that the task might be more challenging.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_16_20.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, respectively, and the target distribution P over 2000 iterations for three different benchmarks: (a) Illustrative Ex. 3, (b) SLIA/name-combine-4-long, and (c) SLIA/initials-small. The KL divergence measures how different the generated distributions are from the ideal distribution P. Lower KL divergence indicates a better alignment with the target distribution. The plot helps visualize the convergence of the proposed ASAp algorithm towards the desired distribution, as compared to the GCD method.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_1.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp (our proposed algorithm) and GCD (existing grammar-constrained decoding), and the target LLM distribution P, across three different benchmarks (Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small). The KL divergence measures how different the generated distributions are from the ideal distribution P. Lower KL divergence indicates better alignment with the original LLM&rsquo;s distribution, implying higher quality output while preserving grammatical correctness. The plots show KL divergence over the number of samples, illustrating the convergence behavior of the two algorithms.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_2.jpg alt></figure></p><blockquote><p>This figure displays the expected values of the probability distributions for QASAP, QGCD, and P. It shows the convergence of ASAp towards the target distribution P across three different benchmarks: Illustrative Example 3, SLIA/name-combine-4-long, and SLIA/initials-small. Each benchmark demonstrates how ASAp&rsquo;s expectation values approach those of P, indicating the algorithm&rsquo;s ability to generate samples in line with the original LLM distribution. The figure also illustrates how QGCD is less successful in this regard.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_3.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp (QASAp) and GCD (QGCD) and the target distribution P over 2000 iterations for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The KL divergence is a measure of how different two probability distributions are. A lower KL divergence indicates that the generated distribution is closer to the target distribution. In all three cases, ASAp demonstrates a lower KL divergence, indicating its superiority to GCD in matching the target distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_4.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD and the target distribution P for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The KL divergence is a measure of how different two probability distributions are. A lower KL divergence indicates that the generated distribution is closer to the target distribution. The x-axis represents the number of samples, and the y-axis represents the KL divergence. The figure shows how the KL divergence changes as more samples are generated, giving an indication of convergence to the target distribution. The dashed lines represent the expected KL divergence, highlighting the difference between ASAp and GCD in their approximation of the target distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_5.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by the ASAp and GCD algorithms (QASAp and QGCD, respectively) and the target distribution P (the original LLM distribution) across multiple benchmarks. The KL divergence is a measure of how different two probability distributions are. Lower KL divergence indicates better alignment between the generated and target distributions. The figure visualizes this divergence over many sampling iterations. Three benchmarks are shown, each one demonstrating different convergence speeds. This shows how ASAp, despite starting with a GCD strategy, improves its alignment to P over time.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_6.jpg alt></figure></p><blockquote><p>This figure shows the empirical expectations of QASAP, QGCD, and P for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The x-axis represents the number of samples, and the y-axis represents the expectation. The figure demonstrates the convergence of QASAP towards the target distribution P, while QGCD often deviates considerably. Each subplot corresponds to a different benchmark and provides a visual comparison of the expectations from the three methods (ASAP, GCD and P). This helps in visualizing how well the adaptive sampling with approximate expected futures (ASAp) and grammar constrained decoding (GCD) methods approximate the desired probability distribution compared to the true distribution from the language model (P).</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_7.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD, denoted as QASAp and QGCD respectively, compared to the original LLM&rsquo;s distribution P. The KL divergence is a measure of how different two probability distributions are. Lower values indicate that the distributions are closer, meaning ASAp and GCD are better approximations of P. The figure shows that ASAp&rsquo;s KL divergence decreases much faster and approaches lower values than GCD&rsquo;s KL divergence, suggesting that ASAp better preserves the original LLM&rsquo;s distribution while still ensuring the output is grammatical. Multiple subfigures show results for different benchmarks (Illustrative Ex. 3, SLIA/name-combine-4-long, SLIA/initials-small).</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_8.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp (QASAp) and GCD (QGCD) compared to the original LLM&rsquo;s distribution (P). The KL divergence measures how different the sampling distributions of ASAp and GCD are from the true distribution of the language model, conditioned on the grammar. Lower KL divergence indicates better alignment with the LLM&rsquo;s distribution. The figure shows results for multiple benchmarks, with each subplot representing a different task (&lsquo;Illustrative Ex. 3&rsquo;, &lsquo;SLIA/name-combine-4-long&rsquo;, and &lsquo;SLIA/initials-small&rsquo;). The x-axis represents the number of samples, and the y-axis represents the KL divergence.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_9.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions produced by ASAp and GCD and the target distribution P for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The KL divergence measures how different the generated distributions are from the target distribution. A lower KL divergence indicates better alignment with the target. The x-axis represents the number of samples generated, and the y-axis represents the KL divergence. The plots show that ASAp typically converges to a lower KL divergence than GCD, indicating that it produces samples closer to the target distribution.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_10.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD and the original LLM distribution (P), for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The KL divergence measures how different the generated distributions are from the original LLM distribution. Lower KL divergence indicates better alignment with the original LLM distribution. Each subplot shows the KL divergence over a number of iterations, giving a visual representation of the convergence of both algorithms towards or away from the original distribution. The x-axis represents the number of samples used for the KL divergence calculation, and the y-axis shows the KL divergence value. This allows to see whether either method better respects the LLM distribution while adhering to grammatical constraints.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_11.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp (QASAp) and GCD (QGCD) and the true distribution P over different iterations. The plots shows the convergence of KL divergence to the target distribution P for three different benchmarks: an illustrative example and two benchmarks from the SLIA task. The y-axis represents the KL divergence, while the x-axis represents the number of iterations. The plots illustrate that ASAp converges more consistently and to a lower KL divergence than GCD across all benchmarks, demonstrating that it better aligns with the true distribution P.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_12.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by the ASAp and GCD algorithms and the original LLM distribution (P). The KL divergence measures how different the generated distributions are from the original LLM distribution. Lower KL divergence indicates that the generated distribution is closer to the original LLM distribution. The figure shows this divergence for multiple benchmarks (Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small). It shows how the KL divergence changes with the number of samples generated, illustrating the convergence properties of each decoding algorithm. ASAp generally shows better alignment with the LLM&rsquo;s distribution over time than GCD.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_13.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp (QASAp) and GCD (QGCD) and the target distribution P for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The KL divergence measures how different the generated distributions are from the ideal distribution. Lower KL divergence indicates a better alignment with the target distribution. The x-axis represents the number of iterations/samples, and the y-axis represents the KL divergence.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_14.jpg alt></figure></p><blockquote><p>This figure displays the expected values of QASAP, QGCD, and the original LLM distribution P across iterations for three different benchmarks. It demonstrates how the expected values of QASAP converge to those of P over time, indicating the algorithm&rsquo;s ability to align the sampling distribution with the LLM&rsquo;s distribution while maintaining grammatical constraints. In contrast, QGCD&rsquo;s expected values often deviate significantly from P, illustrating how existing constrained decoding methods can distort the LLM&rsquo;s distribution. The plots show that while in some cases, both ASAp and GCD converge to the same expectation of P, in many cases, ASAp shows closer alignment with P. Therefore, this demonstrates that ASAp is a superior algorithm in cases where high quality of the generated outputs (according to the LLM) are desired.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_15.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions produced by the ASAp and GCD algorithms (QASAp and QGCD, respectively) and the original LLM distribution (P). The KL divergence is a measure of how different two probability distributions are. Lower KL divergence indicates better alignment with the original distribution. The figure shows KL divergence over the number of samples generated, illustrating the convergence properties of ASAp and GCD. Different subfigures represent different tasks or experiments, highlighting how the algorithms perform on various structured prediction problems.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_16.jpg alt></figure></p><blockquote><p>This figure shows the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD from the target distribution P for three different benchmarks (Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small). The KL divergence is a measure of how different two probability distributions are. A lower KL divergence indicates that the two distributions are more similar. In all three benchmarks, ASAp converges to a lower KL divergence than GCD, indicating that ASAp produces outputs that are more similar to the outputs generated by the LLM than GCD. The plots illustrate the convergence of KL divergence over 2,000 iterations or samples.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_17.jpg alt></figure></p><blockquote><p>This figure shows the KL divergence between the distributions generated by ASAp and GCD, and the target distribution P, across different numbers of samples. The plots illustrate how the KL divergence changes as the number of samples increases for various benchmarks. Lower KL divergence indicates better alignment with the target distribution, suggesting that ASAp better approximates the target distribution than GCD. The benchmarks used are shown in the subplots.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_18.jpg alt></figure></p><blockquote><p>This figure displays the empirical expectation values of QASAP, QGCD, and the original LLM distribution P across different iterations for three benchmarks: an illustrative example and two from the SLIA task (name-combine-4-long and initials-small). The plots show how the expectation values of QASAP converge towards the true expectations given by P, which represents the ideal GAD distribution, while QGCD&rsquo;s expectations often deviate significantly. This visualization demonstrates the improvement of ASAp over existing methods in aligning the sampling distribution with the desired grammatical constraints.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_19.jpg alt></figure></p><blockquote><p>This figure displays the Kullback-Leibler (KL) divergence between the distributions generated by ASAp and GCD (QASAp and QGCD, respectively) and the original LLM distribution (P) across three different tasks: an illustrative example (a), SLIA/name-combine-4-long (b), and SLIA/initials-small (c). The plots show how the KL divergence changes over the number of iterations. Lower KL divergence indicates better alignment between the generated distribution and the original LLM&rsquo;s distribution, suggesting better preservation of the LLM&rsquo;s original distribution after applying the constraints. It demonstrates how ASAp better maintains alignment with the original distribution than GCD in each case, although the rate of convergence varies across tasks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_17_20.jpg alt></figure></p><blockquote><p>This figure shows the KL divergence between the distributions produced by ASAp and GCD, respectively, and the original LLM distribution P. The plots show how the KL divergence changes over 2000 iterations of sampling for different benchmarks. A lower KL divergence indicates that the algorithm&rsquo;s output distribution is closer to the original LLM distribution, indicating that ASAp better preserves the LLM&rsquo;s distribution while ensuring grammaticality.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5G7ve8E1Lu/figures_18_1.jpg alt></figure></p><blockquote><p>This figure shows the Kullback-Leibler (KL) divergence between the distributions obtained by ASAp and GCD, and the original LLM distribution (P), for three different benchmarks: Illustrative Ex. 3, SLIA/name-combine-4-long, and SLIA/initials-small. The KL divergence measures how different the distributions are. A lower KL divergence indicates a closer match to the original LLM&rsquo;s distribution. The plots show the KL divergence over time, allowing comparison of the convergence rates of ASAp and GCD.</p></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-070c9c03879c5a2589f71163f0afdb38 class=gallery><img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5G7ve8E1Lu/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/&amp;title=Grammar-Aligned%20Decoding" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/&amp;text=Grammar-Aligned%20Decoding" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/&amp;subject=Grammar-Aligned%20Decoding" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_posters/5G7ve8E1Lu/index.md",oid_likes="likes_posters/5G7ve8E1Lu/index.md"</script><script type=text/javascript src=/neurips2024/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/neurips2024/posters/qd8blc0o0f/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">GRANOLA: Adaptive Normalization for Graph Neural Networks</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/neurips2024/posters/utnzkl5buc/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Gradual Domain Adaptation via Manifold-Constrained Distributionally Robust Optimization</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
AI Paper Reviewer</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/neurips2024/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/neurips2024/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>