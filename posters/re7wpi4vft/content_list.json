[{"type": "text", "text": "Your Diffusion Model is Secretly a Noise Classifier and Benefits from Contrastive Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yunshu $\\mathbf{W}\\mathbf{u}^{1}$ , Yingtao Luo2, Xianghao $\\mathbf{Kong^{1}}$ , Evangelos E. Papalexakis1, Greg Ver Steeg1 ", "page_idx": 0}, {"type": "text", "text": "1University of California Riverside, 2Carnegie Mellon University {ywu380,xkong016,epapalex,gregoryv}@ucr.edu, yingtaol@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models learn to denoise data and the trained denoiser is then used to generate new samples from the data distribution. In this paper, we revisit the diffusion sampling process and identify a fundamental cause of sample quality degradation: the denoiser is poorly estimated in regions that are far Outside Of the training Distribution (OOD), and the sampling process inevitably evaluates in these OOD regions. This can become problematic for all sampling methods, especially when we move to parallel sampling which requires us to initialize and update the entire sample trajectory of dynamics in parallel, leading to many OOD evaluations. To address this problem, we introduce a new self-supervised training objective that differentiates the levels of noise added to a sample, leading to improved OOD denoising performance. The approach is based on our observation that diffusion models implicitly define a log-likelihood ratio that distinguishes distributions with different amounts of noise, and this expression depends on denoiser performance outside the standard training distribution. We show by diverse experiments that the proposed contrastive diffusion training is effective for both sequential and spiagrnaillfiecl asnettltyi.n 1gs, and it improves the performance and speed of parallel samplers ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Denoising diffusion models [29] achieve state-of-the-art performance on various unsupervised learning tasks and have intriguing theoretical connections to methods like denoising autoencoders [37], VAEs [6], stochastic differential equations [20, 34], information theory [13], and score matching [31, 32]. Diffusion models are presented with data samples corrupted by a forward dynamical process that progressively adds more Gaussian noise and trained to reverse this dynamics or denoise the corrupted samples. Samples are then generated by applying the reverse dynamics on images of pure Gaussian noise to produce high-quality samples from the target distribution. ", "page_idx": 0}, {"type": "text", "text": "The key to the success of diffusion models is the dynamics that gradually bridges the source and a target distribution, but it suffers from slow sampling, as sequentially simulating these dynamics can take thousands of denoising steps for one sample. Most recent works attempt to expedite the sequential dynamics by taking fewer, larger steps [30, 11, 18, 22]. However, the complexity of these samplers and the need for expensive sampling hyper-parameter grid searches tailored to specific datasets makes them difficult to generalize. ", "page_idx": 0}, {"type": "text", "text": "Shih et al. 2024 suggests a different approach by randomly initializing the entire path of the reverse dynamics and then updating all the steps in the path in parallel. The parallel sampling approach promises to drastically reduce wall-clock time at the cost of increased parallel computation. However, it encounters a problem that has largely gone unnoticed in the sequential sampling literature: while sequential paths sampled during generation are designed carefully to stay as close as possible to the forward paths that add noise to data, the parallel sampler often evaluates in regions far from where the score estimate (the denoiser) is trained, as illustrated in Fig. 1. The shading shows that the error of the score estimate is large in these regions, leading to poor performance for parallel samplers. Discretization errors can lead to similar issues, even for standard sequential samplers. ", "page_idx": 0}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/598b677624bcbf245360d044ed11e664c95513f2a675a0ca77557b4464f7b10e.jpg", "img_caption": ["Figure 1: We plot the error in the score estimate for an 1D two mode Gaussian example where diffusion dynamics bridge between a Gaussian and a mixture (see Appendix A.3). Regions near the standard forward training data paths have lower error magnitude (light), whereas other areas have higher error magnitude (dark). While sequential samplers adhere as closely as possible to low-error regions, parallel samplers initialize and update the entire sample trajectory (blue trajectories), leading to evaluations in high-error regions. When the sampling trajectory is initialized, most are inevitably in the OOD regions and will update to the low-error regions gradually. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We propose to improve the training of diffusion models so that the error of the denoiser is reduced in OOD regions, and we hypothesize that this should significantly improve the performance for parallel samplers as they require more OOD evaluations. Our approach starts with an unexpected connection: the optimal MSE denoiser that defines the diffusion dynamics also defines an optimal noise classifier that distinguishes between samples with different amounts of noise. This provides a useful additional signal for training, because optimizing for the noise classification task involves evaluating the denoiser for one noise level on samples from distributions at different noise levels, while standard MSE optimization only evaluates the denoiser on samples from the matching noisy distribution. Accurate denoiser evaluation in regions that are OOD for standard diffusion training is important for robust sampling dynamics. ", "page_idx": 1}, {"type": "text", "text": "Contributions: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We use the information-theoretic formulation of diffusion to draw connections between diffusion, log-likelihood ratio estimation, and classification. This reveals that optimal diffusion denoisers are also implicitly optimal classifiers for predicting the amount of noise added to an image.   \n\u2022 We leverage the noise classifier (via density ratio estimation [4]) interpretation to introduce a novel self-supervised loss function for regularizing diffusion model training, which we call the Contrastive Diffusion Loss (CDL). CDL provides training signal in regions that are OOD for the standard MSE diffusion loss.   \n\u2022 We show that CDL improves the trade-off between generation speed and sample quality, and that this advantage is consistent across different models, hyper-parameters, and sampling schemes. The improvement is especially substantial for parallel diffusion samplers [28] which rely heavily on OOD denoiser evaluations. ", "page_idx": 1}, {"type": "text", "text": "2 Diffusion Model Background: Optimal Denoisers are Density Estimators ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The defining feature of diffusion models is a sequence of distributions that progressively add noise to the data, from which we then learn to recover the original data. The (\u201cvariance preserving\u201d [34]) channel that mixes the signal $\\textbf{\\em x}$ with Gaussian noise is defined as $\\pmb{x}_{\\alpha}\\equiv\\sqrt{\\sigma(\\alpha)}\\pmb{x}+\\sqrt{\\sigma(-\\alpha)}\\pmb{\\epsilon}$ with $\\pmb{\\epsilon}\\sim\\mathcal{N}(0,\\mathbb{I}),\\pmb{x}\\sim p(\\pmb{x})$ , where $\\alpha$ represents the log of the Signal-to-Noise Ratio (SNR), $p(x)$ is the unknown data distribution for $\\pmb{x}\\in\\mathbb{R}^{d}$ , and $\\sigma(\\cdot)$ is the sigmoid function. We define the sequence of intermediate distributions drawn according to this channel with a subscript as $p_{\\alpha}(x)$ . By definition, we express $\\begin{array}{r}{\\operatorname*{lim}_{\\alpha\\to\\infty}p_{\\alpha}({\\pmb x})=p({\\pmb x})}\\end{array}$ in this paper. Note that we use a different scaling convention for noise from [11] and [6], where the former one takes ${\\pmb x}+\\sigma{\\pmb\\epsilon}$ as the forward noising channel and the latter one takes $\\sqrt{\\alpha_{t}}\\pmb{x}+\\sqrt{1-\\alpha_{t}}\\pmb{\\epsilon}$ as the forward noising channel. For further detailed relationships among these scaling conventions, please check App. B.3. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The minimum mean square error (MMSE) estimator $\\hat{\\epsilon}$ for recovering $\\epsilon$ from the noisy channel that mixes $\\textbf{\\em x}$ and $\\epsilon$ can be derived via variational calculus and written as follows. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}(x_{\\alpha},\\alpha)\\equiv\\mathbb{E}_{\\epsilon\\sim p(\\epsilon|x_{\\alpha})}[\\epsilon]=\\arg\\operatorname*{min}_{\\tilde{\\epsilon}(\\cdot,\\cdot)}\\mathbb{E}_{p(\\epsilon)p(x)}[\\|\\epsilon-\\tilde{\\epsilon}(x_{\\alpha},\\alpha)\\|_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Sampling from the true posterior is typically intractable, but by using a neural network to approximate the solution to the regression optimization problem, we can get an approximation for $\\hat{\\epsilon}$ . From [13], we see that log-likelihood can be written exactly in terms of an expression that depends only on the MMSE solution to the Gaussian denoising problem, i.e. ", "page_idx": 2}, {"type": "equation", "text": "$$\n-\\log p(\\pmb{x})=c+1/2\\int_{-\\infty}^{\\infty}\\mathbb{E}_{p(\\pmb{\\epsilon})}[\\|\\pmb{\\epsilon}-\\hat{\\pmb{\\epsilon}}(\\pmb{x}_{\\alpha},\\alpha)\\|_{2}^{2}]\\,d\\alpha.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The constant, $c=d/2\\log(2\\pi e)-{\\textstyle{\\frac{d}{2}}}\\int_{0}^{\\infty}d\\bar{\\alpha}\\;\\sigma(\\bar{\\alpha})$ does not depend on data and will play no role in our approach, as it cancels out in our derivations in Sec. 3. ", "page_idx": 2}, {"type": "text", "text": "3 What Your Diffusion Model is Hiding: Noise Classifiers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now introduce our first main result, which shows that diffusion models implicitly define optimal noise classifiers. Eq. (2) expresses the probability density of the data directly in terms of the denoising function. If we apply Eq. (2) to the noisy distributions that bridge the data and a Gaussian, $p_{\\zeta}(x)$ , we can see that all mixture densities can be written in terms of the same optimal denoising function, $\\hat{\\epsilon}(\\cdot,\\cdot)$ . The complete derivation is presented in App. A.2. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{-\\log p_{\\zeta}({\\pmb x})=c+1/2\\displaystyle\\int_{-\\infty}^{\\infty}d\\alpha\\,\\mathbb{E}_{p({\\pmb\\epsilon})}[\\|{\\pmb\\epsilon}-{\\pmb b}\\cdot\\hat{\\pmb\\epsilon}({\\pmb x}_{\\alpha},\\beta)\\|_{2}^{2}]}\\\\ {{\\pmb x}_{\\alpha}\\equiv\\sqrt{\\sigma(\\alpha)}{\\pmb x}+\\sqrt{\\sigma(-\\alpha)}{\\pmb\\epsilon}}\\\\ {\\beta\\equiv\\sigma^{-1}(\\sigma(\\zeta)\\sigma(\\alpha)),\\;{\\pmb b}\\equiv\\sqrt{\\sigma(-\\alpha)/\\sigma(-\\beta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively, if we find the optimal denoising function for the data distribution, it may be hypothesized that it can denoise an already noisy version of the data distribution. Using Eq. 2, this directly translates into an expression for density of mixture distributions. Differences in log likelihoods lead to cancellation of constants, and these Log Likelihood Ratios (LLR) are related to the optimal classifiers [4] as we show below. ", "page_idx": 2}, {"type": "text", "text": "To connect LLRs with classification, consider the following generative model. We generate a random binary label $q(y=\\pm1)=1/2$ . Then, conditioned on $y$ , we sample from some distribution $q(\\pmb{x}|\\boldsymbol{y})$ . Given samples $({\\pmb x},y)\\sim q({\\pmb x},y)=q({\\pmb x}|y)q(y)$ , the Bayes optimal classifier is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(y|x)=\\cfrac{q(x|y)q(y)}{q(x)}=\\cfrac{q(x|y)q(y)}{q(x|y=1)q(y=1)+q(x|y=-1)q(y=-1)}}\\\\ &{\\qquad\\qquad=1/(1+\\cfrac{q(x|-y)}{q(x|y)})=1/(1+\\exp(y(\\log q(x|y=-1)-\\log q(x|y=1))))}\\\\ &{\\log q(y|x)=-\\log(1+\\exp(y\\log\\cfrac{q(x|y=-1)}{q(x|y=1)}))=-\\operatorname{softplus}(y\\log\\cfrac{q(x|y=-1)}{q(x|y=1)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the second line, because $\\forall y,q(y)=1/2$ , these constants cancel out. Then we can just expand definitions and re-arrange to write in terms of log probabilities. ", "page_idx": 2}, {"type": "text", "text": "Contrastive Diffusion Loss (CDL) Our next contribution is to use the new connection between diffusion denoisers and noise classifiers to define a new training objective. We set the distributions $q(\\pmb{x}|y=1)$ and $q(\\pmb{x}|y=-1)$ to be two distributions at different noise levels that we can write in terms of the optimal diffusion denoiser from Eq. 3. So we have $q(\\mathbf{\\boldsymbol{x}}|y=1)\\equiv p(\\mathbf{\\boldsymbol{x}})$ , the data distribution, and $q(\\mathbf{\\emx}|y\\,=\\,-1)\\,\\equiv\\,p_{\\zeta}(\\mathbf{\\emx})$ , for some noise level, $\\zeta$ . Then given a sample $({\\pmb x},y)\\sim q({\\pmb x},y)$ the per-sample cross-entropy loss for the noise classifier, Eq. (6), is as follows. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C D L}=\\mathbb{E}_{q(\\pmb{x},\\pmb{y})}\\left[\\mathrm{softplus}(y(\\log p_{\\zeta}(\\pmb{x})-\\log p(\\pmb{x})))\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We can estimate both densities directly from our denoising model using Eq. (3), with the constants canceling out in the process. This loss differs significantly from the standard diffusion loss. Intuitively, to distinguish between a sample from the data distribution, $p(x)$ , versus a noisy version of the data distribution, $p_{\\zeta}(x)$ , we need to evaluate denoisers on points from both distributions. In standard diffusion training, denoisers at noise level $\\zeta$ are only trained on samples from $p_{\\zeta}(x)$ . ", "page_idx": 3}, {"type": "text", "text": "Limitations: We highlight that CDL is more expensive to compute than the standard diffusion loss, significantly increasing the total cost of diffusion model training. Implementation details appear in App. B.4 and training cost details appear in App. B.5. ", "page_idx": 3}, {"type": "text", "text": "Choice of noise to contrast Next, let\u2019s break the Log-Likelihood Ratio (LLR) term in Eq. (7) down to see how to choose $\\zeta$ to maximize the benefit of CDL. Combining Eq. (2) and Eq. (3) we have Eq. (8), where the constant cancels out. ", "page_idx": 3}, {"type": "equation", "text": "$$\nL L R=\\log p_{\\zeta}({\\bf x})-\\log p({\\bf x})=\\int_{-\\infty}^{\\infty}d\\alpha\\;\\mathbb{E}_{p({\\epsilon})}[\\|\\epsilon-\\hat{\\epsilon}(z,\\alpha)\\|_{2}^{2}]-\\mathbb{E}_{p({\\epsilon})}[\\|\\epsilon-b\\hat{\\epsilon}(z,\\beta)\\|_{2}^{2}]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that the input $\\textbf{\\em x}$ to the LLR term may come from two different distributions, which breaks the standard synchronous denoising pair $(\\boldsymbol{x}_{\\alpha},\\alpha)$ into asynchronous. When it\u2019s from data distribution $x\\sim p(x)$ , $z=z_{\\alpha}$ ; and when it\u2019s from some noisy data distribution ${\\pmb x}\\sim p_{\\zeta}({\\pmb x})$ , $z=z_{\\beta}$ . ", "page_idx": 3}, {"type": "text", "text": "From Eq. (8) we see that $\\hat{\\epsilon}(\\cdot,\\cdot)$ is trained on four pairs: $(z_{\\alpha},\\alpha)$ , $(z_{\\beta},\\beta)$ , $(z_{\\beta},\\alpha)$ and $(z_{\\alpha},\\beta)$ , where $\\beta\\equiv\\sigma^{-1}(\\sigma(\\alpha)\\sigma(\\zeta))<\\operatorname*{min}(\\alpha,\\zeta)$ (Eq. 5). During standard training, only the first two pairs are trained (Eq. 1). This means that our CDL objective trains the denoiser to perform correctly even for samples from distributions that are noisier or cleaner than the specified noise level (a pair like $(z_{\\beta},\\alpha)$ or $(z_{\\alpha},\\beta))$ . This can be useful for both sequential and parallel sampling settings. During sequential sampling, extra error noise added due to discretization errors can be corrected by the denoiser trained with CDL. As for parallel sampling, CDL helps with evaluations on asymmetric pairs $(z_{\\beta},\\alpha)$ or $(z_{\\alpha},\\beta)$ which we refer to OOD regions for standard diffusion loss. ", "page_idx": 3}, {"type": "text", "text": "In practice, diffusion training pipelines are highly tuned on popular datasets like CIFAR10 and ImageNet, so the amplitude of discretization errors during sampling is small, meaning that errors won\u2019t nudge points too far away from the true trajectory. Therefore, when evaluating CDL objective, we sample some large-valued $\\zeta\\mathbf{s}$ , which corresponds to classifying only small differences in noise levels. Empirically we find that $\\zeta\\sim U n i f o r m[6,15]$ or $\\zeta\\sim l o g i s t i c[6,15]$ performed equally good. ", "page_idx": 3}, {"type": "text", "text": "Denoising, sampling dynamics, and the score connection We have focused so far on denoising and density estimation, but we now want to connect this discussion to the primary use case for diffusion models and the focus of Sec. 4, sampling. There are many choices in how to implement sampling dynamics [11], but all of them rely on the score function, $\\nabla_{x}\\log{p_{\\alpha}(\\pmb{x})}$ . The score function points toward regions of space with high likelihood, and by slowly transitioning (or annealing), from the score function of a noisy distribution to one closer to the data distribution, we can build reliable sampling dynamics. To connect denoisers with sampling we must show that a denoising function, $\\hat{\\epsilon}$ , that is optimal according to Eq. 1 also specifies the score function. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{x}\\log{p_{\\alpha}(\\pmb{x})}=-\\frac{\\hat{\\epsilon}(\\pmb{x},\\alpha)}{\\sqrt{\\sigma(-\\alpha)}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The derivation is straightforward and is given in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "4 Sequential and Parallel Sampling with Diffusion Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Sampling dynamics are typically presented in terms of a stochastic process $\\{x_{t}\\}_{t=1}^{T}$ with timestep, $t$ , rather than in terms of log SNR, $\\alpha$ . We will denote ${\\pmb x}_{t}\\equiv{\\pmb x}_{\\alpha(t)},p_{t}({\\pmb x})\\equiv\\dot{p}_{\\alpha(t)}({\\pmb x})$ , to connect to our previous notation, with $\\alpha(t)$ representing a monotonic relationship described in App. B.3. Note that decreasing log-SNR $\\alpha$ corresponds to increase timestep $t$ , since smaller log-SNR means there is more noise added to the data. ", "page_idx": 3}, {"type": "text", "text": "The general form of sampling dynamics is a process of slowly transitioning from samples of a simple and tractable distribution to the target distribution. Specifically, start with an isotropic Gaussian $\\pmb{x}_{T}\\sim\\mathcal{N}(0,\\mathbb{I})$ , the sampler steps through a series of intermediate distributions with noise levels $\\{T,T-1,\\ldots,1\\}$ following the score estimates. Many works [34, 11] interpret diffusion models as stochastic differential equations (SDEs). The forward process is in the form of ", "page_idx": 4}, {"type": "equation", "text": "$$\nd\\pmb{x}_{t}=\\underbrace{f(\\pmb{x}_{t},t)}_{\\mathrm{drift~}s}d t+\\underbrace{g(t)}_{\\mathrm{diffusion}}d\\pmb{w}_{t},\\qquad\\pmb{x}_{0}\\sim p(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{w}_{t}$ is the standard Wiener process/Brownian motion, $f$ and $g$ are drift coefficient and diffusion coefficient of $\\pmb{x}_{t}$ separately. The reverse process of Eq. 10 is then used to generate samples ", "page_idx": 4}, {"type": "equation", "text": "$$\nd x_{t}=\\underbrace{\\left(f(x_{t},t)x-g^{2}(t)\\nabla_{x}\\log p_{t}(x)\\right)}_{\\mathrm{drift~}s}d t+\\underbrace{g(t)}_{\\mathrm{diffusion}}d w_{t},\\qquad x_{T}\\sim p(x)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Depending on choices for $f,g$ , we can get either a stochastic or ordinary (deterministic) differential equation. Either way, numerical differential equation solvers are used to approximate the true dynamics. The solver introduces discretization errors at each step, causing the trajectory to deviate into the OOD region where the score (or denoiser) is poorly estimated, further compounding the errors. More discretization steps reduce accumulated error and leads to better sample quality, at the expense of more sequential computation. As a result, a significant limitation of diffusion models is that they require many iterations to produce high quality samples. ", "page_idx": 4}, {"type": "text", "text": "Sequential Sampling The influential diffusion sampler DDPM [6] iterates over thousands of discretization steps in simulating the dynamics. Recently, many sequential sampling methods have been developed to take fewer and larger steps while introducing less error [30, 22, 11]. Specifically, Karras et al. 2022 studies the curvature shape of SDE/ODE trajectory and suggests a discretization technique where the resulting tangent of the solution trajectory always points towards the denoiser output. However, speeding up the sequential sampling sacrifices generation quality. Furthermore, the SOTA sequential samplers [18, 11, 30] require hyperparameter tuning and grid search on specific datasets, which poses challenges to the generalization of these samplers to other datasets. ", "page_idx": 4}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/64d992581548f0a8ca02044a239ef1596ead9d1a82da39c983d4dd54bcf689de.jpg", "img_caption": ["Figure 2: The computation graph of Picard iteration for parallel sampling [28] "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Parallel Sampling Shih et al. 2024 explores a parallel sampling scheme, where the entire reverse process path is randomly initialized and then all steps in the path are updated in parallel. Parallel sampling is based on the method of Picard iteration, an old technique for solving ODEs through fixed-point iteration. An ODE is defined by a drift function $s(\\pmb{x},t)$ and initial value $\\pmb{x}_{0}$ . In the integral form, the value at time $t$ can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}=\\pmb{x}_{0}+\\int_{0}^{t}s(\\pmb{x}_{u},u)\\,d u\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In other words, the value at time $t$ must be initial value plus the integral of the derivative along the path of the solution. This formula suggests a natural way of solving the ODE by starting with a guess of the solution $\\{\\pmb{x}_{t}^{k+1}:0\\leq t\\leq1\\}$ at initial iteration $k=0$ , and iteratively refining by updating the value at every time $t$ until convergence 2 ", "page_idx": 4}, {"type": "text", "text": "(Picard Iteration) ", "text_level": 1, "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{k+1}=\\pmb{x}_{0}^{k}+\\int_{0}^{t}s(\\pmb{x}_{u}^{k},u)\\,d u\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To perform Picard iterations numerically, which is shown in Fig. 2, we can write the discretized form of Eq. 12 with step size $1/T$ , for $t\\in[0,T]$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{k+1}=\\pmb{x}_{0}^{k}+\\frac{1}{T}\\sum_{i=0}^{t-1}s(\\pmb{x}_{i}^{k},i/T)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We see that the expensive computations $\\{s(\\pmb{x}_{i}^{k},i/T):i\\in[0,T)\\}$ can be performed in parallel. After some number of Picard iterations, the error difference between two iterates \u2225xtk+1\u2212xtk \u222522 drops below some convergence threshold. This converged trajectory, $\\pmb{x}_{t}^{*}$ , should be close to the sequential sampler trajectory. Looking at the example in Fig. 1, we show the trajectories of three iterations $k=1,2,3$ . The trajectories before convergence are consistently appearing in the regions with high score error. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We sample from models fine-tuned on Contrastive Diffusion Loss (CDL) via both parallel and sequential diffusion samplers across a variety of generation tasks, including complex low-dimensional manifold 2D synthetic data and real-world image generation. Our results demonstrate that employing CDL as a regularizer in models trained with standard diffusion loss enhances density estimation and sample quality while also accelerating convergence in parallel sampling. All sampling tests are done on A6000 GPUs. We visualize CDL image generation examples in App. C. ", "page_idx": 5}, {"type": "text", "text": "Training configuration Our method is architecture-agnostic. In synthetic experiments, we adopt a simple MLP architecture with positional encoding for timesteps 3, as it is one of the most versatile models in the literature. In real-world experiments, we consider the standard diffusion loss with two training configurations: DDPM by Ho et al. 2020 and EDM by Karras et al. 2022. For more details on model training, data split, and hyper-parameters, please refer to App. B.2. ", "page_idx": 5}, {"type": "text", "text": "Generation quality metrics For real-world data, our intrinsic metric is Frec\u00b4het Inception Distance (FID) score [5]. The number of images we generated for FID computation follows their baseline models\u2019 FID settings, and the FID scores are computed between 5, 0000 generated images and all available real images. ", "page_idx": 5}, {"type": "text", "text": "For synthetic data, to measure how well the generated samples resemble samples from the ground truth distribution, we use the (unbiased) kernel estimator of the squared Maximum Mean Discrepancy (MMD), with Gaussian kernel with bandwidth set empirically as described in App. B.1. ", "page_idx": 5}, {"type": "text", "text": "Sampling speed metrics We adopt the following three metrics: (1) Neural function evaluations (NFE) for all settings, i.e. how many times the denoiser is evaluated to produce a sample; (2) For the parallel setting, we report the number of parallel Picard Iterations; (3) Furthermore for the parallel setting, the wall-clock time is reported. While parallel sampling can use fewer total iterations and less wall-clock time than a sequential sampler, this may come at the cost of an increase in the total number of function evaluations. This gap is called the algorithm inefficiency. In the subsequent section, we use contrastive diffusion loss as a training regularizer for standard diffusion losses and refer to the corresponding models as CDL-regularized models. ", "page_idx": 5}, {"type": "text", "text": "5.1 Parallel Sampling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the parallel setting, we use Parallel DDPM sampler [28] with 1000-step diffusion sampling. And for synthetic experiment, to reflect sampling speed by only number of Picard iteration and wall-clock time, we don\u2019t use the sliding window technique, and the 2D data is small enough to fit the whole sampler trajectory in GPU memory. While for real-world experiment, sliding window is still applied. ", "page_idx": 5}, {"type": "text", "text": "Synthetic Dataset We consider the 2D Dino dataset [19], characterized by its highly nonlinear density concentrated on a low-dimensional manifold. For baselines, we employ the standard DDPM loss [6], as all standard diffusion losses similarly minimize a sum of MSE losses between the actual and estimated denoisers. Both CDL-regularized and DDPM-objective-trained models are trained with a MLP where timestep is encoded by positional encoding. We train it for 2000 epochs to ensure convergence and check the training and validation loss curve to avoid overfitting. ", "page_idx": 5}, {"type": "text", "text": "In Fig. 4, it is clearly demonstrated that the parallel generated samples from the CDL-regularized model is much better than the model trained only with the standard DDPM loss, especially around the chin, eyes and paws, where the manifolds are close to each other and difficult to distinguish and learn. From the MMD plot, we see that comparing to the baseline curve trained only with standard DDPM loss, the CDL-regularized curve converges faster with smaller number of Picard iterations and better sample quality (lower MMD scores). Table. 1 shows the sampling speed results, from where we see that CDL-regularized model converges faster with lower final MMD and better sample quality. ", "page_idx": 5}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/0ec75a7aa98b8c8f2fc4d773b8a7d2fdb77e23360e8aaa3674a422506e1efe09.jpg", "img_caption": ["Table 1: Parallel DDPM sampling speed results. We generate 2, 000 samples.Figure 5: Parallel sampling MMD plot. We can see that the iterations till MMD threshold. Both NFE and Time are counted till parallel CDL-regularized model and DDPM model converge themselves at 35 and 36 Picard iterations, separately. convergence. ", "Table 2: Evaluating FID score (lower is better) of parallel DDPM sampler on real-world datasets using 5, 0000 samples. For reported FID scores, we run three sets of random seeds and reported the average with uncertainty. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Real-world Datasets We select $\\cdot\\mathrm{DDPM}++$ cont. (VP)\u201d and $\\mathrm{\\cdot}\\mathrm{\\Phi}^{\\mathrm{sucsh}++}$ cont. (VE)\u201d models by [11] trained on CIFAR-10 at $32\\times32$ , unconditional FFHQ, and unconditional AFHQv2 [15, 10, 3] as baselines, comparing to the corresponding CDL-regularized models. We adopt the pre-trained models from Ho et al. $2020^{\\bar{4}}$ and Karras et al. $2\\bar{0}22^{5}$ . More experimental results can be found in App. B.2. As shown in Tab. 2, CDL-regularized models always outperformed baselines with respect to FID scores. ", "page_idx": 6}, {"type": "table", "img_path": "RE7wPI4vfT/tmp/4f77ef03176fd48ecf6b14c702b40f2f527a316a6b8978f23c023f4d125eba78.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Sequential Sampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While CDL clearly improves parallel sampling quality and convergence speed, we also show that it improves the trade-off between generation speed and sample quality in the sequential setting. As for sequential diffusion sampling choices, we consider the DDPM sampler from Ho et al. 2020, and both the deterministic and stochastic samplers from Karras et al. 2022. To ensure fair comparisons, we adopt the original sampling hyper-parameter settings for all baselines. ", "page_idx": 6}, {"type": "text", "text": "Deterministic samplers For FID test, we follow the exact sampling settings outlined in Karras et al. 2022 for each dataset. FID scores are reported in Tab. 3, for sequential deterministic EDM samplers, CDL objective ensures that the generation quality is consistently similar or better. ", "page_idx": 6}, {"type": "text", "text": "In principle, increasing NFE has the potential to decrease the overall discretization errors, consequently leading to improved sample quality. However, in practice we observed an unusual behavior6 with the Karras deterministic sampler \u2013 as NFE increases, the FID score deteriorates (Fig. 6). In contrast to EDM models, CDL-regularized models exhibit a more stable FID score. This partially resolves the deterministic sampler sensitivity while improving the quality. ", "page_idx": 6}, {"type": "text", "text": "Stochastic samplers In practice, stochastic samplers often yield superior performance compared to deterministic ones. However this is not true in Karras et al. 2022: stochastic samplers outperform deterministic ones only on challenging datasets, for simpler datasets, the introduction of stochasticity not only fails to enhance performance but exhibits image degradation issues, characterized by a loss of detail. They attribute this phenomenon to L2-trained denoisers excessively removing noise at each step (always remove more than it should), and propose to slightly increase the standard deviation $\\bar{(S_{\\mathrm{noise}})}\\,^{7}$ of newly added noise to 1.007. We argue that this approach may not totally resolve the issue and instead complicates the hyperparameter grid search process by introducing an additional parameter, $S_{\\mathrm{noise}}$ . Also, this $S_{\\mathrm{noise}}$ logically serves the same function as another hyperparameter $\\gamma_{i}$ , where both of them control the amount of noise to add to reach a higher noise level. ", "page_idx": 6}, {"type": "table", "img_path": "RE7wPI4vfT/tmp/989c0c6721c5b5f88e83fa0216c1e0812f0698c4f21edc89d259cc0904d66dc3.jpg", "table_caption": ["Table 3: Evaluating sequential deterministic EDM samplers generation quality. For reported FID scores, we run three sets of random seeds and reported the average with uncertainty. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/5bc02be38543867eac2c7aefd217d0ac37e74147494b41fd467a5038063da6cd.jpg", "img_caption": ["Figure 6: The FID comparison between our CDL and the baseline EDM in the deterministic sampler experiment. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In this experiment, we conducted two stochastic sampling configurations for our baseline EDMtrained models. The first configuration, referred to as EDM-opt, operated at the EDM optimal setting with $S_{\\mathrm{noise}}=1.007$ . The second, named as EDM-sub-opt, used a setting with $S_{\\mathrm{noise}}=1.00$ , effectively disabling $S_{\\mathrm{noise}}$ . As for CDL configuration, we exclusively examined the scenario with $S_{\\mathrm{noise}}=1.00$ to determine whether CDL could address the problem of excessive noise removal. ", "page_idx": 7}, {"type": "text", "text": "The results, as visualized in Figure 7, indicate that CDL outperforms EDM in both $S_{\\mathrm{noise}}$ configurations. Notably, CDL not only improves upon the EDM-sub-opt configuration (dark blue line) but also surpasses the performance of the EDM-opt configuration (light blue line), even at its optimal setting. This not only demonstrates that CDL robustly provides a better sample quality, but also suggests that CDL can eliminate the need for the hyperparameter $S_{\\mathrm{noise}}$ . This reduction enables a more efficient grid search for the optimal EDM sampling settings, potentially enhancing the practicality of using such a sampler for other applications. ", "page_idx": 7}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The generative modeling trilemma [39] seeks generative models that produce samples (i) quickly, (ii) with high quality, and (iii) good mode coverage. Diffusion models excel at the latter two but a large amount of research has attempted to address the problem of slow sampling speed. From the inception of diffusion models [29], dynamics has been at the forefront, so most work has focused on the interpretation of the dynamics as a differential equation that can be sped up through accelerated numerical solvers [30, 34, 42, 9, 17]. Our approach is compatible with any of these approaches as we are sampler agnostic, seeking only to improve the input to the sampler, which is the denoiser or score function estimator, through regularization during the diffusion model training. A separate line of work instead attempts to distill a diffusion model into a faster model that achieves similar sampling quality [27, 35, 21, 38, 39]. In principle, these methods could also benefit from distilling based on a more robust base diffusion model trained with CDL. ", "page_idx": 7}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/4b7e4ce1b98015cca1b6e822185b8ad66ce1400f74a3a173a99eded20e081a68.jpg", "img_caption": ["Figure 7: The FID comparison between our CDL and the baselines EDM in the stochastic sampler experiment on CIFAR-10. CDL\u2019s performance is strictly better for all $S_{\\mathrm{churn}}$ , outperforming the optimal setting of EDM which inflates the standard deviation $S_{\\mathrm{churn}}$ of the newly added noise. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Diffusion models admit a surprisingly diverse array of mathematical perspectives, like variational perspectives [6, 7, 12], differential equations [34, 20], and nonequilibrium thermodynamics [29]. Our approach is mostly inspired by connections between the information-theoretic perspective [13, 14] and the score matching perspective [31, 33, 32, 8, 37]. In particular, we point out that score function estimates in traditional diffusion training are sub-optimal, and the information-theoretic perspective leads to a new objective (CDL) that can improve the score estimate. ", "page_idx": 8}, {"type": "text", "text": "While previous diffusion models focus on log-likelihood estimation, we consider a different approach based on density ratio estimation and noise contrastive estimation [4, 23], which inspired several notable developments in machine learning [1, 25]. A few works have considered contrastive learning inspired modifications to diffusion either to enforce multimodal data relationships [16, 43], for style transfer [41], or for guidance during generation [24], but none use the diffusion model as a noise classifier to improve diffusion training as we do. Most similar to our approach are methods that use Density Ratio Estimation (DRE) to estimate a ratio between the data density and some simple noise distribution. The density ratio can be estimated by learning to contrast between data samples and samples from the noisy distribution [4, 36]. Recent work generalized the idea to consider classifying between samples along a sequence of distributions between source and target [26, 2]. Our contribution is to relate this approach to diffusion models by noting that standard diffusion models implicitly already implement the required classifiers for distinguishing distributions on the path from the data distribution to a Gaussian distribution. Concurrent work makes a similar connection but while we focus on improving diffusion models by interpreting them as noise classifiers, [40] focused on the converse perspective, improving density ratio estimation by interpreting DREs as denoisers. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we introduced a novel connection between diffusion models and optimal noise classifiers. While this relationship has a variety of potential applications that could be explored in future work, we used the connection to propose a new self-supervised loss regularizer for diffusion models, the Contrastive Diffusion Loss (CDL). CDL reduces the error of the learned denoiser in regions that are OOD for the standard loss. We showed that CDL improves the robustness of diffusion models across all types of sampling dynamics, and leads to significant speed-ups for a promising new generation of parallel samplers. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially supported by the National Science Foundation under grant no. IIS 1901379. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts: This paper aims to innovate on the methodology of diffusion models. We anticipate no direct potential societal consequences of our work, as the main focus of this work is in theory and algorithm design. However, it is important to acknowledge the potential risks associated with diffusion models, as misuse can contribute to the spread of disinformation and deepfakes. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.   \n[2] Kristy Choi, Chenlin Meng, Yang Song, and Stefano Ermon. Density ratio estimation via infinitesimal classification. arXiv preprint arXiv:2111.11010, 2021.   \n[3] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8188\u20138197, 2020.   \n[4] Michael Gutmann and Aapo Hyv\u00e4rinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 297\u2013304. JMLR Workshop and Conference Proceedings, 2010.   \n[5] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[7] Chin-Wei Huang, Jae Hyun Lim, and Aaron C Courville. A variational perspective on diffusion-based generative models and score matching. Advances in Neural Information Processing Systems, 34:22863\u2013 22876, 2021.   \n[8] Aapo Hyv\u00e4rinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[9] Alexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Pich\u00e9-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021.   \n[10] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[11] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022.   \n[12] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. arXiv preprint arXiv:2107.00630, 2021.   \n[13] Xianghao Kong, Rob Brekelmans, and Greg Ver Steeg. Information-theoretic diffusion. In International Conference on Learning Representations, 2023. URL https://arxiv.org/abs/2302.03792.   \n[14] Xianghao Kong, Ollie Liu, Han Li, Dani Yogatama, and Greg Ver Steeg. Interpretable diffusion via information decomposition. In The Twelfth International Conference on Learning Representations, 2024.   \n[15] Alex Krizhevsky. Learning multiple layers of features from tiny images. pages 32\u201333, 2009. URL https://www.cs.toronto.edu/\\~kriz/learning-features-2009-TR.pdf.   \n[16] Chaejeong Lee, Jayoung Kim, and Noseong Park. Codi: Co-evolving contrastive diffusion models for mixed-type tabular synthesis. arXiv preprint arXiv:2304.12654, 2023.   \n[17] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022.   \n[18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.   \n[19] Justin Matejka and George Fitzmaurice. Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing. In Proceedings of the 2017 CHI conference on human factors in computing systems, pages 1290\u20131294, 2017.   \n[20] David McAllester. On the mathematics of diffusion models. arXiv preprint arXiv:2301.11108, 2023.   \n[21] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297\u201314306, 2023.   \n[22] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.   \n[23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[24] Yidong Ouyang, Liyan Xie, and Guang Cheng. Improving adversarial robustness by contrastive guided diffusion process. arXiv preprint arXiv:2210.09643, 2022.   \n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[26] Benjamin Rhodes, Kai Xu, and Michael U Gutmann. Telescoping density-ratio estimation. Advances in neural information processing systems, 33:4905\u20134916, 2020.   \n[27] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021.   \n[28] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.   \n[30] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[31] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.   \n[32] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020.   \n[33] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. arXiv preprint arXiv:1905.07088, 2019.   \n[34] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[35] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \n[36] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Bibliography, page 309\u2013326. Cambridge University Press, 2012.   \n[37] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[38] Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality.   \n[39] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. In International Conference on Learning Representations, 2021.   \n[40] Shahar Yadin, Noam Elata, and Tomer Michaeli. Classification diffusion models: Revitalizing density ratio estimation. 2024.   \n[41] Serin Yang, Hyunmin Hwang, and Jong Chul Ye. Zero-shot contrastive loss for text-guided diffusion image style transfer. arXiv preprint arXiv:2303.08622, 2023.   \n[42] Qinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. Advances in Neural Information Processing Systems, 34:16280\u201316291, 2021.   \n[43] Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, and Yan Yan. Discrete contrastive diffusion for cross-modal and conditional generation. arXiv preprint arXiv:2206.07771, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Derivations and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Score Connection ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We derive the following relation. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla_{x}\\log{p_{\\alpha}(\\pmb{x})}=-\\frac{\\hat{\\epsilon}(\\pmb{x},\\alpha)}{\\sqrt{\\sigma(-\\alpha)}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "To keep track of intermediate random variables and their associated distributions, we will use the cumbersome but more precise information theory notation where capitals represent a random variable and lowercase represents values. Define the channel that mixes the signal, $X$ , with Gaussian noise as $Z_{\\alpha}\\equiv\\sqrt{\\sigma(\\alpha)}X^{'}+\\sqrt{\\sigma(-\\alpha)}\\mathcal{E}$ with $\\mathscr{E}\\sim\\mathcal{N}(0,\\mathbb{I})$ and data distribution $p(X)$ , $\\alpha$ represents the log of the Signal-to-Noise Ratio (SNR), and $\\sigma(\\cdot)$ is the sigmoid function. In this notation, the probability that a mixture distribution takes a value, $\\textbf{\\em x}$ , would be written $p(Z_{\\alpha}={\\pmb x})$ . ", "page_idx": 12}, {"type": "text", "text": "We start by re-writing the left-hand side in new notation, and expand the definition based on the noisy channel model, using $\\delta(\\cdot)$ for the Dirac delta. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{\\tau}\\log(Z_{n}=\\tau)=1/\\mu(Z_{n}=\\tau)\\nabla_{\\tau}f_{n}(\\alpha=\\alpha)}\\\\ &{\\qquad=1/\\mu(Z_{n}=\\alpha)\\nabla_{\\tau}\\int\\mathrm{d}\\alpha\\,\\mathrm{d}\\tau-\\sqrt{\\sigma(\\alpha)}(\\boldsymbol{\\mu})\\boldsymbol{g}-\\sqrt{\\sigma(-\\alpha)}\\boldsymbol{\\epsilon})\\boldsymbol{\\gamma}(X=\\boldsymbol{\\mu})\\boldsymbol{\\gamma}(e=\\alpha)}\\\\ &{\\qquad=1/\\mu(Z_{n}=\\alpha)\\int\\mathrm{d}\\alpha\\,\\mathrm{d}\\tau\\,(\\boldsymbol{\\nu})\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}\\,[\\sigma(\\boldsymbol{\\nu})\\boldsymbol{\\epsilon}-\\sqrt{\\sigma(\\alpha)}\\boldsymbol{\\epsilon}]\\boldsymbol{\\gamma}(X=\\boldsymbol{\\mu})\\boldsymbol{\\gamma}(e=\\alpha)}\\\\ &{\\qquad=1/\\mu(Z_{n}=\\alpha)\\int\\mathrm{d}\\alpha\\,\\mathrm{d}(\\tau-\\tau)\\sqrt{\\sigma(-\\alpha)}\\nabla_{\\tau}(\\alpha)\\boldsymbol{\\epsilon}-\\sqrt{\\sigma(-\\alpha)}\\boldsymbol{\\epsilon})\\boldsymbol{\\gamma}(e=\\alpha)}\\\\ &{\\qquad=1/\\mu(Z_{n}=\\alpha)\\frac{1}{\\sqrt{\\sigma(-\\alpha)}}\\int\\mathrm{d}\\alpha\\,\\boldsymbol{\\epsilon}\\,(\\boldsymbol{\\nu}-\\sqrt{\\sigma(\\alpha)})\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}\\,\\sqrt{\\sigma(-\\alpha)}\\boldsymbol{\\epsilon})\\boldsymbol{\\gamma}(X=\\boldsymbol{\\mu})\\boldsymbol{\\gamma}(X=\\boldsymbol{\\nu})\\nabla_{\\tau}\\int\\mathrm{d}\\alpha}\\\\ &{\\qquad=-1/\\mu(Z_{n}=\\alpha)\\frac{1}{\\sqrt{\\sigma(-\\alpha)}}\\int\\mathrm{d}\\alpha\\,\\boldsymbol{\\epsilon}\\,\\delta(\\boldsymbol{\\nu}-\\sqrt{\\sigma(\\alpha)})\\boldsymbol{\\epsilon}-\\sqrt{\\sigma(-\\alpha)}\\boldsymbol{\\epsilon})\\boldsymbol{\\gamma}(X=\\boldsymbol{\\mu})\\boldsymbol{\\gamma}(X=\\boldsymbol{\\nu})\\nabla_{\\tau}\\quad}\\\\ &{\\qquad=-1/\\mu(Z_{n}=\\alpha)\\frac{1}{\\sqrt{\\sigma(-\\alpha)}}\\int\\mathrm{d}\\alpha\\,\\boldsymbol{\\epsilon}\\,\\delta(Z_{n}=\\alpha,\\boldsymbol{\\epsilon})}\\\\ &{\\qquad=-\\frac{1}{\\sqrt{\\sigma(-\\alpha)}}\\int\\mathrm{d}\\alpha\\,\\boldsymbol{\\nu}\\,\\boldsymbol{\\mu}-\\boldsymbol{\\epsilon}\\int\\mathrm{d}\\alpha\\,\\boldsymbol{\\epsilon}\\int\\mathrm{d}\\alpha}\\\\ &{\\qquad=-\\frac{1}{\\sqrt{\\sigma(-\\alpha)}}\\int\\mathrm{d}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In the second line we expand, and in the third we just move the gradient inside the integral. In the fourth line we use the chain rule to relate the gradient over $\\textbf{\\em x}$ to the gradient over $\\epsilon$ (introducing a sign flip). In the fifth line we use integration by parts to move the gradient (second sign flip). Taking the gradient of the Gaussian in the sixth line gives our third sign flip, and the factor of $\\epsilon$ . We can conclude by writing the expression in terms of a conditional distribution, and relating that to the optimal denoiser in Eq. 1. ", "page_idx": 12}, {"type": "text", "text": "A.2 Mixture Distribution Density ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we derive the expression that shows that the density of a continuum of Gaussian mixture distributions can be written in terms of the optimal denoiser, $\\hat{\\epsilon}$ , for the data distribution. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{-\\log p_{\\zeta}({\\pmb x})=c+1/2\\displaystyle\\int_{-\\infty}^{\\infty}d\\bar{\\alpha}\\,\\mathbb{E}_{p({\\pmb\\epsilon})}[\\|{\\pmb\\epsilon}-\\sqrt{\\frac{\\sigma(-\\bar{\\alpha})}{\\sigma(-\\beta)}}\\hat{\\pmb\\epsilon}(\\sqrt{\\sigma(\\bar{\\alpha})}{\\pmb x}+\\sqrt{\\sigma(-\\bar{\\alpha})}{\\pmb\\epsilon},\\beta)\\|_{2}^{2}]}}\\\\ {{\\beta\\equiv\\sigma^{-1}\\left(\\sigma(\\zeta)\\sigma(\\bar{\\alpha})\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "As in the previous section, we will adopt information theory notation. If we define the optimal denoiser for the input distribution, $p_{\\zeta}(x)$ , with a subscript as $\\hat{\\epsilon}_{\\zeta}(\\cdot,\\cdot)$ , we can write the density analogously to Eq. 2. ", "page_idx": 13}, {"type": "equation", "text": "$$\n-\\log p_{\\zeta}(x)=c+1/2\\int_{-\\infty}^{\\infty}d\\bar{\\alpha}\\,\\mathbb{E}_{p(\\epsilon)}[\\|\\epsilon-\\hat{\\epsilon}_{\\zeta}(\\sqrt{\\sigma(\\bar{\\alpha})}x+\\sqrt{\\sigma(-\\bar{\\alpha})}\\epsilon,\\bar{\\alpha})\\|_{2}^{2}]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that we now have to keep track of two log SNR values. One indicates how much noise is added to the new \u201cdata\u201d distribution, the other is how much noise we add and then try to remove with our denoiser. The goal is to relate $\\hat{\\epsilon}_{\\zeta}$ to $\\hat{\\epsilon}$ . We can formally write down the optimal solution using the relation in Eq. 1. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\zeta}(\\pmb{x},\\bar{\\alpha})=\\mathbb{E}_{\\epsilon\\sim p(\\mathcal{E}|Z=\\pmb{x})}[\\pmb{\\epsilon}]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now, however, the noise channel is defined differently. The channel mixes the signal, $\\bar{\\pmb{x}}\\sim p_{\\zeta}(\\bar{X})$ , with Gaussian noise, $\\bar{\\pmb{\\epsilon}}\\,\\sim\\,\\mathcal{N}(0,\\mathbb{I})$ , as $Z\\;\\equiv\\;\\sqrt{\\sigma(\\bar{\\alpha})}\\bar{X}\\,+\\,\\sqrt{\\sigma(-\\bar{\\alpha})}\\bar{\\mathcal{E}}\\:$ . And the noisy variable, $\\bar{X}=\\sqrt{\\sigma(\\zeta)}X+\\sqrt{\\sigma(-\\zeta)}\\mathcal{E}$ , where we must be careful to distinguish the two independent sources of Gaussian noise. ", "page_idx": 13}, {"type": "text", "text": "We start by expanding definitions. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle c(x_{\\zeta},\\bar{\\alpha})=\\displaystyle\\frac{1}{p(Z=x_{\\zeta})}\\int d\\bar{\\epsilon}\\ p(\\bar{\\epsilon}=\\bar{\\epsilon},Z=x_{\\zeta})\\ \\bar{\\epsilon}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\frac{1}{p(Z=x_{\\zeta})}\\int d\\bar{\\epsilon}d\\bar{x}\\ \\delta(x_{\\zeta}-\\sqrt{\\sigma(\\bar{\\alpha})}\\bar{x}-\\sqrt{\\sigma(-\\bar{\\alpha})}\\bar{\\epsilon})\\ p(\\bar{X}=\\bar{x})\\ p(\\bar{\\bar{\\epsilon}}=\\bar{\\epsilon})\\ \\bar{\\epsilon}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\frac{1}{p(Z=x_{\\zeta})}\\int d\\bar{\\epsilon}d\\bar{x}d x d\\ \\delta(x_{\\zeta}-\\sqrt{\\sigma(\\bar{\\alpha})}\\bar{x}-\\sqrt{\\sigma(-\\bar{\\alpha})}\\bar{\\epsilon})\\ \\delta(\\bar{x}-\\sqrt{\\sigma(\\zeta)}x-\\sqrt{\\sigma(-\\zeta)}}\\\\ &{\\quad\\quad\\quad\\quad\\cdot p(X=x)\\ p(\\bar{\\xi}=\\epsilon)\\ \\bar{\\varrho}(\\bar{\\xi}=\\bar{\\epsilon})\\ \\bar{\\epsilon}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\frac{1}{p(Z=x_{\\zeta})}\\int d\\bar{\\epsilon}d\\bar{\\epsilon}d x\\ \\delta(x_{\\zeta}-\\sqrt{\\sigma(\\bar{\\alpha})}(\\sqrt{\\sigma(\\zeta)}x+\\sqrt{\\sigma(-\\zeta)}\\epsilon)-\\sqrt{\\sigma(-\\bar{\\alpha})}\\bar{\\epsilon})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\cdot p(X=x)\\ p(\\bar{\\xi}=\\epsilon)\\ \\bar{\\epsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now do a change of variables, a 2-d rotation with: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bar{\\epsilon}^{\\prime}=a\\bar{\\epsilon}+b\\epsilon,\\epsilon^{\\prime}=-b\\bar{\\epsilon}+a\\epsilon,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\na=\\sqrt{\\sigma(\\bar{\\alpha})\\sigma(-\\zeta)/(1-\\sigma(\\zeta)\\sigma(\\bar{\\alpha}))},b=\\sqrt{\\sigma(-\\bar{\\alpha})/(1-\\sigma(\\zeta)\\sigma(\\bar{\\alpha}))}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This change of variables leads to the following. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\dot{\\alpha}}_{\\alpha}({\\pmb x}_{\\zeta},\\bar{\\alpha})=\\frac{1}{p(Z={\\pmb x}_{\\zeta})}\\displaystyle\\int d\\epsilon^{\\prime}d\\bar{\\epsilon}^{\\prime}d x~\\delta({\\pmb x}_{\\zeta}-\\sqrt{\\sigma(\\zeta)}\\sigma(\\bar{\\alpha}){\\pmb x}-\\sqrt{1-\\sigma(\\zeta)}\\sigma(\\bar{\\alpha})\\bar{\\epsilon}^{\\prime})}}\\\\ {{\\qquad\\qquad\\qquad\\cdot p(X={\\pmb x})~p(\\mathcal{E}^{\\prime}=\\epsilon^{\\prime})~p(\\bar{\\mathcal{E}}^{\\prime}=\\bar{\\epsilon}^{\\prime})~(b\\bar{\\epsilon}^{\\prime}+a\\epsilon^{\\prime})}}\\\\ {{\\qquad=b\\frac{1}{p(Z={\\pmb x}_{\\zeta})}\\displaystyle\\int d\\epsilon^{\\prime}d\\bar{\\epsilon}^{\\prime}d x~\\delta({\\pmb x}_{\\zeta}-\\sqrt{\\sigma(\\zeta)}\\sigma(\\bar{\\alpha}){\\pmb x}-\\sqrt{1-\\sigma(\\zeta)}\\sigma(\\bar{\\alpha})\\bar{\\epsilon}^{\\prime})}}\\\\ {{\\qquad\\qquad\\qquad\\cdot p(X={\\pmb x})~p(\\mathcal{E}^{\\prime}=\\epsilon^{\\prime})~p(\\bar{\\mathcal{E}}^{\\prime}=\\bar{\\epsilon}^{\\prime})~\\bar{\\epsilon}^{\\prime}}}\\\\ {{\\qquad=b\\frac{1}{p(Z={\\pmb x}_{\\zeta})}\\displaystyle\\int d\\epsilon^{\\prime}d\\bar{\\epsilon}^{\\prime}d x~\\delta({\\pmb x}_{\\alpha}-\\sqrt{\\sigma(\\beta)}{\\pmb x}-\\sqrt{1-\\sigma(\\beta)}\\bar{\\epsilon}^{\\prime})~p(X={\\pmb x})~p(\\mathcal{E}^{\\prime}=\\epsilon^{\\prime})~p(\\bar{\\mathcal{E}}}}\\\\ {{\\qquad\\vdots}_{\\bar{\\epsilon}}({\\pmb x}_{\\mathcal{G}},\\bar{\\alpha})=b\\bar{\\epsilon}({\\pmb x}_{\\mathcal{G}},\\beta),\\qquad\\beta\\equiv\\sigma^{-1}(\\sigma(\\bar{\\alpha})\\sigma(\\zeta)),b=\\sqrt{\\sigma(-\\bar{\\alpha})/(1-\\sigma(\\zeta)\\sigma(\\bar{\\alpha}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note in the second line that the expectation of $\\epsilon^{\\prime}$ is zero, and we move the constant for the other term, $b$ , outside the integral. In the third line, we define $\\beta$ which represents the log SNR of the two consecutive noisy channels with $\\zeta,{\\bar{\\alpha}}$ . Then we recognize the resulting integral as Eq. 1, the optimal denoiser for recovering samples from from the original (non-noisy) data distribution in Gaussian noise. ", "page_idx": 13}, {"type": "text", "text": "A.3 Main Plot 1D Two-mode Gaussian\u2019s Analytical Solution ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we calculate the analytical solution to the 1D two-mode Gaussian in Fig. 1. ", "page_idx": 13}, {"type": "text", "text": "Fig. 1 is plotting the norm of difference between the ground-truth denoiser $\\hat{\\epsilon}_{g t}(\\cdot,\\cdot)$ and the estimated denoiser $\\hat{\\epsilon}(\\cdot,\\cdot)$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{denoiser\\_err}(\\pmb{x},\\alpha)=\\|\\hat{\\pmb{\\epsilon}}(\\pmb{x},\\alpha)-\\hat{\\pmb{\\epsilon}}_{g t}(\\pmb{x},\\alpha)\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To get this error plot, we need to analytically calculate ground-truth denoiser $\\hat{\\epsilon}_{g t}(x,\\alpha)$ . From score connection Eq. 9, for any intermediate noisy density $\\log p_{\\alpha}(\\pmb{x})$ , denoiser function $\\pmb{\\hat{\\epsilon}}(\\pmb{x},\\alpha)$ can be derived from score function $\\nabla_{x}\\log{p_{\\alpha}(\\pmb{x})}$ . Therefore, ultimately what we need to calculate here is the score function of any noisy distribution $p_{\\alpha}(x)$ . ", "page_idx": 14}, {"type": "text", "text": "The data we used is a mixture of two Gaussians, $\\mathcal{N}(\\mu=-5,\\mathbb{I})$ and $\\mathcal{N}(\\mu=5,\\mathbb{I})$ , and the noise distribution is consists of data plus noise, then the noisy distribution should also be a mixture of Guassians. We just need to relate the parameters of the noisy mixture of Gaussians to the parameters of the mixture of Gaussians. ", "page_idx": 14}, {"type": "text", "text": "Start with one mode of the Gaussian mixture for the data. We could represent it in terms of the standard normal random variable, $\\epsilon$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pmb{x}_{d}=\\mu+\\pmb{\\sigma}\\pmb{\\epsilon}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now call $x_{\\alpha}$ the random variable after applying a noisy channel with log-SNR $\\alpha$ , here we present sigmoid function as $\\sigma(\\cdot)$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pmb{x}_{\\alpha}=\\sqrt{\\sigma(\\alpha)}\\pmb{x}_{d}+\\sqrt{\\sigma(-\\alpha)}\\pmb{\\epsilon}^{\\prime}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that we use a different $\\epsilon^{\\prime}$ here. Now expand this, and then re-arrange. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\pmb x}}_{\\alpha}=\\sqrt{\\sigma(\\alpha)}(\\mu+\\sigma\\pmb{\\epsilon})+\\sqrt{\\sigma(-\\alpha)}{\\pmb\\epsilon}^{\\prime}}\\\\ {~~~~~=\\sqrt{\\sigma(\\alpha)}\\mu+\\sqrt{\\sigma(\\alpha)}\\sigma{\\pmb\\epsilon}+\\sqrt{\\sigma(-\\alpha)}{\\pmb\\epsilon}^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We want to represent this in a more canonical way to see what the variance and mean of this Gaussian is. Note that for two standard normal random variables, $a\\pm b\\epsilon^{\\prime}$ , we can represent them as a single random variable with the same variance, $\\sqrt{a^{2}+b^{2}}\\epsilon^{\\prime\\prime}$ (reparameterization trick). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pmb{x}_{\\alpha}=\\sqrt{\\sigma(\\alpha)}\\mu+\\sqrt{\\sigma(\\alpha)\\sigma+\\sigma(-\\alpha)}\\pmb{\\epsilon}^{\\prime\\prime}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we see that the noisy Gaussian (one component of a mixture) is just a modified version of the original. We have to change the mean (moving it towards zero when adding noise) and the variance. In our example, we set $\\sigma=1$ , so it simplifies further. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\pmb x}_{\\alpha}=\\sqrt{\\sigma(\\alpha)}\\mu+{\\pmb\\epsilon}^{\\prime\\prime}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So the variance doesn\u2019t change, we just slowly shift the two mixtures together to the center. ", "page_idx": 14}, {"type": "text", "text": "Therefore, for one mode $\\pmb{x}\\sim\\mathcal{N}(\\pmb{\\mu},I)$ , the intermediate noisy log-density is. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log p_{\\alpha}({\\pmb x})=-1/2\\log(2\\pi\\sigma^{2})-\\frac{({\\pmb x}-\\sqrt{\\sigma(\\alpha)}\\mu)^{2}}{2\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Take the gradient of the log-density via torch built-in function torch.autograd.grad(log-density, samples), we have the ground-truth score function $\\hat{\\pmb{\\epsilon}}_{g t}(\\pmb{x},\\alpha)$ . ", "page_idx": 14}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Synthetic Experiment \u2013 Maximum Mean Discrepancy Bandwidth Choice ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Maximum Mean Discrepancy (MMD) is a statistical test used to determine if two distributions are different. It works by comparing the mean embeddings of samples drawn from two distributions in high dimensional feature space. Specifically, if the distributions are the same, the means should be close; if they are different, the means should be far apart. The embeddings are typically constructed using a feature map associated with a kernel function, and here we select the Gaussian kernel: ", "page_idx": 14}, {"type": "equation", "text": "$$\nK(\\pmb{x},\\pmb{y})=\\exp(-\\frac{\\|\\pmb{x}-\\pmb{y}\\|^{2}}{2\\sigma^{2}})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The bandwidth parameter $\\sigma$ of the Gaussian kernel plays a critical role in the sensitivity and performance of MMD. The better bandwidth choice, the more effective MMD computation is. Intuitively, the bandwidth $\\sigma$ controls the scale at which differences between distributions are detected. A small $\\sigma$ makes the kernel sensitive to differences at small scales (fine details), while a large $\\sigma$ highlights differences at larger scales. ", "page_idx": 15}, {"type": "text", "text": "The choice of bandwidth is often related to the variance of the data, and the bandwidth should be on the order of the variance of the data. Through a small experiment where we calculate MMD score between our data and the standard Gaussian under various bandwidths $\\sigma\\mathbf{s}$ , we pick the one that maximizes the MMD score. ", "page_idx": 15}, {"type": "text", "text": "In the synthetic 2D Dino experiment, we plot the relationship between MMD scores and bandwidths (Fig. 8), setting $\\sigma=3e-02$ . ", "page_idx": 15}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/888be2ea55275ef6c0000626dd895aa7be261f445892b8ad27aa434577185239.jpg", "img_caption": ["Figure 8: MMD between Dino data and the standard Gaussian. We plots the relationship between the MMD values and the bandwidth parameter used in the kernel function, and pick the bandwidth value with peak MMD score. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2 Details on Model Training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Model Checkpoints We adopt two models as baselines to fine-tune with CDL: DDPM model provided by Ho et al. 2020 and EDM model provided by Karras et al. 2019. ", "page_idx": 15}, {"type": "text", "text": "For DDPM model, the checkpoint 8 we used is a ema one pre-trained on unconditional CIFAR-10. The reason we are not using the most frequently used checkpoint (https://huggingface.co/ google/ddpm-cifar10-32) is that, this is not EMA checkpoint and our calculation of the FID score of this model on 50, 000 generated image via sequential DDPM sampler gives 12.43. This FID score is much higher than what reported on the original paper 3.17. Here we provide the FID scores of this non-EMA pre-trained model, results shown in Tab. 4. ", "page_idx": 15}, {"type": "table", "img_path": "RE7wPI4vfT/tmp/9376c139e6c1960b28681ad3e82d334f762d27263da00a5acdca44e06c999da4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 4: Evaluating FID score for both parallel and sequential DDPM samplers. FID scores are calculated using 5, 0000 samples. ", "page_idx": 15}, {"type": "table", "img_path": "RE7wPI4vfT/tmp/c7dcbaf84ddcc0e8189c62aae084e98106e09e5d8fda89790f63fbc674a776ed.jpg", "table_caption": [], "table_footnote": ["Table 5: Fine-tuning configurations for different datasets "], "page_idx": 16}, {"type": "text", "text": "For EDM model, in total eight checkpoints we used are $\\cdot\\mathrm{DDPM}++$ cont. (VP)\u201d and $\\mathbf{\\dot{NCSN}++}$ cont. (VE)\u201d models pre-trained on three datasets (CIFAR-10, uncond-FFHQ, and uncond-AFHQv2 [15, 10, 3]) with two training settings (unconditional and conditional) 9 10 11 12 13 14 15 16. ", "page_idx": 16}, {"type": "text", "text": "As for fine-tuning, we train all models with the same training setting in their original papers. For DDPM model, we train each model for 10 epochs and keep \u2019learning rate / batch size\u2019 ratio to be $\\mathrm{\\Delta^{\\prime}10^{-4}/64^{\\circ}}$ , and this training is on two A6000 GPUs. For EDM model, the following table list the exact our fine-tuning configurations, which is still of the same \u2019learning rate/batch size\u2019 ratio. This training is using eight V100 GPUs. ", "page_idx": 16}, {"type": "table", "img_path": "RE7wPI4vfT/tmp/0798516eb0be22ae8df2fd1c0fad43612ba68ae3f004e65c6b00fe1c2c09c69a.jpg", "table_caption": ["More Experimental results with Parallel DDPM Sampler The previous parallel diffusion sampling paper [28] calculates FID scores by using 5, 000 generated images and another 5, 000 randomly selected real images, and to follow the same experimental setting for comparison, we further provide the FID results in Table 6. "], "table_footnote": ["Table 6: Evaluating FID score (lower is better) of parallel DDPM sampler on real-world datasets using 5, 000 samples. \u201cNA\u201d stands for \"Not Applicable\". For reported FID scores, we run three sets of random seeds and reported the average with uncertainty. "], "page_idx": 16}, {"type": "text", "text": "More Details on EDM Fine-tuning As the design choices of EDM model is very comprehensive and complicate, here we list the training noise distribution, loss weighting, network and preconditioning choices we make during CDL fine-tuning in Tab. 7. ", "page_idx": 16}, {"type": "text", "text": "B.3 Relationship Among Log-SNR, Timesteps, and Noise variance Sigma ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To use the pre-trained models in the literature with our CDL loss, we need to translate \"t\", a parameter representing time in a Markov chain that progressively adds noise to data in Ho et al. 2020 and Song et al. 2020, or \" $\\sigma$ \", the variance scale of the Gaussian noise in Karras et al. 2022, to a log-SNR \"\u03b1\". ", "page_idx": 16}, {"type": "table", "img_path": "RE7wPI4vfT/tmp/d0e8b244f52f459ae7014c4547be53db456a058e426062945eb838924ffcadfe.jpg", "table_caption": [], "table_footnote": ["Table 7: CDL finetune on EDM experiment \u2013 fine-tuning design choices. "], "page_idx": 17}, {"type": "text", "text": "Translation between Timesteps and Log-SNR For time-step $t$ in DDPM and stochastic diffusion notation, we recommend readers check Kong et al. 2023 Appendix B.2 about the mapping between $\\alpha$ and $t$ . ", "page_idx": 17}, {"type": "text", "text": "Translation between Noise Variance Sigma and Log-SNR For variance scale of the Gaussian noise $\\sigma$ in EDM, referring to Eq.(7) and (8) in Karras et al. 2022, it\u2019s easy to translate the preconditioning: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\pmb x}_{\\alpha}\\equiv c_{i n}(\\sigma)\\cdot\\left({\\pmb x}+\\sigma{\\pmb\\epsilon}\\right)}\\\\ &{{\\pmb x}_{\\alpha}\\equiv{\\sqrt{\\sigma(\\alpha)}}{\\pmb x}+{\\sqrt{\\sigma(-\\alpha)}}{\\pmb\\epsilon}\\equiv{\\sqrt{\\sigma(\\alpha)}}\\;({\\pmb x}+{\\sqrt{\\frac{\\sigma(-\\alpha)}{\\sigma(\\alpha)}}}{\\pmb\\epsilon})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From Eq. 18 and Eq. 17, we see that $\\begin{array}{r}{\\sigma\\equiv\\sqrt{\\frac{\\sigma(-\\alpha)}{\\sigma(\\alpha)}}}\\end{array}$ , therefore, the relationship between $\\alpha$ and $\\sigma$ should be: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma\\equiv\\exp(-\\alpha/2),\\;\\alpha\\equiv-2\\ln(\\sigma)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.4 Contrastive Loss Implementation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To implement contrastive loss, we follow the definition in Sec. 3. First, we generate a random binary label $y$ . Next, conditioned on y, we sample from either data distribution $p({\\pmb x})$ or the noisy data distribution $p_{\\zeta}(x)$ . We calculate the point-wise log-likelihood, then the contrastive loss in Eq. 7. ", "page_idx": 17}, {"type": "table", "img_path": "RE7wPI4vfT/tmp/b067c1b27dc41e05c77b47c83fa1282fb18f62bd8de31c36227463c298974342.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.5 Training cost ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As we mentioned, CDL training is more expensive to compute than the standard diffusion loss, and here we analysis and give the reason. ", "page_idx": 18}, {"type": "text", "text": "According to Kong et al. 2023, we can write the pointwise standard diffusion loss function as Eq. 19, and therefore the standard diffusion loss is as Eq. 20. ", "page_idx": 18}, {"type": "equation", "text": "$$\nn l l(\\pmb{x})=-\\log p(\\pmb{x})=c+\\mathbb{1}/2\\int_{-\\infty}^{\\infty}\\mathbb{E}_{p(\\pmb{\\epsilon})}[\\|\\pmb{\\epsilon}-\\hat{\\pmb{\\epsilon}}(\\pmb{x}_{\\alpha},\\alpha)\\|_{2}^{2}]\\,d\\alpha.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nn l l=\\mathbb{E}_{p(\\mathbf{x})}[-\\log p(\\mathbf{x})]=c+1/2\\int_{-\\infty}^{\\infty}\\mathbb{E}_{p(\\epsilon)\\ p(\\mathbf{x})}[\\|\\epsilon-\\hat{\\epsilon}(x_{\\alpha},\\alpha)\\|_{2}^{2}]\\ d\\alpha.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To train the standard diffusion loss, we simply need to optimize Eq. 20 by all the training data. However, to train the contrastive diffusion loss, we need to estimate $n l l(x)$ and $n l l(x+\\zeta)$ term in Algo. 1, and there we estimate Eq. 19 by duplicating a single data point $\\textbf{\\em x}$ for $N=100$ times and calculate Eq. 20. This pointwise NLL estimation $n l l(x)$ demands $N=100$ times more computational resources compared to the standard diffusion loss. ", "page_idx": 18}, {"type": "text", "text": "In principle, the contrastive loss Algo. 1 should be executed for the entire training dataset. However, due to the high computational cost, we optimize only one data point per batch instead of utilizing all the training data. ", "page_idx": 18}, {"type": "text", "text": "C Samples Visualization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide visualization of the images generated from pre-trained models fine-tuned via CDL loss. ", "page_idx": 18}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/6d4d88cabf63b1858f9edc1b74f76dfda47fc6a6e43bd1ed1a2d68df99a72ecf.jpg", "img_caption": ["Figure 9: The CDL-loss fine-tuned EDM checkpoint generated examples from Conditional CIFAR-10, via parallel DDPM sampler. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/a8a068e84ef084ab6c84759469414e63c48783e7e4e975dc4cb13d0f406e5ed6.jpg", "img_caption": ["Figure 10: The CDL-loss fine-tuned EDM checkpoint generated examples from Unconditional CIFAR-10, via parallel DDPM sampler. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/38ebd3d6397fcfaad03f9ce896a82d8a55137e2296630374aeb9a40fb0a4f439.jpg", "img_caption": ["Figure 11: The CDL-loss fine-tuned EDM checkpoint generated examples from Unconditional AFHQ, via parallel DDPM sampler. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/ecaef53c41ac077ab3bb09c47950cf73d4631d973b34f794514cea581017d979.jpg", "img_caption": ["Figure 12: The CDL-loss fine-tuned EDM checkpoint generated examples from Unconditional FFHQ, via parallel DDPM sampler. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/38bd06b08d7845f8dec0fcbc412415ab856ccd9f48aa3b9fdeeec07e190089c6.jpg", "img_caption": ["Figure 13: The CDL-loss fine-tuned EDM checkpoint generated examples from Conditional CIFAR10, via sequential EDM sampler. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/d1be470d66b0643e9706b68f928e9a7325579b71184b6ab131db87e4d009a56f.jpg", "img_caption": ["Figure 14: The CDL-loss fine-tuned EDM checkpoint generated examples from Unconditional CIFAR-10, via sequential EDM sampler. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/ad77f344b082cdde6b45ba9e8428cd1be3742694e9977562524459c96b0dbe77.jpg", "img_caption": ["Figure 15: The CDL-loss fine-tuned EDM checkpoint generated examples from Unconditional AFHQ, via sequential EDM sampler. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "RE7wPI4vfT/tmp/7c2bc7c8a71fb0988ede06e40ca5973c196910b5dc61ff3c1e68142cf6b0c102.jpg", "img_caption": ["Figure 16: The CDL-loss fine-tuned EDM checkpoint generated examples from Unconditional FFHQ, via sequential EDM sampler. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We explain the proposed CDL speeds up parallel sampling convergence and improves generation quality. For sequential sampling, it improves the trade-off between sampling quality and gerneration speed. We provide the claims in both abstract and introduction, and justify them in the Results section. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The main limitation to our approach is the increased training cost. We explicitly highlight limitations in the methods development in Sec. 3. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Every theoretical result is supported by a complete proof and assumptions as well, if applicable. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide anonymous code link and full description of training configuration. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide anonymous code link and full description of training configuration. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide all detailds about training and testing details in both experimental section and also in appendix. Since we test on benchmark, our data split are rigously the same as all other baseline settings. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report error bars wherever possible. One set of error bars for Table 2 is missing due to lack of computing resources, but will be added in the final version. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide sufficient information on the computation resources, e.g. GPU information, to reporduce the experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not use human subjects and only used standard datasets for experiments. We acknowledge the potential harms of diffusion models in the broader impacts statement, and do not anticipate that our methodological contributions will alter known issues. We also strive for transparency wherever possible, including full, anonymized code with our submission. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: While we do not expect a direct societal impact from our methodological improvements, we added a broader impacts statement that acknowledges the potential impact of diffusion models. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We used existing standard datasets and public, pretrained models. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We give proper attribution for the open license code, data, and models used throughout the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No new assets are provided. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No crowdsourcing or human subjects research is included. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]