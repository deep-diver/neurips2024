[{"figure_path": "xoCFd1WKpf/tables/tables_6_1.jpg", "caption": "Table 1: Zero-shot cross-modal retrieval. Q indicates variants of our LexVLA. CLIP\u00b9 is the original CLIP [34]; results denoted by (\u00b7)\u00b2 are reported in VDR [48]; results denoted by (\u00b7)\u00b3 are reported in STAIR [5]. \u201cData\u201d is the multi-modal alignment training data size; \u201cLatent\u201d means direct latent feature alignment methods; \u201cLexical\u201d indicates lexical feature alignment methods. R@K, the recall ratio within top-K items.", "description": "This table presents the results of zero-shot cross-modal retrieval experiments on two benchmark datasets, MSCOCO and Flickr30k.  It compares LexVLA (and its variants) against several baseline methods, categorized as either using latent feature alignment or lexical feature alignment. The table shows recall rates (R@K) at different ranks (K=1, 5, 10) for both image-to-text and text-to-image retrieval tasks, highlighting the performance of LexVLA across different training data sizes.  The variants of LexVLA are denoted by Q and compared to the original CLIP, as well as other related methods from published works (VDR and STAIR).", "section": "4 Experiments"}, {"figure_path": "xoCFd1WKpf/tables/tables_7_1.jpg", "caption": "Table 1: Zero-shot cross-modal retrieval. Q indicates variants of our LexVLA. CLIP\u00b9 is the original CLIP [34]; results denoted by (\u00b7)\u00b2 are reported in VDR [48]; results denoted by (\u00b7)\u00b3 are reported in STAIR [5]. \u201cData\u201d is the multi-modal alignment training data size; \u201cLatent\u201d means direct latent feature alignment methods; \u201cLexical\u201d indicates lexical feature alignment methods. R@K, the recall ratio within top-K items.", "description": "This table presents the results of zero-shot cross-modal retrieval experiments on the MSCOCO and Flickr30k datasets.  It compares LexVLA (and its variants) against several baselines, including the original CLIP and other methods focusing on latent or lexical feature alignment. The table shows the recall at ranks 1, 5, and 10 (R@1, R@5, R@10) for both image-to-text and text-to-image retrieval tasks. Different model implementations are categorized based on their training data size (Data) and alignment approach (Latent or Lexical).", "section": "4.1 Zero-shot cross-modal retrieval"}, {"figure_path": "xoCFd1WKpf/tables/tables_13_1.jpg", "caption": "Table 1: Zero-shot cross-modal retrieval. Q indicates variants of our LexVLA. CLIP\u00b9 is the original CLIP [34]; results denoted by (\u00b7)\u00b2 are reported in VDR [48]; results denoted by (\u00b7)\u00b3 are reported in STAIR [5]. \u201cData\u201d is the multi-modal alignment training data size; \u201cLatent", "description": "This table presents the results of zero-shot cross-modal retrieval experiments on the MSCOCO and Flickr30k datasets.  It compares LexVLA (and its variants) against several other state-of-the-art models, showing the recall rate (R@K) at different top-K ranks. The table highlights LexVLA's performance even when trained on significantly smaller datasets compared to competitors. Different variants of LexVLA are also evaluated and compared against each other.", "section": "4 Experiments"}]