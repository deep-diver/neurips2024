[{"type": "text", "text": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jian Guan1,2, Wei $\\mathbf{W}\\mathbf{u}^{2*}$ , Zujie $\\mathbf{Wen^{2}}$ , Peng $\\mathbf{Xu^{2}}$ , Hongning Wang1, Minlie Huang1 ", "page_idx": 0}, {"type": "text", "text": "The CoAI group, DCST, Institute for Artificial Intelligence, 1State Key Lab of Intelligent Technology and Systems, 1Beijing National Research Center for Information Science and Technology, 1Tsinghua University, Beijing 100084, China. 2Ant Group. {jianguanthu, wuwei19850318,wang.hongn}@gmain.com, {zujie.wzj,peng.x}@antgroup.com, aihuang@tsinghua.edu.cn. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available at https://github.com/JianGuanTHU/AMOR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "LLMs, with astounding performance over general natural language processing (NLP) problems [42, 1, 36], have spurred great interest in building LLM-based agents to solve complex tasks by interacting with external resources such as web knowledge [27], specialized tools [31], etc. ", "page_idx": 0}, {"type": "text", "text": "We focus on developing agents for knowledge-intensive tasks, where the agent completes users\u2019 information-seeking requests by interacting with specific knowledge bases [22]. To address the complexity of such tasks, we posit the desiderata for a qualifying agent as follows: Firstly, the agent should possess a robust reasoning logic about the task to solve individual problems with precise pathways. Secondly, the agent should maintain an adaptive mechanism to adjust to specific environments, rather than staying static. Thirdly, the reasoning process should be amenable to human interventions, enabling humans to steer the agent\u2019s behavior through direct feedback to the process rather than only to the outcome. This ability can significantly facilitate alignment between agent behavior and human intent [39]. ", "page_idx": 0}, {"type": "text", "text": "Although extensive studies have been conducted on building language agents, few, if any, can fulfill all the required criteria due to their uncontrollable reasoning logic, static model capability, or sparse/missing feedback signals, as detailed in Table 1. Consequently, it is still challenging for users to critique, and thus guide existing agents to follow targeted manners, especially when the agents are built upon less powerful LLMs [25]. ", "page_idx": 0}, {"type": "table", "img_path": "jImXgQEmX3/tmp/43d6c97633320d65358d1979e08ca4493b9945cf93ed3b8334175e816cf53594.jpg", "table_caption": ["Table 1: Comparison between AMOR and representative methods for building agents. Appendix A.1 provides a more comprehensive discussion in detail. "], "table_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "jImXgQEmX3/tmp/ffb92946505b3c9933e7fd475d62d4a253bc90022bd24f18236a455133ccb7f2.jpg", "img_caption": ["Figure 1: AMOR\u2019s state transition diagram. Each box represents a state and the corresponding module that is executed when entering the state. There may be multiple categories of execution results distinguished by special branch tokens such as \u201c[NEXT].\u201d Then AMOR determines the next state based on the branch tokens. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We introduce AMOR, an Adaptable MOdulaR knowledge agent that can reason and adapt, with the reasoning process amenable to human supervision, based on open-source LLMs. AMOR\u2019s reasoning logic is formalized as a finite state machine (FSM) [7, 21] that solves problems via a series of executions and transitions over a set of modules (Figure 1). This naturally enables the desired processbased supervision mechanism, allowing users to give feedback to each LLM-controlled module. AMOR supports flexible forms of feedback, either binary judgments regarding the correctness or refinement of the outputs. The reasoning logic and process feedback mechanism together frame how AMOR thinks, acts, and interacts with users and task environments. ", "page_idx": 1}, {"type": "text", "text": "We build AMOR upon an LLM equipped with distinct parameters for different modules to efficiently handle multiple tasks. The training in AMOR happens in two stages: (1) Warm-up: the modular design enables us to construct training data separately for each disentangled module without requiring complete trajectories for specific tasks. As a result, we create a large dataset of $50\\mathrm{k}$ examples covering multiple distinct tasks, simply using public datasets. We fine-tune AMOR on this data for generalization over various knowledge-seeking scenarios. (2) Adaptation: when deployed, we tailor AMOR to the target domain by letting it autonomously address user tasks (i.e., exploration), collecting process feedback for each LLM output, and evolving through further fine-tuning on the exploration trajectories with feedback (i.e., exploitation). Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "I. We propose a general framework for building knowledge agents, featuring FSM-based reasoning logic and a process feedback mechanism. We focus on text corpora as knowledge bases, but the approach can be flexibly extended to other knowledge types and user tasks by customizing the modules and dependencies within the FSM framework. ", "page_idx": 1}, {"type": "text", "text": "II. Experiments across multiple domains show the strong advantage of the FSM-based reasoning logic with $30\\%{-}40\\%$ improvements over baselines when based on off-the-shelf LLMs (e.g., GPT- $.4^{2}$ ). Switching to fine-tuned LLMs, the warm-up stage empowers AMOR to generalize to multiple domains and surpass strong baselines. After we adapt AMOR to specific domains, subsequent domain-specific adaptations reveal that process feedback is significantly more effective in improving the reasoning process than outcome feedback. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Language agents. Interest is surging in building agents for tasks necessitating multi-step reasoning. Existing work falls into two groups. The first group focuses on designing agent architectures, such as CoT\u2019s step-by-step reasoning [44], ReAct\u2019s integration of reasoning, action, and observation to allow tool use [51], and CODEPLAN\u2019s two-stage reasoning framework that first generates a codeform plan and then realizes low-level reasoning steps [45]. Nevertheless, such free-form reasoning constraints human intervention. In contrast, modular agents follow a pipeline to execute specialized modules [19, 14, 11, 3, 52], improving the ease of intervention. The second group aims to design adaptive mechanisms for adapting agents to specific scenarios. ToT [50] and Reflexion [35] use environment feedback for multi-path pruning and iterative single-path refinement, respectively, but suffer from poor inference efficiency and need for real-time feedback. As a fine-tuning approach, recent work equipped open-source LLMs with specific agent abilities by learning from examples synthesized based on human priors [5], or expert trajectories from humans [27] or GPT-4 [53, 4] with correctness validation through outcome feedback. In contrast, our modular agent AMOR employs FSM-based reasoning with a stronger capacity for handling complex tasks than simple pipelines and adapts effectively to specific environments via process feedback. ", "page_idx": 2}, {"type": "text", "text": "Retrieval-augmented generation (RAG). The RAG paradigm augments the inputs of LLMs with retrieved passages to enhance factuality [12, 22, 10]. Recent studies have developed interleaved reasoning-retrieval for better information recall than one-step retrieval [38, 16, 28]. However, retrieval may introduce noise that leads to low-quality answers [34]. To tackle this, Self-RAG [2] trained LLMs to selectively perform retrieval and utilize retrieved passages. Unlike RAG approaches, AMOR emphasizes an explainable reasoning process for proactively decomposing questions and seeking evidence for grounded generation, and allows for process feedback from humans. Nevertheless, RAG mainly focuses on integrating parametric factual knowledge in LLMs and retrieved non-parametric knowledge, which is less explainable and intervenable. ", "page_idx": 2}, {"type": "text", "text": "3 AMOR agent ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "AMOR relies on three key techniques: FSM-based reasoning logic, a process feedback mechanism, and a two-stage fine-tuning strategy. We detail the definition of the reasoning logic and its specification assuming the knowledge base is a text corpus in $\\S3.1$ , the method for fine-tuning open-source LLMs as a warm-up stage in $\\S3.2$ , and the adaptation stage driven by process feedback in $\\S3.3$ . ", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 FSM-based Reasoning Logic Input: Agent at the state $s=s_{0}$ ; $Q$ : Question. Output: $A$ : Final Answer; $R$ : Reasoning Process. 1 $R=[\\,]$ 2 while $s\\neq s_{N-1}$ do 3 $y=m(s)\\ \\ /\\ /$ Obtain the output $y$ given $s$ from the corresponding module $m$ . 4 R.append({\u201cstate\u201d: $s$ , \u201coutput\u201d: $y\\}$ ) 5 A = y 6 return A, R ", "page_idx": 2}, {"type": "text", "text": "3.1 Reasoning logic ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Algorithm 1 outlines how to deduce the answer $A$ for an input question $Q$ with a reasoning process $R$ using FSM-based reasoning logic, which can be defined by a quadruple: $\\{\\boldsymbol{S},\\mathcal{M},\\boldsymbol{\\mathcal{E}},\\mu\\}$ , where ", "page_idx": 2}, {"type": "text", "text": "\u2022 ${\\cal S}=\\{s_{0},\\dots,s_{N-1}\\}$ is a set of states with $s_{0}$ as the initial state and $s N\\!-\\!1$ as the final state. Each state holds variables to track context information.   \n\u2022 $\\mathcal{M}=\\left\\{m_{0},\\dots,m_{N-1}\\right\\}$ is a set of modules with $m_{k}$ triggered when the reasoning flow reaches state $s_{k}$ . The modules are categorized into two types: (a) Tool modules $\\langle\\mathcal{M}_{\\mathrm{TOOL}})$ for invoking tools, and (b) LLM modules $(\\mathcal{M}_{\\sf L L M})$ for calling LLMs. ", "page_idx": 2}, {"type": "image", "img_path": "jImXgQEmX3/tmp/b151e06970a7598bf35a7314c533f6ffc9f357c13e1eb87271bb9aeb9fcd8d97.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: On the top left is a sample question from Musique [37], providing ample information (in green) for constructing training examples for four LLM modules of AMOR (bottom). We augment extra knowledge (in blue) for the Judge and Answer module by invoking the SearchDoc and SearchPsg tools (top right). In each example, we highlight the prompt in purple to format the current state (before \u201cOutput:\u201d) and output (after \u201cOutput:\u201d), and use \u201c||\u201d to separate different examples for training. ", "page_idx": 3}, {"type": "text", "text": "\u2022 $\\mathcal{E}$ is the set of all possible outputs of $\\mathcal{M}$ .   \n\u2022 $\\mu:S\\times\\mathcal{E}\\to S$ is the transition function that determines the next state of the reasoning flow given the current state and the execution result of the corresponding module. ", "page_idx": 3}, {"type": "text", "text": "When the external knowledge base is a text corpus, an instantiation of the reasoning logic can be represented by the state transition diagram in Figure 1. In this case, $\\mathcal{M}_{\\mathrm{TOOL}}$ perform document and passage retrieval using external retrievers; while $\\mathcal{M}_{\\mathrm{LLM}}$ leverage the LLM to analyze and digest the question, documents, and passages to deduce the final answer. To distinguish different types of outputs from a module that requires different subsequent modules, we employ a set of special branch tokens such as \u201c[NEXT]\u201d to guide $\\mu$ to determine the next state. In summary, AMOR answers question $Q$ by (1) iteratively decomposing $Q$ to a sub-query $q$ at state $s_{0}$ , and finding the answer $a$ to $q$ and the evidence passage $e$ through iterative knowledge retrieval, relevance evaluation, retrieval refinement (i.e., \u201cPassage Retrieval\u201d), and answer extraction, until no more knowledge is needed; and (2) deducing the final answer $A$ based on the collected evidence passages at the final state. ", "page_idx": 3}, {"type": "text", "text": "Defining reasoning logic as an FSM offers three advantages: (1) Structured Thinking. FSM makes specifications of inter-step dependencies (e.g., prioritization, branch selection) easy, and thus enables narrowing down the exploration space. (2) Skill Disentanglement. By decomposing complex tasks into modular steps, one can independently construct training data for each module, which significantly reduces the difficulty of implementing AMOR with open-source LLMs (cf., $\\S3.2)$ . This feature also allows AMOR to focus on single steps, thereby mitigating the weakness of LLMs in reasoning over long context formed by task-solving trajectories [24]. (3) Intervenable Workflow. The structured reasoning process enables users to easily diagnose the agent\u2019s mistakes and provide process feedback for improving the reasoning capability of the agent (\u00a73.3). ", "page_idx": 3}, {"type": "text", "text": "3.2 Warming-up open-source LLMs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Open-source LLMs are observed to fall short in complex agent tasks [46, 25]. Recent studies have improved their reasoning abilities through imitation learning using trajectories from advanced LLMs such as GPT-4 [53, 4]. However, even GPT-4 can struggle with producing high-quality reasoning trajectories [29]. ", "page_idx": 3}, {"type": "text", "text": "AMOR\u2019s modular design enables us to construct training data for each module separately from existing datasets without simulating the whole trajectories, thus greatly alleviating the above issue. Formally, given a sample question $Q$ with annotations of the final answer $\\hat{A}$ , all sub-queries and answers $\\hat{H}=[(\\hat{q}_{0},\\hat{a}_{0}),(\\hat{q}_{1},\\hat{a}_{1}),\\cdot\\cdot\\cdot]$ , and all evidence passages $\\hat{\\boldsymbol E}=[\\hat{e}_{0},\\hat{e}_{1},\\cdot\\cdot\\cdot]$ , we can directly transform these annotations into a suitable format to serve as training data for Decompose and Complete in Figure 1. Since Judge and Answer require multiple types of retrieved knowledge (e.g., relevant or not), we employ retrieval tools to augment the input. Figure 2 exemplifies the construction pipeline, which can be easily extended to other knowledge-intensive datasets and specific domains. Appendix A.4 shows more details. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "When fine-tuning open-source LLMs to handle multiple tasks defined by different modules, we are inspired by the Mixture-of-Experts approach [33] to learn distinct Feed-Forward Network (FFN) parameters in the final quarter of the Transformer blocks to balance the trade-off between performance and inference efficiency. These module-specific parameters are initialized using the original model\u2019s FFN layers. We call the proposed architecture Module-Aware Mixture-of-Experts (MA-MoE)3. Then, we fine-tune the MA-MoE model with the standard language modeling loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}=-\\mathbb{E}_{m\\in\\mathcal{M}_{\\mathrm{LLM}},(\\hat{s},\\hat{y})\\in\\mathcal{D}_{m}}\\lambda_{m}\\log\\pi_{\\theta_{m}}(\\hat{y}|\\hat{s}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pi$ refers to the policy model MA-MoE that maps the state $\\hat{s}$ to an action ${\\hat{y}},\\,\\theta_{m}$ denotes the parameter for the module $m\\in\\mathcal{M}_{\\sf L L M}$ , $\\mathcal{D}_{m}$ is the corresponding collection of training examples, $(\\hat{s},\\hat{y})$ is a state-output pair from $\\mathcal{D}_{m}$ , and $\\{\\lambda_{m}\\}$ are tunable hyper-parameters. ", "page_idx": 4}, {"type": "text", "text": "3.3 Adaptation through process feedback ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Feedback is crucial for adapting language agents to specific environments [40], especially when dealing with unseen, long-tail, or everchanging domain knowledge. Prior agents commonly used outcome feedback for adaptation which assesses the correctness of intermediate steps based on the success or failure of the outcome [53, 4]. However, outcome feedback is too sparse to improve intermediate reasoning [23]. Recent studies also highlighted that LLMs\u2019 reasoning steps are likely to contradict the outcome [26], which means that outcome feedback may inevitably introduce noise during training (see examples in Appendix B.8). In contrast, AMOR\u2019s process feedback mechanism can effectively alleviate these issues. ", "page_idx": 4}, {"type": "table", "img_path": "jImXgQEmX3/tmp/97daafc9d741d9768872dde36dad740ae4f989fbf3e20621e800858ce0148f40.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 describes the adaptation mechanism of AMOR parameterized by $\\theta$ , specifically as three steps: (1) Exploration. AMOR answers the input question $\\dot{Q}$ by interacting with a knowledge base. (2) Feedback Collection. AMOR\u2019s reasoning process for $Q$ is evaluated with feedback $f_{k}$ for the output $y_{k}$ of the LLM at each step during reasoning, which is either \u201cright/wrong\u201d or a refined version of $y$ . We convert $y$ into a feedback-refined target output $\\tilde{y}$ based on the feedback $f_{k}$ and determine the immediate reward $o_{k}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{y}_{k},o_{k}=\\left\\{\\begin{array}{l l}{y_{k},1}&{\\mathrm{if}\\ f_{k}=\\mathrm{\\Gamma^{*}r i g h t^{*}},}\\\\ {y_{k},0}&{\\mathrm{if}\\ f_{k}=\\mathrm{\\Gamma^{*}w r o n g^{*}},}\\\\ {f_{k},1}&{\\mathrm{if}\\ f_{k}\\mathrm{\\is\\refinement}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(3) Exploitation. Every $T$ steps of the former exploration and feedback collection, we optimize the initial policy based on the resulting trajectories and corresponding feedback [32]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}=-\\mathbb{E}_{m\\in\\mathcal{M}_{\\mathrm{LM}},(s_{k},\\tilde{y}_{k},o_{k})\\in\\mathcal{R}_{m}}\\lambda_{m}[o_{k}-\\beta]\\mathrm{log}\\big(\\pi_{\\theta_{m}}(\\tilde{y}_{k}|s_{k})/\\pi_{\\theta_{m}}^{\\mathrm{wrT}}(\\tilde{y}_{k}|s_{k})\\big)\\big],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "jImXgQEmX3/tmp/4674bafc0ffe9c8d4d397a5f00806f34c0876d12e40979d08bec6842cf09f6d5.jpg", "table_caption": ["Table 2: Automatic annotation strategy for silver process feedback for different LLM modules. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{R}_{m}\\subseteq\\mathcal{R}$ denotes the training examples for module $m$ , $\\pi_{\\boldsymbol{\\theta}}^{\\mathrm{WFT}}$ refers to the initial warm-up policy. Notably, this loss function is non-differentiable, necessitating the use of a specialized optimization technique. We use a recently proposed alignment algorithm KTO [9] with an MLE regularization [47] for optimization, which optimizes the policy without requiring paired human preferences. Crucially, when optimizing a particular module $m$ , the gradient induced by the feedback signal propagates through the entire MA-MoE model, except for the FFN layers corresponding to other modules. This targeted optimization approach enables AMOR to effectively align its outputs with the desired intermediate results and final answers, leveraging the fine-grained process feedback provided by human supervisors. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Tools modules. We construct retrievers for both SearchDoc and SearchPsg using Contriever-MS MARCO [15]. SearchDoc retrieves a single document snippet per query, while SearchPsg fetches the top three relevant passages from a given document. By invoking NextDoc, at most nine more document snippets are returned. Appendix B.1 presents more details. ", "page_idx": 5}, {"type": "text", "text": "Warm-up datasets. We employ four question-answering (QA) datasets to warm up open-source LLMs, including 2WikiMultiHopQA [13], Musique [37], NaturalQuestions [20] and BoolQ [6]. They vary in levels of question complexity (single- or multi-hop), answer types (phrase spans or yes/no), and types of dependency structures between sub-queries (e.g., serial or parallel), etc. Appendix A.4 shows the statistics in detail. ", "page_idx": 5}, {"type": "text", "text": "Adaptation & evaluation datasets. We consider three benchmarks, by which we simulate different deployment scenarios: (1) HotpotQA [49]: a challenging multi-hop QA dataset built on Wikipedia articles. We use the Wikipedia dump provided in [15] as the knowledge base. (2) PubMedQA [17]: a biomedical QA dataset that requires answering a question by \u201cyes/no\u201d given a PubMed abstract. We adapt the data to retrieval-based QA by piling all $274\\mathrm{k}$ abstracts provided in the paper as a knowledge base, where each document comprises one abstract passage. (3) QASPER [8]: answering questions in free form based on a long NLP paper. For each question, we regard the corresponding paper as a knowledge base and each section of the paper as a document with several passages. We use the training and validation sets for adaptation fine-tuning and the test sets for evaluation. For evaluation metrics, we use exact match (EM) and F1 scores for HotpotQA and QASPER; and the accuracy (ACC) of \u201cyes/no\u201d for PubMedQA. More details are in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "Feedback annotation. Considering limited resources, we simulate human behavior and provide silver feedback to AMOR\u2019s reasoning processes based on the gold answer A\u02c6 and gold evidence passages $\\hat{\\boldsymbol E}=[\\hat{e}_{0},\\hat{e}_{1},\\cdot\\cdot\\cdot]$ for each target question $Q$ , which are already included in the training and validation data of the three benchmarks. Table 2 shows how we annotate the feedback for each LLM output $y$ . Note that AMOR is applicable for gold feedback from humans in realistic applications. Appendix B.3 discusses the accuracy of the silver feedback through human evaluation. ", "page_idx": 5}, {"type": "text", "text": "Implementation details. We set $\\lambda_{m}$ in Eq. 1 and Eq. 3 to 1 for all modules, $I=1$ in Algorithm 2, and $T$ to the size of the training set for each dataset, and fine-tune LLAMA-2-7B/13B-Chat for two epochs with a learning rate of $2\\mathrm{e}^{-5}$ using 8 NVIDIA 80GB A100 GPUs. While applying ", "page_idx": 5}, {"type": "table", "img_path": "jImXgQEmX3/tmp/8fe31f1a6c3f420faaf86657134e4d6a67877ab9b89a6c5b28a67d33056ddd10.jpg", "table_caption": ["Table 3: Refining each module output $y$ to $\\tilde{y}$ based on the outcome feedback $f_{o}$ to adapt AMOR, where $^{\\l}y$ denotes converting the binary output $y$ to its opposite label. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "AMOR for inference, we use greedy decoding for all generations. Besides, we set the maximum number of decomposed sub-queries to the maximum count of gold evidence passages, i.e., $2/1/1$ for HopotQA/PubMedQA/QASPER, respectively. Once the maximum number is reached, AMOR is transited to state $s_{6}$ (\u201cTask Completion\u201d) to finalize the answer. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare AMOR to various baselines with or without fine-tuning: (1) CoT [44]: it prompts an off-the-shelf LLM to generate the answer through step-by-step reasoning. (2) RAG: One-Step Retrieval (OneR) uses the question as a query to retrieve top- $\\mathcal{K}$ document snippets with the SearchDoc module to augment the input. We set $K$ as the maximum number of gold evidence passages in each dataset. Under the fine-tuning setting, we use the gold evidence passages for training. Self-RAG [2] selectively performs retrieval and utilizes retrieved passages while does not explicitly introduce question decomposition. They can be viewed as simplifications of AMOR. (3) ReAct [51]: it interleaves thought, action, and observation steps. An action can be either invoking the retrieval tools or finalizing an answer. We also compare AMOR with fine-tuned ReAct-style agents including AgentLM [53] and FIREACT [4]. We set the maximum number of action steps to 20. (4) Modular Agents: ReWoo [46] follows a pipeline that plans all sub-goals, generates actions, and then executes, while LUMOS [52] applies this pipeline iteratively, tackling one sub-goal at a time with each interaction. Both agents utilize GPT-3.5 as a supplementary QA tool during action generation. Similar to AMOR, they modularize language agents; however, they lack explicit mechanisms for assessing the relevance of retrieved information. Under the setting without fine-tuning, we provide in-context examples for the baselines following their official implementations. ", "page_idx": 6}, {"type": "text", "text": "Furthermore, we also conduct ablation studies to investigate the influence of different components, resulting in two more baselines: (1) AMORWFT: AMOR with only warm-up fine-tuning, without further adaptation; and (2) AMOROutcome: outcome feedback instead of process feedback is utilized in adaptation after AMOR is warmed-up. Specifically, we determine the target output and corresponding immediate reward for an LLM module as detailed in Table 3, and then adapt AMOR using Eq. 3. For clarity, we denote our final method as AMORProcess. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 4 reports the evaluation results of AMOR and baselines on three datasets, revealing three key findings: (1) The FSM paradigm is clearly advantageous to prior agent frameworks. $\\mathrm{AMOR}_{\\mathrm{w/o}}$ FT delivers strong performance by improving $41.9\\%$ , $32.1\\%$ , and $41.2\\%$ over ReAct on average when built on top of off-the-shelf LLMs, including L-7B, GPT-3.5, and GPT-4, respectively. This indicates that our proposed FSM paradigm is more effective in leveraging LLMs for complex reasoning. (2) Warm-up fine-tuning generally enhances AMOR in downstream tasks. When based on L-7B, AMORWFT outperforms $\\mathrm{AMOR}_{\\mathrm{w/o\\FT}}$ T across all datasets. Furthermore, AMORWFT also surpasses other fine-tuned ReAct-style and modular agents, even including FIREACT that is fine-tuned with in-domain HotpotQA trajectories from GPT-4. This suggests the potential of utilizing existing datasets for developing powerful agents with well-defined reasoning logic. (3) Process feedback is more effective than outcome feedback in facilitating the adaptation of agents. The order that AMORProcess $>$ AMOROutcome $>$ AMORWFT indicates the impact of feedback in terms of tailoring agent behavior to specific domains, and process feedback is more helpful than outcome feedback for leading to the correct final answers. ", "page_idx": 6}, {"type": "text", "text": "Additionally, Table 5 presents the results from employing various model architectures (including our proposed MA-MoE model, the standard MoE model, and the Transformer model) as well as optimization algorithms (namely KTO and Supervised Fine-Tuning, i.e., SFT). The MoE model is identical to the MA-MoE model except it lacks module-specific awareness. SFT refers to optimizing the model only for outputs $\\tilde{y}_{k}$ that receive an immediate reward $o_{k}=1$ , using standard language modeling loss. The results show: (1) The standard MoE architecture struggles to effectively differentiate its experts for multitasking scenarios, leading to performance comparable to the Transformer model. In contrast, the MA-MoE\u2019s module-specific awareness enables it to handle diverse tasks within the agent more adeptly. (2) KTO outperforms SFT in aligning the agent\u2019s performance with external feedback, owing to its exploitation of negative samples. ", "page_idx": 6}, {"type": "table", "img_path": "jImXgQEmX3/tmp/b39b6b5b72a333af73b7deab4c1f3eb26a52c7c6c795aefc398eed3f7e750e7f.jpg", "table_caption": ["Table 4: Results of AMOR and baselines. \u201cL-7/13B\u201d is short for \u201cLLAMA-2-7/13B-Chat.\u201d We highlight the best results in bold and underline the second best. Models marked with \u2217are fine-tuned on the target datasets. Results marked with \u2020 are reported in the original paper and those marked with \u2021 are reported in [4]. N/A means the method does not apply to the datasets. $\\mathrm{AMOR}_{\\mathrm{Process}}$ outperforms baselines under the same setting significantly $p<0.01$ , sign test). "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "jImXgQEmX3/tmp/c1417aaa051a7872c5ad44ff3f781890eaaae2fd42843347b3900cc124d536b5.jpg", "table_caption": ["Table 5: Results of $\\mathbf{AMOR}_{\\mathrm{Process}}$ based on L-7B with different architectures and optimization algorithms. The architecture setting is also applied on the warm-up fine-tuning stage. $\\dagger$ refers to our final method. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Discussions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The main results have substantiated the benefits of different components of AMOR for successfully completing tasks. Nonetheless, we are still curious about four key research questions: (1) RQ1: ", "page_idx": 7}, {"type": "table", "img_path": "jImXgQEmX3/tmp/90972a325a8f8e1a78dbc464c74ea39fd347ae060cf6ea563738f3630b4e4807.jpg", "table_caption": ["Table 6: Recall scores under different settings. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "How do the AMOR variants differ in the ability to collect evidence? (2) RQ2: Is process feedback more data-efficient than outcome feedback for adaptation? (3) RQ3: What if using human feedback for adaptation of AMOR? (4) RQ4: To what extent does feedback-driven adaptation enhance the AMOR\u2019s reasoning process? Besides, Appendix B.6 and B.7 further demonstrate the efficient token usage of AMOR and the flexibility of AMOR\u2019s reasoning framework, respectively. ", "page_idx": 8}, {"type": "text", "text": "RQ1: Evidence collection comparison. We use recall of gold evidence passages $(\\hat{E})$ among those collected by AMOR (E) to assess AMOR\u2019s ability to collect evidence, formally as #{# E{\u02c6 E\u2229\u02c6}E }. ", "page_idx": 8}, {"type": "text", "text": "As shown in Table 6, we observe: (1) Warm-up fine-tuning consistently enhances evidence collection, with AMORWFT achieving higher recall than $\\mathrm{AMOR}_{\\mathrm{w/o\\FT}}$ across all datasets. (2) Adaptation through outcome feedback (AMOROutcome) exerts a negligible impact on the recall results compared with AMORWFT, suggesting the superiority of AMOROutcome to AMORWFT in final answers (see Table 4) may stem from the improvement of Complete. (3) Process feedback is crucial to improve the evidence collection ability, with AMORProcess substantially outperforming the other variants. ", "page_idx": 8}, {"type": "text", "text": "RQ2: Data efficiency for adaptation. We aim to compare the data efficiency of different feedback types for adaptation in terms of the number of exploratory instances required. To this end, we adjust the exploration steps $T$ in Algorithm 2, selecting values at intervals of 200, ranging up to 2,000 steps on HotpotQA. Appendix B.5 further discusses the cases with $I>1$ in Algorithm 2 where AMOR is optimized over multiple rounds. ", "page_idx": 8}, {"type": "text", "text": "Figure 3 shows the post-adaptation performance of AMOR varying with the number of exploratory instances (i.e., $T$ ). Compared to AMOROutcome, AMORProcess requires significantly fewer exploration steps to achieve compa", "page_idx": 8}, {"type": "image", "img_path": "jImXgQEmX3/tmp/6148608faeecbc5443ca3348ffecd159fd14165829ed950aff63d0b4f2ef9ca2.jpg", "img_caption": ["Figure 3: EM/F1 on HotpotQA varying with the number of exploratory instances for adaptation. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "rable performance. Notably, AMOROutcome shows a marked decline in performance when exposed to a limited number of exploratory instances $\\phantom{\\rule{0ex}{0ex}}\\!\\!<800\\phantom{\\rule{0ex}{0ex}}$ , suggesting a reduced adaptability in explorationlimited scenarios. Conversely, AMORProcess\u2019s robust performance under such constraints highlights its superior adaptability and efficiency with minimal interaction. ", "page_idx": 8}, {"type": "text", "text": "RQ3: Adaptation through human feedback Due to limited resources, we use automatically annotated silver feedback as a proxy for gold human feedback in our main experiments. We would like to emphasize that our experimental framework is inherently designed to seamlessly incorporate human feedback in place of its automated counterpart. To illustrate this, we carry out a human study to demonstrate how AMOR is adapted through human feedback on HotpotQA. For this study, we hire an NLP expert to provide human feedback for each module within AMORWFT on 2,000 exploratory instances following the annotation strategy in Appendix B.3. Table 7 shows the adaptation results using the collected human feedback. ", "page_idx": 8}, {"type": "table", "img_path": "jImXgQEmX3/tmp/414e9e40b4a560bfaa8421a2b274060aaeee34b3b26b5b25a4cf3abd41e48aef.jpg", "table_caption": ["Table 7: Adaptation results of AMOR through human feedback on HotpotQA based on L-7B. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "jImXgQEmX3/tmp/442ef0ab6520358256da07712c2e542066e640f804b3e0a97f2683f7b3a73171.jpg", "table_caption": ["Table 8: Accuracy of four LLM modules. All AMOR variants are based on L-7B. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The results distinctly suggest: human feedback more effectively adapts AMOR to specific knowledge environments than automatic feedback. Our study lays a robust groundwork for the practical deployment and real-world utilization of the AMOR framework. ", "page_idx": 9}, {"type": "text", "text": "RQ4: Reasoning process assessment. To measure the accuracy of AMOR\u2019s reasoning process, we performed a human study on HotpotQA, which involved: (1) selecting 50 random questions; (2) manually annotating the gold feedback $f_{\\mathrm{human}}$ for each LLM module output the following instructions using the same annotation protocol in Appendix B.3; and (3) calculating the accuracy of each LLM module output based on $f_{\\mathrm{human}}$ (1/0 indicating \u201cright/wrong\u201c). ", "page_idx": 9}, {"type": "text", "text": "Table 8 presents the accuracy of AMOR variants, affirming RQ1\u2019s findings: process feedback significantly improves the reasoning process over AMORWFT that lacks adaptation, while outcome feedback has a negligible effect. Moreover, $\\operatorname{AMOR}_{\\operatorname{Process}}$ relatively lags in the Decompose and Complete modules, hinting that future enhancements could focus on including more corresponding data during two fine-tuning stages. ", "page_idx": 9}, {"type": "text", "text": "4.4 Case study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Appendix B.8 presents several examples to further illustrate AMOR\u2019s strengths in reasoning logic and intervenability, as well as the limitations of relying on outcome feedback for adaptation, emphasizing the crucial role of process feedback. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we develop AMOR, an adaptable modular agent designed for knowledge-intensive tasks, featuring FSM-based reasoning logic and a process feedback mechanism. Based on opensource LLMs, AMOR undergoes a two-stage fine-tuning: initial warm-up to generalize across task environments and subsequent domain-specific adaptation through process feedback. Extensive experiments demonstrate AMOR\u2019s advantages over strong baselines across multiple domains. Further discussions highlight the effectiveness and efficiency of process feedback in adaptation. compared to previous agents. Future work will explore extending our paradigm to more knowledge types (e.g., structured knowledge bases) and broader agent tasks, ultimately empowering LLMs to autonomously design FSM-based reasoning logic on top of our paradigm. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers and area chairs for their valuable feedback and insightful comments that helped improve this work. This work was supported by the NSFC projects(Key project with No. 61936010). This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024.   \n[3] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17682\u201317690, Mar. 2024.   \n[4] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023.   \n[5] Chuanqi Cheng, Jian Guan, Wei Wu, and Rui Yan. From the least to the most: Building a plug-and-play visual reasoner via data synthesis. arXiv preprint arXiv:2406.19934, 2024.   \n[6] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, 2019.   \n[7] Edmund M Clarke, Orna Grumberg, and Michael C Browne. Reasoning about networks with many identical finite-state processes. In Proceedings of the fifth annual ACM symposium on Principles of distributed computing, pages 240\u2013248, 1986.   \n[8] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, 2021.   \n[9] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model alignment as prospect theoretic optimization. In Forty-first International Conference on Machine Learning, 2024.   \n[10] Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, and Hao Peng. Language models hallucinate, but may excel at fact verification. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1090\u20131111, Mexico City, Mexico, June 2024. Association for Computational Linguistics.   \n[11] Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. In The Twelfth International Conference on Learning Representations, 2024.   \n[12] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929\u20133938. PMLR, 2020.   \n[13] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, 2020.   \n[14] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and J\u00fcrgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024.   \n[15] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022.   \n[16] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969\u20137992, Singapore, December 2023. Association for Computational Linguistics.   \n[17] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567\u20132577, 2019.   \n[18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769\u20136781, Online, November 2020. Association for Computational Linguistics.   \n[19] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations, 2023.   \n[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019.   \n[21] David Lee and Mihalis Yannakakis. Principles and methods of testing finite state machines-a survey. Proceedings of the IEEE, 84(8):1090\u20131123, 1996.   \n[22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.   \n[23] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.   \n[24] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157\u2013173, 2024.   \n[25] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating LLMs as agents. In The Twelfth International Conference on Learning Representations, 2024.   \n[26] Ziyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, and Jieyu Zhao. Score: A framework for self-contradictory reasoning evaluation. arXiv preprint arXiv:2311.09603, 2023.   \n[27] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.   \n[28] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687\u20135711, Singapore, December 2023. Association for Computational Linguistics.   \n[29] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. ToolLLM: Facilitating large language models to master $16000+$ real-world APIs. In The Twelfth International Conference on Learning Representations, 2024.   \n[30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[31] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.   \n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017.   \n[33] Noam Shazeer, \\*Azalia Mirhoseini, \\*Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. In International Conference on Learning Representations, 2017.   \n[34] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Sch\u00e4rli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 31210\u201331227. PMLR, 23\u201329 Jul 2023.   \n[35] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[37] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539\u2013554, 2022.   \n[38] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10014\u201310037, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[39] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.   \n[40] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023.   \n[41] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023.   \n[42] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. Survey Certification.   \n[43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[45] Jiaxin Wen, Jian Guan, Hongning Wang, Wei Wu, and Minlie Huang. Codeplan: Unlocking reasoning potential in large langauge models by scaling code-form planning, 2024.   \n[46] Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. Rewoo: Decoupling reasoning from observations for efficient augmented language models. arXiv preprint arXiv:2305.18323, 2023.   \n[47] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation. In Forty-first International Conference on Machine Learning, 2024.   \n[48] Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi, et al. In-context learning with retrieved demonstrations for language models: A survey. arXiv preprint arXiv:2401.11624, 2024.   \n[49] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, 2018.   \n[50] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023.   \n[52] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Agent lumos: Unified and modular training for open-source language agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12380\u201312403, Bangkok, Thailand, August 2024. Association for Computational Linguistics.   \n[53] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Methodology ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Comparing AMOR with related works in detail ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As illustrated in Figure 4, AMOR addresses the issues of prior reasoning methods in terms of three aspects: ", "page_idx": 14}, {"type": "text", "text": "\u2022 AMOR is equipped with a controllable FSM-based reasoning logic with a stronger capacity for handling complex tasks than simple pipelines employed by Self-RAG, ReWoo, LUMOS. For instance, if no relevant passages are retrieved from a document, AMOR can dynamically transit to the next document, while LUMOS would be constrained to generate answers based on the irrelevant passages, potentially leading to incorrect or low-quality outputs.   \n\u2022 AMOR adapts to new environments through exploration and exploitation. AMOR is designed with an adaptation fine-tuning stage, enabling it to adapt effectively to specific domains based on human feedback. This adaptive mechanism sets AMOR apart from prior modular agents that lack the ability to incorporate expert guidance.   \n\u2022 AMOR enables humans to conveniently and effectively intervene and provide feedback at each reasoning step. AMOR introduces a process feedback mechanism that enables humans to provide direct feedback on the individual modules within the FSM-based reasoning process. This approach facilitates a more natural and interpretable form of supervision, allowing for targeted improvements and fine-tuning of specific reasoning components. ", "page_idx": 14}, {"type": "text", "text": "In summary, AMOR achieves more controllable, adaptable, and human-guided reasoning capabilities compared to existing methods. ", "page_idx": 14}, {"type": "image", "img_path": "jImXgQEmX3/tmp/9919d6d56e6b6b052ca89d9adf150d48ad6ad651be4627e0ce97d9417f323e25.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 4: Left: Elaboration regarding the advantages and drawbacks when comparing AMOR with prior agents in terms of three aspects in Table 1. Right: The reasoning processes of AMOR and related works. ", "page_idx": 14}, {"type": "text", "text": "A.2 Full algorithm of AMOR ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 2 in the main paper illustrates a general FSM-based reasoning logic, which can be adapted to various agent environments by defining the FSM including the states, modules, etc. ", "page_idx": 14}, {"type": "text", "text": "As shown in Algorithm 3, AMOR provides an instantiation of the FSM-based reasoning logic for the knowledge-seeking scenarios following the state transition diagram in Figure 1 in the main paper. We expect to extend this work to more environments in the future. ", "page_idx": 14}, {"type": "text", "text": "A.3 Prompts for LLM modules ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 10, 11, 12 and 13 show the prompts for four LLM modules in AMOR under the \u201cWithout Fine-tuning\u201d setting on HotpotQA. They can be converted to the \u201cWith Fine-tuning\u201d setting by removing the in-context examples. The prompts for PubMedQA and QASPER are similar. ", "page_idx": 14}, {"type": "text", "text": "Data: AMOR at the initial state $s=s_{0}$ $(Q,H,E)$ ; $Q$ : Question; $H=[\\,]$ : All solved sub-queries and answers; ", "page_idx": 15}, {"type": "text", "text": "$E=[\\,]$ : All evidence passages collected by AMOR.   \nResult: $A$ : Final Answer; $R$ : Reasoning Process.   \n1 while True do   \n2 if $s=s_{0}$ then   \n3 $\\boldsymbol{y}=\\mathsf{D e c o m p o s e}(s.Q,s.H)$   \n4 $R$ .append({\u201cstate\u201d: s, \u201coutput\u201d: $y]$ )   \n$/\\,/$ Transit to the next state.   \n5 if $y$ starts with \u201c[NEXT]\u201d then   \n6 Extract the next sub-query $q$ from $y$   \n7 $s=s_{1}(s.Q,s.H,s.\\dot{E},q)$   \n8 else if $y$ starts with \u201c[FINISH]\u201d then   \n9 $s=s_{6}(s.Q,s.E)$   \n10 else if $s=s_{1}$ then   \n11 $y=$ SearchDoc(s.q)   \n// Transit to the next state.   \n12 $\\begin{array}{l}{D,d=[y],y}\\\\ {s=s_{2}(s.Q,s.H,s.E,s.q,D,d)}\\end{array}$   \n13   \n14 else if $s=s_{2}$ then   \n15 $\\boldsymbol{y}=\\mathsf{J u d g e}(s.Q,s.H,s.q,s.d)$   \n16 R.append( $\\{$ \u201cstate\u201d: s, \u201coutput\u201d: $y\\}$ )   \n$/\\,/$ Transit to the next state.   \n17 if $y$ starts with \u201c[IRRELEVANT]\u201d then   \n18 $s=s_{3}(s.Q,s.H,s.E,s.q,s.D)$   \n19 else if $y$ starts with \u201c[RELEVANT]\u201d then   \n20 $s=s_{4}(s.Q,s.H,s.E,s.q,s.D,s.d)$   \n21 else if $s=s_{3}$ then   \n22 $y={\\mathsf{N e x t D o c}}()$   \n// Transit to the next state.   \n23 if $d$ is NONE then   \n24 $\\begin{array}{r l}&{H=s.H+\\left[\\left(s.q,{^{\\ast}\\mathrm{No}}\\;\\mathrm{Answer}^{\\ast}\\right)\\right]}\\\\ &{E=s.E+\\left[s.D[0]\\right]}\\\\ &{s=s_{0}(s.Q,H,E)}\\end{array}$   \n25   \n26   \n27 else   \n28 $\\begin{array}{l}{{D,d=s.D+[y],y}}\\\\ {{s=s_{2}(s.Q,s.H,s.E,s.q,D,d)}}\\end{array}$   \n29   \n30 else if $s=s_{4}$ then   \n31 $\\boldsymbol{y}=\\mathsf{S e a r c h P s g}(s.q,s.d)$   \n// Transit to the next state.   \n32 $\\begin{array}{l}{P=y}\\\\ {s=s_{5}(s.Q,s.H,s.E,s.q,s.D,P)}\\end{array}$   \n33   \n34 else if $s=s_{5}$ then   \n35 $\\boldsymbol{y}=\\mathsf{A n s w e r}(Q,H,q,P)$   \n36 R.append( $\\{$ \u201cstate\u201d: s, \u201coutput\u201d: $y\\}$ )   \n$/\\,/$ Transit to the next state.   \n37 if $o$ starts with \u201c[UNANSWERABLE]\u201d then   \n38 $s=s_{3}(s.Q,s.H,s.E,s.q,s.D)$   \n39 else if $o$ starts with \u201c[ANSWERABLE]\u201d then   \n40 Extract the answer $\\footnote{T w o t y p i c a l a p p l i c a t i o n s c e n a r i o s f o r t h e p r o p o s e d s y s t e m a r e h e a l t h c a r e,a n d l o g i s t i c s a n d w a r e h o u s i n g,i n w h i c h m u l t i p l e I o T d e v i c e s a r e d e p l o y e d c l o s e t o t h e r e c e i v e r a n d t h e t i m e d e l a y b e t w e e n t h e d i r e c t l i n k a n d b a c k s c a t t e r l i n k i s t h u s n e g l i g i b l e.}$ and the evidence $p$ from $y$   \n41 $\\begin{array}{l}{H=s.H+[s.q,a]}\\\\ {E=s.E+[e]}\\\\ {s=s_{0}(s.Q,H,E)}\\end{array}$   \n42   \n43   \n44 else if $s=s_{6}$ then   \n45 y =Complete(s.Q, s.E)   \n46 R.append({\u201cstate\u201d: s, \u201coutput\u201d: y})   \n47 A = y // Reach the final state.   \n48 break   \n49 return $A,R$ ", "page_idx": 15}, {"type": "table", "img_path": "jImXgQEmX3/tmp/63fd34a858e5666d22f39c92d25cce98a87e4d8d85e0eeca0ed4b94fdff676c8.jpg", "table_caption": ["Table 9: Statistics of the warm-up data. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.4 Construction of warm-up examples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we elaborate the pipeline to collect training examples for the warm-up stage of AMOR. Given a sample question $Q$ with annotations of the final answer $\\hat{A}$ , all sub-queries and answers $\\hat{H}=[(\\hat{q}_{j},\\hat{a}_{j})]_{j=0}^{J-\\bar{1}}$ , and all evidence passages $\\hat{E}=[\\hat{e}_{j}]_{j=0}^{J-1}$ , where $J$ is the number of necessitated sub-queries of $Q$ , we construct training examples for four LLM modules of AMOR as follows: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Decompose $(Q,H)$ : We construct a total of $J+1$ training examples for this module. For each of the $J$ sub-queries, we create an example with the main question $Q$ and the preceding sub-queries and answers $H=\\hat{H}_{<j}$ as the input, and the next sub-query $\\hat{q}_{j}$ coupled with the branch token \u201c[NEXT]\u201d as the output (for $j=0,1,\\dotsc,J-1)$ . Here, $\\hat{H}_{<j}$ denotes the sequence containing the first $j$ pairs of sub-queries and their corresponding answers from $\\hat{H}$ . Additionally, we create one example where the input includes $Q$ and the complete set of sub-queries and answers $H={\\hat{H}}$ , with the branch token \u201c[FINISH]\u201d as the output, indicating the end of the decomposition. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Judge $(Q,H,q,d)$ : For this module, the input consists of the main question $Q$ , the previous sub-queries and answers $H\\;=\\;\\hat{H}_{<j}$ , the current sub-query $q\\ =\\ \\hat{q}_{j}$ , and a document snippet $d$ (for $j=0,1,\\cdot\\cdot\\cdot,J-1)$ . The output is a branch token that classifies the snippet $d$ as either \u201c[RELEVANT]\u201d or \u201c[IRRELEVANT]\u201d in relation to the current sub-query ${\\hat{q}}_{j}$ . We consider three scenarios for the document snippet $d$ : (1) When $d$ is the gold evidence passage $\\hat{e}_{j}$ , the output is \u201c[RELEVANT]\u201d. (2) When $d$ is a passage from a different document from $\\hat{e}_{j}$ , it is marked as \u201c[IRRELEVANT]\u201d. We obtain this type of snippet, denoted as $d_{j}$ , by using ${\\hat{q}}_{j}$ as the query in SearchDoc, ensuring it originates from a distinct document compared to $\\hat{e}_{j}$ . (3) When $d$ is a passage from the same document as $\\hat{e}_{j}$ but is not $\\hat{e}_{j}$ itself, it is deemed \u201c[RELEVANT]\u201d. We acquire such snippets by invoking SearchPsg with ${\\hat{q}}_{j}$ to retrieve passages from the same document as $\\hat{e}_{j}$ , excluding $\\hat{e}_{j}$ from the results. We refer to this set of passages as $P^{-}$ , considering each of them relevant to $\\hat{q}_{j}$ . These varied document snippet scenarios are designed to train the module to discern the relevance of a query to a document based solely on portions of the document content. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Answer $(Q,H,q,P)$ . Similar to the Judge module, the input for this module comprises the main question $Q$ , the previous sub-queries and answers $H=\\hat{H}_{<j}$ , the current sub-query $q=\\hat{q}_{j}$ , and a set of passages $P$ from the same document. The output is either the branch token [UNANSWERABLE]\u201d or a combination of the branch token [ANSWERABLE]\u201d, the corresponding answer $\\hat{a}_{j}$ , and evidence passage $\\hat{e}_{j}$ . We consider two scenarios for $P$ : (1) When $P$ does not include $\\hat{e}_{j}$ , indicating that the sub-query ${\\hat{q}}_{j}$ cannot be answered, the output is \u201c[UNANSWERABLE]\u201d. Here, $P$ is set to the previously mentioned set $P^{-}$ . (2) When $P$ includes $\\hat{e}_{j}$ , suggesting that $\\hat{q}_{j}$ is answerable, we create $P$ by replacing a random passage in $P^{-}$ with $\\hat{e}_{j}$ . For both scenarios, we present the passages to the module in random order when constructing training examples. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Complete $(Q,E)$ . We construct one training example for this module by setting the input to the main question $Q$ and gold evidence E\u02c6 and the output to the final answer A\u02c6. ", "page_idx": 16}, {"type": "text", "text": "After generating examples from the warm-up datasets using the aforementioned pipeline, we randomly select a specified number of examples. This random sampling aims to ensure a balanced representation of the various modules and branch tokens in the final dataset. Table 9 shows the detailed statistics of the warm-up data. ", "page_idx": 16}, {"type": "table", "img_path": "jImXgQEmX3/tmp/bae7700607b56105c26b27540dc734dfe4fc11b0b4de4a7ee5df3288c6d9e9f6.jpg", "table_caption": ["Table 10: Prompt for the Decompose module for HotpotQA. ", "Table 11: Prompt for the Judge module for HotpotQA. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "jImXgQEmX3/tmp/366b59d26a0c2af258d18ea2791e927a3c85514d2629a5f7a08afc07f20525a9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Tool modules ", "page_idx": 17}, {"type": "text", "text": "Tool implementation in AMOR. We implement both SearchDoc and SearchPsg by adapting Contriever. Given a query, SearchDoc first uses Contriver to retrieve a number of passages from a specific knowledge base and only retains the most relevant passage from each document to serve as the document\u2019s representative snippet. Then, SearchDoc returns the top one document snippet and NextDoc can return at most nine more snippets from the remaining ones. On the other hand, SearchPsg returns the top three passages within a given document retrieved using Contriever. ", "page_idx": 17}, {"type": "image", "img_path": "jImXgQEmX3/tmp/9cb115d23fab909f2ac9f0a57d9e3542261fc043628cd4c2469d3db9a6edec48.jpg", "img_caption": ["Table 12: Prompt for the Answer module for HotpotQA. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "jImXgQEmX3/tmp/8bc53ee0a806530f1bb8ed0e0fa829f4b3a63cdb604a215030eb3a5faa085ea1.jpg", "img_caption": ["Table 13: Prompt for the Complete module for HotpotQA. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "The operation of these tools mirrors the hierarchical interaction paradigm that humans use with search engines [51, 52]: they first identify a relevant document based on short snippets and then refine the search results by focusing within the document. ", "page_idx": 18}, {"type": "text", "text": "Performance comparison across different tool implementations. On PubMedQA and QASPER, we implement the tools of both AMOR and all baselines using Contriever for a fair comparison. However, for the HotpotQA dataset, the baselines used various approaches for tool implementation, and Table 4 in the main paper directly shows the performance reported in their original papers. To bridge the gap between AMOR and baselines in tool implementations, we conduct experiments to investigate how AMOR performs when using the same tool implementation as two representative baselines, including LUMOS and AgentLM4, respectively. Please kindly note that LUMOS utilizes GPT-3.5 as a tool for answering questions based on retrieved passages. Therefore, we utilize CPT3.5 to implement the Complete module when comparing AMOR with LUMOS. Furthermore, the adaption to tool implementations of LUMOS and AgentLM necessitates several adjustments in AMOR, including (1) When using \u201cWikipedia API\u201d to implement SearchDoc, the Decompose module should identify entity names for SearchDoc. (2) When using \u201cExact Keyword Match\u201d to implement SearchPsg, the Decompose module should predict keywords for SearchPsg. (3) We craft elaborate rules to create warm-up data that follow the above formats for the Decompose module. For example, we use the title of the gold passage as the target entity name for a sub-query and use the longest common string between each sub-query and the corresponding gold passage as the target keyword for \u201cExact Keyword Match.\u201d ", "page_idx": 18}, {"type": "table", "img_path": "jImXgQEmX3/tmp/c727b2811b9d6c93b0d4a1d9508791fce4047d3b21165b9869c93ccac0bbeb20.jpg", "table_caption": ["Table 14: Comparison results between AMOR and several representative baselines based on L-7B using the same tools on HotpotQA. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "jImXgQEmX3/tmp/e45f9d3bcfc8928d358219aceb6ee68b93b8a3d32a7aa10df68a3b4e427658f6.jpg", "table_caption": ["Table 15: Datasets for adaptation and evaluation. Avg. Len refers to the average length of passages in the corresponding knowledge base, counted by the GPT tokenizer [30]. Val is the validation set. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "As demonstrated in Table 14, AMORWFT, without being fine-tuned on HotpotQA, surpasses both LUMOS and AgentLM in performance by employing identical tool implementations. Moreover, AMORProcess, upon fine-tuning with process feedback on HotpotQA, exhibits substantial and significant enhancements in performance. These outcomes collectively underscore AMOR\u2019s superior performance compared to baseline models and its robustness and adaptability across various tool implementations. ", "page_idx": 19}, {"type": "text", "text": "B.2 Adaptation & evaluation datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We describe how we process the datasets as follows: (1) HotpotQA: Each document is a Wikipedia article. Since the original test set is hidden, we randomly sample 500 examples from the original validation set for evaluation and split the training set for adaptation fine-tuning and validation. (2) PubMedQA [17]: We follow the official split. And we only remain examples whose answers are \u201cyes\u201d or \u201cno\u201d and discard those labeled \u201cmaybe.\u201d (3) QASPER [8]: For each question, we regard the corresponding paper as a knowledge base and each section of the paper as a document with the section name as the title (e.g., \u201cExperiments::Datasets\u201d) including several passages. Although many LLMs support context longer than the average paper length of $7\\mathbf{k}$ tokens, we focus on testing the ability of language agents to seek and utilize information in this work. We exclude questions that are labeled \u201cunanswerable.\u201d Since the original test set is also hidden, we use the original validation set for evaluation and redivide the training set for training and validation. Table 15 shows the statistics of the three datasets. ", "page_idx": 19}, {"type": "text", "text": "B.3 Reasoning process assessment ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To investigate the extent to which the adaptation stage enhances AMOR\u2019s reasoning process, we conducted a human study with one NLP expert using the HotpotQA test set, Table 16 demonstrates the protocol for annotating the gold feedback $f_{\\mathrm{human}}$ and then we calculate the accuracy of the ", "page_idx": 19}, {"type": "table", "img_path": "jImXgQEmX3/tmp/c579f2d4162cc3360df3f41df22186b8e7cbd999d912767860918dcb05f2af06.jpg", "table_caption": ["Table 16: Manual annotation strategy for gold process feedback for different LLM modules. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "jImXgQEmX3/tmp/8b959a330d1330e12bd5efd84277e411db76bf0174bc87e20e1c957fe955aa53.jpg", "table_caption": ["Table 17: Accuracy of the silver feedback for four LLM modules based on L-7B. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "jImXgQEmX3/tmp/00b676c4dc39c99436f60a388f8caa5c09ad0d3f4bd3c656a57da4c355381ab6.jpg", "table_caption": ["Table 18: Proportion of cases where the corresponding error exists. All agents are based on L-7B. N/A means the LUMOS agent does not explicitly execute the relevance judgment step. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "automatic silver feedback $f$ by comparing it to the gold human feedback. Based on $f_{\\mathrm{human}}$ , we measured the accuracy of each LLM module\u2019s output $y$ (denoted as $\\mathrm{ACC}_{m}.$ ) as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{ACC}_{m}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if~}f_{\\mathrm{human}}=\\mathrm{^*_{right}}\\mathrm{,}}\\\\ {\\,}\\\\ {1}&{\\mathrm{if~}f_{\\mathrm{human}}\\mathrm{~is~a~refinement~of~}y\\mathrm{~and~}f_{\\mathrm{human}}=y,}\\\\ {\\,}\\\\ {0}&{\\mathrm{if~}f_{\\mathrm{human}}=\\mathrm{^*_{wrong}}\\mathrm{,}}\\\\ {\\,}\\\\ {0}&{\\mathrm{if~}f_{\\mathrm{human}}\\mathrm{~is~a~refinement~of~}y\\mathrm{~and~}f_{\\mathrm{human}}\\neq y.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The accuracy of the reasoning process $\\mathrm{ACC}_{m}$ has been discussed in Table 8 of the main paper. Furthermore, Table 17 presents the accuracy of the silver feedback $\\mathrm{ACC}_{f}$ for $\\operatorname{AMOR}_{\\operatorname{Process}}$ . The silver feedback achieves an $\\mathrm{ACC}_{f}$ above $80\\%$ for all modules, lending credibility to the use of silver feedback in the adaptation experiments. ", "page_idx": 20}, {"type": "text", "text": "B.4 Error analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 8 of the main paper has presented a detailed analysis of the accuracy of each module within AMOR on HotpotQA, which suggests the relative weakness of AMOR in question decomposition and task completion. We further conduct a comprehensive error analysis by manually examining the error proportion of different agents. We summarize six error types and show the results of manual annotation in Table 18. ", "page_idx": 20}, {"type": "text", "text": "Our thorough analysis accentuates the specific strengths and weaknesses of different agents, unequivocally demonstrating AMOR\u2019s relative improvements across key aspects of the complex reasoning process. ", "page_idx": 20}, {"type": "text", "text": "Furthermore, we would like to emphasize a significant advantage of AMOR: the ease of diagnosing errors by examining the outputs of its modular architecture. Unlike other agents, where errors are ", "page_idx": 20}, {"type": "text", "text": "Table 19: Performance of AMOR parameterized by $\\theta_{i}$ during multi-round adaptation. In the $i$ -th iteration $(i=0,1,2)$ , $\\mathrm{AMOR}_{\\theta_{i}}$ is used to explore over the same set of questions or different ones and then is updated to A $\\mathrm{\\bf~\\Lambda}_{\\mathrm{MOR}_{\\theta_{i+1}}}$ based on the exploratory instances. ", "page_idx": 21}, {"type": "table", "img_path": "jImXgQEmX3/tmp/d0f7a6a3f7661263bd08e344c2725242b8e9b6570e4eae985997dc10d8f4c729.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "jImXgQEmX3/tmp/40656213293208959b219fdd3ca289c6922179e06a34e0c5b03128bc483ab924.jpg", "table_caption": ["Table 20: Average step/token numbers of different agents. For ReAct and AgentLM, a step refers to a \u201cThought,\u201d \u201cAction,\u201d or \u201cObservation\u201d step. For AMOR, a step means a reasoning step within a certain module. And tokens count both input and output tokens. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "often intertwined and obscure (see a ReAct example in Figure 6), AMOR facilitates the provision of precise, module-specific feedback. We will add the above analysis in our revision. ", "page_idx": 21}, {"type": "text", "text": "B.5 Multi-round adaptation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the main paper, we set $I=1$ in Algorithm 2 for all experiments, which means that all exploratory instances in the adaptation stage are induced by the warm-up policy AMORWFT. We call this setting \u201csingle-round adaptation.\u201d We are curious about how multi-round adaptation influences the performance of AMOR by adjusting $I$ . For the $i$ -th iteration $(i=1,2,\\cdots\\,,I)$ , we denote the initial parameter of AMOR as $\\theta_{i-1}$ , which is used to explore over a set of input questions and is updated to $\\theta_{i}$ after exploitation using these exploratory instances. $\\mathrm{AMOR}_{\\theta_{0}}$ is exactly AMORWFT. During different iterations, we can provide either the same or different questions for AMOR to explore over. The case with the same set of questions is used to simulate an exploration-limited scenario. Note that in this case, the exploratory instances with the same questions are still different due to the ever-changing policy leading to different outputs. ", "page_idx": 21}, {"type": "text", "text": "Table 19 shows the performance of AMOR under the multi-round adaptation setting with $I=3$ . We find that the performance is almost unchanged whether using the same or different input questions for each adaptation round. This result suggests that one iteration may be sufficient for the adaptation fine-tuning stage in our study. ", "page_idx": 21}, {"type": "text", "text": "B.6 Token efficiency ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Language agents interact with environments to solve problems through frequent calls of LLMs, leading to huge costs in terms of token consumption. Building agents with minimal token usage is essential for curbing deployment costs [46]. ", "page_idx": 21}, {"type": "text", "text": "Table 20 displays the average number of steps and tokens used by AMOR and ReAct-style agents to answer a question. ReAct-style agents, lacking explicit modeling of inter-step dependencies, require the inclusion of all preceding information in the input for each step. This often results in undesired redundancy. In contrast, AMOR consumes significantly fewer tokens with each module relying only on essential historical information, which highlights the token efficiency of its architecture. When built upon GPT-4, $\\mathrm{AMOR}_{\\mathrm{w/o~FT}}$ uses fewer steps but more tokens than AMORProcess based on L-7B due to the additional in-context examples inserted into the prompts of GPT-4. ", "page_idx": 21}, {"type": "table", "img_path": "jImXgQEmX3/tmp/9591b877b16bbbba52a73f21f293bb9116730e9488736dfd35bf4316272b6780.jpg", "table_caption": ["Table 21: AMOR can be enhanced through targeted fine-tuning and flexibly accommodate additional tools. All results are based on L-7B. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "B.7 Flexibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "FSM-based reasoning logic is flexible in facilitating targeted enhancements of specific modules and easily accommodating new tools. We conduct two experiments as follows on HotpotQA to demonstrate the flexibility of AMOR, with results shown in Table 21. ", "page_idx": 22}, {"type": "text", "text": "(1) Targeted fine-tuning. Table 8 reveals that the Complete module of $\\mathrm{AMOR}_{\\mathrm{Process}}$ still falls short in performance, achieving only $\\sim50\\%$ accuracy. We construct 6k examples for the module from the original training set of HotpotQA by treating the final answer $\\hat{A}$ as input, and the question $Q$ and evidence passages $\\hat{E}$ as output, and then fine-tune the L-7B model on the data. Table 21 shows the performance gains when substituting the original Complete module in AMORProcess with this individually fine-tuned L-7B model. ", "page_idx": 22}, {"type": "text", "text": "(2) Accommodating new tools. Numerous studies have demonstrated the benefits of retrievalbased in-context learning, where a retriever selectively curates tailored demonstrations for each specific input [48]. We implement this by inserting a new state $s_{6}^{\\prime}$ , named \u201cDemonstration Retrieval,\u201d before the final state $s_{6}$ shown in the state transition diagram in Figure 1, making AMOR reach $s_{6}^{\\prime}$ when Decompose outputs \u201c[FINISH]\u201d at state $s_{0}$ . The new state $s_{6}^{\\prime}$ holds two variables, including the main question $Q$ and all collected evidence $E$ , and employs a tool module SearchDemo to retrieve the top $K$ similar questions to $Q$ from an external demonstration memory, along with their answers and evidence, collectively noted as $\\boldsymbol{K}=[Q_{k},\\hat{A}_{k},\\hat{E}_{k}]_{k=1}^{K}$ . Subsequently, at state $s_{6}$ , the Complete module takes as the in-context examples, helping generate the final answer given $Q$ and $E$ . We use the HotpotQA training set as our demonstration memory and employ Contriever-MS MARCO [15] to implement the SearchDemo module, setting $K$ to 5. We fine-tune the L-7B model on the training set to act as the Complete module while ensuring that the demonstration does not include the target question. As Table21 indicates, this integration of such an additional tool further improves $\\operatorname{AMOR}_{\\operatorname{Process}}$ with targeted fine-tuning. ", "page_idx": 22}, {"type": "text", "text": "Additionally, AMOR\u2019s reasoning logic can be easily expanded from single-path to multi-path reasoning, akin to the approaches used in Self-Consistency [41], ToT [50], and GoT [3]. This can be achieved by generating multiple outputs within specific modules and incorporating modules that synthesize these multi-path results. Consequently, we advocate for the adoption of the FSM paradigm in the design of future agents. This framework offers the dual beneftis of flexibility and the capacity to adapt agents based on process feedback. ", "page_idx": 22}, {"type": "text", "text": "B.8 Case study ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We annotate the silver outcome feedback $f_{o}$ for the Complete module at the final state as $\\hat{A}$ if all gold evidence passages are successfully collected (i.e., $\\hat{E}\\subseteq E)$ , and \u201cwrong\u201d otherwise. ", "page_idx": 22}, {"type": "text", "text": "We demonstrate the advantages of the FSM-based reasoning logic and process feedback mechanism through the comparison between AMORProcess and ReAct in Figure 5 and 6, respectively. We observe that ReAct without explicit reasoning logic constraints fails to decompose the question and terminates retrieval prematurely in \u201cThought/Action 5.\u201d Besides, ReAct also mixes right and wrong steps in \u201cThought 2/4/5,\u201d making it challenging for users to critique and improve the agent in a targeted manner. In contrast, AMOR successfully answers the question with a controllable reasoning logic and allows direct process feedback to drive the evolution. ", "page_idx": 22}, {"type": "text", "text": "Additionally, Table 22 shows a case where AMORWFT correctly answers a question with the right evidence, yet employs a wrong reasoning process. This underscores the potential unreliability of using ", "page_idx": 22}, {"type": "image", "img_path": "jImXgQEmX3/tmp/a1343f48b5caaf92213524c17acabdbd7583094aef58e7bbbaa9e9b31dcb4252.jpg", "img_caption": ["Figure 5: An example demonstrating how AMORProcess answers a complex question from HotpotQA. Users are allowed to provide direct process feedback to drive the evolution of the agent. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "jImXgQEmX3/tmp/3e455ed5339ea826a893dbc1a00737533ea8b617740973580617a1cf90b16cb2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 6: A failure case of ReAct (built upon GPT-3.5) when answering a complex question from HotpotQA by reasoning over retrieved knowledge. The wrong steps are highlighted in red while the right ones are in green. ", "page_idx": 23}, {"type": "text", "text": "Table 22: A case where AMORWFT answers a question from HotpotQA with the right final answer and evidence but a wrong reasoning process. Each line denotes a reasoning step and shows the output of the corresponding module. The steps highlighted in green are right while those in red are wrong. ", "page_idx": 23}, {"type": "image", "img_path": "jImXgQEmX3/tmp/d04e330a2b7073712f227806d963891e5252b5b05d676bb5871c31d7b35e7bbd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "outcome feedback to judge the correctness of the reasoning process and the necessity of employing process feedback for adapting agents to specific environments. ", "page_idx": 23}, {"type": "text", "text": "C Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This study has demonstrated the benefits of two components: (1) explicitly defined FSM-based reasoning logic, and (2) the process feedback mechanism. Nonetheless, a notable limitation must be acknowledged when extending our approach to other tasks. While we have made initial efforts to outline the general principles for crafting the FSM in $\\S3.1$ and show the flexibility of adapting AMOR\u2019s FSM in Appendix B.7, it still requires a human-driven design process. Looking ahead, our future work aims to enable LLMs to autonomously instantiate FSM-based reasoning logic in Algorithm 1 for diverse user tasks, thereby reducing reliance on human design. Furthermore, we believe that the FSM-based reasoning logic makes it easier for humans to supervise LLMs that potentially outperform humans on the task. ", "page_idx": 24}, {"type": "text", "text": "D Broader impacts and safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The innovation introduced through AMOR carries significant potential for both positive and negative societal impacts. ", "page_idx": 24}, {"type": "text", "text": "On the positive side, the ability of AMOR to adapt to diverse knowledge environments and domains through supervised reasoning could lead to advancements in personalized education, healthcare diagnostics, and customer service. Such applications could democratize access to information and expertise, bridging gaps in knowledge and service availability across different regions and socioeconomic groups. ", "page_idx": 24}, {"type": "text", "text": "However, the sophisticated reasoning capability of AMOR also brings about considerations of misuse, such as the generation of disinformation or aiding in the automation of social engineering attacks. Furthermore, if not properly balanced, the tailored knowledge adaptation could unintentionally reinforce biases present in the training data or human feedback, leading to unfair outcomes in decision-making processes that might disproportionately affect marginalized groups. ", "page_idx": 24}, {"type": "text", "text": "To mitigate these concerns, it\u2019s crucial to engage in transparent development and deployment practices, including bias audits and the establishment of ethical guidelines for use. Additionally, mechanisms for detecting and correcting misinformation or biased reasoning paths should be incorporated into the system\u2019s design. ", "page_idx": 24}, {"type": "text", "text": "Given the potential for misuse inherent in powerful language models like AMOR, it\u2019s vital to implement safeguards. AMOR will be made available under a framework that requires users to agree to ethical usage guidelines before access. Our intention is to maximize AMOR\u2019s societal benefits while curtailing the potential for negative impacts, thus ensuring responsible deployment and use of this advanced agent framework. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We propose a general framework for building knowledge agents, featuring FSM-based reasoning logic and a process feedback mechanism. We focus on text corpora as knowledge bases, but the approach can be flexibly extended to other knowledge types and user tasks by customizing the modules and dependencies within the FSM framework ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Please see Appendix Section C ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Please see Section 4.1 ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please see the supplementary files. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see Section 4.1 ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Please see Section 4.2 ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 27}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Please see Section 4.1 ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We have read and conformed with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Please see Appendix Section D ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is ", "page_idx": 28}, {"type": "text", "text": "being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Please see Appendix Section D. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Please see Section 4.1 ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Please see Supplementary File ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We hire an NLP expert for manual annotation and pay him $\\mathbb{S}7.25/\\mathrm{h}$ . Please see Table 16 for detailed annotation instruction. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: IRB is not applicable to our country. But all potential risks were fully disclosed to the participant prior to his involvement in our study. We ensured that the participant provided informed consent, being fully aware of what the research entailed and any risks they might face. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]