[{"figure_path": "jImXgQEmX3/tables/tables_1_1.jpg", "caption": "Table 1: Comparison between AMOR and representative methods for building agents. Appendix A.1 provides a more comprehensive discussion in detail.", "description": "This table compares AMOR with other methods for building language agents, focusing on their reasoning logic, adaptive mechanisms, and feedback mechanisms.  It highlights the differences in reasoning approaches (e.g., tool invoking vs. FSM-based reasoning), how agents adapt to new environments (e.g., imitation learning vs. exploration/exploitation), and what type of feedback is used (e.g., outcome vs. process feedback).  Appendix A.1 provides a more detailed analysis of these differences.", "section": "1 Introduction"}, {"figure_path": "jImXgQEmX3/tables/tables_4_1.jpg", "caption": "Table 1: Comparison between AMOR and representative methods for building agents. Appendix A.1 provides a more comprehensive discussion in detail.", "description": "This table compares AMOR with several other methods for building language agents.  It highlights key differences in reasoning logic (how the agent makes decisions), inter-step dependency (how steps in the reasoning process relate to each other), adaptive mechanism (how the agent adjusts to new situations), and feedback mechanism (how humans can provide feedback to improve the agent).  The table shows that AMOR uses a finite state machine for reasoning, allowing for more control and human intervention, while other methods use less structured approaches.", "section": "1 Introduction"}, {"figure_path": "jImXgQEmX3/tables/tables_5_1.jpg", "caption": "Table 2: Automatic annotation strategy for silver process feedback for different LLM modules.", "description": "This table details the automatic annotation process used to generate silver process feedback for the different LLM modules within the AMOR framework.  For each module (Decompose, Judge, Answer, Complete), it specifies the module's output (y) and the corresponding silver feedback (f) generated based on whether the module's output is correct or needs refinement.  The feedback is determined by comparing the module's output to the ground truth (gold standard). This automatic generation is essential for training the model without requiring manual human annotation for every single instance. The conditions for generating the \"right\" and \"wrong\" feedback are clearly explained for each module's output.", "section": "4 Experiments"}, {"figure_path": "jImXgQEmX3/tables/tables_6_1.jpg", "caption": "Table 3: Refining each module output y to \u1ef9 based on the outcome feedback fo to adapt AMOR, where y denotes converting the binary output y to its opposite label.", "description": "This table describes how the target output and immediate reward are determined for each module in AMOR during adaptation based on outcome feedback.  For Decompose, Judge, and Answer modules, the target output remains the same (\u1ef9k = y) and the reward is 1 if the feedback matches the actual answer (fo = \u00c2), otherwise 0. For the Complete module, the target output changes to the actual answer (\u1ef9k = \u00c2) and reward is 1 only if all the evidence passages are included, otherwise 0.", "section": "4 Experiments"}, {"figure_path": "jImXgQEmX3/tables/tables_7_1.jpg", "caption": "Table 4: Results of AMOR and baselines. \"L-7/13B\" is short for \"LLAMA-2-7/13B-Chat.\" We highlight the best results in bold and underline the second best. Models marked with * are fine-tuned on the target datasets. Results marked with \u2020 are reported in the original paper and those marked with + are reported in [4]. N/A means the method does not apply to the datasets. AMORProcess outperforms baselines under the same setting significantly (p < 0.01, sign test).", "description": "This table presents the experimental results of AMOR and several baselines on three datasets: HotpotQA, PubMedQA, and QASPER.  It shows the performance (EM, F1, and ACC) of each method with and without fine-tuning, using different LLMs (L-7B, GPT-3.5, GPT-4, and L-13B).  The table highlights the significant improvement achieved by AMORProcess, particularly its advantage over other methods when fine-tuned on the target datasets. Statistical significance (p<0.01) is noted for the superior performance of AMORProcess.", "section": "4.2 Main results"}, {"figure_path": "jImXgQEmX3/tables/tables_7_2.jpg", "caption": "Table 5: Results of AMORProcess based on L-7B with different architectures and optimization algorithms. The architecture setting is also applied on the warm-up fine-tuning stage. \u2020 refers to our final method.", "description": "This table presents the results of the AMORProcess model, using the LLAMA-2-7B-Chat language model as a base.  The table compares the performance of four different model architectures (MA-MoE, MA-MoE with Supervised Fine-Tuning, MoE with Supervised Fine-Tuning, and a standard Transformer model with Supervised Fine-Tuning). It shows the performance on three question answering datasets (HotpotQA, PubMedQA, and QASPER) in terms of Exact Match (EM) and F1 scores, and accuracy (ACC) for PubMedQA.  The table highlights the impact of different architectures and optimization techniques on the overall model performance.", "section": "4.2 Main results"}, {"figure_path": "jImXgQEmX3/tables/tables_8_1.jpg", "caption": "Table 4: Results of AMOR and baselines. \"L-7/13B\" is short for \"LLAMA-2-7/13B-Chat.\" We highlight the best results in bold and underline the second best. Models marked with * are fine-tuned on the target datasets. Results marked with \u2020 are reported in the original paper and those marked with + are reported in [4]. N/A means the method does not apply to the datasets. AMORProcess outperforms baselines under the same setting significantly (p < 0.01, sign test).", "description": "This table presents the performance comparison of AMOR and various baseline methods across three question-answering datasets: HotpotQA, PubMedQA, and QASPER.  It shows the performance of different models with and without fine-tuning and highlights the impact of different adaptation strategies (process feedback vs outcome feedback).  The results demonstrate the superiority of AMORProcess across all scenarios.", "section": "4.2 Main results"}, {"figure_path": "jImXgQEmX3/tables/tables_9_1.jpg", "caption": "Table 7: Adaptation results of AMOR through human feedback on HotpotQA based on L-7B.", "description": "This table shows the results of adapting the AMOR model using human feedback on the HotpotQA dataset.  It compares the performance of AMOR using automatic feedback versus human feedback, measured by Exact Match (EM) and F1 scores.  The results highlight the impact of human feedback on the model's performance.", "section": "4 Experiments"}, {"figure_path": "jImXgQEmX3/tables/tables_9_2.jpg", "caption": "Table 8: Accuracy of four LLM modules. All AMOR variants are based on L-7B.", "description": "This table presents the accuracy of each of the four LLM modules (Decompose, Judge, Answer, Complete) within three different versions of the AMOR model: AMORProcess, AMORWFT, and AMOROutcome.  All models are based on the L-7B language model. The table allows for a comparison of the performance of each module under different training and adaptation conditions, highlighting the impact of process-based feedback on model accuracy.", "section": "4 Experiments"}, {"figure_path": "jImXgQEmX3/tables/tables_16_1.jpg", "caption": "Table 9: Statistics of the warm-up data.", "description": "This table shows the number of training examples created for each module and each branch token in the warm-up stage.  The data is categorized by four datasets (2WikiMultiHopQA, Musique, NaturalQuestions, BoolQ) and by module (Decompose, Judge, Answer, Complete). Each module has specific branch tokens indicating the outcome of the module's execution, affecting the next stage in the process. The overall row sums up the total number of training examples generated for each dataset.", "section": "3.2 Warming-up open-source LLMs"}, {"figure_path": "jImXgQEmX3/tables/tables_17_1.jpg", "caption": "Table 1: Comparison between AMOR and representative methods for building agents. Appendix A.1 provides a more comprehensive discussion in detail.", "description": "This table compares AMOR with several other methods for building language agents, focusing on their reasoning logic, inter-step dependency, adaptive mechanism, and feedback mechanism.  It highlights the differences in how these agents approach problem-solving, adapt to new situations, and incorporate human feedback. The table shows that AMOR stands out with its specialized modules, finite state machine reasoning, and process-based feedback.", "section": "1 Introduction"}, {"figure_path": "jImXgQEmX3/tables/tables_19_1.jpg", "caption": "Table 14: Comparison results between AMOR and several representative baselines based on L-7B using the same tools on HotpotQA.", "description": "This table compares the performance of AMOR and several other methods (LUMOS and AgentLM) on the HotpotQA dataset.  All models use the L-7B language model. The key difference is in the retrieval methods used for SearchDoc and SearchPsg, as well as the final Complete module. AMOR uses Contriever for both retrieval tasks, while LUMOS uses the Wikipedia API and DPR, and AgentLM uses the Wikipedia API with Exact Keyword Matching.  The table shows that AMORProcess, with its process feedback mechanism, significantly outperforms the other methods in terms of Exact Match (EM) accuracy.", "section": "4 Experiments"}, {"figure_path": "jImXgQEmX3/tables/tables_19_2.jpg", "caption": "Table 15: Datasets for adaptation and evaluation. Avg. Len refers to the average length of passages in the corresponding knowledge base, counted by the GPT tokenizer [30]. Val is the validation set.", "description": "This table presents the details of three datasets used for adapting and evaluating the AMOR model.  For each dataset, it shows the knowledge base used (e.g., Wikipedia articles, PubMed abstracts), the average length of passages in tokens, and the number of training, validation, and test instances.", "section": "4.1 Experimental setup"}, {"figure_path": "jImXgQEmX3/tables/tables_20_1.jpg", "caption": "Table 2: Automatic annotation strategy for silver process feedback for different LLM modules.", "description": "This table presents the automatic annotation strategy employed to generate silver process feedback for the different Large Language Model (LLM) modules within the AMOR framework.  For each module (Decompose, Judge, Answer, Complete), it specifies the module's output (y) and the corresponding silver process feedback (f) that is automatically generated.  The silver feedback is designed to be a proxy for human feedback and is used in the adaptation phase of the AMOR training process. The feedback can be a binary judgment (\"right\", \"wrong\") or a more refined classification (e.g., [RELEVANT], [IRRELEVANT]).  This automatic generation of feedback allows for efficient adaptation of the AMOR agent to specific domains.", "section": "4 Experiments"}, {"figure_path": "jImXgQEmX3/tables/tables_20_2.jpg", "caption": "Table 17: Accuracy of the silver feedback for four LLM modules based on L-7B.", "description": "This table presents the accuracy of the automatically generated silver feedback for each of the four LLM modules in the AMOR framework.  The accuracy is measured against gold-standard human feedback for each module: Decompose, Judge, Answer, and Complete.  The high accuracy scores suggest that the automatically created silver feedback is a reliable proxy for human feedback.", "section": "B.3 Reasoning process assessment"}, {"figure_path": "jImXgQEmX3/tables/tables_20_3.jpg", "caption": "Table 18: Proportion of cases where the corresponding error exists. All agents are based on L-7B. N/A means the LUMOS agent does not explicitly execute the relevance judgment step.", "description": "This table compares the error rates of three different methods (LUMOS, AMOR without fine-tuning, and AMOR with process feedback) across six categories of errors.  The error categories are related to the different steps of the question-answering pipeline, such as question decomposition, retrieval, relevance judgment, answer extraction, and task completion. The table highlights that the method AMORProcess significantly reduces error rates compared to the other two methods.", "section": "4.2 Main results"}, {"figure_path": "jImXgQEmX3/tables/tables_21_1.jpg", "caption": "Table 19: Performance of AMOR parameterized by i during multi-round adaptation. In the i-th iteration (i = 0,1,2), AMOR\u03b8i is used to explore over the same set of questions or different ones and then is updated to AMOR\u03b8i+1 based on the exploratory instances.", "description": "This table shows the performance of AMOR across three iterations of multi-round adaptation.  Each iteration starts with a set of parameters (\u03b8i) and explores using either the same questions (same questions column) or different questions (different questions column) in each iteration. The performance metrics (EM and F1) are reported for each round of adaptation (\u03b81, \u03b82, \u03b83).  The results show that performance does not change significantly with same or different questions across rounds.", "section": "4.2 Main results"}, {"figure_path": "jImXgQEmX3/tables/tables_21_2.jpg", "caption": "Table 20: Average step/token numbers of different agents. For ReAct and AgentLM, a step refers to a \"Thought,\" \"Action,\" or \"Observation\" step. For AMOR, a step means a reasoning step within a certain module. And tokens count both input and output tokens.", "description": "This table compares the average number of steps and tokens used by different language agents (ReAct, AgentLM, AMOR) across three question answering datasets (HotpotQA, PubMedQA, QASPER).  The metrics are broken down by agent and dataset, showing the efficiency of each agent in terms of both computation steps and token usage.  Note that the definition of \"step\" differs between the agents; for ReAct and AgentLM, a step represents a unit of thought, action, or observation, while for AMOR, a step corresponds to a reasoning step within a specific module.", "section": "4.2 Main results"}, {"figure_path": "jImXgQEmX3/tables/tables_22_1.jpg", "caption": "Table 21: AMOR can be enhanced through targeted fine-tuning and flexibly accommodate additional tools. All results are based on L-7B.", "description": "This table presents the results of three different versions of the AMOR model on the HotpotQA dataset. The first row shows the performance of the original AMORProcess model. The second row shows the performance after targeted fine-tuning of the Complete module. The third row shows the performance after adding an additional tool called SearchDemo. The results demonstrate that both targeted fine-tuning and adding new tools can improve the performance of the AMOR model.", "section": "4.2 Main results"}]