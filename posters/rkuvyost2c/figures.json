[{"figure_path": "rkuVYosT2c/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the leverage score sparsification algorithm used in Theorem 1. Each row of the sketch mixes \u00d5(1/\u20ac) leverage score samples from A. Remarkably, the e-error guarantee of the subsampled estimator is retained as e-bias of the sketched estimator.", "description": "This figure illustrates the leverage score sparsification algorithm used to prove Theorem 1 in the paper.  It shows how a large data matrix A is first subsampled to a smaller matrix with O(d/\u03b5) rows.  Then, a sketching matrix is applied to this subsample, creating a final sketch with only O(d) rows. The key takeaway is that while the error guarantee of the subsampled estimator is \u03b5, the bias of the sketched estimator remains at \u03b5. This demonstrates that a smaller sketch can achieve the same level of bias.", "section": "1 Introduction"}, {"figure_path": "rkuVYosT2c/figures/figures_9_1.jpg", "caption": "Figure 2: Distributed averaging experiment on YearPredictionMSD and Abalone datasets [8], showing that sparse sketching can be used to compress the data while preserving near-unbiasedness without increasing the estimation cost (see Appendix F for results on the Boston dataset).", "description": "This figure displays the results of a distributed averaging experiment conducted on two datasets: YearPredictionMSD and Abalone. The experiment evaluates the effectiveness of using sparse sketching to compress data while maintaining near-unbiasedness in least squares estimation.  Different lines represent different sketch sizes and non-zero entries per row, but with the same total cost, demonstrating that compressing the data does not lead to increased bias, despite reducing the error, showcasing a \"free lunch\" in distributed averaging.", "section": "5 Experiments"}, {"figure_path": "rkuVYosT2c/figures/figures_29_1.jpg", "caption": "Figure 2: Distributed averaging experiment on YearPredictionMSD and Abalone datasets [8], showing that sparse sketching can be used to compress the data while preserving near-unbiasedness without increasing the estimation cost (see Appendix F for results on the Boston dataset).", "description": "The figure shows the results of a distributed averaging experiment using sparse sketching on two datasets, YearPredictionMSD and Abalone.  The experiment demonstrates that using sparse sketching to compress the data does not introduce significant bias, while maintaining near-unbiasedness and without increasing the computational cost.  Multiple lines on the graph show the effect of varying the sketch size and sparsity, all while keeping the total computational cost of sketching constant.", "section": "5 Experiments"}, {"figure_path": "rkuVYosT2c/figures/figures_30_1.jpg", "caption": "Figure 2: Distributed averaging experiment on YearPredictionMSD and Abalone datasets [8], showing that sparse sketching can be used to compress the data while preserving near-unbiasedness without increasing the estimation cost (see Appendix F for results on the Boston dataset).", "description": "The figure displays the results of a distributed averaging experiment conducted on the YearPredictionMSD and Abalone datasets.  The experiment compares the relative error of averaged sketched-and-solved least squares estimators using different sketching techniques with varying sketch sizes and sparsity.  It demonstrates that sparse sketching compresses data effectively while maintaining low bias, suggesting that in distributed settings, the bias is minimal even when the space on each machine is limited.  The results for the Boston dataset are available in Appendix F.", "section": "5 Experiments"}]