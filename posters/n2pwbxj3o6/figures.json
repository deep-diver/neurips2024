[{"figure_path": "N2PwbxJ3o6/figures/figures_2_1.jpg", "caption": "Figure 2: Systematic comparison between three different frameworks for in-context example selection. (a) VPR, which uses pair-wise ranker trained with contrastive learning to calculate the relevance score for each alternative. (b) List-wise ranker with naive aggregation, in which alternatives are split into non-overlapped subsets. These subsets are ranked with the proposed list-wise ranker. Then we iteratively select the best example in each subset and rank them. (c) List-wise ranker with consistency-aware aggregator, in which alternatives are first shuffled and predicted with list-wise ranker into an observation pool. These partial observations are then aggregated with the proposed aggregator to achieve a global ranking.", "description": "This figure systematically compares three different frameworks for selecting in-context examples in Visual In-Context Learning (VICL).  (a) shows the VPR method, which uses a pairwise ranker and contrastive learning. (b) illustrates a naive approach using a list-wise ranker on non-overlapping subsets of alternatives. (c) presents the proposed Partial2Global method, which uses a list-wise ranker on shuffled subsets of alternatives and aggregates the results using a consistency-aware aggregator to achieve a more robust global ranking.", "section": "Methodology"}, {"figure_path": "N2PwbxJ3o6/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparison between our method and VPR, specifically SupPR, in foreground segmentation. In each item we present the image grid in the same order as the input of MAE-VQGAN, i.e. in-context example and its label in the first row, query image and its prediction in the second row. The IoU is listed below each image grid.", "description": "This figure compares the performance of the proposed method (Ours) and the baseline method (SupPR) on foreground segmentation. Each example shows a query image and its prediction from both methods, using the same in-context examples. The IoU (Intersection over Union) score is displayed for each prediction, which quantifies the accuracy of the segmentation.  The figure demonstrates that the proposed method generally achieves higher IoU scores, indicating more accurate foreground segmentation results.", "section": "4.2 Main results"}, {"figure_path": "N2PwbxJ3o6/figures/figures_8_1.jpg", "caption": "Figure 4: (a) Scatter plot of visual similarity against IoU for VPR on segmentation. (b) Scatter plot of visual similarity against IoU for our method on segmentation. (c) Visualization of several cases with uncorrelated visual similarity and IoU. The first row presents samples with low similarity but proper in-context performance. The second row presents samples with high similarity but poor in-context performance. Captions below each image grid denote IoU and visual similarity sequentially.", "description": "This figure shows the correlation between visual similarity and the Intersection over Union (IoU) for foreground segmentation using both the Visual Prompt Retrieval (VPR) method and the proposed Partial2Global method.  The scatter plots in (a) and (b) visualize this relationship, revealing that while high visual similarity is often associated with good performance, there are many cases where high visual similarity does not guarantee high IoU. Part (c) provides example images with low visual similarity but high IoU and vice versa, demonstrating the complexity of choosing effective in-context examples based solely on visual similarity.", "section": "Model analysis"}, {"figure_path": "N2PwbxJ3o6/figures/figures_12_1.jpg", "caption": "Figure 5: Qualitative comparison between our method and VPR, specifically SupPR, in single object detection. For simplicity we present the bounding boxes on images instead of showing the image grids. In each item the left image denotes the in-context example and the right one denotes the query.", "description": "This figure shows a qualitative comparison of the proposed method (Ours) and the Visual Prompt Retrieval (VPR) method, specifically SupPR, on the task of single object detection.  For each example, the left image displays the in-context example selected by each method, and the right image shows the query image along with the resulting bounding boxes. The IoU (Intersection over Union) scores are shown below each pair of images. The comparison visually demonstrates that the proposed method consistently selects better in-context examples resulting in significantly improved object detection accuracy.", "section": "4.2 Main results"}]