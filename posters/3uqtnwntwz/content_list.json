[{"type": "text", "text": "Zero-to-Hero: Enhancing Zero-Shot Novel View Synthesis via Attention Map Filtering ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ido Sobol1 Chenfeng $\\mathbf{X}\\mathbf{u}^{2}$ Or Litany1,3 ", "page_idx": 0}, {"type": "text", "text": "1Technion 2UC Berkeley 3NVIDIA ", "page_idx": 0}, {"type": "text", "text": "https://zero2hero-nvs.github.io/ ", "page_idx": 0}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/998ae6ca7d2040ebead0284cdc54524071c2fccc7db639079d98acdcb0594700.jpg", "img_caption": ["Figure 1: Novel views generated from a single source image (far left column) at a specific target view angle (with different seeds), compared between Zero123-XL [27] and our Zero-to-Hero method. Operating during inference, our method achieves significantly higher fidelity and maintains authenticity to the original image, all while ensuring realistic variation in the results (e.g. variations in chair backs in the top row). The ground-truth target view is displayed in the far right column. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generating realistic images from arbitrary views based on a single source image remains a significant challenge in computer vision, with broad applications ranging from e-commerce to immersive virtual experiences. Recent advancements in diffusion models, particularly the Zero-1-to-3 model, have been widely adopted for generating plausible views, videos, and 3D models. However, these models still struggle with inconsistencies and implausibility in new views generation, especially for challenging changes in viewpoint. In this work, we propose Zero-to-Hero, a novel test-time approach that enhances view synthesis by manipulating attention maps during the denoising process of Zero-1-to-3. By drawing an analogy between the denoising process and stochastic gradient descent (SGD), we implement a filtering mechanism that aggregates attention maps, enhancing generation reliability and authenticity. This process improves geometric consistency without requiring retraining or significant computational resources. Additionally, we modify the self-attention mechanism to integrate information from the source view, reducing shape distortions. These processes are further supported by a specialized sampling schedule. Experimental results demonstrate substantial improvements in fidelity and consistency, validated on a diverse set of out-of-distribution objects. Additionally, we demonstrate the general applicability and effectiveness of Zero-to-Hero in multi-view, and image generation conditioned on semantic maps and pose. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The pursuit of realistic image synthesis at arbitrary views, given only a single source image, has long been a cornerstone challenge in computer vision and graphics. This technology can cater to countless applications, such as interactive product inspection, robot-scene interaction, and immersive virtual experiences. In this work, we aim to advance this important line of research by improving the generation of novel views that are plausible and faithful to the input image. A recent, promising approach, Zero-1-to-3 [27] has developed a foundation model to synthesize novel views based on a single source image and a target view angle. By leveraging a pre-trained, image-conditioned stable diffusion model backbone [3], fine-tuned with target camera poses, and trained on paired source and target views from a vast collection of 3D models [10, 9], Zero-1-to-3 can generalize beyond its training set and generate plausible novel views. As a result, this model has quickly gained popularity, inspiring subsequent work in 3D and 4D scene generation [8, 24, 39, 28, 26, 25, 57, 33, 45, 35, 20]. ", "page_idx": 1}, {"type": "text", "text": "Despite its remarkable ability, Zero-1-to-3 has been observed to generate views that are implausible, or inconsistent with the input object in terms of shape and appearance [24, 8]. Previous works have tried to mitigate these issues by retraining diffusion models with more data [9] or to generate multiple views [39, 25, 26, 24, 8, 28, 56]. Despite substantial improvement, both approaches are resource-intensive due to the required re-training on large-scale 3D datasets. Another line of work attempts to consolidate inconsistencies across multiple generated views through a 3D representation like NeRF [14, 30]. However, direct aggregation often results in blurry outputs, as observed by [24]. Instead, ViVid-1-to-3 [24] employed a multiview representation that naturally supports the use of a video foundation model. Nevertheless, this approach requires generating the entire trajectory from the source to the target view, adding significant complexity and computational overhead. Notably, the denoising process in Zero-1-to-3 remains unchanged. ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose Zero-to-Hero, a novel test-time technique that addresses view synthesis artifacts through attention map manipulation. Recognizing attention maps as crucial for latent predictions, we hypothesize that enhancing robustness in attention maps predictions can significantly reduce generation misalignment. To achieve this, we draw an analogy between the denoising process in diffusion models and stochastic gradient descent (SGD) optimization of neural networks. Specifically, we relate network weights that predict local gradients at each optimization step, based on sampled training examples and labels, to the denoising network\u2019s attention maps that predict latent representations from sampled noise at each denoising step. In this work, we view the generation (denoising) process as an unrolled optimization, with attention maps as parameters of a score prediction model. Inspired by gradient aggregation and weight-averaging techniques that improve prediction robustness (e.g., consistency training [47]), we propose a flitering mechanism to enhance attention map reliability. This mechanism comprises iterative map aggregation within denoising steps and map averaging across denoising steps. The result is more reliable maps, particularly during the early denoising stages when coarse output shapes are determined, leading to more plausible and realistic views. ", "page_idx": 1}, {"type": "text", "text": "To further promote consistency with the input, we modify the self-attention operation by running a parallel generation branch using the identity pose, incorporating its keys and values into the attention layer of the target view. Unlike previous applications of this technique [29, 5, 1]), we find it beneficial in view synthesis to limit its use to the early denoising stages, preventing shape distortions. Our unique denoising procedure is further complemented by a novel sampling schedule that emphasizes early and late denoising stages, maximizing performance. Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To address the main limitations of the Zero-1-to-3 model, we perform an in-depth analysis and identify self-attention maps as the main candidate for correcting generation artifacts. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We establish a conceptual analogy between model weights in stochastic gradient descent-based network training and the role of attentions map updates during generation of a denoising diffusion model. Based on this, we propose a simple yet powerful attention map filtering process resulting in enhanced target shape generation. We supplement our filtering technique with identity view information injection and a specialized sampling schedule.   \n\u2022 Our method requires no additional training, and it avoids the overhead of external models or generating multiple views. ", "page_idx": 2}, {"type": "text", "text": "Through comprehensive experiments on out-of-distribution objects, we demonstrate that our technique robustifies Zero-1-to-3 and its extended version, Zero123-XL, leading to views that are more faithful to both the input image and desired camera transformation. Our results show significant and consistent improvements across both appearance and shape evaluation metrics. Additionally, we find that Zero-to-Hero naturally generalizes to additional tasks including multi-view, and image generation conditioned on semantic maps and pose. In all cases, we observed significant improvement in condition following and visual quality. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Novel View Synthesis with Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models have dominated various generative applications [17, 11, 38, 36, 41]. Particularly, novel-view synthesis, as a core of applications like augmented reality and simulations, naturally enjoys the benefits of high-fidelity zero-shot synthesis with diffusion models. One line of works [51, 27, 44, 22, 54, 24] is to generate a novel-view image given a source image (i.e., image-to-image). These approaches typically involve training a diffusion model conditioned on both an arbitrary camera pose and the source view. For instance, the representative work, Zero-1-to-3, fine-tunes a pre-trained Stable Diffusion model [36] by replacing the text prompt with camera pose and CLIP features[34]. Moreover, another research trajectory [32, 21, 14, 7, 46] proposes generating a 3D representation from a single image (i.e., image-to-3D), which allows for sampling desired views from these 3D models. Our method, Zero-to-Hero, builds on the first approach (specifically Zero-1-to-3 and Zero123-XL) and distinguishes itself by eliminating the need for extensive training. Instead, it offers a test-time, plug-and-play approach that significantly enhances visual quality and consistency. ", "page_idx": 2}, {"type": "text", "text": "2.2 Test-Time Refinement in Diffusion-Based Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A common test-time refinement strategy in diffusion generation is leveraging guidance [29, 2, 18] to direct the sampling process with additional conditions. For example, Repaint [29] utilizes a maskthen-renoise strategy to refine the generation results. Repaint also introduces a per-step resampling technique, where given a horizon-size h, a latent $z_{t}$ is re-noised to $z_{t+h}$ and then denoised again to $z_{t}$ multiple times. They observe that resampling helps to generate more harmonized outputs, given an external guidance or condition. Restart [55] offers a sampling algorithm based on a variation of resampling within a chosen interval of steps. Our method is inspired by the strategy of per-step resampling. We show that it serves as a powerful correction mechanism throughout the generation process, even when no external guidance or condition is provided. ", "page_idx": 2}, {"type": "text", "text": "2.3 Attention Map Manipulation in Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Stable Diffusion [36] utilizes attention to enforce the condition information onto the generated results. Previous works demonstrate that manipulating the attention operation can achieve new capabilities [49, 5, 1, 57]. For example, MasaCtrl [5] uses Mutual Self-Attention where source and target images are generated jointly while sharing information, by injecting source image keys and values to the target through self-attention. Here we employ Mutual Self-Attention in the context of novel view synthesis. Differently to prior works we find it beneficial to limit its use to the early denoising stages. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Zero-1-to-3: Challenges and Limitations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Zero-1-to-3 is a pioneering method for novel view synthesis based on a diffusion model, which has gained significant popularity. This model is built upon the image-conditioned variant of Stable ", "page_idx": 2}, {"type": "text", "text": "Diffusion (SD) [36], fine-tuned specifically for novel view synthesis. Zero-1-to-3 is conditioned on a source image and relative transformation to the desired view angle $[\\mathcal{R}|\\mathcal{T}]$ . Maintaining the SD architecture, these conditions are integrated in two ways. First, a CLIP [34] embedding of the input image is concatenated with the relative transformation $[\\mathcal{R}|\\mathcal{T}]$ and mapped to the original CLIP dimension to form a global pose-CLIP embedding, interacting with the UNet layers through cross-attention, enriching the generation with high-level semantic information. In parallel, the input image is channel-concatenated with the denoised image, helping the model preserve the identity and details of the synthesized object. ", "page_idx": 3}, {"type": "text", "text": "While Zero-1-to-3 [27] has achieved substantial progress in novel view synthesis, several common issues limit its practical application. Firstly, the generated images might not fti real-world distributions, resulting in implausible and unrealistic outputs (e.g., first row in Fig.1). Secondly, the target image may appear plausible but be inconsistent with the input image in terms of shape or appearance (e.g., fifth row in Fig.1). ", "page_idx": 3}, {"type": "text", "text": "In this work, we identify the critical role of self-attention maps in high-quality generation and propose a novel filtering process that enhances robustness without requiring further training. This process addresses the aforementioned issues, ensuring reliable and consistent results. ", "page_idx": 3}, {"type": "text", "text": "3.2 Leveraging Gradient and Weight Aggregation for Improved Prediction Consistency ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, we draw a conceptual analogy between gradients and weights in stochastic gradient descent (SGD), and latents and attention maps in denoising diffusion models. Leveraging this analogy, we adapt techniques from SGD to enhance prediction consistency in diffusion models. Below, we summarize general techniques in SGD that improve the training process. ", "page_idx": 3}, {"type": "text", "text": "SGD is a fundamental tool in network training [4], designed to navigate the weight (network parameter) space towards local minima. For a neural network $F(x;\\theta)$ with parameters $\\theta$ , SGD samples training data points $x_{i}$ and their corresponding labels $y_{i}$ , and computes the gradient of the loss function $L\\bar{(F(x_{i};\\bar{\\theta}),y_{i})}$ with respect to $\\theta$ to update the parameters. In practice, aggregation of gradients and network weights during training is often performed to reduce variance and improve convergence. Gradient aggregation typically involves averaging gradient values over a batch, while weight aggregation accounts for the history of the weights in each update. ", "page_idx": 3}, {"type": "text", "text": "Notable examples include temporal averaging in Adam optimizer [23], Stochastic Weight Averaging (SWA) [19] and teacher networks [47] used in consistency training by employing an exponential moving average (EMA) of a student network to maintain high-quality predictions. This technique is prevalent in semi-supervised and representation learning [13, 15, 6]. For a detailed study of EMA in network training, we refer readers to [31]. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work we are concerned with the task of single image novel view synthesis. Formally, given an input image of an object and a relative camera transformation towards a desired target view, our goal is to generate the image at that target view. Specifically, we build upon the seminal work of Zero1-to-3 [27] which tackled this task via a diffusion model. As detailed in Sec. 3.1 Zero-1-to-3 often struggles to generate plausible and input-consistent images. In this work we propose Zero-to-Hero\u2013 a novel training-free technique that significantly improves its generation quality through attention map manipulation. This section is structured as follows. (4.1) through network architecture analysis we identify self-attention maps as key for robust view generation; (4.2) drawing inspiration from SGD convergence-enhancement techniques, we outline our novel attention map filtering pipeline; (4.3) details each step of the map filtering; (4.4) introduces the mutual self-attention which we use for shape guidance at early generation stages; (4.5) Finally, our proposed hourglass scheduler is introduced for more efficient utilization of generation steps. Fig. 2 depicts Zero-to-Hero\u2019s main modules. An ablation of these modules is provided in Tab. 2 and in Sec. 8.6 of the appendix. ", "page_idx": 3}, {"type": "text", "text": "4.1 Analyzing the Role of Cross- and Self-Attention Layers in Novel View Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Zero-1-to-3 builds upon the image-to-image variant of Stable Diffusion [36], which utilizes a UNet [37] architecture as its backbone and incorporates both self- and cross-attention layers. In this subsection, we analyze the roles of these components and their contributions to the generated view. This analysis aims to identify effective intervention points for enhancing generation quality. ", "page_idx": 3}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/41c207faa47e0a67fbb823b7d4f50e37cadb0bb90a7671bd13663b5f31608df9.jpg", "img_caption": ["Figure 2: Zero-to-Hero main modules. (Left) Two denoising steps of the generation process of both the source (top) and target views (bottom). Each denoising step is iterated $R$ times (\u201cresampling\u201d). (Right-top) Attention map filtering: Robustifying attention maps via an aggregation of same step and previous steps attention maps. (Right-bottom) Mutual self-attention: Guiding target shape through the keys and values of the source generation branch. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Global pose conditioning through cross-attention. Prior work [16] has demonstrated that the cross-attention layers in text-to-image diffusion models, which link text tokens to the latent image, are spatially aware and can be used for spatial manipulation. We first investigate these cross-attention layers, as they are the only components in the model through which the target pose is injected. In the text-to-image variant of Stable Diffusion, the generation is conditioned on a prompt $\\mathcal{P}$ containing $C$ tokens, each encoded with CLIP into an embedding, resulting in a condition $\\stackrel{\\star}{c_{T2I}}\\in\\mathbb{R}^{C\\times d_{C L I P}}$ . However, in Zero-1-to-3, the condition is a pose-CLIP embedding $c\\in\\mathbb{R}^{1\\times d_{C L I P}}$ , project to keys $K_{t}\\in\\mathbb{R}^{1\\times d}$ Formally, given a sample $z_{t}\\in\\mathbb{R}^{\\hat{H}\\times W\\times d_{z}}$ and its corresponding queries $\\stackrel{\\triangledown}{\\boldsymbol{Q}}_{t}\\in\\mathbb{R}^{H\\times W\\,\\check{\\times}\\,d}$ , the pre-softmax attention map $\\boldsymbol{\\mathcal{A}}$ between $Q_{t}$ and $K_{t}$ has dimensions $H\\times W\\times1$ . Given that the summation of the softmax is always 1, the post-softmax attention map $\\mathsf{s o f t r a n}a x(A)$ in Zero-1-to-3 is a constant all-ones matrix. A visual demonstration is presented in Fig. 7 in the appendix. ", "page_idx": 4}, {"type": "text", "text": "The post-softmax attention map is used to compute a weighted sum over the values matrix $V\\in\\mathbb{R}^{1\\times d}$ , obtained by a transformation of the condition $c$ . Since the attention matrix is an all-ones matrix, we conclude that the cross-attention operation degenerates into a global bias term, lacking spatially aware operations. Computing similarity scores in the cross-attention layers is redundant as these scores are never used. While in principle it is possible to improve the global bias term by additional optimization objectives and extra training overhead, we focus on the self-attention layers to enhance the results and mitigate the consistency issues while avoiding retraining the model. ", "page_idx": 4}, {"type": "text", "text": "Spatial information flow through self-attention. By monitoring the self-attention layers during the generation process, we observe that random noise introduced to the latent representation also introduces randomness to the attention maps. This randomness, while promoting generation diversity, can often lead to undesired strong correlations, that are misaligned with the true target. These strong correlations may persist through the denoising process, resulting in accumulated errors and visual artifacts. ", "page_idx": 4}, {"type": "text", "text": "Given the above observation and the insight about the spatial-degeneracy in the cross-attention layers, we hypothesize that the self-attention layers preserve the information about the structure and geometry of the generated image, through the similarity scores between different elements in the latent vector. To validate our hypothesis, we conduct a straightforward experiment to assess Zero-1-to-3\u2019s performance using the \u2019ground truth\u2019 self-attention maps, which reflect the most accurate connections considering the true target image. Specifically, we selected two images, $I^{\\mathrm{src}}$ and $I^{\\mathrm{tgt}}$ , with known relative camera parameters $[\\mathcal{R}|\\mathcal{T}]$ . We first encode $I^{\\mathrm{tgt}}$ to obtain the clean latent $z_{0}^{\\mathrm{tgt}}$ and then add subtle noise to obtain the corresponding noisy latent for timestep \u03c4init, zitngitt. A single denoising step is performed on the noisy latent, and we save the self-attention maps from each layer in the UNet, considering these maps as the ground truth (GT) maps. In our experiments, $\\tau_{\\mathrm{init}}=5$ . Next, we sample random Gaussian noise and denoise it to regenerate the target image. During each denoising step, we replace the computed self-attention maps with the GT maps, without altering any other components (e.g., cross-attention layers, residual blocks) or the latents, queries, keys, and values. We report the results in the experiment section in Tab. 1, showing a significant improvement in all metrics. Note that the results obtained with the GT attention maps are a strict upper bound, as the GT maps contain information about areas that are invisible in the source view. Our experiment validates that through improved self-attention maps, the generated image becomes more plausible. Visual examples are shown in Fig. 3 and in Fig. 8 in the appendix, refer to Fig. 1 for the results of Zero123-XL and Zero-to-Hero for the same views. ", "page_idx": 4}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/2f013c6386f1d57a17b340803f890784f7d96dd4bf65efe2a08df469ed96a3b5.jpg", "img_caption": ["Figure 3: Through the injection of ground-truth attention maps extracted from the target view, we demonstrate that Self-attention maps are key to robust view synthesis. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2 From SGD to Diffusion Models: Attention Map Filtering as Weight-Space Manipulation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We draw a conceptual analogy between a denoising process of a diffusion model, and SGD based network optimization as both progress through gradient prediction of a loss function, and log probability (the score)[43], respectively. Building on the discussion from the previous section, we treat the self-attention maps as parameters $M$ in the denoising process $z_{t-1}=\\mu(z_{t};M_{t},\\psi)$ and define their update process as a mapping1: $M_{t}\\rightarrow z_{t-1}\\rightarrow M_{t-1}$ . Here the map $M_{t-1}$ results from passing the latent $z_{t-1}$ through the attention layers. This process is analogous to gradient descent optimization in neural networks, where each step adjusts the weights in the direction of the gradient of a loss function, such as the log probability in classification tasks. We denote this weight update as a mapping $\\theta_{t}\\to\\hat{y_{t}}\\to\\theta_{t+1}$ , where $\\hat{y}_{t}=\\dot{F}(x_{t};\\theta)$ , and the updated parameters $\\theta_{t+1}$ result from a gradient step. ", "page_idx": 5}, {"type": "text", "text": "As detailed in Sec. 3.2, gradient and weight aggregation are essential for robust convergence. We outline this process in three steps illustrating the analogy between network parameter updates and attention map filtering. Fig. 4 further illustrates the analogy. ", "page_idx": 5}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/de12a16f09f622f19c030ce7e6ddb0e4a8c62990691268a41ab7cd117a491f4c.jpg", "table_caption": ["Step-by-Step Analogy From network parameters to attention maps "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.3 Robust View Generation via Attention Map Filtering ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now discuss in detail each of the map flitering steps. A scheme of the different flitering modules is provided in Fig. 2 (Left, and Top-right). ", "page_idx": 5}, {"type": "text", "text": "Latent refinement via per-step resampling. Inspired by previous studies [29, 2], we implement per-step resampling throughout the image generation process. We select a range of timesteps $[t_{m i n},t_{m a x}]$ , where each denoised sample $z_{t}$ is re-noised with the proper noise ratio to the previous sampled timestep $z_{t+1}$ and denoised back to $z_{t}$ for $R$ iterations2. We concur with previous studies that resampling functions as an effective corrective mechanism to the generated image, as can be seen in Tab. 2). Experimentally, we find that it is particularly useful during the initial stages of the denoising process. Through resampling we progressively generate $R$ attention maps with different noise patterns. We propose to leverage these intermediate maps to further boost performance through in- and cross-step attention map manipulations. ", "page_idx": 5}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/c18bdeb9281dd15851f7f1759412c170266f41b169fce3636a4f8f77fe95c5b9.jpg", "img_caption": ["Figure 4: From SGD to Diffusion Models: An illustration of our conceptual analogy. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/59c52a2e300c5d99008bc735da37d9cc3ce7cd5f60dd1fa40357e5a9ab95dfc2.jpg", "img_caption": ["Figure 5: Attention map filtering in action. We compare the attention scores of zero123-XL (top) and Zero-to-Hero (bottom) wrt the region marked with a purple circle at different denoising steps. Both methods are initialized with the same seed. We observe that the strong correlation values in the upper right corner lead to exaggerated content creation (note the unrealistically elongated neck). Conversely, through flitering, Zero-to-Hero mitigates these artifacts, leading to robust view synthesis. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In-step map update. We propose a novel attention pooling function $f$ , to update the attention maps within the denoising step. Specifically, a self-attention map is refined based on previous maps $\\{\\bar{M_{t,k}}\\}_{k=1}^{r-1}$ created at the same timestep. Since resampling is a sequential process, we perform a progressive aggregation scheme: $\\widetilde{M}_{t,r}\\,=\\,f(M_{t,r},\\{\\widetilde{M}_{t,k}\\}_{k=1}^{r-1})$ . We found the element-wise minpooling operator ${\\bar{f}}(a,b)=m i n(a,b)$ to perform best in our experiments. We discuss other options for $f$ in the appendix. ", "page_idx": 6}, {"type": "text", "text": "Cross-step map averaging. Resampling tends to favour \u201cconservative\u201d generations, often gradually neglecting fine image details like buttons and eyes as denoising progresses. This phenomenon it not resolved by the in-step update. To mitigate this issue, we propose to pass the self-attention map at time $t$ , $\\widetilde{M}_{t}$ to the next step in the denoising process. We implement this cross-step aggregation via EMA: $\\widetilde M_{t-1}=\\alpha M_{t}+(1-\\alpha)\\widetilde M_{t-1}$ , for $\\alpha\\in[0,1]$ . This method effectively balances past priors with c urrent data to enhance the denoising results. In practice we apply both methods in tandem, as detailed in Sec. 8.5. An example of our attention filtering is presented in Fig. 5. Note that our filtering mechanism is applied to the pre-softmax attention maps. ", "page_idx": 6}, {"type": "text", "text": "4.4 Early-Stage Shape Guidance via Mutual Self-Attention ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Of the two challenges describe in the introduction, attention map filtering handles well generating realistic outputs. Yet, we observe it is sometimes insufficient to enforce consistency with the input appearance and structure. We propose to utilize mutual self-attention as a complementary technique, to propagate information from the input to the target. Similar to prior works [49, 5, 1, 57, 52], we generate the input and target views in parallel. At each timestep $t$ , we obtain queries, keys and values of the self-attention layers for the input and the target branches. Similar to the mutual self-attention introduced in [5], we modify the self-attention operation of the target by replacing the target keys and values with those of the input branch: $A t t n(Q^{t\\bar{g}t},K^{i n},V^{i n})$ (Fig. 2 (bottom-right)). ", "page_idx": 7}, {"type": "text", "text": "Previous studies initiate mutual self-attention (MSA) at a later denoising step (small $t$ ). We find that at that stage, the structure has already been determined. Instead, to guide the structure of the target to be more consistent with the input, we find it more effective when applied from the beginning of the denoising process. While MSA is effective at transferring the appearance and structure of the input to the target, it may overfit. We find that terminating the MSA process once the structure has been stabilized becomes crucial in mitigating overfitting. In practice we find that applying MSA during the first third of the denoising process is a good rule of thumb for optimal results. We refer the reader to Sec. 8.5 in the appendix for further details. ", "page_idx": 7}, {"type": "text", "text": "We view early-stage MSA as a complementary filtering scheme. Building on the property that generating the input view (which the model is conditioned on) is a much easier task for the model compared to generating novel views, keys and values predicted in the input branch are \u201ccleaner\u201d. Mapping them to the target view thus refines the predicted scores, facilitating a more stable and reliable output. ", "page_idx": 7}, {"type": "text", "text": "4.5 Hourglass Sampling Scheduler ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As resampling is a time-consuming operation, significantly increasing the number of function evaluation (NFE), we seek an efficient scheduling scheme that will enable robust and high quality generation, while preserving the fast generation times of Zero-1-to-3. We use the common DDIM sampling [42], and propose an efficient scheduling scheme to select the sampled timesteps. In the experiments section, we demonstrate that increasing the number of denoising steps does not necessarily improve the performance, and therefore we aim to use an overall small number of sampled timesteps. Specifically, we find that denser sampling during the beginning of the denoising process is crucial to promote realism. We also find that denser sampling at the final steps enhances fine details. Therefore, we suggest a double heavy-tailed scheduling scheme we call Hourglass, according to which we divide the generation process into 3 stages, as detailed in 8.5. Within each stage, we sample steps uniformly via DDIM. However, in the first and last stages we sample steps more densely (at a higher rate) than in the middle stage, by a factor of $\\lambda_{d e n}$ . ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets. We evaluate our method on two datasets, following previous works. Firstly, Google Scanned Objects (GSO) Dataset [12], which includes 1030 scanned household objects. However, the dataset\u2019s imbalance (e.g., 254 objects are categorized as \u201cshoe\u201d) and the high proportion of symmetrical shapes limit its reliability for evaluation. To address this, inspired by [24], we select a challenging subset of 50 objects from GSO, avoiding symmetrical and repetitive shapes. For each object, we render 8 random views (details in Sec. 8.9 of the appendix) and synthesize the remaining views from each source view, generating each target view with 3 different seeds to calculate the average score per measure. Secondly, RTMV Dataset [48], which consists of 3D scenes. Each scene contains 20 random objects. For evaluation, we randomly select 50 scenes, and 8 random views per scene. The evaluation process is the same as described for GSO. ", "page_idx": 7}, {"type": "text", "text": "Metrics. We report the image quality metrics PSNR, SSIM [50] and LPIPS [59]. As these metrics are sensitive to slight color variations, we segment the generated targets and their corresponding real images, via thresholding, and report the Intersection Over Union (IoU) score. ", "page_idx": 7}, {"type": "text", "text": "5.1 Evaluations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Quantitative evaluation. We evaluate Zero-to-Hero against zero-1-to-3 and on zero123-XL. We report all metrics for the original models using 25, 50 and 100 DDIM steps, and for our method applied to both models. In Tab. 1 and in Tab. 3, we provide the results for GSO and RTMV datasets, respectively. We include the number of sampled timesteps $\\mathrm{T}$ and the total number of network evaluation NFE (accounting for resampling). Zero-to-Hero consistently improves performance across all metrics, taking a significant step towards bridging the performance gap to GT attention maps. All implementation details, including analysis of the inference cost of our modules, are provided in Sec. 8.5 of the appendix. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/f6839b598b018a105db300f86a38174f666361ebddc3beb32acc5969d487c4e1.jpg", "table_caption": ["Table 1: Quantitative evaluation on a challenging GSO subset. Zero-to-Hero consistently improves performance upon baselines, taking a significant step towards oracle map performance (bottom rows). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Qualitative evaluation. In Fig. 1, Fig. 9 and Fig. 10, we visually demonstrate how Zero-to-Hero is able to mitigate various artifacts generated by Zero-1-to-3, from implausible results to incorrect poses. In Fig. 1 and in Fig. 9, we present 3 examples per target view, generated with 3 random seeds, to emphasize the consistency and robustness our method offers. ", "page_idx": 8}, {"type": "text", "text": "5.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To assess the contribution of Zero-to-Hero different components to the final performance, we evaluate our pipeline on our challenging GSO subset, starting from the baseline Zero123-XL and gradually adding each module. The results are summarized in Tab. 2. When reporting the results of the vanilla Zero123-XL, we use its best score achieved by running the model with 25, 50, and 100 steps. A similar comparison against zero-1-to-3 is included in Sec. 8.6 of the appendix, demonstrating consistent behavior. We also analyze the affect of resampling and our attention filtering on the generation diversity in Sec. 8.7 of the appendix. Additionally, we provide qualitative results, demonstrating the common contributions of AMF and MSA in Fig. 10 in the appendix. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Ablation Study. We demonstrate the importance of each of Zero-to-Hero modules, applied to the base method Zero123-XL: Sample scheduling (Hourglass), Resampling (Resample), Attention map filtering (AMF), and Early-Stage Mutual Self-Attention (MSA). Consistent conclusions are reached with the base model Zero-1-to-3 and are shown in Sec. 8.6 of the appendix. ", "page_idx": 8}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/99dd3aae85eeae15c1d44292ad162cbf451ebfbc2222e918c91c244f563df923.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Attention Map Filtering Beyond Novel View Synthesis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Although our work addresses the core limitations of single view synthesis models, the condition enforcing effect of our Attention Map Filtering (AMF) is more general. We have conducted several preliminary experiments which demonstrate promising results. Further details are provided in Sec. 8.8. ", "page_idx": 8}, {"type": "text", "text": "Conditional image generation. A brief study of ControlNet models [58] demonstrated that they suffer from similar limitations as Zero-1-to-3 and its follow ups. Namely, lack of condition enforcement and frequent appearance of visual artifacts. We implemented our proposed AMF module for two pre-trained ControlNet models. We provide qualitative results for pose- and segmentation-conditioned ControlNet models in Fig. 6 and Fig. 13, respectively. ", "page_idx": 8}, {"type": "text", "text": "Multi-view synthesis. We integrate AMF into MVDream [40], a text-to-multiview model, and find that it helps to mitigate the same issues as in the single view case. In Fig. 14, we provide qualitative results. ", "page_idx": 9}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/a094442a707abba5394817ced52d96251e6d7ad9450fe90c6846aef923fcd0ef.jpg", "img_caption": ["Figure 6: Qualitative results for pose-conditioned ControlNet. Qualitative results for pre-trained pose-conditioned ControlNet, without and with AMF. Both methods are initialized with the same seed. AMF leads to results that are more plausible and better align with the conditions. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusions and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced Zero-to-Hero, a training-free method to boost the robustness of novel view synthesis. We enhanced the performance of a pre-trained Zero-1-to-3 diffusion model using two key innovations: a test-time attention map filtering mechanism that enhances output realism, and an effective use of source view information to improve input consistency. Our approach also features a novel timestep scheduler for maintaining computational efficiency. In future work, we aim to refine our method by developing trainable filtering mechanisms, enhancing pose authenticity via cross-attention manipulation, and extending our approach to other diffusion-based generative tasks. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Our method, operating at test-time, is limited by the generative capabilities of the pre-trained model. If Zero-1-to-3 cannot correctly generate the target pose, our method may not enhance the output, as demonstrated in the inset figure. ", "page_idx": 9}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/a1adf2cdc38dc9177b8f6566a9de13676e3f1b0e2ceea76ef3af0d0b3927feb7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Broader impact. Through enhancing view synthesis, our method offers significant benefits to applications in virtual reality, augmented reality, and robotics. As it requires no further training, it is readily accessible. However, this accessibility also simplifies the creation of realistic novel views, which could be exploited for malicious purposes such as deepfakes. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We sincerely thank Matan Atzmon for impactful discussions and James Lucas for his invaluable feedback. Or Litany is a Taub fellow and is supported by the Azrieli Foundation Early Career Faculty Fellowship. This research was supported in part by an academic gift from Meta. The authors gratefully acknowledge this support. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel Cohen-Or. Crossimage attention for zero-shot appearance transfer. arXiv preprint arXiv:2311.03335, 2023. [2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 843\u2013852, 2023. [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [4] L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers, pages 177\u2013186. Springer, 2010. [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22560\u2013 22570, 2023. [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021. [7] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4217\u20134229, 2023. [8] Yabo Chen, Jiemin Fang, Yuyang Huang, Taoran Yi, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, and Qi Tian. Cascade-zero123: One image to highly consistent 3d with self-prompted nearby views. arXiv preprint arXiv:2312.04424, 2023. [9] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of $10\\mathrm{m}{+3}\\mathrm{d}$ objects. Advances in Neural Information Processing Systems, 36, 2024.   \n[10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13142\u201313153, 2023.   \n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[12] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 2553\u20132560. IEEE, 2022.   \n[13] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.   \n[14] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In International Conference on Machine Learning, pages 11808\u201311826. PMLR, 2023.   \n[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.   \n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[19] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.   \n[20] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 $\\{\\backslash\\mathrm{deg}\\}$ dynamic object generation from monocular video. arXiv preprint arXiv:2311.02848, 2023.   \n[21] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023.   \n[22] Yash Kant, Aliaksandr Siarohin, Michael Vasilkovsky, Riza Alp Guler, Jian Ren, Sergey Tulyakov, and Igor Gilitschenski. invs: Repurposing diffusion inpainters for novel view synthesis. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u201312, 2023.   \n[23] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[24] Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, and Kwang Moo Yi. Vivid-1-to-3: Novel view synthesis with video diffusion models. arXiv preprint arXiv:2312.01305, 2023.   \n[25] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3 $-45++$ : Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885, 2023.   \n[26] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36, 2024.   \n[27] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023.   \n[28] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453, 2023.   \n[29] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11461\u201311471, 2022.   \n[30] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.   \n[31] Daniel Morales-Brotons, Thijs Vogels, and Hadrien Hendrikx. Exponential moving average of weights in deep learning: Dynamics and beneftis. Transactions on Machine Learning Research, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[32] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. ", "page_idx": 12}, {"type": "text", "text": "[33] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, HsinYing Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. ", "page_idx": 12}, {"type": "text", "text": "[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021. ", "page_idx": 12}, {"type": "text", "text": "[35] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. ", "page_idx": 12}, {"type": "text", "text": "[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022. ", "page_idx": 12}, {"type": "text", "text": "[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015. ", "page_idx": 12}, {"type": "text", "text": "[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022. ", "page_idx": 12}, {"type": "text", "text": "[39] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero1 $^{23++}$ : a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. ", "page_idx": 12}, {"type": "text", "text": "[40] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multiview diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. ", "page_idx": 12}, {"type": "text", "text": "[41] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2023. ", "page_idx": 12}, {"type": "text", "text": "[42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. ", "page_idx": 12}, {"type": "text", "text": "[43] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. ", "page_idx": 12}, {"type": "text", "text": "[44] Bernard Spiegl, Andrea Perin, St\u00e9phane Deny, and Alexander Ilin. Viewfusion: Learning composable diffusion models for novel view synthesis. arXiv preprint arXiv:2402.02906, 2024. ", "page_idx": 12}, {"type": "text", "text": "[45] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. ", "page_idx": 12}, {"type": "text", "text": "[46] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22819\u201322829, 2023. ", "page_idx": 12}, {"type": "text", "text": "[47] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017. ", "page_idx": 12}, {"type": "text", "text": "[48] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Charles Loop, Nathan Morrical, Koki Nagano, Towaki Takikawa, and Stan Birchfield. Rtmv: A ray-traced multi-view synthetic dataset for novel view synthesis. IEEE/CVF European Conference on Computer Vision Workshop (Learn3DG ECCVW), 2022, 2022.   \n[49] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1921\u20131930, 2023.   \n[50] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.   \n[51] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628, 2022.   \n[52] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, CL Chen, and Lei Zhang. Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092, 2023.   \n[53] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7623\u20137633, 2023.   \n[54] Chenfeng Xu, Huan Ling, Sanja Fidler, and Or Litany. 3difftection: 3d object detection with geometry-aware diffusion features. CVPR, 2023.   \n[55] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola. Restart sampling for improving generative processes. Advances in Neural Information Processing Systems, 36:76806\u201376838, 2023.   \n[56] Xianghui Yang, Yan Zuo, Sameera Ramasinghe, Loris Bazzani, Gil Avraham, and Anton van den Hengel. Viewfusion: Towards multi-view consistency via interpolated denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9870\u20139880, 2024.   \n[57] Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Wangbo Yu, Munan Ning, and Li Yuan. Repaint123: Fast and high-quality one image to 3d generation with progressive controllable 2d repainting. arXiv preprint arXiv:2312.13271, 2023.   \n[58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[59] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018. ", "page_idx": 13}, {"type": "text", "text": "8 Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "8.1 Cross-Attention in Zero-1-to-3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We demonstrate the degenerated cross-attention layers in Zero-1-to-3 in Fig. 7. We generate a random target view using Zero123-XL and extract the cross-attention map generated at timestep 900 in the last layer of the UNet. The same behaviour holds across all timesteps and layers. The displayed map is created by averaging over all the attention heads. ", "page_idx": 14}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/45117cd6667bdc84a83a93f36e69aaaf73814188b4d1c8cc1f8fb2fbd2e3ebe6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 7: Cross-Attention in Zero-1-to-3. (Left) The cross-attention map before applying softmax.   \n(Right) The degenerated all-ones attention map, produced by applying softmax on the left map. ", "page_idx": 14}, {"type": "text", "text": "8.2 Self-Attention in Zero-1-to-3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Fig. 8 we present additional examples showing the performance of Zero123-XL when the selfattention maps are replaced with the \u2019ground truth\u2019 maps, extracted from the real target as described in Sec. 4.1. ", "page_idx": 14}, {"type": "text", "text": "8.3 Qualitative Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Fig. 9, we provide more qualitative results that reinforce the effectiveness of Zero-to-Hero. ", "page_idx": 14}, {"type": "text", "text": "8.4 Quantitative Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "On the GSO evaluation, shown in Tab. 1 in the main paper, Zero123-XL improved performance over Zero-1-to-3 by utilizing $12\\mathbf{x}$ more data, yielding gains of [0.45, 0.003, -0.01, $2.9\\%]$ in PSNR, SSIM, LPIPS, and IoU, respectively. When applied to Zero-1-to-3, Our method achieved comparable gains, [0.40, 0.008, -0.01, $\\bar{1.8\\%}]$ , without using any additional data or further training. Notably, when applied to Zero123-XL, our method resulted in even larger improvements [0.63, 0.1, -0.01, $1.9\\%]$ (over Zero123-XL), demonstrating that these performance boosts cannot be solely achieved with merely more data. ", "page_idx": 14}, {"type": "text", "text": "Additionally, in Tab. 3, we report all metrics on RTMV dataset, showing substantial improvement over baselines. ", "page_idx": 14}, {"type": "text", "text": "8.5 Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We evaluated our method on the default checkpoint of Zero-1-to-3 and on Zero123-XL. We used the same hyper-parameters for both models, as follows. All experiments were run on a single NVIDIA RTX 4090. ", "page_idx": 14}, {"type": "text", "text": "Note that the length of the forward process of Stable Diffusion is $T=1000$ . ", "page_idx": 14}, {"type": "text", "text": "Attention map filtering pipeline. As mentioned in the method section of the main paper, we apply both in-step and cross-step aggregation in tandem. In detail, we preserve attention map history ", "page_idx": 14}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/b608738ea7d4ac25da5fcc74e0e1cfe83dd1b027dfd8ba9b9833fdb0a358b6d4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 8: Through the injection of ground-truth attention maps extracted from the target view, we demonstrate that self-attention maps are key to robust view synthesis. ", "page_idx": 15}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/70d74677d98f5b99572689659b1748cc6f9b54c972ca3962129aa78d01481c0e.jpg", "table_caption": ["Table 3: Quantitative evaluation on RTMV dataset. Zero-to-Hero consistently improves performance upon baselines. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "information denoted as $H_{t}$ . Subsequently, we integrate this historical information through a simple blending technique at each denoising step, along with our in-step map update: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widetilde{M}_{t,r}=\\alpha_{c}\\cdot f(M_{t,r},\\{\\widetilde{M}_{t,k}\\}_{k=1}^{r-1})+(1-\\alpha_{c})\\cdot H_{t+1},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\alpha_{c}\\in[0,1]$ , $f$ is an attention pooling function and $t+1$ represents the previous sampled step. ", "page_idx": 15}, {"type": "text", "text": "The historical information is updated at the last resampling iteration (the R-th iteration) of each timestep as follows: $H_{t}=\\alpha_{h}\\cdot\\overleftarrow{\\widetilde{M}}_{t,R}+(1-\\alpha_{h})\\cdot H_{t+1}$ , where $\\alpha_{h}$ is a decay factor within the range [0, 1]. This historical data is init ialized from the first refined self-attention map at the final resampling step, expressed as $H_{T}=\\widetilde{M}_{T,R}$ . ", "page_idx": 15}, {"type": "text", "text": "Resampling is performed during timesteps $t\\in[800,1000]$ , with $\\scriptstyle\\mathbf{R}=5$ . ", "page_idx": 15}, {"type": "text", "text": "In-step map update. In-step map update is applied at timesteps $\\mathfrak{t}\\in[800,1000]$ , with element-wise min-pooling as a denoising function in all our experiments. We find that min-pooling is useful during the earlier steps of the denoising, where the noise is substantial. ", "page_idx": 15}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/a33e1df99d917ea3935646c79b315e6d787309222d8a7d2f367343b2e93de380.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 9: Novel views generated from a single source image (far left column) at a specific target view angle (with different seeds), compared between Zero123-XL [27] and our Zero-to-Hero method. The ground-truth target view is displayed in the far right column. ", "page_idx": 16}, {"type": "text", "text": "We explored several simple denoising functions: ", "page_idx": 16}, {"type": "text", "text": "1. Averaging: $\\begin{array}{r}{\\widetilde M_{t,r}=\\frac{1}{r}(M_{t,r}+\\sum_{k=1}^{r-1}\\widetilde M_{t,k})}\\end{array}$ , which can be extended to weighted averaging.   \n2. Min pooling: $\\widetilde{M}_{t,r}=m i n(M_{t,r},\\{\\widetilde{M}_{t,k}\\}_{k=1}^{r-1})$ .   \n3. Exponential Moving Average: $\\widetilde{M}_{t,r}=\\alpha_{i}\\cdot M_{t,r}+(1-\\alpha_{i})\\cdot\\widetilde{M}_{t,r-1},\\mathrm{for}\\,\\alpha_{i}\\in[0,1].$ ", "page_idx": 16}, {"type": "text", "text": "While all three improved performance over vanilla resampling, we find min-pooling to work best during the early stages of the denoising process. ", "page_idx": 16}, {"type": "text", "text": "Cross-step map averaging. We perform cross-step information passing at timesteps $t\\in[600,1000]$ , with $\\alpha_{h}=0.5$ and $\\alpha_{c}=0.2$ . ", "page_idx": 16}, {"type": "text", "text": "Early-stage mutual self-attention. We apply Early-Stage mutual self-attention on timesteps $\\mathfrak{t}\\in[600$ , 1000] for Zero123-XL and on timesteps t $\\in[700$ , 1000] for Zero-1-to-3. As mentioned in Sec. 3, the input latent vector is concatenated to the generated latent vector in Zero-1-to-3. Therefore, the target queries contain information about the input view, introducing a bias towards the input view during the generation process. Although mutual self-attention manages to enforce consistency with the input, it sometimes causes pose shifts and visual artifacts. Therefore, We only apply mutual self-attention in the decoder layers of the UNet with spatial resolution of $16\\times16$ and $32\\times32$ (the two largest resolutions, since the latent dimension of Zero-1-to-3 is $32\\times32$ ). ", "page_idx": 16}, {"type": "text", "text": "Hourglass scheduling. We divide the denoising process into 3 generation stages. We define the early stage during timesteps $t\\in[\\tau_{e},\\tau_{T}]$ , where the general shape and geometry are determined. $\\tau_{T}$ is the length of the forward process[17, 42]. We define the middle stage for $t\\in[\\tau_{m},\\tau_{e}]$ , where both the shape and appearance are refined gradually. We define the last stage for $t\\in[0,\\tau_{m}]$ , where mostly fine-details are refined and added. As describes in the main paper, within each stage we sample steps uniformlly via DDIM sampling. However, the sampling rate is higher for the early and late stages. ", "page_idx": 16}, {"type": "text", "text": "In our experiments, we determine the different stages of the denoising process as follows, setting $\\lambda_{d e n}=5$ . The early stage is defined for $t\\in[800,1000]$ and we sample 10 timesteps uniformly within this interval. The middle stage is defined for $t\\in[200,800]$ and we sample 6 timesteps uniformly within this interval. The last stage is defined for $t\\in[0,200]$ and we sample 10 timesteps uniformly within this interval. In total, we sample 26 timesteps throughout the denoising process. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Setting the hype-parameters. All hyperparameters were tuned based on a small validation set of objects. Due to limited computational resources, the tuning process was not exhaustive. We found that the method\u2019s performance is not highly sensitive to most parameters. ", "page_idx": 17}, {"type": "text", "text": "1. Cross-step weight (alpha): We experimented with values ranging from 0.1 to 0.9 in increments of 0.1. Alpha determines the weight assigned to previous predictions in the cross-step aggregation. In Zero123-XL, early predictions were generally reliable, so a larger weight yielded better results. We maintained the same parameters for Zero-1-to-3. In the experiments conducted with additional models such as ControlNet and MVDream, a smaller weight generally produced better outcomes.   \n2. Resampling iterations (R): We observed that the model\u2019s performance is relatively insensitive to the choice of R, with values between 4 and 8 yielding similar results. While some objects beneftied from larger values $(\\sim\\!10)$ , the overall improvement was minimal. Additionally, as shown in Fig. 11, large values of R $(\\sim\\!15\\!-\\!20)$ can reduce diversity. For additional experiments with models like ControlNet and MVDream, we fixed R at 5 and did not test other values.   \n3. Filtering schedule: We apply resampling and our filtering mechanism during timesteps $t\\in[t_{m i n},T]$ . To set $t_{m i n}$ , we consider values from 500 to 900, in increments of 100.   \n4. MSA schedule: We apply Early-stage MSA in timesteps $t\\,\\in\\,[t_{m i n},T]$ . To set $t_{m i n}$ , we consider values from 400 to 1000, in increments of 100. ", "page_idx": 17}, {"type": "text", "text": "Additional Inference Cost Analysis. Our proposed modules add a computational overhead to the base model. In the paper, we addressed this by counting the overall number of function evaluations (NFE) and keeping it on par with the base model to ensure a fair comparison. We discuss the individual computational overhead of each module. ", "page_idx": 17}, {"type": "text", "text": "1. Resampling: Similar to the total number of denoising steps, T, the number of resampling iterations, R, linearly increases the NFE. Our chosen value for R and the timesteps at which we apply it resulted in mapping 26 denoising steps to a total of 66 NFE. Resampling does not introduce additional computational cost over adding standard denoising steps. 2. Mutual self-attention: The main additional cost of MSA is the necessity to generate the input view in addition to the target views, meaning the effective number of generated samples is increased by one. 3. Attention map flitering: We apply AMF during the early steps of the denoising process. We find that the in- and cross-step operation adds a small computational overhead. We maintain two additional instances of each attention map: the attention map from the previous timestep (for cross-step updates) and the refined map from the current timestep (for in-step updates). ", "page_idx": 17}, {"type": "text", "text": "We provide running times (in seconds) of Zero-1-to-3 and Zero-to-Hero for the same NFE (66) in Tab. 4. Note that the number of samples reported in the table does not include the input view generated in Zero-to-Hero. Overall, we manage to provide competitive running times. If both MSA and AMF are active (requiring the generation of the input), the running time is increased by approximately 1-1.5 seconds. If only AMF is active, the overhead is much smaller, averaging around 0.5 seconds (there is no need to generate the extra input view). ", "page_idx": 17}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/9ec7ee5ec48a8a4120cfe2e36b9c96d438910057cdb0b480b63198527c38079e.jpg", "table_caption": ["Table 4: Runtime analysis of the computational overhead of Zero-to-Hero. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "8.6 Ablation Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conduct ablation studies focused on each component on our method, and present the results for the default checkpoint of Zero-1-to-3 and for Zero123-XL. ", "page_idx": 18}, {"type": "text", "text": "In addition to the main ablation table shown in the main paper, we present the same study on Zero-1- to-3 in Tab. 5. Additionally, we provide qualitative results, demonstrating the common contributions of AMF and MSA in Fig. 10. ", "page_idx": 18}, {"type": "text", "text": "Hourglass scheduling. We run the original model with 25, 50 and 100 steps sampled uniformly using DDIM, and show that the hourglass scheduling, with 26 steps sampled in total, performs better or on par. The main benefit of our Hourglass scheduling is reducing the number of steps, but it also slightly improves the overall performance. Results are presented for Zero-1-to-3 in Tab. 8 and for Zero123-XL in Tab. 6. ", "page_idx": 18}, {"type": "text", "text": "Attention map filtering. We use the hourglass scheduling for all further experiments. We run the original model with and without resampling and attention filtering. Our experiments demonstrate that adding the attention flitering in addition to the resampling mechanism improves the performance across all metrics. Results are presented for Zero-1-to-3 in Tab. 9 and for Zero123-XL in Tab. 7. ", "page_idx": 18}, {"type": "text", "text": "Early-Stage mutual self-attention. To the best of our knowledge, most prior works utilizing MSA fall into two categories: ", "page_idx": 18}, {"type": "text", "text": "1. Training-free based methods [5]: In these works, MSA is typically applied after the general structure of the target is formed to transfer appearance details from the input to the target. In this scenario, MSA does not contribute to the initial structure formation of the target. 2. Training or fine-tuning based methods [52, 53]: These works incorporate MSA layers within the training or fine-tuning process. ", "page_idx": 18}, {"type": "text", "text": "Our approach is distinct as we employ MSA in a training-free manner, but crucially, we apply it from the beginning of the denoising process until the target structure stabilizes\u2014a phase we term \"Early-Stage Shape Guidance\". To validate the impact of early-stage MSA on structure, we conducted a simple experiment using Zero123-XL with 50 DDIM steps. We measure the effect of activating MSA at different stages of denoising on Intersection over Union (IoU) metric, noting that image quality metrics improved similarly. The results are reported in Tab. 10. Our approach demonstrate a significant improvement in the structural integrity of the image. Applying MSA in the later stages of the denoising process may hinder the results as it introduces a bias towards the input image. ", "page_idx": 18}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/47e07f5b7abb8700de89cd4c1ea29f77cb1306bf9764ddad611cb0968008c0e0.jpg", "table_caption": ["Table 5: Ablation study \u2014 Zero-1-to-3. We demonstrate the importance of each of Zero-to-Hero modules, applied to the base method Zero-1-to-3: Sample scheduling (Hourglass), Resampling (Resample), Attention map filtering (AMF), and Early stage Mutual Self-Attention (MSA). "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 6: Ablation study: Hourglass scheduling \u2014 Zero123-XL. We demonstrate the superiority of our Hourglass scheduling over uniform DDIM sampling with different number of denoising steps. The experiments are based on Zero123-XL. ", "page_idx": 18}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/def07ad11af386a84ff611c1daf053703668d0326f6e2a3b6ad2ab7d3dfb81b6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 7: Ablation study: Attention map flitering \u2014 Zero123-XL. We demonstrate the importance of Attention Map Filtering (AMF) over only applying Resampling (Resample). The experiments are based on Zero123-XL. ", "page_idx": 19}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/573b2993932c37a9523967a4cd575e09c387d1797845949fb78b34552206289a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/1f4954b084197ba0b33a0dc2b250e76e581d225efcdbaf45c832e1b62a7c6546.jpg", "table_caption": ["Table 8: Ablation study: Hourglass scheduling \u2014 Zero-1-to-3. We demonstrate the superiority of our Hourglass scheduling over uniform DDIM sampling with different number of denoising steps. The experiments are based on Zero-1-to-3. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/6a4c1a4c40a3fd1b6900d9246d28ab4c4bfa18996512763fb5b77b91481c3aab.jpg", "table_caption": ["Table 9: Ablation study: Attention map flitering \u2014 Zero-1-to-3. We demonstrate the importance of Attention Map Filtering over only applying Resampling (Resample). The experiments are based on Zero-1-to-3. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "3uQtNWNTwz/tmp/e10cf3f03d03c78be1a0793f9e74070a7c03236f168268e5c079028281ff75b1.jpg", "table_caption": ["Table 10: Ablation study: Attention map flitering \u2014 Zero-1-to-3. We demonstrate the importance of Attention Map Filtering over only applying Resampling (Resample). The experiments are based on Zero-1-to-3. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "8.7 Generation Diversity Ablation Study ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "While our attention filtering manages to generate more robust and consistent results, we find that excessive it might limit the generation diversity. We explore the two main factors that can reduce the diversity: the number of filtering iterations $R$ and the interval of timesteps $[t_{m i n},t_{m a x}]$ where we apply filtering. ", "page_idx": 19}, {"type": "text", "text": "Number of flitering iterations. In Fig. 11, we show the effect of different values of $R$ on the diversity of the results. All other hyper-parameters remain unchanged. We find that as $R$ grows, the generation diversity reduces, while the level of realism improves ", "page_idx": 19}, {"type": "text", "text": "Filtering schedule. We apply filtering during timesteps $t\\,\\in\\,[t_{m i n},T]$ . In Fig. 12, we show the effect of different values of $t_{m i n}$ on the diversity of the results. All the hyper-parameters besides $t_{m i n}$ remain unchanged. We observe that as we apply filtering for longer periods of the denoising steps, the results are more realistic but less diverse. ", "page_idx": 19}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/19d78228ac365413aab76d6f26f96af9598edfa75d453b0de1a26238a735e16b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 10: Qualitative results, illustrating where MSA and AMF excel. The first two rows present cases where MSA fixed black and random textures in the target views. This contribution shows a larger improvement in image quality metrics, although MSA usually cannot fully resolve significant structural issues. The third and fourth rows show structural improvements achieved with AMF. Finally, the last row shows a case where neither technique worked well enough on its own, but the combination did the work: the original result of Zero123-XL shows an implausible right hand and an incorrect pose of the right sword. MSA lead to more stable result, where the hand is not out-of-distribution, but could not resolve the incorrect pose of the sword. AMF lead to reasonable right hand and placement of the sword, but the sword is not aligned well the the texture of the original sword. The final result show correct structure and texture for the right hand and sword. ", "page_idx": 20}, {"type": "text", "text": "Through our ablation study, we find that in practice, a moderate application of attention maps flitering is sufficient for improving fidelity while preserving diversity. While excessive resampling might limit diversity (e.g. using large values of R), we only apply our flitering mechanism in the earlier steps of the denoising process, using a small value of R (5 in all our experiments). This enables our method to maintain diversity effectively. ", "page_idx": 20}, {"type": "text", "text": "When evaluating diversity against base models, we observed that some results of the base models were highly implausible, as can be seen in Fig 1 and Fig. 9 in the appendix. For example, Zero-1-to-3 produces out-of-distribution results, or results that are not aligned with either the input image or the target pose, leading to seemingly larger variance. However, this \"diversity\" is largely due to misalignment and artifacts, which comes at the expense of fidelity. ", "page_idx": 20}, {"type": "text", "text": "In our case, our model continues to generate diverse results that are both plausible and better aligned with the conditions (e.g. the various chair backs and turtles heights in Fig. 1). ", "page_idx": 20}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/aa956f01e103f5520bde35b1e11b4b6340576725ccffc823b4b81d58b5ed3821.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 11: Generation diversity wrt flitering iterations. We select an extreme change in viewpoint, and show how different choices of $R$ , the number of filtering iterations, affect the diversity of generated outputs. As flitering iterations increase, the results become less diverse and more realistic. ", "page_idx": 21}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/61ffc5faed662fd3b71d5a16b628a31bf9b557e17b20d47977ba1a815cb1882c.jpg", "img_caption": ["Figure 12: Generation diversity wrt flitering schedule. We select an extreme change in viewpoint, and show how different choices of the timesteps interval where flitering is applied, affect the diversity of results. As flitering is applied for longer periods during the denoising process, the results become less diverse and more realistic. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "8.8 Attention Map Filtering Beyond Novel View Synthesis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this subsection, we describe several preliminary experiments where we integrate attention map filtering to other base models. Remarkably, the integration of our proposed method into other base models is straightforward, demonstrating its applicability and simplicity. ", "page_idx": 22}, {"type": "text", "text": "In all the mentioned experiments, resampling is performed during timesteps $t\\in[700,1000]$ , with $\\scriptstyle\\mathbf{R}=5$ . Additionally, the cross-step parameters were slightly tuned for each base model. Mutual Self-Attention is not implemented, as it is not immediately applicable. ", "page_idx": 22}, {"type": "text", "text": "Conditional image generation. A brief study of ControlNet models [58] demonstrated that they suffer from similar limitations as zero123 and its follow ups. Namely, lack of condition enforcement and frequent appearance of visual artifacts. We implemented our proposed AMF module for two pre-trained ControlNet models. In Fig. 6 in the main paper, we provide qualitative results for poseconditioned ControlNet. In Fig. 13, we provide qualitative results for segmentation-conditioned ControlNet. We find that attention map filtering robustly mitigates artifacts across various prompts and seeds for both base models. ", "page_idx": 22}, {"type": "text", "text": "Multi-view synthesis. We integrate AMF into MVDream [40], a text-to-multiview model, and find that it helps to mitigate the same issues as in the single view case. We provide qualitative results in Fig. 14. ", "page_idx": 22}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/252ab3aeffba4fd69504c48155313f311b45931a6acb9ed5eb220b537cc5d66f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 13: Qualitative results for segmentation-conditioned ControlNet. Qualitative results for pre-trained segmentation-conditioned ControlNet, without and with AMF. Both methods are initialized with the same seed. AMF leads to results that are more plausible and better align with the conditions, including intricate details (e.g. the base model fails to generate the faucet and the right stool in the third row, while our method generates both and better align with the prompt.) ", "page_idx": 22}, {"type": "image", "img_path": "3uQtNWNTwz/tmp/8ee544a5008906d47a6a1c676ce1c425b003b7ce1d0595e1e1ab85f7066fb121.jpg", "img_caption": ["Figure 14: Qualitative results for MVDream. Qualitative results for MVDream, without and with AMF. Both methods are initialized with the same seed, and we provide results with two random seed per prompt. AMF leads to results that are more plausible and spatially consistent, while also better align with the conditions. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "8.9 Data ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Dataset creation. For each object, we render various views by randomly selecting camera parameters, an elevation $\\theta\\in[0,\\pi$ , an azimuth $\\phi\\in[0,2\\pi]$ and a radius $r\\in[1.5,2.\\dot{2}]$ . We manually select 8 views for each object by removing images captured from close viewpoints and flitering camera viewpoints only from the upper half of the unit sphere, to avoid significant lighting effects that swing the results regardless of the output geometry. ", "page_idx": 24}, {"type": "text", "text": "Dataset assets. We provide a list of the objects picked for the challenging subset of GSO. ", "page_idx": 24}, {"type": "text", "text": "1. 3D_Dollhouse_Sink 2. 3D_Dollhouse_Swing 3. 3D_Dollhouse_TablePurple 4. adiZero_Slide_2_SC 5. Air_Hogs_Wind_Flyers_Set_Airplane_Red 6. BALANCING_CACTUS 7. Chelsea_lo_fl_rdheel_nQ0LPNF1oMw 8. CHICKEN_RACER 9. Circo_Fish_Toothbrush_Holder_14995988 10. COAST_GUARD_BOAT 11. CREATIVE_BLOCKS_35_MM 12. Dino_5 13. Down_To_Earth_Ceramic_Orchid_Pot _Asst_Blue 14. FIRE_ENGINE 15. Great_Dinos_Triceratops_Toy 16. Guardians_of_the_Galaxy_Galactic_Battlers _Rocket_Raccoon_Figure 17. Hilary 18. Imaginext_Castle_Ogre 19. My_First_Rolling_Lion 20. My_First_Wiggle_Crocodile 21. My_Little_Pony_Princess_Celestia 22. Nickelodeon_Teenage_Mutant_Ninja _Turtles_Leonardo 23. Nickelodeon_Teenage_Mutant_Ninja _Turtles_Michelangelo 24. Nintendo_Mario_Action_Figure 25. Nintendo_Yoshi_Action_Figure ", "page_idx": 24}, {"type": "text", "text": "26. Olive_Kids_Mermaids_Pack_n   \n_Snack_Backpack   \n27. Ortho_Forward_Facing   \n28. Ortho_Forward_Facing_3Q6J2oKJD92   \n29. Ortho_Forward_Facing_QCaor9ImJ2G   \n30. Racoon   \n31. Razer_Kraken_Pro_headset_Full_size_Black   \n32. Remington_TStudio_Hair_Dryer   \n33. Rubbermaid_Large_Drainer   \n34. Schleich_African_Black_Rhino   \n35. Schleich_Hereford_Bull   \n36. Schleich_Lion_Action_Figure   \n37. Schleich_S_Bayala_Unicorn_70432   \n38. Smith_Hawken_Woven_BasketTray_Organizer   \n_with_3_Compartments_95_x_9_x_13   \n39. Sonny_School_Bus   \n40. Sootheze_Cold_Therapy_Elephant   \n41. SORTING_TRAIN   \n42. Toysmith_Windem_Up_Flippin_Animals_Dog   \n43. Squirrel   \n44. Transformers_Age_of_Extinction_Mega_1Step   \n_Bumblebee_Figure   \n45. TriStar_Products_PPC_Power_Pressure_Cooker   \n_XL_in_Black   \n46. Vtech_Roll_Learn_Turtle   \n47. W_Lou_z0dkC78niiZ   \n48. Weisshai_Great_White_Shark   \n49. Whey_Protein_Vanilla   \n50. ZX700_mzGbdP3u6JB ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The abstract and introduction reflect all our main contributions for the goal of improving the reliability and quality of novel views generated by Zero-1-to-3 and its variations. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper both discuss extensively the limitations of the work, and provide an example for a failure case to demonstrate these limitations. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper do not include theoretical results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide all implementation details, both for our method and dataset creation, making our method and results fully reproducible. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: We intend to release all code and data used in our paper in time for the camera-ready version. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper specifies all relevant details to understand all the experiments and results shown. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Each generation task (defined by a source view and camera viewpoint) is generated 3 times with different seeds, and the final score is the average of all score. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We note all experiments were run on a single NVIDIA RTX 4090. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The research follows the NeurIPS Code of Ethics in full. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss broader impacts: both positive and negative. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our method is training-free, builds upon an already released pre-trained model. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We cite the original paper that introduces Google Scanned Objects and all license details will be provided with the code and data. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not create 3D new assets. However, we generate a dataset of renderings that is well documented and will be released. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper doesn\u2019t involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper doesn\u2019t involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]