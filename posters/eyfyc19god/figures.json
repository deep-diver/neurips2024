[{"figure_path": "eyfYC19gOd/figures/figures_1_1.jpg", "caption": "Figure 1: We propose a novel explicit representation method for dynamic scene rendering that decomposes the space-time 4D encoding into the 3D format without the unsuitable low-rank assumption. We achieve significant improvements over the state-of-the-art models [44, 50] in rendering quality.", "description": "This figure shows a comparison of different dynamic scene rendering methods.  The input is a 4D space-time representation of a scene.  Grid4D (the proposed method) decomposes this 4D input into four 3D representations (xyz, xyt, yzt, xzt), using hash encoding, to avoid the limitations of previous plane-based methods that rely on the low-rank assumption. The figure visually demonstrates the improvement in rendering quality achieved by Grid4D compared to 4D-GS and DeformGS, showcasing more detail and better visual fidelity.", "section": "Introduction"}, {"figure_path": "eyfYC19gOd/figures/figures_3_1.jpg", "caption": "Figure 2: Comparison of our proposed 4D decomposed hash encoding with the plane-based explicit representation [44]. (a) Compared to the plane-based methods based on the low-rank assumption, our methods reduce the overlap ratio in the features from a half to a quarter when encoding points A and B with heavily overlapping coordinates. (b) is the t-SNE [39] visualization of all the features, and the colors denote the corresponding represented deformations. The diversity of colors demonstrates that the reduced overlap makes the features represent different deformations more effectively.", "description": "This figure compares the proposed 4D decomposed hash encoding method with the existing plane-based explicit representation method.  Subfigure (a) shows how the proposed method reduces feature overlap, improving the discriminative power of the features for deformation prediction. Subfigure (b) uses t-SNE visualization to show that the reduced overlap results in features that better represent different deformations.", "section": "3.2 4D Decomposed Hash Encoding"}, {"figure_path": "eyfYC19gOd/figures/figures_4_1.jpg", "caption": "Figure 3: The overview of Grid4D. Given the canonical Gaussians and the timestamp, we first encode the decomposed input separately. Then we apply the directional attention scores generated by the spatial static features to the temporal dynamic features, and we decode the features with a tiny multi-head MLP. Finally, the Gaussians deformed by the predicted deformations are splatted by the differentiable rasterization operation [13] to render the images for supervision.", "description": "This figure shows the overall architecture of the Grid4D model. It starts with canonical Gaussians and their timestamps as input.  These inputs are then processed through a 4D decomposed hash encoding, separating the spatial and temporal features.  Spatial features are passed through a spatial MLP to generate directional attention scores, which are then applied to the temporal features (processed by a temporal MLP). A multi-head decoder combines these features to produce deformation parameters for the Gaussians. Finally, differentiable rasterization renders the deformed Gaussians to produce the final output images.", "section": "3 Method"}, {"figure_path": "eyfYC19gOd/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative comparisons on the synthetic D-NeRF dataset [31] with our baselines [8, 44, 50].", "description": "This figure compares the visual quality of scene reconstruction by Grid4D against three other state-of-the-art methods: TiNeuVox, 4D-GS, and DeformGS.  Each row represents a different dynamic scene from the D-NeRF dataset, showing several frames from the sequence. The red boxes highlight areas where the differences in reconstruction quality between the methods are most apparent. The aim is to visually demonstrate that Grid4D produces higher-fidelity reconstructions compared to the baselines. Ground truth images are provided for reference.", "section": "4.2 Comparisons"}, {"figure_path": "eyfYC19gOd/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative comparisons on the real-world HyperNeRF [28] dataset.", "description": "This figure compares the visual quality of several dynamic scene rendering methods on the real-world HyperNeRF dataset.  The models compared are TiNeuVox, 4D-GS, DeformGS, and Grid4D (the authors' proposed method). Each row represents a different scene with multiple frames showing the reconstruction quality at various time points. Red boxes highlight areas where differences in reconstruction are apparent. The Ground Truth row provides the original images for reference, allowing for a direct comparison of each method's accuracy in reconstructing the dynamic scenes.", "section": "4.2 Comparisons"}, {"figure_path": "eyfYC19gOd/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative comparisons on the synthetic D-NeRF dataset [31] with our baselines [8, 44, 50].", "description": "This figure compares the visual results of Grid4D with three baseline methods (4D-GS, DeformGS, TiNeuVox) on four dynamic scenes from the D-NeRF dataset.  Each scene shows a sequence of frames, illustrating the model's ability to render novel views of the dynamic scene over time. The red boxes highlight areas with fine details, showing differences in rendering quality between the models. Grid4D demonstrates superior performance in terms of visual fidelity compared to baselines, showing more detailed and accurate results that are closer to the ground truth.", "section": "4.2 Comparisons"}, {"figure_path": "eyfYC19gOd/figures/figures_9_1.jpg", "caption": "Figure 3: The overview of Grid4D. Given the canonical Gaussians and the timestamp, we first encode the decomposed input separately. Then we apply the directional attention scores generated by the spatial static features to the temporal dynamic features, and we decode the features with a tiny multi-head MLP. Finally, the Gaussians deformed by the predicted deformations are splatted by the differentiable rasterization operation [13] to render the images for supervision.", "description": "This figure illustrates the overall architecture of the Grid4D model. It begins with canonical Gaussians and their timestamp as input. These inputs are then encoded separately using the 4D decomposed hash encoding method into spatial and temporal features. A directional attention mechanism combines these features, and a multi-head decoder predicts the Gaussian deformations. Finally, differentiable rasterization renders the deformed Gaussians into images, which are used for model supervision during training.", "section": "3 Method"}, {"figure_path": "eyfYC19gOd/figures/figures_13_1.jpg", "caption": "Figure 8: Architecture of our multi-head directional attention decoder.", "description": "This figure shows the architecture of the multi-head directional attention decoder used in Grid4D.  It illustrates the process of generating attention scores from spatial features using a small spatial MLP (Multilayer Perceptron), scaling those scores to a directional range (-1, 1), and then applying those scores to temporal features via a dot product operation before decoding the final deformation parameters using another MLP.  The output of this decoder provides the deformation parameters (rotation, translation, scaling) for the Gaussians.", "section": "3.3 Multi-head Directional Attention Decoder"}, {"figure_path": "eyfYC19gOd/figures/figures_15_1.jpg", "caption": "Figure 4: Qualitative comparisons on the synthetic D-NeRF dataset [31] with our baselines [8, 44, 50].", "description": "This figure shows a qualitative comparison of the dynamic scene rendering results produced by Grid4D and three baseline methods (4D-GS, DeformGS, and TiNeuVox) on four sequences from the D-NeRF dataset. Each sequence contains several frames showing different poses of the same object. The image demonstrates that Grid4D outperforms the baseline models in terms of visual quality, especially in handling complex deformations and details.", "section": "4.2 Comparisons"}, {"figure_path": "eyfYC19gOd/figures/figures_15_2.jpg", "caption": "Figure 5: Qualitative comparisons on the real-world HyperNeRF [28] dataset.", "description": "This figure shows qualitative comparisons of different dynamic scene rendering methods (TiNeuVox, 4D-GS, DeformGS, and Grid4D) on the real-world HyperNeRF dataset. Each row represents a different scene, and each column represents a different method. The ground truth images are shown in the last column. The figure visually demonstrates the performance of each method on various scenes with complex motions and dynamic objects.", "section": "4.2 Comparisons"}, {"figure_path": "eyfYC19gOd/figures/figures_16_1.jpg", "caption": "Figure 1: We propose a novel explicit representation method for dynamic scene rendering that decomposes the space-time 4D encoding into the 3D format without the unsuitable low-rank assumption. We achieve significant improvements over the state-of-the-art models [44, 50] in rendering quality.", "description": "This figure compares the results of three different methods for dynamic scene rendering:  Space-Time 4D Input, 4D-GS, and Grid4D (the authors' method). Each method is shown rendering the same dynamic scene from several different viewpoints.  Grid4D uses a novel 4D decomposed hash encoding to represent the scene, while the other methods rely on less effective approaches. The figure demonstrates that Grid4D produces significantly higher-fidelity results.", "section": "Introduction"}, {"figure_path": "eyfYC19gOd/figures/figures_16_2.jpg", "caption": "Figure 4: Qualitative comparisons on the synthetic D-NeRF dataset [31] with our baselines [8, 44, 50].", "description": "This figure compares the visual quality of dynamic scene rendering results generated by Grid4D and three baseline methods (TiNeuVox, 4D-GS, DeformGS) on four sequences from the D-NeRF dataset.  Each sequence shows a different dynamic scene (Lego, Jumping Jacks, Hook, Hell Warrior) with multiple frames representing different stages of the action. The comparison demonstrates Grid4D's superior ability to render detailed, realistic, and sharp images compared to the baseline models, which suffer from various artifacts such as blurry edges and lack of fine details.  The ground truth images are also included for reference.", "section": "4.2 Comparisons"}, {"figure_path": "eyfYC19gOd/figures/figures_17_1.jpg", "caption": "Figure 4: Qualitative comparisons on the synthetic D-NeRF dataset [31] with our baselines [8, 44, 50].", "description": "This figure compares the visual quality of dynamic scene rendering results generated by Grid4D and three baseline methods (TiNeuVox [8], 4D-GS [44], DeformGS [50]) on the D-NeRF dataset.  Each row represents a different dynamic scene, showing the results from the four methods and the corresponding ground truth. The comparison visually demonstrates Grid4D's superior performance in rendering quality compared to the other methods.", "section": "4.2 Comparisons"}]