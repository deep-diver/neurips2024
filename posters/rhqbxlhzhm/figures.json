[{"figure_path": "RHQbxlhzhm/figures/figures_7_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed optimization methods (quadratic and cubic surrogates) against existing Newton-type methods (exact Newton, quasi-Newton, proximal Newton) for training Cox Proportional Hazards models.  Two regularization scenarios are shown: l2 regularization and combined l1 + l2 regularization.  The plots demonstrate that the proposed methods achieve monotonic loss decrease and superior speed compared to existing methods. Note that the Newton-type methods fail to converge for weak regularization in the l2 scenario, while in the l1+l2 scenario, their speed is considerably lower than that of the proposed methods.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_7_2.jpg", "caption": "Figure 2: Variable selection on synthetic datasets with high correlation (correlation level p = 0.9). From left to right, the sample sizes are 1200, 1000, and 800, respectively. The F1 score (the higher the better) is closely related to the support recovery rate. On the left two plots, we can see our method recovers the true variables significantly better than other methods (100% recovery rate on the left plot; true support size is 15). As the sample size decreases, the F1 score decreases for all methods.", "description": "This figure compares the performance of four different variable selection methods on three synthetic datasets with high feature correlation.  The datasets vary in sample size (1200, 1000, and 800). The x-axis represents the number of selected features (support size), and the y-axis represents the F1-score, a metric that balances precision and recall in feature selection. The results show that the proposed 'BeamSearch (Ours)' method significantly outperforms existing methods, achieving 100% support recovery on the largest dataset. As expected, performance degrades slightly as the sample size decreases for all methods.", "section": "4 Experiments"}, {"figure_path": "RHQbxlhzhm/figures/figures_8_1.jpg", "caption": "Figure 26: 5-fold Cross-validation on EmployeeAttrition dataset. Comparision with other cox models, metric: CIndex", "description": "This figure compares the performance of several Cox models (skglmALassoCox, abess, sksurvCoxnet, and BeamSearch) on the Employee Attrition dataset.  The plots show the relationship between support size (number of selected features) and the concordance index (C-index), a measure of predictive accuracy for survival models.  The top row displays training C-index versus support size while the bottom row displays testing C-index versus support size. The goal is to show that our BeamSearch method can achieve a good C-index (high predictive accuracy) with a smaller support size (fewer features) than other existing Cox models. This indicates it is more efficient and potentially more interpretable.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_8_2.jpg", "caption": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex", "description": "This figure compares the performance of different Cox proportional hazards models on the Dialysis dataset in terms of C-index. The x-axis represents the support size (number of selected features), and the y-axis represents the C-index for both training and testing sets.  The plot shows that BeamSearch (the authors' method) achieves higher C-index values with smaller support sizes compared to other methods like skglmALassoCox, abess, and sksurvCoxnet.  This demonstrates the effectiveness of BeamSearch in selecting important features and building a sparse yet accurate model.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_17_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival method with existing Newton-type methods for training Cox Proportional Hazards models on the Flchain dataset.  The plots show that FastSurvival converges faster and avoids the loss explosion issues encountered by the Newton-type methods, especially when regularization is weak.  Both l2 and l1+l2 regularization scenarios are shown.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_34_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "The figure compares the performance of the proposed optimization methods (quadratic and cubic surrogates) with existing Newton-type methods (exact Newton, quasi-Newton, and proximal Newton) for training Cox Proportional Hazards (CPH) models on the Flchain dataset.  The left plots show loss vs. iterations for l2-regularized (\u03bb2 = 1) and l1 + l2-regularized (\u03bb1 = 1, \u03bb2 = 5) CPH problems.  It demonstrates that the proposed methods ensure monotonic loss decrease, unlike the Newton-type methods which show loss explosion or slow convergence in the l2 case and slower convergence in l1 + l2.  The right plots show loss vs. time, highlighting the superior computational efficiency of the proposed methods. Appendix D provides more results on different datasets.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_34_2.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "The figure compares the performance of the proposed methods (quadratic and cubic surrogates) with three baseline Newton-type methods (exact Newton, quasi-Newton, proximal Newton) for training Cox Proportional Hazards (CPH) models on the Flchain dataset.  It shows that the proposed methods maintain monotonically decreasing loss functions, while the baselines often encounter issues of loss explosion, especially when regularization is weak.  Furthermore, the proposed methods achieve significantly faster convergence times.", "section": "Experiments"}, {"figure_path": "RHQbxlhzhm/figures/figures_35_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival method against existing optimization methods for training Cox Proportional Hazards (CPH) models.  The plots show the loss function value over iterations and time for both l2-regularized and l1+l2-regularized CPH problems, highlighting the superior speed and convergence of FastSurvival, especially when handling weak regularization where other methods fail to converge.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_35_2.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival method against several baseline methods for training Cox Proportional Hazards models.  The left plots demonstrate that FastSurvival avoids the issue of exploding losses observed in the baseline Newton-type methods, especially when regularization is weak.  The right plots showcase that FastSurvival achieves significant speed improvements compared to the baselines due to its efficient computation of first and second-order derivatives, leading to faster convergence in terms of wall-clock time.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_36_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival method with existing methods for training Cox Proportional Hazards models on the Flchain dataset.  It shows that FastSurvival achieves monotonic loss decrease and is significantly faster than existing methods, even when dealing with high-dimensional or highly correlated data, addressing the vanishing second-order derivative issues of existing Newton-based methods.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_36_2.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival method against existing Newton-type methods for training Cox Proportional Hazards (CPH) models.  It shows that FastSurvival converges much faster and more reliably than other methods, particularly when regularization is weak.  The plots illustrate both the number of iterations and the elapsed time to reach convergence for l2 and l1+l2 regularized CPH problems, highlighting FastSurvival's computational efficiency.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_37_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "The figure compares the performance of the proposed FastSurvival method against existing Newton-type methods for training Cox Proportional Hazards models on the Flchain dataset. It shows that FastSurvival ensures monotonic loss decrease and converges faster, especially when regularization is weak, highlighting its computational efficiency and robustness.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_37_2.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "The figure compares the performance of the proposed methods with existing Newton-type methods in solving l2-regularized and l1+l2-regularized CPH problems. The left plots show the loss versus the number of iterations, while the right plots illustrate the loss versus the elapsed time. The results demonstrate that the proposed methods converge faster and maintain monotonic loss decrease, unlike the Newton-type methods which often show loss explosion.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_38_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival methods against existing Newton-type methods for training Cox Proportional Hazards models on the Flchain dataset.  It demonstrates that FastSurvival achieves monotonic loss decrease and superior speed, particularly when regularization is weak, unlike the Newton-type methods which experience loss explosions.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_38_2.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival methods against existing Newton-type methods for training Cox Proportional Hazards models on the Flchain dataset.  It shows that under weak regularization, Newton methods fail to converge, while FastSurvival maintains monotonically decreasing loss. Furthermore, it demonstrates that FastSurvival's computational efficiency leads to faster convergence in wall-clock time, even when compared to approximate Newton methods.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_39_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival method against other optimization methods for training Cox Proportional Hazards models. It shows that FastSurvival ensures monotonic loss decrease and faster convergence compared to baseline methods, even when dealing with high-dimensional and correlated datasets.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_39_2.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "The figure shows the efficiency of the proposed methods against existing methods for l2 and l1+l2 regularized Cox proportional hazard models. The results indicate that the proposed methods are significantly faster than existing methods and ensure monotonic loss decrease, unlike existing methods that show loss explosion when regularization is weak.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_40_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed optimization methods (quadratic and cubic surrogates) with existing Newton-type methods (exact Newton, quasi-Newton, proximal Newton) for training Cox Proportional Hazards (CPH) models on the Flchain dataset.  Two regularization scenarios are shown: l2 regularization (\u03bb2 = 1) and combined l1 + l2 regularization (\u03bb1 = 1, \u03bb2 = 5). The plots illustrate loss versus number of iterations and loss versus elapsed time. The results demonstrate the superior convergence speed and stability of the proposed methods compared to the baselines.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_40_2.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed optimization methods (quadratic and cubic surrogates) against existing Newton-type methods (exact, quasi, and proximal Newton) for training Cox Proportional Hazards (CPH) models on the Flchain dataset.  It shows that the proposed methods exhibit faster convergence and prevent loss explosion, a common issue with Newton-type methods, especially when regularization is weak. The efficiency of the proposed methods is attributed to their low computational cost per iteration.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_41_1.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed methods (quadratic and cubic surrogates) with existing Newton-type methods (exact Newton, quasi-Newton, and proximal Newton) for training Cox Proportional Hazards (CPH) models on the Flchain dataset.  It shows that under weak regularization, Newton-type methods fail to converge, while the proposed methods demonstrate monotonic loss decrease.  Furthermore, even under stronger regularization, the proposed methods show significantly faster convergence in terms of both iterations and wall-clock time.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_41_2.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb2 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l1 + l2-regularized problem with \u03bb1 = 1 and \u03bb2 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival method against existing Newton-type methods (exact Newton, quasi-Newton, proximal Newton) for training Cox Proportional Hazards (CPH) models.  Two regularization scenarios are shown: l2 regularization (\u03bb2 = 1 and \u03bb2 = 5) and l1 + l2 regularization (\u03bb1 = 1, \u03bb2 = 5).  The plots demonstrate that FastSurvival exhibits faster convergence and avoids the loss explosion issues observed in the baseline Newton methods.", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_42_1.jpg", "caption": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex", "description": "This figure compares the performance of different Cox proportional hazards models on the Dialysis dataset in terms of C-index.  The x-axis represents the support size (number of features used in the model), while the y-axis shows the C-index (a measure of predictive accuracy). The plot includes results for four models: skglmALassoCox, abess, sksurvCoxnet, and BeamSearch (the authors' proposed method). Separate lines display the training and testing C-indices for each model. The results suggest that BeamSearch achieves similar or better predictive performance with a substantially smaller support size than the other methods, highlighting its ability to select important features and build more efficient models.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_42_2.jpg", "caption": "Figure 22: 5-fold Cross-validation on Dialysis dataset. Comparision with non-cox models, metric: CIndex", "description": "This figure compares the performance of the proposed BeamSearch method with several non-Cox models on the Dialysis dataset, using the C-index metric.  The x-axis represents the support size (number of selected features), while the y-axis represents the C-index score (higher is better), a measure of a model's ability to correctly rank survival times.  Error bars indicate variability across the 5-fold cross-validation runs. The plot shows that, as support size increases, the BeamSearch method achieves a higher C-index on both training and testing sets compared to other methods, indicating superior performance in variable selection for the Cox proportional hazards model.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_43_1.jpg", "caption": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex", "description": "This figure compares the performance of different Cox proportional hazards models on the Dialysis dataset in terms of C-index.  The x-axis represents the support size (number of selected features), and the y-axis represents the C-index, a measure of predictive accuracy.  The plot shows that as support size increases, C-index improves for all models but the BeamSearch (Ours) model consistently outperforms baselines. The error bars depict the standard deviation across 5-fold cross-validation. This suggests that the proposed beam search method is effective for variable selection in Cox models, leading to improved performance with a sparser model.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_43_2.jpg", "caption": "Figure 2: Variable selection on synthetic datasets with high correlation (correlation level p = 0.9). From left to right, the sample sizes are 1200, 1000, and 800, respectively. The F1 score (the higher the better) is closely related to the support recovery rate. On the left two plots, we can see our method recovers the true variables significantly better than other methods (100% recovery rate on the left plot; true support size is 15). As the sample size decreases, the F1 score decreases for all methods.", "description": "This figure compares the performance of the proposed beam search method with other variable selection methods on three synthetic datasets with varying sample sizes. The datasets are designed to have a high correlation level between the features (p=0.9). The F1 score, which measures the accuracy of the variable selection, is used as the evaluation metric. The results show that the proposed beam search method achieves significantly better F1 scores than other methods, particularly in the datasets with larger sample sizes (1200 and 1000 samples). As the sample size decreases, the F1 scores decrease for all methods, reflecting the challenges associated with variable selection when the data is limited.", "section": "Experiments"}, {"figure_path": "RHQbxlhzhm/figures/figures_43_3.jpg", "caption": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex", "description": "This figure compares the performance of different Cox proportional hazards models on the Dialysis dataset in terms of C-index.  The x-axis represents the support size (number of features used in the model), while the y-axis shows the C-index, a measure of predictive accuracy.  The plot displays the results for training and testing data, revealing how different models perform with varying levels of sparsity.  The green line represents the proposed BeamSearch method. ", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_44_1.jpg", "caption": "Figure 26: 5-fold Cross-validation on EmployeeAttrition dataset. Comparision with other cox models, metric: CIndex", "description": "This figure displays the results of a 5-fold cross-validation experiment on the EmployeeAttrition dataset, comparing the performance of four different Cox models in terms of the C-index metric.  The x-axis represents the support size (number of selected features), while the y-axis shows the C-index values. Error bars indicate the standard deviation of the C-index across the five folds.  The four models compared are: skglmALassoCox, abess, sksurvCoxnet, and BeamSearch (the authors' proposed method). The figure helps illustrate the tradeoff between model sparsity (support size) and predictive performance (C-index) for each model.  It is particularly useful in showing how well the BeamSearch method performs compared to other established Cox models for variable selection, especially when handling highly correlated data.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_44_2.jpg", "caption": "Figure 27: 5-fold Cross-validation on EmployeeAttrition dataset. Comparision with non-cox models, metric: CIndex", "description": "This figure compares the performance of different variable selection methods on the Employee Attrition dataset, focusing on the C-index metric. The x-axis represents the support size (number of selected features), and the y-axis shows the C-index.  The plot visualizes how the C-index varies as more features are included in the model.  Several different models, both Cox-based and non-Cox-based, are compared, illustrating how the new BeamSearch method compares against well-established techniques.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_45_1.jpg", "caption": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex", "description": "This figure compares the performance of different Cox proportional hazards models on the Dialysis dataset in terms of C-index.  The x-axis represents the support size (number of features used in the model), and the y-axis represents the C-index score (a measure of the model's ability to correctly rank the risk of failure for different samples).  The plot shows that as support size increases, the C-index generally increases for all methods, indicating improved predictive performance. However, the BeamSearch (Ours) method appears to consistently outperform the baselines, suggesting better variable selection capabilities and potentially a more efficient model.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_45_2.jpg", "caption": "Figure 2: Variable selection on synthetic datasets with high correlation (correlation level p = 0.9). From left to right, the sample sizes are 1200, 1000, and 800, respectively. The F1 score (the higher the better) is closely related to the support recovery rate. On the left two plots, we can see our method recovers the true variables significantly better than other methods (100% recovery rate on the left plot; true support size is 15). As the sample size decreases, the F1 score decreases for all methods.", "description": "This figure displays the results of variable selection experiments on three synthetic datasets with high feature correlation (p=0.9).  Each dataset has a different sample size (1200, 1000, and 800). The plots show the F1 score (a measure of accuracy in recovering the true relevant features) plotted against the number of selected features (support size). The results demonstrate the superior performance of the proposed BeamSearch method, especially in larger datasets, highlighting its ability to accurately identify relevant features even with high correlation.  As the sample size decreases, the performance of all methods degrades.", "section": "Experiments"}, {"figure_path": "RHQbxlhzhm/figures/figures_45_3.jpg", "caption": "Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the l2-regularized problem with \u03bb\u2082 = 1. For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the l\u2081 + l\u2082-regularized problem with \u03bb\u2081 = 1 and \u03bb\u2082 = 5. The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.", "description": "This figure compares the performance of the proposed FastSurvival method against existing optimization methods for training Cox Proportional Hazards (CPH) models.  It shows the convergence speed and computational efficiency of FastSurvival on the Flchain dataset under both l\u2082 and l\u2081+l\u2082 regularization.  The results highlight that FastSurvival maintains a monotonically decreasing loss, unlike the baseline methods that often experience loss explosion. The superior efficiency is attributed to the cost of each iteration being linearly proportional to the sample size (O(n)).", "section": "4.1 Accessing How Fast Our Methods Converge to Optimal Solutions"}, {"figure_path": "RHQbxlhzhm/figures/figures_46_1.jpg", "caption": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex", "description": "This figure compares the performance of different Cox proportional hazards models on the Dialysis dataset in terms of C-index, a metric for evaluating the discriminatory ability of survival models.  The models compared include skglmALassoCox, abess, sksurvCoxnet, and BeamSearch (the authors' proposed method).  The x-axis represents the number of features (support size) used in the model, and the y-axis shows the C-index for both training and test sets.  The plot helps demonstrate the trade-off between model sparsity (number of features) and prediction accuracy.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_46_2.jpg", "caption": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex", "description": "The figure shows the results of a 5-fold cross-validation experiment on the Dialysis dataset.  The experiment compares the performance of several Cox proportional hazards models, including the authors' proposed BeamSearch method, across different support sizes (number of selected features).  The metric used to evaluate performance is the Concordance Index (CIndex), a measure of how well the model ranks observations according to their risk of experiencing an event (e.g. death).  The plots show both the training CIndex and test CIndex for each model and support size.  The goal is to assess how the performance of each method varies as the number of selected features changes.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_47_1.jpg", "caption": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex", "description": "This figure compares the performance of different Cox proportional hazards models on the Dialysis dataset using 5-fold cross-validation.  The metric used to evaluate performance is the concordance index (C-index), which measures the ability of the model to correctly rank the survival times of two randomly selected individuals. The x-axis represents the support size (number of features included in the model), and the y-axis shows the C-index. The figure shows that our proposed BeamSearch method generally achieves higher C-index scores (better performance) with smaller support sizes compared to other Cox models such as skglmALassoCox, abess, and sksurvCoxnet.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}, {"figure_path": "RHQbxlhzhm/figures/figures_47_2.jpg", "caption": "Figure 2: Variable selection on synthetic datasets with high correlation (correlation level p = 0.9). From left to right, the sample sizes are 1200, 1000, and 800, respectively. The F1 score (the higher the better) is closely related to the support recovery rate. On the left two plots, we can see our method recovers the true variables significantly better than other methods (100% recovery rate on the left plot; true support size is 15). As the sample size decreases, the F1 score decreases for all methods.", "description": "This figure compares the performance of the proposed beam search method with other methods for variable selection on synthetic datasets with high feature correlation.  The x-axis shows the support size (number of selected features), and the y-axis shows the F1 score, a metric representing the accuracy of feature selection. The results demonstrate that the proposed beam search method significantly outperforms other methods, achieving perfect recovery of the true features (F1 score = 1.0) on datasets with larger sample sizes (1200, 1000).  As the sample size reduces to 800, the F1 score decreases for all methods, but the proposed method maintains better performance than the baselines.", "section": "4 Experiments"}, {"figure_path": "RHQbxlhzhm/figures/figures_47_3.jpg", "caption": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex", "description": "This figure compares the performance of different Cox proportional hazards models on the Dialysis dataset using 5-fold cross-validation. The metric used is the concordance index (C-index), which measures the ability of the model to correctly rank pairs of observations based on their predicted risk of experiencing an event (in this case, likely death or some failure related event).  The x-axis represents the support size (the number of features used in the model), and the y-axis represents the C-index.  It visualizes the trade-off between model sparsity (fewer features, smaller support size) and predictive accuracy (higher C-index). The lines likely represent mean values, and the error bars indicate variability (e.g. standard deviation) across the folds.", "section": "4.2 Accessing How Well Our Methods Perform Variable Selection"}]