[{"type": "text", "text": "FastSurvival: Hidden Computational Blessings in Training Cox Proportional Hazards Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiachang Liu' , Rui Zhang?, Cynthia Rudin2 1Cornell University, 2Duke University jiachang.liu@cornell.edu, r.zhang@duke.edu, cynthia@cs.duke.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Survival analysis is an important research topic with applications in healthcare, business, and manufacturing. One essential tool in this area is the Cox proportional hazards (CPH) model, which is widely used for its interpretability, fexibility, and predictive performance. However, for modern data science challenges such as high dimensionality (both $n$ and $p$ ) and high feature correlations, current algorithms to train the CPH model have drawbacks, preventing us from using the CPH model at its full potential. The root cause is that the current algorithms, based on the Newton method, have trouble converging due to vanishing second order derivatives when outside the local region of the minimizer. To circumvent this problem, we propose new optimization methods by constructing and minimizing surrogate functions that exploit hidden mathematical structures of the CPH model. Our new methods are easy to implement and ensure monotonic loss decrease and global convergence. Empirically, we verify the computational efficiency of our methods. As a direct application, we show how our optimization methods can be used to solve the cardinality-constrained CPH problem, producing very sparse high-quality models that were not previously practical to construct. We list several extensions that our breakthrough enables, including optimization opportunities, theoretical questions on CPH's mathematical structure, as well as other CPH-related applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Survival analysis, which studies time-to-event data, is an important research topic with a wide range of real-world applications. In medicine, survival analysis has been employed to model when a patient will die [44, 38, 10]. In business, it is useful for attrition prediction [30] (when an employees resigns) and churn prediction [31] (when a customer unsubscribes), and in manufacturing, it is used to predict when a physical system breaks down [42, 50]. A fundamental tool in analyzing such data is the Cox proportional hazards (CPH) model [8], a linear model under the assumption that features have a multiplicative effect on the risk of failure/event. Simple yet powerful, the CPH model has enjoyed great popularity due to its modeling fexibility (when coupled with additive models [27, 28, 67, 11, 1]). Moreover, in contrast to black box models, it is both interpretable and accurate. ", "page_idx": 0}, {"type": "text", "text": "However, with the advent of larger sample and feature spaces and more complex data, new challenges arise in using the CPH model to its full potential. Ideally, practitioners want to produce CPH models repeatedly, with feature engineering and preprocessing between iterations. Additionally, they want the CPH model to identify important variables [64, 14], even in presence of highly correlated features. However, current optimization methods for training the CPH model do not meet these needs. Current algorithms [62, 22, 23, 54], based on the generic Newton's method, are computationally intensive. More importantly, due to both vanishing second-order derivatives [53] and the use of approximation strategies that trade precision for effciency, existing optimization methods have trouble converging, either with the loss blowing up or the algorithm converging very slowly when we require the precision necessary to handle correlated variables. The latter issue is the core reason for incorrect variable selection when features are highly correlated. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we propose new optimization methods to train the CPH model and show that there is not necessarily a precision-efficiency tradeoff. Despite the CPH model being seemingly amenable to classical optimization approaches such as coordinate descent, this has not been attempted for the original CPH loss function due to its daunting complexity. However, through careful examination, we show instead that the complexity of the CPH loss function is really a blessing, rather than a curse. We discover hidden mathematical structures that allow us to design very efficient algorithms. We show both the first and second-order derivatives at each coordinate can be computed exactly in linear time complexity $(O(n))$ . Moreover, we show both derivatives are Lipschitz-continuous by making novel connections with the second and third central moment calculation in probability theory and statistics. All these discoveries lead us to design algorithms that essentially minimize a quadratic surrogate function and a cubic surrogate function, respectively. They are extremely easy to implement. ", "page_idx": 1}, {"type": "text", "text": "Empirically, we demonstrate the superior speed of our algorithms on large-scale datasets. In general, ours are significantly faster than all existing methods and rapidly converge to optimal high-precision solutions. Because our methods produce high-quality solutions, we apply them for variable selection in challenging regimes where features are highly correlated. We solve difficult cardinality-constrained CPH problems and produce models that are much sparser than the state-of-the-art methods. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are: (1) We find a critical flaw in the current optimization algorithms for the CPH method by pinpointing that they converge slowly with low precision. Sometimes, the loss does not converge and explodes. (2) To circumvent this issue, we propose novel algorithms that minimize a quadratic and a cubic surrogate function, respectively, with guaranteed convergence and loss descent at each iteration. The core novelty lies in discovering hidden mathematical structure, which allows for an efficiency way $(O(n))$ of calculating the second-order partial derivatives exactly. In addition, we show the first and second order partial derivatives are Lipschitz continuous. To calculate these Lipschitz constants, we leverage second and third central moments from theoretical statistics and probability theory. (3) Empirically, our method enjoys fast speed in training the loss function and results in superior performance when solving cardinality-constrained problems. ", "page_idx": 1}, {"type": "text", "text": "Our work constitutes a methodological breakthrough in training CPH models. At the end of the paper, we also discuss several exciting extensions and follow-up questions, showing how our new perspectives and discoveries open doors to many new research opportunities. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given a time-to-event dataset of $n$ samples with $\\{{\\pmb x}_{i},t_{i},\\delta_{i}\\}_{i=1}^{n}$ , where $\\pmb{x}\\in\\mathbb{R}^{p}$ is the feature vector with length $p$ $t_{i}\\in\\mathbb{R}$ is the observation time, and ${\\delta_{i}\\in\\{0,1\\}}$ is an indicator with 1 indicating that a failure event has happened, the CPH model can be used to learn and predict the risk of failure, commonly known as the hazard function in survival analysis. The CPH model predicts the hazard $h_{i}(t)$ for sample $i$ in a semiparametric way [62]. For review of related work, please see Appendix B. ", "page_idx": 1}, {"type": "equation", "text": "$$\nh_{i}(t)=h_{0}(t)e^{\\mathbf{x}_{i}^{T}\\boldsymbol{\\beta}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $h_{0}(t)$ is a baseline hazard function shared by all samples, and $\\beta\\in\\mathbb{R}^{p}$ is the parameter of interest. The nice thing about the CPH model is that $h_{0}(t)$ cancels out if we look only at the ratio of hazards of sample $i$ vs. all remaining samples at time $t_{i}$ i.e., ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\frac{h_{i}(t_{i})}{\\sum_{j\\in R_{i}}h_{j}(t_{i})}=\\frac{e^{\\mathbf{x}_{i}^{T}\\boldsymbol{\\beta}}}{\\sum_{j\\in R_{i}}e^{\\mathbf{x}_{j}^{T}\\boldsymbol{\\beta}}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Wwhere $R_{i}:=\\{j\\mid t_{j}\\geq t_{i}\\}$ is the set of indices whose observation time is greater than or equal to that ofsample $i$ . Such ratios are also called partial likelihoods. To estimate the parameter of interest, $\\beta$ we maximize the joint partial likelihood of all samples with failure events, which can be written as ", "page_idx": 1}, {"type": "equation", "text": "$$\nL(\\beta)=\\Pi_{i|\\delta_{i}=1}\\frac{e^{\\pmb{x}_{i}^{T}\\beta}}{\\sum_{j\\in R_{i}}e^{\\pmb{x}_{j}^{T}\\beta}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This is equivalent to minimizing the negative log partial likelihood [62], which is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell(\\beta)=-\\log L(\\beta)=\\sum_{i=1}^{n}\\delta_{i}\\left[\\log\\left(\\sum_{j\\in R_{i}}e^{\\mathbf{x}_{j}^{T}\\beta}\\right)-\\mathbf{x}_{i}^{T}\\beta\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The loss function $\\ell(\\beta)$ , while convex, is very mathematically involved. In addition to the double sum, the inner summation over $j$ is with respect to a different index set $R_{i}$ ,for each outer summation index $i$ . Such daunting complexity makes it difficult to employ first-order methods such as gradient descent because we cannot easily pick the right step size for each iteration, which plays a crucial role in practical running time. Therefore, past efforts have been focused on developing Newton-type (second-order) methods, where the loss function is approximated by a second-order Taylor expansion: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell(\\beta+\\Delta\\beta)\\approx\\ell(\\beta)+\\nabla_{\\beta}\\ell(\\beta)^{T}\\Delta\\beta+\\frac{1}{2}\\Delta\\beta^{T}\\nabla_{\\beta}^{2}\\ell(\\beta)\\Delta\\beta:=f(\\Delta\\beta).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The function $f(\\Delta\\beta)$ can be minimized by solving a linear system: $\\Delta\\hat{\\beta}=-(\\nabla_{\\beta}^{2}\\ell(\\beta))^{-1}\\nabla_{\\beta}\\ell(\\beta)$ To reveal the computational nuances more explicitly, we use an intermediate variabie $\\eta$ With $\\eta=X\\beta$ Then we can rewrite the approximation function $f(\\Delta\\beta)$ as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\Delta\\beta)=\\ell(\\beta+\\Delta\\beta)\\approx\\ell(\\boldsymbol{\\eta})+\\nabla_{\\boldsymbol{\\eta}}\\ell(\\boldsymbol{\\eta})^{T}\\boldsymbol{X}\\Delta\\beta+\\frac{1}{2}\\Delta\\beta^{T}\\boldsymbol{X}^{T}\\nabla_{\\boldsymbol{\\eta}}^{2}\\ell(\\boldsymbol{\\eta})\\boldsymbol{X}\\Delta\\beta.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "At each iteration, calculating the Hessian matrix $\\nabla_{\\eta}^{2}\\ell(\\eta)$ requires $O(n^{2})$ complexity. Past methods on the CPH model have resorted to various approximation strategies by replacing $\\nabla_{\\eta}^{2}\\ell(\\eta)$ With $H(\\eta)$ to reduce the computational complexity: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{H(\\pmb{\\eta})=\\dot{\\nabla_{\\eta}^{2}}\\ell(\\pmb{\\eta})}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\neq\\eta\\,n o\\;a p p r o x i m a t i}\\\\ &{H(\\pmb{\\eta})_{i j}=\\left\\{\\left[\\nabla_{\\eta}^{2}\\ell(\\eta)\\right]_{i i}\\quad\\mathrm{~if~}i=j\\right.}&{\\quad\\quad\\quad\\quad\\neq i g n o r e\\;o f\\!f\\!\\cdot\\!d i a g o n a l\\;t e r m}\\\\ &{H(\\pmb{\\eta})=\\dot{\\mathrm{diag}}(\\nabla_{\\eta}\\ell(\\pmb{\\eta})+\\delta),}&{\\quad\\quad\\quad\\neq d i a g o n a l\\;u p p e r\\;b o u n d\\;o n\\;\\nabla_{\\eta}^{2}\\ell(\\pmb{\\eta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathrm{diag(\\cdot)}$ constructs a matrix with its diagonal equal to the input vector and other entries equal to 0. There are two major problems with the above approaches. One common problem is that the these Newton-type methods inherently have trouble converging beyond the local region of minimizers without backtrack line search [53]. We provide a concrete example to demonstrate this issue in the experiment section. Ideally, we want to avoid backtracking because this increases the running time. In contrast, our methods do not have this flaw and guarantee global convergence. ", "page_idx": 2}, {"type": "text", "text": "The other problem is that when the above approaches do converge to the optimal solutions, none of them can converge with high precision fast enough (in a practical sense). The exact Newton's method [22] has a local quadratic convergence rate, but each iteration can take a long time. Quasi Newton [62] and proximal Newton [51]  methods are computationally much cheaper to evaluate per iteration, but they make less progress toward the optimal solution. In the next section, we show that, by exploiting hidden mathematical structure, we can obtain the best of both worlds: cheap evaluation per iteration and fast convergence with respect to the number of iterations. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1  New Formulas for First, Second, and Third Order Partial Derivatives ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As we have mentioned, the reason for the diagonal approximations of $\\nabla_{\\pmb{\\eta}}^{2}\\ell(\\pmb{\\eta})$ is to reduce the complexity of the mathematics and associated high computational cost. Here, we take a completely different approach from past methods. First, we avoid making any approximations and embrace the full Hessian matrix. Second, we bypass the intermediate step of calculating the Hessian in the sample space $\\eta$ and focus on the Hessian in the feature space $\\beta$ . The involved mathematics may already sound complicated, but we do not stop here. We apply these two ideas not only to the second order partial derivatives but the third order partial derivatives as well. Although this seems like a burdensome task, we show that the end result is very elegant and has an intuitive interpretation. Out of complexity comes simplicity. We summarize the relevant results in the first following theorem: ", "page_idx": 2}, {"type": "text", "text": "Theorem 3.1. For the CPH loss function defined in Equation (4), the first, second, and third order partial derivatives with respect to coordinate $l$ are: ", "page_idx": 3}, {"type": "text", "text": "Ist order partial derivative: ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\ell(\\beta)}{\\partial\\beta_{l}}}=\\sum_{i=1}^{n}\\delta_{i}\\left(\\sum_{k\\in R_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}{e^{\\eta_{j}}}}}X_{k l}\\right)-\\sum_{i=1}^{n}\\delta_{i}X_{i l}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2nd order partial derivative: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\ell(\\beta)}{\\partial\\beta_{l}^{2}}=\\sum_{i=1}^{n}\\delta_{i}\\left[\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}-\\left(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3rd order partial derivative: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial^{3}\\ell(\\beta)}{\\partial\\beta_{l}^{3}}=\\sum_{i=1}^{n}\\delta_{i}\\left[\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{3}+2\\left(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\right)^{3}\\right.}\\\\ {\\left.-3\\left(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}\\right)\\left(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\right)\\right].}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof can be found in Appendix A. The first, second, and third order partial derivatives all have a probabilistic interpretation. Notice that for any $i$ the coefficients in front of $X_{k l},X_{k l}^{2}$ and $X_{k l}^{3}$ are nonnegative and sum up to 1, i.e., $e^{\\eta_{k}}/(\\sum_{j\\in R_{i}}e^{\\eta_{j}})\\geq0$ and $\\begin{array}{r}{\\sum_{k\\in R_{i}}[e^{\\eta_{k}}/(\\sum_{j\\in R_{i}}e^{\\tilde{\\eta}_{j}^{*}})]=1}\\end{array}$ Then, we can regard these coefficients as a discrete probability distribution. Thus, for Equation (8), the term inside $[\\cdot]$ resembles the variance or second order central moment formula: $\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=$ $E[(X-\\mathbb{E}[{\\dot{X}}])^{2}]$ . For Equation (9), the term inside $[\\cdot]$ resembles the skewness or third order central moment formula: $\\mathbb{E}[X^{3}]^{\\prime}+2(\\mathbb{E}[X])^{3}-3\\mathbb{E}[X^{2}]\\mathbb{E}[\\dot{X}]^{\\prime}=\\mathbb{E}[(X-\\mathbb{E}[X])^{3}]$ ", "page_idx": 3}, {"type": "text", "text": "One may wonder whether for higher orders (order $r\\geq4\\AA$ ), the relationship between the $r$ -thorder partial derivative and $r$ -th central moment still preserve. The answer is no and this can be easily deduced from the following lemma. The proof can be found in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.2. Let us define $C_{r}$ to be the $r$ -th central moment with ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{r}:=\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then we can calculate the partial derivative of $C_{r}$ with respect to $\\beta_{l}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\beta_{l}}\\Big(C_{r}\\Big)=C_{r+1}-r\\cdot C_{2}\\cdot C_{r-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "From Lemma 3.2, we can see why the connection to central moment does not work for higher order partial derivatives. If $r\\,=\\,2$ , the second term in Equation (11) disappears, i.e., $C_{r-1}=C_{1}=0$ Therefore, we get $\\partial C_{2}/\\partial\\beta_{l}=C_{3}$ . However, for $r\\geq3$ $C_{r-1}$ in general is not zero, so we cannot extrapolate this pattern to higher order partial derivatives. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 forms the basis upon which we build everything else. These results are not only mathematically interesting but also have significant implications for computation, which we elaborate in the next two sections. ", "page_idx": 3}, {"type": "text", "text": "3.2  Time Complexity of First and Second Order Partial Derivative Calculation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "From the connections to the second and third central moment, we have the following corollary regarding the time complexity of calculating the first, second, and third order derivatives: ", "page_idx": 3}, {"type": "text", "text": "Corollary 3.3. For the CPH model, the time complexities to calculate $\\frac{\\partial\\ell(\\beta)}{\\partial\\beta_{l}}$ and $\\frac{\\partial^{2}\\ell(\\beta)}{\\partial\\beta_{l}^{2}}$ are $O(n)$ ", "page_idx": 3}, {"type": "text", "text": "This is a surprising result, especially for the second order partial derivatives. The intermediate Hessian, $\\nabla_{\\pmb{\\eta}}^{2}\\bar{\\ell}(\\pmb{\\eta})$ , takes $O(n^{2})$ to compute, so we would expect the second order partial derivative, alt)= e XTV2,(m) e, would take (n2) to compute as well Yt, th time complexity is just $O(n)$ . We use the first order partial derivative formula, Equation (7), as an example to explain why this happens. We ignore the second term $\\textstyle\\sum_{i=1}^{n}\\delta_{i}X_{i l}$ because it is just a constant. The first term in Equation (7) can be rewritten as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\delta_{i}\\left(\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)=\\sum_{i=1}^{n}\\delta_{i}\\left(\\frac{\\sum_{k_{1}\\in R_{i}}e^{\\eta_{k_{1}}}X_{k_{1}l}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let us focus on the numerator inside the parenthesis for now. For the entire sequence $(i=1,2,...,n)$ of numerator terms, we can obtain all of them together at the cost of $O(n)$ by performing reverse cumulative summation. The same is true when we obtain the entire sequence of denominators. Once we have all numerators and denominators, calculating the entire sequence of ratios inside the parenthesis also costs $O(n)$ . Finally, multiplying each ratio with $\\delta_{i}$ and summing up all these products costs $O(n)$ as well. Therefore, the computational cost to calculate the first order partial derivative is $O(n)$ . We can apply the same idea to the second order partial derivative formula, Equation 8, to show that the computational complexity is also $O(n)$ . Note that the reverse cumulative summation trick has already been explored in [62] for calculating the diagonal of $\\nabla_{\\pmb{\\eta}}^{2}\\ell(\\pmb{\\eta})$ in the sample space $\\eta$ , but this trick has not been used to calculate the partial derivatives in the feature space $\\beta$ ", "page_idx": 4}, {"type": "text", "text": "We will later see how this $O(n)$ time complexity allows us to design a second order optimization method whose evaluation cost per iteration is just as cheap as a first order optimization method. Before we discuss that, let us continue and discuss another computational implication of Theorem 3.1. ", "page_idx": 4}, {"type": "text", "text": "3.3 Lipschitz-Continuity Property of First and Second Order Partial Derivatives ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The connection to the central moment calculation allows us to conclude that the first and second order partial derivatives are Lipschitz-continuous. Moreover, we can calculate these Lipschitz constants explicitly. Recall that for a univariate function $f(x)$ , we say that the function is Lipschitzcontinuous [5] if there exists $L\\geq0$ such that for any two points in the domain of $f$ ,i.e., $x,y\\in\\mathcal{D}(f)$ we have $|f(x)-f(y)|\\leq L|x-y|$ . The value $L$ is called the Lipschitz constant for function $f(\\cdot)$ If the function is continuously differentiable, the previous definition is equivalent to the condition where the first order derivative is bounded, $|f^{\\prime}(x)|^{\\ }\\leq L$ for any $x\\in\\mathbb R$ [5]. ", "page_idx": 4}, {"type": "text", "text": "Not only can we say that the first and second order partial derivatives are Lipschitz-continuous, but we can also calculate the Lipschitz constants explicitly. We summarize the results in the theorem below. The proof can be found in Appendix A ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. For the second order partial derivatives in Equation (8), its absolute values are boundedbythefollowingformula: ", "page_idx": 4}, {"type": "equation", "text": "$$\n0\\leq\\frac{\\partial^{2}\\ell(\\pmb{\\eta})}{\\partial\\beta_{l}^{2}}\\leq\\frac{1}{4}\\sum_{i=1}^{n}\\delta_{i}\\big(\\operatorname*{max}_{k_{1}\\in R_{i}}X_{k_{1}l}-\\operatorname*{min}_{k_{1}\\in R_{i}}X_{k_{1}l}\\big)^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For the third order partial derivatives in Equation (9), its absolute values are bounded by the following formula: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial^{3}\\ell(\\pmb{\\eta})}{\\partial\\beta_{l}^{3}}\\right|\\leq\\frac{1}{6\\sqrt{3}}\\sum_{i=1}^{n}\\delta_{i}\\left|\\operatorname*{max}_{k_{1}\\in R_{1}}X_{k_{1}l}-\\operatorname*{min}_{k_{1}\\in R}X_{k_{1}l}\\right|^{3}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The availability of these Lipschitz constants suggests that we might construct surrogate functions. ", "page_idx": 4}, {"type": "text", "text": "3.4  Quadratic and Cubic Surrogate Functions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now have all the tools at hand to attack the original optimization problem. For a univariate convex $f(x)$ ,ifwehaveaccessto $L_{2}$ , the Lipschitz constant for its first order derivative, then we can construct the following quadratic surrogate function $g_{x}(\\cdot)$ [53]: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(x+\\Delta x)\\leq f(x)+f^{\\prime}(x)\\Delta x+{\\frac{1}{2}}L_{2}\\Delta x^{2}=:g_{x}(\\Delta x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If wehave access to $L_{3}$ , the Lipschitz constant for its second order derivative, then we can construct the following cubic surrogate function $h_{x}(\\cdot)$ [53]: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x+\\Delta x)\\leq f(x)+f^{\\prime}(x)\\Delta x+{\\frac{1}{2}}f^{\\prime\\prime}(x)\\Delta x^{2}+{\\frac{1}{6}}L_{3}|\\Delta x|^{3}=:h_{x}(\\Delta x)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A nice thing about these surrogate functions is that their minimizers can be computed analytically: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{\\displaystyle\\operatorname{argmin}_{\\Delta x}g_{x}(\\Delta x)=-{\\frac{1}{L_{2}}}f^{\\prime}(x)}\\\\ {\\displaystyle\\operatorname{argmin}_{\\Delta x}h_{x}(\\Delta x)=\\operatorname{sgn}(f^{\\prime}(x))\\cdot{\\frac{f^{\\prime\\prime}(x)-{\\sqrt{(f^{\\prime\\prime}(x))^{2}+2L_{3}|f^{\\prime}(x)|}}}{L_{3}}},}\\end{array}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "wherethefunction $\\operatorname{sgn}({\\mathord{\\cdot}})$ extractsthesign $(+\\ \\mathrm{or}\\ -)$ of the input. The analytical solution to the quadratic surrogate function is well known, but the analytical solution to this cubic surrogate function has not been well studied. We provide a derivation for Equation (18) in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Since these surrogate functions are convex and are upper bounds of the original functions, minimizing them will lead to a decrease of the original function $f(x)$ as well. This explains why our methods ensure monotonic decrease in loss and guarantee global convergence. The final algorithms are very easy to understand and can be thought of as coordinate descent-type methods. We anticipate these core ideas can be applied to solve a wide range of problems related to the CPH model. In the next subsection, we showcase two problems our algorithms can tackle. ", "page_idx": 5}, {"type": "text", "text": "3.5   Applications to Regularized and Constrained Problems ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Regularized Problem  The first problem is the regularized CPH problem whose penalty terms are separable. The penalties that qualify for this category include LASSO [64], ElasticNet [72], SCAD [15], MCP [68], etc. For the $\\ell_{1}$ -regularized problems, we can in fact find analytical solutions 2. ", "page_idx": 5}, {"type": "text", "text": "For the quadratic surrogate function, solving the $\\ell_{1}$ -regularized problem in Equation (15) is equivalent to solving the following optimization problem (with $a=f^{\\prime}(x)$ $b=L_{2}$ , and $c=x$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\operatorname*{argmin}_{\\Delta x}a\\Delta x+\\frac{1}{2}b\\Delta x^{2}+\\lambda_{1}|c+\\Delta x|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The solution for the above problem is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\left\\{\\begin{array}{l l}{-(a-\\lambda_{1})/b}&{\\mathrm{if}\\quad b c-a<-\\lambda_{1}}\\\\ {-(a+\\lambda_{1})/b}&{\\mathrm{if}\\quad b c-a>\\lambda_{1}}\\\\ {-c}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For the cubic surrogate function, solving the $\\ell_{1}$ -regularized problem of Equation (16) is equivalent to solving the following optimization problem (with $a=f^{\\prime}(\\bar{x}),b=f^{\\prime\\prime}(x),c=L_{3}$ ,and $d=x$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\operatorname*{argmin}_{\\Delta x}a\\Delta x+\\frac{1}{2}b\\Delta x^{2}+\\frac{1}{6}c|x|^{3}+\\lambda_{1}|d+\\Delta x|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "whose solution is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\left\\{\\begin{array}{l l}{\\mathrm{sgn}(d)\\Bigl(-b+\\sqrt{b^{2}-2c(\\mathrm{sgn}(d)a+\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)a+\\lambda_{1}\\leq0}\\\\ {\\mathrm{sgn}(d)\\Bigl(b+\\sqrt{b^{2}+2c(\\mathrm{sgn}(d)a-\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)(a-b d)-\\frac{1}{2}c d^{2}>\\lambda_{1}}\\\\ {\\mathrm{sgn}(d)\\Bigl(b+\\sqrt{b^{2}+2c(\\mathrm{sgn}(d)a+\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)(a-b d)-\\frac{1}{2}c d^{2}<-\\lambda_{1}}\\\\ {-d}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Equation (20) is well known in a slightly different format. Equation (22) has not been well studied in the past. We provide derivations for both in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Constrained Problem The second problem is the cardinality-constrained CPH problem. Recently, the beam search framework (a combination of the beam search method [66] from natural language processing and generalized orthogonal matching pursuit [13]) has shown promise in finding near optimal solutions for a class of $l_{0}$ -constrained nonconvex problems, including sparse ridge regression [47] and sparse logistic regression [48]. ", "page_idx": 6}, {"type": "text", "text": "Similar to the generalized orthogonal matching pursuit algorithm, we expand our support (starting from an empty set) by adding one feature at a time until the cardinality is satisfied. However, instead of selecting features based on partial derivatives, we select features based on which coefficient, if optimized, can result in the largest decrease of the loss function. After the feature is added into the support, we fine-tune all nonzero coefficients in the support. Additionally, during each support expansion step, we select multiple feature candidate instead of the best one, similar to the core idea in beam search. We use our coordinate descent methods to solve the feature selection step and the coefficient fine-tuning step. ", "page_idx": 6}, {"type": "text", "text": "Although the beam search framework has already been proposed for other cardinality-constrained problems, it cannot be applied directly to the CPH model without our coordinate descent methods to select features, especially in the highly correlated settings. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We test the effectiveness of our optimization methods on both synthetic and real-world datasets. We run experiments for both regularized and constrained problems mentioned in Section 3.5. Our main objectives are: 1) When minimizing the same objective functions, how fast can our methods converge to the optimal solutions when compared with all existing optimization methods for the CPH model? 2) When coupled with the beam search framework, how well can our methods help with variable selection when compared with the state-of-the-arts methods, especially for challenging scenarios where features are highly correlated? ", "page_idx": 6}, {"type": "text", "text": "4.1  Accessing How Fast Our Methods Converge to Optimal Solutions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare our methods (one based on the quadratic surrogate function and the other based on the cubic surrogate function) with the existing optimization methods outlined in Section 2: exact Newton method, the quasi Newton method, and the proximal Newton method. We run on both $\\ell_{2}$ -regularized CPH problems and $\\ell_{1}+\\ell_{2}$ -regularized CPH problems. The choices of these regularizations are: $\\lambda_{2}=\\bar{\\{}0,}1\\}$ and $\\lambda_{2}=\\{1,5\\}$ . The coefficients are all initialized to be O. In the main paper, we show results on the Flchain dataset in Figure 1. More results on other datasets can be found in Appendix D. During each iteration, the baseline methods [62, 51] optimize all coefficients at once, whereas our methods optimize coefficients sequentially with respect to the original loss function. To assess the per-iteration convergence rate, we plot the CPH loss against the number of iterations. To assess the practical running speed, we plot the CPH loss against the overall time elapsed (wall clock). From the left two plots of loss vs. number of iterations, we see that the Newton-type baselines sometimes have losses that blow up or increase during the initial phase of optimization. This is a common problem of Newton's method. Our methods are the only ones with monotonically decreasing loss curves. This is the main reason why only our method can be used for the beam search framework in the variable selection experiments. From the right two plots of loss vs. overall time elapsed, we can see that our methods are significantly faster than the baselines. This is due to the fact that both our first and second order partial derivatives are very cheap to compute (with time complexity ${\\cal O}(n).$ O,aswehave explained in Section 3.2. ", "page_idx": 6}, {"type": "text", "text": "4.2 Accessing How Well Our Methods Perform Variable Selection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare our method with both Cox-based methods and other model classes. For Cox-based models, we run on both synthetic datasets and real-world datasets. For other model classes, we only run on the real-world datasets. To assess how well different methods select important variables, features are highly correlated in all datasets. Synthetic datasets are generated with high correlation level, $\\rho=0.9$ . For each continuous feature on the real-world datasets, we perform binary thresholding for preprocessing [49] to obtain many one-hot encoded binary features. This preprocessing step result in highly correlated features on which it is challenging to perform variable selection. We use the following metrics to evaluate our solution qualities: CPH loss, CIndex, and IBS. On the synthetic datasets where we know the true coefficients, we also calculate the F1 score. We perform 5-fold cross validation and report the mean and standard deviation of different metrics on both the training and test sets. For details about the experimental setup, please see Appendix C. For Cox-based methods, we compare our method with Coxnet, Abess, and Adaptive Lasso. ", "page_idx": 6}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/9748afed5a7752bf81fadda1dcf8ed7498a3e05a1e0e89ec2d01f8ebcefc4023.jpg", "img_caption": ["Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are onthe $\\ell_{2}$ -regularized problem with $\\lambda_{2}=1$ . For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the $\\ell_{1}+\\ell_{2}$ -regularized problem with $\\lambda_{1}=1$ and $\\lambda_{2}\\,=\\,5$ . The exact Newton method cannot be directly applied, so we compare only with quasi Newton [62] and proximal Newton [51] methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/b8b5a65a281b36944edb2035cf8cc3aaa446b7f559dfdacf3a562856f19b6283.jpg", "img_caption": ["Figure 2: Variable selection on synthetic datasets with high correlation (correlation level $\\rho=0.9)$ From left to right, the sample sizes are 1200, 1000, and 800, respectively. The F1 score (the higher the better) is closely related to the support recovery rate. On the left two plots, we can see our method recovers the true variables significantly better than other methods ( $10\\bar{0}\\%$ recovery rate on the left plot; true support size is 15). As the sample size decreases, the F1 score decreases for all methods. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Results on the synthetic datasets are shown in Figure 2. We plot support size vs F1 score. The F1 score is closely related to the support recovery rate. Our method performs significantly better than the baselines. In particular, on the leftmost plot with 1200 samples, our method is the only one to achieve $I O O\\%$ recovery rate; the true support size is 15 and we recover all 15 features with a model of size 15. Results for the Employee Attrition dataset are shown in Figure 3. We plot support size vs. CIndex and support size vs. IBs. Similar to the trend on the synthetic datasets, our method performs significantly better than the baselines in terms of both metrics. Lastly, we compare our method with other model classes on the Dialysis dataset. The results are shown in Figure 4. We plot support size vs. CIndex and support size vs. IBS. The results indicate that other model classes are prone to overfitting on the training sets. Our method achieves the best accuracy-sparsity tradeoff. We are able to obtain solutions with the smallest number of coefficients without losing predictive performance. ", "page_idx": 7}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/85a3db3bbc276388d922c317339bc36bbda761dda87b80bd624844b576fe72ee.jpg", "img_caption": ["Figure 3: Variable selection on the Employee Attrition dataset. We show support size vs. CIndex (left two plots, the higher the better) and support size vs. IBS score (right two plots, the lower the better). We compare our method with Cox-based sparse learning methods. For both metrics, our method is significantly better than other baselines. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/ea844ddd16cfaa11e14966991533ea15289a59d9f5301fd670fb0842fcde3d55.jpg", "img_caption": ["Figure 4: Variable selection on the Dialysis dataset. We show support size vs. CIndex (left two plots, the higher the better) and support size vs. IBS score (right two plots, the lower the better). We compare our method with other model classes. For both metrics, our method obtains solutions that are significantly sparser than other model classes without losing accuracy on the test sets. Other model classes are prone to overfitting on the training sets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "All these results demonstrate the superior sparse learning capability of our method. For more results, with all baselines on all datasets, please see Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Limitations of FastSurvival  Our work focuses on efficient training and effective variable selection of the CPH model. Other model classes, such as trees, random forests, and neural networks, have their own unique merits in capturing complex patterns when the linear (or in our case, additive) model assumption is not satisfied. Another limitation is using the CPH model itself, since its assumptions do not always hold. Handling this question is out of scope for this work. ", "page_idx": 8}, {"type": "text", "text": "5  Conclusion and Future Outlook ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We presented new optimization methods to train the Cox proportional hazards (CPH) model by constructing and minimizing either a quadratic or a cubic surrogate function. We achieve computational efficiency by exploiting the hidden mathematical structures discovered for the CPH model. Our algorithms are able to train the model significantly faster than previous approaches while avoiding the issue of loss explosion. Furthermore, when applied to the variable selection problem, our method can produce solutions with much fewer parameters while maintaining predictive performance. There are many possible extensions to build upon this work. On the optimization side, it will be interesting to see whether we can derive analytical solutions for other types of regularizers mentioned in Section 3.5. On the theoretical side, questions remain whether higher order partial derivatives are Lipschitz-continuous and how to compute these Lipschitz constants. On the application side, we can apply our method to solve the CPH models with time-varying features [16], stratifications [40], and feature interactions [45]. ", "page_idx": 8}, {"type": "text", "text": "Code Availability ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Implementations of FastSurvival discussed in this paper are available at https : //github . com/ jiachangliu/FastSurvival. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors gratefully acknowledge funding support from the National Institutes of Health under 5R01-DA054994 and the Department of Energy under DE-SC0021358. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  A. Bender, A. Groll, and F. Scheipl. A generalized additive model approach to time-to-event analysis. Statistical Modelling, 18(3-4):299-321, 2018.   \n[2] Q. Bertrand, Q. Klopfenstein, P.-A. Bannier, G. Gidel, and M. Massias. Beyond 11: Faster and better sparse models with skglm. Advances in Neural Information Processing Systems, 35:38950-38965, 2022.   \n[3] D. Bertsimas, J. Dunn, E. Gibson, and A. Orfanoudaki. Optimal survival trees. Machine Learning, 111(8):2951-3023, 2022.   \n[4] D. Bertsimas, A. King, and R. Mazumder. Best subset selection via a modern optimization lens. The Annals of Statistics, pages 813-852, 2016.   \n[5] S. Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends@ in Machine Learning, 8(3-4):231-357, 2015.   \n[6] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks for multivariate time series with missing values. Scientific Reports, 8(1):6085, 2018.   \n[7]  T. Ching, X. Zhu, and L. X. Garmire. Cox-nnet: an artificial neural network method for prognosis prediction of high-throughput omics data. PLoS Computational Biology, 14(4):e1006076, 2018.   \n[8]  D. R. Cox. Regression models and life-tables. Journal of the Royal Statistical Society: Series B (Methodological), 34(2):187-202, 1972.   \n[9] 1. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 57(11):1413-1457, 2004.   \n[10] A. Dispenzieri, J. A. Katzmann, R. A. Kyle, D. R. Larson, T. M. Therneau, C. L. Colby, R. J. Clark, G. P. Mead, S. Kumar, L. J. Melton II, et al. Use of nonclonal serum immunoglobulin free light chains to predict overall survival in the general population. In Mayo Clinic Proceedings, volume 87, pages 517-523. Elsevier, 2012.   \n[11]  V. B. Djeundje and J. Crook. Identifying hidden patterns in credit risk survival data using generalised additive models. European Journal of Operational Research, 277(1):366-376, 2019.   \n[12]  E. Drysdale. Survset: An open-source time-to-event dataset repository.  arXiv preprint arXiv:2203.03094, 2022.   \n[13] E. R. Elenberg, R. Khanna, A. G. Dimakis, and S. Negahban. Restricted strong convexity implies weak submodularity. The Annals of Statistics, 46(6B):3539-3568, 2018.   \n[14]  J. Fan, Y. Feng, and Y. Wu. High-dimensional variable selection for Cox's proportional hazards model. In Borrowing Strength: Theory Powering Applications-A Festschrift for Lawrence D. Brown, volume 6, pages 70-87. Institute of Mathematical Statistics, 2010.   \n[15]  J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American statistical Association, 96(456):1348-1360, 2001.   \n[16]  L. D. Fisher and D. Y. Lin. Time-dependent covariates in the cox proportional-hazards regression model. Annual review of public health, 20(1):145-157, 1999.   \n[17] S. Fotso et al. PySurvival: Open source package for survival analysis modeling, 2019.   \n[18] J. Friedman, T. Hastie, H. Hofing, and R. Tibshirani. Pathwise coordinate optimization. The Annals of Applied Statistics, 1(2):302 - 332, 2007.   \n[19] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432-441, 2008.   \n[20] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, 33(1):1, 2010.   \n[21] E. Giunchiglia, A. Nemchenko, and M. van der Schaar. Rnn-surv: A deep recurrent model for survival analysis. In Artijficial Neural Networks and Machine Learning-ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part IlI 27, pages 23-32. Springer, 2018.   \n[22] J. Goeman. penalized: L1 (lasso) and L2 (ridge) penalized estimation in glms and in the Cox model. R package version 09-21 2008, 2008.   \n[23] J. J. Goeman. L1 penalized estimation in the Cox proportional hazards model. Biometrical journal, 52(1):70-84, 2010.   \n[24] E. Graf, C. Schmoor, W. Sauerbrei, and M. Schumacher. Assessment and comparison of prognostic classification schemes for survival data. Statistics in Medicine, 18(17-18):2529-- 2545, 1999.   \n[25] J. Gui and H. Li. Penalized Cox regression analysis in the high-dimensional and low-sample size setings, with applications to microarray gene expression data. Bioinformatics, 21(13):3001 3008, 2005.   \n[26] F. E. Harrell Jr, K. L. Lee, and D. B. Mark. Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors. Statistics in Medicine, 15(4):361-387, 1996.   \n[27]  T. Hastie and R. Tibshirani. Generalized additive models: some applications. Journal of the American Statistical Association, 82(398):371-386, 1987.   \n[28]  T. Hastie and R. Tibshirani.  Generalized additive models for medical research. Statistical methods in medical research, 4(3):187-196, 1995.   \n[29] H. Hazimeh and R. Mazumder. Fast best subset selection: Coordinate descent and local combinatorial optimization algorithms. Operations Research, 68(5):1517-1537, 2020.   \n[30]  S. Hill. Employee churn prediction, 2017.   \n[31] H. Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI: https://doi.org/10.24432/C5NC77.   \n[32] T. Hothorn, K. Hornik, and A. Zeileis. Unbiased recursive partitioning: A conditional inference framework. Journal of Computational and Graphical Statistics, 15(3):651-674, 2006.   \n[33]  T. Hothorm, K. Hormik, and A. Zeileis. ctre: Conditional inference trees. The Comprehensive R Archive Network, 8, 2015.   \n[34] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear svm. In Proceedings of the 25th international conference on Machine learning, pages 408-415, 2008.   \n[35] T. Huisman, J. G. van der Linden, and E. Demirovic. Optimal survival trees: A dynamic programming approach. arXiv preprint arXiv:2401.04489, 2024.   \n[36] H. Ishwaran and U. B. Kogalur. Random survival forests for r. R News, 7(2):25-31, 2007.   \n[37] H. Ishwaran, U. B. Kogalur, E. H. Blackstone, and M. S. Lauer. Random survival forests. Annals of Applied Statistics, 2(3):841-860, 2008.   \n[38] J. D. Kalbfeisch and R. L. Prentice. The statistical analysis of failure time data. John Wiley & Sons, 2011.   \n[39] J. L. Katzman, U. Shaham, A. Cloninger, J. Bates, T. Jiang, and Y. Kluger. Deepsurv: personalized treatment recommender system using a cox proportional hazards deep neural network. BMC Medical Research Methodology, 18(1):1-12, 2018.   \n[40] D. G. Kleinbaum and M. Klein. Survival analysis a self-learning text. Springer, 1996.   \n[41]  J. Ko. Solving the Cox proportional hazards model and its applications. PhD thesis, Master's thesis. EECS Department, University of California, Berkeley, 2017.   \n[42] B. Kunzer, M. Berges, and A. Dubrawski. The digital twin landscape at the crossroads of predictive maintenance, machine learning and physics based modeling. arXiv preprint arXiv:2206.10462, 2022.   \n[43]  M. LeBlanc and J. Crowley. Survival trees by goodness of split. Journal of the American Statistical Association, 88(422):457-467, 1993.   \n[44] S. Lemeshow, S. May, and D. W. Hosmer Jr. Applied survival analysis: regression modeling of time-to-event data. John Wiley & Sons, 2011.   \n[45]  R. Li and L. Chambless. Test for additive interaction in proportional hazards models. Annals of epidemiology, 17(3):227-236, 2007.   \n[46]  Y. li, V. Rakesh, and C. Reddy. Project success prediction in crowdfunding environments. In Proceedings of the NinthACM International Conference on WebSearch and Data Mining, WSDM '16, pages 247-256, 02 2016.   \n[47] J. Liu, S. Rosen, C. Zhong, and C. Rudin. Okridge: Scalable optimal k-sparse ridge regression. Advances in Neural Information Processing Systems, 36, 2024.   \n[48] J. Liu, C. Zhong, B. Li, M. Seltzer, and C. Rudin. Fasterrisk: Fast and accurate interpretable risk scores. Advances in Neural Information Processing Systems, 35:17760-17773, 2022.   \n[49] J. Liu, C. Zhong, M. Seltzer, and C. Rudin. Fast sparse classification for generalized linear and additive models. Proceedings of machine learning research, 151:9304, 2022.   \n[50]  G. Moat and S. Coleman. Survival analysis and predictive maintenance models for non-sensored assets in facilities management. In 2021 IEEE international conference on big data (Big Data), pages 4026-4034. IEEE, 2021.   \n[51] B. Moufad, P-A. Bannier, Q. Bertrand, Q. Klopfenstein, and M. Massias. skglm: improving scikit-learn for regularized generalized linear models. Journal of Machine Learning Research, 23:1-5, 2023.   \n[52]  Y. Nesterov et al. Lectures on convex optimization, volume 137. Springer, 2018.   \n[53]  Y. Nesterov and B. T. Polyak. Cubic regularization of newton method and its global performance. Mathematical programming, 108(1):177-205, 2006.   \n[54] M. Y. Park and T. Hastie. L1-regularization path algorithm for generalized linear models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 69(4):659-677, 2007.   \n[55] J. Platt. Sequential minimal optimization: A fast algorithm for training support vector machines. Technical report, Microsoft, April 1998.   \n[56]  S. Polsterl. scikit-survival: A library for time-to-event analysis built on top of scikit-learn. Journal of Machine Learning Research, 21(212):1-6, 2020.   \n[57] S. Polsterl, N. Navab, and A. Katouzian. Fast training of support vector machines for survival analysis. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECMLDD2015ort,Portugal, Sptmbe711, 2015,Procedings, Part I1, pa 243-259. Springer, 2015.   \n[58]  T. Popoviciu. Sur les \u00e9quations algebriques ayant toutes leurs racines r\u00e9elles. Mathematica, 9(129-145):20, 1935.   \n[59] B. D. Ripley and R. M. Ripley. Neural networks as statistical methods in survival analysis. Clinical Applications of Artificial Neural Networks, 237:255, 2001.   \n[60] M. SA Carvalho, R. Henderson, S. Shimakura, and I. P. S. C. Sousa. Survival of hemodialysis patients: modeling differences in risk of dialysis centers. International Journal for Quality in Health Care, 15(3):189-196, 2003.   \n[61] R. Sharma, M. Gupta, and G. Kapoor. Some better bounds on the variance with applications. Journal of Mathematical Inequalities, 4(3):355-363, 2010.   \n[62] N. Simon, J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for cox's proportional hazards model via coordinate descent. Journal of Statistical Software, 39(5):1, 20i1.   \n[63]  A. Tarkhan and N. Simon. Bigsurvsgd: Big survival data analysis via stochastic gradient descent. arXiv preprint arXiv:2003.00116, 2020.   \n[64] R. Tibshirani. The lasso method for variable selection in the Cox model. Statistics in medicine, 16(4):385-395, 1997.   \n[65] V. Van Belle, K. Pelckmans, J. A. Suykens, and S. Van Huffel. Support vector machines for survival analysis. In Proceedings of the third international conference on computational intelligence in medicine and healthcare (cimed2007), pages 1-8, 2007.   \n[66] S. Wiseman and A. M. Rush. Sequence-to-sequence learning as beam-search optimization. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296-1306, 2016.   \n[67] S. N. Wood. Generalized additive models: an introduction with R. chapman and hall/CRC, 2017.   \n[68] B. C.-H. ZHANG. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894-942, 2010.   \n[69] H. H. Zhang and W. Lu. Adaptive lasso for Cox's proportional hazards model. Biometrika, 94(3):691-703, 2007.   \n[70] R. Zhang, R. Xin, M. Seltzer, and C. Rudin. Optimal sparse survival trees. arXiv preprint arXiv:2401.15330, 2024.   \n[71] J. Zhu, X. Wang, L. Hu, J. Huang, K. Jiang, Y. Zhang, S. Lin, and J. Zhu. abess: a fast bestsubset selection library in python and r. Journal of Machine Learning Research, 23(202):1-7, 2022.   \n[72] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301-320, 2005. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix to FastSurvival: Hidden Computational Blessings in Training Cox Proportional Hazards Models ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Derivations and Proofs 15 ", "page_idx": 13}, {"type": "text", "text": "A.1 First, Second, and Third Order Partial Derivatives 15   \nA.2PartialDerivative of $r$ -th Central Moment .\uff0e\u00b7 20   \nA.3 First and Second Order Partial Derivatives Are Lipschitz-Continuous 23   \nA.4 Analytical Solution to the Cubic Surrogate Problem . . . 25   \nA.5Analytical Solution to the $\\ell_{1}$ -regularized Quadratic and Cubic Surrogate Problems 26 ", "page_idx": 13}, {"type": "text", "text": "B Related Work 30 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C Experimental Setup Details 32 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Computing Platforms 32   \nC.2 Datasets, Baselines, and Licenses. 32   \nC.3  Details about Variable Selection Experiments . . 34   \nD.1Optimization on $\\ell_{1}$ and $\\ell_{1}+\\ell_{2}$ -regularizedProblems 35   \nD.2 Variable Selection for the CPH Model . . 43 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Derivations and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 First, Second, and Third Order Partial Derivatives ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The derivation for the first order partial derivative is shown in Section A.1.1. The derivation for the second order partial derivative is shown in Section A.1.2. The derivation for the third order partial derivative is shown in Section A.1.3. ", "page_idx": 14}, {"type": "text", "text": "A.1.1 First Order Partial Derivative ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We want to show that ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\ell(\\beta)}{\\partial\\beta_{l}}}=\\sum_{i=1}^{n}\\delta_{i}\\left(\\sum_{k\\in R_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}{e^{\\eta_{j}}}}}X_{k l}\\right)-\\sum_{i=1}^{n}\\delta_{i}X_{i l}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "0\u03b21 $$\n\\begin{array}{r l}{\\mathbf{F}:\\quad}&{\\begin{array}{r l}&{x_{i,j\\in\\mathcal{N}_{I}}(\\mathbf{\\hat{x}}_{i,j})}\\\\ &{:=\\displaystyle\\sum_{k=1}^{n}\\sum_{i=1}^{n}\\sum_{m_{i,j}=1\\atop i\\neq j}(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j})}\\\\ &{:=\\displaystyle\\sum_{k=1}^{n}\\sum_{i=1}^{n}\\sum_{\\sigma_{i}=1}^{\\sigma_{i}}\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)\\cos\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)}\\\\ &{:=\\displaystyle\\sum_{k=1}^{n}\\sum_{\\sigma_{i}=1}^{n}\\sum_{\\sigma_{i}=1}^{\\sigma_{i}}\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)\\cos\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)}\\\\ &{:=\\displaystyle\\sum_{k=1}^{n}\\sum_{\\sigma_{i}=1}^{n}\\sum_{\\sigma_{i}=1}^{\\sigma_{i}}\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)\\cos\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)}\\\\ &{:=\\displaystyle\\sum_{k=1}^{n}\\sum_{\\sigma_{i}=1}^{n}\\sum_{\\sigma_{i}=1}^{\\sigma_{i}}\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)\\cos\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)\\sin\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)}\\\\ &{:=\\displaystyle\\sum_{k=1}^{n}\\sum_{\\sigma_{i}=1}^{n}\\sum_{\\sigma_{i}=1}^{\\sigma_{i}}\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)\\cos\\left(\\sum_{l\\neq k_{i}}^{m}\\mathbf{\\hat{x}}_{i,j}\\right)}\\\\ &{:=\\displaystyle\\sum_{k=1}^{n}\\sum_{\\sigma_{i}=1}^{n}\\sum_{\\sigma \n$$r $[\\cdot]$ $[\\cdot]$ enki Xkil - ", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n=\\sum_{i=1}^{n}\\delta_{i}\\big(\\sum_{k_{1}\\in R_{i}}\\frac{1}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}e^{\\eta_{k_{1}}}X_{k_{1}l}-X_{i l}\\big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "# distribute $\\sum_{i=1}^{n}\\delta_{i}$ inside (-) #move $e^{\\eta_{k_{1}}}$ to the numerator # change notation $k_{1}$ to $k$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ }&{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big(\\sum_{k_{1}\\in R_{i}}\\frac{1}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}e^{\\eta_{k_{1}}}X_{k_{1}l}\\Big)-\\sum_{i=1}^{n}\\delta_{i}X_{i l}}\\\\ {\\ }&{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big(\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\Big)-\\sum_{i=1}^{n}\\delta_{i}X_{i l}}\\\\ {\\ }&{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\Big)-\\sum_{i=1}^{n}\\delta_{i}X_{i l}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.1.2 Second Order Partial Derivative ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We want to show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\ell(\\beta)}{\\partial\\beta_{l}^{2}}=\\sum_{i=1}^{n}\\delta_{i}\\left[\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}-\\left(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "${\\begin{array}{r l r}{\\lefteqn{{\\frac{\\partial^{2}\\ell(\\beta)}{\\partial\\beta_{l}^{2}}}={\\frac{\\partial^{2}\\ell(\\eta)}{\\partial\\beta_{l}^{2}}}}}\\\\ &{}&{=\\sum_{k_{2}=1}^{n}{\\frac{\\partial}{\\partial\\eta_{k_{2}}}}{\\Big(}{\\frac{\\partial\\ell(\\eta)}{\\partial\\beta_{l}}}{\\Big)}{\\frac{\\partial\\eta_{k_{2}}}{\\partial\\beta_{l}}}}\\\\ &{}&{=\\sum_{k_{2}=1}^{n}{\\frac{\\partial}{\\partial\\eta_{k_{2}}}}\\{\\sum_{i=1}^{n}{\\frac{e^{\\eta_{k}}}{k\\in R_{i}}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}}X_{k l}\\}-\\sum_{i=1}^{n}\\delta_{i}X_{i l}{\\Big|}{\\frac{\\partial\\eta_{k_{2}}}{\\partial\\beta_{l}}}}\\\\ &{}&{\\neq p|p|u g_{i}\\,\\,j\\widehat{e}_{j l}\\,{\\widehat{f}}_{f\\overline{{{e m}}}\\,\\,t h}\\,e\\,e n d\\,r e s u l t f o r\\,t h e\\,f r i x t\\,o r d e r\\,p a r t i a l\\,d e r i v a t i v e\\,a b o v e}\\\\ &{}&{=\\sum_{k_{2}=1}^{n}{\\frac{n}{\\mu\\eta_{k_{2}}}}\\ \\sum_{i=1}^{n}{\\frac{e^{\\eta_{k}}}{\\partial\\beta_{l}}}{\\Big(}{\\frac{\\partial\\eta_{k}}{\\partial\\beta_{l}}}{\\Big)}_{k}\\,\\,\\mathrm{.}}\\\\ &{}&{=\\sum_{k_{2}=1}^{n}{\\frac{\\partial}{\\partial\\eta_{k_{2}}}}\\{\\sum_{i=1}^{n}{\\frac{\\partial\\ell(\\eta_{k})}{\\partial\\beta_{l}}}{\\frac{e^{\\eta_{k}}}{\\partial\\beta_{l}}}{\\Big(}{\\frac{\\partial\\eta_{k}}{\\partial\\beta_{l}}}{\\Big)}}\\\\ &{}&\\end{array}}$   \n$=\\sum_{i=1}^{n}\\delta_{i}\\Big[\\sum_{k_{2}=1}^{n}\\frac{\\partial}{\\partial\\eta_{k_{2}}}\\big(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\big)\\frac{\\partial\\eta_{k_{2}}}{\\partial\\beta_{l}}\\Big]-\\sum_{k_{2}=1}^{n}\\frac{\\partial}{\\partial\\eta_{k_{2}}}\\Big(\\sum_{i=1}^{n}\\delta_{i}X_{i l}\\Big)\\frac{\\partial\\eta_{k_{2}}}{\\partial\\beta_{l}}$ $\\sum_{k_{2}=1}^{n}\\frac{\\partial}{\\partial\\eta_{k_{2}}}\\big(\\cdot\\big)\\frac{\\partial\\eta_{k_{2}}}{\\partial\\beta_{l}}$ mb inside each tem inside []; for the frst em, also ex  \nchange the summation orders by summing over $i$ first and $k_{2}$ later   \n$=\\sum_{i=1}^{n}\\delta_{i}\\Big[\\sum_{k_{2}=1}^{n}\\frac{\\partial}{\\partial\\eta_{k_{2}}}\\big(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\big)\\frac{\\partial\\eta_{k_{2}}}{\\partial\\beta_{l}}\\Big]$ # second term is $0$ because the input to $\\frac{\\partial}{\\partial\\eta_{k_{2}}}(\\cdot)$ is a constant   \n$=\\sum_{i=1}^{n}\\delta_{i}\\Big\\{\\sum_{k_{2}=1}^{n}\\big[\\sum_{k\\in R_{i}}\\frac{\\partial}{\\partial\\eta_{k_{2}}}\\Big(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\Big)X_{k l}\\big]\\frac{\\partial\\eta_{k_{2}}}{\\partial\\beta_{l}}\\Big\\}$ #move ankz inside the summation of k   \n$\\begin{array}{l}{{}}\\\\ {{}}\\\\ {{}}\\\\ {{}}\\\\ {{}\\quad=\\sum_{i=1}^{n}{\\delta_{i}\\Big[\\sum_{2}^{n}{\\sum_{\\ell\\in{\\cal R}_{i}}\\frac{\\left(\\sum_{j\\in{\\cal R}_{i}}e^{j\\mathrm{i}\\mathrm{j}_{\\ell}}e^{j\\mathrm{i}_{\\ell}}\\mathrm{l}_{k\\ell\\ell}-{e^{j\\mathrm{i}_{\\ell}}e^{j\\mathrm{i}_{\\ell}}\\mathrm{J_{1}}}_{y_{2}\\geq y_{\\ell}}{X}_{k i}\\right)\\partial_{\\bar{\\cal R}_{i}}}{\\left(\\sum_{j\\in{\\cal R}_{i}}e^{j\\mathrm{i}_{\\ell}}\\mathrm{l}_{y_{2}\\geq y_{\\ell}}{X}_{k i}\\right)}}}\\\\ {{}}\\\\ {{}\\qquad{}\\qquad=\\sum_{i=1}^{n}{\\delta_{i}\\Big[\\sum_{2}^{n}{\\big(\\sum_{j\\in{\\cal R}_{i}}\\frac{\\left(\\sum_{j\\in{\\cal R}_{i}}e^{j\\mathrm{i}_{\\ell}}\\right)e^{j\\mathrm{i}_{\\ell}}\\mathrm{l}_{k\\ell\\ell}-{e^{j\\mathrm{i}_{\\ell}}e^{j\\mathrm{i}_{\\ell}}\\mathrm{l}_{y_{2}\\geq y_{\\ell}}}{\\big(\\sum_{j\\in{\\cal R}_{i}}e^{j\\mathrm{i}_{\\ell}}\\mathrm{l}_{y_{2}\\geq y_{\\ell}}{X}_{k i}\\big)}}\\chi_{k i,j}\\Big]}}\\\\ {{}}\\\\ {{}\\qquad=\\sum_{i=1}^{n}{\\delta_{i}\\Big[\\sum_{2}^{n}{\\big(\\sum_{j\\in{\\cal R}_{i}}\\frac{\\left(\\sum_{j\\in{\\cal R}_{i}}e^{j\\mathrm{i}_{\\ell}}\\right)e^{j\\mathrm{i}_{\\ell}}\\mathrm{l}_{k\\ell\\ell}-{e^{j\\mathrm{i}_{\\ell}}e^{j\\mathrm{i}_{\\ell}}\\mathrm{l}_{y_{2}\\geq y_{\\ell}}}{\\big(\\sum_{j\\in{\\cal R}_{i}}e^{j\\mathrm{i}_{\\ell}}\\big)^{2}}}\\chi_{k i}X_{k i}\\Big)}\\Big]}}\\\\ {{}}\\\\ {{}\\qquad=\\sum_{i=1}^{n}{\\delta_{i}\\Big[\\sum_{2}^{n}{\\sum_{\\ell\\in{\\cal R}_{i $ om calculus $\\frac{\\partial\\eta_{k_{2}}}{\\partial\\beta_{l}}=X_{k_{2}l}$ #move Xket inside keRi ", "page_idx": 15}, {"type": "text", "text": "# divide the fraction into two parts ", "page_idx": 15}, {"type": "text", "text": "$=\\sum_{i=1}^{n}\\delta_{i}\\Big[\\sum_{k_{2}=1}^{n}\\sum_{k\\in R_{i}}\\Big(\\frac{e^{\\eta_{k}}\\mathbb{1}_{k=k_{2}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}X_{k_{2}l}-\\frac{e^{\\eta_{k}}e^{\\eta_{k_{2}}}\\mathbb{1}_{y_{k_{2}}\\geq y_{i}}}{\\big(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\big)^{2}}X_{k l}X_{k_{2}l}\\Big)\\Big]$ # distribute $X_{k l}X_{k_{2}l}$ inside ()   \n$=\\sum_{i=1}^{n}\\delta_{i}\\Big[\\sum_{k\\in R_{i}}\\sum_{k_{2}=1}^{n}\\Big(\\frac{e^{\\eta_{k}}\\mathbb{1}_{k=k_{2}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}X_{k_{2}l}-\\frac{e^{\\eta_{k}}e^{\\eta_{k_{2}}}\\mathbb{1}_{y_{k_{2}}\\geq y_{i}}}{\\big(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\big)^{2}}X_{k l}X_{k_{2}l}\\Big)\\Big]$ # exchange summation orders; sum over $k_{2}$ and then $k$   \n$=\\sum_{i=1}^{n}\\delta_{i}\\Big[\\sum_{k\\in\\cal R_{i}}\\Big(\\sum_{k_{2}=1}^{n}\\frac{e^{\\eta_{k}}\\mathbb{1}_{k=k_{2}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}X_{k_{2}l}-\\sum_{k_{2}=1}^{n}\\frac{e^{\\eta_{k}}e^{\\eta_{k_{2}}}\\mathbb{1}_{y_{k_{2}}\\geq y_{i}}}{\\left(\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}\\right)^{2}}X_{k l}X_{k_{2}l}\\Big)\\Big]$ #distribute $\\sum_{k_{2}=1}^{n}$ inside $(\\cdot)$   \n$=\\sum_{i=1}^{n}\\delta_{i}\\Big[\\sum_{k\\in R_{i}}\\Big(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}-\\sum_{k_{2}\\in R_{i}}\\frac{e^{\\eta_{k}}e^{\\eta_{k_{2}}}}{\\left(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\right)^{2}}X_{k l}X_{k_{2}l}\\Big)\\Big]$   \nfor the first term, only $k_{2}=k$ is left because of the indicator $\\mathbb{1}_{k=k_{2}}$ ; for the second   \n#term, the expression can be simplified because $R_{i}$ is a set of indices whose time is   \ngreater than or equal to $y_{i}$ enk   \n$\\begin{array}{r l}&{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big(\\sum_{k\\in\\mathbb{R}_{i}}\\frac{\\mathsf{c}_{i}}{\\sum_{j\\in\\mathbb{R}_{i}}\\epsilon^{\\eta_{j}}}X_{k l}^{2}-\\displaystyle\\sum_{k\\in\\mathbb{R}_{i}}\\sum_{k\\geq\\in\\mathbb{R}_{i}}\\frac{\\mathsf{c}_{i}}{\\big(\\sum_{j\\in\\mathbb{R}_{i}}\\epsilon^{\\eta_{j}}\\big)^{2}}X_{k l}X_{k2l}\\Big)}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big[\\sum_{k\\in\\mathbb{R}_{i}}\\frac{\\mathsf{c}^{\\eta_{k}}}{\\sum_{j\\in\\mathbb{R}_{i}}\\epsilon^{\\eta_{j}}}X_{k l}^{2}-\\Big(\\displaystyle\\sum_{k\\in\\mathbb{R}_{i}}\\frac{\\mathsf{c}^{\\eta_{k}}}{\\sum_{j\\in\\mathbb{R}_{i}}\\epsilon^{\\eta_{j}}}X_{k l}\\Big)\\Big(\\displaystyle\\sum_{k\\geq\\in\\mathbb{R}_{i}}\\frac{\\mathsf{c}^{\\eta_{k}}}{\\sum_{j\\in\\mathbb{R}_{i}}\\epsilon^{\\eta_{j}}}X_{k_{2}l}\\Big)\\Big]}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big[\\sum_{k\\in\\mathbb{R}_{i}}\\frac{\\mathsf{c}^{\\eta_{k}}}{\\sum_{j\\in\\mathbb{R}_{i}}\\epsilon^{\\eta_{j}}}X_{k l}^{2}-\\Big(\\displaystyle\\sum_{k\\in\\mathbb{R}_{i}}\\frac{\\mathsf{c}^{\\eta_{k}}}{\\sum_{j\\in\\mathbb{R}_{i}}\\epsilon^{\\eta_{j}}}X_{k l}\\Big)^{2}\\Big]}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big[\\sum_{k\\in\\mathbb{R}_{i}}\\frac{\\mathsf{c}^{\\eta_{k}}}{\\sum_{j\\in\\mathbb{R}_{i}}\\epsilon^{\\eta_{j}}}X_{k l}^{2}-\\Big(\\displaystyle\\sum_{k\\in\\mathbb{R}_{i}}\\frac{\\mathsf{c}^{\\eta_{k}}}{\\sum_{j\\in\\mathbb{R}_{i}}\\epsilon^{\\eta_{j}}}X_{k l}\\Big)^{2}\\Big]}\\end{array}$ ms   \ntthe two terms of the product are exactly the same and can be simplified because $k$ and   \n#   \n$k_{2}$ areindependent ", "page_idx": 16}, {"type": "text", "text": "A.1.3 Third Order Partial Derivative ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We want to show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{3}\\ell(\\beta)}{\\partial\\beta_{l}^{3}}=\\sum_{i=1}^{n}\\delta_{i}\\left[\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{3}+2\\left(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\right)^{3}\\right.}\\\\ {\\left.-3\\left(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}\\right)\\left(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\right)\\right].}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r l}&{\\displaystyle\\frac{\\partial^{3}\\ell(\\beta)}{\\partial\\beta_{l}^{3}}=\\frac{\\partial^{3}\\ell(\\eta)}{\\partial\\beta_{l}^{3}}}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{k_{3}=1}^{n}\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\bigg(\\frac{\\partial^{2}\\ell(\\eta)}{\\partial\\beta_{l}^{2}}\\bigg)\\frac{\\partial\\eta_{k_{3}}}{\\partial\\beta_{l}}}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{k_{3}=1}^{n}\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\bigg\\{\\sum_{i=1}^{n}\\delta_{i}\\bigg[\\sum_{k_{4}}\\frac{e^{\\eta_{k}}}{k_{4}\\,R_{i}}\\frac{e^{\\eta_{k}}}{Z_{j\\in{\\cal R}_{i}}\\,e^{\\eta_{j}}}X_{k l}^{2}-\\Big(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{Z_{j\\in{\\cal R}_{i}}\\,e^{\\eta_{j}}}X_{k l}^{k}\\Big)^{2}\\Big]\\bigg\\}\\frac{\\partial\\eta_{k_{3}}}{\\partial\\beta_{l}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\displaystyle\\eta\\,p\\,i u g\\,i n\\,\\frac{\\partial\\ell(\\eta)}{\\partial\\beta_{l}}\\,f r o m\\,t h e\\,e n d\\,r e s u l t\\,f o r\\,t h e\\,s e c o n d\\,o r d e r\\,p a r i a l\\,d e r i v_{l}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\bigg\\{\\sum_{k_{3}=1}^{n}\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\bigg[\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{Z_{j\\in{\\cal R}_{i}}\\,e^{\\eta_{j}}}X_{k l}^{2}-\\Big(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{Z_{j\\in{\\cal R}_{i}}\\,e^{\\eta_{j}}}X_{k l}\\Big)^{2}\\bigg]\\frac{\\partial\\eta_{k_{3}}}{\\partial\\beta_{l}}\\bigg\\}}\\end{array}$ mcalculus itiveabove # exchange summation orders; sum over $k_{3}$ and then i by moving $\\frac{\\partial}{\\partial\\eta_{k_{3}}}(\\cdot)\\frac{\\partial\\eta_{k_{3}}}{\\partial\\beta_{l}}$ anka inside into theinner summation ", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r l}&{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big\\{\\sum_{k_{3}=1}^{n}\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\Big[\\underset{k\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}-\\Big(\\underset{k\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\Big)^{2}\\Big]X_{k_{3}l}\\Big\\}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big\\{\\sum_{k_{3}=1}^{n}\\Big\\{\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\Big(\\underset{k\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}\\Big)-\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\Big[\\Big(\\underset{k\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\Big)^{2}\\Big]\\Big\\}X_{k_{3}l}\\Big\\}}\\end{array}$ # distribute $\\frac{\\partial}{\\partial\\eta_{k_{3}}}$ inside []   \n$=\\sum_{i=1}^{n}\\delta_{i}\\bigg\\{\\sum_{k_{3}=1}^{n}\\Big\\{\\sum_{k\\in{\\cal R}_{i}}\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\Big(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\Big)X_{k l}^{2}-\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\Big[\\Big(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}\\Big)^{2}\\Big]\\Big\\}X_{k_{3}l}\\bigg\\}$ #for the frst term, move $\\frac{\\partial}{\\partial\\eta_{k_{3}}}(\\cdot)$ inside the sumation   \n$\\begin{array}{r l}{\\lefteqn{=\\sum_{i=1}^{n}\\delta_{i}\\Big\\{\\sum_{k_{3}=1}^{n}\\Big[\\sum_{k\\in R_{i}}\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\Big(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\Big)X_{k l}^{2}}}\\\\ &{\\qquad\\qquad-\\,2\\Big(\\underset{k\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\Big)\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\Big(\\underset{k\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\Big)\\Big]X_{k_{3}l}\\Big\\}}\\end{array}$ ", "page_idx": 17}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "# apply product rule from calculus to the second term ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\sum_{i=1}^{n}\\delta_{i}\\Bigg\\{\\sum_{k_{3}=1}^{n}\\Big\\{\\sum_{k\\in R_{i}}\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\Big(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\Big)X_{k l}^{2}}}\\\\ &{\\qquad\\qquad-\\,2\\Big(\\underset{k\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\Big)\\Big[\\sum_{k\\in R_{i}}\\frac{\\partial}{\\partial\\eta_{k_{3}}}\\Big(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\Big)X_{k l}\\Big]\\Bigg\\}X_{k_{3}l}\\Bigg\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "#for the second term, move $\\frac{\\partial}{\\partial\\eta_{k_{3}}}(\\cdot)$ inside the summation ", "page_idx": 17}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/3cd4b7aaa0e98349a9205b053816fec1b198adb0bbafbf3b9d0e32888c35fe49.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "# distribute $X_{k_{3}l}$ inside $\\{\\cdot\\}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n=\\sum_{i=1}^{n}\\delta_{i}\\Biggl\\{\\sum_{k_{3}=1}^{n}\\Bigl\\{\\sum_{k\\in{\\cal R}_{i}}\\Bigl(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\mathbb{1}_{k=k_{3}}-\\frac{e^{\\eta_{k}}e^{\\eta_{k_{3}}}}{\\bigl(\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}\\bigr)^{2}}\\mathbb{1}_{y_{k_{3}}\\geq y_{i}}\\Bigr)X_{k l}^{2}X_{k_{3}l}\\Biggr\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\left.2\\bigg(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\bigg)\\bigg[\\sum_{k\\in R_{i}}\\bigg(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\mathbb{1}_{k=k_{3}}-\\frac{e^{\\eta_{k}}e^{\\eta_{k_{3}}}}{\\big(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\big)^{2}}\\mathbb{1}_{y_{k_{3}}\\geq y_{i}}\\bigg)X_{k l}\\bigg]X_{k_{3}l}\\bigg\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n=\\sum_{i=1}^{n}\\delta_{i}\\Big\\{\\sum_{k_{3}=1}^{n}\\Big[\\sum_{k\\in R_{i}}\\Big(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\mathbb{1}_{k=k_{3}}-\\frac{e^{\\eta_{k}}e^{\\eta_{k_{3}}}}{\\big(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\big)^{2}}\\mathbb{1}_{y_{k_{3}}\\geq y_{i}}\\Big)X_{k l}^{2}X_{k_{3}l}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n-2\\bigg(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}\\bigg)\\bigg(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\mathbb{1}_{k=k_{3}}X_{k l}-\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}e^{\\eta_{k}}3}{\\left(\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}\\right)^{2}}\\mathbb{1}_{y_{k_{3}}\\geq y_{i}}X_{k l}\\bigg)X_{k_{3}l}\\bigg)\\bigg\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n=\\sum_{i=1}^{n}\\biggl\\{\\delta_{i}\\sum_{k_{3}=1}^{n}\\biggl[\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\mathbb{1}_{k=k_{3}}X_{k l}^{2}X_{k_{3}l}-\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}e^{\\eta_{k_{3}}}}{\\bigl(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\bigr)^{2}}\\mathbb{1}_{y_{k_{3}}\\geq y_{i}}X_{k l}^{2}X_{k_{3}l}-\\sum_{j\\in R_{i}}\\frac{e^{\\eta_{k}}e^{\\eta_{j}}}{\\bigl(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\bigr)^{2}}\\mathbb{1}_{y_{k_{3}}\\geq y_{i}}X_{k l}^{2}X_{k_{3}l}\\biggr]\\biggr\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\,2\\biggl(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\biggr)\\biggl(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\mathbb{1}_{k=k_{3}}X_{k l}-\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}e^{\\eta_{k}}}{\\left(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\right)^{2}}\\mathbb{1}_{y_{k_{3}}\\geq y_{i}}X_{k l}\\biggr)X_{k_{3}l}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "#for the frst term, distribute $\\sum_{k\\in R_{i}}(\\cdot)X_{k l}^{2}X_{k_{3}l}$ inside () ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\sum_{i=1}^{n}\\delta_{i}\\bigg\\{\\sum_{k_{3}=1}^{n}\\Big[\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\mathbb{1}_{k=k_{3}}X_{k l}^{2}X_{k_{3}l}-\\displaystyle\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}e^{\\eta_{k_{3}}}}{\\big(\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}\\big)^{2}}\\mathbb{1}_{y_{k_{3}}\\geq y_{i}}X_{k l}^{2}X_{k_{3}l}\\bigg]}}\\\\ {{\\displaystyle~~-2\\Big(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}\\Big)\\Big(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}\\mathbb{1}_{k=k_{3}}X_{k_{3}l}\\Big)}}\\\\ {{\\displaystyle~~+~2\\Big(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}\\Big)^{2}\\Big(\\frac{e^{\\eta_{k_{3}}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\Big)\\mathbb{1}_{y_{k_{3}}\\geq y_{i}}X_{k_{3}l}\\Big]\\bigg\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "#for the third term, distribute 2(keR (en: $\\begin{array}{r}{2(\\sum_{k\\in R_{i}}(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}))(\\cdot)X_{k_{3}l}}\\end{array}$ R: e\"s )(-)Xkgt inside () ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{}}&{{=\\displaystyle\\sum_{i=1}^{n}{\\delta_{i}\\Big\\{\\Big(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\Big)X_{k l}^{2}\\Big(\\sum_{k_{3}=1}^{n}{\\mathbb{1}}_{k=k_{3}}X_{k_{3}l}\\Big)}}}\\\\ {{}}&{{\\displaystyle~~-\\left(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}^{2}\\right)\\Big(\\sum_{k_{3}=1}^{n}{\\mathbb{1}}_{y_{k_{3}}\\geq y_{i}}X_{k_{3}l}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\Big)}}\\\\ {{}}&{{\\displaystyle~~~-2\\Big(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}\\Big)\\Big[\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}\\Big(\\sum_{k_{3}=1}^{n}{\\mathbb{1}}_{k=k_{3}}X_{k_{3}l}\\Big)\\Big]}}\\\\ {{}}&{{\\displaystyle~~~+2\\Big(\\sum_{k\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k l}\\Big)^{2}\\Big[\\sum_{k}\\Big(\\frac{e^{\\eta_{k}}}{\\sum_{i\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\Big){\\mathbb{1}}_{y_{k_{3}\\geq y_{i}}}X_{k_{3}l}\\Big]\\Big\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "# exchange summation orders; distribute $\\sum_{k_{3}=1}^{n}$ into each of the four terms inside [-] ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\left[\\left(\\sum_{k\\in R_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}}\\right)X_{k l}^{3}\\right.}\\\\ {\\displaystyle\\left.\\quad-\\left(\\sum_{k\\in R_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}}X_{k l}^{2}\\right)\\left(\\sum_{k_{3}\\in R_{i}}X_{k_{3}l}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}}\\right)\\right.}\\\\ {\\displaystyle\\left.\\quad-2\\bigcap_{k\\in R_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}\\sum_{\\ell^{\\eta_{j}}}X_{k l}}}\\right)\\left(\\sum_{k\\in R_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}}X_{k l}^{2}\\right)}\\\\ {\\displaystyle\\left.\\quad+\\left.2\\bigcap_{k\\in R_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}}X_{k l}\\right)^{2}\\left(\\sum_{k_{3}\\in R_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}}X_{k_{3}l}\\right)\\right]}\\end{array}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "# simplify the summation over $k_{3}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\bigg[\\bigg(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\bigg)X_{k l}^{3}}\\\\ &{\\quad-\\left(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}\\right)\\bigg(\\sum_{k\\in R_{i}}X_{k l}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\bigg)}\\\\ &{\\quad-\\displaystyle2\\bigg(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\bigg)\\bigg(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}\\bigg)}\\\\ &{\\quad+\\displaystyle2\\bigg(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\bigg)^{2}\\bigg(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\bigg)\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "#changenotationbyreplacing $k_{3}$ With $k$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\sum_{i=1}^{n}\\delta_{i}\\bigg[\\bigg(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\bigg)X_{k l}^{3}}\\\\ {\\displaystyle~~-\\,3\\bigg(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}\\bigg)\\bigg(\\sum_{k\\in R_{i}}X_{k l}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\bigg)}\\\\ {\\displaystyle~~~+\\,2\\bigg(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\bigg)^{3}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "# simplify all relevant terms ", "page_idx": 18}, {"type": "text", "text": "A.2 Partial Derivative of $r$ -th Central Moment ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Recall that the $r$ -th central moment $C_{r}$ is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nC_{r}:=\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We need to show that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\beta_{l}}\\Big(C_{r}\\Big)=C_{r+1}-r\\cdot C_{2}\\cdot C_{r-1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\frac{\\partial}{\\partial\\beta_{i}}}\\left(C_{r}\\right)}\\\\ &{={\\frac{\\partial}{\\partial\\beta_{i}}}\\left[\\sum_{k\\in{\\bar{\\cal R}}_{i}}{\\frac{e^{\\mu_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}}\\left(X_{k i}-\\sum_{k\\in{\\bar{\\cal R}}_{i}}{\\frac{e^{\\mu_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{i}}}}X_{k i}\\right)\\right]}\\\\ &{=\\displaystyle\\sum_{k\\in{\\cal R}_{i}}{\\frac{\\partial}{\\partial\\beta_{i}}}\\left[\\sum_{j\\in{\\cal R}_{i}}e^{\\frac{e^{\\mu_{k}}}{\\sum_{k\\in{\\bar{\\cal R}}_{i}}\\frac{e^{\\mathrm{{e}}({\\bf R}_{i})}}{\\sum_{k\\in{\\bar{\\cal R}}_{i}}\\frac{e^{\\mathrm{{e}}({\\bf R}_{i})}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}}}X_{k i}\\right]\\right]}\\\\ &{=\\displaystyle\\sum_{k\\in{\\cal R}_{i}}\\left\\{{\\frac{\\partial}{\\partial\\beta_{i}}}\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{i}}}\\right)\\left(X_{k i}-\\sum_{k\\in{\\bar{\\cal R}}_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{i}}}}X_{k i}\\right)^{r}\\right.}\\\\ &{\\qquad\\left.\\qquad+\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\right){\\frac{\\partial}{\\partial\\beta_{i}}}\\left[\\left(X_{k i}-\\sum_{k\\in{\\bar{\\cal R}}_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{i}}}}X_{k i}\\right)^{r}\\right]\\right\\}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "# apply the product rule from calculus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k\\in R_{i}}\\left[\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r}\\right.}}\\\\ &{}&{\\left.+\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)r\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r-1}\\frac{\\partial}{\\partial\\beta_{l}}\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n=\\sum_{k\\in R_{i}}\\left\\{\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\right)r\\left(X_{k l}-\\sum_{k_{1}\\in{\\cal R}_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r-1}\\left[\\sum_{k_{1}\\in{\\cal R}_{i}}\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}\\right)X_{k_{1}l}\\right]\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "# move the partial diferentil operator $\\frac{\\partial}{\\partial\\beta_{l}}$ inside $s u m_{k_{1}\\in R_{i}}$ move the negative sign - in the second term to the front ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\sum_{R_{i}}}{\\sum}\\left[\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)\\left(X_{k l}-\\underset{k_{1}\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}}\\right)^{r}\\right]}\\\\ &{\\underset{k\\in R_{i}}{\\sum}\\left\\{\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)r\\left(X_{k l}-\\underset{k_{1}\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r-1}\\left[\\underset{k_{1}\\in R_{i}}{\\sum}\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)X_{k_{1}l}\\right]\\right\\}}\\\\ &{\\underset{\\sum_{R_{i}}}{\\sum}\\left[\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)\\left(X_{k l}-\\underset{k_{1}\\in R_{i}}{\\sum}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$-\\left.r\\left[\\sum_{k_{1}\\in R_{i}}\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)X_{k_{1}}\\right]\\left[\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r-1}\\right]$ #move the two valuesin the second temoutsideof $\\sum_{k\\in R_{i}}$ because they are independent of $k$ $\\begin{array}{r l}{\\lefteqn{=\\sum_{k\\in R_{i}}\\left[\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r}\\right]}\\quad}&{}\\\\ &{-\\,r\\left[\\sum_{k_{1}\\in R_{i}}\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)X_{k_{1}l}\\right]C_{r-1}}\\end{array}$ simplify by replacing the last value in the second term with $C_{r-1}$ becausethecentral #moment definition ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Letusfouson the solutionfor the uboblm $\\begin{array}{r}{\\frac{\\partial}{\\partial\\beta_{l}}\\left[e^{\\eta_{k}}/\\left(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\right)\\right]}\\end{array}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial\\beta_{1}}\\Bigg(\\sum_{j=0}^{\\infty}\\mathrm{e}^{j\\omega_{1}}}\\\\ &{=\\displaystyle\\sum_{k=1}^{\\infty}\\frac{\\partial}{\\partial\\alpha_{1}}\\Biggl(\\sum_{j=0}^{\\infty}\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}\\Biggr)\\frac{\\partial\\beta_{2}}{\\partial k_{1}}}\\\\ &{=\\displaystyle\\sum_{k=1}^{\\infty}\\left[\\sum_{j=0}^{\\infty}\\mathrm{e}^{\\mathrm{i}\\omega_{1}t}\\mathrm{e}^{\\mathrm{i}\\omega_{1}t}\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}\\right]\\frac{\\partial\\beta_{3}}{\\partial k_{1}}\\Biggl(\\sum_{j=0}^{\\infty}\\mathrm{e}^{-\\mathrm{i}\\omega_{2}t}\\Biggr)\\Biggl[\\frac{\\partial\\beta_{3}}{\\partial k_{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\sigma\\frac{\\partial\\beta_{4}}{\\partial k_{1}}\\Biggr)\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}}\\\\ &{=\\displaystyle\\sum_{k=1}^{\\infty}\\left[\\sum_{j=0}^{\\infty}\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}\\right]\\frac{\\partial\\beta_{4}}{\\partial k_{1}}\\Biggr[\\sum_{j=0}^{\\infty}\\mathrm{e}^{\\mathrm{i}\\omega_{1}t}\\mathrm{e}^{\\mathrm{i}\\omega_{1}t}\\mathrm{e}^{-\\mathrm{i}\\omega_{2}t}\\mathrm{e}^{-\\mathrm{i}\\omega_{2}t}\\mathrm{e}^{\\mathrm{i}\\omega_{2}t}\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}\\mathrm{e}^{\\mathrm{i}\\omega_{2}t}\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}}\\\\ &{=\\displaystyle\\sum_{k=1}^{\\infty}\\sum_{j=0}^{\\infty}\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}\\sum_{k=1}^{\\infty}\\mathrm{e}^{-\\mathrm{i}\\omega_{1}t}\\mathrm{e}^{-\\mathrm{i}\\omega_{2}t}\\mathrm{e}^{\\mathrm{i}\\omega_{1}t}\\mathrm{e}^{\\mathrm{i}\\omega_{2}t}\\mathrm{e}^{-\\mathrm{i}\\omega_{2}\n$$calculus calculus rivative nside [] mations mations ", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us now plug this result back into the original problem: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{\\partial}{\\partial\\beta_{l}}\\left(C_{r}\\right)}\\\\ &{=\\displaystyle\\sum_{k\\in R_{i}}\\left[\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r}\\right]}\\\\ &{\\quad-\\displaystyle r\\left[\\sum_{k_{1}\\in R_{i}}\\frac{\\partial}{\\partial\\beta_{l}}\\left(\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\right)X_{k_{1}l}\\right]C_{r-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "# pick up from where we left for the original problem ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\sum_{k\\in R_{i}}\\left[\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k l}-\\sum_{k_{2}\\in R_{i}}\\frac{e^{\\eta_{k_{2}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{2}l}\\right)\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r}\\right]}\\quad}&{}\\\\ &{-\\,r\\left[\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k_{1}l}-\\sum_{k_{2}\\in R_{i}}\\frac{e^{\\eta_{k_{2}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{2}l}\\right)X_{k_{1}l}\\right]C_{r-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "# plug in the solution to the subproblem ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\sum_{k\\in R_{i}}\\left[\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r+1}\\right]}\\quad}&{}\\\\ &{-\\left.{r\\left[\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k_{1}l}-\\sum_{k_{2}\\in R_{i}}\\frac{e^{\\eta_{k_{2}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{2}l}\\right)X_{k_{1}l}\\right]C_{r-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "#fot independent from each other ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\sum_{k\\in R_{i}}\\left[\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r+1}\\right]}\\quad}&{}\\\\ &{-\\ r\\left[\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k_{1}l}^{2}-\\sum_{k_{2}\\in R_{i}}\\frac{e^{\\eta_{k_{2}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}X_{k_{2}l}\\right)\\right]C_{r-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "#for the second term, move $X_{k_{1}l}$ inside (-) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\sum_{k\\in R_{i}}\\left[\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r+1}\\right]}\\quad}&{}\\\\ &{-\\ r\\left(\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}^{2}-\\sum_{k_{1}\\in R_{i}}\\sum_{k_{2}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}e^{\\eta_{k_{2}}}}{\\left(\\sum_{j\\in R_{i}}e^{\\eta_{j}}\\right)^{2}}X_{k_{1}l}X_{k_{2}l}\\right)C_{r-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "#for the second term, distribute kER into the two terms inside () ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\sum_{k\\in R_{i}}\\left[\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}\\left(X_{k l}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r+1}\\right]}\\\\ &{-\\ r\\left[\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}^{2}-\\left(\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{2}\\right]C_{r-1}}\\\\ &{+\\underbrace{\\left[\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}\\ e^{\\eta_{j}}}X_{k_{1}l}^{2}-\\sum_{k_{1}\\in R_{i}}\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\right)^{r}\\right]C_{r-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "in thesecond term, $k_{1}$ and $k_{2}$ are independent dummy variables, so we can turn a # double sum of products into a products of sums; we can further simplify this into a square of a sum because the two terms equal to the same value ", "page_idx": 21}, {"type": "equation", "text": "$$\n=C_{r+1}-r\\cdot C_{2}\\cdot C_{r-1}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "# simplify by using the definition of central moment ", "page_idx": 21}, {"type": "text", "text": "A.3First and Second Order Partial Derivatives Are Lipschitz-Continuous ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. To show that the first and second order partial derivatives are Lipschitz-continuous, we need to show that the second and third order partial derivatives are bounded, respectively. ", "page_idx": 22}, {"type": "text", "text": "First order partial derivative is Lipschitz-continuous  To show that the first order partial derivative with respect to each coordinate is Lipschitz, we need to show that the second order partial derivative with respect to each coordinate is bounded. Recall that the second order partial derivative with respect to the $l$ -th coordinate can be expressed as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\frac{\\partial^{2}\\ell(\\beta)}{\\partial\\beta_{l}^{2}}}=\\sum_{i=1}^{n}\\delta_{i}\\bigg[\\sum_{k\\in{\\cal R}_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}}X_{k l}^{2}-\\big(\\sum_{k\\in{\\cal R}_{i}}{\\frac{e^{\\eta_{k}}}{\\sum_{j\\in{\\cal R}_{i}}e^{\\eta_{j}}}}X_{k l}\\big)^{2}\\bigg]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It suffices to show that each term inside the bracket is bounded. ", "page_idx": 22}, {"type": "text", "text": "$\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}$ $X_{k l}^{2}$ $X_{k l}$ can be thought of as the probability of a particular distribution because all terms are greater than or equal to O and sum up to $1$ ", "page_idx": 22}, {"type": "text", "text": "For notational convenience, let us use $\\textbf{\\em a}$ to denote the probability of this specific distribution with ax =seR \"g Then we ca rewrite each erm isid ] as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}-\\biggl(\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\biggr)^{2}=\\sum_{k\\in R_{i}}a_{k}X_{k l}^{2}-\\bigl(\\sum_{k\\in R_{i}}a_{k}X_{k l}\\bigr)^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The right-hand side is nothing but the variance of $\\{X_{k l}\\}_{k\\in R_{i}}$ with respect to the distribution $\\{a_{k}\\}_{k\\in R_{i}}$ Since the variance is always non-negative, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k\\in R_{i}}a_{k}X_{k l}^{2}-(\\sum_{k\\in R_{i}}a_{k}X_{k l})^{2}\\geq0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let us now denote $a:=\\operatorname*{min}_{k\\in R_{i}}X_{k l}$ as the minimum of this given set, $b:=\\operatorname*{max}_{k\\in R_{i}}X_{k l}$ as the maximum of this given set, $Z$ as a random variable with values restricted to $[a,b]$ . We are going to show that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k\\in\\cal R_{i}}a_{k}X_{k l}^{2}-(\\sum_{k\\in\\cal R_{i}}a_{k}X_{k l})^{2}\\leq\\operatorname*{max}_{Z}\\bigl[\\mathbb{E}[Z^{2}]-(\\mathbb{E}[Z])^{2}\\bigr].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We achieve this through two steps. ", "page_idx": 22}, {"type": "text", "text": "First, suppose the random variable can only take fnite $|R_{i}|$ number of values $\\{Z_{1},Z_{2},...,Z_{|R_{i}|}\\}$ with probability $\\left\\{p_{1},p_{2},...,p_{|R_{i}|}\\right\\}$ , where $|R_{i}|$ is the cardinality of the set $R_{i}$ . Then, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k\\in{\\cal R}_{i}}a_{k}X_{k l}^{2}-\\big(\\sum_{k\\in{\\cal R}_{i}}a_{k}X_{k l}\\big)^{2}\\leq\\operatorname*{max}_{Z,p}\\big[{\\mit\\mathbb{E}}_{p}[Z^{2}]-({\\mit\\mathbb{E}}_{p}[Z])^{2}\\big],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the expectation is taken with respect to the distribution $\\pmb{p}$ . The above inequality holds because the left-hand side is a specific instance of the expression inside $[\\cdot]$ , so taking $\\operatorname*{max}(\\cdot)$ produces the inequality above. ", "page_idx": 22}, {"type": "text", "text": "Next, if we drop the assumption that the random variable $Z$ canonlytake $|R_{i}|$ number of values, then the maximum variance we can achieve is no smaller than before. Mathematically, this means that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{Z,p}\\bigl[\\mathbb{E}_{p}[Z^{2}]-(\\mathbb{E}_{p}[Z])^{2}\\bigr]\\leq\\operatorname*{max}_{Z}\\bigl[\\mathbb{E}[Z^{2}]-(\\mathbb{E}[Z])^{2}\\bigr].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining Inequality (24) and Inequality (25), we arrive at Inequality (23). Lastly, note that the Popoviciu Inequality [58] tells us that for a bounded random variable restricted to $[a,b]$ , the maximum variance it can achieve is (@-b)\u00b2 , 4\uff0ci.e.\uff0c ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{Z}\\bigl[\\mathbb{E}[Z^{2}]-(\\mathbb{E}[Z])^{2}\\bigr]\\leq\\frac{(b-a)^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This allows us to conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k\\in R_{i}}a_{k}X_{k l}^{2}-\\left(\\sum_{k\\in R_{i}}a_{k}X_{k l}\\right)^{2}\\leq\\frac{(b-a)^{2}}{4}=\\frac{(\\operatorname*{max}_{k\\in R_{i}}X_{k l}-\\operatorname*{min}_{k\\in R_{i}}X_{k l})^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging in this inequality into the expression of the second order partial derivative, we can show that the second order partial derivative is bounded: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\le\\displaystyle\\frac{\\partial^{2}\\ell(\\beta)}{\\partial\\beta_{l}^{2}}=\\sum_{i=1}^{n}\\delta_{i}\\Big[\\displaystyle\\sum_{k_{1}\\in R_{i}}\\displaystyle\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}^{2}-\\big(\\displaystyle\\sum_{k_{1}\\in R_{i}}\\displaystyle\\frac{e^{\\eta_{k_{1}}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k_{1}l}\\big)^{2}\\Big]}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\frac{1}{4}\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\big(\\displaystyle\\operatorname*{max}_{k\\in R_{i}}X_{k l}-\\displaystyle\\operatorname*{min}_{k\\in R_{i}}X_{k l}\\big)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Second order partial derivative is Lipschitz-continuous  This proof is similar to the first part above. We need to show that the third order partial derivative with respect to each coordinate is bounded. Recall that the third order partial derivative can be expressed as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial^{3}\\ell(\\beta)}{\\partial\\beta_{l}^{3}}=\\displaystyle\\sum_{i=1}^{n}\\delta_{i}\\Big[\\displaystyle\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{3}+2\\big(\\displaystyle\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\big)^{3}}\\\\ {-\\,3\\Big(\\displaystyle\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}\\Big)\\Big(\\displaystyle\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\Big)\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As we have done in the first part, if we use $\\textbf{\\em a}$ to denote the probability of this specific distribution Wih $\\begin{array}{r}{a_{k}=\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}}\\end{array}$ we ca rewritecachtem inside $[\\cdot]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{3}+2\\big(\\displaystyle\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\big)^{3}-3\\Big(\\displaystyle\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}^{2}\\Big)\\Big(\\displaystyle\\sum_{k\\in R_{i}}\\frac{e^{\\eta_{k}}}{\\sum_{j\\in R_{i}}e^{\\eta_{j}}}X_{k l}\\Big)}\\\\ &{=\\displaystyle\\sum_{k\\in R_{i}}a_{k}X_{k l}^{3}+2\\big(\\displaystyle\\sum_{k\\in R_{i}}a_{k}X_{k l}\\big)^{3}-3\\big(\\displaystyle\\sum_{k\\in R_{i}}a_{k}X_{k l}^{2}\\big)\\big(\\displaystyle\\sum_{k\\in R_{i}}a_{k}X_{k l}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, as in the first part, let us now denote $a:=\\operatorname*{min}_{k\\in R_{i}}X_{k l}$ as the minimum of this given set, $b:=\\operatorname*{max}_{k\\in R_{i}}X_{k l}$ as the maximum of this given set, $Z$ as a random variable with values restricted to $[a,b]$ . Using the exact same logic, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{|\\sum_{k\\in R_{i}}a_{k}X_{k l}^{3}+2(\\sum_{k\\in R_{i}}a_{k}X_{k l})^{3}-3(\\sum_{k\\in R_{i}}a_{k}X_{k l}^{2})(\\sum_{k\\in R_{i}}a_{k}X_{k l})|}}\\\\ &{\\leq\\operatorname*{max}_{Z}|\\mathbb{E}[Z^{3}]+2\\mathbb{E}[Z]^{3}-3\\mathbb{E}[Z^{2}]\\mathbb{E}[Z]|||}\\\\ &{=\\operatorname*{max}_{Z}|\\mathbb{E}[(Z-\\mathbb{E}[Z])^{3}]|}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The expression inside $\\left|\\cdot\\right|$ on the right-hand side is known as the third central moment (skewedness) in statistics. Fortunately, we can derive an explicit formula for the maximum of the absolute third central moment of a bounded variable. ", "page_idx": 23}, {"type": "text", "text": "According to [61], we have the following inequality involving the second and third central moment: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[(Z-\\mathbb{E}[Z])^{2}]+\\Big(\\frac{\\mathbb{E}[(Z-\\mathbb{E}[Z])^{3}]}{2\\mathbb{E}[(Z-\\mathbb{E}[Z])^{2}]}\\Big)^{2}\\leq\\frac{1}{4}(b-a)^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From this, we can derive an upper bound on the third central moment: ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\mathbb{E}[(Z-\\mathbb{E}[Z])^{3}]|\\le2\\mathbb{E}[(Z-\\mathbb{E}[Z])^{2}]\\sqrt{\\frac{1}{4}(b-a)^{2}-\\mathbb{E}[(Z-\\mathbb{E}[Z])^{2}]}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For notational convenience, let us denote $V:=\\mathbb{E}[(Z-\\mathbb{E}[Z])^{2}]$ , then the right-hand side above can be expressed as a function of $\\mathrm{v}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(V):=2V\\sqrt{\\frac{1}{4}(b-a)^{2}-V}=\\sqrt{(b-a)^{2}V^{2}-4V^{3}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Because $V$ is the variance, $V\\in[0,\\frac{1}{4}(b-a)^{2}]$ .Additionally,because $\\sqrt{\\cdot}$ is monotonically increasing, the maximum is achieved either at the points where the first order derivative of $(b-a)^{2}V^{2}-4V^{3}$ ", "page_idx": 23}, {"type": "text", "text": "with respect to $V$ is O or at the boundary, O and $\\textstyle{\\frac{1}{4}}(b-a)^{2}$ . Let us calculate the points where the first order derivative is O: ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\frac{d}{d V}}\\left[(b-a)^{2}V^{2}-4V^{3}\\right]=2(b-a)^{2}V-12V^{2}=0\\Longleftrightarrow V={\\frac{(b-a)^{2}}{6}}\\quad{\\mathrm{or}}\\quad V=0={\\frac{\\Gamma}{2}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To obtainte maximum vale achievable, we calculate the values at points V = 0, V = (b ,and $\\begin{array}{r}{V=\\frac{1}{4}(b-a)^{2}}\\end{array}$ and pick the maximum value afterwards: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f(0)=2\\times0\\sqrt{\\frac{1}{4}(b-a)^{2}-0}=0}}\\\\ {{f(\\frac{1}{6}(b-a)^{2})=2\\times\\frac{1}{6}(b-a)^{2}\\sqrt{\\frac{1}{4}(b-a)^{2}-\\frac{1}{6}(b-a)^{2}}=\\frac{1}{6\\sqrt{3}}|b-a|^{3}}}\\\\ {{f(\\frac{1}{4}(b-a)^{2})=2\\times\\frac{1}{4}(b-a)^{2}\\sqrt{\\frac{1}{4}(b-a)^{2}-\\frac{1}{4}(b-a)^{2}}=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, temximvalacheable $\\textstyle{\\frac{1}{6{\\sqrt{3}}}}\\left|b-a\\right|^{3}$ ", "page_idx": 24}, {"type": "text", "text": "We now show that upper bound on the absolute value of the third central moment is actually tight by providing with a concrete example. For a random variable $Z$ let $\\mathbb{P}[Z=a]=\\frac{1}{4}$ \uff0c $\\begin{array}{r}{\\mathbb{P}[Z=b]\\stackrel{\\bullet}{=}\\frac{\\intercal}{4}}\\end{array}$ , and $\\begin{array}{r}{\\mathbb{P}[Z=\\frac{a+b}{2}]=\\frac{1}{2}}\\end{array}$ We can veritythat $\\begin{array}{r}{\\mathbb{E}[(Z-\\mathbb{E}[Z])^{3}]=\\frac{1}{6\\sqrt{3}}|b-a|^{3}}\\end{array}$ thus roing tat tis uper bound is indeed tight. ", "page_idx": 24}, {"type": "text", "text": "This helps us to arrive at the following inequality: ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\sum_{k\\in R_{i}}a_{k}X_{k l}^{3}+2(\\sum_{k\\in R_{i}}a_{k}X_{k l})^{3}-3(\\sum_{k\\in R_{i}}a_{k}X_{k l}^{2})(\\sum_{k\\in R_{i}}a_{k}X_{k l})|\\leq\\frac{1}{6\\sqrt{3}}|\\operatorname*{max}_{k\\in R_{i}}X_{k l}-\\operatorname*{min}_{k\\in R_{i}}X_{k l}|^{3}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, for the upper bound of the third order partial derivative, we have the following explicit formula: ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\frac{\\partial^{3}\\ell(\\beta)}{\\partial\\beta_{l}^{3}}|\\leq\\frac{1}{6\\sqrt{3}}\\sum_{i=1}^{n}\\delta_{i}|\\underset{k\\in\\cal R_{i}}{\\operatorname*{max}}X_{k l}-\\underset{k\\in\\cal R_{i}}{\\operatorname*{min}}X_{k l}|^{3}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "A.4 Analytical Solution to the Cubic Surrogate Problem ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let $f(x)$ be a convex function whose first, second, and third derivatives all exist. Let $h_{x}(\\Delta x):=$ $f(x)\\dot{+}\\;\\dot{f}^{\\prime}(x)\\Delta x+{\\textstyle{\\frac{1}{2}}}f^{\\prime\\prime}(x)\\Delta x^{2}+{\\textstyle{\\frac{1}{6}}}{\\cal L}_{3}|\\Delta x|^{3}$ be the cubic surrogate function [53] of $f(x)$ \uff0cwhere $L_{3}>0$ is the Lipschitz-constant of the second derivative $f^{\\prime\\prime}(x)$ . Then, the minimum of this surrogate function is achieved at the following point: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\operatorname*{argmin}_{\\Delta x}h_{x}(\\Delta x)=\\operatorname*{sgn}(f^{\\prime}(x))\\cdot\\frac{f^{\\prime\\prime}(x)-\\sqrt{(f^{\\prime\\prime}(x))^{2}+2L_{3}|f^{\\prime}(x)|}}{L_{3}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We discuss three cases: $f^{\\prime}(x)>0$ $f^{\\prime}(x)<0$ ,and $f^{\\prime}(x)=0$ ", "page_idx": 24}, {"type": "text", "text": "Case 1 $f^{\\prime}(x)>0$ ", "page_idx": 24}, {"type": "text", "text": "If $f^{\\prime}(x)>0$ ,then $\\Delta\\hat{x}<0$ . For the sake of contradiction, suppose $\\Delta\\hat{x}>0$ , which means $h_{x}(\\Delta x)$ achieves its minimum at some point with $\\Delta x>0$ . However, we arrive at a contradiction because ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{x}(0)=f(x)<f(x)+f^{\\prime}(x)\\Delta x+\\frac{1}{2}f^{\\prime\\prime}(x)\\Delta x^{2}+\\frac{1}{6}L_{3}|\\Delta x|^{3}=h_{x}(\\Delta x),\\mathrm{~for~}\\Delta x>0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, the minimum is achieved either at $\\Delta x=0$ or $\\Delta x<0$ . However, since $\\Delta x^{2}$ and $|\\Delta x|^{3}$ grow slower than $|\\Delta x|$ when $\\Delta x$ is close to O, there exists some $\\Delta x\\,<\\,0$ such that $f^{\\prime}(x)\\Delta x+$ $\\begin{array}{r}{\\frac12f^{\\prime\\prime}(x)\\Delta x^{2}+\\frac16L_{3}^{'}|\\Delta\\dot{x}|^{3}<0}\\end{array}$ . Thus, $h_{x}(0)$ cannot be the minimum value, and we are left with the ", "page_idx": 24}, {"type": "text", "text": "minimum value achieved at some $\\Delta x<0$ . If $\\Delta x<0$ $h_{x}(\\Delta x)=f(x)+f^{\\prime}(x)\\Delta x+{\\textstyle{\\frac{1}{2}}}f^{\\prime\\prime}(x)\\Delta x^{2}-$ $\\begin{array}{r}{{\\frac{1}{6}}L_{3}(\\Delta x)^{3}}\\end{array}$ . Note that the second order derivative of $h_{x}(\\Delta x)$ is greater than or equal to O since ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{d^{2}}{d\\Delta x^{2}}h_{x}(\\Delta x)=f^{\\prime\\prime}(x)-L_{3}\\Delta x\\geq0.}\\\\ {\\#\\int^{\\prime\\prime}(x)\\geq0\\;b e c a u s e\\;f(x)\\;i s\\;c o n\\nu e x,\\,0\\;;}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, $h_{x}(\\Delta x)$ is a convex function with respect to $\\Delta x$ when $\\Delta x<0$ , and its minimum value is achieved when the first order derivative is O. When the derivative with respect to $\\Delta x$ is $0$ ,wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\nf^{\\prime}(x)+f^{\\prime\\prime}(x)\\Delta x-{\\frac{1}{2}}(\\Delta x)^{2}=0\\Longleftrightarrow\\Delta x={\\frac{f^{\\prime\\prime}(x)\\pm{\\sqrt{(f^{\\prime\\prime}(x))^{2}+2L_{3}f^{\\prime}(x)}}}{L_{3}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $f^{\\prime}(x)>0$ only one root $\\begin{array}{r}{\\frac{f^{\\prime\\prime}(x)-\\sqrt{(f^{\\prime\\prime}(x))^{2}+2L_{3}f^{\\prime}(x)}}{L_{3}}}\\end{array}$ satisfing the condition $\\Delta x<0$ Thus, when $f^{\\prime}(x)<0$ ,we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\frac{f^{\\prime\\prime}(x)-\\sqrt{(f^{\\prime\\prime}(x))^{2}+2L_{3}f^{\\prime}(x)}}{L_{3}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Case 2 $f^{\\prime}(x)<0$ ", "page_idx": 25}, {"type": "text", "text": "If $f^{\\prime}(x)\\,<\\,0$ ,we have $\\Delta\\hat{x}\\;>\\;0$ using the same logic as above. If $\\Delta x\\,>\\,0$ $h_{x}(\\Delta x)\\,=\\,f(x)\\,+$ $\\begin{array}{r}{f^{\\prime}(x)\\dot{\\Delta x}+\\frac12f^{\\prime\\prime}(x)\\Delta x^{2}+\\frac16L_{3}(\\Delta x)^{\\bar{3}}}\\end{array}$ . Note that the second order derivative of $h_{x}(\\Delta x)$ is also greater than or equal to O since ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{d^{2}}{d\\Delta x^{2}}h_{x}(\\Delta x)=f^{\\prime\\prime}(x)+L_{3}\\Delta x\\geq0.}\\\\ {\\#\\,f^{\\prime\\prime}(x)\\geq0\\;b e c a u s e\\;f(x)\\;i s}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, $h_{x}(\\Delta x)$ is a convex function with respect to $\\Delta x$ when $\\Delta x>0$ , and its minimum value is achieved when the first derivative is O. When the derivative with respect to $\\Delta x$ is O,wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\nf^{\\prime}(x)+f^{\\prime\\prime}(x)\\Delta x+{\\frac{1}{2}}(\\Delta x)^{2}=0\\Longleftrightarrow\\Delta x={\\frac{-f^{\\prime\\prime}(x)\\pm{\\sqrt{(f^{\\prime\\prime}(x))^{2}-2L_{3}f^{\\prime}(x)}}}{L_{3}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since f(x)<0, only one root =f\"(\u00b1)+(f\"(\u00b1)-2Laf(\u00b1) satisfing the condition $\\Delta x>0$ Thus, when $f^{\\prime}(x)<0$ ,wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\frac{-f^{\\prime\\prime}(x)+\\sqrt{(f^{\\prime\\prime}(x))^{2}-2L_{3}f^{\\prime}(x)}}{L_{3}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Case 3 $f^{\\prime}(x)=0$ ", "page_idx": 25}, {"type": "text", "text": "When $f^{\\prime}(x)=0$ , the minimum value of $h_{x}(\\Delta x)$ is achieved at $\\Delta x=0$ ", "page_idx": 25}, {"type": "text", "text": "The explicit formulas for the three cases above can be unified into one succinct formula below: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\mathrm{sgn}(f^{\\prime}(x))\\cdot\\frac{f^{\\prime\\prime}(x)-\\sqrt{(f^{\\prime\\prime}(x))^{2}+2L_{3}|f^{\\prime}(x)|}}{L_{3}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "A.5Analytical Solution to the $\\ell_{1}$ -regularized Quadratic and Cubic Surrogate Problems ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "$\\ell_{1}$ -regularized quadratic surrogate problem  We have the following $\\ell_{1}$ -regularized quadratic surrogate problem: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\operatorname*{argmin}_{\\Delta x}a\\Delta x+\\frac{1}{2}b\\Delta x^{2}+\\lambda_{1}|c+\\Delta x|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The solution for the above problem is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\left\\{\\begin{array}{l l l}{-(a-\\lambda_{1})/b}&{\\mathrm{if}}&{b c-a<-\\lambda_{1}}\\\\ {-(a+\\lambda_{1})/b}&{\\mathrm{if}}&{b c-a>\\lambda_{1}}\\\\ {-c}&{\\mathrm{otherwise}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof.Since the function $a\\Delta x+\\textstyle{\\frac{1}{2}}b\\Delta x^{2}+\\lambda_{1}|c+\\Delta x|$ is convex, the condition for this function to achieve the minimum value is for its differential to include O. The differential of this function is: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\partial_{\\Delta x}\\left(a\\Delta x+\\frac{1}{2}b\\Delta x^{2}+\\lambda_{1}|c+\\Delta x|\\right)=\\left\\{\\!\\!\\!\\begin{array}{l l l}{a+b\\Delta x+\\lambda_{1}}&{\\mathrm{if}}&{\\Delta x>-c\\left(\\frac{1}{2}\\right)}\\\\ {a+b\\Delta x-\\lambda_{1}}&{\\mathrm{if}}&{\\Delta x<-c\\left(\\frac{1}{2}\\right)}\\\\ {a+b\\Delta x+[-\\lambda_{1},\\lambda_{1}]}&{\\mathrm{if}}&{\\Delta x=-c\\left(\\frac{1}{2}\\right)}\\end{array}\\!\\!\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "1. For the first condition, if the differential contains O, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\na+b\\Delta x+\\lambda_{1}=0\\quad\\Rightarrow\\quad\\Delta x=-{\\frac{a+\\lambda_{1}}{b}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "However, because we require $\\Delta x>-c$ ,we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n-{\\frac{a+\\lambda_{1}}{b}}>-c\\Rightarrow b c-a>\\lambda_{1}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "2. For the second condition, if the differential contains O, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\na+b\\Delta x-\\lambda_{1}=0\\quad\\Rightarrow\\quad\\Delta x=-{\\frac{a-\\lambda_{1}}{b}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "However, because we require $\\Delta x<-c$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n-\\frac{a-\\lambda_{1}}{b}<-c\\Rightarrow b c-a<-\\lambda_{1}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "3. For the third condition, if the differential contains O, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{0\\in a+b\\Delta x+[-\\lambda_{1},\\lambda_{1}]}&{{}\\Rightarrow a+b\\Delta x-\\lambda_{1}\\leq0\\leq a+b\\Delta x+\\lambda_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "However, because we require $\\Delta x=-c$ ,we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n-\\lambda_{1}\\leq b c-a\\leq\\lambda_{1}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\ell_{1}$ -regularized cubic surrogate problem  We have the following $\\ell_{1}$ -regularized cubic surrogate problem: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\operatorname*{argmin}_{\\Delta x}a\\Delta x+\\frac{1}{2}b\\Delta x^{2}+\\frac{1}{6}c|x|^{3}+\\lambda_{1}|d+\\Delta x|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The solution to the above problem is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\left\\{\\begin{array}{l l}{\\mathrm{sgn}(d)\\Bigl(-b+\\sqrt{b^{2}-2c(\\mathrm{sgn}(d)a+\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)a+\\lambda_{1}\\leq0}\\\\ {\\mathrm{sgn}(d)\\Bigl(b+\\sqrt{b^{2}+2c(\\mathrm{sgn}(d)a-\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)(a-b d)-\\frac{1}{2}c d^{2}>\\lambda_{1}}\\\\ {\\mathrm{sgn}(d)\\Bigl(b+\\sqrt{b^{2}+2c(\\mathrm{sgn}(d)a+\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)(a-b d)-\\frac{1}{2}c d^{2}<-\\lambda_{1}}\\\\ {-d}&{\\mathrm{otherwise}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Like the first part, since the function $a\\Delta x+\\textstyle{\\frac{1}{2}}b\\Delta x^{2}+\\textstyle{\\frac{1}{6}}c|x|^{3}+\\lambda_{1}|d+\\Delta x|$ isconvex,the condition for this function to achieve the minimum value is for its differential to include O. We discuss the differential of this function in two cases: $d\\geq0$ and $d<0$ ", "page_idx": 26}, {"type": "text", "text": "\u00b7When $d\\geq0$ , the differential of this function is: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\partial_{\\Delta x}\\left(a\\Delta x+\\frac{1}{2}b\\Delta x^{2}+\\frac{1}{6}c|x|^{3}+\\lambda_{1}|d+\\Delta x|\\right)}&{{}}\\\\ {\\displaystyle\\qquad\\left(a+b\\Delta x+\\frac{1}{2}c\\Delta x^{2}+\\lambda_{1}\\qquad\\qquad\\qquad\\mathrm{if}\\quad\\Delta x>0\\right.}\\\\ {\\displaystyle a+b\\Delta x+\\frac{1}{2}c[-\\Delta x^{2},\\Delta x^{2}]+\\lambda_{1}\\quad\\mathrm{if}\\quad\\Delta x=0}\\\\ {\\displaystyle a+b\\Delta x-\\frac{1}{2}c\\Delta x^{2}+\\lambda_{1}\\qquad\\qquad\\mathrm{if}\\quad-d<\\Delta x<0}\\\\ {\\displaystyle a+b\\Delta x-\\frac{1}{2}c\\Delta x^{2}+[-\\lambda_{1},\\lambda_{1}]\\quad}&{{}\\mathrm{if}\\quad\\Delta x=-d}\\\\ {\\displaystyle a+b\\Delta x-\\frac{1}{2}c\\Delta x^{2}-\\lambda_{1}\\qquad\\qquad\\qquad\\mathrm{if}\\quad\\Delta x<-d}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We discuss these 5 cases one by one. ", "page_idx": 26}, {"type": "text", "text": "1. For the first condition, if the differential contains O, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\na+b\\Delta{\\hat{x}}+{\\frac{1}{2}}c\\Delta{\\hat{x}}^{2}+\\lambda_{1}=0\\Rightarrow\\Delta{\\hat{x}}={\\frac{-b\\pm{\\sqrt{b^{2}-2c(a+\\lambda_{1})}}}{c}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "However, because we require $\\Delta x>0$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lambda_{1}=-(a+b\\Delta x+\\frac{1}{2}c\\Delta x^{2})<-a\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This means that we can only have one root because the other root violates $\\Delta x>0$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\frac{-b+\\sqrt{b^{2}-2c(a+\\lambda_{1})}}{c}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "2. For the second condition, if the differential contains O, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{0\\in a+b\\Delta\\hat{x}+\\displaystyle\\frac{1}{2}c[-\\Delta\\hat{x}^{2},\\Delta\\hat{x}^{2}]+\\lambda_{1}}\\\\ {\\Rightarrow a+b\\Delta\\hat{x}-\\displaystyle\\frac{1}{2}c\\Delta\\hat{x}^{2}+\\lambda_{1}\\leq0\\leq a+b\\Delta\\hat{x}+\\displaystyle\\frac{1}{2}c\\Delta\\hat{x}^{2}+\\lambda_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "However, because we require $\\Delta x=0$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\na+\\lambda_{1}=0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "3. For the third condition, if the differential contains O, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\na+b\\Delta{\\hat{x}}-{\\frac{1}{2}}c\\Delta{\\hat{x}}^{2}+\\lambda_{1}=0\\Rightarrow\\Delta{\\hat{x}}={\\frac{-b\\pm{\\sqrt{b^{2}+2c(a+\\lambda_{1})}}}{-c}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "However, because we require $-d<\\Delta x<0$ ,we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lambda_{1}=-\\left(a+b\\Delta\\hat{x}-\\frac{1}{2}c\\Delta\\hat{x}^{2}\\right)>-a\\Rightarrow a+\\lambda_{1}>0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This means that we can only have one root because the other root violates the condition $-d<\\Delta x<0$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\frac{b-\\sqrt{b^{2}+2c(a+\\lambda_{1})}}{c}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, since the root is between $-d$ and O, and the coefficients in front of $\\Delta x,-{\\frac{1}{2}}c$ is negative, we have $a-b d-{\\textstyle\\frac{1}{2}}c d^{2}+\\lambda_{1}<0$ ", "page_idx": 27}, {"type": "text", "text": "4. For the fourth condition, if the differential contains O, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\in a+b\\Delta\\hat{x}-\\displaystyle\\frac{1}{2}c\\Delta\\hat{x}^{2}+[-\\lambda_{1},\\lambda_{1}]}\\\\ {\\Rightarrow a+b\\Delta\\hat{x}-\\displaystyle\\frac{1}{2}c\\Delta\\hat{x}^{2}-\\lambda_{1}\\leq0\\leq a+b\\Delta\\hat{x}-\\displaystyle\\frac{1}{2}c\\Delta\\hat{x}^{2}+\\lambda_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "However, because we require $\\Delta x=-d$ ,we have ", "page_idx": 27}, {"type": "equation", "text": "$$\na-b d-{\\frac{1}{2}}c d^{2}-\\lambda_{1}\\leq0\\leq a-b d-{\\frac{1}{2}}c d^{2}-\\lambda_{1}\\Rightarrow|a-b d-{\\frac{1}{2}}c d^{2}|\\leq0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "5. For the fifth condition, if the differential contains O, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\na+b\\Delta\\hat{x}-\\frac{1}{2}c\\Delta\\hat{x}^{2}-\\lambda_{1}=0\\Rightarrow\\Delta\\hat{x}=\\frac{-b\\pm\\sqrt{b^{2}+2c(a-\\lambda_{1})}}{-c}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "However, because we require $\\Delta x<-d$ ,we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lambda_{1}=a+b\\Delta\\hat{x}-\\frac{1}{2}c\\Delta\\hat{x}^{2}<a\\Rightarrow a-\\lambda_{1}>0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This means that we can only have one root because the other root violates the condition $\\Delta x<-d\\leq0$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\frac{b-\\sqrt{b^{2}+2c(a-\\lambda_{1})}}{c}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, because the left root is less than $-d$ and the coefficient in front $\\Delta x^{2},-{\\textstyle{\\frac{1}{2}}}c,$ is negative, we have $a-b d-{\\textstyle\\frac{1}{2}}c d^{2}-\\lambda_{1}>0$ ", "page_idx": 27}, {"type": "text", "text": "\u00b7When $d<0$ , the differential of this function is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\partial_{\\Delta x}\\left(a\\Delta x+\\displaystyle\\frac{1}{2}b\\Delta x^{2}+\\displaystyle\\frac{1}{6}c|x|^{3}+\\lambda_{1}|d+\\Delta x|\\right)}\\\\ &{}&{\\left(a+b\\Delta x+\\displaystyle\\frac{1}{2}c\\Delta x^{2}+\\lambda_{1}\\right.\\qquad\\qquad\\mathrm{if}\\quad\\Delta x>-d\\right.}\\\\ &{}&{\\left.=\\left\\{a+b\\Delta x+\\frac{1}{2}c\\Delta x^{2}+[-\\lambda_{1},\\lambda_{1}]\\quad\\mathrm{~if}\\quad\\Delta x=-d\\right.}\\\\ &{}&{\\mathrm{if}\\quad0<\\Delta x<-d}\\\\ &{}&{a+b\\Delta x+\\displaystyle\\frac{1}{2}c[-\\Delta x^{2},\\Delta x^{2}]-\\lambda_{1}\\quad\\mathrm{if}\\quad\\Delta x=0}\\\\ &{}&{a+b\\Delta x-\\displaystyle\\frac{1}{2}c\\Delta x^{2}-\\lambda_{1}\\quad\\mathrm{~if}\\quad\\Delta x<0}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Similar to the previous part when $d\\geq0$ , we discuss the 5 cases one by one but omit the details because the logic and the reasoning process are exactly the same: ", "page_idx": 28}, {"type": "text", "text": "1. For the first condition, if the differential contains O, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\frac{-b+\\sqrt{b^{2}-2c(a+\\lambda_{1})}}{c}\\quad\\mathrm{and}\\quad a-b d+\\frac{1}{2}c d^{2}+\\lambda_{1}<0\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "2. For the second condition, if the differential contains O, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=-d\\quad\\mathrm{and}\\quad\\vert a-b d+\\frac{1}{2}c d^{2}\\vert\\leq\\lambda_{1}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "3. For the third condition, if the differential contains O, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\frac{-b+\\sqrt{b^{2}-2c(a-\\lambda_{1})}}{c}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "4. For the fourth condition, if the differential contains O, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=0\\quad\\mathrm{and}\\quad a-\\lambda_{1}=0\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "5. For the fifth condition, if the differential contains O, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\frac{b-\\sqrt{b^{2}+2c(a-\\lambda_{1})}}{c}\\quad\\mathrm{and}\\quad a-\\lambda_{1}>0\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can combine the two situations where $d\\geq0$ and $d<0$ and obtain a unified formula: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\left\\{\\begin{array}{l l}{\\mathrm{sgn}(d)\\Bigl(-b+\\sqrt{b^{2}-2c(\\mathrm{sgn}(d)a+\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)a+\\lambda_{1}<0}\\\\ {0}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)a+\\lambda_{1}=0}\\\\ {\\mathrm{sgn}(d)\\Bigl(b+\\sqrt{b^{2}+2c(\\mathrm{sgn}(d)a-\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)(a-b d)-\\frac{1}{2}c d^{2}>\\lambda_{1}}\\\\ {\\mathrm{sgn}(d)\\Bigl(b+\\sqrt{b^{2}+2c(\\mathrm{sgn}(d)a+\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)(a-b d)-\\frac{1}{2}c d^{2}<-\\lambda_{1}}\\\\ {-d}&{\\mathrm{otherwise}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The first and second equations above can be further unified into just one equation $\\operatorname{sgn}(d){\\Bigl(}-b+\\quad$ $\\sqrt{b^{2}-2c(\\mathrm{sgn}(d)a+\\lambda_{1})}\\biggr)/c$ $\\operatorname{sgn}(d)a+\\lambda_{1}\\leq0$ so we finally have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta\\hat{x}=\\left\\{\\begin{array}{l l}{\\mathrm{sgn}(d)\\Bigl(-b+\\sqrt{b^{2}-2c(\\mathrm{sgn}(d)a+\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)a+\\lambda_{1}\\leq0}\\\\ {\\mathrm{sgn}(d)\\Bigl(b+\\sqrt{b^{2}+2c(\\mathrm{sgn}(d)a-\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)(a-b d)-\\frac{1}{2}c d^{2}>\\lambda_{1}}\\\\ {\\mathrm{sgn}(d)\\Bigl(b+\\sqrt{b^{2}+2c(\\mathrm{sgn}(d)a+\\lambda_{1})}\\Bigr)/c}&{\\mathrm{if}\\quad\\mathrm{sgn}(d)(a-b d)-\\frac{1}{2}c d^{2}<-\\lambda_{1}}\\\\ {-d}&{\\mathrm{otherwise}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "B Related Work ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Optimization for CPH  One way to train the CPH model is through gradient descent [63]. However, because the CPH loss is complex, it is difficult to pick the right step sizes, so gradient descent tends to be slow when we want to obtain solutions with high precision. To speed up the training process, people have used the Newton method [25, 22, 23]. The drawback of this approach is that it is computationally intensive to calculate the full Hessian matrix. Moreover, the vanilla Newton method cannot be applied to solve the $\\ell_{1}$ -regularized problem. To alleviate this problem, quasi Newton [62] and proximal Newton [51] methods have been proposed. However, as we have shown in our experiments, these Newton methods have the faw of training loss blow up due to vanishing second order derivatives. A generic binary search method [41] has also been proposed, but the algorithm has been shown to be slower than the quasi Newton method. In contrast to all these approaches, our method is computationally efficient, can easily handle different regularizers, and guarantees that the loss decreases monotonically. ", "page_idx": 29}, {"type": "text", "text": "Modern First and Second-order Optimization Methods  Our work is greatly inspired by the modern developments for convex optimization [52], but the general principle cannot be rigidly applied. For first order methods, as we mentioned about gradient descent in the last paragraph, it is difficult to choose the right stepsize for fast convergence. Instead of performing gradient descent, our method performs coordinate descent, which has been shown to be effective in training other statistical models [9, 18, 19, 20, 34, 55]. We give an explicit formula, by leveraging the Popoviciu's inequality on variances [58], to calculate the Lipschitz constant at each coordinate. We also design a second order method (still under the coordinate descent framework) based on the cubic-regularization of the Newton method [53]. To calculate the Lipschizt constant, we make the connections to the third central moment and Bhatia-Davis's inequality [61]. Moreover, we are able to exploit the mathematical structures of the CPH model to compute the second order partial derivatives at the computational complexityof $O(n)$ , making the evaluation per iteration of our second order method as fast as that of our first order method. ", "page_idx": 29}, {"type": "text", "text": "Variable Selection and Interpretability for the CPH Model If we can find sparse solutions whose predictive performances are as good as dense solutions, we can better interpret which features play important roles. A popular way to select important variables for the CPH model is adding an $\\ell_{1}$ regularization term [64], commonly known as the LASsO method. We can also use the ElasticNet method, which adds an $\\ell_{1}+\\ell_{2}$ regularization term [62]. Another way is to apply the Adaptive LASSO [69], which repeatedly use the absolute values of coefficients obtained from the previous iteration as weights of $\\ell_{1}$ regularizations for parameters in the current iteration. These above approaches all use convex regularizers and have difficulty obtaining high-quality solutions when the support size is small. The reason is that these convex regularizers penalize the magnitude of the coefficients while promoting sparsity. To avoid this issue, recently, solving the $\\ell_{0}$ -constrained problem [49, 29, 4] has attracted lots of attention and shown to produce much sparser models without losing accuracy. For the $\\ell_{0}$ -constrained CPH problem, ABESS [71] has proposed to use a hybrid method of greedy selection and feature swapping to solve the problem heuristically. However, as we have shown in our experiments, this method cannot handle highly correlated features. Our method also solves the $\\ell_{0}$ -constrained CPH problem but uses the beam search framework [66, 48, 47]. Although the beam search framework has already existed, this frameowork cannot be applied to the CPH model without our coordinate descent algorithm. We need to use coordinate descent for support expansion as well as coefficient finetuning, in which other Newton-type methods all have issues with losses potentially blowing up. ", "page_idx": 29}, {"type": "text", "text": "Other Model Classes for Survival Analysis In addition to the CPH model, there are some other model classes that can be applied to analyze time-to-event data. One model class is the survival tree models [70, 3, 35]. Survival trees have the advantage of capturing non-linear interactions between features. However, when sparse models are desired, the accuracy of sparse trees is compromised by the fact that all samples in the same leaf node share the same predictions. One way to overcome this issue is to construct ensembles of trees using random forest or boosting techniques [36, 32, 33]. Another model class for survival analysis is based on neural networks [39, 7, 6, 59, 21]. However, for all these other model classes mentioned, they are not very interpretable due to large parameter space. The CPH model, which is the focus of our work, provides both interpretability and good accuracy. For applications involving high stakes decisions, it is desirable to produce models that are as sparse as possible without losing accuracy. In this work and especially the variable selection experiments, we push the limit of sparsity-accuracy tradeoff curve for this model class. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "C  Experimental Setup Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "C.1  Computing Platforms ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "All experiments were run on the Intel(R) Xeon(R) CPU E5-2680 v3 Processor, 2.50GHz. We set the memory limit to be 100GB. ", "page_idx": 31}, {"type": "text", "text": "C.2  Datasets, Baselines, and Licenses ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We have a summary of datasets for experiments in Table 1. ", "page_idx": 31}, {"type": "table", "img_path": "RHQbxlhzhm/tmp/41d276207978c7a74d518ecc9d46408d1fd9d804d69f6fc6a4b898135c4d2e35.jpg", "table_caption": ["Table 1: Datasets Summary. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Synthetic Data Generation Process  The synthetic data used in the paper is generated according to the following process, similar to [71]: ", "page_idx": 31}, {"type": "text", "text": "Firstly, from a Gaussian distribution ${\\mathcal{N}}(\\mathbf{0},\\Sigma)$ where the first entry is the mean and the second entry is the covariance matrix with size $p\\times p$ , we sample features: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{i}\\sim\\mathcal{N}(\\mathbf{0},\\Sigma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The covariance matrix is defined as $\\Sigma_{j l}=\\rho^{|j-l|}$ , where $\\rho\\in(0,1]$ is the correlation parameter. When $\\rho$ is large, the features in $\\pmb{x}_{i}$ are more correlated. We create a k-sparse coefficient vector $\\beta^{*}\\in\\mathbb{R}^{p}$ The entries of $\\beta^{*}$ are either 1 or O0. If $j\\mathrm{mod}(p/k)=0$ then $\\beta_{j}^{*}=1$ ; otherwise, $\\beta_{j}^{*}=0$ ", "page_idx": 31}, {"type": "text", "text": "Secondly, we generate the death time $t_{i}$ according to the following equation: ", "page_idx": 31}, {"type": "equation", "text": "$$\nt_{i}=\\left(-\\frac{\\log V_{i}}{e^{x_{i}^{T}\\beta^{*}}}\\right)^{s},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $V_{i}\\sim U(0,1)$ (samples are drawn from a uniform distribution on the interval $[0,1])$ and $s$ is a hyperparameter. In our experiments, we set $s=0.1$ ", "page_idx": 31}, {"type": "text", "text": "Lastly, we generate the censoring time, the censoring indicator, and change the death time to observation time. We sample the censoring time from a uniform distribution: $C_{i}\\sim U(0,1)$ .If the death time is bigger than the censoring time, we have the indicator equal to 1; otherwise, we have the indicator equal to O. Specifically, we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\delta_{i}=\\mathbb{1}_{t_{i}>C_{i}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Afterwards, we change the death time to observation time, taking into consideration of censoring: ", "page_idx": 31}, {"type": "equation", "text": "$$\nt_{i}=\\operatorname*{min}(t_{i},C_{i}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We form a triplet $(x_{i},t_{i},\\delta_{i})$ and return this triplet as one sample ", "page_idx": 31}, {"type": "text", "text": "Real-world survival data: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u00b7 Flchain: Use of nonclonal serum immunoglobulin free light chains to predict overall survival in the general population [10]. The event is death.   \n\u00b7 Kickstarterl: Data from a popular crowdfunding platform, used to predict project success [46]. We used the version from https: //dmkd. cs.vt .edu/projects/survival/ data/. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 Dialysis: Data from a survival study of dialysis patients, which aims to assess quality of renal replacement therapy at dialysis centers in Rio de Janeiro, Brazil [60]. \u00b7 EmployeeAttrition: The task of predicting when an IBM employee will quit. The event is an IBM employee's leaving [30]. ", "page_idx": 32}, {"type": "text", "text": "Licenses  We list the licenses of the software packages used in this paper: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 Abess: The license of this package is GPL-3.   \n\u00b7 skglm: The license of this package is BSD-3.   \n\u00b7 Scikit-survival (SkSurv): The license of this package is GPL-3.   \n\u00b7 Flchain: We use the dataset from the Scikit-Survival [56] package. The GitHub link to this dataset is https: //github.com/sebp/scikit-survival/tree/master/ sksurv/datasets/data. The license of this package is GPL-3.   \n\u00b7 Kickstarter1: We use the dataset from the Virginia Tech. The link to this dataset is https : //dmkd. cs. vt .edu/projects/survival/data/. There is no license associated with this dataset. This means we cannot modify any part of the dataset, which we have obeyed while doing experiments on this dataset.   \n\u00b7 Dialysis: We use the dataset from the SurvSet [12] package. The GitHub link to this dataset is https: //raw.githubusercontent .com/ErikinBC/SurvSet/main/ SurvSet/ _datagen/output /Dialysis . csv. The license of this package is GPL-3.   \n\u00b7 EmployeeAttrition: We use the dataset from the PySurvival [17] package. The GitHub link to this dataset is https://github.com/square/pysurvival/blob/master/ pysurvival/datasets/employee_attrition.csv. The license of this package is Apache-2. ", "page_idx": 32}, {"type": "text", "text": "Baselines  We compared our method against various survival models: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 Abess: Adaptive Best-Subset Selection (ABESS) algorithm [71] for Cox proportional hazards model. We used the Cox model in abess python package Version 0.4.6.   \n\u00b7 SkglmALassoCox: Cox model with the adaptive Lasso regularization [69]. We used the implementation from skglm [2, 51].   \n\u00b7 SksurvCoxnet: Cox's proportional hazard's model with elastic net penalty [62]. We used the implementation from Scikit-survival (SkSurv): scikit-survival version-0.20.0 (https : //scikit-survival.readthedocs.io/en/stable/index.html).   \n\u00b7 SksurvTree: A greedy decision tree model using log-rank splitting rule [43]. We used the implementation from sksurv.   \n\u00b7 SksurvRSF: Random survival forest [37] algorithm. We used the implementation from Sksurv.   \n\u00b7 SksurvGBST: Gradient-boosted Cox proportional hazards loss with regression trees as base learner. In each stage, a regression tree is fit on the negative gradient of the loss function. We used the implementation from Sksurv.   \n\u00b7 SksurvNaiveSVM: Naive version of linear Survival Support Vector Machine [65]. We used the implementation from Sksurv.   \n\u00b7 SksurvFastSVM: Effcient Training of linear Survival Support Vector Machine [57]. We used the implementation from Sksurv. ", "page_idx": 32}, {"type": "text", "text": "Evaluation Metrics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": ". CIndex: The full name of this metric score is Harrell's Concordance Indices [26]. It is used to evaluate the discrimination ability of a survival model. It assesses how well the model ranks observations based on their predicted risk of experiencing an event (e.g., death, disease recurrence) over time. The higher the CIndex score, the better the model. ", "page_idx": 32}, {"type": "text", "text": "2. IBS: The Integrated Brier score was proposed by [24] to evaluate survival models across all possible time threshold. The IBS score takes the Brier score a step further by integrating it across all possible time points within the follow-up period of interest. This provides a single score summarizing the model's performance over the entire time range. The lower the IBS score, the better the model. 3. F1-score, Precision, Recall: Suppose the true coefficients are $\\beta^{*}$ and the estimated coefficients are $\\hat{\\beta}$ Then the precision score can be calculated as $P\\;=\\;|\\mathrm{supp}(\\beta^{*})\\cup$ $\\operatorname{supp}(\\hat{\\beta})|/|\\operatorname{supp}(\\hat{\\beta})|$ where $\\operatorname{supp}(\\cdot)$ extracts the support (indices whose coefficients are nonzero) of the input vector. The recall score can be calculated as $R\\,=\\,|\\mathrm{supp}(\\beta^{*})\\cup$ $\\operatorname{supp}(\\hat{\\beta})|/|\\operatorname{supp}(\\beta^{*})|$ . We calculate the F1 score as $\\mathrm{Fl}=2P R/(P+R)$ ", "page_idx": 33}, {"type": "text", "text": "C.3Details about Variable Selection Experiments ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Collection and Setup:  We ran 5-fold cross-validation (random seed 0) on the following datasets: Dialysis, Flchain, Kickstarter1, EmployeeAttrition, SyntheticHighCorrHighDim1, SyntheticHighCorrHighDim2, SyntheticHighCorrHighDim3. In order to create highly correlated features, we encoded continuous features into binarized features, by considering 10o0 quantiles for each continuous column. For each dataset, we ran algorithms with different configurations and evaluated fitted models with metrics described in Appendix C.2: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Abess: We ran this algorithm with 30 different configurations: support size, $k$ , ranging from 1 to 30, forcing the number of non-zero coefficients in the Cox model to be exact $k$ .We set primary_model_fit_mac_iter to be 20, approximate_Newton to be False. All other parameters were set to the default.   \n\u00b7 SksurvCoxnet: We ran this algorithm with 30 different configurations: support size, $k$ \uff0c ranging from 1 to 30, forcing the number of non-zero coefficients in the Cox model to be exact $k$ . We set l1_ratio to be 1.0, alpha_min_ratio to be 0.01. All other parameters were set to the default.   \n\u00b7 SkglmALassoCox: We ran this algorithm with 9 different L1 regularization penalty parameters (alpha): 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100. All other parameters were set to the default.   \n\u00b7 SksurvTree: We ran this algorithm with 8 different configurations: max depth limit, $d$ ranging from 2 to 9, and a corresponding maximum leaf limit $2^{d}$ . The random state was set to 2024 and all other parameters were set to the default.   \n\u00b7 SksurvRSF, SksurvGBST: We ran this algorithm with $8\\times5$ configurations: max depth limit, $d$ , ranging from 2 to 9, and 5 different total numbers of estimators (10, 50, 100, 500, 100). The random state was set to 2024 and all other parameters were set to the default.   \n\u00b7 SksurvNaiveSVM, SksurvFastSVM: We ran this algorithm with 9 different $\\ell_{2}$ regularization penalty parameters (alpha): 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100. All other parameters were set to the default.   \n\u00b7 SksurvCoxPHBeamSearch (our method): We ran this algorithm with 30 different configurations: support size, $k$ , ranging from 1 to 30, forcing the number of non-zero coefficients in the Cox model to be exact $k$ ", "page_idx": 33}, {"type": "text", "text": "Recording experimental results: For each method with specific configuration, we have a set of up to 5 fitted models on each dataset. Some metrics may be unavailable: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Precision, recall, and fl-score are not available on real-world data as we do not know the true coeffcients.   \n\u00b7 The losses on the training and testing folds of cox models are not applied to non-Cox models.   \n\u00b7 SksurvNaiveSVM and SksurvFastSVM were not able to provide IBS and AUC.   \n\u00b7 Some methods\u2019 training time exceeded our 3-hour time limit (We noticed that sksurvNaiveSVM often timed out). ", "page_idx": 33}, {"type": "text", "text": "For Cox and SVM models, we recorded the number of non-zero coefficients as the support size. For tree based models we recorded the number of nodes as the support size. We plotted the standard deviation of support size and various metric scores as corresponding error bars. ", "page_idx": 33}, {"type": "text", "text": "D Additional Results ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "D.1 Optimization on $\\ell_{1}$ and $\\ell_{1}+\\ell_{2}$ -regularized Problems ", "page_idx": 34}, {"type": "text", "text": "D.1.1  Results on Flchain ", "page_idx": 34}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/a8451e6d3dcbba34c953f967348382aa0c16e30df1d880ede2e53f8539a4b273.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 5: Optimization on the Flchain dataset with $\\lambda_{1}\\,=\\,0$ and $\\lambda_{2}=1.0$ .The baselines (exact Newton, quasi Newton, and proximal Newton) all have the losses blow up. In contrast, our methods based on the quadratic and cubic surrogate functions have the losses monotonically decreasing. ", "page_idx": 34}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/f765661b70c7973fb4f0f9b18fedc8aed6cfccaea86ac23f696418493ccfb4a1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 6: Optimization on the Flchain dataset with $\\lambda_{1}=0$ and $\\lambda_{2}=5.0$ . The baseline, exact Newton, has the losses blow up despite a stronger $\\ell_{2}$ regularization. The other two baselines, quasi Newton and proximal Newton, do not have this issue when $\\ell_{2}$ increases but are significantly slower than our methods. ", "page_idx": 34}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/7cafe2fd07c4cf7ce124cf2a7dec81e9f2105b3b1e1a39635304fa2880e60971.jpg", "img_caption": ["Figure 7: Optimization on the Flchain dataset with $\\lambda_{1}\\,=\\,1.0$ and $\\lambda_{2}\\,=\\,1.0$ . The exact Newton method can be applied to solve the $\\ell_{1}$ -regularized problems, so we only compare with quasi Newton and proximal Newton. These two baselines both have the losses blow up when the $\\ell_{2}$ regularization is weak. In contrast, our methods based on the quadratic and cubic surrogate functions have losses that monotonically decrease. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/2d01c5341f2d31d274d1a10eed558559ed681a59695135c8fe3fce94dd972ae1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 8: Optimization on the Flchain dataset with $\\lambda_{1}\\,=\\,1.0$ and $\\lambda_{2}\\,=\\,5.0$ . The exact Newton method can be applied to solve the $\\ell_{1}$ -regularized problems, so we only compare with quasi Newton and proximal Newton. Stronger $\\ell_{2}$ regularization helps these two baselines avoid the losses going into infinity. However, these two baselines are still significantly slower than our methods. ", "page_idx": 35}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/f20d9607bd8a358d6e22621dc27f5f806ad77d799ab04a20e966a078fc402f96.jpg", "img_caption": ["D.1.2 Results on Employee Attrition "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 9: Optimization on the Employee Attrition dataset with $\\lambda_{1}=0$ and $\\lambda_{2}=1.0$ .Althoughour methods make less progress toward the minimum loss per iteration (left plot), we are significantly faster than other methods in terms of elapsed time (wall clock) due to cheap evaluation cost per iteration. For ease of figure reading, we only give a partial plot with a few iterations. When the number of iterations is large,our methods achievebetter losses than the baselinemethods. ", "page_idx": 36}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/58ae07bb2ff6ec1c5ef59563a1b6b227ec10502ac6a5a2a191cd697ef7e47539.jpg", "img_caption": ["Figure 10: Optimization on the Employee Attrition dataset with $\\lambda_{1}=0$ and $\\lambda_{2}=5.0$ .Althoughour methods make less progress toward the minimum loss per iteration (left plot), we are significantly faster than other methods in terms of elapsed time (wall clock) due to cheap evaluation cost per iteration. For ease of figure reading, we only give a partial plot with a few iterations. When the number of iterations is large, our methods achieve better losses than the baseline methods. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/f813dda75d47bec55706f8e3204ac19cbc243a8441b2794e177233f773d1fa3a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Figure 11: Optimization on the Employee Attrition dataset with $\\lambda_{1}=1.0$ and $\\lambda_{2}=1.0$ .The exact Newton method can be applied to solve the $\\ell_{1}$ -regularized problems, so we only compare with quasi Newton and proximal Newton. Although our methods make less progress toward the minimum loss per iteration (left plot), we are significantly faster than other methods in terms of elapsed time (wall clock) due to cheap evaluation cost per iteration. For ease of figure reading, we only give a partial plot with a few iterations.When the number of iterations is large,our methods achieve better losses thanthebaselinemethods. ", "page_idx": 37}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/44f09584674a92efcad158f15e110ea6e7eba92e81c1dc420f20c6db815ad97d.jpg", "img_caption": ["Figure 12: Optimization on the Employee Attrition dataset with $\\lambda_{1}=1.0$ and $\\lambda_{2}=5.0$ .Theexact Newton method can be applied to solve the $\\ell_{1}$ -regularized problems, so we only compare with quasi Newton and proximal Newton. Although our methods make less progress toward the minimum loss per iteration (left plot), we are significantly faster than other methods in terms of elapsed time (wall clock) due to cheap evaluation cost per iteration. For ease of figure reading, we only give a partial plot with a few iterations. When the number of iterations is large, our methods achieve better losses thanthebaselinemethods. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/4862c40f005e9e468a78ef7d19f16e8ce374dd28c84a55729bd57d01338094e8.jpg", "img_caption": ["D.1.3  Results on Kickstarterl ", "Figure 13: Optimization on the Kickstarter1 dataset with $\\lambda_{1}=0$ and $\\lambda_{2}=1.0$ .The baselines (exact Newton, quasi Newton, and proximal Newton) all have losses that blow up. In contrast, our methods based on the quadratic and cubic surrogate functions have monotonically decreasing losses. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/7eac6df67bae23c15741d525d09dab648633a957f2eee0287d0a9307cb10e1fa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 14: Optimization on the Kickstarter1 dataset with $\\lambda_{1}=0$ and $\\lambda_{2}=5.0$ .The baselines (exact Newton, quasi Newton, and proximal Newton) all have the losses blow up. In contrast, our methods based on the quadratic and cubic surrogate functions have monotonically decreasing losses. ", "page_idx": 38}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/a19a5ab9ec781dac417eb8e4aa7496ad39055f959be481e3eebd6d77e02d1b29.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 15: Optimization on the Kickstarter1 dataset with $\\lambda_{1}=1.0$ and $\\lambda_{2}=1.0$ . The exact Newton method can be applied to solve the $\\ell_{1}$ -regularized problems, so we only compare with quasi Newton and proximal Newton. These two baselines both have the losses blow up when both the $\\ell_{1}$ and $\\ell_{2}$ regularizations are weak. In contrast, our methods based on the quadratic and cubic surrogate functions have monotonically decreasing losses. ", "page_idx": 39}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/a24df16ee9f0cf8bf8dd6935e095f93f098d16b63550f2e0fafdb3435dbec98b.jpg", "img_caption": ["Figure 16: Optimization on the Kickstarter1 dataset with $\\lambda_{1}=1.0$ and $\\lambda_{2}=5.0$ .The exact Newton method can be applied to solve the $\\ell_{1}$ -regularized problems, so we only compare with quasi Newton and proximal Newton. These two baselines both have the losses blow up even when we have a stronger $\\ell_{2}$ regularization. In contrast, our methods based on the quadratic and cubic surrogate functions have monotonically decreasing losses. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/4c248b3ddbea8547570fa0b05065cb5ddb261de6a29775ab31930b0e0878b8de.jpg", "img_caption": [], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Figure 17: Optimization on the Dialysis dataset with $\\lambda_{1}=0$ and $\\lambda_{2}=1.0$ Although our methods make less progress toward the minimum loss per iteration (left plot), we are significantly faster than other methods in terms of elapsed time (wall clock) due to cheap evaluation cost per iteration. For ease of figure reading, we only give a partial plot with a few iterations. When the number of iterations is large, our methods achieve better losses than the baseline methods. ", "page_idx": 40}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/226316a79d291819675f319e7400de1dfcc8e881ab1b0ddd0a3bbab07aa93602.jpg", "img_caption": ["Figure 18: Optimization on the Dialysis dataset with $\\lambda_{1}=0$ and $\\lambda_{2}=5.0$ Although our methods make less progress toward the minimum loss per iteration (left plot), we are significantly faster than other methods in terms of elapsed time (wall clock) due to cheap evaluation cost per iteration. For ease of figure reading, we only give a partial plot with a few iterations. When the number of iterations is large, our methods achieve better losses than the baseline methods. "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/67a9396c1a71625918cd9073dceffa3364defd88a03b555ef21fa622563e6014.jpg", "img_caption": [], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "Figure 19: Optimization on the Dialysis dataset with $\\lambda_{1}=1.0$ and $\\lambda_{2}=1.0$ . The exact Newton method can be applied to solve the $\\ell_{1}$ -regularized problems, so we only compare with quasi Newton and proximal Newton. Although our methods make less progress toward the minimum loss per iteration (left plot), we are significantly faster than other methods in terms of elapsed time (wall clock) due to cheap evaluation cost per iteration. For ease of figure reading, we only give a partial plot with a few iterations. When the number of iterations is large, our methods achieve better losses thanthebaselinemethods. ", "page_idx": 41}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/e23cea3b08b77133aa7c1e40c5899777541ab3d91c81cbff048a20b12234ff46.jpg", "img_caption": [], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "Figure 20: Optimization on the Dialysis dataset with $\\lambda_{1}=1.0$ and $\\lambda_{2}=5.0$ . The exact Newton method can be applied to solve the $\\ell_{1}$ -regularized problems, so we only compare with quasi Newton and proximal Newton. Although our methods make less progress toward the minimum loss per iteration (left plot), we are significantly faster than other methods in terms of elapsed time (wall clock) due to cheap evaluation cost per iteration. For ease of figure reading, we only give a partial plot with a few iterations. When the number of iterations is large, our methods achieve better losses thanthebaselinemethods. ", "page_idx": 41}, {"type": "text", "text": "D.2Variable Selection for the CPH Model ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "D.2.1  Results on Dialysis ", "page_idx": 42}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/ea87fee3867546d2b5bfe85ad36741771a764903f5d874719d204f4ad1bc9e60.jpg", "img_caption": [], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Figure 21: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CIndex ", "page_idx": 42}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/0df1bc0fee7f0fc271ba90edb9f969e85a741309172544037053490ca32bcfd0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Figure 22: 5-fold Cross-validation on Dialysis dataset. Comparision with non-cox models, metric: CIndex ", "page_idx": 42}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/d5962b3ad0b850fe6b7e97184d5d82929be5b199b21be8ee89f366a6b426e7ba.jpg", "img_caption": ["Figure 23: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: IBS "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/51a85cd594cb01acbf877752c826114945ee4fbaaa778ff5f46c290f843f49e0.jpg", "img_caption": ["Figure 24: 5-fold Cross-validation on Dialysis dataset. Comparision with non-cox models, metric: IBS "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/4e660c0f8d20ea59dd89f152847b90273a7947f49e90d6d61c9eb73c937d0ddb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "Figure 25: 5-fold Cross-validation on Dialysis dataset. Comparision with other cox models, metric: CPHLoSS ", "page_idx": 43}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/3b9c0f1226f3c507d904fab9b6a1208b430ea4dab7b0db1d84402880aaa40a9c.jpg", "img_caption": ["D.2.2 Results on EmployeeAttrition "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Figure 26: 5-fold Cross-validation on EmployeeAttrition dataset. Comparision with other cox models, metric: CIndex ", "page_idx": 44}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/bce2c8de4df3a0e378299b4daccf1f0e16eb5f1ebe15e3e53b3ce12572f985de.jpg", "img_caption": [], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Figure 27: 5-fold Cross-validation on EmployeeAttrition dataset. Comparision with non-cox models, metric: CIndex ", "page_idx": 44}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/b77c6eb88bca11ee933a7d0df8a7d8c27c8713fe3972af2e7dfc022f2a550171.jpg", "img_caption": [], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 28: 5-fold Cross-validation on EmployeeAttrition dataset. Comparision with other cox models, metric: IBs ", "page_idx": 45}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/1a59379190a9aca7bdb9f58ef16f6cf364e8963a1c9a05b65fa9198399d15f7d.jpg", "img_caption": ["Figure 29: 5-fold Cross-validation on EmployeeAttrition dataset. Comparision with non-cox models, metric: IBS "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/0b5df59d4e8c4883f4fb604cb049d503d739723fab39fa75c3da1bde66e5e14e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 30: 5-fold Cross-validation on EmployeeAttrition dataset. Comparision with other cox models, metric: CPH Loss ", "page_idx": 45}, {"type": "text", "text": "D.2.3 Results on Kickstarterl ", "text_level": 1, "page_idx": 46}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/ca733b9b53c50ae330b4af81e7017f8e1e694fe943f699b8c2530d8a6b2d7bd7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "Figure 31: 5-fold Cross-validation on kickstarterl dataset. Comparision with other cox models, metric: CIndex ", "page_idx": 46}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/1e40807a96d8a861e87fd834cfc55caa9c8d259a970067500a0c05d8bc67ba82.jpg", "img_caption": [], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "Figure 32: 5-fold Cross-validation on kickstarterl dataset. Comparision with non-cox models, metric: CIndex ", "page_idx": 46}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/88b5a886d6947129923321ab220db53568e689e860ac2f7dcc0751abbfd007a0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "Figure 33: 5-fold Cross-validation on kickstarterl dataset. Comparision with other cox models, metric: IBs ", "page_idx": 47}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/758e8d1830fe049b113199fa26a276fce9b8d60f000867a67cd44fae7d104763.jpg", "img_caption": ["Figure 34: 5-fold Cross-validation on kickstarterl dataset. Comparision with non-cox models, metric: IBS "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "RHQbxlhzhm/tmp/7bf507b095fcefff1afb14a1c9661d0c82570068f2ec6f7e2dcf3796f6302c73.jpg", "img_caption": [], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "Figure 35: 5-fold Cross-validation on kickstarterl dataset. Comparision with other cox models, metric: CPH Loss ", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 48}, {"type": "text", "text": "Justification: The abstract and intro clearly state the contributions. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: Limitations are discussed above the conclusion section. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 48}, {"type": "text", "text": "Justification: All proofs are provided in the appendix. All assumptions are provided. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Experimental setups are clearly stated in the Appendix. Code for the method and paper's experiments is included in a public GitHub repository. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: All baselines and real-world datasets are publicly accessible. The synthetic data generation process is clearly stated in the Appendix. Code for the method and paper's experiments is included in a public GitHub repository. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: This is described in the experimental section and the Appendix Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 50}, {"type": "text", "text": "Justification: Error bars are present. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: This was reported in the manuscript. See Appendix C. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 51}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper follows the full code of ethics. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper involves survival analysis. We have never heard of survival analysis having negative societal impacts. It has positive impacts, particularly in medicine and reliability analysis for equipment/machinery. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: The paper develops its own assets. Other works (datasets and baseline software packages) are properly cited. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 52}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: Documentation is provided with our code. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 53}, {"type": "text", "text": "Justification: The paper does not use human subjects. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 53}, {"type": "text", "text": "[NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This is not relevant to us. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]