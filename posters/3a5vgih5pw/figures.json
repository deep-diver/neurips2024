[{"figure_path": "3A5VgiH5Pw/figures/figures_1_1.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is co, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "The figure shows four examples of medical image analysis using the Med-MICN model. For each example, the image is displayed along with the concept prediction scores for different concepts (represented as bars), the concept reasoning rules which explain the relationships between concepts and predictions (represented as logical expressions), and a saliency map which highlights the regions of the image that were most important for the model's decision. The figure demonstrates how the model integrates multiple aspects of interpretability to achieve a comprehensive understanding of the decision-making process.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_3_1.jpg", "caption": "Figure 2: (a) module, output rich dimensional interpretable conceptual information for the specified disease through the multimodal model and convert the conceptual information into text vectors through the text embedding module; (b) module, access the image to the image embedder to get the image features, and then match with the conceptual textual information to get the relevant attention region. Then, we get the influence score of the relevant region information through pooling, and finally send it to the filter to sieve out the concept information with weak relevance to get the disease concept of image information.", "description": "This figure shows the two modules of the Med-MICN framework.  Module (a) uses a multimodal model (like GPT-4V) to generate concepts for a given disease, then converts those concepts into text embeddings using a text encoder.  Module (b) takes an image as input, uses an image encoder to extract features, and aligns those features with the concept embeddings generated in module (a). This alignment process helps pinpoint relevant image regions related to each concept and filters out weak associations, resulting in a refined set of image-concept relationships.", "section": "3 Preliminaries"}, {"figure_path": "3A5VgiH5Pw/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of the Med-MICN Framework. The Med-MICN framework consists of four primary modules: (1) Feature Extraction Module: In the initial step, image features are extracted using a backbone network to obtain pixel-level features. (2) Concept Embedding Module: The extracted features are fed into the concept embedding module. This module outputs concept embeddings while passing through a category classification linkage layer to obtain predicted category information. (3) Concept Semantic Alignment: Concurrently, a Vision-Language Model (VLM) is used to annotate the image features, generating concept category annotations aligned with the predicted categories. (4) Neural Symbolic Layer: After obtaining the concept embeddings, they are input into the Neural Symbolic layer to derive conceptual rules. Finally, the concept embeddings obtained from module (2) are concatenated with the original image embeddings and fed into the final category prediction layer to produce the ultimate prediction results.", "description": "This figure presents a detailed overview of the Med-MICN framework's architecture, illustrating its four main modules: feature extraction, concept embedding, concept semantic alignment, and a neural symbolic layer.  Each module's function and interconnection are clearly depicted, showing how image features are processed to generate concept embeddings, align with concept labels, and ultimately contribute to the final classification prediction, incorporating both neural and symbolic reasoning.", "section": "4 Medical Multi-dimensional Interpretable Concept Network"}, {"figure_path": "3A5VgiH5Pw/figures/figures_8_1.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is co, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "This figure demonstrates the multi-dimensional interpretability of the Med-MICN framework.  It shows four example images with their corresponding concept predictions, concept reasoning rules, and saliency maps. Each section visually represents how the model arrives at its classification decision from different perspectives, aligning those perspectives to improve the overall interpretability and trust of the model's predictions. The y-axis in the concept prediction graphs shows a sequence of concepts, starting with 'Peripheral ground-glass opacities' as c0 and increasing sequentially to c7.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_20_1.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is c0, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "The figure showcases the Med-MICN framework's multidimensional interpretability by visualizing concept predictions, concept reasoning rules, and saliency maps for four medical image samples (two COVID and two NonCOVID). Each row represents a sample, showing its image, saliency map, concept predictions, and concept reasoning rules. The concept predictions indicate the model's confidence scores for various concepts, and the concept reasoning rules depict the logical rules used by the model for its prediction.  The saliency maps highlight the image regions that most significantly influenced the model's decisions for each concept. The alignment of these different aspects contributes to a more comprehensive and interpretable understanding of the model's predictions.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_20_2.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is c0, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "This figure showcases the multidimensional interpretability of the Med-MICN framework. It displays four examples of medical image classification, each showing the concept prediction scores, concept reasoning rules, and saliency maps.  The alignment of these different explanatory methods is a key feature of Med-MICN, enhancing trust and understanding of the model's predictions.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_21_1.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is co, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "This figure showcases Med-MICN's multi-dimensional interpretability by visualizing four example cases.  Each case displays three types of explanations: concept prediction scores, concept reasoning rules, and saliency maps. The alignment of these explanations from different angles provides a more comprehensive and trustworthy interpretation than methods using only one type of explanation.  The example shows how Med-MICN combines various interpretative methods to achieve more robust and reliable results.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_21_2.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is co, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "This figure showcases the multi-dimensional interpretability of the Med-MICN framework. It displays four examples of medical images with their corresponding concept predictions, concept reasoning rules, and saliency maps. Each example demonstrates how Med-MICN aligns different aspects of interpretability, providing a comprehensive understanding of the model's decision-making process.  The concepts are shown on the y-axis, progressing from c0 to c7, representing different features related to the diagnosis.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_22_1.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is co, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "This figure showcases the multidimensional interpretability of the Med-MICN model.  It displays four examples of medical image analysis, each showing a different image class (COVID or NonCOVID) with its corresponding concept predictions, concept reasoning rules, and saliency maps. The concept predictions are visualized as bar charts, illustrating the contribution of individual concepts to the final prediction. The concept reasoning rules provide logical relationships between the concepts, and the saliency maps highlight the relevant regions in the image that contribute most to the classification decision. The alignment of these three dimensions enhances the model's interpretability, helping clinicians understand the basis for the model's decisions.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_22_2.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is co, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "This figure showcases the multi-dimensional interpretability of the Med-MICN model.  It displays four examples of medical image analysis. For each image, it shows the concept prediction scores, saliency map highlighting relevant image regions, and concept reasoning rules summarizing how the model arrived at its prediction. The alignment of these different aspects of interpretability is a key feature of Med-MICN.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_23_1.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is co, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "This figure shows the multidimensional interpretability of Med-MICN, a novel framework for medical image classification. It visualizes how Med-MICN aligns three different aspects of interpretability: concept prediction (probability scores for different concepts), concept reasoning rules (logical rules explaining how concepts relate to the classification outcome), and saliency maps (visualizing the regions of the image that are most influential for the prediction). The alignment of these three aspects helps to ensure that the model's interpretations are consistent and reliable. Each row in the figure represents a separate medical image, and the concepts are arranged along the y-axis. ", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_23_2.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is c0, and along the y-axis, it sequentially becomes c1,..., c7.", "description": "This figure showcases the multidimensional interpretability of Med-MICN.  It displays four example cases, each showing the model's prediction, a saliency map highlighting relevant image regions, and concept reasoning rules.  The alignment between these different explanatory approaches improves the model's interpretability and allows for a more comprehensive understanding of its predictions.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_24_1.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is co, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "This figure shows the multi-dimensional interpretability of the Med-MICN model.  It displays how the model aligns different aspects of interpretability, including concept prediction scores (how strongly the model associates specific concepts with the image), concept reasoning rules (logical rules connecting concepts to predictions), and saliency maps (visual highlights indicating important image regions). The alignment of these different interpretability aspects is a key contribution of the proposed model, making it easier for users to understand the model's reasoning. The example focuses on the concept of \"Peripheral ground-glass opacities,\" which is shown as a sequence of concepts (C0 to C7) to illustrate the model's multidimensional reasoning process.", "section": "1 Introduction"}, {"figure_path": "3A5VgiH5Pw/figures/figures_24_2.jpg", "caption": "Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The 'Peripheral ground-glass opacities' is co, and along the y-axis, it sequentially becomes c1,..., C7.", "description": "This figure illustrates the multidimensional interpretability of the Med-MICN framework.  It shows four examples of medical image analysis, each with a different image class (COVID or NonCOVID). For each image, Med-MICN provides three types of explanations: concept predictions (a bar chart showing the probability of each concept being present), concept reasoning rules (a graphical representation showing the logical relationship between concepts), and saliency maps (a heatmap highlighting the image regions contributing most to the prediction). The alignment of these different explanations is a key feature of Med-MICN, enhancing its interpretability and trustworthiness.  The concepts along the y-axis, C1 to C7, sequentially represent increasingly specific aspects of 'Peripheral ground-glass opacities'.", "section": "1 Introduction"}]