{"references": [{"fullname_first_author": "Holger Caesar", "paper_title": "nuScenes: A multimodal dataset for autonomous driving", "publication_date": "2020-06-14", "reason": "This paper introduces a large-scale, multimodal dataset crucial for training and evaluating SpatialRGPT's 3D spatial reasoning capabilities."}, {"fullname_first_author": "Andreas Geiger", "paper_title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "publication_date": "2012-06-14", "reason": "The KITTI dataset, introduced in this paper, is a widely used benchmark for autonomous driving tasks and a significant source of data for SpatialRGPT."}, {"fullname_first_author": "Shuran Song", "paper_title": "SUN RGB-D: A RGB-D scene understanding benchmark suite", "publication_date": "2015-06-07", "reason": "This paper presents the SUN RGB-D dataset, which provides 3D annotations and is important for training and evaluating SpatialRGPT's indoor scene understanding capabilities."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-11", "reason": "CLIP, introduced in this paper, is a foundation model for vision-language tasks and is used as the visual backbone in SpatialRGPT."}, {"fullname_first_author": "Boyuan Chen", "paper_title": "SpatialVLM: Endowing vision-language models with spatial reasoning capabilities", "publication_date": "2024-06-16", "reason": "SpatialVLM is a closely related work that improves VLMs' spatial understanding, and it shares similarities with the methodology of SpatialRGPT."}]}