[{"figure_path": "JKEIYQUSUc/tables/tables_6_1.jpg", "caption": "Table 1: SpatialRGPT-Bench results.  are Blind LLMs with Language Referral.  are VLMs with Language Referral.  are Region-aware VLMs. Numbers in the top table represent success rates (\u2191), while the bottom table includes success rates (\u2191) and absolute relative error (\u2193).", "description": "This table presents the results of the SpatialRGPT-Bench benchmark, comparing the performance of SpatialRGPT against various baseline models across different spatial reasoning tasks.  The upper part of the table shows the success rates for qualitative tasks (e.g., relative spatial relationships), while the lower part displays both success rates and the absolute relative errors for quantitative tasks (e.g., distance and dimensions).  The baseline models include blind LLMs (relying on text and language only), VLMs with language referral (leveraging visual and language information), and region-aware VLMs (specifically focused on region-level understanding).", "section": "4.1 3D Spatial Reasoning Benchmarks"}, {"figure_path": "JKEIYQUSUc/tables/tables_6_2.jpg", "caption": "Table 2: Comparison of SpatialRGPT and base model performance on general VLM benchmarks.", "description": "This table compares the performance of SpatialRGPT and a baseline model (VILA-1.5-3B) on several general VLM benchmark datasets.  These benchmarks assess various visual and language understanding capabilities, not specifically focused on spatial reasoning. The results show that SpatialRGPT generally achieves comparable or slightly improved performance, indicating that incorporating spatial reasoning capabilities does not negatively affect performance on broader VQA tasks.", "section": "4 Experiments"}, {"figure_path": "JKEIYQUSUc/tables/tables_8_1.jpg", "caption": "Table 3: Region-level classification results. We follow the evaluation in RegionCLIP [65] and RegionGPT [19], report the results of object classification with ground-truth box on COCO-2017 validation set.", "description": "This table presents the results of a region-level object classification task.  It compares the performance of several vision-language models, including the proposed SpatialRGPT models, on the COCO-2017 validation set.  The evaluation metric is accuracy, and the models are evaluated based on their ability to correctly classify objects within ground-truth bounding boxes.", "section": "4 Experiments"}, {"figure_path": "JKEIYQUSUc/tables/tables_8_2.jpg", "caption": "Table 1: SpatialRGPT-Bench results.  are Blind LLMs with Language Referral. are VLMs with Language Referral. are Region-aware VLMs. Numbers in the top table represent success rates (\u2191), while the bottom table includes success rates (\u2191) and absolute relative error (\u2193).", "description": "This table presents a comparison of different vision-language models (VLMs) on the SpatialRGPT-Bench benchmark. The benchmark focuses on evaluating the ability of VLMs to perform spatial reasoning tasks, including tasks related to relative directions, distances, and object sizes.  The models are categorized into three groups: blind LLMs, VLMs with language referral, and region-aware VLMs. For each model, the table shows both qualitative (success rates) and quantitative (absolute relative error) results for various spatial reasoning tasks.", "section": "4.1 3D Spatial Reasoning Benchmarks"}, {"figure_path": "JKEIYQUSUc/tables/tables_16_1.jpg", "caption": "Table 5: Augmented SpatialRGPT-Bench results. Numbers represent success rates (\u2191) and absolute relative error (\u2193).", "description": "This table presents the results of an ablation study conducted on the augmented SpatialRGPT-Bench.  The study evaluated the model's performance using different questions and answers from the training data to check for generalization.  The table compares the success rates (indicated by an upward-pointing arrow) and absolute relative errors (indicated by a downward-pointing arrow) of the GPT-4V-Turbo model and the SpatialRGPT-7B model. The metrics are shown for several spatial reasoning categories (above/below, left/right, big/small, tall/short, wide/thin, behind/front) and for distance measurements (direct, horizontal, vertical).  Higher success rates and lower relative errors indicate better performance.", "section": "A Ablation Study on Augmented SpatialRGPT-Bench"}, {"figure_path": "JKEIYQUSUc/tables/tables_16_2.jpg", "caption": "Table 1: SpatialRGPT-Bench results.  are Blind LLMs with Language Referral.  are VLMs with Language Referral.  are Region-aware VLMs. Numbers in the top table represent success rates (\u2191), while the bottom table includes success rates (\u2191) and absolute relative error (\u2193).", "description": "This table presents a comparison of the performance of different Vision-Language Models (VLMs) on the SpatialRGPT-Bench benchmark.  The benchmark focuses on evaluating the ability of VLMs to perform spatial reasoning tasks, such as determining relative directions, distances, and object sizes. The table is divided into two sections. The top section shows the success rates of the models for qualitative spatial reasoning tasks, categorized by the type of spatial relationship (e.g., above/below, left/right, big/small). The bottom section displays both the success rate and absolute relative error for quantitative spatial reasoning tasks, including tasks involving direct, horizontal, and vertical distance measurements, as well as measurements of object width, height, and direction. The table highlights the superior performance of SpatialRGPT models compared to other baseline models, demonstrating their enhanced ability to handle spatial reasoning tasks.", "section": "4.1 3D Spatial Reasoning Benchmarks"}, {"figure_path": "JKEIYQUSUc/tables/tables_16_3.jpg", "caption": "Table 1: SpatialRGPT-Bench results.  are Blind LLMs with Language Referral.  are VLMs with Language Referral.  are Region-aware VLMs. Numbers in the top table represent success rates (\u2191), while the bottom table includes success rates (\u2191) and absolute relative error (\u2193).", "description": "This table presents the results of evaluating different models on the SpatialRGPT-Bench benchmark.  The top part shows the success rates for qualitative spatial reasoning tasks (e.g., relative position, size comparison).  The bottom part shows both success rates and the average error for quantitative tasks (e.g., distances, angles).  Three model categories are compared: blind LLMs, VLMs with language referral, and region-aware VLMs. SpatialRGPT consistently outperforms the others.", "section": "4.1 3D Spatial Reasoning Benchmarks"}, {"figure_path": "JKEIYQUSUc/tables/tables_17_1.jpg", "caption": "Table 8: Ablation study on effect of different input modalities to SpatialRGPT. Numbers in the top table represent success rates (\u2191), while the bottom table includes success rates (\u2191) and absolute relative error (\u2193).", "description": "This table presents the ablation study results on the effect of using different input modalities (mask vs. box) for SpatialRGPT on the SpatialRGPT-Bench benchmark.  It shows the success rates and absolute relative errors for various spatial reasoning tasks. The top section displays qualitative results (success rates) for relative spatial relationships (above/below, left/right, big/small, etc.), while the bottom section presents quantitative results (success rates and errors) for metric measurements (direct distance, horizontal distance, vertical distance, width, height, direction). This allows for the evaluation of SpatialRGPT's robustness to different input types and whether the model's performance is affected by this variation.", "section": "4.1 3D Spatial Reasoning Benchmarks"}, {"figure_path": "JKEIYQUSUc/tables/tables_21_1.jpg", "caption": "Table 1: SpatialRGPT-Bench results.  \u2191 are Blind LLMs with Language Referral. \u2191 are VLMs with Language Referral. \u2191 are Region-aware VLMs. Numbers in the top table represent success rates (\u2191), while the bottom table includes success rates (\u2191) and absolute relative error (\u2193).", "description": "This table presents a comparison of different Vision-Language Models (VLMs) on the SpatialRGPT-Bench benchmark.  The benchmark evaluates the models' ability to perform spatial reasoning tasks, specifically focusing on relative spatial relationships (e.g., above, below, left, right) and metric measurements (e.g., distance, width, height). Three categories of models are compared: blind LLMs with language referral, VLMs with language referral, and region-aware VLMs.  The table shows success rates (higher is better) and absolute relative error (lower is better) for each category of model on both qualitative and quantitative spatial reasoning tasks. The results demonstrate that SpatialRGPT significantly outperforms the other models.", "section": "4.1 3D Spatial Reasoning Benchmarks"}, {"figure_path": "JKEIYQUSUc/tables/tables_21_2.jpg", "caption": "Table 1: SpatialRGPT-Bench results. \u2191 are Blind LLMs with Language Referral. \u2191 are VLMs with Language Referral. \u2191 are Region-aware VLMs. Numbers in the top table represent success rates (\u2191), while the bottom table includes success rates (\u2191) and absolute relative error (\u2193).", "description": "This table presents the results of the SpatialRGPT-Bench, a benchmark designed to evaluate the spatial reasoning capabilities of Vision-Language Models (VLMs).  It compares the performance of various models, including blind LLMs (that rely only on text), VLMs with language reference (that can access both text and visual data), and region-aware VLMs (that can reason about specific regions in an image).  The table is divided into two parts: the top part shows success rates for qualitative tasks (e.g., spatial relationships between objects), while the bottom part presents success rates and relative errors for quantitative tasks (e.g., measuring distances and dimensions).  Overall, this table illustrates SpatialRGPT's effectiveness in improving upon the performance of existing VLMs in spatial reasoning tasks.", "section": "4.1 3D Spatial Reasoning Benchmarks"}]