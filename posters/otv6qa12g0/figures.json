[{"figure_path": "oTv6Qa12G0/figures/figures_7_1.jpg", "caption": "Figure 1: Scaling laws for the number of examples across different concept set sizes. As predicted by Theorem 1, smaller concept sizes n < 100 benefit the small sample regime, whereas larger concept sizes n > 200 are superior in the larger sample regime. We plot the mean and 1\u03c3 bars over ten seeds.", "description": "This figure shows how the accuracy of a concept-based model changes with the number of training examples and the size of the concept set.  It validates Theorem 1 from the paper, which predicts that smaller concept sets are better in low-data regimes, while larger sets are better with more data. The plot shows the mean accuracy and standard deviation across 10 different random seeds.", "section": "6 Empirical Validation"}, {"figure_path": "oTv6Qa12G0/figures/figures_8_1.jpg", "caption": "Figure 2: Concept representations reduce the effect of distribution shift over using the CLIP embedding directly, as seen by a more pronounced increase in accuracy on more imbalanced datasets. We plot the mean and 1 \u03c3 bars over 10 seeds.", "description": "This figure shows the impact of concept-based representations on model robustness to distribution shifts.  The experiment uses a dataset with varying degrees of class imbalance, simulating a shift in data distribution.  Three different output model types (linear, tree, and k-NN) were tested. The y-axis shows the increase in accuracy achieved by the concept-based model compared to a baseline model using the CLIP embeddings directly. The x-axis shows the percentage of minority classes, with a smaller percentage indicating a larger distribution shift.  The shaded areas represent the standard deviation across ten different random seeds.  The results show that concept-based representations significantly improve model accuracy in the presence of distribution shifts, particularly for larger shifts (smaller percentage of minority classes).  This improvement holds across the different output model types.", "section": "Empirical Validation"}, {"figure_path": "oTv6Qa12G0/figures/figures_9_1.jpg", "caption": "Figure 3: Concept representations consistently improve the out-of-distribution accuracy compared to directly using the CLIP embedding across different classes and distribution shifts. For three of the horse datasets, the CBM correctly identifies the presence of a horse.", "description": "This figure displays the results of an out-of-distribution generalization experiment.  The experiment uses three different tasks (dog vs. cat, bus vs. truck, and elephant vs. horse) with four different training datasets for each task, where the testing data represents various levels of distribution shifts. The bars show the accuracy of the concept-based model (CBM) and the vanilla model (using CLIP embedding directly).  The results indicate that the CBM substantially outperforms the vanilla model in almost all cases, highlighting the robustness of the concept-based approach to distribution shifts.", "section": "Empirical Validation"}, {"figure_path": "oTv6Qa12G0/figures/figures_24_1.jpg", "caption": "Figure 1: Scaling laws for the number of examples across different concept set sizes. As predicted by Theorem 1, smaller concept sizes n < 100 benefit the small sample regime, whereas larger concept sizes n > 200 are superior in the larger sample regime. We plot the mean and 1 \u03c3 bars over ten seeds.", "description": "This figure shows the relationship between the number of training examples and the performance of concept bottleneck models (CBMs) with varying concept set sizes on the CIFAR-10 dataset.  The results are consistent with Theorem 1 in the paper.  For smaller training sets, smaller concept sets perform better because they avoid overfitting. As the amount of training data increases, larger concept sets are preferred because they capture more of the underlying data distribution, which leads to improved generalization.", "section": "Empirical Validation"}, {"figure_path": "oTv6Qa12G0/figures/figures_24_2.jpg", "caption": "Figure 4: Comparison of a linear probe on the CLIP [32] representation against our CBM across different training sizes for three datasets: CUB-200-2011 [37] in Figure 4a, Food-101 [7] in Figure 4c, and DTD [9] in Figure 4b. This figure illustrates the performance of both models across varying data availability, highlighting the increased data efficiency of CBMs in the low sample regime and the effect of misspecification in the large sample regime (see Theorem 1).", "description": "This figure compares the performance of a linear probe on CLIP embeddings against the proposed CBM across different training sizes for three datasets (CUB-200-2011, Food-101, and DTD).  It demonstrates the improved sample efficiency of CBMs in low-data regimes and the impact of misspecification in larger datasets, aligning with Theorem 1's predictions.", "section": "D.1 Data-Efficiency Experiments"}, {"figure_path": "oTv6Qa12G0/figures/figures_24_3.jpg", "caption": "Figure 1: Scaling laws for the number of examples across different concept set sizes. As predicted by Theorem 1, smaller concept sizes n < 100 benefit the small sample regime, whereas larger concept sizes n > 200 are superior in the larger sample regime. We plot the mean and 1 \u03c3 bars over ten seeds.", "description": "This figure shows the accuracy of the concept bottleneck model (CBM) and the baseline model on the CIFAR-10 dataset as a function of the number of training examples and the size of the concept set.  The results demonstrate that smaller concept sets are more effective in low-data regimes, while larger concept sets perform better when more data is available.  This aligns with the theoretical predictions of Theorem 1 in the paper, which suggests a trade-off between sample efficiency and misspecification error in the CBM.", "section": "Empirical Validation"}, {"figure_path": "oTv6Qa12G0/figures/figures_26_1.jpg", "caption": "Figure 5: Performance comparison between our concept set curation method and the Label-Free Concept Bottleneck Model (LF-CBM) [30] on CIFAR-10 [25]. The results demonstrate a consistent performance gap, with our method outperforming LF-CBM by more than 7.3% across all training set sizes and reaching up to 17.7%. This comparison underscores the impact of effective concept set selection in concept bottleneck models, validating the importance of our theoretical insights.", "description": "This figure compares the performance of the proposed concept set curation method with the Label-Free Concept Bottleneck Model on CIFAR-10.  The results show that the proposed method consistently outperforms the Label-Free model across various training set sizes, highlighting the importance of careful concept selection for improved model performance.  The performance gap is quite substantial, indicating a significant benefit from the proposed concept set generation process. This empirical result supports the paper's theoretical findings regarding concept set properties and their influence on model accuracy.", "section": "D.3 Comparison with Label-Free Concept Bottleneck Models"}]