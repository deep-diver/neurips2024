[{"type": "text", "text": "Cluster-Learngene: Inheriting Adaptive Clusters for Vision Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qiufeng Wang1,2, $\\mathbf{X}\\mathbf{u}\\:\\mathbf{Y}\\mathbf{a}\\mathbf{n}\\mathbf{g}^{1,2^{\\dagger}}$ , Fu Feng1,2, Jing Wang1,2, and Xin Geng1,2\u2020 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Engineering, Southeast University, Nanjing 210096, China   \n2Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Southeast University, Ministry of Education, China {qfwang, xuyang_palm, fufeng, wangjing91, xgeng}@seu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, the merging of vast datasets with powerful computational resources has led to the emergence of large pre-trained models in the field of deep learning. However, the common practices often overgeneralize the applicability of these models, overlooking the task-specific resource constraints. To mitigate this issue, we propose Cluster-Learngene, which effectively clusters critical internal modules from a large ancestry model and then inherits them to initialize descendant models of elastic scales. Specifically, based on the density characteristics of attention heads, our method adaptively clusters attention heads of each layer and position-wise feed-forward networks (FFNs) in the ancestry model as the learngene. Moreover, we introduce priority weight-sharing and learnable parameter transformations that expand the learngene to initialize descendant models of elastic scales. Through extensive experimentation, we demonstrate that Cluster-Learngene not only is more efficient compared to other initialization methods but also customizes models of elastic scales according to downstream task resources. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The evolution of deep learning has been profoundly influenced by the confluence of expansive data sources and robust computational capabilities. This collaboration has given rise to large pre-trained foundation models [11, 10, 41, 5], particularly those built upon the Transformer [47, 11], such as the Vision Transformers (ViTs) [11]. The pre-trained foundation models, being widely deployed in various devices like smartphones or edge devices, serve as the initialization point [16, 2, 19, 64, 49, 50] for diverse downstream applications. However, this dominant methodology implicitly assumes that a one-size-fits-all approach, i.e., the whole foundation model is universally apt for every application, neglecting the specific resource constraints (e.g., memory, FLOPs, or latency) inherent to certain downstream tasks. Furthermore, not all tasks demand the full power of these extensive foundation models. This naturally raises a pivotal question: Can we extract and harness the condensed part of these foundation models to achieve a harmonious balance between accuracy and resource efficiency? ", "page_idx": 0}, {"type": "text", "text": "To achieve the goal of efficiently initializing models, [49, 50] introduce the innovative Learngene framework inspired by the observation of genes (cf. Fig. 1 (a)). As showcased in Fig. 1 (b), Learngene framework is designed in two pivotal stages. In the first stage, the significant knowledge is condensed from a large ancestry model into a more compact part termed as learngene. In the next stage, this learngene is inherited to initialize the descendant models of elastic scales. \\* Previous works [49, 50] predominantly focus on extracting a few integral layers as the learngene and manually stacking them with the randomly initialized layers. ", "page_idx": 0}, {"type": "image", "img_path": "92vVuJVLVW/tmp/f55bf0ac38699f482da8179eae441e3f286b27a0976de75edafb83c7491e9d51.jpg", "img_caption": ["Figure 1: (a) The ancestry of biological organisms condenses evolutionary information into information-dense genes to initialize their diverse descendants [62, 17]. (b) The Learngene framework condenses the significant knowledge from an ancestry model into a more compact part termed learngene and then inherited to initialize the descendant models of elastic scales. (c) The density of attention heads across the different layers of the ancestry model, which employs the DeiT-B [46]. (d) An illustration of our idea. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, such approaches struggle with inherent limitations: (i) The strategy of extracting certain integral layers overlooks the potential existence of learngene within these layers, leading to the preservation of many redundant weights. (ii) The approach of manually stacking the learngene with randomly initialized layers lacks the adaptability to scale the model, preventing the initialization of downstream models with custom dimensions. ", "page_idx": 1}, {"type": "text", "text": "As mentioned earlier, the Learngene framework aims to preserve the most generalizable part of the ancestry model while eliminating redundant weights that weaken representational capacity. Recent studies [42, 56] have visualized the mean attention distance of ViTs, offering deeper insights into weight redundancy among attention heads across different layers. As illustrated in Fig. 1 (c), the lower layers focus on both local and global perspectives, leading to a more sparse density of attention heads. Conversely, the higher layers prioritize a global context, resulting in a compact density. A notable observation is the repetitive functionality across many attention heads especially in the higher layers, which inevitably leads to weight redundancy. ", "page_idx": 1}, {"type": "text", "text": "Inspired by the above observation, we propose the Cluster-Learngene, an innovative approach that adaptively extracts internal modules in ViTs as the learngene, e.g., attention heads and position-wise feed-forward networks (FFNs). Firstly, to extract the cluster centroids of the attention heads (i.e., head centroids) across each layer of the ancestry model, we cluster the attention heads within each layer of the ancestry model based on their density characteristics. As depicted in Fig. 1 (c-d), the attention heads in the first layer exhibit a sparse density, resulting in five clusters, whereas the attention heads in the last layer cluster more compactly, forming a single group. Furthermore, our method includes clustering FFNs by assessing the distance density of head centroids in adjacent layers of the ancestry model. Specifically, when the distance density of head centroids in adjacent layers is similar, we inherit the FFN from the shallower of these adjacent layers (i.e., FFN centroid) as the learngene. As illustrated in Fig. 1 (c), the similar densities of attention heads in the 7-th and 8-th layers lead to proximate head centroids, enabling these layers to only require the inheritance from the 7th layer as the FFN centroid. Overall, Cluster-Learngene extracts critical parameters containing significant knowledge, as the extracted part represents attention heads/FFNs with similar semantics. ", "page_idx": 1}, {"type": "text", "text": "In the inheriting stage, to achieve the initialization of descendant models with varying number of attention heads, we adopt the priority weight-sharing. We start by ranking the head centroids based on the size of their respective clusters, arranging them in descending order of priority. Subsequently, we perform weight-sharing by distributing these head centroids to initialize the attention heads of the descendant models. If the number of attention heads in a specific layer aligns perfectly with the number of centroids, they are evenly shared. However, if they fail to align perfectly, any remaining centroids are shared according to the remainder. Moreover, we apply learnable parameters to transform FFN centroids into multiple FFNs, thus enabling the initialization of descendant models with elastic scales. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: (i) We propose the adaptive clustering to extract the head and FFN centroids as the learngene, ensuring the preservation of significant knowledge within the ancestry model. (ii) To achieve the initialization of descendant models, we introduce priority weight-sharing that favors head centroids within larger clusters and employ learnable parameters to transform the FFN centroids into multiple FFNs. (iii) Comprehensive experimental evaluations across datasets of different scales reveal that Cluster-Learngene not only outperforms traditional initialization strategies but also stands toe-to-toe with more resource-demanding fine-tuning methodologies. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Model Initialization: Over the years, various initialization techniques have been proposed including the popular random initialization, Xavier initialization [13] and the Kaiming initialization [19]. Recently, the use of pre-trained foundation models has gained prominence as an initialization strategy before fine-tuning for specific tasks [11, 10, 41, 57, 37, 5, 61, 22, 15, 53, 33, 32, 7, 18, 39, 54, 29, 25, 26, 51, 28, 44, 55, 59, 40, 58, 36, 31, 21, 60]. However, such an approach necessitates pre-training separate models for each downstream task, which can lead to substantial computational resource consumption. In contrast, Cluster-Learngene presents a unique model initialization method that alleviates the need for multiple pre-training steps. ", "page_idx": 2}, {"type": "text", "text": "Density-based Clustering: Clustering aims to group similar data points together while separating dissimilar ones. A wide array of approaches has been explored, including partitioning-based clustering [14, 1, 27], hierarchical clustering [35, 8, 65, 52], and density-based clustering [23, 43, 6, 3], and so on. In particular, density-based clustering operates by taking into account the density relationships between data points to form clusters. Inspired by this, our method adopts a similar principle by assessing the density of attention heads to retain essential head centroids that represent significant knowledge. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Learngene framework is primarily divided into two phases in Fig. 1 (b): the significant knowledge is condensed from an ancestry model into a more compact part termed as learngene and then inherited to initialize the descendant models of assorted scales. Specifically, in phase 1, our Cluster-Learngene selects mean attention distance as the density metric and uses it to cluster the attention heads of each layer and FFNs in the ancestry model as the learngene, because they can effectively represent attention heads/FFNs with similar semantics. The pseudocode for this phase is presented in Algorithm 1. In phase 2, to initialize the descendant models, we employ priority weight-sharing of head centroids for varying number of attention heads as illustrated in Fig.2 and leverage learnable parameters to expand the FFN centroids into multiple FFNs. Next, we briefly introduce some preliminaries related to ViTs. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the ViT architecture, an input image is first divided into $N$ non-overlapping patches, and each patch is linearly embedded into a flat vector of size $D$ . The ViT encoder consists of alternating layers of multi-head self-attention (MSA) and FFN blocks. Let $H$ denote the total number of heads in each layer. For the $h^{t h}$ head, the query $\\mathbf{Q}_{h}\\in\\mathbb{R}^{N\\times d_{k}}$ , key ${\\bf K}_{h}\\in\\mathbb{R}^{N\\times d_{k}}$ , and value ${\\bf V}_{h}\\in\\mathbb{R}^{N\\times d_{v}}$ are linearly generated through learned weight matrices $\\mathbf{W}_{h}^{Q}\\,\\in\\,\\mathbb{R}^{D\\times d_{k}}$ , $\\mathbf{W}_{h}^{K}\\,\\in\\,\\mathbb{R}^{D\\times d_{k}}$ , and $\\mathbf{W}_{h}^{V}\\,\\in\\,\\mathbb{R}^{D\\times d_{v}}$ , where $d_{k}$ and $d_{v}$ are the dimensions of the key and value vectors, respectively. The SA mechanism of the $i$ -th head can be represented as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{A}^{h}=\\operatorname{Attention}(\\mathbf{Q}_{h},\\mathbf{K}_{h},\\mathbf{V}_{h})=\\operatorname{softmax}\\left({\\frac{\\mathbf{Q}_{h}\\mathbf{K}_{h}^{\\top}}{{\\sqrt{d_{k}}}}}\\right)\\mathbf{V}_{h}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "MSA allows the model to jointly attend to information at different positions from different representational subspaces at different positions: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MultiHead}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\mathrm{Concat}(\\mathbf{A}^{1},\\dots,\\mathbf{A}^{H})\\mathbf{W}^{O},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{W}^{O}\\in\\mathbb{R}^{H d_{v}\\times D}$ is a learned weight matrix. Besides, the FFN can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{FFN}(\\mathbf{x})=\\mathrm{ReLU}(\\mathbf{x}\\mathbf{W}_{1}+\\mathbf{b}_{1})\\mathbf{W}_{2}+\\mathbf{b}_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "1 Input: Number of layers as $L$ , set of attention heads in the $l^{t h}$ layer as $S_{l}$ , radius as $E p s$ , density threshold as MinHds, and distance function as Dist.   \n2 Output: The centroids of attention head in all clusters. ", "page_idx": 3}, {"type": "text", "text": "3 Initialize all attention heads as unvisited and an empty list for clusters ", "page_idx": 3}, {"type": "image", "img_path": "92vVuJVLVW/tmp/f94cddb13e409c4d6adf3eb04f1636aacf3ba78bec28347529c5dacf87e93ef4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{x}\\in\\mathbb{R}^{N\\times D}$ is the input, $\\mathbf{W}_{1}\\,\\in\\,\\mathbb{R}^{D\\times d_{f f}}$ and $\\mathbf{W}_{2}\\in\\mathbb{R}^{d_{f f}\\times D}$ are the weight matrices, and $\\mathbf{b}_{1}\\in\\mathbb{R}^{d_{f f}}$ and $\\mathbf{b}_{2}\\in\\mathbb{R}^{D}$ are the bias vectors. $d_{f f}$ is the dimension of the intermediate layer. ", "page_idx": 3}, {"type": "text", "text": "3.2 Adaptively Learngene Clustering ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.2.1 Density metric on attention heads ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a pre-trained ancestry model with $L$ layers and $H_{a}$ attention heads per layer, let the attention weights for the $h^{t h}$ head in the $l^{t h}$ layer be denoted by the matrix $\\mathbf{A}^{(l,h)}\\in\\mathbb{R}^{N\\times N}$ . The element $A_{i,j}^{(l,h)}$ represents the attention weight from position $i$ to position $j$ . The distance matrix $\\mathrm{T}\\in\\mathbb{R}^{N\\times N}$ is defined with the Euclidean distance between any two positions $i$ and $j$ in the sequence, given by $T_{i,j}=\\sqrt{\\left(x_{i}-x_{j}\\right)^{2}+\\left(y_{i}-y_{j}\\right)^{2}}$ . The mean attention distance for the $h^{t h}$ head in the $l^{t h}$ layer, encapsulating the weighted distance for each position $i$ across the sequence, is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{d}^{(l,h)}=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{N}A_{i,j}^{(l,h)}\\times T_{i,j}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To deduce this metric for every head across all layers, iterate the above computation for every $l\\in\\{1,\\ldots,L\\}$ and $h\\in\\{1,\\ldots,H_{a}\\}$ . As depicted in Fig. 1 and Appendix A.1, while the lower layers simultaneously attend to both local and global features, leading to a more dispersed distribution of attention heads, the higher layers predominantly focus on global aspects, causing a tighter concentration of attention heads. As a result, there is a significant overlap in the semantic representations among many attention heads, especially in the higher layers, leading to weight redundancy. ", "page_idx": 3}, {"type": "text", "text": "3.2.2 Cluster for MSA ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivated by the empirical observations, we extract cluster centroids [43, 6, 3] of attention heads in ViTs as the learngene inherited into the descendant models, thus aggregating similar semantics into the head centroids. To realize this, we select $\\tilde{d}$ as a density metric for adaptively clustering the attention heads of the ancestry model at each layer, without setting the number of clusters in advance. This realization prompts the formulation of the definitions and lemmas, which scaffold our adaptive clustering approach. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Eps-neighborhood of an attention head). The Eps-neighborhood of an attention head $a$ , denoted as $N_{E p s}(a)$ , is defined as: $N_{E p s}(a)\\;=\\;\\{b\\;\\in\\overrightharpoonup\\;|\\;\\bar{D}i s t(a,b)\\;\\le\\;E p s\\}$ , where $D i s t(a,b)=\\left|\\tilde{d}^{(a)}-\\tilde{d}^{(\\bar{b})}\\right|$ denotes the difference in $\\tilde{d}$ values between attention heads a and $b$ . ", "page_idx": 4}, {"type": "text", "text": "Our approach could require for each head in a cluster that there are at least a Minimum number of Heads (MinHds) in an Eps-neighborhood of that head. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (density-reachable). Transitioning from the neighborhood concept, an attention head a is considered density-reachable from another head $b$ with respect to Eps and MinHds if there is a sequence of heads $a_{1},\\ldots,a_{n}$ such that $a_{1}=b,a_{n}=a,$ , and each head in this sequence lies within the Eps-neighborhood of its preceding head. ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (density-connected). Broadening our purview, attention heads a and b are labeled density-connected with respect to Eps and MinHds if there exists an intermediary head o from which both a and b are density-reachable. ", "page_idx": 4}, {"type": "text", "text": "Considering all attention heads in layer $l$ as $S_{l}$ , a cluster $C$ based on $E p s$ and MinHds is identified as a non-empty subset of $S_{l}$ that satisfies the conditions: (i) Maximality: For any heads $a$ and $b$ in the sequence, if $a$ resides within $C$ and $b$ is density-reachable from $a$ dictated by $E p s$ and $M i n H d s$ , then $b$ seamlessly becomes part of $C$ . (ii) Connectivity: Within $C$ , each pairing $a,b$ maintains a density-connection, anchored by Eps and MinHds. ", "page_idx": 4}, {"type": "text", "text": "Therefore, upon satisfying these two conditions, we cluster all attention heads of each layer into different lists of clusters. The pseudo-code for this process is summarized in Algorithm 1. For each cluster $C$ in the list of clusters, we select the attention head $C_{\\mathrm{lg}}$ closest to the cluster centroid as the learngene. This is because it effectively represents the functionality of all attention heads in the cluster. Formally, the formula can be expressed as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nC_{\\mathrm{lg}}=\\arg\\operatorname*{min}_{c\\in C}\\left|\\tilde{d}^{(c)}-\\bar{d}^{(C)}\\right|\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\bar{d}^{(C)}$ is computed as the average $\\tilde{d}$ across all attention heads within this cluster $C$ . The lemma presented below is pivotal in substantiating the correctness of our clustering algorithm. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 Presuming an attention head a belongs to $S_{l}$ and satisfies the condition $|N_{E p s}(a)|\\geq$ MinHds. Then, the set $O=\\{o\\mid o\\in S_{l}$ and o is density-reachable from a with respect to Eps and MinHds} collectively shapes a cluster. ", "page_idx": 4}, {"type": "text", "text": "3.2.3 Cluster for FFN ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Based on the distances between attention heads discussed earlier, we can further cluster FFNs as the learngene. For adjacent layers sharing the same number $N_{C}$ of head centroids, a key criterion is established: if the average distance across all head centroids between these layers is less than the threshold $\\varepsilon$ , it suggests redundant similarity in their representational capabilities. This criterion is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{N_{C}}\\sum_{k=1}^{N_{C}}{D i s t\\left(C_{l,k},C_{l+1,k}\\right)}<\\varepsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Under this condition, we enhance model efficiency by preserving only the FFN from the shallower of these adjacent layers as the FFN centroid, thereby maintaining essential functionality while eliminating redundancy. Moreover, we also select those FFNs that do not fti this established criterion as the learngene because they exhibit representational capacities independent of adjacent layers. ", "page_idx": 4}, {"type": "text", "text": "3.3 Learngene Inheriting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.3.1 Expanding head centroids with priority weight-sharing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since the head centroids have been extracted as the learngene, we further expand them for initializing the descendant models with varying number of attention heads. For an ancestry model with $L$ layers, the $l^{t h}$ layer has $c_{l}$ head centroids of weight $\\mathbf{A}^{(l,1)},\\ldots,\\mathbf{A}^{(l,c_{l})}$ . Importantly, the head centroids at each layer are sorted in descending order based on the size of their respective cluster, i.e., centroids representing more attention heads in the ancestry model are ranked higher. These head centroids condense significant knowledge and ensure the initialization of descendant models without performance degradation. Assume the descendant model has $H_{d}$ attention heads for each layer. To achieve the desired expansion of heads to initialize the descendant models, we adopt the priority weight-sharing and Fig. 2 illustrates two scenarios: ", "page_idx": 4}, {"type": "image", "img_path": "92vVuJVLVW/tmp/7bb2a9728162fdb3a46d1eed53f58dd284d404273e810733e176841b1f2e5efa.jpg", "img_caption": ["Figure 2: Illustration of priority weight-sharing. The darker the color, the larger the cluster size associated with the head centroid. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "\u2022 When $H_{d}$ is divisible by $c_{l}$ : The weights of head centroids are shared $\\frac{H_{d}}{c_{l}}$ times in sequence. For instance, centroids of weights $\\mathbf{A}^{(L,1)}$ and $\\mathbf{A}^{(L,2)}$ each share their weights across four attention heads, which are then directly assigned to eight attention heads of the descendant model in layer $L$ . \u2022 When $H_{d}$ is not divisible by $c_{l}$ : The weights of the head centroids are sequentially shared $\\left\\lfloor\\frac{H_{d}}{c_{l}}\\right\\rfloor$ times, followed by appending $\\mathbf{A}^{(l,1)},\\ldots,\\mathbf{A}^{(l,H_{d}}$ mod ) at the end. As an illustration, we share the centroids of weights $\\mathbf{A}^{(1,1)},\\ldots,\\mathbf{A}^{(1,5)}$ once and then append $\\mathbf{A}^{(1,1)},\\ldots,\\mathbf{A}^{(1,3)}$ , thus initializing eight attention heads of the descendant model in the first layer. ", "page_idx": 5}, {"type": "text", "text": "For the attention heads in the descendant models, we introduce the hyperparameter $\\begin{array}{r}{\\omega=\\frac{H_{a}}{H_{d}}}\\end{array}$ to denote the factor by which the number of attention heads is reduced compared to the ancestry model. In addition to uniformly setting the number of attention heads for each layer with the hyperparameter $\\omega$ , we also explore two other possibilities in Appendix A.7: incrementing and decrementing the count of attention heads with layer depth. According to the adjustments in the number of attention heads, the weights $\\mathbf{W}^{O}$ of the projection layer are also proportionally pruned and then inherited by the descendant models. \u2020 ", "page_idx": 5}, {"type": "text", "text": "3.3.2 Expanding FFN ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As outlined in Section 3.2, we retain the shallowest FFN determined by distance-based clustering. For this FFN centroid, we introduce learnable parameters to transform it into multiple FFNs, facilitating the initialization of descendant models at elastic scales. This process is summarized in the formula: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{FFN}_{t}(\\mathbf{x})=\\big(\\mathrm{ReLU}\\big((\\mathbf{x}(\\mathbf{W}_{1}+\\mathbf{b}_{1})\\widehat{\\mathbf{W}}_{t})\\mathbf{W}_{2}+\\mathbf{b}_{2}\\big)\\widehat{\\mathbf{W}}_{t},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\widehat{\\mathbf{W}}_{t}$ denotes the newly introduced learnable parameters to expand the $t^{t h}$ FFN. These parameters are designed for minimal learning overhead, yet they are highly effective in quickly adapting descendant models to downstream tasks. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. To condense the learngene, we employ the ImageNet-1K, a collection of 1.2 million training images and 50,000 validation images distributed across 1,000 classes as part of the ILSVRC2012 competition [9]. After initializing the descendant models with the learngene, we proceed to fine-tune these models on diverse downstream tasks. These tasks include iNaturalist-2019 [45], Food101 [4], Oxford Flowers [38], Stanford Cars [12], CIFAR-10 [24], CIFAR-100 [24], CUB-200-2011 [48]. For detailed dataset descriptions, see Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Training settings. During the learngene clustering, We set Eps, $\\varepsilon=10$ , $M i n H d s=1\\,{\\stackrel{.}{\\dd}}$ , ensuring that each attention head is included in a unique cluster. In the learngene inheriting phase, we train the descendant models on downstream tasks for 500 epochs, including a 10-epoch warm-up period, except for iNaturalist-2019, where we train for 100 epochs with a 5-epoch warm-up. The initial learning rate is set to $5\\times10^{-4}$ for most tasks, except for Stanford Cars where it is $5\\times10^{-3}$ , and a weight decay of 0.05. ", "page_idx": 6}, {"type": "text", "text": "Architectures. Both the ancestry model and descendant models are variants derived from DeiT [46]. In terms of width, there are three types of DeiT: Tiny, Small, and Base. Furthermore, as detailed in Section 4.2.4, we implement experiments on Swin Transformer [30] to demonstrate the applicability of our method across various backbones. The learnable parameters in Eqns. (7) are implemented through a nonlinear mapping such as a neural network with the rectified linear units (ReLU). ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results of Model Initialization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we validate the capabilities of Cluster-Learngene in efficiently initializing models and measure model performance with Top-1 accuracy. ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Initializing Descendant Models of Elastic Scales ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As illustrated in Fig. 3, for the Tiny-scale descendant models with only 32 attention heads, ClusterLearngene outperforms Pretraining-Finetuning on ImageNet. Moreover, the performance of ClusterLearngene improves with an increase in the number of attention heads and FFNs. This shows that Cluster-Learngene can adaptively initialize the descendant models with varying number of attention heads and FFNs, catering to downstream resource constraints. In contrast, Pretraining-Finetuning requires retraining for each model variant, significantly raising storage and training costs, particularly when dealing with models of elastic scales. Therefore, our method resolves the limitations of the onesize-ftis-all approach seen in Pretraining-Finetuning. ", "page_idx": 6}, {"type": "text", "text": "Moreover, we initialize 22 descendant models on ImageNet-1K, each with different configurations, such as the number of attention heads $H_{d}$ per layer and the quantity of FFNs $L_{d}$ in descendant models. As shown in Tab. 1, Cluster-Learngene swiftly initializes models of varying scales and proves competitive in overall performance. For example, the Small-scale descendant model with $H_{d}\\,=\\,6$ and ", "page_idx": 6}, {"type": "text", "text": "Table 1: Comparisons of performance on ImageNet-1K between models trained FromScratch with 100 epochs and those initialized via Cluster-Learngene fine-tuned for 50 epochs. ", "page_idx": 6}, {"type": "table", "img_path": "92vVuJVLVW/tmp/d5e8cd9a7192b4d34017ed1eda41bbcf04536f1d449b7c5d539e30e61189427d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "$L_{d}=12$ illustrates this point. Cluster-Learngene not only outperforms From-Scratch by ${\\bf9.87\\%}$ but also manages to reduce $2\\times$ training times. ", "page_idx": 6}, {"type": "text", "text": "4.2.2 Efficiently Initializing Large Models on ImageNet ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The results in Fig. 4 reveal that Cluster-Learngene outperforms Pretraining-Finetuning with fewer inherited parameters. For example, in the case of Small-scale descendant models, Cluster-Learngene inherits only 15.1M parameters and attains a performance of $78.43\\%$ , while Pretraining-Finetuning with 18.0M parameters achieves less than $75\\%$ . These results demonstrate the advanced initialization ability of Cluster-Learngene, as the clustered learngene retains the critical generalizable knowledge. ", "page_idx": 6}, {"type": "text", "text": "4.2.3 Initialization Results on Different Downstream Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct a comparative analysis of our approach for initializing descendant or downstream models, as follows: (i) Pretraining-Finetuning: This approach pre-trains DeiT on ImageNet and ", "page_idx": 6}, {"type": "image", "img_path": "92vVuJVLVW/tmp/b4c2be41c8a561632171bf5d27e7e26c2343d5982fadbd4d9e18abef93025946.jpg", "img_caption": ["Figure 3: Initializing descendant models of elastic scales. \u201cL6/9/12\" denote descendant models with 6, 9, and 12 layers, respectively. For a fair comparison, the downstream models in PretrainingFinetuning inherit parameters from 12 layers of the pre-trained model, with the inherited number of attention heads matching those in Cluster-Learngene. We fine-tune 50 epochs for all models. In (a), the hyperparameter $\\omega$ takes values ranging from 1 to $\\frac18$ (i.e., the number of attention heads in descendant models is eight times that of the ancestry model). In (b), $\\omega$ ranges from 2 to $\\textstyle{\\frac{1}{4}}$ . Continuing this pattern, in (c), $\\omega$ ranges from a maximum of 4 to a minimum of $\\frac{1}{2}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "92vVuJVLVW/tmp/0558cbdf7b42f1810ab78a1ba4451c2f190868b83f6f5ea96731175b9575658e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Efficiently initializing large models on ImageNet. \u201cFront/middle/last\" refer to inheriting parameters from the front, middle, or last 10 layers of a pretrained model to initialize 12-layer descendant models. All approaches are fine-tuned for 50 epochs. \u201cI-Params\u201d means the number of Inherited parameters in the downstream/descendant models, measured in MB. ", "page_idx": 7}, {"type": "image", "img_path": "92vVuJVLVW/tmp/57c52e480a462c111a411f9da40e26d1a9f80ac97f5ff8a40c84653f37ae3793.jpg", "img_caption": ["Figure 5: Faster convergence. Different points represent results for varying epochs and the hyperparameter $\\omega$ is set to 1.0 for our method. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "92vVuJVLVW/tmp/4a3e2a950e22c84f5ac9c1cd0e9b9c549216fe6ba5884d2aaa8b70e36b9aaa6e.jpg", "table_caption": ["Table 2: DeiT-Small Results on downstream datasets. $\\uparrow$ represents the performance improvement achieved by Cluster-Learngene, when compared to the best method excluding Pretraining-Finetuning. All results are derived from the 6-layer downstream models. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "subsequently fine-tunes the entire model on downstream tasks. (ii) From-Scratch: We commence with a randomly initialized DeiT model on the downstream datasets. (iii) Heuristic-Learngene [49]: This strategy involves extracting the last six layers from a DeiT model pre-trained on ImageNet and then stacking them with randomly initialized lower layers to construct a new model. (iv) Weight-Transformation [63]: This method employs Weight Transformation to pre-train DeiT on ImageNet, followed by fine-tuning the entire model to adapt it to specific downstream tasks. (v) Auto-Learngene [50]: The first six layers are extracted from the DeiT and then stacked with randomly initialized higher layers to initialize the descendant models. ", "page_idx": 7}, {"type": "text", "text": "As illustrated in Tab. 2, our Cluster-Learngene significantly outperforms both From-Scratch and Weight-Transformation. When compared to other Learngene methods, such as Auto-Learngene, Cluster-Learngene exceeds by $11.17\\%$ on the iNaturalist-2019 (iNat-2019). These results highlight the superior capability of Cluster-Learngene in efficiently initializing descendant models. Moreover, on six datasets, the performance of Cluster-Learngene outperforms that of Pretraining-Finetuning, where the entire model is fine-tuned. This phenomenon can be attributed to the more universally significant knowledge within learngene, allowing it to adapt effectively to various downstream tasks. ", "page_idx": 7}, {"type": "text", "text": "4.2.4 Faster Convergence ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We provide a detailed comparison of training efficiency between our approach and From-Scratch on ImageNet. As shown in Fig. 5 (a), Cluster-Learngene requires only $\\mathbf{4.0\\times}$ less training overhead compared to From Scratch on Small-scale descendant models. A key advantage of our approach is that descendant models initialized with the learngene achieve faster convergence, owing to a superior initialization point. ", "page_idx": 8}, {"type": "text", "text": "4.2.5 Higher Data Efficiency ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further conduct experiments on Base-scale descendant models over different percentages of training data from ImageNet-1K (IN-1K). As shown in Tab. 3, while our method does not outperform the From-Scratch on the entire dataset, its performance exhibits greater stability as the amount of training data decreases. For instance, with only $25\\%$ of the training data, Cluster-Learngene outperforms FromScratch by $7.09\\,\\%$ , while requiring only $\\frac{1}{6}$ of the training cost. This higher data efficiency of our method is attributed to the significant knowledge within the learngene, which helps descendant mod", "page_idx": 8}, {"type": "text", "text": "Table 3: Initialization of descendant models with diverse training samples. The symbol $\\uparrow$ denotes the performance gap between our approach and the From-Scratch method. Cluster-Learngene initializes the descendant model over 50 training epochs. In contrast, From-Scratch results are achieved after 300 training epochs. ", "page_idx": 8}, {"type": "table", "img_path": "92vVuJVLVW/tmp/3735116524f181dc883d388e4bab2a8ce013da6607f6de2bde84e0e91ea1190b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "els mitigate overfitting, especially in scenarios with limited data. ", "page_idx": 8}, {"type": "text", "text": "4.3 Analysis and Ablation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide further analysis and ablation of Cluster-Learngene. \u00a7 Unless otherwise specified, we conduct experiments on CIFAR-100 and use Small-scale DeiT as the ancestry model. ", "page_idx": 8}, {"type": "text", "text": "4.3.1 Comparison of the Clustering Method ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Additionally, we compare the results of $k$ -means clustering [20] of attention heads with cluster centroids $(k)$ set at 1, 2, and 3. Tab. 4 shows that Cluster-Learngene not only outperforms in clustering efficiency but also adaptively adjusts the count of cluster centroids for each model layer, unlike $k$ -means that requires predefined the numbers of cluster centroids. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Comparison of the clustering method. All results are from the 6-layer downstream models. ", "page_idx": 8}, {"type": "table", "img_path": "92vVuJVLVW/tmp/5cabeb44a7aeb66bc35c883a4a3f323daa50b8d7c33020bbdadc05caae6edb72.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.2 Comparison of Priority Weight-sharing ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Table 5, our priority weight-sharing is compared against three alternative weight-sharing methods: following the original sequence of heads, by ascending d\u02dc of heads, and by descending $\\tilde{d}$ of heads. The results validate the effectiveness of priority weight-sharing, as our method accounts for the fact that the larger the cluster a head belongs to, the richer the critical knowledge it represents. ", "page_idx": 8}, {"type": "text", "text": "Table 5: Comparison of priority weight-sharing. All results are derived from the 6-layer downstream models. ", "page_idx": 8}, {"type": "table", "img_path": "92vVuJVLVW/tmp/1ab93a5f3972eee66d5de630b27c8e561d2cb714402e8579545293c00d29d99f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.3 Qualitative Visualization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We illustrate attention representations in Fig. 6 to explain which significant knowledge is inherited by learngene. To clarify the visualization, we apply a power exponent of $\\gamma=0.25$ . In the first layer of the ancestry model, a head centroid is clustered from heads 1, 2, 4, and 5 to initiate the head 1 in the descendant model, and so forth. Then, weight-sharing is applied to expand head centroids, e.g., sharing twice to initialize the descendant model. ", "page_idx": 8}, {"type": "image", "img_path": "92vVuJVLVW/tmp/1cd59d1590c29237e27297bb8fa2190a98d3391f28346dac96a3be18998c6975.jpg", "img_caption": ["Figure 6: Visualization of attention representations $(197\\times197)$ . We perform the following normalization operation on all attention heads $\\mathbf{A}$ of the ancestry model and descendant model: $\\left(\\frac{{\\bf A}_{i,j}}{255}\\right)^{\\gamma}$ . The descendant model is trained for 50 epochs, and $\\omega$ is set to $\\textstyle{\\frac{1}{4}}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "In the first layer, heads 1, 2, 4, and 5 with similar semantics form the largest cluster, mainly focusing on attention patterns along the main diagonal. Additionally, heads 3 and 4 present distinct semantics, with head 4 reflecting more abstract and high-level features akin to the final layer. Thus, the first learngene layer integrates three principal representation patterns from the ancestry model. In contrast, the representations in the final layer of the ancestry model exhibit significant repetition, leading to the clustering of a single-head centroid for initializing the attention heads of the descendant model. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose Cluster-Learngene to adaptively cluster attention heads, extracting head centroids and FFN centroids as the learngene. Subsequently, we adopt the priority weight-sharing of head centroids for varying number of attention heads and leverage learnable parameters to expand the FFN centroids into multiple FFNs, enabling adaptation to diverse downstream resource constraints. Extensive experiments validate the efficiency and scalability of our initialization method. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the National Science Foundation of China (62125602, 62076063), the Key Program of Jiangsu Science Foundation (BK20243012), the Fundamental Research Funds for the Central Universities (2242024k30035) and the Big Data Computing Center of Southeast University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Mohiuddin Ahmed, Raihan Seraj, and Syed Mohammed Shamsul Islam. The k-means algorithm: A comprehensive survey and performance evaluation. Electronics, 9(8):1295, 2020.   \n[2] Devansh Arpit, V\u00edctor Campos, and Yoshua Bengio. How to initialize your network? robust initialization for weightnorm & resnets. Advances in Neural Information Processing Systems, 32, 2019.   \n[3] Panthadeep Bhattacharjee and Pinaki Mitra. A survey of density based clustering algorithms. Frontiers of Computer Science, 15:1\u201327, 2021.   \n[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014.   \n[5] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \n[6] Adil Abdu Bushra and Gangman Yi. Comparative analysis review of pioneering dbscan and successive density-based clustering algorithms. IEEE Access, 9:87918\u201387935, 2021.   \n[7] Shiming Chen, Ziming Hong, Guo-Sen Xie, Wenhan Yang, Qinmu Peng, Kai Wang, Jian Zhao, and Xinge You. Msdn: Mutually semantic distillation network for zero-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7612\u20137621, June 2022. [8] Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn, and Claire Mathieu. Hierarchical clustering: Objective functions and algorithms. Journal of the ACM (JACM), 66(4):1\u201342, 2019.   \n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[12] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual census estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.   \n[13] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.   \n[14] Greg Hamerly and Charles Elkan. Learning the k in k-means. Advances in neural information processing systems, 16, 2003.   \n[15] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):7436\u20137456, 2021.   \n[16] Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. Advances in Neural Information Processing Systems, 31, 2018.   \n[17] Uri Hasson, Samuel A Nastase, and Ariel Goldstein. Direct fit to nature: an evolutionary perspective on biological and artificial neural networks. Neuron, 105(3):416\u2013434, 2020.   \n[18] Haoyu He, Zizheng Pan, Jing Liu, Jianfei Cai, and Bohan Zhuang. Efficient stitchable task adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28555\u201328565, June 2024.   \n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[20] Abiodun M Ikotun, Absalom E Ezugwu, Laith Abualigah, Belal Abuhaija, and Jia Heming. K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data. Information Sciences, 622:178\u2013210, 2023.   \n[21] ZHAO Yongxin ZHANG Shenglin GONG Zican JI Yuhe, HAN Jing. Log anomaly detection through gpt-2 for large scale systems. ZTE Communications, 21(3):70, 2023.   \n[22] Runhao Jiang, Jie Zhang, Rui Yan, and Huajin Tang. Few-Shot Learning in Spiking Neural Networks by Multi-Timescale Optimization. Neural Computation, 33(9):2439\u20132472, 08 2021. ISSN 0899-7667. doi: 10.1162/neco_a_01423.   \n[23] Hans-Peter Kriegel, Peer Kr\u00f6ger, J\u00f6rg Sander, and Arthur Zimek. Density-based clustering. Wiley interdisciplinary reviews: data mining and knowledge discovery, 1(3):231\u2013240, 2011. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. ", "page_idx": 11}, {"type": "text", "text": "[25] Lujun Li, Peijie, Zhenheng Tang, Xiang Liu, Qiang Wang, Wenhan Luo, Wei Xue, Qifeng Liu, Xiaowen Chu, and Yike Guo. Discovering sparsity allocation for layer-wise pruning of large language models. In NeuIPS, 2024.   \n[26] Wei Li, Lujun Li, Mark Lee, and Shengjie Sun. Als: Adaptive layer sparsity for large language models via activation correlation assessment. In NeuIPS, 2024.   \n[27] Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 8547\u20138555, 2021.   \n[28] Shuxia Lin, Miaosen Zhang, Ruiming Chen, Xu Yang, Qiufeng Wang, and Xin Geng. Linearly decomposing and recomposing vision transformers for diverse-scale models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.   \n[29] Biao Liu, Ning Xu, and Xin Geng. Progressively selective label enhancement for language model alignment. arXiv preprint arXiv:2408.02599, 2024.   \n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[31] WANG Jiabo LIU Yang LUO Wenjian LIU Qinbo, JIN Zhihao. Msra-fed: A communication-efficient federated learning method based on model split and representation aggregate. ZTE Communications, 20 (3):35, 2022.   \n[32] Xuran Meng and Jianfeng Yao. Impact of classification difficulty on the weight matrices spectra in deep learning and application to early-stopping. J. Mach. Learn. Res., 24(1), March 2024. ISSN 1532-4435.   \n[33] Xuran Meng, Difan Zou, and Yuan Cao. Benign overftiting in two-layer relu convolutional neural networks for xor data. In Forty-first International Conference on Machine Learning.   \n[34] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019.   \n[35] Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clustering: an overview. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 2(1):86\u201397, 2012.   \n[36] ZOU Xiaojing DOU Yutao Albert Y. ZOMAYA NAN Yucen, FANG Minghao. A collaborative medical diagnosis system without sharing patient data. ZTE Communications, 20(3):3, 2022.   \n[37] Zanlin Ni, Yulin Wang, Jiangwei Yu, Haojun Jiang, Yu Cao, and Gao Huang. Deep incubation: Training large models by divide-and-conquering. 2022.   \n[38] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages 722\u2013729. IEEE, 2008.   \n[39] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Stitchable neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16102\u201316112, June 2023.   \n[40] Yingzhe Peng, Chenduo Hao, Xu Yang, Jiawei Peng, Xinting Hu, and Xin Geng. Learnable in-context vector for visual question answering. arXiv preprint arXiv:2406.13185, 2024.   \n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 2021.   \n[42] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? Advances in Neural Information Processing Systems, 34:12116\u201312128, 2021.   \n[43] Erich Schubert, J\u00f6rg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu. Dbscan revisited, revisited: why and how you should (still) use dbscan. ACM Transactions on Database Systems (TODS), 42(3):1\u201321, 2017.   \n[44] Chongjie Si, Zhiyi Shi, Shifan Zhang, Xiaokang Yang, Hanspeter Pfister, and Wei Shen. Unleashing the power of task-specific directions in parameter efficient fine-tuning. arXiv preprint arXiv:2409.01035, 2024.   \n[45] Kiat Chuan Tan, Yulong Liu, Barbara Ambrose, Melissa Tulig, and Serge Belongie. The herbarium challenge 2019 dataset. arXiv preprint arXiv:1906.05372, 2019.   \n[46] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.   \n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[48] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.   \n[49] Qiu-Feng Wang, Xin Geng, Shu-Xia Lin, Shi-Yu Xia, Lei Qi, and Ning Xu. Learngene: From open-world to your learning task. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8557\u20138565, 2022.   \n[50] Qiufeng Wang, Xu Yang, Shuxia Lin, and Xin Geng. Learngene: Inheriting condensed knowledge from the ancestry model to descendant models. arXiv preprint arXiv:2305.02279, 2023.   \n[51] Qiufeng Wang, Xu Yang, Haokun Chen, and Xin Geng. Vision transformers as probabilistic expansion from learngene. In Forty-first International Conference on Machine Learning, 2024.   \n[52] Yu Wang, Xinjie Yao, Pengfei Zhu, Weihao Li, Meng Cao, and Qinghua Hu. Integrated Heterogeneous Graph Attention Network for Incomplete Multi-modal Clustering. International Journal of Computer Vision, 132(9):3847\u20133866, September 2024. ISSN 1573-1405. doi: 10.1007/s11263-024-02066-y.   \n[53] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition. Advances in neural information processing systems, 34:11960\u201311973, 2021.   \n[54] Yongrong Wu, Jingyu Lin, Houjin Chen, Dinghao Chen, Lvqing Yang, and Jianbing Xiahou. A graphtransformer network for scene text detection. In International Conference on Intelligent Computing, pages 680\u2013690. Springer, 2023.   \n[55] Shiyu Xia, Miaosen Zhang, Xu Yang, Ruiming Chen, Haokun Chen, and Xin Geng. Transformer as linear expansion of learngene. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 16014\u201316022, 2024.   \n[56] Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, and Yue Cao. Revealing the dark secrets of masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14475\u201314485, 2023.   \n[57] Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, and Xinchao Wang. Deep model reassembly. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 25739\u201325753. Curran Associates, Inc., 2022.   \n[58] Xu Yang, Yingzhe Peng, Haoxuan Ma, Shuo Xu, Chi Zhang, Yucheng Han, and Hanwang Zhang. Lever lm: Configuring in-context sequence to lever large vision language models. arXiv e-prints, pages arXiv\u20132312, 2023.   \n[59] Yao Yao, Zuchao Li, and Hai Zhao. Sirllm: Streaming infinite retentive llm. arXiv preprint arXiv:2405.12528, 2024.   \n[60] Hua Yuan, Yu Shi, Ning Xu, Xu Yang, Xin Geng, and Yong Rui. Learning from biased soft labels. Advances in Neural Information Processing Systems, 36, 2023.   \n[61] Wei Yuan, Hongzhi Yin, Tieke He, Tong Chen, Qiufeng Wang, and Lizhen Cui. Unified question generation with continual lifelong learning. In Proceedings of the ACM Web Conference 2022, WWW \u201922, page 871\u2013881, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450390965. doi: 10.1145/3485447.3511930.   \n[62] Anthony M Zador. A critique of pure learning and what artificial neural networks can learn from animal brains. Nature communications, 10(1):1\u20137, 2019.   \n[63] Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Minivit: Compressing vision transformers with weight multiplexing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12145\u201312154, 2022.   \n[64] Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-distillation: Towards efficient and compact neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4388\u20134403, 2021.   \n[65] Pengfei Zhu, Xinjie Yao, Yu Wang, Binyuan Hui, Dawei Du, and Qinghua Hu. Multi-view deep subspace clustering networks. arXiv preprint arXiv:1908.01978, 2019. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "92vVuJVLVW/tmp/397291c4d34a02aed06d02387a26e35576309cf6d127cd13a4eed9c21addd153.jpg", "table_caption": ["Table 6: Characteristics of the downstream datasets "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Mean Attention Distance in DeiT-S and DeiT-Ti ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Fig. 7 illustrates the mean attention distance for two other variants of DeiT. In both variants, the lower layers exhibit a dual focus on both local and global aspects, resulting in a relatively sparse distribution of attention heads. Conversely, the higher layers prioritize the global context, leading to a more compact distribution of attention heads. Importantly, many attention heads in these layers exhibit repetitive functionality, contributing to weight redundancy. In the process of FFN clustering, the shallowest layers are preserved as the FFN centroids across different configurations: in DeiT-Tiny, this applies to layers (2,3) and (11,12); in DeiT-Small, to layers (10,11,12); and in DeiT-Base, to layers (7,8) and (10,11,12). ", "page_idx": 14}, {"type": "image", "img_path": "92vVuJVLVW/tmp/739aa44f13ead694b7c64a199c5779cae2c8d3e7c9543f0dccae152dab2e8c76.jpg", "img_caption": [], "img_footnote": ["Figure 7: The distribution density of attention heads across the different layers of the ancestry model, which employs the DeiT-S and DeiT-Ti [46]. "], "page_idx": 14}, {"type": "text", "text": "A.2 Downstream Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tab. 6 presents the details of all downstream tasks. ", "page_idx": 14}, {"type": "text", "text": "A.3 Projection Layer ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "According to the adjustments in the number of attention heads, the weights $\\mathbf{W}^{O}$ of the projection layer are also proportionally pruned or expanded with the hyperparameter $\\omega$ and then inherited by ", "page_idx": 14}, {"type": "text", "text": "the descendant models. Additionally, we directly inherit the weights of layer normalization, patch embeddings, and position embeddings in the ancestry model, which constitute only a small fraction of all weights. ", "page_idx": 15}, {"type": "text", "text": "A.4 Faster Convergence ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide a detailed comparison of training efficiency on ImageNet for the Tiny-scale descendant models. As shown in Fig. 8 (b), Cluster-Learngene requires only $\\mathbf{4.0\\times}$ less training overhead compared to From Scratch on Small-scale descendant models. A key advantage of our approach is that descendant models initialized with the learngene achieve faster convergence, owing to a superior initialization point. ", "page_idx": 15}, {"type": "image", "img_path": "92vVuJVLVW/tmp/fc737a093b424d379e62f12cda6d51ddb20eb8c3c3e50febc0bc6f2ecbbab9d7.jpg", "img_caption": ["Figure 8: Faster convergence. Different points represent results for varying epochs and the hyperparameter $\\omega$ is set to 1.0 for our method. ", "Table 7: Ablation on the selection of head centroids. All results are derived from the 6-layer downstream models. we conduct experiments on CIFAR-100 and use DeiT-Small as the ancestry model. ", "Table 8: Ablation on $\\varepsilon$ . All results are derived from the 6-layer downstream models. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.5 Ablation on the Selection of Head Centroids ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conduct an ablation to assess whether inheriting parameters from the nearest module to the cluster centroid or averaging parameters after clustering heads and FFNs is more effective. Table 7 shows Cluster-Learngene excelling when inheriting from the closest heads/FFNs to the centroid, a logical approach as these modules aptly represent similar semantics within a cluster. ", "page_idx": 15}, {"type": "table", "img_path": "92vVuJVLVW/tmp/39356d88f1616c81462c15da5262c45656eb225ae61e3622e01506b0f85e49b3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.6 Ablation on $\\varepsilon$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 8 shows the results for different values of $\\varepsilon$ . $\\varepsilon=1$ implies no FFN clustering, potentially causing negative transfer. $\\varepsilon\\,=\\,100$ means clustering all FFNs in adjacent layers with identical head centroid counts, resulting in an excessive cluster of FFNs and subsequent degradation in the performance of initialized descendant models. Our Cluster-Learngene $\\;\\varepsilon=10$ ) strikes a good balance between these issues. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\varepsilon=1\\quad\\varepsilon=10\\quad\\varepsilon=100}{85.27\\quad85.38\\quad\\quad80.45}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "table", "img_path": "92vVuJVLVW/tmp/a370519c56e72af78576042992930e8018f8654f15053cb784dd60505fb14f1c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 9: Increment or decrement the count of attention heads. \u201cDecrementing\u201d denotes halving the number of attention heads in the first four layers, reducing them by a quarter in the middle four layers, and maintaining them in the last four layers relative to the ancestry model. Conversely, \u201cIncrementing\u201d represents the opposite pattern. ", "page_idx": 16}, {"type": "text", "text": "A.7 Variation in the Count of Attention Head with Model Depth ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Tab. 9 presents two scenarios where the number of attention heads varies across different layers. Across all descendant model configurations, \u201cIncrementing\u201d consistently outperforms \u201cDecrementing\u201d by a margin of $1.45\\%$ in terms of accuracy. These findings align with previous research [34, 30], which suggests that setting more attention heads in higher layers can assist these layers in learning more abstract and high-level feature representations. ", "page_idx": 16}, {"type": "text", "text": "A.8 Initializing Descendant Models of Elastic Scales ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As illustrated in Fig. 3, Cluster-Learngene incurs a total of $300+50*10=800$ epochs. In contrast, pre-trained models require retraining a new model of elastic scale for 300 epochs each, followed by fine-tuning. Considering only the training expense of new models, training 10 such models would require at least $300*10=3000$ epochs. Therefore, our algorithm can reduce the computational cost by at least $3\\times$ . This efficiency demonstrates how our method overcomes the one-size-fits-all limitation inherent in the Pretraining-Finetuning approach. ", "page_idx": 16}, {"type": "text", "text": "A.9 Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The performance of descendant models within Cluster-Learngene heavily depends on the quality of the ancestry model. If the ancestry model harbors inherent limitations or biases, these issues may propagate and even amplify within the descendant models, potentially compromising their effectiveness. Therefore, ensuring the robustness and fairness of the ancestry model is paramount for maintaining the performance of descendant models in Cluster-Learngene. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Please see Section 4. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Please see Appendix. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Please see Section 3. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Please see Section 4. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Please see Appendix. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Please see Section 4. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Please see Section 4. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please see Section 4. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please see Appendix. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Please see Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 20}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Please see the Experimtent Section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]