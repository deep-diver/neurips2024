[{"figure_path": "92vVuJVLVW/tables/tables_6_1.jpg", "caption": "Table 1: Comparisons of performance on ImageNet-1K between models trained From-Scratch with 100 epochs and those initialized via Cluster-Learngene fine-tuned for 50 epochs.", "description": "This table compares the Top-1 accuracy and computational resource usage (parameters and FLOPs) of models trained from scratch on ImageNet-1K and those initialized using Cluster-Learngene.  It shows the performance of different model sizes (Tiny, Small, Base) and varying numbers of attention heads and layers, demonstrating the efficiency and scalability of Cluster-Learngene.", "section": "4.2.1 Initializing Descendant Models of Elastic Scales"}, {"figure_path": "92vVuJVLVW/tables/tables_7_1.jpg", "caption": "Table 2: DeiT-Small Results on downstream datasets. \u2191 represents the performance improvement achieved by Cluster-Learngene, when compared to the best method excluding Pretraining-Finetuning. All results are derived from the 6-layer downstream models.", "description": "This table compares the performance of Cluster-Learngene against other initialization methods (Pretraining-Finetuning, From-Scratch, Heuristic-Learngene, Weight-Transformation, and Auto-Learngene) on six different downstream datasets (iNat-2019, Food-101, Flowers, Cars, CIFAR-10, CIFAR-100, CUB-200).  The \"\u2191\" symbol indicates the performance improvement of Cluster-Learngene over the best-performing alternative method. The number of inherited parameters (I-Params) is also listed for each method.", "section": "4.2.3 Initialization Results on Different Downstream Tasks"}, {"figure_path": "92vVuJVLVW/tables/tables_8_1.jpg", "caption": "Table 3: Initialization of descendant models with diverse training samples. The symbol \u2191 denotes the performance gap between our approach and the From-Scratch method. Cluster-Learngene initializes the descendant model over 50 training epochs. In contrast, From-Scratch results are achieved after 300 training epochs.", "description": "This table compares the performance of Cluster-Learngene and From-Scratch methods on initializing descendant models using different proportions of ImageNet-1k training data (100%, 50%, and 25%).  The results highlight Cluster-Learngene's ability to achieve comparable or even better accuracy with significantly fewer training epochs, demonstrating higher data efficiency.", "section": "4.2 Main Results of Model Initialization"}, {"figure_path": "92vVuJVLVW/tables/tables_8_2.jpg", "caption": "Table 1: Comparisons of performance on ImageNet-1K between models trained From-Scratch with 100 epochs and those initialized via Cluster-Learngene fine-tuned for 50 epochs.", "description": "This table compares the performance of models trained from scratch with those initialized using Cluster-Learngene on the ImageNet-1K dataset. It shows the top-1 accuracy, number of attention heads (Hd), number of layers (Ld), number of parameters (in millions), and floating point operations (in billions) for different model configurations.  The results demonstrate the effectiveness of Cluster-Learngene in improving model performance compared to training from scratch, particularly for smaller models with fewer resources.", "section": "4.2.1 Initializing Descendant Models of Elastic Scales"}, {"figure_path": "92vVuJVLVW/tables/tables_8_3.jpg", "caption": "Table 5: Comparison of priority weight-sharing. All results are derived from the 6-layer downstream models.", "description": "This table compares the performance of four different weight-sharing methods for initializing descendant models in the Cluster-Learngene approach.  The \"Origin\" method uses the original sequence of heads.  \"Increasing d\" and \"Decreasing d\" sort the heads based on their density metric (d) values in ascending and descending order respectively.  \"Ours\" represents the proposed priority weight-sharing method, which prioritizes head centroids from larger clusters. The results show that the proposed priority weight-sharing method outperforms other methods in terms of accuracy on the 6-layer downstream models.", "section": "4.3.2 Comparison of Priority Weight-sharing"}, {"figure_path": "92vVuJVLVW/tables/tables_14_1.jpg", "caption": "Table 6: Characteristics of the downstream datasets", "description": "This table presents the characteristics of the seven datasets used for evaluating the performance of the Cluster-Learngene model on downstream tasks after initializing descendant models.  The datasets vary significantly in size and the number of classes, including Oxford Flowers, CUB-200-2011, Stanford Cars, CIFAR-10, CIFAR-100, Food101, and iNaturalist-2019. The table shows the total number of images, the number of images used for training, validation, and testing, as well as the number of classes in each dataset.", "section": "4.1 Experimental Setting"}, {"figure_path": "92vVuJVLVW/tables/tables_15_1.jpg", "caption": "Table 7: Ablation on the selection of head centroids. All results are derived from the 6-layer downstream models. we conduct experiments on CIFAR-100 and use DeiT-Small as the ancestry model.", "description": "This table presents the results of an ablation study to determine the best method for selecting head centroids.  It compares four different approaches: averaging the parameters of attention heads and FFNs, selecting the parameter set that minimizes the distance to the centroid for attention heads only, selecting the parameter set that minimizes the distance to the centroid for FFNs only, and combining both selection methods. The accuracy on CIFAR-100 using a 6-layer DeiT-Small downstream model is reported for each approach.", "section": "4.3.2 Comparison of Priority Weight-sharing"}, {"figure_path": "92vVuJVLVW/tables/tables_16_1.jpg", "caption": "Table 9: Increment or decrement the count of attention heads. \u201cDecrementing\u201d denotes halving the number of attention heads in the first four layers, reducing them by a quarter in the middle four layers, and maintaining them in the last four layers relative to the ancestry model. Conversely, \"Incrementing\" represents the opposite pattern.", "description": "This table shows the impact of varying the number of attention heads across different layers of the descendant models.  Two scenarios are compared: \"Decrementing\", where the number of heads is reduced in the early layers and gradually increased towards the later layers, and \"Incrementing\", where the opposite happens. The results demonstrate the effect of this architectural change on model performance.", "section": "4.2 Main Results of Model Initialization"}]