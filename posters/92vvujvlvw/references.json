{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduces the Transformer architecture, a fundamental building block upon which the Cluster-Learngene method is based."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduces Vision Transformers (ViTs), which are directly used in the Cluster-Learngene experiments and analysis."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-07-01", "reason": "This paper presents DeiT, a data-efficient ViT model, the architecture used for the main experiments in Cluster-Learngene."}, {"fullname_first_author": "Ze Liu", "paper_title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "publication_date": "2021-10-01", "reason": "This paper introduces Swin Transformer, an alternative architecture to ViT used to test the generality of Cluster-Learngene."}, {"fullname_first_author": "Qiu-Feng Wang", "paper_title": "Learngene: From open-world to your learning task", "publication_date": "2022-01-01", "reason": "This paper introduces the original Learngene framework, which is extended and improved upon by the Cluster-Learngene method."}]}