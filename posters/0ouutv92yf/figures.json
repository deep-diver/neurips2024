[{"figure_path": "0oUutV92YF/figures/figures_1_1.jpg", "caption": "Figure 1: Protein sequence, structure, and function.", "description": "This figure illustrates the three core types of information crucial for understanding proteins: their amino acid sequence, their three-dimensional structure, and their biological functions (represented by Gene Ontology annotations).  The sequence is shown as a linear chain of amino acids, the structure is a 3D model of the folded protein, and the functions are described using GO terms, which categorize proteins based on their molecular function, biological process, and cellular component.", "section": "1 Introduction"}, {"figure_path": "0oUutV92YF/figures/figures_3_1.jpg", "caption": "Figure 2: The overall framework of ProtGO consists of two branches: a teacher model in the source domain and a student model in the target domain, connected by a knowledge distillation loss.", "description": "The figure illustrates the architecture of ProtGO, a multimodal protein representation learning framework. It consists of two branches: a teacher model trained on data including sequence, structure, and function information, and a student model trained primarily on sequence and structure data.  The teacher model utilizes a customized graph neural network (GNN) and an annotation encoder to generate hybrid embeddings. Knowledge distillation, specifically minimizing the Kullback-Leibler (KL) divergence between the teacher and student latent space distributions, guides the training of the student model.  This approach facilitates the transfer of functional knowledge from the teacher to the student, even when functional annotations are limited for the student's training data. Domain adaptation is employed to ensure that the student model can generalize well to new data.", "section": "3.2 Overall Framework"}, {"figure_path": "0oUutV92YF/figures/figures_7_1.jpg", "caption": "Figure 3: The KL training loss curves (a), (c) and test performance (b), (d) on the tasks of fold classification and EC number prediction. The red curve denotes that Lkd conducts its function, while the green curve denotes we calculated the value of Lkd, but it is not involved in the process of the gradient backpropagation (BP).", "description": "This figure shows the training loss curves and test performance for fold classification and EC number prediction tasks. Two versions are compared: one where the KL divergence loss (Lkd) is used for domain adaptation, and another where it is not. The results indicate the effectiveness of incorporating Lkd in the training process for improving performance.", "section": "4.5 Ablation Study"}, {"figure_path": "0oUutV92YF/figures/figures_8_1.jpg", "caption": "Figure 3: The KL training loss curves (a), (c) and test performance (b), (d) on the tasks of fold classification and EC number prediction. The red curve denotes that Lkd conducts its function, while the green curve denotes we calculated the value of Lkd, but it is not involved in the process of the gradient backpropagation (BP).", "description": "This figure shows the training and test performance of the ProtGO model on fold classification and EC number prediction tasks. It compares the performance when the KL divergence loss (Lkd) is included in the training process (red curves) versus when it is excluded (green curves).  The results demonstrate the impact of the KL loss on the model's learning and generalization ability.", "section": "4.5 Ablation Study"}]