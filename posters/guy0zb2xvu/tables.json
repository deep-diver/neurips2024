[{"figure_path": "GuY0zB2xVU/tables/tables_5_1.jpg", "caption": "Table 1: In-distribution and out-distribution results comparing different history window sizes. Metric is the Relative L2 loss.", "description": "This table compares the in-distribution and out-of-distribution generalization performance of different methods (Transolver, FNO, CNN, and GEPS) for solving parametric PDEs (Burgers and Gray-Scott equations) using temporal conditioning.  The \"History\" column shows the number of past states used for conditioning (3, 5, or 10), while \"In-d\" represents in-distribution performance and \"Out-d\" represents out-of-distribution performance.  Relative L2 loss is the performance metric.", "section": "3.3 In and out-of-distribution generalization performance with temporal conditioning"}, {"figure_path": "GuY0zB2xVU/tables/tables_9_1.jpg", "caption": "Table 2: In-distribution and Out-of-distribution results on 32 new test trajectories per environment. For out-of-distribution generalization, models are fine-tuned on 1 trajectory per environment. Metric is the relative L2 loss. '-' indicates inference has diverged.", "description": "This table presents the in-distribution and out-of-distribution generalization performance of various methods (LEADS, CAVIA, FOCA, CODA, GEPS, APHYNITY, Phys-Ad, GEPS-Phy) across four datasets (Pendulum, Gray-Scott, Burgers, Kolmogorov).  The metrics used are the relative L2 loss for in-distribution and out-of-distribution generalization, with models fine-tuned using one trajectory per environment in the out-of-distribution setting.  A '-' indicates divergence during inference.", "section": "5.3 Generalization results"}, {"figure_path": "GuY0zB2xVU/tables/tables_9_2.jpg", "caption": "Table 3: In-distribution and Out-distribution results. Metric is the relative L2.", "description": "This table presents the in-distribution and out-of-distribution generalization performance of GEPS and CoDA models on a larger dataset with a larger model.  The in-distribution results evaluate the model's ability to generalize to unseen initial conditions within the training environments, while out-of-distribution results assess its ability to adapt to entirely new environments with limited data. The relative L2 loss metric measures the prediction error. The table also shows the number of parameters for each model, highlighting GEPS's parameter efficiency.", "section": "5.4 Scaling to a larger dataset"}, {"figure_path": "GuY0zB2xVU/tables/tables_14_1.jpg", "caption": "Table 2: In-distribution and Out-of-distribution results on 32 new test trajectories per environment. For out-of-distribution generalization, models are fine-tuned on 1 trajectory per environment. Metric is the relative L2 loss. '-' indicates inference has diverged.", "description": "This table presents the in-distribution and out-of-distribution generalization results of different methods on four dynamical systems (Pendulum, Gray-Scott, Burgers, Kolmogorov).  For out-of-distribution, the models are fine-tuned on only one trajectory per new environment.  The results are evaluated using the relative L2 loss, and a '-' indicates that the inference diverged.", "section": "5.3 Generalization results"}, {"figure_path": "GuY0zB2xVU/tables/tables_18_1.jpg", "caption": "Table 5: Relative MSE loss with respect to number of adaptation trajectories when adapting context vectors ce.", "description": "This table presents the results of an experiment to evaluate the impact of the number of adaptation trajectories on the performance of three different models (CODA, CAVIA, and GEPS) when adapting context vectors. The results are reported as the Relative MSE loss for different numbers of adaptation trajectories (1, 4, 8, and 16).", "section": "D.2 Number of adaptation trajectories"}, {"figure_path": "GuY0zB2xVU/tables/tables_18_2.jpg", "caption": "Table 6: Relative MSE loss with respect to number of adaptation trajectories when adapting low-rank adaptation parameters.", "description": "This table shows the results of an experiment to evaluate the impact of the number of adaptation trajectories on the performance of the GEPS model when using low-rank adaptation of parameters.  The experiment was conducted on the Kolmogorov flow equation. The Relative Mean Squared Error (MSE) is reported for different numbers of adaptation trajectories (1, 4, 8, and 16). The results show that reducing the number of adaptation trajectories improves the model's performance.", "section": "D.2 Number of adaptation trajectories"}, {"figure_path": "GuY0zB2xVU/tables/tables_19_1.jpg", "caption": "Table 7: Relative MSE loss with respect to parameter initialization", "description": "This table presents the relative Mean Squared Error (MSE) loss for different parameter initialization methods (Kaiming, Xavier, LoRA init, and Orthogonal) applied to the Gray-Scott and Burgers equations. The results show that orthogonal initialization achieves the lowest loss for both equations, indicating its superior performance in this context.", "section": "D.3.1 Code dimension"}, {"figure_path": "GuY0zB2xVU/tables/tables_19_2.jpg", "caption": "Table 8: Relative MSE on the full trajectory with varying code dimension", "description": "This table shows the performance of both CoDA and GEPS models on Burgers and Gray-Scott equations by varying the code dimension.  It demonstrates the impact of code dimension on model performance, comparing the relative mean squared error (MSE) achieved by each method.  The results highlight the trade-off between code size and model accuracy.  A smaller code size may lead to faster adaptation but could sacrifice prediction accuracy.", "section": "D.3.1 Code dimension"}, {"figure_path": "GuY0zB2xVU/tables/tables_20_1.jpg", "caption": "Table 2: In-distribution and Out-of-distribution results on 32 new test trajectories per environment. For out-of-distribution generalization, models are fine-tuned on 1 trajectory per environment. Metric is the relative L2 loss. '-' indicates inference has diverged.", "description": "This table presents the in-distribution and out-of-distribution generalization performance of various models on four different datasets (Pendulum, Gray-Scott, Burgers, and Kolmogorov).  In-distribution results assess the model's ability to predict trajectories with unseen initial conditions within the same training environments. Out-of-distribution results evaluate the model's ability to adapt to completely new environments (different PDE parameters) using only one trajectory for fine-tuning.  The metric used is the relative L2 loss, and '-' indicates that the inference process for that model and environment diverged.", "section": "5.3 Generalization results"}, {"figure_path": "GuY0zB2xVU/tables/tables_21_1.jpg", "caption": "Table 10: Number of parameters (# Params) and training time (Time) for all different adaptive conditioning methods.", "description": "This table presents a comparison of the number of parameters and training time required for different adaptive conditioning methods (LEADS, CAVIA, CODA, and GEPS) across four different dynamical systems (Pendulum, Gray-Scott, Burgers, and Kolmogorov).  It highlights the computational efficiency of GEPS compared to other methods, particularly noticeable in the larger datasets.", "section": "D.3.4 Training time and number of parameters"}, {"figure_path": "GuY0zB2xVU/tables/tables_22_1.jpg", "caption": "Table 1: In-distribution and out-distribution results comparing different history window sizes. Metric is the Relative L2 loss.", "description": "This table presents the in-distribution and out-of-distribution generalization performance of different models on the Burgers and Gray-Scott PDEs when using different history window sizes (3, 5, and 10).  The relative L2 loss is used as the performance metric.  The results demonstrate the performance of classical ERM (Empirical Risk Minimization) approaches compared to the adaptive conditioning approach (GEPS). The in-distribution results show performance on unseen trajectories from the training environments; out-of-distribution results show performance on unseen trajectories and environments.", "section": "3 Motivations: ERM versus adaptive approaches for parametric PDEs"}, {"figure_path": "GuY0zB2xVU/tables/tables_22_2.jpg", "caption": "Table 12: Framework hyper-parameters", "description": "This table lists the hyperparameters used for training and adaptation for each of the four dynamical systems considered in the paper: Pendulum, Burgers, Gray-Scott, and Kolmogorov Flow.  For each system, the table specifies the context size (c), network depth, width, activation function, batch size for training and adaptation, number of epochs for training and adaptation, and the learning rate used for both training and adaptation.  Additionally, it indicates whether teacher forcing was used during training.", "section": "E Hyper-parameter details"}]