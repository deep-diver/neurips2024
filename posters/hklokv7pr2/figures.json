[{"figure_path": "hKloKv7pR2/figures/figures_4_1.jpg", "caption": "Figure 1: Toy experiments. (a) Left point S0 denoting start and ST is the only rewarded, target location. Black curves visualize behavior trajectories \u03b2. (b) Best behavior policy \u03b2* according to the data, and the optimal policy \u03c0* that provides the optimal (shortest path) solution. (c) Results of the policy \u03b2 trained by minimizing \u2212Q\u03b2(s,\u03c0(s)) + BC and policy \u03c0 trained by our algorithm 1.", "description": "This figure demonstrates the results of the proposed Partial Policy Learning (PPL) algorithm on a simplified toy environment.  Panel (a) shows several suboptimal expert trajectories attempting to reach a reward location. Panel (b) contrasts these with the best observed behavior and the true optimal policy. Panel (c) finally shows how the proposed PPL algorithm and standard behavior cloning approach perform in comparison to the true optimal.", "section": "4 Toy Experiments"}, {"figure_path": "hKloKv7pR2/figures/figures_7_1.jpg", "caption": "Figure 1: Toy experiments. (a) Left point S0 denoting start and ST is the only rewarded, target location. Black curves visualize behavior trajectories \u03b2. (b) Best behavior policy \u03b2* according to the data, and the optimal policy \u03c0* that provides the optimal (shortest path) solution. (c) Results of the policy \u03b2 trained by minimizing \u2212Q\u03b2(s, \u03c0(s)) + BC and policy \u03c0 trained by our algorithm 1.", "description": "This figure shows a comparison of different policies in a simple toy environment. The goal is to find the shortest path from a starting point (S0) to a reward location (ST).  Panel (a) displays the trajectories followed by three suboptimal expert policies. Panel (b) shows the optimal policy (\u03c0*) that achieves the shortest path and the best policy (\u03b2*) found in the data among the experts.  Panel (c) contrasts the performance of a behavior cloning policy (\u03b2) and the policy (\u03c0) learned by the proposed algorithm. The proposed algorithm outperforms the behavior cloning policy by finding a path closer to the optimal solution.", "section": "4 Toy Experiments"}, {"figure_path": "hKloKv7pR2/figures/figures_13_1.jpg", "caption": "Figure 1: Toy experiments. (a) Left point S0 denoting start and ST is the only rewarded, target location. Black curves visualize behavior trajectories \u03b2. (b) Best behavior policy \u03b2* according to the data, and the optimal policy \u03c0* that provides the optimal (shortest path) solution. (c) Results of the policy \u03b2 trained by minimizing -Q\u03b2(s, \u03c0(s))+BC and policy \u03c0 trained by our algorithm 1.", "description": "This figure shows a comparison of different policies on a simple toy environment.  Part (a) displays the environment and example trajectories of suboptimal behavior policies. Part (b) illustrates the optimal policy (\u03c0*) and the best policy found in the data (\u03b2*). Part (c) compares the performance of a behavior cloning policy (\u03b2) with the proposed Partial Policy Learning (PPL) algorithm (\u03c0), highlighting the PPL algorithm's ability to extract a better policy by stitching together elements from the dataset.", "section": "4 Toy Experiments"}]