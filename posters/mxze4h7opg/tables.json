[{"figure_path": "MXze4H7opg/tables/tables_3_1.jpg", "caption": "Table 1: Perplexity (PPL) of training and pruning with random versus top sparsity for LLaMA 60M on 1.1B tokens.", "description": "The table presents the perplexity scores (PPL) achieved using different training and pruning methods on the LLaMA 60M language model trained on 1.1B tokens. It compares the full-rank model with low-rank approximations (Lo) and further experiments that combine low-rank with different sparse pruning and training strategies. Notably, it highlights that using random sparse support for training is comparable to using top sparse support, motivating the use of random fixed-support sparse learning combined with low-rank learning in SLTrain.", "section": "3 SLTrain: proposed sparse plus low-rank pretraining"}, {"figure_path": "MXze4H7opg/tables/tables_4_1.jpg", "caption": "Table 2: Validation perplexity (PPL(\u2193)), number of parameters in millions (Param), and estimated total memory cost in G (Mem). The perplexity results for all the baselines are taken from [59]. For SLTrain, we use the same rank as other baselines and fix \u03b4 = 0.03.", "description": "This table compares the performance of different methods for pretraining LLMs across various metrics.  It shows the validation perplexity (a measure of how well the model predicts text), the number of parameters (model size), and the estimated memory cost for full-rank training, three low-rank baselines (Low-Rank [24], ReLoRA [32], GaLore [59]), and the proposed SLTrain method.  The results demonstrate that SLTrain achieves a perplexity comparable to full-rank training with significantly reduced parameter count and memory usage.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_6_1.jpg", "caption": "Table 2: Validation perplexity (PPL(\u2193)), number of parameters in millions (Param), and estimated total memory cost in G (Mem). The perplexity results for all the baselines are taken from [59]. For SLTrain, we use the same rank as other baselines and fix \u03b4 = 0.03.", "description": "This table compares the performance of different pretraining methods across various model sizes in terms of perplexity, parameter count, and memory usage.  The methods compared include Full-Rank, Low-Rank, ReLORA, GaLore, and SLTrain.  The table highlights that SLTrain achieves comparable perplexity to Full-Rank training while significantly reducing parameter count and memory usage.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_8_1.jpg", "caption": "Table 3: Throughput tokens/seconds for LLaMA 350M (on 1\u00d780G A100 GPU) 1B (on 4\u00d780G A100 GPUs).", "description": "This table shows the throughput, measured in tokens per second, achieved by three different training methods (Full-Rank, GaLore, and SLTrain) on two different sizes of the LLaMA language model (350M and 1B parameters).  The results highlight the relative efficiency of each method in terms of processing speed during training.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_8_2.jpg", "caption": "Table 4: Validation perplexity, actual memory footprint per GPU, and throughput tokens/seconds (Tokens/sec) for LLaMA 7B on 1.4B tokens.", "description": "This table compares the performance of 8-bit GaLore and 8-bit SLTrain on the LLaMA 7B model, focusing on validation perplexity, actual memory footprint per GPU, and throughput (tokens per second).  It highlights the memory efficiency gains achieved by SLTrain while maintaining a comparable level of performance.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_8_3.jpg", "caption": "Table 2: Validation perplexity (PPL(\u2193)), number of parameters in millions (Param), and estimated total memory cost in G (Mem). The perplexity results for all the baselines are taken from [59]. For SLTrain, we use the same rank as other baselines and fix \u03b4 = 0.03.", "description": "This table compares the performance of different models in terms of perplexity, the number of parameters, and memory usage.  It presents results for several LLAMA models of different sizes (60M, 130M, 350M, 1B parameters) pretrained using different methods: Full-Rank, Low-Rank, ReLORA, GaLore, and SLTrain.  The SLTrain results use a fixed sparsity parameter (\u03b4 = 0.03).  The table shows that SLTrain achieves comparable perplexity to Full-Rank while significantly reducing parameter count and memory requirements.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_9_1.jpg", "caption": "Table 6: Ablation comparison with low-rank and sparse parameterization along with change of rank r and sparsity \u03b4. Validation perplexity (\u2193) and parameter size and estimated memory cost in brackets.", "description": "This table presents an ablation study comparing the performance of different parameterizations for the SLTrain model. It shows how changes in rank (r) and sparsity (\u03b4) affect the validation perplexity (a lower score is better). The memory cost is also shown in gigabytes (G). This allows a comparison of model size against the performance improvement. Notably, the table displays results for various configurations of the SLTrain model alongside the full-rank baseline. It demonstrates that while adding more parameters generally improves performance, it is accompanied by an increase in memory.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_9_2.jpg", "caption": "Table 7: Results training LLaMA 350M (with batch size=128 per GPU) and LLaMA 1B (with batch size=32 per GPU). Validation perplexity (PPL) (\u2193), number of parameters in millions (Param) and actual max memory allocated per GPU in G (Mem).", "description": "This table compares the performance of full-rank training and SLTrain with different sparsity ratios (\u03b4) on LLaMA 350M and 1B models. It shows the validation perplexity, the number of parameters (in millions), and the maximum memory used per GPU.  The results demonstrate that increasing the sparsity ratio while maintaining comparable perplexity leads to significant reductions in parameter size and memory usage.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_19_1.jpg", "caption": "Table 2: Validation perplexity (PPL(\u2193)), number of parameters in millions (Param), and estimated total memory cost in G (Mem). The perplexity results for all the baselines are taken from [59]. For SLTrain, we use the same rank as other baselines and fix \u03b4 = 0.03.", "description": "This table compares the performance of different models on the LLaMA dataset.  The models are compared across three metrics: validation perplexity (PPL), number of parameters (in millions), and estimated memory cost (in gigabytes).  The baselines used are full-rank training, low-rank training, ReLoRA, and GaLore.  SLTrain results are shown, using the same rank as other methods, with \u03b4 fixed at 0.03. Lower PPL values are better, indicating higher model accuracy. Lower parameter and memory costs indicate more efficient models. The table helps to assess the trade-off between model performance and resource utilization.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_20_1.jpg", "caption": "Table 6: Ablation comparison with low-rank and sparse parameterization along with change of rank r and sparsity \u03b4. Validation perplexity (\u2193) and parameter size and estimated memory cost in brackets.", "description": "This table presents the results of an ablation study on the impact of varying the rank (r) and sparsity (\u03b4) parameters on the performance of the proposed SLTrain model.  It compares the validation perplexity, parameter size, and estimated memory cost for different combinations of r and \u03b4 values, alongside a full-rank baseline. The results help in understanding the trade-off between model complexity and performance achieved by adjusting the hyperparameters.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_20_2.jpg", "caption": "Table 6: Ablation comparison with low-rank and sparse parameterization along with change of rank r and sparsity \u03b4. Validation perplexity (\u2193) and parameter size and estimated memory cost in brackets.", "description": "This table presents an ablation study comparing the performance of different configurations of the SLTrain model by varying the rank (r) and sparsity (\u03b4) parameters.  The results show the validation perplexity, parameter size (in millions), and estimated memory cost (in gigabytes) for different combinations of r and \u03b4.  The goal is to evaluate how the balance of low-rank and sparse components affect model performance and resource usage.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_21_1.jpg", "caption": "Table 11: Hyperparameters of SLTrain for fine-tuning. The batch size, number of epochs and rank r follows from the choice in [59].", "description": "This table shows the hyperparameters used for fine-tuning the RoBERTa base model with SLTrain on the GLUE benchmark.  It specifies the batch size, number of epochs, rank (r), learning rate, sparsity (\u03b4), and scaling factor (\u03b1) for each of the eight GLUE tasks (COLA, STS-B, MRPC, RTE, SST-2, MNLI, QNLI, QQP).  The rank and epochs are consistent with the settings in the paper referenced by [59], while the other hyperparameters are tuned specifically for SLTrain on this fine-tuning task.", "section": "G Fine-tuning LLMs"}, {"figure_path": "MXze4H7opg/tables/tables_21_2.jpg", "caption": "Table 2: Validation perplexity (PPL(\u2193)), number of parameters in millions (Param), and estimated total memory cost in G (Mem). The perplexity results for all the baselines are taken from [59]. For SLTrain, we use the same rank as other baselines and fix \u03b4 = 0.03.", "description": "This table compares the performance of different model training methods on various sizes of the LLaMA language model.  The metrics compared are validation perplexity (PPL), number of parameters (in millions), and estimated total memory cost (in gigabytes).  The results show SLTrain's comparable performance to full-rank training with reduced parameter size and memory cost.  The baseline methods for comparison include Full-Rank, Low-Rank, ReLORA, and GaLore.  For SLTrain, a consistent sparsity ratio (\u03b4) of 0.03 is used across different model sizes.", "section": "5.1 Pretraining LLMs"}]