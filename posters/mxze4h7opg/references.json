{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-00-00", "reason": "This paper introduces the Low-Rank Adaptation (LoRA) technique, which is a foundation for the SLTrain method and is extensively referenced and compared throughout the paper."}, {"fullname_first_author": "Vladislav Lialin", "paper_title": "ReLoRA: High-rank training through low-rank updates", "publication_date": "2024-00-00", "reason": "ReLoRA is a key baseline method for comparison, representing a state-of-the-art approach in low-rank pretraining that SLTrain improves upon."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023-00-00", "reason": "The LLaMA models are the primary experimental subject of this paper, so understanding their design and properties is crucial for interpreting the results."}, {"fullname_first_author": "Tianqi Chen", "paper_title": "Training deep nets with sublinear memory cost", "publication_date": "2016-04-06", "reason": "This work addresses memory efficiency in deep learning, a significant concern that SLTrain aims to address, making this a highly relevant reference."}, {"fullname_first_author": "Jiawei Zhao", "paper_title": "Galore: Memory-efficient LLM training by gradient low-rank projection", "publication_date": "2024-00-00", "reason": "GaLore is another key baseline for comparison that provides a different approach to memory-efficient pretraining, highlighting an alternative strategy."}]}