[{"figure_path": "7X5zu6GIuW/tables/tables_15_1.jpg", "caption": "Table 1: The number of videos used for each environment.", "description": "This table shows the number of Do's videos and Don'ts videos used for each environment in the DoDont experiments.  For the online DeepMind Control (DMC) suite experiments, 4 Do's videos and 4 Don'ts videos were used. In the Kitchen environment, 10 Don'ts videos were used with the Do's videos coming from the D4RL dataset. The offline Zero-shot DMC experiments utilized 12 videos per task from the ExORL dataset for the Don'ts, while the Do's videos were not specified in this table.", "section": "C Implementation Details of DoDont"}, {"figure_path": "7X5zu6GIuW/tables/tables_17_1.jpg", "caption": "Table 2: Hyperparameters used in Online DoDont. We adopt default hyperparameters from METRA [39], introducing only one additional hyperparameter.", "description": "This table lists the hyperparameters used in the online version of the DoDont algorithm.  Most values are taken directly from the METRA algorithm, with only one additional hyperparameter added for the instruction network.  The table shows the hyperparameter name, and its corresponding value. Values may differ based on the environment (DMC or Kitchen) and whether the environment uses state-based or pixel-based inputs. ", "section": "C Implementation Details of DoDont"}, {"figure_path": "7X5zu6GIuW/tables/tables_17_2.jpg", "caption": "Table 3: Hyperparameters used in Offline DoDont. We adopt default hyperparameters from HILP [37], introducing only one additional hyperparameter.", "description": "This table lists the hyperparameters used in the offline version of the DoDont algorithm.  Most values are adopted directly from the HILP algorithm, with only one additional hyperparameter introduced by DoDont for the instruction network.  The table details hyperparameters controlling learning rate, optimizer, minibatch size, network architecture, target network smoothing, latent dimensions, sampling strategy for latent vector inference, loss functions, discount and expectile values for Hilbert space calculations, target smoothing, and the instruction network coefficient.", "section": "C Implementation Details of DoDont"}, {"figure_path": "7X5zu6GIuW/tables/tables_19_1.jpg", "caption": "Table 4: Full results on the zero-shot RL performance. The table shows the zero-shot RL performance averaged over four seeds in each setting. We adopted the results from HILP [37]", "description": "This table presents a comprehensive comparison of zero-shot reinforcement learning performance across different algorithms (FDM, FB, HILP, and DoDont) on various locomotion and manipulation tasks.  The results are averaged over four random seeds for each setting, providing statistical robustness. The tasks include running, walking, standing, jumping, and reaching in different directions for both cheetah and quadruped robots, as well as reaching tasks for Jaco. The algorithms are evaluated on two datasets (APS and RND) to assess the generalization performance.", "section": "E.1 Zero-shot RL"}]