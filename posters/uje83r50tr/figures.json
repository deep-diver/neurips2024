[{"figure_path": "ujE83r50tR/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison between prior MLLMs (left) and our Octopus (right). Left: Prior MLLMs typically adopt the purely sequential inference: the LLM head infers the response token-by-token, regardless whether the response token is more aligned with recognition (e.g., detection) or understanding. Sequentially inferring the position is slow. Right: In contrast, Octopus establishes a \"parallel recognition \u2192 sequential understanding\" framework. The bottom LLM layers first recognize potential objects via visual grounding or referring segmentation (in Appendix:B). The recognition results (coupled with visual tokens) are relayed into top LLM layers. The top LLM layers thus do NOT infer the object position but instead, they select boxes (or masks) that have already been detected. The entire Octopus LLM head (recognition + understanding) is trained end-to-end.", "description": "This figure compares the sequential processing in previous Multimodal Large Language Models (MLLMs) with the proposed Octopus model.  Prior MLLMs process visual recognition and understanding sequentially within the LLM head, generating responses token-by-token.  This is slow, especially for tasks needing precise object location.  Octopus, conversely, uses parallel recognition in lower LLM layers (via object queries, a DETR mechanism) before sequentially processing in the upper layers.  The upper layers select previously detected objects, making the overall process much faster.  The entire Octopus LLM head is trained end-to-end.", "section": "1 Introduction"}, {"figure_path": "ujE83r50tR/figures/figures_3_1.jpg", "caption": "Figure 2: The overall training process of Octopus. We omit the visual backbone to highlight the LLM head. Octopus inputs multiple object queries into the LLM head, in addition to the image tokens and text prompt tokens,. After passing through several bottom LLM layers, the object queries are fed into a DETR decoder for parallel recognition. The recognition is not the canonical close-set detection, but can be class-agnostic detection, visual grounding, referring segmentation, etc., depending on the user prompt (as shown in Fig.3). Afterwards, these object queries, coupled with the image token and text tokens, are then sent into the upper LLM layers for sequential understanding. When the users ask for spotting the mentioned objects (e.g., visual grounding), Octopus finds the object query that detects each object (e.g., the 2nd object query detects the cat) and points out this object query via a corresponding index token \u201c<d2>\u201d. The structure of Octopus and its training details are elaborated in Section 3.2 and Section 3.3.", "description": "This figure illustrates the training process of the Octopus model.  It shows how multiple object queries are processed in parallel by the lower LLM layers and a DETR decoder, resulting in object recognition (which can adapt to different tasks like grounding or segmentation based on user input). These results are then fed into the upper LLM layers for sequential understanding.  A key feature is the use of index tokens, such as <d2>, to link the detected objects to the final LLM output.", "section": "3 Approach"}, {"figure_path": "ujE83r50tR/figures/figures_4_1.jpg", "caption": "Figure 1: Comparison between prior MLLMs (left) and our Octopus (right). Left: Prior MLLMs typically adopt the purely sequential inference: the LLM head infers the response token-by-token, regardless whether the response token is more aligned with recognition (e.g., detection) or understanding. Sequentially inferring the position is slow. Right: In contrast, Octopus establishes a \"parallel recognition \u2192 sequential understanding\" framework. The bottom LLM layers first recognize potential objects via visual grounding or referring segmentation (in Appendix:B). The recognition results (coupled with visual tokens) are relayed into top LLM layers. The top LLM layers thus do NOT infer the object position but instead, they select boxes (or masks) that have already been detected. The entire Octopus LLM head (recognition + understanding) is trained end-to-end.", "description": "This figure compares the sequential inference method used by prior Multi-modal Large Language Models (MLLMs) with the parallel recognition and sequential understanding approach of the Octopus model. The left side shows the sequential approach where the LLM head generates the response token-by-token, including object positions.  The right side depicts Octopus's method.  Octopus uses bottom LLM layers for parallel object recognition using object queries.  The recognition results are then sent to the top LLM layers for sequential understanding, eliminating the need to infer object positions again in the top layers. This parallel processing is more efficient, particularly in visual grounding tasks.", "section": "1 Introduction"}, {"figure_path": "ujE83r50tR/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison between prior MLLMs (left) and our Octopus (right). Left: Prior MLLMs typically adopt the purely sequential inference: the LLM head infers the response token-by-token, regardless whether the response token is more aligned with recognition (e.g., detection) or understanding. Sequentially inferring the position is slow. Right: In contrast, Octopus establishes a \u201cparallel recognition \u2192 sequential understanding\u201d framework. The bottom LLM layers first recognize potential objects via visual grounding or referring segmentation (in Appendix:B). The recognition results (coupled with visual tokens) are relayed into top LLM layers. The top LLM layers thus do NOT infer the object position but instead, they select boxes (or masks) that have already been detected. The entire Octopus LLM head (recognition + understanding) is trained end-to-end.", "description": "This figure compares the sequential approach of prior Multimodal Large Language Models (MLLMs) with the proposed Octopus model.  Prior MLLMs process visual recognition and understanding sequentially, token-by-token. Octopus, in contrast, uses parallel recognition (in lower LLM layers) to identify objects and then relays the results to the upper LLM layers for sequential understanding. This parallel approach is faster and more efficient.", "section": "1 Introduction"}, {"figure_path": "ujE83r50tR/figures/figures_13_1.jpg", "caption": "Figure 5: Visualization of the referring segmentation results. Octopus directly makes pixel-wise predictions rather than predicting the vertexes of the segmentation mask.", "description": "This figure demonstrates Octopus's referring segmentation capabilities.  Instead of predicting bounding boxes, Octopus directly predicts pixel-wise segmentation masks, providing a more nuanced and detailed result. The images show example queries and the resulting segmentations.", "section": "B Application on referring segmentation"}]