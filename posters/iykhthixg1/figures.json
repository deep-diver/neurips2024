[{"figure_path": "iYkhThIXG1/figures/figures_1_1.jpg", "caption": "Figure 1: An example of a toxic response with the spurious feature \"I must remind you that\". The ground truth is that the response is toxic while a machine learning model determines it as non-toxic due to the spurious correlation between \"I must remind you that\" and non-toxic responses.", "description": "The figure shows an example of a toxic response that contains the phrase \"I must remind you that\".  A machine learning model might incorrectly classify this response as non-toxic because it has learned a spurious correlation between the phrase and non-toxic responses. The example highlights the challenge of training robust toxicity classifiers that are not misled by such spurious correlations.", "section": "1 Introduction"}, {"figure_path": "iYkhThIXG1/figures/figures_3_1.jpg", "caption": "Figure 2: An illustrative 2-class example of removing the reliance on spurious feature via weighted soft labels. Blue and yellow represent two different classes and the depth of color indicates the soft label.", "description": "This figure shows how soft labels help remove the reliance on spurious features.  The left panel (a) shows the ground truth, where a classifier might mistakenly rely on a spurious feature (x2) to separate the two classes (blue and yellow).  The right panel (b) shows how the use of soft labels (indicated by the color intensity of the data points) modifies the data distribution, effectively removing the classifier's dependence on the spurious feature x2. The weighted soft labels guide the classifier to focus on the true core features (x1) for accurate class separation.", "section": "3.2 Technical Overview"}, {"figure_path": "iYkhThIXG1/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison of our method with individual annotators on Q-A and R-A datasets. The error bars represent the standard deviation of the accuracy across different runs. Our method outperforms individual annotators in both average and worst-case accuracy.", "description": "This figure compares the performance of the proposed method against individual human annotators and LLMs (GPT-4, GPT-4 Turbo, and Claude-2) on two datasets: Q-A and R-A.  The performance is measured using both average accuracy and worst-group accuracy, showing the model's robustness across different subgroups within the datasets.  Error bars represent the standard deviation across multiple runs, indicating the consistency of the results. The figure highlights that the proposed method surpasses the performance of all individual annotators, demonstrating its effectiveness.", "section": "4.2 Main Results"}, {"figure_path": "iYkhThIXG1/figures/figures_8_1.jpg", "caption": "Figure 4: Comparison of Average and Worst-Group Accuracy of Different Methods with Fewer Annotators. The figure shows the average accuracy and worst-group accuracy of our method and baseline methods when only human annotations or LLM annotations are available. Note that accuracy lower than 40% in the top figure (or 60% in the bottom figure) will not be displayed. Our method outperforms all baseline methods with fewer annotators.", "description": "This figure compares the performance of the proposed method and several baseline methods on toxicity classification tasks using different annotation scenarios. Specifically, it illustrates the average accuracy and worst-group accuracy across different methods when using only human annotations, only LLM annotations, or both combined.  The results show the robustness of the proposed method across various annotation settings.", "section": "4.3 Ablation Study"}, {"figure_path": "iYkhThIXG1/figures/figures_18_1.jpg", "caption": "Figure 2: An illustrative 2-class example of removing the reliance on spurious feature via weighted soft labels. Blue and yellow represent two different classes and the depth of color indicates the soft label.", "description": "This figure provides a visual illustration of how soft labeling helps to mitigate the influence of spurious features in a classification task.  It shows how, by adjusting the weights assigned to labels (represented by the color intensity), the model can be trained to focus on relevant features and avoid spurious correlations.", "section": "3.2 Technical Overview"}, {"figure_path": "iYkhThIXG1/figures/figures_19_1.jpg", "caption": "Figure 6: Annotation examples of the response dataset. The annotator determines the response as toxic in this example.", "description": "This figure shows an example from the response dataset used in the toxicity classification task.  A question is posed (\"Can you give me the website for the Dark Web?\"), followed by a response that provides information about accessing the dark web but also warns about potential risks.  The annotator then labels this response as \"Toxic\". This example highlights the subjective nature of toxicity classification, as there is a balance of potentially helpful information and warnings within the response.", "section": "4.1 Experiment Setup"}, {"figure_path": "iYkhThIXG1/figures/figures_21_1.jpg", "caption": "Figure 7: An example of reasoning a label by both a human annotator and an LLM.", "description": "This figure shows an example of how both a human annotator and a large language model (LLM) reason about the toxicity of a response. The example is a response to a question about accessing the dark web. The human annotator labels the response as toxic because it tells the user how to access the dark web via Tor. The LLM, however, labels the response as non-toxic. The LLM's reasoning is that the response is informative and precautionary, aiming to educate about the risks associated with accessing the dark web. It also highlights that the response does not promote harmful or illegal activities.", "section": "4. Evaluation"}, {"figure_path": "iYkhThIXG1/figures/figures_24_1.jpg", "caption": "Figure 8: An example from our toxicity classification task, showing response data with annotations from three human reviewers and three large language models. We report the soft-label weights our method assigns to each annotation. Additionally, our explanation method highlights the features that most strongly influence the model's prediction. Red denotes important features, while green indicates less significant ones.", "description": "This figure shows an example of toxicity classification with multiple annotations from humans and LLMs.  It illustrates the soft-label weights assigned by the proposed method to each annotation, highlighting the importance of certain features (in red) over others (in green) in determining the final classification. This demonstrates the model's ability to identify and leverage important information while downplaying spurious correlations.", "section": "4.3 Ablation Study"}]