[{"type": "text", "text": "Soft-Label Integration for Robust Toxicity Classification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zelei Cheng Northwestern University Evanston, USA zelei.cheng@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Xian Wu Northwestern University Evanston, USA xianwu2024@u.northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Jiahao Yu Northwestern University Evanston, USA jiahao.yu@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Shuo Han Northwestern University Evanston, USA shuo.han.1@u.northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Xin-Qiang Cai   \nThe University of Tokyo   \nTokyo, Japan   \nxinqiang.cai@riken.jp ", "page_idx": 0}, {"type": "text", "text": "Xinyu Xing Northwestern University Evanston, USA xinyu.xing@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Disclaimer. This paper contains uncensored toxic content that might be offensive. ", "page_idx": 0}, {"type": "text", "text": "Toxicity classification in textual content remains a significant problem. Data with labels from a single annotator fall short of capturing the diversity of human perspectives. Therefore, there is a growing need to incorporate crowdsourced annotations for training an effective toxicity classifier. Additionally, the standard approach to training a classifier using empirical risk minimization (ERM) may fail to address the potential shifts between the training set and testing set due to exploiting spurious correlations. This work introduces a novel bi-level optimization framework that integrates crowdsourced annotations with the soft-labeling technique and optimizes the soft-label weights by Group Distributionally Robust Optimization (GroupDRO) to enhance the robustness against out-of-distribution (OOD) risk. We theoretically prove the convergence of our bi-level optimization algorithm. Experimental results demonstrate that our approach outperforms existing baseline methods in terms of both average and worst-group accuracy, confirming its effectiveness in leveraging crowdsourced annotations to achieve more effective and robust toxicity classification. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) are rapidly being adopted in applications such as conversations [1, 2], AI-assisted programming [3], and education [4]. However, despite impressive capabilities, the interaction between humans and LLMs can generate harmful, biased, or factually incorrect content [5, 6]. For example, users may ask LLMs to generate toxic content, such as hate speech, misinformation, or violent threats, which can have severe consequences for individuals and communities. Recent studies on jailbreaking LLMs also show that adversarial prompts can elicit toxic responses from models [7, 6, 8, 9]. Therefore, there is a pressing need to develop a robust toxicity classification model that can effectively identify and mitigate harmful content generated by LLMs. ", "page_idx": 0}, {"type": "image", "img_path": "iYkhThIXG1/tmp/97b70ffbc9488d46b7cab1996dd9ab809baa596eaf72036d51f1e2bce12d98f8.jpg", "img_caption": ["Figure 1: An example of a toxic response with the spurious feature \"I must remind you that\". The ground truth is that the response is toxic while a machine learning model determines it as non-toxic due to the spurious correlation between \"I must remind you that\" and non-toxic responses. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Traditional toxicity classification methods [10\u201313], typically reliant on labels from a single annotator per instance, fall short of capturing the diversity of human perspectives [14]. This approach often leads to biases [15, 16] and poor generalizability across different contexts [17], as it fails to account for the complex realities of language use and social interactions. Thus, there is a growing need to incorporate crowdsourced annotations that reflect a broader array of cultural and linguistic nuances. Additionally, Arjovsky et al. [18] point out that the model trained by empirical risk minimization (ERM) may exploit the spurious correlations that are easier to fti instead of learning the causal components, which suffers from distribution shifts from training to testing domains [19]. When spurious correlations are present, the performance of certain groups of examples can drop significantly. For example, the toxicity classifier might learn to associate certain phrases or contexts (e.g., \u201cI must remind you that\u201d in Figure 1) with non-toxic behavior, despite the overall response being harmful. ", "page_idx": 1}, {"type": "text", "text": "To overcome the above challenges, we propose a bi-level optimization framework that incorporates crowdsourced annotations through soft-labeling techniques to enhance the robustness and reliability of toxicity classification systems. The proposed framework consists of two optimization loops: an inner loop that minimizes the ERM loss on training samples with learned soft labels, and an outer loop that assesses the model\u2019s dependency on spurious features by evaluating the out-of-distribution (OOD) risk and optimizing the soft-label weights accordingly. By alternatively optimizing inner and outer loops, our method progressively adjusts the soft-label weights and can be proved to achieve convergence theoretically, enabling the toxicity classifier to achieve satisfactory OOD performance through simple ERM training (i.e., inner loop optimization). ", "page_idx": 1}, {"type": "text", "text": "Empirically, we evaluate our method on the toxic question classification and response classification datasets provided by a third-party security company and the public HateXplain dataset [20]. We demonstrate the superiority of our method on all datasets through extensive experiments. Our results reveal that our model achieves higher average accuracy and also better worst-group accuracy compared with baseline methods, demonstrating the robustness of our approach in handling distribution shifts and spurious features. Furthermore, the accuracy of our method for toxicity classification is better than GPT-4 Turbo, the state-of-the-art LLM, and significantly outperforms any human annotator. ", "page_idx": 1}, {"type": "text", "text": "By integrating multiple annotations and adopting a robust optimization approach, our study not only advances the technological frontiers of toxicity classification but also contributes to the broader discourse on ethical AI practices, promoting more nuanced and equitable online interactions. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Bi-level Optimization. Bi-level optimization [21] has attracted significant attention due to its ability to handle hierarchical decision-making tasks including meta learning [22\u201326], neural architecture search [27\u201329], sample re-weighting [30, 31, 25], label denoising [32], etc. For example, in metalearning [26], bi-level optimization provides a way to learn the initial parameters of a model which leads to fast adaptation and good generalization for various learning tasks. In this work, we formulate the toxicity classification from multiple annotations as a bi-level optimization problem where we alternate between minimizing the empirical risk minimization (ERM) loss on training samples with learned soft labels and optimizing the soft-label weights against the out-of-distribution (OOD) risk. ", "page_idx": 2}, {"type": "text", "text": "Learning from Partial Labels. Training a classifier from partial labels implicitly requires determining the ground truth from multiple annotations. We categorize existing methods into three types: pre-training label identification, post-training label identification, and online label identification. ", "page_idx": 2}, {"type": "text", "text": "Pre-training label identification. Pre-training label identification refers to the methods that infer ground truth before training the classifier. Some baseline methods such as Majority Voting (MV) [33] and Participant-Mine Voting (PM) [34, 35] directly infer a true label from crowdsourced multiple labels [36], with MV assuming equal annotator quality and PM accounting for worker quality differences. However, both MV and PM assume annotator quality is instance-independent, which is often not the case due to varying cultural and educational backgrounds. Probabilistic models [37\u201339] like Snorkel use statistical dependencies to infer true labels but can be limited by non-independent annotators like GPT-4 and GPT-4 Turbo. ", "page_idx": 2}, {"type": "text", "text": "Post-training Label Identification. This approach involves training models to approximate annotators\u2019 labels and then aggregating these approximations. Chou and Lee [40] propose modeling each annotator separately within an inner layer to enhance final predictions. Similarly, Davani et al. [41] train multiple models to predict each annotator\u2019s label, subsequently applying majority voting to determine the final label. ", "page_idx": 2}, {"type": "text", "text": "Online label identification. Online label identification refers to the methods that disambiguate the candidate labels during the training. There are generally two categories of methods. The first one is average-based methods [42\u201344] which treats each candidate label equally in the model training phase and minimizes the average loss over all candidate labels, assuming equal likelihood for each, which is unrealistic. The second one is identification-based methods which directly maximizes the probability of exactly one candidate label [45\u201347]. Lv et al. [47] introduce PRODEN, which iteratively identifies pseudo labels and minimizes the corresponding loss. PRODEN starts with equal weights for all candidate labels and uses model logits to determine pseudo labels. However, incorrect initial assumptions can lead to local minima. ", "page_idx": 2}, {"type": "text", "text": "Distributionally Robust Optimization. Distributionally robust optimization (DRO) optimizes the worst-case loss in an uncertainty set of test distributions [48\u201352]. Sagawa et al. [48] propose GroupDRO to learn a robust model to minimize the loss of the worst group when the dataset has group annotations. Oren et al. [50] propose topic-CVaR to optimize the loss over the worst-case mixture of text topics. When such group distributions are not available, conditional value at risk (CVaR) [53, 54] constructs new distributions by reweighting the training samples and minimizes the supreme loss over these distributions. In this work, we leverage the GroupDRO technique to learn a robust soft-label weight estimator. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Technique ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Setup and Assumption ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a toxicity classification task with $C$ classes, with a training dataset $\\mathcal D_{t r}:=\\{(\\mathbf X^{(i)},\\tilde{\\mathbf y}_{i})\\}_{i=1}^{n_{t r}}$ . Here, $\\mathbf{X}^{(i)}$ represents the input text, and $\\tilde{\\mathbf{y}}_{i}$ denotes the associated labels annotated by workers or experts. Each text instance in the training set is annotated by $M$ workers, resulting in a set of possible labels $\\tilde{\\mathbf{y}}_{i}:=\\{y_{i}^{j}\\}_{j=1}^{M}$ , where $y_{i}^{j}\\,\\in\\,[C]^{\\'}:=\\,\\{1,2,...,C\\}$ . We assume that the correct ground-truth label is included in $\\Tilde{\\mathbf{y}}_{i}$ . Additionally, a small, clean validation set $\\mathcal{D}_{v}:=\\{(\\mathbf{X}^{(i)},y_{i})\\}_{i=1}^{n_{v}}$ is provided, which is sampled from the same distribution as the training set $\\mathcal{D}_{t r}$ , where $n_{v}\\ll n_{t r}$ . Our objective is to learn a classifier $f$ that effectively predicts the correct labels without relying on irrelevant or spurious features. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 Technical Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall our goal is to train an optimal classifier that does not depend on spurious correlations, a naive approach might involve using existing outof-distribution (OOD) risk loss functions, such as distributionally robust optimization (DRO). However, a significant issue arises from the absence of ground-truth labels in the training set. Training a robust model directly using DRO on the clean validation set could result in limited available data, potentially compromising the overall performance. Considering these, we propose a bi-level formulation to address these challenges. As illustrated in Figure 2, we reduce the classifier $f$ \u2019s dependence on spurious features through soft re-labeling. In this example, we identify $x_{1}$ and $x_{2}$ as the core and spurious features, respectively, and aim to train a classifier that does not rely on the spurious feature $x_{2}$ . ", "page_idx": 3}, {"type": "image", "img_path": "iYkhThIXG1/tmp/44b24119993a1f7ddf351e128a62b9ba27e4c3a684076ba3dd842fb40ccbdf68.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: An illustrative 2-class example of removing the reliance on spurious feature via weighted soft labels. Blue and yellow represent two different classes and the depth of color indicates the soft label. ", "page_idx": 3}, {"type": "text", "text": "Without re-labeling, even if the training set had the ground truths, the classifier would still be biased towards $x_{2}$ . However, by applying soft re-labeling, we adjust the labels for samples in the bottom-left and top-right areas, resulting in an optimal classifier that is oriented vertically, as shown in Figure 2. This adjustment ensures that the newly trained classifier $f$ does not depend on $x_{2}$ . Motivated by these, we formulate the task of learning soft labels to remove the spurious features as a bi-level optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{w}}{\\mathrm{minimize}}\\ \\mathcal{R}(\\mathcal{D}_{v},\\theta^{*}(\\mathbf{w}))\\quad\\mathrm{subject\\,to}\\quad\\theta^{*}(\\mathbf{w})\\in\\arg\\underset{\\theta}{\\mathrm{min}}\\,\\mathcal{L}(\\mathcal{D}_{t r},\\theta;\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where w is the soft-label weight vector which indicates the importance of each annotator. The outer objective function can be any OOD risk loss function (i.e., group DRO loss). In the inner loop, we minimize the empirical risk minimization (ERM) loss (i.e., cross-entropy loss) on training samples with learned soft labels, resulting in a model denoted as $\\boldsymbol{\\theta}^{*}(\\mathbf{w})$ . In the outer loop, we assess the model\u2019s dependency on spurious features by evaluating the OOD risk and optimizing the soft-label weights accordingly. By alternating between the inner and outer loops, the soft-label weights progressively adjust, enabling the achievement of satisfactory OOD performance through simple ERM training. ", "page_idx": 3}, {"type": "text", "text": "3.3 Technical Details ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We design a bi-level optimization process consisting of an inner-loop optimization and an outer-loop optimization to simultaneously update the learned soft-label weight w and model parameters $\\pmb{\\theta}$ . We begin by addressing the parameterization of the soft-label weight function w in Eqn. (1). Although we could parameterize w as an $m$ -dimensional vector, it does not account for the relationship between the feature and label as annotated by the worker. Thus, we capture the weight of annotated labels $\\tilde{\\mathbf{y}}_{i}$ for the sample $\\mathbf{X}^{(i)}$ through a neural network $v_{\\theta}\\,:\\,{\\bf X}^{(i)}\\,\\,\\bar{\\rightarrow}\\,\\,{\\bf v}^{(i)}\\,\\in\\,\\mathbb{R}^{\\bar{m}}$ . After obtaining the normalized soft-label weights $\\bar{\\mathbf{v}}^{(i)}$ through the softmax function, the final soft-label $\\bar{\\mathbf{y}}_{i}$ is determined by taking the weighted sum of the one-hot vectors ${\\bf e}_{i}^{j}$ in the potential label set $\\tilde{\\mathbf{y}}_{i}$ , where the weights are explicitly provided by $\\mathbf{v}^{(i)}$ . With the soft-labels computed, we can now turn to the outer-level optimization. Motivated by [55], we initiate by pseudo-updating the parameter vector $\\pmb{\\theta}$ , thereby establishing a relationship between w and the optimized parameters $\\theta^{\\prime}$ . Specifically, $\\theta^{\\prime}$ approximate ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 The bi-level optimization algorithm for training the toxicity classifier. ", "page_idx": 4}, {"type": "text", "text": "Input: Training dataset $\\mathcal D_{t r}:=\\{(\\mathbf X^{(i)},\\tilde{\\mathbf y}_{i})\\}_{i=1}^{n_{t r}}$ , validation dataset $\\mathcal{D}_{v a l}:=\\{(\\mathbf{X}^{(i)},y_{i})\\}_{i=1}^{n_{v}}$ , max   \nnumber of steps $T$   \nOutput: Toxicity classifier $f_{\\theta}$   \nInitialization: Initialize the soft-label weights ${\\bf w}_{0}$ and the classifier parameter $\\pmb{\\theta}_{0}$   \nfor $t=1,2,\\ldots,T$ do Sample batch data $\\{\\mathbf{X},\\tilde{\\mathbf{y}}\\}$ from the training dataset $\\mathcal{D}_{t r}$ Sample batch data $\\{\\mathbf{X},y\\}$ from the validation dataset $\\mathcal{D}_{v a l}$ Pseudo update $\\pmb{\\theta}_{t+1}^{\\prime}$ as Eqn. (2) and update the soft-label weights $\\mathbf{w}_{t+1}$ as Eqn. (3) Update $\\pmb{\\theta}_{t+1}$ as Eqn. (4)   \nend for ", "page_idx": 4}, {"type": "text", "text": "$\\boldsymbol{\\theta}^{*}(\\mathbf{w})$ through one-step inner loop gradient descent. We then update w to make the induced $\\theta^{\\prime}$ minimize the outer loss $\\mathcal{R}$ . Regarding the inner-loop optimization, $\\pmb{\\theta}$ is directly updated to minimize $\\mathcal{L}$ . We provide the full algorithm in Algorithm 1. Detailed explanations of the optimization process are provided below. ", "page_idx": 4}, {"type": "text", "text": "Outer-loop optimization: Updating w. Denote ${\\bf w}_{t}$ be the soft-label weights at time step $t$ . Given the weights ${\\bf w}_{t}$ , we first pseudo update the parameter $\\pmb{\\theta}_{t}$ via one-step gradient descent and obtain $\\pmb{\\theta}_{t+1}^{\\prime}$ . Please note that we do not intend to actually update the parameter $\\pmb{\\theta}$ but only save the gradients during the pseudo update for further gradient computation of ${\\bf w}_{t}$ . Mathematically, the pseudo update of $\\pmb{\\theta}_{t}$ can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}^{\\prime}=\\pmb{\\theta}_{t}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}(\\pmb{\\theta}_{t};\\mathbf{w}_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mu$ is the step size for updating $\\pmb{\\theta}$ . After computing $\\pmb{\\theta}_{t+1}^{\\prime}$ , we use the following formula to update w via gradient descent: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\alpha\\nabla_{\\mathbf{w}}\\mathcal{R}(\\pmb{\\theta}_{t+1}^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha$ is the step size for updating w. The OOD risk function $\\mathcal{R}$ is a GroupDRO loss computed in the validation set. Mathematically, $\\begin{array}{r}{\\mathcal{R}(\\pmb{\\theta})=\\operatorname*{max}_{g\\in\\mathcal{G}}\\mathbb{E}_{(x,y)\\sim P_{g}}{\\left[\\ell(\\pmb{\\theta};(x,y))\\right]}}\\end{array}$ where $\\mathcal{G}$ denotes the set of all groups, $P_{g}$ denotes the data distribution within the group $g$ , and $l$ is the cross-entropy loss. ", "page_idx": 4}, {"type": "text", "text": "Inner-loop optimization: Updating $\\pmb{\\theta}$ . Once we have the soft-label weights $\\mathbf{w}_{t}$ , we can update the parameter $\\pmb{\\theta}$ via single-step optimization as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{L}{=-}\\mathbb{E}_{(\\mathbf{X}^{(i)},\\tilde{\\mathbf{y}}_{i})\\sim\\mathcal{D}_{t r}}\\!\\left[\\sum_{c=1}^{C}\\bar{y}_{i c}\\log f_{c}(\\mathbf{X}^{(i)};\\pmb{\\theta})\\right]}\\end{array}$ . $f_{c}$ represents the probability of the $c$ -th class of $f(.)$ that is determined as the true label. $\\bar{y}_{i c}$ is the $c$ -th element of the soft label ${\\bar{y}}_{i}$ where the soft label is a weighted aggregation over $M$ one-hot vectors of annotations, i.e., $\\bar{y}_{i}=\\mathbf{v}^{(i)}[\\mathbf{e}_{i}^{1},\\ldots,\\mathbf{e}_{i}^{M}]^{T}$ . ", "page_idx": 4}, {"type": "text", "text": "3.4 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Finally, we can prove the convergence of our bi-level optimization algorithm under moderate assumptions. The convergence analysis follows from a similar idea as the proof in [56]. We first introduce the following necessary assumptions. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1 (Smoothness of $\\mathcal{R}$ ). The OOD risk function $\\mathcal{R}$ is Lipschitz-smooth with a constant $L$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1 is a common assumption in the analysis of bi-level optimization [56, 57, 24, 58].   \nAdditionally, we assume that the gradients of $\\mathcal{L},\\mathcal{R}$ and their inner product are bounded. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2 (Lower bound of the inner product of the gradients). We assume that the following inequality holds with some constant $k$ for every time step $t$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}(\\pmb{\\theta}_{t+1}^{\\prime})^{T}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1})\\geq k\\|\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1})\\|^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 3.3 (Bounded gradients of $\\mathcal{L}$ and $\\mathcal{R}$ ). The gradients of $\\mathcal{L}$ and $\\mathcal{R}$ are bounded by $\\sigma$ .   \n$\\nabla_{\\mathbf{w}}\\nabla_{\\theta}\\mathcal{L}(\\mathbf{\\theta};\\mathbf{w})$ is bounded by $\\sigma^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "Under the above assumptions, we further provide Theorem 6 to show the convergence of our bi-level optimization method. The proof of Theorem 3.4 can be found in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4 (Convergence). Under Assumption 3.1 and Assumption 3.2 and setting the step size $\\mu\\,\\leq\\,{\\frac{2k}{L}}$ , our bi-level optimization algorithm can ensure that the risk function $\\mathcal{R}$ monotonically decreases with respect to the time step $t$ , i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\pmb{\\theta}_{t+1})\\leq\\mathcal{R}(\\pmb{\\theta}_{t})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The equality in Eqn. (6) holds if the gradient of the risk function $\\mathcal{R}$ with respect to w becomes 0 at some time step $t_{\\mathrm{:}}$ , i.e., $\\nabla_{w}\\mathcal{R}(\\pmb{\\theta}_{t})=0$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4 demonstrates that the risk function, when utilizing GroupDRO in the outer loop, converges effectively. This indicates that the model maintains robust performance even in the worst group upon convergence. Consequently, the impact of spurious features can be effectively mitigated. Additionally, we prove the convergence rate of our bi-level optimization method as $\\dot{O}(\\frac{1}{\\epsilon^{2}})$ . The details of the proof are in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 (Convergence rate). Let the total number of training steps as $T$ and set the step size $\\begin{array}{r}{\\alpha\\,=\\,\\frac{k_{1}}{\\sqrt{T}}}\\end{array}$ for some constant $k_{1}$ where $\\begin{array}{r}{0\\ <\\ k_{1}\\ <\\ \\frac{2}{L}}\\end{array}$ and $\\textstyle\\mu\\,=\\,{\\frac{k_{2}}{T}}$ for some constant $k_{2}$ . Under Assumption 3.1 and Assumption 3.3, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1\\leq t\\leq T}\\mathbb{E}\\left[\\|\\nabla_{w}\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)\\|_{2}^{2}\\right]\\leq O\\left(\\frac{1}{\\sqrt{T}}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 implies that if we want $\\begin{array}{r}{\\operatorname*{min}_{1\\leq t\\leq T}\\mathbb{E}\\left[\\left\\|\\nabla_{w}\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)\\right\\|_{2}^{2}\\right]\\leq\\epsilon}\\end{array}$ , we have to train $\\begin{array}{r}{O\\big(\\frac{1}{\\epsilon^{2}}\\big)}\\end{array}$ steps. Furthermore, as the training step increases, the gradient of the risk function with respect to w is gradually close to 0. If the risk function $\\mathcal{R}$ is convex with respect to $\\mathbf{w}$ , it essentially means that w gradually converges to the optimal $\\mathbf{w}^{*}$ that minimizes the risk function. ", "page_idx": 5}, {"type": "text", "text": "4 Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we start with the experimental setup, including the datasets, baselines, and metrics. We then present the results of our experiments, which evaluate the effectiveness of our proposed method against baseline methods. Finally, we conduct an ablation study to compare the performance of our method with alternative design choices. We release the data and code in https://github. com/chengzelei/crowdsource_toxicity_classification. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experiment Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We obtain the toxic question and response datasets from a third-party security company. The toxic question dataset is classified into 15 categories based on the OpenAI usage policy retrieved in 2023 as shown in Table 4. The response classification task is a binary classification problem, where the responses are labeled as toxic or non-toxic. Each data point is associated with three human annotations and three LLM-generated annotations (GPT-4, GPT-4 Turbo, and Claude-2). To better reflect the real-world scenario where the source or the number of annotators is limited, we have six datasets: Q-H, Q-L, Q-A, R-H, R-L, and R-A, where Q-H and R-H are annotated by humans, Q-L and R-L are annotated by LLMs, and Q-A and R-A are annotated by all annotators. ", "page_idx": 5}, {"type": "text", "text": "For each classification task, we have a large training set with crowdsourced annotations (i.e., 6,941 samples for toxic question classification and 28,194 samples for toxic response classification) and a testing set containing 2,000 samples with ground truth. The validation set with ground truth includes a small number of samples (i.e., 1,000 samples) from the training set. Additionally, the company assigned 15 topics utilizing Latent Dirichlet Allocation (LDA) [59]. We further construct the groups based on both topics and true labels. The details of the groups can be found in Appendix C.2. ", "page_idx": 5}, {"type": "text", "text": "In addition, we conduct our experiments on the public HateXplain dataset [20]. It contains three classes \u2013 \u201chatespeech\u201d, \u201coffensive\u201d, \u201cnormal\u201d. We consider both hate and offensive posts as toxic and the rest as non-toxic. Each record includes a post and three human annotations. The true labels are determined as the majority vote of three human annotations following [60]. We further utilize ", "page_idx": 5}, {"type": "text", "text": "GPT-4, GPT-4 Turbo, and Claude 2 to label these comments. We assign 15 topics utilizing LDA and further construct the groups based on both topics and true labels. ", "page_idx": 6}, {"type": "text", "text": "Baseline methods. Besides the six individual annotations, we compare our method with the following baseline methods \u2013\u2013 $\\textcircled{1}$ Pre-training label identification: This method involves generating true labels for supervised learning through three approaches. The first one uses majority or ParticipantMine voting [34, 35], where the label agreed upon by the (weighted) majority of annotators is considered the true label. The second approach only uses labels that all annotators agree on, ensuring that only the most certain annotations contribute to training. The third approach \u201cSnorkel\u201d [38] constructs a probabilistic graph model to learn the correlation between different annotations and infer the true label. $\\circledcirc$ Post-training identification: This approach trains an ensemble of models, where each model is trained to estimate each annotator\u2019s labels [41]. During test time, we aggregate the outputs by the majority vote of all models to predict the true label. $\\circled{3}$ Online label identification: This approach utilizes techniques from semi-supervised learning where all possible labels selected by annotators are considered. We employ methods such as the average-label learning framework [42\u201344] which minimizes the average loss over all potential labels, and PRODEN [47], which optimizes the loss with respect to the progressively identified ground truth. $\\circledast$ Soft-label Learning: This approach assigns different weights to the losses with respect to each unique candidate label selected by annotators. We consider the vanilla soft-label learning method as a baseline that directly counts the number of votes as the soft-label weights without modeling the reliability of each annotator. ", "page_idx": 6}, {"type": "text", "text": "Metrics. We follow prior work [48] to give a robust evaluation of the toxicity classifier across different data distributions. We evaluate the toxicity classifier\u2019s performance on each group, calculating the classification accuracy for each group. We report two key metrics: Average Accuracy, which is the mean accuracy across all groups, providing a general measure of model performance; and Worst-Group Accuracy, which highlights the lowest accuracy observed among all groups, underscoring the model\u2019s performance in the most challenging scenarios. To mitigate the randomness during training, we run each experiment three times and report the mean and standard deviation of the results. ", "page_idx": 6}, {"type": "text", "text": "Implementation. We implement the proposed method using PyTorch. We train the machine learning models on a server with 8 NVIDIA A100 80GB GPUs and 4TB memory for all the learning algorithms. The toxicity classifier is based on \u201cRoBERTa-large\u201d infrastructure and the soft-label weight estimator is based on \u201cRoBERTa-base\u201d infrastructure. We list the hyper-parameter settings for all experiments in Appendix C.3. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Compare with baseline methods. In Table 1, we show the average accuracy and worst-group accuracy of our method and the baseline methods on the datasets from the third-party security company. As shown in the table, our method outperforms all baseline methods in terms of both average accuracy and worst-group accuracy across two classification tasks. Baseline methods do not consider the out-of-distribution risk and therefore show worse performance regarding worst-group accuracy. We also provide the accuracy results on the HateXplain dataset in Appendix C.4. These results demonstrate the effectiveness of our method in learning from multiple annotators with soft labeling to improve the toxicity classifier\u2019s performance and eliminate the out-of-distribution risk with GroupDRO. ", "page_idx": 6}, {"type": "text", "text": "Compare with human and proprietary LLM annotations. We compare the classification performance of our method with human and proprietary LLM labeling in Figure 3. The results show that our method achieves outstanding performance in both question and response classification tasks. The accuracy of our method for question classification is comparable to GPT-4 Turbo, the state-of-the-art LLM, and significantly outperforms any human annotator. For response classification, our method surpasses all annotations, including GPT-4 Turbo, by a large margin. Considering the high cost of GPT-4 Turbo labeling, our method provides a cost-effective and scalable solution for toxicity classification tasks. ", "page_idx": 6}, {"type": "text", "text": "Time complexity comparison with baseline methods. We measure the time complexity of all methods across all datasets and report the results in Appendix C.5. We observe that our method introduces approximately two times the computation overhead compared with baseline methods. The additional computation overhead originates from the pseudo-update of the model parameter $\\pmb{\\theta}$ and the update of the soft-label weights w. Note that we utilize a smaller model (i.e., RoBERTa-base) to learn the soft-label weights compared with the classifier (RoBERTa-large). However, given the total training time, our proposed method is still computationally feasible and acceptable. ", "page_idx": 6}, {"type": "table", "img_path": "iYkhThIXG1/tmp/bf14618b4d7c6099fcf73c1ab3fa79340db3562921991d53c288f4bbdc5ae304.jpg", "table_caption": ["Table 1: Comparison of Average and Worst-Group Accuracy Across Different Baseline Methods for Toxicity Classification. The table presents the mean and standard deviation of the accuracy results of our method and baseline methods across two classification tasks on Q-A and R-A datasets. Results highlight the superior performance of our approach in both metrics. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "iYkhThIXG1/tmp/854cbf5f33d1677c145c1d3867f59e5db97734a9f53561c908eeab89c0f97a2e.jpg", "img_caption": ["Figure 3: Comparison of our method with individual annotators on Q-A and R-A datasets. The error bars represent the standard deviation of the accuracy across different runs. Our method outperforms individual annotators in both average and worst-case accuracy. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct an ablation study to demonstrate the superiority of our design with alternative designs and compare the performance of our method with fewer annotators. ", "page_idx": 7}, {"type": "text", "text": "Learning with fewer annotators. We assess the performance of our method with fewer annotators and compare it with other methods in Figure 4. The figure first shows that our method still outperforms all baseline methods in two classification tasks in terms of both average accuracy and worst-group accuracy when only human annotations or LLM annotations are available. This demonstrates that our method is robust and effective in learning from fewer annotators, providing a cost-effective solution for toxicity classification tasks. ", "page_idx": 7}, {"type": "text", "text": "We also observe that the annotation quality of LLMs and humans varies for different tasks. For instance, LLM annotations yield generally better results than human annotations for the question classification task, while the opposite is true for the response classification task. This finding aligns with the result in Figure 3. Thus, baseline methods may be particularly sensitive to the quality of the annotators. Specifically, for the response classification task, the classification performance of baselines is much lower when all annotators are present compared to when only human annotators are present. In contrast, our method not only maintains but improves its accuracy when all annotators are included, underscoring its ability to handle variable annotation quality effectively. Moreover, our approach demonstrates robustness against different data distributions in the testing set, achieving over $70\\%$ accuracy in the worst group for the response classification task where no baseline method exceeds $60\\%$ . ", "page_idx": 7}, {"type": "image", "img_path": "iYkhThIXG1/tmp/65ababf54e17e3f7448007bd9c5e0c5df3854583e7c5f0a6383e00cbb6eb3f7c.jpg", "img_caption": ["Figure 4: Comparison of Average and Worst-Group Accuracy of Different Methods with Fewer Annotators. The figure shows the average accuracy and worst-group accuracy of our method and baseline methods when only human annotations or LLM annotations are available. Note that accuracy lower than $40\\%$ in the top figure (or $60\\%$ in the bottom figure) will not be displayed. Our method outperforms all baseline methods with fewer annotators. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "iYkhThIXG1/tmp/3849aea1be3bc97bf26ea9045a023fc5a36b8dbbf6823fd74a8aa159cf5e858d.jpg", "table_caption": ["Table 2: Comparison of Average and Worst-Group Accuracy Across Different Baseline Methods with GroupDRO for Toxicity Classification. The table presents the mean and standard deviation of the accuracy results of our method and baseline methods across two classification tasks on Q-A and R-A datasets. Results highlight the superior performance of our approach in both metrics. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Baseline methods with GroupDRO. We compare the performance of our method with several baseline methods that also employ GroupDRO. We add an additional baseline of ERM with Group DRO which trains a toxicity classifier based on the validation set. Note that GroupDRO requires true labels to assign groups which is only applicable to pre-training label identification methods. As detailed in Table 2, we have two observations. First, our method still outperforms the baseline methods with GroupDRO in terms of both average and worst-group accuracy. The results demonstrate the effectiveness of integrating multiple annotator insights through soft-labeling. Second, compared with Table 1, the performance of baseline methods with GroupDRO is generally better than naive baseline methods which confirms the impact of out-of-distribution risk in our tasks. ", "page_idx": 8}, {"type": "text", "text": "Alternative design - our method with CVaR DRO. We investigate an alternative design of our method which incorporates the CVaR DRO technique [54] to address the out-of-distribution risk without prior knowledge of groups. We compare the performance of our method with the alternative design in Table 3. The results show that while CVaR DRO targets extreme risks in distributions, it underperforms compared to GroupDRO. This finding highlights GroupDRO\u2019s capability in utilizing group-specific information to optimize performance, demonstrating its effectiveness in addressing the real-world toxicity classification problem. ", "page_idx": 8}, {"type": "table", "img_path": "iYkhThIXG1/tmp/5d5732fbd0b7a5fbe6c97f5e9fc317ff516c4b9b5bd2b6c517b6e694ef2e714e.jpg", "table_caption": ["Table 3: Comparison of Average and Worst-Group Accuracy with Alternative Design (CVaR DRO) for Toxicity Classification. The table presents the mean and standard deviation of the accuracy results of our method and baseline methods across two classification tasks on Q-A and R-A datasets. Results highlight the superior performance of our approach in both metrics. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce a novel bi-level optimization framework that incorporates soft-labeling techniques alongside GroupDRO to tackle the OOD risk of toxicity classification with crowdsourced annotations. By leveraging multi-source annotations, our approach captures a broader spectrum of the annotator\u2019s judgment, enhancing the system\u2019s ability to handle the inherent ambiguities in defining toxic content. We present a theoretical analysis of convergence and demonstrate its superior performance over toxic question and response datasets. We hope that our work will inspire further research in developing ethically aware and technically robust AI-driven moderation tools. ", "page_idx": 9}, {"type": "text", "text": "Our work suggests several promising directions for future research. First, it would be interesting to investigate the extension of our toxicity classification framework to multi-modal contents, where toxicity may manifest not just in text but through images, videos, and their combinations, presenting unique challenges and requiring novel adaptation strategies. Second, while our model leverages annotations from multiple sources to enhance the accuracy of toxicity classification, it remains dependent on the quality and representativeness of these annotations. Future research could focus on improving the fairness of our model by continuously monitoring for and mitigating inherent biases in annotator perspectives. This would involve regular audits, updates to training data, and adjustments to model parameters to bolster both the effectiveness and fairness of the system. Finally, the versatility of our framework could extend beyond toxicity classification to other large language model safety applications, such as LLM alignment through reinforcement learning from feedback (RLHF). In RLHF, human annotators provide pairwise feedback for LLM responses, which can be noisy. Our bi-level optimization framework could be adapted to assess the quality of this feedback and select the most reliable inputs for fine-tuning LLMs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This project was supported in part by NSF Grants 2225234 and 2225225. The third-party dataset was provided by Sec3 Inc. and we release the dataset under the agreement of Sec3 Inc. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yang Bai, Ge Pei, Jindong Gu, Yong Yang, and Xingjun Ma. Special characters attack: Toward scalable training data extraction from large language models. arXiv preprint arXiv:2405.05990, 2024.   \n[2] Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip Torr, Zhifeng Li, and Wei Liu. Inducing high energy-latency of large vision-language models with verbose images. In Proc. of ICLR, 2024.   \n[3] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Proc. of CHI (extended abstracts), 2022.   \n[4] Silvia Milano, Joshua A McGrane, and Sabina Leonelli. Large language models challenge the future of higher education. Nature Machine Intelligence, 2023.   \n[5] Maribeth Rauh, John Mellor, Jonathan Uesato, Po-Sen Huang, Johannes Welbl, Laura Weidinger, Sumanth Dathathri, Amelia Glaese, Geoffrey Irving, Iason Gabriel, et al. Characteristics of harmful text: Towards rigorous benchmarking of language models. In Proc. of NeurIPS, 2022.   \n[6] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715, 2023.   \n[7] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. GPTFuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253, 2023.   \n[8] Jiahao Yu, Haozheng Luo, Jerry Yao-Chieh, Wenbo Guo, Han Liu, and Xinyu Xing. Enhancing jailbreak attack against large language models through silent tokens. arXiv preprint arXiv:2405.20653, 2024.   \n[9] Kuofeng Gao, Yang Bai, Jiawang Bai, Yong Yang, and Shu-Tao Xia. Adversarial robustness for visual grounding of multimodal large language models. arXiv preprint arXiv:2405.09981, 2024.   \n[10] Navoneel Chakrabarty. A machine learning approach to comment toxicity classification. In Proc. of CIPR, 2020.   \n[11] Joshua K Lee, Yuheng Bu, Deepta Rajan, Prasanna Sattigeri, Rameswar Panda, Subhro Das, and Gregory W Wornell. Fair selective classification via sufficiency. In Proc. of ICML, 2021.   \n[12] Cicero dos Santos, Igor Melnyk, and Inkit Padhi. Fighting offensive language on social media with unsupervised text style transfer. In Proc. of ACL, 2018.   \n[13] Robert Adragna, Elliot Creager, David Madras, and Richard Zemel. Fairness and robustness in invariant learning: A case study in toxicity classification. arXiv preprint arXiv:2011.06485, 2020.   \n[14] Nitesh Goyal, Ian D Kivlichan, Rachel Rosen, and Lucy Vasserman. Is your toxicity my toxicity? exploring the impact of rater identity on toxicity annotation. In Proc. of CSCW, 2022.   \n[15] David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, and Eric Smith. Robbie: Robust bias evaluation of large generative language models. In Proc. of EMNLP, 2023.   \n[16] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating unintended bias in text classification. In Proc. of AIES, 2018.   \n[17] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In Proc. of ICML, 2021.   \n[18] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[19] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In Proc. of ICLR, 2020.   \n[20] Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. Hatexplain: A benchmark dataset for explainable hate speech detection. In Proc. of AAAI, 2021.   \n[21] Jerome Bracken and James T McGill. Mathematical programs with optimization problems in the constraints. Operations research, 1973.   \n[22] Xiaorong Qin, Xinhang Song, and Shuqiang Jiang. Bi-level meta-learning for few-shot domain generalization. In Proc. of CVPR, 2023.   \n[23] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.   \n[24] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In Proc. of ICML, 2021.   \n[25] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Metaweight-net: Learning an explicit mapping for sample weighting. In Proc. of NeurIPS, 2019.   \n[26] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. In Proc. of NeurIPS, 2019.   \n[27] Chao Xue, Xiaoxing Wang, Junchi Yan, Yonggang Hu, Xiaokang Yang, and Kewei Sun. Rethinking bi-level optimization in neural architecture search: A gibbs sampling perspective. In Proc. of AAAI, 2021.   \n[28] Jiequan Cui, Pengguang Chen, Ruiyu Li, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast and practical neural architecture search. In Proc. of ICCV, 2019.   \n[29] Pengfei Hou, Ying Jin, and Yukang Chen. Single-darts: Towards stable architecture search. In Proc. of ICCV, 2021.   \n[30] Xiao Zhou, Yong Lin, Renjie Pi, Weizhong Zhang, Renzhe Xu, Peng Cui, and Tong Zhang. Model agnostic sample reweighting for out-of-distribution learning. In Proc. of ICML, 2022.   \n[31] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In Proc. of ICML, 2018.   \n[32] Guoqing Zheng, Ahmed Hassan Awadallah, and Susan Dumais. Meta label correction for noisy label learning. In Proc. of AAAI, 2021.   \n[33] Michael J Franklin, Donald Kossmann, Tim Kraska, Sukriti Ramesh, and Reynold Xin. Crowddb: answering queries with crowdsourcing. In Proc. of SIGMOD, 2011.   \n[34] Bahadir Aydin, Yavuz Selim Yilmaz Yavuz Selim Yilmaz, Yaliang Li, Qi Li, Jing Gao, and Murat Demirbas. Crowdsourcing for multiple-choice question answering. In Proc. of AAAI, 2014.   \n[35] Qi Li, Yaliang Li, Jing Gao, Bo Zhao, Wei Fan, and Jiawei Han. Resolving conflicts in heterogeneous data by truth discovery and source reliability estimation. In Proc. of ACM SIGMOD, 2014.   \n[36] Yudian Zheng, Guoliang Li, Yuanbing Li, Caihua Shan, and Reynold Cheng. Truth inference in crowdsourcing: Is the problem solved? In Proc. of the VLDB Endowment, 2017.   \n[37] Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and Christopher R\u00e9. Training complex models with multi-task weak supervision. In Proc. of AAAI, 2019.   \n[38] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R\u00e9. Snorkel: Rapid training data creation with weak supervision. In Proc. of VLDB Endowment, 2017.   \n[39] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R\u00e9. Data programming: Creating large training sets, quickly. In Proc. of NeurIPS, 2016.   \n[40] Huang-Cheng Chou and Chi-Chun Lee. Every rating matters: Joint learning of subjective labels and individual annotators for speech emotion classification. In Proc. of ICASSP, 2019.   \n[41] Aida Mostafazadeh Davani, Mark D\u00edaz, and Vinodkumar Prabhakaran. Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics, 2022.   \n[42] Rong Jin and Zoubin Ghahramani. Learning with multiple labels. In Proc. of NeurIPS, 2002.   \n[43] Timothee Cour, Ben Sapp, and Ben Taskar. Learning from partial labels. The Journal of Machine Learning Research, 12:1501\u20131536, 2011.   \n[44] Min-Ling Zhang and Fei Yu. Solving the partial label learning problem: An instance-based approach. In Proc. of IJCAI, 2015.   \n[45] Hongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and Zhouchen Lin. Leveraged weighted loss for partial label learning. In Proc. of ICML, 2021.   \n[46] Lei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, and Masashi Sugiyama. Provably consistent partial-label learning. In Proc. of NeurIPS, 2020.   \n[47] Jiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. Progressive identification of true labels for partial-label learning. In Proc. of ICML, 2020.   \n[48] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In Proc. of ICLR, 2019.   \n[49] Kenji Kawaguchi and Haihao Lu. Ordered sgd: A new stochastic optimization framework for empirical risk minimization. In Proc. of AISTATS, 2020.   \n[50] Yonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and Percy Liang. Distributionally robust language modeling. In Proc. of EMNLP-IJCNLP, 2019.   \n[51] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 2013.   \n[52] John C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized empirical likelihood approach. Mathematics of Operations Research, 2021.   \n[53] R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal of risk, 2000.   \n[54] Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally robust optimization. In Proc. of NeurIPS, 2020.   \n[55] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proc. of ICML, 2017.   \n[56] Lan-Zhe Guo, Zhen-Yu Zhang, Yuan Jiang, Yu-Feng Li, and Zhi-Hua Zhou. Safe deep semisupervised learning for unseen-class unlabeled data. In Proc. of ICML, 2020.   \n[57] Risheng Liu, Yaohua Liu, Wei Yao, Shangzhi Zeng, and Jin Zhang. Averaged method of multipliers for bi-level optimization without lower-level strong convexity. In Proc. of ICML, 2023.   \n[58] Daouda Sow, Kaiyi Ji, and Yingbin Liang. On the convergence theory for hessian-free bilevel algorithms. In Proc. of NeurIPS, 2022.   \n[59] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 2003.   \n[60] Xinlei He, Savvas Zannettou, Yun Shen, and Yang Zhang. You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content. In Proc. of IEEE S&P, 2024.   \n[61] Houshang H Sohrab. Basic real analysis, volume 231. Springer, 2003.   \n[62] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proc. of EMNLP, 2020.   \n[63] Shuoyang Ding and Philipp Koehn. Evaluating saliency methods for neural language models. In Proc. of NAACL, 2021.   \n[64] Zelei Cheng, Xian Wu, Jiahao Yu, Wenhai Sun, Wenbo Guo, and Xinyu Xing. Statemask: Explaining deep reinforcement learning through state mask. In Proc. of NeurIPS, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First, we provide the following lemma to demonstrate the property of Lipschitz-smoothness. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 ([61]). If function $g(x)$ is Lipschitz-smooth with a constant $L$ , then we have the following inequality: ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(x_{2})\\leq g(x_{1})+\\nabla g(x_{1})^{T}(x_{2}-x_{1})+\\frac{L}{2}\\|x_{2}-x_{1}\\|^{2},\\quad\\forall x_{1},x_{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Let\u2019s define a function $h(t)$ as $h(t)=g(x_{1}+t(x_{2}-x_{1}))$ where $0\\le t\\le1$ . The first-order derivative of $h(t)$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\nh^{\\prime}(t)=\\nabla g(x_{1}+t(x_{2}-x_{1}))^{T}(x_{2}-x_{1})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $g(x)$ is Lipschitz-smooth with constant $L$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad h^{\\prime}(t)-h^{\\prime}(0)}\\\\ &{=(\\nabla g(x_{1}+t(x_{2}-x_{1}))-\\nabla g(x_{1}))^{T}(x_{2}-x_{1})}\\\\ &{=\\cfrac{1}{t}(\\nabla g(x_{1}+t(x_{2}-x_{1}))-\\nabla g(x_{1}))^{T}(t x_{1}+t x_{2})}\\\\ &{=\\cfrac{1}{t}(\\nabla g(x_{1}+t(x_{2}-x_{1}))-\\nabla g(x_{1}))^{T}((x_{1}+t(x_{2}-x_{1}))-x_{1})}\\\\ &{\\leq\\cfrac{L}{t}\\Vert(x_{1}+t(x_{2}-x_{1}))-x_{1})\\Vert^{2}}\\\\ &{=\\cfrac{L}{t}\\Vert t(x_{2}-x_{1})\\Vert^{2}}\\\\ &{=t L\\Vert x_{2}-x_{1}\\Vert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $\\begin{array}{r}{g(x_{2})=h(1)=h(0)+\\int_{0}^{1}h^{\\prime}(t)d t}\\end{array}$ and $g(x_{1})=h(0)$ . Given that $h^{\\prime}(t)\\leq h^{\\prime}(0)\\!+\\!t L\\|x_{2}-$ $x_{1}\\|^{2}$ , we further have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{g(x_{2})=h(1)=h(0)+\\displaystyle\\int_{0}^{1}h^{\\prime}(t)d t}\\\\ {\\displaystyle\\leq h(0)+\\displaystyle\\int_{0}^{1}[h^{\\prime}(0)+t L\\|x_{2}-x_{1}\\|^{2}]d t}\\\\ {\\displaystyle=h(0)+\\displaystyle\\left[h^{\\prime}(0)t+\\displaystyle\\frac{L t^{2}}{2}\\|x_{2}-x_{1}\\|^{2}\\right]_{0}^{1}}\\\\ {\\displaystyle=h(0)+h^{\\prime}(0)+\\displaystyle\\frac{L}{2}\\|x_{2}-x_{1}\\|^{2}}\\\\ {\\displaystyle=g(x_{1})+\\nabla g(x_{1})^{T}(x_{2}-x_{1})+\\displaystyle\\frac{L}{2}\\|x_{2}-x_{1}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We can now prove the convergence in Theorem 3.4. ", "page_idx": 14}, {"type": "text", "text": "Proof. Given the assumption that $\\mathcal{R}$ is Lipschitz-smooth with a constant $L$ , following Lemma 1, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(\\pmb{\\theta}_{t+1}\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)\\leq\\nabla_{\\pmb{\\theta}}\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)^{T}\\left(\\pmb{\\theta}_{t+1}-\\pmb{\\theta}_{t}\\right)+\\frac{L}{2}\\left\\|\\left(\\pmb{\\theta}_{t+1}-\\pmb{\\theta}_{t}\\right)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall that the update rule in Eqn. (4) tells us $\\pmb{\\theta}_{t+1}-\\pmb{\\theta}_{t}=-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1})$ . Inserting in Eqn. (12), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(\\pmb{\\theta}_{t+1}\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)\\leq-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{R}\\left(\\pmb{\\theta}_{t+1}\\right)^{T}\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1}\\right)+\\frac{L\\mu^{2}}{2}\\left\\Vert\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1}\\right)\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Under Assumption 3.2, we further have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(\\pmb{\\theta}_{t+1}\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)\\leq-\\mu k\\left\\Vert\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1}\\right)\\right\\Vert^{2}-\\frac{L\\mu^{2}}{2}\\left\\Vert\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1}\\right)\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If we set the step size $\\begin{array}{r}{\\mu\\leq\\frac{2k}{L}}\\end{array}$ , we can ensure that $\\mathcal{R}\\left(\\pmb{\\theta}_{t+1}\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)\\leq0$ . ", "page_idx": 15}, {"type": "text", "text": "Additionally, if $\\nabla_{w}\\mathcal{R}(\\pmb{\\theta}_{t})=0$ , it implies that the algorithm converges and $\\mathcal{R}(\\pmb{\\theta}_{t+1})=\\mathcal{R}(\\pmb{\\theta}_{t})$ . ", "page_idx": 15}, {"type": "text", "text": "B Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Based on the update rule of $\\pmb{\\theta}$ in Eqn. (4), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}\\left(\\pmb{\\theta}_{t+1}\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)}\\\\ &{=\\mathcal{R}\\left(\\pmb{\\theta}_{t}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t-1}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t-1};\\mathbf{w}_{t}\\right)\\right)}\\\\ &{=\\![\\mathcal{R}\\left(\\pmb{\\theta}_{t}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t};\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t-1}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t-1};\\mathbf{w}_{t+1}\\right)\\right]}\\\\ &{\\quad+\\left[\\mathcal{R}\\left(\\pmb{\\theta}_{t-1}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t-1};\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t-1}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t-1};\\mathbf{w}_{t}\\right)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let\u2019s define a function $F(\\pmb{\\theta},\\mathbf{w})=\\pmb{\\theta}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}(\\pmb{\\theta};\\mathbf{w})$ . The above equation can be transformed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}\\left(\\theta_{t+1}\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)}\\\\ &{=\\left[\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right)\\right]+\\left[\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t}\\right)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the first term, note that $\\mathcal{R}$ is Lipschitz-smooth with a constant $L$ under Assumption 3.1. By Lemma 1, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right)}\\\\ &{{\\le}\\nabla_{F}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right)^{T}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)-F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right)}\\\\ &{\\quad{+}\\displaystyle\\frac{L}{2}\\|\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)-F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right)\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We observe that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)-F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\|}\\\\ &{=\\|\\left[\\pmb{\\theta}_{t}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)\\right]-\\left[\\pmb{\\theta}_{t-1}-\\mu\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right]\\|}\\\\ &{=\\|\\left[\\pmb{\\theta}_{t}-\\pmb{\\theta}_{t-1}\\right]-\\mu\\left[\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)-\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right]\\|}\\\\ &{=\\!\\mu\\|\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)+\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t}\\right)-\\nabla_{\\pmb{\\theta}}\\mathcal{L}\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Under Assumption 3.3, the gradient of $\\mathcal{L}$ is bounded by $\\sigma$ , by the triangle inequality, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)-F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\|\\le3\\mu\\sigma}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Under Assumption 3.3, the gradient of $\\mathcal{R}$ is also bounded by $\\sigma$ . Combining with Eqn. (19) , we can derive the upper bound of $\\bar{\\mathcal{R}}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{\\bar{w}}_{t+1}\\right)\\right)$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right)\\leq3\\mu\\sigma^{2}+\\frac92L\\mu^{2}\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the second term, under Assumption 3.1, $\\mathcal{R}$ is Lipschitz smooth with a constant $L$ . By Lemma 1, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t}\\right)\\right)}\\\\ {\\leq\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t}\\right)\\right)^{T}\\left(\\mathbf{w}_{t+1}-\\mathbf{w}_{t}\\right)+\\displaystyle\\frac{L}{2}\\left\\|\\mathbf{w}_{t+1}-\\mathbf{w}_{t}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recall that the update rule of w is $\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\alpha\\nabla_{\\mathbf{w}}\\mathcal{R}(F(\\pmb{\\theta}_{t},\\mathbf{w}_{t}))$ . Thus, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq-\\alpha\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t}\\right)\\right)^{T}\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)+\\displaystyle\\frac{L\\alpha^{2}}{2}\\left\\|\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)\\right\\|^{2}}\\\\ &{=\\left(\\frac{L\\alpha^{2}}{2}-\\alpha\\right)\\left\\|\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)\\right\\|^{2}}\\\\ &{\\quad+\\alpha\\left(\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)-\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t}\\right)\\right)\\right)^{T}\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Under Assumption 3.3, $\\nabla_{\\mathbf{w}}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta},\\mathbf{w})$ is bounded by $\\sigma^{\\prime}$ and $\\mathcal{L}$ has $\\sigma$ -bounded gradients. Then we can derive the upper bound of $\\nabla_{\\mathbf{w}}\\mathcal{R}(F(\\pmb{\\theta},\\mathbf{w}))$ based on the chain\u2019s rule: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\mathbf{w}}\\mathcal{R}(F(\\pmb{\\theta},\\mathbf{w}))\\|=\\|\\nabla_{\\mathbf{w}}F(\\pmb{\\theta},\\mathbf{w})^{T}\\nabla_{F}\\mathcal{R}(F(\\pmb{\\theta},\\mathbf{w}))\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\|\\mu\\nabla_{\\mathbf{w}}\\nabla_{\\theta}\\mathcal{L}(\\pmb{\\theta},\\mathbf{w})^{T}\\nabla_{F}\\mathcal{R}(F(\\pmb{\\theta},\\mathbf{w}))\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mu\\sigma\\sigma^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we further have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t+1}\\right)\\right)-\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t}\\right)\\right)}\\\\ &{\\le\\left(\\frac{L\\alpha^{2}}{2}-\\alpha\\right)\\left\\Vert\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)\\right\\Vert^{2}}\\\\ &{\\quad+\\alpha\\left(\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)-\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t-1},\\mathbf{w}_{t}\\right)\\right)\\right)^{T}\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)}\\\\ &{\\le\\left(\\frac{L\\alpha^{2}}{2}-\\alpha\\right)\\left\\Vert\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)\\right\\Vert^{2}+2\\alpha\\mu^{2}\\sigma^{2}\\sigma^{\\prime2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining Eqn. (20) and Eqn. (24), we can derive that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{R}\\left(\\pmb{\\theta}_{t+1}\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)}\\\\ {\\displaystyle\\leq\\left(\\frac{L\\alpha^{2}}{2}-\\alpha\\right)\\left\\Vert\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)\\right\\Vert^{2}+3\\mu\\sigma^{2}+\\frac{9}{2}L\\mu^{2}\\sigma^{2}+2\\alpha\\mu^{2}\\sigma^{2}\\sigma^{\\prime2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing up both sides from $t=1$ to $T$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}\\left(\\theta_{T+1}\\right)-\\mathcal{R}\\left(\\theta_{1}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\left(\\frac{L\\alpha^{2}}{2}-\\alpha\\right)\\Vert\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\theta_{t},\\mathbf{w}_{t}\\right)\\right)\\Vert^{2}+T\\left(3\\mu\\sigma^{2}+\\frac{9}{2}L\\mu^{2}\\sigma^{2}+2\\alpha\\mu^{2}\\sigma^{2}\\sigma^{\\prime2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Rearranging the terms, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left(\\alpha-\\frac{L\\alpha^{2}}{2}\\right)\\left\\|\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)\\right\\|^{2}}\\\\ &{\\leq\\!\\mathcal{R}\\left(\\pmb{\\theta}_{1}\\right)-\\mathcal{R}\\left(\\pmb{\\theta}_{T+1}\\right)+T\\left(3\\mu\\sigma^{2}+\\frac{9}{2}L\\mu^{2}\\sigma^{2}+2\\alpha\\mu^{2}\\sigma^{2}\\sigma^{\\prime2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the step size $\\begin{array}{r}{\\alpha=\\frac{k_{1}}{\\sqrt{T}}}\\end{array}$ for some constant $k_{1}$ where $\\begin{array}{r}{0<k_{1}<\\frac{2}{L}}\\end{array}$ , we find that $\\begin{array}{r}{\\alpha-\\frac{L\\alpha^{2}}{2}\\geq0}\\end{array}$ and $\\nabla_{\\mathbf{w}}\\mathcal{R}\\left(F\\left(\\pmb{\\theta}_{t},\\mathbf{w}_{t}\\right)\\right)=\\dot{\\nabla_{\\mathbf{w}}}\\mathcal{R}\\left(\\pmb{\\theta}_{t}\\right)$ . Therefore, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{t}{\\mathrm{min}}\\mathbb{E}\\left[\\left\\|\\nabla_{\\infty}\\mathcal{R}(\\theta_{t})\\right\\|^{2}\\right]\\leq\\frac{\\sum_{t=1}^{T}\\Big(\\alpha-\\frac{L\\alpha^{2}}{2}\\Big)\\left\\|\\nabla_{\\infty}\\mathcal{R}(\\theta_{t})\\right\\|^{2}}{T(\\alpha-\\frac{L\\alpha^{2}}{2})}}\\\\ &{\\leq\\frac{\\mathcal{R}(\\theta_{1})-\\mathcal{R}(\\theta_{T+1})+T\\left(3\\mu\\sigma^{2}+\\frac{9}{2}L\\mu^{2}\\sigma^{2}+2\\alpha\\mu^{2}\\sigma^{2}\\sigma^{2}\\right)}{T\\alpha\\left(1-\\frac{L\\alpha}{2}\\right)}}\\\\ &{\\leq\\frac{\\mathcal{R}(\\theta_{1})-\\mathcal{R}(\\theta_{T+1})+T\\left(3\\mu\\sigma^{2}+\\frac{9}{2}L\\mu^{2}\\sigma^{2}+2\\alpha\\mu^{2}\\sigma^{2}\\sigma^{2}\\right)}{\\alpha\\sqrt{T}(\\sqrt{T}-1)}}\\\\ &{=\\frac{\\mathcal{R}(\\theta_{1})-\\mathcal{R}(\\theta_{T+1})}{\\alpha\\sqrt{T}(\\sqrt{T}-1)}+\\frac{\\sigma\\mu\\sqrt{T}}{\\alpha(\\sqrt{T}-1)}\\left(3\\sigma+\\frac{9}{2}L\\mu\\sigma+2\\alpha\\mu\\sigma\\sigma^{2}\\right)}\\\\ &{=\\frac{\\mathcal{R}(\\theta_{1})-\\mathcal{R}(\\theta_{T+1})}{k_{1}(\\sqrt{T}-1)}+\\frac{\\sigma\\mathcal{R}_{2}}{k_{1}(\\sqrt{T}-1)}\\left(3\\sigma+\\frac{9}{2}L\\mu\\sigma+2\\alpha\\mu\\sigma\\sigma^{2}\\right)}\\\\ &{=\\sigma\\left(\\frac{3}{\\sqrt{T}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that the third inequality holds since $\\begin{array}{r}{1-\\frac{L\\alpha}{2}\\geq1-\\frac{1}{\\sqrt{T}}}\\end{array}$ ", "page_idx": 16}, {"type": "table", "img_path": "iYkhThIXG1/tmp/5e4bf71b3c3b3d24868ea00a5aa0e1481a1f76a2172ac375b22f22cc9def7e02.jpg", "table_caption": ["Table 4: Definition of each class in our question dataset. We provide the definition of 15 classes and the corresponding representative behaviors. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Details of Evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Baseline Implementations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Regarding baseline approaches, we use the code released by the authors or implement our own version if the authors don\u2019t release the code. Specifically, as for Snorkel, we use their released open-sourced code from https://github.com/snorkel-team/snorkel. In terms of PRODEN, we refer to its official implementation from https://github.com/lvjiaqi77/PRODEN. Regarding GroupDRO and CVaR DRO, we refer to the implementation from https://github.com/x-zho14/MAPLE. ", "page_idx": 17}, {"type": "text", "text": "C.2 Details of the Dataset ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "First, we introduce basic task information about our question and response datasets. ", "page_idx": 17}, {"type": "text", "text": "Toxic question classification. The toxic question classification encompasses 15 distinct classes, each derived from the usage policy of OpenAI as of 2023, detailed in Table 4. The dataset comprises 6,941 entries, with each entry receiving annotations from three human annotators and three large language models (LLMs): GPT-4, GPT-4 Turbo, and Claude-2. A subset of 1,000 entries, which includes verified ground truth, serves as the validation set. The remaining 2,162 entries are used as the testing set, ensuring that they are disjoint from the training data to maintain the integrity of model evaluations. ", "page_idx": 17}, {"type": "image", "img_path": "iYkhThIXG1/tmp/790517c92ffab99421e886747b57cd7505649c6626d56db5330800c66ce64b31.jpg", "img_caption": ["Figure 5: Annotation examples of the question dataset. The annotator classifies the questions into one of the 15 classes (which are the ground truths of these examples). "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Toxic response classification. The toxic response classification task is binary, requiring annotators to categorize responses as toxic (labeled as \u201c1\u201d) or non-toxic (labeled as $\\,^{\\bullet}0^{\\bullet}$ ). Instances of what constitutes a toxic response are elaborated upon in Classes 0-13, as shown in Table 4. The dataset comprises 28,194 entries, each annotated by three human evaluators and three large language models (LLMs): GPT-4, GPT-4 Turbo, and Claude-2. Of these, 1,000 entries with verified ground truth are designated for the validation set, while an additional 2,000 disjoint entries are reserved for the testing set, ensuring comprehensive and unbiased model assessment. ", "page_idx": 18}, {"type": "text", "text": "The three human annotators were recruited by a third-party security company and trained by professional staff. They were thoroughly briefed on the definition of each class within the question and response datasets, accompanied by specific annotation examples, as depicted in Figure 5 and Figure 6. In addition to human annotators, the security company also enlisted three commercial large language model (LLM) APIs to provide annotations for the data. The prompts used for guiding the LLMs in annotating the question and response data are detailed below. ", "page_idx": 18}, {"type": "image", "img_path": "iYkhThIXG1/tmp/3ef4eb5b050c0ce5e46ed0c2a1d8e2f7776ba254b8dbc89377f4f5e1a274ed9b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: Annotation examples of the response dataset. The annotator determines the response as toxic in this example. ", "page_idx": 19}, {"type": "text", "text": "PROMPT FOR INSTRUCTING LLMS TO LABEL QUESTION DATA ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "You are now an assistant to label the category of the given question. I will give you the question and you should tell me which category the question belongs to.   \n0. Illegal activity;   \n1. Child Sexual Abuse Material or any content that exploits or harms children;   \n2. Generation of hateful, harassing, or violent content;   \n3. Generation of malware;   \n4. Activity that has a high risk of physical harm;   \n5. Activity that has a high risk of economic harm;   \n6. Fraudulent or deceptive activity;   \n7. Adult content, adult industries, and dating apps;   \n8. Political campaigning or lobbying;   \n9. Activity that violates people\u2019s privacy;   \n10. Engaging in the unauthorized practice of law, or offering tailored legal advice without a qualified person reviewing the information;   \n11. Offering tailored financial advice without a qualified person reviewing the information; 12. Telling someone that they have or do not have a certain health condition, or providing instructions on how to cure or treat a health condition;   \n13. High-risk government decision-making;   \n14. None of the above.   \nYou should read the whole question for this task and think step by step to see which type the question belongs to. Again remember to answer the category number only. Do not make explanations. Here is the question: [placeholder]. Your label: ", "page_idx": 19}, {"type": "text", "text": "PROMPT FOR INSTRUCTING LLMS TO LABEL RESPONSE DATA ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "You are now an assistant to label the toxicity of the given response. I will give you the question and the corresponding response. You should tell me if the response is toxic or not. Reply \u201c1\u201d if the response is toxic. Reply $\\,^{\\bullet}0^{\\bullet}$ if the response is not toxic. You should read the whole question and the response for this task and think step by step whether the response is toxic. Again remember to answer \u201c1\u201d or $\\,^{\\bullet\\bullet}\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}\\!\\,^{\\bullet}$ only. Do not make explanations. Here is the question: [placeholder 1]. The corresponding response to label is: [placeholder 2]. Your label: ", "page_idx": 19}, {"type": "table", "img_path": "iYkhThIXG1/tmp/77a4225c9d6299f784e6de4e9acc989f55c582e5bb59587aff47a41f375a3e05.jpg", "table_caption": ["Table 5: Top-10 words of each topic in the question dataset. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "iYkhThIXG1/tmp/fcbd5417c3bdf580b1f66af3175ababbc82af9e6217ede60f72528e5bce1288a.jpg", "table_caption": ["Table 6: Top-10 words of each topic in the response dataset. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Then, we introduce the details of group annotations in our dataset. ", "page_idx": 20}, {"type": "text", "text": "Topics in the question and response datasets. A third-party security company employed Latent Dirichlet Allocation (LDA) to identify and summarize 15 topics in the validation set, aligning with the number of classes in the question set. The top ten words for each topic, both in the question and response datasets, are listed in Table 5 and Table 6, respectively. ", "page_idx": 20}, {"type": "text", "text": "Construction of groups based on topics. Building on the topic information provided by the company, we constructed groups by integrating topic and label categories, adhering to methodologies outlined in previous work [17]. ", "page_idx": 20}, {"type": "text", "text": "Additionally, we have noted significant variations in the quality of annotations, particularly highlighting a pronounced disparity between Claude-2 and other annotators. The subjective nature of toxicity and the diverse backgrounds and interpretations of annotators contribute to these differences. An illustrative example of how a human annotator and an LLM reason about a label is presented in Figure 7, demonstrating their differing interpretations and focus points in determining toxicity. ", "page_idx": 20}, {"type": "text", "text": "This variation underscores the necessity of involving multiple annotators to capture a broader spectrum of perspectives, thereby enhancing the reliability of toxicity annotations. There is also a notable variability in the performance of large language models (LLMs) in annotating toxic content. Our findings, as depicted in Figure 3, show that GPT-4 and GPT-4 Turbo provide significantly more accurate annotations compared to Claude-2. This difference emphasizes the importance of modeling the soft label weights to improve the overall efficacy and robustness of toxicity classification systems. ", "page_idx": 20}, {"type": "image", "img_path": "iYkhThIXG1/tmp/010e0164023b13c0f70152de21c0e332789a97be73d96fd2a6bab37be2c9d2be.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "iYkhThIXG1/tmp/39908b6e58f3eed84462f03d878abdd86a8ee043750899928f7fad4022c77259.jpg", "table_caption": ["Figure 7: An example of reasoning a label by both a human annotator and an LLM. ", "Table 7: Training hyper-parameter settings of our method. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.3 Hyper-parameters and training details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The common hyper-parameter setting is as Table 7 shows. The toxicity classifier and soft-label weight estimator are both implemented based on the transformers library of version 4.34.1 [62]. The training time of one experiment with eight A100 GPUs for our models is as follows: the question set requires only about 5 minutes to train, while the more complex response set completes training in approximately one hour and a half. These durations are manageable and demonstrate the practicality of our approach in real-world settings. ", "page_idx": 21}, {"type": "table", "img_path": "iYkhThIXG1/tmp/b22ba86d00e4977e7017497f0de166c8adf1e57d7caa631c4ea30069bd26d34c.jpg", "table_caption": ["Table 8: Comparison of accuracy using different methods on the public HateXplain dataset. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "iYkhThIXG1/tmp/9f978e196436a29d89a00b0016ecc377bb7861da0962bdd9ed080fabb95801eb.jpg", "table_caption": ["Table 9: Time complexity comparison of different methods on all datasets. We report the GPU hours of each experiment with one A100 80GB GPU. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.4 Experiments on the HateXplain dataset ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We report the average accuracy and worst-group accuracy of all methods in the HateXplain dataset in Table 8. We observe that our method still outperforms other baselines. ", "page_idx": 22}, {"type": "text", "text": "C.5 Time complexity comparison ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We compare the computational overhead induced by the bi-level optimization process and compare it with the traditional single-loop optimization methods (i.e., the baseline methods) on our datasets (question and answer) and one additional public dataset HateXplain. We utilize 8 Nvidia A100 GPUs to train a toxicity classifier and measure the corresponding computational overhead in terms of training time. The results are reported in Table 9. We observe that our proposed bi-level optimization method introduces approximately two times the computation overhead compared with baseline methods. The additional computation overhead originates from the update of the soft-label weight. However, given the total training time, our proposed method is still computationally feasible and acceptable. ", "page_idx": 22}, {"type": "text", "text": "D Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The dataset for toxicity classification, which includes potentially toxic questions and responses, requires careful handling to mitigate safety risks associated with the sensitive nature of the content. The following safeguards were implemented: ", "page_idx": 22}, {"type": "text", "text": "D.1 Access Control ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Access to the dataset is restricted to authorized personnel only. This includes a rigorous vetting process for researchers and developers who wish to use the data, ensuring that it is used solely for the intended research purposes. ", "page_idx": 22}, {"type": "text", "text": "D.2 Ethical Guidelines ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "All users of the dataset are required to adhere to strict ethical guidelines that prohibit the use of data for any purposes that could lead to harm or discrimination. This includes Responsible Conduct of Research (RCR) training and regular audits of research activities. ", "page_idx": 23}, {"type": "text", "text": "D.3 Transparent Documentation and Usage Guidelines ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide comprehensive documentation and clear usage guidelines with the dataset. These guidelines help users understand the context and limitations of the data, promoting responsible usage and preventing misuse. The documentation also details the annotation process, including how human and LLM annotations were generated and verified. ", "page_idx": 23}, {"type": "text", "text": "D.4 Use Case Restrictions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The dataset is only made available for specific, approved use cases that align with promoting safety and understanding in LLMs. Any application that intends to use the dataset to generate or promote toxic content is strictly prohibited. ", "page_idx": 23}, {"type": "text", "text": "E Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Potential Positive Societal Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our research contributes to enhancing the accuracy and reliability of toxicity classification systems, which are crucial for maintaining healthy online environments. By developing more nuanced models that utilize multiple annotations per data point, we address the inherent subjectivity and variability in determining what constitutes toxic content. This approach not only improves the precision of toxicity detection but also helps in creating safer communication spaces by effectively filtering harmful content. ", "page_idx": 23}, {"type": "text", "text": "Moreover, by incorporating diverse perspectives through multiple annotations, our models are better equipped to understand and respect cultural and contextual differences in language use. This sensitivity is particularly important in global platforms where the definition of offensive or harmful language can vary significantly. As a result, our work supports the creation of more inclusive and respectful online communities. ", "page_idx": 23}, {"type": "text", "text": "Additionally, the methodology developed in our study can be adapted for other complex moderation tasks, such as identifying misinformation or hate speech, further broadening the impact of our research in safeguarding public discourse. ", "page_idx": 23}, {"type": "text", "text": "E.2 Potential Negative Societal Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "While our models offer significant benefits, they also pose potential risks. One major concern is the \"black-box\" nature of the toxicity classifier. To foster user trust, we introduce a novel explanation method inspired by saliency maps [63] and StateMask [64], which enhances the interpretability of the model\u2019s outputs. Our method deconstructs potentially toxic content into several blocks, each representing a feature that could be core or spurious. By analyzing the change in prediction logits when each block is removed, we assess the impact of each feature on the model\u2019s decision. This allows us to rank the relative importance of features contributing to the final prediction. For example, as shown in Figure 8, our method demonstrates that the prediction relies primarily on the core feature, \"The Dark Web can be accessed through various anonymous networks such as Tor,\" while disregarding spurious features like \"I\u2019m sorry\" and \"I cannot.\" Notably, $82\\%$ of non-toxic responses in the dataset contain $^{\\prime\\prime}\\mathrm{I}$ cannot,\" and $81\\%$ include \"I\u2019m sorry,\" leading the model to unintentionally fti a spurious correlation between these phrases and non-toxic labels. Our explanation method effectively demonstrates that our method can reduce the influence of such spurious features. ", "page_idx": 23}, {"type": "text", "text": "Moreover, while our models aim for high accuracy, they are not flawless and may occasionally misclassify content, resulting in either unjust censorship of legitimate speech or failure to detect nuanced toxicity. Such errors can have profound consequences for freedom of expression, potentially suppressing minority voices if unchecked. Additionally, adversaries may develop sophisticated ", "page_idx": 23}, {"type": "image", "img_path": "iYkhThIXG1/tmp/abb4dbed4f176cd89f5d1655a211eccacc17e6b802612172d6e9263afed8d785.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 8: An example from our toxicity classification task, showing response data with annotations from three human reviewers and three large language models. We report the soft-label weights our method assigns to each annotation. Additionally, our explanation method highlights the features that most strongly influence the model\u2019s prediction. Red denotes important features, while green indicates less significant ones. ", "page_idx": 24}, {"type": "text", "text": "attacks to bypass the toxicity classifier. To mitigate these risks, we emphasize the importance of robust safeguards, including regular audits of model decisions and frequent updates to the classifier. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide a theoretical analysis of convergence and experiment results in Section 3.4 and Section 4.2 correspondingly. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We report the dataset size and the training time in Appendix C. We provide a discussion of limitations and future work in Section 5 of the main text. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We list the assumptions required by the theorems in Section 3.4 and provide proofs of theorems in Appendix A and Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We carefully discuss the settings of each experiment and provide the code and data in the github repository https://github.com/chengzelei/crowdsource_ toxicity_classification. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the code and data in the supplementary material and provide a brief instruction. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We present the details of our dataset and evaluation in Appendix C. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The results are accompanied by error bars/standard deviation. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the computing resources in Section 4.1 of the main text and provide the training time details in Appendix C.3. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The authors have reviewed and followed the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss both positive and negative broader impacts in Appendix E. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the safeguards related to the responsible release of data in Appendix D. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We obtained the datasets from a third-party security company and released the data with their consent. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We introduce the details of the datasets in Appendix C.2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Although the datasets were collected by a third-party security company, we provide the instructions for annotators under the consent of the company in Appendix C.2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We submitted our proposal of research before conducting the project. The IRB office determined that our research is not research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]