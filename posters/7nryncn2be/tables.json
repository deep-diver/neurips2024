[{"figure_path": "7NrYnCN2be/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with SOTA methods on common benchmarks and Union-B. * means we use publicly released checkpoints to evaluate the method. \u2020 means we reproduce the methods with the same configuration. For training data: SL - MJSynth and SynthText; RL - Real labeled data; RU - Union14M-U; RU\u00b9 - Book32, TextVQA, and ST-VQA; RU2 - Places2, OpenImages, and ImageNet ILSVRC 2012. Cur, M-O, Art, Ctl, Sal, M-W, and Gen represent Curve, Multi-Oriented, Artistic, Contextless, Salient, Multi-Words, and General. P(M) means the model size.", "description": "This table compares the proposed ViSu method with other state-of-the-art (SOTA) scene text recognition (STR) methods on common benchmarks (IIIT-SVT, IC13, IC15, SVTP, CUTE) and the challenging Union-B benchmark.  It shows the word accuracy for various text styles (curve, multi-oriented, artistic, contextless, salient, multi-word, general). The table also indicates the training data used by each method (synthetic, real labeled, real unlabeled) and the model size.", "section": "4 Experiment"}, {"figure_path": "7NrYnCN2be/tables/tables_7_1.jpg", "caption": "Table 2: Comparison with SOTA methods on several challenging benchmarks. All symbols have the same meaning as in Table. 1.", "description": "This table compares the performance of various state-of-the-art (SOTA) scene text recognition (STR) methods on several challenging benchmarks.  It shows the accuracy of each method on datasets like WordArt, ArT, COCO, and Uber.  The table allows for a direct comparison of the proposed method (ViSu) against other SOTA methods, highlighting its performance on particularly difficult text images.", "section": "4 Experiment"}, {"figure_path": "7NrYnCN2be/tables/tables_8_1.jpg", "caption": "Table 1: Comparison with SOTA methods on common benchmarks and Union-B. * means we use publicly released checkpoints to evaluate the method. \u2020 means we reproduce the methods with the same configuration. For training data: SL - MJSynth and SynthText; RL - Real labeled data; RU - Union14M-U; RU\u00b9 - Book32, TextVQA, and ST-VQA; RU2 - Places2, OpenImages, and ImageNet ILSVRC 2012. Cur, M-O, Art, Ctl, Sal, M-W, and Gen represent Curve, Multi-Oriented, Artistic, Contextless, Salient, Multi-Words, and General. P(M) means the model size.", "description": "This table compares the proposed ViSu method with other state-of-the-art (SOTA) scene text recognition methods on standard benchmarks (IIIT, SVT, IC13, IC15, SVTP, CUTE) and a more challenging benchmark (Union-B).  It shows word accuracy, broken down by various text characteristics (e.g., curve, multi-oriented, artistic, etc.), and indicates the training data used (synthetic, real labeled, real unlabeled). The table also provides model size information.", "section": "4 Experiment"}, {"figure_path": "7NrYnCN2be/tables/tables_8_2.jpg", "caption": "Table 1: Comparison with SOTA methods on common benchmarks and Union-B. * means we use publicly released checkpoints to evaluate the method. \u2020 means we reproduce the methods with the same configuration. For training data: SL - MJSynth and SynthText; RL - Real labeled data; RU - Union14M-U; RU\u00b9 - Book32, TextVQA, and ST-VQA; RU\u00b2 - Places2, OpenImages, and ImageNet ILSVRC 2012. Cur, M-O, Art, Ctl, Sal, M-W, and Gen represent Curve, Multi-Oriented, Artistic, Contextless, Salient, Multi-Words, and General. P(M) means the model size.", "description": "This table compares the proposed ViSu method with other state-of-the-art (SOTA) scene text recognition (STR) methods on several benchmark datasets.  It shows the word accuracy achieved by each method on various subsets of the benchmarks, categorized by the type of training data used (synthetic, real labeled, and real unlabeled).  The table also indicates whether publicly released checkpoints or reproduced methods were used for evaluation, and provides additional context on the datasets used (e.g., indicating what sub-datasets were used for real unlabeled data).", "section": "4 Experiment"}, {"figure_path": "7NrYnCN2be/tables/tables_15_1.jpg", "caption": "Table 1: Comparison with SOTA methods on common benchmarks and Union-B. * means we use publicly released checkpoints to evaluate the method. \u2020 means we reproduce the methods with the same configuration. For training data: SL - MJSynth and SynthText; RL - Real labeled data; RU - Union14M-U; RU\u00b9 - Book32, TextVQA, and ST-VQA; RU2 - Places2, OpenImages, and ImageNet ILSVRC 2012. Cur, M-O, Art, Ctl, Sal, M-W, and Gen represent Curve, Multi-Oriented, Artistic, Contextless, Salient, Multi-Words, and General. P(M) means the model size.", "description": "This table compares the performance of the proposed ViSu model with other state-of-the-art (SOTA) methods on standard scene text recognition (STR) benchmarks (IIIT, SVT, IC13, IC15, SVTP, CUTE) and the more challenging Union-B benchmark.  It shows the word accuracy for various text styles (curve, multi-oriented, artistic, etc.) and indicates the training data used (synthetic, real labeled, and real unlabeled data).  Model size is also listed.", "section": "4 Experiment"}, {"figure_path": "7NrYnCN2be/tables/tables_16_1.jpg", "caption": "Table 2: Comparison with SOTA methods on several challenging benchmarks. All symbols have the same meaning as in Table. 1.", "description": "This table compares the performance of the proposed ViSu method against other state-of-the-art (SOTA) methods on several challenging benchmark datasets.  The datasets include WordArt, ArT, COCO, and Uber, each representing different text recognition challenges. The table shows the accuracy achieved by each method on each dataset, highlighting the superior performance of ViSu on these more difficult benchmarks.", "section": "4.3 Comparison with SOTA"}, {"figure_path": "7NrYnCN2be/tables/tables_16_2.jpg", "caption": "Table 1: Comparison with SOTA methods on common benchmarks and Union-B. * means we use publicly released checkpoints to evaluate the method. \u2020 means we reproduce the methods with the same configuration. For training data: SL - MJSynth and SynthText; RL - Real labeled data; RU - Union14M-U; RU\u00b9 - Book32, TextVQA, and ST-VQA; RU\u00b2 - Places2, OpenImages, and ImageNet ILSVRC 2012. Cur, M-O, Art, Ctl, Sal, M-W, and Gen represent Curve, Multi-Oriented, Artistic, Contextless, Salient, Multi-Words, and General. P(M) means the model size.", "description": "This table compares the proposed ViSu model with other state-of-the-art (SOTA) scene text recognition (STR) methods on standard benchmarks (IIIT-SVT, IC13, IC15, SVTP, CUTE) and the more challenging Union-Benchmark.  The table indicates the datasets used for training (synthetic, real labeled, and real unlabeled data), performance metrics (accuracy), model parameters, and different text styles or conditions within the Union-Benchmark. The results demonstrate the superior accuracy of the ViSu model, especially when dealing with the more challenging datasets.", "section": "4 Experiment"}, {"figure_path": "7NrYnCN2be/tables/tables_17_1.jpg", "caption": "Table 8: Performance on Union-B with different configuration settings for OGS. The first row represents the baseline model without OGS. Character-level means that all characters in a sample have independent random font or orientation. Instance-level means that means that all characters in a sample have a unified font or orientation, but different samples are independent.", "description": "This table presents ablation study results on the Union-B benchmark, focusing on the impact of different configurations within the Online Generation Strategy (OGS).  It explores variations in font and orientation randomization (character-level vs. instance-level) and the inclusion/exclusion of background and text color variations. The results demonstrate the effectiveness of the proposed OGS configurations on improving the overall accuracy on the Union-B benchmark.", "section": "4.4.1 Online Generation Strategy"}, {"figure_path": "7NrYnCN2be/tables/tables_18_1.jpg", "caption": "Table 1: Comparison with SOTA methods on common benchmarks and Union-B. * means we use publicly released checkpoints to evaluate the method. \u2020 means we reproduce the methods with the same configuration. For training data: SL - MJSynth and SynthText; RL - Real labeled data; RU - Union14M-U; RU\u00b9 - Book32, TextVQA, and ST-VQA; RU\u00b2 - Places2, OpenImages, and ImageNet ILSVRC 2012. Cur, M-O, Art, Ctl, Sal, M-W, and Gen represent Curve, Multi-Oriented, Artistic, Contextless, Salient, Multi-Words, and General. P(M) means the model size.", "description": "This table compares the proposed ViSu method with other state-of-the-art (SOTA) methods on several scene text recognition benchmarks.  It shows the accuracy of each method on various datasets (common benchmarks and the challenging Union14M-Benchmark), broken down by different text styles and orientations. The table also indicates the type of training data used (synthetic, real labeled, and real unlabeled), and the model size.", "section": "4 Experiment"}]