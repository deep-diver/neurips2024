[{"type": "text", "text": "Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiangxin Zhou1,2,4,\u2217 Dongyu Xue4,\u2217 Ruizhe Chen3,4,\u2217 ", "page_idx": 0}, {"type": "text", "text": "Zaixiang Zheng4 Liang Wang1,2 Quanquan Gu4,\u2020 ", "page_idx": 0}, {"type": "text", "text": "1School of Artificial Intelligence, University of Chinese Academy of Sciences 2New Laboratory of Pattern Recognition (NLPR),   \nState Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA)   \n3College of Computer Science and Electronic Engineering, Hunan University 4ByteDance Research ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody sequencestructure co-design as an optimization problem towards specific preferences, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion. Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity simultaneously, demonstrating the superiority of our approach. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Antibodies, vital proteins with an inherent Y-shaped structure in the immune system, are produced in response to an immunological challenge. Their primary function is to discern and neutralize specific pathogens, typically referred to as antigens, with a significant degree of specificity [39]. The specificity mainly comes from the Complementarity Determining Regions (CDRs), which accounts for most binding affinity to specific antigens [24, 15, 49, 2]. Hence, the design of CDRs is a crucial step in developing potent therapeutic antibodies, which plays an important role in drug discovery. ", "page_idx": 0}, {"type": "text", "text": "Traditional in silico antibody design methods rely on sampling or searching protein sequences over a large search space to optimize the physical and chemical energy, which is inefficient and easily trapped in bad local minima [1, 31, 47]. Recently, deep generative models have been employed to model protein sequences in nature for antibody design [5, 17]. Following the fundamental biological principle that structure determines function numerous efforts have been focused on antibody sequence-structure co-design [22, 21, 36, 29, 30, 37], which demonstrate superiority over sequence design-based methods. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, the main evaluation metrics in the aforementioned works are amino acid recovery (AAR) and root mean square deviation (RMSD) between the generated antibody and the real one. This is controversial because AAR is susceptible to manipulation and does not precisely gauge the quality of the generated antibody sequence. Meanwhile, RMSD does not involve side chains, which are vital for antigen-antibody interaction. Besides, it is biologically plausible that a specific antigen can potentially bind with multiple efficacious antibodies [45, 12]. This motivates us to examine the generated structures and sequences of antibodies through the lens of energy, which reflects the rationality of the designed antibodies and their binding affinity to the target antigens. We have noted that nearly all antibody ", "page_idx": 1}, {"type": "image", "img_path": "GN2GXjPyN8/tmp/44a6f1812d42c3fd8b75e82e96ab9e8293653fa281eae0cc876a328d5dfd7251.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: The third CDR in the heavy chain, CDR-H3 (colored in yellow), of real antibody (left) and synthetic antibody (right) designed by MEAN [29] for a given antigen (PDB ID: 4cmh). The rest parts of antibodies except CDR-H3 are colored in blue. The antigens are colored in gray. We use red (resp. black) dotted lines to represent clashes between a CDR-H3 atom and a framework/antigen atom (resp. another CDR-H3 atom). We consider a clash occurs when the overlap of the van der Waals radii of two atoms exceeds $0.6\\mathring\\mathrm{A}$ . ", "page_idx": 1}, {"type": "text", "text": "sequence-structure co-design methods struggle to produce antibodies with low energy. This suggests the presence of irrational structures and inadequate binding affinity in antibodies designed by these methods (see Fig. 1). We attribute this incapability to the insufficient model training caused by a scarcity of high-quality data. ", "page_idx": 1}, {"type": "text", "text": "To tackle the above challenges and bridge the gap between in silico antibody sequence-structure co-design methods and the intrinsic need for drug discovery, we formulate the antibody design task as an antibody optimization problem with a focus on better rationality and functionality. Inspired by direct preference optimization [DPO, 41] and self-play fine-tuning techniques [10] that achieve huge success in the alignment of large language models (LLMs), we proposed a direct energy-based preference optimization method named ABDPO for antibody optimization. More specifically, we first pre-train a conditional diffusion model on real antigen-antibody datasets, which simultaneously captures sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks. We then progressively fine-tune this model using synthetic antibodies generated by the model itself given an antigen with energy-based preference. This preference is defined at a fine-grained residue level, which promotes the effectiveness and efficiency of the optimization process. To fulflil the requirement of various optimization objectives, we decompose the energy into multiple types so that we can incorporate prior knowledge and mitigate the interference between confilcting objectives (e.g., repulsion and attraction energy) to guide the optimization process. Fine-tuning with self-synthesized energy-based antibody preference data represents a revolutionary solution to address the limitation of scarce high-quality real-world data, a significant challenge in this domain. We highlight our main contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We tackle the antibody sequence-structure co-design problem through the lens of energy from the perspectives of both rationality and functionality.   \n\u2022 We propose direct residue-level energy-based preference optimization to fine-tune diffusion models for designing antibodies with rational structures and high binding affinity to specific antigens.   \n\u2022 We introduce energy decomposition and confilct mitigation techniques to enhance the effectiveness and efficiency of the optimization process.   \n\u2022 Experiments show ABDPO\u2019s effectiveness in generating antibodies with energies resembling natural antibodies and generality in optimizing multiple preferences. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Antibody Design. The application of deep learning to antibody design can be traced back to at least [35, 43, 3]. In recent years, sequence-structure co-design of antibodies has attracted increasing attention. Jin et al. [22] proposed to simultaneously design sequences and structures of CDRs in an autoregressive way and iteratively refine the designed structures. Jin et al. [21] further utilized the epitope and focused on designing CDR-H3 with a hierarchical message passing equivariant network. Kong et al. [29] incorporated antigens and the light chains of antibodies as conditions and designed CDRs with E(3)-equivariant graph networks via a progressive full-shot scheme. Luo et al. [36] proposed a diffusion model that takes residue types, atom coordinates and side-chain orientations into consideration to generate antigen-specific CDRs. Kong et al. [30] focused on epitope-binding CDR-H3 design and modelled full-atom geometry. Recently, Martinkus et al. [37] proposed AbDiffuser, a novel diffusion model for antibody design, that incorporates more domain knowledge and physics-based constraints and also enables side-chain generation. Besides, Wu and Li [48], Gao et al. [19] and Zheng et al. [52] introduced pre-trained protein language model to antibody design. Distinct from the above works, our method places a stronger emphasis on designing and optimizing antibodies with low energy and high binding affinity. ", "page_idx": 1}, {"type": "image", "img_path": "GN2GXjPyN8/tmp/4fab230b45e77af894fa128bad5af06c837c31409d6fbd154ce4a81ef71ed2e5.jpg", "img_caption": ["Figure 2: Overview of ABDPO. This process can be summarized as: (a) Generate antibodies with the pre-trained diffusion model; (b) Evaluate the multiple types of residue-level energy and construct preference data; (c) Compute the losses for energy-based preference optimization and mitigate the conflicts between losses of multiple types of energy; (d) Update the diffusion model. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Alignment of Generative Models. Solely maximizing the likelihood of training data does not always lead to a model that satisfies users\u2019 preferences. Recently, many efforts have been made on the alignment of the generative models to human preferences. Reinforcement learning has been introduced to learning from human/AI feedback to large language models, such as RLHF [40] and RLAIF [33]. Typically, RLHF consists of three phases: supervised fine-tuning, reward modeling, and RL fine-tuning. Similar ideas have also been introduced to text-to-image generation, such as DDPO [7], DPOK [16] and DiffAC [53]. They view the generative processes of diffusion models as a multi-step Markov Decision Process (MDP) and apply policy gradient for fine-tuning. Rafailov et al. [41] proposed direct preference optimization (DPO) to directly fine-tune language models on preference data, which matches RLHF in performance. Recently, DPO has been introduced to text-to-image generation [46, 6]. Notably, in the aforementioned works, models pre-trained with large-scale datasets have already shown strong performance, in which case alignment further increases users\u2019 satisfaction. In contrast, in our work, the model pre-trained with limited real-world antibody data is insufficient in performance. Therefore, preference optimization in our case is primarily used to help the model understand the essence of nature and meet the requirement of antibody design. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present ABDPO, a direct energy-based preference optimization method for designing antibodies with reasonable rationality and functionality (Fig. 2). We first define the antibody generation task and introduce the diffusion model for this task in Sec. 3.1. Then we introduce residue-level preference optimization for fine-tuning the diffusion model and analyze its advantages in effectiveness and efficiency in Sec. 3.2. Finally, in Sec. 3.3, we introduce the energy decomposition and describe how to mitigate the conflicts when optimizing multiple types of energy. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We focus on designing CDR-H3 of the antibody given antigen structure as CDR-H3 contributes the most to the diversity and specificity of antibodies [49, 2] and the rest part of the antibody including the frameworks and other CDRs. Following Luo et al. [36], each amino acid is represented by its type $\\mathbf{s}_{i}\\,\\in\\,\\{\\begin{array}{r l}\\end{array}$ {ACDEFGHIKLMNPQRSTVWY}, $\\mathbf{C}_{\\alpha}$ coordinate $\\mathbf{x}_{i}\\,\\in\\,\\mathbb{R}^{3}$ , and frame orientation $\\mathbf{O}_{i}\\,\\in\\,\\mathrm{SO}(3)$ [28], where $i\\,=\\,1,\\ldots,N$ and $N$ is the number of the amino acids in the protein complex. We assume the CDR-H3 to be generated has $m$ amino acids, which can be denoted by $\\mathcal{R}=\\{(\\mathbf{s}_{j},\\mathbf{x}_{j},\\mathbf{O}_{j})|j=n+1,\\ldots,n+m\\}$ , where $n+1$ is the index of the first residue in CDR-H3 sequence. The rest part of the antigen-antibody complex can be denoted by $\\mathcal{P}=\\{(\\mathbf{s}_{i},\\mathbf{x}_{i},\\mathbf{O}_{i})|i\\in$ $\\{1,\\cdot\\cdot,N\\}\\backslash\\{n+1,\\cdot\\cdot\\cdot,n+m\\}\\}$ . The antibody generation task can be then formulated as modeling the conditional distribution $P(\\mathcal{R}|\\mathcal{P})$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Denoising Diffusion Probabilistic Model [DDPM, 20] have been introduced to antibody generation by Luo et al. [36]. This approach consists of a forward diffusion process and a reverse generative process. The diffusion process gradually injects noises into data as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{s}_{j}^{t}|\\mathbf{s}_{j}^{0})=\\mathcal{C}\\left(\\mathbb{1}(\\mathbf{s}_{j}^{t})\\middle|\\bar{\\alpha}^{t}\\mathbb{1}(\\mathbf{s}_{j}^{0})+\\bar{\\beta}^{t}\\mathbb{1}/K\\right),}\\\\ &{q(\\mathbf{x}_{j}^{t}|\\mathbf{x}_{j}^{0})=\\mathcal{N}\\left(\\mathbf{x}_{j}^{t}\\middle|\\sqrt{\\bar{\\alpha}^{t}}\\mathbf{x}_{j}^{0},\\bar{\\beta}^{t}I\\right),}\\\\ &{q(\\mathbf{O}_{j}^{t}|\\mathbf{O}_{j}^{0})=\\mathcal{Z}\\mathcal{G}_{\\mathrm{SO}(3)}\\left(\\mathbf{O}_{j}^{t}|\\mathrm{ScaleRot}\\left(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{O}_{j}^{0}\\right),\\bar{\\beta}^{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $(\\mathbf{s}_{j}^{0},\\mathbf{x}_{j}^{0},\\mathbf{O}_{j}^{0})$ are the noisy-free amino acid at time step 0 with index $j$ , and $(\\mathbf{s}_{j}^{t},\\mathbf{x}_{j}^{t},\\mathbf{O}_{j}^{t})$ are the noisy amino acid at time step $t$ . $\\mathbb{1}(\\cdot)$ is the one-hot operation. $\\{\\beta^{t}\\}_{t=1}^{T}$ is the noise schedule for the diffusion process [20], and we define $\\begin{array}{r}{\\bar{\\alpha}^{t}=\\prod_{\\tau=1}^{t}(1-\\beta^{\\tau})}\\end{array}$ and $\\bar{\\beta}^{t}=1-\\bar{\\alpha}^{t}$ . $K$ is the number of amino acid types. Here, $\\mathcal{C}(\\cdot),\\mathcal{N}(\\cdot)$ , and $\\mathcal{Z G}_{\\mathrm{SO}(3)}(\\cdot)$ are categorical distribution, Gaussian distribution on $\\mathbb{R}^{3}$ , and isotropic Gaussian distribution on SO(3) [32] respectively. ScaleRot scales the rotation angle with fixed rotation axis to modify the rotation matrix [18]. ", "page_idx": 3}, {"type": "text", "text": "Correspondingly, the reverse generative process learns to recover data by iterative denoising. The denoising process $p(\\mathcal{R}^{t-1}|\\mathcal{R}^{t},\\mathcal{P})$ from time step $t$ to time step $t-1$ is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\mathbf{s}_{j}^{t-1}|\\mathcal{R}^{t},\\mathcal{P})=\\mathcal{C}(\\mathbf{s}_{j}^{t-1}|\\,f_{\\theta_{1}}(\\mathcal{R}^{t},\\mathcal{P})[j]),}\\\\ &{p(\\mathbf{x}_{j}^{t-1}|\\mathcal{R}^{t},\\mathcal{P})=\\mathcal{N}(\\mathbf{x}_{j}^{t-1}|\\,f_{\\theta_{2}}(\\mathcal{R}^{t},\\mathcal{P})[j],\\beta^{t}I),}\\\\ &{p(\\mathbf{O}_{j}^{t-1}|\\mathcal{R}^{t},\\mathcal{P})=\\mathcal{Z}\\mathcal{G}_{\\operatorname{SO}(3)}(f_{\\theta_{3}}(\\mathcal{R}^{t},\\mathcal{P})[j],\\beta^{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{R}^{t}\\,=\\,\\{\\mathbf{s}_{j},\\mathbf{x}_{j},\\mathbf{O}_{j}\\}_{j=n+1}^{n+m}$ y  ids etnhoei sninogis nye sueraqlu neentcweo raknsd.  sWtreu ucttiulirzee  oSf E(C3D)-Re-quHi3v aarti atnitm nee tswteopr $t$ $f_{\\theta_{1}},f_{\\theta_{2}},f_{\\theta_{3}}$ [23, 25] as denoising networks because proteins are structures in three-dimensional space, and their properties and characteristics should remain invariant regardless of their observation views. $f(\\cdot)[j]$ denotes the output that corresponds to the $j$ -th amino acid. The training objective of the reverse generative process is to minimize the Kullback\u2013Leibler (KL) divergence between the variational distribution $p$ and the posterior distribution $q$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL=\\mathbb{E}_{\\mathcal{R}^{t}\\sim q}\\bigg[\\frac{1}{m}\\sum_{j=n+1}^{n+m}\\mathbb{D}_{\\mathrm{KL}}\\Big(q(\\mathcal{R}^{t-1}[j]|\\mathcal{R}^{t},\\mathcal{R}^{0},\\mathcal{P})\\big|\\big|p_{\\theta}(\\mathcal{R}^{t-1}[j]|\\mathcal{R}^{t},\\mathcal{P})\\Big)\\bigg].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With some algebra, we can simplify the above objective and derive the reconstruction loss at time step $t$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{s}^{t}=\\mathbb{E}_{\\mathcal{R}^{t}}\\left[\\displaystyle\\frac{1}{m}\\sum_{j=n+1}^{n+m}\\mathbb{D}_{\\mathrm{KL}}\\big(q(s_{j}^{t-1}|s_{j}^{t},s_{j}^{0})\\big|\\big|p(s_{j}^{t-1}|\\mathcal{R}^{t},\\mathcal{P})\\big)\\right],}\\\\ &{L_{\\mathbf{x}}^{t}=\\mathbb{E}_{\\mathcal{R}^{t}}\\left[\\displaystyle\\frac{1}{m}\\sum_{j=n+1}^{n+m}\\big\\|{\\mathbf{x}}_{j}^{0}-f_{\\theta_{2}}(\\mathcal{R}^{t},\\mathcal{P})\\big\\|^{2}\\right],}\\\\ &{L_{\\mathbf{o}}^{t}=\\mathbb{E}_{\\mathcal{R}^{t}}\\left[\\displaystyle\\frac{1}{m}\\sum_{j=n+1}^{n+m}\\big\\|({\\mathbf{O}}_{j}^{0})^{\\top}f_{\\theta_{3}}(\\mathcal{R}^{t},\\mathcal{P})[j]-I\\big\\|_{F}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{R}^{t}\\sim q(\\mathcal{R}^{t}|\\mathcal{R}^{0})$ and $\\mathcal{R}^{0}\\sim P(\\mathcal{R}|\\mathcal{P})$ , and $\\|\\cdot\\|_{F}$ is the matrix Frobenius norm. Note that as Luo et al. [36] mentioned, Eqs. (1) and (3) are an empirical perturbation-denosing process instead of a rigorous one. Thus the terminology $K L$ -divergence may not be proper for orientation $\\mathbf{O}$ . Nevertheless, we can still approximately derive an empirical reconstruction loss for orientation $\\mathbf{O}$ as above that works in practice. The overall loss is $L\\approx\\mathbb{E}_{t\\sim\\mathrm{U}[1,T]}[L_{\\mathrm{s}}^{t}+L_{\\mathbf{x}}^{t}+L_{\\mathbf{O}}^{t}]$ . After optimizing this loss, we can start with the noises from the prior distribution and then apply the reverse process to generate antibodies. ", "page_idx": 3}, {"type": "text", "text": "3.2 Direct Energy-based Preference Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Only the antibodies with considerable sequence-structure rationality and binding affinity can be used as effective therapeutic candidates. Fortunately, these two properties can be estimated by biophysical energy. Thus, we introduce direct energy-based preference optimization to fine-tune the pre-trained diffusion models for antibody design. ", "page_idx": 4}, {"type": "text", "text": "Inspired by RLHF [40], we can fine-tune the pre-trained model to maximize the reward as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{\\theta}}\\mathbb{E}_{\\mathcal{R}^{0}\\sim p_{\\theta}}[r(\\mathcal{R}^{0})]-\\beta\\mathbb{D}_{\\mathrm{KL}}(p_{\\pmb{\\theta}}(\\mathcal{R}^{0})\\|p_{\\mathrm{ref}}(\\mathcal{R}^{0})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p_{\\theta}$ (resp. $p_{\\mathrm{ref}})$ is the distribution induced by the model being fine-tuned (resp. the fixed pre-trained model), $\\beta$ is a hyperparameter that controls the KL divergence regularization, and $r(\\cdot)$ is the reward function. The optimal solution to the above objective takes the form: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\theta^{*}}(\\mathcal{R}^{0})=\\frac{1}{Z}p_{\\mathrm{ref}}(\\mathcal{R}^{0})\\exp\\Big(\\frac{1}{\\beta}r(\\mathcal{R}^{0})\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Following Rafailov et al. [41], we turn to the DPO objective as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{\\mathrm{DPO}}\\!=\\!-\\mathbb{E}_{\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0}}\\!\\left[\\log\\sigma\\!\\left(\\beta\\mathrm{sgn}(\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0})\\!\\left[\\log\\!\\frac{p_{\\theta}(\\mathcal{R}_{1}^{0})}{p_{\\mathrm{ref}}(\\mathcal{R}_{1}^{0})}\\!-\\!\\log\\!\\frac{p_{\\theta}(\\mathcal{R}_{2}^{0})}{p_{\\mathrm{ref}}(\\mathcal{R}_{2}^{0})}\\right]\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma(\\cdot)$ is sigmoid and $\\mathrm{sgn}(\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0})$ indicate the preference over $\\mathcal{R}_{1}^{0}$ and $\\mathcal{R}_{2}^{0}$ . We use $\"\\succ\"$ to denote the preference. Specifically, $\\mathrm{sgn}(\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0})={1}$ (resp. $-1)$ if $\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0}$ (resp. $\\mathcal{R}_{2}^{0}\\prec\\mathcal{R}_{1}^{0}$ ) in which case we call $\\mathcal{R}_{1}^{0}$ (resp. $\\mathcal{R}_{2}^{0},$ ) the \u201cwinning\u201d sample and $\\mathcal{R}_{2}^{0}$ (resp. $\\mathcal{R}_{1}^{0}$ ) the \u201closing\u201d sample, and $\\mathrm{sgn}(\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0})=0$ if they tie. $\\mathcal{R}_{1}^{\\tilde{0}}$ and $\\mathcal{R}_{2}^{0}$ are a pair of data sampled from the Bradley-Terry [BT, 8] model with reward $r(\\cdot)$ , i.e., $p(\\dot{\\mathcal{R}}_{1}^{0}\\succ\\tilde{\\mathcal{R}}_{2}^{0})=\\dot{\\sigma}\\big(r(\\mathcal{R}_{1}^{0})-r(\\mathcal{R}_{2}^{\\dot{0}})\\big)$ . Please refer to Appendix $\\mathbf{C}$ for more detailed derivations. ", "page_idx": 4}, {"type": "text", "text": "Due to the intractable $p_{\\theta}(\\mathcal{R}^{0})$ , following Wallace et al. [46], we introduce latent variables $\\mathcal{R}^{1:T}$ and utlize the evidence lower bound optimization (ELBO). In particular, $L_{\\mathrm{DPO}}$ can be modified as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{DPO-Diffusion}}=-\\mathbb{E}_{\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0}}\\left[\\log\\sigma\\left(\\beta\\mathbb{E}_{\\mathcal{R}_{1}^{1:T},\\mathcal{R}_{2}^{1:T}}\\bigg[\\mathrm{sgn}(\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0})\\bigg(\\log\\frac{p_{\\theta}(\\mathcal{R}_{1}^{0:T})}{p_{\\mathrm{ref}}(\\mathcal{R}_{1}^{0:T})}-\\log\\frac{p_{\\theta}(\\mathcal{R}_{2}^{0:T})}{p_{\\mathrm{ref}}(\\mathcal{R}_{2}^{0:T})}\\bigg)\\right]\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Following Wallace et al. [46], we can utilize Jensen\u2019s inequality and convexity of function $-\\log\\sigma$ to derive the following upper bound of $L$ DPO-Diffusion: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{L}_{\\mathrm{DPO-Diffusion}}=-\\mathbb{E}_{t,\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0},(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t}),(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t})}\\Bigg[}\\\\ &{\\log\\sigma\\biggl(\\beta T\\mathrm{sgn}(\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0})\\biggl[\\log\\frac{p_{\\theta}(\\mathcal{R}_{1}^{t-1}|\\mathcal{R}_{1}^{t})}{p_{\\mathrm{ref}}(\\mathcal{R}_{1}^{t-1}|\\mathcal{R}_{1}^{t})}-\\log\\frac{p_{\\theta}(\\mathcal{R}_{2}^{t-1}|\\mathcal{R}_{2}^{t})}{p_{\\mathrm{ref}}(\\mathcal{R}_{2}^{t-1}|\\mathcal{R}_{2}^{t})}\\biggr]\\biggr)\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $t\\sim\\mathcal{U}(0,T)$ , $(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t})$ and $(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t})$ are sampled from reverse generative process of $\\mathcal{R}_{1}^{0}$ and $\\mathcal{R}_{2}^{0}$ , respectively, i.e., $(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t})\\sim p_{\\pmb{\\theta}}(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t}|\\mathcal{R}_{1}^{0})$ and $(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t})\\sim p_{\\pmb{\\theta}}(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t}|\\mathcal{R}_{2}^{0})$ . ", "page_idx": 4}, {"type": "text", "text": "In our case, the antibodies with low energy are desired. Thus, we define the reward $r(\\cdot)$ as $-\\mathcal{E}(\\cdot)/\\tau$ , where $\\mathcal E(\\cdot)$ is the energy function and $\\tau$ is the temperature. Different from the text-to-image generation where the (latent) reward is assigned to a complete image instead of a pixel [46], we know more fine-grained credit assignment. Specifically, it is known that $\\begin{array}{r}{\\mathcal{E}(\\mathcal{R}^{0})\\,=\\,\\sum_{j=n+1}^{n+m}\\mathcal{E}(\\mathcal{R}^{0}[j])}\\end{array}$ i.e., the energy of an antibody is the summation of the energy of its amino acids [4]. Thus the preference can be measured at the residue level instead of the entire CDR level. Besides, we have $\\begin{array}{r}{\\log p_{\\pmb{\\theta}}(\\mathcal{R}^{t-1}|\\mathcal{R}^{t})=\\sum_{j=n+1}^{n+m}\\log p_{\\pmb{\\theta}}(\\mathcal{R}^{t-1}[j]|\\mathcal{R}^{t}),}\\end{array}$ jn=+nm+1 log p\u03b8(Rt\u22121[j]|Rt), which is a common assumption of diffusion models. Thus we can derive a residue-level DPO-Diffusion loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathrm{residue-DP0-Diffusion}}=-\\mathbb{E}_{t,\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0},(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t}),(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t})}\\Bigg[}\\\\ &{\\log\\sigma\\Bigg(\\beta T\\sum_{j=n+1}^{n+m}\\mathrm{sgn}(\\mathcal{R}_{1}^{0}[j],\\mathcal{R}_{2}^{0}[j])\\Bigg[\\log\\frac{p_{\\theta}(\\mathcal{R}_{1}^{t-1}[j]|\\mathcal{R}_{1}^{t})}{p_{\\mathrm{ref}}(\\mathcal{R}_{1}^{t-1}[j]|\\mathcal{R}_{1}^{t})}-\\log\\frac{p_{\\theta}(\\mathcal{R}_{2}^{t-1}[j]|\\mathcal{R}_{2}^{t})}{p_{\\mathrm{ref}}(\\mathcal{R}_{2}^{t-1}[j]|\\mathcal{R}_{2}^{t})}\\Bigg]\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus, by Jensen\u2019s inequality and the convexity of $-\\log\\sigma$ , we can further derive $\\tilde{L}_{1}$ residue-DPO-Diffusion, which is an upper bound of $L_{1}$ esidue-DPO-Diffusion: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{L}_{\\mathrm{residue-DPO-Diffusion}}=-\\mathbb{E}_{t,\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0},(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t}),(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t})}\\Bigg[}\\\\ &{\\sum_{j=n+1}^{n+m}\\log\\sigma\\Bigg(\\beta T\\mathrm{sgn}(\\mathcal{R}_{1}^{0}[j],\\mathcal{R}_{2}^{0}[j])\\Bigg[\\log\\frac{p_{\\theta}(\\mathcal{R}_{1}^{t-1}[j]|\\mathcal{R}_{1}^{t})}{p_{\\mathrm{ref}}(\\mathcal{R}_{1}^{t-1}[j]|\\mathcal{R}_{1}^{t})}-\\log\\frac{p_{\\theta}(\\mathcal{R}_{2}^{t-1}[j]|\\mathcal{R}_{2}^{t})}{p_{\\mathrm{ref}}(\\mathcal{R}_{2}^{t-1}[j]|\\mathcal{R}_{2}^{t})}\\Bigg]\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The gradients of $\\tilde{L}_{\\mathrm{DPO-Diffusion}}$ and $\\tilde{L}_{\\mathrm{re}}$ sidue-DPO-Diffusion w.r.t the parameters $\\pmb{\\theta}$ can be written as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\tilde{L}_{\\mathrm{DPO-Diffusion}}=-\\beta T\\mathbb{E}_{t,\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0},(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t}),(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t})}\\biggl[\\sum_{j=n+1}^{n+m}\\!\\mathrm{sgn}(\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\cdot\\sigma(\\hat{r}(\\mathcal{R}_{2}^{0})-\\hat{r}(\\mathcal{R}_{1}^{0}))\\biggr(\\nabla_{\\theta}\\log p_{\\theta}(\\mathcal{R}_{1}^{t-1}[j]|\\mathcal{R}_{1}^{t})\\!-\\!\\nabla_{\\theta}\\log p_{\\theta}(\\mathcal{R}_{2}^{t-1}[j]|\\mathcal{R}_{2}^{t})\\biggr)\\biggr],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\tilde{L}_{\\mathrm{residue-DPO-Diffusion}}\\!=\\!-\\beta T\\mathbb{E}_{t,\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0},(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t}),(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t})}\\!\\left[\\sum_{j=n+1}^{n+m}\\!\\mathrm{sgn}(\\mathcal{R}_{1}^{0}[j],\\mathcal{R}_{2}^{0}[j])\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\left.\\cdot\\sigma\\big(\\hat{r}(\\mathcal{R}_{2}^{0}[j])-\\hat{r}(\\mathcal{R}_{1}^{0}[j])\\big)\\Big(\\nabla_{\\theta}\\log p_{\\theta}(\\mathcal{R}_{1}^{t-1}[j]|\\mathcal{R}_{1}^{t})\\!-\\!\\nabla_{\\theta}\\log p_{\\theta}(\\mathcal{R}_{2}^{t-1}[j]|\\mathcal{R}_{2}^{t})\\Big)\\right]\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{r}(\\cdot):=\\log(p_{\\pmb\\theta}(\\cdot)/p_{\\mathrm{ref}}(\\cdot))$ , which can be viewed as the estimated reward by current policy $p_{\\theta}$ . ", "page_idx": 5}, {"type": "text", "text": "We can see that $\\nabla_{\\theta}\\tilde{L}_{\\mathrm{DPO}}$ -Diffusion actually reweight $\\nabla_{\\theta}\\log p_{\\theta}(\\mathcal{R}^{t-1}[j]|\\mathcal{R}^{t})$ with the estimated reward of the complete antibody while $\\nabla_{\\theta}\\tilde{L}_{\\mathrm{~\\,~}}$ residue-DPO-Diffusion does this with the estimated reward of the amino acid itself. In this case, $\\nabla_{\\theta}\\tilde{L}_{\\mathrm{DPO-Diffusion}}$ will increase (resp. decrease) the likelihood of all amino acids of the \u201cwinning\u201d sample (resp. \u201closing\u201d) at the same rate, which may mislead the optimization direction. In contrast, $\\nabla_{\\theta}\\tilde{L}_{1}$ residue-DPO-Diffusion does not have this issue and can fully utilize the residue-level signals from estimated reward to effectively optimize antibodies. ", "page_idx": 5}, {"type": "text", "text": "We further approximate the objective L\u02dcresidue-DPO-Diffusion by sampling from the forward diffusion process $q$ instead of the reverse generative process $p_{\\theta}$ to achieve diffusion-like efficient training. With further replacing $\\log{\\frac{p_{\\theta}}{p_{\\mathrm{ref}}}}$ with $\\begin{array}{r}{-\\log\\frac{q}{p_{\\theta}}+\\log\\frac{p_{\\mathrm{ref}}}{q}}\\end{array}$ which is exactly $-\\mathbb{D}_{K L}(q\\|p_{\\pmb{\\theta}})+\\mathbb{D}_{K L}(q\\|p_{\\mathrm{ref}})$ when taking expectation with respect to $q$ , we can derive the final loss for fine-tuning the diffusion model as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathrm{ABDPO}}=-\\mathbb{E}_{t,\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0},(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t}),(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t})}\\left[\\sum_{j=n+1}^{n+m}\\log\\sigma\\Big(\\!-\\!\\beta T\\mathrm{sgn}(\\mathcal{R}_{1}^{0}[j],\\mathcal{R}_{2}^{0}[j])\\!\\right)\\right.}\\\\ &{\\qquad\\left.\\cdot\\left\\lbrace\\mathbb{D}_{\\mathrm{KL},1}^{t}(q\\|p_{\\theta})[j]-\\mathbb{D}_{\\mathrm{KL},1}^{t}(q\\|p_{\\mathrm{ref}})[j]-\\mathbb{D}_{\\mathrm{KL},2}^{t}(q\\|p_{\\theta})[j]+\\mathbb{D}_{\\mathrm{KL},2}^{t}(q\\|p_{\\mathrm{ref}})[j]\\right\\rbrace\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0}\\sim p_{\\pmb{\\theta}}(\\mathcal{R}).$ , $(\\mathcal{R}_{1}^{t-1},\\mathcal{R}_{1}^{t})$ and $(\\mathcal{R}_{2}^{t-1},\\mathcal{R}_{2}^{t})$ are sampled from forward diffusion process of $\\mathcal{R}_{1}^{0}$ and $\\mathcal{R}_{2}^{0}$ , respectively, which can be much more efficient than the reverse generative process that involves hundreds of model forward estimation. Here we use $\\mathbb{D}_{\\mathrm{KL,1}}^{t}(q||p_{\\pmb{\\theta}})[j]$ to denote $\\mathbb{D}_{\\mathrm{KL}}\\big(q\\big(\\mathcal{R}_{1}^{t-1}[j]\\vert\\mathcal{R}^{t-1},\\mathcal{R}^{0}\\big)\\|p_{\\theta}\\big(\\mathcal{R}_{1}^{t-1}[j]\\vert\\mathcal{R}^{0}\\big)\\big)$ . Similar for $\\mathbb{D}_{\\mathrm{KL,1}}^{t}(q\\|p_{\\mathrm{ref}})[j]$ , $\\mathbb{D}_{\\mathrm{KL,2}}^{t}(q||p_{\\pmb{\\theta}})[j]$ , and $\\mathbb{D}_{\\mathrm{KL,2}}^{t}(q||p_{\\mathrm{ref}})[j]$ . These KL divergence can be estimated as in Eqs. (5) to (7). ", "page_idx": 5}, {"type": "text", "text": "3.3 Energy Decomposition and Conflict Mitigation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The energy usually consists of different types, such as attraction and repulsion. Empirically, direct optimization on single energy will lead to some undesired \u201cshortcuts\u201d. Specifically, in some cases, repulsion dominates the energy of the antibody so the model will push antibodies as far from the antigen as possible to decrease the repulsion during optimization, and finally fall into a bad local minima. This effectively reduces the repulsion, but also completely eliminates the attraction between antibodies and antigens, which seriously impairs the functionality of the antibody. This motivates us to explicitly express the energy with several distinct terms and then control the optimization process towards our preference. ", "page_idx": 5}, {"type": "text", "text": "Inspired by Yu et al. [51], we utilize \u201cgradient surgery\u201d to alleviate interference between different types of energy during energy preference optimization. More specifically, we have $\\begin{array}{r}{\\mathcal{E}(\\cdot)=\\sum_{v=1}^{V}w_{v}\\dot{\\mathcal{E}_{v}}(\\cdot)}\\end{array}$ where $V$ is the number of types of energy, and $w_{v}$ is a constant weight for the $v$ -th  kind of energy. For each type of energy $\\mathcal{E}_{v}(\\cdot)$ , we compute its corresponding energy preference gradient $\\nabla_{\\theta}L_{v}$ as ", "page_idx": 5}, {"type": "table", "img_path": "GN2GXjPyN8/tmp/9257dbcccc8fa3a113f873f4ecf49dc75716b015c3a7e401e2394cbe35ddd0aa.jpg", "table_caption": ["Table 1: Summary of AAR, RMSD, CDR $E_{\\mathrm{total}}$ , CDR-Ag $\\Delta G$ (kcal/mol), pLL, PHR, and $\\mathsf{N}_{\\mathrm{success}}$ of antibodies designed by our model and baselines. $\\left(\\downarrow\\right)/$ (\u2191) denotes a smaller / larger number is better. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Eq. (8), and then alter the gradient by projecting it onto the normal plane of the other gradients (in a random order) if they have conflicts. This process works as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}L_{v}\\leftarrow\\nabla_{\\theta}L_{v}-\\frac{\\operatorname*{min}{(\\nabla_{\\theta}L_{v}^{\\top}\\nabla_{\\theta}L_{u},0)}}{\\left\\|\\nabla_{\\theta}L_{u}\\right\\|^{2}}\\nabla_{\\theta}L_{u},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $v\\in\\{1,\\ldots,V\\}$ and $\\boldsymbol{u}=\\mathtt{S h u f f l e}(1,\\dots,V)$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset Curation To pre-train the diffusion model for antibody generation, we use the Structural Antibody Database [SAbDab, 13] under IMGT [34] scheme as the dataset. We collected antigenantibody complexes with both heavy and light chains and protein antigens and discarded the duplicate data with the same CDR-L3 and CDR-H3 sequence. The remaining complexes are used to cluster via MMseqs2 [44] with $40\\%$ sequence similarity as the threshold based on the CDR-H3 sequence of each complex. We then select the clusters that do not contain complexes in RAbD benchmark [1] and split the complexes into training and validation sets with a ratio of 9:1 (1786 and 193 complexes respectively). Specifically, the validation set is composed of clusters that only contain one complex. The test set consists of 55 eligible complexes from the RAbD benchmark (details in Appendix D.2). ", "page_idx": 6}, {"type": "text", "text": "For the synthetic data used in ABDPO fine-tuning, 10,112 samples are randomly sampled for each antigen-antibody complex in the test set using the aforementioned pre-trained diffusion model. Then, we use pyRosetta [9] to apply the side-chain packing for these samples. ", "page_idx": 6}, {"type": "text", "text": "Preference Definition To apply ABDPO, we need to build the preference dataset and construct the \u201cwinning\u201d and \u201closing\u201d pair. The accurate relationship between preferences based on in silico with wet-lab experimental results is a scientific issue that remains unresolved, with a wide range of opinions. ABDPO\u2019s solution to this open question is to provide a generic framework that allows for arbitrary definitions and combinations of preferences to satisfy various requirements in antibody design. ", "page_idx": 6}, {"type": "text", "text": "To demonstrate the effectiveness of ABDPO, we define the preferences as lower total energy and lower binding energy. The two energies are defined on residue level, specifically, (1) ResCDR $E_{\\mathrm{total}}$ is the total energy of each residue within the designed CDR, and is used to represent the overall rationality of the corresponding residue; (2) ResCDR-Ag $\\Delta\\mathbf G$ is the interaction energy between each designed CDR residue and the target antigen, representing the functionality of the corresponding residue. ResCDR-Ag \u2206G is further decomposed into (2.1) ResCDR-Ag $E_{\\mathrm{nonRep}}$ , the sum of the interaction energies except repulsion between the designed CDR residue and the antigen, and (2.2) ResCDR- $\\mathbf{\\cdotAg}$ $E_{\\mathrm{Rep}}$ , the repulsion energy between the design CDR residue and the antigen. ", "page_idx": 6}, {"type": "text", "text": "As a generic framework, ABDPO also supports non-energy-based preferences. To verify this, we demonstrate an advanced version named $\\mathrm{ABDPO+}$ . ABD $\\mathrm{PO+}$ incorporates two additional preferences: pseudo log-likelihood (pLL) from AntiBERTy [42] and the percent of hydrophobicity residues (PHR). Different from the previously mentioned energy-based preferences, pLL and PHR are defined on the whole CDR level. For pLL, a higher value is considered better and is designated as \u201cwinning\u201d, conversely; for PHR, a lower value is preferable. ", "page_idx": 6}, {"type": "text", "text": "Baselines We compare our model with various representative antibody sequence-structure co-design baselines. HERN [21] designs sequences of antibodies autoregressively with the iterative refinement of structures; MEAN [29] generates sequences and structures of antibodies via a progress fullshot scheme; dyMEAN [30] designs antibodies sequences and structures with full-atom modeling; DiffAb [36] models antibody distributions with a diffusion model that considers the amino acid type, $\\textstyle\\mathbf{C}_{\\alpha}$ positions and side-chain orientations, which is a more rigorous generative model than the above baselines. The side-chain atoms are packed by pyRosetta. For dyMEAN, we (1) provide the ground-truth framework structure as input like other methods, (2) only use its generated backbones and pack the side-chain atoms by pyRosetta for a more fair comparison. ", "page_idx": 6}, {"type": "image", "img_path": "GN2GXjPyN8/tmp/42ecebf6332182f35d8fb1c38b7b44df0e3209ce7027f9ba90e69f14019b66cc.jpg", "img_caption": ["Figure 3: Visualization of reference antibodies in RAbD and antibodies designed by ABDPO given specific antigens (PDB ID: 1iqd (left), 1ic7 (middle), and 2dd8 (right)). The unit of energy annotated is kcal/mol and omitted here for brevity. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Evaluation Following the previous studies, we preliminarily evaluate the generated sequence and structure with AAR and $\\mathrm{C}\\alpha$ RMSD. Besides, we carry out a series of more reasonable metrics. We utilize the preferences aforementioned to evaluate the designed antibodies from multiple perspectives, but at the whole CDR level. Specifically, (1) CDR $E_{\\mathrm{total}}$ , the total energy of the designed CDR, is utilized to evaluate the rationality by aggregating all ResCDR $E_{\\mathrm{total}}$ of residues within the CDR; (2) CDR-Ag $\\Delta G$ denotes the difference in total energy between the bound state and the unbound state of the CDR and antigen, which is calculated to evaluate the functionality. PHR and pLL remain the same definition as above. All methods are able to generate multiple antibodies for a specific antigen (a randomized version of MEAN, rand-MEAN, is used here). We employ each method to design 192 antibodies for each complex, and we report the mean metrics across all 55 complexes. We further report the number of successfully designed antibody-antigen complexes, $\\mathbf{N}_{\\mathrm{success}}$ , to evaluate their rationality and functionality comprehensively. The design for an antibody-antigen complex is considered as \u201csuccessful\u201d when at least one generated sample holds energies close to or lower than the natural one, i.e., for both of the two energy types, $E_{\\mathrm{generated}}<E_{\\mathrm{natural}}+\\mathrm{std}(E_{\\mathrm{natural}}^{\\mathrm{all-complexes}})$ ", "page_idx": 7}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We report the evaluation metrics in Tab. 1. As the results show, ABDPO performs significantly superior to other antibody sequence-structure co-design methods in the two energy-based metrics, CDR $E_{\\mathrm{total}}$ and CDR-Ag $\\Delta G$ , while maintaining the AAR and RMSD. With the two additional preferences, $\\mathrm{ABDPO+}$ avoids the expense of the increased PHR while achieving better performance than DiffAb in remaining metrics (even surpassing DiffAb in AAR). This demonstrates the effectiveness and compatibility of ABDPO in terms of optimizing multi-objectives simultaneously. We have also provided the detailed evaluation results for each complex in Appendix E.2. ", "page_idx": 7}, {"type": "text", "text": "We do not consider AAR and RMSD as the main reference evaluation metrics as their inadequacy (refer to Appendix A for more details). With the new evaluation methods, issues that used to be hidden by AAR and RMSD are exposed. It is observed that structural clashes can not be avoided completely in any method, resulting in the high energy values of generated antibodies, even for ABDPO and $\\mathrm{ABDPO+}$ . The structural clashes between CDR and the antigen finally lead to the unreasonable high CDR-Ag $\\Delta G$ . However, the primary goal in antibody design is to generate at least one effective antibody. Given the complexity of protein interactions, it is not plausible that every generated antibody will yield effectiveness. Therefore, $\\mathbf{N}_{\\mathrm{success}}$ is a more valuable metric. ABDPO and ABDPO $^+$ are the only two to achieve successful cases, with 9 and 5 successful cases out of 55 complexes, respectively. Following this concept, we also rank the designed antibodies for each complex by a uniform strategy (see Appendix D.3), calculate the metrics of the highest-ranked design for each complex, and report the mean metrics across the 55 complexes (see Appendix E.1). Notably, ABDPO is the only method that achieves CDR-Ag $\\Delta G$ lower than 0. ", "page_idx": 7}, {"type": "image", "img_path": "GN2GXjPyN8/tmp/2008e6ec80a8829e04b1adc715acf6cb452bb07c8ea050687df30378a88fffbe.jpg", "img_caption": ["Figure 4: Changes of median CDR $E_{\\mathrm{total}}$ , $E_{\\mathrm{nonRep}}$ , $E_{\\mathrm{Rep}}$ , and CDR-Ag \u2206G (kcal/mol) overoptimization steps, shaded to indicate interquartile range (from 25-th percentile to 75-th percentile). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We also visualize three cases (PDB ID: 1iqd, 1ic7, and 2dd8) in Fig. 3. It is shown that ABDPO can design CDRs with both fewer clashes and proper relative spatial positions towards the antigens, and even better energy performance than that of natural antibodies. ", "page_idx": 8}, {"type": "text", "text": "We conduct another two experiments to demonstrate further the generality of ABDPO: (1) directly incorporate auxiliary training losses for those properties of which gradients are computable; (2) introduce energy minimization before energy calculation, which is more in line with the real workflow. ABDPO shows consistent performance and demonstrates its generality. Please refer to Appendix F for related details. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our approach comprises three main novel designs, including residue-level direct energy-based preference optimization, energy decomposition, and confilct mitigation by gradient surgery. Thus we perform comprehensive ablation studies to verify our hypothesis on the effects of each respective design component. Here we take the experiment on one complex (PDB ID: 1a14) as the example. Here, we apply more fine-tuning steps and additionally introduce $E_{\\mathrm{nonRep}}$ (aggregation of ResCDR-Ag $E_{\\mathrm{nonRep}}$ within the designed CDR), $E_{\\mathrm{Rep}}$ (aggregation of ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\;{\\dot{E}}_{\\mathrm{Rep}})$ for a more obvious and detailed comparison. More cases of ablation studies can be found in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Effects of Residue-level Energy Preference Optimization We hypothesize that residue-level DPO leads to more explicit and intuitive gradients that can promote effectiveness and efficiency compared with the vanilla DPO [46] as the analysis in Sec. 3.2. To validate this, we compare ABDPO with its counterpart with the CDR-level preference instead of residue-level. As Fig. 4 shows, regarding the counterpart (blue dotted line), the changes in all metrics are not obvious, while almost all metrics rapidly converge to an ideal state in ABDPO (red line). This demonstrated the effects of residue-level energy preference in improving the optimization efficiency. ", "page_idx": 8}, {"type": "text", "text": "Effects of Energy Decomposition In generated antibodies, the huge repulsion caused by clashes accounts for the majority of the two types of energy. This prevents us from using the $\\Delta G$ as an optimization objective directly as the model is allowed to minimize repulsion by keeping antibodies away from antigens, quickly reducing the energies. To verify this, we compared ABDPO with a version that directly optimize $\\Delta G$ . As shown in Fig. 4, without energy decomposition (green dashed line), both $E_{\\mathrm{Rep}}$ and $E_{\\mathrm{nonRep}}$ quickly diminish to 0, indicating that there is no interaction between the generated antibodies and antigens. Conversely, ABDPO (red line) can minimize $E_{\\mathrm{Rep}}$ to 0 while maintaining $E_{\\mathrm{nonRep}}$ , which means the interactions are preserved. ", "page_idx": 8}, {"type": "text", "text": "Effects of Gradient Surgery To show the effectiveness of gradient surgery in mitigating confilcts when optimizing multiple objectives, we compare ABDPO and its counterpart without gradient surgery. As Fig. 4 shows, the counterpart (purple dashed line) can only slightly optimize CDR-Ag $E_{\\mathrm{nonRep}}$ but incurs strong repulsion (i.e., $E_{\\mathrm{Rep}},$ ), learning to irrational structures. ABDPO (red line) can converge to a state where CDR $E_{\\mathrm{total}}$ and $\\dot{E}_{\\mathrm{Rep}}$ achieve a conspicuously low point, suggesting the generated sequences and structures are stable, and $E_{\\mathrm{nonRep}}$ is still significantly less than zero, showing that considerable binding affinity is kept. ", "page_idx": 8}, {"type": "text", "text": "Comparison with Supervised Fine-tuning Supervised Fine-tuning (SFT) can be an alternative way of generating antibodies with lower energy. For SFT, we first select the top $10\\%$ high-quality samples from ABDPO training data on a complex (PDB ID: 1a14). We fine-tune the diffusion model under the same settings as ABDPO. Results in Tab. 2 show that SFT only marginally surpasses the pre-trained diffusion model, and ABDPO performs significantly superior to SFT. We attribute the performance of ABDPO to the preference optimization scheme and the fine-grained residue-level energy rather than the entire CDR. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Comparison of ABDPO and supervised fine-tuning (SFT) on 1a14. ", "page_idx": 9}, {"type": "table", "img_path": "GN2GXjPyN8/tmp/399994c6cdb0a77ec15aee46edc1305bc1c8eb92d6e6fd94a7f7b11eecfe10d7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we rethink antibody sequence-structure co-design through the lens of energy and propose ABDPO for designing antibodies meeting multi-objectives like rationality and functionality. The introduction of direct energy-based preference optimization along with energy decomposition and conflict mitigation by gradient surgery shows promising results in generating antibodies with low energy and high binding affinity. With ABDPO, existing computing software and domain knowledge can be easily combined with deep learning techniques, jointly facilitating the development of antibody design. Limitations and future work are discussed in Appendix H. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jared Adolf-Bryfogle, Oleks Kalyuzhniy, Michael Kubitz, Brian D Weitzner, Xiaozhen Hu, Yumiko Adachi, William R Schief, and Roland L Dunbrack Jr. 2018. RosettaAntibodyDesign (RAbD): A general framework for computational antibody design. PLoS computational biology, 14(4):e1006112.   \n[2] Rahmad Akbar, Philippe A. Robert, Milena Pavlovi\u00b4c, Jeliazko R. Jeliazkov, Igor Snapkov, Andrei Slabodkin, C\u00e9dric R. Weber, Lonneke Scheffer, Enkelejda Miho, Ingrid Hob\u00e6k Haff, Dag Trygve Tryslew Haug, Fridtjof Lund-Johansen, Yana Safonova, Geir K. Sandve, and Victor Greiff. 2021. A compact vocabulary of paratope-epitope interactions enables predictability of antibody-antigen binding. Cell Reports, 34(11):108856.   \n[3] Rahmad Akbar, Philippe A Robert, C\u00e9dric R Weber, Michael Widrich, Robert Frank, Milena Pavlovi\u00b4c, Lonneke Scheffer, Maria Chernigovskaya, Igor Snapkov, Andrei Slabodkin, et al. 2022. In silico proof of principle of machine learning-based antibody design at unconstrained scale. In MAbs, volume 14, page 2031482. Taylor & Francis. [4] Rebecca F. Alford, Andrew Leaver-Fay, Jeliazko R. Jeliazkov, Matthew J. O\u2019Meara, Frank P. DiMaio, Hahnbeom Park, Maxim V. Shapovalov, P. Douglas Renfrew, Vikram K. Mulligan, Kalli Kappel, Jason W. Labonte, Michael S. Pacella, Richard Bonneau, Philip Bradley, Roland L. Jr. Dunbrack, Rhiju Das, David Baker, Brian Kuhlman, Tanja Kortemme, and Jeffrey J. Gray. 2017. The Rosetta All-Atom Energy Function for Macromolecular Modeling and Design. Journal of Chemical Theory and Computation, 13(6):3031\u20133048. PMID: 28430426.   \n[5] Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church. 2019. Unified rational protein engineering with sequence-based deep representation learning. Nature methods, 16(12):1315\u20131322. [6] Anonymous. 2023. Proximal Preference Optimization for Diffusion Models.   \n[7] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. 2023. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301. [8] Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika, 39(3/4):324\u2013345.   \n[9] Sidhartha Chaudhury, Sergey Lyskov, and Jeffrey J. Gray. 2010. PyRosetta: a script-based interface for implementing molecular modeling algorithms using Rosetta. Bioinformatics, 26(5):689\u2013691.   \n[10] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024. Selfplay fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335.   \n[11] Gavin E Crooks, Gary Hon, John-Marc Chandonia, and Steven E Brenner. 2004. WebLogo: a sequence logo generator. Genome research, 14(6):1188\u20131190.   \n[12] Jinhui Dong, Seth J Zost, Allison J Greaney, Tyler N Starr, Adam S Dingens, Elaine C Chen, Rita E Chen, James Brett Case, Rachel E Sutton, Pavlo Gilchuk, et al. 2021. Genetic and structural basis for SARS-CoV-2 variant neutralization by a two-antibody cocktail. Nature microbiology, 6(10):1233\u20131244.   \n[13] James Dunbar, Konrad Krawczyk, Jinwoo Leem, Terry Baker, Angelika Fuchs, Guy Georges, Jiye Shi, and Charlotte M Deane. 2014. SAbDab: the structural antibody database. Nucleic acids research, 42(D1):D1140\u2013D1146.   \n[14] Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al. 2017. OpenMM 7: Rapid development of high performance algorithms for molecular dynamics. PLoS computational biology, 13(7):e1005659.   \n[15] Stefan Ewert, Annemarie Honegger, and Andreas Pl\u00fcckthun. 2004. Stability improvement of antibodies for extracellular and intracellular applications: CDR grafting to stable frameworks and structure-based framework engineering. Methods, 34(2):184\u2013199. Intrabodies.   \n[16] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. 2023. Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. In Thirty-seventh Conference on Neural Information Processing Systems.   \n[17] Noelia Ferruz, Steffen Schmidt, and Birte H\u00f6cker. 2022. ProtGPT2 is a deep unsupervised language model for protein design. Nature communications, 13(1):4348.   \n[18] Jean Gallier and Dianna Xu. 2003. Computing exponentials of skew-symmetric matrices and logarithms of orthogonal matrices. International Journal of Robotics and Automation, 18(1):10\u201320.   \n[19] Kaiyuan Gao, Lijun Wu, Jinhua Zhu, Tianbo Peng, Yingce Xia, Liang He, Shufang Xie, Tao Qin, Haiguang Liu, Kun He, et al. 2023. Pre-training Antibody Language Models for AntigenSpecific Computational Antibody Design. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 506\u2013517.   \n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc.   \n[21] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. 2022. Antibody-antigen docking and design via hierarchical structure refinement. In International Conference on Machine Learning, pages 10217\u201310227. PMLR.   \n[22] Wengong Jin, Jeremy Wohlwend, Regina Barzilay, and Tommi S. Jaakkola. 2022. Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design. In International Conference on Learning Representations.   \n[23] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. 2021. Learning from Protein Structure with Geometric Vector Perceptrons. In International Conference on Learning Representations.   \n[24] Peter T Jones, Paul H Dear, Jefferson Foote, Michael S Neuberger, and Greg Winter. 1986. Replacing the complementarity-determining regions in a human antibody with those from a mouse. Nature, 321(6069):522\u2013525.   \n[25] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. 2021. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583\u2013589.   \n[26] Kazutaka Katoh and Daron M Standley. 2013. MAFFT multiple sequence alignment software version 7: improvements in performance and usability. Molecular biology and evolution, 30(4):772\u2013780.   \n[27] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.   \n[28] Miltiadis Kofinas, Naveen Shankar Nagaraja, and Efstratios Gavves. 2021. Roto-translated Local Coordinate Frames For Interacting Dynamical Systems. In Advances in Neural Information Processing Systems.   \n[29] Xiangzhe Kong, Wenbing Huang, and Yang Liu. 2023. Conditional Antibody Design as 3D Equivariant Graph Translation. In The Eleventh International Conference on Learning Representations.   \n[30] Xiangzhe Kong, Wenbing Huang, and Yang Liu. 2023. End-to-End Full-Atom Antibody Design. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 17409\u201317429. PMLR.   \n[31] Gideon D Lapidoth, Dror Baran, Gabriele M Pszolla, Christoffer Norn, Assaf Alon, Michael D Tyka, and Sarel J Fleishman. 2015. Abdesign: A n algorithm for combinatorial backbone design guided by natural conformations and sequences. Proteins: Structure, Function, and Bioinformatics, 83(8):1385\u20131406.   \n[32] Adam Leach, Sebastian M Schmon, Matteo T Degiacomi, and Chris G Willcocks. 2022. Denoising diffusion probabilistic models on so (3) for rotational alignment. In ICLR 2022 Workshop on Geometrical and Topological Representation Learning.   \n[33] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267.   \n[34] Marie-Paule Lefranc, Christelle Pommi\u00e9, Manuel Ruiz, V\u00e9ronique Giudicelli, Elodie Foulquier, Lisa Truong, Val\u00e9rie Thouvenin-Contet, and G\u00e9rard Lefranc. 2003. IMGT unique numbering for immunoglobulin and T cell receptor variable domains and Ig superfamily V-like domains. Developmental & Comparative Immunology, 27(1):55\u201377.   \n[35] Ge Liu, Haoyang Zeng, Jonas Mueller, Brandon Carter, Ziheng Wang, Jonas Schilz, Geraldine Horny, Michael E Birnbaum, Stefan Ewert, and David K Gifford. 2020. Antibody complementarity determining region design using high-capacity machine learning. Bioinformatics, 36(7):2126\u20132133.   \n[36] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. 2022. Antigen-Specific Antibody Design and Optimization with Diffusion-Based Generative Models for Protein Structures. In Advances in Neural Information Processing Systems.   \n[37] Karolis Martinkus, Jan Ludwiczak, WEI-CHING LIANG, Julien Lafrance-Vanasse, Isidro Hotzel, Arvind Rajpal, Yan Wu, Kyunghyun Cho, Richard Bonneau, Vladimir Gligorijevic, and Andreas Loukas. 2023. AbDiffuser: full-atom generation of in-vitro functioning antibodies. In Thirty-seventh Conference on Neural Information Processing Systems.   \n[38] Sanzo Miyazawa and Robert L Jernigan. 1985. Estimation of effective interresidue contact energies from protein crystal structures: quasi-chemical approximation. Macromolecules, 18(3):534\u2013552.   \n[39] Kenneth Murphy and Casey Weaver. 2016. Janeway\u2019s immunobiology. Garland science.   \n[40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.   \n[41] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In Thirty-seventh Conference on Neural Information Processing Systems.   \n[42] Jeffrey A Ruffolo, Jeffrey J Gray, and Jeremias Sulam. 2021. Deciphering antibody affinity maturation with language models and weakly supervised learning. arXiv preprint arXiv:2112.07782.   \n[43] Koichiro Saka, Taro Kakuzaki, Shoichi Metsugi, Daiki Kashiwagi, Kenji Yoshida, Manabu Wada, Hiroyuki Tsunoda, and Reiji Teramoto. 2021. Antibody design using LSTM based deep generative model from phage display library for affinity maturation. Scientific reports, 11(1):5852.   \n[44] Martin Steinegger and Johannes S\u00f6ding. 2017. MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature biotechnology, 35(11):1026\u20131028.   \n[45] Gabriel D Victora and Michel C Nussenzweig. 2012. Germinal centers. Annual review of immunology, 30:429\u2013457.   \n[46] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. 2023. Diffusion Model Alignment Using Direct Preference Optimization. arXiv preprint arXiv:2311.12908.   \n[47] Shira Warszawski, Aliza Borenstein Katz, Rosalie Lipsh, Lev Khmelnitsky, Gili Ben Nissan, Gabriel Javitt, Orly Dym, Tamar Unger, Orli Knop, Shira Albeck, et al. 2019. Optimizing antibody affinity and stability by the automated design of the variable light-heavy chain interfaces. PLoS computational biology, 15(8):e1007207.   \n[48] Fang Wu and Stan Z. Li. 2023. A Hierarchical Training Paradigm for Antibody Structuresequence Co-design. In Thirty-seventh Conference on Neural Information Processing Systems.   \n[49] John L Xu and Mark M Davis. 2000. Diversity in the CDR3 Region of VH Is Sufficient for Most Antibody Specificities. Immunity, 13(1):37\u201345.   \n[50] Jason Yim, Brian L. Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. 2023. SE(3) diffusion model with application to protein backbone generation. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 40001\u201340039. PMLR.   \n[51] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824\u20135836.   \n[52] Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei YE, and Quanquan Gu. 2023. Structureinformed Language Models Are Protein Designers. In International Conference on Machine Learning.   \n[53] Xiangxin Zhou, Liang Wang, and Yichi Zhou. 2024. Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process. arXiv preprint arXiv:2403.04154. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Motivation for Choosing Energy as Evaluation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "There are many inadequacies in using AAR and RMSD as the main evaluation metrics in AI-based antibody design. Antibody design is a typical function-oriented protein design task, necessitating a more fine-grained measure of discrepancy compared to general protein design tasks. Especially when the part of the antibody to be designed and evaluated, CDR-H3, is usually shorter, more precise evaluation becomes particularly important. ", "page_idx": 13}, {"type": "image", "img_path": "GN2GXjPyN8/tmp/991fa23ea559b3e990f18a98335b5ec82d1dc210298eb15949d1bbb2036e6bdd.jpg", "img_caption": ["Figure 5: A: Tyr (Y) and Phe (F) differ by only one oxygen atom. In contrast, there is a substantial difference between Gly (G) and Trp (W). Gly lacks a side chain, whereas Trp possesses the largest side chain of all amino acids. B: the visualization of the frequency of occurrence of each amino acid at various positions in RAbD CDR-H3 sequences. The sequences are initially aligned using MAFFT [26] and subsequently visualized with WebLogo [11]. The width of each column corresponds to the frequency of occurrence at that position. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "For AAR, there are two main limitations in measuring the similarity between the generated sequence and the reference sequence. The first limitation is located in measuring the difference in different incorrect recoveries. Among the 20 common amino acids, some have high similarity between them, such as Tyr and Phe, while others have significant differences, such as Gly and Trp (Fig. 5A). When an amino acid in CDR is erroneously recovered to different amino acids, their impact will also vary. However, AAR does not differentiate between these different types of errors, only identifying them as \u201cincorrect\u201d. ", "page_idx": 13}, {"type": "text", "text": "A further, more serious issue is that AAR is easily hacked. Although the CDR region is often considered hypervariable, a mild conservatism in sequence still exists (Fig. 5B), which allows the model to obtain satisfactory AAR using a simple but incorrect way - directly generating the amino acids with the highest probability of occurrence at each position, while ignoring the condition of the given antigen which is extremely harmful to the specificity of antibodies. We made a simple attempt by simply counting the amino acids with the highest frequency of occurrence at various positions in all samples in SAbDab, and then composing them into a CDR-H3 sequence, which looks roughly like \u201c $\\mathrm{\\Delta(RD+rand(Y,G)*+FDY^{*}}$ , achieving an AAR of $\\mathbf{38.77\\%}$ on the RAbD dataset. ", "page_idx": 13}, {"type": "image", "img_path": "GN2GXjPyN8/tmp/725b375cace4724f140af1dde132d2680eae002a19bc0e49724b249da59ae334.jpg", "img_caption": ["Figure 6: The distribution of CDR-Ag $E_{\\mathrm{nonRep}}$ (left) and CDR-Ag $E_{\\mathrm{Rep}}$ (right) formed by the whole CDR atoms (colored in red) and solely by CDR side-chain atoms (colored in blue) among SAbDab dataset. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "While RMSD fails to measure the discrepancies on side-chain atoms, in general, the calculation of RMSD focuses on the alpha carbon atom or the four backbone atoms due to their stable existence in any type of amino acid and thus ignores the side-chain atoms. However, side-chain atoms in the CDR region are extremely important as they contribute to most of the interactions between the CDR and the antigen. Our analyses on the SAbDab dataset also prove the importance of the side chain in CDR-Antigen interaction in terms of energy. As shown in Fig. 6, the distribution of energies formed by the whole residues in CDR is colored in red while the distribution of energies formed only by side-chain atoms of CDR is colored in blue. The interaction energy formed by side-chain atoms accounts for the vast majority of the total interaction energy in both types of energy. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "The above reasons have led us to abandon AAR and RMSD as learning objectives and evaluation metrics, and instead use energy as our goal. Energy can simultaneously consider the relationship between structure and sequence, distinguish different generation results in more detail, and importantly, reflect the rationality and functionality of antibodies in a more fundamental way. Despite the various shortcomings of AAR and RMSD, we have demonstrated that the antibodies generated by ABDPO achieve lower AAR and comparable RMSD compared to those generated by other methods . However, in practice, ABDPO-generated antibodies exhibit distinct binding patterns to antigens, differing from reference antibodies, and demonstrate significantly better energy performance than those produced by other methods. This further highlights the inadequacies of using AAR and RMSD as evaluation metrics in antibody design tasks, exposing their vulnerability to being \u201chacked\u201d. ", "page_idx": 14}, {"type": "text", "text": "B Energy Calculation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In ABDPO, we conduct the calculation on ResCDR $E_{\\mathrm{total}}$ at residue level, and a more fine-grained calculation on the two functionality-associated energies at the sub-residue level. We use Rosetta to calculate all types of energies in this paper. ", "page_idx": 14}, {"type": "text", "text": "We denote the residue with the index $i$ in the antibody-antigen complex as $A_{i}$ , then $A_{i}^{s c}$ and $A_{i}^{b b}$ represent the side chain and backbone of the residue respectively. ", "page_idx": 14}, {"type": "text", "text": "For the energies in the proposed preference, we describe the function for energies of a Single residue as ES, and $\\mathrm{ES}_{\\mathrm{total}}$ is the sum of all types of energy with the default weight in REF15 [4]. The function for interaction energies between Paired residues is described as EP, which consists of six different energy types: $\\mathrm{EP}_{\\mathrm{hbond}}$ , $\\mathrm{EP_{att}}$ , $\\mathrm{EP}_{\\mathrm{rep}}$ , $\\mathrm{EP}_{\\mathrm{sol}}$ , $\\mathrm{EP_{elec}}$ , and $\\mathrm{EP_{lk}}$ . ", "page_idx": 14}, {"type": "text", "text": "Following the settings previously mentioned in Sec. 3.1, the indices of residues within the CDR-H3 range from $n+1$ to $n+m$ , and the indices of residues within the antigen range from $g+1$ to $g+k$ . Then, for the CDR residue with the index $j$ , the three types of energy are defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Res}_{\\mathrm{CDR}}\\;E_{\\mathrm{total}}^{j}=\\mathrm{ES}_{\\mathrm{total}}(A_{j}),}\\\\ &{\\mathrm{Res}_{\\mathrm{CDR}}\\mathrm{-Ag}\\;E_{\\mathrm{nonRep}}^{j}=\\displaystyle\\sum_{i=g+1\\;\\mathrm{e}\\in\\{\\mathrm{hbond},\\mathrm{stelec},\\mathrm{lec},\\mathrm{le}\\}}\\Big(\\mathrm{EP}_{\\mathrm{e}}(A_{j}^{s c},A_{i}^{s c})+\\mathrm{EP}_{\\mathrm{e}}(A_{j}^{s c},A_{i}^{b b})\\Big),}\\\\ &{\\mathrm{Res}_{\\mathrm{CDR}}\\mathrm{-Ag}\\;E_{\\mathrm{Rep}}^{j}=\\displaystyle\\sum_{i=g+1}^{g+k}\\Big(\\mathrm{EP}_{\\mathrm{rep}}(A_{j}^{s c},A_{i}^{s c})+\\mathrm{EP}_{\\mathrm{rep}}(A_{j}^{s c},A_{i}^{b b})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+2\\times\\mathrm{EP}_{\\mathrm{rep}}(A_{j}^{b b},A_{i}^{s c})+2\\times\\mathrm{EP}_{\\mathrm{rep}}(A_{j}^{b b},A_{i}^{b b})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It can be observed from Eqs. (11) and (12) that the two functionality-associated energies, namely ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\ E_{\\mathrm{nonRep}}$ and ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\ E_{\\mathrm{Rep}}$ , which collectively describe the interaction energy between CDR and the antigen, are computed at the level of side-chain and backbone. ResCDR-Ag $E_{\\mathrm{nonRep}}$ is only calculated on the interactions caused by the side-chain atoms in the CDR-H3 region, while ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\,E_{\\mathrm{Rep}}$ assigns a greater cost to the repulsions caused by the backbone atoms in the CDR-H3 region. This modification is carried out according to the fact that the side-chain atoms contribute the vast majority of energy to the interaction between CDR-H3 and antigens (Fig. 6), and $E_{\\mathrm{nonRep}}$ exhibits a benefit in interactions, while $E_{\\mathrm{Rep}}$ could be regarded as a cost. ", "page_idx": 14}, {"type": "text", "text": "The fine-grained calculation of ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\ E_{\\mathrm{nonRep}}$ and ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\ E_{\\mathrm{Rep}}$ is indispensable. Without the fine-grained calculation, the model tends to generate poly-G CDR-H3 sequences, such as \u201cGGGGGGGGGGG\u201d for any given antigen and the rest of the antibody. The most likely reason for this is that G, Glycine, can maximize the reduction of clashes and gain satisfactory CDR $E_{\\mathrm{total}}$ and ResCDR-Ag $E_{\\mathrm{Rep}}$ as it doesn\u2019t contain side chain and simultaneously form a weak attraction to the antigen solely relying on its backbone atoms. ", "page_idx": 14}, {"type": "text", "text": "We emphasize that the two functionality-associated energies, ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\,E_{\\mathrm{nonRep}}$ and ${\\mathrm{Res}}_{\\mathrm{CDR}^{-}}{\\mathrm{Ag}}\\,E_{\\mathrm{Rep}}$ are calculated exclusively at the sub-residue level when serving as the determination of preference in guiding the direct energy-based preference optimization process. However, when these energies are used as evaluation metrics, they are calculated at the residue level, in which the greater cost to the repulsions attributed to the backbone atoms is negated. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C Theoretical Justification ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we show the detailed mathematical derivations of formulas in Sec. 3.2. Although many of them are similar to Rafailov et al. [41], we still present them in detail for the sake of completeness. Besides, we will also present the details of preference data generation. ", "page_idx": 15}, {"type": "text", "text": "First, we will show the derivation of the optimal solution of the KL-constrained reward-maximization objective, i.e., $\\begin{array}{r}{\\operatorname*{max}_{p_{\\theta}}\\mathbb{E}_{\\mathcal{R}^{0}\\sim p_{\\theta}}[r(\\mathcal{R}^{0})]-\\operatorname{\\'}\\!\\!\\operatorname{\\mathcal{B}}\\!\\mathbb{D}_{\\mathrm{KL}}(p_{\\theta}(\\mathcal{R}^{0})||p_{\\mathrm{ref}}(\\mathcal{R}^{0}))}\\end{array}$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p_{\\theta}}{\\operatorname*{max}}\\mathbb{E}_{\\mathcal{R}^{0}\\sim p_{\\theta}}\\big[r(\\mathcal{R}^{0})\\big]-\\beta\\mathbb{D}_{\\mathbb{K L}}\\big(p_{\\theta}(\\mathcal{R}^{0})\\big)\\|{p_{\\mathrm{ref}}(\\mathcal{R}^{0})}\\big)}\\\\ &{=\\underset{p_{\\theta}}{\\operatorname*{max}}\\mathbb{E}_{\\mathcal{R}^{0}\\sim p_{\\theta}}\\bigg[r(\\mathcal{R}^{0})-\\beta\\log\\frac{p_{\\theta}(\\mathcal{R}^{0})}{p_{\\mathrm{ref}}(\\mathcal{R}^{0})}\\bigg]}\\\\ &{=\\underset{p_{\\theta}}{\\operatorname*{min}}\\mathbb{E}_{\\mathcal{R}^{0}\\sim p_{\\theta}}\\bigg[\\log\\frac{p_{\\theta}(\\mathcal{R}^{0})}{p_{\\mathrm{ref}}(\\mathcal{R}^{0})}-\\frac{1}{\\beta}r(\\mathcal{R}^{0})\\bigg]}\\\\ &{=\\underset{p_{\\theta}}{\\operatorname*{min}}\\mathbb{E}_{\\mathcal{R}^{0}\\sim p_{\\theta}}\\bigg[\\log\\frac{p_{\\theta}(\\mathcal{R}^{0})}{\\frac{1}{Z}p_{\\mathrm{ref}}(\\mathcal{R}^{0})\\exp\\big(\\frac{1}{\\beta}r(\\mathcal{R}^{0})\\big)}-\\log Z\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $Z$ is the partition function that does not involve the model being trained, i.e., $p_{\\theta}$ . And we can define ", "page_idx": 15}, {"type": "equation", "text": "$$\np^{*}(\\mathcal{R}^{0}):=\\frac{1}{Z}p_{\\mathrm{ref}}(\\mathcal{R}^{0})\\exp\\Big(\\frac{1}{\\beta}r(\\mathcal{R}^{0})\\Big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With this, we can now arrive at ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{min}_{p_{\\theta}}\\mathbb{E}_{\\mathcal{R}^{0}\\sim p_{\\theta}}\\left[\\log\\frac{p_{\\theta}(\\mathcal{R}^{0})}{p^{*}(\\mathcal{R}^{0})}\\right]-\\log Z}}\\\\ &{=\\operatorname*{min}_{p_{\\theta}}\\mathbb{E}_{\\mathcal{R}^{0}\\sim p_{\\theta}}[\\mathbb{D}_{\\mathrm{KL}}(p_{\\theta}\\|p^{*})]+Z}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $Z$ does not depend on $p_{\\theta}$ , we can directly drop it. According to Gibb\u2019s inequality that KLdivergence is minimized at 0 if and only if the two distributions are identical. Hence we arrive at the optimum as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\theta^{*}}(\\mathcal{R}^{0})=p^{*}(\\mathcal{R}^{0})=\\frac{1}{Z}p_{\\mathrm{ref}}(\\mathcal{R}^{0})\\exp\\Big(\\frac{1}{\\beta}r(\\mathcal{R}^{0})\\Big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we will show that the objective that maximizes likelihood on preference data sampled from $p(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})=\\sigma\\big(r(\\mathcal{R}_{1}^{0})-r(\\bar{\\mathcal{R}}_{2}^{0})\\big)$ , which is exactly $L_{\\mathrm{DPO}}$ , leads to the same optimal solution. For this, we need to express the pre-defined reward $r(\\cdot)$ with the optimal policy $p^{*}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nr(\\mathcal{R}^{0})=\\beta\\log\\frac{p^{*}(\\mathcal{R}^{0})}{p_{\\mathrm{ref}}(\\mathcal{R}^{0})}+Z\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The we plugin the expression of $r(\\cdot)$ into $p(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})=\\sigma\\big(r(\\mathcal{R}_{1}^{0})-r(\\mathcal{R}_{2}^{0})\\big)$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})=\\sigma(r(\\mathcal{R}_{1}^{0})-r(\\mathcal{R}_{2}^{0}))}\\\\ &{\\qquad\\qquad\\qquad=\\sigma\\bigg(\\beta\\log\\frac{p^{*}(\\mathcal{R}_{1}^{0})}{p_{\\mathrm{ref}}\\mathcal{R}_{1}^{0})}-\\beta\\log\\frac{p^{*}(\\mathcal{R}_{2}^{0})}{p_{\\mathrm{ref}}(\\mathcal{R}_{2}^{0})}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $Z$ is canceled out. For brevity, we use the following notation for brevity: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\theta}(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})=\\sigma\\bigg(\\beta\\log\\frac{p_{\\theta}(\\mathcal{R}_{1}^{0})}{p_{\\mathrm{ref}}(\\mathcal{R}_{1}^{0})}-\\beta\\log\\frac{p_{\\theta}(\\mathcal{R}_{2}^{0})}{p_{\\mathrm{ref}}(\\mathcal{R}_{2}^{0})}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With this, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p_{\\theta}}{\\operatorname*{min}}\\,L_{\\mathrm{DPO}}=\\underset{p_{\\theta}}{\\operatorname*{min}}-\\mathbb{E}_{\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0}\\sim p(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})}p_{\\theta}(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})}\\\\ &{\\phantom{\\operatorname*{min}}=\\underset{p_{\\theta}}{\\operatorname*{max}}\\,\\mathbb{E}_{\\mathcal{R}_{1}^{0},\\mathcal{R}_{2}^{0}\\sim p(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})}p_{\\theta}(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})}\\\\ &{\\phantom{\\operatorname*{min}}=\\underset{p_{\\theta}}{\\operatorname*{min}}\\,\\mathbb{D}_{\\mathrm{KL}}\\left(p(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})\\Big|\\Big|p_{\\theta}(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Again with Gibb\u2019s inequality, we can easily identify that $p_{\\pmb\\theta}(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})=p(\\mathcal{R}_{1}^{0}\\succ\\mathcal{R}_{2}^{0})$ achieves the minimum. Thus $\\begin{array}{r}{p^{*}(\\mathcal{R}^{0})=\\frac{1}{Z}p_{\\mathrm{ref}}(\\mathcal{R}^{0})\\exp\\Big(\\frac{1}{\\beta}r(\\mathcal{R}^{0})\\Big)}\\end{array}$ is also the optimal solution of $L_{\\mathrm{DPO}}$ . ", "page_idx": 16}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Model Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The architecture of the diffusion model used in our method is the same as Luo et al. [36]. The input of the model is the perturbed CDR-H3 and its surrounding context, i.e., 128 nearest residues of the antigen or the antibody framework around the residues of CDR-H3. The input is composed of single residue embeddings and pairwise embeddings. The single residue embedding encodes the information of its amino acid types, torsional angles, and 3D coordinates of all heavy atoms. The pairwise embedding encodes the Euclidean distances and dihedral angles between the two residues. The sizes of the single residue feature and the residue-pair features are 1285 and 64, respectively Then the features are processed by Multiple Layer Perceptrons (MLPs). The number of layers is 6. The size of the hidden state in the layers is 128. The output of the model is the predicted categorical distribution of amino acid types, $C_{\\alpha}$ coordinates, and a $s o(3)$ vector for the rotation matrix. ", "page_idx": 16}, {"type": "text", "text": "The number of diffusion steps is 100. We use the cosine $\\beta$ schedule with $s=0.01$ suggested in Ho et al. [20] for amino acid types, $C_{\\alpha}$ coordinates, and orientations. ", "page_idx": 16}, {"type": "text", "text": "D.2 Training Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Pre-training Following Luo et al. [36], the diffusion model is first trained via the gradient descent method Adam [27] with init_learning_rat $\\mathtt{o}{=}1\\mathtt{e}{-}4$ , $\\mathtt{b e t a s}{=}(0.9,0.999)$ , batch_size $=\\mathtt{16}$ , and clip_gradient_norm $=\\!100$ . During the training phase, the weight of rotation loss, position loss, and sequence loss are each set to 1.0. We also schedule to decay the learning rate multiplied by a factor of 0.8 and a minimum learning rate of $5e-6$ . The learning rate is decayed if there is no improvement for the validation loss in 10 evaluations. The evaluation is performed for every 1000 training steps. We trained the model on one NVIDIA A100 80G GPU and it could converge within 30 hours and $200\\mathbf{k}$ steps. ", "page_idx": 16}, {"type": "text", "text": "Test set The original RAbD dataset contains 60 antibody-antigen complexes. In this study, we hope all the complex consists of an antibody heavy chain and a light chain, and at least one protein antigen chain. In practice, 2ghw and 3uzq lack light chains, while 3h3b lacks heavy chains. 5d96 was excluded because of the incorrect chain ID information in rabd_summary.jsonl3, where heavy chain $J$ and light chain $I$ do not bind to antigen chain $A$ . As for 4etq, we actually conducted the training (CDR $E_{\\mathrm{total}}{=}70.55$ , CDR-Ag $\\Delta G{=}{-}4.57\\$ ), but HERN reported an error when running for this complex, so we did not report it. ", "page_idx": 16}, {"type": "text", "text": "Pair data construction In terms of the construction of \u201cwinning\u201d and \u201closing\u201d data pair, we did not pre-define \u201cprefered\u201d and \u201cnon-prefered\u201d datasets but rather constructed a unified data pool. During each training step, the paired data used for DPO training is randomly sampled from the data pool. Although their energies and properties have been pre-calculated, the \u201cwinning\u201d and \u201closing\u201d labels are determined in real time. In practice, we used several labels, involving three different preferences related to energy and two preferences related to non-energy-based properties. The \u201cwinning\u201d and \u201closing\u201d labels among these preferences are not necessarily consistent. Therefore, the loss for each type of energy/preference is calculated separately and then aggregated with different weights to update the entire model. Moreover, as the training progresses, we continuously sample new data, calculate their energy, add them to the data pool, and discard some of the older post-added data simultaneously to ensure that the data stays in sync with the policy. ", "page_idx": 16}, {"type": "text", "text": "Fine-tuning For ABDPO fine-tuning, the pre-trained diffusion model is further fine-tuned via the gradient descent method Adam with init_learning_rate $\\L_{\\L}=\\L_{1}\\L\\Theta-5$ , betas $=$ (0.9,0.999), and clip_gradient_norm $=\\!100$ . The batch size is 48. More specifically, in a batch, there are 48 pairs of preference data. We do not use a decay learning rate and do not use weight decay in the fine-tuning process. And we use $\\beta\\,=\\,0.01$ and 0.005 in Eq. (8). We use the hyperparameter search space as follows. As for the three energies introduced in Sec. 4.1, we use 8:8:2 to reweight them (i.e., ResCDR $E_{\\mathrm{total}}$ , ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\;E_{\\mathrm{nonRep}}$ , and ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\ E_{\\mathrm{Rep}})$ , and reweight pLL and PHR in ABDPO $^+$ to 1. In practice, different antibody-antigen complexes prefer different hyperparameters. For a fair comparison with baselines, we do not carefully picked the optimal hyperparameter for each complex but use a uniform hyperparameter. We fine-tune the pre-trained diffusion model on four NVIDIA A800 40G GPUs for 1,800 steps for each antigen, separately. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D.3 Ranking Strategy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To rank the numerous generated antibodies with multiple energy labels, we applied a simple ranking strategy based on single energy metrics. The CDR $E_{\\mathrm{total}}$ and the CDR-Ag $\\Delta G$ of each antibody are ranked independently. Then, a composite ranking score for each antibody is defined as the sum of its CDR $E_{\\mathrm{total}}$ rank and CDR-Ag $\\Delta G$ rank (for $\\mathrm{ABDPO+}$ , PHR and pLL are also involved). Finally, the antibodies are ranked according to these composite scores. We acknowledge that this ranking strategy has several limitations. For instance: ", "page_idx": 17}, {"type": "text", "text": "1. Equal weights are assigned to all energy types and properties, despite them having differing importance in reality. 2. The distribution patterns of different energy types and properties can vary, with these distributions usually being non-uniform. This could result in scenarios where minor numerical differences in the top-ranking CDR-Ag $\\Delta G$ values coincide with larger differences in CDR $E_{\\mathrm{total}}$ , potentially leading to the selection of samples with suboptimal CDR $E_{\\mathrm{total}}$ . ", "page_idx": 17}, {"type": "text", "text": "However, addressing these issues would require extensive and in-depth exploration of antibody binding mechanisms and energy calculation methodologies. We chose this straightforward, yet impartial, ranking strategy for two key reasons: ", "page_idx": 17}, {"type": "text", "text": "1. The primary goal of this work is to reformulate the antibody design task as an energy-focused optimization problem and propose a feasible implementation, rather than to delve into the mechanisms of antibody-antigen binding;   \n2. Our approach is designed to avoid introducing statistical biases or preferences based on potentially erroneous prior knowledge or favoritism towards particular antibody design methods. ", "page_idx": 17}, {"type": "text", "text": "E More Evaluation Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Evaluation Results for Ranked Top-1 Design ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Tab. 1, we have reported the average results of all antibodies designed by our method and other baselines. Here we provide the evaluation results for the ranked top-1 design in Tab. 3 (refer to the ranking strategy in Appendix D.3). ", "page_idx": 17}, {"type": "table", "img_path": "GN2GXjPyN8/tmp/c8a83955c9fed5ae0d09d6de157642d7b0334c123772a88d55d2315f8d397a55.jpg", "table_caption": ["Table 3: Average performance of top-1 designs of 55 complexes designed by baselines and our model. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E.2 Detailed Evaluation Results for each Complex ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Tab. 4 and Tab. 5, we list the CDR $E_{\\mathrm{total}}$ , CDR-Ag $\\Delta G$ , PHR and pLL of the reference antibody in RAbD and the average/ranked top-1 antibodies designed by HERN, MEAN, dyMEAN, DiffAb, ABDPO, and ABDPO $^+$ for each complex in the test set separately. In Tab. 5, we highlight the energy values of the designed complexes that surpass the natural one in terms of two energies simultaneously with bold text. ", "page_idx": 17}, {"type": "table", "img_path": "GN2GXjPyN8/tmp/c61c86b99abe14b8bf770486b766b7a6571567a95ac892580ca01daeb070e7b9.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "GN2GXjPyN8/tmp/e72c0743c537b3827486a380224ea23a40cf47c6a7e62807ca7beadbef155339.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "GN2GXjPyN8/tmp/efbfb1248a11185b5b7bc19d94a7386a03812bad26998a25f9e32ab02754e445.jpg", "img_caption": ["Figure 7: Left: the distribution of peptide bond length within CDR-H3 in the SAbDab dataset; Right: the kernel density estimation (KDE) function fit on the natural peptide bond length distribution. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Arbitrary Preferences ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1 Incorporating Auxiliary Loss ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A predominant advantage of the ABDPO is its unique capacity to seamlessly integrate traditional bioinformatics, computational biology, and computational chemistry tools \u2014 those incapable of directly computing gradients \u2014 into the training regimen of AI models. This integration significantly broadens the ABDPO\u2019s applicability and versatility in antibody design. However, it is pertinent to acknowledge the existence of antibody energies/properties for which gradient calculations are feasible. Indeed, fundamental geometric characteristics, such as bond lengths, angles, and torsion angles, alongside more intricate properties predicted by deep-learning models, are gradient-computable. These gradient-computable features offer an explicit direction for optimization, potentially enhancing the effectiveness and efficiency of the model optimization process. ", "page_idx": 20}, {"type": "text", "text": "In light of this, we initiated another experiment aimed at exploring ABDPO\u2019s compatibility with traditional gradient-based losses, extending beyond the DPO loss. Specifically, we propose a special version based on $\\mathrm{ABDPO+}$ , $\\mathrm{ABDPO++}$ , which incorporates an auxiliary loss about peptide bond length. As a covalent bond, the variation range of peptide bond lengths is very limited, and thus we can consider the length of peptide bonds to be a fixed value and then utilize an MSE loss to directly penalize the unreasonable peptide bond length in generated antibodies. ", "page_idx": 20}, {"type": "text", "text": "In practice, we consider the ground truth peptide bond length to be 1.3310 (the average length of peptide bonds within CDR-H3 in SAbDab, the distribution could be seen in Fig. 7 left) and apply the auxiliary loss only when the sampled t is near 0 $t<15$ in this experiment while $T$ is 100), and the weight is set to 0.25. The peptide bond length is calculated based on the predicted $(\\mathbf{s}_{j}^{0},\\mathbf{x}_{j}^{0},\\mathbf{O}_{j}^{0})$ which is denoised with one step from $(\\mathbf{s}_{j}^{t},\\mathbf{x}_{j}^{t},\\mathbf{O}_{j}^{t})$ , then an MSE loss of peptide bond length can be calculated. Finally, this auxiliary loss, together with various DPO losses, updates the model through the conflict mitigation mentioned in Sec. 3.3. ", "page_idx": 20}, {"type": "table", "img_path": "GN2GXjPyN8/tmp/83dc3d689909957a4bd5c6a0427397aaddf98a4f0531b4d39c08072a832c1da9.jpg", "table_caption": ["Table 6: Summary of CDR $E_{\\mathrm{total}}$ , CDR-Ag $\\Delta G$ (kcal/mol), pLL, PHR, $\\mathrm{C{-}N_{\\mathrm{score}}}$ , AAR, and RMSD of reference antibodies and antibodies designed by ABDPOW/O and baselines in the experiment involves auxiliary loss. $\\left(\\downarrow\\right)/$ (\u2191) denotes a smaller / larger number is better. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "To evaluate the consistency of generated antibodies\u2019 peptide bond length to the natural antibodies, we fti a Kernel Density Estimation function using the length of peptide bonds found within the CDR-H3 region of natural antibodies (shown in Fig. 7 right), then the density of the generated peptide bond length, $\\mathrm{C{-}N_{\\mathrm{score}}}$ , is used to represent the consistency. We report the average experiment result in Tab. 6. It can be observed that $\\mathrm{ABDPO++}$ significantly optimized the length of the peptide bond, achieving the best $\\mathrm{C{-}N_{\\mathrm{score}}}$ of 4.51, while maintaining the optimization to the other 4 preferences. The experimental result demonstrates the compatibility of AbDPO with traditional gradient-based losses, indicating that AbDPO has a wider scope in actual application. ", "page_idx": 21}, {"type": "text", "text": "F.2 Incorporating Energy Minimization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Energy minimization is indispensable in the standard protein design protocol and is typically applied to the raw co-crystal structure and the generated structure. Most existing AI-based antibody design methods have not undergone similar operations, but to verify the performance of ABDPO in a more realistic workflow environment, we have also proposed another version based on ABDPO $^+$ that integrates energy minimization, ABDPOW/O. ", "page_idx": 21}, {"type": "text", "text": "For the minimization of the raw co-crystal structure, we compared the performance of baseline methods trained with and without minimized co-crystal structure but observed no significant difference. A possible reason for this is that most of the methods do not generate the side chain and thus are not sensitive to energy minimization, which mainly optimizes the side-chain conformation. Thus we follow the previous studies, and directly use raw co-crystal structure to train the baseline models and the pre-trained model in ABDPO. ", "page_idx": 21}, {"type": "text", "text": "We carry out minimization during the evaluation phase and apply the minimization to the generated antibodies before energy calculation. Therefore, the preference dataset used in ABDPOW/O is built upon the minimized energy. The energy minimization process consists of two parts, peptide bond length rectification and loop refinement. We first set the length of the peptide bond to 1.3310, the average length of the peptide bonds within CDR-H3 in the SAbDab dataset. Then we use LoopMover_Refine_ $.C C D$ from pyRosetta to refine the structure of the designed CDR loop. To reduce time consumption in loop refinement, we set the outer_cycles to 1 and max_inner_cycles to 10 (a bigger number of cycles will lead to better energy performance undoubtedly, but also makes the time consumption uncontrollable). ", "page_idx": 21}, {"type": "text", "text": "Another modification of ABDPOW/O compared to $\\mathrm{ABDPO+}$ is that the decomposition of ResCDR-Ag $\\Delta\\mathbf G$ into ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-Ag}}\\,E_{\\mathrm{nonRep}}$ and ${\\mathrm{Res}}_{\\mathrm{CDR}}{\\mathrm{-}}{\\mathrm{Ag}}\\,E_{\\mathrm{Rep}}$ is canceled. Energy decomposition is indispensable in the main experiment because of the huge repulsion, and is not necessary in this experiment as the repulsion would be diminished by the post-minimization process. ", "page_idx": 21}, {"type": "table", "img_path": "GN2GXjPyN8/tmp/07469f901a4486ecf6532f90cc25c8d0248a806c3c1243d19a8f2ec87d07f95d.jpg", "table_caption": ["Table 7: Summary of CDR $E_{\\mathrm{total}}$ , CDR-Ag $\\Delta G$ (kcal/mol), PHR, and pLL of reference antibodies and antibodies designed by ABDPOW/O and baselines in the experiment involves energy minimization. $\\left(\\downarrow\\right)/\\left(\\uparrow\\right)$ denotes a smaller / larger number is better. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "In Tab. 7, we report the average values of the evaluation metrics for all the generated antibodies in this experiment. Given that the peptide bond length has been rectified, measuring the C-N score is deemed unnecessary in this context. It can be observed that the post-minimization eliminates most of the clashes between the designed antibodies and the corresponding antigens, making CDR-Ag $\\Delta G$ fall within a reasonable range of value. ABDPOW/O still achieves the best performance in the two energy-based metrics, CDR $E_{\\mathrm{total}}$ and $\\mathrm{CDR-Ag}\\;\\Delta G,$ , and surpasses DiffAb in all metrics. This experiment proves (1) the effectiveness of ABDPO in a more realistic setting, and (2) the ability of ABDPO to optimize the energies/properties not directly calculated from the generated antibodies. The values of the two sequence-related metrics, PHR and pLL, for the baseline methods slightly differ from those in Tab. 1. This discrepancy arises because we imposed a maximum processing time during the loop refinement phase, leading to the exclusion of samples whose refinement was incomplete within the allocated time. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "G Extended Ablation Studies ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Due to the massive training cost in the RAbD benchmark, we investigate the effectiveness and necessity of each proposed component on five representative antigens, whose PDB IDs are 1a14, 2dd8, 3cx5, 4ki5, and 5mes. From the results in Fig. 8, it is clear that ABDPO can significantly boost the overall performance of ablation cases. Note that moving averages are applied to smooth out the curves to help in identifying trends, including Fig. 4. We present observations and constructive insights of the three proposed components as follows: ", "page_idx": 22}, {"type": "text", "text": "1. The residue-level DPO is vital for training stability specifically for CDR $E_{\\mathrm{total}}$ . As aforementioned in Section 3.2, the residue-level DPO implicitly provides fine-grained and rational gradients. In contrast, vanilla DPO (without residue-level DPO) may impose unexpected gradients on stable residues, which incurs the adverse direction of optimization. According to each energy curve in Figure 8, we observe that residue-level DPO surpasses vanilla DPO by at least one energy term.   \n2. Without Energy Decomposition, all five cases appear undesired \u201cshortcuts\u201d aforementioned in Section 3.3. We observe that the energy of CDR $E_{\\mathrm{total}}$ exhibits a slight performance improvement over the ABDPO after the values of attraction and repulsion reach zero. We suppose that is the result of the combined effects of low attraction and repulsion. Because the generated CDR-H3 is far away from the antigen in this case, the model can concentrate on refining CDR $E_{\\mathrm{total}}$ without the interference of attraction and repulsion.   \n3. The Gradient Surgery can keep a balance between attraction and repulsion. We can see the curves of $\\mathrm{E_{nonRep}}$ are consistently showing a decline, while the curves of $\\mathrm{E_{Rep}}$ are showing an increase. This observation verifies that ABDPO without Gradient Surgery is unable to optimize $\\mathrm{E_{nonRep}}$ and $\\mathrm{E_{Rep}}$ simultaneously. Additionally, the increase in attraction significantly impacts the repulsion, causing the repulsion to fluctuate markedly. ", "page_idx": 22}, {"type": "text", "text": "H Limitations and Future Work ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Diffusion Process of Orientations As Luo et al. [36] stated and we have mentioned in Sec. 3.1, Eq. (1) is not a rigorious diffusion process. Thus the loss in Eq. (7) cannot be rigorously derived from the KL-divergence in Eq. (4), though they share the idea of reconstructing the ground truth data by prediction. However, due to the easy implementation and fair comparison with the generative baseline, i.e., DiffAb [36], we adopt Eq. (7) in the ABDPO loss in Eq. (8). In practice, we empirically find that it works well. FrameDiff [50], a protein backbone generation model, adopts a noising process and a rotation loss that are well compatible with the theory of score-based generative models (also known as diffusion models). In the future, we modify the diffusion process of orientations as Yim et al. [50] for potential further improvement. ", "page_idx": 22}, {"type": "text", "text": "Energy Estimation In this work, we utilize Rosetta/pyRosetta to calculate energy, although it is already one of the most authoritative energy simulation software programs and widely used in protein design and structure prediction , the final energy value is still difficult to perfectly match the actual experimental results. In fact, any computational energy simulation software, whether it is based on force field methods such as OpenMM [14] or statistical methods like the Miyazawa-Jernigan potential [38], will exhibit certain biases and cannot fully simulate reality. Sometimes there is a significant difference between the energy calculated by the software and the results observed experimentally. One possible reason is that theoretical calculations often rely on the designed sequence and structure of antibodies; meanwhile, in actual experiments, the actual folding of the CDR region into the designed structure can be difficult, which leads to significant discrepancies in theoretical calculations. An in vitro experiment is the only way to verify the effectiveness of the designed antibodies. However, due to the significant amount of time consumed by in vitro experiments and considering that the main goal of our work is to propose a novel view of antibody design, we did not perform the in vitro experiment. ", "page_idx": 22}, {"type": "text", "text": "Future Work on Preference Definition The preferences used in ABDPO determine the tendency of antibody generation, and we will strive to continue exploring the definition of preference to more closely align the antibody design process with the real-world environment of antibody activity. Further, we aim to synchronize the preference with the outcomes of in vitro experiments and expect that our method will ultimately generate effective antibodies in real-world applications. The exploration of preference can be divided into two aspects: enhancing existing preferences and integrating new components or energies. ", "page_idx": 22}, {"type": "image", "img_path": "GN2GXjPyN8/tmp/f9dde8c855ce9155e5b9373b8a7ef71ed7474bc35b1a919ee9a60b220be73f46.jpg", "img_caption": ["Figure 8: Changes of median CDR $E_{\\mathrm{total}}$ , CDR-Ag $E_{\\mathrm{nonRep}}$ , CDR-Ag $E_{\\mathrm{Rep}}$ , and CDR-Ag $\\Delta G$ (kcal/mol) over-optimization steps, shaded to indicate interquartile range (from 25-th percentile to 75-th percentile). The rows represent PDB 1a14, 2dd8, 3cx5, 4ki5, and 5mes respectively, in a top-down order. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "1. The improvement to current preference: (1) performing more fine-grained calculations on the current three types of energy, such as decomposing CDR $E_{\\mathrm{total}}$ into interactions between the CDR and the rest of the antibody, interactions within the CDR, and energy at the single amino acid level; (2) exploring the varying importance of preferences for antibodies and determining the relative weights of each preference during the optimization and ranking of generated antibodies. ", "page_idx": 23}, {"type": "text", "text": "2. The incorporation of new components or energies is intended to address additional challenges in antibody engineering, focusing on aspects such as antibody stability, solubility, immunogenicity, and expression level. Additionally, we consider integrating components that target antibody specificity. ", "page_idx": 23}, {"type": "text", "text": "I Potential Societal Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our work on antibody design can be used in developing potent therapeutic antibodies and accelerate the research process of drug discovery. The generality of our method extends beyond its current application; it is adaptable for various computer-aided design scenarios including, but not limited to, small molecule, material, and chip design. It is also needed to ensure the responsible use of our method and refrain from using it for harmful purposes. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work in Appendix H ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper provides the full set of assumptions and a complete (and correct) proof for each theoretical result. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results of the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: The release of code requires approval. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper specifies all the training and test details in Appendix D.2. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: We lack sufficient computing power to complete such statistical tests. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper provides sufficient information on the computer resources. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper discusses both positive and negative impacts of the work performed in Appendix I. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]