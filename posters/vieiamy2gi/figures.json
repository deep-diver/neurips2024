[{"figure_path": "vieIamY2Gi/figures/figures_0_1.jpg", "caption": "Figure 1: Two-dimensional projections of Manywell samples from models trained by different algorithms. Our proposed replay buffer with local search is capable of preventing mode collapse.", "description": "This figure compares the sampling results of various diffusion models on the 32-dimensional Manywell distribution.  The different models are trained using different algorithms, including the authors' proposed method incorporating a replay buffer and local search.  The plots show two-dimensional projections of the resulting samples, illustrating the impact of the different training methods on the quality and diversity of the samples generated. The authors' method, as evidenced by the image, is shown to prevent mode collapse, a common problem with diffusion models where the samples cluster around a few modes instead of covering the entire distribution.", "section": "5 Experiments"}, {"figure_path": "vieIamY2Gi/figures/figures_6_1.jpg", "caption": "Figure 1: Two-dimensional projections of Manywell samples from models trained by different algorithms. Our proposed replay buffer with local search is capable of preventing mode collapse.", "description": "This figure displays two-dimensional projections of samples generated from the Manywell distribution using different sampling methods.  The visualizations show the effectiveness of the proposed method (replay buffer with local search) in preventing mode collapse, a common issue in generative modeling where the model fails to capture the diversity of the target distribution. The figure allows a visual comparison of the sample distributions obtained with various algorithms, highlighting the improved mode coverage and reduced mode collapse achieved by incorporating the proposed replay buffer and local search strategy.", "section": "5 Experiments"}, {"figure_path": "vieIamY2Gi/figures/figures_7_1.jpg", "caption": "Figure 2: Effect of exploration variance on models trained with TB on the 25GMM energy. Exploration promotes mode discovery, but should be decayed over time to optimally allocate the modeling power to high-likelihood trajectories.", "description": "This figure shows the impact of different exploration strategies on the performance of diffusion models trained using the trajectory balance (TB) objective on a 25-dimensional Gaussian Mixture Model (25GMM).  The x-axis represents the exploration rate, and the y-axis represents the estimated log partition function (log Z). Two lines are shown: one for models trained with constant exploration and another for models trained with decaying exploration. The results indicate that while exploration is beneficial for discovering multiple modes, allowing it to decay over time improves the model's ability to accurately estimate the partition function by focusing the model's capacity on higher-probability regions of the target distribution.", "section": "5.2 Results"}, {"figure_path": "vieIamY2Gi/figures/figures_8_1.jpg", "caption": "Figure 3: Left: Distribution of X0, X0.1,..., X\u2081 learned by 10-step samplers with fixed (top) and learned (middle) forward policy variance on the 25GMM energy. The last step of sampling the fixed-variance model adds Gaussian noise of a variance close to that of the components of the target distribution, preventing the the sampler from sharply capturing the modes. The last row shows the policy variance learned as a function of x\u2081 at various time steps t (white is high variance, blue is low), showing that less noise is added around the peaks near t = 1. The two models' log-partition function estimates are -1.67 and -0.62, respectively. Right: For varying number of steps T, we plot the log Z obtained by models with fixed and learned variance. Learning policy variances gives similar samplers with fewer steps.", "description": "This figure compares the performance of 10-step samplers with fixed and learned forward policy variance on the 25GMM energy. The left panel shows the distribution of samples at different time steps for both models, highlighting how learning the variance allows the model to capture the modes more sharply. The right panel shows how learning the policy variance leads to similar results with fewer steps.", "section": "5.2 Results"}, {"figure_path": "vieIamY2Gi/figures/figures_20_1.jpg", "caption": "Figure 1: Two-dimensional projections of Manywell samples from models trained by different algorithms. Our proposed replay buffer with local search is capable of preventing mode collapse.", "description": "This figure shows two-dimensional projections of samples generated from the 32-dimensional Manywell distribution using different sampling methods.  The goal is to visualize the performance of various sampling algorithms in capturing the multiple modes of the distribution.  The figure demonstrates that the proposed method, which uses a replay buffer and local search, effectively prevents the model from collapsing to a single mode, thus improving sample quality compared to other methods.", "section": "5 Experiments"}, {"figure_path": "vieIamY2Gi/figures/figures_23_1.jpg", "caption": "Figure E.1: Illustration of each sampler trained with varying capacities of replay buffers, depicting 2,000 samples. As the capacity of the buffer increases, the number of modes captured by the sampler also increases.", "description": "This figure shows the impact of replay buffer capacity on the sample quality of three different samplers.  Each subplot displays 2000 samples generated by a sampler trained with a different buffer capacity (30,000, 60,000, and 600,000). The increase in buffer capacity leads to better exploration of the target distribution's modes, as evidenced by the increased number of modes captured in the samples.  This supports the claim that a larger replay buffer enhances the sampler's ability to recall and utilize past low-energy samples.", "section": "E.2 Ablation study for local search-guided GFlowNets"}, {"figure_path": "vieIamY2Gi/figures/figures_23_2.jpg", "caption": "Figure E.2: Ablation study for prioritized replay buffer and step size \u03b7 scheduling of local search. Mean and standard deviation are plotted based on five independent runs.", "description": "This figure presents an ablation study on two aspects of the local search algorithm used in the paper: the use of a prioritized replay buffer and the dynamic adjustment of the step size.  The left subplot (a) shows that prioritized sampling, where samples with higher ranks (based on their unnormalized target density) are more likely to be selected, leads to faster convergence than uniform sampling. The right subplot (b) demonstrates that dynamically adjusting the step size to maintain a target acceptance rate of 0.574 (theoretically optimal for high-dimensional MALA) outperforms using a fixed step size. This ablation study highlights the effectiveness of these techniques in improving the performance of the local search method.", "section": "E.2 Ablation study for local search-guided GFlowNets"}, {"figure_path": "vieIamY2Gi/figures/figures_23_3.jpg", "caption": "Figure E.2: Ablation study for prioritized replay buffer and step size \u03b7 scheduling of local search. Mean and standard deviation are plotted based on five independent runs.", "description": "This figure presents the ablation study results for two hyperparameters of the proposed local search method: the prioritized replay buffer and the dynamically adjusted step size.  The left subplot (a) compares the performance of prioritized sampling (using the rank-based approach) against uniform sampling. The right subplot (b) shows the impact of dynamically adjusting the step size (\u03b7) to maintain an optimal acceptance rate (0.574) compared to using a fixed step size (\u03b7 = 0.01).  The plots show the log ZRW (log partition function with importance weights) across 25,000 training iterations, revealing that both techniques lead to significant performance improvements.", "section": "E.2 Ablation study for local search-guided GFlowNets"}]