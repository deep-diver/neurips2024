[{"figure_path": "u1Z3HWz4VJ/figures/figures_1_1.jpg", "caption": "Figure 1: Multiple-norm tradeoff with robust fine-tuning: We observe that fine-tuning on l\u221e-AT model using l\u2081 examples drastically reduces l\u221e robustness. RAMP preserves more l\u221e and union robustness.", "description": "This figure visualizes the changes in l1, l2, l\u221e, and union robustness when fine-tuning a pre-trained l\u221e-adversarial training (AT) model with l1 examples.  It compares the robustness after one epoch of fine-tuning using three different methods: l1 fine-tuning, Extreme Adversarial Training (EAT), and the proposed RAMP method.  The results show that l1 fine-tuning and EAT lead to a substantial decrease in l\u221e robustness, while RAMP effectively maintains both l\u221e and union robustness.", "section": "Connect natural training (NT) with AT"}, {"figure_path": "u1Z3HWz4VJ/figures/figures_7_1.jpg", "caption": "Figure 2: l\u221e AT-GP with PGD [Madry et al., 2017] with = 0.031 on CIFAR-10 improves accuracy and robustness. Pre-training on Dn for 50 epochs further boosts the performance.", "description": "The figure shows two plots: (a) Clean Accuracy and (b) Robust Accuracy (PGD-20). Both plots compare the performance of four different training methods: AT, AT-GP, AT-GP-pre, and AT-pre against the number of epochs. AT-GP represents adversarial training with gradient projection, AT-GP-pre represents pre-training on natural data before adversarial training with gradient projection, and AT-pre represents pre-training on natural data before adversarial training. The plots show that AT-GP and AT-GP-pre achieve higher accuracy and robustness compared to AT and AT-pre, and pre-training further improves the performance.", "section": "4.2 Connecting Natural Training with AT"}, {"figure_path": "u1Z3HWz4VJ/figures/figures_9_1.jpg", "caption": "Figure 1: Multiple-norm tradeoff with robust fine-tuning: We observe that fine-tuning on l\u221e-AT model using l\u2081 examples drastically reduces l\u221e robustness. RAMP preserves more l\u221e and union robustness.", "description": "This figure visualizes the change in l1, l2, and l\u221e robustness when fine-tuning a pre-trained l\u221e-adversarial training (AT) model with l1 examples.  It shows that standard fine-tuning and E-AT significantly reduce the l\u221e robustness after just one epoch. In contrast, the RAMP framework maintains substantially more l\u221e robustness and union robustness (overall robustness against multiple norms). This highlights the effectiveness of RAMP in mitigating the trade-offs between robustness against different perturbation types.", "section": "Connect natural training (NT) with AT"}, {"figure_path": "u1Z3HWz4VJ/figures/figures_19_1.jpg", "caption": "Figure 2:  l\u221e AT-GP with PGD [Madry et al., 2017] with \u03b5\u221e = 0.031 on CIFAR-10 improves accuracy and robustness. Pre-training on Dn for 50 epochs further boosts the performance. AT-GP; RN-18 l\u221e-GP-pre pre-trains 40 epochs on Dn before AT-GP is applied.", "description": "This figure shows the effect of gradient projection (GP) on adversarial training (AT).  It compares the performance of standard AT, AT with GP (AT-GP), AT-GP with pre-training (AT-GP-pre), and standard AT with pre-training (AT-pre) on CIFAR-10. The plots display the clean accuracy and robustness against the l\u221e-norm perturbation using Projected Gradient Descent (PGD). The results demonstrate that incorporating GP improves both clean accuracy and robustness and that pre-training further enhances the performance.", "section": "4.2 Connecting Natural Training with AT"}, {"figure_path": "u1Z3HWz4VJ/figures/figures_24_1.jpg", "caption": "Figure 1: Multiple-norm tradeoff with robust fine-tuning: We observe that fine-tuning on l\u221e-AT model using l\u2081 examples drastically reduces l\u221e robustness. RAMP preserves more l\u221e and union robustness.", "description": "This figure visualizes the changes in robustness against different lp norms (l1, l2, l\u221e) when fine-tuning a model pre-trained with adversarial training on the l\u221e norm.  It shows that standard fine-tuning with l1 examples significantly reduces the robustness against the l\u221e norm, while the proposed RAMP method better preserves both the l\u221e robustness and the overall union robustness (robustness across multiple norms). The histograms illustrate the distribution of accuracy across different perturbation levels after applying each method.", "section": "Connect natural training (NT) with AT"}, {"figure_path": "u1Z3HWz4VJ/figures/figures_25_1.jpg", "caption": "Figure 1: Multiple-norm tradeoff with robust fine-tuning: We observe that fine-tuning on l\u221e-AT model using l\u2081 examples drastically reduces l\u221e robustness. RAMP preserves more l\u221e and union robustness.", "description": "This figure visualizes the changes in l1, l2, and l\u221e robustness during the fine-tuning of an l\u221e-adversarial training model using l1 examples.  The results are compared against standard adversarial training and another defense called E-AT. The key observation is that standard fine-tuning causes a substantial drop in l\u221e robustness after just one epoch, whereas the proposed RAMP method better maintains both l\u221e and overall (union) robustness.  This highlights RAMP\u2019s effectiveness in mitigating the trade-off between robustness against different perturbation types.", "section": "Connect natural training (NT) with AT"}, {"figure_path": "u1Z3HWz4VJ/figures/figures_25_2.jpg", "caption": "Figure 1: Multiple-norm tradeoff with robust fine-tuning: We observe that fine-tuning on l\u221e-AT model using l\u2081 examples drastically reduces l\u221e robustness. RAMP preserves more l\u221e and union robustness.", "description": "This figure visualizes the trade-offs among robustness against different perturbation norms (l1, l2, and l\u221e) during the fine-tuning process. It compares the performance of three methods: standard l1 fine-tuning, E-AT (Extreme Adversarial Training), and RAMP (Robustness Against Multiple Perturbations).  The histograms show that while standard l1 fine-tuning and E-AT significantly reduce l\u221e robustness after one epoch, RAMP effectively maintains l\u221e and union robustness, demonstrating its effectiveness in balancing the trade-offs between clean accuracy and robustness against multiple norms.", "section": "Connect natural training (NT) with AT"}, {"figure_path": "u1Z3HWz4VJ/figures/figures_26_1.jpg", "caption": "Figure 1: Multiple-norm tradeoff with robust fine-tuning: We observe that fine-tuning on l\u221e-AT model using l\u2081 examples drastically reduces l\u221e robustness. RAMP preserves more l\u221e and union robustness.", "description": "This figure visualizes the changes in l1, l2, and l\u221e robustness when fine-tuning a pre-trained l\u221e-adversarial training (AT) model using l1 examples.  It shows a significant drop in l\u221e robustness after just one epoch of fine-tuning with standard AT and E-AT methods.  In contrast, the RAMP method effectively preserves both l\u221e and union robustness, indicating its superior ability to maintain robustness against multiple norms.", "section": "Connect natural training (NT) with AT"}]