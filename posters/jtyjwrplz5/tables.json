[{"figure_path": "jTyjwRpLZ5/tables/tables_1_1.jpg", "caption": "Table 1: The dependence of simple regret on T (number of function evaluations), d (dimension) and M (parameter describing strong convexity). Our results are highlighted in comparison to the prior works.", "description": "This table compares the upper and lower bounds of simple regret, a measure of performance in stochastic zeroth-order optimization, found in previous research to the upper and lower bounds derived in this paper. The simple regret depends on the number of function evaluations (T), the dimension of the problem (d), and a parameter (M) describing the strong convexity of the objective function.  The table shows that the current work provides tighter bounds than those previously established, achieving the minimax optimal sample complexity.", "section": "Summary of technical contributions"}, {"figure_path": "jTyjwRpLZ5/tables/tables_4_1.jpg", "caption": "Table 1: The dependence of simple regret on T (number of function evaluations), d (dimension) and M (parameter describing strong convexity). Our results are highlighted in comparison to the prior works.", "description": "This table compares the dependence of simple regret on the number of function evaluations (T), dimensionality (d), and strong convexity parameter (M) across different stochastic zeroth-order optimization algorithms.  It highlights the upper and lower bounds achieved by each algorithm, showing how the proposed algorithm achieves a tighter characterization of the minimax simple regret than previous methods. The table helps illustrate the improvement obtained by leveraging higher-order smoothness (Lipschitz Hessian) conditions.", "section": "Summary of technical contributions"}]