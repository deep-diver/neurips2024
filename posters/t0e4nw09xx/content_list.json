[{"type": "text", "text": "Universal Rates for Active Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Steve Hanneke Purdue University steve.hanneke@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Amin Karbasi Yale University amin.karbasi@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Shay Moran Technion, Google Research smoran@technion.ac.il ", "page_idx": 0}, {"type": "text", "text": "Grigoris Velegkas Yale University grigoris.velegkas@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work we study the problem of actively learning binary classifiers from a given concept class, i.e., learning by utilizing unlabeled data and submitting targeted queries about their labels to a domain expert. We evaluate the quality of our solutions by considering the learning curves they induce, i.e., the rate of decrease of the misclassification probability as the number of label queries increases. The majority of the literature on active learning has focused on obtaining uniform guarantees on the error rate which are only able to explain the upper envelope of the learning curves over families of different data-generating distributions. We diverge from this line of work and we focus on the distribution-dependent framework of universal learning whose goal is to obtain guarantees that hold for any fixed distribution, but do not apply uniformly over all the distributions. We provide a complete characterization of the optimal learning rates that are achievable by algorithms that have to specify the number of unlabeled examples they use ahead of their execution. Moreover, we identify combinatorial complexity measures that give rise to each case of our tetrachotomic characterization. This resolves an open question that was posed by Balcan et al. (2010). As a byproduct of our main result, we develop an active learning algorithm for partial concept classes that achieves exponential learning rates in the uniform setting. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The most prototypical type of learning is that of supervised learning, where an algorithm is given as input n labeled data points sampled i.i.d. from some unknown distribution and the goal is to output a function that has low probability of misclassifying new data from the same distribution. One caveat with this passive model of learning is that it fails to capture the settings in which unlabeled data is easily accessible, but obtaining their labels is costly. A natural example that fits this description is that of webpage classification. It is easy for any web crawler to collect information about billions of different webpages in a very short amount of time, however understanding which category a webpage belongs to usually requires human feedback. Perhaps the most commonly used way to model this problem is through the active learning framework in which the learner is given access to a large stream of unlabeled data and a budget of $n$ queries that it can submit to a domain expert in order to obtain the label of some datapoint. The learner\u2019s goal is to submit queries for the labels of the most informative examples, and, as a result, eliminate some redundancy in the information content of labeled data. ", "page_idx": 0}, {"type": "text", "text": "In this paper we study the optimal learning rates that are achievable by an active learning algorithm. In the passive learning setting, it is common to measure the quality of an algorithm by its learning curve, i.e., plotting the decay of its error rate as the number of training examples increases. In contrast, in the active learning setting, the resource we are interested in is the number of label queries the algorithm requests, so it is natural to consider the rate of decay of the misclassification probability as the number of queries $n$ increases. Most of the prior works on active learning have focused on obtaining uniform guarantees on the error rate, i.e., guarantees that hold uniformly over the data-generating distributions. In this work we follow a different path and we view the problem through the lens of universal rates that was recently introduced by Bousquet et al. (2021). In this framework we are aiming for guarantees that hold for all distributions, but they do not hold uniformly over them. In other words, we allow for distribution-dependent constants in the error rate. To make the distinction between uniform and universal rates clear, we first recall what uniform learnability of a hypothesis class $\\mathbb{H}$ means. We say that $\\mathbb{H}$ is uniformly learnable at rate $R(n)$ if there exists a learning rule $\\hat{h}_{n}$ such that ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\left(\\exists C,c>0\\right)\\left(\\forall\\mathbf{P}\\in\\operatorname{RE}(\\mathbb{H})\\right)\\,\\mathrm{it}\\,\\,\\mathrm{holds~that}\\,\\,\\mathbb{E}[\\operatorname{er}_{\\mathbf{P}}({\\hat{h}}_{n})]\\leq C\\cdot R(c\\cdot n),\\forall n\\in\\mathbb{N}\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The above expression states that there exists a learning rule $\\hat{h}_{n}$ and distribution-independent constants such that for all realizable distributions the expected error rate of the classifier is at most $C\\cdot R(c\\cdot n)$ . The difference in the definition of universal learnability is that we swap the order of the quantifiers. To be more precise, we say that a class $\\mathbb{H}$ is universally learnable at rate $R(n)$ if there exists a learning rule $\\hat{h}_{n}$ such that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\left(\\forall\\mathrm{P}\\in\\mathrm{RE}(\\mathbb{H})\\right)\\left(\\exists C,c>0\\right)\\,\\mathrm{such}\\,\\mathrm{that}\\,\\,\\mathbb{E}[\\mathrm{erp}(\\hat{h}_{n})]\\leq C\\cdot R(c\\cdot n),\\forall n\\in\\mathbb{N}\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Note that in the above definition the constants $c,C$ are distribution-dependent. As is evident from our main result and from prior results in universal learning (Bousquet et al., 2021; Hanneke et al., 2022; Kalavasis et al., 2022; Bousquet et al., 2022; Hanneke et al., 2023) this change in the definition affects the landscape of the optimal learning rates significantly. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Active Learning. There has been a very long line of work deriving theoretical guarantees for active learning, both in the realizable and the agnostic setting (Cohn et al., 1994; Dasgupta, 2005; Balcan et al., 2006; Hanneke, 2007b,a; Dasgupta et al., 2007; Hanneke, 2009; Balcan et al., 2009, 2010; Hanneke, 2012, 2014; Wiener et al., 2015; Hanneke and Yang, 2015; Beygelzimer et al., 2016). As we alluded to before, most of these works focus on obtaining minmax guarantees, i.e., the bounds they provide hold in the worst case over a family of distributions. To be more precise, while many of these results have distribution-dependent guarantees, they are typically expressed in a way that aims to match a lower bound on the minmax performance over a family o distributions, with respect to some parameter, like the disagreement coefficient. For these reasons the results in these works do not capture the full spectrum of universal rates. We also remark that there are a few works that do study universal rates, e.g. Balcan et al. (2010); Hanneke (2012); Yang and Hanneke (2013), but none of them have derived a complete characterization of the optimal rates. ", "page_idx": 1}, {"type": "text", "text": "Universal Rates. The study of universal learning rates was put forth in the seminal work of Bousquet et al. (2021) who derived a complete characterization of the optimal rates in the supervised learning setting. Later, Kalavasis et al. (2022) extended these result to the multiclass setting, with a bounded number of classes, and Hanneke et al. (2023) improved upon this result by characterizing multiclass classification with an infinite number of labels. Subsequently, the work of Bousquet et al. (2022) derived more fine-grained results for binary classification compared to Bousquet et al. (2021). The work that is most closely related to ours is Hanneke et al. (2022) which derives a complete characterization of the optimal learning rates in a very general interactive learning setting. In that setting, the learner is allowed to submit arbitrary binary valued queries about the unlabeled data. We provide a detailed comparison between our results and theirs in Section 1.5. Very recently, Attias et al. (2024) studied universal rates in the context of regression. ", "page_idx": 1}, {"type": "text", "text": "Partial Concept Classes. The vast majority of the literature in learning theory has focused on total concept classes, i.e., classes that consist of functions that are defined everywhere on the instance domain. Recently, Alon et al. (2021) proposed a learning theory of partial concept classes, i.e., classes that consist of functions that can be undefined on some parts of the instance domain. Later, Kalavasis et al. (2022) extended some of the results to the multiclass setting, with a finite number of labels. Recently, Cheung et al. (2023) studied partial concept classes in the context of online learning and they showed that there are such classes that are online learnable but none of their \u201cextensions\u201d to total concept classes is online learnable. The advantage of partial concepts is that they provide a convenient way to express data-dependent constraints. En route of obtaining our main result, we design active learning algorithms for partial concept classes. For a more detailed discussion about partial concepts we refer the reader to Appendix A.2. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.2 Formal Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Learning Model. We now present formally the learning setting that we consider in this work. There is a domain $\\mathcal{X}$ , which we assume to be a Polish space, and a concept class $\\mathbb{H}\\subseteq\\{0,1\\}^{x}$ , which satisfies standard measurability assumptions (see Definition A.1). We define a classifier $h:\\mathcal{X}\\to\\{0,1\\}$ to be a universally measurable function and its error rate is defined as $\\mathrm{er}_{\\mathrm{P}}(h):=$ $\\mathrm{P}\\left[(x,y):h(x)\\neq y\\right]$ , where $\\mathrm{P}$ is the data-generating distribution over ${\\mathcal{X}}\\times\\{0,1\\}$ . When $\\mathrm{P}$ is clear from the context we might drop the subscript $\\mathrm{P}$ in $\\exp(h)$ . We call $\\mathrm{P}$ realizable with respect to the hypothesis class $\\mathbb{H}$ if $\\operatorname*{inf}_{h\\in\\mathbb{H}}\\operatorname{er}_{\\mathrm{P}}(h)=0$ . We denote by $\\mathrm{P}_{\\mathcal{X}}$ be the marginal distribution of $\\mathrm{P}$ on $\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "Active Learning Model. We define an active learning algorithm to be a sequence of universally measurable functions which, given access to a stream of unlabeled data points from $\\mathcal{X}$ that are drawn i.i.d. from $\\mathrm{P}_{\\mathcal{X}}$ and a label query budget $n$ , output a classifier $\\hat{h}_{n}:\\mathcal{X}\\times\\{0,1\\}$ . In this work we consider a non-adaptive active learning model, with respect to the unlabeled data. In particular, the learning algorithm needs to specify a function $u:\\mathbb{N}\\to\\mathbb{N}$ so that $u(n)$ is the number of unlabeled points it observes and $n$ is the number of points for which it can request the label. The number $u(n)$ is specified before the execution of the algorithm and cannot be modified based on the realization of the unlabeled sequence or the answers that it gets for the labels of the points it queries. We place no bound whatsoever on the function $u(\\cdot)$ . Also, the algorithm can only request the labels of points it has observed and not arbitrary points from $\\mathcal{X}$ . We emphasize that the label requests that the algorithm makes can be adaptive, and can depend on answers to previous label queries. To the best of our knowledge, the active learning algorithms that have been proposed in the literature either fti into this model or they can be modified to satisfy the non-adaptivity restriction with negligible performance loss. ", "page_idx": 2}, {"type": "text", "text": "Learning Rates. We now define formally what it means for an algorithm to achieve a learning rate $R(n)$ in the universal learning model. We adopt the definition of Bousquet et al. (2021). ", "page_idx": 2}, {"type": "text", "text": "Definition 1.1 (Learning Rates (Bousquet et al., 2021)). Fix a concept class $\\mathbb{H},$ , and let $R:\\mathbb{N}\\rightarrow$ $[0,1],R(n)\\stackrel{n\\to\\infty}{\\longrightarrow}0$ be a rate function, where n is the label query budget of the learner. ", "page_idx": 2}, {"type": "text", "text": "\u2022 H is learnable at rate $R$ if there is a learning algorithm $\\hat{h}_{n}$ such that for every realizable distribution P, there exist $c,C$ for which $\\mathbb{E}[\\mathrm{er}(\\hat{h}_{n})]\\leq C R(c n),\\forall n\\in\\mathbb{N}.$ .   \n\u2022 $\\mathbb{H}$ is not learnable at rate faster than $R$ if for all learning algorithms $\\hat{h}_{n}$ there exists $a$ realizable distribution $\\mathrm{P}$ and $c,C$ for which $\\mathbb{E}[\\mathrm{er}(\\hat{h}_{n})]\\,\\geq\\,C R(c n),$ , for infinitely many $n\\in\\mathbb N$ .   \n\u2022 H is learnable with optimal rate $R$ if it is learnable at rate $R$ and it is not learnable at rate   \nfaster than $R$ .   \n\u2022 H admits arbitrarily fast rates if for all rate functions $R$ , it is learnable at rate $R$ .   \n\u2022 $\\mathbb{H}$ requires arbitrarily slow rates if for all rate functions $R$ , it is not learnable at rate faster than $R$ . ", "page_idx": 2}, {"type": "text", "text": "Combinatorial Measures. We now define some combinatorial complexity measures that our characterization relies on. To make the presentation easier to follow, we provide informal definitions. For the formal ones, we refer the reader to Appendix A.3. We first describe the Littlestone tree that was introduced by Bousquet et al. (2021). ", "page_idx": 2}, {"type": "text", "text": "Definition 1.2 (Littlestone Tree, Informal (see Definition A.8) (Bousquet et al., 2021)). A Littlestone tree for $\\mathbb{H}\\subseteq\\{0,1\\}^{\\scriptscriptstyle X}$ is a complete binary tree of depth $d\\leq\\infty$ whose nodes are labeled by elements of $\\mathcal{X}$ and the edges to the left, right child are labeled by $0,1$ . We require that for every level $0\\leq n<d$ and every path from the root to a node at level n there is some $h\\in\\mathbb{H}$ that realizes this path. We say that $\\mathbb{H}$ has an infinite Littlestone tree if it has a Littlestone tree of depth $d=\\infty$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We underline that this notion can be thought of as an infinite extension of the Littlestone dimension (Littlestone, 1988) of $\\mathbb{H}$ . Recall that the Littlestone dimension is defined to be the largest $d\\in\\mathbb{N}$ for which $\\mathbb{H}$ has a Littlestone tree of such depth and it is $\\infty$ if one can construct Littlestone trees of arbitrary depth. Crucially, this is not the same as having a single tree whose depth is infinite, so one can see that infinite Littlestone dimension is not the same as having an infinite Littlestone tree. ", "page_idx": 3}, {"type": "text", "text": "We next give the definition of the Vapnik-Chervonenkis-Littlestone tree (VCL) that was introduced by Bousquet et al. (2021). ", "page_idx": 3}, {"type": "text", "text": "Definition 1.3 (VCL Tree, Informal (see Definition A.10) (Bousquet et al., 2021)). A VCL tree for $\\mathbb{H}\\subseteq\\{0,1\\}^{\\scriptscriptstyle X}$ is a complete tree of depth $d\\leq\\infty$ such that every level $0\\leq n<d$ has nodes that are labeled by $\\chi^{n+1}$ with branching factor $2^{n+1}$ and whose $2^{n+1}$ edges connecting a node to its children are labeled by the elements of $\\{0,1\\}^{n+1}$ . We require that for every node at any level $0\\leq n<d$ , the path from the root to this node is realized by some $h\\in\\mathbb H$ . We say that $\\mathbb{H}$ has an infinite VCL tree if it has a VCL tree of depth $d=\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "Intuitively, the VCL tree combines the notions of the Littlestone tree and the VC dimension (Vapnik and Chervonenkis, 1971; Blumer et al., 1989). The differences between the Littlestone tree and the VCL tree are that in the latter the size of the nodes increases linearly with the level and the branching factor increases exponentially, whereas in the former all the nodes are singletons and the branching factor is always two. ", "page_idx": 3}, {"type": "text", "text": "We are now ready to introduce a new combinatorial measure which we call the star tree. ", "page_idx": 3}, {"type": "text", "text": "Definition 1.4 (Star Tree, Informal (see Definition A.9)). A star tree for $\\mathbb{H}\\subseteq\\{0,1\\}^{\\scriptscriptstyle X}$ is a complete tree of depth $d\\leq\\infty$ such that every level $0\\leq n<d$ has nodes that are labeled by $(\\mathcal{X}\\times\\{0,1\\})^{n+1}$ with branching factor $n+1$ and whose $n+1$ edges connecting a node to its children are labeled by $\\{0,\\ldots,n\\}$ . The label of the edge indicates the element of the node whose label along the path is flipped. We require that for every node at level $0\\leq n<d,$ , the path from the root to this node is realized by $\\mathbb{H}$ . We say that $\\mathbb{H}$ has an infinite star star tree if it has a star tree of depth $d=\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "Essentially, this definition combines the structure of a Littlestone tree with the notion of the star number (Hanneke and Yang, 2015). Every node at level $n$ consists of $n+1$ labeled points and there are $n+1$ edges attached to it. Each edge indicates which of the $n+1$ points of the labeled node has its label flipped along every path that this edge is part of. ", "page_idx": 3}, {"type": "text", "text": "1.3 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We are now ready to state the main results of this work. Our first main result is a complete characterization of the optimal learning rates that a class $\\mathbb{H}$ admits in the active learning setting. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1.5. For every concept class $\\mathbb{H}$ exactly one of the following cases holds. ", "page_idx": 3}, {"type": "text", "text": "\u2022 H is actively learnable at arbitrarily fast rates.   \n\u2022 H is actively learnable at an optimal rate $e^{-n}$ .   \n\u2022 H is actively learnable at $o(1/n)$ rates, but requires rates arbitrarily close to $1/n$ .   \n\u2022 H requires arbitrarily slow rates for active learning. ", "page_idx": 3}, {"type": "text", "text": "Our next result characterizes exactly when these rates occur by specifying combinatorial complexity measures of $\\mathbb{H}$ that determine which case of the tetrachotomy it falls into. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1.6. For every concept class $\\mathbb{H}$ the following hold. ", "page_idx": 3}, {"type": "text", "text": "\u2022 If H does not have an infinite Littlestone tree, then it is learnable at arbitrarily fast rates.   \n\u2022 If H has an infinite Littlestone tree but does not have an infinite star tree, then it is learnable at optimal rate e .   \n\u2022 If $\\mathbb{H}$ has an infinite star tree but does not have an infinite VCL tree, then it is learnable at optimal rate $o(1/n)$ , but requires rates arbitrarily close to $1/n$ . ", "page_idx": 3}, {"type": "text", "text": "We remark that the landscape of the optimal rates looks significantly different compared to the passive setting (Bousquet et al., 2021) and the general interactive learning setting (Hanneke et al., 2022). Our main result answers an open question that was posed in Balcan et al. (2010), which asks for necessary and sufficient conditions for learnability at an exponential rate in the active learning setting. ", "page_idx": 4}, {"type": "text", "text": "1.4 Examples ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now present examples of classes that witness each case of our tetrachotomic characterization. ", "page_idx": 4}, {"type": "text", "text": "Arbitrarily-fast rates: Bousquet et al. (2021) give examples with no infinite Littlestone tree (e.g., thresholds on the integers, and positive halfspaces on $\\mathbb{N}^{d}$ ), hence learnable at arbitrarily fast rates. ", "page_idx": 4}, {"type": "text", "text": "Exponential rates: Famously, threshold classifiers $[a,\\infty)$ over $\\mathbb{R}$ have an infinite Littlestone tree and are actively learnable at exponential rate (even uniformly). A more interesting example is the class of interval classifiers $[a,b]$ on $\\mathbb{R}$ , which (also famously) has uniform rate $1/n$ for active learning (it has infinite star number), and has an infinite Littlestone tree, but it has no infinite star tree: for any choice of $x$ in the root node, the edge labeling $x$ as 1 has a version space with star number equal 4 (it is effectively 2 disjoint threshold problems), so the depth of that subtree is bounded. Therefore, by our theory, it is actively learnable at $e^{-n}$ universal rate (in stark contrast to the uniform rate $1/n)$ ). ", "page_idx": 4}, {"type": "text", "text": "Sublinear rates: Halfspaces on $\\mathbb{R}^{d}$ have an infinite star tree but no infinite VCL tree. To construct that star tree, the key observation is that any set in convex position is a star set (Balcan et al., 2010). ", "page_idx": 4}, {"type": "text", "text": "Arbitrarily slow rates: Bousquet et al. (2021) provide examples of classes with an infinite VCL tree, such as the class of all claffisifiers, so these require arbitrarily slow rates in our setting as well. ", "page_idx": 4}, {"type": "text", "text": "1.5 Comparison to Supervised Learning and General Interactive Learning Setting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now compare our characterization to the results of Bousquet et al. (2021) that prove an analogous result in the supervised learning setting. Let us first recall their main result which shows that in this learning model a class is universally learnable at: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Exponential rate $e^{-n}$ if and only if it does not have an infinite Littlestone tree.   \n\u2022 Linear rate $1/n$ if and only if it has an infinite Littlestone tree but does not have an infinite VCL tree.   \n\u2022 Arbitrarily slow rates if and only if it has an infinite VCL tree. ", "page_idx": 4}, {"type": "text", "text": "As one can see, our results illustrate the advantage that active learning algorithms have over their supervised counterparts. We underline that the only case where active learning does not offer an improvement compared to the passive setting is when $\\mathbb{H}$ has an infinite VCL tree. ", "page_idx": 4}, {"type": "text", "text": "Let us now discuss the interactive learning model that was considered by Hanneke et al. (2022). In this general model of interaction, the learner is allowed to ask arbitrary binary queries about any subset of the unlabeled data. In particular, these queries include, but are not limited to, label queries, comparison queries, and general membership queries. They show that the optimal rates that any class $\\mathbb{H}$ admits are the following: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Arbitrarily fast if and only if it does not have an infinite Littlestone tree.   \n\u2022 Exponential if and only if it has an infinite Littlestone tree but not an infinite VCL tree.   \n\u2022 Arbitrarily slow if and only if it has an infinite VCL tree. ", "page_idx": 4}, {"type": "text", "text": "We underline that the algorithm by Hanneke et al. (2022) that achieves arbitrarily fast rates uses only label queries and the number of unlabeled data $u(n)$ that it uses can be chosen statically prior to the execution of the algorithm. Therefore, this result applies to the setting we consider in our work. Moreover, the lower bounds they provide also apply to our setting since (i) the queries that Hanneke et al. (2022) consider are more general, and (ii) their lower bounds hold even when the learner knows the marginal distribution $\\mathrm{P}_{\\mathcal{X}}$ . As is evident from our main result, the active learning setting provides a richer landscape of optimal rates compared to the general interactive learning setting of Hanneke et al. (2022). In particular, just the absence of an infinite VCL tree for $\\mathbb{H}$ does not necessarily imply that we can achieve (at least) exponential rates. To get this result, Hanneke et al. (2022) make strong use of these general queries in order to provide an algorithm that has a binary-search flavor and can learn partial concept classes with finite VC dimension at an exponential rate. Our main result shows that such guarantees are not achievable by using only label queries. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "1.6 Technical Challenges ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The most technically innovative part of our work is the $o(1/n)$ algorithm. Let us set up some terminology to facilitate our discussion. The version space of a concept class, given a labeled dataset $S$ , is the set of concepts that classify all the elements of $S$ correctly. Moreover, the VCL game is a Gale-Stewart game defined by Bousquet et al. (2021) that was used in their passive learning algorithm that achieves $1/n$ universal rate. The original (passive) learner from Bousquet et al. (2021) partitions a portion of the data into some number $B(n)$ of batches, from which they construct partial concept classes of finite VC dimension by using the batches to play the VCL game against the player\u2019s winning strategy, and for each resulting partial concept class they run a separate learner on another portion of data and return a majority vote of their resulting classifiers. The latter part of this strategy could never work in the active learning setting, since the number of batches $B(n)$ must be an increasing function of $n$ , and applying an active learner for each partial concept class separately would require each to use (nearly) $\\Omega(n)$ queries (to get the $o(1/n)$ guarantee there), so the total number of queries would be (nearly) $\\Omega(B(n)n)\\gg n$ , violating the label budget $n$ . To resolve this, we ended up completely re-imagining how to use these partial concept classes. Rather than running a separate algorithm for each class, we run a single active learning algorithm, where individual decisions of whether to query involve voting over the partial concept classes. We first extend an algorithm of Hanneke (2012) (for total concepts) to achieve $o(1/n)$ rate for partial concept VC classes (this required completely re-formulating Hanneke\u2019s analysis). The resulting algorithm involves estimating the probability that a random $k$ -tuple is VC shattered by certain constrained version spaces. We replace this with an estimated average (over a select subset of partial concept classes) of this shattering probability, which we show composes appropriately with the analysis of the algorithm to obtain the $o(1/n)$ rate. Comparing our work to the general interactive learning setting considered by Hanneke et al. (2022), the main differences are (i) we design a new algorithm that uses only label queries and achieves exponential rates when $\\mathbb{H}$ does not have an infinite star tree, a combinatorial measure we introduce in our work, (ii) we prove a $o(1/n)$ lower bound in the setting where $\\mathbb{H}$ has an infinite star tree, and (iii) we propose a novel active learning algorithm that achieves sublinear rates when $\\mathbb{H}$ does not have an infinite VCL tree. ", "page_idx": 5}, {"type": "text", "text": "2 Arbitrarily Fast Rates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As we explained before, the first case of the tetrachotomy in our characterization is a direct implication of the results by Hanneke et al. (2022). To be more precise, they design an algorithm which achieves arbitrarily fast rates using only label queries. In order to do that, they need the number of unlabeled points $u(n)$ to be an arbitrarily fast increasing function, that, nevertheless, can be specified in a non-adaptive manner prior to the execution of the algorithm. The result is summarized in Theorem 2.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.1 (Hanneke et al. (2022)). If H does not have an infinite Littlestone tree it is actively learnable with arbitrarily fast rates. ", "page_idx": 5}, {"type": "text", "text": "For completeness, we present their algorithm in Figure 1. Similary with the upper bound, the lower bound is an immediate consequence of a result from Hanneke et al. (2022), since the learner can submit more informative queries in their model. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.2 (Hanneke et al. (2022)). If H has an infinite Littlestone tree, then $\\mathbb{H}$ is not actively learnable at rate faster than exponential $e^{-n}$ . This holds even $i f\\mathrm{P}_{\\mathcal{X}}$ is known to the learner. ", "page_idx": 5}, {"type": "text", "text": "3 Exponentially Fast Rates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we prove the second case in the tetrachotomy we have stated, i.e., that $\\mathbb{H}$ is learnable at an exponential rate if and only if it has an infinite Littlestone tree and it does not have an infinite star tree. Our proof consists of two parts. First, we show that if $\\mathbb{H}$ does not have an infinite star tree it is learnable at an exponentially fast rate. Then, we show that whenever $\\mathbb{H}$ has an infinite star tree, the best achievable rate cannot exceed $o(1/n)$ . The omitted details can be found in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "3.1 Exponential Rates Algorithm: High-Level Overview ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The high-level approach to get the exponential rates algorithm follows the same spirit of the approaches from Bousquet et al. (2021); Hanneke et al. (2022). First, we design an appropriate Gale-Stewart game (cf. Appendix A.2), i.e., a game between a learner and an adversary, that is associated with $\\mathbb{H}$ in which the learner has a winning strategy if and only if $\\mathbb{H}$ does not have an infinite star tree. The next step is to show that, in the limit, the winning strategy of the learner gives rise to a partial concept class $\\mathcal{F}$ that has finite star number (see Definition A.7). Since this result is asymptotic, our approach is to consider several instances of this game, execute them for a finite number of steps, and obtain a partial concept class from each one. Then, we aggregate these classes into a majority class. The intuition is that, with high probability, most of the games will have induced classes whose star number is bounded by a distribution-dependent constant, so then we can show that the majority class will also have bounded star number, and this bound is distribution-dependent. Thus, our task boils down to actively learning a partial concept class whose star number is finite. In order to do that, we extend the approach that Hanneke and Yang (2015) used for total concept classes to the regime of partial classes. We believe that this result could be of independent interest. ", "page_idx": 6}, {"type": "text", "text": "3.2 The Star Tree Game ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first outline the Gale-Stewart game we use. Recall that every node of a star tree at depth $n$ consists of $n+1$ points along with their labels. There are $n+1$ edges that connect the node with its children and the label of every edge indicates the point whose label is flipped along any path that uses this edge. Let us now describe the game $\\mathfrak{G}$ that we use in this setting. In every round $\\tau\\geq1$ we have the following interaction between the learner $\\mathrm{P_{L}}$ and the adversary $\\mathrm{P_{A}}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\vec{\\xi_{\\tau}},\\vec{\\zeta_{\\tau}})=(\\xi_{\\tau}^{0},\\dots,\\xi_{\\tau}^{\\tau-1},\\zeta_{\\tau}^{0},\\dots,\\zeta_{\\tau}^{\\tau-1})\\in\\mathcal{X}^{\\tau}\\times\\{0,1\\}^{\\tau}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{H}_{\\vec{\\xi}_{1},\\vec{\\zeta}_{1},\\eta_{1},\\dots,\\vec{\\xi}_{r},\\vec{\\zeta}_{r},\\eta_{r}}:=\\left\\{h\\in\\mathbb{H}:\\begin{array}{l l}{h(\\xi_{s}^{i})=\\zeta_{s}^{i},}&{\\mathrm{if~}\\eta_{s}\\neq i}\\\\ {h(\\xi_{s}^{i})=1-\\zeta_{s}^{i},}&{\\mathrm{if~}\\eta_{s}=i}\\end{array},1\\leq s\\leq\\tau,0\\leq i<s\\right\\}=\\emptyset.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "It is easy to see that the winning condition for $\\mathrm{P_{L}}$ is finitely decidable (cf. Appendix A.2), hence $\\mathfrak{G}$ is a Gale-Stewart game. Recall that this means exactly one player between the adversary and the learner has a winning strategy. Using a result regarding the measurability of winning strategies in Gale-Stewart games that was shown by Bousquet et al. (2021) (see Theorem A.3) we can prove the following connection between $\\mathfrak{G}$ and the existence of infinite star trees ( see Appendix C.1 for the proof). ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.1. The class $\\mathbb{H}$ does not have an infinite star tree if and only if $P_{L}$ has a universally measurable winning strategy in $\\mathfrak{G}$ . ", "page_idx": 6}, {"type": "text", "text": "The first step in our approach, is to make use of some $\\Theta(n)$ label queries in order to obtain the labels of $\\Theta(n)$ many points. The idea these labeled points in order to simulate the Gale-Stewart game we described above (see Appendix C.3 for the details). The main technical issue we need to handle is that we have no control over the number of rounds the game needs in order to terminate. Using ideas that have appeared in the universal learning literature, we use a portion of these labeled points to estimate some number $\\hat{t}_{n}$ so that, with at least some constant probability over the dataset, the game will terminate within $\\hat{t}_{n}$ many rounds. Then, we split the remaining of the labeled dataset into batches of size $\\hat{t}_{n}$ and we run the game on each batch. The outcome of each game gives us a pattern-avoidance function, i.e., a function that takes an input tuples of arbitrarily labeled points and changes the label of one of them so that the resulting labeled tuple is not consistent with the data-generating distribution P. In other words, the output of this function could not have been generated by the P. A technical complication we need to handle is that we obtain multiple such pattern avoidance functions, some of which are incorrect, but to make the presentation cleaner we explain the idea using a single pattern avoidance function $\\widetilde g_{t^{*}}$ that produces inconsistent labels. The formal setting is handled in Appendix C.4. One way to think about this function is that it provides data-dependent constraints. Thus, it is natural to express such a constraint through a partial concept class. We define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\mathbf{\\Sigma}}=\\left\\{f:\\mathcal{X}\\to\\{0,1,\\boldsymbol{\\star}\\}:(x_{1},f(x_{1})),(x_{2},f(x_{2})),\\dots,(x_{t^{*}},f(x_{t^{*}}))\\notin\\mathrm{image}(\\widetilde{g}_{t^{*}}),\\forall x_{1},\\dots,x_{t^{*}}\\in\\mathcal{X}_{t^{*}},f(x_{t^{*}})\\in\\mathrm{or}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Notice that the constraint we have placed on $\\mathcal{F}$ is satisfied if $f(x_{i})\\,=\\,\\star$ , for some $i\\in[t^{*}]$ . We consider the natural extension of the notion of star sets to the case of partial concept classes, i.e., we say that a labeled set $S$ with labels in $\\{0,1\\}$ is a star set if $S$ and its adjacent sets $S^{\\prime}$ , whose labels are still restricted to be in $\\{0,1\\}$ , are obtainable using functions from $\\mathcal{F}$ (see Definition A.7). A key observation is that the star number of $\\mathcal{F}$ is bounded by $t^{*}-1$ . Another difficulty we need to overcome is that we do not know $t^{*}$ , since it is a random variable that depends on the realized sequence and its distribution might have heavy tails. To make our approach easier to follow, let us first assume that we do know $t^{*1}$ . Then, our task boils down to actively learning a partial concept class. ", "page_idx": 7}, {"type": "text", "text": "3.3 Active Learning of Partial Concept Classes with Finite Star Number ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now present an algorithm that achieves exponential rates when actively learning a partial concept class $\\mathcal{F}$ that star number $\\mathfrak{s}<\\infty$ . The idea of our approach is to reduce the problem of actively learning a partial concept to the well-studied problem of actively learning a total concept class. The algorithm is presented in Figure 2. Let us explain the high-level ideas of the algorithm. First, we consider a large enough set of unlabeled data. Our goal is to find their labels using logarithmically many queries. To do that, we consider the uniform distribution over these unlabeled examples. Then, we use an algorithm from Hanneke and Yang (2015) (see Theorem C.1) which guarantees exponential rates when applied to a class with finite star number. Because the underlying distribution on the sample is uniform, with high probability, the algorithm will find the correct labels of all the points. Finally, we feed these labeled examples to the one-inclusion graph algorithm (Theorem A.5) to get the desired result. We are now ready to state our theorem. The proof is postponed to Appendix C.2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.2. There exists an active learning algorithm $\\boldsymbol{\\mathcal{A}}$ for a partial concept class $\\mathcal{F}$ which given a label budget n and access to unlabeled samples from a realizable distribution $\\mathrm{P^{*}}$ returns a classifier $\\hat{h}_{n}$ such that $\\mathbb{E}\\mathrm{P}^{*}[\\mathrm{er}(\\hat{h}_{n})]\\leq c_{1}\\mathrm{d}e^{-c_{2}n/\\mathfrak{s}}$ , where $c_{1},c_{2}$ are absolute numerical constants, and ${\\mathfrak{s}},\\mathrm{d}$ is the star number, VC dimension of $\\mathcal{F}$ . ", "page_idx": 7}, {"type": "text", "text": "3.4 Slower than Exponential is Sublinear ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The next step in the characterization is to show that if $\\mathbb{H}$ has an infinite star tree, then it does not admit rates faster than sublinear. The proof starts by picking a random path on the infinite star tree. The target distribution is supported only on nodes of the selected path. Then, given some algorithm $\\hat{h}_{n}$ , we distribute the mass of the target probability distribution across the path, potentially skipping some nodes of it, in a way that creates an infinite sequence $n_{i_{1}},n_{i_{2}},\\ldots{}$ , so that when the learner has label budget $n_{i_{j}}$ , with some constant probability, it will only observe unlabeled points up to level $k_{i_{j}}$ . Moreover, with at least some constant probability, it will not query the point of that level whose label is filpped along the target path. On that event, it makes a mistake with probability at least $C\\cdot p_{i_{j}}/k_{i_{j}}$ , where $C$ is some absolute constant. Our choice of $p_{i_{j}},k_{i_{j}}$ guarantees that $R(n_{j})>p_{i_{j}}/k_{i_{j}}$ , where $R(\\cdot)$ is the target sublinear rate function. Finally, we apply Fatou\u2019s lemma to get the desired result. For the full proof and the formal theorem statement, we refer the reader to Appendix C.5 ", "page_idx": 7}, {"type": "text", "text": "4 Sublinear Rates ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our approach to achieve sublinear rates in the setting where $\\mathbb{H}$ does not have an infinite VCL tree shares some high-level ideas with the one in Section 3, but many technical challenges make it significantly more involved. The main obstacle is that there is no active learning algorithm that achieves sublinear rates for VC classes uniformly over all realizable distributions. Recall that in the exponential rates setting, such an algorithm does exist (Hanneke and Yang, 2015). Instead, the sublinear rates algorithm for VC classes from Hanneke (2012) depends on distribution-dependent constants in the sample complexity. The omitted details from this section can be found in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Instead of the star tree Gale-Stewart game that was used to get the exponential rates guarantee, we use the VCL Gale-Stewart game Bousquet et al. (2021) (cf. Figure 6). To be more precise, we use $\\lfloor n/5\\rfloor$ of the label budget to get th\u221ae labels of $\\lfloor n/5\\rfloor$ unlabeled poin\u221ats that come i.i.d. from $\\mathrm{P}_{\\mathcal{X}}$ . Then, we execute the VCL game on $\\Theta({\\sqrt{n}})$ different batches of size $\\Theta({\\sqrt{n}})$ . Each of these games induces a partial concept class. We show how to obtain a $\\mathrm{P}$ -dependent bound on the VC dimension that holds for most of these classes. Moreover, we show that for most of these classes the data-generating distribution $\\mathrm{P}$ is realizable. Finally, we design a single active learning algorithm that combines information from all these classes and achieves sublinear learning rates. This algorithm builds upon Hanneke (2012) but is modified to work with partial concept classes instead of total concept classes. This requires a very different analysis and is the most technically involved part of our work. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Let us now explain the main ideas of this algorithm and the challenges behind it. As we mentioned before, the number of queries that the algorithm from Hanneke (2012) needs to achieve the sublinear error rate depends on the underlying data-generating distribution. Thus, we cannot just get a large enough number of unlabeled samples, consider the uniform distribution over them and use the algorithm on this distribution. This is because the learning rate for $\\mathbb{H}$ would depend on the uniform distribution $U_{S}$ over the sample $S$ and not on the data-generating distribution $\\mathrm{P}$ . To illustrate the ideas of the algorithm, we consider five different streams of i.i.d. (unlabeled) data $S_{1},S_{2},S_{3},S_{4},S_{5}$ . For the purposes of the subsequent discussion, we can imagine that these streams have infinite size, but as explained in description of the algorithm, we only need $\\mathrm{poly}(n)$ unlabeled points. Let us first explain the use of $S_{5}$ . We use $n/5$ of our query budget to obtain the labels of the first $n/5$ points and then we use them to train a supervised learning algorithm from Bousquet et al. (2021) that achieves linear rate $O(1/n)$ . This is used for technical reasons in our analysis and in order to ensure that the classifier we output has, at most, linear error rate no matter how the active learning component of our algorithm behaves. Next, we use $n/5$ of the query budget to obtain the l\u221aabels of the first $n/5$ points from $S_{1}$ . Then, we run the VCL game on these labeled datasets of size ${\\sqrt{n}}/5$ and obtain $\\sqrt{n}$ differen\u221at pattern avoidance functions $\\widehat{y}_{\\sqrt{n}/5}^{i}$ that take as input \u2113i\u221an/5 points (cf. Appendix D.2), where i \u2208[ n]. Let ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\sqrt{n}/5}^{i}:=\\left\\{f:\\mathcal{X}\\to\\{0,1,\\star\\}:(f(x_{1}),\\dots,f(x_{\\ell_{\\sqrt{n}/5}}))\\neq\\widehat{y}_{\\sqrt{n}/5}^{i}(x_{1},\\dots,x_{\\ell_{\\sqrt{n}/5}})\\right\\},i\\in[\\sqrt{n}]\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "First, we show that for a $(1\\mathrm{~-~}o(1))$ -fraction of these partial concept classes $\\mathrm{P}$ is a realizable distr\u221aibution. Intuitively, this means that the partial concept class we obtain by running the VCL game on $\\sqrt{n}$ many points is the same as the class we would have obtained if we were to run the game on infinitely many points (cf. Lemma D.4). Next, we need to estimate some number $\\widehat{\\mathrm{d}}_{n}\\in\\mathbb{N}$ which, as $n\\to\\infty$ , converges to the $9/10$ -quantile $\\mathrm{d^{*}}$ of the distribution of the VC dimen sion of the partial concept classes that are obtained by running the VCL game on infinitely many samples. We let ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\mathsf{l}}_{n}:=\\operatorname*{min}_{d\\in\\mathbb{N}}\\left\\{\\exists i_{1},i_{2},\\dotsc,i_{9/10\\cdot\\sqrt{n}}\\in[\\sqrt{n}]:i_{1}<i_{2}<\\dotsc<i_{9/10\\cdot\\sqrt{n}},\\mathrm{d}\\left(\\mathcal{F}_{\\sqrt{n}/5}^{i_{j}}\\right)\\leq d,\\forall j\\in[9/10\\sqrt{n}]\\right\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathrm{d}(\\mathcal{F})$ denotes the VC dimension of class $\\mathcal{F}$ . Lemma D.5 shows that, for large enough $n$ , $\\widehat{\\mathrm{d}}_{n}\\,=\\,\\mathrm{d}^{*}$ , with high probability, where $\\mathrm{d}^{*}\\,\\in\\,\\mathbb{N}$ is such that with probability at least $9/10$ over the random draw2 of the partial class, its VC dimension is at most $\\mathrm{d^{*}}$ . One technical complication w\u221ae need to handle is that the concept classes we have obtained are estimated from a game on $\\sqrt{n}$ many points instead of infinitely many points, so a $o(1)$ -fraction of them do not correspond to samples from the correct distribution. To do that we use a robust version of the well-known Dvoretzky\u2013Kiefer\u2013Wolfowitz (DKW) inequality (cf. Theorem D.1) for estimating the CDF. ", "page_idx": 8}, {"type": "text", "text": "Next, we use another $n/5$ of the query budget to obtain the labels of the first $n/5$ points of $S_{2}$ . We will make two distinctions regarding this stream. For the purposes of the analysis, we consider a fixed stream of infinitely many i.i.d. samples from $\\mathrm{P}$ and we denote it by $S_{2}^{\\infty}$ , but in the actual algorithm we use a dataset of size $\\Theta(n)$ and we denote it by $S_{2}^{n}$ . We define ", "page_idx": 8}, {"type": "equation", "text": "$$\nV_{n/5}^{i}:=\\left\\{f\\in\\mathcal{F}_{\\sqrt{n}/5}^{i}:f(x)=y\\;\\mathrm{for}\\;\\mathrm{the}\\;\\mathrm{frst}\\;n/5\\;\\mathrm{points}\\;\\mathrm{in}\\;S_{2}^{\\infty}\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "to be the version space of $\\mathcal{F}_{\\sqrt{n}/5}^{i}$ defined on the first $n/5$ examples of $S_{2}^{\\infty}$ . Moreover, we let $V_{\\mathrm{d}^{\\ast},n/5}$ be a random version space that is sampled from the following process: we run the VCL game on an infinite stream of labeled data from $\\mathrm{P}$ to get a pattern avoidance function $\\widetilde{y}$ and then we define the partial concept class $\\widetilde{\\mathcal F}$ in the same way as before. If the VC dimension of this class is greater than $\\mathrm{d^{*}}$ we discard it and restart the process. Otherwise, we let $V_{\\mathrm{d}^{*},n/5}$ be the version space of $\\widetilde{\\mathcal F}$ on the first $n/5$ labeled points of $S_{2}^{\\infty}$ . We take $\\mathrm{d^{*}}$ to be the $9/10-$ quantile ofP as described before (cf. Lemma D.5). Given some $k\\in\\mathbb{N}$ , let $p_{n,k}:=\\mathbb{E}[\\mathrm{P}^{k}(x_{1},\\ldots,x_{k}\\,\\backslash\\$ C shattered by $V_{\\mathrm{d}^{*},n/5})|S_{2}^{\\infty}]$ where the expectation is over the draw of $V_{\\mathrm{d}^{\\ast},n/5}$ , given the fixed $S_{2}^{\\infty}$ . Let $k^{*}\\in\\mathbb{N}$ be the largest number such that $\\operatorname*{lim}_{n\\to\\infty}p_{n,k^{*}}\\neq0$ . Notice that since, by definition, the VC dimension is bounded by $\\mathrm{d^{*}}$ such a number $k^{*}$ exists. From here on, we will only consider the version spaces $V_{n/5}^{i}$ that are obtained from some partial class with VC dimension at most $\\widehat{\\mathrm{d}}_{n}$ . Let $p_{n,k,i}\\,:=\\,\\mathrm{P}^{k}(x_{1},...\\,,x_{k}\\,\\lor\\!\\ensuremath{\\mathrm{C}}$ shattered by $V_{n/5}^{i}$ ). Notice that for every $k\\leq{\\widehat{\\mathrm{d}}}_{n}$ and every $i\\in[{\\sqrt{n}}]$ we can estimate this quantity to arbitrary precision using only unlabeled examples. We denote by $\\bar{\\widehat{p_{n,k,i}}}$ these estimates. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We now consider the first $n^{2}$ unlabeled points of the third data stream $S_{3}$ . For each such point $X$ , let $p_{n,k}^{X}:=\\mathbb{E}[\\mathrm{P}^{k}(x_{1},\\dots,x_{k},X\\,\\backslash\\,$ VC shattered by $V_{\\mathrm{d}^{*},n/5})|S_{2}^{\\infty},X]$ . Moreover, for each $y\\in\\{0,1\\}$ let $V_{\\mathrm{d}^{*},n/5}^{(X,y)}:=\\{f\\in V_{\\mathrm{d}^{*},n/5}:f(X)=y\\},\\;\\;p_{n,k}^{(X,y)}:=\\mathbb{E}[\\mathrm{P}^{k}(x_{1},\\ldots,x_{k}~\\mathrm{VC}~\\mathrm{sl}~\\!\\mathrm{X})].$ hattered by $V_{\\mathrm{d}^{*},n/5}^{(X,y)})|S_{2}^{\\infty},X],y\\in\\left\\{0,1\\right\\}.$ ", "page_idx": 9}, {"type": "text", "text": "We define the quantities pnX,k,i, p(n,Xk,,yi) in the same way for the realized version spaces. Again, Similarly as before, we denote these estimates by $\\widehat{p}_{n,k,i}^{X},\\widehat{p}_{n,k,i}^{(X,y)}$ . The idea is to make use of Lemma D.4 only affect our estimates by some $o(1)$ . This is formalized in Proposition D.6. Thus, we can use $\\widehat{p}_{n,k,i},\\widehat{p}_{n,k,i}^{X},\\widehat{p}_{n,k,i}^{(X,y)}$ , in order to estimate $p_{n,k,},p_{n,k}^{X},p_{n,k}^{(X,y)}$ , p(n,Xk,y). We denote these estimates bypn,k,,pnX,k, p(n,Xk,y). These are the key quantities we use to infer the labels of unlabeled points. ", "page_idx": 9}, {"type": "text", "text": "Our algorithm tries to infer the label of each unlabeled point $X\\,\\in\\,S_{3}$ (cf. Figure 7) in the following way: if $\\widehat{p}_{n,k}^{X}\\ \\geq\\ \\frac{\\widehat{p}_{n,k}}{2}$ then we query the label of $X$ , otherwise we infer the label to be $\\operatorname*{max}_{y\\in\\{0,1\\}}\\widehat{p}_{n,k}^{(X,y)}$ p (n,Xk,y). Lemma D.7 shows that the inferences are correct, when n is large enough. The main ingredient of the proof that remains to be handled is to show that the number of label queries we submit is sublinear in $n$ . For that, it is sufficient to show that the probability that we query the label of a point is $o(1)$ . Lemma D.8 shows that when $k=k^{*}$ , this is indeed the case. ", "page_idx": 9}, {"type": "text", "text": "Finally, since we do not know the true value of $k^{*}$ , we run the algorithm for every $k\\leq{\\widehat{\\mathrm{d}}}_{n}$ . The active learning component of the algorithm gives us $\\widehat{\\mathrm{d}}_{n}+1$ different labeled datasets, which we use to train $\\widehat{\\mathrm{d}}_{n}+1$ instances of a supervised learning algorithm, such as the one from Bousquet et al. (2021). Our analysis so far has shown that for sufficiently large $n$ , at least one of these datasets will be correctly labeled with size $\\omega(n)$ . Thus, since the supervised learning algorithm has error linear in the size of its training set, at least one of these executions will give a classifier that has error $o(1/n)$ . The last step is to run a tournament among the $\\mathrm{d}_{n}+2$ different classifiers3 to choose the best one. This is handled by Lemma D.9 (Hanneke, 2012). The main result of this section (cf. Theorem D.10), follows as a corollary of the results we have discussed. All the steps are summarized in Figure 7. ", "page_idx": 9}, {"type": "text", "text": "Lastly, a lower bound from Hanneke et al. (2022) completes our characterization (cf. Theorem D.11). ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work we have provided a complete characterization of the optimal learning rates in active learning. It is an open question if it also holds when the learner knows the distribution $\\mathrm{P}_{\\mathcal{X}}$ . ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032), ONR (N00014- 19-1-2406), and the AI Institute for Learning-Enabled Optimization at Scale (TILOS). Shay Moran is a Robert J. Shillman Fellow; he acknowledges support by ISF grant 1225/20, by BSF grant 2018385, by Israel PBC-VATAT, by the Technion Center for Machine Learning and Intelligent Systems (MLIS), and by the the European Union (ERC, GENERALIZATION, 101039692). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. Grigoris Velegkas was supported in part by the AI Institute for Learning-Enabled Optimization at Scale (TILOS). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "N. Alon, S. Hanneke, R. Holzman, and S. Moran. 2021. A Theory of PAC Learnability of Partial Concept Classes. In Proceedings of the $62^{\\mathrm{nd}}$ Annual Symposium on Foundations of Computer Science.   \nIdan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, and Grigoris Velegkas. 2024. Universal Rates for Regression: Separations between Cut-Off and Absolute Loss. In The Thirty Seventh Annual Conference on Learning Theory. PMLR, 359\u2013405.   \nM.-F. Balcan, A. Beygelzimer, and J. Langford. 2006. Agnostic Active Learning. In Proceedings of the $23^{\\mathrm{rd}}$ International Conference on Machine Learning.   \nM.-F. Balcan, A. Beygelzimer, and J. Langford. 2009. Agnostic Active Learning. J. Comput. System Sci. 75, 1 (2009), 78\u201389.   \nM.-F. Balcan, S. Hanneke, and J. Wortman Vaughan. 2010. The True Sample Complexity of Active Learning. Machine Learning 80, 2\u20133 (2010), 111\u2013139.   \nA. Beygelzimer, D. J. Hsu, J. Langford, and C. Zhang. 2016. Search improves label for active learning. In Advances in Neural Information Processing Systems 29.   \nA. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. 1989. Learnability and the VapnikChervonenkis Dimension. Journal of the Association for Computing Machinery 36, 4 (1989), 929\u2013965.   \nOlivier Bousquet, Steve Hanneke, Shay Moran, Jonathan Shafer, and Ilya Tolstikhin. 2022. FineGrained Distribution-Dependent Learning Curves. arXiv preprint arXiv:2208.14615 (2022).   \nO. Bousquet, S. Hanneke, S. Moran, R. van Handel, and A. Yehudayoff. 2021. A Theory of Universal Learning. In Proceedings of the $53^{\\mathrm{rd}}$ Annual ACM Symposium on the Theory of Computing.   \nTsun-Ming Cheung, Hamed Hatami, Pooya Hatami, and Kaave Hosseini. 2023. Online Learning and Disambiguations of Partial Concept Classes. arXiv preprint arXiv:2303.17578 (2023).   \nD. Cohn, L. Atlas, and R. Ladner. 1994. Improving Generalization with Active Learning. Machine Learning 15, 2 (1994), 201\u2013221.   \nDonald L Cohn. 2013. Measure theory. Vol. 1. Springer.   \nS. Dasgupta. 2005. Coarse Sample Complexity Bounds for Active Learning. In Advances in Neural Information Processing Systems 18.   \nS. Dasgupta, D. Hsu, and C. Monteleoni. 2007. A General Agnostic Active Learning Algorithm. In Advances in Neural Information Processing Systems 20.   \nDavid Gale and Frank M Stewart. 1953. Infinite games with perfect information. Contributions to the Theory of Games 2, 245-266 (1953), 2\u201316.   \nS. Hanneke. $2007\\mathrm{a}$ . A Bound on the Label Complexity of Agnostic Active Learning. In Proceedings of the $24^{\\mathrm{th}}$ International Conference on Machine Learning.   \nS. Hanneke. 2007b. Teaching Dimension and the Complexity of Active Learning. In Proceedings of the $20^{\\mathrm{th}}$ Conference on Learning Theory.   \nS. Hanneke. 2009. Theoretical Foundations of Active Learning. Ph. D. Dissertation. Machine Learning Department, School of Computer Science, Carnegie Mellon University.   \nS. Hanneke. 2012. Activized Learning: Transforming Passive to Active with Improved Label Complexity. Journal of Machine Learning Research 13, 5 (2012), 1469\u20131587.   \nS. Hanneke. 2014. Theory of Disagreement-Based Active Learning. Foundations and Trends in Machine Learning 7, 2\u20133 (2014), 131\u2013309.   \nSteve Hanneke, Amin Karbasi, Shay Moran, and Grigoris Velegkas. 2022. Universal Rates for Interactive Learning. In Advances in Neural Information Processing Systems.   \nSteve Hanneke, Shay Moran, and Qian Zhang. 2023. Universal Rates for Multiclass Learning. In The Thirty Sixth Annual Conference on Learning Theory. PMLR, 5615\u20135681.   \nS. Hanneke and L. Yang. 2015. Minimax Analysis of Active Learning. Journal of Machine Learning Research 16, 12 (2015), 3487\u20133602.   \nD. Haussler, N. Littlestone, and M. Warmuth. 1994. Predicting $\\{0,1\\}$ -Functions on Randomly Drawn Points. Information and Computation 115, 2 (1994), 248\u2013292.   \nWilfrid Hodges, Hodges Wilfrid, et al. 1993. Model theory. Cambridge University Press.   \nAlkis Kalavasis, Grigoris Velegkas, and Amin Karbasi. 2022. Multiclass Learnability Beyond the PAC Framework: Universal Rates and Partial Concept Classes. arXiv preprint arXiv:2210.02297 (2022).   \nAlexander Kechris. 2012. Classical descriptive set theory. Vol. 156. Springer Science & Business Media.   \nN. Littlestone. 1988. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning 2 (1988), 285\u2013318.   \nV. Vapnik and A. Chervonenkis. 1971. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications 16, 2 (1971), 264\u2013280.   \nY. Wiener, S. Hanneke, and R. El-Yaniv. 2015. A Compression Technique for Analyzing Disagreement-Based Active Learning. Journal of Machine Learning Research 16, 4 (2015), 713\u2013745.   \nL. Yang and S. Hanneke. 2013. Activized Learning with Uniform Classification Noise. In Proceedings of the $30^{\\mathrm{th}}$ International Conference on Machine Learning. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Omitted Details from Section 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Formal Definition of Learning Setting ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We recall some basic notions of measures and probabilities on Polish spaces. For a detailed treatment the reader is referred to Kechris (2012); Cohn (2013). Our presentation follows Bousquet et al. (2021). ", "page_idx": 12}, {"type": "text", "text": "Polish Spaces. A Polish space is a separable topological space that admits a complete metric. For example, this category includes $\\mathbb{R}^{n}$ , any compact metric space, any separable Banach space, etc. ", "page_idx": 12}, {"type": "text", "text": "Universally Measurable Functions. Let $\\mathfrak{F}$ be the Borel $\\sigma$ -field on some Polish space $\\mathcal{X}$ and let $\\mu$ be a probability measure. We denote by $\\mathfrak{F}_{\\mu}$ the completion of $\\mathfrak{F}$ under $\\mu$ , i.e., the collections of all subsets of $\\mathcal{X}$ that differ from a Borel set on a set of zero measure. A set $B\\subseteq\\mathcal{X}$ is called universally measurable if $B\\in\\mathfrak{F}_{\\mu}$ for every probability measure $\\mu$ . Moreover, a function $f:\\mathcal X\\to\\mathcal Y$ is called universally measurable if $f^{-1}(B)$ is universally measurable, for any universally measurable set $B$ . An important property is that universally measurable sets and functions on Polish spaces are the same as Borel sets, from a probabilistic point of view. ", "page_idx": 12}, {"type": "text", "text": "We are now ready to provide the definition of measurability of a concept class $\\mathbb{H}$ . ", "page_idx": 12}, {"type": "text", "text": "Definition A.1 (Measurability of $\\mathbb{H}$ ). Let $\\mathcal{X}$ be a Polish space. We say that a concept class $\\mathbb{H}\\subseteq$ $\\{0,1\\}^{\\mathcal{X}}$ , is measurable if there is a Polish space $\\Theta$ and a Borel-measurable map $h:\\mathcal{X}\\times\\Theta\\rightarrow\\{0,1\\}$ so that $\\mathbb{H}=\\{h(\\theta,\\cdot):\\theta\\in\\Theta\\}$ . ", "page_idx": 12}, {"type": "text", "text": "We underline that Definition A.1 is very general and its only requirement is that $\\mathbb{H}$ can be parameterized in some reasonable way. ", "page_idx": 12}, {"type": "text", "text": "A.2 Omitted Preliminaries ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Gale-Stewart Games. We briefly discuss some basic and useful facts about Gale-Stewart games, a concept that was recently introduced to the learning theory community by Bousquet et al. (2021). Our discussion follows Bousquet et al. (2021); Hanneke et al. (2022). We refer to Bousquet et al. (2021) and references therein for a more detailed presentation. Let us fix sequences of sets $\\mathcal X_{t},\\mathcal X_{t}$ for $t\\geq1$ . We consider infinite games between two players, a learner $\\mathrm{P_{L}}$ and an adversary $\\mathrm{P_{A}}$ , where in each round $t\\geq1$ , $\\mathrm{P_{A}}$ selects an element $x_{t}\\in\\mathcal X_{t}$ , and then $\\mathrm{P_{L}}$ selects an element $y_{t}\\in\\mathcal{Y}_{t}$ . The rules of the game are defined by a set $\\mathcal{W}\\subseteq\\prod_{t\\ge1}(\\mathcal{X}_{t}\\times\\mathcal{Y}_{t})$ of winning sequences for $\\mathrm{P_{L}}$ . This means that after an infinite sequence of consecutive plays $x_{1},y_{1},x_{2},y_{2},\\ldots,$ we say that $\\mathrm{P_{L}}$ wins if $(x_{1},y_{1},x_{2},y_{2},\\ldots)\\in\\mathcal{W}$ ; otherwise the winner is $\\mathrm{P_{A}}$ . ", "page_idx": 12}, {"type": "text", "text": "A strategy is a rule used by each of the players to determine their next move, given the current state and the history of the game. Formally, a strategy for $\\mathrm{P_{A}}$ is a sequence of functions $f_{t}~:$ $\\begin{array}{r}{\\prod_{s<t}(\\mathcal{X}_{s}\\times\\mathcal{Y}_{s})\\,\\to\\,\\chi_{t}}\\end{array}$ for $t\\,\\geq\\,1$ , so that $\\mathrm{P_{A}}$ plays $x_{t}\\,=\\,f_{t}(x_{1},y_{1},\\dots,x_{t-1},y_{t-1})$ in round $t$ Similarly, a strategy for $\\mathrm{P_{L}}$ is a sequence of $\\begin{array}{r}{g_{t}:\\prod_{s<t}(\\mathcal{X}_{s}\\times\\mathcal{Y}_{s})\\times\\mathcal{X}_{t}\\to\\mathcal{Y}_{t}}\\end{array}$ for $t\\geq1$ , so that $\\mathrm{P_{L}}$ plays $y_{t}=g_{t}(x_{1},y_{1},\\ldots,x_{t-1},y_{t-1},x_{t})$ in round $t$ . We say that a strategy for $\\mathrm{P_{A}}$ is winning if playing that strategy always makes $\\mathrm{P_{A}}$ win the game no matter what $\\mathrm{P_{L}}$ plays; a winning strategy for $\\mathrm{P_{L}}$ is defined similarly. The main question in these infinite games is determine conditions under which one of the two players has a winning strategy in the game. We remark that when the game is finite it is not hard to easy that exactly one of the two players has such a strategy. In the context of infinite games, such a condition was introduced by Gale and Stewart (1953): a $\\mathcal{W}$ is finitely decidable if for every sequence of plays $(x_{1},y_{1},x_{2},y_{2},\\ldots)\\in\\mathcal{W}$ , there exists some $n<\\infty$ so that ", "page_idx": 12}, {"type": "equation", "text": "$$\n(x_{1},y_{1},\\dotsc,x_{n},y_{n},x_{n+1}^{\\prime},y_{n+1}^{\\prime},x_{n+2}^{\\prime},y_{n+2}^{\\prime},\\dotsc)\\in{\\mathcal{W}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for all choices of $x_{n+1}^{\\prime},y_{n+1}^{\\prime},x_{n+2}^{\\prime},y_{n+2}^{\\prime},\\ldots$ In words, the condition that $^{\\bullet\\bullet}\\mathcal{W}$ is finitely decidable\u201d means that if $\\mathrm{P_{L}}$ wins the game, then she knows that after playing a finite number of rounds. Conversely, $\\mathrm{P_{A}}$ wins the game when $\\mathrm{P_{L}}$ does not win after any finite number of rounds. ", "page_idx": 12}, {"type": "text", "text": "Such a game whose set $\\mathcal{W}$ is finitely decidable is called a Gale-Stewart game. We use the following important result about Gale-Stewart games. ", "page_idx": 12}, {"type": "text", "text": "Remark A.2 (Gale and Stewart (1953); Hodges et al. (1993); Kechris (2012)). In any Gale-Stewart game exactly one between $P_{A}$ and $P_{L}$ has a winning strategy. ", "page_idx": 12}, {"type": "text", "text": "The above result is purely existential and provides no information about the complexity of the winning strategies. Importantly, it is unclear whether winning strategies can be chosen to be measurable. The next result from Bousquet et al. (2021) addresses this concern. ", "page_idx": 13}, {"type": "text", "text": "Theorem A.3 (Theorem B.1 from Bousquet et al. (2021)). Let $\\{X_{t}\\}_{t\\ge1}$ be Polish spaces and $\\{Y_{t}\\}_{t\\ge1}$ be countable sets. Consider a Gale-Stewart game whose set $\\begin{array}{r}{\\bar{\\mathcal{W}}\\subseteq\\prod_{t\\geq1}(\\bar{X_{t}}\\times Y_{t})}\\end{array}$ of winning strategies for $P_{L}$ is finitely decidable and coanalytic. Then there is a universally measurable winning strategy. ", "page_idx": 13}, {"type": "text", "text": "The following remark shows that we can, equivalently, let the strategy of the the learner $\\mathrm{P_{L}}$ and the adversary $\\mathrm{P_{A}}$ depend only on the choices of their opponent in the previous rounds. ", "page_idx": 13}, {"type": "text", "text": "Remark A.4 (Bousquet et al. (2021)). The strategy of $P_{L}$ is defined to be a sequence of functions $y_{t}=f_{t}(x_{1},y_{1},\\ldots,x_{t-1},y_{t-1})$ of the history of the game, where $y_{1},\\ldots,y_{t-1}$ are defined similarly. Thus, we can equivalently let $y_{t}=f_{t}(x_{1},...,x_{t-1})$ . The same holds for the strategy of ${\\boldsymbol{P}}_{A}$ . ", "page_idx": 13}, {"type": "text", "text": "Partial Concept Classes. The traditional PAC learning framework studies (mainly) total concept classes, i.e., classes of functions $h:\\mathcal{X}\\to\\{0,1\\}$ that are defined on every point $x\\in\\mathscr{X}$ . The caveat with total functions is that they do not provide a direct way to express data-dependent constraints. For example, if the space $\\mathcal{X}$ is high-dimensional but the data that the leaner has to classify lie in a low-dimensional space, it is not clear how to encode this restriction through total concepts. This is very relevant to practical application of machine learning such as classification of images; the space $\\mathcal{X}$ is the set of all possible values of the pixels of the image but most of these configurations of the pixels do not even correspond to an image of an object of interest. Alon et al. (2021) proposed and studied an extension of the PAC framework that allows one to capture such assumptions using partial concept classes, i.e., sets of functions $f:\\mathcal{X}\\to\\{0,1,\\star\\}$ , where $f(x)=\\star$ means that $f$ is undefined at $x$ . In the context of classification of images, when a classifier returns $\\star$ it means that $x$ does not belong to the space of valid images. Alon et al. (2021) extend a lot of notions, such as PAC learnability and the VC dimension, from the setting of total concept classes to the setting of partial concept classes (see, e.g., Definition A.6). To show how one can use partial classes, to express data-dependent constraints, we remark that Alon et al. (2021) illustrated how the class of $d_{\\cdot}$ -dimensional linear classifiers with margin $\\gamma>0$ can be formulated as a partial class: we say that a sample $(x_{1},y_{1}),\\dotsc...,(x_{n},y_{n})\\in\\mathbb{R}^{d}\\times\\{\\bar{0},1\\}$ is $(R,\\gamma)$ -separable if all the points $x_{1},\\ldots,x_{n}$ lie in a (euclidean) ball of radius $R$ , the 0-labeled examples and the 1-labeled examples are linearly separable, and the (euclidean) distance between the 0-labeled examples and 1-labeled examples is at least $2\\gamma$ . Then, the class ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{F}_{R,\\gamma}=\\big\\{f:\\mathbb{R}^{d}\\to\\{0,1\\star\\}:(\\forall x_{1},\\ldots,x_{n})\\in\\mathrm{supp}(f):}&{}&\\\\ {(x_{1},(f(x_{1})),\\ldots,(x_{n},(f(x_{n}))\\,\\mathrm{is}\\,(R,\\gamma)-\\mathrm{separable}\\big\\},}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\operatorname{supp}(f)$ is the set of all points where $f(x)\\neq\\star$ , expresses the set of functions that satisfy these constraints. Remarkably, the VC dimension of $\\mathcal{F}$ is bounded by $\\begin{array}{r}{O\\left(\\frac{R^{2}}{\\gamma^{2}}\\right)}\\end{array}$ (Alon et al., 2021). Perhaps surprisingly, even though the PAC learnability of partial classes is characterized by the VC dimension (as it is the case with total classes), the ERM algorithm provably fails to learn partial classes. Thus, the algorithmic landscape is much richer and complicated compared to total classes. Moreover, there is no algorithmic way to extend a partial concept class to a total concept class without significantly increasing its VC dimension. For details, we refer to (Alon et al., 2021). ", "page_idx": 13}, {"type": "text", "text": "One-Inclusion Graph Algorithm. We state formally the guarantees of the one-inclusion graph algorithm (Haussler et al., 1994). ", "page_idx": 13}, {"type": "text", "text": "Theorem A.5 (One-Inclusion Graph Algorithm (Haussler et al., 1994)). For any (total) concept class $\\mathbb{H}$ whose VC dimension is bounded by $\\mathrm{d}<\\infty$ , there is an algorithm $\\mathbb{A}:(\\mathcal{X}\\times\\{0,1\\})^{*}\\times\\mathcal{X}\\overset{\\cdot}{\\rightarrow}\\{0,1\\}$ such that for any $n\\in\\mathbb N$ and any sequence $\\{(x_{1},y_{1}),\\dotsc,(x_{n},y_{n})\\}\\in(\\dot{x}\\times\\{0,1\\})^{\\bar{n}}$ that is realizable w.r.t. H, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{n!}\\sum_{\\sigma\\in\\mathrm{Sym}(n)}\\mathbb{1}\\big\\{\\mathbb{A}(x_{\\sigma(1)},y_{\\sigma(1)},\\cdot\\cdot\\cdot,x_{\\sigma(n-1)},y_{\\sigma(n-1)},x_{\\sigma(n)})\\neq y_{\\sigma(n)}\\big\\}\\leq\\frac{\\mathrm{d}}{n},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\operatorname{Sym}(n)$ denotes the symmetric group of permutations of $\\{1,\\ldots,n\\}$ . ", "page_idx": 13}, {"type": "text", "text": "In particular, Theorem A.5 implies immediately that if $(x_{1},y_{1}),\\ldots,(x_{n},y_{n})$ are i.i.d. from $\\mathrm{P}$ then (th2e0 2c1l)a ssshiofiwere ${\\tilde{h}}_{n}(\\cdot):=\\mathbb{A}(x_{1},y_{1},\\ldots,x_{n},y_{n},\\cdot)$ tihaals $\\begin{array}{r}{\\mathbb{E}[\\mathrm{er}(\\tilde{h}_{n})]\\,\\leq\\,\\frac{\\mathrm{d}}{n+1}}\\end{array}$ . We remark that Alon et al. ", "page_idx": 14}, {"type": "text", "text": "A.3 Omitted Definitions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition A.6 (VC Dimension of Partial Concept Classes (Alon et al., 2021)). For a partial concept class $\\mathcal{F}\\,\\subseteq\\,\\{0,1,\\star\\}^{\\mathcal{X}}$ , the VC dimension of $\\mathcal{F}$ is defined to be the largest number $\\mathrm{~d~}\\in\\mathbb{N}$ such that $\\exists(x_{1},\\ldots,x_{\\mathrm{d}})\\,\\in\\,\\chi^{\\mathrm{d}}$ such that $\\{(f(x_{1}),\\dots,f(x_{\\mathrm{d}}))\\,:\\,f\\,\\in\\mathcal{F}\\}=\\{0,1\\}^{d}$ . Such a sequence $(x_{1},\\ldots,x_{\\mathrm{d}})$ is said to be shattered by $\\mathcal{F}$ . If there is no bound on d we say that the VC dimension is $\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "The following definition of the star number is an adaptation of the definition in (Hanneke and Yang, 2015). ", "page_idx": 14}, {"type": "text", "text": "Definition A.7 (Star Number of Partial Concept Classes). For a partial concept class $\\mathcal{F}\\subseteq\\{0,1,\\star\\}^{\\mathcal{X}}$ , the star number of $\\mathcal{F}$ is defined to be the largest number ${\\mathfrak{s}}\\in\\mathbb{N}$ such that $\\exists(x_{1},\\ldots,x_{s})\\in\\mathcal{X}^{s}$ such that $\\exists f_{0}\\in{\\mathcal{F}}$ and $\\forall i\\in[\\mathfrak{s}]$ , $\\exists f_{i}\\in{\\mathcal{F}}:f_{i}(x_{i})=1-f_{0}(x_{i}),f_{i}(x_{j})=f_{0}(x_{j})\\neq\\star,\\forall j\\in[\\mathfrak{s}]\\setminus\\{i\\}$ }. If there is no bound on s we say that the star number is $\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "Definition A.8 (Littlestone Tree (Bousquet et al., 2021)). A Littlestone tree for $\\mathbb{H}\\subseteq\\{0,1\\}^{\\scriptscriptstyle X}$ is a complete binary tree of depth $d\\leq\\infty$ whose internal nodes are labeled by $\\mathcal{X}$ , and whose two edges connecting a node to its children are labeled by $\\{0,1\\}$ , such that every path of length at most $d$ emanating from the root is consistent with a concept $h\\in\\mathbb H$ . Formally, a Littlestone tree is a collection ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bigcup_{0\\leq\\ell<d}\\left\\{x_{\\vec{u}}:\\vec{u}\\in\\{0,1\\}^{\\ell}\\right\\}=\\left\\{x_{\\emptyset}\\right\\}\\cup\\left\\{x_{0},x_{1}\\right\\}\\cup\\left\\{x_{00},x_{01},x_{10},x_{11}\\right\\}\\cup..\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "such that for every path $\\vec{y}\\in\\{0,1\\}^{d}$ and $n\\,<\\,d,$ , there exists $h\\,\\in\\,\\mathbb{H}$ so that $h(x_{\\vec{y}_{\\le\\ell}})\\,=\\,y\\ell{+}1$ for $0\\leq\\ell\\leq n$ . We say that $\\mathbb{H}$ has an infinite Littlestone tree if there is a Littlestone tree for $\\mathbb{H}$ of depth $d=\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "Definition A.9 (Star Tree). A Star tree for $\\mathbb{H}\\subseteq\\{0,1\\}^{\\scriptscriptstyle X}$ of consists of a collection ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bigcup_{0\\leq\\ell<d}\\left\\{(x_{\\vec{u}},z_{\\vec{u}})\\in\\left(\\mathcal{X}^{\\ell+1},\\{0,1\\}^{\\ell+1}\\right):\\vec{u}\\in\\{0\\}\\times\\{0,1\\}\\times.\\,.\\,.\\times\\{0,1,.\\,.\\,,\\ell\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "such that for every level $n<d,$ and $\\vec{y}\\in\\{0\\}\\times\\{0,1\\}\\times...\\times\\{0,1,...\\,,n-1\\}$ , there exists some $h\\in\\mathbb H$ such that $h(x_{\\vec{y}_{\\le k}}^{i})=f(z_{\\vec{y}_{\\le k}}^{i},y_{k+1})$ for all $0\\leq i\\leq k$ and $0\\leq k\\leq n$ , where we denote ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\vec{y}_{\\leq k}=(y_{1},y_{2},\\dotsc,y_{k}),\\ \\ x_{\\vec{y}_{\\leq k}}=(x_{\\vec{y}_{\\leq k}}^{0},\\dotsc,x_{\\vec{y}_{\\leq k}}^{k}),\\ \\ z_{\\vec{y}_{\\leq k}}=(z_{\\vec{y}_{\\leq k}}^{0},\\dotsc,z_{\\vec{y}_{\\leq k}}^{k}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nf(z_{\\vec{y}_{\\le k}}^{i},y_{k+1})=\\left\\{\\vphantom{\\frac{1}{z_{\\vec{y}_{\\le k}}^{i}}}\\varepsilon_{\\vec{y}_{\\le k}}^{i},\\right.\\ \\left.i f y_{k+1}\\ne i\\mathrm{~.~}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We say that $\\mathbb{H}$ has an infinite star tree if it has a star tree of depth $d=\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "Definition A.10 (VCL Tree Bousquet et al. (2021)). A Vapnik-Chervonenkis-Littlestone $(V C L)$ tree for $\\mathbb{H}\\subseteq\\{0,1\\}^{x}$ of depth $d\\leq\\infty$ consists of a collection ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bigcup_{0\\leq\\ell<d}\\{x_{\\vec{u}}\\in\\mathcal{X}^{\\ell+1},\\vec{u}\\in\\{0,1\\}\\times\\{0,1\\}^{2}\\times\\ldots\\times\\{0,1\\}^{\\ell}\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "such that for every level $n<d,$ , and $\\vec{y}\\in\\{0,1\\}\\times\\{0,1\\}^{2}\\times\\{0,1\\}^{n+1}$ , there exists some $h\\in\\mathbb H$ such that $h(x_{\\vec{y}_{\\le k}}^{i})=\\dot{y}_{k+1}^{i}$ for all $0\\leq i\\leq k$ and $0\\leq k\\leq n$ , where we denote ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\vec{y}_{\\leq k}=\\big(y_{1}^{0},\\big(y_{2}^{0},y_{2}^{1}\\big),\\dots,\\big(y_{k}^{0},\\dots,y_{k}^{k-1}\\big)\\big),\\;\\;x_{\\vec{y}_{\\leq k}}=\\big(x_{\\vec{y}_{\\leq k}}^{0},\\dots,x_{\\vec{y}_{\\leq k}}^{k}\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We say that $\\mathbb{H}$ has an infinite VCL tree if it has a VCL tree of depth $d=\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "Figure 1: Arbitrarily Fast Rates Algorithm (Hanneke et al., 2022) ", "page_idx": 15}, {"type": "image", "img_path": "T0e4Nw09XX/tmp/7f28b0253a4d9d60760db4908000ac5650d280a91c7c48ed1f615bd90ad2636d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Omitted Details from Section 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C Omitted Details from Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before we prove our main result of this section, we present an omitted proof from the main body. ", "page_idx": 15}, {"type": "text", "text": "Proof. Assume that $\\mathbb{H}$ has an infinite star tree. Then, the adversary can start from the root of the tree and play $(\\vec{\\xi_{1}},\\vec{\\zeta_{1}})=(\\vec{x}_{\\mathcal{O}},\\vec{z}_{\\mathcal{O}})$ and for every strategy $\\{\\eta_{\\tau}\\}_{\\tau\\geq1}$ of the learner, $\\mathrm{P_{A}}$ can follow the path $\\vec{y}_{\\le\\tau}=(\\eta_{1},\\eta_{2},\\dots,\\eta_{\\tau})$ and play $(\\vec{x}_{\\vec{y}_{\\leq\\tau}},\\vec{z}_{\\vec{y}_{\\leq\\tau}})$ in round $\\tau+1$ . Since the tree is infinite, there is no $\\tau^{\\ast}\\in\\mathbb{N}$ such that $\\mathbb{H}_{\\vec{\\xi}_{1},\\vec{\\zeta}_{1}\\eta_{1},\\dots,\\vec{\\xi}_{\\tau^{*}},\\vec{\\zeta}_{\\tau^{*}},\\vec{\\eta}_{\\tau^{*}}}=\\overline{{\\otimes}}$ . ", "page_idx": 15}, {"type": "text", "text": "For the other direction, assume that the adversary has a winning strategy. Then, we can define the tree ", "page_idx": 15}, {"type": "equation", "text": "$$\nT=\\bigcup_{0\\leq\\ell<\\infty}\\big\\{(\\vec{x}_{\\vec{\\eta}},\\vec{z}_{\\vec{\\eta}})\\in\\big(\\mathcal{X}^{\\ell+1},\\{0,1\\}^{\\ell+1}\\big):\\vec{\\eta_{i}}\\in\\{0,1,\\ldots,i-1\\},1\\leq i\\leq\\ell\\big\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{(\\vec{x}_{\\eta_{1},\\dots,\\eta_{\\tau-1}},\\vec{z}_{\\eta_{1},\\dots,\\eta_{\\tau-1}})=(\\vec{\\xi}_{\\tau}(\\eta_{1},\\dots,\\eta_{\\tau-1}),\\vec{\\zeta}_{\\tau}(\\eta_{1},\\dots,\\eta_{\\tau-1}))}\\end{array}$ . The tree $\\tau$ is infinite by the definition of the winning strategy in $\\mathfrak{G}$ for $\\mathrm{P_{A}}$ . To prove the universal measurability of the winning strategy of $\\mathrm{P_{L}}$ , using Lemma A.3 it suffices to show that the set of winning strategies $\\mathcal{W}$ of the learner ", "page_idx": 15}, {"type": "text", "text": "is coanalytic. To this end, we consider its complement ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathcal{W}^{c}=\\left\\lbrace(\\vec{\\xi},\\vec{\\zeta},\\vec{\\eta})\\in\\bigcup_{t=1}^{\\infty}\\big(\\mathcal{X}^{t}\\times\\{0,1\\}^{t}\\times\\{0,\\dots,t-1\\big\\}\\big):\\mathbb{H}_{\\vec{\\xi}_{1},\\vec{\\zeta}_{1},\\eta_{1},\\dots,\\vec{\\xi}_{t},\\vec{\\zeta}_{t},\\eta_{t}}\\neq\\mathcal{O}\\right\\rbrace}\\\\ {\\displaystyle=\\bigcap_{t=1}^{\\infty}\\bigcup_{\\theta\\in\\Theta}\\bigcap_{s=1}^{t}\\left\\lbrace(\\vec{\\xi},\\vec{\\zeta},\\vec{\\eta}):\\begin{array}{l l}{h(\\theta,\\vec{\\xi}_{s}^{i})=\\vec{\\zeta}_{s}^{i},}&{\\mathrm{if~}\\eta_{s}\\neq i}\\\\ {h(\\theta,\\vec{\\xi}_{s}^{i})=1-\\vec{\\zeta}_{s}^{i},}&{\\mathrm{if~}\\eta_{s}=i}\\end{array},0\\leq i<s\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Notice that the set $\\left\\{\\begin{array}{l l}{h(\\theta,\\vec{\\xi_{s}^{i}})=\\vec{\\zeta_{s}^{i}},}&{\\mathrm{if}\\;\\eta_{s}\\neq i}\\\\ {h(\\theta,\\vec{\\xi_{s}^{i}})=1-\\vec{\\zeta_{s}^{i}},}&{\\mathrm{if}\\;\\eta_{s}=i}\\end{array},0\\leq i<s\\right\\}$ is Borel by the measurability assumption. Also, the intersection in the above expression are countable and the union is a projection of a Borel set. Hence, $\\mathcal{W}^{c}$ is analytic, so we have proven the claim. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C.2 Active Learning of Partial Classes with Finite Star Number: Exponential Uniform Rate ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now explain the approach to actively learn a partial concept class with finite star number at a uniform exponential rate. We first state a result from Hanneke and Yang (2015) about active learning of total concept classes with finite star dimension that we utilize in our construction. ", "page_idx": 16}, {"type": "text", "text": "Theorem C.1 (Hanneke and Yang (2015)). There exists an active learning algorithm $\\boldsymbol{\\mathcal{A}}$ for a total concept class $\\mathbb{H}$ which given a label budget $n=c\\cdot{\\mathfrak{s}}\\log(1/\\varepsilon)$ , where c is an absolute numerical constant and s is the star number of the class, and access to samples from a realizable distribution $\\mathrm{P^{*}}$ returns a classifier $\\hat{h}_{n}$ such that, with probability arbitrarily close to one, $\\mathrm{P}^{*}[\\hat{h}_{n}(x)\\neq y]\\leq\\varepsilon$ . ", "page_idx": 16}, {"type": "text", "text": "We underline that the success probability of this algorithm does not depend on the query budget. ", "page_idx": 16}, {"type": "text", "text": "Our algorithm for partial classes can be found in Figure 2. We now prove the main result in this setting that we described in Theorem 3.2. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3.2. Let $\\mathcal{F}$ be a partial concept class whose star number s is finite. Notice that since $\\mathrm{d}\\le\\mathfrak{s}$ , its VC dimension is also finite. Let $S=(X_{1},\\ldots,X_{m})$ be $m$ samples of the stream that $\\boldsymbol{\\mathcal{A}}$ has access to, where $m$ is some value to be determined. We also let ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\widehat\\mathbb{H}}=\\left\\{h:S\\rightarrow\\{0,1\\}:{\\exists}f\\in{\\mathcal{F}}{\\mathrm{~s.t.~}}h(X_{i})=f(X_{i}),\\forall i\\in[m]\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now notice that $\\widehat{\\mathbb H}$ is, by definition, a total concept class and its star number, VC dimension is also at most ${\\mathfrak{s}},d$ , respec tively. Moreover, since $\\mathrm{P^{*}}$ is realizable with respect to $\\mathcal{F}$ we have that, except for a measure-zero event, $\\widehat{\\mathbb{H}}\\neq\\varnothing$ . This can be seen as follows. Since the distribution $\\mathrm{P^{*}}$ is realizable, there exists a sequence of  functions $f_{k}\\in\\mathcal{F}$ so that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{P}^{*}[f_{k}(x)\\neq y]<\\frac{1}{2^{k}}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let us fix $m\\geq1$ . We have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\operatorname*{Pr}[\\exists s\\leq m:f_{k}(x_{s})\\neq y_{s}]\\leq m\\sum_{k=1}^{\\infty}\\operatorname{P}^{*}[f_{k}(x)\\neq y]<\\infty\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality is due to union bound. By Borel-Cantelli, with probability one, there exists for every $m\\geq1$ a hypothesis $f\\in\\mathcal F$ so that $f(\\bar{x_{s}})=y_{s}$ for all $s\\leq m$ . For the rest of the proof, we condition on this probability one event $E_{0}$ . We definite $\\widehat{\\mathrm{P}}^{*}$ to be the uniform distribution on the (labeled) sample. Our previous discussion shows that $\\widehat{\\mathrm{P}}^{\\ast}$ is realizable with respect to $\\widehat{\\mathbb H}$ . Now let us run algorithm $\\mathcal{A^{\\prime}}$ from Theorem C.1 with $\\varepsilon=1/(2m)$ and success probability $1/(2m)$ . Let $E_{1}$ be the event that the error of the output $\\tilde{h}$ of $\\mathcal{A^{\\prime}}$ is at most $1/(2m)$ . We condition on this event for the rest of the proof. Notice that in this case, the algorithm will have figured out the correct label of every $X_{i},1\\leq i\\leq m$ . Indeed, if for some $i\\in[m]$ we have that $\\tilde{h}(X_{i})\\neq Y_{i}$ , then $\\mathrm{er}(\\tilde{h})\\geq{1}/{m}$ . Notice that, by the definition of $\\widehat{\\mathbb H}$ , we have that $\\mathfrak{s}(\\widehat{\\mathbb{H}})\\leq\\mathfrak{s}(\\mathcal{F})$ . Moreover, the number of labels that $\\mathcal{A^{\\prime}}$ requests is at most $c\\cdot{\\mathfrak{s l o g}}(2m)$ , where $c$ is some absolute numerical constant. ", "page_idx": 16}, {"type": "text", "text": "Since we have $m$ labeled samples, we can use them as input to the one-inclusion graph algorithm (see Theorem A.5). This algorithm guarantees that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{er}(\\hat{h}_{m})|E_{0},E_{1}]\\leq\\frac{\\mathrm{d}(\\mathcal{F})}{m+1}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{er}(\\hat{h}_{m})]\\leq\\frac{\\mathrm{d}(\\mathcal{F})}{m+1}+\\frac{1}{m}<\\frac{\\mathrm{d}(\\mathcal{F})+1}{m}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows immediately from the definitions that $\\mathrm{d}({\\mathcal{F}})\\ \\leq\\ {\\mathfrak{s}}({\\mathcal{F}})$ . The result follows by picking $m={\\frac{e^{c\\cdot n/\\mathfrak{s}}}{2}}$ \u53e3 2 ", "page_idx": 17}, {"type": "text", "text": "Exponential Rates for Partial Classes ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. m \u2190e c\u00b72n/s, where c is the constant in Theorem C.1.   \n2. Take a sample of unlabeled data $S=\\{X_{1},\\ldots,X_{m}\\}$ .   \n3. Define $\\widehat{\\mathbb H}$ to be the total class of the concepts in $\\mathcal{F}$ that are not undefined on $S$ , i.e., ${\\widehat{\\mathbb{H}}}=\\left\\{h:S\\rightarrow\\{0,1\\}:\\exists f\\in{\\mathcal{F}}{\\mathrm{~s.t.~}}h(X_{i})=f(X_{i}),\\forall i\\in[m]\\right\\}$ .   \n4. Let $\\widehat{\\mathrm{P}}$ be the uniform distribution over the sample $\\{(X_{1},Y_{1}),\\ldots,(X_{m},Y_{m})\\}$ , where $Y_{i}$ i s the true label of $X_{i},i\\in[m]$ .   \n5. Use the active learning algorithm from Hanneke and Yang (2015) on the distribution $\\widehat{\\mathrm{P}}$ (see Theorem C.1).   \n6. The aforementioned algorithm can find the labels of these $m$ points after making at most $O(\\mathfrak{s}(\\widehat{\\mathbb{H}})\\log m)$ queries, where $\\mathfrak{s}(\\widehat{\\mathbb{H}})$ is the star number of $\\widehat{\\mathbb H}$ .   \n7. Use these $m$ labeled points as input to the one-inclusion graph algorithm (Haussler et al., 1994) and get \u02c6h (see Theorem A.5).   \n8. TVhCi sd ailmgeonristihomn  ogfu .antees linear error in $m$ , i.e., $\\begin{array}{r}{\\mathbb{E}[\\mathrm{er}(\\hat{h})]\\leq\\frac{\\mathrm{d}(\\mathcal{F})}{m+1}}\\end{array}$ , where $\\mathrm{d}({\\mathcal{F}})$ is the $\\mathcal{F}$   \n9. Return $\\hat{h}_{m}$ . ", "page_idx": 17}, {"type": "text", "text": "Star Tree Gale-Stewart Game with Finite Sequence and Estimation: Input is a labeled ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "sequence $\\{(x_{1},y_{1}),\\dots,(x_{N},y_{N})\\}$ 1. Use $\\lfloor N/2\\rfloor$ of the data to estimate $\\hat{t}_{N}$ (cf. Lemma C.5.). Let $\\hat{N}=\\lfloor N/2\\hat{t}_{N}\\rfloor$ . 2. Use the remaining $\\lfloor N/2\\rfloor$ of the data to run the star tree game (cf. Figure 5) and obtain $\\widetilde{g}_{\\hat{t}_{N}}^{i},1\\leq i\\leq\\hat{N}$ 3. Estimate for $1\\leq i\\leq\\hat{N}$ $\\mathcal{F}_{i}=\\left\\{f:\\mathcal{X}\\to\\{0,1,*\\}:\\forall(x_{1},\\ldots,x_{\\tau_{i_{N}}})\\in\\mathcal{X}^{\\tau_{i_{N}}},\\,\\Big(x_{1},f(x_{1}),\\ldots,x_{\\tau_{i_{N}}},f(x_{\\tau_{i_{N}}})\\Big)\\notin\\mathrm{image}\\left(\\widetilde{g}_{\\ell_{i_{N}}}^{i}\\right):\\mathcal{X}\\to\\{0,1,*\\}.$ . 4. Estimate the $9/16-$ majority class F $\\begin{array}{r l}&{\\frac{\\tau\\hat{N}}{m}=\\Bigg\\{f:\\mathcal{X}\\rightarrow\\{0,1,\\star\\}:\\forall\\ell\\in\\mathbb{N},\\forall(x_{1},\\ldots,x_{\\ell})\\in\\mathcal{X}^{\\ell},\\exists\\,\\mathrm{at}\\,\\mathrm{least}\\,(9/16)\\hat{N}\\mathrm{\\;classes\\;}\\mathcal{F}_{j},1\\leq j\\leq\\hat{N}\\,\\mathrm{at}\\,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\exists f\\in\\mathcal{F}_{j}\\mathrm{\\;with\\;}(\\hat{f}(x_{1}),\\ldots,\\hat{f}(x_{\\ell}))=(f(x_{1}),\\ldots,f(x_{\\ell}))}\\\\ &{:\\mathrm{eurn}\\left\\{G_{\\ell}:(\\mathcal{X}\\times\\{0,1\\})^{\\ell}\\rightarrow\\{0,1\\}\\right\\}_{\\ell\\in\\mathbb{N}}\\mathrm{\\;where}}\\\\ &{\\Upsilon_{\\ell}(x_{1},y_{1},\\ldots,x_{\\ell},y_{\\ell})=\\mathbb{1}\\left\\{\\exists f\\in\\mathcal{F}_{m}^{\\hat{N}}:(f(x_{1}),\\ldots,f(x_{\\ell}))=(y_{1},\\ldots,y_{\\ell})\\right\\}.}\\end{array}$ s.t. ) . 5. R ", "page_idx": 17}, {"type": "text", "text": "Figure 3: Majority Class Estimation Through Star Games. ", "page_idx": 17}, {"type": "text", "text": "Exponential Rates: Input is an unlabeled data stream $\\{x_{1},x_{2},\\ldots,\\}$ and query budget $n$ . 1. Set number of unlabeled examples $u_{n}=\\lfloor n/2\\rfloor+e^{c\\cdot n}$ , where $c$ is the absolute constant in Theorem C.1. 2. Use $\\lfloor n/2\\rfloor$ label queries to get the labels of the first $\\lfloor n/2\\rfloor$ unlabeled points. 3. Call the Gale-Stewart subroutine (Figure 3) with points $\\left\\{(x_{1},y_{1}),\\dots,(x_{\\lfloor n/2\\rfloor},y_{\\lfloor n/2\\rfloor})\\right\\}$ . 4. Create the partial concept class $\\begin{array}{r}{\\mathcal{F}_{G}=\\left\\{f:\\mathcal{X}\\to\\left\\{0,1,\\star\\right\\}:\\forall\\ell\\in\\mathbb{N},\\forall(x_{1},\\ldots,x_{\\ell})\\in\\mathcal{X}^{\\ell},G_{\\ell}(x_{1},f(x_{1}),\\ldots,x_{\\ell},f(x_{\\ell}))=1\\right\\}.}\\end{array}$ 5. Run Figure 2 on the partial class ${\\mathcal{F}}_{G}$ with budget $\\lfloor n/2\\rfloor$ and return its output. ", "page_idx": 18}, {"type": "text", "text": "C.3 The Online Setting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To build some intuition, we first show how to use the game between the learner and the adversary discussed in Section 3 to get an asymptotic result. We call this the online setting. Assume that there is an infinite labeled sequence ${\\cal S}=(x_{1},y_{1},x_{2},y_{2},\\dots)$ that is consistent with $\\mathbb{H}$ , i.e., for any $t<\\infty$ , there exists $h\\in\\mathbb{H}:h(x_{t})=y_{t}$ . We use $S$ to simulate the game $\\mathfrak{G}$ between $\\mathrm{P_{A}}$ and $\\mathrm{P_{L}}$ which gives rise to a partial concept class $\\mathcal{F}$ with finite star number. This approach is shown in Figure 5. ", "page_idx": 18}, {"type": "text", "text": "Star Gale-Stewart Game with Infinite Sequence 1. Let $g_{1}:\\mathcal{X}\\times\\{0,1\\}\\to\\{0,1\\}$ be the function that corresponds to the strategy of $\\mathrm{P_{L}}$ in the first round of $\\mathfrak{G}$ . 2. Initialize $\\tau_{0}\\gets1$ . 3. Let $\\widetilde{g}_{1}(x,y)=(x,1-y)$ # In the first round the learner just flips the label. 4. For every $t\\geq1$ : \u2022 If $(x_{t-\\tau_{t-1}+1},y_{t-\\tau_{t-1}+1},\\ldots,x_{t},y_{t})\\in\\mathrm{image}(\\widetilde{g}_{\\tau_{t-1}})$ , witnessed by $\\vec{c}_{\\tau_{t-1}}^{\\prime}$ : # proceed to the next round in $\\mathfrak{G}$ . \u2013 $\\tau_{t}\\gets\\tau_{t-1}+1$ . \u2013 $g_{\\tau_{t}}(\\cdot,\\cdot):=\\eta_{\\tau_{t}}(\\vec{c}_{1}^{\\prime},\\cdot\\cdot\\cdot,\\vec{c}_{\\tau_{t-1}}^{\\prime}\\cdot,\\cdot)$ , where $\\eta_{\\tau_{t}}(\\vec{c}_{1}^{\\prime},\\cdot\\cdot\\cdot,\\vec{c}_{\\tau_{t-1}}^{\\prime}\\cdot,\\cdot)$ is the strategy of $\\mathrm{P_{L}}$ in round $\\tau_{t}$ of $\\mathfrak{G}$ , given its history. \u2013 $\\begin{array}{r l}&{\\cdot\\;\\widetilde{g}_{\\tau_{t}}(x_{0},y_{0},\\ldots,x_{\\tau_{t}-1},\\bar{y_{\\tau_{t}-1}}):=(x_{0},\\overleftarrow{\\tilde{y}_{0}},\\ldots,x_{\\tau_{t}-1},\\widetilde{y}_{\\tau_{t}-1}):}\\\\ &{\\;\\;\\left\\{\\begin{array}{l l}{\\widetilde{y}_{i}=y_{i},}&{\\mathrm{~if~}g_{\\tau_{t}}(x_{0},y_{0},\\ldots,x_{\\tau_{t}-1},y_{\\tau_{t}-1})\\neq i}\\\\ {\\overleftarrow{y_{i}}=1-y_{i},}&{\\mathrm{~if~}g_{\\tau_{t}}(x_{0},y_{0},\\ldots,x_{\\tau_{t}-1},y_{\\tau_{t}-1})=i}\\end{array}\\right.,0\\leq i<\\tau_{t}\\right\\}}\\end{array}$ \u2022 Else: \u03c4t \u2190\u03c4t\u22121. ", "page_idx": 18}, {"type": "text", "text": "We first show as $t~\\rightarrow~\\infty$ the game converges, i.e., there are no more realizable $\\tau_{t^{*}}$ -tuples $(x_{1},y_{1},\\dots,x_{\\tau_{t^{*}}},y_{\\tau_{t^{*}}})$ that belong to the image of $\\widetilde{g}_{\\tau_{t^{*}}}$ , for some finite number $t^{*}$ . To simplify the notation, we define $\\widetilde g_{t^{*}}=\\widetilde g_{\\tau_{t^{*}}}$ . Recall that this fu  nction is defined in Figure 5. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.2. If H does not have an infinite star tree, for any sequence $x_{1},y_{1},x_{2},y_{2},\\dotsc$ , that is consistent with $\\mathbb{H}$ , there exists some finite number $t^{*}\\in\\mathbb{N}$ , such that for all $t>t^{*}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(x_{t-\\tau_{t-1}+1},y_{t-\\tau_{t-1}+1}\\dots,x_{t},\\dots,y_{t})\\notin\\mathrm{image}(\\widetilde{g}_{t-1}),\\tau_{t}=\\tau_{t-1},\\widetilde{g}_{t}=\\widetilde{g}_{t-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Suppose that $\\left(x_{t-\\tau_{t-1}+1},\\ldots,x_{t},y_{t-\\tau_{t-1}+1},\\ldots,y_{t}\\right)\\ \\in\\ \\mathrm{image}(\\widetilde{g}_{t-1})$ happens an infinite sequence of times $t_{1},t_{2},\\ldots.$ Since $\\eta_{\\tau_{t}}$ is a winning strategy for the lear ner in $\\mathfrak{G}$ , we have that for some finite $t^{*}$ it holds that $\\mathbb{H}_{\\vec{\\xi}_{1},\\vec{\\zeta}_{1},\\eta_{1},\\dots,\\vec{\\xi}_{t^{*}},\\vec{\\zeta_{t^{*}}},\\eta_{t^{*}}}=\\mathcal{O}$ (see Equation (1)), where $\\vec{\\xi_{i}},\\vec{\\zeta_{i}},\\eta_{i}$ are defined in Figure 5. Hence, we have arrived in a contradiction. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Similarly as in (Bousquet et al., 2021), we remark the following about the measurability of the functions. ", "page_idx": 19}, {"type": "text", "text": "Remark C.3 (Adapted from Bousquet et al. (2021)). The strategy $\\tau_{t}$ depends in a universally measurable way on $x_{\\leq t},y_{\\leq t}$ . Moreover, the functions $\\widetilde{g}_{t}$ are universally measurable with respect to $x_{\\leq t},y_{\\leq t}$ and the arguments of their input. To be more  p recise, for all $t\\geq0$ , there exist the following universally measurable functions ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\cap_{t}:(\\mathcal{X}\\times\\{0,1\\})^{t}\\to\\{1,\\dots,t+1\\},\\quad\\tilde{G}_{t}:(\\mathcal{X}\\times\\{0,1\\})^{t}\\times\\left(\\bigcup_{s<t}(\\mathcal{X}\\times\\{0,1\\})^{s}\\right)\\to(\\mathcal{X}\\times\\{0,1\\})^{t}\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{\\pi}}_{t}=T_{t}(x_{1},y_{1},\\dots,x_{t},y_{t}),\\quad\\tilde{g}_{t}(w_{1},z_{1},\\dots,w_{\\tau_{t}},z_{\\tau_{t}})=\\widetilde{G}_{t}(x_{1},y_{1},\\dots,x_{t},y_{t},w_{1},z_{1},\\dots,w_{\\tau_{t}},z_{\\tau_{t}})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.4 From the Online to the Statistical Setting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section we show how to utilize our results in the online setting to design an algorithm that achieves exponential rates in the statistical setting. Our approach is similar in spirit as in Bousquet et al. (2021); Hanneke et al. (2022). In particular, we adapt the approach of Hanneke et al. (2022) from the VCL game to the star tree game. The key steps are outlined in Figure 3. ", "page_idx": 19}, {"type": "text", "text": "Let $\\mathrm{P}$ be a realizable distribution with respect to $\\mathbb{H}$ and $X_{1},Y_{1},X_{2},Y_{2},\\dots$ , be an i.i.d. sequence from P. For the remaining of the section, we assume that $\\mathbb{H}$ does not have an infinite star tree. Let us now set up the notation that we use in our proof. We denote ", "page_idx": 19}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u03c4 $\\begin{array}{r}{\\mathfrak{f}:=T_{t}(X_{1},Y_{1},\\dots,X_{t},Y_{t}),\\quad\\tilde{g}_{t}(w_{1},z_{1},\\dots,w_{t_{r}},z_{t_{r}}):=\\tilde{G}_{t}(X_{1},\\dots,X_{t},Y_{1},\\dots,Y_{t},w_{1},z_{1},\\dots,w_{t_{r}},z_{t_{r}}).}\\end{array}$ t\u03c4 , zt\u03c4 ) where these functions are defined in Remark C.3. For any integer $k\\geq1$ , we define a \u201cflipping\u201d function $\\widetilde{g}_{k}:(\\mathcal{X}\\times\\{0,1\\})^{k}\\to(\\mathcal{X}\\times\\{0,1\\})^{k}$ to be a function that filps one of the labels of its input from 0 t o  1 or vice versa. We also define the probability of error of this function as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{per}(\\widetilde{g}_{k})=\\mathrm{per}^{k}(\\widetilde{g}_{k})=\\mathrm{P}^{\\otimes\\,k}\\left((x_{1},y_{1},\\ldots,x_{k},y_{k}):(x_{1},y_{1},\\ldots,x_{k},y_{k})\\in\\mathrm{image}(\\widetilde{g}_{k})\\right)\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "i.e., the probability that a $k$ -tuple drawn i.i.d. from $\\mathrm{P}$ belongs to the image of $\\!\\!\\!\\!\\!\\!\\widetilde{g}_{k}$ . When there is no confusion about the domain of $\\!\\!\\!\\!\\widetilde{g}_{k}$ we will omit its dependence on $k$ . Our first  o bservation is that we get an algorithm that is consistent in the statistical setting. Our proof is an adaptation of Lemma 5.7 in Bousquet et al. (2021). ", "page_idx": 19}, {"type": "text", "text": "Lemma C.4. It holds that $\\mathrm{P}[\\mathrm{per}(\\widetilde{g}_{t})>0]\\rightarrow0$ as $t\\to\\infty$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. We know that $\\mathrm{P}$ is realizable so we can pick a sequence of hypotheses $h_{k}\\,\\in\\,\\mathbb{H}$ such that $\\mathrm{er}(\\dot{h_{k}})\\leq2^{-k},k\\in\\mathbb{N}$ . Taking a union bound over all $t\\geq1$ , we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{k\\in\\mathbb{N}}{\\mathrm{P}}[h_{k}(X_{s})\\neq Y_{s},{\\mathrm{for~some~}}s\\leq s]\\leq t\\sum_{k\\in\\mathbb{N}}{\\mathrm{er}}(h_{k})<\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, by the Borel-Cantelli lemma, it holds that $h(X_{s})\\;=\\;Y_{s},\\forall s\\;\\leq\\;t$ for some $h\\ \\in\\ \\mathbb{H}$ (with probability one). Let ", "page_idx": 19}, {"type": "equation", "text": "$$\nT=\\operatorname*{sup}\\left\\{s\\ge1:(X_{s-\\tau_{s-1}+1},Y_{s-\\tau_{s-1}+1},\\ldots,X_{s},Y_{s})\\in\\mathrm{image}\\left(\\widetilde{g}_{s-1}\\right)\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we know from Lemma C.2 that $T$ is finite with probability one. Recall the law of large numbers for $m$ -dependent sequences: if $Z_{1},Z_{2}\\ldots$ , is an i.i.d. sequence of random variables, then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\to\\infty}\\frac{1}{n}\\sum_{i=1}^{n}f(Z_{i+1},\\dots,Z_{i+m})=\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname*{lim}_{n\\to\\infty}\\frac{m}{n}\\sum_{j=0}^{\\lfloor n/m\\rfloor}f(Z_{m j+1+i},\\dots,Z_{m(j+1)+i})+o(1)=\\mathbb{E}[f(Z_{m j+1},\\dots,Z_{m(n+m)+i})]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{P}[\\mathrm{per}^{\\tau_{t}}(\\widetilde{g}_{t})=0]=\\mathrm{P}\\left[\\displaystyle\\operatorname*{lim}_{S\\rightarrow\\infty}\\frac{1}{S}\\sum_{s=t+1}^{t+S}\\mathbb{1}_{(X_{s},Y_{s},\\ldots,X_{s+\\tau_{t}-1},Y_{s+\\tau_{t}-1})\\in\\mathrm{image}(\\widetilde{g}_{t})}=0\\right]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathrm{P}\\left[\\displaystyle\\operatorname*{lim}_{S\\rightarrow\\infty}\\frac{1}{S}\\sum_{s=t+1}^{t+S}\\mathbb{1}_{(X_{s},Y_{s},\\ldots,X_{s+\\tau_{t}-1},Y_{s+\\tau_{t}-1})\\in\\mathrm{image}(\\widetilde{g}_{t})}=0,T\\leq t\\right]}\\\\ &{\\qquad\\qquad=\\mathrm{P}[T\\leq t].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We know that $T$ is finite with probability one, so we have that $\\operatorname{P}[\\mathrm{per}^{\\tau_{t}}(\\widetilde{g}_{t})>0]\\le\\operatorname{P}[T>t]\\to0$ , as t \u2192\u221e. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.4 guarantees that for some $t^{*}\\in\\mathbb{N}$ , the Gale-Stewart game played on a set of $t^{*}$ points will have converged with probability at least $7/8$ , i.e., $\\operatorname*{Pr}\\left[\\mathrm{per}(\\tilde{g}_{t^{*}})\\stackrel{}{>}0\\right]\\stackrel{}{\\leq}1/8$ . This number $t^{\\star}$ depends on the distribution and is unknown to the learner. We first show how the learner can estimate a similar $\\hat{t}_{n}$ from the data. Our approach is an adaptation of Lemma 5.10 in Bousquet et al. (2021), but we present the proof for completeness. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.5 (Adaptation of Lemma 5.10 in (Bousquet et al., 2021)). For any $N\\in\\mathbb{N}$ , there exists a universally measurable $\\hat{t}_{N}=\\hat{t}_{N}(X_{1},Y_{1},...,X_{\\lfloor N/2\\rfloor},Y_{\\lfloor N/2\\rfloor})$ whose definition does not depend on $\\mathrm{P}$ so that the following holds. Set the critical time $t^{*}\\in\\tilde{\\mathbb{N}}$ be such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\mathrm{per}(\\widetilde{g}_{t^{*}})>0\\right]\\leq1/8\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the probability is over the training set of the algorithm. There exist $C,c>0$ that depend on $\\mathrm{P},t^{\\star}$ but not $N$ so that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[\\hat{t}_{N}\\in T^{\\star}]\\geq1-C e^{-c N}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the probability is over the training of the estimator $\\hat{t}_{N}$ and $T^{*}$ is the set ", "page_idx": 20}, {"type": "equation", "text": "$$\nT^{*}=\\{1\\leq t\\leq t^{\\star}:\\mathrm{Pr}\\left[\\mathrm{per}(\\widetilde{g}_{t})>0\\right]\\leq3/8\\}\\ .\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We split this labeled set into two parts. The idea is to use the first one to run the Gale-Stewart game and the other one to estimate whether it has converged or not. For each $\\begin{array}{r}{1\\leq t\\leq\\lfloor\\frac{N}{4}\\rfloor}\\end{array}$ and $1\\leq i\\leq\\left\\lfloor{\\frac{N}{4t}}\\right\\rfloor$ , we let ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\tau_{t}^{i}:=T_{t}(X_{(i-1)t+1},Y_{(i-1)t+1},\\dots,X_{i t},Y_{i t})}\\\\ {\\widetilde{g}_{t}^{i}(w_{1},z_{1},\\dots,w_{\\tau_{t}^{i}},z_{\\tau_{t}^{i}}):=\\widetilde{G}_{t}(X_{(i-1)t+1},Y_{(i-1)t+1},\\dots,X_{i t},Y_{i t},w_{1},z_{1},\\dots,w_{\\tau_{t}^{i}},z_{\\tau_{t}^{i}})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "be the estimated quantities when we run Figure 5 on the $i$ -th batch of the data. Next, for each $t$ we estimate $\\mathrm{Pr}[\\mathrm{per}(\\widetilde{g}_{t})>0]$ by the fraction of the $\\widetilde{g}_{t}^{i}$ for which there is a tuple on the second quarter of the data that belo ngs to the image of g ti. For ev e ry fixed t, the data that the functions g ti i\u2264\u230aN/2t\u230b are trained on are independent of each other and of the second half of the training set. This means that we can view every $\\left\\{\\widetilde{g}_{t}^{i}\\right\\}_{i\\leq\\lfloor N/2t\\rfloor}$ as an independent draw of the distribution of $\\widetilde g_{t}$ . To estimate the error of the algorithm we use the second quarter of the training set. We let ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{e}_{t}=\\frac{1}{\\lfloor N/4t\\rfloor}\\sum_{i=1}^{\\lfloor N/4t\\rfloor}\\mathbb{1}_{\\left\\{\\exists s:\\lfloor N/4\\rfloor+1\\le s\\le\\lfloor N/2\\rfloor-\\tau_{t}^{i},\\left(X_{s+1},Y_{s+1},\\ldots,X_{s+\\tau_{t}},Y_{s+\\tau_{t}}\\right)\\in\\mathrm{image}(\\tilde{g}_{t}^{i})\\right\\}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{e}_{t}\\le e_{t}=\\frac{1}{\\lfloor N/4t\\rfloor}\\sum_{i=1}^{\\lfloor N/4t\\rfloor}\\mathbb{1}_{\\{\\mathrm{per}(\\widetilde{g}_{t})>0\\}}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with probability one. We define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{t}_{N}=\\operatorname*{inf}\\{t\\le\\lfloor N/4\\rfloor:\\hat{e}_{t}<1/4\\}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we assume that inf $\\emptyset=\\infty$ . ", "page_idx": 20}, {"type": "text", "text": "We now want to bound the probability that $\\hat{t}_{N}>t^{\\star}$ . Using Hoeffding\u2019s inequality we get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{v}_{\\mathbf{r}}\\left[\\hat{t}_{N}>t^{\\star}\\right]\\le\\operatorname*{Pr}\\left[\\hat{e}_{t^{\\star}}\\ge\\frac14\\right]\\le\\operatorname*{Pr}\\left[e_{t^{\\star}}\\ge\\frac14\\right]=\\operatorname*{Pr}\\left[e_{t^{\\star}}-\\frac18\\ge\\frac18\\right]=\\operatorname*{Pr}\\left[e_{t^{\\star}}-\\mathbb{E}[e_{t^{\\star}}]\\ge\\frac18\\right]\\le e^{\\operatorname*{Pr}\\left[e_{t^{\\star}}\\right]}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This implies that $\\hat{t}_{N}\\leq t^{\\star}$ except for an event with exponentially small probability. Moreover, for all $1\\leq t\\leq t^{\\star}$ that $\\operatorname*{Pr}\\left[\\mathrm{per}(\\widetilde{g}_{t})>\\bar{0}\\right]>\\frac{3}{8}$ , there is some $\\varepsilon>0$ such that $\\begin{array}{r}{\\mathrm{Pr}\\left[\\mathrm{per}(\\widetilde{g}_{t})\\stackrel{\\cdot}{>}\\varepsilon\\right]>\\frac{1}{4}+\\frac{1}{16}}\\end{array}$ (this holds by continuity). Now fix some $1\\leq t\\leq t^{\\star}$ such that $\\operatorname*{Pr}\\left[\\mathrm{per}(\\widetilde{g}_{t})>0\\right]>\\frac{3}{8}$ (if it exists). Then, using Hoeffding\u2019s inequality again we get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\frac{1}{\\lfloor N/4t\\rfloor}\\sum_{i=1}^{\\lfloor N/4t\\rfloor}\\mathbb{1}_{\\left\\{\\mathrm{per}(\\tilde{g}_{t}^{i})>\\varepsilon\\right\\}}<\\frac{1}{4}\\right]\\leq e^{-\\lfloor N/4t^{\\star}\\rfloor/128}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Whenever $g$ is a function such that $\\mathrm{per}(g)>\\varepsilon$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{v}_{\\mathrm{T}}\\left[\\{\\exists s:\\lfloor N/4\\rfloor+1\\leq s\\leq\\lfloor N/2\\rfloor-\\tau,(X_{s+1},Y_{s+1},\\ldots,X_{s+\\tau},Y_{s+\\tau})\\in\\mathrm{image}(g)\\}\\right]\\geq1-(1-\\varepsilon)^{\\mathrm{U}}\\mathrm{T}^{2}\\mathrm{T}^{2}\\mathrm{T}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since there are $\\lfloor(N-4)/4\\tau\\rfloor$ disjoint intervals of length $\\tau$ . As we mentioned before, $\\{\\widetilde{g}_{t}^{i}\\}_{i\\leq\\lfloor N/4t\\rfloor}$ are independent of $(X_{s},Y_{s})_{s>\\lfloor n/4\\rfloor}$ . Thus, applying a union bound we get that the probability that all $\\widetilde{g}_{t}^{i}$ that have $\\mathrm{per}^{\\tau_{t}^{i}}(\\widetilde{g}_{t}^{i})>\\varepsilon$ make at least one error on the second half of the training set is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}\\left[\\mathbb{1}_{\\left\\{\\mathrm{per}^{\\tau_{t}^{i}}(\\tilde{g}_{t}^{i})>\\varepsilon\\right\\}}\\leq\\mathbb{1}_{\\left\\{\\exists s:\\lfloor N/4\\rfloor+1\\leq s\\leq\\lfloor N/2\\rfloor-\\tau_{t}^{i},(X_{s+1},Y_{s+1},\\dots,X_{s+\\tau_{t}^{i}},Y_{s+\\tau_{t}^{i}})\\in\\mathrm{image}(\\tilde{g}_{t}^{i})\\right\\}}\\right]}\\\\ &{\\geq1-\\left\\lfloor\\frac{N}{4t}\\right\\rfloor(1-\\varepsilon)^{\\lfloor(N-4)/4t^{*}\\rfloor}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality holds since $\\tau_{t}^{i}\\leq t^{*}$ . Thus, we get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[\\hat{t}_{N}=t]\\le\\mathrm{Pr}\\left[\\hat{e}_{t}<\\frac{1}{4}\\right]\\le\\left\\lfloor\\frac{N}{4}\\right\\rfloor(1-\\varepsilon)^{\\lfloor(N-4)/4t^{*}\\rfloor}+e^{-\\left\\lfloor\\frac{N}{4t^{*}}\\right\\rfloor/128}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the previous estimates and applying a union bound, we get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[\\hat{t}_{N}\\notin T^{\\star}]\\le e^{-\\lfloor N/4t^{\\star}\\rfloor/32}+t^{\\star}\\left\\lfloor\\frac{N}{4}\\right\\rfloor(1-\\varepsilon)^{\\lfloor(N-4)/4t^{\\star}\\rfloor}+t^{\\star}e^{-\\lfloor N/4t^{\\star}\\rfloor/128}\\le C e^{-c n}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some constants $C,c>0$ . ", "page_idx": 21}, {"type": "text", "text": "Essentially, Lemma C.5 tells us that we can estimate a batch size $\\hat{t}_{n}$ , using only information from the data, so that, with probability $1-C e^{-c n}$ , the star tree game will have converged when we run it on $\\hat{t}$ labeled points. Following the approach of Hanneke et al. (2022), the idea is to run the game multiple times, then create the partial concept classes that are induced by each execution, and, finally, aggregate them into a majority class (see Figure 3). We show that if most of the games have converged, then the majority class has bounded star number. This is made formal in Lemma C.6. We remark that Hanneke et al. (2022) showed a similar result regarding the VC dimension of the majority class. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.6. The majority class $\\mathcal{F}_{m}^{\\hat{N}}$ defined in Figure 3 has star number that is bounded by some distribution-dependent constant $\\hat{\\pmb{\\mathfrak{s}}}$ , with probability at least $1-e^{-c\\hat{N}}$ , for some absolute constant $c>0$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. We know that there exists some $\\mathfrak{s}^{*}$ such that, with probability at least $9/10$ , the star of any partial concept class ${\\mathcal{F}}_{i}$ is at most $\\mathfrak{s}^{*}$ . We let $X_{i}=\\mathbb{1}_{\\left\\{{\\mathrm{star~number~of~}\\mathcal{F}}_{i}\\ >\\ \\mathfrak{s}^{\\ast}\\right\\}}$ . Notice the all the $X_{i}$ \u2019s are i.i.d. Bernoulli random variables with $p\\leq1/10$ , since the data that induce the classes ${\\mathcal{F}}_{i}$ are i.i.d.. Thus, we can use Hoeffding\u2019s inequality to bound the probability that more than $2/10$ of them have star number greater than $\\mathfrak{s}^{*}$ as follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{i=1}^{\\hat{N}}X_{i}\\geq(2/10)\\hat{N}\\right]=\\operatorname*{Pr}\\left[\\sum_{i=1}^{\\hat{N}}X_{i}-(1/10)\\hat{N}\\geq(15/72)\\hat{N}\\right]\\leq e^{-\\hat{N}/50}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We let ${\\mathcal{E}}_{1}$ be the complement of the event above and we condition on it for the rest of the proof. So, we know that at least $8/10\\hat{N}$ of the partial concept classes ${\\mathcal{F}}_{i}$ have star number bounded by $\\pmb{\\mathfrak{s}}^{*}$ . We will bound the size $\\ell$ of the star set of $\\mathcal{F}_{m}^{\\hat{N}}$ . For any star sequence $S=(x_{1},y_{1},\\dots,x_{\\ell},y_{\\ell})\\in(\\mathcal{X}\\times\\{0,1\\})^{\\ell}$ of $\\mathcal{F}_{m}^{\\hat{N}}$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{\\ell}\\sum_{j=1}^{\\ell}\\mathbb{1}\\left\\{\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\mathbb{1}\\left\\{\\exists f\\in\\mathcal{F}_{i}:f(x_{j})=1-y_{j},f(x_{p})=y_{p},\\forall p\\in[\\ell]\\setminus\\{j\\}\\right\\}>9/16\\right\\}=1\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using Markov\u2019s inequality, we get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{\\ell}\\sum_{j=1}^{\\ell}\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\mathbb{1}\\left\\{\\exists f\\in\\mathcal{F}_{i}:f(x_{j})=1-y_{j},f(x_{p})=y_{p},\\forall p\\in[\\ell]\\setminus\\{j\\}\\right\\}>\\frac{9}{16}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Swapping the summation gives us ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\frac{1}{\\ell}\\sum_{j=1}^{\\ell}\\mathbb{1}\\left\\{\\exists f\\in\\mathcal{F}_{i}:f(x_{j})=1-y_{j},f(x_{p})=y_{p},\\forall p\\in[\\ell]\\setminus\\{j\\}\\right\\}>\\frac{9}{16}\\iff}\\\\ &{\\displaystyle\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\frac{1}{\\ell}\\sum_{j=1}^{\\ell}\\mathbb{1}\\left\\{\\nexists f\\in\\mathcal{F}_{i}:f(x_{j})=1-y_{j},f(x_{p})=y_{p},\\forall p\\in[\\ell]\\setminus\\{j\\}\\right\\}\\leq\\frac{7}{16}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using Markov\u2019s inequality again, we get that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\mathbb{1}\\left\\{\\frac{1}{\\ell}\\sum_{j=1}^{\\ell}\\mathbb{1}\\left\\{\\nexists{f}\\in\\mathcal{F}_{i}:f(x_{j})=1-y_{j},f(x_{p})=y_{p},\\forall p\\in[\\ell]\\setminus\\{j\\}\\right\\}>9/16\\right\\}\\leq}\\\\ &{\\displaystyle\\frac{16}{9}\\cdot\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\frac{1}{\\ell}\\sum_{j=1}^{\\ell}\\mathbb{1}\\left\\{\\nexists{f}\\in\\mathcal{F}_{i}:f(x_{j})=1-y_{j},f(x_{p})=y_{p},\\forall p\\in[\\ell]\\setminus\\{j\\}\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\hat{\\mathrm{V}}}\\displaystyle\\sum_{i=1}^{\\hat{N}}\\mathbb{1}\\left\\{\\frac{1}{\\ell}\\sum_{j=1}^{\\ell}\\mathbb{1}\\left\\{\\vec{\\beta}f\\in\\mathcal{F}_{i}:f(x_{j})=1-y_{j},f(x_{p})=y_{p},\\forall p\\in[\\ell]\\setminus\\{j\\}\\right\\}>9/16\\right\\}<7/9\\iff}\\\\ &{\\frac{1}{\\hat{\\mathrm{V}}}\\displaystyle\\sum_{i=1}^{\\hat{N}}\\mathbb{1}\\left\\{\\frac{1}{\\ell}\\displaystyle\\sum_{j=1}^{\\ell}\\mathbb{1}\\left\\{\\exists f\\in\\mathcal{F}_{i}:f(x_{j})=1-y_{j},f(x_{p})=y_{p},\\forall p\\in[\\ell]\\setminus\\{j\\}\\right\\}\\geq7/16\\right\\}\\geq2/9\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, we have shown that at least $2/9$ of the partial concept classes witness at least $7/16$ of the star patterns. By the definition of the star number, this means that all these classes have star number at least $7/16\\cdot m-1$ . This is because we can drop all the points from $S$ that correspond to star patterns that the class does not realize and the remaining set of labeled points is a star set. Since at least one these classes\u2019 star number is at most $\\mathfrak{s}^{*}$ we have that $7/16\\cdot m-1\\leq{\\mathfrak{s}}^{*}$ , which means that $m\\leq16/7(\\mathfrak{s}^{*}+1)$ . Thus, we have shown that the star number of $\\mathcal{F}_{m}^{\\hat{N}}$ is at most $16/7(\\mathfrak{s}^{*}+1)=\\hat{\\mathfrak{s}}$ , with probability at least $1-e^{-c\\hat{N}}$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Similarly as in Hanneke et al. (2022), we define a sequence of universally measurable functions $\\left\\{G_{\\ell}:(\\bar{\\mathcal{X}}\\times\\{0,1\\})^{\\ell}\\rightarrow\\{0,1\\}\\right\\}_{\\ell\\in\\mathbb{N}}$ where ", "page_idx": 22}, {"type": "equation", "text": "$$\nG_{\\ell}(x_{1},y_{1},\\ldots,x_{\\ell},y_{\\ell})=\\mathbb{1}\\left\\{\\exists f\\in{\\mathcal{F}}_{m}^{\\hat{N}}:(f(x_{1}),\\ldots,f(x_{\\ell}))=(y_{1},\\ldots,y_{\\ell})\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "An equivalent interpretation of the previous lemma is that the star number of the partial class on which $\\{G_{\\ell}\\}_{\\ell\\in\\mathbb{N}}$ returns 1 has a distribution-dependent bound, with high probability. Lemma C.7 is an important component in the derivation of our result and it shows that $\\{G_{\\ell}\\}_{\\ell\\in\\mathbb{N}}$ return 1 on all finite subsets of the correctly labeled data sequence, with high probability. This result follows similarly as Lemma 7 in Hanneke et al. (2022). ", "page_idx": 22}, {"type": "text", "text": "Lemma C.7. The $\\{G_{\\ell}\\}_{\\ell\\in\\mathbb{N}}$ as defined above are universally measurable functions, and with probability at least $1-C e^{-c N},C,c>0,$ , the following hold: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The star number of the class $\\mathcal{F}_{G}=\\left\\{f:\\mathcal{X}\\longrightarrow\\left\\{0,1,\\star\\right\\}:\\forall\\ell\\in\\mathbb{N},\\forall(x_{1},\\ldots,x_{\\ell})\\in\\mathcal{X}^{\\ell}:G_{\\ell}(x_{1},f(x_{1}),\\ldots,x_{\\ell},f(x_{\\ell}))=1\\right\\}$ has a finite distribution-dependent upper bound \u02c6s.   \n\u2022 $\\forall\\ell\\in\\mathbb{N},$ , for $(x_{1},y_{1},\\dotsc,x_{\\ell},y_{\\ell})\\sim\\mathrm{P}^{\\ell}$ , $G_{\\ell}(x_{1},y_{1},\\dots,x_{\\ell},y_{\\ell})=1$ with conditional probability one (given $G_{\\ell}$ ). ", "page_idx": 22}, {"type": "text", "text": "Proof. We condition on the event ${\\mathcal{E}}_{0}$ described in Lemma C.6. Notice that, by definition, a labeled sequence $S=\\left(x_{1},y_{1}\\ldots,x_{\\ell},y_{\\ell}\\right)\\in\\left({\\mathcal{X}}\\times\\{0,1\\}\\right)^{\\ell}$ is a star set of the class ${\\mathcal{F}}_{G}$ if and only if $S$ is a star set of $\\mathcal{F}_{m}^{\\hat{N}}$ . Hence, the bound on the star number follows immediately from Lemma C.6. We also condition on the event ${\\mathcal{E}}_{1}$ described in Lemma C.5. We now bound the probability that at least $(13/32)\\hat{N}$ of the functions $\\widetilde{g}_{\\hat{t}_{N}}^{i}$ have p $\\mathrm{er}\\left(\\widetilde{g}_{\\hat{t}_{N}}^{i}\\right)>0$ . For any $t\\in T^{*}$ , using Hoeffding\u2019s inequality we get that ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{v}\\left[\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\mathbb{1}\\Big\\{\\mathrm{per}\\left(\\tilde{g}_{\\hat{t}_{N}}^{i}\\right)>0\\Big\\}>\\frac{13}{32}\\right]=\\mathrm{Pr}\\left[\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\mathbb{1}\\Big\\{\\mathrm{per}\\left(\\tilde{g}_{\\hat{t}_{N}}^{i}\\right)>0\\Big\\}-\\frac{3}{8}>\\frac{1}{32}\\right]\\leq\\exp\\left(-\\hat{N}/512\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since we know that $\\hat{t}_{n}\\leq t^{*}$ , taking a union bound over all $1\\leq t\\leq t^{*}$ we get that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[\\displaystyle\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\mathbb{1}\\left\\{\\mathrm{per}\\left(\\tilde{g}_{\\hat{t}_{N}}^{i}\\right)>0\\right\\}>\\displaystyle\\sum\\frac{13}{32},\\hat{t}_{n}\\in T^{*}\\right]\\leq\\displaystyle\\sum_{t\\in T^{*}}\\mathrm{Pr}\\left[\\displaystyle\\frac{1}{\\hat{N}}\\sum_{i=1}^{\\hat{N}}\\mathbb{1}\\left\\{\\mathrm{per}\\left(\\tilde{g}_{\\hat{t}_{N}}^{i}\\right)>0\\right\\}>\\frac{13}{32}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq t^{*}\\exp\\left(-\\hat{N}/512\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, except for an event with exponentially small probability, at least $19/32$ of the functions $\\widetilde{g}_{\\hat{t}_{N}}^{i}$ have $\\mathrm{per}\\left(\\widetilde{g}_{\\hat{t}_{N}}^{i}\\right)$ , with probability one. Notice that, by definition, if the partial class ${\\mathcal{F}}_{i}$ that corresponds to such a function cannot produce a labeling $\\vec{y}_{\\ell}\\in\\{0,1\\}^{\\ell}$ of some tuple $\\vec{x}_{\\ell}\\in\\mathcal{X}^{\\ell}$ where $\\vec{x}_{\\ell}\\sim\\mathrm{P}_{X}^{\\ell}$ , we can infer that this $\\vec{y}_{\\ell}$ is not the correct labeling, and this inference will be valid with probability one over the draw of $\\vec{x}_{\\ell}$ . Thus, with probability one, if the $9/16$ -majority cannot produce some labeling we have that at least one such partial class Fj that corresponds to a correct g tj\u02c6N cannot produce this labeling, so it is not the correct one. As a result, with probability one, if $\\mathcal{F}_{m}^{\\hat{N}}$ cannot produce $\\vec{y_{\\ell}}$ for $\\vec{x}_{\\ell}$ we know that this is not the correct labeling. The measurability of $\\{G_{\\ell}\\}_{\\ell\\in\\mathbb{N}}$ follows by the measurability of all the g ti\u02c6N i\u2208N\u02c6 . The proof of the lemma follows by noticing that the probability of all the events we have conditioned on can be bounded by $1-C^{\\prime}e^{-c^{\\prime}\\hat{N}}$ and that $\\hat{N}=\\lfloor N/2\\hat{t}_{N}\\rfloor\\geq\\lfloor N/2t^{*}\\rfloor$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "We are now ready to prove the result about active learning at exponential universal rates. Our approach is summarized in Figure 4. ", "page_idx": 23}, {"type": "text", "text": "Theorem C.8. Assume that $\\mathbb{H}\\subseteq\\{0,1\\}^{x}$ does not have an infinite star tree. Then, $\\mathbb{H}$ admits an active learning algorithm that achieves exponential rates. ", "page_idx": 23}, {"type": "text", "text": "Proof. First, notice that the Gale-Stewart subroutine Figure 3 uses $\\lfloor n/2\\rfloor$ points and the exponential rates algorithm for partial classes uses at most $\\lfloor e^{c\\cdot n}\\rfloor$ points. This is because we define a uniform distribution over these points during its execution. To prove the learning rate guarantees, we first condition on the event $\\mathcal{E}_{0}$ described in Lemma C.7. Then, we have some partial concept class ${\\mathcal{F}}_{G}$ whose star number is finite by some distribution-dependent number $\\hat{\\mathfrak{s}}$ and has the property that $\\forall\\ell\\in\\mathbb{N}$ , for $(x_{1},y_{1},\\dotsc,x_{\\ell},y_{\\ell}^{\\prime})\\sim\\mathrm{P}^{\\ell}$ , $f(x_{1})=y_{1},\\dotsc,f(x_{\\ell})=y_{\\ell}$ for some $f\\in\\mathcal{F}_{G}$ . Thus, the conditions of Theorem C.1 are satisfied and the conditional error rate of the output of our algorithm is $\\mathbb{E}[\\mathrm{er}(\\hat{h}_{n})|\\mathcal{E}_{0}]\\leq c_{1}\\hat{\\mathrm{d}}e^{-c_{2}n/\\hat{\\mathfrak{s}}}$ . Since $\\mathcal{E}_{0}$ happens with probability at least $1-\\bar{C}e^{-\\tilde{c}n}$ , we see that the unconditional error of the output is $\\mathbb{E}[\\mathrm{er}(\\hat{h}_{n})]\\,\\le\\,c_{1}\\hat{\\mathrm{d}}e^{-C_{2}n/\\hat{\\mathfrak{s}}}+\\tilde{C}e^{-\\tilde{c}n}\\,\\le\\,C e^{-c n}$ , for some distribution-dependent constants $c,C>0$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.5 Sublinear Rates Lower Bound ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem C.9. Assume that $\\mathbb{H}\\subseteq\\{0,1\\}^{\\scriptscriptstyle X}$ has an infinite star tree and let $R(n)$ be a rate function with $R(n)=o(1/n)$ . Then, $\\mathbb{H}$ requires rate $R(n)$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Let ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\vec{t}=\\bigcup_{0\\leq\\ell<\\infty}\\left\\{(x_{\\vec{u}},z_{\\vec{u}})\\in\\left(\\mathcal{X}^{\\ell+1},\\{0,1\\}^{\\ell+1}\\right):\\vec{u}\\in\\{0\\}\\times\\{0,1\\}\\times\\ldots\\times\\{0,1,\\ldots,\\ell\\}\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "be an infinite star tree. We also fix the learning algorithm $\\hat{h}_{n}$ . Recall that $\\{u(n)\\}_{n\\in\\mathbb{N}}$ denotes the number of unlabeled samples that $\\hat{h}_{n}$ uses. ", "page_idx": 24}, {"type": "text", "text": "We first pick a path on the tree uniformly at random. To be more precise, let $\\vec{y}=(y_{1},y_{2},\\ldots)$ be a sequence of independent random variables where $y_{i}$ is uniformly distributed in $\\{0,\\ldots,i-1\\}$ . The idea is that $\\vec{y}$ specifies a random path on the tree. We define the target distribution $\\mathrm{P}_{\\vec{y}}$ in an inductive way by specifying sequences $\\{\\bar{k}_{i},p_{i},n_{i}\\}_{i\\geq1}$ and putting mass $p_{i}$ on the node of the random path at level $k_{i}$ . Conditional on the given node, we distribute the mass among its points uniformly. Let us now be formal. For the base of the induction, we let $p_{2}=1/2$ and $k_{1}=1$ . We will specify the value of $p_{1}$ later. For the inductive step, for any $i\\geq2$ , given the value of $p_{i},k_{i-1}$ , we will specify $k_{i},p_{i+1}$ . We let $k_{i}$ be the smallest integer so that $k_{i}>k_{i-1}$ and $R(\\lfloor k_{i}/8\\rfloor)\\,<\\,p_{i}/k_{i}$ . Notice that since $\\mathbf{\\dot{\\cal{R}}}(n)=o(1/n)$ , $k_{i}<\\infty$ . We let $n_{i}=\\lfloor k_{i}/8\\rfloor$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{i+1}=\\operatorname*{min}\\left\\{\\frac{1}{4u_{n_{i}}},\\frac{p_{i}}{4}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Notice that since $\\{p_{i}>0\\}_{i\\geq2},\\textstyle\\sum_{i\\geq2}p_{i}<1^{4}$ , by setting $\\begin{array}{r}{p_{1}=1-\\sum_{i\\geq2}p_{i}}\\end{array}$ the sequence $\\{p_{i}\\}_{i\\ge1}$ can be used as a probability distribution. We are now ready to define $\\mathrm{P}_{\\vec{y}}$ on ${\\mathcal{X}}\\times\\{0,1\\}$ . To make the notation simpler, for any $\\vec{y}$ and for $k\\geq1$ ", "page_idx": 24}, {"type": "equation", "text": "$$\nw_{\\vec{y}\\leq k-1}^{j}={\\left\\{\\begin{array}{l l}{1-z_{\\vec{y}\\leq k-1}^{j},}&{{\\mathrm{if~}}j=y_{k}}\\\\ {z_{\\vec{y}\\leq k-1}^{j},}&{{\\mathrm{otherwise}}}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $i\\geq1$ , let ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{P}_{\\vec{y}}\\left(x_{\\vec{y}\\le k_{i}-1}^{j},w_{\\vec{y}\\le k_{i}-1}^{j}\\right)=\\frac{p_{i}}{k_{i}},1\\le j\\le k_{i}-1\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Notice that, by the definition of $p_{i}$ , this is indeed a probability distribution. We now show that $\\mathrm{P}_{\\vec{y}}$ is realizable. Since $\\vec{t}$ is a star tree, there exists some $h\\in\\mathbb{H}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\nh\\left(x_{\\vec{y}_{\\le k}}^{j}\\right)=w_{\\vec{y}\\le k-1}^{j}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $0\\le j\\le k$ , and $1\\leq k\\leq n$ . Thus, for the probability of error of $h$ we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname{er}_{\\vec{y}}(h):=\\mathrm{P}_{\\vec{y}}\\left[(x,y)\\in\\mathcal{X}\\times\\{0,1\\}:h(x)\\neq y\\right]\\leq\\sum_{k>n}p_{k}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "so by taking $n\\to\\infty$ we see that $\\mathrm{P}_{\\vec{y}}$ is realizable for every $\\vec{y}$ . Moreover, we note that the mapping $\\vec{y}\\rightarrow\\mathrm{P}_{\\vec{y}}$ is measurable. ", "page_idx": 24}, {"type": "text", "text": "We now let $(X,Y),(X_{1},Y_{1}),(X_{2},Y_{2}),\\dots$ be i.i.d. samples drawn from $\\mathrm{P}_{\\vec{y}}$ . We can draw them as ", "page_idx": 24}, {"type": "equation", "text": "$$\nX=x_{\\vec{y}\\le T-1}^{J},\\quad Y=w_{\\vec{y}\\le T-1}^{J},\\quad X_{j}=x_{\\vec{y}\\le T_{j}-1}^{J_{j}},\\quad Y_{j}=w_{\\vec{y}\\le T_{j}-1}^{J_{j}}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(T,J),(T_{1},J_{1}),(T_{2},J_{2}),..\\,.$ are i.i.d. random variables, independent of the path $\\vec{y}$ , with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[T=k_{i},J=j]=\\frac{p_{i}}{k_{i}}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $i\\geq1,0\\leq j\\leq k_{i}-1$ . ", "page_idx": 24}, {"type": "text", "text": "For all $i\\in\\mathbb N$ , let us define the event $E_{0}^{i}\\,=\\,\\bigl\\{T_{1}\\le k_{i},T_{2}\\le k_{i},\\dots,T_{u_{n_{i}}}\\le k_{i}\\bigr\\}$ under which the algorithm has not received any unlabeled points from any level $k_{j},j>i$ . Let us also call $E_{1}^{i}$ the event that $\\hat{h}_{n_{i}}$ does not query the point whose label is filpped, i.e., the point xy\u20d7k\u2264iki\u22121. Also, we denote Ei2 the event that the learner classifies at least one point of level $k_{i}$ incorrectly. Then, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[E_{2}^{i}|E_{1}^{i},E_{0}^{i}]\\ge\\left(1-\\frac{1}{(7/8\\cdot k_{i})}\\right)\\ge\\frac{6}{7}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "since there are at least $(7/8)\\cdot k_{i}$ points on this level that the learner has not queried, and under the events we have conditioned on the filpped point along the path is chosen uniformly at random among these points. We now bound $\\mathrm{Pr}[E_{0}^{i}]$ . Notice that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{j\\geq i+1}p_{j}\\leq{\\frac{4}{3}}p_{i+1}\\leq{\\frac{p_{i}}{3}}\\leq{\\frac{1}{12\\cdot u_{n_{i}}}}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "4This is because $p_{i+1}\\leq p_{i}/4$ . ", "page_idx": 24}, {"type": "text", "text": "Thus, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[E_{0}^{i}]\\geq\\left(1-\\sum_{j\\geq i+1}p_{j}\\right)^{u_{n_{i}}}\\geq\\left(1-\\frac{1}{12\\cdot u_{n_{i}}}\\right)^{u_{n_{i}}}\\geq\\frac{9}{10}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, we need to bound $\\mathrm{Pr}[E_{1}^{i}|E_{0}^{i}]$ . Notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[E_{1}^{i}|E_{0}^{i}]\\geq{\\frac{7}{8}}\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "since the learner has $n$ label queries and there are at least $8n$ points. Putting it together, we can bound ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}[E_{2}^{i}]=\\mathrm{Pr}[E_{2}^{i}|E_{1}^{i},E_{0}^{i}]\\cdot\\mathrm{Pr}[E_{1}^{i}|E_{0}^{i}]\\cdot\\mathrm{Pr}[E_{0}^{i}]}\\\\ &{\\quad\\quad\\quad\\geq\\frac{6}{7}\\cdot\\frac{7}{8}\\cdot\\frac{9}{10}}\\\\ &{\\quad\\quad\\quad=\\frac{27}{40}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which using reverse Markov\u2019s inequality implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\vec{y}}\\left[\\operatorname*{Pr}[E_{2}^{i}|\\vec{y}]\\geq\\frac{9}{40}\\right]\\geq\\frac{18}{31}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\hat{h}_{n_{i}}(X)\\neq Y|\\vec{y}]\\ge\\frac{p_{i}}{k_{i}}\\cdot\\operatorname*{Pr}[E_{2}^{i}|\\vec{y}]\\ge R(n_{i})\\cdot\\operatorname*{Pr}[E_{2}^{i}|\\vec{y}]\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, we can see that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\vec{y}}\\left[\\operatorname*{Pr}[\\hat{h}_{n_{i}}\\neq Y|\\vec{y}]\\geq\\frac{9}{40}\\cdot R(n_{i})\\right]\\geq\\frac{18}{31}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using Fatou\u2019s lemma, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{\\varepsilon}\\left[\\underset{i\\rightarrow\\infty}{\\operatorname*{lim}}\\displaystyle\\operatorname*{sup}\\frac{1}{R(n_{i})}\\operatorname*{Pr}[\\hat{h}_{n_{i}}(X)\\neq Y|\\bar{y}]\\right]\\geq\\mathbb{E}\\left[\\operatorname*{lim}_{i\\rightarrow\\infty}\\displaystyle\\frac{1}{R(n_{i})}\\cdot\\operatorname*{min}\\left\\lbrace\\operatorname*{Pr}[\\hat{h}_{n_{i}}(X)\\neq Y|\\bar{y}],9/40\\cdot R(n_{i})\\right.\\right.}\\\\ &{\\left.\\left.\\geq\\underset{i\\rightarrow\\infty}{\\operatorname*{lim}}\\displaystyle\\frac{1}{R(n_{i})}\\mathbb{E}\\left[\\operatorname*{min}\\left\\lbrace\\operatorname*{Pr}[\\hat{h}_{n_{i}}(X)\\neq Y|\\bar{y}],9/40\\cdot R(n_{i})\\right\\rbrace\\right.\\right.\\right.}\\\\ &{\\left.\\left.\\geq\\underset{i\\rightarrow\\infty}{\\operatorname*{lim}}\\displaystyle\\operatorname*{sup}\\frac{1}{R(n_{i})}\\operatorname*{Pr}\\left[\\operatorname*{Pr}[\\hat{h}_{n_{i}}(X)\\neq Y|\\bar{y}]\\geq9/40\\cdot R(n_{i})\\right]\\cdot\\frac{1}{2}\\right.\\right.}\\\\ &{\\left.\\left.\\geq\\frac{9}{40}\\cdot\\frac{18}{31}\\right.\\right.}\\\\ &{=\\frac{81}{620}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality is by the definition of min, the second inequality follows from Fatou\u2019s lemma, and the third inequality follows from Markov\u2019s inequality. We remark that Fatou\u2019s lemma can be applied here since ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{R(n_{i})}\\operatorname*{min}\\left\\{\\operatorname*{Pr}\\left[\\hat{h}_{n_{i}}(X)\\neq Y|\\vec{y}\\right],9/40\\cdot R(n_{i})\\right\\}\\leq9/40\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, there must exist a realization of $\\vec{y}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{er}_{\\vec{y}}(\\hat{h}_{n})|\\vec{y}]\\geq C\\cdot R(n)\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "infinitely often, where $C$ is some absolute constant. Choosing $\\mathrm{~P~}=\\mathrm{~P~}_{\\vec{y}}$ for this realization of $\\vec{y}$ concludes the proof. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "D Omitted Details from Section 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 Useful Tools ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem D.1 (Dvoretzky\u2013Kiefer\u2013Wolfowitz (DKW) Inequality). Let $F$ be the CDF of some distribution $\\mathcal{D}$ supported on $\\mathbb{R}$ , $n\\in\\mathbb{N}$ and $F_{n}$ be the empirical CDF obtained from n i.i.d. samples from $\\mathcal{D}$ . Then, $\\forall\\varepsilon>0$ we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{sup}_{x\\in\\mathbb{R}}|F_{n}(x)-F(x)|>\\varepsilon\\right]\\le2e^{-2n\\varepsilon^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "image", "img_path": "T0e4Nw09XX/tmp/14fc289e136dba10b4887be387336410929c8dc278d1a4d9f1e45ce0458b0705.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.2 The VCL Game ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For completeness, we provide various algorithms and results that appeared in Bousquet et al. (2021); Hanneke et al. (2022), which we utilize in our approach to get sublinear learning rates. It is instructive to start with an informal description of a pattern avoidance function $g:\\mathcal{X}^{k}\\stackrel{\\smile}{\\rightarrow}\\{0,1\\}^{k}$ . Intuitively, this function takes as input a $k$ -tuple of unlabeled data generated by a realizable distribution $\\mathrm{P}_{\\mathcal{X}}$ and it returns a labeling of these data that is not the true labeling by $\\mathrm{P}$ . They define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{per}(g)=\\mathrm{P}^{k}[x_{1},\\ldots,x_{k}:g(x_{1},\\ldots,x_{k})=(y_{1},\\ldots,y_{k})]\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Bousquet et al. (2021) provide a way to obtain such a pattern avoidance function for any realizable distribution. This is obtained by a VCL game in Figure 6 which returns a function $\\hat{\\vec{y}}_{\\tau_{t}}$ . Their following result shows that as the number of points $N$ used in the VCL game grows, the probability that we obtain a pattern avoidance function that is incorrect goes to zero. ", "page_idx": 26}, {"type": "text", "text": "Lemma D.2 (Correct Pattern Avoidance Function (Restatement of Lemma 5.7 (Bousquet et al., 2021))). Consider the VCL game in Figure 6. Then, $\\mathrm{Pr}[\\mathrm{per}(\\hat{\\vec{y}}_{\\tau_{t}})>0]\\rightarrow0$ , as $N\\rightarrow\\infty$ . ", "page_idx": 26}, {"type": "text", "text": "Finally, we remark that Bousquet et al. (2021) showed the following result. ", "page_idx": 26}, {"type": "text", "text": "Theorem D.3 (Supervised Learning Linear Rates (Bousquet et al., 2021)). If H does not have an infinite VCL tree there exists a supervised learning algorithm that can learn $\\mathbb{H}$ at linear rate. ", "page_idx": 26}, {"type": "text", "text": "D.3 Omitted Details of the Sublinear Rates Algorithm ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma D.4. Let ${\\mathcal F}_{\\sqrt{n}/5}^{i},i\\,\\in\\,[\\sqrt{n}]$ be the partial concept classes that are obtained by playing the VCL game on ${\\sqrt{n}}/5$ examples that are generated i.i.d. from P. Then, with probability at least $1-e^{-C\\cdot n^{1/4}}$ , where $C>0$ is some absolute constant, $\\mathrm{P}$ is a realizable distribution for $a$ $(1-o(1))$ \u2212fraction of these classes. ", "page_idx": 26}, {"type": "text", "text": "Proof. Let $\\widehat{\\vec{y}}_{\\sqrt{n}/5}^{i}$ be the pattern avoidance function that is obtained from the $i-$ th batch of ${\\sqrt{n}}/5$ i.i.d. samples from $\\mathrm{P}$ . Since we know that $\\mathrm{Pr}[\\mathrm{per}(\\widehat{\\vec{y}}_{\\sqrt{n}/5}^{i})>0]\\rightarrow0$ as $n\\to\\infty$ (cf. Lemma D.2), for any $\\varepsilon>0$ there is some $b_{\\varepsilon}$ such that if the batch size $b=\\sqrt{n}/5$ is at least $b_{\\varepsilon}$ , then $\\mathrm{Pr}[\\mathrm{per}(\\widehat{\\vec{y}}_{\\sqrt{n}/5}^{i})>$ $0]<\\varepsilon$ . Hence, for every $n$ there is some $\\varepsilon_{n}=o(1)$ so that when the batch size is ${\\sqrt{n}}/5$ we have that $\\operatorname*{Pr}[\\mathrm{per}({\\widehat{\\vec{y}}}_{\\sqrt{n}/5}^{i})>0]<\\varepsilon_{n}$ . Notice that whenever $\\mathrm{per}(\\widehat{\\vec{y}}_{\\sqrt{n}/5}^{i})=0$ the distribution $\\mathrm{P}$ is realizable with respect to the corresponding class ${\\mathcal{F}}_{\\sqrt{n}/5}^{i}$ . Let us set $\\delta_{n}=n^{-1/8}$ . Using Hoeffding\u2019s inequality we have that with probability at least $1\\!-\\!e^{-C\\cdot\\delta_{n}^{2}\\cdot\\sqrt{n}}=1\\!-\\!e^{C\\cdot n^{1/4}}$ , where $C$ is some absolute constant, at least $\\left(1-\\varepsilon_{n}-\\delta_{n}\\right)$ \u2212fraction of the $\\widehat{\\vec{y}}_{\\sqrt{n}/5}^{i}$ will have $\\mathrm{per}(\\widehat{\\vec{y}}_{\\sqrt{n}/5}^{i})=0$ . Since $\\delta_{n}+\\varepsilon_{n}=o(1)$ we have shown the claim. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Lemma D.5. Let $\\widetilde{\\mathrm{P}}$ be the distribution of the VC dimension of the partial concept classes that are obtained by playing the VCL game on infinitely many i.i.d. samples from $\\mathrm{P}$ . Let $\\mathrm{d^{*}}$ be the $9/10\\$ - quantile of $\\widetilde{\\mathrm{P}}$ . Then, for $n\\geq n_{\\mathrm{P}}$ , where $n_{\\mathrm{P}}$ is some $\\mathrm{P}$ -dependent constant we have that $\\widehat{\\mathrm{d}}_{n}=\\mathrm{d}^{*}$ , with probability at least 1 \u22123e\u2212CP\u00b7n1/4, where $C_{\\mathrm{P}}$ is a constant the depends on the data-generating distribution $\\mathrm{P}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Consider the following experiment. We sample an infinite sequence of labeled points from $\\mathrm{P}$ and we run the VCL game (cf. Figure 6) on this sequence. Let $\\widehat{y}$ be the pattern avoidance function that is obtained from that game defined over\u2113many points and let ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{F}}:=\\left\\{f:\\mathcal{X}\\to\\{0,1,\\star\\}:(f(x_{1}),\\dots,f(x_{\\widehat{\\ell}}))\\neq\\widehat{y}(x_{1},\\dots,x_{\\widehat{\\ell}})\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $d^{*}\\in\\mathbb{N}$ be the smallest number such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\mathrm{d}(\\widehat{\\mathcal{F}})\\leq d^{*}]>9/10\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $F$ be the CDF of the distribution of the VC dimension of $\\mathcal{F}$ , let $d_{1}<d^{*}<d_{2}$ be the largest, smallest number5 such that $F(d_{1})\\neq F(d^{*}),F(d_{2})\\neq F(d^{*})$ and let $C_{\\mathrm{P}}=\\operatorname*{min}\\{F(d^{*})-F(d_{1}\\bar{)},F(d_{2})-$ $F(d^{*})\\}/4$ . Let $F_{\\sqrt{n}}$ be the empirical CDF of the distribution obtained on $\\sqrt{n}$ i.i.d. samples. Notice that whenever we estimate $F$ with accuracy $C_{\\mathrm{P}}$ the empirical $9/10$ -quantile will the same as the true $9/10$ -quantile. The DKW inequality (Theorem D.1) shows that this happens with probability $1-2e^{-2C_{\\mathrm{P}}^{2}k}$ . Let us call this even $E_{1}$ and condition on it. ", "page_idx": 27}, {"type": "text", "text": "Next, we need to handle the fact that we do not obtain samples from partial concept c\u221alasses that are generated from VCL games on infinitely many i.i.d. samples from $\\mathrm{P}$ but merely on $\\sqrt{n}$ many such samples. Let us consider the following experiment. We run the VCL game on $k$ streams of infinitely many i.i.d. samples from $\\mathrm{P}$ and if the game has not terminated within the first $\\sqrt{n}$ rounds, we rewind the pattern avoidance function to the one that is obtained in that round. Then, Lemma D.4 shows that, with probability at least 1 \u2212e\u2212C\u00b7n1/4, we will only rewind the pattern avoidance function of a $o(1)-$ fraction of these games will have converged. We call this event $E_{2}$ and we condition on it. Let us denote by ${\\widehat{F}}_{k}$ the empirical CDF of the VC dimension of the partial concept classes obtained by this experiment. Under $E_{2}$ , we have that $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathbb{N}}|F_{k}(x)-\\widehat{F}_{k}(x)|=o(1).}\\end{array}$ Thus, by taking $n$ large enough so that the $o(1)<<C_{\\mathrm{P}}$ , we see that the estimation of $\\widehat{\\mathrm{d}}_{n}$ using the samples that are obtained from the truncated VCL game also converges to $\\mathrm{d^{*}}$ . The bou nd on the probability of error follows from a union bound over $E_{1},E_{2}$ . \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Proposition D.6. For $n>n_{\\mathrm{P}}$ , where $n_{\\mathrm{P}}$ is some distribution-dependent constant, given $m=O(n^{3})$ unlabeled samples we can estimate pn,k, pnX,k, pn,k with accuracy $o(1)$ , with probability at least $1-C_{\\mathrm{P}}\\cdot e^{-C_{\\mathrm{P}}^{\\prime}n^{1/4}}$ , where $C_{\\mathrm{P}},C_{\\mathrm{P}}^{\\prime}>0$ , are distribution-dependent constants. ", "page_idx": 27}, {"type": "text", "text": "Proof. As we alluded to before, using $O(n^{3})$ unlabeled samples in total we can estimate the quantities pn,k,i, pn,k,i, pn,k,i with accuracy $1/n$ for each version space V ni/5, with probability at least $1\\,-\\,C_{1}\\sqrt{n}e^{-C_{2}\\sqrt{n}}$ , where $C_{1},C_{2}~>~0$ , are absolute constants. Let us denote these estimates p n,k,i, p nX,k,i ,p(n,Xk,,yi) We condition on this event for the rest of the proof. ", "page_idx": 27}, {"type": "text", "text": "Assume that $n$ is large enough and condition on the events of Lemma D.4, Lemma D.5. Let us denote by $i_{1},\\ldots,i_{k}$ the indices of the partial concept classes whose VC dimension is bounded by $\\widehat{\\mathrm{d}}_{n}=\\mathrm{d}^{*}$ . By definition, $k\\geq9/10{\\sqrt{n}}$ . Consider the following statistical experiment. We run the VCL game on infinitely many i.i.d. samples from $\\mathrm{P}$ , we define the partial concept class obtained from the pattern avoidance function of the game, if the class has VC dimension at most $\\mathrm{d^{*}}$ we keep the sample, otherwise we discard it. Then, we define its version space on the first $n/5$ samples of $S_{2}^{\\infty}$ . Let us call ${\\widetilde{V}}^{i}$ the version space obtained by the $i$ -th sample of this experiment. Then, $\\mathrm{P}^{k}(x_{1},\\ldots,x_{k}\\mathrm{~VC~}$ shattered by ${\\widetilde{V}}^{i}$ ) is an unbiased sample from the distribution whose expected value $p_{n,k}$ we are trying to estim ate. Similarly for the rest of the quantities we have defined. Let us call these samples $\\widetilde{p}_{n,k,i},\\widetilde{p}_{n,k,i}^{X},\\widetilde{p}_{n,k,i}^{(X,y)}$ and denote by $\\widetilde{S}_{n},\\widetilde{S}_{n}^{X},\\widetilde{S}_{n}^{(X,y)}$ their empirical estimates over $9/10\\sqrt{n}$ many samples. Hoeffding\u2019s inequality gives us that $|\\widetilde{S}_{n}-p_{n,k}|=o(1),|\\widetilde{S}_{n}^{X}-p_{n,k}^{X}|=$ $o(1),|\\widetilde{S}_{n}^{(X,y)}-p_{n,k}^{(X,y)}|=o(1)$ , with probability at least $1-C e^{-C^{\\prime}\\cdot n^{1/4}}$ , for some absolute constants $C,C^{\\prime}>0$ . We condition on this event. ", "page_idx": 27}, {"type": "text", "text": "Notice that, under the events we have conditioned on, the samples $p_{n,k,i},p_{n,k,i}^{X},p_{n,k,i}^{(X,y)}$ we obtain from running the VCL game on O( n) many samples, differ from the samplespn,k,i,pnX,k,i,p(n,Xk,,yi) ", "page_idx": 27}, {"type": "text", "text": "on a sublinear number of terms. Moreover, the absolute difference on the terms they disagree on is bounded by 1. Thus, if we denote by $S_{n},S_{n}^{X},S_{n}^{(X,y)}$ the average of these values we have that $|\\widetilde{S}_{n}-S_{n}|=o(1),|\\widetilde{S}_{n}^{X}-S_{n}^{X}|=o(1),|\\widetilde{S}_{n}^{(X,y)}-S_{n}^{X}|=o(1)$ . Finally, let $\\widehat{S}_{n},\\widehat{S}_{n}^{X},\\widehat{S}_{n}^{(X,y)}$ be the av erages of $\\widehat{p}_{n,k,i},\\widehat{p}_{n,k,i}^{X},\\widehat{p}_{n,k,i}^{(X,y)}$ . Since $|\\widehat{p}_{n,k,i}-p_{n,k,i}|=o(1),|\\widehat{p}_{n,k,i}^{X}-p_{n,k,i}^{X}|=o(1),|\\widehat{p}_{n,k,i}^{(X,y)}-$ $p_{n,k,i}^{(X,y)}|=o(1)$ , usi n g the  tr iangle inequali ty on all the previous esti mations we see that $|\\widehat{S}_{n}-p_{n,k}|=$ $o(1),|\\widehat{S}_{n}^{X}-p_{n,k}^{X}|=o(1),|\\widehat{S}_{n}^{(X,y)}-p_{n,k}^{(X,y)}|=o(1)$ )\u2212p(n,Xk,y)| = o(1), with probability at least 1\u2212CP \u00b7e\u2212C\u2032Pn1/4. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.7. Let $n>C_{S_{2}^{\\infty}}$ , where $C_{S_{2}^{\\infty}}$ is some constant that depends on $S_{2}^{\\infty}$ , P. Then, with probability at least $1-C_{\\mathrm{P}}\\cdot e^{-C_{\\mathrm{P}}^{\\prime}n^{1/4}}$ 4, for every k \u2264k\u2217, ifpnX,k < $\\begin{array}{r}{i f\\widehat{p}_{n,k}^{X}<\\frac{\\widehat{p}_{n,k}}{2}}\\end{array}$ n2,kthen y = argmaxy\u2208{0,1} n,k p(X,y)is the correct label of $X$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Let $n>C_{S_{2}^{\\infty}}$ be large enough so that for all $k\\leq k^{*}$ we have that: ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "equation", "text": "$$\np_{n,k}\\leq100/99\\cdot\\operatorname*{lim}_{n\\rightarrow\\infty}p_{n,k}\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u2022 the guarantee from Proposition D.6 gives us that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\widehat{p}_{n,k}^{X}<\\frac{\\widehat{p}_{n,k}}{2}\\implies p_{n,k}^{X}<\\frac{3}{4}\\cdot p_{n,k}\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u2022 and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p_{n,k}^{(X,y)}\\geq\\displaystyle\\frac{99}{100}p_{n,k}\\implies\\widehat{p}_{n,k}^{(X,y)}\\geq\\displaystyle\\frac{95}{100}\\widehat{p}_{n,k}}}\\\\ {{p_{n,k}^{(X,y)}<\\displaystyle\\frac{80}{100}p_{n,k}\\implies\\widehat{p}_{n,k}^{(X,y)}<\\displaystyle\\frac{90}{100}\\widehat{p}_{n,k}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We condition on these events for the rest of the proof. Since $p_{n,k}\\leq100/99\\cdot\\operatorname*{lim}_{n\\to\\infty}p_{n,k}$ for the correct label y\u2217we have that p(n,Xk,y\u2217)\u226599/100\u00b7pn,k. Moreover, since pnX,k < 3/4\u00b7pn,k for the other label y\u00af = 1 \u2212y\u2217it holds that p(n,Xk,y\u00af $p_{n,k}^{(X,\\bar{y})}<80/100\\cdot p_{n,k}$ . Thus, under the event we have conditioned on we have that argmaxy\u2208{0,1} $\\mathrm{argmax}_{y\\in\\{0,1\\}}\\,\\widehat{p}_{n,k}^{(X,y)}$ p(n,Xk,y)will indeed be the correct label. The correctness probability follows directly from Propositio n D.6. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma D.8. For $k=k^{*}$ we have that $\\begin{array}{r}{\\operatorname*{Pr}\\left[\\widehat{p}_{n,k^{*}}^{X}\\geq\\frac{\\widehat{p}_{n,k^{*}}}{2}\\right]=o(1).}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Proof. Essentially, the proof of this result follows by Markov\u2019s inequality. Let us consider some $n$ saon idt  csoufnfdiciteiso tno  obno tuhned event described in Proposition D.6. Then, $\\begin{array}{r}{\\widehat{p}_{n,k^{*}}^{X}\\geq\\frac{\\widehat{p}_{n,k^{*}}}{2}\\implies}\\end{array}$ $p_{n,k^{*}}^{X}\\geq\\frac{p_{n,k^{*}}}{4}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[p_{n,k^{*}}^{X}\\geq\\frac{p_{n,k^{*}}}{4}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using Markov\u2019s inequality, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[p_{n,k^{*}}^{X}\\geq\\frac{p_{n,k^{*}}}{4}\\right]\\leq\\frac{4\\mathbb{E}\\left[p_{n,k^{*}}^{X}\\right]}{p_{n,k^{*}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{4p_{n,k^{*}+1}}{p_{n,k^{*}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=o(1)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we have used the fact that $\\mathbb{E}[p_{n,k^{*}}^{X}]=p_{n,k^{*}+1}$ and that $p_{n,k^{*}}$ is bounded below by a constant. ", "page_idx": 28}, {"type": "text", "text": "We now state the guarantees of ActiveSelect (Hanneke, 2012). We note that the original statement of the result did not specify the size of the unlabeled dataset that the algorithm uses, but a straightforward adaptation of it shows that with $O(n^{3})$ many unlabeled data its error guarantee changes increases by $O(\\Bar{1}/n^{2})$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma D.9 (Hanneke (2012)). A call to ActiveSelect (cf Figure 8) with $N$ classifiers $\\{h_{1},\\dotsc,h_{N}\\}$ , a query budget of $m$ , and $O(N^{2}/n^{3})$ unlabeled points makes at most m label queries and if $h_{\\widehat{k}}$ is the classifier it returns, then with probability at least $1-C_{1}\\cdot N e^{-C_{2}\\cdot m/(N^{2}\\log(N))}$ , $w e$ hav e $\\begin{array}{r}{\\mathrm{er}(h_{\\widehat{k}})\\leq2\\operatorname*{min}_{k\\in[N]}\\mathrm{er}(h_{k})+O(1/\\bar{n}^{2})}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "We are now ready to prove the main result in this section. The algorithm is summarized in Figure 7. ", "page_idx": 29}, {"type": "text", "text": "Theorem D.10. Assume that $\\mathbb{H}\\subseteq\\{0,1\\}^{x}$ does not have an infinite VCL tree. Then, $\\mathbb{H}$ admits an active learning algorithm that achieves sublinear rates. ", "page_idx": 29}, {"type": "text", "text": "Proof. We have all the main ingredients we need to prove our result. Let us fix some $S_{2}^{\\infty}$ whose elements are i.i.d. from $\\mathrm{P}$ . Conditioning on the results of Lemma D.7, Lemma D.8, we have that for large enough $n$ , which depends on $S_{2}^{\\infty}$ , we can obtain a labeled dataset $L_{n}$ whose size is $\\omega(n)$ . Feeding this dataset into the supervised learning algorithm from Bousquet et al. (2021) gives as a classifier $\\widetilde{h}_{n}$ that, conditioned on $S_{2}^{\\infty}$ and the events $E$ we have described, has error ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{er}(\\widetilde{h}_{n})|S_{2}^{\\infty},E]=o(1/n)\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Moreover, the probability of $E$ is also $o(1/n)$ , so we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\operatorname{er}(\\widetilde{h}_{n})|S_{2}^{\\infty}]=o(1/n)\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, for the output $\\widehat{h}_{n}$ of ActiveSelect, we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{er}(\\widehat{h}_{n})|S_{2}^{\\infty}]=o(1/n)}\\\\ &{\\qquad\\mathbb{E}[\\mathrm{er}(\\widehat{h}_{n})]=O(1/n)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second bound comes from the fact that we feed into ActiveSelect the output of the passive learning algorithm $\\widehat{h}_{n}^{p}$ from Bousquet et al. (2021) (cf. Theorem D.3), whose bound does not not depend on $S_{2}^{\\infty}$ . ", "page_idx": 29}, {"type": "text", "text": "Let us call $R(n)$ the error rate of our algorithm. To show that $\\mathbb{E}[R(n)]=o(1/n)$ it suffices to prove that lim $\\begin{array}{r}{\\operatorname*{sup}_{n\\rightarrow\\infty}\\mathbb{E}[n\\cdot R(n)]=0}\\end{array}$ . Fatou\u2019s lemma gives us that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}n\\cdot\\mathbb{E}[\\cdot R(n)]=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}n\\cdot\\mathbb{E}[\\cdot R(n)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\underset{S_{2}^{\\infty}}{\\mathbb{E}}[\\mathbb{E}[n\\cdot R(n)|S_{2}^{\\infty}]]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{S_{2}^{\\infty}}{\\mathbb{E}}[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}n\\cdot\\mathbb{E}[R(n)|S_{2}^{\\infty}]]}\\\\ &{\\qquad\\qquad\\qquad=0\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where Fatou\u2019s lemma applies since $\\mathbb{E}[n\\cdot R(n)|S_{2}^{\\infty}]\\leq C_{\\mathrm{P}}$ where $C_{\\mathrm{P}}$ is some $\\mathrm{P}-$ dependent constant.   \nThis is because, almost surely over $S_{2}^{\\infty}$ , the expected error of $\\widehat{h}_{n}^{p}$ is bounded by $C_{\\mathrm{P}}^{\\prime}/n$ . ", "page_idx": 29}, {"type": "text", "text": "D.4 Arbitrarily Slow Rates Lower Bound ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem D.11 (Hanneke et al. (2022)). If $\\mathbb{H}$ has an infinite VCL tree, no active learning algorithm can achieve rates that are faster than arbitrarily slow. ", "page_idx": 29}, {"type": "text", "text": "D.5 Omitted Figures ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Sublinear Rates Algorithm [Adaptation from Hanneke (2012)] Input is a label budget $N$ and a passive learning algorithm $A_{p}$ ", "page_idx": 30}, {"type": "text", "text": "1. Let $S_{1},S_{2},S_{3},S_{4},S_{5},U$ be sets of $O(N^{3})$ unlabeled points that are i.i.d. from $\\mathrm{P}_{\\mathcal{X}}$ .   \n2. Request the labels of the first $N/5$ points of $S_{5}$ and run $A_{p}$ on this set. Let $\\widehat{h}_{N}^{p}$ be its output. Req\u221auest the labels of the first $N/5$ points of $S_{1}$ , split them into batches of size $O(\\sqrt{N})$ and run the VCL game on them (cf. Figure 6).   \n4. Let $\\widehat{\\boldsymbol{y}}_{\\sqrt{N}/t}^{i},i\\in[\\sqrt{N}]$ , be the pattern avoidance functions from the previous step and for every $i\\in[\\sqrt{N}]$ define ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}_{\\sqrt{N}/5}^{i}:=\\left\\{f:\\mathcal{X}\\rightarrow\\left\\{0,1,\\star\\right\\}:(f(x_{1}),\\dots,f(x_{\\ell_{\\sqrt{N}/5}^{i}}))\\neq\\right.}\\\\ {\\left.\\widehat{y}_{\\sqrt{N}/5}^{i}(x_{1},\\dots,x_{\\ell_{\\sqrt{N}/5}^{i}})\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "5. Let ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{d}}_{N}:=\\operatorname*{min}_{d\\in\\mathbb{N}}\\left\\{\\exists i_{1},i_{2},\\ldots,i_{9/10}.\\sqrt{N}\\in[\\sqrt{N}]{:}\\right.}\\\\ {\\left.i_{1}<i_{2}<...<i_{9/10}.\\sqrt{N},\\mathrm{d}(\\mathcal{F}_{\\sqrt{N}/5}^{i_{j}})\\leq d,\\forall j\\in[9/10\\sqrt{N}]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "6. Request the labels of the first $N/5$ points of $S_{2}$ and let ", "page_idx": 30}, {"type": "equation", "text": "$$\nV_{N/5}^{i}:=\\left\\{f\\in\\mathcal{F}_{\\sqrt{N}/5}^{i}:f(x)=y\\;\\mathrm{for}\\;\\mathrm{these}\\;N/5\\;\\mathrm{point}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": ". For $k=0,\\ldots,\\widehat{\\mathbf{d}}_{N}$ : (a) Let ${\\widetilde{S}}_{3}$ be the next $n^{2}$ examples in $S_{3}$ and $\\widetilde{m}_{N}=N/(5(\\widehat{d}_{N}+1))$ be the label bud get. (b) Let $\\widehat{S}=\\{\\}$ (c) For each point $X\\in\\widetilde{S}_{3},y\\in\\{0,1\\}$ : i. EstimatepN,k,p NX,k, p (NX,,ky)using V Ni/5 and fresh unlabeled points from U as described in Proposition D.6. ii. If $\\widehat{p}_{N,k}^{X}\\le\\widehat{p}_{N,k}/2$ infer $\\boldsymbol{y}^{*}=\\operatorname*{argmax}_{\\boldsymbol{y}\\in\\{0,1\\}}\\boldsymbol{\\widehat{p}}_{N,k}^{(X,y)}$ , otherwise request the label $y^{\\ast}$ of $X$ . iii. Append $(X,y^{*})$ to $\\widehat S$ and update the label budget $\\widetilde{m}_{N}$ . If $\\widetilde{m}_{N}=0$ jump to Step (d). (d) Run $A_{p}$ on $\\widehat S$ and let $\\widehat{h}_{k}$ be its output. ", "page_idx": 30}, {"type": "text", "text": "8. Return ActiveSelect $\\left(\\left\\{\\widehat{h}_{0},...,\\widehat{h}_{k},\\widehat{h}_{N}^{p}\\right\\},N/5,S_{4}\\right)$ (cf. Figure 8). ", "page_idx": 30}, {"type": "text", "text": "Figure 8: ActiveSelect Algorithm (Hanneke, 2012). ", "page_idx": 31}, {"type": "text", "text": "ActiveSelect (Hanneke, 2012) Input is a set of classifiers $\\{h_{1},\\ldots,h_{L}\\}$ , label budget $m$ ,   \nsequence of unlabeled examples $\\boldsymbol{\\mathcal{U}}$ . 1. For each $j,k\\in\\{1,\\ldots,N\\}$ s.t. $j<k$ : (a) Let $R_{j k}$ be the first j(L\u2212j)m ln(eL) points in U \u2229{x : hj(x) \u0338= hk(x)} (if such value exists). (b) Request the labels for $R_{j k}$ and let $Q_{j k}$ be the set of labeled examples. (c) Let $m_{k j}=\\mathrm{er}_{Q_{j k}}(h_{k})$ . 2. Return $h_{\\hat{k}}$ , where $\\begin{array}{r}{\\hat{k}=\\operatorname*{max}\\left\\{k\\in\\{1,\\dots,L\\}:\\operatorname*{max}_{j<k}m_{k j}\\leq7/12\\right\\}.}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The main body and supplementary material of the submission provide proofs for all the claims. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We formally define the mathematical model our results hold for. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We present full proofs in the main body and the supplementary material. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The work is primarily theoretical and does not have any immediate sociatal impact. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]