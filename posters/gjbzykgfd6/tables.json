[{"figure_path": "gJbZyKGfd6/tables/tables_7_1.jpg", "caption": "Table 1: Test accuracy based on a nested 5-fold cross-validation (%, mean \u00b1 standard error). Results corresponding to methods marked with * are directly sourced from [17].", "description": "This table presents the test accuracy results of the HyperLogic model and other baseline models across four datasets (magic, adult, house, heloc). The accuracy is calculated using nested 5-fold cross-validation, and the results are reported as mean \u00b1 standard error.  The baseline models include DR-Net, CG, BRS, and RIPPER. Results for the latter three models are taken from the cited reference [17].", "section": "5.2 Performance Comparison"}, {"figure_path": "gJbZyKGfd6/tables/tables_8_1.jpg", "caption": "Table 2: Examples of optimal rule sets learned from different training runs on the heloc dataset", "description": "This table presents three different optimal rule sets obtained from three separate training sessions using the HyperLogic model on the HELOC dataset.  Each rule set includes a set of logical rules aiming to predict a binary outcome.  The \"Train Acc\" and \"Test Acc\" columns show the training and testing accuracy of each version. The variations between versions highlight the ability of HyperLogic to uncover multiple diverse, high-performing rule sets for the same task, which would not be accessible using a standard approach limited to producing a single set of rules.", "section": "5.2 Performance Comparison"}, {"figure_path": "gJbZyKGfd6/tables/tables_14_1.jpg", "caption": "Table 3: The F1 score (\u00b1 std) of two methods among 11 synthetic datasets", "description": "This table presents the F1 scores achieved by DIFFNAPS and HyperLogic on 11 synthetic datasets with varying numbers of categories (K).  Each dataset is tested with a fixed input dimension of 5000, and each category contains 1000 samples. The F1 score, a measure of a test's accuracy, is presented with its standard deviation, allowing for a statistical comparison of the two methods' performance across different dataset complexities.", "section": "B.1 Large Synthetic Datasets"}, {"figure_path": "gJbZyKGfd6/tables/tables_14_2.jpg", "caption": "Table 4: Comparison of HyperLogic, DIFFNAPS, and CLASSY across 4 real datasets.", "description": "This table compares the performance of HyperLogic against two other methods, DIFFNAPS and CLASSY, on four real-world biological datasets.  It shows the number of samples (n), features (D), and classes (K) in each dataset, along with the number of discovered patterns (#P), average pattern length (|P|), and Area Under the Curve (AUC) score achieved by each method.  The results indicate HyperLogic's competitive performance, particularly on larger datasets.", "section": "5.2 Performance Comparison"}]