[{"figure_path": "gJbZyKGfd6/figures/figures_1_1.jpg", "caption": "Figure 1: The framework of HyperLogic: Hypernetwork generates \u03b8 for the main network, which is a rule-learning network. An example of a main rule-learning network is shown on the right, from which rules can be extracted based on the learned network weights.", "description": "The figure illustrates the HyperLogic framework, which uses a hypernetwork to generate weights for a main rule-learning network.  The hypernetwork takes random samples from a distribution as input and outputs weights for different parts of the main network. The main network, in this case, a simple two-layer rule-learning network, learns if-then rules from the generated weights. The rules are extracted from the learned weights in the rule-learning network.  The process involves two loss functions: one for the hypernetwork and one for the task (rule learning). The combined loss is used to train the whole system and extract meaningful if-then rules from the data.", "section": "3 HyperLogic"}, {"figure_path": "gJbZyKGfd6/figures/figures_7_1.jpg", "caption": "Figure 3: Model complexity and rule complexity comparison", "description": "This figure compares the model complexity and rule complexity of HyperLogic and DR-Net across four datasets: magic, adult, house, and heloc. Model complexity represents the sum of the number of rules and the total number of conditions in the rule set, while rule complexity is the average number of conditions in each rule. The results show that HyperLogic achieves a lower model complexity and rule complexity compared to DR-Net across all four datasets.", "section": "5.3 Rule Analysis"}, {"figure_path": "gJbZyKGfd6/figures/figures_7_2.jpg", "caption": "Figure 4: Analysis of the impact of M2 on all datasets", "description": "This figure analyzes the effect of the hyperparameter M2 (number of rule sets sampled for selecting the optimal rule set) on the performance of HyperLogic across four datasets: magic, adult, house, and heloc.  The left panel shows the test accuracy, the middle panel depicts model complexity (sum of rules and conditions), and the right panel displays average rule complexity (conditions per rule). The plots reveal the trend of performance metrics across different values of M2. For instance, it shows the point where model complexity and rule complexity are balanced with accuracy.", "section": "5.3 Rule Analysis"}, {"figure_path": "gJbZyKGfd6/figures/figures_8_1.jpg", "caption": "Figure 5: Analysis of the impact of M\u2081 on magic dataset", "description": "This figure analyzes how the hyperparameter M\u2081, which controls the number of weight samples generated by the hypernetwork, affects the performance of HyperLogic on the magic dataset.  The left panel shows the number of unique rule sets generated as a function of the number of samples drawn, for different values of M\u2081.  The right panel shows the test accuracy and Jaccard similarity (a measure of rule set diversity) as a function of M\u2081.  The results suggest that an intermediate value of M\u2081 (around 5) provides a good balance between the diversity of the rule sets and the accuracy of the model.", "section": "5.3 Rule Analysis"}, {"figure_path": "gJbZyKGfd6/figures/figures_9_1.jpg", "caption": "Figure 6: Analysis of the impact of \u03bb\u2081 on magic dataset", "description": "This figure analyzes how the hyperparameter \u03bb\u2081, which controls the diversity regularization in the HyperLogic model, affects the number of unique rule sets generated, the test accuracy of the model, and the Jaccard similarity score between those rule sets.  The left panel shows that increasing \u03bb\u2081 leads to a greater number of unique rule sets generated. The right panel indicates a trade-off; while a larger \u03bb\u2081 increases the diversity of rule sets (higher Jaccard score), it can slightly decrease the test accuracy, suggesting that an optimal balance needs to be found.", "section": "5.3 Rule Analysis"}, {"figure_path": "gJbZyKGfd6/figures/figures_9_2.jpg", "caption": "Figure 7: Test accuracy using top L rule sets in ensemble learning.", "description": "This figure displays the test accuracy achieved by using ensemble learning with varying numbers (L) of top-performing rule sets. The test accuracy is initially observed to increase as L increases, suggesting that the combination of diverse rule sets enhances the overall performance. However, after a certain point, increasing L leads to slightly reduced test accuracy. This indicates that incorporating an excessive number of rule sets can negatively impact the overall prediction accuracy, possibly due to overfitting or interference amongst rule sets.", "section": "5.4 Ensemble Learning"}, {"figure_path": "gJbZyKGfd6/figures/figures_16_1.jpg", "caption": "Figure 5: Analysis of the impact of M\u2081 on magic dataset", "description": "This figure analyzes how the hyperparameter M\u2081 (number of weight samples in the training stage) impacts the performance of HyperLogic on the MAGIC dataset.  The left panel shows the number of unique rule sets generated as a function of the sample size for different values of M\u2081.  The right panel displays boxplots of the test accuracy and Jaccard similarity score for varying M\u2081, illustrating the trade-off between diversity and accuracy. A small M\u2081 might not fully explore the parameter space during training, potentially affecting diversity and accuracy. Conversely, a large M\u2081 increases the number of rules but may not significantly enhance the performance and potentially cause overfitting. The optimal M\u2081 value appears to balance this trade-off, yielding both diverse rule sets and high accuracy. ", "section": "5.3 Rule Analysis"}, {"figure_path": "gJbZyKGfd6/figures/figures_16_2.jpg", "caption": "Figure 6: Analysis of the impact of \u03bb\u2081 on magic dataset", "description": "This figure analyzes how the hyperparameter \u03bb\u2081, which controls diversity regularization, affects the number of unique rule sets generated, test accuracy, and Jaccard similarity scores.  The x-axis represents the number of samples drawn from the hypernetwork.  Three lines show the results for different values of \u03bb\u2081 (0.01, 0.1, and 1). The left subplot shows that as \u03bb\u2081 increases, the number of unique rule sets generated also increases, indicating that higher values of \u03bb\u2081 promote greater diversity. The right subplot shows that while increased \u03bb\u2081 leads to greater diversity (lower Jaccard similarity), it can negatively impact test accuracy after reaching a peak.", "section": "5.3 Rule Analysis"}, {"figure_path": "gJbZyKGfd6/figures/figures_16_3.jpg", "caption": "Figure 5: Analysis of the impact of M\u2081 on magic dataset", "description": "This figure analyzes how the hyperparameter M\u2081 (number of weight samples to approximate the expectation) affects the performance of HyperLogic on the MAGIC dataset.  The left panel shows the number of unique rule sets generated as the sample size increases for different values of M\u2081.  The right panel shows the test accuracy and Jaccard similarity (a measure of rule set diversity) for the different values of M\u2081.  It demonstrates that a small value of M\u2081 (such as 5) strikes a good balance between diversity and accuracy.", "section": "5.3 Rule Analysis"}, {"figure_path": "gJbZyKGfd6/figures/figures_16_4.jpg", "caption": "Figure 5: Analysis of the impact of M\u2081 on magic dataset", "description": "This figure analyzes how the hyperparameter M\u2081 (number of weight samples to approximate the expectation) affects the performance of HyperLogic on the MAGIC dataset.  The left panel shows the number of unique rule sets generated as the sample size increases for different values of M\u2081. The right panel shows box plots of the test accuracy and a line plot of the Jaccard similarity score for the generated rules across varying M\u2081 values. This illustrates the trade-off between rule diversity (indicated by the number of unique rule sets and Jaccard score) and accuracy. ", "section": "5.3 Rule Analysis"}, {"figure_path": "gJbZyKGfd6/figures/figures_16_5.jpg", "caption": "Figure 6: Analysis of the impact of \u03bb\u2081 on magic dataset", "description": "The figure analyzes how the hyperparameter \u03bb\u2081, which controls the diversity regularization, affects the diversity and accuracy of the generated rules in the MAGIC dataset. The left subplot shows the number of unique rule sets generated as the sample size increases for different values of \u03bb\u2081. The right subplot presents a box plot of the test accuracy and the Jaccard similarity (a measure of diversity) across different values of \u03bb\u2081.  The results indicate that increasing \u03bb\u2081 leads to greater rule set diversity but might slightly decrease the accuracy.", "section": "5.3 Rule Analysis"}, {"figure_path": "gJbZyKGfd6/figures/figures_17_1.jpg", "caption": "Figure 6: Analysis of the impact of \u03bb\u2081 on magic dataset", "description": "This figure analyzes how the hyperparameter \u03bb\u2081, which controls the diversity regularization in the HyperLogic model, affects the number of unique rule sets generated and the model's test accuracy.  The left panel shows that as the sample size increases, the number of unique rule sets generated increases for all values of \u03bb\u2081.  However, the increase is steeper for higher values of \u03bb\u2081 indicating that increasing \u03bb\u2081 leads to a greater diversity of rule sets. The right panel shows that while a higher \u03bb\u2081 leads to increased diversity (as measured by Jaccard similarity), it also slightly decreases the model's test accuracy.  This suggests there's a tradeoff between the diversity of rules generated and their overall predictive accuracy.", "section": "5.3 Rule Analysis"}]