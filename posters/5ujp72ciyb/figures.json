[{"figure_path": "5ujp72CiYB/figures/figures_1_1.jpg", "caption": "Figure 1: Visual tokens are redundant in MLLMs. Left: The accuracy of the LLaVA-1.5-7B [28] model(without re-train) on the GQA [20] benchmarks varies with different percentages of retained visual tokens. The x-axis represents the percentage of original visual tokens preserved after applying 1D average pooling with varying stride sizes S applied in i-th Transformer layer. Right: Visual tokens receive less attention from the [ANS] token as we go deeper into its layers of LLaVA-1.5-7B model. These findings collectively suggest a significant redundancy within the visual tokens of the MLLMs.", "description": "This figure shows experimental results supporting the claim that visual tokens are redundant in Multimodal Large Language Models (MLLMs). The left panel shows that reducing the number of visual tokens (up to 70%) at inference time by average pooling has minimal impact on the accuracy of visual question answering on the GQA benchmark.  The right panel demonstrates that visual tokens receive progressively less attention from the system's answer token as the model processes information through its layers. These findings collectively support the hypothesis of significant redundancy in visual tokens.", "section": "1 Introduction"}, {"figure_path": "5ujp72CiYB/figures/figures_4_1.jpg", "caption": "Figure 2: Example of Visual Context Compressor in a multi-modal LLM.", "description": "This figure illustrates how the Visual Context Compressor is integrated into a multi-modal large language model (MLLM). The compressor is applied to the visual tokens at the k-th transformer layer, reducing their length from L to Lout. This reduces the number of visual tokens processed by the subsequent layers, improving efficiency. The compression ratio is calculated as N*L/((N-K)*Lout + K*L), where N is the total number of transformer layers, K is the layer at which the compressor is applied, L is the initial length of visual tokens and Lout is the length after compression. ", "section": "3.2 Visual Context Compressor"}, {"figure_path": "5ujp72CiYB/figures/figures_5_1.jpg", "caption": "Figure 3: Training & inference paradigm comparison for conventional setting (A) and LLaVolta (B). Meta framework of LLaVolta consists three training stages: Stage I with heavy visual compression; Stage II with light visual compression in deeper layer; Stage III with subtle compression with wider token window without loss of performance. This can accelerate the training and inference by 18+% while maintaining performance.", "description": "This figure illustrates the difference between the traditional approach of training and inference for multi-modal LLMs and the proposed LLaVolta method.  The traditional approach uses full visual tokens throughout the process. In contrast, LLaVolta introduces a three-stage training scheme with progressively decreasing compression ratios. Stage I starts with heavy compression, Stage II uses light compression in deeper layers, and Stage III uses subtle compression with a wider token window during inference, aiming to improve efficiency without sacrificing performance.", "section": "3.3 LLaVolta as a Light, Staged Training Scheme"}]