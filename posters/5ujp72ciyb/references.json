{"references": [{"fullname_first_author": "J.-B. Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-31", "reason": "This paper introduces Flamingo, a foundational visual language model that heavily influenced the design and development of the current work's multi-modal LLMs."}, {"fullname_first_author": "J. Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-12-31", "reason": "BLIP-2 is a key reference showcasing advanced techniques in multi-modal language-image pre-training, which is directly relevant to the efficient training methods explored in this work."}, {"fullname_first_author": "H. Liu", "paper_title": "LLaVA: A Visual Language Model", "publication_date": "2023-12-31", "reason": "The LLaVA model serves as the primary foundation upon which the proposed methods for efficient multi-modal learning are built and evaluated."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-12-31", "reason": "This paper introduces CLIP, a crucial visual encoder utilized in the current work, providing a strong foundation for visual context compression techniques."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-31", "reason": "This paper introduces the Transformer architecture, a fundamental building block for LLMs which is critical for the understanding of visual context compression strategies."}]}