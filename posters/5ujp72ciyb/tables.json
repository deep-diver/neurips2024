[{"figure_path": "5ujp72CiYB/tables/tables_5_1.jpg", "caption": "Table 1: Instantiations of LLaVolta schemes. deeper indicates that the compressor's position in the LLM shifts from the shallow layer (e.g., 2) to a deeper layer (e.g., 16). wider indicates that the compressor's stride decreases while the number of visual tokens increases. Last stage compression refers to using compressor at last stage for efficient inference.", "description": "This table presents different instantiations of the LLaVolta training scheme.  It shows variations in the number of training stages, the layer where the visual context compressor is applied, the stride of the compressor, the compression ratio, and the number of epochs. The variations illustrate different approaches to progressively reduce compression during training, aiming for optimal training efficiency and performance.", "section": "3.3 LLaVolta as a Light, Staged Training Scheme"}, {"figure_path": "5ujp72CiYB/tables/tables_5_2.jpg", "caption": "Table 1: Instantiations of LLaVolta schemes. deeper indicates that the compressor's position in the LLM shifts from the shallow layer (e.g., 2) to a deeper layer (e.g., 16). wider indicates that the compressor's stride decreases while the number of visual tokens increases. Last stage compression refers to using compressor at last stage for efficient inference.", "description": "This table presents various instantiations of the LLaVolta training schemes.  It shows different configurations for the number of stages, the layer at which the compressor is applied, the stride of the compressor, the compression ratio (CR), and the number of epochs for each stage. The table illustrates how different approaches to applying visual context compression during training can lead to various training efficiency and performance tradeoffs.", "section": "3.3 LLaVolta as a Light, Staged Training Scheme"}, {"figure_path": "5ujp72CiYB/tables/tables_7_1.jpg", "caption": "Table 2: Performance of LLaVolta. See the definition of each training scheme in Tab. 1. \u2020: average across stages. First five derived schemes for training acceleration achieve competitive results while reducing 16% training time. The last scheme, last stage compression, achieved the shortest training time (12.4 hours) and the lowest inference cost (5.47 TFLOPs), but also the highest average performance (62.1%). We report average results across three runs, with the standard deviation written at the bottom right of the average result. The last stage compression training achieves the best average performance across thirteen benchmarks, outperforming the baseline (LLaVA-v1.5-7B) while requiring significantly less training time.", "description": "This table presents the performance of different LLaVolta training schemes across thirteen benchmarks.  It shows that several schemes achieve competitive results while reducing training time significantly. The best performing scheme is \"last stage compression\", which achieves the highest average performance while requiring the shortest training time and lowest inference cost.", "section": "4.3 Main Results: LLaVolta"}, {"figure_path": "5ujp72CiYB/tables/tables_7_2.jpg", "caption": "Table 3: Comparison among different visual compressors. Higher values are preferred. All methods except VCC are set to the compression ratio of 556% to approximate VCC's 514% [53] for a fair comparison. The best scores are marked as gray and the second best are underlined. Attention-based compressors (i.e., FastV and VCC) excel during the inference phase, yet their application to the training phase proves challenging. Average pooling shows a more stable performance during the training phase.", "description": "This table compares the performance of different visual compression methods (random dropping, K-Means, FastV, VCC, and average pooling) used within a multi-modal language model (MLLM).  The comparison includes both inference and training phases.  It highlights the trade-offs between attention-based methods (generally better inference, but unstable training) and average pooling (more stable training, competitive inference).", "section": "4.4 Ablation Study"}, {"figure_path": "5ujp72CiYB/tables/tables_8_1.jpg", "caption": "Table 4: Parametric vs. nonparametric visual compressor. We follow miniGPT-4 [54] that uses Q-Former pre-trained from BLIP-2 [23] as the parametric compressor (All other aspects are maintained as in LLaVA to ensure a fair comparison). Ours: pooling with stride 64 on LLM layer 1 to ensure comparable CRs. Our nonparametric compressor outshines the parametric Q-Former counterpart in terms of both performance and training efficiency.", "description": "This table compares the performance of different visual compressors, including a parametric compressor (Q-Former) and a non-parametric compressor (average pooling).  The results demonstrate that the proposed non-parametric approach outperforms the parametric method in terms of both performance and training efficiency, even when using a higher compression ratio.", "section": "4.4 Ablation Study"}, {"figure_path": "5ujp72CiYB/tables/tables_8_2.jpg", "caption": "Table 2: Performance of LLaVolta. See the definition of each training scheme in Tab. 1. \u2020: average across stages. First five derived schemes for training acceleration achieve competitive results while reducing 16% training time. The last scheme, last stage compression, achieved the shortest training time (12.4 hours) and the lowest inference cost (5.47 TFLOPs), but also the highest average performance (62.1%). We report average results across three runs, with the standard deviation written at the bottom right of the average result. The last stage compression training achieves the best average performance across thirteen benchmarks, outperforming the baseline (LLaVA-v1.5-7B) while requiring significantly less training time.", "description": "This table presents the performance of different LLaVolta training schemes across various benchmarks.  It highlights the trade-off between training time, inference cost, and model performance, showcasing the effectiveness of the proposed LLaVolta approach in achieving competitive results with significantly reduced training time and cost.", "section": "4.3 Main Results: LLaVolta"}, {"figure_path": "5ujp72CiYB/tables/tables_8_3.jpg", "caption": "Table 2: Performance of LLaVolta. See the definition of each training scheme in Tab. 1. \u2020: average across stages. First five derived schemes for training acceleration achieve competitive results while reducing 16% training time. The last scheme, last stage compression, achieved the shortest training time (12.4 hours) and the lowest inference cost (5.47 TFLOPs), but also the highest average performance (62.1%). We report average results across three runs, with the standard deviation written at the bottom right of the average result. The last stage compression training achieves the best average performance across thirteen benchmarks, outperforming the baseline (LLaVA-v1.5-7B) while requiring significantly less training time.", "description": "This table presents the performance of different LLaVolta training schemes on thirteen benchmark datasets.  It compares metrics like training time, inference cost (TFLOPs), and accuracy across various multi-modal understanding tasks. The table highlights the trade-off between training efficiency and performance, showcasing the superior performance of the 'last stage compression' scheme which achieves the highest average performance with significantly reduced training time and inference cost.", "section": "4.3 Main Results: LLaVolta"}, {"figure_path": "5ujp72CiYB/tables/tables_9_1.jpg", "caption": "Table 2: Performance of LLaVolta. See the definition of each training scheme in Tab. 1. \u2020: average across stages. First five derived schemes for training acceleration achieve competitive results while reducing 16% training time. The last scheme, last stage compression, achieved the shortest training time (12.4 hours) and the lowest inference cost (5.47 TFLOPs), but also the highest average performance (62.1%). We report average results across three runs, with the standard deviation written at the bottom right of the average result. The last stage compression training achieves the best average performance across thirteen benchmarks, outperforming the baseline (LLaVA-v1.5-7B) while requiring significantly less training time.", "description": "This table presents the performance comparison of different LLaVolta training schemes (single-stage, two-stage, three-stage, etc.) across various multi-modal benchmarks. It shows that the last-stage compression training scheme achieves the best performance with the lowest training time and inference cost, outperforming the baseline while reducing training time by 16%.", "section": "4.3 Main Results: LLaVolta"}, {"figure_path": "5ujp72CiYB/tables/tables_9_2.jpg", "caption": "Table 2: Performance of LLaVolta. See the definition of each training scheme in Tab. 1. \u2020: average across stages. First five derived schemes for training acceleration achieve competitive results while reducing 16% training time. The last scheme, last stage compression, achieved the shortest training time (12.4 hours) and the lowest inference cost (5.47 TFLOPs), but also the highest average performance (62.1%). We report average results across three runs, with the standard deviation written at the bottom right of the average result. The last stage compression training achieves the best average performance across thirteen benchmarks, outperforming the baseline (LLaVA-v1.5-7B) while requiring significantly less training time.", "description": "This table presents the performance comparison of different LLaVolta training schemes across 13 benchmarks.  It shows that LLaVolta significantly reduces training time and inference costs while improving or maintaining performance compared to the baseline LLaVA-v1.5-7B model.", "section": "4.3 Main Results: LLaVolta"}, {"figure_path": "5ujp72CiYB/tables/tables_13_1.jpg", "caption": "Table 9: Effects of non-uniform stage splitting at the two-stage set-up. Performance decreases as the proportion of Stage 2 decreases, albeit at the expense of lower compression ratios.", "description": "This table shows the results of experiments using a two-stage training scheme with varying proportions of training time allocated to each stage.  The results demonstrate that model performance decreases as the proportion of training time dedicated to the second stage (which uses a lower compression ratio) decreases.  This suggests a trade-off between training efficiency and performance.", "section": "A.1 Non-uniform Stage Splitting"}, {"figure_path": "5ujp72CiYB/tables/tables_13_2.jpg", "caption": "Table 10: Training struturally distinct MLLMs with LLaVolta.Comparison of our method with the Mini-Gemini (MGM-2B) baseline, which uses a multi-resolution visual encoding strategy. Our approach demonstrates competitive performance while reducing training time by 18% (18.1 hours vs. 14.8 hours) and achieving higher scores. This ablation highlights LLaVolta's ability to adapt to different model structures and sophisticated visual encoding strategies.", "description": "This table compares the performance of the proposed LLaVolta method against a Mini-Gemini baseline on various multi-modal benchmarks.  It highlights the ability of LLaVolta to achieve comparable or even better results with reduced training time, demonstrating its adaptability to different model architectures.", "section": "4.4 Ablation Study"}]