[{"figure_path": "RSs4o7CSqe/figures/figures_1_1.jpg", "caption": "Figure 1: The conditions selection statistics during the sampling process of the LLVIP dataset. The distinct process of sampling has different favor of the conditions. The crucial role that diverse conditions play in controlling various image generation processes. Throughout the diffusion sampling, different conditions are dynamically selected to best suit the generation requirement at each stage.", "description": "This figure shows the selection frequency of different conditions during the sampling process of the LLVIP dataset. The x-axis represents the sampling steps, and the y-axis represents the conditions.  The heatmap shows that different conditions are selected with different frequencies at different stages of the sampling process. In the initial stages, random noise influences condition selection. In intermediate stages, content components are emphasized. Finally, texture details are prioritized. This dynamic condition selection ensures that the fusion process remains responsive to the specific requirements at each stage.", "section": "4 Method"}, {"figure_path": "RSs4o7CSqe/figures/figures_3_1.jpg", "caption": "Figure 2: Illustrates the pipeline of the proposed CCF. The framework comprises two components: a sampling process utilizing a pre-trained DDPM and a condition bank with SCS.", "description": "The figure illustrates the overall architecture of the proposed Conditional Controllable Fusion (CCF) framework. It shows how the framework combines a pre-trained Denoising Diffusion Probabilistic Model (DDPM) with a condition bank that uses a sampling-adaptive condition selection (SCS) mechanism. The input consists of multi-source images (e.g., visible and infrared images). The DDPM is used as the core for image generation, and conditions from the condition bank are progressively injected into the sampling process. The condition bank contains basic conditions (e.g., MSE, high-frequency, low-frequency), enhanced conditions (e.g., SSIM, entropy, spatial frequency), and task-specific conditions. The SCS mechanism dynamically selects the optimal condition set based on the current sampling stage. The output of the process is the fused image, which is conditionally controllable.", "section": "4 Method"}, {"figure_path": "RSs4o7CSqe/figures/figures_4_1.jpg", "caption": "Figure 3: Qualitative comparisons of our CCF and the competing methods on VIF fusion task of LLVIP dataset.", "description": "This figure displays a qualitative comparison of various image fusion methods applied to visible-infrared (VIF) fusion tasks using the LLVIP dataset.  It visually demonstrates the results of different approaches including Swin Fusion, DIVFusion, MUFusion, CDD, DDFM, Text-IF, TC-MoA and the proposed CCF method. The comparison highlights visual differences in terms of detail preservation, noise reduction, and overall image quality.", "section": "5.1 Evaluation on Multi-Modal Image Fusion"}, {"figure_path": "RSs4o7CSqe/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative comparisons of various methods in MFF task from MFFW dataset.", "description": "This figure shows a qualitative comparison of different image fusion methods on multi-focus fusion tasks using the MFFW dataset.  The top row displays the source images (Source 1 and Source 2), while subsequent rows showcase results from U2Fusion, DeFusion, DDFM, Text-IF, TC-MoA, and the proposed CCF method.  The green and red boxes highlight specific regions where the methods' relative strengths and weaknesses are apparent.  Overall, the figure visually demonstrates the ability of the CCF method to preserve details and textures compared to alternative approaches.", "section": "5.2 Evaluation on Multi-Focus Fusion"}, {"figure_path": "RSs4o7CSqe/figures/figures_7_1.jpg", "caption": "Figure 5: The visualization of the w/o and with task-specific conditions.", "description": "This figure shows a comparison of object detection results on fused images with and without using task-specific conditions. The top row displays the results without task-specific conditions, showing missed detections (highlighted in cyan boxes). The bottom row shows improved detection results obtained by incorporating task-specific conditions, accurately identifying objects that were missed in the top row. This visually demonstrates the benefit of integrating task-specific conditions to enhance performance on downstream tasks like object detection.", "section": "5.4 Task-specific Conditions"}, {"figure_path": "RSs4o7CSqe/figures/figures_8_1.jpg", "caption": "Figure 10: Qualitative Comparisons of w/o and with the detection condition.", "description": "This figure compares qualitative results of image fusion with and without using a detection condition. It shows that adding the detection condition improves the results by better highlighting important details, likely objects detected by the model.", "section": "5.4 Task-specific Conditions"}, {"figure_path": "RSs4o7CSqe/figures/figures_14_1.jpg", "caption": "Figure 7: Qualitative comparisons of our CCF and the competing methods on VIF fusion task of TNO dataset", "description": "This figure presents a qualitative comparison of various image fusion methods on a visible-infrared image fusion (VIF) task, using the TNO dataset.  It showcases the results of different methods side-by-side, allowing for a visual assessment of their performance in terms of detail preservation, noise reduction, and overall image quality.  The methods compared include DenseFuse, RFN-Nest, UMF-CMGR, YDTR, U2Fusion, DeFusion, DDFM, and the proposed CCF method.  The comparison highlights the strengths and weaknesses of each approach in terms of preserving fine details in various regions of the image.  Bounding boxes are used to highlight specific areas where differences are more easily apparent, enabling a better understanding of the relative performance of each technique.", "section": "5.1 Evaluation on Multi-Modal Image Fusion"}, {"figure_path": "RSs4o7CSqe/figures/figures_15_1.jpg", "caption": "Figure 8: Framework of our neural network architecture.", "description": "The figure shows the architecture of the neural network used in the proposed CCF framework. It's a U-Net-like architecture with six layers, each processing different resolutions (32x32, 16x16, and 8x8). The architecture incorporates skip connections between layers, likely for better gradient flow and feature integration.  It also includes \"down stream\" and \"down and right stream\" components, suggesting different pathways for processing feature information. The sequential arrangement of the layers, along with skip connections, and the multiple streams is typical of image processing networks, which are commonly used for feature extraction and fusion tasks.", "section": "4 Method"}, {"figure_path": "RSs4o7CSqe/figures/figures_16_1.jpg", "caption": "Figure 9: Example of enhanced conditions selection in denoising iteration for rapidly changing scenarios.", "description": "This figure shows how the sampling-adaptive condition selection (SCS) mechanism dynamically chooses enhanced conditions during the denoising process.  The top panel illustrates a scenario with smoothly changing conditions, while the bottom shows a scenario with rapidly changing conditions. Different conditions (SSIM, MSE, SD, Edge, High-Frequency, Spatial Frequency) are selected at different stages depending on the image generation needs. This adaptive selection ensures the fusion process remains responsive to changing environmental conditions.", "section": "4.3 Sampling-adaptive Condition Selection"}, {"figure_path": "RSs4o7CSqe/figures/figures_17_1.jpg", "caption": "Figure 10: Qualitative Comparisons of w/o and with the detection condition.", "description": "This figure presents a qualitative comparison of image fusion results with and without the integration of a detection condition.  The left-hand side shows fusion results without the detection condition, while the right-hand side shows results with the detection condition incorporated. Each row represents a different image pair undergoing fusion.  The red bounding boxes highlight objects detected in the images, demonstrating how the inclusion of the detection condition improves the accuracy and detail of object detection in the fused images. This visual comparison effectively illustrates how task-specific conditions can enhance the quality and contextual relevance of image fusion.", "section": "5.4 Task-specific Conditions"}, {"figure_path": "RSs4o7CSqe/figures/figures_18_1.jpg", "caption": "Figure 11: Visualization of medical image fusion.", "description": "This figure shows the results of applying the proposed CCF method to medical image fusion tasks. It presents three examples of multimodal image fusion: MRI-SPECT, MRI-CT, and MRI-PET. For each example, the figure displays the input images (MRI, SPECT, CT, or PET), and the corresponding fused image produced by CCF. The fused images demonstrate the ability of CCF to effectively integrate complementary information from different modalities to produce a more comprehensive representation of the underlying anatomy.", "section": "H Experiments on Medical Image Fusion"}]