[{"type": "text", "text": "Masked Hard-Attention Transformers Recognize Exactly the Star-Free Languages ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andy Yang David Chiang Dana Angluin University of Notre Dame University of Notre Dame Yale University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The expressive power of transformers over inputs of unbounded size can be studied through their ability to recognize classes of formal languages. In this paper, we establish exact characterizations of transformers with hard attention (in which all attention is focused on exactly one position) and attention masking (in which each position only attends to positions on one side). With strict masking (each position cannot attend to itself) and without position embeddings, these transformers are expressively equivalent to linear temporal logic (LTL), which defines exactly the star-free languages. A key technique is the use of Boolean RASP as a convenient intermediate language between transformers and LTL. We then take numerous results known for LTL and apply them to transformers, showing how position embeddings, strict masking, and depth all increase expressive power. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Significant progress has been made in the last few years on characterizing the expressivity of transformers (Vaswani et al., 2017) in terms of well-understood classes of formal languages (Strobl et al., 2024b). Results have been obtained for a wide range of variants of transformers, and nearly all take the form of either upper bounds (transformers recognize only languages in class $C$ ) or lower bounds (transformers recognize all languages in class $C$ ). In this paper, we establish exact characterizations of transformers with hard attention (in which all attention is focused on exactly one position) and attention masking (in which each position $i$ only attends to positions on one side of $i$ ). ", "page_idx": 0}, {"type": "text", "text": "With strict masking (in which each position cannot attend to itself) and without position embeddings, these transformers recognize exactly the class of star-free regular languages. The left side of Figure 1 summarizes our results relating masked hard-attention transformers and linear temporal logic (LTL), which defines exactly the star-free regular languages. ", "page_idx": 0}, {"type": "text", "text": "A key technique in these proofs is the use of B-RASP, which, like RASP (Weiss et al., 2021), is a small programming language that compiles into transformers. B-RASP is restricted to Boolean values and compiles to masked hard-attention transformers. Additionally, a masked hard-attention transformer can be decompiled back to a B-RASP program. We use B-RASP as an intermediate language between transformers and LTL. ", "page_idx": 0}, {"type": "text", "text": "The equivalence of masked hard-attention transformers with LTL (and other equivalent characterizations, like counter-free automata and first-order logic) enables us to take numerous results known for LTL and apply them to transformers, as shown on the right side of Figure 1: ", "page_idx": 0}, {"type": "text", "text": "\u2022 Strict future-masked rightmost-hard attention is sufficient; adding past-masked, non-masked, and/or leftmost-hard attention does not increase expressivity (Section 5.1). \u2022 Strict masking is important (Section 5.2); without it, masked hard-attention transformers are less expressive, recognizing only the stutter-invariant star-free languages. ", "page_idx": 0}, {"type": "text", "text": "masked hard-attention transformers ", "page_idx": 1}, {"type": "image", "img_path": "FBMsBdH0yz/tmp/56201da3d0ddea35da1fd7994165dcdc1d785eac07769718da7e8002de0f3e13.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Overview of results in this paper. One-way arrows denote strict inclusion; two-way arrows denote equivalence. $\\mathrm{PE}=$ position embedding. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Adding position embeddings increases the class of recognized languages to other wellstudied classes (Section 5.3); for example: \u2013 With rational sinusoidal position embeddings, masked hard-attention transformers recognize exactly the regular languages in $\\mathbf{AC}^{0}$ . \u2013 With arbitrary finite-image position embeddings, they are equivalent to LTL[Mon] (linear temporal logic with arbitrary monadic predicates).   \n\u2022 Adding more layers always increases expressive power (Section 5.4). ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $\\Sigma$ be a finite alphabet, and let $w=w_{1}\\cdot\\cdot\\cdot w_{n}$ be an input string of length $n$ , where each $w_{i}\\in\\Sigma$ . Throughout, we assume that $w$ is not empty. We write $\\Sigma^{+}$ for the set of all non-empty strings over $\\Sigma$ . (We disallow empty strings because several formalisms used here require a designated position where an accept/reject decision appears. Adding a BOS or EOS token not in $\\Sigma$ for this purpose would make it possible to handle the empty string.) We write $[n]$ for the set $\\{1,\\ldots,n\\}$ . ", "page_idx": 1}, {"type": "text", "text": "The star-free regular languages are the closure of \u2205, $\\{\\epsilon\\}$ , and $\\{\\sigma\\}$ for each $\\sigma\\textbf{\\in}\\Sigma$ , under the operations of union, concatenation, and complementation. For example: ", "page_idx": 1}, {"type": "text", "text": "\u2022 $\\Sigma^{*}$ is star-free because $\\Sigma^{*}=\\varnothing^{\\complement}$ .   \n\u2022 $(a b)^{*}$ is star-free because $(a b)^{*}=(b\\Sigma^{*}\\cup\\Sigma^{*}a\\cup\\Sigma^{*}a a\\Sigma^{*}\\cup\\Sigma^{*}b b\\Sigma^{*})^{\\mathsf{c}}.$ \u2022 $(a a)^{*}$ is regular but not star-free. ", "page_idx": 1}, {"type": "text", "text": "This class of languages has several other characterizations, including counter-free automata (Appendix B.5), first-order logic with order (McNaughton and Papert, 1971), and linear temporal logic (Kamp, 1968), which is what we will focus on in this paper. ", "page_idx": 1}, {"type": "text", "text": "2.2 Transformer variants ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The original transformer (Vaswani et al., 2017), designed for machine translation, had both an encoder and a decoder. In practice, both encoder-only models like BERT (Devlin et al., 2019) and decoderonly models like GPT (Brown et al., 2020) are common. Like much previous work on transformer expressivity (e.g. Hahn, 2020), we study an encoder-only setup, where the input is a string and the output is a binary classification; but our results could easily be adapted to a decoder-only setting where the input is a prefix and the output is the next symbol. ", "page_idx": 1}, {"type": "text", "text": "The transformers studied here use unique hard attention (or simply hard attention), in which an attention head focuses all attention on the position with the highest score, with ties broken to the left or right. Although this is different from the soft attention in actual transformers, theoretical studies unavoidably involve models of the real objects of study, and we are using unique-hard attention as a stepping-stone towards understanding real transformers. However, unique-hard attention may be more appropriate than it appears: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Real transformers are often observed to focus attention on a very small number of positions (Merrill et al., 2021). On Dyck languages, they have been found to learn effectively uniquehard attention in their second layer (Ebrahimi et al., 2020, Figure 1).   \n\u2022 There exist soft-attention transformers that compute parity (Chiang and Cholak, 2022), but in practice, transformers cannot learn parity (Bhattamishra et al., 2020). Unique-hard attention transformers also cannot compute parity (Hahn, 2020), so they are in some sense more realistic.   \n\u2022 Hard attention has occasionally been used in practice in previous research on interpretability (Kinley, 2020) and efficiency (Gupta et al., 2021; Xu et al., 2021). ", "page_idx": 2}, {"type": "text", "text": "In this paper, we use future masking, in which every position may only attend to positions to its left. This kind of masking is common in decoder-only models and has been studied in encoder-only models as well (Bhattamishra et al., 2020). We also consider past masking (Yao et al., 2021). ", "page_idx": 2}, {"type": "text", "text": "2.3 Previous work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Pe\u00b4rez et al. (2021) show that average-hard attention transformer encoder\u2013decoders, where the decoder runs for a polynomial number of steps before accepting or rejecting a string, recognize all of $\\mathbf{P}$ (that is, all languages decidable by a deterministic Turing machine in polynomial time). Merrill and Sabharwal (2024) prove another version of this result, and further observe that all such transformers are in P. This result is the only other exact characterization of any transformer variant that we are aware of. ", "page_idx": 2}, {"type": "text", "text": "Hao et al. (2022) show that (non-masked) hard-attention transformer encoders with arbitrary position embeddings have an upper bound of $\\mathbf{AC}^{0}$ (that is, languages defined by circuit families with polynomial size, unbounded fan-in, and bounded depth), and Barcel\u00b4o et al. (2024) show that they have a lower bound of LTL[Mon], which is linear temporal logic with all possible monadic numerical predicates. They leave open the question of whether these transformers are equivalent to LTL[Mon]\u2014a question which, with suitable adjustments, we answer here in the affirmative. ", "page_idx": 2}, {"type": "text", "text": "3 Boolean RASP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "RASP (Weiss et al., 2021) is a programming language intended to help programmers \u201cthink like transformers.\u201d It has the same basic operations as transformers, but it is easier to compose these operations in RASP than to write transformers by hand. Variants of RASP have been used fruitfully to study transformers\u2019 length-generalization capabilities (Zhou et al., 2024) and expressive power (Strobl et al., $2024\\mathrm{a}$ ; Yang and Chiang, 2024). In this section, we define a version of RASP restricted to Boolean values, which we call Boolean RASP or B-RASP. As we will see, it can be compiled into masked hard-attention transformers, and masked hard-attention transformers can be decompiled back into B-RASP. We use it as an intermediate language between transformers and LTL, and find it more convenient to work with than either of them. ", "page_idx": 2}, {"type": "text", "text": "3.1 Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The input to a B-RASP program is a string $w=w_{1}\\cdot\\cdot\\cdot w_{n}\\in\\Sigma^{+}$ . There is one type of data, a Boolean vector, which is a vector of Boolean values indexed by $i\\in[n]$ . The initial Boolean vectors are $Q_{\\sigma}$ for each $\\sigma\\in\\Sigma$ , where $Q_{\\sigma}(i)=1$ iff $w_{i}=\\sigma$ . ", "page_idx": 2}, {"type": "text", "text": "A B-RASP program is a sequence of operations that compute new Boolean vectors. Although they may have descriptive names, and names may be reused, here, to streamline definitions and proofs, we assume that all the Boolean vectors are numbered consecutively. That is, $P_{1},\\dots,P_{|\\Sigma|}$ are the initial Boolean vectors $Q_{\\sigma}$ for $\\sigma\\in\\Sigma$ , and the Boolean vectors computed by the program are numbered ", "page_idx": 2}, {"type": "table", "img_path": "FBMsBdH0yz/tmp/5410a87befa4f8c48a1f758ab3d995090d74bcda7adf940a93ce46c8e16e7bd3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Examples related to $L_{1,2}$ (Dyck-1 of depth 2). The left bracket is $\\ell$ and the right bracket is $r$ starting from $P_{|\\Sigma|+1}$ without repetition. After the first $t$ vectors, vector $P_{t+1}$ is computed using one of the following operations. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Position-wise operations. $P_{t+1}(i)$ can be be computed by $P_{t+1}(i):=R(i)$ , where $R(i)$ is a Boolean combination of zero or more of $\\{P_{1}(i),\\ldots,P_{t}(i)\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Attention operations. $P_{t+1}(i)$ can be computed by either of ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{t+1}(i):=\\bullet_{j}\\left[M(i,j),S(i,j)\\right]\\;V(i,j):D(i)}\\\\ &{P_{t+1}(i):=\\bullet_{j}\\left[M(i,j),S(i,j)\\right]\\;V(i,j):D(i)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where: ", "page_idx": 3}, {"type": "text", "text": "\u2022 $M(i,j)$ , the mask predicate, is one of $M(i,j)=1$ (no masking), $M(i,j)=(j<i)$ (strict future masking), or $M(i,j)=(j>i)$ (strict past masking).   \n\u2022 $S(i,j)$ , the score predicate, is a Boolean combination of zero or more atomic formulas from $\\{P_{1}(i),\\ldots,P_{t}(i)\\}\\cup\\{P_{1}(j),\\ldots,P_{t}(j)\\}$ .   \n\u2022 $V(i,j)$ , the value predicate, has the same form as the score predicate.   \n\u2022 $D(i)$ , the default value predicate, is a Boolean combination of zero or more atomic formulas from $\\{P_{1}(i),\\ldots,P_{t}(i)\\}$ . ", "page_idx": 3}, {"type": "text", "text": "For each $i\\in[n]$ , let $j_{i}$ be the minimum (if the operator is $\\blacktriangleleft$ ) or maximum (if $\\blacktriangleright$ ) value of $j\\in[n]$ such that $M(i,j)=1$ and $S(i,j)=1$ . If $j_{i}$ exists, then $P_{t+1}(i)=V(i,j_{i})$ . If $j_{i}$ does not exist, then $P_{t+1}(i)=D(i)$ . ", "page_idx": 3}, {"type": "text", "text": "If $P$ is a Boolean vector computed by program $\\mathcal{P}$ , we write $w\\v{i}=P(i)$ just in case $P(i)=1$ when $\\mathcal{P}$ is run on input string $w$ . To make a B-RASP program $\\mathcal{P}$ recognize a language, one Boolean vector $Y$ is designated the output vector, and position $n$ is designated the output position. Then, the input string $w$ is accepted iff $w\\models Y(n)$ . To make a B-RASP program compute a length-preserving sequence-to-sequence function from $\\Sigma^{+}$ to $\\Gamma^{+}$ , we designate a collection of output Boolean vectors $Y_{\\gamma}$ indexed by the symbols $\\gamma\\in\\Gamma$ , and consider the output at position $i$ to be $\\gamma$ iff $Y_{\\gamma}(i)$ is true. ", "page_idx": 3}, {"type": "text", "text": "3.2 Example: Dyck-1 of depth 2 ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As an example, we consider the Dyck language with 1 pair of parentheses, limited to depth 2, or $L_{1,2}$ for short. It is recognized by the DFA in Figure 2a, where $\\ell$ and $r$ are left and right brackets. We show how to define this language in B-RASP, with a construction very similar to that of Yao et al. (2021). ", "page_idx": 3}, {"type": "text", "text": "Consider the input string $\\ell\\ell r r\\ell\\ell r\\ell r r$ , which should be accepted. The basic idea is to identify brackets that are immediately matched $(\\mathcal{\\ell}\\underline{{\\ell}}r r\\mathcal{\\ell}\\underline{{\\ell}}r\\ell r r)$ , then look at the remaining brackets $(\\underline{{\\ell}}\\ell\\ r\\underline{{r}}\\ell\\ r\\ell r\\underline{{r}}r)$ to make sure they are matched. We describe the B-RASP program for this problem below; the resulting Boolean vectors are shown in Figure 2b. ", "page_idx": 3}, {"type": "text", "text": "We first construct Boolean vectors $P_{\\ell}(i)$ and $S_{r}(i)$ that indicate whether the predecessor (respectively, successor) symbol of $i$ is $\\ell$ (respectively, $r$ ). This is done with attention operations: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{\\ell}(i):=\\star_{j}\\,[j<i,1]\\ Q_{\\ell}(j):0}\\\\ {S_{r}(i):=\\bullet_{j}\\,[j>i,1]\\ Q_{r}(j):0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Vector $P_{\\ell}(i)$ makes position $i$ attend to the position immediately to its left, and its value predicate $Q_{\\ell}(j)$ tests whether that position has an $\\ell$ . Vector $S_{r}$ is similar. ", "page_idx": 4}, {"type": "text", "text": "The Boolean vector $I(i)$ indicates whether position $i$ is in a consecutive pair $\\ell r$ , that is, whether it is immediately matched: ", "page_idx": 4}, {"type": "equation", "text": "$$\nI(i):=(Q_{\\ell}(i)\\wedge S_{r}(i))\\vee(Q_{r}(i)\\wedge P_{\\ell}(i)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The Boolean vectors $B_{\\ell}(i)$ and $A_{r}(i)$ test if the symbol before (respectively, after) $i$ that is not immediately matched is $\\ell$ (respectively, $r$ ). Then $C$ checks each position $i$ to see if it is immediately matched, or it has $\\ell$ and the following not-immediately-matched symbol is $r$ , or it has $r$ and the preceding not-immediately-matched symbol is $\\ell$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{\\ell}(i):=\\star_{j}\\left[j<i,\\lnot I(j)\\right]\\,Q_{\\ell}(j):0}\\\\ &{A_{r}(i):=\\bullet_{j}\\left[j>i,\\lnot I(j)\\right]\\,Q_{r}(j):0}\\\\ &{\\,\\,C(i):=I(i)\\lor(Q_{\\ell}(i)\\land A_{r}(i))\\lor(Q_{r}(i)\\land B_{\\ell}(i)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, the output Boolean vector $Y$ tests if $C(i)$ is true everywhere: ", "page_idx": 4}, {"type": "equation", "text": "$$\nY(i):=\\star_{j}\\ [1,\\neg C(j)]\\ 0:1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Boolean vectors for deciding non-membership of $\\ell r r\\ell\\ell\\ell r r r\\ell$ in $L_{1,2}$ are shown in Figure 2c. It is straightforward to generalize this technique to recognize Dyck- $k$ of depth $D$ in B-RASP.1 For another example program for an associative recall task, please see Appendix A. A B-RASP simulator that allows one to write and run additional examples can be found at https://b-rasp.github.io/. ", "page_idx": 4}, {"type": "text", "text": "3.3 Normal forms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In B-RASP, the value predicate $V(i,j)$ depends on both $i$ (the query position) and $j$ (the key/value position), but in actual transformers, it depends on $j$ only. The dependence on $i$ is sometimes convenient, but it does not change expressivity (see Appendix B.1). ", "page_idx": 4}, {"type": "text", "text": "The score predicate $S(i,j)$ depends on both $i$ and $j$ in both B-RASP and actual transformers. Perhaps surprisingly, in B-RASP, it too can be made to depend only on $j$ without reducing expressivity, but as a tradeoff the program may become exponentially larger in size (see Appendix B.2). ", "page_idx": 4}, {"type": "text", "text": "3.4 Equivalence with linear temporal logic ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We prove that B-RASP recognizes exactly the star-free languages, by proving that B-RASP is equivalent to linear temporal logic. Appendix B.5 gives another proof of the star-free-to-B-RASP direction via counter-free automata. ", "page_idx": 4}, {"type": "text", "text": "In linear temporal logic or LTL (Kamp, 1968), every formula implicitly depends on a single \u201ctime\u201d (or position). The atomic formulas are $Q_{\\sigma}$ for every $\\sigma\\in\\Sigma$ , and we have the usual connectives \u2227, \u2228, and $\\neg$ , as well as operators since and until.2 For any input string $w=w_{1}\\cdot\\cdot\\cdot w_{n}$ and position $i\\in[n]$ , ", "page_idx": 4}, {"type": "text", "text": "we define $w,i\\Vdash\\phi$ as follows: ", "page_idx": 5}, {"type": "text", "text": "$\\begin{array}{r l}&{w,i\\models Q_{\\sigma}}\\\\ &{w,i\\models\\phi_{1}\\land\\phi_{2}}\\\\ &{w,i\\models\\phi_{1}\\lor\\phi_{2}}\\\\ &{w,i\\models\\neg\\phi_{1}}\\\\ &{w,i\\models\\phi_{1}\\ \\mathbf{since}\\ \\phi.}\\end{array}$ if $w_{i}=\\sigma$ if $w,i\\models\\phi_{1}$ and $w,i\\Vdash\\phi_{2}$ $\\mathrm{if}\\;w,i\\models\\phi_{1}\\;\\mathrm{or}\\;w,i\\models\\phi_{2}$ if $w,i\\not\\in\\phi_{1}$ 2 if for some $j<i$ , we have $w,j\\vert=\\phi_{2}$ , and for all $k$ such that $j<k<i$ , we have $w,k\\vDash\\phi_{1}$   \n\ud835\udc64, \ud835\udc56|= \ud835\udf191 until \ud835\udf192 if for some $j>i$ , we have $w,j\\vert=\\phi_{2}$ , and for all $k$ such that $i<k<j$ , we have $w,k\\vDash\\phi_{1}$ . ", "page_idx": 5}, {"type": "text", "text": "To use a formula $\\phi$ of LTL to define a language over $\\Sigma$ , for an input string $w\\in\\Sigma^{+}$ of length $n$ we designate the last position as the output position, so that $w\\in\\mathcal{L}(\\phi)$ if and only if $w,n\\models\\phi$ . ", "page_idx": 5}, {"type": "text", "text": "For example, let $\\Sigma=\\{a,b,\\#\\}$ and consider the following formulas: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{1}=Q_{\\#}}\\\\ &{\\phi_{2}=Q_{\\#}\\wedge(Q_{b}\\ \\mathbf{since}\\ Q_{\\#})}\\\\ &{\\phi_{3}=Q_{\\#}\\wedge(Q_{b}\\ \\mathbf{since}\\ (Q_{\\#}\\wedge(Q_{a}\\ \\mathbf{since}\\ Q_{\\#})))}\\\\ &{\\phi_{4}=Q_{\\#}\\wedge(Q_{b}\\ \\mathbf{since}\\ (Q_{\\#}\\wedge(Q_{a}\\ \\mathbf{since}\\ (Q_{\\#}\\wedge\\lnot(0\\ \\mathbf{since}\\ 1))))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The formula $\\phi_{1}$ defines the language $\\Sigma^{*}\\#$ , which contains all and only strings with a # in the last position. The formula $\\phi_{2}$ defines the language $\\Sigma^{*}\\#b^{*}\\#$ , and $\\phi_{3}$ defines the language $\\Sigma^{*}\\#a^{*}\\#b^{*}\\#$ Finally, $\\phi_{4}$ defines the language $\\#a^{*}\\#b^{*}\\#$ , because \u00ac(0 since 1) is only true at the first position. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. For any formula of LTL that defines a language $L\\subseteq\\Sigma^{+}$ , there is a B-RASP program that recognizes $L$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. See Appendix B.3. This is shown via direct construction. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. For any B-RASP program that recognizes a language $L\\subseteq\\Sigma^{+}.$ , there is a formula of LTL that defines $L$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. See Appendix B.4. We use the unary normal forms (Section 3.3) to facilitate this proof. \u25a1 ", "page_idx": 5}, {"type": "text", "text": "4 Masked Hard-Attention Transformers ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Definition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A masked hard-attention transformer layer with width $d>0$ is a length-preserving function ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{l a y e r\\colon(\\mathbb{R}^{d})^{+}\\to(\\mathbb{R}^{d})^{+}}}\\\\ {{\\ \\ (x_{1},\\ldots,x_{n})\\mapsto(y_{1},\\ldots,y_{n})}}\\\\ {{\\ (c_{1},\\ldots,c_{n})=a t t(x_{1},\\ldots,x_{n})+(x_{1},\\ldots,x_{n})}}\\\\ {{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ y_{i}=\\!\\!\\!\\!\\!\\iint_{}(c_{i})+c_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\ni=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The self-attention layer att is specified by ", "page_idx": 5}, {"type": "text", "text": "\u2022 A score function, which is a bilinear function $f_{S}\\colon\\ensuremath{\\mathbb{R}}^{d}\\times\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}.$ .   \n\u2022 A mask, which is $M(i,j)=1$ (no masking), $M(i,j)=(j<i)$ (strict future masking), or $M(i,j)=(i<j)$ (strict past masking).   \n\u2022 A tie-breaking function $C$ to select one element of a finite non-empty set $I\\subset\\mathbb{N}_{+}$ , which is either $C(I)=\\operatorname*{min}I$ (choose leftmost position) or $C(I)=\\operatorname*{max}I$ (choose rightmost position).   \n\u2022 A value function, which is a linear transformation $f_{V}\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}^{d}$ . ", "page_idx": 5}, {"type": "text", "text": "The layer works as follows, for each $i\\in[n]$ . Let $\\begin{array}{r l}&{U_{i}=\\{j\\in[n]\\mid M(i,j)=1\\}}\\\\ &{B_{i}=\\{j\\in U_{i}\\mid(\\forall j^{\\prime}\\in U_{i})(f_{S}(x_{i},x_{j^{\\prime}})\\leq f_{S}(x_{i},x_{j}))\\}}\\end{array}$ unmasked positions best-scoring unmasked positions ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "If $U_{i}\\neq\\emptyset$ , let $j_{i}=C(B_{i})$ and output $c_{i}=f_{V}(x_{j_{i}})$ ; but if $U_{i}=\\emptyset$ , output $c_{i}=\\mathbf{0}$ . ", "page_idx": 6}, {"type": "text", "text": "The function ${\\it f}\\!n$ is a feed-forward neural network with 2 layers and ReLU activations in between. Then a masked hard-attention transformer is a length-preserving function ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{T}\\colon\\Sigma^{+}\\to(\\mathbb{R}^{d})^{+}}\\\\ {\\mathcal{T}=l a y e r_{k}\\circ\\cdots\\circ l a y e r_{1}\\circ e m b}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where emb: $\\Sigma^{+}\\,\\rightarrow\\,(\\mathbb{R}^{d})^{+}$ is a position-wise function (a word embedding), and each layer\u2113 is a masked hard-attention transformer layer. ", "page_idx": 6}, {"type": "text", "text": "We write $[\\mathcal{T}(w)]_{i}$ for the final activation value at position $i\\,\\in\\,[n]$ when $\\mathcal{T}$ is run on input $w$ . To use $\\mathcal{T}$ as a language recognizer, we add an output layer, which linearly projects $[\\mathcal{T}(w)]_{n}$ to a scalar. If the result is nonnegative, we accept $w$ ; otherwise, we reject. The exact criterion does not matter much, as the transformers we construct only output $+{\\frac{1}{2}}$ or $-{\\frac{\\bar{1}}{2}}$ , and could easily be changed to another convention. The language recognized by $\\mathcal{T}$ (with the output layer) is the set of strings it accepts. ", "page_idx": 6}, {"type": "text", "text": "Our definition above differs from the standard definition (Vaswani et al., 2017) in a few ways besides unique-hard attention, which was discussed above in Section 2.2. Ours lacks layer normalization and position embeddings, but we add them in Sections 4.3 and 5.3, respectively. We only use single-head attention; multi-head attention can be simulated by summing the outputs of multiple single-head attentions, or it can be added to the definition, as in Appendix D.3.1. Our attention masking is strict, but we consider non-strict masking in Section 5.2. ", "page_idx": 6}, {"type": "text", "text": "4.2 Equivalence with B-RASP ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theorem 3. For any B-RASP program that recognizes a language $L\\ \\subseteq\\ \\Sigma^{+}$ , there is a masked hard-attention transformer (with output layer) that recognizes $L$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. See Appendix C.1. Attention layers simulate attention operations, and FFNs simulate positionwise operations. \u25a1 ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. For any masked hard-attention transformer (with output layer) that recognizes a language $L\\subseteq\\Sigma^{+}$ , there is a B-RASP program that recognizes $L$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. See Appendix C.2. To convert a masked hard-attention transformer to B-RASP, we first show that all of the intermediate values computed by the transformer are drawn from a finite set and therefore can be represented using $O(1)$ bits.3 \u25a1 ", "page_idx": 6}, {"type": "text", "text": "4.3 Layer normalization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Standard transformers (Vaswani et al., 2017) include layer normalization (Ba et al., 2016), but our definition above does not. Since layer normalization is a position-wise function, the proof of Lemma 24 is unaffected. But the construction of Lemma 21 does need to be modified to circumvent layer normalization (cf. Chiang et al., 2023, Proposition 22). Previously, we used 1 to represent true and 0 to represent false; now, we use a pair of activations to represent a truth value, $(1,0)$ for true and $(0,1)$ for false. This ensures that every vector has mean and variance independent of the input $w$ , so we can set the parameters of each layer normalization so that it has no effect. (In the proof of Theorem 3, we use a flag to indicate whether there are any unmasked positions or not. This flag already uses the encoding described above, and does not need to be modified.) ", "page_idx": 6}, {"type": "text", "text": "5 Further Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this final section, we leverage results from temporal logic and the equivalences established above to obtain numerous new results for masked hard-attention transformers (and B-RASP). ", "page_idx": 6}, {"type": "text", "text": "5.1 Asymmetric attention ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our definitions of both B-RASP and masked hard-attention transformers include both leftmost-hard and rightmost-hard attention, and both future and past masking. But we can use the fact that, in LTL, if the output is read out only at the last position, it suffices to have only since and not until (Gabbay et al., 1980) to obtain the following result. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. Both B-RASP and transformers with only future-masked rightmost-hard attention recognize exactly the star-free languages. ", "page_idx": 7}, {"type": "text", "text": "Proof. Any star-free language can be defined in LTL using only since (Gabbay et al., 1980), and restricting Theorem 1 to translate from LTL with only since into B-RASP will only use future-masked \u25b6. Therefore, B-RASP with only future-masked $\\blacktriangleright$ can define any star-free language. Similarly, the translation (Theorem 3) from B-RASP with only future-masked $\\blacktriangleright$ to masked hard-attention transformers only uses future-masked rightmost-hard attention. Therefore, transformers with only future-masked rightmost-hard attention can define any star-free language. \u25a1 ", "page_idx": 7}, {"type": "text", "text": "Note that this applies only in a setting where we accept or reject strings by looking at the output at the last position. It does not apply to other settings, like transduction (Strobl et al., 2024a). ", "page_idx": 7}, {"type": "text", "text": "5.2 Non-strict masking ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our definitions of both B-RASP and masked hard-attention transformers use strict masking, in which a position cannot attend to itself. Standard transformers, however, use non-strict masking. We can modify the definitions to use non-strict masking, that is, $i\\le j$ or $j\\leq i$ . ", "page_idx": 7}, {"type": "text", "text": "Non-strictness is known to reduce expressivity in LTL (Peled and Wilke, 1997), so it reduces expressivity in B-RASP and masked hard-attention transformers as well. Intuitively, non-strict masked operations are unable to distinguish between consecutive positions that have the same symbol. More formally, a language over $\\Sigma$ is called stutter-invariant4 iff for all $u,\\nu\\in\\Sigma^{*}$ and $\\sigma\\in\\Sigma$ , $u\\sigma\\nu\\in L$ iff $u\\sigma\\sigma\\nu\\,\\in\\,L$ . An example of a language that is stutter-invariant star-free is $(a^{+}b^{+})^{*}$ (where $\\sigma^{+}$ means \u201cone or more occurrences of $\\sigma^{\\bullet}$ ); a language that is star-free but not stutter-invariant is $(a b)^{*}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 6. Both B-RASP and masked hard-attention transformers with only non-strict masking recognize exactly the stutter-invariant star-free languages. ", "page_idx": 7}, {"type": "text", "text": "Proof. Peled and Wilke (1997) prove that LTL with non-strict since\u2032 and until\u2032 recognizes exactly the stutter-invariant star-free languages. The proofs of Theorems 1 and 2 may be adapted to use non-strict temporal operators and non-strict masking. Thus, non-strict B-RASP and non-strict LTL are equivalent. Similarly, using $j\\leq i$ or $j\\geq i$ as $M(i,j)$ in the proofs of Theorems 3 and 4, we can show that non-strict masked hard-attention transformers are equivalent to non-strict B-RASP. \u25a1 ", "page_idx": 7}, {"type": "text", "text": "In Section 3.2, we showed how to define $L_{1,2}$ , Dyck-1 of depth 2. Bhattamishra et al. $(2020,\\,\\S7.1)$ find experimentally that $L_{1,2}$ is not learnable by transformers, and they argue that it is not even expressible by transformers (with soft attention, non-strict masking, and no position embeddings). The reason is that while reading the prefix of $\\ell$ \u2019s at the start of the string, the soft-attention layer computes the same value vector at every position and cannot count the number of occurrences of $\\ell$ . However, with the addition of a BOS symbol, soft attention can measure what fraction of symbols are $\\ell$ , overcoming this limitation as observed empirically by Ebrahimi et al. (2020). The similarities between how strict masking in the hard attention setting and the addition of BOS in soft attention both enable positions to be distinguished are notable for future investigation. ", "page_idx": 7}, {"type": "text", "text": "5.3 Position embeddings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our definition of a transformer does not, so far, include position embeddings; all information about ordering comes from attention masking. A position embedding is a family of functions $\\Theta=(\\theta_{n})_{n\\geq0}$ where $\\theta_{n}(i)$ is a scalar or vector representation of position $i$ in a string of length $n$ . Then the input layer emb becomes the sum of a word embedding and a position embedding. ", "page_idx": 7}, {"type": "text", "text": "We say that $\\Theta$ has finite image if $\\textstyle\\bigcup_{n\\geq0}\\operatorname{Im}\\theta_{n}$ is finite. In general, our results extend to transformers with any position embedding that has finite image. The class of languages recognized may grow, and we give a recipe for characterizing the new class of languages. ", "page_idx": 7}, {"type": "text", "text": "We can add numerical predicates to LTL and initial Boolean vectors to B-RASP as follows. Let $\\Pi\\,=\\,(\\pi_{n})_{n\\geq0}$ be a family of functions $\\pi_{n}\\colon[n]\\ \\to\\ \\{0,1\\}$ . Then there is an additional predicate symbol $\\Pi$ such that for any string $w$ with length $n$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w\\models\\Pi(i)\\ \\mathrm{iff}\\ \\pi_{n}(i)=1}\\\\ {w,i\\models\\Pi\\ \\mathrm{iff}\\ \\pi_{n}(i)=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For example, if $\\mathrm{Mid}_{n}(i)$ is true iff $n$ is odd and $i=\\lceil n/2\\rceil$ , then we can define the language $\\{\\#a^{m}\\#b^{m}\\#\\mid$ $m\\geq0\\}$ in LTL[Mid] as: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\phi=Q_{\\#}\\wedge(Q_{b}\\ \\mathbf{since}\\ (\\mathbf{Mid}\\wedge Q_{\\#}\\wedge(Q_{a}\\ \\mathbf{since}\\ (Q_{\\#}\\wedge\\lnot(0\\ \\mathbf{since}\\ 1))))).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "A similar definition could be written in B-RASP[Mid]. ", "page_idx": 8}, {"type": "text", "text": "Theorem 7. Let $\\Theta=(\\theta_{n})_{n\\geq0}$ be a position embedding with finite image. There exists a collection of predicates $\\mathcal{P}_{\\Theta}$ such that the following classes of languages are the same: ", "page_idx": 8}, {"type": "text", "text": "\u2022 languages recognized by masked hard-attention transformers with position embedding $\\Theta$ \u2022 languages defined by B-RASP $[\\mathcal{P}_{\\Theta}]$   \n\u2022 languages defined by $\\mathbf{LTL}[\\mathcal{P}_{\\Theta}]$ . ", "page_idx": 8}, {"type": "text", "text": "Proof. See Appendix D.1. ", "page_idx": 8}, {"type": "text", "text": "We discuss two important special cases below. ", "page_idx": 8}, {"type": "text", "text": "Sinusoidal position embeddings The original transformer (Vaswani et al., 2017) used position embeddings with coordinates of the form $\\sin(2\\pi f i)$ or $\\cos(2\\pi f i)$ . If the $f$ \u2019s are rational (though in the original definition they were not), then the position embeddings form a finite set, so Lemma 22 still holds. For any even $d$ , let us define a rational sinusoidal positional embedding with $d$ dimensions to be a position embedding $\\Theta=(\\theta_{n})_{n\\geq0}$ where ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\theta_{n}(i)=\\big[\\sin2\\pi f_{1}i\\quad\\cos2\\pi f_{1}i\\quad\\cdot\\cdot\\cdot\\quad\\sin2\\pi f_{d/2}i\\quad\\cos2\\pi f_{d/2}\\big]^{\\top}\\qquad f_{1},\\dots,f_{d/2}\\in\\mathbb{Q}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Corollary 8. Masked hard-attention transformers with rational sinusoidal position embeddings recognize exactly the regular languages in $\\mathbf{\\bar{AC}^{0}}$ (that is, regular languages definable by $a$ family of Boolean circuits with polynomial size and constant depth). ", "page_idx": 8}, {"type": "text", "text": "Proof. This uses the fact that the regular languages in $\\mathbf{AC}^{0}$ are exactly the languages definable in first-order logic with modular predicates (Barrington et al., 1992). See Appendix D.2 for details. \u25a1 ", "page_idx": 8}, {"type": "text", "text": "An example of a language that belongs to this class but is not star-free is $(a a)^{*}$ . The classic example of a language that is regular but not in $\\mathbf{AC}^{0}$ is PARITY $=\\{w\\in\\{a,b\\}^{*}\\mid w$ has an odd number of $^b$ \u2019s} (Furst et al., 1984). ", "page_idx": 8}, {"type": "text", "text": "Arbitrary position embeddings Finally, we may consider arbitrary position embeddings, subject to the condition of finite image. The corresponding collection of predicates is the set of all possible monadic predicates, which we call Mon.5 ", "page_idx": 8}, {"type": "text", "text": "Corollary 9. Masked hard-attention transformers that have position embeddings with finite image recognize exactly the languages definable in LTL[Mon]. ", "page_idx": 8}, {"type": "text", "text": "Barcel\u00b4o et al. (2024) show that any language definable in LTL[Mon] can be recognized by a hardattention transformer without attention masking and with some position embedding (with infinite image), but left the other direction as an open question. Here, by making use of attention masking and restricting position embeddings to those with finite image, we have obtained an exact characterization. ", "page_idx": 8}, {"type": "text", "text": "The addition of attention masking appears to be important. With finite image position embeddings but without attention masking, there must be two positions $i$ and $j$ with the same position embedding (by the pigeonhole principle), so an unmasked attention transformer would not be able to distinguish one string with $a$ and $^b$ at positions $i$ and $j$ and another string with $a$ and $b$ at positions $j$ and $i$ . So no masked hard-attention transformer with finite image position embeddings and unmasked attention can recognize the language $\\#a^{*}\\#b^{*}\\#$ , but we showed already how to define this language even in LTL. ", "page_idx": 8}, {"type": "text", "text": "5.4 Depth hierarchy ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Finally, we establish that (unlike with feed-forward networks), we can always increase the expressive power of masked hard-attention transformers by adding more self-attention layers. We consider masked hard-attention transformers with only future masking (as is typical in practice) and with only rightmost-hard attention. Other masking and tie-breaking schemes are treated in Appendix D.3. We also add multi-head attention (as is typical in practice). ", "page_idx": 9}, {"type": "text", "text": "First, we define depth for all models in this paper. The layer depth of a masked hard-attention transformer is the number of attention layers. The temporal depth of an LTL formula is as follows: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{d}\\mathrm{p}(Q_{\\sigma})=0\\qquad\\mathrm{d}\\mathrm{p}(\\neg\\phi)=\\mathrm{d}\\mathrm{p}(\\phi)}\\\\ &{\\quad\\quad\\mathrm{d}\\mathrm{p}(\\phi\\wedge\\psi)=\\mathrm{d}\\mathrm{p}(\\phi\\vee\\psi)=\\operatorname*{max}(\\mathrm{d}\\mathrm{p}(\\phi),\\mathrm{d}\\mathrm{p}(\\psi))}\\\\ &{\\quad\\mathrm{d}\\mathrm{p}(\\phi\\ \\mathrm{since}\\ \\psi)=\\mathrm{d}\\mathrm{p}(\\phi\\ \\mathbf{until}\\ \\psi)=\\operatorname*{max}(\\mathrm{d}\\mathrm{p}(\\phi),\\mathrm{d}\\mathrm{p}(\\psi))+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The attention depth of a B-RASP expression is defined as follows: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathrm{p}(Q_{\\sigma}(i))=0\\qquad\\mathrm{d}\\mathrm{p}(\\neg P(i))=\\mathrm{d}\\mathrm{p}(P(i))\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {\\mathrm{d}\\mathrm{p}(P_{1}(i)\\wedge P_{2}(i))=\\mathrm{d}\\mathrm{p}(P_{1}(i)\\vee P_{2}(i))=\\operatorname*{max}(\\mathrm{d}\\mathrm{p}(P_{1}(i)),\\mathrm{d}\\mathrm{p}(P_{2}(i))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We then extend this definition to B-RASP operations. If $P(i):=\\phi(i)$ (a position-wise operation), ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathrm{p}(P(i))=\\mathrm{d}\\mathrm{p}(\\phi(i)).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "equation", "text": "$$\n^{>}(i):=\\bullet_{j}\\left[M(i,j),S(i,j)\\right]~V(i,j):D(i)~\\mathrm{or}~P(i):=\\bullet_{j}\\left[M(i,j),S(i,j)\\right]~V(i,j):D(i).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathrm{p}(P(i))=\\operatorname*{max}(\\mathrm{d}\\mathrm{p}(S(i,j)),\\mathrm{d}\\mathrm{p}(V(i,j)),\\mathrm{d}\\mathrm{p}(D(i)))+1.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Finally, the attention depth of a program is the maximum of the attention depths of all of its operations. ", "page_idx": 9}, {"type": "text", "text": "Let $\\mathbf{MUHAT}(\\pmb{\\triangleright}F)_{k}$ (respectively, $\\mathbf{B}\\mathbf{-}\\mathbf{RASP}(\\mathbf{\\pm}F)_{k})$ be the languages recognizable by multi-head transformers of depth $k$ (respectively, B-RASP programs of depth $k$ ) using only future-masked rightmost-hard attention. Let $\\mathbf{LTL}(\\mathbf{since})_{k}$ be the languages definable by LTL formulas of depth $k$ without until. ", "page_idx": 9}, {"type": "text", "text": "Theorem 10. For every $k\\,\\geq\\,0$ , there is a language $L_{k+1}$ such that no multi-head masked hardattention transformer of depth $k$ recognizes $L_{k}$ , but a transformer of depth $(k\\!+\\!1)$ does recognize $L_{k+1}$ . ", "page_idx": 9}, {"type": "text", "text": "Proof. The constructions in the proofs of Theorems 1 and 2 preserve depth, so $\\mathbf{B}{\\mathrm{-}}\\mathbf{R}\\mathbf{A}\\mathbf{S}\\mathbf{P}(\\bullet{F})_{k}=$ $\\mathbf{LTL}(\\mathbf{since})_{k}$ . Moreover, by Theorem 4 (shallower version in Appendix C.2), and by Theorem 27 (a depth-preserving version of Theorem 3 found in Appendix D.3), $\\mathbf{MUHAT}(\\star F)_{k}=\\mathbf{B}{\\cdot}\\mathbf{RASP}(\\star F)_{k}$ . Finally, Etessami and Wilke (2000) prove that $\\mathbf{LTL}(\\mathbf{since})_{k}\\subset\\mathbf{LTL}(\\mathbf{since})_{k+1}.$ . Namely, the classes are separated by $L_{k+1}={\\mathrm{STAIR}}_{k+1}$ , which is the language over $\\Sigma=\\{a,b,c\\}$ of strings which, after deleting $c$ \u2019s, contain $a^{k+1}$ as a substring. This gives the following picture: ", "page_idx": 9}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\cdots\\subset}&{\\mathbf{LTL}(\\mathbf{since})_{k}\\ \\ \\subset\\ \\mathbf{LTL}(\\mathbf{since})_{k+1}\\ \\ \\ \\subset\\cdots}\\\\ {\\qquad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{|}!}&{\\ \\ \\ \\ \\leq\\ \\mathbf{LTL}(\\mathbf{since})_{k+1}}\\\\ {\\mathbf{B-RASP}(\\mathbf{\\boldsymbol{\\upmu}}\\mathbf{\\boldsymbol{\\mu}}\\mathbf{\\boldsymbol{\\nu}})_{k}\\ \\ \\ }&{\\mathbf{B-RASP}(\\mathbf{\\boldsymbol{\\upmu}}\\mathbf{\\boldsymbol{\\nu}}F)_{k+1}}\\\\ {\\qquad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{|}\\!}&{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{|}}\\\\ {\\mathbf{MUHHAT}(\\mathbf{\\boldsymbol{\\upmu}}\\mathbf{\\boldsymbol{F}})_{k}\\ \\ \\ }&{\\mathbf{MUHAT}(\\mathbf{\\boldsymbol{\\upmu}}\\mathbf{\\boldsymbol{F}})_{k+1}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Therefore, $\\mathbf{MUHAT}(\\star F)_{k}\\subset\\mathbf{MUHAT}(\\star F)_{k+1}.$ . ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work focuses exclusively on masked hard-attention transformers. We discussed the rationale for hard attention in Section 2.2. These results do not apply to softmax-attention transformers, although they demonstrate what kinds of results one might hope to obtain for softmax-attention transformers. Nor do they apply to transformers with unmasked attention. ", "page_idx": 9}, {"type": "text", "text": "Finally, our restriction of position embeddings to have finite image, and in particular our restriction of sinusoidal position embeddings to have angles that are rational multiples of $\\pi$ , does not exactly match the standard definition. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Peter Cholak, Anthony Widjaja Lin, Anand Pillay, and the anonymous reviewers, including the reviewers of a previous version of this paper, for their helpful comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. In NIPS Deep Learning Symposium. ", "page_idx": 10}, {"type": "text", "text": "Pablo Barcelo\u00b4, Alexander Kozachinskiy, Anthony Widjaja Lin, and Vladimir Podolskii. 2024. Logical languages accepted by transformer encoders with hard attention. In Proceedings of the 12th International Conference on Learning Representations (ICLR).   \nDavid A. Mix Barrington, Kevin Compton, Howard Straubing, and Denis Th\u00b4erien. 1992. Regular languages in $N C^{I}$ . Journal of Computer and System Sciences, 44(3):478\u2013499.   \nDavid A. Mix Barrington, Neil Immerman, Clemens Lautemann, Nicole Schweikardt, and Denis Th\u00b4erien. 2005. First-order expressibility of languages with neutral letters or: The Crane Beach conjecture. Journal of Computer and System Sciences, 70(2):101\u2013127.   \nSatwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020. On the ability and limitations of Transformers to recognize formal languages. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096\u20137116.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 1877\u20131901.   \nDavid Chiang and Peter Cholak. 2022. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 7654\u20137664.   \nDavid Chiang, Peter Cholak, and Anand Pillay. 2023. Tighter bounds on the expressivity of transformer encoders. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 5544\u20135562.   \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional Transformers for language understanding. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT), pages 4171\u20134186.   \nJavid Ebrahimi, Dhruv Gelda, and Wei Zhang. 2020. How can self-attention networks recognize Dyck-n languages? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4301\u20134306.   \nKousha Etessami and Thomas Wilke. 2000. An until hierarchy and other applications of an Ehrenfeucht\u2013Fra\u00a8\u0131sse\u00b4 game for temporal logic. Information and Computation, 160(1-2):88\u2013108.   \nDan Friedman, Alexander Wettig, and Danqi Chen. 2023. Learning Transformer programs. In Advances in Neural Information Processing Systems 36 (NeurIPS).   \nMerrick Furst, James B. Saxe, and Michael Sipser. 1984. Parity, circuits, and the polynomial-time hierarchy. Mathematical Systems Theory, 17:13\u201327.   \nDov Gabbay, Amir Pnueli, Saharon Shelah, and Jonathan Stavi. 1980. On the temporal analysis of fairness. In Proceedings of the 7th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL), pages 163\u2013173.   \nAnkit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. 2021. Memory-efficient transformers via top-k attention. In Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing, pages 39\u201352.   \nMichael Hahn. 2020. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156\u2013171.   \nYiding Hao, Dana Angluin, and Robert Frank. 2022. Formal language recognition by hard attention Transformers: Perspectives from circuit complexity. Transactions of the Association for Computational Linguistics, 10:800\u2013810.   \nJohan Anthony Willem Kamp. 1968. Tense Logic and the Theory of Linear Order. Ph.D. thesis, University of California, Los Angeles.   \nJambay Kinley. 2020. Two-Stream Transformer Architecture With Discrete Attention for Better Interpretrability and Separation of Model Concerns. Bachelor\u2019s thesis, Harvard College.   \nOded Maler. 2010. On the Krohn-Rhodes cascaded decomposition theorem. In Time for Verification: Essays in Memory of Amir Pnueli, pages 260\u2013278. Springer.   \nRobert McNaughton and Seymour Papert. 1971. Counter-Free Automata. Number 65 in M.I.T. Press Research Monographs. The M.I.T. Press.   \nWilliam Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A. Smith. 2021. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1766\u20131781.   \nWilliam Merrill and Ashish Sabharwal. 2024. The expressive power of transformers with chain of thought. In Proceedings of the 12th International Conference on Learning Representations (ICLR).   \nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context learning and induction heads. arXiv:2209.11895.   \nDoron Peled and Thomas Wilke. 1997. Stutter-invariant temporal properties are expressible without the next-time operator. Information Processing Letters, 63(5):243\u2013246.   \nBinghui Peng. 2023. Personal communication.   \nJorge P\u00b4erez, Pablo Barcel\u00b4o, and Javier Marinkovic. 2021. Attention is Turing-complete. Journal of Machine Learning Research, 22:75:1\u201375:35.   \nM. P. Sch\u00a8utzenberger. 1965. On finite monoids having only trivial subgroups. Information and Control, 8(2):190\u2013194.   \nLena Strobl, Dana Angluin, David Chiang, Jonathan Rawski, and Ashish Sabharwal. 2024a. Transformers as transducers. arXiv:2404.02040.   \nLena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin. 2024b. What formal languages can transformers express? A survey. Transactions of the Association for Computational Linguistics, 12:543\u2013561.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30 (NIPS).   \nGail Weiss, Yoav Goldberg, and Eran Yahav. 2021. Thinking like Transformers. In Proceedings of the 38th International Conference on Machine Learning (ICML), pages 11080\u201311090.   \nHongfei Xu, Qiuhui Liu, Josef van Genabith, and Deyi Xiong. 2021. Learning hard retrieval decoder attention for Transformers. In Findings of the Association for Computational Linguistics: EMNLP, pages 779\u2013785.   \nAndy Yang and David Chiang. 2024. Counting like transformers: Compiling temporal counting logic into softmax transformers. In Proceedings of the Conference on Language Modeling (CoLM).   \nShunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. 2021. Self-attention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 3770\u20133785.   \nHattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. 2024. What algorithms can Transformers learn? A study in length generalization. In Proceedings of the 12th International Conference on Learning Representations (ICLR). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional B-RASP Example: Associative Recall ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We consider a variation of the simple associative recall task studied by Friedman et al. (2023) and many others in connection with induction heads and in-context learning (Olsson et al., 2022). Inputs are strings over the alphabet $\\{a,b,c,l,2,3\\}$ , where letters and numbers alternate, starting with a letter and ending with a number, for example $w=a3b2b1a2c l a l c3$ . The output alphabet adds the symbol ?. The desired output sequence copies the letters, and for a number at position $i$ , if $\\sigma$ is the letter at position $i-1$ , then the most recent previous occurrence of $\\sigma$ is found, say at position $j$ , and the number at position $j+1$ is output. If there is no previous occurrence of $\\sigma$ , then ? is output instead. For the given example input $w$ , the output should be $y=a?b?b2a3c?a2c I$ . ", "page_idx": 13}, {"type": "text", "text": "The Boolean vector $P_{a}$ determines whether there is an $a$ in the preceding position. Similarly, the Boolean vectors $P_{b}$ and $P_{c}$ indicate whether there is a $^b$ or $c$ , respectively, in the preceding position: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{P_{a}(i):=\\star_{j}\\,[j<i,1]\\ \\,Q_{a}(j):0}}\\\\ {{P_{b}(i):=\\star_{j}\\,[j<i,1]\\ \\,Q_{b}(j):0}}\\\\ {{P_{c}(i):=\\star_{j}\\,[j<i,1]\\ \\,Q_{c}(j):0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The program has an output Boolean vector $Y_{\\sigma}$ for each symbol $\\sigma\\in\\Sigma$ indicating whether the output symbol is $\\sigma$ . For $\\sigma\\in\\{I,2,3\\}$ , if position $i$ is preceded by a letter, the output Boolean vector $Y_{\\sigma}$ attends to the most recent position $j$ preceded by the same letter (if any), and returns the value of $\\boldsymbol{Q}_{\\sigma}(\\boldsymbol{j})$ for that $j$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y_{I}(i):=\\star_{j}\\left[j<i,(P_{a}(i)\\wedge P_{a}(j))\\vee(P_{b}(i)\\wedge P_{b}(j))\\vee(P_{c}(i)\\wedge P_{c}(j))\\right]~Q_{I}(j):0}\\\\ {Y_{2}(i):=\\star_{j}\\left[j<i,(P_{a}(i)\\wedge P_{a}(j))\\vee(P_{b}(i)\\wedge P_{b}(j))\\vee(P_{c}(i)\\wedge P_{c}(j))\\right]~Q_{2}(j):0}\\\\ {Y_{3}(i):=\\star_{j}\\left[j<i,(P_{a}(i)\\wedge P_{a}(j))\\vee(P_{b}(i)\\wedge P_{b}(j))\\vee(P_{c}(i)\\wedge P_{c}(j))\\right]~Q_{3}(j):0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For $\\sigma\\in\\{a,b,c\\}$ , the output Boolean vector $Y_{\\sigma}$ is just $Q_{\\sigma}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Y_{a}(i):=Q_{a}(i)}\\\\ &{Y_{b}(i):=Q_{b}(i)}\\\\ &{Y_{c}(i):=Q_{c}(i).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, the output Boolean vector $Y_{?}$ is true if the position is preceded by a letter but no number was assigned: ", "page_idx": 13}, {"type": "equation", "text": "$$\nY_{?}(i):=(P_{a}(i)\\vee P_{b}(i)\\vee P_{c}(i))\\wedge\\neg(Y_{I}(i)\\vee Y_{2}(i)\\vee Y_{3}(i)).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The Boolean vectors in the computation for the example string $w$ are shown below. ", "page_idx": 13}, {"type": "table", "img_path": "FBMsBdH0yz/tmp/6bf3e90ab3d30a326df281fc07769bffb28aeedf2d729b86223adb71895e811b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Proofs for Section 3 (Boolean RASP) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Unary value predicate ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proposition 11. Every B-RASP program is equivalent to one in which all value predicates $V(i,j)$ depend only on $j$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. This can be seen by the fact that the simulation of since/until (Appendix B.3) does not use a value predicate that depends on $i$ , but we can show this more directly by induction on the structure of $V(i,j)$ . We only show how to handle $\\blacktriangleright$ ; the case of $\\blacktriangleleft$ is very similar. ", "page_idx": 14}, {"type": "text", "text": "Consider an attention operation with the form ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(i):=\\star_{j}\\;[M(i,j),S(i,j)]\\;\\;V(i,j):D(i).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The base cases are $V(i,j)=B(j)$ , for some Boolean vector $B$ , which already has the desired form, and $V(i,j)=B(i)$ , in which case $P$ is equivalent to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A(i):=\\star_{j}\\,[M(i,j),S(i,j)]\\ 1:0}\\\\ &{P(i):=(A(i)\\wedge B(i))\\vee(\\neg A(i)\\wedge D(i)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $V(i,j)=V_{1}(i,j)\\land V_{2}(i,j)$ , then $P(i)$ is equivalent to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A(i):=\\star_{j}\\left[M(i,j),S(i,j)\\right]\\ 1:0}\\\\ &{C_{1}(i):=\\star_{j}\\left[M(i,j),S(i,j)\\right]\\ V_{1}(i,j):0}\\\\ &{C_{2}(i):=\\star_{j}\\left[M(i,j),S(i,j)\\right]\\ V_{2}(i,j):0}\\\\ &{P(i):=(A(i)\\wedge C_{1}(i)\\wedge C_{2}(i))\\vee(\\neg A(i)\\wedge D(i)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly for disjunction and negation. ", "page_idx": 14}, {"type": "text", "text": "B.2 Unary score predicate ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 12. Every B-RASP program is equivalent to one in which all score predicates $S(i,j)$ depend only on $j$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Again, we only show the case of $\\blacktriangleright$ , as the case of $\\blacktriangleleft$ is very similar. Consider a B-RASP attention operation $P$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(i):=\\bullet_{j}\\,\\left[M(i,j),S(i,j)\\right]\\;V(j):D(i).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\nS(i,j)=f(A_{1}(i),\\dots,A_{T_{A}}(i),B_{1}(j),\\dots,B_{T_{B}}(j))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $f$ is some Boolean function, and the $A_{t}$ and $B_{t}$ are $\\mathbf{B}$ -RASP operations. Let $\\mathcal{A}\\quad=$ $\\{A_{1},\\ldots,A_{T_{A}}\\}$ , and for each $\\chi\\subseteq\\mathcal{A}$ , define an assignment of truth values to the $A_{t}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nA_{t}^{\\chi}=\\left\\{{1\\atop0}\\quad A_{t}\\in\\chi\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and use it to define a unary score $S^{\\chi}(j)$ which uses the truth assignment of $\\chi$ plugged into the $A_{t}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nS^{\\chi}(j)=f(A_{1}^{\\chi},\\ldots,A_{T_{A}}^{\\chi},B_{1}(j),\\ldots,B_{T_{B}}(j)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, $P(i)$ is equivalent to $P^{\\prime}(i)$ , where: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P^{\\chi}(i):=\\bullet_{j}\\left[M(i,j),S^{\\chi}(j)\\right]\\;V(j):D(i)\\qquad\\qquad\\qquad\\qquad\\quad\\mathrm{for~each~}\\chi\\subseteq\\mathcal{R}}\\\\ &{P^{\\prime}(i):=\\displaystyle\\bigvee_{\\chi\\subseteq\\mathcal{R}}\\left(P^{\\chi}(i)\\wedge\\bigwedge_{t\\in[T_{A}]}\\left(A_{t}(i)\\leftrightarrow A_{t}^{\\chi}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To see why, observe that for any $i$ , there is exactly one truth assignment $\\chi_{i}$ that satisfies $\\bigwedge_{t\\in[T_{A}]}\\left(A_{t}^{\\textit{\\bullet}}(i)\\leftrightarrow A_{t}^{\\chi}\\right)$ . This $\\chi_{i}$ also makes $S^{\\chi_{i}}(j)$ equivalent to $S(i,j)$ , and $P^{\\chi_{i}}(i)$ equivalent to $P(i)$ . ", "page_idx": 14}, {"type": "text", "text": "Note that each attention operation translates into as many as $2^{T_{A}+T_{B}}$ operations. ", "page_idx": 14}, {"type": "text", "text": "B.3 Proof of Theorem 1 (LTL to B-RASP) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 1 follows from the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 13. For any formula $\\phi$ of LTL, there is a B-RASP program with a Boolean vector $P_{\\phi}$ such that, for any input $w$ of length \ud835\udc5band all $i\\in[n]$ , we have $w,i\\Vdash\\phi$ iff $w\\models P_{\\phi}(i)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. By induction on the structure of the formula $\\phi$ . We assume that a B-RASP program always contains the initial Boolean vectors $Q_{\\sigma}$ for $\\sigma\\in\\Sigma$ . ", "page_idx": 15}, {"type": "text", "text": "If $\\phi=Q_{\\sigma}$ : Add operation ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{\\phi}(i):=Q_{\\sigma}(i).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\phi\\,=\\,\\neg\\phi_{1}$ : By the induction hypothesis, convert $\\phi_{1}$ to B-RASP operations, including one that computes $P_{\\phi_{1}}$ . Then add the operation ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{\\phi}(i):=\\neg P_{\\phi_{1}}(i).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\phi=\\phi_{1}\\wedge\\phi_{2}$ : By the induction hypothesis, convert $\\phi_{1}$ to B-RASP operations, including one that computes $P_{\\phi_{1}}$ , then convert $\\phi_{2}$ to B-RASP operations, including one that computes $P_{\\phi_{2}}$ . Then add operation ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{\\phi}(i):=P_{\\phi_{1}}(i)\\wedge P_{\\phi_{2}}(i).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\phi=\\phi_{1}\\vee\\phi_{2}$ : Similar, but add ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{\\phi}(i):=P_{\\phi_{1}}(i)\\lor P_{\\phi_{2}}(i).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\phi=\\phi_{1}$ since $\\phi_{2}$ : Similar, but add ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{\\phi}(i):=\\bullet_{j}\\left[j<i,\\neg P_{\\phi_{1}}(j)\\lor P_{\\phi_{2}}(j)\\right]\\;P_{\\phi_{2}}(j):0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\phi=\\phi_{1}$ until $\\phi_{2}$ : Similar, but add ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{\\phi}(i):=\\bullet_{j}\\left[j>i,\\neg P_{\\phi_{1}}(j)\\lor P_{\\phi_{2}}(j)\\right]\\;P_{\\phi_{2}}(j):0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.4 Proof of Theorem 2 (B-RASP to LTL) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 2 follows from the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 14. For any Boolean vector $P$ of a B-RASP program $\\mathcal{P}$ , there is a formula $\\phi_{P}$ of LTL such that for any input $w$ of length \ud835\udc5band all $i\\in[n]$ , we have $w\\v{i}=P(i)$ iff $w,i\\in\\phi_{P}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. First, by Lemma 12 and Proposition 11 we can rewrite $\\mathcal{P}$ to an equivalent program such that every attention operation only uses unary scores and unary values. ", "page_idx": 15}, {"type": "text", "text": "Each initial Boolean vector $Q_{\\sigma}(i)$ translates to the atomic formula $Q_{\\sigma}$ . ", "page_idx": 15}, {"type": "text", "text": "For each operation $P_{t}(i)$ (for $t>|\\Sigma|)$ ), if it is a position-wise operation, that is, ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{t}(i):=f(P_{1}(i),\\ldots,P_{t-1}(i))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $f$ is a Boolean function, then by the inductive hypothesis, there are LTL formulas $\\phi_{t}$ for each $P_{t}(i)$ . Then we convert $P_{t}(i)$ into $\\phi_{t}=f(\\phi_{1},\\ldots,\\phi_{t-1})$ . ", "page_idx": 15}, {"type": "text", "text": "If $P_{t}(i)$ is an attention operation, define ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r l}&{\\mathrm{exists}_{<}\\;\\phi=1\\;\\mathrm{since}\\;\\phi}\\\\ &{\\mathrm{exists}_{>}\\;\\phi=1\\;\\mathrm{until}\\;\\phi}\\\\ &{\\quad\\mathrm{exists}\\;\\phi=(\\mathrm{exists}_{<}\\;\\phi)\\lor\\phi\\lor(\\mathrm{exists}_{>}\\;\\phi)}\\\\ &{\\mathrm{ightmost}\\;\\phi=\\phi\\land\\lnot(\\mathrm{exists}_{>}\\;\\phi).}\\end{array}$ also known as eventually ", "page_idx": 15}, {"type": "text", "text": "If $P_{t}(i)$ uses $\\blacktriangleright$ and future masking, that is, ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{t}(i):=\\bullet_{j}\\left[j<i,S(j)\\right]\\;V(j):D(i)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then by the inductive hypothesis, there are LTL formulas $\\phi_{S},\\phi_{V}$ and $\\phi_{D}$ for the corresponding B-RASP operations. Then we can convert $P_{t}(i)$ into the LTL formula ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi_{t}=\\big(\\neg\\phi_{S}\\;\\mathbf{since}\\;(\\phi_{S}\\wedge\\phi_{V})\\big)\\vee\\big(\\neg(\\mathbf{exists}_{<}\\;\\phi_{S})\\wedge\\phi_{D}\\big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $P_{t}(i)$ uses $\\blacktriangleright$ and past masking, that is, ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{t}(i):=\\bullet_{j}\\left[j>i,S(j)\\right]\\;V(j):D(i)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi_{t}=\\big(\\mathbf{exists}_{>}\\,\\big((\\mathbf{rightmost}\\,\\phi_{S})\\wedge\\phi_{V}\\big)\\big)\\vee\\big(\\neg(\\mathbf{exists}_{>}\\,\\phi_{S})\\wedge\\phi_{D}\\big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "And if $P_{t}(i)$ uses $\\blacktriangleright$ with no masking, that is, ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{t}(i):=\\bullet_{j}\\left[1,S(j)\\right]\\,\\,V(j):D(i)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi_{t}=\\big(\\mathbf{exists}\\,\\,\\big((\\mathbf{rightmost}\\,\\,\\phi_{S})\\wedge\\phi_{V}\\big)\\big)\\vee\\big(\\neg(\\mathbf{exists}\\,\\,\\phi_{S})\\wedge\\phi_{D}\\big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The cases for $\\blacktriangleleft$ are symmetric. ", "page_idx": 16}, {"type": "text", "text": "B.5 Counter-free automata to B-RASP ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we give an alternative proof of Theorem 1 using the fact that the star-free languages are exactly those recognized by counter-free automata. ", "page_idx": 16}, {"type": "text", "text": "A deterministic finite automaton (DFA) is a tuple $A=(\\Sigma,Q,\\delta)$ , where $\\Sigma$ is the input alphabet, $Q$ is the finite set of states, and $\\delta\\colon Q\\times\\Sigma\\rightarrow Q$ is the transition function. A counter-free automaton is a DFA in which no string induces a permutation on any subset of $Q$ other than the identity. Schu\u00a8tzenberger (1965) proved that the star-free languages are exactly those recognized by counter-free automata. ", "page_idx": 16}, {"type": "text", "text": "Theorem 15. For any counter-free DFA that recognizes a language $L\\subseteq\\Sigma^{+}$ , there is $a$ B-RASP program that recognizes $L$ . ", "page_idx": 16}, {"type": "text", "text": "A counter-free automaton can be decomposed using Krohn-Rhodes theory into a cascade of identityreset automata, each of which can be simulated in B-RASP. ", "page_idx": 16}, {"type": "text", "text": "Maler (2010) gives the following automata-theoretic version of the Krohn\u2013Rhodes decomposition theorem, which we explain below. ", "page_idx": 16}, {"type": "text", "text": "Theorem 16 (Maler, 2010, Theorem 3). For every [deterministic finite] automaton \ud835\udc34there exists a cascade decomposition ", "page_idx": 16}, {"type": "equation", "text": "$$\nC=B_{1}\\circ B_{2}\\circ\\cdot\\cdot\\cdot\\circ B_{k}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "such that the following are true. ", "page_idx": 16}, {"type": "text", "text": "1. Each $B_{i}$ is a permutation\u2013reset automaton.   \n2. There is a homomorphism $\\phi$ from $C$ to $A$ .   \n3. Any permutation group in some $B_{i}$ is homomorphic to a subgroup of the transformation semigroup of $A$ . ", "page_idx": 16}, {"type": "text", "text": "The pair $(C,\\phi)$ is called a cascade decomposition of $A$ . ", "page_idx": 16}, {"type": "text", "text": "If $B_{1}=(\\Sigma,Q_{1},\\delta_{1})$ and $\\boldsymbol{B}_{2}=(Q_{1}\\times\\Sigma,Q_{2},\\delta_{2})$ are DFAs, their cascade product $C=B_{1}\\circ B_{2}$ is the automaton $(\\Sigma,Q_{1}\\times Q_{2},\\delta)$ such that $\\delta(q_{1}q_{2},\\sigma)=(\\delta_{1}(q_{1},\\sigma),\\delta_{2}(q_{2},(q_{1},\\sigma)))$ . (To reduce clutter, we write tuples of states without commas.) The cascade product allows the automaton $B_{2}$ to see the current state of $B_{1}$ in deciding what transition to take. We define iterated cascade product inductively by $B_{1}\\circ B_{2}\\circ\\cdots\\circ B_{k}=(B_{1}\\circ B_{2}\\circ\\cdots B_{k-1})\\circ B_{k}$ . This allows each $B_{i}$ to see the current state of all $B_{j}$ with $j\\leq i$ in deciding what transition to take. (For readers more familiar with finite transducers (Mealy machines), $B_{1}$ and $B_{2}$ could be thought of as transducers whose transitions are all of the form \ud835\udc5e : ( ) \ud835\udc5f . Then the cascade product is just composition.) ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "FBMsBdH0yz/tmp/5d7b96a574302f20e6c05f7291fada8272a08d8be53fc1b9238c84cbb76f87d8.jpg", "img_caption": ["(a) Automaton $A_{3}$ : move right $(R)$ , move left $(L)$ . "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "FBMsBdH0yz/tmp/fe03db882243dc4646a0021807e154b8d445ee54619f703e8ecf55183f364d87.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(b) Cascade of identity-reset automata for $A_{3}$ . Omitted transitions are self-loops; for example, inputs $A,R$ and $B,L$ are self-loops on $C$ and $D$ . ", "page_idx": 17}, {"type": "image", "img_path": "FBMsBdH0yz/tmp/d4a5c7fd59bf77e66fe35436f8ae3b6bf17f2e40bff1ca010dff67a7ac10d1a5.jpg", "img_caption": ["(c) The global automaton of the cascade product in part (b). For state $q$ , the number in parentheses is the state $\\phi(q)$ in $A_{3}$ . ", "Figure 3: Example automaton and its cascade decomposition. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "In Property (1), a permutation\u2013reset automaton is a DFA $A=(\\Sigma,Q,\\delta)$ such that for every $\\sigma\\in\\Sigma$ , the mapping $q\\mapsto\\delta(q,\\sigma)$ is either a permutation (for all $r$ , there is a $q$ such that $\\delta(q,\\sigma)=r)$ ) or constant (there is a $q_{\\sigma}$ such that $\\delta(q,\\sigma)=q_{\\sigma}$ for all $q$ ). ", "page_idx": 17}, {"type": "text", "text": "Property (2) says that there is a mapping $\\phi$ from the states of $C$ to the states of $A$ such that for any state $q$ of $C$ , we have $\\phi(\\delta_{C}(q,\\sigma))=\\delta_{A}(\\phi(q),\\sigma)$ . ", "page_idx": 17}, {"type": "text", "text": "Property (3) implies that if the automaton $A$ is counter-free, then all of the automata $B_{i}$ in the cascade decomposition are identity\u2013reset automata. An identity\u2013reset automaton is a DFA $A\\,=\\,(\\Sigma,Q,\\delta)$ such that for every $\\sigma\\in\\Sigma$ , the mapping $q\\mapsto\\delta(q,\\sigma)$ is either the identity $(\\delta(q,\\sigma)=q$ for all $q$ ) or constant (there is a $q_{\\sigma}$ such that $\\delta(q,\\sigma)=q_{\\sigma}$ for all $q$ ). ", "page_idx": 17}, {"type": "text", "text": "For example, consider the automaton $A_{3}$ shown in Figure 3a. A decomposition of this automaton into a cascade product of three identity\u2013reset automata is shown in Figure 3b. The global automaton derived from the decomposition is shown in Figure 3c with the homomorphism $\\phi$ to states of $A_{3}$ . ", "page_idx": 17}, {"type": "text", "text": "Let $A=(\\Sigma,Q,\\delta)$ be a DFA and $s\\in{\\cal Q}$ . For a string $w_{1}\\cdot\\cdot\\cdot w_{n}\\in\\Sigma^{*}$ , the sequence of states traversed by $A$ from state $s$ on this input is $q_{0},\\ldots,q_{n}$ , where $q_{0}\\,=\\,s$ and for each $k$ , $q_{k+1}\\,=\\,\\delta(q_{k},w_{k})$ . A B-RASP program $p$ simulates $A$ started in state $s$ iff for every input word $w\\in\\Sigma^{*}$ , the output Boolean vectors of $p$ on input $w$ encode the sequence of states traversed by $A$ from state $s$ on input $w$ . The state at position $i$ is the state before the symbol at position $i$ is read. ", "page_idx": 17}, {"type": "text", "text": "Lemma 17. Let $\\boldsymbol{B}=(\\Sigma,Q,\\delta)$ be any identity\u2013reset automaton, and let $s\\in{\\cal Q}$ be a start state. There exists a B-RASP program $\\mathcal{P}_{B}$ that simulates $B$ started in state $s$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. For each state $r\\in\\mathcal{Q}$ , let $R_{r}\\subseteq\\Sigma$ be the state of symbols that reset to $r$ (that is, $\\delta(q,\\sigma)=r$ for all $q\\in{\\cal Q}^{\\mathrm{~\\,~}}$ ). Let $\\textstyle R=\\bigcup_{r\\in Q}R_{r}$ . To determine if $B$ is in state $q\\ne s$ before reading $w_{i}$ , it is sufficient to attend to the closest position $j<i$ that contains a symbol from $R$ , if any. If $j$ exists and $w_{j}\\in R_{q}$ , then $B$ is in state $q$ at position $i$ . Otherwise, it is not. ", "page_idx": 18}, {"type": "text", "text": "The case of state $s$ is slightly different. In the case that there is no position $j\\,<\\,i$ that contains a symbol from $R$ , then $B$ never left the initial state $s$ , so it is still in state $s$ at position $i$ . ", "page_idx": 18}, {"type": "text", "text": "In B-RASP, we can define a Boolean vector $B_{q}(i)$ , which is true iff $B$ is in state $q$ at position $i$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{B_{q}(i):=\\star_{j}\\left[j<i,\\displaystyle\\sqrt{Q_{\\sigma}(j)}\\right]\\,\\displaystyle\\bigvee_{\\sigma\\in R_{q}}\\!\\!\\!\\!Q_{\\sigma}(j):0\\quad\\mathrm{for}\\;q\\neq s}}\\\\ {{B_{s}(i):=\\star_{j}\\left[j<i,\\displaystyle\\sqrt{Q_{\\sigma}(j)}\\right]\\,\\displaystyle\\bigvee_{\\sigma\\in R_{s}}\\!\\!\\!\\!Q_{\\sigma}(j)):1.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma 18. Let $B_{1}\\,=\\,(\\Sigma,Q_{1},\\delta_{1})$ be a $D F A$ that can be simulated from state $s_{1}$ by $a$ B-RASP program $\\mathcal{P}_{B_{1}}$ . Let $\\boldsymbol{B}_{2}=(Q_{1}\\times\\Sigma,Q_{2},\\delta_{2})$ be an identity\u2013reset automaton and let $C=B_{1}\\circ B_{2}$ . Then there is a B-RASP program $\\mathcal{P}_{C}$ that simulates $C$ started in state $(s_{1},s_{2})$ for an arbitrary $s_{2}\\in Q_{2}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $B_{1,q}(i)$ be predicates that test whether $B_{1}$ is in state $q$ at position $i$ started in state $s_{1}$ , and let $B_{2,q}(i)$ be predicates that test whether $B_{2}$ is in state $q$ at position $i$ started in state $s_{2}$ (by Lemma 17). Define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{(q,\\sigma)}^{\\prime}(i):=B_{1,q}(i)\\wedge Q_{\\sigma}(i)\\qquad\\qquad\\qquad q\\in Q_{1},\\sigma\\in\\Sigma}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and for every line in the definition of the $B_{2,q}$ , replace every occurrence of $Q_{(q,\\sigma)}$ with $Q_{\\left(q,\\sigma\\right)}^{\\prime}.$ This yields the definition of new predicates $B_{2,q}^{\\prime}$ . Then we can define predicates $C_{(q,r)}(i)$ that test whether $C=B_{1}\\circ B_{2}$ is in state $(q,r)$ at position $i$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nC_{(q,r)}(i):=B_{1,q}(i)\\wedge B_{2,r}^{\\prime}(i)\\qquad\\qquad\\qquad q\\in Q_{1},r\\in Q_{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By induction on $k$ we have the following. ", "page_idx": 18}, {"type": "text", "text": "Lemma 19. Let $C=B_{1}\\circ B_{2}\\circ\\cdot\\cdot\\cdot\\circ B_{k}$ be a cascade product such that each $B_{i}$ is an identity\u2013reset automaton, and $s_{i}$ is a state of $B_{i}$ . Then there is $a$ B-RASP program $\\mathcal{P}_{C}$ that can simulate $C$ from state $(s_{1},s_{2},\\ldots,s_{k})$ . ", "page_idx": 18}, {"type": "text", "text": "If we add instructions to the program in Lemma 19 that compute the homomorphism $\\phi$ (from property (2) of Theorem 16) from the states of the cascade product $C$ to the automaton $A$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\boldsymbol{A}_{r}(i):=\\underset{\\boldsymbol{q}\\in\\mathcal{Q}}{\\bigcup}C_{\\boldsymbol{q}}(i)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then we get a B-RASP program $\\mathcal{P}_{A}$ that simulates $A$ started in state $s$ . ", "page_idx": 18}, {"type": "text", "text": "Finally, we add to this program position-wise operations $Y_{q}(i)$ that decide whether $A$ started in state $s$ ends up in state $q$ after reading the symbol at position $i$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nY_{r}(i):=\\bigvee_{\\begin{array}{l}{q\\in Q}\\\\ {\\sigma\\in\\Sigma}\\\\ {\\delta(q,\\sigma)=r}\\end{array}}(A_{q}(i)\\wedge Q_{\\sigma}(i)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $f$ is the final state of $A$ , then let $Y_{f}$ be the output vector of the program. Since $Y_{f}(n)=1$ if and only if $A$ accepts $w$ , this completes the proof of Theorem 15. ", "page_idx": 18}, {"type": "text", "text": "C Proofs for Section 4 (Masked Hard-Attention Transformers) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Proof of Theorem 3 (B-RASP to masked hard-attention transformers) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We will make use of the following lemma repeatedly: ", "page_idx": 18}, {"type": "text", "text": "Lemma 20. Any function $f\\colon\\{0,1\\}^{d}\\to\\{0,1\\}^{d}$ can be computed by a two-layer FFN with ReLU activation functions. ", "page_idx": 19}, {"type": "text", "text": "Proof. Any Boolean function can be written in full disjunctive normal form (DNF), which is a disjunction of conjunctions, and each conjunction is a conjunction of one positive or negative literal for each of the arguments, so that at most one conjunction is 1 for any assignment. ", "page_idx": 19}, {"type": "text", "text": "Each component of $f$ can be put into full DNF and computed by a two-layer FFN with ReLU activation functions. The first layer computes all the negations and conjunctions, using the fact that for any Boolean values $\\rangle_{1},b_{2},\\dots,b_{m}$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\neg b_{1}=1-b_{1}}\\\\ {b_{1}\\wedge b_{2}\\wedge...\\wedge b_{m}=\\operatorname{ReLU}(b_{1}+b_{2}+...+b_{m}-(m-1)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second layer computes the disjunction simply by adding the values of the conjunctions. \u25a1 ", "page_idx": 19}, {"type": "text", "text": "Let $\\mathcal{P}$ be a B-RASP program with Boolean vectors $P_{1},\\L...,P_{T}$ . We say that a masked hard-attention transformer $\\mathcal{T}$ with width $d\\geq T$ simulates $\\mathcal{P}$ iff for every input $w\\in\\Sigma^{+}$ with length $n$ , we have, for all $i\\in[n]$ and $t\\in[T]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n[\\mathcal{T}(w)]_{i,t}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\;w\\models P_{t}(i)}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem 3 follows from the following lemma: ", "page_idx": 19}, {"type": "text", "text": "Lemma 21. Let $\\mathcal{P}$ be a B-RASP program. There exists a masked hard-attention transformer $\\mathcal{T}_{\\mathcal{P}}$ that simulates $\\mathcal{P}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. We prove that the first $t$ operations of $\\mathcal{P}$ can be simulated by a masked hard-attention transformer, by induction on $t$ . ", "page_idx": 19}, {"type": "text", "text": "The base case is $t\\,=\\,|\\Sigma|$ . For $c\\,\\in\\,1,\\ldots,|\\Sigma|$ , let $\\sigma_{c}$ be the $c$ -th symbol in $\\Sigma$ . Let $e m b(\\sigma_{c})$ be the one-hot vector with $[e m b(\\sigma_{c})]_{c}=1$ . ", "page_idx": 19}, {"type": "text", "text": "For the inductive step, assume that the Boolean vectors $P_{1},\\ldots,P_{t}$ can be simulated by a masked hard-attention transformer. We want to show that $P_{t+1}$ can be simulated as well. ", "page_idx": 19}, {"type": "text", "text": "If $P_{t+1}(i)$ is a Boolean combination of $\\{P_{1}(i),\\ldots,P_{t}(i)\\}$ , it can be computed by a two-layer FFN by Lemma 20. ", "page_idx": 19}, {"type": "text", "text": "The most important case is if $P_{t+1}(i)$ is an attention operation, either of ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{t+1}(i):=\\bullet_{j}\\left[M(i,j),S(i,j)\\right]\\;V(i,j):D(i)}\\\\ &{P_{t+1}(i):=\\bullet_{j}\\left[M(i,j),S(i,j)\\right]\\;V(i,j):D(i).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We only show the $\\blacktriangleright$ case; $\\blacktriangleleft$ is similar. ", "page_idx": 19}, {"type": "text", "text": "We need to slightly modify the value predicate: ", "page_idx": 19}, {"type": "equation", "text": "$$\nV^{\\prime}(i,j)=(S(i,j)\\wedge V(i,j))\\vee(\\neg S(i,j)\\wedge D(i)).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This does not change the behavior of $P_{t+1}$ (because $S(i,j)$ is always true when evaluating the value predicate), but it will preserve the correct behavior in a transformer, where attention attends to the leftmost maximum score. By Proposition 11, the attention operation can be rewritten so that that the value predicate depends only on $j$ . So, without loss of generality, assume that there is a Boolean function $g$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\nV^{\\prime}(i,j)=g(P_{1}(j),\\ldots,P_{t}(j)).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The score predicate $S(i,j)$ can be written in full DNF in terms of the $P_{\\ell}(i)$ and $P_{\\ell}(j)$ . We separate each conjunction of $S(i,j)$ into a conjunction of literals depending on $i$ and a conjunction of literals depending on $j$ . Thus, there is a collection of Boolean functions $\\alpha_{\\ell}$ and $\\beta_{\\ell}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\nS(i,j)=\\bigvee_{\\ell=1}^{m}\\left(\\alpha_{\\ell}(P_{1}(i),\\dots,P_{t}(i))\\wedge\\beta_{\\ell}(P_{1}(j),\\dots,P_{t}(j))\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We construct two layers, as follows. For brevity, we write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\vec{P}(i)=\\left[\\begin{array}{c}{P_{1}(i)}\\\\ {\\vdots}\\\\ {P_{t}(i)}\\end{array}\\right]}\\\\ {\\vec{\\alpha}(\\nu_{1},\\dots,\\nu_{t})=\\left[\\begin{array}{c}{\\alpha_{1}\\bigl(\\nu_{1},\\dots,\\nu_{t}\\bigr)}\\\\ {\\vdots}\\\\ {\\alpha_{m}\\bigl(\\nu_{1},\\dots,\\nu_{t}\\bigr)}\\end{array}\\right]}&{{}\\quad}&{\\vec{\\beta}(\\nu_{1},\\dots,\\nu_{t})=\\left[\\begin{array}{c}{\\beta_{1}\\bigl(\\nu_{1},\\dots,\\nu_{t}\\bigr)}\\\\ {\\vdots}\\\\ {\\beta_{m}\\bigl(\\nu_{1},\\dots,\\nu_{t}\\bigr).}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and for any $m$ we write $\\mathbf{0}^{m}$ for the $m$ -dimensional zero vector. ", "page_idx": 20}, {"type": "text", "text": "Assume that the input to the first layer is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\boldsymbol{x}_{i}^{(1)}=\\left[\\begin{array}{c}{\\hat{P}(i)}\\\\ {0}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\\\ {0}\\\\ {0}\\\\ {0}\\\\ {0}\\\\ {\\vdots}\\end{array}\\right]\\;\\mathrm{sov}\\;}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The first self-attention has value vectors set to 0, so that the residual connection computes the identity function $(c_{i}^{(1)}=x_{i}^{(1)})$ ), and the first position-wise FFN can be constructed, by Lemma 20, so that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{i}^{(2)}=f_{p}^{(1)}\\left(c_{i}^{(1)}\\right)+c_{i}^{(1)}}\\\\ {\\tilde{P}^{(i)}}\\\\ {\\mathbf{\\Phi}^{(2)}}\\\\ {\\vdots}\\\\ {\\overline{{\\alpha}}(P_{1}(i),\\ldots,P_{t}(i))}\\\\ {=\\left|\\frac{\\vec{A}(P_{1}(i),\\ldots,P_{t}(i))}{\\vec{B}(P_{1}(i),\\ldots,P_{t}(i))}\\right|\\,\\overrightarrow{\\mathrm{query}}}\\\\ {\\mathbf{\\Phi}^{(1)}}\\\\ {g(P_{1}(i),\\ldots,P_{t}(i))\\left|\\begin{array}{l}{\\mathrm{lst~dealt~flag~to~rure}}\\\\ {\\mathrm{value}}\\\\ {\\vdots}\\end{array}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the second layer, the self-attention has mask $M$ . The score function is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{S}^{(2)}\\left(x_{i}^{(2)},x_{j}^{(2)}\\right)=\\left(x_{i}^{(2)}\\right)^{\\top}W^{S}\\,x_{j}^{(2)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\vdots}\\\\ &{=\\left[\\vec{\\mathcal{A}}(P_{1}(i),\\ldots,P_{t}(i))\\right]^{\\top}\\left[\\begin{array}{c c c c}{\\cdot\\,,}&&&\\\\ &{0^{m\\times m}}&{\\mathbf{I}^{m\\times m}}&\\\\ &{\\mathbf{0}^{m\\times m}}&{\\mathbf{0}^{m\\times m}}&\\\\ &&&{\\cdot\\,\\cdot\\left.\\cdot\\left[\\begin{array}{c c c c}{\\vec{\\mathcal{A}}(P_{1}(j),\\ldots,P_{t}(j))}&&&\\\\ &&&{\\vdots}&\\\\ &&&&{\\vdots}\\end{array}\\right]\\right]\\left[\\vec{\\mathcal{A}}(P_{1}(j),\\ldots,P_{t}(j))\\right]}\\\\ &{=\\displaystyle\\sum_{\\ell=1}^{m}\\alpha_{\\ell}(P_{1}(i),\\ldots,P_{t}(i))\\,\\beta_{\\ell}(P_{1}(j),\\ldots,P_{t}(j))}\\\\ &{=S(i,j)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{0}^{m\\times m}$ and $\\mathbf{I}^{m\\times m}$ are the $m\\times m$ zero and identity matrices. ", "page_idx": 21}, {"type": "text", "text": "The value function $f_{V}^{(2)}$ is such that for any $j\\in[n]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{V}^{(2)}\\!\\left(x_{j}^{(2)}\\right)=\\left[\\begin{array}{c}{{0^{\\prime}}}\\\\ {{0}}\\\\ {{\\!\\!\\!0^{T-t-1}\\!\\!\\!1}}\\\\ {{\\vdots}}\\\\ {{\\!\\!\\!0^{m}}}\\\\ {{\\!\\!\\!0^{m}}}\\\\ {{\\!\\!\\!1}}\\\\ {{\\!\\!\\!0^{-1}\\!\\!\\!}}\\\\ {{\\!\\!\\!0^{0}\\!\\!\\!}}\\\\ {{\\!\\!\\!g(P_{1}(j),\\ldots,P_{t}(j))\\!\\!\\!}}\\\\ {{\\vdots}}\\end{array}\\right]\\!\\mathrm{~cha~}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So the output of the second self-attention, after the residual connection, is as follows. ", "page_idx": 21}, {"type": "text", "text": "If $i>1$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nc_{i}^{(2)}=f_{V}^{(2)}\\bigg(x_{j_{i}}^{(2)}\\bigg)+x_{i}^{(2)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nc_{i}^{(2)}=\\mathbf{0}+x_{i}^{(2)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=f_{V}^{*}\\left\\langle x_{i}^{0}\\right\\rangle+x_{i}^{*}=}\\\\ &{=\\left[\\begin{array}{c}{\\vec{P}\\left(i\\right)}\\\\ {\\rho\\left(i\\right)}\\\\ {0^{T-t-1}}\\\\ {\\vdots}\\\\ {\\vec{Q}\\left(P_{1}(i),\\ldots,P_{t}(i)\\right)}\\\\ {\\vdots}\\\\ {\\vec{P}\\left(P_{1}(i),\\ldots,P_{t}(i)\\right)}\\\\ {0}\\\\ {\\vdots}\\\\ {g\\left(P_{1}(i),\\ldots,P_{t}(i)\\right)}\\\\ {\\vdots}\\\\ {g\\left(P_{1}(j),\\ldots,P_{t}(j_{i})\\right)}\\\\ {\\vdots}\\end{array}\\right]\\mathrm{~af~flag~(falsc~}}\\\\ &{~~~~~~~~~\\left[\\begin{array}{c}{\\vec{Q}\\left(i\\right)}\\\\ {\\vec{Q}\\left(P_{1}(i),\\ldots,P_{t}(j_{i})\\right)}\\\\ {\\vdots}\\\\ {\\vec{Q}\\left(P_{1}(i),\\ldots,P_{t}(i)\\right)}\\\\ {\\vdots}\\\\ {\\vec{Q}\\left(P_{t}(i),\\ldots,P_{t}(i)\\right)}\\\\ {\\vdots}\\end{array}\\right]\\mathrm{~atendif~flag~(falsc~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\begin{array}{c}{\\vec{P}(i)}\\\\ {0}\\\\ {\\mathbf{0}^{T-t-1}}\\\\ {\\vdots}\\\\ {\\vec{\\partial}(P_{1}(i),\\dots,P_{t}(i))}\\\\ {\\vec{\\beta}(P_{1}(i),\\dots,P_{t}(i))}\\\\ {0}\\\\ {1}\\\\ {g(P_{1}(i),\\dots,P_{t}(i))}\\\\ {0}\\\\ {\\vdots}\\end{array}\\right]\\mathrm{~default~flag~(true)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The second feed-forward network $f_{P}^{(2)}$ checks the default flag. If it is $\\left[\\!\\!\\begin{array}{l}{1}\\\\ {0}\\end{array}\\!\\!\\right]$ , it copies the attended-to value $g(P_{1}(j_{i}),\\dots{},P_{t}(j_{i}))$ to the $\\left(t+1\\right)$ -st coordinate. If it is $\\left[\\!\\!\\begin{array}{l}{0}\\\\ {1}\\end{array}\\!\\!\\right]$ , it computes $D(i)$ (using Lemma 20) in the $\\left(t+1\\right)$ -st coordinate. Thus, the output after the residual connection is as follows. ", "page_idx": 21}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\overrightarrow{\\tau}}&{=f_{\\rightarrow}^{(2)}\\left(c_{i}^{(2)}\\right)+c_{i}^{(2)}}\\\\ &{=\\left[\\begin{array}{c c c c}{\\tilde{P}(\\hat{\\mu})}&{0}&{0}\\\\ {g(P_{1}(\\hat{\\mu}),\\ldots,P_{t}(j_{l}))}&{0}&{0}\\\\ {0}&{0}&{\\tilde{P}(j_{1},\\ldots,P_{t}(j_{l}))}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\tilde{\\sigma}(P_{1}(i),\\ldots,P_{t}(i))}&{0}&{\\tilde{P}(j_{1},\\ldots,P_{t}(i))}\\\\ {0}&{1}&{\\tilde{P}(j_{1},\\ldots,P_{t}(j_{l}))}\\\\ {g(P_{1}(i),\\ldots,P_{t}(i))}&{g(P_{1}(j_{l}),\\ldots,P_{t}(j_{l}))}&{\\tilde{P}(j_{l})}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\gamma}&{=f_{\\gamma}^{(2)}\\left(c_{i}^{(2)}\\right)+c_{i}^{(2)}}&{}\\\\ &{}&{+\\left[\\begin{array}{c c c}{\\hat{P}(i)}&{}&{}\\\\ {}&{\\hat{D}(i)}&{}\\\\ {}&{\\hat{D}(i)}&{}\\\\ {}&{0^{T-t-1}}&{}\\\\ {}&{}&{\\vdots}\\\\ {\\hat{\\alpha}(P_{1}(i),\\ldots,P_{t}(i))}&{}\\\\ {}&{}&{\\vdots}\\\\ {}&{}&{}\\\\ {}&{0}&{}\\\\ {g(P_{1}(i),\\ldots,P_{t}(i))}&{}\\\\ {}&{}&{0}\\end{array}\\right]}\\\\ &{}&{\\vdots}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In either case, the $\\left(t+1\\right)$ -st coordinate is now 1 if $w\\v{k}=P_{t+1}(i)$ and 0 otherwise. ", "page_idx": 21}, {"type": "text", "text": "The last step is to construct the output layer, which simply projects the final-layer activation vectors down to the coordinate that simulates $Y$ and subtracts $\\frac{1}{2}$ . This completes the proof of Theorem 3. ", "page_idx": 21}, {"type": "text", "text": "C.2 Proof of Theorem 4 (masked hard-attention transformers to B-RASP) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The key to the translation from masked hard-attention transformers to B-RASP is the following lemma: ", "page_idx": 22}, {"type": "text", "text": "Lemma 22. Let $\\mathcal{T}$ be a masked hard-attention transformer. There is a finite set $\\mathbb{F}\\subseteq\\mathbb{R}$ such that for all input strings $w$ , all the attention scores and activations computed by $\\mathcal{T}$ on input $w$ belong to $\\mathbb{F}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We prove that regardless of the input, layer $k$ has at most $(|\\Sigma|+1)^{2^{k}}-1$ different possible output activation vectors, by induction on $k$ . The base case is just the embedding function. Since there are no position embeddings, the embedding at position $i$ is determined entirely by $w_{i}$ , so there are at most $|\\Sigma|\\leq(|\\Sigma|+1)^{2^{k}}-1$ possible activation vectors. ", "page_idx": 22}, {"type": "text", "text": "Assume that $\\mathcal{T}$ has $\\left(k+1\\right)$ layers and that layer $k$ has at most $(|\\Sigma|+1)^{2^{k}}-1$ possible activation vectors. The self-attention\u2019s output at position $i$ depends only on two vectors: (1) layer $k$ \u2019s output at position $i$ (because of the residual connection) and (2) either layer $k$ \u2019s output at position $j_{i}$ (the position that $i$ attends to) or 0 (if there are no unmasked positions). Thus the number of possible activation vectors that the self-attention can output is at most ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\left(\\left\\vert\\Sigma\\right\\vert+1\\right)^{2^{k}}-1\\right)\\left(\\left\\vert\\Sigma\\right\\vert+1\\right)^{2^{k}}=\\left(\\left(\\left\\vert\\Sigma\\right\\vert+1\\right)^{2^{k}}\\right)^{2}-\\left(\\left\\vert\\Sigma\\right\\vert+1\\right)^{2^{k}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left(\\left\\vert\\Sigma\\right\\vert+1\\right)^{2^{k+1}}-\\left(\\left\\vert\\Sigma\\right\\vert+1\\right)^{2^{k}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\leq\\left(\\left\\vert\\Sigma\\right\\vert+1\\right)^{2^{k+1}}-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "And the number of possible activation vectors that the position-wise FFN can output is also at most $(|\\Sigma|+1)^{2^{k+1}}-1$ . ", "page_idx": 22}, {"type": "text", "text": "As for the attention scores, at layer $\\left(k+1\\right)$ , every attention score depends on two activation vectors from layer $k$ , so there are at most $\\left((|\\Sigma|+1)^{2^{k}}-1\\right)^{2}\\leq\\left(|\\Sigma|+1\\right)^{2^{k}}-1$ possible scores. ", "page_idx": 22}, {"type": "text", "text": "Then $\\mathbb{F}$ is the union over all layers of the possible attention scores and components of the possible activation vectors. \u25a1 ", "page_idx": 22}, {"type": "text", "text": "So any attention score or component of an activation vector can be represented using $B=\\lceil\\log_{2}\\left|\\mathbb{F}\\right|\\rceil$ bits. Define a mapping $\\langle\\cdot\\rangle:\\mathbb{F}\\overset{\\cdot}{\\longrightarrow}\\{0,\\ldots,2^{B}-1\\}$ such that $u<\\nu$ iff $\\langle u\\rangle<\\langle\\nu\\rangle$ , and write $\\langle\\nu\\rangle_{b}$ for the bit of $\\langle\\nu\\rangle$ with place value $2^{b}$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma 23. For any function $f\\colon\\ensuremath{\\mathbb{F}}\\to\\ensuremath{\\mathbb{F}}_{!}$ , there are Boolean formulas $\\phi_{b}(x_{1},\\dots,x_{B})$ for $b\\in[B]$ such that for any $x\\in\\mathbb{R}^{*}$ , $\\phi_{b}(\\langle x\\rangle_{1},\\ldots,\\langle x\\rangle_{B})$ holds iff $\\langle f(x)\\rangle_{b}=1$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. One way to define $\\phi_{b}$ is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi_{b}(x_{1},\\ldots,x_{B})=\\bigvee_{\\stackrel{x\\in\\mathbb{F}}{\\langle f(x)\\rangle_{b}=1}}\\left(\\bigwedge_{b^{\\prime}\\in[B]}x_{b^{\\prime}}\\wedge\\bigwedge_{\\stackrel{b^{\\prime}\\in[B]}{\\langle x\\rangle_{b^{\\prime}}=0}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!-x_{b^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Depending on the mapping $\\langle\\cdot\\rangle$ , more efficient definitions may be possible. ", "page_idx": 22}, {"type": "text", "text": "Hopefully, it is clear how to generalize this lemma to functions $\\mathbb{F}^{d}\\times\\mathbb{F}^{d}\\rightarrow\\{0,1\\}$ or $\\mathbb{F}^{d}\\rightarrow\\mathbb{F}^{d}$ . ", "page_idx": 22}, {"type": "text", "text": "Next, we prove Theorem 4. Let $\\mathcal{T}$ be a masked hard-attention transformer with width $d$ . Let $B$ be the number of bits needed to store $\\mathcal{T}$ \u2019s activation vector components and attention scores, by Lemma 22. A B-RASP program $\\mathcal{P}$ simulates $\\mathcal{T}$ if in $\\mathcal{P}$ there are Boolean vectors $Y_{c,b}$ for $c\\in[d]$ and $0\\le b<B$ such that for any input $w\\;\\in\\;\\Sigma^{+}$ of length $n$ , for all $i\\;\\in\\;[n]$ , $c\\ [d]$ , and $0\\,\\le\\,b\\,<\\,B$ , we have $w\\v{j}=Y_{c,b}(i)$ iff $\\langle[\\mathcal{T}(w)]_{i,c}\\rangle_{b}=1$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma 24. For any masked hard-attention transformer $\\mathcal{T}$ , there is a B-RASP program $\\mathcal{P}_{T}$ that simulates $\\mathcal{T}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We proceed by induction on the depth of $\\mathcal{T}$ . The base case is the input embedding function emb, which is simulated by Boolean vectors for $c\\in[d]$ and $0\\le b<B$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{Emb}_{c,b}(i):=\\bigwedge_{\\sigma\\in\\Sigma}\\left(Q_{\\sigma}(i)\\right)\\to\\left\\langle[e m b(\\sigma)]_{c}\\right\\rangle_{b}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Assume that the first $k$ layers of $\\mathcal{T}$ are simulated by a program $\\mathcal{P}$ . We extend $\\mathcal{P}$ to simulate layer $\\left(k+1\\right)$ as follows. ", "page_idx": 23}, {"type": "text", "text": "If the self-attention uses rightmost-hard attention with mask $M(i,j)$ , assume (by Lemma 23) that the score function $f_{S}(i,j)$ has been converted to Boolean expressions $S_{b}^{\\prime}(i,j)$ for the $b$ -th bit of the score for positions $i$ and $j$ , and the value function $f_{V}(j)$ has been converted to Boolean expressions $V_{c,b}^{\\prime}(j)$ for the $^b$ -bit of the $c$ -th coordinate of the value. ", "page_idx": 23}, {"type": "text", "text": "We give two translations. The first version has depth 1, which is important in Section 5.4. The second version is deeper in general, but much smaller. ", "page_idx": 23}, {"type": "text", "text": "Shallower version: ", "page_idx": 23}, {"type": "text", "text": "Because $\\mathbb{F}$ is finite, by Lemma 23 we can define, for all $\\nu\\in\\mathbb{R}$ , predicates ", "page_idx": 23}, {"type": "text", "text": "Then for each $\\nu\\in\\mathbb{F}$ , add operations for $\\mathrm{Max}_{\\nu}(i)$ , which check that the score $\\nu$ is the maximum, and $\\mathrm{Rightmost}_{\\nu,c,b}(i)$ , which retrieve the value at the rightmost position with score $\\nu$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{Max}_{\\nu}(i):=\\star_{j}\\left[M(i,j),S_{>\\nu}(i,j)\\right]\\ 0:1\\ \\qquad}\\\\ {\\operatorname{Rightmost}_{\\nu,c,b}(i):=\\star_{j}\\left[M(i,j),S_{\\nu}(i,j)\\right]\\ V_{c,b}^{\\prime}(j):0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then we can add operations for $\\mathrm{Att}_{c,b}(i)$ , which hold just in case the $b$ -th bit of the $c$ -th coordinate of the attention output is 1, by taking a disjunction over the finitely many possible scores: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{Att}_{c,b}(i):=\\bigvee_{\\nu\\in\\mathbb{F}}(\\mathbf{M}\\mathbf{ax}_{\\nu}(i)\\wedge\\operatorname{Rightmost}_{\\nu,c,b}(i)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Smaller version: ", "page_idx": 23}, {"type": "text", "text": "We need to define a predicate Argmax $(i,j)$ that tests whether $j$ maximizes $S(i,j)$ . To do this, we define a sequence of Boolean vectors that test whether $j$ maximizes bits $b,\\ldots,B-1$ of $S(i,j)$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Argmax}_{B}(i,j)=1\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathrm{Max}_{b}(i):=\\bullet_{j}\\left[M(i,j),\\mathrm{Argmax}_{b+1}(i,j)\\wedge S_{b}^{\\prime}(i,j)\\right]\\ 1:0}}\\\\ {{\\mathrm{Argmax}_{b}(i,j)=\\displaystyle\\bigwedge_{b^{\\prime}=b}^{B-1}\\left(S_{b^{\\prime}}^{\\prime}(i,j)\\leftrightarrow\\mathrm{Max}_{b^{\\prime}}(i)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Argmax}(i,j)=\\mathrm{Argmax}_{0}(i,j)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, we add operations that simulate attention: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Att}_{c,b}(i):=\\bullet_{j}\\left[M(i,j),\\mathrm{Argmax}(i,j)\\right]\\;V_{c,b}^{\\prime}(i,j):0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To simulate leftmost-hard attention, simply change $\\blacktriangleright$ to $\\blacktriangleleft$ . ", "page_idx": 23}, {"type": "text", "text": "For the position-wise feed-forward network, use Lemma 23. ", "page_idx": 23}, {"type": "text", "text": "The last step in the program is to use position-wise operations to simulate $\\mathcal{T}$ \u2019s output layer, yielding an output Boolean vector \ud835\udc4c. This completes the proof of Theorem 4. ", "page_idx": 23}, {"type": "text", "text": "D Proofs for Section 5 (Further Results) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 Proof of Theorem 7 (position embeddings can be simulated by predicates) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Because $\\Theta$ has finite image, Lemma 22 still holds for any masked hard-attention transformer with position embedding $\\Theta$ . Let $\\mathcal{P}_{\\Theta}$ be the collection of predicates that test whether the $^b$ -th bit of the $c$ -th coordinate of $\\theta_{n}(i)$ is set. The proof of equivalence of masked hard-attention transformers with B-RASP extends easily to equivalence of masked hard-attention transformers with position embedding $\\Theta$ and $\\mathbf{B}{\\mathrm{-}}\\mathbf{R}{\\mathbf{A}}{\\mathbf{S}}\\mathbf{P}[\\mathcal{P}_{\\Theta}]$ . When converting a transformer to a $\\mathbf{B-RASP}[\\mathcal{P}_{\\Theta}]$ program, we represent each coordinate of $\\Theta$ with $B$ predicates from $\\mathcal{P}_{\\Theta}$ . When converting a $\\mathbf{B-RASP}[\\mathcal{P}_{\\Theta}]$ program to a transformer, we represent each predicate in $\\mathcal{P}_{\\Theta}$ with its own coordinate, whose value is in $\\{0,1\\}$ . ", "page_idx": 23}, {"type": "text", "text": "Since Theorems 1 and 2 hold for any collection of unary predicate symbols, $\\mathbf{B-RASP}[\\mathcal{P}_{\\Theta}]$ is equivalent to $\\mathbf{LTL}[\\mathcal{P}_{\\Theta}]$ . ", "page_idx": 23}, {"type": "text", "text": "D.2 Proof of Corollary 8 (masked hard-attention transformers with sinusoidal position embeddings recognize the regular languages in $\\mathbf{AC}^{0}$ ) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let MOD be the collection of predicates $\\mathrm{MOD}_{m}^{r}(i)$ for all $0\\leq r<m$ , which hold just in case $i\\equiv r$ (mod $m$ ). ", "page_idx": 24}, {"type": "text", "text": "Let $\\Theta$ be a sinusoidal positional embedding. Since the $f_{c}$ are rational, $\\Theta$ has finite image. By Theorem 7, transformers with positional embedding $\\Theta$ are equivalent to $\\mathbf{LTL}[\\mathcal{P}_{\\Theta}]$ . ", "page_idx": 24}, {"type": "text", "text": "It\u2019s easy to see that every predicate in $\\mathcal{P}_{\\Theta}$ can be expressed in terms of MOD; for the converse, observe that we can use a 2-layer ReLU network to compute $\\mathrm{MOD}_{m}^{r}$ (Chiang et al., 2023, Lemma 20): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!\\!\\!\\!\\!\\!\\!h(i)=\\mathrm{ReLU}\\left(\\sin2\\pi r/m\\sin2\\pi i/m+\\cos2\\pi r/m\\cos2\\pi i/m-\\cos2\\pi/m\\right)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\mathrm{ReLU}(\\cos(2\\pi(i-r)/m))}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\mathrm{ReLU}(\\cos(2\\pi/m)h(i).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus transformers with sinusoidal positional embeddings are equivalent to LTL[MOD], which is equivalent to ${\\bf F O}[<,\\mathrm{MOD}]$ (Kamp, 1968), which defines exactly the class of regular languages in $\\bar{\\mathbf{AC}}^{0}$ (Barrington et al., 1992). ", "page_idx": 24}, {"type": "text", "text": "D.3 Details for Section 5.4 (depth hierarchy) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "D.3.1 Multi-head attention ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To prove Theorem 10 and related results, we need to make Theorem 3 more efficient in terms of the depth of the constructed transformer. To do this, we\u2019ll need to make use of multi-head attention. This allows multiple self-attentions at the same depth to be run in parallel. In a multi-head masked hard-attention transformer transformer layer, the equation for the self-attention (Equation (1)) is replaced by ", "page_idx": 24}, {"type": "equation", "text": "$$\n(c_{1},\\ldots,c_{n})=\\sum_{h=1}^{H}a t t.h(x_{1},\\ldots,x_{n})+(x_{1},\\ldots,x_{n})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where each att. $h$ is a self-attention layer. ", "page_idx": 24}, {"type": "text", "text": "It is straightforward to extend Theorem 4 to multi-head masked hard-attention transformers, simulating a multi-head masked hard-attention transformer of depth $k$ with a B-RASP program of depth $k$ . Each head at depth $k$ can be simulated by a B-RASP attention operation of attention depth $k$ , and their sum can be simulated by a position-wise operation (Lemma 23). ", "page_idx": 24}, {"type": "text", "text": "D.3.2 Parallel composition ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The parallelization is accomplished by the following construction. ", "page_idx": 24}, {"type": "text", "text": "Lemma 25. A transformer $\\mathcal{T}_{1}$ of depth $k_{1}$ with $H_{1}$ heads and a transformer $\\mathcal{T}_{2}$ of depth $k_{2}$ with $H_{2}$ heads can be parallel-composed into a transformer $\\mathcal{T}_{1}\\oplus\\mathcal{T}_{2}$ of depth $\\operatorname*{max}(k_{1},k_{2})$ with $H_{1}+H_{2}$ heads such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\mathcal{T}_{1}\\oplus\\mathcal{T}_{2})(w)=\\left[\\mathcal{T}_{1}(w)\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. First, add layers that compute the identity function to the shallower transformer so that both have depth max $(k_{1},k_{2})$ . ", "page_idx": 24}, {"type": "text", "text": "Next, concatenate their word embedding vectors ", "page_idx": 24}, {"type": "equation", "text": "$$\n(e m b_{1}\\oplus e m b_{2})(\\sigma)=\\left[e m b_{1}(\\sigma)\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "At each level, we compose the self-attentions using multiple heads to simulate them in parallel. For each multi-head self-attention layer $a t t_{1}$ and $a t t_{2}$ at the same depth in each transformer, we use multiple heads to simulate both $a t t_{1}$ and $a t t_{2}$ in parallel. Let $a t t_{1}.h.f_{S}$ be the score function of ", "page_idx": 24}, {"type": "text", "text": "the $h$ -th head of $a t t_{1}$ , and similarly for $a t t_{1}.h.M,a t t_{1}.h.C$ , and $a t t_{1}.h.f_{V}$ , and similarly for $a t t_{2}$ . Let $d={d_{1}+d_{2}}$ and $H=H_{1}+H_{2}$ . Construct a new self-attention layer $a t t_{1}\\oplus a t t_{2}$ with ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\textrm{l}\\oplus a t t_{2}\\big),h.f_{S}(x_{i},x_{j})=\\left\\{\\!\\!a t t_{1},h.f_{S}([x_{i}]_{\\mathrm{L}(\\cdot)},[x_{j}]_{d+1:d})\\!\\!\\right.}&{\\left.1\\leq h\\leq H_{1}\\right.}\\\\ {\\textrm{l}\\qquad\\qquad\\!}&{=\\!\\!\\!}\\\\ {\\left(a t t_{1}\\oplus a t_{2}\\!\\right),h.M=\\left\\{\\!\\!a t t_{1},h.M\\!\\!\\right.}&{\\left.1\\leq h\\leq H_{1}\\!\\right.}\\\\ {\\qquad\\qquad\\qquad\\qquad\\left.\\left(a t t_{2},\\!\\!\\left(h-H_{1}\\right)\\!\\!\\right)\\!\\!,H}&{\\left.H_{1}+1\\leq h\\leq H}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\right.}\\\\ {\\left(a t t_{1}\\oplus a t t_{2}\\right),h.C=\\left\\{\\!\\!a t t_{1},h.C\\!\\!\\right.}&{\\left.1\\leq h\\leq H_{1}\\!\\right.}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left[\\!\\left[\\begin{array}{c}{\\left(\\!\\!\\!-\\!h t_{1}\\!,\\!\\cdot\\!\\!,f_{S}(x_{1}\\!\\!\\!_{1:\\mathrm{L}:\\!\\!})\\!\\!\\right)\\!\\!,d\\!\\!\\!_{1}\\!\\leq h\\leq H}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.1\\leq h\\leq H_{1}\\right.}\\end{array}\\right.\\right.}\\\\ {\\left.\\big(a t t_{1}\\oplus a t t_{2}\\big),h.f_{V}(x)=\\left\\{\\!\\!\\left[\\begin{array}{c c}{\\!\\!\\!\\qquad\\qquad\\!\\mathbf{q}^{d_{2}}\\!\\!\\!\\!}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ {\\!\\!\\!\\!\\!\\!\\qquad\\qquad\\qquad\\quad\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the feed-forward networks ${\\hat{J}}\\!n_{1}$ and ${\\hat{J}}\\!n_{2}$ , create a new network ${\\hat{p}}\\!n_{1}\\oplus{\\hat{p}}\\!n_{2}$ with ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{(f\\!\\!f\\!\\!n_{1}\\oplus f\\!\\!f\\!\\!n_{2}).W^{(1)}=\\left[\\!\\!\\begin{array}{c c}{{\\![f\\!\\!f\\!n_{1}.W^{(1)}}}&{{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is straightforward to verify the correctness of this construction. ", "page_idx": 25}, {"type": "text", "text": "D.3.3 B-RASP to masked hard-attention transformers, preserving depth ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We give a more efficient version of Theorem 3, which uses parallel composition to optimize the depth of the constructed transformer. ", "page_idx": 25}, {"type": "text", "text": "Lemma 26. Let $\\mathcal{T}$ be a transformer (without output layer) with width \ud835\udc51and depth $k$ , and whose activations are in $\\{0,1\\}$ . For any function $g\\colon\\{0,1\\}^{\\dot{d}}\\rightarrow\\dot{\\{0,1\\}}^{d}$ , there is a transformer $(g\\circ{\\mathcal{T}})$ with depth $k$ such that, for all $w$ and $i$ , $[(g\\circ\\mathcal{T})(w)]_{i}=g([\\mathcal{T}(w)]_{i})$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. If $k=0$ : $\\mathcal{T}$ consists of just an embedding function emb: $\\Sigma\\rightarrow\\{0,1\\}^{d}$ . Then $g\\circ\\mathcal{T}=g\\circ$ emb is also an embedding function and therefore a depth-0 transformer. ", "page_idx": 25}, {"type": "text", "text": "If $k>0$ : Let $f\\colon\\{0,1\\}^{d}\\to\\{0,1\\}^{d}$ be the top FFN of $\\mathcal{T}$ . Then $g\\circ f$ is also a function $\\{0,1\\}^{d}\\rightarrow$ $\\{0,1\\}^{d}$ and can therefore be computed by a single FFN, by Lemma 20. \u25a1 ", "page_idx": 25}, {"type": "text", "text": "Theorem 27. For any B-RASP program $\\mathcal{P}$ of depth $k$ that recognizes a language $L\\subseteq\\Sigma^{+}$ , there is $a$ multi-head masked hard-attention transformer with depth $k$ that recognizes $L$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Let $\\mathcal{P}$ be any B-RASP program. For any operation $P_{t}(i)$ of $\\mathcal{P}$ , we say that a transformer $\\mathcal{T}_{t}$ with width $d$ simulates $P_{t}(i)$ if there is a $c\\in[d]$ such that, for all $w\\in\\Sigma^{+}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\mathcal{T}_{t}(w)]_{i,c}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\;w\\models P_{t}(i)}\\\\ {0}&{\\mathrm{otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We prove the following statement by induction on $t$ : For any operation $P_{t}(i)$ of $\\mathcal{P}$ with depth $k$ , there is a multi-head masked hard-attention transformer with depth $k$ that simulates $P_{t}(i)$ . ", "page_idx": 25}, {"type": "text", "text": "The base cases are $t\\leq|\\Sigma|$ , where every operation can be simulated by a transformer with depth 0 using one-hot word embeddings, just as in the proof of Theorem 3. ", "page_idx": 25}, {"type": "text", "text": "If $t>|\\Sigma|$ , assume that each previous operation $P_{t^{\\prime}}(i)$ with depth $k^{\\prime}$ can be simulated by a transformer with depth $k^{\\prime}$ . We will construct a transformer of depth $k$ that simulates $P_{t}(i)$ . ", "page_idx": 25}, {"type": "text", "text": "\u2022 If $P_{t}(i)$ is an attention operation, then its $S,V$ , and $D$ predicates have depth at most $\\left(k-1\\right)$ and therefore depend only on operations which can be simulated by transformers of depth $\\left(k-1\\right)$ , by the inductive hypothesis. Parallel-compose all of these into a single transformer (using Lemma 25) to obtain a transformer $\\tilde{T}_{t}$ of depth $\\left(k-1\\right)$ that simulates all the operations that $P_{t}(i)$ depends on. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Then, extend $\\tilde{T}_{t}$ to compute the attention operation as in the proof of Theorem 3. Although that construction uses two transformer layers, the first layer\u2019s self-attention just computes the identity function, and its FFN can be fused with the last FFN in $\\tilde{T}_{t}$ by Lemma 26. So there exists a transformer $T_{t}$ of depth $k$ that simulates $P_{t}(i)$ . ", "page_idx": 26}, {"type": "text", "text": "\u2022 If $P_{t}(i)$ is a position-wise operation of depth $k$ , it may depend on earlier operations that also have depth $k$ . By the inductive hypothesis, these can be simulated by transformers of depth $k$ . Parallel-compose them to get a transformer $\\tilde{T}_{t}$ of depth $k$ , by Lemma 25. Then, the computation of $P_{t}(i)$ itself can be fused with the last feed-forward network in $\\tilde{T}_{t}$ by Lemma 26. This again results in a transformer $T_{t}$ of depth $k$ that simulates $P_{t}(i)$ . ", "page_idx": 26}, {"type": "text", "text": "Thus, if $P_{T}$ is the output vector of $\\mathcal{P}$ and has depth $k$ , then there is a transformer that simulates $P_{T}$ and has depth $k$ . Add an output layer that transforms the output at position $n$ to $+{\\frac{1}{2}}$ if $w\\,\\vDash\\,P_{T}(n)$ and $-\\frac{1}{2}$ otherwise. Then, $T$ recognizes the same language that $\\mathcal{P}$ recognizes. \u25a1 ", "page_idx": 26}, {"type": "text", "text": "D.3.4 Other attention variants ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Earlier, we used transformers and B-RASP with only future-masked rightmost-hard-attention, and LTL with only since. Theorem 10 showed that ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\cdots\\subset}&{\\mathbf{LTL}(\\mathbf{since})_{k}\\ \\ \\subset\\ \\ \\mathbf{LTL}(\\mathbf{since})_{k+1}\\ \\ \\ \\subset\\cdots}\\\\ {\\qquad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\mu}_{||}}&{\\qquad\\qquad\\ \\ \\ \\ \\mathbf{\\mu}_{||}}\\\\ {\\mathbf{B-RASP}(\\mathbf{\\boldsymbol{\\nu}}\\mathbf{\\cdot}F)_{k}\\ \\ \\ }&{\\mathbf{B-RASP}(\\mathbf{\\boldsymbol{\\nu}}F)_{k+1}}\\\\ {\\qquad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\mu}_{||}}&{\\qquad\\qquad\\ \\ \\ |}\\\\ {\\mathbf{MUHHAT}(\\mathbf{\\boldsymbol{\\nu}}F)_{k}\\ \\ \\ }&{\\mathbf{MUHAT}(\\mathbf{\\boldsymbol{\\nu}}F)_{k+1}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The separating language was ${\\mathrm{STAIR}}_{k+1}$ , which is the language over $\\Sigma=\\{a,b,c\\}$ of strings which, after deleting $c$ \u2019s, contain $a^{k+1}$ as a substring. We can recognize ${\\mathrm{STAIR}}_{k}$ with a formula $\\varphi_{k}=1$ since $\\gamma_{k}$ , defined as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{1}=Q_{a}}\\\\ &{\\gamma_{k}=Q_{a}\\land(Q_{c}\\;\\mathbf{since}\\;\\gamma_{k-1}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note the slight deviation from Etessami and Wilke (2000), because their presentation of LTL used the next and eventually operators as well as non-strict until\u2032. ", "page_idx": 26}, {"type": "text", "text": "Next, we allow both future-masked rightmost and past-masked leftmost attention. We will notate these with $\\mathbf{MUHAT}(\\star F,\\star P)_{k}$ and $\\mathbf{B-RASP}(\\star F,\\dot{\\mathbf{\\omega}}^{\\star P})_{k}$ , respectively. In LTL, we allow access to both temporal operators; let $\\mathbf{LTL}_{k}$ be the languages definable by formulas of depth $k$ . ", "page_idx": 26}, {"type": "text", "text": "Proposition 28. Restricted to only rightmost future-masked and leftmost past-masked attention, multi-head masked hard-attention transformers with depth $\\left(k+1\\right)$ are strictly more expressive than multi-head masked hard-attention transformers with depth $k$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. The constructions in Theorems 1 and 2 preserve depth for rightmost future-masked and leftmost past-masked attention, so $\\mathbf{B}\\mathbf{-RASP}(\\mathbf{\\boldsymbol{\\upnu}}F,\\mathbf{\\boldsymbol{\\bullet}}P)_{k}=\\mathbf{LTL}_{k}$ . Moreover, the constructions in Theorems 3 and 4 are identical regardless of attention type, so $\\mathbf{MUHAT}(\\star F,\\,\\bullet P)_{k}=\\mathbf{B}{\\mathbf{-RASP}}(\\star F,\\,\\bullet P)_{k}$ . Finally, Etessami and Wilke (2000) show that with access to both temporal operators it is still the case that $\\mathbf{LTL}_{k-1}\\subsetneq\\mathbf{LTL}_{k}$ (except the separating language is ${\\mathrm{STAIR}}_{2k}$ instead of ${\\mathrm{STAIR}}_{k}$ ). Thus, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{~LTL}_{k}\\;\\;\\;\\;\\;\\;\\;\\;}&{\\subseteq\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and in particular, ", "page_idx": 26}, {"type": "text", "text": "Finally, we allow all six types of attention. These will be notated with $\\mathbf{M}\\mathbf{U}\\mathbf{H}\\mathbf{A}\\mathbf{T}_{k}$ and $\\mathbf{B}{\\mathrm{-}}\\mathbf{R}\\mathbf{A}\\mathbf{S}\\mathbf{P}_{k}$ . ", "page_idx": 26}, {"type": "text", "text": "Proposition 29. With access to future-, past-, and no masking and both leftmost-hard and rightmosthard attention, multi-head masked hard-attention transformers of depth $(2k+1)$ are strictly more expressive than multi-head masked hard-attention transformers of depth $k$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. As above, $\\mathbf{MUHAT}_{k}=\\mathbf{B}{-}\\mathbf{RASP}_{k}$ . However, in the proof of Theorem 2, the simulations of leftmost future-masked and rightmost past-masked attention require two levels of nesting of since and until, so it only shows that $\\mathbf{B}\\mathbf{-RASP}_{k}\\subseteq\\mathbf{LTL}_{2k}$ . As in the previous proof, $\\mathbf{LTL}_{2k}\\subsetneq\\mathbf{LTL}_{2k+1}$ (Etessami and Wilke, 2000). Finally, by Theorem 1, we again have $\\mathbf{LTL}_{2k+1}\\subseteq\\mathbf{B-RASP}_{2k+1}$ . Using all these observations, we conclude that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\cdots\\subsetneq\\underset{\\bigcup}{\\mathbf{L}}\\mathbf{T}\\mathbf{L}_{2k}}&{\\subsetneq\\underset{\\bigcup}{\\mathbf{L}}\\mathbf{T}\\mathbf{L}_{2k+1}\\quad\\subset\\cdots}\\\\ {\\underset{\\mathbf{B}\\mathbf{-R}\\mathbf{A}\\mathbf{S}\\mathbf{P}_{k}}{\\mathbf{B}}}&{\\mathbf{B}\\mathbf{-R}\\mathbf{A}\\mathbf{S}\\mathbf{P}_{2k+1}}\\\\ {\\underset{\\bigcup}{\\mathbf{\\upmu}}\\mathbf{U}\\mathbf{H}\\mathbf{A}\\mathbf{T}_{k}}&{\\mathbf{M}\\mathbf{U}\\mathbf{H}\\mathbf{A}\\mathbf{T}_{2k+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus $\\mathbf{MUHAT}_{k}\\subset\\mathbf{MUHAT}_{2k+1}$ . ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The abstract and introduction both summarize the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Sections 2.2 and 6 address the main limitation of the paper, which is that the kind of transformers studied (unique-hard attention) differ from actual transformers. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Theorems 1 to 7 and 10 and Corollary 8 all have proofs in the appendix and brief proof ideas in the main text, when appropriate. Corollary 9 is very straightforward and in our opinion does not need an explicit proof. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include any experimental results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The only code is a public github page which contains a B-RASP simulator implemented in HTML and Javascript. Using it, it is straightforward to recreate and run the example programs in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not include any experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not include any experimental results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not include any experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and believe that the research in this paper conforms to it. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper is purely theoretical, and we do not foresee any direct societal impacts. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper is purely theoretical and poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The only new asset is a webpage hosted in a publicly accessible github repository. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]