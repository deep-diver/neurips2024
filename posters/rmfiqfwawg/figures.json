[{"figure_path": "RMfiqfWAWg/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison between our work and previous work. Previous methods only use pre-tuned parameters \u03b1 to transfer knowledge from a single expert. In contrast, our method dynamically adjusts the proportion of knowledge transferred from multiple experts at each decoding step during inference.", "description": "This figure compares the authors' proposed method with previous methods for knowledge transfer in large language models. Previous methods used a static weight (\u03b1) to transfer knowledge from a single small model to a larger model.  The authors' method dynamically adjusts the weights from multiple small models at each decoding step, resulting in more adaptable and accurate knowledge transfer.", "section": "1 Introduction"}, {"figure_path": "RMfiqfWAWg/figures/figures_6_1.jpg", "caption": "Figure 2: Compare the pre-defined \u03b1 with the dynamic \u03b1 for different tasks.", "description": "This figure compares the performance of using a pre-defined alpha (\u03b1) value versus the dynamically adjusted alpha value in the proposed adaptive logit fusion approach across different tasks.  The pre-defined \u03b1 values tested were 0.5, 1.0, and 1.5, while 'Ours' represents the dynamically learned \u03b1.  The x-axis represents the different tasks (GSM8K, TruthfulQA, TriviaQA, CNN/DM, and MMLU), and the y-axis shows the performance, likely measured as accuracy or another relevant metric.  The bars visually compare the performance of each method on each task, highlighting the superiority of the dynamic \u03b1 learning approach in achieving better performance across the board.", "section": "4.3 Baselines"}, {"figure_path": "RMfiqfWAWg/figures/figures_6_2.jpg", "caption": "Figure 3: The variation of \u03b1 in knowledge transfer for the GSM8K expert. Lower Bound and Upper Bound represent the minimum and maximum values obtained during the optimization process. In the", "description": "This figure shows how the weight \u03b1, which controls the knowledge transfer from the GSM8K expert model to the large language model, changes during the decoding process.  The x-axis represents the decoding step, and the y-axis represents the value of \u03b1. The plot shows the actual \u03b1 value during optimization (in gold), along with the upper and lower bounds of \u03b1 found during the optimization process (dashed lines). The variability and range of the \u03b1 values illustrate the adaptive nature of the knowledge transfer process and its sensitivity to the specifics of each decoding step.", "section": "4.3 Baselines"}, {"figure_path": "RMfiqfWAWg/figures/figures_7_1.jpg", "caption": "Figure 2: Compare the pre-defined \u03b1 with the dynamic \u03b1 for different tasks.", "description": "This figure compares the performance of using a pre-defined alpha (\u03b1) value versus a dynamically adjusted alpha value in the proposed logit arithmetic method.  The x-axis represents different tasks, and the y-axis represents the alpha value.  The figure shows that using a dynamically adjusted alpha leads to better performance compared to a fixed alpha value across different tasks.", "section": "4.3 Baselines"}, {"figure_path": "RMfiqfWAWg/figures/figures_8_1.jpg", "caption": "Figure 5: Enhance in-context learning and task arithmetic using our method.", "description": "This figure compares the performance of in-context learning (ICL), the proposed method, and the combination of both (ICL+Ours) across three categories: single-domain tasks, multi-domain tasks, and the average of all tasks.  The bar chart illustrates that combining the proposed method with ICL leads to improved performance, particularly for multi-domain tasks, demonstrating the synergistic effects of both techniques.", "section": "5.3 Comparison the Weak-to-strong Approach with Other Methods"}, {"figure_path": "RMfiqfWAWg/figures/figures_8_2.jpg", "caption": "Figure 5: Enhance in-context learning and task arithmetic using our method.", "description": "This figure compares the performance of three methods:  In-context learning (ICL), the proposed dynamic logit fusion method (\"Ours\"), and a combination of both (ICL+Ours).  The comparison is done across three categories: single-domain tasks, multi-domain tasks, and an average across all tasks.  The results show that the proposed method outperforms in-context learning alone and that combining the methods yields even better results, especially for multi-domain tasks.", "section": "5.3 Comparison the Weak-to-strong Approach with Other Methods"}, {"figure_path": "RMfiqfWAWg/figures/figures_15_1.jpg", "caption": "Figure 6: The architecture of our method. The small llama represents the small model, while the large llama represents the large model. MS/ML denotes the logits of the small/large language model. MS represents the logits of a small expert language model for task t. The lower part of the figure illustrates our optimization in the decoding process, where each circle represents a decoding step. The upper part of the figure shows how our method transfers the {\u03b1t}T=1 value for each expert at each decoding step, dynamically adjusting the {\u03b1t}T=1 value for each expert, transferring knowledge from the small models to the larger model.", "description": "This figure illustrates the architecture and process of the proposed method.  The top section shows the k-th decoding step, where multiple small expert models (small llamas) contribute to the logits of a large language model (large llama). Their logit outputs are combined through a weighted sum, where weights (\u03b11, \u03b12, \u03b13) are learned adaptively. The bottom section depicts the adaptive knowledge transfer optimization process, showing how these weights are dynamically adjusted at each decoding step to optimally fuse knowledge from the experts.", "section": "3 Methodology"}]