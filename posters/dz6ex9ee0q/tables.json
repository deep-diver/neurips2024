[{"figure_path": "dz6ex9Ee0Q/tables/tables_7_1.jpg", "caption": "Table 1: Adaptive local attack on Cora ML. The best and second are marked.", "description": "This table presents the results of an adaptive local attack on the Cora ML dataset.  The attack is designed to misclassify specific target nodes (local attack) by perturbing the graph structure. The table shows the accuracy of different graph neural network (GNN) models under varying attack budgets (percentage of edges allowed to be perturbed relative to the target node's degree). The performance of various GNNs and baseline methods are listed under varying attack strength, from 0% (clean) to 200%. The best and second-best performing models for each attack budget are highlighted.", "section": "4.2 Adversarial Robustness"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_7_2.jpg", "caption": "Table 1: Adaptive local attack on Cora ML. The best and second are marked.", "description": "This table presents the results of an adaptive local attack on the Cora ML dataset.  The attack involves perturbing a limited number of edges near the target node.  The table shows how various graph neural network (GNN) models, both standard and robust, perform under this attack, as measured by node classification accuracy. The percentage of edges allowed to be perturbed is given on the x-axis, with performance degradation shown as the attack budget increases. The best and second-best performing models are marked for each perturbation level.", "section": "4.2 Adversarial Robustness"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_20_1.jpg", "caption": "Table 1: Adaptive local attack on Cora ML. The best and second are marked.", "description": "This table presents the results of an adaptive local attack on the Cora ML dataset.  It shows the performance (accuracy) of various GNN models, including both standard and robust models, under different attack budgets (percentage of edges perturbed relative to the target node's degree). The best and second-best performing models for each budget are highlighted.  The goal is to demonstrate the robustness (or lack thereof) of different GNN architectures against adaptive attacks, where the attacker adjusts its strategy based on the model's response.", "section": "4.2 Adversarial Robustness"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_20_2.jpg", "caption": "Table 1: Adaptive local attack on Cora ML. The best and second are marked.", "description": "This table presents the results of an adaptive local attack on the Cora ML dataset.  The experiment measures the robustness of various graph neural network (GNN) models against adversarial attacks by gradually increasing the attack budget (percentage of edges allowed to be perturbed relative to the target node's degree). The table shows the classification accuracy of each model at different attack budget levels (0%, 20%, 50%, 100%, 150%, 200%).  The best and second-best performing models for each budget are highlighted.", "section": "4.2 Adversarial Robustness"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_21_1.jpg", "caption": "Table 2: Adaptive global attack on Cora ML. The best and second are marked.", "description": "This table presents the results of a global adaptive attack on the Cora ML dataset.  The attack perturbs the graph structure to adversarially affect node classification accuracy.  The table shows the performance (accuracy \u00b1 standard deviation) of various GNN models (including the proposed RUNG) at different attack budgets (percentage of perturbed edges).  The best and second-best performing models for each budget are highlighted.", "section": "4.2 Adversarial Robustness"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_21_2.jpg", "caption": "Table 1: Adaptive local attack on Cora ML. The best and second are marked.", "description": "This table presents the results of an adaptive local attack on the Cora ML dataset.  The attack is designed to perturb the graph structure locally around target nodes, and its effectiveness is measured by the classification accuracy under different attack budgets (percentage of allowed edge perturbations). The table compares the performance of various GNN models, including standard models and robust defenses, demonstrating their resilience to the attack and highlighting the top-performing models.", "section": "4.2 Adversarial Robustness"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_23_1.jpg", "caption": "Table 2: Adaptive global attack on Cora ML. The best and second are marked.", "description": "This table presents the results of a global adaptive attack on the Cora ML dataset.  The experiment evaluates the robustness of various graph neural network (GNN) models against this attack.  The \"Budget\" column represents the percentage of edges that were perturbed in the graph. The remaining columns show the classification accuracy (with standard deviation) of each model under varying attack budgets.  The best-performing models for each attack budget are marked.", "section": "4.2 Adversarial Robustness"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_23_2.jpg", "caption": "Table 7: Poisoning Attacks on Cora ML.", "description": "This table presents the results of poisoning attacks on the Cora ML dataset.  It shows the classification accuracy (%) of different GNN models (GCN, APPNP, SoftMedian, RUNG-l1, and RUNG) under various poisoning attack budgets (5%, 10%, 20%, 30%, and 40%).  The results demonstrate the robustness of the RUNG models against poisoning attacks, showing higher accuracy than other models, especially as the attack budget increases.", "section": "E.3 Poisoning Attacks"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_23_3.jpg", "caption": "Table 9: Global PGD Attacks on Ogbn-Arxiv.", "description": "This table presents the results of global PGD attacks on the large-scale Ogbn- Arxiv dataset.  It compares the performance of GCN, APPNP, SoftMedian, RUNG-l1 (a variant of RUNG using the l1 penalty), and RUNG (the proposed model) under different attack intensities (1%, 5%, and 10%). The \"Clean\" column shows the accuracy without any attack.  The results demonstrate the relative robustness of the different models against these attacks.", "section": "E.4 Large Scale Ogbn-Arxiv"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_23_4.jpg", "caption": "Table 10: Adversarial Training vs Normal Training on RUNG.", "description": "This table compares the performance of RUNG under normal training and adversarial training against adaptive attacks with varying budgets (5%, 10%, 20%, 30%, 40%).  The results show the accuracy of the model on the clean data and under attack with different attack budgets. Adversarial training improves the model's robustness against attacks. ", "section": "E.5 Adversarial Training"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_24_1.jpg", "caption": "Table 11: Graph Injection Attack on Citeseer.", "description": "This table presents the results of a graph injection attack on the Citeseer dataset.  The \"Clean\" column shows the accuracy of each model on clean data, while the \"Attacked\" column shows the accuracy after a graph injection attack.  The models compared include GCN, APPNP, GNNGuard, SoftMedian, RUNG-l1, and RUNG-MCP (the proposed model).  The table demonstrates the relative robustness of each model against graph injection attacks, showing that RUNG-MCP outperforms other models in maintaining accuracy after the attack.", "section": "E.6 Graph Injection Attack"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_27_1.jpg", "caption": "Table 12: Comparison between APPNP-based and GCN-based QN-IRLS on Cora.", "description": "This table presents a comparison of the performance of different models on the Cora dataset under adaptive attacks.  The models compared include RUNG-l1 (using the l1 penalty), RUNG (using the MCP penalty), RUNG-l1-GCN (l1 penalty applied to GCN), and RUNG-GCN (MCP penalty applied to GCN). The performance is measured by accuracy under different attack budgets (0%, 5%, 10%, 20%, 30%, 40%).  This allows a comparison of the robustness of the models with different penalty functions and GCN architectures.", "section": "4.2 Adversarial Robustness"}, {"figure_path": "dz6ex9Ee0Q/tables/tables_27_2.jpg", "caption": "Table 1: Adaptive local attack on Cora ML. The best and second are marked.", "description": "This table presents the results of an adaptive local attack on the Cora ML dataset.  It compares the performance of various graph neural network (GNN) models, including baselines and robust GNNs, at different attack budgets (percentage of edges perturbed relative to a node's degree).  The models' node classification accuracy is shown, illustrating their robustness against adaptive attacks.  The best and second-best performing models for each attack budget are highlighted.", "section": "4.2 Adversarial Robustness"}]