[{"type": "text", "text": "Robust Graph Neural Networks via Unbiased Aggregation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhichao Hou1\u2217 Ruiqi Feng1\u2217 Tyler Derr2 Xiaorui Liu1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1North Carolina State University, 2Vanderbilt University ", "page_idx": 0}, {"type": "text", "text": "{zhou4,xliu96}@ncsu.edu ruiqifeng.2024@gmail.com tyler.derr@vanderbilt.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The adversarial robustness of Graph Neural Networks (GNNs) has been questioned due to the false sense of security uncovered by strong adaptive attacks despite the existence of numerous defenses. In this work, we delve into the robustness analysis of representative robust GNNs and provide a unified robust estimation point of view to understand their robustness and limitations. Our novel analysis of estimation bias motivates the design of a robust and unbiased graph signal estimator. We then develop an efficient Quasi-Newton Iterative Reweighted Least Squares algorithm to solve the estimation problem, which is unfolded as robust unbiased aggregation layers in GNNs with theoretical guarantees. Our comprehensive experiments confirm the strong robustness of our proposed model under various scenarios, and the ablation study provides a deep understanding of its advantages. Our code is available at https://github.com/chris-hzc/RUNG. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph neural networks (GNNs) have gained tremendous popularity in recent years due to their ability to capture topological relationships in graph-structured data [1]. However, most GNNs are vulnerable to adversarial attacks, which can lead to a substantial decline in predictive performance [2, 3, 4]. Despite the numerous defense strategies proposed to robustify GNNs, a recent study has revealed that most of these defenses are not as robust as initially claimed [5]. Specifically, under adaptive attacks, they easily underperform the multi-layer perceptrons (MLPs) which do not utilize the graph topology information at all [5]. Therefore, it is imperative to thoroughly investigate the limitations of existing defenses and develop innovative robust GNNs to securely harness the topology information in graphs. ", "page_idx": 0}, {"type": "text", "text": "Existing defenses attempt to bolster the resilience of GNNs using diverse approaches. For instance, Jaccard-GCN [6] and SVD-GCN [7] aim to denoise the graph by removing potential adversarial edges during the pre-processing procedure, while ProGNN [3] learns the clean graph structure during the training process. GRAND [8] and robust training [9, 10] also improve the training procedure through data augmentation. GNNGuard [2] and RGCN [11] reinforce their GNN architectures by heuristically reweighting edges in the graph. Additionally, there emerge some ODEs-inspired architectures including the GraphCON [12] and HANG [13] that demonstrate decent robustness. Although most of these defenses exhibit decent robustness against transfer attacks, i.e., the attack is generated through surrogate models, they encounter catastrophic performance drops when confronted with adaptive adversarial attacks that directly attack the victim model [5]. ", "page_idx": 0}, {"type": "text", "text": "Concerned by the false sense of security, we provide a comprehensive study on existing defenses under adaptive attacks. Our preliminary study in Section 2 indicates that SoftMedian [4], TWIRLS [14], and ElasticGNN [15] exhibit closely aligned performance and notably outperform other defenses despite their apparent architectural differences. However, as attack budgets increase, these defenses still experience a severe performance decrease and underperform the graph-agnostic MLPs. These observations are intriguing, but the underlying reasons are still unclear. ", "page_idx": 1}, {"type": "text", "text": "To unravel the aligned robustness and performance degradation of SoftMedian, TWIRLS, and ElasticGNN, we delve into their theoretical understanding and unveil their inherent connections and limitations in the underlying principles. Specifically, their improved robustness can be understood from a unified view of $\\ell_{1}$ -based robust graph smoothing. Moreover, we unearth the problematic estimation bias of $\\ell_{1}$ -based graph smoothing that allows the adversarial impact to accumulate as the attack budget escalates, which provides a plausible explanation of their declining robustness. Motivated by these understandings, we propose a robust and unbiased graph signal estimator to reduce the estimation bias in GNNs. We design an efficient Quasi-Newton IRLS algorithm that unrolls as robust unbiased aggregation layers to safeguard GNNs against adversarial attacks. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We provide a unified view of $\\ell_{1}$ -based robust graph signal smoothing to justify the improved and closely aligned robustness of representative robust GNNs. Moreover, we reveal their estimation bias, which explains their severe performance degradation as the attack budgets increase. \u2022 We propose a robust and unbiased graph signal estimator to mitigate the estimation bias in $\\ell_{1}$ - based graph signal smoothing and design an efficient Quasi-Newton IRLS algorithm to solve the non-smooth and non-convex estimation problem with theoretical guarantees. \u2022 The proposed algorithm can be readily unfolded as feature aggregation building blocks in GNNs, which not only provides clear interpretability but also covers many classic GNNs as special cases. \u2022 Comprehensive experiments demonstrate that our proposed GNN significantly improves the robustness while maintaining clean accuracy. We also provide comprehensive ablation studies to validate its working mechanism. ", "page_idx": 1}, {"type": "text", "text": "2 Estimation Bias Analysis of Robust GNNs ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In Section 2.1, we conduct a preliminary study to evaluate the robustness of several representative robust GNNs. In Section 2.2, we establish a unified view as $\\ell_{1}$ -based models to uncover the inherent connections of three well-performing GNNs, including SoftMedian, TWIRLS and ElasticGNN. In Section 2.3, we leverage the bias of $\\ell_{1}$ -based estimation to explain the catastrophic performance degradation in the preliminary experiments. ", "page_idx": 1}, {"type": "text", "text": "Notation. Let $\\mathcal{G}~=~\\{\\mathcal{V},\\mathcal{E}\\}$ be a graph with node set $\\gamma\\ =\\ \\{v_{1},\\ldots,v_{n}\\}$ and edge set $\\mathcal{E}\\mathbf{\\Sigma}=\\mathbf{\\Sigma}$ $\\{e_{1},\\ldots,e_{m}\\}$ . The adjacency matrix of $\\mathcal{G}$ is denoted as $A\\;\\in\\;\\{0,1\\}^{n\\times n}$ and the graph Laplacian matrix is $L=D-A$ . $D=\\mathrm{diag}(d_{1},\\ldots,d_{n})$ is the degree matrix where $d_{i}=|\\mathcal{N}(i)|$ and $\\mathcal{N}(i)$ is the neighborhood set of $v_{i}$ . The node feature matrix is denoted as $F=[\\pmb{f}_{1},\\dots,\\pmb{f}_{n}]^{\\top}\\in\\mathbb{R}^{n\\times d}$ , and $f^{(0)}\\left({\\pmb F}^{(0)}\\right)$ denotes the node feature vector (matrix) before graph smoothing in decoupled GNN models. Let $\\Delta\\,\\in\\,\\{-1,0,1\\}^{m\\times n}$ be the incidence matrix whose $l_{\\cdot}$ -th row denotes the $l$ -th edge $e_{l}\\,=\\,(i,j)$ such that $\\Delta_{l i}\\,=\\,-1,\\Delta_{l j}\\,=\\,1,\\Delta_{l k}\\,=\\,0\\;\\forall k\\;\\notin\\;\\{i,j\\}$ . $\\tilde{\\Delta}$ is its normalized version : $\\tilde{\\Delta}_{l j}=\\Delta_{l j}/\\sqrt{d_{j}}$ . For a vector $\\pmb{x}\\in\\mathbb{R}^{d}$ , we use $\\ell_{1}$ -based gragh smoothing penalty to denote either $\\begin{array}{r}{\\|\\pmb{x}\\|_{1}=\\sum_{i}|\\pmb{x}_{i}|}\\end{array}$ or $\\|\\pmb{x}\\|_{2}=\\sqrt{\\sum_{i}\\pmb{x}_{i}^{2}}$ . Note that we use $\\ell_{2}$ -based gragh smoothing penalty to denote $\\|\\pmb{x}\\|_{2}^{2}=\\sum_{i}\\pmb{x}_{i}^{2}$ . ", "page_idx": 1}, {"type": "text", "text": "2.1 Robustness Analysis ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To test the robustness of existing GNNs without the false sense of security, we perform a preliminary evaluation of existing robust GNNs against adaptive attacks. We choose various baselines including the undefended MLP, GCN [16], some of the most representative defenses in [5], and two additional robust models TWIRLS [14] and ElasticGNN [15]. We execute adaptive local evasion topological attacks and test the node classification accuracy on the Cora ML and Citeseer datasets. The detailed settings follow Section 4.1. From Figure 1, it can be observed that: ", "page_idx": 1}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/c804474d98b93d6e04af69d9ed589d88015f70f51ce4ef3810dcec2523ba4f8f.jpg", "img_caption": ["Figure 1: Robustness analysis under adaptive local attack. The perturbation budget ( $x$ -axis) is the number of edges allowed to be perturbed relative to the target node\u2019s degree. SoftMedian, TWIRLS, and ElasticGNN (blue curves) exhibit similarly aligned competitive robustness among all the selected robust GNNs, but all models experience catastrophic performance degradation as the attack budget increases. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "\u2022 Among all the selected robust GNNs, only SoftMedian, TWIRLS, and ElasticGNN exhibit notable and closely aligned improvements in robustness whereas other GNNs do not show obvious improvement over undefended GCN.   \n\u2022 SoftMedian, TWIRLS, and ElasticGNN encounter a similar catastrophic performance degradation as the attack budget scales up. Their accuracy easily drops below that of the graph-unware MLP, indicating their failure in safely exploiting the topology of the data. ", "page_idx": 2}, {"type": "text", "text": "2.2 A Unified View of Robust Estimation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our preliminary study provides intriguing observations in Section 2.1, but the underlying reasons behind these phenomena remain obscure. This motivates us to delve into their theoretical understanding and explanation. In this section, we will compare the architectures of three well-performing GNNs, aiming to reveal their intrinsic connections. ", "page_idx": 2}, {"type": "text", "text": "SoftMedian [4] substitutes the GCN aggregation for enhanced robustness with the dimension-wise median $\\overline{m_{i}}\\ \\stackrel{-}{\\in}\\ \\mathbb R^{d}$ for all neighbors of each node $\\textit{i}\\in\\textit{\\V}$ . However, computing the median involves operations like ranking and selection, which is not compatible with the back-propagation training of GNNs. Therefore, the median is approximated as a differentiable weighted sum j\u2208N(i) w(fj, mi)fj, \u2200i \u2208V, where mi is the exact non-differentiable dimension-wise median,Z $\\pmb{f}_{j}$ is the feature vector of the $j$ -th neighbor, $\\overbar{\\boldsymbol{w}}(\\mathbf{\\boldsymbol{x}},\\pmb{\\boldsymbol{y}})=e^{-\\beta\\|\\pmb{x}-\\pmb{y}\\|_{2}}$ , and $\\begin{array}{r}{Z=\\sum_{k}w(f_{k},\\pmb{m}_{k})}\\end{array}$ is a normalization factor. In this way, the aggregation assigns the largest weights t o the neighbors closest to the actual median. ", "page_idx": 2}, {"type": "text", "text": "TWIRLS [14] utilizes the iteratively reweighted least squares (IRLS) algorithm to optimize the objective with parameter $\\lambda$ , and $\\rho(y)=y$ is the default: ", "page_idx": 2}, {"type": "equation", "text": "$$\n2\\lambda\\sum_{(i,j)\\in\\mathcal{E}}\\rho(\\|\\tilde{\\boldsymbol{f}}_{i}-\\tilde{\\boldsymbol{f}}_{j}\\|_{2})+\\sum_{i\\in\\mathcal{V}}\\|\\tilde{\\boldsymbol{f}}_{i}-\\tilde{\\boldsymbol{f}}^{(0)}\\|_{2}^{2},\\tilde{\\boldsymbol{f}}_{i}=(1+\\lambda d_{i})^{-\\frac{1}{2}}\\boldsymbol{f}_{i}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "ElasticGNN [15] proposes the elastic message passing which unfolds the proximal alternating predictor-corrector (PAPC) algorithm to minimize the objective with parameter $\\lambda_{\\{1,2\\}}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{i\\in\\mathcal{V}}\\|f_{i}-f_{i}^{(0)}\\|_{2}^{2}+\\lambda_{1}\\sum_{(i,j)\\in\\mathcal{E}}\\left\\|\\frac{f_{i}}{\\sqrt{d_{i}}}-\\frac{f_{j}}{\\sqrt{d_{j}}}\\right\\|_{p}+\\lambda_{2}\\sum_{(i,j)\\in\\mathcal{E}}\\left\\|\\frac{f_{i}}{\\sqrt{d_{i}}}-\\frac{f_{j}}{\\sqrt{d_{j}}}\\right\\|_{2}^{2},\\mathrm{where~}p\\in\\{1,2\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "A Unified View of Robust Estimation. While these three approaches have seemingly different architectures, we provide a unified view of robust estimation to illuminate their inherent connections. First, the objective of TWIRLS in Eq. (1) can be considered as a particular case of ElasticGNN with $\\lambda_{2}=0$ and $p=2$ when neglecting the difference in the node degree normalization. However, TWIRLS and ElasticGNN leverage different optimization solvers, i.e., IRLS and PAPC, which leads to vastly different GNN layers. Second, SoftMedian approximates the computation of medians in a soft way of weighted sums, which can be regarded as approximately solving the dimension-wise median estimation problem [17]: $\\begin{array}{r}{\\arg\\operatorname*{min}_{\\pmb{f}_{i}}\\bar{\\sum}_{j\\in\\mathcal{N}(i)}\\|\\pmb{\\hat{f}_{i}}^{*}-\\pmb{f}_{j}\\|_{1}}\\end{array}$ . Therefore, SoftMedian can be regarded as the ElasticGNN with $\\lambda_{2}=0$ and $p=1$ . We also note that the SoftMedoid [18] approach also resembles ElasticGNN with $\\lambda_{2}=0$ and $p=2$ , and the Total Variation GNN [19] also utilizes an $\\ell_{1}$ estimator in spectral clustering. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The above analyses suggest that SoftMedian, TWIRLS, and ElasticGNN share the same underlying design principle of $\\ell_{1}$ -based robust graph signal estimation, i.e. a similar graph smoothing objective with edge difference penalties $\\|\\pmb{f}_{i}-\\pmb{f}_{j}\\|_{1}$ or $\\|\\pmb{f}_{i}-\\pmb{f}_{j}\\|_{2}$ . However, they adopt different approximation solutions that result in distinct architecture designs. This unified view of robust estimation clearly explains their closely aligned performance. Besides, the superiority $\\ell_{1}$ -based models over the\u221a $\\ell_{2}$ -based models such as GCN [16], whose graph smoothing objective is essentially $\\begin{array}{r}{\\sum_{(i,j)\\in\\mathcal{E}}\\|f_{i}/\\sqrt{d_{i}}-f_{j}/\\sqrt{d_{j}}\\|_{2}^{2}}\\end{array}$ [20], can be explained since $\\ell_{1}$ -based graph smoothing mitigates the impact of the outliers [15]. ", "page_idx": 3}, {"type": "text", "text": "2.3 Bias Analysis and Performance Degradation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The unified view of $\\ell_{1}$ -based graph smoothing we established in Section 2.2 not only explains their aligned robustness improvement but also provides a perspective to understand their failure as attack budgets scale up through an estimation bias analysis. ", "page_idx": 3}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/e5b458488eeddf16eb414322133e826ac5885a1dcecf07f39c5f06731db7479d.jpg", "img_caption": ["Figure 2: Different mean estimators in the presence of outliers. The clean samples are the majority of data points following the Gaussian distribution $\\mathcal{N}((0,0),1\\cdot I)$ , while the outliers are data points that deviate significantly from the main data pattern, following $\\tilde{\\mathcal{N}}((8,8),0.5\\cdot I)$ . $\\ell_{2}$ -estimator deviates far from the true mean, while the $\\ell_{1}$ -based estimator is more resistant to outliers. However, as the ratio of outliers escalates, the $\\ell_{1}$ -based estimator encounters a greater shift from the true mean, but our estimator still maintains a position close to the ground truth. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Bias of $\\ell_{1}$ -based Estimation. In the literature of high-dimensional statistics, it has been well understood that the $\\ell_{1}$ regularization will induce an estimation bias. In the context of denoising [21] or variable selection [22], small coefficients $\\beta$ are undesirable. To exclude small $\\beta$ in the estimation, a soft-thresholding operator can be derived as $\\mathbf{S}_{\\lambda}(\\beta)=\\mathrm{sign}(\\beta)\\operatorname*{max}(|\\beta|-\\lambda,0)$ . As a result, large $\\beta$ are also shrunk by a constant, so the $\\ell_{1}$ estimation is biased towards zero. ", "page_idx": 3}, {"type": "text", "text": "A similar bias effect also occurs in graph signal estimation in the presence of adversarial attacks. For example, in TWIRLS (Eq. (1)), after the graph aggregation $\\begin{array}{r}{\\tilde{f}_{i}^{(\\bar{k+1})}=\\sum_{j\\in\\mathcal{N}(i)}w_{i j}\\tilde{f}_{j}^{(k)}}\\end{array}$ where $w_{i j}\\,=\\,\\|\\tilde{\\pmb{f}}_{i}-\\tilde{\\pmb{f}}_{j}\\|_{2}^{-1}$ , the edge difference ${\\tilde{f}}_{i}-{\\tilde{f}}_{j}$ will shrink towards zero. Consequently, every adversarial edge the attacker adds will induce a bias that can be accumulated and amplified when the attack budget scales up. ", "page_idx": 3}, {"type": "text", "text": "Numerical Simulation. To provide a more intuitive illustration of the estimation bias of $\\ell_{1}$ -based models, we simulate a mean estimation problem on synthetic data since most message passing schemes in GNNs essentially estimate the mean of neighboring node features. The results in Figure 2 shows that $\\ell_{1}$ -based estimator is more resistant than $\\ell_{2}$ -based estimator. However, as the ratio of outliers escalates, the $\\ell_{1}$ -based estimator encounters a greater shift from the true mean due to the accumulated bias caused by outliers. This observation explains why $\\ell_{1}$ -based graph smoothing models suffer from catastrophic degradation under large attack budgets. The detailed simulation settings and results are available in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3 Robust GNNs with Unbiased Aggregation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we first design a robust unbiased estimator to reduce the bias in graph signal estimation in Section 3.1 and propose an efficient second-order IRLS algorithm to compute the robust estimator with theoretical convergence guarantees in Section 3.2. Finally, we unroll the proposed algorithm as the robust unbiased feature aggregation layers in GNNs in Section 3.3. ", "page_idx": 4}, {"type": "text", "text": "3.1 Robust and Unbiased Graph Signal Estimator ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our study and analysis in Section 2 have shown that while $\\ell_{1}$ -based methods outperform $\\ell_{2}$ -based methods in robustness, they still suffer from the accumulated estimation bias, leading to severe performance degradation under large perturbation budgets. This motivates us to design a robust and unbiased graph signal estimator that derives unbiased robust aggregation for GNNs with stronger resilience to attacks. ", "page_idx": 4}, {"type": "text", "text": "Theoretically, the estimation bias in Lasso regression has been discovered and analyzed in highdimensional statistics [23]. Statisticians have proposed adaptive Lasso [23] and many non-convex penalties such as Smoothly Clipped Absolute Deviation (SCAD) [24] and Minimax Concave Penalty (MCP) [25] to alleviate this bias. Motivated by these advancements, we propose a Robust and Unbiased Graph signal Estimator (RUGE) as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{\\pmb{F}}\\mathcal{H}(\\pmb{F})=\\sum_{(i,j)\\in\\mathcal{E}}\\rho_{\\gamma}(\\left\\|\\frac{\\pmb{f}_{i}}{\\sqrt{d_{i}}}-\\frac{\\pmb{f}_{j}}{\\sqrt{d_{j}}}\\right\\|_{2})+\\lambda\\sum_{i\\in\\mathcal{V}}\\|\\pmb{f}_{i}-\\pmb{f}_{i}^{(0)}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\rho_{\\gamma}(y)$ denotes the function that penalizes the feature differences on edges by MCP: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{\\gamma}(y)={\\left\\{\\!\\!\\begin{array}{l l}{y-{\\frac{y^{2}}{2\\gamma}}}&{{\\mathrm{if~}}y<\\gamma}\\\\ {{\\frac{\\gamma}{2}}}&{{\\mathrm{if~}}y\\geq\\gamma}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As shown in Figure 3, MCP closely approximates the $\\ell_{1}$ norm when $y$ is small since the quadratic term $\\frac{y^{\\bar{2}}}{2\\gamma}$ is negligible, and it becomes a constant value when $y$ is large. This transition can be adjusted by the thresholding parameter $\\gamma$ . When $\\gamma$ approaches infinity, the penalty $\\rho_{\\gamma}(y)$ reduces to the $\\ell_{1}$ norm. Conversely, when $\\gamma$ is very small, the \u201cvalley\u201d of $\\rho_{\\gamma}$ near zero is exceptionally sharp, so $\\rho_{\\gamma}(y)$ approaches the $\\ell_{0}$ norm and becomes a constant for a slightly larger $y$ . This enables RUGE to suppress smoothing on edges whose node differences exceeding the threshold $\\gamma$ . This not only mitigates the estimation bias against outliers but also maintains the estimation accuracy in the absence of outliers. The simulation in Figure 2 verifies that our proposed estimator $(\\eta(\\pmb{x}):=\\rho_{\\gamma}(\\|\\pmb{x}\\|_{2}))$ can recover the true mean despite the increasing outlier ratio when the outlier ratio is below the theoretical optimal breakdown point. ", "page_idx": 4}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/196fff6b16aa0c055b019e4358c30f929ca24761f37135ed13b791607517fb5e.jpg", "img_caption": ["Figure 3: Penalties. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 Quasi-Newton IRLS ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Despite the advantages discussed above, the proposed RUGE in Eq. (3) is non-smooth and non-convex, which results in challenges for deriving efficient numerical solutions that can be readily unfolded as neural network layers. In the literature, researchers have developed optimization algorithms for MCP-related problems, such as the Alternating Direction Multiplier Method (ADMM) and Newtontype algorithms [24, 25, 26]. However, due to their excessive computation and memory requirements as well as the incompatibility with back-propagation training, these algorithms are not well-suited for the construction of feature aggregation layers in GNNs. To solve these challenges, we propose an efficient Quasi-Newton Iteratively Reweighted Least Squares algorithm (QN-IRLS) to solve the estimation problem in Eq. (3). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "IRLS. Before stepping into our QN-IRLS, we first introduce the main idea of iteratively reweighted least squares (IRLS) [27] and analyze its weakness in convergence. IRLS aims to circumvent the non-smooth $\\mathcal{H}(F)$ in Eq. (3) by computing its quadratic upper bound $\\hat{\\mathcal{H}}$ based on $\\pmb{F}^{(k)}$ in each iteration $k$ and optimizing $\\hat{\\mathcal{H}}(\\mathcal{F})$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{H}}(\\pmb{F})=\\sum_{(i,j)\\in\\mathcal{E}}W_{i j}^{(k)}\\left\\|\\frac{f_{i}}{\\sqrt{d_{i}}}-\\frac{{\\pmb{f}}_{j}}{\\sqrt{d_{j}}}\\right\\|_{2}^{2}+\\lambda\\sum_{i\\in\\mathcal{V}}\\|{\\pmb{f}}_{i}-{\\pmb{f}}_{i}^{(0)}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where W i(jk) $\\begin{array}{r}{W_{i j}^{(k)}=1_{i\\neq j}\\frac{d\\rho_{\\gamma}(y_{i j})}{d y_{i j}^{2}}\\big|_{y_{i j}=y_{i j}^{(k)}}^{}3}\\end{array}$ , where $y_{i j}^{(k)}={\\left|{\\left|{{\\mathbf{f}_{i}^{(k)}}}\\right/}{\\sqrt{d_{i}}}-{\\mathbf{f}_{j}^{(k)}}/{\\sqrt{d_{j}}}\\right|}\\right|_{2}$ and $\\rho_{\\gamma}(\\cdot)$ is the MCP function. For the detailed proof of the upper bound, please refer to Lemma 1 in Appendix $\\mathbf{B}$ . Then, each iterative step of IRLS can be formulated as the first-order gradient descent for ${\\hat{\\mathcal{H}}}(F)$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\pmb F}^{(k+1)}={\\pmb F}^{(k)}-\\eta\\nabla\\hat{\\mathcal{H}}({\\pmb F}^{(k)})={\\pmb F}^{(k)}-\\eta\\left((\\hat{\\pmb Q}^{(k)}-2{\\pmb W}^{(k)}\\odot\\tilde{\\pmb A}){\\pmb F}^{(k)}-2\\lambda{\\pmb F}^{(0)}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta$ is the step size, $\\hat{Q}^{(k)}=2(\\mathrm{diag}(q^{(k)})\\!+\\!\\lambda I)$ , and $\\begin{array}{r}{\\pmb q_{m}^{(k)}=\\sum_{j}{W_{m j}^{(k)}\\pmb A_{m j}/d_{m}}}\\end{array}$ . Its convergence condition is given in Theorem 1, with a proof in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "nTohne-odreecrme a1s.i nIfg $\\pmb{F}^{(k)}$ ,  tthhee nu pa dsautfef icriuelnet  icno Endqi.t i(o6)n  fwohr $\\rho$ $W$ ieiss  tthhaatt $\\frac{d\\rho(y)}{d y^{2}}$ eips $\\forall y\\in(0,\\infty)$ $\\mathcal{H}(\\dot{F}^{(k+1)})\\leq\\mathcal{H}(F^{(k)})$ size \u03b7 satisfies $0<\\eta\\leq\\|d i a g(\\pmb{q}^{(k)})-\\pmb{W}^{(k)}\\odot\\tilde{\\pmb{A}}+\\lambda\\pmb{I}\\|_{2}^{-1}$ . ", "page_idx": 5}, {"type": "text", "text": "Quasi-Newton IRLS. Theorem 1 suggests the difficulty in the proper selection of stepsize for (first-order) IRLS due to its non-trivial dependency on the graph $(\\tilde{A})$ and the dynamic terms $(\\pmb q^{(k)}$ and $W^{(k)})$ 4. The dilemma is that a small stepsize will lead to slow convergence but a large step easily causes divergence and instability as verified by our experiments in Section 4.3 (Figrue 5), which reveals its critical shortcoming for the construction of GNN layers. ", "page_idx": 5}, {"type": "text", "text": "To overcome this limitation, we aim to propose a second-order Newton method, ${\\pmb F}^{(k+1)}~=$ $\\mathbf{F}^{(k)}-(\\nabla^{2}\\hat{\\mathcal{H}}(\\mathbf{F}^{(k)}))^{-1}\\nabla\\hat{\\mathcal{H}}(\\mathbf{F}^{(k)})$ , to achieve faster convergence and stepsize-free hyperparameter tuning by better capturing the geometry of the optimization landscape. However, obtaining the analytic expression for the inverse Hessian matrix $(\\nabla^{2}\\hat{\\mathcal{H}}(\\pmb{F}^{(k)}))^{-1}\\ \\in\\ \\mathbb{R}^{n\\times n}$ is intractable, and the numerical solution requires expensive computation for large graphs. Therefore, we propose a novel Quasi-Newton IRLS algorithm (QN-IRLS) that approximates the Hessian matrix $\\nabla^{2}\\hat{\\mathcal{H}}(\\pmb{F}^{(k)})=2(\\mathrm{diag}(\\pmb{q}^{(k)})-\\pmb{W}^{(k)}\\odot\\tilde{\\pmb{A}}+\\lambda\\pmb{I})$ by the diagonal matrix $\\hat{\\pmb Q}^{(k)}=2(\\mathrm{diag}(\\pmb q^{(k)})+\\lambda\\pmb I)$ such that the inverse is trivial. The proposed QN-IRLS works as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{F}^{(k+1)}=\\boldsymbol{F}^{(k)}-\\bigl(\\hat{\\boldsymbol{Q}}^{(k)}\\bigr)^{-1}\\nabla\\hat{\\mathcal{H}}(\\boldsymbol{F}^{(k)})=(\\mathrm{diag}(\\boldsymbol{q}^{(k)})+\\lambda\\boldsymbol{I})^{-1}\\left((\\boldsymbol{W}^{(k)}\\odot\\tilde{\\boldsymbol{A}})\\boldsymbol{F}^{(k)}+\\lambda\\boldsymbol{F}^{(0)}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $(\\hat{Q}^{(k)})^{-1}$ automatically adjusts the per-coordinate stepsize according to the local geometry of the optimization landscape, $\\pmb q^{(k)}$ and $W^{(k)}$ are defined as in Eq. (5) and (6). In this way, QN-IRLS provides faster convergence without needing to select a stepsize. The convergence is guaranteed by Theorem 2 with the proof in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. If ${\\pmb F}^{(k+1)}$ follows update rule in Eq. (7), where $\\rho$ satisfies that $\\frac{d\\rho(y)}{d y^{2}}$ is non-decreasing $\\forall y\\in(0,\\infty)$ , it is guaranteed that $\\mathcal{H}(F^{(k+1)})\\leq\\mathcal{H}(F^{(k)})$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 GNN with Robust Unbiased Aggregation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The proposed QN-IRLS provides an efficient algorithm to optimize the RUGE in Eq. (3) with a theoretical convergence guarantee. Instantiated with $\\rho=\\rho_{\\gamma}$ , each iteration in QN-IRLS in Eq. (7) can be used as one layer in GNNs, which yields the Robust Unbiased Aggregation (RUNG): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{F}^{(k+1)}=(\\mathrm{diag}(\\pmb{q}^{(k)})+\\lambda\\pmb{I})^{-1}\\left((\\pmb{W}^{(k)}\\odot\\tilde{\\pmb{A}})\\pmb{F}^{(k)}+\\lambda\\pmb{F}^{(0)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/a08b2c582c0ae5dd79625467478cb50c57e4eb411141707236179a1b00a55b9e.jpg", "img_caption": ["Figure 4: d\u03c1(2y) . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Interpretability. The proposed RUNG can be interpreted intuitively with edge reweighting. In Eq. (8), the normalized adjacency matrix $\\tilde{A}$ is reweighted by $W^{(k)}$ , where $\\begin{array}{r}{W_{i j}^{(k)}=\\frac{d\\rho(y)\\overline{{}}}{d y^{2}}\\big|_{y=y_{i j}^{(k)}}.}\\end{array}$ dy2 |y=yi(jk ). It is shown in Figure 4 that $W_{i j}$ becomes zero for any edge $e_{k}=(i,j)$ with a node difference $y_{i j}^{(k)}\\geq\\gamma$ thus pruning suspicious edges. This implies RUNG\u2019s strong robustness under large-budget adversarial attacks. With the inclusion of the skip connection ${\\pmb F}^{(0)}$ , $\\mathrm{diag}(q^{(k)})+\\lambda I$ can be seen as a normalizer of the layer output. ", "page_idx": 6}, {"type": "text", "text": "Relations with Existing GNNs. RUNG can adopt different $\\rho$ that Theorem 2 allows, thus covering many classic GNNs as special cases. When $\\dot{\\rho}(y)\\;=\\;y^{2}$ , RUNG in Eq. (8) exactly reduces to APPNP [28] $\\begin{array}{r}{(F^{(k+1)}=\\frac{\\bar{\\mathbf{\\alpha}}_{1}}{1+\\lambda}\\tilde{\\pmb{A}}F^{(k)}+\\frac{\\lambda}{1+\\lambda}\\pmb{F}^{(\\dot{0})})}\\end{array}$ and GCN $(\\pmb{F}^{(k+1)}=\\bar{\\tilde{A}}\\pmb{F}^{(k)})$ if chosing $\\lambda=0$ When $\\rho(y)=y$ , the objective of RUGE is equivalent to ElasticGNN with $p=2$ , which is analogous to SoftMedian and TWIRLS due to their inherent connections as $\\ell_{1}$ -based graph smoothing. ", "page_idx": 6}, {"type": "text", "text": "Complexity analysis. RUNG is scalable with time complexity of $O(k(m{+}n)d)$ and space complexity $O(m+n d)$ , where $m$ is the number of edges, $d$ is the number of features, $n$ is the number of nodes, and $k$ is the number of GNN layers. Therefore, the complexity of our RUNG is comparable to normal GCN (with a constant difference) and it is feasible to implement. The detailed discussions about computation efficiency can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we perform comprehensive experiments to validate the robustness of the proposed RUNG. Besides, ablation studies show its convergence and defense mechanism. ", "page_idx": 6}, {"type": "text", "text": "4.1 Experiment Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We test our RUNG with the node classification task on two widely used real-world citation networks, Cora ML and Citeseer [29], as well as a large-scale networks Ogbn-Arxiv [30]. We adopt the data split of $10\\%$ training, $10\\%$ validation, and $80\\bar{\\%}$ testing, and report the classification accuracy of the attacked nodes following [5]. Each experiment is averaged over 5 different random splits. ", "page_idx": 6}, {"type": "text", "text": "Baselines. To evaluate the performance of RUNG, we compare it to $\\ell_{2}$ other representative baselines. Among them, MLP, GCN [16], APPNP [28], and GAT [31] are undefended vanilla models. GNNGuard [2], RGCN [11], GRAND [8], ProGNN [3], Jaccard-GCN [6], SVD-GCN [7], EvenNet [32], HANG [13], NoisyGNN [33], and GARNET [34] are representative robust GNNs. Besides, SoftMedian and TWIRLS are representative methods with $\\ell_{1}$ -based graph smoothing 5. We also evaluate a variant of TWIRLS with thresholding attention (TWIRLS-T). For RUNG, we test two variants: default RUNG (Eq. (8)) and RUNG- $\\ell_{1}$ with $\\ell_{1}$ penalty $(\\rho(y)=y)$ ). ", "page_idx": 6}, {"type": "text", "text": "Hyperparameters. The model hyperparameters including learning rate, weight decay, and dropout rate are tuned as in [5]. Other hyperparameters follow the settings in the original papers. RUNG uses an MLP connected to 10 graph aggregation layers following the decoupled GNN architecture of APPNP. $\\begin{array}{r}{\\hat{\\lambda}=\\frac{1}{1+\\lambda}}\\end{array}$ is tuned in {0.7, 0.8, 0.9}, and $\\gamma$ tuned in $\\{0.5,1,2,3,5\\}$ . We chose the hyperparameter setting that yields the best robustness without a notable impact (smaller than $1\\%$ ) on the clean accuracy following [35]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Attack setting. We use the PGD attack [36] to execute the adaptive evasion and poisoning topology attack since it delivers the strongest attack in most settings [5]. The adaptive attack setting is provided in Appendix F. The adversarial attacks aim to misclassify specific target nodes (local attack) or the entire set of test nodes (global attack). To avoid a false sense of robustness, our adaptive attacks directly target the victim model instead of the surrogate model. Additionally, we include the transfer attacks with a 2-layer GCN as the surrogate model. We also include graph injection attack following the setting in TDGIA [37]. ", "page_idx": 7}, {"type": "text", "text": "4.2 Adversarial Robustness ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/e1dd67e2de5d85161d7cb9dbfec7ce62f913b2e6b4056c4e5b9037b7d4f2fec2.jpg", "table_caption": ["Table 1: Adaptive local attack on Cora ML. The best and second are marked. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/049494b1a7d01cd5cf0774577e757cadf6d7df228160b6341c66773f80668a71.jpg", "table_caption": ["Table 2: Adaptive global attack on Cora ML. The best and second are marked. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/742c97fb806d0b1f2e8593ac998a5db92f56b5f411bf6dd282ec38ceed55d54b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Here we evaluate the the performance of RUNG against the baselines under different settings. The results of local and global adaptive attacks on Cora ML are presented in Table 1 and Table 2, while those on Citeseer are presented in Table 3 and Table 4 in Appendix E due to space limits. We summarize the following analysis from Cora ML, noting that the same observations apply to Citeseer. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Under adaptive attacks, many existing defenses are not significantly more robust than undefended models. The $\\ell_{1}$ -based models such as TWIRLS, SoftMedian, and RUNG- $\\ell_{1}$ demonstrate considerable and closely aligned robustness under both local and global attacks, which supports our unified $\\ell_{1}$ -based robust view analysis in Section 2.2.   \n\u2022 RUNG exhibits significant improvements over all baselines across various budgets under both global and local attacks. Local attacks are stronger than global attacks since local attacks concentrate on targeted nodes. The robustness improvement of RUNG appears to be more remarkable in local attacks.   \n\u2022 When there is no attack, RUNG largely preserves an excellent clean performance. RUNG also achieves state-of-the-art performance under small attack budgets. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Convergence. To verify the advantage of our QN-IRLS method in Eq (7) over the first-order IRLS in Eq (6), we show the objective $\\mathcal{H}$ on each layer in Figure 5. It can be observed that our stepsize-free QN-IRLS demonstrates the best convergence as discussed in Section 3. ", "page_idx": 8}, {"type": "text", "text": "Estimation bias. The bias effect in $\\ell_{1}$ -based GNNs and the unbiasedness of our RUNG can be empirically verified. We quantify the bias with $\\textstyle\\sum_{i\\in\\mathcal{V}}\\|f_{i}-f_{i}^{\\star}\\|_{2}^{2}$ , where $f_{i}^{\\star}$ and $\\pmb{f}_{i}$ denote the aggregated feature on the clean graph and attacked graph, respectively. As shown in Figure 6, when the budget scales up, $\\ell_{1}$ GNNs exhibit a notable bias, whereas RUNG has nearly zero bias. We provide comprehensive discussion of unbiasedness of RUNG in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Defense Mechanism To further investigate how our defense takes effect, we a\u221analyze the edges added under adaptive attacks. The distribution of the node feature differences $\\left|\\left|f_{i}/\\sqrt{d_{i}}-f_{j}/\\sqrt{d_{j}}\\right|\\right|_{2}$ of attacked edges is shown in Figure 7 for different graph signal estimators. It can be observed that our RUNG forces the attacker to focus on the edges with a small feature difference, indicating that our RUNG can improve robustness by down-weighting or pruning some malicious edges that connect distinct nodes. Therefore, the attacks become less influential, which explains why RUNG demonstrates outstanding robustness. ", "page_idx": 8}, {"type": "text", "text": "Transfer Attacks. In addition to the adaptive attack, we also conduct a set of transfer attacks that take every baseline GNN as the surrogate model to comprehensively test the robustness of RUNG, following the unit test attack protocol proposed in [5]. We summarize the results on Cora ML and Citeseer in Figure 9 and Figure 10 in Appendix E due to space limits. All transfer attacks are weaker than the adaptive attack in Section 4.2, indicating the necessity to evaluate the strongest adaptive attack to avoid the false sense of security emphasized in this paper. Note that the attack transferred from RUNG model is slightly weaker than the adaptive attack since the surrogate and victim RUNG models have different model parameters in the transfer attack setting. ", "page_idx": 8}, {"type": "text", "text": "Hyperparameters. Due to the space limit, we provide the additional ablation studies on the hyperparameters (including $\\gamma$ and $\\lambda$ in MCP as well as the number of layers) of RUNG in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "The results offer an overview strategy for the choice of optimal hyperparameters. We can observe that $\\gamma$ in MCP has a significant impact on the performance of RUNG. Specifically, a larger $\\gamma$ makes RUNG closer to an $\\ell_{1}$ -based model, while a smaller $\\gamma$ encourages more edges to be pruned. This pruning helps RUNG to remove more malicious edges and improve robustness, although a small $\\gamma$ may slightly reduce clean performance. ", "page_idx": 9}, {"type": "text", "text": "Robustness under various scenarios. Besides the evaluation under strong adaptive attacks, we also validate the consistent effectiveness of our proposed RUNG under various scenarios, including Transfer attacks (Appendix E.2), Poisoning attacks (Appendix E.3), Large scale datasets (Appendix E.4), Adversarial training (Appendix E.5), Graph injection attacks (Appendix E.6). ", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To the best of our knowledge, although there are works unifying existing GNNs from a graph signal denoising perspective [20], no work has been dedicated to uniformly understand the robustness and limitations of robust GNNs such as SoftMedian [4], SoftMedoid [18], TWIRLS [14], ElasticGNN [15], and TVGNN [19] from the $\\ell_{1}$ robust statistics and bias analysis perspectives. To mitigate the estimation bias, MCP penalty is promising since it is well known for its near unbiasedness property [25] and has been applied to the graph trend flitering problem [26] to promote piecewise signal modeling, but their robustness is unexplored. Nevertheless, other robust GNNs have utilized alternative penalties that might alleviate the bias effect. For example, GNNGuard [2] prunes the edges whose cosine similarity is too small. Another example is that TWIRLS [14] with a thresholding penalty can also exclude edges using graph attention. However, the design of their edge weighting or graph attention is heuristic-based and exhibits suboptimal performance compared to the RUNG proposed in this work. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion & Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a unified view of $\\ell_{1}$ robust graph smoothing to uniformly understand the robustness and limitations of representative robust GNNs. The established view not only justifies their improved and closely aligned robustness but also explains their severe performance degradation under large attack budgets by a novel estimation bias analysis. To mitigate the estimation bias, we propose a robust and unbiased graph signal estimator. To solve this non-trivial estimation problem, we design a novel and efficient Quasi-Newton IRLS algorithm that can better capture the landscape of the optimization problem and converge stably with a theoretical guarantee. This algorithm can be unfolded and used as a building block for constructing robust GNNs with Robust Unbiased Aggregation (RUNG). As verified by our experiments, RUNG provides the best performance under strong adaptive attacks among all the baselines. Furthermore, RUNG also covers many classic GNNs as special cases. Most importantly, this work provides a deeper understanding of existing approaches and reveals a principled direction for designing robust GNNs. ", "page_idx": 9}, {"type": "text", "text": "Regarding the limitations, first, the improvement of RUNG is more significant under large budgets compared to the robust baselines. Second, we primarily include experiments on homophilic graphs in the main paper, but we can generalize the proposed robust aggregation to heterophilic graphs in future work. Third, although our Quasi-Newton IRLS algorithm has exhibited excellent convergence compared to the vanilla IRLS, the efficiency of RUNG could be further improved. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zhichao Hou and Dr. Xiaorui Liu are supported by the National Science Foundation (NSF) National AI Research Resource Pilot Award, Amazon Research Award, NCSU Data Science Academy Seed Grant Award, and NCSU Faculty Research and Professional Development Award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yao Ma and Jiliang Tang. Deep learning on graphs. Cambridge University Press, 2021. ", "page_idx": 10}, {"type": "text", "text": "[2] Xiang Zhang and Marinka Zitnik. Gnnguard: Defending graph neural networks against adversarial attacks. Advances in neural information processing systems, 33:9263\u20139275, 2020.   \n[3] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 66\u201374, 2020.   \n[4] Simon Geisler, Tobias Schmidt, Hakan \u00b8Sirin, Daniel Z\u00fcgner, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Robustness of graph neural networks at scale. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 7637\u20137649. Curran Associates, Inc., 2021.   \n[5] Felix Mujkanovic, Simon Geisler, Stephan G\u00fcnnemann, and Aleksandar Bojchevski. Are defenses for graph neural networks robust? In Neural Information Processing Systems, NeurIPS, 2022.   \n[6] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu. Adversarial examples on graph data: Deep insights into attack and defense. arXiv preprint arXiv:1903.01610, 2019.   \n[7] Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. All you need is low (rank) defending against adversarial attacks on graphs. In Proceedings of the 13th International Conference on Web Search and Data Mining, pages 169\u2013177, 2020.   \n[8] Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov, and Jie Tang. Graph random neural networks for semi-supervised learning on graphs. Advances in neural information processing systems, 33:22092\u201322103, 2020.   \n[9] Zhijie Deng, Yinpeng Dong, and Jun Zhu. Batch virtual adversarial training for graph convolutional networks. AI Open, 2023.   \n[10] Jinyin Chen, Xiang Lin, Hui Xiong, Yangyang Wu, Haibin Zheng, and Qi Xuan. Smoothing adversarial training for gnn. IEEE Transactions on Computational Social Systems, 8(3):618\u2013629, 2020.   \n[11] Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks against adversarial attacks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1399\u20131407, 2019.   \n[12] T Konstantin Rusch, Ben Chamberlain, James Rowbottom, Siddhartha Mishra, and Michael Bronstein. Graph-coupled oscillator networks. In International Conference on Machine Learning, pages 18888\u201318909. PMLR, 2022.   \n[13] Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, and Wee Peng Tay. Adversarial robustness in graph neural networks: A hamiltonian approach. Advances in Neural Information Processing Systems, 36, 2024.   \n[14] Yongyi Yang, Tang Liu, Yangkun Wang, Jinjing Zhou, Quan Gan, Zhewei Wei, Zheng Zhang, Zengfeng Huang, and David Wipf. Graph neural networks inspired by classical iterative algorithms. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11773\u201311783. PMLR, 18\u201324 Jul 2021.   \n[15] Xiaorui Liu, Wei Jin, Yao Ma, Yaxin Li, Hua Liu, Yiqi Wang, Ming Yan, and Jiliang Tang. Elastic graph neural networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 6837\u20136849. PMLR, 18\u201324 Jul 2021.   \n[16] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. ", "page_idx": 10}, {"type": "text", "text": "[17] Peter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004. ", "page_idx": 11}, {"type": "text", "text": "[18] Simon Geisler, Daniel Z\u00fcgner, and Stephan G\u00fcnnemann. Reliable graph neural networks via robust aggregation. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA, 2020. Curran Associates Inc.   \n[19] Jonas Berg Hansen and Filippo Maria Bianchi. Total variation graph neural networks. In Proceedings of the 40th international conference on Machine learning. ACM, 2023.   \n[20] Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on graph neural networks as graph signal denoising. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, CIKM \u201921, page 1202\u20131211, New York, NY, USA, 2021. Association for Computing Machinery.   \n[21] David L Donoho. De-noising by soft-thresholding. IEEE transactions on information theory, 41(3):613\u2013627, 1995.   \n[22] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267\u2013288, 1996.   \n[23] Hui Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association, 101(476):1418\u20131429, 2006.   \n[24] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348\u20131360, 2001.   \n[25] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894 \u2013 942, 2010.   \n[26] Rohan Varma, Harlin Lee, Jelena Kovac\u02c7evic\u00b4, and Yuejie Chi. Vector-valued graph trend flitering with non-convex penalties. IEEE transactions on signal and information processing over networks, 6:48\u201362, 2019.   \n[27] Paul W Holland and Roy E Welsch. Robust regression using iteratively reweighted least-squares. Communications in Statistics-theory and Methods, 6(9):813\u2013827, 1977.   \n[28] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.   \n[29] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[30] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.   \n[31] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al. Graph attention networks. stat, 1050(20):10\u201348550, 2017.   \n[32] Runlin Lei, Zhen Wang, Yaliang Li, Bolin Ding, and Zhewei Wei. Evennet: Ignoring odd-hop neighbors improves robustness of graph neural networks. Advances in Neural Information Processing Systems, 35:4694\u20134706, 2022.   \n[33] Sofiane Ennadir, Yassine Abbahaddou, Johannes F Lutzeyer, Michalis Vazirgiannis, and Henrik Bostr\u00f6m. A simple and yet fairly effective defense for graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 21063\u201321071, 2024.   \n[34] Chenhui Deng, Xiuyu Li, Zhuo Feng, and Zhiru Zhang. Garnet: Reduced-rank topology learning for robust and scalable graph neural networks. In Learning on Graphs Conference, pages 3\u20131. PMLR, 2022.   \n[35] Aleksandar Bojchevski and Stephan G\u00fcnnemann. Certifiable robustness to graph perturbations. In Neural Information Processing Systems, 2019.   \n[36] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin. Topology attack and defense for graph neural networks: an optimization perspective. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 3961\u20133967, 2019.   \n[37] Xu Zou, Qinkai Zheng, Yuxiao Dong, Xinyu Guan, Evgeny Kharlamov, Jialiang Lu, and Jie Tang. Tdgia: Effective injection attacks on graph neural networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 2461\u20132471, 2021.   \n[38] Emmanuel J. Cand\u00e8s, Michael B. Wakin, and Stephen P. Boyd. Enhancing sparsity by reweighted $l_{1}$ minimization. Journal of Fourier Analysis and Applications, 14(5-6):877\u2013905, Dec 2008. Funding by NSF.   \n[39] Amir Beck and Shoham Sabach. Weiszfeld\u2019s method: Old and new results. Journal of Optimization Theory and Applications, 164(1):1\u201340, 2015.   \n[40] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267\u2013288, 1996.   \n[41] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. 2010.   \n[42] Simon Geisler, Tobias Schmidt, Hakan \u00b8Sirin, Daniel Z\u00fcgner, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Robustness of graph neural networks at scale. Advances in Neural Information Processing Systems, 34:7637\u20137649, 2021. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Bias Accumulation of $\\ell_{1}$ Models ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Details of the Numerical Simulation Settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To provide a more intuitive illustration of the estimation biases of different models, we simulate a mean estimation problem on synthetic data since most message passing schemes in GNNs essentially estimate the mean of neighboring node features. In the context of mean estimation, the bias is measured as the distances between different mean estimators and the true mean. We firstly generated clean samples $\\{{\\pmb x}_{i}\\}_{i=1}^{n}$ (blue dots) and the outlier samples $\\{x_{i}\\}_{i=n+1}^{n+m}$ (red dots) from 2-dimensional Gaussian distributions, $\\mathcal{N}((0,0),1)$ and $\\mathcal{N}((8,8),0.5)$ , respectively. We calculate the mean of clean samples $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{x}_{i}$ as the ground truth of the mean estimator. Then we estimate the mean of all the samples by solving $\\begin{array}{r}{\\arg\\operatorname*{min}_{z}\\sum_{i=1}^{n+m}\\eta(z-x_{i})}\\end{array}$ using the Weiszfeld method [38, 39], where $\\eta(\\cdot)$ can take different norms such as $\\ell_{2}\\ \\mathrm{norm}\\parallel\\cdot\\parallel_{2}^{2}$ and $\\ell_{1}$ norm $\\|\\cdot\\|_{2}$ . ", "page_idx": 13}, {"type": "text", "text": "The mean estimators are formulated as minimization operators ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bar{z}=\\underset{z}{\\arg\\operatorname*{min}}\\sum_{i}^{n+m}\\eta(z-\\pmb{x}_{i}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $n$ is the number of clean samples and $m$ is the number of adversarial samples. ", "page_idx": 13}, {"type": "text", "text": "$\\ell_{1}$ estimator. The $\\ell_{1}$ estimator $(\\eta({\\boldsymbol{y}}):=\\|{\\boldsymbol{y}}\\|_{2})$ , essentially is the geometric median. We adopted the Weiszfeld method to iteratively reweight $_{\\textit{z}}$ to minimize the objective, following ", "page_idx": 13}, {"type": "equation", "text": "$$\nz^{(k+1)}=\\frac{\\sum_{i}w_{i}^{(k)}x_{i}}{\\sum_{i}w_{i}^{(k)}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{w_{i}^{(k)}~=~\\frac{1}{\\|\\pmb{z}^{(k)}-\\pmb{x}_{i}\\|_{2}}}\\end{array}$ . This can be seen as a gradient descent step of $z^{(k+1)}~=~z^{(k)}~-$ \u03b1\u2207z i \u2225z \u2212xi\u22252 = z(k+1) \u2212\u03b1  i\u2225zz((kk))\u2212\u2212xxii\u22252 . Taking \u03b1 = $\\begin{array}{r}{\\alpha=\\frac{1}{\\sum_{i}w_{i}^{(k)}}}\\end{array}$ instantly yields Eq. (10). ", "page_idx": 13}, {"type": "text", "text": "MCP-based estimator. We therefore adopt a similar approach for the MCP-based estimator (\u201cOurs\u201d in Fig. Figure 2), where $\\eta(\\pmb{y}):=\\rho_{\\gamma}(\\pmb{y})$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z}^{(k+1)}={z}^{(k)}-\\alpha\\nabla_{z}\\displaystyle\\sum_{i}\\rho_{\\gamma}(\\|z-\\pmb{x}_{i}\\|_{2})}\\\\ {~~~~~~={z}^{(k)}-\\alpha\\displaystyle\\sum_{i}\\operatorname*{max}(0,\\frac{1}{\\|z^{(k)}-\\pmb{x}_{i}\\|_{2}}-\\frac{1}{\\gamma})({z}^{(k)}-\\pmb{x}_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Denoting $\\begin{array}{r}{\\operatorname*{max}(0,\\|z^{(k)}-x_{i}\\|_{2}^{-1}-\\frac{1}{\\gamma})}\\end{array}$ as $w_{i}$ , and then $\\begin{array}{r}{\\alpha\\,=\\,\\frac{1}{\\sum_{i}w_{i}}}\\end{array}$ i1 wi yields a similar reweighting   \niteration z(k+1) =  i wi(k)xi. i wi(k) ", "page_idx": 13}, {"type": "text", "text": "$\\ell_{2}$ estimator. It is worth noting that the same technique can be applied to the $\\ell_{2}$ estimator with $\\rho(z):=\\|z\\|_{2}^{2}$ . The iteration becomes ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{z^{(k+1)}=z^{(k)}-\\alpha\\nabla_{z}\\displaystyle\\sum_{i}\\|z-{\\pmb x}_{i}\\|_{2}^{2}}}\\\\ {{\\displaystyle=z^{(k)}-\\alpha\\sum_{i}(z^{(k)}-{\\pmb x}_{i}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and $\\begin{array}{r}{\\alpha\\,=\\,\\frac{1}{n+m}}\\end{array}$ yields $\\textstyle z^{(k+1)}\\,=\\,{\\frac{1}{n+m}}\\sum_{i}x_{i}$ n+1m i xi, which gives the mean of all samples in one single iteration. ", "page_idx": 13}, {"type": "text", "text": "Similarities between this mean estimation scenario and our QN-IRLS in graph smoothing can be observed, both of which involve iterative reweighting to estimate similar objectives. The approximated Hessian in our QN-IRLS resembles the Weiszfeld method, canceling the $\\bar{z}^{(k)}$ by tuning the stepsize. ", "page_idx": 13}, {"type": "text", "text": "In Figure 2, we visualize the generated clean samples and outliers, as well as the ground truth means and the mean estimators with $\\eta(\\cdot)\\,=\\,\\|\\,\\cdot\\,\\|_{2}^{2}$ or $\\|\\cdot\\|_{2}$ under different outlier ratios $(15\\%$ , ", "page_idx": 13}, {"type": "text", "text": "$30\\%,\\,45\\%)$ . The results show that the $\\ell_{2}$ -based estimator deviates far from the true mean, while the $\\ell_{1}$ -based estimator is more resistant to outliers, which explains why $\\ell_{1}$ -based methods exhibit stronger robustness. However, as the ratio of outliers escalates, the $\\ell_{1}$ -based estimator encounters a greater shift from the true mean due to the accumulated bias caused by outliers. This observation explains why $\\ell_{1}$ -based graph smoothing models suffer from catastrophic degradation under large attack budgets. Our estimator keeps much closer to the ground truth than other estimators with the existence of outliers. ", "page_idx": 14}, {"type": "text", "text": "A.2 Additional Simulation Results and Discussions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here, we complement Figure 2 with the settings of higher attack budgets. As the outlier ratio exceeds the breakdown point $50\\%$ , we observe that our MCP-based mean estimator can correctly recover the majority of the samples, i.e. converge to the center of \u201coutliers\u201d. ", "page_idx": 14}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/273693689e5e64a2041dc74e9dacb31be912581363117936cfaf6347c2b53222.jpg", "img_caption": ["Figure 8: The trajectory of our MCP-based mean estimator. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Convergence Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To begin with, we will provide an overview of our proof, followed by a detailed presentation of the formal proof for the convergence analysis. ", "page_idx": 14}, {"type": "text", "text": "Overview of proof. First, for both IRLS and QN-IRLS, we construct, for $\\pmb{F}^{(k)}$ in every iteration $k$ , a quadratic upper bound $\\hat{\\mathcal{H}}$ that satisfies $\\hat{\\mathcal{H}}+C\\geq\\mathcal{H}$ where the equality is reached at $\\pmb{F}^{(k)}$ . Then we can minimize $\\hat{\\mathcal{H}}$ to guarantee the iterative descent of $\\mathcal{H}$ since $\\begin{array}{r}{\\dot{\\mathcal{H}}(F^{(k+1)})\\leq\\hat{\\mathcal{H}}(F^{(k+1)})+C\\leq}\\end{array}$ $\\hat{\\mathcal{H}}(\\pmb{F}^{(k)})+C=\\mathcal{H}(\\pmb{F}^{(k)})$ . ", "page_idx": 14}, {"type": "text", "text": "To find the ${\\pmb F}^{(k+1)}$ such that $\\hat{\\mathcal{H}}(F^{(k+1)})\\leq\\hat{\\mathcal{H}}(F^{(k)})$ , IRLS simply adopts the plain gradient descent $\\pmb{F}^{(k+1)}=\\pmb{F}^{(k)}-\\eta\\nabla\\hat{\\mathcal{H}}(\\pmb{F}^{(k)})$ whose convergence condition can be analyzed with the $\\beta.$ -smoothness of the quadratic $\\hat{\\mathcal{H}}$ (Theorem 1). To address the problems of IRLS as motivated in Section 3.2, our Quasi-Newton IRLS utilizes the diagonal approximate Hessian Q\u02c6 to scale the update step size in different dimensions respectively as $\\pmb{F}^{(k+1)}=\\pmb{F}^{(k)}-\\hat{\\pmb{Q}}^{-1}\\nabla\\hat{\\mathcal{H}}(\\pmb{F}^{(k)})$ . Thereafter, by bounding the Hessian with $2\\hat{Q}$ , the descent condition of $\\hat{\\mathcal{H}}$ is simplified (Theorem 2). ", "page_idx": 14}, {"type": "text", "text": "Lemma 1. For any $\\rho(y)$ satisfying $\\frac{d\\rho(y)}{d y^{2}}$ is non-increasing, denote $\\begin{array}{r}{y_{i j}:=\\left\\|\\frac{f_{i}}{\\sqrt{d_{i}}}-\\frac{\\boldsymbol{f}_{j}}{\\sqrt{d_{j}}}\\right\\|_{2}}\\end{array}$ , then $\\begin{array}{r}{\\pmb{\\mathcal{H}}(\\pmb{F})=\\sum_{(i,j)\\in\\mathcal{E},i\\neq j}\\rho(y_{i j})+\\lambda\\sum_{i\\in\\mathcal{V}}\\|\\pmb{f}_{i}-\\pmb{f}_{i}^{(0)}\\|_{2}^{2}}\\end{array}$ has the following upper bound: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\pmb{F})\\leq\\hat{\\mathcal{H}}(\\pmb{F})+C=\\sum_{(i,j)\\in\\mathcal{E},i\\neq j}W_{i j}^{(k)}y_{i j}^{2}+\\lambda\\sum_{i\\in\\mathcal{V}}\\|\\pmb{f}_{i}-\\pmb{f}_{i}^{(0)}\\|_{2}^{2}+C,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{W_{i j}^{(k)}=\\frac{\\partial\\rho(y)}{\\partial y^{2}}\\big|_{y=y_{i j}^{(k)}}}\\end{array}$ and $\\begin{array}{r}{y_{i j}^{(k)}=\\left\\|\\frac{\\pmb{f}_{i}^{(k)}}{\\sqrt{d_{i}}}-\\frac{\\pmb{f}_{j}^{(k)}}{\\sqrt{d_{j}}}\\right\\|_{2}}\\end{array}$ and $C=\\mathcal{H}(F^{(k)})\\!-\\!\\hat{\\mathcal{H}}(F^{(k)})$ is a constant. The equality in Eq. (15) is achieved when $\\pmb{F}=\\pmb{F}^{(k)}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $v=y^{2}$ and define $\\psi(v):=\\rho(y)=\\rho(\\sqrt{v})$ . Then $\\psi$ is concave since d\u03c8(v) = d\u03c1(y) is non-increasing. According to the concavity property, we have $\\psi(v)\\leq\\psi(v_{0})+\\psi^{\\prime}(\\nu)\\big|_{\\nu=v_{0}}(v-v_{0})$ . Substitute $v=y^{2},v_{0}=y_{0}^{2}$ , we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\rho({\\boldsymbol y})\\leq y^{2}\\frac{\\partial\\rho({\\boldsymbol y})}{\\partial y^{2}}\\big\\rvert_{{\\boldsymbol y}={\\boldsymbol y}_{0}}-y_{0}^{2}\\frac{\\partial\\rho({\\boldsymbol y})}{\\partial y^{2}}\\big\\rvert_{{\\boldsymbol y}={\\boldsymbol y}_{0}}+\\rho({\\boldsymbol y}_{0})}\\\\ {\\displaystyle=y^{2}\\frac{\\partial\\rho({\\boldsymbol y})}{\\partial y^{2}}\\big\\rvert_{{\\boldsymbol y}={\\boldsymbol y}_{0}}+C({\\boldsymbol y}_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequality is reached when $y=y_{0}$ . Next, substitute $y=y_{i j}$ and $y_{0}=y_{i j}^{(k)}$ , we can get $\\rho(y_{i j})\\leq\\pmb{W}_{*j}^{(k)}y_{i j}^{2}+C_{\\cdot}(y_{i j}^{(k)})$ ) hich takes the equality at yij = yi(jk ) w . Finally, by summing up both sides and add a regularization term, we can prove the Eq. (15). \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Remark 1. It can be seen that the definition of $\\hat{\\mathcal{H}}$ depends on $\\pmb{F}^{(k)}$ , which ensures that the bound is tight when $\\pmb{F}=\\pmb{F}^{(k)}$ . This tight bound condition is essential in the majorization-minimization algorithm as seen in Theorem $^{\\,l}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. For $\\begin{array}{r}{\\hat{\\mathcal{H}}=\\sum_{(i,j)\\in\\mathcal{E},i\\neq j}W_{i j}y_{i j}^{2}+\\lambda\\sum_{i\\in\\mathcal{V}}\\|f_{i}-f_{i}^{(0)}\\|_{2}^{2}}\\end{array}$ , the gradient and Hessian w.r.t. $F^{6}$ satisfy ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{F_{m n}}\\hat{\\mathcal{H}}(F)=2\\left((d i a g(\\pmb{q})-W\\odot\\tilde{\\pmb{A}}+\\lambda I)\\pmb{F}-\\lambda\\pmb{F}^{(0)}\\right)_{m n},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{F_{m n}}\\nabla_{F_{k l}}\\hat{\\mathcal{H}}(F)=2\\left(d i a g(\\pmb{q})-W\\odot\\tilde{\\pmb{A}}+\\lambda\\pmb{I}\\right)_{m k},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\pmb q_{m}\\;=\\;\\sum_{j}{W_{m j}\\pmb A_{m j}/d_{m}}}\\end{array}$ and $\\begin{array}{r}{\\tilde{A}_{i j}\\,=\\,\\frac{A_{i j}}{\\sqrt{d_{i}d_{j}}}}\\end{array}$ is the symmetrically normalized adjacency matrix. ", "page_idx": 15}, {"type": "text", "text": "Proof. Follow $A\\;=\\;A^{\\top}$ and define $\\begin{array}{r l r}{y_{i j}^{2}\\!\\!}&{{}:=}&{\\!\\!\\left\\|\\frac{{\\mathbf{f}}_{i}}{\\sqrt{d_{i}}}-\\frac{{\\mathbf{f}}_{j}}{\\sqrt{d_{j}}}\\right\\|_{2}^{2}}\\end{array}$ , then the first-order gradient of $\\sum_{(i,j)\\in\\mathcal{E},i\\neq j}W_{i j}y_{i j}^{2}$ will be ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{F}_{n}}\\bigg(\\underset{0\\leq t\\leq t\\leq t\\delta_{1}}{\\sum_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}V_{t_{0}(\\mathbf{F}_{n})}\\bigg)}\\\\ &{=\\underset{0\\leq t\\leq t\\leq t}{\\sum_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}\\mathbf{W}_{t_{0}(\\mathbf{F}_{n})}^{(\\mathbf{F}_{n})}\\frac{\\partial_{t_{0}}^{2}}{\\partial\\mathbf{F}_{n}^{(\\mathbf{F}_{n})}}}\\\\ &{=\\underset{0\\leq t\\leq t}{\\sum_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}\\mathbf{W}_{t_{0}(\\mathbf{F}_{n})}\\frac{\\partial_{t_{0}}^{2}}{\\partial\\mathbf{F}_{n}}}\\\\ &{=\\underset{0\\leq t\\leq t}{\\sum_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}\\mathbf{W}_{t_{0}(\\mathbf{F}_{n}-\\frac{t}{\\delta_{1}})}\\bigg(\\underset{0\\leq t\\leq t}{\\sum_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}\\frac{\\partial_{t_{0}}^{2}}{\\partial\\mathbf{F}_{n}}}\\\\ &{=\\underset{0\\leq t\\leq t}{\\sum_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}\\mathbf{W}_{t_{0}(\\mathbf{F}_{n}-\\frac{t}{\\delta_{1}})}\\frac{\\partial_{t_{0}}^{2}}{\\partial\\mathbf{F}_{n}}-\\frac{p_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}{p_{0}(\\mathbf{F}_{n})}}\\\\ &{=\\underset{0\\leq t\\leq t}{\\sum_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}\\mathbf{W}_{t_{0}(\\mathbf{F}_{n}-\\frac{t}{\\delta_{1}})}\\frac{\\partial_{t_{0}}^{2}}{\\partial\\mathbf{F}_{n}}-\\frac{p_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}{p_{0}(\\mathbf{F}_{n})}}\\\\ &{=2\\underset{0\\leq t\\leq t}{\\sum_{\\mathbf{F}_{n}\\in\\mathcal{S}_{n}}}\\mathbf{W}_{t_{0}(\\mathbf{F}_{n}-\\frac{t}{\\delta_{1}})}p_{\\mathbf{F}_{n} \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the second-order hessian will be: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{F_{m}}^{2}F_{i\\alpha}\\left(\\underset{(i,j)\\in\\mathcal{E},i\\not=j}{\\sum}W_{i j}y_{i j}^{2}\\right)}\\\\ &{=\\underset{(i,j)\\in\\mathcal{E},i\\not=j}{\\sum}W_{i j}\\frac{\\partial y_{i j}^{2}}{\\partial F_{m}\\partial{F}_{k l}}}\\\\ &{=2\\frac{\\partial}{\\partial F_{k l}}\\left(\\underset{j}{\\sum}W_{m j}\\big(\\frac{A_{m j}}{d_{m}}F_{m n}-\\frac{A_{m j}}{\\sqrt{d_{m}d_{j}}}F_{j n}\\big)\\right)}\\\\ &{=2(q_{m}\\delta_{m k}-\\underset{j}{\\sum}W_{m j}\\frac{A_{m j}}{\\sqrt{d_{m}d_{j}}}\\delta_{j k})\\delta_{n l}}\\\\ &{=2(\\mathrm{diag}(q)-W\\odot\\bar{A})_{m k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Remark 2. From Eq. (21) to Eq. (24), one can assume $m\\not\\in\\mathcal{N}(m)$ , and thus $W_{m m}=0.$ . However, as we know, a self-loop is often added to $\\pmb{A}$ to facilitate stability by avoiding zero-degree nodes that cannot be normalized. This is not as problematic as it seems, though. Because (i,j)\u2208E,i\u0338=j intrinsically excludes the diagonal terms, we can simply assign zero to the diagonal terms of $W$ so that the term of $j=m$ is still zero in Eq. (24), as defined in Eq. (5). ", "page_idx": 16}, {"type": "text", "text": "To minimize $\\hat{\\mathcal{H}}$ , the gradient descent update rule takes the form of Eq. (6). One may assume that when $\\eta$ is chosen to be small enough, $\\bar{\\mathcal{H}}(F^{(k+1)})\\leq\\hat{\\mathcal{H}}(F^{(k)})$ . For a formal justification, we have Theorem 1 to determine the convergence condition of $\\eta$ . ", "page_idx": 16}, {"type": "text", "text": "Theorem 1. If $\\pmb{F}^{(k)}$ follows the update rule in Eq. (6), where the $\\rho$ satisfies that $\\frac{d\\rho(y)}{d y^{2}}$ is nondecreasing for $y\\in(0,\\infty).$ , then a sufficient condition for $\\mathcal{H}(F^{(k+1)})\\leq\\mathcal{H}(F^{(k)})$ is that the step size $\\eta$ satisfies $0<\\eta\\leq\\|d i a g(\\pmb{q}^{(k)})-\\pmb{W}^{(k)}\\odot\\tilde{\\pmb{A}}+\\lambda\\pmb{I}\\|_{2}^{-1}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. The descent of ${\\hat{\\mathcal{H}}}(F)$ can ensure the descent of $\\mathcal{H}(F)$ since $\\begin{array}{r}{\\mathcal{H}(F^{(k+1)})\\leq\\hat{\\mathcal{H}}(F^{(k+1)})\\leq}\\end{array}$ $\\hat{\\mathcal{H}}(\\pmb{F}^{(k)})=\\mathcal{H}(\\pmb{F}^{(k)})$ . Therefore, we only need to prove $\\hat{\\mathcal{H}}(F^{(k+1)})\\leq\\hat{\\mathcal{H}}(F^{(k)})$ . ", "page_idx": 17}, {"type": "text", "text": "Noting that $\\hat{\\mathcal{H}}$ is a quadratic function and $\\pmb{F}^{(k+1)}-\\pmb{F}^{(k)}=-\\eta\\nabla\\hat{\\mathcal{H}}(\\pmb{F}^{(k)})$ , then $\\hat{\\mathcal{H}}(\\pmb{F}^{(k+1)})$ and $\\hat{\\mathcal{H}}(\\pmb{F}^{(k)})$ can be connected using Taylor expansion6, where $\\nabla\\hat{\\mathcal{H}}$ and $\\nabla^{2}{\\hat{\\boldsymbol{H}}}$ is given in Lemma 2: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathcal{H}}(F^{(k+1)})-\\hat{\\mathcal{H}}(F^{(k)})}\\\\ &{=\\mathrm{tr}\\left(\\nabla\\hat{\\mathcal{H}}(F^{(k)})^{\\top}(F^{(k+1)}-F^{(k)})\\right)}\\\\ &{~~~+\\frac{1}{2}\\mathrm{tr}\\left((F^{(k+1)}-F^{(k)})^{\\top}\\nabla^{2}\\hat{\\mathcal{H}}(F^{(k)})(F^{(k+1)}-F^{(k)})\\right)}\\\\ &{=\\mathrm{tr}\\left(-\\eta\\nabla\\hat{\\mathcal{H}}(F^{(k)})^{\\top}\\nabla\\hat{\\mathcal{H}}(F^{(k)})\\right)}\\\\ &{~~~+\\mathrm{tr}\\left(\\frac{\\eta^{2}}{2}\\nabla\\hat{\\mathcal{H}}(F^{(k)})^{\\top}\\nabla^{2}\\hat{\\mathcal{H}}(F^{(k)})\\nabla\\hat{\\mathcal{H}}(F^{(k)})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Insert $\\nabla^{2}\\hat{\\mathcal{H}}(\\pmb{F}^{(k)})=2(\\mathrm{diag}(\\pmb{q})-\\pmb{W}^{(k)}\\odot\\tilde{\\pmb{A}}+\\lambda\\pmb{I})$ from Lemma 2 into the above equation and we can find a sufficient condition for $\\hat{\\mathcal{H}}(F^{(k+1)})\\leq\\hat{\\mathcal{H}}(F^{(k)})$ to be ", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\eta+\\|\\mathrm{diag}(\\pmb{q})-\\pmb{W}^{(k)}\\odot\\tilde{\\pmb{A}}+\\hat{\\lambda}\\pmb{I}\\|_{2}\\eta^{2}\\leq0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "or ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta\\leq\\|\\mathrm{diag}(q)-W^{(k)}\\odot\\tilde{\\mathbf{A}}+\\hat{\\lambda}\\mathbf{I}\\|_{2}^{-1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we prove that when taking the Quasi-Newton-IRLS step as in Eq. (7), the objective $\\hat{\\mathcal{H}}$ is guaranteed to descend. Since the features in different dimensions are irrelevant, we simplify our notations as if feature dimension was 1. One may easily recover the general scenario by taking the trace. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. $2\\hat{Q}\\!-\\!\\nabla^{2}\\hat{\\mathcal{H}}(\\pmb{y})$ is positive semi-definite, where $\\begin{array}{r}{\\hat{\\mathcal{H}}=\\sum_{(i,j)\\in\\mathcal{E},i\\neq j}W_{i j}y_{i j}^{2}+\\lambda\\sum_{i\\in\\mathcal{V}}\\|f_{i}-}\\end{array}$ $\\pmb{f}_{i}^{(0)}||_{2}^{2}$ , $\\hat{Q}=2(d i a g(\\pmb q)+\\lambda\\pmb I),$ , and $\\begin{array}{r}{\\pmb q_{m}=\\sum_{j}{W_{m j}\\pmb A_{m j}/d_{m}}}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. In Lemma 2, we have $\\nabla^{2}\\hat{\\mathcal{H}}(\\pmb{y})=2(\\mathrm{diag}(\\pmb{q})+\\lambda\\pmb{I}-\\pmb{W}\\odot\\tilde{\\pmb{A}})$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n2\\hat{\\cal Q}-\\nabla^{2}\\hat{\\mathcal{H}}(y)=2(\\mathrm{diag}(q)+\\lambda I+W\\odot\\tilde{A}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall how we derived Eq. (27) from Eq. (15), where we proved that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{(i,j)\\in\\mathcal{E},i\\neq j}W_{i j}\\|\\frac{f_{i}}{\\sqrt{d_{i}}}-\\frac{f_{j}}{\\sqrt{d_{j}}}\\|_{2}^{2}=\\mathrm{tr}(\\boldsymbol{F}^{\\top}(\\mathrm{diag}(q)-\\boldsymbol{W}\\odot\\tilde{\\boldsymbol{A}})\\boldsymbol{F}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which holds for all $\\pmb{F}$ . Similarly, the equation still holds after flipping the sign before $f_{j}/\\sqrt{d_{j}}$ and $W\\odot\\tilde{A}$ . We then have this inequality: $\\forall{\\pmb F},{\\forall\\lambda}\\geq0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq\\displaystyle\\sum_{(i,j)\\in\\mathcal{E},i\\neq j}W_{i j}\\|\\frac{f_{i}}{\\sqrt{d_{i}}}+\\frac{f_{j}}{\\sqrt{d_{j}}}\\|_{2}^{2}=\\mathrm{tr}(F^{\\top}(\\mathrm{diag}(q)+W\\odot\\tilde{A})F)}\\\\ &{\\quad\\leq\\mathrm{tr}(F^{\\top}(\\mathrm{diag}(q)+W\\odot\\tilde{A}+\\lambda I)F).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, $(\\mathrm{diag}(q)+W\\odot\\tilde{\\mathbf{A}}+\\lambda I)\\succeq0$ , and thus $2\\hat{\\boldsymbol{Q}}-\\nabla^{2}\\hat{\\mathcal{H}}(\\boldsymbol{y})\\succeq0$ ", "page_idx": 17}, {"type": "text", "text": "Using Lemma 1 and Lemma 3 we can prove Theorem 2. Note that we continue to assume #feature $\\c=1$ for simplicity but without loss of generality6. ", "page_idx": 17}, {"type": "text", "text": "Theorem 2. If ${\\pmb F}^{(k+1)}$ follows update rule in Eq. (7), where $\\rho$ satisfies that $\\frac{d\\rho(y)}{d y^{2}}$ is non-decreasing $\\forall y\\in(0,\\infty)$ , it is guaranteed that $\\mathcal{H}(F^{(k+1)})\\leq\\mathcal{H}(F^{(k)})$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Following the discussions in Theorem 1, we only need to prove $\\hat{\\mathcal{H}}(F^{(k+1)})\\leq\\hat{\\mathcal{H}}(F^{(k)})$ .For the quadratic $\\hat{\\mathcal{H}}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{H}}(\\pmb{x})=\\hat{\\mathcal{H}}(\\pmb{y})+\\nabla\\hat{\\mathcal{H}}(\\pmb{y})^{\\top}(\\pmb{x}-\\pmb{y})+\\frac{1}{2}(\\pmb{x}-\\pmb{y})^{\\top}\\nabla^{2}\\hat{\\mathcal{H}}(\\pmb{y})(\\pmb{x}-\\pmb{y}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can define $\\mathcal{Q}(\\pmb{y})=2\\hat{Q}(\\pmb{y})$ in Lemma 3 such that $\\mathcal{Q}(\\pmb{y})-\\nabla^{2}\\hat{\\mathcal{H}}(\\pmb{y})\\succeq0$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall z,z^{\\top}\\mathcal{Q}(y)z\\geq z^{\\top}\\nabla^{2}\\hat{\\mathcal{H}}(y)z.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then an upper bound of $\\hat{\\mathcal{H}}(\\pmb{x})$ can be found by inserting Eq. (45) into Eq. (44). ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{H}}(\\pmb{x})\\leq\\hat{\\mathcal{H}}(\\pmb{y})+\\nabla\\hat{\\mathcal{H}}(\\pmb{y})^{\\top}(\\pmb{x}-\\pmb{y})+\\frac{1}{2}(\\pmb{x}-\\pmb{y})^{\\top}\\mathcal{Q}(\\pmb{y})(\\pmb{x}-\\pmb{y}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, insert $\\mathscr{Q}=2\\hat{Q}$ into Eq. (46). Note that $\\hat{Q}:=2(\\mathrm{diag}(q)+\\hat{\\lambda}I)$ , so $\\hat{Q}\\succeq0$ and $\\hat{Q}^{\\top}=\\hat{Q}$ . Thereafter, the update rule $\\pmb{x}=\\pmb{y}-\\hat{Q}^{-1}\\nabla\\hat{\\mathcal{H}}(\\pmb{y})$ in Eq. (7) gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\hat{\\mathcal{H}}(\\boldsymbol{x})-\\hat{\\mathcal{H}}(\\boldsymbol{y})}\\\\ &{\\le\\nabla\\hat{\\mathcal{H}}(\\boldsymbol{y})^{\\top}(\\boldsymbol{x}-\\boldsymbol{y})+\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{y})^{\\top}\\mathcal{Q}(\\boldsymbol{y})(\\boldsymbol{x}-\\boldsymbol{y})}\\\\ &{=\\nabla\\hat{\\mathcal{H}}(\\boldsymbol{y})^{\\top}(\\boldsymbol{x}-\\boldsymbol{y})+2\\left(\\hat{Q}^{\\frac{1}{2}}(\\boldsymbol{x}-\\boldsymbol{y})\\right)^{\\top}\\left(\\hat{Q}^{\\frac{1}{2}}(\\boldsymbol{x}-\\boldsymbol{y})\\right)}\\\\ &{=2\\nabla\\hat{\\mathcal{H}}(\\boldsymbol{y})^{\\top}\\hat{Q}^{-1}\\nabla\\hat{\\mathcal{H}}(\\boldsymbol{y})-2\\nabla\\hat{\\mathcal{H}}(\\boldsymbol{y})^{\\top}(\\hat{Q}^{-\\frac{1}{2}})^{\\top}\\hat{Q}^{-\\frac{1}{2}}\\nabla\\hat{\\mathcal{H}}(\\boldsymbol{y})}\\\\ &{=\\boldsymbol{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, our QN-IRLS in Eq. (7) is guaranteed to descend. ", "page_idx": 18}, {"type": "text", "text": "C Computation Efficiency ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our RUNG model preserves advantageous efficiency even adopting the quasi-Newton IRLS algorithm. ", "page_idx": 18}, {"type": "text", "text": "C.1 Time Complexity Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Each RUNG layer involves computing $W,q_{\\mathrm{{r}}}$ , and the subsequent aggregations. We elaborate on them one by one. We denote the number of feature dimensions $d$ , the number of nodes $n$ , and the number of edges $m$ , which are assumed to satisfy $m\\gg1$ , $n\\gg1$ and $d\\gg1$ . The number of layers is denoted as $k$ . The asymptotic computation complexity is denoted as $O(\\cdot)$ . ", "page_idx": 18}, {"type": "text", "text": "Computation of $W\\odot A$ and $W_{\\odot}\\triangleleft$ . $\\begin{array}{r}{W:=\\mathbf{1}_{i\\neq j}\\,\\frac{d\\rho_{\\gamma}(y_{i j})}{d y_{i j}^{2}}}\\end{array}$ d\u03c1d\u03b3y(2yij)is the edge weighting matrix dependent on the node feature matrix $\\pmb{F}$ . The computation of $\\begin{array}{r}{y_{i j}=\\|\\frac{\\textbf f_{i}}{\\sqrt{d_{i}}}-\\frac{\\textbf f_{j}}{\\sqrt{d_{j}}}\\|_{2}}\\end{array}$ is $O(d)$ and that of $\\frac{d\\rho_{\\gamma}(y_{i j})}{d y_{i j}^{2}}$ is $\\mathcal{O}(1)$ . $W_{i j}$ only needs computing when $(i,j)\\in\\mathcal{E}$ , because $\\forall(i,j)\\notin\\mathcal{E},W_{i j}$ will be masked out by $\\pmb{A}$ or $\\tilde{A}$ anyways. Each element of $W$ involves computation time of $O(d)$ and $m$ elements are needed. In total, $W$ costs $\\mathcal{O}(m d)$ , and $W\\odot A$ and $W\\odot\\tilde{A}$ cost $\\mathcal{O}(m d+m)=\\mathcal{O}(m d)$ . ", "page_idx": 18}, {"type": "text", "text": "Computation of $\\hat{Q}^{-1}$ . $\\begin{array}{r}{\\hat{Q}^{-1}:=\\frac{1}{2}(\\mathrm{diag}(\\pmb{q}^{(k)})+\\lambda\\pmb{I})^{-1}}\\end{array}$ is the inverse Hessian in our quasi-Newton IRLS. Because $\\hat{Q}$ is designed to be a diagonal matrix, its inverse can be evaluated as element-wise reciprocal which is efficient. As for $\\begin{array}{r}{\\pmb q:=\\sum_{j}{W_{m j}^{(k)}A_{m j}/d_{m}}}\\end{array}$ , only existing edges $(i,j)\\in\\mathcal{E}$ need evaluation in the summation. Therefore, this computation costs $\\mathcal{O}(m)$ . Thus, $\\hat{Q}^{-1}$ costs ${\\mathcal{O}}(m)$ in total. ", "page_idx": 18}, {"type": "text", "text": "Computation of aggregation. An RUNG layer follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pmb{F}^{(k+1)}=2\\hat{\\pmb{Q}}^{-1}\\left((\\pmb{W}^{(k)}\\odot\\tilde{\\pmb{A}})\\pmb{F}^{(k)}+\\lambda\\pmb{F}^{(0)}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which combines the quantities calculated above. An extra graph aggregation realized by the matrix multiplication between $W\\odot\\tilde{A}$ and $\\pmb{F}$ is required, costing $\\mathcal{O}(m d)$ . The subsequent addition to ${\\pmb F}^{(0)}$ and the multiplication to the diagonal $\\hat{Q}^{-1}$ both cost $O(n d)$ . ", "page_idx": 18}, {"type": "text", "text": "Stacking layers. RUNG unrolls the QN-IRLS optimization procedure, which has multiple iterations. Therefore, the convergence increase that QN-IRLS introduces allows a RUNG with fewer layers and increases the overall complexity. It is worth noting that the QN-IRLS utilizes a diagonal approximated Hessian, and thus the computation per iteration is also kept efficient as discussed above. ", "page_idx": 19}, {"type": "text", "text": "Summing up all the costs, we have the total computational complexity of our RUNG, $\\mathcal{O}((m+n)k d)$ .   \nOur RUNG thus scales well to larger graph datasets such as ogbn-arxiv. ", "page_idx": 19}, {"type": "text", "text": "Space Complexity Analysis The only notable extra storage cost is $W$ whose sparse layout takes up $\\mathcal{O}(m)$ . This is the same order of size as the adjacency matrix itself, thus not impacting the total asymptotic complexity. ", "page_idx": 19}, {"type": "text", "text": "C.2 Alternative Perspective ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In fact, the above analysis can be simplified when we look at the local aggregation behavior of RUNG. For node $i$ , it\u2019s updated via aggregation $\\begin{array}{r}{\\pmb{f}_{i}=\\frac{2}{\\pmb{\\hat{Q}}_{i i}^{-1}}((\\sum_{j\\in\\mathcal{N}(i)}W_{i j}\\pmb{f}_{j})+\\lambda\\pmb{f}_{i}^{(0)})}\\end{array}$ . The summation over neighbors\u2019 will give in the total time complexity in each feature dimension, and involves $O(d)$ computations for each neighbor. This sums up to $\\mathcal{O}(m d)$ as well. Essentially, the high efficiency of RUNG originates from that every edge weighting in our model involves only the 2 nodes on this edge. ", "page_idx": 19}, {"type": "text", "text": "D Comprehensive Bias Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide multiple evidence from various perspectives to reveal the estimation bias of $\\ell_{1}$ -based estimation as follows. ", "page_idx": 19}, {"type": "text", "text": "(1) From the theoretical perspective, the extensive literature on high-dimensional statistics [40, 41] has proved that $\\ell_{1}$ regularization induces an estimation bias. ", "page_idx": 19}, {"type": "text", "text": "(2) From the algorithm perspective, in Section 2.3, we provide the explanation on the $\\ell_{1}$ -based estimation problem solver. Specifically, the soft-thresholding operator $S_{\\lambda}^{\\bar{\\lambda}}(\\theta):=\\mathrm{sign}(\\theta)\\operatorname*{max}(|\\theta|-$ $\\lambda,0)$ induced by the $\\ell_{1}$ regularized problem causes a constant shrinkage for $\\theta$ larger than $\\lambda$ , enforcing the estimator to be biased towards zero with the magnitude $\\lambda$ . ", "page_idx": 19}, {"type": "text", "text": "(3) From the numerical simulation in Section 2.3, we provide an example of mean estimation to verify this estimation bias. As shown in Figure 2, the $\\ell_{1}$ estimator (green) deviates further from the true mean as the ratio of outliers escalates. This can be clearly explained as the effect of the accumulation of estimation bias. In other words, each outlier results in a constant bias, and the bias accumulates with more outliers. ", "page_idx": 19}, {"type": "text", "text": "(4) From the performance perspective, $\\ell_{1}$ -based GNNs such as SoftMedian, TWIRLS, and RUNG- $\\ell_{1}$ (the $\\ell_{1}$ variant of our model) suffer from significant performance degradation when the attack budget increases. ", "page_idx": 19}, {"type": "text", "text": "(5) From our ablation study in Figure 6, we quantify the estimation bias of the aggregated feature $f_{i}^{\\star}$ on the attacked graph from the feature $f_{i}$ on the clean graph: $\\textstyle\\sum_{i\\in\\mathcal{V}}\\|f_{i}-\\bar{f}_{i}^{*}\\|_{2}^{2}$ . The results demonstrate that $\\ell_{1}$ -based GNN produces biased estimation under adversarial attacks and the bias indeed scales up with the attack budget. However, our proposed RUNG method exhibits a nearly zero estimation bias under the same attacking budgets. ", "page_idx": 19}, {"type": "text", "text": "All of this evidence can convincingly support our claim that $\\ell_{1}$ -based robust estimator suffers from the estimation bias, which validates the motivation of our new algorithm design. ", "page_idx": 19}, {"type": "text", "text": "E Additional experiment results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we present the experiment results that are not shown in the main paper due to space limits. ", "page_idx": 20}, {"type": "text", "text": "E.1 Adaptive Attacks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 3 and Table 4 are the results of adaptive local and global attacks on Citeseer, referred to in Section 4.2. ", "page_idx": 20}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/443077304797ec32015a193b67c68ce2ffc672a9c762279894d8ac57e053dcbc.jpg", "table_caption": ["Table 3: Adaptive local attack on Citeseer. The best and second are marked. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/e1e5b90b3ec65687c7423043b61d105df2b6876a3e2a2b948d3427e99c0fa5d0.jpg", "table_caption": ["Table 4: Adaptive global attack on Citeseer. The best and second are marked. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.2 Transfer Attacks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 5 and Table 6 are the results of transfer global attacks on Cora ML and Citeseer. Figure 9 and Figure 10 are the experiment results of our RUNG attacked by transfer attacks generated on different surrogate models as mentioned in Section 4.3. ", "page_idx": 20}, {"type": "text", "text": "Figure 11 shows results of global evasion transfer attacks between different models on Cora ML. Our observations are summarized below: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The attacks generated by RUNG are stronger when applied to more robust models like SoftMedian, while are not strong against undefended or weakly defended models. ", "page_idx": 20}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/54159e555ee3ecc22818daa184631184c15f5d13240ea7bbbc4fe43f4bb2e82f.jpg", "table_caption": ["Table 5: Transfer global evasion attack on Cora ML. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/3bbb9d9c458e2b9cb346a63f53c03495422fbaf5d27a4af1b300aba118d2ee2f.jpg", "table_caption": ["Table 6: Transfer global evasion attack on Citeseer. "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/ac8d3b1477012631f6a4f09741394dce132820bd953b422bc5b0566e37fc35b2.jpg", "img_caption": ["Figure 9: Transfer global attack from different surrogate models to our RUNG on Cora ML. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "\u2022 For $\\ell_{1}$ GNNs, the attacks are the strongest when transferred from $\\ell_{1}$ GNNs. This supports again our unified view on $\\ell_{1}$ GNNs. An exception is TWIRLS because it only has one attention layer and does not always converge to the actual $\\ell_{1}$ objective. ", "page_idx": 21}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/ee4b834b9f9ce1332d090f1b5987e7cd67fa93adaa6e1c8b207107f3dc2093a5.jpg", "img_caption": ["Figure 10: Transfer global attack from different surrogate models to our RUNG on Citeseer. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/ad5b96e64b9374373b8debf9c1b458687d8de309d43d7bc2797b5b08bf161c18.jpg", "img_caption": ["Figure 11: Transfer global attack between different model pairs on Cora. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.3 Poisoning Attacks ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We provide the experiment results under poisoning attacks on Cora ML and Citeseer in Table 7 and Table 8, respectively. ", "page_idx": 22}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/c8cee932df5930272344fd3f18cb38fa157b66336f6b95252d7fdb2f2b0c241d.jpg", "table_caption": ["Table 7: Poisoning Attacks on Cora ML "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/ed162ccbe9cb9313f2fec27ea9a8565cff1cfbc4ca748fc8893d90c60a720bfd.jpg", "table_caption": ["Table 8: Poisoning Attacks on Citeseer. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.4 Large Scale Ogbn-Arxiv ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the large scale datasets, we can not directly apply the vanilla PGD attack [36] on them due to excessive requirement of memory and computation on dense matrix. Alternatively, we leverage the PRBCD [42] to scale the PGD attack instead of manipulating on the dense adjacency matrix. We conduct experiments on large scale Ogbn-Arxiv and the results in Table 9 verified the superior robustness of our RUNG model. RUNG\u2019s robustness outperforms its $\\ell_{1}$ variant which delivers similar performance as SoftMedian and Elastic GNN due to their shared $\\ell_{1}$ graph smoothing. ", "page_idx": 23}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/b40b90423e4dccd5499c11a182698304e9049514220b8da2385a215c4a5d2414.jpg", "table_caption": ["Table 9: Global PGD Attacks on Ogbn-Arxiv. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.5 Adversarial Training ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We conduct the adversarial training following [36] and present the results in Table 10. From the results, we can observe that with adversarial training, the robustness of RUNG can be further improved. ", "page_idx": 23}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/f75852f92eee855eb50e6932f83eaaa4e943748fb594c2d5f2856dafa064040c.jpg", "table_caption": ["Table 10: Adversarial Training vs Normal Training on RUNG. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.6 Graph Injection Attack ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The injection attack was conducted following the settings in [37] to evaluate the robustness of different methods. We set up the budget on the number of injected nodes as 100 and the budget on degree as 200. The results in Table 11 show that our RUNG significantly outperforms the baseline models. ", "page_idx": 23}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/2797492302e5a7ba87e0d05e4fbf0a39fc0746a1bbfc21297064e907dd7bb113.jpg", "table_caption": ["Table 11: Graph Injection Attack on Citeseer. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F Adaptive Attack Settings ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For the adaptive PGD attack we adpoted in the experiments, we majorly followed the algorithm in [5] in the adaptive evasion attack. For the sake of completeness, we describe it below: ", "page_idx": 24}, {"type": "text", "text": "In summary, we consider the topology attack setting where the adjacency matrix $\\pmb{A}$ is perturbed by $\\delta{\\bf A}$ whose element $\\delta A_{i j}\\,\\in\\,\\bar{\\{}0,}1\\bar{\\}$ . The budget $B$ is defined as $\\dot{\\boldsymbol{B}}\\geq\\|\\delta\\pmb{A}\\|_{0}$ . The PGD attack involves first relaxing $\\pmb{A}$ from binary to continuous so that a gradient ascent attack can be conducted on the relaxed graph. ", "page_idx": 24}, {"type": "text", "text": "During the attack, the minimization problem below is solved: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\delta\\pmb{A}_{\\star}=\\operatorname*{arg\\,min}_{\\delta\\pmb{A}}\\mathcal{L}_{\\mathrm{atack}}(\\mathbf{G}\\mathbf{N}\\mathbf{N}_{\\theta}(\\pmb{A}+(\\pmb{I}-2\\pmb{A})\\odot\\delta\\pmb{A},\\pmb{F}),y_{\\mathrm{target}}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathcal{L}$ is carefully designed attack loss function [4, 5], $\\pmb{A}$ , $\\pmb{F}$ and $y_{\\mathrm{target}}$ are respectively the graph, node feature matrix and ground truth labels in the dataset, $\\theta$ is the parameters of the GNN under attack which are not altered in the evasion attack setting. $\\left(\\pmb{I}-2\\pmb{A}\\right)\\odot\\delta\\pmb{A}$ is the calculated perturbation that \u201cfilps\u201d the adjacency matrix between 0 and 1 when it is perturbed. The gradient of $\\frac{\\mathcal{L}_{\\mathrm{attack}}}{\\delta A}$ is computed and utilized to update the perturbation matrix $\\delta{\\bf A}$ . ", "page_idx": 24}, {"type": "text", "text": "After the optimization problem is solved, $\\delta{\\bf A}$ is projected back to the feasible domain of $\\delta A_{i j}\\in\\{1\\}$ . The adjacency matrix serves as a probability matrix allowing a Bernoulli sampling of the binary adjacency matrix $A^{\\prime}$ . The sampling is executed repeatedly so that an $A^{\\prime}$ producing the strongest perturbation is finally generated. ", "page_idx": 24}, {"type": "text", "text": "G Additional Ablation Study of RUNG ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "G.1 Hyperparameters ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The choice of the hyperparameters $\\gamma$ and $\\lambda$ is crucial to the performance of RUNG. We therefore experimented with their different combinations and conducted adaptive attacks on Cora as shown in Fig. 12. ", "page_idx": 24}, {"type": "text", "text": "Recall the formulation of RUNG in Eq.(8): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F^{(k+1)}=(\\mathrm{diag}(q^{(k)})+\\lambda I)^{-1}\\left((W^{(k)}\\odot\\tilde{A})F^{(k)}+\\lambda F^{(0)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the formulation, $\\lambda$ controls the intensity of the regularization in the graph smoothing. In our experiments, we tune $\\begin{array}{r}{\\hat{\\lambda}:=\\frac{1}{1+\\lambda}}\\end{array}$ which is normalized into $(0,\\ 1)$ . In Figure 12, the optimal value of $\\hat{\\lambda}$ can be found almost always near 0.9 regardless of the attack budget. This indicates that our penalty function $\\rho_{\\gamma}$ is decoupled from $\\gamma$ which makes the tuning easier, contrary to the commonly used formulation of MCP [25]. ", "page_idx": 24}, {"type": "text", "text": "On the other hand, $\\gamma$ has a more intricate impact on the performance of RUNG. Generally speaking, the smaller $\\gamma$ is, the more edges get pruned, which leads to higher robustness and a lower clean accuracy. We begin our discussion in three cases: ", "page_idx": 24}, {"type": "text", "text": "Small attack budget $(0\\%,5\\%,10\\%)$ ). The performance is largely dependent on clean accuracy. Besides, when $\\gamma\\to\\infty$ , RUNG becomes a state-of-the-art robust $\\ell_{1}$ model. Therefore, a small $\\gamma$ ", "page_idx": 24}, {"type": "text", "text": "likely introduces more harm to the clean performance than robustness increments over $\\ell_{1}$ models.   \nThe optimal $\\gamma$ thus at least recovers the performance of $\\ell_{1}$ models. ", "page_idx": 25}, {"type": "text", "text": "Large attack budget $(20\\%,30\\%,40\\%)$ . In these cases, $\\gamma\\to\\infty$ is no longer a good choice because $\\ell_{1}$ models are beginning to suffer from the accumulated bias effect. The optimal $\\gamma$ is thus smaller (near 0.5). However, for fairness, we chose the same $\\gamma$ under different budgets in our experiments, so the reported RUNG fixes $\\gamma=3$ . In reality, however, when we know the possible attack budgets in advance, we can tune $\\gamma$ for an even better performance. ", "page_idx": 25}, {"type": "text", "text": "Very large attack budget $(50\\%$ , $60\\%$ ). We did not include these scenarios because almost all GNNs perform poorly in this region. However, we believe it can provide some insights into robust graph learning. Under these budgets, more than half of the edges are perturbed. In the context of robust statistics (e.g. mean estimation), the estimator will definitely break down. However, in our problem of graph estimation, the input node features offer extra information allowing us to exploit the graph information even beyond the breakdown point. In the \u201cpeak\u201d near (0.9, 0.5), RUNG achieves $>70\\%$ accuracy which is higher than MLP. This indicates that the edge weighting of RUNG is capable of securely harnessing the graph information even in the existence of strong adversarial attacks. The \u201cridge\u201d near a $\\hat{\\lambda}=0.2$ , on the other hand, emerges because of MLP. When the regularization dominates, $\\lambda\\rightarrow\\infty$ , and $\\hat{\\lambda}\\to0$ . A small $\\lambda$ is then connected to a larger emphasis on the input node feature prior. Under large attack budgets, MLP delivers relatively good estimation, so a small $\\hat{\\lambda}$ is beneficial. ", "page_idx": 25}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/f75acec55b042905f645ad6e5fee8936fccabe8155982b4bd82b902d334d20fd.jpg", "img_caption": ["Figure 12: The performance dependence of RUNG with different hyperparameters $\\gamma$ and $\\lambda$ . The performance is evaluated under different attack budgets. The attack setting is the global evasion attack and the dataset is Cora. Note the $\\mathbf{X}$ -axis is $\\begin{array}{r}{\\hat{\\lambda}:=\\frac{1}{1+\\lambda}}\\end{array}$ 1+1\u03bb instead of \u03bb. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "G.2 GNN Layers ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In RUNG, QN-IRLS is unrolled into GNN layers. We would naturally expect RUNG to have enough number of layers so that the estimator converges as desired. We conducted an ablation study on the performance (clean and adversarial) of RUNG with different layer numbers and the results are shown in Fig. Figure 13. We make the following observations: ", "page_idx": 25}, {"type": "text", "text": "\u2022 As the layer number increases, RUNG exhibits better performance. This verifies the effectiveness of our proposed RUGE, as well as the stably converging QN-IRLS. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The performance of RUNG can achieve a reasonably good level even with a small layer number (3-5 layers) with accelerated convergence powered by QN-IRLS. This can further reduce the computation complexity of RUNG. ", "page_idx": 25}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/d96332de5cbd23fce4bda4fee70ce220c7b899dbdc3fd0a3e8ed62fbe568642b.jpg", "img_caption": ["Figure 13: The performance dependence of RUNG on the number of aggregation layers. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "H Robustness of GCN and APPNP ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In addition to the formulation in section 3 the main text, we can simply apply our edge reweighting technique to the GCN architecture. Essentially the aggregation operation in GCN can be viewed as an APPNP layer with $\\hat{\\lambda}=0$ . The GCN version of a layer in our model can then be formulated as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{F}^{(k+1)}=\\operatorname{ReLU}((\\pmb{W}^{(k)}\\odot\\tilde{\\pmb{A}})\\pmb{F}^{(k)}\\pmb{M}^{(k)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $_M$ is the learned weight matrix in GCN. ", "page_idx": 26}, {"type": "text", "text": "GCN-based defenses are less robust than APPNP-based defenses GCN consists of layers in which both feature transformation and message passing are included. This graph convolution operation will weaken defense methods that rely on edge weighting, such as GNNGuard, models using $l_{1}$ penalty as well as our method using MCP penalty. ", "page_idx": 26}, {"type": "text", "text": "Consider an edge that is added by the attacker7. A successful defense should detect the attack on this edge by the large difference of nodes connected by this edge, and then assign a zero weight or at least a smaller weight to this edge. However in GCN, even if this edge is detected in the first layer\u2019s message passing, the subsequent feature transformation makes the node difference less likely to be preserved until the second layer. This is where the attack can evade the defense and is thus a vulnerability allowing adaptive attacks through. According to our experiments, using different defense parameters in different layers of GCN, unfortunately, does not help much either. ", "page_idx": 26}, {"type": "text", "text": "On the other hand, in APPNP node features are also altered along the message-passing layers, but the node distance change is more regulated than in GCN since MLP is decoupled from the graph smoothing layers. In the latter submodule, node differences simply decrease, which allows our defense based on node differences. ", "page_idx": 26}, {"type": "text", "text": "Experiments It can be seen from Figure 14 that the correlation of node feature differences in different layers of GCN is about 4 times less than in APPNP, which means that an attack detected in the first layer is less likely to continue to be detected in the second layer than in APPNP. This property of GCN makes the many defense methods using GCN architecture as a backbone less robust, as shown in the experiment results in Table 12 and Table 13. Nevertheless, our GCN-based MCP model outperforms the SOTA models using $l_{1}$ methods. ", "page_idx": 26}, {"type": "image", "img_path": "dz6ex9Ee0Q/tmp/d95fc3ddfde64e05de7c299bd6e7c9a0a51c2f919078689666cbc84cfb25b786.jpg", "img_caption": ["Figure 14: The distances $\\|\\pmb{f}_{i}\\ -\\pmb{f}_{j}\\|_{2}$ between connected nodes in different layers are shown. The linear transform operation in aggregation layers of GCN reduces the correlation between the node distance at different propagation layers. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/09399cd0653214f4d59fc48c4ab2f1d721074abfc4eb05d79f90ae7a5ef8c064.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "dz6ex9Ee0Q/tmp/b22ede71935512e1b377043f8ba099cff949ea9fe2a3fc8e040250ead468ce8a.jpg", "table_caption": ["Table 12: Comparison between APPNP-based and GCN-based QN-IRLS on Cora. "], "table_footnote": ["Table 13: Comparison between APPNP-based and GCN-based QN-IRLS on Citeseer. "], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our abstract and introduction (Section 1) closely follow the contribution (at the end of Section 1) in the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the limitations of the work in the Section 6. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the theory in Section 3.2 and the detailed proof in the Appendix B. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the detailed experimental setting in Section 4.1. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We will organize the data and code after the submission. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https:// nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide the detailed experimental setting in Section 4.1. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 30}, {"type": "text", "text": "\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: We include the theoretical computation analysis in Appendix C. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The paper closely follows NeurIPS Code of Ethics ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper poses no such risks. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety fliters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper does not use existing assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 32}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]