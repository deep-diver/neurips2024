[{"figure_path": "QUYLbzwtTV/figures/figures_2_1.jpg", "caption": "Figure 1: Teacher-Mixture in fairness and robustness. Panel (a) shows the generalisation errors\u2014for the subpopulations + (blue) and \u2013 (red)\u2014obtained through simulation (crosses) and predicted by the theory (solid lines) for a network with linear activation. The inset shows the same comparison for the order parameters: R+ (blue), R_ (red), M (green), and Q (orange). Panels (b-d) exemplify the different scenarios achievable in the TM model investigated in Sec. 4. Panel (b) represent a model for robustness where a spurious feature-given by the shift vector-can mislead the classifier, see Sec. 4.1. Panels (c,d) are instead discussed in Sec. 4.2 and represent two models of fairness. First, Panel (b) has no shift, v = 0, allowing us to remove the confounding effects. Finally, Panel (d) shows the general fairness problem.", "description": "This figure demonstrates the generalization error for different subpopulations in a teacher-mixture model. Panel (a) compares simulated and theoretical results using a linear activation network, showing a good fit. The inset shows a comparison for the order parameters. Panels (b) through (d) showcase various scenarios achievable using the teacher-mixture model, relating to robustness and fairness.", "section": "Insights"}, {"figure_path": "QUYLbzwtTV/figures/figures_4_1.jpg", "caption": "Figure 1: Teacher-Mixture in fairness and robustness. Panel (a) shows the generalisation errors\u2014for the subpopulations + (blue) and \u2013 (red)\u2014obtained through simulation (crosses) and predicted by the theory (solid lines) for a network with linear activation. The inset shows the same comparison for the order parameters: R+ (blue), R_ (red), M (green), and Q (orange). Panels (b-d) exemplify the different scenarios achievable in the TM model investigated in Sec. 4. Panel (b) represent a model for robustness where a spurious feature-given by the shift vector-can mislead the classifier, see Sec. 4.1. Panels (c,d) are instead discussed in Sec. 4.2 and represent two models of fairness. First, Panel (b) has no shift, v = 0, allowing us to remove the confounding effects. Finally, Panel (d) shows the general fairness problem.", "description": "This figure shows the generalization error of a linear classifier trained using SGD on a dataset generated from a teacher-mixture model. Panel (a) compares the theoretical predictions of generalization error with the results from simulations, showing a good match. The inset in panel (a) compares the order parameters from theory and simulations. Panels (b-d) demonstrate different scenarios (robustness and fairness) achievable with the teacher-mixture model.", "section": "Insights"}, {"figure_path": "QUYLbzwtTV/figures/figures_5_1.jpg", "caption": "Figure 3: The crossing phenomenon. Panel (a) (left side) shows the loss curves of sub-population \u2013 (in red) and sub-population + in blue along with the overall loss (in purple). We observe a crossing cause by a higher variance but lower representation in sub-population -. The background colours represent the different phases of bias that are characterised by the evolution of the order parameters shown in Panel (a) (right side). Panel (b) shows the presence of the crossing phenomenon in a large portion of the parameter space using a phase diagram. Blue indicates an asymptotic preference for sub-population + and red the opposite. Dark colours indicates regions where bias is consistent across training, while regions in light colours undergo a crossing phenomenon. White indicates that learning rate was too high and training diverged. Parameters: v = 0, \u0394+ = 1, T+ = 0.9, \u03b7 = 0.1.", "description": "This figure demonstrates the \"crossing phenomenon\" where the loss curves of two sub-populations intersect during training.  Panel (a) shows the loss curves for sub-population + (blue) and sub-population \u2013 (red), along with the total loss (purple). The crossing is caused by sub-population \u2013 having higher variance but lower representation. The right panel shows the order parameters over time illustrating the different bias phases. Panel (b) presents a phase diagram highlighting the parameter regions where the crossing phenomenon occurs.  The color scheme in the phase diagram indicates the asymptotic preference of the classifier (blue for sub-population +, red for sub-population \u2013). Dark colors show consistent bias, light colors show a bias crossing, and white represents divergence due to high learning rates.", "section": "4 Insights"}, {"figure_path": "QUYLbzwtTV/figures/figures_7_1.jpg", "caption": "Figure 7: The Crossing Phenomenon The left shows the 'crossing' of the loss curves on the negative sub-population in red (higher variance and lower representation) and positive sub-population in blue (lower variance but greater representation) along with the overall loss in purple obtained as a weighted average of the two. It also marks TR as the dashed vertical line and t\u1ff7 as the dotted vertical line. The right side shows the evolution of the order parameters and a transient term. The horizontal blue and red dash-dotted line mark the optimal value of Q for the positive-subpopulation and negative sub-populations respectively. The parameters are v = 0, p = 0.8, \u0394+ = 0.1, \u0394_ = 1, \u03a4+ = 0.9, \u03b7 = 0.1.", "description": "This figure shows the phenomenon of bias crossing in which the loss curves for different sub-populations intersect during training, highlighting the non-monotonic nature of bias in this setting. The left panel displays the loss curves for the positive and negative sub-populations and their average, illustrating the crossing behavior.  The right panel shows the evolution of multiple order parameters which help to explain this behavior, showing three distinct learning phases. Key parameters determining this behavior are also provided.", "section": "E.2 Analysis of teacher alignment (TR) and student magnitude (TQ) timescales"}, {"figure_path": "QUYLbzwtTV/figures/figures_8_1.jpg", "caption": "Figure 5: Numerical simulations on MNIST. The figure shows the average (solid lines) and standard deviation (shaded area) of 100 simulations run in this framework. In particular the upper plots show the test loss and lower plots the test accuracy for subpopulation + (blue) and (red). Panel (a) an example of crossing phenomenon obtained by imposing \u221aA+ = 1, \u221a\u2206_ = 0.2, and p = 0.1. Panel (b) shows the double crossing, obtained by introducing an additional timescale to the previous case by tuning label imbalance. Panel (c) explore the effect of changing A while keeping a constant A+ = 1.", "description": "This figure presents numerical simulation results on the MNIST dataset using a 2-layer neural network.  It demonstrates the evolution of test loss and accuracy for two subpopulations (+) and (-) over multiple training epochs. Three panels showcase different scenarios: a single crossing phenomenon, a double crossing phenomenon (introducing a label imbalance), and the effect of varying variance while keeping one variance constant. Each panel shows the average and standard deviation over 100 simulations, illustrating the robustness of the observed phenomena.", "section": "Ablations using numerical simulations"}, {"figure_path": "QUYLbzwtTV/figures/figures_8_2.jpg", "caption": "Figure 5: Numerical simulations on MNIST. The figure shows the average (solid lines) and standard deviation (shaded area) of 100 simulations run in this framework. In particular the upper plots show the test loss and lower plots the test accuracy for subpopulation + (blue) and (red). Panel (a) an example of crossing phenomenon obtained by imposing \u221aA+ = 1, \u221a\u2206_ = 0.2, and p = 0.1. Panel (b) shows the double crossing, obtained by introducing an additional timescale to the previous case by tuning label imbalance. Panel (c) explore the effect of changing A while keeping a constant A+ = 1.", "description": "This figure presents the results of numerical simulations conducted on the MNIST dataset to validate the theoretical findings. It showcases three scenarios: a single crossing phenomenon, a double crossing phenomenon, and an analysis of the impact of changing the variance of one subpopulation while keeping the variance of the other constant. Each panel displays the test loss and accuracy for two subpopulations over multiple epochs. The results demonstrate the presence of multiple time scales in the bias dynamics and how they lead to non-monotonic behavior of the classifier.", "section": "Ablations using numerical simulations"}, {"figure_path": "QUYLbzwtTV/figures/figures_9_1.jpg", "caption": "Figure 5: Numerical simulations on MNIST. The figure shows the average (solid lines) and standard deviation (shaded area) of 100 simulations run in this framework. In particular the upper plots show the test loss and lower plots the test accuracy for subpopulation + (blue) and (red). Panel (a) an example of crossing phenomenon obtained by imposing \u221aA+ = 1, \u221a\u2206_ = 0.2, and p = 0.1. Panel (b) shows the double crossing, obtained by introducing an additional timescale to the previous case by tuning label imbalance. Panel (c) explore the effect of changing A while keeping a constant A+ = 1.", "description": "This figure shows the results of numerical simulations on the MNIST dataset using a variation of the Teacher-Mixture model.  The top row shows test loss, and the bottom row shows test accuracy, for two sub-populations (+ in blue, - in red). Panel (a) demonstrates a single crossing phenomenon where the loss curves for the two subpopulations intersect during training.  Panel (b) shows a double crossing, indicating a more complex bias evolution with multiple timescale dynamics. Panel (c) explores how the initial bias in learning depends on the variances of the subpopulations.", "section": "Ablations using numerical simulations"}, {"figure_path": "QUYLbzwtTV/figures/figures_25_1.jpg", "caption": "Figure 7: The Crossing Phenomenon The left shows the 'crossing' of the loss curves on the negative sub-population in red (higher variance and lower representation) and positive sub-population in blue (lower variance but greater representation) along with the overall loss in purple obtained as a weighted average of the two. It also marks TR as the dashed vertical line and t\u1ff7 as the dotted vertical line. The right side shows the evolution of the order parameters and a transient term. The horizontal blue and red dash-dotted line mark the optimal value of Q for the positive-subpopulation and negative sub-populations respectively. The parameters are v = 0, p = 0.8, \u0394+ = 0.1, \u0394_ = 1, \u03a4+ = 0.9, \u03b7 = 0.1.", "description": "The left panel of the figure shows the loss curves for the negative (red) and positive (blue) sub-populations, as well as the overall loss (purple). The crossing of the loss curves is highlighted, with the dashed and dotted vertical lines indicating important timescales. The right panel displays the dynamics of order parameters over time, illustrating the three phases of bias.", "section": "E.2 Analysis of teacher alignment (TR) and student magnitude (TQ) timescales"}, {"figure_path": "QUYLbzwtTV/figures/figures_26_1.jpg", "caption": "Figure 3: The crossing phenomenon. Panel (a) (left side) shows the loss curves of sub-population \u2013 (in red) and sub-population + in blue along with the overall loss (in purple). We observe a crossing cause by a higher variance but lower representation in sub-population -. The background colours represent the different phases of bias that are characterised by the evolution of the order parameters shown in Panel (a) (right side). Panel (b) shows the presence of the crossing phenomenon in a large portion of the parameter space using a phase diagram. Blue indicates an asymptotic preference for sub-population + and red the opposite. Dark colours indicates regions where bias is consistent across training, while regions in light colours undergo a crossing phenomenon. White indicates that learning rate was too high and training diverged. Parameters: v = 0, \u0394+ = 1, T+ = 0.9, \u03b7 = 0.1.", "description": "This figure shows the loss curves and phase diagrams demonstrating the \"crossing phenomenon.\"  The left panel shows the loss curves of two sub-populations, highlighting how the sub-population with higher variance initially has faster learning but asymptotically the sub-population with higher representation (product of representation and variance) takes over. The right panel shows a phase diagram illustrating the prevalence of the crossing phenomenon across different parameter combinations.", "section": "Insights"}, {"figure_path": "QUYLbzwtTV/figures/figures_26_2.jpg", "caption": "Figure 3: The crossing phenomenon. Panel (a) (left side) shows the loss curves of sub-population \u2013 (in red) and sub-population + in blue along with the overall loss (in purple). We observe a crossing cause by a higher variance but lower representation in sub-population \u2013. The background colours represent the different phases of bias that are characterised by the evolution of the order parameters shown in Panel (a) (right side). Panel (b) shows the presence of the crossing phenomenon in a large portion of the parameter space using a phase diagram. Blue indicates an asymptotic preference for sub-population + and red the opposite. Dark colours indicates regions where bias is consistent across training, while regions in light colours undergo a crossing phenomenon. White indicates that learning rate was too high and training diverged. Parameters: v = 0, \u0394+ = 1, T+ = 0.9, \u03b7 = 0.1.", "description": "This figure shows the loss curves for two sub-populations with different variances and mixing probabilities, along with the overall loss.  Panel (a) demonstrates a \"crossing\" phenomenon where initially the higher variance sub-population has lower loss, but asymptotically the sub-population with higher representation (and product of variance and representation) dominates. Panel (b) displays a phase diagram illustrating the regions in parameter space exhibiting this crossing behavior.", "section": "Insights"}, {"figure_path": "QUYLbzwtTV/figures/figures_28_1.jpg", "caption": "Figure 1: Teacher-Mixture in fairness and robustness. Panel (a) shows the generalisation errors\u2014for the subpopulations + (blue) and \u2013 (red)\u2014obtained through simulation (crosses) and predicted by the theory (solid lines) for a network with linear activation. The inset shows the same comparison for the order parameters: R+ (blue), R_ (red), M (green), and Q (orange). Panels (b-d) exemplify the different scenarios achievable in the TM model investigated in Sec. 4. Panel (b) represent a model for robustness where a spurious feature-given by the shift vector-can mislead the classifier, see Sec. 4.1. Panels (c,d) are instead discussed in Sec. 4.2 and represent two models of fairness. First, Panel (b) has no shift, v = 0, allowing us to remove the confounding effects. Finally, Panel (d) shows the general fairness problem.", "description": "This figure demonstrates the generalization errors for two sub-populations in a teacher-student model using a linear activation function. Panel (a) compares the theoretical predictions with simulation results, highlighting the accuracy of the theoretical model. The inset shows the same comparison for order parameters R+, R-, M, and Q. Panels (b) through (d) illustrate different scenarios in the Teacher-Mixture (TM) model. Panel (b) showcases a robustness model where a spurious feature leads to misclassification; Panels (c) and (d) present two fairness models, with Panel (c) being a simplified case and Panel (d) showing the general fairness problem. The figure illustrates the TM model's ability to represent different fairness and robustness scenarios.", "section": "Insights"}, {"figure_path": "QUYLbzwtTV/figures/figures_28_2.jpg", "caption": "Figure 5: Numerical simulations on MNIST. The figure shows the average (solid lines) and standard deviation (shaded area) of 100 simulations run in this framework. In particular the upper plots show the test loss and lower plots the test accuracy for subpopulation + (blue) and \u2013 (red). Panel (a) an example of crossing phenomenon obtained by imposing \u221aA+ = 1, \u221a\u2206_ = 0.2, and p = 0.1. Panel (b) shows the double crossing, obtained by introducing an additional timescale to the previous case by tuning label imbalance. Panel (c) explore the effect of changing A while keeping a constant A+ = 1.", "description": "This figure shows the results of numerical simulations on the MNIST dataset using a rotated version of the dataset to mimic the teacher-mixture model presented in the paper.  It demonstrates three key phenomena related to bias evolution during training: (a) a single crossing of the loss curves for two subpopulations, (b) a double crossing due to label imbalance, and (c) the impact of changing the variance of one subpopulation while keeping the other constant. The results validate the theoretical analysis by showing the predicted multi-phase behavior in a more realistic setting.", "section": "Ablations using numerical simulations"}, {"figure_path": "QUYLbzwtTV/figures/figures_29_1.jpg", "caption": "Figure 1: Teacher-Mixture in fairness and robustness. Panel (a) shows the generalisation errors\u2014for the subpopulations + (blue) and \u2013 (red)\u2014obtained through simulation (crosses) and predicted by the theory (solid lines) for a network with linear activation. The inset shows the same comparison for the order parameters: R+ (blue), R_ (red), M (green), and Q (orange). Panels (b-d) exemplify the different scenarios achievable in the TM model investigated in Sec. 4. Panel (b) represent a model for robustness where a spurious feature-given by the shift vector-can mislead the classifier, see Sec. 4.1. Panels (c,d) are instead discussed in Sec. 4.2 and represent two models of fairness. First, Panel (b) has no shift, v = 0, allowing us to remove the confounding effects. Finally, Panel (d) shows the general fairness problem.", "description": "This figure shows the generalization errors for different sub-populations and how the theoretical predictions match the simulation results for a linear activation network. The inset shows a comparison of order parameters. It also exemplifies different fairness and robustness models using the teacher-mixture framework, showing how spurious features and heterogeneous data can affect the classifier's behavior during learning. ", "section": "Insights"}, {"figure_path": "QUYLbzwtTV/figures/figures_29_2.jpg", "caption": "Figure 1: Teacher-Mixture in fairness and robustness. Panel (a) shows the generalisation errors\u2014for the subpopulations + (blue) and \u2013 (red)\u2014obtained through simulation (crosses) and predicted by the theory (solid lines) for a network with linear activation. The inset shows the same comparison for the order parameters: R+ (blue), R_ (red), M (green), and Q (orange). Panels (b-d) exemplify the different scenarios achievable in the TM model investigated in Sec. 4. Panel (b) represent a model for robustness where a spurious feature-given by the shift vector-can mislead the classifier, see Sec. 4.1. Panels (c,d) are instead discussed in Sec. 4.2 and represent two models of fairness. First, Panel (b) has no shift, v = 0, allowing us to remove the confounding effects. Finally, Panel (d) shows the general fairness problem.", "description": "This figure shows the generalization errors for two subpopulations in a teacher-student learning model. Panel (a) compares the theoretical predictions with simulation results for a linear classifier. Panels (b) through (d) illustrate different scenarios for robustness and fairness, demonstrating how the model can represent various bias situations.", "section": "Insights"}]