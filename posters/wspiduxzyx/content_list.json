[{"type": "text", "text": "Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Md Ashiqur Rahman1, Robert Joseph George2, Mogab Elleithy2, Daniel Leibovici2, Zongyi $\\mathbf{Li^{2}}$ , Boris Bonev3, Colin White2, Julius Berner2, Raymond A. Yeh1, Jean Kossaifi3, Kamyar Azizzadenesheli3, Anima Anandkumar2 1Purdue University, 2Caltech, 3NVIDIA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs) due to complex geometries, interactions between physical variables, and the limited amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to function spaces. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO\u2019s potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations, fluid-structure interactions, and Rayleigh-B\u00e9nard convection, we found CoDA-NO to outperform existing methods by over $36\\%$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many science and engineering challenges involve solving partial differential equations (PDEs). A PDE can represent physical phenomena such as fluid dynamics, wave propagation, material deformation, etc., but to describe many real-world systems, multiple such PDEs must be coupled together, viz., multi-physics modeling [1]. For instance, in subsurface engineering, equations of flow, thermodynamics, and microchemistry are coupled together [2]; in materials science, physics at multiple scales are involved in modeling [3], and in weather forecasting, atmospheric processes involve interactions of wave propagation and fluid dynamics [4]. ", "page_idx": 0}, {"type": "text", "text": "Traditionally, numerical methods have been devised to solve PDEs. However, they typically require discretization of PDEs on fine grids to capture the physical phenomena accurately. Consequently, these computational requirements often exceed available memory and computational budgets for realworld applications. Beyond these obstacles present in individual PDE problems, the convergence of numerical solvers in multiphysics systems presents major difficulties arising from intricate interactions among multiple coupled PDEs. ", "page_idx": 0}, {"type": "text", "text": "Deep learning techniques have emerged as faster alternatives to numerical solvers for PDEs in many applications. They are typically trained using supervised learning with data obtained from solvers. This becomes a challenge when only limited data is available, especially in the case of multiphysics simulations, which are expensive and challenging for numerical solvers. Instead, obtaining data from simpler simulations where only a subset of the \u201cphysics\" is incorporated is more convenient and less expensive. In other words, instead of getting data from coupled PDE systems, we can obtain data by solving individual PDEs. While the solutions of the two systems can be very different, they share common features and can benefti from a combined learning framework. Can we design a systematic curriculum learning scheme for learning multiphysics systems? ", "page_idx": 0}, {"type": "image", "img_path": "wSpIdUXZYX/tmp/1e86c50a6db2ac99fd8b403f34d3fb381a7ef65d9e71e54f3a967c01fa594ada.jpg", "img_caption": ["Figure 1. CoDA-NO adapts seamlessly to new multi-physics systems. Pre-trained on fluid dynamics data (Navier-Stokes equation with $u_{x}$ , $u_{y}$ , and $p$ ) using the masked-reconstruction objective, CoDA-NO easily adapts to multi-physics fluid-solid interaction systems (new $d_{x}$ and $d_{y}$ variables) without any architectural changes. ", ""], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "More generally, a foundation model trained on different kinds of PDEs can learn representations across multiple domains and then transfer them to new problems. Such foundation models have found immense success in computer vision and natural language processing [5, 6]. Foundation models are first trained in a self-supervised manner on large and often unlabeled datasets. Then they can be efficiently adapted or fine-tuned to a broad range of downstream tasks with minimal to no additional data or training. ", "page_idx": 1}, {"type": "text", "text": "Recent works have attempted to train a foundation model for solving PDEs [7\u20139]. However, these methods only work on predetermined PDEs with a fixed number of variables, and none of them consider multi-physics PDEs, and they are mostly restricted to uniform grids, limiting their applicability. For example, standard patching-based approaches used in Vision Transformers (ViTs) [10] often struggle with discontinuities in predicted functions and changing resolutions [11]. Since they are limited to fixed uniform grids, they cannot generalize to resolutions different from the training resolutions. ", "page_idx": 1}, {"type": "text", "text": "To handle varying resolutions and grids, neural operators [12, 13] have been introduced as a deep learning framework for learning mappings between function spaces. Neural operators are guaranteed to converge to a unique operator in the limit of increasingly fine discretizations (of the computational domain). This property is known as discretization convergence, making them agnostic to the discretization of the input and output functions and suitable for approximating solution operators of PDEs. Neural operators can replace numerical solvers while being significantly faster in several scenarios [14, 15]. While some of the previous PDE foundation models [7, 9] use neural operators, they still cannot handle multiphysics or coupled PDEs. They also cannot adapt to new variables that are not predetermined at the beginning of training. ", "page_idx": 1}, {"type": "text", "text": "Our Approach: We propose a novel transformer neural operator architecture with codomain attention (CoDA-NO) layers designed to handle varying combinations of physical phenomena modeled through coupled PDEs. We partition the input function codomain-wise into a set of token functions, each corresponding to distinct physical variables of the PDE. The CoDA-NO model processes this set of functions as input, extending the transformer architecture from a finite-dimensional vector space to an infinite-dimensional function space. This extension is achieved by carefully redesigning positional encodings, the self-attention mechanism, and normalization techniques. ", "page_idx": 1}, {"type": "text", "text": "In our architecture, each token is treated as a function, capturing cross-function dynamics through attention mechanisms while maintaining discretization convergence. This design empowers the architecture to handle functions discretized on grids of varying resolutions. Specifically, each token function is subjected to the following operations: (i) concatenation with a learned positional embedding, (ii) lifting to a higher-dimensional co-domain, and (iii) functional attention mechanisms to compute interactions. We use Fourier neural operators (FNOs) [16] rather than traditional multi-layer perceptrons (MLPs) to create the representations for keys, values, and queries, which helps maintain the functional nature of the input data. Details can be found in Sec. 3 and Alg. 1. ", "page_idx": 1}, {"type": "text", "text": "CoDA-NO can be applied to varying numbers of input functions (on different geometries) and adapt to novel PDEs with fewer or additional interacting variables, as illustrated in Fig. 1. This allows us to learn multiple PDE systems in one model. ", "page_idx": 1}, {"type": "text", "text": "To demonstrate CoDA-NO\u2019s generalizability across diverse physical systems, we examine two settings: multiphysics problems and a collection of single-physics problems. ", "page_idx": 1}, {"type": "image", "img_path": "wSpIdUXZYX/tmp/c7b896bceddc7c9599e1dc589928524b548519e904da5f111320da05b4174f00.jpg", "img_caption": ["(a) Illustration of CoDA-NO architecture. Each of the input physical variables, \u2018variable $1^{\\circ}$ and \u2018variable $2^{\\circ}$ , depicted with two different colors (yellow and blue), incorporate a learnable variable-specific positional encoder (VSPE). These variables, along with the corresponding VSPE, are passed through GNO layers to transform from non-uniform to latent uniform grids. Codomain attention tokenizes the latent functions along the codomain. Each token undergoes transformations with $\\kappa$ , $\\mathcal{Q}$ , and $\\nu$ operators yielding key, query, and value functions $\\{k^{1},k^{2}\\}$ , $\\{q^{\\daleth},q^{2}\\}$ , and $\\{v^{1},v^{2}\\}$ . The resulting function is computed using a self-attention mechanism in function space followed by an integral operator $\\mathcal{T}_{p e r}$ . Finally, the output function on the target geometry is generated by passing through stacked CoDA-NO blocks, followed by an additional GNO layer. "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "wSpIdUXZYX/tmp/4a2a1415295d0de9a9bed78b42864496bd05dd45423cea1964cabe6e1b382f71.jpg", "img_caption": ["(b) Self-supervised pre-training and fine-tuning with CoDA-NO. Model, pre-trained on the Navier-Stokes equations dataset (with $u_{x}$ , $u_{y}$ , and $p$ ) in a self-supervised way, can be fine-tuned to a fluid-solid interaction dataset (new $d_{x}$ and $d_{y}$ variables) by only including two extra VSPEs and a predictor module. ", "Figure 2. (a) CoDA-NO architecture. (b) Self-supervised pre-training and fine-tuning process with CoDA-NO. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "For the multiphysics scenario, we examine two distinct systems. First, we consider a fluid-structure interaction problem [17] governed by the incompressible Navier-Stokes equation and the Elastic wave equation. The fluid-structure interaction problem is representative of the multi-physics behavior of various real-world problems, e.g., climate and atmosphere modeling. It also provides an additional challenge of irregular meshes on a complex geometry. ", "page_idx": 2}, {"type": "text", "text": "Instead of directly learning to solve the full multiphysics problem, we start with a curriculum where we first learn the basic fluid dynamics without the elastic wave equation, governed by the incompressible Navier-Stokes equation, with velocity and pressure as variables. We pre-train CoDANO in a self-supervised manner on snapshots of fluid flows by masking different parts of the velocity or pressure fields. Using few-shot supervised fine-tuning, we show that our model can adapt to unseen viscosities and additional displacement fields given by the elastic wave equation. We use graph neural operator (GNO) layers [18] as encoders and decoders to handle time-varying irregular meshes of the fluid-structure interaction problems. For the few-shot learning problem, our model achieves $36.8\\%$ lower errors on average compared to the best-performing baseline trained from scratch on the target problem. ", "page_idx": 2}, {"type": "text", "text": "The second system involves Rayleigh-B\u00e9nard convection, where the Navier-Stokes and heat (energy) equations are coupled in a regular $2D$ domain. Similar to the first case, we pre-train CoDA-NO with an incompressible Navier-Stocks equation involving just the velocity term. Then, we fine-tuned the model to predict velocity and temperature using few-shot training samples. Here, too, the pre-trained CoDA-NO significantly outperforms the baseline, reducing the prediction error by a factor of two. ", "page_idx": 2}, {"type": "text", "text": "We also train CoDA-NO on a diverse set of PDEs, which form a subset of PDEBench [19] and demonstrate superior performance and parameter efficiency over prior approaches in learning all of those PDE systems. CoDA-NO consistently outperforms the FNO architecture trained on the same set of PDEs, reducing test error by up to $43\\%$ while only requiring $2\\%$ of the parameters. ", "page_idx": 2}, {"type": "text", "text": "Our contributions are as follows: ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "\u2022 We propose a co-domain attention neural operator that efficiently learns solution operators to PDEs by formulating transformer operations in function space and ensuring discretization convergence. \u2022 The proposed architecture enables self-supervised learning in function space for diverse physical systems by handling varying numbers of input functions and geometries. \u2022 CoDA-NO achieves state-of-the-art performance in generalizing to unknown physical systems with very limited data. That is, CoDA-NO can be viewed as the first foundation neural operator for multiphysics problems. ", "page_idx": 3}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Transformers for solving PDEs. Recent work [20] proposes a method to weight variables/codomains of the input function based on the weights calculated from the PDE parameters. Another study [21] proposes a scalable transformer architecture by combining a projection operator to a one-dimensional domain and a learnable factorized kernel. In contrast to these works, CoDA-NO provides a complete attention operator by considering each physical variable as a token function, i.e., an infinite-dimensional vector, extending traditional transformers that only operate on finite-dimensional tokens. ", "page_idx": 3}, {"type": "text", "text": "Self-supervised learning. Self-supervised learning (SSL) has been proposed to tackle the issue of limited labeled data [22\u201324]. It allows the training of large foundation models on massive amounts of unlabeled data in the field of computer vision and natural language processing. Subsequently, these models can be successfully applied to a wide range of downstream tasks with minimal to no additional task-specific data [6, 25\u201327]. ", "page_idx": 3}, {"type": "text", "text": "Pre-training for PDE solving. Models that are pre-trained in a self-supervised fashion have also gained traction in the domain of scientific computing. One recent study [8] proposes pretraining the models with autoregressive tasks on a diverse dataset of multiple PDEs. These models can then be fine-tuned for specific downstream PDEs. Several recent studies have investigated task-agnostic approaches through masking-and-reconstruction [22] and the consistency of representations under symmetry transformations [16, 28, 29]. Recent work [7] also sheds light on the transferability of these models between different systems of PDEs. While these methods achieve good performance, the target (downstream) PDE must maintain a strict resemblance to the ones used for pretraining. In addition, adapting these models for PDEs with new additional physical variables is not possible. Additionally, ViT-based patching approaches [11] disrupt the continuity and are not resolution-agnostic. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let us first define our setting and provide a brief introduction to neural operators. For further details, we refer to Sec. A in the appendix. ", "page_idx": 3}, {"type": "text", "text": "For an input function $a\\colon\\mathcal{D}\\rightarrow\\mathbb{R}^{d_{i n}}$ , we will denote the $d_{i n}$ -dimensional output space $\\mathbb{R}^{d_{i n}}$ as the codomain. We consider the components of the codomain as different physical variables, given by real-valued functions over the input domain $\\mathcal{D}$ , i.e., $a\\,=\\,[a^{1},\\cdot\\cdot\\cdot,a^{\\dot{d_{i n}}}]$ with $a^{i}:\\mathcal{D}\\stackrel{}{\\rightarrow}\\mathbb{R}$ . The same applies to the output function $u\\colon D\\rightarrow\\mathbb{R}^{d_{o u t}}$ . We define the action of a pointwise operator $\\mathcal{H}:\\{f\\overset{.}{:}\\mathcal{D}\\rightarrow\\mathbb{R}^{d_{f}}\\}\\rightarrow\\mathbf{\\dot{\\{g}}}:\\mathcal{D}\\rightarrow\\mathbb{R}^{d_{g}}\\}$ given by a function $h_{\\theta}:\\mathbb{R}^{d_{f}}\\rightarrow\\mathbb{R}^{d_{g}}$ with parameters $\\theta$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{H}[f](x)=h_{\\theta}\\big(f(x)\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, we define an integral operator $\\mathcal{T}:\\{f:\\mathcal{D}\\rightarrow\\mathbb{R}^{d_{f}}\\}\\rightarrow\\{g:\\mathcal{D}\\rightarrow\\mathbb{R}^{d_{g}}\\}$ given by a kernel function $k_{\\phi}$ with parameters $\\phi$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nT[f](x)=\\int_{\\mathcal{D}}k_{\\phi}(x,y)f(y)\\,\\mathrm{d}y.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Problem Statement. Our objective is to construct a general neural operator architecture that explicitly represents the interaction between the physical variables of PDE systems. Such an architecture should be able to learn and predict various systems without being constrained to a fixed number of variables. ", "page_idx": 3}, {"type": "text", "text": "Let\u2019s consider two input functions $a\\colon\\mathcal{D}\\rightarrow\\mathbb{R}^{d_{i n}}$ and $\\tilde{a}\\colon\\mathcal{D}\\rightarrow\\mathbb{R}^{\\tilde{d}_{i n}}$ of two different PDE with corresponding output functions $u\\colon D\\rightarrow\\mathbb{R}^{d_{o u t}}$ and $\\tilde{u}\\colon\\mathcal{D}\\rightarrow\\mathbb{R}^{\\tilde{d}_{o u t}}$ . In general, the functions $a$ and $\\tilde{a}$ represent $d_{i n}$ and $\\tilde{d}_{i n}$ physical variables over the domain $\\mathcal{D}$ with $d_{i n}\\neq\\tilde{d}_{i n}$ . We aim to design neural operator architectures $\\mathcal{G}$ that can both be applied to $a$ as well as $\\tilde{a}$ despite the different codomains of the input as well as output functions. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Such property provides the possibility to evaluate or finetune the operator on PDEs with different numbers of variables than those on which it was trained. In particular, when the PDE systems have overlapping physical variables $\\{a^{i}\\}_{i=1}^{d_{i n}}\\cap\\{\\tilde{a}^{i}\\}_{i=1}^{\\tilde{d}_{i n}}\\neq\\emptyset$ , this naturally allows to transfer learned knowledge from one system to the other. We will next describe the details of the CoDA-NO layers and architecture to achieve this goal. ", "page_idx": 4}, {"type": "text", "text": "Neural Operator on Sets. As we consider the vector-valued input function $a$ as a set of $d_{i n}$ functions $\\{a^{1},a^{2},\\bar{\\dots},a^{d_{i n}}\\}$ that represents different physical variables of the PDE, we seek to construct operators that act on sets of input functions with different cardinalities. ", "page_idx": 4}, {"type": "text", "text": "For an efficient implementation of operators on sets of functions, we mimic transformer architectures and share weights across different variables. Specifically, we can define the integral operator $\\mathcal{T}_{p e r}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{p e r}[a]=\\bigg[\\mathcal{Z}[a^{1}],\\dots,\\mathcal{Z}[a^{d_{i n}}]\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{a}=[a^{1},\\dots,a^{d_{i n}}]$ and $\\mathcal{T}$ is a regular integral operator as described in Eq. (2). Such construction makes the operator permutation-equivariant with respect to the order of the variables in the set. Following the same mechanism, we can also define permutation-equivariant pointwise operator $\\mathcal{H}_{p e r}$ with a shared pointwise operator $\\mathcal{H}$ (see Eq. (1)). We will use $\\mathrm{FNO}_{p e r}$ and ${\\mathrm{GNO}}_{p e r}$ to denote permutation-equivariant operators using a shared GNO and FNO, respectively. ", "page_idx": 4}, {"type": "text", "text": "CoDA-NO Layer. To explain the CoDA-NO layer, let us assume the input function $a$ has been processed into a latent function $w:\\,\\mathcal{D}\\,\\rightarrow\\,\\mathbb{R}^{d}$ . We partition the function into a set of so-called token functions $w^{j}\\;:\\;\\mathcal{D}\\;\\rightarrow\\;\\mathbb{R}^{d^{\\prime}}$ with $w^{j}~\\in~\\mathcal{W}$ for $j~\\in~\\{1,\\ldots T\\}$ along the codomain, such that $w=\\left[w^{1},\\cdot\\cdot\\cdot w^{T}\\right]$ (and where each $w^{j}$ is associated with precisely one of the physical input variables). That is, $w$ represents the codomain-wise concatenation of the token functions $w^{j}$ and $\\begin{array}{r}{d^{\\prime}=\\frac{d}{T}}\\end{array}$ . If no other value is specified, we assume that $d^{\\prime}=1$ . The CoDA-NO layer now processes the token functions using an extension of the self-attention mechanism to the function space (see Appendix Sec. B and Fig. 2a). ", "page_idx": 4}, {"type": "text", "text": "Let us begin by introducing a single-head CoDA-NO layer. Later, we will expand the concept to multi-head codomain attention. We extend the key, query, and value matrices of the standard attention (see Appendix Sec. B for details) to operators mapping token functions $w^{j}\\colon D\\rightarrow\\mathbb{R}^{d^{\\prime}}$ to key, query, and value functions. We define the key, query, and value operators as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K:\\mathcal{W}\\to\\{k^{j}:\\mathcal{D}\\to\\mathbb{R}^{d_{k}}\\},\\ \\ \\mathcal{Q}:\\mathcal{W}\\to\\{q^{j}:\\mathcal{D}\\to\\mathbb{R}^{d_{q}}\\},\\ \\ \\mathcal{V}:\\mathcal{W}\\to\\{v^{j}:\\mathcal{D}\\to\\mathbb{R}^{d_{v}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assuming $d_{k}=d_{q}$ , we denote by $k^{j}=K[w^{j}]$ , $q^{j}\\,=\\,\\mathcal{Q}[w^{j}]$ , and $\\boldsymbol{v}^{j}=\\boldsymbol{\\mathcal{V}}[\\boldsymbol{w}^{j}]$ the key, query, and value functions of the token functions, respectively. ", "page_idx": 4}, {"type": "text", "text": "Next, we calculate the output (token) functions $o^{j}:\\mathcal{D}\\rightarrow\\mathbb{R}^{d_{v}}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{o}^{j}=\\mathsf{S o f t m a x}\\left(\\left[\\begin{array}{c}{\\frac{\\langle q^{j},k^{1}\\rangle}{\\tau}}\\\\ {\\vdots}\\\\ {\\frac{\\langle q^{j},k^{T}\\rangle}{\\tau}}\\end{array}\\right]\\right)[\\boldsymbol{v}^{1},\\boldsymbol{\\cdot},\\boldsymbol{\\cdot},\\boldsymbol{v}^{T}]^{\\top},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tau$ is the temperature hyperparameter. Here, $\\langle.,.\\rangle$ denotes a suitable dot product in the function space. We take the $L^{2}(\\mathcal{D},\\mathbb{R}^{d_{k}})$ -dot product given by $\\langle q^{j},k^{m}\\rangle=\\textstyle\\int_{\\mathcal{D}}\\langle q^{j}(x),k^{m}(x)\\rangle\\,\\mathrm{d}x$ , where the integral can be discretized using quadrature rules, similar to the integral operator in Eq. (2). ", "page_idx": 4}, {"type": "text", "text": "To implement multi-head attention, we apply the (single-head) attention mechanism described above separately for multiple heads $h\\in\\{\\bar{1},\\cdot\\cdot\\cdot H\\}$ using $\\kappa^{h},\\mathcal{Q}^{h}$ , and $\\nu^{h}$ to obtain $o^{j,h}$ . We then concatenate these outputs $o^{j,h}$ along the codomain and get $c^{j}:=[o^{j,1},\\cdot\\cdot\\cdot o^{j,H}]$ . Finally, we use an operator ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{M}:\\{c^{j}:\\mathcal{D}\\rightarrow\\mathbb{R}^{H\\cdot d_{v}}\\}\\rightarrow\\{o^{j}:\\mathcal{D}\\rightarrow\\mathbb{R}^{d_{v}}\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "to get the output function $o^{j}$ . ", "page_idx": 4}, {"type": "text", "text": "We obtain the output of the attention mechanism by concatenating $o^{j}\\mathrm{s}$ as $o=[o^{1},o^{2},\\dots o^{T}]$ . Finally, we complete the CoDA-NO layer by applying a permutation-equivariant integral operator $\\mathcal{T}_{p e r}$ on $o$ . When CoDA-NO is acting on functions sampled on a uniform grid, the internal operators $\\mathcal{K}^{h},\\mathcal{Q}^{h},\\mathcal{V}^{h},\\mathcal{M}$ , and $\\mathcal{T}$ are implemented as FNOs. ", "page_idx": 5}, {"type": "text", "text": "Function Space Normalization. Normalization is a vital aspect of deep learning architectures. However, when it comes to neural operators mapping infinite-dimensional functions, this topic remains largely unexplored. We now provide a natural extension. Given a function $w$ , let $w^{j}:\\mathcal{D}\\rightarrow\\mathbb{R}^{d^{\\prime}}$ be a token. Then we calculate the mean $\\mu\\in\\mathbb{R}^{d^{\\prime}}$ and standard deviation $\\sigma\\in\\mathbb{R}^{d^{\\prime}}$ for this token as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu^{j}=\\int_{\\mathcal{D}}w^{j}(x)\\,\\mathrm{d}x,\\ \\ \\sigma^{j}=\\bigg(\\int_{\\mathcal{D}}(w^{j}(x)-\\mu^{j})^{\\circ2}\\,\\mathrm{d}x\\bigg)^{\\circ\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\circ r$ denotes the elementwise (Hadamard) $r^{t h}$ -power. The normalization operator can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathrm{Norm}}[w^{j}](x)=\\big({\\mathbf{g}}\\oslash\\sigma^{j}\\big)\\odot\\big(w^{j}(x)-\\mu^{j}\\big)+{\\mathbf{b}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $\\mathbf{b}\\in\\mathbb{R}^{d^{\\prime}}$ and $\\mathbf{g}\\in\\mathbb{R}^{d^{\\prime}}$ are learnable bias and gain vectors and $\\oslash$ and $\\odot$ denote elementwise division and multiplication operation. This normalization can be seen as an extension of instance normalization [30] for function spaces. Similarly, normalization variants, such as group norm, layer norm, and batch norm, extend to operator learning with these definitions of statistics [31\u201333]. ", "page_idx": 5}, {"type": "text", "text": "Variable Specific Positional Encoding (VSPE). We learn positional encoders $e^{i}$ : $\\mathcal{D}\\,\\rightarrow\\,\\mathbb{R}^{d_{e n}}$ for each physical variable $i\\ \\in$ $\\left\\{1,\\ldots,d_{i n}\\right\\}$ , for the given vector-valued input function $\\textit{a}=\\ [a^{1},\\cdot\\cdot\\cdot,a^{d_{i n}}]$ . We concatenate each positional encoding $e^{i}$ with the respective variable $a^{i}:\\mathcal{D}\\dot{\\rightarrow}\\;\\mathbb{R}$ along the codomain to obtain extended input functions $\\bar{a}^{i}\\,=\\,[a^{i},e^{i}]$ . Next, we apply a shared pointwise lifting operator $\\mathcal{P}:\\bar{\\{a^{i}\\,:\\,}}\\mathcal{D}\\,\\rightarrow$ $\\mathbb{R}^{d_{e n}+1}\\}\\to\\{\\bar{w}^{i}:{\\mathcal{D}}\\to\\mathbb{R}^{D}\\}$ , typically with $D\\;>\\;d_{e n}\\,+\\,1$ . Finally, we concatenate $\\bar{w}^{i}$ , $i\\in\\{1,\\ldots d_{i n}\\}$ , to get the lifted latent function ", "page_idx": 5}, {"type": "equation", "text": "$$\nw=[\\bar{w}^{1},\\dots,\\bar{w}^{d_{i n}}]\\colon{\\cal D}\\to\\mathbb{R}^{{\\cal D}\\cdot d_{i n}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the previous paragraphs, we used $d\\,=$ $D\\,\\cdot\\,d_{i n}$ and, to maintain the permutationequivariance property of the operator, $d^{\\prime}$ must divide $D$ . ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 presents the pseudocode for the CoDA-NO architecture applied to input functions $[a^{1},a^{2}]$ , mapping two different physical variables on a uniform grid in a 1D domain, to the solution functions $[u^{1},u^{2}]$ . It assumes $d^{\\prime}=D$ while designing the CoDA-NO layer. Notably, to incorporate another function $\\dot{a}^{3}$ , representing a new physical variable, it is only necessary to introduce a corresponding parameter for the new VSPE, denoted as $\\kappa^{3}$ . ", "page_idx": 5}, {"type": "text", "text": "To effectively handle non-uniform complex geometries, we follow the GINO architecture ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Adaptation of CoDA-NO from two physical variables $a^{1},\\stackrel{\\bullet}{a}$ to a new variable $a^{3}$ on uniform $1D$ grid. Only the parts in \u2018Blue\u2019 are additionally required to incorporate the new variable a3. ", "page_idx": 5}, {"type": "text", "text": "Require: a1, a2, a3 \u2208R1\u00d7n ", "page_idx": 5}, {"type": "text", "text": "Variable Specific Positional Encoding: ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "// Learnable Fourier coefficients $\\begin{array}{r l}&{\\kappa^{1},\\kappa^{2},\\kappa^{3}\\in\\mathbb{C}^{d_{e n}\\times\\frac{n}{2}}}\\\\ &{\\bar{a}^{1}\\gets\\mathsf{c o n c a t e n a t e}(a^{1},\\mathsf{i F F T}(\\kappa^{1}))}\\\\ &{\\bar{a}^{2}\\gets\\mathsf{c o n c a t e n a t e}(a^{2},\\mathsf{i F F T}(\\kappa^{2}))}\\\\ &{\\bar{a}^{3}\\gets\\mathsf{c o n c a t e n a t e}(a^{3},\\mathsf{i F F T}(\\kappa^{3}))}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Lifting: ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "// Lifting from $\\mathbb{R}^{d_{e n}+1\\times n}$ to $\\mathbb{R}^{D\\times n}$ with $D>d_{e n}+1$ w1 \u2190pointwiseMLP(\u00afa1) $w^{2}\\leftarrow$ pointwiseMLP(\u00afa2) $w^{3}\\gets$ pointwiseMLP(\u00afa3) ", "page_idx": 5}, {"type": "text", "text": "Codomain Attention Block: $\\times L$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "$k^{1},k_{-}^{2},k_{-}^{3}\\gets\\mathsf{F N O}_{k}(w^{1}),\\mathsf{F N O}_{k}(w^{2}),\\mathsf{F N O}_{k}(w^{3})$ $q^{1},q^{2},q^{3}\\gets\\mathsf{F N O}_{q}(w^{1}),\\mathsf{F N O}_{q}(w^{2}),\\mathsf{F N O}_{q}(w^{3})$ v1, v2, v3  FNOv(w1), ${\\mathrm{FNO}}_{v}(w^{2})$ , $\\mathrm{FNO}_{v}(w^{3})$ // Compute attention coefficients M for $i,j\\in\\{1,2,3\\}$ do   \n$\\mathbf{M}[i,j]\\leftarrow\\langle q^{i},\\mathbf{\\bar{k}}^{j}\\rangle$   \nend for   \n$\\mathbf{M}\\gets\\mathbf{sof}\\,\\mathbf{tmax}\\left(\\frac{\\mathbf{M}[i,j]}{N}\\right)$   \n// Compute attention outputs   \nfor $i\\in\\{1,2,3\\}$ do   \nend $\\begin{array}{r}{o^{i}\\leftarrow\\sum_{j\\in\\{1,2,3\\}}v^{j}\\times\\mathbf{M}[i,j]}\\end{array}$   \n$//$ Normalize and Residual   \n$\\begin{array}{r l}&{o^{1}\\leftarrow\\mathrm{norm}(o^{1})+w^{1}}\\\\ &{o^{2}\\leftarrow\\mathrm{norm}(o^{2})+w^{2}}\\\\ &{o^{3}\\leftarrow\\mathrm{norm}(o^{3})+w^{3}}\\\\ &{w^{1},w^{2},w^{3}\\leftarrow\\mathrm{FNO}_{\\mathbb{Z}}(o^{1}),\\mathtt{F N O}_{\\mathbb{Z}}(o^{2}),\\mathtt{F N O}_{\\mathbb{Z}}(o^{3})}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Projection: ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "// Projection from $\\mathbb{R}^{D\\times n}$ to $\\mathbb{R}^{1\\times n}$ $u^{1}\\gets$ pointwiseMLP(w1) $\\boldsymbol{u}^{2}\\leftarrow$ pointwiseMLP(w2) $\\boldsymbol{u}^{3}\\leftarrow$ pointwiseMLP(w3) return u1, u2, u3 ", "page_idx": 5}, {"type": "text", "text": "[34], where a GNO is used as an encoding and decoding module. Given a set of evaluations of an input function $a$ on a mesh, as represented by $\\{a(x_{i}^{i n})\\}_{i=1}^{\\bar{n}}$ , where $\\{x_{i}^{i n}\\}_{i=1}^{n}\\subset{\\mathcal{D}}_{i n}$ , our first step involves concatenation of each physical variables with respective VSPEs (see Fig. 2a). ", "page_idx": 6}, {"type": "text", "text": "Next, we use $\\mathrm{GNO}_{p e r}$ to transform the function $a$ into a new function $w_{0}$ on a uniform latent grid, represented by {xigrid}in=\u20321. Finally, we apply $l$ stacked CoDA-NO layers to $w_{0}$ to obtain the encoded function $w_{l}$ , which acts as a representation of the input function $a$ . ", "page_idx": 6}, {"type": "text", "text": "The decoding module is essentially a mirrored version of the encoding module. It starts by applying another block of $l$ stacked CoDA-NO layers to the encoded function $w_{l}$ to obtain $w_{L}$ . Subsequently, it uses another ${\\mathrm{GNO}}_{p e r}$ operator to transform $w_{L}$ on a uniform grid to an approximation $u$ of the solution function on an arbitrary output grid $\\{u(x_{i}^{o u t})\\}_{i=1}^{n^{\\prime}}$ . The architecture is summarized in Fig. 2a. ", "page_idx": 6}, {"type": "text", "text": "Model Training. To seamlessly adapt to multi-physics PDEs with limited data, we propose a twostage training process: Self-supervised pretraining is followed by a supervised fine-tuning stage. For a summary, we refer to Fig. 2b. ", "page_idx": 6}, {"type": "text", "text": "Pre-training. In the context of self-supervised pretraining, the objective is to train the model to reconstruct the original input function from its masked version. Within this phase, the model\u2019s encoding component is denoted as the Encoder, while the decoding component comprises the Reconstructor. The values of the input function at a specific percentage of mesh points are randomly masked to zero, and certain variables (channels/co-domains) of the input function are entirely masked to zero. The model is then trained to reconstruct the original input from this masked version. ", "page_idx": 6}, {"type": "text", "text": "We emphasize that the self-supervised learning phase is agnostic of the downstream supervised task and only requires snapshots of simulations of the physical systems. ", "page_idx": 6}, {"type": "text", "text": "Fine-tuning. In the supervised fine-tuning phase, the Reconstructor is omitted from the decoding module and replaced by a randomly initialized Predictor module. The parameters of the Encoder and VSPEs are copied from pre-trained weights. If the fine-tuning (target) PDE introduces variables that are not present in the pre-training PDE; we train additional variable encoders only for these newly introduced variables (see Fig. 2b). This ensures that the model adapts to the expanded set of variables needed for the fine-tuning task with minimal additional parameters. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct experiments on two coupled PDEs: fluid-structure interaction and Rayleigh-B\u00e9nard convection system. We also test our model on a diverse set of PDEs from PDEBench [19]. The code is available at https://github.com/neuraloperator/CoDA-NO. ", "page_idx": 6}, {"type": "text", "text": "Modeling Fluid-Structure Interaction. We consider the following problems: (a) a fluid dynamics problem, where a Newtonian, incompressible fluid impinges on a rigid object, and (b) a fluid-structure interaction problem between a Newtonian, incompressible fluid and an elastic, compressible solid object [17]. We denote $\\Omega_{t}^{f}$ (resp. $\\Omega_{t}^{s}$ ) as the domain occupied by the fluid (resp. the solid) at time $t$ The dynamics of the fluid are governed by the Navier-Stokes equations ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho^{f}\\frac{\\partial\\boldsymbol{u}}{\\partial t}+\\rho^{f}\\nabla\\cdot\\left(\\boldsymbol{u}\\otimes\\boldsymbol{u}\\right)=\\nabla\\cdot\\boldsymbol{\\sigma^{f}},\\;\\nabla\\cdot\\boldsymbol{u}=0,\\;\\;\\;\\mathrm{in}\\;\\Omega_{t}^{f}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $u$ and $\\rho^{f}$ denote the fluid velocity and density, respectively. And $\\sigma^{f}$ denotes the Cauchy stress tensor, given by $\\pmb{\\sigma}^{f}=-p\\mathbb{I}+\\mu(\\nabla u+\\mathbf{\\dot{V}}u^{T})$ , where $\\mathbb{I}$ is the identity tensor, $p$ the fluid pressure, and $\\mu$ the fluid dynamic viscosity. ", "page_idx": 6}, {"type": "text", "text": "For fluid-structure interaction, the deformable solid is governed by the elastodynamics equations ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho^{s}\\frac{\\partial^{2}d}{\\partial t^{2}}=\\nabla.(J\\pmb{\\sigma^{s}}(\\mathbf{F}^{-1})^{T})\\qquad\\mathrm{in}\\;\\Omega_{t}^{s}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $\\mathbf{F}=\\mathbb{I}+\\nabla d$ and $J=\\operatorname*{det}(\\mathbf{F})$ . Here $d,\\,\\rho^{s},\\,F$ , and $\\sigma^{s}$ denote the deformation field, the solid density, the deformation gradient tensor, and the Cauchy stress tensor, respectively (see Eq. (18) in the Appendix). The fluid dynamics (resp. the fluid-structure interaction) problem considers a fluid flow past a fixed, rigid cylinder with a rigid (resp. elastic) strap attached. The details regarding the geometric setup (see Fig. 3), time-dependent inlet boundary condition, and the initial conditions are ", "page_idx": 6}, {"type": "text", "text": "Modeling Rayleigh-B\u00e9nard Convection. The Rayleigh-B\u00e9nard convection system governs the flow of a fluid layer heated from below and cooled from above. The governing equations for the RayleighB\u00e9nard system consist of the incompressible Navier-Stokes equations coupled with an energy equation for heat transfer. The system is modeled as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\frac{\\partial{\\bf u}}{\\partial t}+{\\bf u}\\cdot\\nabla{\\bf u}+\\nabla P-\\nu\\nabla^{2}{\\bf u}-\\alpha g{\\bf T}{\\hat{\\bf z}}=0}}\\\\ {\\displaystyle{\\frac{\\partial T}{\\partial t}+{\\bf u}\\cdot\\nabla{\\bf T}-\\kappa\\nabla^{2}{\\bf T}=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Dataset Description and Generation. To study the fluid-structure interaction system, two datasets, the fluid-structure interaction $\\mathbf{\\Delta}\\mathbf{NS+EW}$ dataset) and the fluid dynamics(NS dataset), are generated using the TurtleFSI package [35]. ", "page_idx": 7}, {"type": "text", "text": "We simulate the fluid-structure interaction and the fluid dynamics test cases described above up to time $T_{f}=10$ , using a constant time-step $\\begin{array}{r}{\\delta t=\\frac{T_{f}}{n}}\\end{array}$ , where $n=1000$ . The data sets are composed of solution trajectories $[u_{t},p_{t},d_{t}]$ (resp. $[u_{t},p_{t}])$ , which denote the approximate solution of the fluidstructure interaction problem (resp. the fluid dynamics problem) at times $t=i\\delta t,i\\in\\{0,\\ldots,n\\}$ . These trajectories are generated on the basis 3 parameters $(\\mu,c_{1},c_{2})$ describing combinations of fluid viscosities $\\mu\\in\\{0.5,\\bar{1},5,10\\}$ and inlet conditions, $(c_{1},c_{2})\\in\\mathcal{T}$ . ", "page_idx": 7}, {"type": "text", "text": "For our setup, the fluid considered is water, with a density of $1000k g.m^{-3}$ and a maximum inlet velocity of approximately $4m.s^{-1}$ , leading to Reynolds $(R e)$ numbers in the range $200-4000$ (for $\\mu$ between $10-0.5)$ . Modeling fluid-solid interaction or only fluid motion with such high Reynolds numbers is challenging and serves as a benchmark problem [12, 17] (See Sec. C.2 for a detailed explanation). ", "page_idx": 7}, {"type": "text", "text": "To study the Rayleigh-B\u00e9nard convention system, we degenerate two different PDE datasets. Firstly, we generate Rayleigh-B\u00e9nard convection system with $R a$ number $12\\times10^{3}$ and $20\\times10^{3}$ . We set the temperature difference between the top (cold) and bottom (hot) boundaries to 1. We assume no-slip boundary conditions, and to start the convection process, we also add initial temperature perturbation. Additionally, we generate incompressible Navier-Stocks equations with Reynold number $R e=500$ with cyclic boundary condition on a uniform $2D$ grid [36] (for details, see Appendix Sec. C.3). ", "page_idx": 7}, {"type": "text", "text": "Experiment Setup. For the fluid-structure interaction system, we conduct two distinct pretraining procedures for CoDA-NO and obtain two pretrained models: $\\mathcal{G}_{\\mathtt{N S-E W}}^{\\mathtt{p}}$ and $\\mathcal{G}_{\\mathtt{N S}}^{\\mathtt{p}}$ . The former is pretrained on a fluid-structure interaction dataset that combines the Navier-Stokes equation and the elastic wave equation, denoted as $\\mathcal{G}_{\\mathtt{N S-E W}}^{\\mathtt{p}}$ . The latter, $\\mathcal{G}_{\\mathtt{N S}}^{\\mathtt{p}}$ , is pretrained on a fluid motion dataset governed solely by the Navier-Stokes equation. In both scenarios, the pretraining involves utilizing 8000 snapshots of flow and displacement fields with ${R e}\\in\\{200,2000\\}$ . ", "page_idx": 7}, {"type": "text", "text": "The supervised task involves training the model to predict the system\u2019s state at the subsequent time step based on its current state. For the fluid-structure interaction dataset, we train an operator $\\mathcal{G}_{\\mathrm{NS-EW}}$ such that $\\mathcal{G}_{\\sf N S-E W}:\\left[u_{t},p_{t},d_{t}\\right]\\:\\rightarrow\\:\\left[u_{t+\\delta t},p_{t+\\delta t},d_{t+\\delta t}\\right]$ , where $u,p$ , and $d$ are the velocity, pressure, and mesh deformation fields (see Sec. 4). For the data with only fluid motion, we train the operator $\\mathcal{G}_{\\mathrm{NS}}$ which maps between the current and next time step velocity and pressure field as $\\mathcal{G_{\\mathrm{NS}}}:\\left[u_{t},p_{t}\\right]\\rightarrow\\left[u_{t+\\delta t},p_{t+\\delta t}\\right]$ . ", "page_idx": 7}, {"type": "text", "text": "The pretrained model for both datasets is fine-tuned for unseen viscosity $\\mu=5.0(R e=400)$ with different numbers of a few shot examples. The inlet conditions of these simulations are excluded from the pretraining data. So, the target PDEs\u2019 viscosity and inlet conditions are absent in the per-taining dataset. We test the model\u2019s adaptability on a more turbulent fluid-solid interaction dataset with $R e=4000(\\mu=0.5)$ by finetuning both pretrained models $\\mathcal{G}_{\\mathtt{N S-E W}}^{\\mathtt{p}}$ and $\\mathcal{G}_{\\mathtt{N S}}^{\\mathtt{p}}$ on each dataset. ", "page_idx": 7}, {"type": "text", "text": "For the Rayleigh-B\u00e9nard convention system, we pretrain a CoDA-NO model, denoted as $\\mathcal{G}_{\\mathtt{N S}}^{\\mathtt{p}}$ , on the incompressible Navier-Stokes equations using 40, 000 snapshots in a self-supervised manner. The supervised task for this system is to train an operator, $\\mathcal{G}_{\\mathrm{NS-T}}:[u_{t},\\mathbf{T}_{t}]\\rightarrow[u_{t+\\delta t},\\mathbf{T}_{t+\\delta t}].$ , where $u$ represents velocity and $T$ represents temperature. The pretrained model $\\dot{\\mathcal{G}}_{\\mathtt{N S}}^{\\mathtt{p}}$ is fine-tuned for the supervised task of solving Rayleigh-B\u00e9nard convection using different numbers of a few shot training samples. ", "page_idx": 7}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/404b1f1013ba4e65eb23f1482c2ba0b6c23be7f222411daf40fe186ab726a807.jpg", "table_caption": ["Table 1. Test $L_{2}$ loss for fluid dynamics (NS) and fluid-solid interaction $\\mathrm{NS+EW})$ ) datasets with viscosity $R e=400$ and $R e=4000$ for different numbers of few-shot training samples. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Baselines. For comparison on the supervised tasks on fluid-structure interaction system, we train GINO [18], DeepONet [37], graph neural network (GNN) [38], vision transformer (ViT) [10], and the Unet [39] model from scratch. The mesh points of the NS and $\\scriptstyle\\mathrm{NS}+\\mathrm{EW}$ datasets are irregular and change for each sample. So, to efficiently handle irregular mesh, in the branch network of DeepONet, we use a GNN layer followed by MLPs. Also, as ViT and Unet can handle irregular mesh, we follow the architecture of GINO and use a GNN layer to query the latent function on a uniform grid. We then apply Unet and ViT to the uniform grid, followed by another GNN layer, to get the output at the desired query points. For the Rayleigh-B\u00e9nard convection system, we train Unet [39] and FNO [12] from scratch and compare them against our proposed model. ", "page_idx": 8}, {"type": "text", "text": "It should be noted that employing the existing models for pertaining and subsequent finetuning on the target datasets is nontrivial due to complex geometry and the changes in the number of physical variables between the pertaining and target datasets. We report the $L^{\\bar{2}}$ error between the predicted and target functions, which serves as a measure of model performance. Additional implementation details are provided in the Appendix Sec. H. ", "page_idx": 8}, {"type": "text", "text": "Results. In Tab. 1, we report the performance of our model and the baselines for modeling the fluid-structure interaction. We observe that the pretrained CoDA-NO model performs better than the baselines. Importantly, the performance gain is higher when the number of few-shot examples is very low. This demonstrates the sample efficiency and generalization capability of CoDA-NO to previously unseen physical systems. ", "page_idx": 8}, {"type": "text", "text": "Next, when CoDA-NO is pretrained solely on the NS dataset, it shows an impressive ability to adapt to the more challenging $\\scriptstyle\\mathrm{NS}+\\mathrm{EW}$ dataset. Finally, when CoDA-NO is pretrained on the more intricate $\\scriptstyle\\mathrm{NS}+\\mathrm{EW}$ dataset, it easily adapts to the simpler NS dataset through fine-tuning. This underscores the capability of the CoDA-NO to adjust between different PDEs with varying numbers of variables seamlessly. ", "page_idx": 8}, {"type": "text", "text": "Also, we notice that pretrained CoDA-NO performs better than CoDA-NO trained from scratch, demonstrating the effectiveness of the pretraining scheme. We also provide the energy spectra of the predicted fluid flow by the different models in Sec. F.4 where we observe that the energy spectrum remains closest to the ground truth. ", "page_idx": 8}, {"type": "text", "text": "In Tab. 2, we present the result on modeling the Rayleigh-B\u00e9nard convention. We observe that pretrained CoDA-NO outperforms every other baseline and adapted to the new temperature variable, $T$ , of the Rayleigh-B\u00e9nard system. Similar to the fluid-structure interaction problem, we also observe that the pretrained CoDA-NO outperforms CoDA-NO trained from scratch, which underlines the effectiveness of our pretraining and adaptation mechanism. ", "page_idx": 8}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/68ea256fbe8cd3b468cf725bca220a744d2ca90fe664e996c00d9bbea1416aa7.jpg", "table_caption": ["Table 2. Test $L_{2}$ error for Rayleigh-B\u00e9nard convection system with coupled Navier-Stokes and energy (heat) equation with Rayleigh number $\\check{R a}=12\\times10^{3}$ and $R a\\dot{=}20\\times10^{3}$ for different few shot examples. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Additionally, we also conduct experiments on various PDEs from the PDEBench dataset [19], where we show superior performance and parameter efficiency (see Appendix Sec. G). ", "page_idx": 9}, {"type": "text", "text": "Adaptation to More Turbulent Fluid-Structure Interaction. We also test the adaptation capability of our pretrained model on a more turbulent fluid-solid interaction scenario with viscosity $\\mu=0.5$ with a Reynolds number of 4000. From Tab. 1, we can observe that, even though the model is pretrained on data with lower Reynold\u2019s number $(200\\mathrm{~-~}2000)$ , it can seamlessly adapt to more turbulent flow and outperform baselines with a significant margin. ", "page_idx": 9}, {"type": "text", "text": "Ablation Studies. To demonstrate the effect of each of the proposed components, namely, codomain attention, normalization layer, VSPE, and pertaining, we present the result of a detailed ablation study in Appendix Sec. F.1. We observe that substituting the codomain attention with regular patch-based attention impacts the model\u2019s performance. In particular, removing the normalization layer prevents the model from converging. ", "page_idx": 9}, {"type": "text", "text": "We also provide an ablation study on fine-tuning methods. Instead of fine-tuning all the parameters, here, we freeze the parameters of the \u201cEncoder\" and only train the parameters of the \u201cPredictor\" and VSPEs. This minimized the number of trainable parameters during fine-tuning. Also, in this case, we performed significantly better than the other models (see Appendix Sec. F.5). ", "page_idx": 9}, {"type": "text", "text": "We also provide the results for the zero-shot super-resolution task, where we directly predict the output function on a much denser mesh than the training mesh. Our findings show that CoDA-NO outperforms other baselines significantly (see Appendix Sec. F.2). ", "page_idx": 9}, {"type": "text", "text": "Additionally, we have conducted a comparative analysis of the parameter count and computational cost for each model, which points to the overfitting problem of the baseline when learning complex multi-physics PDEs (see Appendix Sec. F.3). ", "page_idx": 9}, {"type": "text", "text": "Limitations. In general, CoDA-NO\u2019s performance on target PDEs is influenced by the number of training examples, and we highlight the potential for further enhancement through the integration of physics-informed approaches. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce CoDA-NO, a versatile pre-trained model architecture designed for seamless adaptation to Partial Differential Equations (PDEs) featuring diverse variable compositions. Departing from conventional patch-based attention modules, CoDA-NO innovatively extends the transformer to function spaces by computing attention across co-domains. Leveraging a flexible variable encoding scheme and a graph-based neural operator module, CoDA-NO exhibits adaptability to any target PDE, accommodating new and previously unseen variables with arbitrary input-output geometries during fine-tuning. Our empirical evaluations demonstrate that CoDA-NO consistently outperforms baselines across varying amounts of training data and exhibits robustness in handling missing variables. Our findings on complex multiphysics simulations underscore the efficacy and adaptability of CoDA-NO, positioning it as a valuable tool for addressing challenges in machine learning for PDEs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "A. Anandkumar is supported in part by Bren endowed chair, ONR (MURI grant N00014-18-12624), and by the AI2050 senior fellow program at Schmidt Sciences. We thank David Pitt for his support in adding our code to the neuraloperator library, facilitating broader use and accessibility. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Gilbert Strang. Computational science and engineering. Optimization, 2007. ", "page_idx": 10}, {"type": "text", "text": "[2] Gege Wen, Zongyi Li, Qirui Long, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. Real-time high-resolution co 2 geological storage prediction using nested fourier neural operators. Energy & Environmental Science, 16(4):1732\u20131741, 2023. 1   \n[3] Burigede Liu, Nikola Kovachki, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, Andrew M Stuart, and Kaushik Bhattacharya. A learning-based multiscale method and its application to inelastic impact problems. Journal of the Mechanics and Physics of Solids, 2022. 1   \n[4] Geoffrey K Vallis. Atmospheric and oceanic fluid dynamics. Cambridge University Press, 2017. 1   \n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. 2   \n[6] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 2021. 2, 4 [7] Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael Mahoney, and Amir Gholami. Towards foundation models for scientific machine learning: Characterizing scaling and transfer behavior. arXiv preprint arXiv:2306.00258, 2023. 2, 4 [8] Michael McCabe, Bruno R\u00e9galdo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, et al. Multiple physics pretraining for physical surrogate models. arXiv preprint arXiv:2310.02994, 2023. 4, 22 [9] Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian Song, and Jun Zhu. Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training. arXiv preprint arXiv:2403.03542, 2024. 2, 19, 22   \n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 2, 9   \n[11] Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, and Baining Guo. Styleswin: Transformer-based gan for high-resolution image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. 2, 4   \n[12] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, et al. Fourier neural operator for parametric partial differential equations. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 2, 8, 9, 22   \n[13] Kamyar Azzizadenesheli, Nikola Kovachki, Zongyi Li, Miguel Liu-Schiaffini, Jean Kossaif,i and Anima Anandkumar. Neural operators for accelerating scientific simulations and design. arXiv preprint arXiv:2309.15325, 2023. 2   \n[14] Jean Kossaif,i Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, and Anima Anandkumar. Multi-grid tensorized fourier neural operator for high resolution PDEs, 2023. 2   \n[15] Boris Bonev, Thorsten Kurth, Christian Hundt, Jaideep Pathak, Maximilian Baust, Karthik Kashinath, and Anima Anandkumar. Spherical fourier neural operators: Learning stable dynamics on the sphere. In Proceedings of the International Conference on Machine Learning (ICML), 2023. 2   \n[16] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020. 2, 4, 14   \n[17] Stefan Turek and Jaroslav Hron. Proposal for numerical benchmarking of fluid-structure interaction between an elastic object and laminar incompressible flow. Springer, 2006. 3, 7, 8, 16   \n[18] Zongyi Li, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaif,i Shourya Prakash Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, and Anima Anandkumar. Geometry-informed neural operator for large-scale 3d pdes. arXiv preprint arXiv:2309.00583, 2023. 3, 9   \n[19] Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pfl\u00fcger, and Mathias Niepert. PDEBENCH: An extensive benchmark for scientific machine learning, 2023. 3, 7, 10, 19, 22   \n[20] Makoto Takamoto, Francesco Alesiani, and Mathias Niepert. Learning neural pde solvers with parameter-guided channel attention. arXiv preprint arXiv:2304.14118, 2023. 4   \n[21] Zijie Li, Dule Shu, and Amir Barati Farimani. Scalable transformer for pde surrogate modeling. arXiv preprint arXiv:2305.17560, 2023. 4   \n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. 4   \n[23] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n[24] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 2020. 4   \n[25] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 2023. 4   \n[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 2022.   \n[27] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. 4   \n[28] Bonan Xu, Yuanye Zhou, and Xin Bian. Self-supervised learning based on transformer for flow reconstruction and prediction. arXiv preprint arXiv:2311.15232, 2023. 4   \n[29] Gr\u00e9goire Mialon, Quentin Garrido, Hannah Lawrence, Danyal Rehman, Yann LeCun, and Bobak Kiani. Self-supervised learning with lie symmetries for partial differential equations. In ICLR 2023 Workshop on Physics for Machine Learning, 2023. 4   \n[30] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. 6   \n[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning. PMLR, 2015. 6   \n[32] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \n[33] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), 2018. 6   \n[34] Zongyi Li, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaif,i Shourya Prakash Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, et al. Geometry-informed neural operator for large-scale 3d pdes. arXiv preprint arXiv:2309.00583, 2023. 7   \n[35] Aslak W Bergersen, Andreas Slyngstad, Sebastian Gjertsen, Alban Souche, and Kristian ValenSendstad. turtlefsi: A robust and monolithic fenics-based fluid-structure interaction solver. Journal of Open Source Software, 2020. 8   \n[36] Valentin Duruisseaux, Miguel Liu-Schiaffini, Julius Berner, and Anima Anandkumar. Towards enforcing hard physics constraints in operator learning frameworks. ICML 2024 AI for Science Workshop, 2024. 8   \n[37] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193, 2019. 9   \n[38] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. Advances in neural information processing systems, 2016. 9   \n[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention. Springer, 2015. 9   \n[40] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. arXiv preprint arXiv:2108.08481, 2021. 14   \n[41] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXiv preprint arXiv:2003.03485, 2020. 14   \n[42] Dick Kachuma and Ian Sobey. Linear instability of asymmetric poiseuille flows. 2007. 15   \n[43] Sukhendu Ghosh. Relative effects of asymmetry and wall slip on the stability of plane channel flow. Fluids, 2017. 15   \n[44] Anders Logg, Kent-Andre Mardal, and Garth Wells. Automated solution of differential equations by the finite element method: The FEniCS book. Springer Science & Business Media, 2012. 16   \n[45] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM. 23 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendix is organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 In Sec. A, we provide a brief introduction of Neural Operators.   \n\u2022 In Sec. B, we describe regular attention mechanism.   \n\u2022 In Sec. C, we describe the detailed regarding the dataset generation.   \n\u2022 In Sec. D, we provide a comparison between the CoDA-NO and FNO architecture.   \n\u2022 In Sec. E, we provide the visualization of ground truth vs predicted solution function by CoDA-NO.   \n\u2022 In Sec. F, we provide the ablation studies and additional evaluation metrics.   \n\u2022 In Sec. G, we provide the results of our experiment on PDEs from the PDEBench dataset.   \n\u2022 In Sec. H, we provide additional implementation details. ", "page_idx": 13}, {"type": "text", "text": "A Neural Operators ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Neural Operators are a class of deep learning architectures designed to learn maps between infinitedimensional function spaces [40]. A Neural Operator seeks to approximate an operator $\\mathcal{G}$ that maps an input function $a\\in A$ to its corresponding output function $u\\in\\mathcal{U}$ by building a parametric map $\\mathcal{G}_{\\phi}:$ $A\\rightarrow\\mathcal{U}$ . The typical architecture of a Neural Operator can be described as $\\mathcal{G}_{\\phi}=\\mathcal{P}\\circ T_{L}\\circ...\\,T_{1}\\circ\\mathcal{L}$ . Here, $\\mathcal{L}\\colon a\\rightarrow w_{0}$ and $\\mathcal{P}\\colon w_{L}\\to u$ are lifting and pointwise projection operators, respectively. The action of any pointwise operator $\\mathcal{H}:\\{f:\\mathcal{D}\\overset{\\smile}{\\to}\\mathbb{R}^{\\hat{d}_{f}}\\}\\to\\{g:\\mathcal{D}\\overset{\\cdot}{\\to}\\mathbb{R}^{d_{g}}\\}$ can be defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{H}[f](x)=h_{\\theta}(f(x)),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $h_{\\theta}:\\mathbb{R}^{d_{f}}\\,\\rightarrow\\,\\mathbb{R}^{d_{g}}$ is any function with parameters $\\theta$ . The integral operator $\\smash{T_{l}:\\,w_{l-1}\\to w_{l}}$ performs a kernel integration over the input function $w_{l-1}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{T}_{l}[w_{l-1}](x)=\\int_{\\mathcal{D}_{l-1}}k_{l}(x,y)w_{l-1}(y)\\,\\mathrm{d}y.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, $\\mathcal{D}_{l-1}$ is the domain of the function $w_{l-1}$ . In the case of Fourier Neural operators (FNO) [16], a convolution kernel, i.e., $k_{l}(x,y)=k_{l}(x-y)$ was used. By the convolution theorem, this enables the representation of an integral operator as a pointwise multiplication of the Fourier coefficients as follows $w_{l}=\\mathcal{F}^{-1}(\\mathcal{F}(k_{l})\\odot\\bar{\\mathcal{F}}(\\bar{w_{l-1}}))$ . ", "page_idx": 13}, {"type": "text", "text": "For the Graph neural operator (GNO) [41], a small neighborhood $B_{r}(x)\\cap\\mathcal{D}_{l-1}$ around the point $x$ is considered instead of integrating over the whole domain $\\mathcal{D}_{l-1}$ , such that Eq. (2) changes to ", "page_idx": 13}, {"type": "equation", "text": "$$\nw_{l}(x)=\\int_{B_{r}(x)\\cap D_{l-1}}k_{l}(x,y)w_{l-1}(y)\\,\\mathrm{d}y.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Given a set of evaluations of the function $w_{l-1}$ on points $\\{y_{i}\\}_{i=1}^{n}\\subset{\\mathcal{D}}_{l-1}$ , the kernel integral can be approximated by ", "page_idx": 13}, {"type": "equation", "text": "$$\nw_{l}(x)\\approx\\sum_{y_{i}\\in B_{r}(x)}k_{l}(x,y_{i})w_{l-1}(y_{i})q_{i},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $q_{i}\\in\\mathbb{R}$ are suitable quadrature weights [40]. The discretized kernel integral can be viewed as a message passing on graphs, where the neighborhood of each point $x$ consists of all points within radius $r$ . ", "page_idx": 13}, {"type": "text", "text": "B Attention mechanism for finite-dimensional vectors ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Given three sets of vectors, so-called queries $\\{{\\bf q}_{i}\\}_{i=1}^{N_{q}}$ , keys $\\{\\mathbf{k}_{i}\\}_{i=1}^{N_{k}}$ , and values $\\{\\mathbf{v}_{i}\\}_{i=1}^{N_{v}}$ with $N_{k}=N_{v}$ and matching dimensions of queries and keys, attention mechanism calculates weighted sums of the value vectors. Specifically, the set of output vectors $\\{\\mathbf{o}_{i}\\}_{i=1}^{N_{q}}$ can be expressed that ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\bf o}_{i}={\\bf a}^{i}[{\\bf v}_{1},...\\,{\\bf v}_{N_{v}}]^{\\top},\\quad i=1,..\\,.\\,N_{q},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbf{a}^{i}=\\mathtt{S o f t M a x}[\\frac{\\langle\\mathbf{q}_{i},\\mathbf{k}_{1}\\rangle}{\\tau},\\dots,\\frac{\\langle\\mathbf{q}_{i},\\mathbf{k}_{N_{k}}\\rangle}{\\tau}]}\\end{array}$ and $\\tau$ is the temperature term. For the self-attention mechanism, the key, query, and value vectors are calculated from some input sequence $\\{{\\mathbf{z}}\\}_{i=1}^{L}$ using the key, query, and value matrices $\\mathbf{K},\\mathbf{Q}$ , and $\\mathbf{V}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{q}_{i}=\\mathbf{Q}\\mathbf{z}_{i},\\quad\\mathbf{k}_{i}=\\mathbf{K}\\mathbf{z}_{i},\\quad\\mathbf{v}_{i}=\\mathbf{V}\\mathbf{z}_{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Dataset Description ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "wSpIdUXZYX/tmp/f064aff84cd4536bae98739d5e44149a9ace4fcfa2905cac436a3bd1f4a4a31d.jpg", "img_caption": ["Figure 3. Visualization of horizontal velocity $u_{x}$ at $t$ and $t+\\delta t$ time step. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "C.1 Fluid-Structure Interaction System ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here, we provide the details on generating the fluid-structure interaction dataset involving NavierStokes and Elastic wave equations. ", "page_idx": 14}, {"type": "text", "text": "Fluid-structure interaction model. Under the Kirchoff St-Venant model, the Cauchy stress tensor $\\sigma^{s}$ verifies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pmb{\\sigma}^{s}=\\frac{1}{J}\\mathbf{F}(\\lambda^{s}(t r(\\mathbf{E}))\\mathbb{I}+2\\mu^{s}\\mathbf{E})\\mathbf{F}^{T}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\lambda^{s}$ and $\\mu^{s}$ are the Lame coefficients, and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{E}=\\frac12(\\mathbf{F}\\mathbf{F}^{T}-\\mathbb{I}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Inlet Boundary Condition. Time-dependent inlet boundary conditions consist of $4^{t h}$ order polynomials velocity profiles which vanish at the channel walls [42, 43]. The inlet conditions are given by ", "page_idx": 14}, {"type": "equation", "text": "$$\nu_{c_{1},c_{2}}^{\\mathcal{Z}}(y,t)=v(t)\\cdot\\frac{y(y-H)\\left(y-c_{1}\\frac{H}{2}\\right)\\left(y-c_{2}\\frac{H}{2}\\right)}{H(1-c_{1})(1-c_{2})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here $v$ is the ramp function defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nv(t)=\\left\\{\\begin{array}{c l l}{70\\cdot\\left(1-\\cos\\left(\\frac{\\pi t}{2}\\right)\\right)}&{\\mathrm{if}}&{0\\leq t<2}\\\\ {140}&{\\mathrm{if}}&{t\\geq2}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $(c_{1},c_{2})\\in\\mathcal{T}$ , where ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathcal{Z}}=\\left\\{(a,b)\\in\\{-6,-4,-2,0,2,4,6\\}^{2}\\mid a\\leq b\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "are enforced at the inlet $x=0$ . ", "page_idx": 14}, {"type": "text", "text": "Geometric setup, boundary, and initial conditions. In the considered setup (see also Figure 3), a fluid flows past a fixed cylinder of radius $R\\,=\\,0.05$ centered at $(x_{c},y_{c})\\overset{\\cdot}{=}(0.2,0.2)$ in a twodimensional channel of length $L=2.5$ and width $H=0.41$ . A deformable elastic strap of length $\\ell=0.35$ and height $h=0.02$ is attached to the back of the cylinder. Note that, in the test cases considering fluid motion exclusively, the elastic strap is assumed to be rigid. ", "page_idx": 14}, {"type": "text", "text": "In the case of the fluid-structure interaction, the interaction conditions arise from the mechanical equilibrium at the boundaries of the strap, which are given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\pmb\\sigma}^{f}\\cdot{\\bf n}={\\pmb\\sigma}^{s}\\cdot{\\bf n}}}\\\\ {~~~~~~~u=\\frac{\\partial d}{\\partial t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where n denotes a unit normal vector to the fluid-solid interface. No-slip boundary conditions are imposed on the fluid velocity at the top (resp .bottom) boundaries of the channel at $y=0$ (resp. $y\\;=\\;H$ ), as well as on the boundaries of the cylinder and the elastic strap. Outflow boundary conditions are imposed at $x=2.5$ by enforcing the values $p=0$ for the pressure. ", "page_idx": 15}, {"type": "text", "text": "The initial conditions ", "page_idx": 15}, {"type": "equation", "text": "$$\n(u,p,d)=(0,0,0)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the displacement $d=0$ corresponds to a perfectly horizontal elastic strap and is imposed at time $t=0$ . ", "page_idx": 15}, {"type": "text", "text": "Details regarding the data set generation. The TurtleFSI package provides a monolithic solver for the fluid-structure interaction test case, that is, combining the equations describing the solid and fluid evolution into one coupled system based on an Arbitrary Eulerian-Lagrangian (ALE) formulation of the problem and developed on the FEniCS computing environment [44]. ", "page_idx": 15}, {"type": "text", "text": "The initial conditions are expressed at set $X\\,=\\,X_{S}\\cup X_{\\mathcal{F}}$ of mesh points, corresponding to the union of the solid and fluid domains. In the ALE formulation, at each snapshot $0\\leq t\\leq t_{M}$ of the simulation, the solution is given at a set of mesh points $X_{t}=X+d_{t}$ , where $d_{t}$ denotes the mesh displacement. In particular, the snapshots $u_{t}$ (resp. $p_{t}$ ) correspond to numerical approximations of the velocity (resp. the pressure) at the mesh points $X_{t}$ . Notably, while equation (10) governs the deformation field in the solid domain $\\Omega_{t}^{s}$ , the displacements $d_{t}$ are obtained through an extension of the deformation field to the fluid domain $\\Omega_{t}^{f}$ via a biharmonic extrapolation. ", "page_idx": 15}, {"type": "text", "text": "In all the cases considered, the values $\\rho^{f}\\,=\\,1.0\\,\\times\\,10^{3}$ , $\\rho^{s}\\,=\\,1.0\\,\\times\\,10^{3}$ , $\\lambda^{s}\\,=\\,4.0\\,\\times\\,10^{6}$ and $\\mu^{s}=2.0\\times10^{6}$ were used. The simulations were performed using a constant time step $\\delta t=0.01$ . ", "page_idx": 15}, {"type": "text", "text": "C.2 Justification of Experiment Design ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For our setup, the fluid considered is water, with a density of $1000\\;\\mathrm{kg}.\\mathrm{m}{-3}$ and a maximum inlet velocity of approximately $4m.s^{-1}$ , leading to Reynolds numbers in the range $200\\!-\\!2000$ ( $\\mu=10\\!-\\!1)$ for our experiments. Only when the flow becomes turbulent can ample movements of the elastic strap (Fig. 4) be observed in the fluid-structure interaction case. Modeling fluid-solid interaction or only fluid motion with such a Reynolds number is quite challenging and used as a benchmark problem [17]. ", "page_idx": 15}, {"type": "text", "text": "Modeling fluid-solid interaction with an even higher Reynolds number requires a very high computational cost. Because TurtleFSI\u2019s (used in this study) fluid solver, including its\u2019 fluid-structure interaction solver, uses a direct numerical simulation (DNS) of fluid dynamics and does not employ any turbulence models. This means that in order to accurately capture the small-scale energy-dissipating vortices that form when the flow interacts with the cylinder and strap at high Reynolds numbers, a very fine spatial domain discretization is required. Furthermore, an extremely small time step $(\\Delta t)$ is necessary to ensure numerical stability. For these reasons, the contribution [17], which introduced the benchmark fluid-structure interaction problem studied here, only deals with flows that have Reynolds numbers less than or equal to 200. ", "page_idx": 15}, {"type": "text", "text": "It\u2019s crucial to highlight a significant disparity between the pre-training and finetuning stages, particularly concerning examples with viscosities 1 and 10. This disparity arises from the utilization of distinct inlet boundary conditions during the pre-training and finetuning phases. Consequently, even though the viscosities align with the pre-training dataset during finetuning on PDEs featuring $\\mu\\in\\{1,10\\}$ , the model faces formidable challenges in adapting due to variations in inlet conditions. The finetuning dataset with viscosity ${=}5$ has different viscosity as well as intel conditions compared to the pre-training dataset, serving as an out-of-distribution PDE setup. ", "page_idx": 15}, {"type": "text", "text": "C.3 Generating Rayleigh-B\u00e9nard dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The initial temperature field is initialized with a linear gradient between the hot bottom boundary, $\\mathbf{T}_{\\mathrm{bottom}}\\,=\\,1$ , and the cold top boundary, $\\mathbf{T}_{\\mathrm{top}}\\,=\\,0$ . To induce instability and initiate convection, temperature perturbations are introduced in localized regions of the domain. A region centered at $\\left({\\frac{L_{x}}{4}},{\\frac{L_{y}}{4}}\\right)$ is perturbed to $\\mathbf{T}=1$ , while a region near the middle of the domain, centered at $\\left({\\frac{L_{x}}{2}},{\\frac{L_{y}}{2}}\\right)$ , is set to $\\mathbf{T}=-1$ . These perturbations break the symmetry and help to trigger the onset of convection patterns. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "For the incompressible Navier-Stocks equation, we consider two-dimensional Kolmogorov flow (a form of the Navier-Stokes equations) for a viscous, incompressible fluid, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbf{u}}{\\partial t}=-\\mathbf{u}\\cdot\\nabla\\mathbf{u}-\\nabla p+\\frac{1}{R e}\\Delta\\mathbf{u}+\\sin(n y)\\hat{\\mathbf{x}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with the incompressibility constraint $\\nabla\\cdot\\mathbf{u}=0$ on the domain $[0,2\\pi]^{2}\\times(0,\\infty)$ . The initial condition is given as $\\mathbf{u}(\\cdot,0)=\\mathbf{u}_{0}$ , where $\\mathbf{u}$ denotes the velocity, $p$ the pressure, and $R e$ is the Reynolds number which we set to 500 for our simulation. ", "page_idx": 16}, {"type": "text", "text": "D Comparison with FNO ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We would like to bring out a distinction. In FNO, the mixing of channels happens in the Fourier space in the spectral layer through the linear transform $R$ applied to the Fourier coefficients. This defines a weighting of some sort on the input channels and how they are mixed. The mixing is global since the Fourier transform is a global operation. In CoDA-NO, however, the mixing of channels happens in the spatial domain through the attention mechanism as well as in the Fourier space because $\\mathcal{K}^{h},\\mathcal{Q}^{h},\\dot{\\mathcal{V}}^{h},\\mathcal{M}$ , and $\\mathcal{T}$ are all implemented as FNO\u2019s. The attention weights determine how the channels are mixed, allowing for a more flexible and input-dependent mixing. This added flexibility enables CoDA-NO to better capture complex interactions and dependencies between different physical variables, especially in multiphysics problems where the relationships between variables can be intricate and vary depending on the input conditions. This also implies that this is more efficient as it can seamlessly incorporate additional or fewer variables during fine-tuning, avoiding retraining the whole model from scratch like FNO would have to, which can be computationally expensive. ", "page_idx": 16}, {"type": "text", "text": "E Visualization of Results ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "wSpIdUXZYX/tmp/1301f4ee8d0d6061f91d29a9e581a6526309d78b34ad8b764994a71c5926de9e.jpg", "img_caption": ["Figure 4. Visualization of CoDA-NO prediction. We plot the horizontal velocity $u_{x}$ for the fluid-structure interaction problem. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "F Additional Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Ablation of Proposed components ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 3 shows that replacing the codomain attention with a regular attention mechanism or the removal of any of these designed components significantly impacts the model\u2019s performance. We also observe that our proposed normalization technique is crucial for effective training. ", "page_idx": 17}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/f834af1b438b8760b3b6221b92b366dcad9d461cb42561897cacc22a56ff7218.jpg", "table_caption": ["Table 3. Evaluating $L_{2}$ loss across different models using various pre-training datasets and varying numbers of few-shot training samples. \"\\*\" indicates configurations that did not converge due to excessive training error. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "F.2 Zero-Shot Super Resolution Test ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here, we present the results of our zero-shot super-resolution models (see Tab. 4) on complex fluidsolid interaction problems. We train the models with 1317 mesh points on the domain (xy plain). However, during inference, the solution function is queried directly on a denser and non-uniform target mesh consisting of 2193 points. ", "page_idx": 17}, {"type": "text", "text": "We observe that the zero-shot super-resolution performance of CoDA-No is significantly better than the other baselines. ", "page_idx": 17}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/79bdde60501d9d397ac5a58b804ea47c606bc4eec1a3126b428219e031fb6d92.jpg", "table_caption": ["Table 4. Zero Shot Super Resolution Performance on Fluid-Solid (NS-EW) Interaction Problem"], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "F.3 Parameter Count and Computational Cost. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Now the present the number of parameters and training/interference time taken by the proposed model along with different baselines used in the study in Tab. 5. It might seem that models are not compared fairly, as the CoDA-NO has a higher parameter count. However, here, we test the models on a few shot learning problems. Increasing the baselines\u2019 parameter count worsens the overfitting problem. ", "page_idx": 17}, {"type": "text", "text": "To demonstrate this fact, we perform experiments on a fluid-solid interaction dataset with an increased parameter count. We will observe that increasing the parameter count almost always negatively impacts the performance, especially for very few hot learning scenarios (see Tab. 6). ", "page_idx": 18}, {"type": "text", "text": "We also note that the additional model parameters and computation are required to learn rich inter-variable dependencies during pre-training and generalize from single to multi-physics during finetuning. Furthermore, the zero-shot super-resolution capability of CoDA-NO is discussed in Sec. F.2. CoDA-NO is a justified choice due to its seamless adaptation to various PDEs, remarkable performance gap, and zero-shot super-resolution capability despite having a little more computational overhead. ", "page_idx": 18}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/3e28c62dcca8ef37d16a97531a78f04ef332a6785b160c4a5205d33dcaa9de5d.jpg", "table_caption": ["Table 5. Comparison of Inference Time, Training Time (in sec.) per sample, and Number of Parameters for different models. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/26748ff41c4e728cdf2b37b4eb8ec92347e8d9985c66314a5ee9d63dbb893c2c.jpg", "table_caption": ["Table 6. Overfitting of Baselines with Higher Parameters (in $\\times1e6)$ on NS-EW dataset "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "F.4 Energy Spectrum ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, we show the energy spectrum for the NS-EW dataset for $\\mu=5$ calculated from the test set (see Fig. 5). All models are trained on 100 training examples. Due to numerical error, the measured spectral energy does not decay smoothly in the high-frequency region. However, our models\u2019 energy spectrum remains closest to the ground truth. ", "page_idx": 18}, {"type": "text", "text": "F.5 Ablation on Finetuning Technique ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, in Tab. 7, Tab. 9, and Tab. 8, we present some additional ablation studies on our model\u2019s performance when we keeping the weight of the \u201cEncoder\" frozen during supervised fine-tuning. ", "page_idx": 18}, {"type": "text", "text": "F.6 Error bar ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "G PDEBench experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We finally compare CoDA-NO, FNO, and the recently proposed DPOT [9] on PDEBench [19]. DPOT (Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training) is a large-scale pre-training approach for learning PDE representations. It utilizes an auto-regressive denoising strategy and a Fourier transformer architecture for efficient pre-training on diverse PDE datasets. ", "page_idx": 18}, {"type": "text", "text": "To assess the performance of these models on a diverse set of PDEs, we conduct experiments on three single-physics datasets from the PDEBench: Shallow Water Equations (SWE), Diffusion Equations (DIFF), and Navier-Stokes Equations (NS). We follow the same pretraining and finetuning procedure for both models\u2019 datasets. And we evaluate the models on the following two tasks ", "page_idx": 18}, {"type": "text", "text": "\u2022 Reconstructive task: The models are trained on the respective datasets during pretraining to use the self-supervised learning objective to reconstruct the masked input. The error achieved through this is called the \u201cReconstruction error\". This allows the models to learn meaningful representations of the underlying physical systems. ", "page_idx": 18}, {"type": "image", "img_path": "wSpIdUXZYX/tmp/4f42425d719eadbe27fa11e4c3990a302dd23d7011bd5bf10f601ea3a4e23adf.jpg", "img_caption": ["Figure 5. Energy Spectrum of the Velocity Field of the fluid on the fluid-solid interaction dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/cb8f8d849513e7ffde3915b3227749da445c3897ce4a79ca6271229ccb7adab6.jpg", "table_caption": ["Table 7. Test errors ( $L_{2}$ loss) for fluid dynamics (NS) and fluid-solid interaction $(\\mathbf{NS+EW})$ datasets with viscosity $\\mu=1.0$ for different numbers of few-shot training samples. The pre-training is done with 8000 samples taken from NS and $\\scriptstyle\\mathrm{NS+EW}$ datasets with viscosities $\\mu\\in\\{1.0,10.0\\}$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/14b292497acb39887e456f8eb8d7286faff5e923d4da0b743d1b3bd09ab274bf.jpg", "table_caption": ["Table 8. Test errors $\\mathbf{\\mathcal{L}}_{2}\\left\\|\\mathbf{oss}\\right)$ for fluid dynamics (NS) and fluid-solid interaction $(\\mathbf{NS+EW})$ datasets with viscosity $\\mu=5.0$ for different numbers of few-shot training samples. The pre-training is done with 8000 samples taken from NS and $\\scriptstyle\\mathrm{NS+EW}$ datasets with viscosities $\\mu\\in\\{1.0,10.0\\}$ . "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/d798aba0e43ef3dd371c2d19987de7b14f38a580e33a969d6a7dad1a78a4d3fe.jpg", "table_caption": ["Table 9. Test errors ( $L_{2}$ loss) for fluid dynamics (NS) and fluid-solid interaction $(\\mathbf{NS+EW})$ datasets with viscosity $\\mu=10.0$ for different numbers of few-shot training samples. The pre-training is done with 8000 samples taken from NS and $\\scriptstyle\\mathrm{NS+EW}$ datasets with viscosities $\\mu\\in\\{1.0,10.0\\}$ . "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/38891715cfc338948791dc61a388f6a29983ed04a318351e07aa76a845315c76.jpg", "table_caption": ["Table 10. Error bar representing standard deviation over three runs with different number of few shot example for $\\scriptstyle\\mathrm{NS+EW}$ dataset for $R e=400$ "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "\u2022 Predictive task: Subsequently, we finetune the pre-trained models using a supervised learning objective, where the goal is to minimize the \u201cPrediction error\" by accurately predicting the next 5 timesteps given the input history. We try to learn the solution operator that maps the state of the system from time $t\\in[0,\\dot{T}]$ to the state at time $t\\in[T,\\bar{T}+5]$ , effectively predicting the next 5 timesteps given the history up to time $T$ . ", "page_idx": 20}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/b2fd580f47fa4fba84e4781f1951d6ec6fca1a10e2d10f836b54e294f79328bd.jpg", "table_caption": ["Table 11. Error bar representing standard deviation over three runs with different number of few shot example for NS dataset for $R e=400$ "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/c36be5df2ffdcb1d96d7bd48f128d556f3807404f9187c759a65190fb8eedf37.jpg", "table_caption": ["Table 12. Test errors $L_{2}$ error) for CoDA-NO vs FNO on 2D datasets from PDEBench. SWE indicates shallow water equations, DIFF indicates the diffusion equation. $\\scriptstyle\\mathrm{NS+DIFF+SWE}$ means pretraining and fine-tuning on a combined Navier-Stokes, diffusion, and shallow water equations dataset. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 12 presents CoDA-NO and FNO [12] test errors on the single-physics PDEs datasets sourced from PDEBench [19]. For the single-physics experiments, CoDA-NO consistently outperforms FNO, improving generalization (predictive error) up to ${\\bf{43}}\\%$ over FNO, indicating its ability to capture complex dynamics and dependencies within these systems. We report additional details and experimental results in Sec. G comparing FNO, DPOT [9] and CoDA-NO where CoDA-NO demonstrates superior performance and parameter efficiency. It is worth noting that DPOT and MPP [8] are significantly bigger models but can also handle a larger set of PDEs. ", "page_idx": 21}, {"type": "text", "text": "In addition to the single-physics experiments, we also explore the potential of joint pretraining and finetuning across multiple PDE systems. We create a combined dataset by merging the SWE, DIFF, and NS datasets, even though these PDEs do not share any common physical variables or governing equations. Both FNO and CoDA-NO were pre-trained and fine-tuned on this dataset for 35 epochs. ", "page_idx": 21}, {"type": "text", "text": "DPOT, with its large-scale pre-training approach, demonstrates strong performance on the SWE and DIFF datasets. Even with a 500 million parameter model (DPOT-L-500), DPOT achieves impressive results, reducing the test error to 0.0017 on SWE and 0.0073 on DIFF. It is also important to note that DPOT is larger as it was pretrained on 12 different datasets, and hence the size is justified. However, it is noteworthy that CoDA-NO, with only 11 million parameters, comes very close to achieving similar generalization performance. CoDA-NO\u2019s test error on DIFF (0.0081) is comparable to DPOT\u2019s performance, as shown in table 15, despite having significantly fewer parameters and fewer finetuning epochs. However, on the other hand, we see that CoDA-NO doesn\u2019t do well on the SWE dataset as shown in table 14; we assume that the case would be the fact that we would need to finetune for more epochs to achieve better results. The SWE task is also a harder dataset; increasing the model complexity and pretraining epochs would help get better results. ", "page_idx": 21}, {"type": "text", "text": "It is important to highlight that DPOT was pre-trained on 12 datasets for 1000 epochs, while CoDANO was pre-trained and fine-tuned on a single dataset for 35 epochs. Despite this difference in pre-training data and epochs, CoDA-NO still achieves competitive results compared to DPOT\u2019s 200/500 epochs of fine-tuning. ", "page_idx": 21}, {"type": "text", "text": "These results suggest that when there is shared physics between the pre-training and fine-tuning datasets, CoDA-NO can effectively leverage this commonality to achieve strong generalization performance. However, when there is no shared physics, as in the case of the combined dataset, CoDA-NO\u2019s performance may not be as remarkable. ", "page_idx": 21}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/bf4b24c3057c977237d319a31d297b5e0c17c996c1fa5b60c62e15406d1bad60.jpg", "table_caption": ["Table 13. Comparison of model parameter sizes for CoDA-NO, FNO, and DPOT. DPOT-FT stands for the Finetuning model used, whereas -T stands for tiny, -S stands for small, -M stands for medium, and -L stands for Large. The pretrained model sizes are present in the original paper but are around the same parameter sizes as the fine-tuned models. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/0d693e484ce67aa8a38ea0dfcc3d6de6065aae6abb6741edc0953c90c0cb06eb.jpg", "table_caption": ["Table 14. Test errors for CoDA-NO vs DPOT on 2D datasets from PDEBench. SWE indicates shallow water equations data. \u201812DATA\u2019 represents the 12PDE PDE datasets DPOT is pretrained on. The $\\mathbf{\\Omega}^{\\leftarrow}-200^{\\circ}$ and \u201c-500\u201d suffixes denote fine-tuning on each subset for 200 and 500 epochs, respectively, which is directly taken from the DPOT paper. All of this was fine-tuned on SWE data. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Table 13 compares the model sizes of CoDA-NO, FNO, and DPOT. CoDA-NO\u2019s model size of 11 million parameters is significantly smaller than FNO\u2019s 1.9 billion parameters and DPOT\u2019s largest model size of 500 million parameters. This highlights CoDA-NO\u2019s parameter efficiency and its ability to achieve competitive performance with a more compact model. ", "page_idx": 22}, {"type": "text", "text": "In summary, these experiments on the PDEBench datasets demonstrate the effectiveness of CoDANO in learning and generalizing to different PDE systems. CoDA-NO\u2019s performance, especially considering its smaller model size and shorter pre-training, showcases its potential as a foundation model for scientific machine learning. The ability to achieve competitive results with DPOT, despite the differences in pre-training data and epochs, further highlights CoDA-NO\u2019s efficiency and generalization capabilities. ", "page_idx": 22}, {"type": "text", "text": "G.1 Ablation on the Size of FNO ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Tab. 16, we present the performance of FNO with a different number of parameters along with the performance of CoDA-NO. ", "page_idx": 22}, {"type": "text", "text": "H Implementation Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The variable encoders are implemented using a multi-layer perceptron mapping the position $x\\in\\mathcal{D}$ to the embedding vector. Following transformers and NeRF [45] model, we use positional encoding instead of raw coordinates. The Encoder and Reconstructor modules use three stacked CoDA-NO layers. The Predictor modules use one layer of CoDA-NO. ", "page_idx": 22}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/681e3e9907937ce670a8c17488285a7efc82f91d3d55ef0238bab07306e59827.jpg", "table_caption": ["Table 15. Test errors for CoDA-NO vs DPOT on 2D datasets from PDEBench. DIFF indicates the diffusion equation data. \u201812DATA\u2019 represents the 12PDE datasets DPOT was trained on. The $\"200\"$ and $\\mathrm{^{66}-500^{\\circ}}$ suffixes denote fine-tuning on each subset for 200 and 500 epochs, respectively, which is directly taken from the DPOT paper. All of this was fine-tuned on DIFF data. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "wSpIdUXZYX/tmp/3596f1d72c0064c0208aef683a024ef86aa44562dfcae364bef16a2dc5cd483d.jpg", "table_caption": ["Table 16. Error in $L_{2}$ norm for models in both the Shallow Water Equation and Diffusion-Reaction experiments. The number of parameters is reported alongside the $L_{2}$ errors for both tasks. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "For every training sample, one of the following two masking choices is selected with equal probability ", "page_idx": 23}, {"type": "text", "text": "\u2022 $50\\%$ of the mesh points of $60\\%$ of variables are masked.   \n\u2022 $30\\%$ of the variables are masked out completely. ", "page_idx": 23}, {"type": "text", "text": "In order to apply masking on an irregular mesh, we select a point at random from the mesh. Following this, we identify the neighboring points within a fixed distance from the selected point and set their values to zero. This process is continued until we have masked out a predetermined portion of all mesh points. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the claims are justified either in the theoretical aspect of the paper or in the experiments. We carefully make the distinctions between what is a heuristic versus what is being proposed based on the given assumptions. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: See line 321. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, the complete and correct proof is provided in the Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the details are either in the main paper or the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All the details are either in the main paper or the appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All the details are either in the main paper or the appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Error bars are reported for the main result. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Provided in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We study a fundamental framework for the downsampling of signals, which is foundation research and has limited negative societal impacts. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper has no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The resources used in the work are cited. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Code will be open-sourced under MIT license. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]