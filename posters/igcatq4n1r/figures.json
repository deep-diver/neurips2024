[{"figure_path": "IGCaTQ4n1R/figures/figures_1_1.jpg", "caption": "Figure 1: Top: Comparison of OpenDlign with traditional open-world 3D learning models. Depth-based (a) and point-based (b) methods employ additional depth or point encoders for pre-training to align with CAD-rendered images. Conversely, OpenDlign (c) fine-tunes only the image encoder, aligning with vividly colored and textured depth-aligned images for enhanced 3D representation. Both rendered and depth-aligned images are utilized solely during training. Bottom: Visual comparison between multi-view CAD-rendered and corresponding depth-aligned images in OpenDlign.", "description": "This figure compares OpenDlign with traditional depth-based and point-based open-world 3D learning methods.  It highlights that OpenDlign uses depth-aligned images instead of CAD-rendered images, resulting in improved 3D representation learning by fine-tuning only the image encoder. The bottom part visually demonstrates the difference in texture and realism between CAD-rendered and depth-aligned images.", "section": "1 Introduction"}, {"figure_path": "IGCaTQ4n1R/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of OpenDlign. In (a), OpenDlign converts point clouds into multi-view depth maps using a contour-aware projection, which then helps generate depth-aligned RGB images with diverse textures, geometrically and semantically aligned with the maps. A transformer block, residually connected to the CLIP image encoder, is fine-tuned to align depth maps with depth-aligned images for robust 3D representation. For zero-shot classification (b), OpenDlign aggregates multi-view logits from both pre-trained and fine-tuned encoders for label prediction. For few-shot classification (c), it employs a logistic regressor trained on multi-view features from the encoders.", "description": "This figure provides a comprehensive overview of the OpenDlign framework. It illustrates the process of converting point clouds into depth maps, generating depth-aligned images, aligning features using a fine-tuned transformer block, and performing both zero-shot and few-shot 3D classifications.", "section": "3 Methodology"}, {"figure_path": "IGCaTQ4n1R/figures/figures_7_1.jpg", "caption": "Figure 1: Top: Comparison of OpenDlign with traditional open-world 3D learning models. Depth-based (a) and point-based (b) methods employ additional depth or point encoders for pre-training to align with CAD-rendered images. Conversely, OpenDlign (c) fine-tunes only the image encoder, aligning with vividly colored and textured depth-aligned images for enhanced 3D representation. Both rendered and depth-aligned images are utilized solely during training. Bottom: Visual comparison between multi-view CAD-rendered and corresponding depth-aligned images in OpenDlign.", "description": "This figure compares OpenDlign with traditional depth-based and point-based open-world 3D learning models. It highlights that OpenDlign uses depth-aligned images generated by a diffusion model for robust multimodal alignment, instead of CAD-rendered images, to fine-tune only the image encoder and improve 3D representation. The bottom part visually shows the difference between multi-view CAD-rendered and depth-aligned images.", "section": "1 Introduction"}, {"figure_path": "IGCaTQ4n1R/figures/figures_8_1.jpg", "caption": "Figure 4: Effect of the number of views on OpenDlign's zero-shot performance.", "description": "This figure shows the impact of the number of viewpoints used for generating depth maps on the zero-shot classification accuracy of OpenDlign.  The results are presented for two different datasets: ModelNet40 (left) and OmniObject3D (right).  The plots show Top-1, Top-3, and Top-5 accuracy for each dataset, as the number of viewpoints increases from 1 to 10.  A dashed horizontal line indicates the Top-1 accuracy achieved by TAMM-PointBERT, a state-of-the-art model, providing a basis for comparison.", "section": "4 Experiments"}, {"figure_path": "IGCaTQ4n1R/figures/figures_14_1.jpg", "caption": "Figure 1: Top: Comparison of OpenDlign with traditional open-world 3D learning models. Depth-based (a) and point-based (b) methods employ additional depth or point encoders for pre-training to align with CAD-rendered images. Conversely, OpenDlign (c) fine-tunes only the image encoder, aligning with vividly colored and textured depth-aligned images for enhanced 3D representation. Both rendered and depth-aligned images are utilized solely during training. Bottom: Visual comparison between multi-view CAD-rendered and corresponding depth-aligned images in OpenDlign.", "description": "This figure compares OpenDlign's architecture with traditional depth-based and point-based open-world 3D learning methods.  It highlights OpenDlign's key advantage: using a diffusion model to generate realistic depth-aligned images instead of relying on less-realistic CAD-rendered images. This improves alignment robustness and avoids needing extra encoders. The bottom part visually demonstrates the difference in texture and realism between CAD-rendered and the depth-aligned images used in OpenDlign.", "section": "1 Introduction"}, {"figure_path": "IGCaTQ4n1R/figures/figures_15_1.jpg", "caption": "Figure 6: Effect of view count on OpenDlign's zero-shot performance on ScanObjectNN [47]", "description": "This figure shows the impact of the number of viewpoints used in OpenDlign on the zero-shot classification accuracy for the ScanObjectNN dataset.  It presents three lines representing top-1, top-3, and top-5 accuracy, plotted against the number of viewpoints (ranging from 1 to 10). A horizontal dashed line indicates the top-1 accuracy achieved by the TAMM-PointBERT model, a state-of-the-art baseline. The figure demonstrates that OpenDlign's accuracy improves consistently as the number of viewpoints increases, surpassing the TAMM-PointBERT baseline with as few as two viewpoints.", "section": "4.5 Ablation Study"}, {"figure_path": "IGCaTQ4n1R/figures/figures_16_1.jpg", "caption": "Figure 7: Comparison between depth maps from PointCLIP V2 [13] and contour-aware projection.", "description": "This figure compares the depth maps generated using PointCLIP V2's projection method and the contour-aware projection method proposed in the paper. It shows that the PointCLIP V2 projection results in blurry depth maps with missing contour and shape details, whereas the contour-aware projection method preserves more of the original objects' contours and structures.", "section": "A.6 Additional Qualitative Analysis"}, {"figure_path": "IGCaTQ4n1R/figures/figures_16_2.jpg", "caption": "Figure 3: 3D shape retrieval results. (a) Two most similar shapes for each image query. (b) Most similar shapes for each text query. (c) Two most similar shapes for combined image and text queries.", "description": "This figure demonstrates the cross-modal retrieval capabilities of OpenDlign. It shows examples of retrieving 3D shapes based on image queries, text queries, and combined image and text queries. The results highlight OpenDlign's ability to find shapes that semantically match different query types, showcasing its effectiveness in understanding the relationships between images, text, and 3D shapes.", "section": "4.4 Cross-Modal Retrieval"}, {"figure_path": "IGCaTQ4n1R/figures/figures_16_3.jpg", "caption": "Figure 9: Examples of low-quality generated depth-aligned images.", "description": "This figure shows examples of depth-aligned images generated by the diffusion model that are of low quality.  The issues include unrealistic renderings, such as a person inside a monitor, a stone washing machine, or a tent floating in the air. These examples highlight the challenge of generating high-quality, realistic depth-aligned images, which is crucial for the success of OpenDlign's multimodal alignment approach. The low-quality examples show some of the difficulties in controlling the diffusion process to produce only suitable images and the need for filtering to eliminate these low-quality images in order to improve the performance of OpenDlign.", "section": "A.6 Additional Qualitative Analysis"}]