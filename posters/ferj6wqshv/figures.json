[{"figure_path": "ferj6WqShv/figures/figures_1_1.jpg", "caption": "Figure 1: Incomplete labels can severely damage cross-modal similarity learning by reducing paired samples. For MIRFlickr-25k annotations (left), 35% unknown classes can completely exclude all negative pairs. For MS COCO (right), even 20% labels under-annotated can fundamentally remove the negative relationships.", "description": "This figure shows the impact of incomplete labels on the number of different pairs available for cross-modal similarity learning.  In both the MIRFlickr-25k and MS COCO datasets, the percentage of positive, negative, and unknown pairs is plotted against the percentage of unknown labels.  The results demonstrate that as the percentage of unknown labels increases, the number of negative pairs dramatically decreases, making cross-modal similarity learning significantly more challenging.", "section": "1 Introduction"}, {"figure_path": "ferj6WqShv/figures/figures_3_1.jpg", "caption": "Figure 2: Our proposed PCRIL consists of two major stages: prompt contrastive recovery and augmented pairwise similarity learning. The prompt contrastive recovery stage effectively perceives incompleteness via label prompts to learn contrastive matching scores with modal samples, recovering informative semantics. The similarity augmentations further eliminate unknown labels through a complementary blending of samples and recover the scarce negative pairs using an adaptive negative masking strategy.", "description": "This figure illustrates the PCRIL framework, which consists of two main stages: prompt contrastive recovery and augmented pairwise similarity learning.  The first stage uses CLIP to learn contrastive matching scores between anchor label sets and their negative variants, progressively identifying promising positive classes from unknown labels.  This helps to recover informative semantics. The second stage addresses extreme cases of unknown labels and negative pair scarcity through two augmentation strategies: complementary semantic augmentation (mixup of unknown-complementary samples) and adaptive negative masking. These augmentations enhance similarity learning by enriching the pairwise similarity supervision.", "section": "3 Proposed Approach"}, {"figure_path": "ferj6WqShv/figures/figures_3_2.jpg", "caption": "Figure 3: The sorted frequency histogram of unique positive label sets in MIRFlickr-25k samples at 70% known labels. This long-tail distribution induces bias for learning because many rare label combinations in the dataset are associated with limited samples.", "description": "The figure shows the distribution of the frequency of unique positive label sets in the MIRFlickr-25k dataset when 70% of the labels are known.  The x-axis represents the sorted index of label sets, and the y-axis represents the frequency of each label set. The distribution is heavily skewed towards a small number of frequent label sets, indicating a long-tail distribution. This imbalance can negatively affect model training due to limited training samples for less frequent label sets.", "section": "3.2 Prompt Contrastive Recovery"}, {"figure_path": "ferj6WqShv/figures/figures_4_1.jpg", "caption": "Figure 4: An example of the potential label tree search process. Discs represent label sets.", "description": "This figure illustrates the Potential Label Tree Search (PLTS) algorithm used in the PCRIL framework.  It shows a greedy search process for identifying potential positive labels from an unknown set. The algorithm starts with an initial set of known positive labels (K<sub>p</sub>(1)) and iteratively adds unknown labels (c<sub>u</sub>) that maximize the class set score (\u03a6\u00b2(K<sub>p</sub>(\u03c9) U {c<sub>u</sub>})), as represented by the nodes and arrows.  The process continues until the score improvement falls below a threshold. The color-coding helps distinguish between known positives (yellow-gold), unknown labels being considered (green), and rejected unknown labels (red).  Each node represents a subset of labels, and the arrows represent the iterative addition or rejection of unknown labels during the search process.", "section": "3.2 Prompt Contrastive Recovery"}, {"figure_path": "ferj6WqShv/figures/figures_8_1.jpg", "caption": "Figure 5: The recovered classes and scores of 3 case images w.r.t. search iterations. For brevity, we only show top-3 results at all steps. The recursive recovery of potential classes results in successive increases in the set scores.", "description": "This figure visualizes the prompt contrastive recovery process using three example images. Each image shows the initial positive label set (Kp) and the scores obtained for potential labels at different iterations of the potential label tree search (PLTS) algorithm.  The visualization demonstrates that the PLTS algorithm iteratively identifies and adds promising positive classes to the Kp, resulting in an increase in the overall set score with each iteration. This illustrates the algorithm's ability to progressively refine its understanding of the relevant labels for each image, leading to improved label recovery.", "section": "3.2 Prompt Contrastive Recovery"}, {"figure_path": "ferj6WqShv/figures/figures_8_2.jpg", "caption": "Figure 6: Recovery of labels and deterministic pairs. The left 2 subgraphs: the (a) recall and (b) precision of recovered positive classes w.r.t initial epochs of prompt tuning. The right 2 subgraphs: pairwise similarity recovery by (c) complementary semantic augmentation and (d) prompt contrastive recovery respectively, on the Flickr and NUS datasets. 'p.', 'n.', and 'u.' stands for positive, negative, and unknown, respectively. Dashed lines are corresponding results w/o applying our modules.", "description": "This figure demonstrates the effectiveness of the proposed methods in recovering labels and pairwise similarities.  The left two subplots show the recall and precision of recovered positive classes over training epochs, highlighting the improvement achieved by the prompt tuning process. The right two subplots illustrate the recovery of pairwise similarities using complementary semantic augmentation and prompt contrastive recovery, respectively, for both the MIRFlickr-25K and NUS-WIDE datasets. The plots clearly show how the proposed methods address the scarcity of negative pairs, especially when there is a high proportion of unknown labels.  The dashed lines represent the baseline performance without the proposed methods.", "section": "4.4 Model Analysis"}, {"figure_path": "ferj6WqShv/figures/figures_12_1.jpg", "caption": "Figure 2: Our proposed PCRIL consists of two major stages: prompt contrastive recovery and augmented pairwise similarity learning. The prompt contrastive recovery stage effectively perceives incompleteness via label prompts to learn contrastive matching scores with modal samples, recovering informative semantics. The similarity augmentations further eliminate unknown labels through a complementary blending of samples and recover the scarce negative pairs using an adaptive negative masking strategy.", "description": "This figure illustrates the PCRIL framework, which is composed of two main stages: prompt contrastive recovery and augmented pairwise similarity learning. The first stage uses label prompts to identify promising positive classes and to learn contrastive matching scores between the anchor set and negative variants.  A tree search process is then employed to recover potential labels. The second stage addresses issues of significant unknown labels and lack of negative pairs using complementary sampling and adaptive negative masking. This figure shows a diagram that presents the overall workflow of the PCRIL method, illustrating its two major stages and sub-components.", "section": "3 Proposed Approach"}, {"figure_path": "ferj6WqShv/figures/figures_15_1.jpg", "caption": "Figure 8: The heatmap case visualization of the learned prompt net output. In every example: left: untrained prompts; right: trained prompts. Our learned prompts contrastively learn to attend to objects of potential classes compared to the untrained prompts.", "description": "This figure shows three examples of heatmap visualizations that compare the attention mechanisms of untrained versus trained prompt networks. Each example focuses on a different missing label (animal, female, water).  The leftmost image in each set shows the attention map produced by an untrained network; the rightmost image shows the attention map produced after training. The heatmaps highlight which parts of the image the network focuses on when generating its outputs. The trained network is shown to focus more strongly on the parts of the image related to the missing labels, suggesting it has successfully learned to attend to objects of potential classes.", "section": "C.3 Completeness Attention Visualization"}, {"figure_path": "ferj6WqShv/figures/figures_15_2.jpg", "caption": "Figure 9: The t-SNE visualization for the binary codes in the baseline (AN) and our PCRIL. Colors correspond to classes. The compact clusters of semantically similar sample points verify our effective learning of discriminative hash codes.", "description": "This figure compares the t-distributed Stochastic Neighbor Embedding (t-SNE) visualizations of the binary hash codes generated by the baseline method (AN, which uses the assumption that all unknown labels are negative) and the proposed PCRIL method.  Different colors represent different semantic classes.  The PCRIL visualization shows tighter, more distinct clusters of points belonging to the same class, indicating that PCRIL learns more discriminative and effective hash codes that better preserve semantic similarities compared to the baseline.", "section": "C More Experimental Analyses"}]