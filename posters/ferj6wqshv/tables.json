[{"figure_path": "ferj6WqShv/tables/tables_6_1.jpg", "caption": "Table 1: The MAP comprisons on Flickr, NUS, and COCO datasets with state-of-the-art CMH methods by different known ratios (30%, 50%, and 70%). We report performance rises in red compared to the second-best results. *: cited results with their original experiment settings. Our proposed PCRIL significantly outperforms both deep and non-deep CMH methods, verifying the ability to recover efficient similarity learning.", "description": "This table presents a comparison of the mean average precision (mAP) achieved by the proposed PCRIL method and several state-of-the-art cross-modal hashing (CMH) methods on three benchmark datasets (Flickr, NUS-WIDE, and COCO).  The comparison is performed under three different scenarios of incomplete labels, where 30%, 50%, and 70% of the labels are known.  The table highlights the superior performance of PCRIL across all datasets and label completeness levels, demonstrating its effectiveness in recovering efficient similarity learning from incomplete data.", "section": "4 Experiments"}, {"figure_path": "ferj6WqShv/tables/tables_6_2.jpg", "caption": "Table 2: The ablation study results on Flickr, NUS, and COCO datasets. B: Baseline CMH method, IU: ignoring unobserved pair relationships, AN: assuming all unknown pairs to be negative, ANM: adaptive negative masking, PCR: prompt contrastive recovery, and CSA: complementary semantic augmentation.", "description": "This table presents the ablation study results, comparing different components of the proposed PCRIL method. It shows the impact of each component on the overall performance across three datasets: Flickr, NUS, and COCO.  The baseline CMH method (B) is compared with variants that incorporate ignoring unobserved pairs (IU), assuming all unknowns as negatives (AN), using adaptive negative masking (ANM), prompt contrastive recovery (PCR), and finally, combining PCR with complementary semantic augmentation (CSA). The results demonstrate the incremental contribution of each component to the final performance.", "section": "4.3 Ablation Study"}, {"figure_path": "ferj6WqShv/tables/tables_6_3.jpg", "caption": "Table 2: The ablation study results on Flickr, NUS, and COCO datasets. B: Baseline CMH method, IU: ignoring unobserved pair relationships, AN: assuming all unknown pairs to be negative, ANM: adaptive negative masking, PCR: prompt contrastive recovery, and CSA: complementary semantic augmentation.", "description": "This ablation study evaluates the contribution of each component of the proposed PCRIL method on three benchmark datasets (Flickr, NUS, and COCO).  It compares the baseline CMH method with various combinations of techniques: ignoring unobserved pairs (IU), assuming all unknowns as negative (AN), adaptive negative masking (ANM), prompt contrastive recovery (PCR), and complementary semantic augmentation (CSA). The results show the impact of each component on the overall performance of the system, highlighting their individual contributions to improving cross-modal hashing with incomplete labels.", "section": "4.3 Ablation Study"}, {"figure_path": "ferj6WqShv/tables/tables_7_1.jpg", "caption": "Table 4: Prompt construction variants compared on Flickr dataset. The MAP and precisions of recovered positive labels (PRECISION) are reported. Our PCRIL can successfully marry multi-label information with CLIP prior knowledge (compared to Conventional) and yield learned prompts for instance-label matching (compared to Phrasal).", "description": "This table compares the performance of three different prompt construction methods on the Flickr dataset in terms of mean average precision (MAP) and precision of recovered positive labels.  The methods are: 1) Phrasal (handcrafted prompt), 2) Conventional (averaging single-class prompts), and 3) Ours (learnable multi-label prompt). The results demonstrate that the proposed 'Ours' method significantly outperforms the other two methods, indicating its effectiveness in leveraging CLIP's prior knowledge and learning instance-specific prompts for improved label recovery. The table also shows precision results for each method at three different known label ratios (30%, 50%, 70%), highlighting the consistent improvement provided by the 'Ours' method across various data conditions.", "section": "4.4 Model Analysis"}, {"figure_path": "ferj6WqShv/tables/tables_7_2.jpg", "caption": "Table 5: Prompt search variants compared on Flickr and NUS datasets. Compared to single-modal recovery, our proposed PLTS can perform instance-level matching to produce more precise results. The one-step all variant validates the effectiveness of our recursive label recovery in PLTS.", "description": "This table presents a comparison of different prompt search variants for cross-modal hashing on the Flickr and NUS datasets.  The variants are: using only image modality, using only text modality, a one-step approach that doesn't use the recursive search, and the authors' proposed approach (PLTS). The table shows the mean average precision (mAP) and precision scores for each variant, broken down by the percentage of known labels (30%, 50%, 70%). The results demonstrate the superior performance of the PLTS method, particularly in achieving higher precision. This highlights the effectiveness of the recursive label recovery strategy in improving the accuracy of cross-modal retrieval.", "section": "4.4 Model Analysis"}, {"figure_path": "ferj6WqShv/tables/tables_14_1.jpg", "caption": "Table 1: The MAP comprisons on Flickr, NUS, and COCO datasets with state-of-the-art CMH methods by different known ratios (30%, 50%, and 70%). We report performance rises in red compared to the second-best results. *: cited results with their original experiment settings. Our proposed PCRIL significantly outperforms both deep and non-deep CMH methods, verifying the ability to recover efficient similarity learning.", "description": "This table presents a comparison of the mean average precision (mAP) achieved by the proposed PCRIL method and several state-of-the-art cross-modal hashing (CMH) methods on three benchmark datasets (Flickr, NUS-WIDE, and COCO). The comparison is performed under three different scenarios of label completeness (30%, 50%, and 70% known labels).  The table highlights the superior performance of PCRIL in recovering efficient similarity learning, even when dealing with significantly incomplete label information.", "section": "4 Experiments"}, {"figure_path": "ferj6WqShv/tables/tables_14_2.jpg", "caption": "Table 7: The analysis for hyperparameters' impact on the model's performance on the Flickr dataset. Evaluated parameters contain the number of complementary samples (K) for each sample and the value of margin (m) used in prompt contrastive learning and PLTS process.", "description": "This table shows the impact of two hyperparameters, the number of complementary samples (K) and the margin (m), on the model's performance, specifically the mean average precision (mAP) and precision, on the Flickr dataset.  The results help determine optimal values for these hyperparameters within the prompt contrastive learning and PLTS process.", "section": "4.3 Ablation Study"}, {"figure_path": "ferj6WqShv/tables/tables_15_1.jpg", "caption": "Table 8: Comparison results on the IAPR TC-12 dataset.", "description": "This table shows the comparison results of the proposed PCRIL method with existing methods (SSAH and DCMHT) on the IAPR TC-12 dataset for different known ratios (30%, 50%, 70%).  The results are presented as mean average precision (mAP) values for image-to-text (i\u2192t) and text-to-image (t\u2192i) retrieval tasks. It demonstrates the effectiveness of PCRIL in achieving consistent improvements across different known label ratios, particularly when fewer labels are available.", "section": "C More Experimental Analyses"}, {"figure_path": "ferj6WqShv/tables/tables_16_1.jpg", "caption": "Table 1: The MAP comprisons on Flickr, NUS, and COCO datasets with state-of-the-art CMH methods by different known ratios (30%, 50%, and 70%). We report performance rises in red compared to the second-best results. *: cited results with their original experiment settings. Our proposed PCRIL significantly outperforms both deep and non-deep CMH methods, verifying the ability to recover efficient similarity learning.", "description": "This table presents a comparison of the mean average precision (mAP) achieved by various Cross-modal Hashing (CMH) methods on three benchmark datasets (Flickr, NUS-WIDE, and COCO) across three different levels of label completeness (30%, 50%, and 70% known labels).  The results show the performance of the proposed PCRIL method against several state-of-the-art CMH techniques.  Performance improvements are highlighted in red, demonstrating the superiority of PCRIL in recovering efficient similarity learning with incomplete labels.", "section": "4 Experiments"}]