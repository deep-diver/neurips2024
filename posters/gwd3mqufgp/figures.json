[{"figure_path": "gwd3MQufGP/figures/figures_1_1.jpg", "caption": "Figure 1: This work aims to address the problem of semantic keypoint comprehension, which aims to understand keypoints across different task scenarios: (a) Keypoint Semantic Understanding takes the object image and a keypoint prompt (i.e., the position of the target keypoint) as inputs, then generate responses that interpret keypoint semantics; (b) Visual Prompt-based Keypoint Detection takes a query image and a support image with a keypoint prompt as inputs and then outputs the corresponding keypoint positions and semantics of the query image; (c) Textual Prompt-based Keypoint Detection utilizes detailed descriptions of keypoints through extensive text, to perform more generalizable keypoint detection.", "description": "This figure illustrates the three tasks of semantic keypoint comprehension addressed in the paper.  (a) Keypoint Semantic Understanding focuses on understanding the semantics of a keypoint given its location in an image. (b) Visual Prompt-based Keypoint Detection involves detecting keypoints in a query image based on information from a support image and its corresponding keypoints. (c) Textual Prompt-based Keypoint Detection utilizes textual descriptions of keypoints to achieve more generalized keypoint detection.", "section": "1 Introduction"}, {"figure_path": "gwd3MQufGP/figures/figures_3_1.jpg", "caption": "Figure 2: We introduce KptLLM, a unified framework designed to address three tasks of semantic keypoint comprehension: \u2460 Keypoint Semantic Undertanding, which processes a support image I<sub>s</sub> and a support keypoint prompt x to generate responses that interpret the semantic information of the specified keypoint; \u2461 Visual Prompt-based Keypoint Detection aims to detect the corresponding keypoint in the query image I<sub>q</sub> based on the understanding of the support keypoint prompt; \u2462 Textual Prompt-based Keypoint Detection leverages textual keypoint descriptions to directly infer the corresponding keypoint positions in the query image.", "description": "KptLLM is a unified framework designed to solve three tasks: Keypoint Semantic Understanding, Visual Prompt-based Keypoint Detection, and Textual Prompt-based Keypoint Detection.  The model takes as input query and support images, a support keypoint prompt (position of the keypoint in the support image), and textual instructions from the user. The visual encoder processes both the query and support images.  The prompt encoder handles the support keypoint prompt location data. A prompt feature extractor integrates the support keypoint and image features. The pre-trained LLM then uses query image features, prompt-oriented features, and textual instructions to generate the textual semantic description of the keypoint and the location of the keypoint in the query image. The chain of thought is used to identify the semantic meaning of the keypoint and then locate its position. ", "section": "3 Methodology"}, {"figure_path": "gwd3MQufGP/figures/figures_7_1.jpg", "caption": "Figure 3: Using the same support image with support keypoints, our model could effectively detect different query images with various poses, object appearances, and environments.", "description": "This figure shows the results of visual prompt-based keypoint detection.  The leftmost image is a support image which is used as a reference by the model for keypoint detection. The model receives this image and the keypoints defined on it. Then, the model uses this information to detect the keypoints on query images, which are shown in the rest of the image.  The results show that the model can successfully detect keypoints in various query images with differences in poses, appearance, and environmental conditions. This demonstrates the generalizability of the model.", "section": "4.3 Visual Prompt-based Keypoint Detection"}]