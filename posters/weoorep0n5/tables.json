[{"figure_path": "WEoOreP0n5/tables/tables_4_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table compares the performance of the proposed Data Adaptive Pathway Discovery (DAPD) method against several baseline methods on four different continuous control tasks from the MuJoCo simulator.  The comparison is made at 95% sparsity, meaning that only 5% of the network's parameters are used. The average episodic return is calculated over the last 10 evaluation episodes across 5 different random seeds, after 1 million training steps. The baseline methods include standard SAC with a dense network (SAC-Dense), RiGL, and RLx2.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_5_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table presents a performance comparison of the proposed Data Adaptive Pathway Discovery (DAPD) method against several baseline algorithms on four different single-task continuous control environments from the MuJoCo simulator.  The comparison focuses on the average episodic return achieved over the final 10 evaluation episodes after 1 million training steps, with results averaged across 5 different random seeds.  The sparsity level is fixed at 95% for all methods except for the SAC-Dense baseline (which uses the full network).", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_6_1.jpg", "caption": "Table 2: We compare the success rate of SAC-DAPD on MetaWorld 10 (MT10). We also show the reduced parameter complexity, FLOP and hence the energy consumption of SAC-DAPD compared to other baseline methods.", "description": "This table compares the performance of SAC-DAPD with other baselines on the MetaWorld 10 benchmark.  It shows the success rate (percentage of tasks successfully completed), the number of parameters, the number of floating point operations (FLOPs), and the estimated energy consumption for each method.  The results highlight the efficiency of SAC-DAPD in terms of reduced parameters and energy consumption while maintaining competitive performance compared to other methods.", "section": "4.2 Scenario 2: Online Multi-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_8_1.jpg", "caption": "Table 3: Performance comparison in MetaWorld offline. We compare the final success rate (mean and std over 10 seeds) of pathways discovery (PD) on MT10 tasks with Offline-MT and Offline-MHMT baselines on offline RL algorithms. We also show the reduced parameter complexity (x times) of PD compared to other baseline method", "description": "This table compares the performance of the proposed Data Adaptive Pathway Discovery (DAPD) method against several baselines on the MetaWorld MT10 benchmark for offline reinforcement learning.  It shows the success rate (mean and standard deviation over 10 seeds) for each method. It also highlights the reduced parameter counts, FLOPs (floating-point operations), and energy consumption of DAPD compared to the baselines, demonstrating its efficiency.", "section": "4.2 Scenario 2: Online Multi-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_23_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table presents a comparison of the proposed Data Adaptive Pathway Discovery (DAPD) method against several baseline algorithms in single-task reinforcement learning experiments using MuJoCo continuous control environments.  The comparison focuses on the average episodic return achieved over the last 10 evaluation episodes, averaged over 5 different random seeds, after 1 million training steps.  The sparsity level for all methods is fixed at 95%, meaning only 5% of the network parameters are used. The baselines include a dense network (SAC-Dense), RiGL, and RLx2.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_25_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table presents a comparison of the proposed Data Adaptive Pathway Discovery (DAPD) method with several baseline algorithms on four continuous control tasks from the MuJoCo simulator.  The comparison focuses on performance at 95% sparsity (meaning only 5% of the network parameters are used). The average episodic return over the final 10 evaluations across 5 different random seeds is reported for each algorithm and task after 1 million training steps.  This allows for an assessment of the relative performance of DAPD compared to other approaches at a high level of sparsity.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_28_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table presents a comparison of the proposed Data Adaptive Pathway Discovery (DAPD) method with several baseline algorithms for single-task reinforcement learning using MuJoCo continuous control environments.  The performance metric used is the average episodic return over the final 10 evaluation episodes, averaged across 5 different random seeds. The table shows the results for four different environments (HalfCheetah-v2, Walker2d-v2, Hopper-v2, and Ant-v2) and demonstrates the relative performance of DAPD compared to baseline methods (SAC-Dense, RiGL, and RLx2) at a high sparsity level of 95%.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_28_2.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table presents a comparison of the proposed Data Adaptive Pathway Discovery (DAPD) method against several baseline methods for single-task reinforcement learning using continuous control environments from MuJoCo.  The comparison focuses on the average episodic return achieved after 1 million training steps, considering the last 10 evaluations and averaging results across 5 different random seeds.  The sparsity level is fixed at 95% for all methods except the \"SAC-Dense\" baseline, which represents a fully connected network.  This table highlights the performance improvement of DAPD over other sparse training methods and also shows its comparative performance versus a dense network.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_29_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table compares the performance of the proposed Data Adaptive Pathway Discovery (DAPD) method with several baseline methods on four different MuJoCo continuous control tasks.  The comparison is done at 95% sparsity, meaning that only 5% of the network parameters are used. The table shows the average episodic return over the last 10 evaluation episodes, averaged over 5 different random seeds, after 1 million training steps. The baselines include a standard dense SAC network (SAC-Dense), RiGL, and RLx2.  The results demonstrate the effectiveness of DAPD in achieving comparable or better performance than the dense model while using significantly fewer parameters.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_30_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table presents the results of single-task experiments conducted using MuJoCo continuous control environments.  The experiments compare the performance of the proposed Data Adaptive Pathway Discovery (DAPD) method against several baseline algorithms at 95% sparsity. The metric used for comparison is the average episodic return over the final 10 evaluations, averaged across 5 different random seeds. Each experiment was run for 1 million training steps.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_31_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table presents a comparison of the proposed Data Adaptive Pathway Discovery (DAPD) method against several baseline methods in single-task reinforcement learning experiments using MuJoCo continuous control environments.  The comparison focuses on the average episodic return achieved after 1 million training steps across 5 different random seeds, considering only the last 10 evaluation episodes for each seed.  The table highlights DAPD's performance relative to a dense network (SAC-Dense) and two existing sparse training methods (RiGL and Rlx2) at a sparsity level of 95%.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_32_1.jpg", "caption": "Table 3: Performance comparison in MetaWorld offline. We compare the final success rate (mean and std over 10 seeds) of pathways discovery (PD) on MT10 tasks with Offline-MT and Offline-MHMT baselines on offline RL algorithms. We also show the reduced parameter complexity (x times) of PD compared to other baseline method", "description": "This table presents the results of offline multi-task reinforcement learning experiments on the MetaWorld MT10 benchmark.  It compares the success rate (averaged over 10 seeds) of the proposed Data Adaptive Pathway Discovery (DAPD) method against two baseline methods: Offline MT (a single dense network for multiple tasks) and Offline MHMT (multiple independent heads on a dense network).  It also shows the significant reduction in the number of parameters achieved by DAPD compared to the baselines.", "section": "4.3 Scenario 3: Offline Multi-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_33_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table compares the average episodic return of the DAPD method with other baselines (SAC-Dense, RiGL, and Rlx2) across four different MuJoCo continuous control tasks (HalfCheetah-v2, Walker2d-v2, Hopper-v2, and Ant-v2).  All methods were tested at a 95% sparsity level.  The results are averaged over the last 10 evaluation episodes and across 5 different random seeds, after training for 1 million steps.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/tables/tables_34_1.jpg", "caption": "Table 1: Performance comparison of DAPD with various baselines at 95% sparsity in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.", "description": "This table compares the average episodic return of the DAPD method with several baseline methods (SAC-Dense, RiGL, and Rlx2) across four different MuJoCo continuous control tasks (HalfCheetah-v2, Walker2d-v2, Hopper-v2, and Ant-v2).  The experiments were conducted with a network sparsity of 95%, meaning only 5% of the network parameters were used. The results are averaged over the last 10 evaluations and 5 random seeds after 1 million training steps.", "section": "4.1 Scenario 1: Online Single-Task RL"}]