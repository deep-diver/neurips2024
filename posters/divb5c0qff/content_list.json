[{"type": "text", "text": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kailai Yang1 Zhiwei Liu1 Qianqian Xie2\u2217 Jimin Huang2 Tianlin Zhang1 Sophia Ananiadou1 ", "page_idx": 0}, {"type": "text", "text": "1 The University of Manchester 2 The Fin AI {kailai.yang,zhiwei.liu,sophia.ananiadou}@manchester.ac.uk {xqq.sincere,zhangtianlin668}@gmail.com;jimin@chancefocus.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model parameters, which require high-cost repetition of their alignment algorithms for each new policy model, and they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), the first policy-agnostic and generalizable method for multi-objective preference alignment. MetaAligner models multi-objective alignment into three stages: (1) dynamic objectives reformulation algorithm reorganizes traditional alignment datasets to supervise the model on performing flexible alignment across different objectives; (2) conditional weak-to-strong correction paradigm aligns the weak outputs of fixed policy models to approach strong outputs with higher preferences in the corresponding alignment objectives, enabling plug-and-play inferences on any policy models, which significantly reduces training costs and facilitates alignment on close-source policy models; (3) generalizable inference method flexibly adjusts target objectives by updating their text descriptions in the prompts, facilitating generalizable alignment to unseen objectives. Experimental results show that MetaAligner achieves significant and balanced improvements in multiobjective alignments on 10 state-of-the-art policy models, and saves up to $93.63\\%$ of GPU training hours compared to previous alignment methods. The model also effectively aligns unseen objectives, marking the first step towards generalizable multi-objective preference alignment. This project is open-sourced here. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent advancements in large language models (LLMs) have focused on generating high-quality responses that align with human expectations and values. At the final stage of alignment, LLMs are supervised on human preference data via reinforcement learning from human feedback (RLHF) [40, 22, 27], where a proxy, directly trained on human preferences data, is leveraged to provide scalar rewards for reinforcement learning (RL) on the target model [22]. ", "page_idx": 0}, {"type": "text", "text": "However, human expectations and values include a broad spectrum of heterogeneous and multidimensional objectives, which makes scalar supervisions inefficient for aligning diverse and inclusive human preferences [3, 24]. These drawbacks motivate further exploration into multiobjective alignment algorithms. Some intuitive methods extend RLHF into multi-objective RLHF (MORLHF) [26, 19, 24]. Due to its substantial computational cost [19, 24] and the unstable nature of the proximal policy optimization (PPO) [25, 15, 23] algorithm, other methods seek to bypass the RL paradigm with multi-objective direct preference optimization (MODPO) [39, 10] or supervised fine-tuning (SFT)-based methods [35, 10], which customized prompting strategies to incorporate multiple reward values into queries explicitly. ", "page_idx": 0}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/fddbf2239f1ef6e408cf8c5edf00eb693edd83158b178d759449def535204d02.jpg", "table_caption": ["Table 1: Comparisons between previous alignment methods and MetaAligner on different features. \"Policy-Agnostic Alignment\" means the alignment algorithm is independent of the target policy model parameters, and \"Generalizability\" denotes zero-shot alignment capability on unseen objectives. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The above methods for multi-objective alignment bear one commonality: the dependence on the policy model\u2019s parameters. This paradigm inevitably brings two key limitations: (1) they require repetition of their high-cost alignment algorithms for each newly-introduced policy model, which is incompatible with the increasing sizes and fast iteration of current foundation models [1, 30, 6, 29]; (2) all target models are statically aligned on pre-determined (e.g. \"Helpful\", \"Harmless\", \"Honest\" [39, 10]) objectives, with currently no efforts in expanding and evaluating their capabilities on unseen objectives. This ignorance leads to poor generalizability of existing multi-objective alignment methods. ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose Meta-Objective Aligner (MetaAligner), the first policy-agnostic and generalizable method for multi-objective preference alignment. MetaAligner models multi-objective alignment into three stages: (1) the dynamic objectives reformulation algorithm reorganizes traditional alignment datasets into dynamic-objective alignment datasets, training MetaAligner to perform flexible alignment across different objectives. It achieves this by incorporating and combining text descriptions of various alignment objectives in a prompt-based manner; (2) the conditional weak-tostrong correction paradigm aligns the weak outputs of policy models to approach strong outputs with higher preferences in the corresponding alignment objectives. During training, MetaAligner is stacked onto policy models to perform objective-aware corrections, where parameters of the policy model are fixed and MetaAligner is optimized with an SFT-based three-step training process: warming up, equal-preference alignment, and contrastive-preference alignment. This paradigm enables MetaAligner to perform plug-and-play inferences on any policy models even without access to their parameters, which significantly reduces training costs and facilitates alignment on close-source LLMs; (3) the generalizable inference method flexibly adjusts target objectives by updating their text descriptions in the prompts. This method can also adapt MetaAligner to unseen objectives and achieve new alignment strategies via in-context learning [14], a new feature with rare previous exploration in alignment of language models. The number of aligned objectives also becomes expandable, theoretically leading to unlimited simultaneous alignment objectives. Table 1 compares key features between MetaAligner and previous methods. As shown, conditional weak-to-strong correction of MetaAligner extends Aligner [12] to multi-objective alignment scenarios, which are not directly solvable by Aligner itself. MetaAligner is also the first multi-objective alignment method to achieve policy-agnostic alignment and generalization to unseen objectives, two key advantages over previous methods such as MORLHF, MODPO, and SFT-based methods. ", "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions are: (1) we propose MetaAligner, the first policy-agnostic method for multi-objective preference alignment. It performs multi-objective alignment efficiently, without tuning the policy models or accessing their parameters. Experimental results show that MetaAligner outperforms previous alignment methods and saves up to $93.63\\%$ of GPU training hours; (2) we utilize MetaAligner to exert zero-shot preference alignment for unseen objectives. To our knowledge, this work marks the first attempt at generalizable multi-objective preference alignment. Experimental results show that MetaAligner can simultaneously perform effective alignment for six unseen objectives while maintaining performance on aligned objectives; (3) We examine MetaAligner on three preference alignment datasets. Experimental results show that MetaAligner improves win rates on multiple objectives across 10 policy models, substantially enhancing responses of state-of-the-art foundation models such as GPT-3.5-Turbo [21] and Claude-3 [2]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "dIVb5C0QFf/tmp/a285a75228e9d01d4d458d52487e0446552bedf25d9880d57e0fc53d94521dc0.jpg", "img_caption": ["Figure 1: Illustrations of Meta-Objective Aligner, which follows a three-stage paradigm. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Multi-Objective Alignment of Language Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In real-world scenarios, human expectations of high-quality responses from AI agents involve considerable variability, with complex interplays such as contradiction (e.g. \"Helpful\" and \"Harmless\") [10, 35] and dependence (e.g. \"Correct\" and \"Informative\") [34]. Multi-objective preference alignment tackles this challenge by aiming to optimize multiple objectives simultaneously. For each query-response pair, the reward vector is formalized as: $\\mathbf{R}(q,y)=[r_{1}(q,y),...,r_{N}(q,y)]^{\\mathrm{T}}$ , where $q,y$ denote a query and a corresponding response, $r_{i}$ denotes the reward values for $i$ -th objective, which is defined, in most cases of preference alignment, under the Bradley-Terry [5] model of preferences. Specifically, for the same prompt $q$ and two responses $(y_{1},y_{2})$ under data distribution $\\mathcal{D}$ , the model assumes: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{\\cal D}(y_{1}\\succ y_{2}|q,i)=\\sigma(r_{i}(q,y_{1})-r_{i}(q,y_{2}))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\sigma$ denotes the logistic function and $P_{D}(y_{1}\\succ y_{2})$ denotes the probability that $y_{1}$ is preferred against $y_{2}$ . MORLHF aims to achieve Pareto optimal among objectives, where the policy model is optimized to maximize a linear scalarization of multiple rewards [26, 19] with a KL-divergence regularization: ", "page_idx": 2}, {"type": "equation", "text": "$$\na r g m a x\\mathbb{E}_{q\\sim\\mathcal{D},y\\sim\\pi_{\\phi}(y\\mid q)}\\left[\\boldsymbol{\\omega}^{\\mathrm{T}}\\mathbf{R}(q,y)\\right]-\\beta\\mathbb{D}_{K L}\\left[\\pi_{\\phi}(y\\mid q)\\right\\Vert\\pi_{r e f}(y\\mid q)\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pi_{\\phi}$ denotes the aligned policy model parameterized by $\\phi,\\pi_{r e f}$ denotes the reference policy model, $\\omega\\,=\\,[\\omega_{1},...,\\omega_{N}]\\;s.t.\\sum_{i=1}^{N}\\omega_{i}\\,=\\,1,\\omega_{i}\\,{\\ge}\\,0$ is the pre-determined heuristic target preference vector. Another paradig m directly built alignment between multiple reward values and their corresponding response by minimizing an SFT loss for the policy model: ", "page_idx": 2}, {"type": "equation", "text": "$$\na r g m i n-\\mathbb{E}_{(q,y)\\sim\\mathcal{D}}\\left[l o g\\,\\pi_{\\phi}(y|q,\\mathbf{R}(q,y))\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where objectives and their corresponding reward values are described with text markers and combined into queries with a static prompting template. Compared to MORLHF, SFT-based multi-objective alignment is proven more cost-efficient and training-stable [35, 10]. ", "page_idx": 2}, {"type": "text", "text": "3 Meta-Objective Aligner ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Existing methods for multi-objective alignment generally face challenges in increasing training costs with new policy models and generalization to unseen objectives. To tackle these challenges, we introduce MetaAligner, which follows a three-stage paradigm: (1) dynamic objectives reformulation for building dynamic multi-objective datasets; (2) conditional weak-to-strong correction for model training; (3) generalizable inference for multi-objective alignment. The paradigm is illustrated in Figure 1. ", "page_idx": 2}, {"type": "text", "text": "3.1 Dynamic Objectives Reformulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose a dynamic objectives reformulation algorithm to construct a dynamic multi-objective dataset, which triggers MetaAligner\u2019s ability for flexible adjustment of alignment objectives. Specifically, any typical multi-objective preference alignment dataset $\\mathcal{D}_{m}$ with $m$ samples and $N$ objectives can be re-organized as $\\{q_{i},y_{i1},y_{i2},P_{i}\\}_{i=1}^{m}$ , where $P_{i}=[p_{i1},...,p_{i N}]^{\\mathrm{T}}$ and $p_{i j}\\,\\in\\,\\{\\succ,\\prec,\\equiv\\}$ indicates the preference on $j$ -th objective. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Dynamic objectives reformulation.   \nRequire: Raw dataset $D_{m}:\\{q_{i},y_{i1},y_{i2},P_{i}\\}_{i=1}^{m}$ Objective text descriptions: $[\\langle d_{1}\\rangle\\,,...,\\langle d_{N}\\rangle]$ ; Prompting template: $\\boldsymbol{\\mathcal{T}}(\\boldsymbol{q},\\boldsymbol{y},\\dot{\\boldsymbol{O}},t)$   \nEnsure: Contrastive subset $\\mathcal{D}_{c}$ ; Equal subset $\\mathcal{D}_{e}$ . 1: $\\mathcal{D}_{c}\\gets\\emptyset$ , $\\mathcal{D}_{e}\\gets\\mathcal{D}$ $\\triangleright$ Initialize the 2 subsets.   \n2: for $i\\in\\{1,...,m\\}$ do $\\triangleright$ Loop on instances. 3: $O_{\\succ}\\leftarrow\\emptyset,O_{\\prec}\\leftarrow\\emptyset,O_{\\equiv}\\leftarrow\\emptyset$   \n4: for $j\\in\\{1,...,N\\}$ do   \n5: if $p_{i j}$ is $\\succ$ then $\\triangleright$ Collect the objectives where $y_{i1}$ outperforms $y_{i2}$ .   \n6: $\\mathcal{O}_{\\succ}\\leftarrow\\mathcal{O}_{\\succ}\\cup\\{\\langle d_{j}\\rangle\\}$ 7: else if $p_{i j}$ is $\\prec$ then $\\triangleright$ Collect the objectives where $y_{i2}$ outperforms $y_{i1}$ .   \n8: $\\mathcal{O}_{\\prec}\\leftarrow\\mathcal{O}_{\\prec}\\cup\\{\\langle d_{j}\\rangle\\}$   \n9: else \u25b7Collect the objectives where y1 and $y_{2}$ performs equally.   \n10: $\\mathcal{O}_{\\equiv}\\leftarrow\\dot{\\mathcal{O}}_{\\equiv}\\dot{\\cup}\\,\\{\\langle d_{j}\\rangle\\}$   \n11: end if   \n12: end for   \n13: if $O_{\\succ}\\neq\\emptyset$ then\u25b7Build the training pairs where $y_{i1}$ is used as the target.   \n14: 15: $\\begin{array}{r l}&{\\bar{t}\\leftarrow b e t t e r}\\\\ &{\\mathcal{O}_{\\succ}\\leftarrow r a n d o m\\_s h u f\\!f\\!e(\\mathcal{O}_{\\succ})}\\\\ &{\\mathcal{D}_{c}\\leftarrow\\mathcal{D}_{c}\\cup\\{(\\mathcal{T}(q_{i},y_{i2},\\mathcal{O}_{\\succ},t),y_{i1})\\}}\\end{array}$   \n16:   \n17: end if   \n18: if $O_{\\prec}\\neq\\emptyset$ then\u25b7Build the training pairs where $y_{i2}$ is used as the target.   \n19: $\\begin{array}{l}{\\overline{{t}}\\leftarrow b e t t e r}\\\\ {\\mathcal{O}_{\\prec}\\leftarrow r a n d o m\\_s h u f\\!f\\!e(\\mathcal{O}_{\\prec})}\\\\ {\\mathcal{D}_{c}\\leftarrow\\mathcal{D}_{c}\\cup\\{(\\mathcal{T}(q_{i},y_{i1},\\mathcal{O}_{\\prec},t),y_{i2})\\}}\\end{array}$   \n20:   \n21:   \n22: end if   \n23: if $O_{\\equiv}\\neq\\emptyset$ then\u25b7Build equally-preferred training pairs.   \n24: $\\begin{array}{r l}&{\\dot{t}\\gets e q u a l}\\\\ &{\\mathcal{O}_{\\equiv}\\gets r a n d o m\\textunderscore s h u f f l e(\\mathcal{O}_{\\equiv})}\\\\ &{\\mathcal{D}_{e}\\gets\\mathcal{D}_{e}\\cup\\{(\\mathcal{T}(q_{i},y_{i2},\\mathcal{O}_{\\equiv},t),y_{i1})\\}}\\end{array}$   \n25:   \n26:   \n27: end if   \n28: end for ", "page_idx": 3}, {"type": "text", "text": "We define a text description for each objective: $[\\langle d_{1}\\rangle\\,,...,\\langle d_{N}\\rangle]$ , where $\\langle d_{j}\\rangle$ denotes the natural language description for $j$ -th objective. Some examples of such descriptions are in Figure 1 and a full list is in Appendix C. With a pre-defined prompting template $\\tau$ , we build a contrastive subset $\\mathcal{D}_{c}$ and another equal subset $\\mathcal{D}_{e}$ from $\\mathcal{D}_{m}$ . $\\mathcal{D}_{c}$ includes all contrastive response pairs where $p\\in\\{\\succ,\\prec\\}$ and $\\mathcal{D}_{e}$ includes all equal response pairs where $p\\in\\{\\equiv\\}$ . For example, we utilize the following template $\\tau(q,y,\\mathcal{O},t)$ in building for the IMHI [34] dataset: ", "page_idx": 3}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/8fcdcc83aadde1347b6b27f33dfde6945682ae3d3cdbab349a1a6d5a79c06604.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "where $q$ denotes the query, $y$ denotes a corresponding response, $\\scriptscriptstyle\\mathcal{O}$ denotes the concatenation of text descriptions for the target objectives, and $t\\in\\{e q u a l,\\bar{b e}t t e r\\}$ depends on the current building subset. Details of the dynamic objectives reformulation algorithm are described in Algorithm 1. For an instance within the processing loop (line 2): $\\{q,y_{1},y_{2},P\\}$ , the algorithm performs a two-step reformulation: (1) collect the sets $\\mathcal{O}_{\\succ}$ , $O_{\\prec}$ , $\\mathcal{O}_{\\equiv}$ that includes objectives where $y_{1}$ outperforms $y_{2},y_{2}$ outperforms $y_{1}$ , and both perform equally (lines 3-12); (2) for each objective set $\\scriptscriptstyle\\mathcal{O}$ , we randomly shuffle the objectives to further trigger the model\u2019s flexible alignment ability, and build query-response pairs based on the corresponding prompting template $\\tau$ (line 13-27). All prompting templates and examples of the algorithm are presented in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "Training on dynamic multi-objective datasets provides three key advantages: (1) instancelevel alternation of the target objectives during training enables MetaAligner to perform flexible ", "page_idx": 3}, {"type": "text", "text": "alignment under different combinations of objectives; (2) mutual alignment between the same response pairs on different objectives fully leverages the supervision information in the preference vectors. (3) the reward-free alignment method (no explicit preference values required) avoids complicated preference-to-reward mapping [35] process in previous SFT-based multi-objective alignment methods. ", "page_idx": 3}, {"type": "text", "text": "3.2 Conditional Weak-to-Strong Correction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Based on the dynamic multi-objective training datasets, we train MetaAligner in a conditional weakto-strong correction manner, which follows an SFT-based training objective and a three-step training paradigm. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Training Objective Derivation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "MetaAligner is a standard conditional seq-to-seq model on top of the original policy model $\\pi_{\\phi}$ , which re-distributes the policy model output $y_{0}$ considering objectives $\\scriptscriptstyle\\mathcal{O}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi^{*}(y|q)=\\delta_{\\theta}(y|\\mathcal{T}(q,y_{0},\\mathcal{O},t))\\pi_{\\phi}(y_{0}|q)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\delta_{\\theta}$ denotes the MetaAligner module parameterized by $\\theta,\\,t$ depends on the training dataset. Conditional weak-to-strong correction directly trains MetaAligner to align the weak policy model output $y_{0}$ to the strong target output $y$ , which has higher preference values in corresponding objectives $\\scriptscriptstyle\\mathcal{O}$ . We have the standard cross-entropy loss as the training objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta,\\phi}{a r g m i n}\\,\\mathcal{L}\\big(\\theta,\\phi;\\mathcal{D}\\big)=-\\mathbb{E}_{(q,y,\\mathcal{O})\\sim\\mathcal{D}}\\left[l o g\\,\\pi^{*}(y|q)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-\\mathbb{E}_{(q,y,\\mathcal{O})\\sim\\mathcal{D}}\\left[l o g\\,\\delta_{\\theta}(y|\\mathcal{T}(q,y_{0},\\mathcal{O},t))\\right]-\\mathbb{E}_{q\\sim\\mathcal{D}}\\left[l o g\\,\\pi_{\\phi}(y_{0}|q)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We fix the parameters of the policy model, thus excluding $\\phi$ from the weight update process. In practice, we use the dynamic multi-objective dataset for supervision, where the weak response in each query-response pair is directly leveraged as samples $y_{0}$ from unknown policy models. Therefore, we eliminate the second term in Eqn. 5 and simplify the training objective as: ", "page_idx": 4}, {"type": "equation", "text": "$$\na r g m i n-\\mathbb{E}_{(q,y_{0},y,\\mathcal{O})\\sim\\mathcal{D}}\\left[l o g\\,\\delta_{\\theta}(y|\\mathcal{T}(q,y_{0},\\mathcal{O},t))\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The above action poses two advantages: (1) the computation resources required for MetaAligner training is detached from policy model size, which enables policy-agnostic and cost-efficient alignment for large policy models; (2) MetaAligner works only via outputs from the policy models, which allows training and inference for alignment on close-source policy models [1, 21, 2]. ", "page_idx": 4}, {"type": "text", "text": "3.2.2 Three-Step Model Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In practice, we utilize an LLM as the base model for MetaAligner, which provides domain knowledge and strong reasoning ability to support the conditional weak-to-strong correction process. We propose a three-step paradigm based on the objective function in Eqn. 6: (1) Warming up. This stage trains the model in identical response pairs with a warm-up subset, a prelude proven effective in residual correction strategies [11, 12]. We randomly sample a subset of the equal subset $\\mathcal{D}_{e}$ as the warm-up subset, but set an identical target response for each instance; (2) Equal-preference alignment. Due to the contrastive nature of their learning paradigm, most previous preference alignment works focus on modeling the residuals between response pairs and ignore the equalpreference response pairs. However, equal preferences are common in many scenarios [34, 7] and enclose useful information such as the principle components of preference modeling regarding each objective. Based on these intuitions, we introduce a novel equal-preference alignment step to fine-tune the warmed-up MetaAligner on the equal subset $\\mathcal{D}_{e}$ ; (3) Contrastive-preference alignment. This stage fine-tunes the MetaAligner on the contrastive preference subset $\\mathcal{D}_{c}$ , which instructs the model to perform conditional weak-to-strong correction on the specified objectives. ", "page_idx": 4}, {"type": "text", "text": "3.3 Generalizable Inference ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "During inference, MetaAligner achieves alignment following the sampling process as in Eqn. 4, where unaligned outputs, sampled from the target policy model, are used as the input for conditional weakto-strong correction. With the prompting-based paradigm, the target objectives for MetaAligner also become expandable and generalizable, a key advantage over previous alignment methods [39, 35, 10]. The generalizability is two-fold: Firstly, users can manipulate the target objectives by adjusting combinations of text descriptions in the objective set $\\scriptscriptstyle\\mathcal{O}$ . For example, in alignment with objectives 1, 3, and 4, we can flexibly shuffle the corresponding descriptions $\\langle d_{1}\\rangle$ , $\\langle\\bar{d_{3}_{}}\\rangle$ , and $\\langle d_{4}\\rangle$ as follows: ${\\mathcal{O}}=\\langle d_{3}\\rangle;\\langle d_{1}\\rangle;\\langle d_{4}\\rangle$ . Secondly, the prompt-based objectives statement enables flexible adjustment of text descriptions for existing objectives and injections of unseen objectives. Following the last example, we have two unseen alignment objectives 5: $\\langle d_{5}^{*}\\rangle$ and 6: $\\langle d_{6}^{*}\\rangle$ , and an updated text description $\\langle d_{3}\\rangle$ for aligned objective 3. We can perform zero-shot alignment on the new objectives by adjusting $\\scriptscriptstyle\\mathcal{O}$ as follows: $\\mathcal{O}^{*}=\\langle d_{3}\\rangle;\\langle d_{1}\\rangle;\\langle d_{4}\\rangle;\\langle d_{5}^{*}\\rangle;\\langle d_{6}^{*}\\rangle$ . This simple pattern can theoretically lead to unlimited simultaneous alignment objectives. We expect MetaAligner to make generalizable weak-to-strong corrections under these unseen conditions via its in-context learning ability. This advancement marks a new exploration into generalizable multi-objective preference alignment. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Datasets. We transfer the following three alignment datasets into dynamic multi-objective datasets: (1) HH-RLHF [3]: a large-scale dataset with 160K prompts and corresponding response pairs. ", "page_idx": 4}, {"type": "text", "text": "We follow Yang et al. [35] and use open-sourced reward models on three objectives: \"Harmless\", \"Helpful\", and \"Humor\" to score the responses; (2) UltraFeedback [7]: a multi-aspect alignment dataset with 64K prompts with preferences obtained from GPT-4, including \"Instruction following\", \"Honest\", \"Truthful\", and \"Helpful\" objectives; (3) IMHI: we create an alignment dataset on the IMHI dataset [34] targeting interpretable mental health analysis. We invite domain experts to label 7.2K response pairs considering 3 objectives: \"Correct\", \"Informative\", and \"Professional\". Figure 2 shows the objective distributions on two datasets. The objectives display balanced overall distributions across objective set sizes, training MetaAligner to adjust targets dynamically. Most objectives also cover considerable proportions in each column category, alleviating label imbalance problems. ", "page_idx": 5}, {"type": "image", "img_path": "dIVb5C0QFf/tmp/c843370bb0f458fa3af1c4e52ad94683c653e9b23403927d9cb5a56eb891f210.jpg", "img_caption": ["Figure 2: Heatmaps of the objective distributions. The columns categorize samples according to the sizes of their objective set. For the lines, \"Overall\" shows their distributions in the training data. Other lines show objective-wise distributions across different categories in the columns. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Models. We train MetaAligner-(1.1B, 7B, 13B) models based on TinyLLaMA-1.1B [37] and LLaMA2-(7B, 13B) [30] foundation models. We utilize MetaAligner to perform multi-objective alignment on the following open-source policy models: LLaMA2- Chat-(7B,13B,70B) [30], Gemma-instruct(2B,7B) [29], and Vicuna-(7B, 13B, 33B) [6]. We also align two advanced close-source foundation models: GPT-3.5-Turbo [21] and Claude-3-Sonnet [2], where model parameters are inaccessible. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metric. On each objective, we quantify the alignment performance of model outputs by comparing their win rates against the ground-truth response provided by the benchmark datasets. Considering the large amounts of ", "page_idx": 5}, {"type": "text", "text": "test samples, we leverage GPT-4 [1], a widely utilized evaluation tool in previous works [10, 28, 18], to perform the judgments. Each target response, ground-truth response, query, and evaluated objectives are provided via prompt engineering. GPT-4 is required to compare and select the response with higher alignment on the specified objective or indicate a tied performance of the two responses. ", "page_idx": 5}, {"type": "text", "text": "More details about the training process, model cards, dataset statistics, IMHI dataset annotation, and evaluation settings are presented in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "4.2 Overall Performance ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "MetaAligner-(1.1B, 7B, 13B) performance on 3 alignment datasets are shown in Table 2. According to the results, the MetaAligner models achieve substantial improvement for most objectives and policy models. For example, on UltraFeedback, there is an average of $11.47\\%$ advantage for MetaAligner1.1B on \"Honest\", $34.39\\%$ for MetaAligner-7B, and $43.79\\%$ for MetaAligner-13B. These results show the general effectiveness of MetaAligner on various upstream models and the feasibility of plugand-play multi-objective alignment. On the mental health analysis benchmark IMHI, MetaAligner models also show remarkable win rates on all objectives, proving their effectiveness in performing multi-objective alignment in domain knowledge-intense scenarios. We further evaluate MetaAligner on each IMHI sub-task and the results are shown in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "From the policy model scale perspective, MetaAligner provides successful alignments to open-source models with sizes ranging from 2B to 70B, significantly extending the size of MetaAligner itself. In the extreme case, MetaAligner-1.1B advances the win rates of LLaMA2-Chat-70B outputs, a policy model with $63\\times$ more parameters, by an average of $12.19\\%$ on HH-RLHF, $13.08\\%$ on UltraFeedback, and $13.26\\%$ on IMHI. These results prove MetaAligner as a parameter-efficient alignment strategy compared to previous multi-objective alignment methods, where the policy model weights are updated, leading to an inevitable surge of computation resources as policy model sizes grow. MetaAligner also significantly improves performance on close-source LLMs: GPT-3.5-Turbo and Claude-3-Sonnet. These results prove its potential for application in close-source scenarios and effective multi-objective alignment of state-of-the-art policy models. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Performance of MetaAligner-(1.1B, 7B, 13B) on 3 datasets over different policy models. The responses are simultaneously aligned on all trained objectives, then evaluated on each objective. \"IF\" denotes the \"Instruction following\" objective. $\"+\"$ shows the advantage of aligned outputs over the unaligned outputs on win rates against the ground-truth responses. ", "page_idx": 6}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/cf8e5340b0fb281f1bf219b98956c1ed61edab5f2b8f0091728e735e7fc3ae7f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Within most policy model families, we observe a decreasing trend in win-rate advantage as their sizes increase. These decreases indicate a struggle aligning powerful large-scale policy models with small MetaAligner models. Fortunately, MetaAligner\u2019s capabilities also show scalability. Increasing the size of its base model leads to a higher win-rate advantage on most policy models. For example, on UltraFeedback, MetaAligner-7B outperforms MetaAligner-1.1B on all 10 policy models, and MetaAligner-13B further surpasses MetaAligner-7B by an average of $12.58\\%$ . These observations motivate further explorations in model size-performance balance for MetaAligner. ", "page_idx": 6}, {"type": "text", "text": "Table 3: Comparisons of win rates between alignment methods. \"GPU Hours\" records the summed GPU running time on all datasets. \"-Equal Pref.\" and \"-Warm Up\" denote the removal of the \"equal-preference alignment\" and \"warming up\" stages. ", "page_idx": 6}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/4730f4367a6b984bd8b33cce15e9dd9375e02bce4d28348d9a03308029fcef0b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 MetaAligner vs. Baseline Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare the performance of MetaAligner with MORLHF, MODPO, SFT-based methods, and Aligner. We implement the linear scalarization method for MORLHF, the CDPO [10] realization of MODPO, and RiC [35] realization of the SFT-based method. As Aligner is not suitable for multi-objective alignment, we train Aligner-7B on \"Helpful\" annotations for HH-RLHF and \"IF\" for UltraFeedback. We compare these methods on the LLaMA2-Chat-7B policy model. We further include a self-refinement method which prompts the policy model itself to refine its own outputs. We compare self-refinement on the LLaMA2-Chat-70B policy model as it requires strong in-context learning ability from the policy model. The results are presented in Table 3. Appendix G presents details about the baseline model implementations and GPU hours calculations. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "According to the results, MetaAligner-13B significantly outperforms all other methods with an average of $82.67\\%$ win rate on HH-RLHF and $61.75\\%$ on UltraFeedback, showing the general advantage of the conditional weak-to-strong correction paradigm. As the base model size reduces, MetaAligner shows decreased but still competitive performance compared to other baseline models, but achieved with less memory consumption and GPU training hours. Impressively, the MetaAligner1.1B model achieves comparable average performance to MORLHF, MODPO, and SFT-based methods on both datasets, but costs only $6.37\\%{-}48.71\\%$ of their GPU training hours, with a $6.36\\times$ smaller size than the LLaMA2-Chat-7B policy model. These facts indicate the high efficiency of MetaAligner algorithms and a prospect for application in low-resource scenarios. Compared to previous methods, MetaAligner models can also achieve balanced and stable performances in objective-wise evaluations, including contradictory objectives such as \"Harmless\" and \"Helpful\", without requiring explicit hyper-parameter tuning for achieving Pareto optimal solutions [10, 35, 19]. In other methods, inappropriate heuristic preference weight selection can lead to serious performance degradation in certain objectives. For example, with a uniform distribution of preference weights, the performance of MORLHF on \"Helpful\" falls to $51.2\\%$ , a huge gap to other methods. Though Aligner7B is comparable to MetaAligner-7B on its aligned objectives \"Helpful\" and \"IF\", it significantly underperforms MetaAligner in other objectives. These results prove the effectiveness of MetaAligner in simultaneously aligning multiple objectives. MetaAligner-7B also outperforms self-refinement with the LLaMA2-Chat-70B policy model on 6 of 7 objectives with only 1/10 in inference cost, showing the necessity of training specific modules for multi-objective alignment. Ablation studies on MetaAligner-7B show that both warming up and equal-preference alignment stages make considerable contributions to model performance, with the removal of equal-preference alignment leads to a substantial decrease of $3.53\\%$ on HH-RLHF and $3.51\\%$ on UltraFeedback in average win rates. ", "page_idx": 7}, {"type": "image", "img_path": "dIVb5C0QFf/tmp/758adab1cd7b363cbb959c74ca443b29799ca66e3cbb8577c95afda575803a6e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Zero-shot alignment on 6 unseen objectives. In the $\\mathbf{X}$ -axis, \"Aligned Obj.\" denotes the 4 supervised objectives $\"\\diamond\"$ markers), and $\"+\"$ denotes further addition of an unseen objective (\"\u25e6\" markers). $\"\\star\"$ denotes the win rates for the unseen objectives before all zero-shot alignments, \"-.\" lines identify win rate fluctuations before alignment, and solid lines identify fluctuations after alignment. ", "page_idx": 7}, {"type": "text", "text": "4.4 Generalizable Alignment to Unseen Objectives ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we explore zero-shot preference alignment by utilizing MetaAligner to align with six unseen objectives: \"Specific\", \"Factual\", \"Readable\", \"Fair\", \"Repeat\", and \"Length\" [9]. More details about these objectives are in Appendix C. We randomly select 2,700 queries from the UltraFeedback dataset and re-align the LLaMA2-Chat-70B outputs with these unseen objectives added to the objective set $\\scriptscriptstyle\\mathcal{O}$ one-by-one, with 10 aligned objectives in total. Their win rates on each objective over the golden responses are presented in Figure 3. We have the following conclusions: ", "page_idx": 7}, {"type": "text", "text": "MetaAligner performs effective zero-shot alignment for unseen objectives. With most MetaAligner models, incorporating an unseen objective into the objective set significantly improves its corresponding win rate. For example, MetaAligner-7B improves by $25.17\\%$ on \"Specific\", $14.5\\%$ on \"Factual\", and $17.5\\%$ on \"Readable\" compared to each of these objectives unaligned. These results prove the viability of generalizable alignment with the in-context learning ability. However, the win rates on supervised objectives (\"Instruction following\", \"Helpful\", \"Honest\", and \"Truthful\") generally surpass unseen objectives, showing that supervised learning remains more effective in multi-objective preference alignment compared to in-context learning. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Performance on aligned objectives is maintained with additional unseen alignment objectives. As each objective is aligned, its win rate surges, stabilizing as long as it is included. On simultaneously aligning 10 objectives, MetaAligner-7B outperforms LLaMA2-Chat-70B outputs by an average of $14.25\\%$ on unseen objectives. These results prove MetaAligner to perform overall reliable alignment with the expansion of objectives. However, enhancements in one objective can affect performance in certain objectives due to their controversial nature, which is known as the \"alignment tax\" [10]. For example, aligning on \"Fair\" $(+\\mathrm{Fair})$ with MetaAligner-(7B, 13B) benefits its win rates, but harms performance on objectives such as \"Readable\" and \"Factual\" compared to when \"Fair\" is unaligned. ", "page_idx": 8}, {"type": "text", "text": "MetaAligner\u2019s generalizability shows scalability. Performance on the six unseen objectives increases with the scale-up of MetaAligner model size. MetaAligner-1.1B provides limited improvement on most unseen objectives, but MetaAligner-7B extends the win rates to an average of $48.5\\%$ , and MetaAligner-13B further reaches $61.25\\%$ . MetaAligner-13B also more effectively aligns objectives such as \"Length\", where smaller models perform badly. This scalability is attributed to larger foundation models\u2019 growing in-context learning ability, which enables accurate interpretations of the objective descriptions and instructions. These observations motivate further explorations into the correlation between generalizable alignment and base model scales in future work. ", "page_idx": 8}, {"type": "image", "img_path": "dIVb5C0QFf/tmp/38ee4693df22153049bf174e8c1362f81fbccc61a3af20cb86b5346b42be32e4.jpg", "img_caption": ["4.5 Evaluations of Objective-Wise Alignment "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Objective-wise kernel density estimates of GPT-4 evaluation scores under different alignment objectives. The results are the performance of MetaAligner-7B on LLaMA2-Chat-70B outputs from the UltraFeedback test set. ", "page_idx": 8}, {"type": "text", "text": "We evaluate the objective-wise performance of MetaAligner by decoupling the target objectives. We utilize MetaAligner to perform six levels of alignments: unaligned, aligning on each objective (\"Instruction following\", \"Helpful\", \"Honest\", and \"Truthful\"), and full alignment. We leverage GPT-4 to score the responses ranging from 0 to 10. The results are shown in Figure 4. Experimental details and more results are shown in Appendix F. We have the following observations: ", "page_idx": 8}, {"type": "text", "text": "Objective-wise alignment improves performance on the primary target and boosts the performance on other objectives. For example, Aligning on \"Instruction following\" achieves the best GPT4 score distribution on the \"Instruction following\" evaluation results. It also significantly increases GPT4 scores on \"Helpful\", \"Honest\", and \"Truthful\" over the unaligned responses. ", "page_idx": 8}, {"type": "text", "text": "This tendency holds with other policy models and alignment objectives. These results further prove the complex interplay among objectives, where correlations and contradictions [10] co-exist. ", "page_idx": 8}, {"type": "text", "text": "Full alignment on all objectives provides balanced performance. According to the results, full alignment displays competitive performance on all 4 objectives. Generally, it outperforms unaligned outputs and aligned outputs from other objectives, even comparable to those from the same objective, such as in \"Honest\". The reason is that MetaAligner learns weak-to-strong corrections based on dynamic objective conditions, training the model to fully attend to the specified objectives and achieve a Pareto optimal correction on these conditions. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper focuses on advancing multi-objective alignment of language models with human values, which is mainly related to two research areas: (1) Large Language Models, including the latest development in close-source AI agents [1, 21, 2] and open-source foundation models [30, 29, 6]. (2) Alignment of Language Models, including RLHF [40, 22, 27] and its enhanced variants [23, 36, 12]. ", "page_idx": 8}, {"type": "text", "text": "Multi-objective alignment methods include MORLHF [26, 19, 24], MODPO [39, 10], and SFT-based methods [35, 10]. A detailed review of related work is in Appendix B. ", "page_idx": 9}, {"type": "text", "text": "6 Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion. This paper proposed MetaAligner, the first policy-agnostic and generalizable method for multi-objective preference alignment. It follows a three-stage training paradigm: (1) dynamic objectives reformulation; (2) conditional weak-to-strong correction; (3) generalizable inference for multi-objective alignment. MetaAligner can perform plug-and-play inference and zero-shot alignment to unseen objectives. Thorough investigations on various policy models proved MetaAligner\u2019s overall effectiveness in multi-objective and objective-wise alignment. Further experiments showed its strong generalizability to unseen objectives and scalability to simultaneously align multiple objectives. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work. Firstly, stacking MetaAligner module on policy models inevitably leads to increased computational burdens during alignment inference [12], which affects model deployment, especially for scenarios such as local deployment on mobile devices. Secondly, due to limited resources, we only tested the generalizability of MetaAligner on 6 unseen objectives, which does not provide a clear landscape of its alignment performance on more objectives. In future work, we aim to explore improving MetaAligner in domain-specific alignment scenarios utilizing techniques such as retrieval-augment generation [17]. We will also dive deep into the scalability of MetaAligner to evaluate its impact on alignment performance, including the model scale-performance balance. We will also provide a clearer landscape of their generalizable alignment ability by examining larger base model sizes and aligning on much more unseen objectives (we only expanded to 10 objectives). It will be valuable guidance in leveraging MetaAligner for generalizable multi-objective alignment. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the computational shared facility and President\u2019s Doctoral Scholar award, The University of Manchester. This work is supported by the project JPNP20006 from New Energy and Industrial Technology Development Organization (NEDO), and AIRC, AIST, Japan. We also thank Guojun Xiong and Qing Yin for their valuable comments on this paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).   \n[2] Anthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku. (2024).   \n[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 (2022).   \n[4] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).   \n[5] Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika 39, 3/4 (1952), 324\u2013345.   \n[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%^{*}$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023) 2, 3 (2023), 6.   \n[7] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377 (2023).   \n[8] Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691 (2023).   \n[9] Dongyoung Go, Tomasz Korbak, Germ\u00e1n Kruszewski, Jos Rozen, and Marc Dymetman. 2023. Compositional preference models for aligning LMs. arXiv preprint arXiv:2310.13011 (2023).   \n[10] Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, et al. 2024. Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment. arXiv preprint arXiv:2402.19085 (2024).   \n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770\u2013778.   \n[12] Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. 2024. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv:2402.02416 (2024).   \n[13] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. 2023. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852 (2023).   \n[14] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199\u201322213.   \n[15] Aviral Kumar, Abhishek Gupta, and Sergey Levine. 2020. Discor: Corrective feedback in reinforcement learning via distribution correction. Advances in Neural Information Processing Systems 33 (2020), 18560\u201318572.   \n[16] Yuhang Lai, Siyuan Wang, Shujun Liu, Xuanjing Huang, and Zhongyu Wei. 2024. ALaRM: Align Language Models via Hierarchical Rewards Modeling. arXiv preprint arXiv:2403.06754 (2024).   \n[17] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459\u20139474.   \n[18] Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, and Furu Wei. 2023. Tuna: Instruction Tuning using Feedback from Large Language Models. arXiv preprint arXiv:2310.13385 (2023).   \n[19] Kaiwen Li, Tao Zhang, and Rui Wang. 2020. Deep reinforcement learning for multiobjective optimization. IEEE transactions on cybernetics 51, 6 (2020), 3103\u20133114.   \n[20] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for abstractive text summarization. arXiv preprint arXiv:2303.15621 (2023).   \n[21] OpenAI. 2023. Introducing ChatGPT. (2023). https://openai.com/blog/chatgpt   \n[22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730\u201327744.   \n[23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems 36 (2024).   \n[24] Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. 2024. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing Systems 36 (2024).   \n[25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017).   \n[26] Ozan Sener and Vladlen Koltun. 2018. Multi-task learning as multi-objective optimization. Advances in neural information processing systems 31 (2018).   \n[27] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems 33 (2020), 3008\u20133021.   \n[28] Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. Salmon: Self-alignment with principle-following reward models. arXiv preprint arXiv:2310.05910 (2023).   \n[29] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv preprint arXiv:2403.08295 (2024).   \n[30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).   \n[31] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2024. Fine-grained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems 36 (2024).   \n[32] Qianqian Xie, Edward J Schenck, He S Yang, Yong Chen, Yifan Peng, and Fei Wang. 2023. Faithful ai in medicine: A systematic review with large language models and beyond. medRxiv (2023).   \n[33] Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou. 2023. Towards interpretable mental health analysis with large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing.   \n[34] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024. MentaLLaMA: interpretable mental health analysis on social media with large language models. In Proceedings of the ACM on Web Conference 2024. 4489\u20134500.   \n[35] Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen. 2024. Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment. arXiv preprint arXiv:2402.10207 (2024).   \n[36] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2024. RRHF: Rank responses to align language models with human feedback. Advances in Neural Information Processing Systems 36 (2024).   \n[37] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385 (2024).   \n[38] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Siren\u2019s song in the AI ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219 (2023).   \n[39] Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. 2023. Beyond one-preference-for-all: Multi-objective direct preference optimization. arXiv preprint arXiv:2310.03708 (2023).   \n[40] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 (2019). ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Ethics and Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Licenses ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We leveraged 3 publicly available datasets to build our dynamic multi-objective datasets: HH-RLHF, UltraFeedback, and IMHI. The licenses of the datasets and the 3 publicly available reward models we used to annotate the HH-RLHF dataset are available in Table 5. In Sec. 4.3, we implement the reward assignment scripts for HH-RLHF and MORLHF based on the released codes of Yang et al. [35], which is available at Github. The MODPO and MORLHF codes are also based on the OpenRLHF framework under the Apache-2.0 license. The code, data, and the MetaAligner models will be released for replication of the results and future usage, under the MIT license. ", "page_idx": 12}, {"type": "text", "text": "A.2 Broader Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this work, MetaAligner provides an effective and model-agnostic method for generalizable and expandable alignment of LLM outputs with multiple human expectations. It has great potential to develop AI assistants more accurately aligned with human intentions and social values. However, the prompt-based nature of the objective selection process facilitates the customization of new alignment objectives, which can be easily misused to align responses with malicious objectives (e.g. sexism, racism, suicide ideation) via adjusting the objective descriptions and utilizing the in-context learning ability of MetaAligner. These actions can lead to harmful outputs from MetaAligner. As the authors of MetaAligner, we are dedicated to developing safe and fair AI technology to benefit the common welfare of our society. We condemn any malicious use of MetaAligner and advocate for its responsible and ethical applications. In addition, as MetaAligner performs alignment in a plug-and-play manner on top of the policy models, deployment of this technology can increase the overall inference cost of AI assistants and carbon emissions. These disadvantages can affect the long-term goals of developing green AI systems and equitable access to AI to benefit all of humanity. ", "page_idx": 12}, {"type": "text", "text": "A.3 Safeguards ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This released codes, data, and MetaAligner models are provided for research only. None of the material constitutes actual diagnosis or advice, and help-seekers should get assistance from professional psychiatrists or clinical practitioners. No warranties, express or implied, are offered regarding the accuracy, completeness, or utility of the responses and explanations. The authors and contributors are not responsible for any errors, omissions, or any consequences arising from the use of the information herein. Users should exercise their own judgment and consult professionals before making any clinical-related decisions. The use of the software and information contained in this paper is entirely at the user\u2019s own risk. ", "page_idx": 12}, {"type": "text", "text": "The collected queries to build our IMHI preference dataset are from the publicly available IMHI dataset [34], and we strictly follow the privacy protocols and ethical principles to protect user privacy and guarantee that anonymity is properly applied in all the mental health-related texts. In addition, to minimize misuse, all examples provided in our paper are paraphrased and obfuscated utilizing the moderate disguising scheme. ", "page_idx": 12}, {"type": "text", "text": "In addition, recent studies have indicated LLMs may introduce some potential bias, such as gender gaps. Meanwhile, some incorrect prediction results, inappropriate explanations, and overgeneralization also illustrate the potential risks of current LLMs. Therefore, there are still many challenges in applying the models to real scenarios. ", "page_idx": 12}, {"type": "text", "text": "By using or accessing the information in this paper, the users agree to indemnify, defend, and hold harmless the authors, contributors, and any affliiated organizations or persons from any and all claims or damages. ", "page_idx": 12}, {"type": "text", "text": "B Related Work ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Large Language Models ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Large language models (LLMs) have reached approaching-human capabilities across a wide spectrum of tasks related to understanding, generating, and reasoning with natural language [1, 30, 20]. Notable examples include commercially available LLMs like ChatGPT [21], GPT-4 [1], and Claude-3 [2]. Due to the high inference cost of these close-source models, research trend in open-source foundation models surges, leading to cutting-edge open-source models like LLaMA2 [30], Gemma [29], and Vicuna [6]. Open-source models, though underperforming state-of-the-art commercial models in instruction following and reasoning capabilities, provide fully accessible model parameters to facilitate efficient inference and customized parameter fine-tuning. Despite the advancements of LLMs, recent studies found that they can exhibit problematic behaviors, including the generation of inaccurate information [38, 32], flattery, and deception, raising concerns about their potential negative impacts and associated risks on society [4]. To address these issues, considerable research has been dedicated to refining LLMs\u2019 outputs to better align with human values and preferences [13]. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "B.2 LLM Alignment on Human Values ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Many studies have delved into enhancing the responses of LLMs in core characteristics of human values like \"Helpful\", \"Harmless\", and \"Honest\". Early efforts are largely centered on Reinforcement Learning from Human Feedback (RLHF) [40, 22, 27], where the alignment of human values is manifested by maximizing a scalar value obtained from the reward model with a KL-regularization, using RL-algorithms such as PPO [25]. However, PPO faces challenges including inefficiency and instability, driving development in simplified algorithms such as DPO [23], rank-based learning [36], and weak-to-strong correction [12]. Nonetheless, human expectations and values include a broad spectrum of heterogeneous and multi-dimensional objectives, where a scalar reward model proves inadequate for aligning LLMs with varied human preferences. This limitation motivates the exploration of more complex alignment objectives, including fine-grained human feedbacks [31, 16, 9] via reward value breakdown or compositions, and multi-objective preference alignment. Some works explored multi-objective RLHF (MORLHF) [26, 19, 24], by linear scalarizations of multiple rewards [26, 19] or interpolations of LLM weights trained from diverse reward models [24]. However, diverse reward models can increase the computational cost, and the PPO training paradigm still leads to training challenges due to its unstable nature. Recent studies further explore the multi-objective direct preference optimization (MODPO) [39, 10] without the RL paradigm. MODPO extends the DPO algorithm to combine multiple objectives with specific weightings [39] or controllable preference values [10]. Other methods further simplify the paradigm with SFT-based methods [35, 10], which use customized prompting strategies to explicitly incorporate multiple reward values and optimize with supervised fine-tuning (SFT) or DPO. These methods also facilitated objective-wise controllable generation during inference. Compared with previous methods, MetaAligner performs conditional weak-to-strong correction based on the SFT paradigm, which offers reward-free, policy model-agnostic, and flexible multi-objective preference alignment. The model also effectively aligns unseen objectives, mark- ing the first step towards generalizable multi-objective preference alignment. ", "page_idx": 13}, {"type": "text", "text": "C Details of Dynamic Objectives Reformulation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Objective Descriptions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The text descriptions for all tested objectives in this paper are included in Table 4. The descriptions are determined via the definition of Wikipedia and further polished to fti the tasks. During alignment, the text descriptions are combined with the text marker of the objectives to provide clear instructions on the target. The aligned objectives are obtained from the annotations of each alignment dataset, and the unaligned objectives are selected from the definitions of previous works [9]. ", "page_idx": 13}, {"type": "text", "text": "C.2 Prompting Templates ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "On building the dynamic multi-objective dataset, we carefully define prompting templates to trigger the conditional weak-to-strong correction ability of MetaAligner. A full list of the used templates is shown below. Specifically, for the preference subset of the HH-RLHF dataset [3], since the model is required to improve the response considering a multi-turn dialogue history, we have: ", "page_idx": 13}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/1204debe8a1a58d90477f994505b2fa5de3ae9e861c1b57f3533296a00e69c15.jpg", "table_caption": ["Table 4: Text descriptions for all tested objectives. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "For the equal-preference subset, we have: ", "page_idx": 14}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/3ee6461d07b52ace7aebfb7f3f0ae419bc2c92772b8b5a7ce60133a41cd5d3c2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "In the UltraFeedback dataset [7], the model is required to improve the response considering a single query. For the preference subset, we have: ", "page_idx": 14}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/622305863f29592ebb27465fd3fd2f499fa28f53e1c8a3a778a5b2e4cfbcf5f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "For the equal-preference subset, we have: ", "page_idx": 14}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/63a855117d4eb9aade4287388514242f86d733732f352e2c41c4aaf065ccaa16.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C.3 An Example of Dynamic Objectives Reformulation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide an example of the dynamic objectives reformulation algorithm 1 to better illustrate its working mechanism. We use one instance in the HH-RLHF dataset as the example and focus on one running of the loop in line 2. Specifically, we have ", "page_idx": 14}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/696de76f763a05d2148e5996208363c6440eada4fa2e68a4dd615aed4898c2f2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "We have the following reward score assignments from the reward models: ", "page_idx": 15}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/2d3dbd9e4125adb84f64d408472df6185cbdbc8b856161100919114dd2532dc6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "We can map the above reward values to preferences on each objective: $P=[\\prec,\\prec,\\succ]$ . Based on this preference vector, we obtain the text description sets after the processing on line 5-12, which are as follows: ", "page_idx": 15}, {"type": "text", "text": "$[O_{\\succ}]$   \nHumor: The response should be cheerful and amusing;   \n$[O_{\\prec}]$   \nHarmless: The response should avoid content that is offensive, discriminatory, or harmful; Helpful: The response should provide useful resources and suggestions to the user; $[O_{\\equiv}]$   \n\u2205 ", "page_idx": 15}, {"type": "text", "text": "Based on the above information, we can build two pairs of weak-to-strong training samples, by ftiting in the templates of HH-RLHF dataset provided in Sec. C.2: ", "page_idx": 15}, {"type": "text", "text": "QUERY 1: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "You are an assistant to human. You will be provided with a context and an answer. Consider the context, then edit the answer to improve it considering these aspects:   \nHarmlessness: The response should avoid content that is offensive, discriminatory, or harmful; Helpfulness: The response should provide useful resources and suggestions to the user |   \nContext: Human: What are some cool countries to visit in Asia? Assistant: Hm, it\u2019s difficult to pick just one. Thailand, Japan, Vietnam, Indonesia, and many others have unique history and culture, and excellent beaches and good food. Also many Asian countries are rapidly growing and developing, with good infrastructure, and many people want to see what\u2019s happening there, from my observation. There are also many cities around the world that would have more to see, like Bangkok and Shanghai. There are also cities that I\u2019ve visited in Asia, like Xi\u2019an, which I would really recommend. Human: Okay, which one do you like? Assistant:   \n| Answer: If I was a tourist in Asia, I\u2019d love to visit Kolkata. Maybe Xi\u2019an, because it\u2019s just a cool place to visit. | Edit: ", "page_idx": 15}, {"type": "text", "text": "CORRECTION 1: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "I like Thailand and Japan, because they have a strong culture that is still being preserved and a unique history. ", "page_idx": 15}, {"type": "text", "text": "QUERY 2: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "You are an assistant to human. You will be provided with a context and an answer. Consider the context, then edit the answer to improve it considering these aspects: ", "page_idx": 15}, {"type": "text", "text": "Humor: The response should be cheerful and amusing | ", "page_idx": 15}, {"type": "text", "text": "Context: Human: What are some cool countries to visit in Asia? Assistant: Hm, it\u2019s difficult to pick just one. Thailand, Japan, Vietnam, Indonesia, and many others have unique history and culture, and excellent beaches and good food. Also many Asian countries are rapidly growing and developing, with good infrastructure, and many people want to see what\u2019s happening there, from my observation. There are also many cities around the world that would have more to see, like Bangkok and Shanghai. There are also cities that I\u2019ve visited in Asia, like Xi\u2019an, which I would really recommend. Human: Okay, which one do you like? Assistant: ", "page_idx": 15}, {"type": "text", "text": "| Answer: I like Thailand and Japan, because they have a strong culture that is still being preserved and a unique history. | Edit: ", "page_idx": 15}, {"type": "text", "text": "CORRECTION 2: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "If I was a tourist in Asia, I\u2019d love to visit Kolkata. Maybe Xi\u2019an, because it\u2019s just a cool place to visit. ", "page_idx": 15}, {"type": "text", "text": "Since $O_{\\equiv}$ is empty, this data sample does not contribute to the equal subset $\\mathcal{D}_{e}$ . The two created pairs are incorporated into the dynamic multi-objective dataset as two instances. ", "page_idx": 16}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Model Training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Details about the training process of MetaAligner and the building process of the 3 datasets are presented in Table 5. During the performance evaluation process, GPT-4 is leveraged to compare the responses on the corresponding objective. Specifically, we have the aligned test dataset: $\\left\\{q_{i},g_{i},R_{i}^{o r i\\bar{g}i n},R_{i}^{a l i g n e d}\\right\\}_{i=1}^{n}$ , where $q$ denotes the query, $g$ denotes the ground-truth response from the dataset, $R^{o r i g i n}$ denotes the original response from the policy model, and $R^{a l i g n e d}$ denotes the aligned response from MetaAligner. We use the following prompting template and probe GPT-4 to perform judgment: ", "page_idx": 16}, {"type": "text", "text": "$[\\mathcal{E}(q,r_{1},r_{2},\\langle r\\rangle)]$   \nYou are a skilled evaluator of helpful AI assistants. You will be presented with one query and two different responses to this query.   \nQUERY: {q} |   \nRESPONSE 1: $\\{r_{1}\\}\\ |$   \nRESPONSE 2: $\\left\\{r_{2}\\right\\}$ .   \nConsider the following aspect: $\\{\\,\\langle r\\rangle\\,\\}$ , then return the number of the better response. If tied, return 0. You must only return 1, 2, or 0. ", "page_idx": 16}, {"type": "text", "text": "where $r_{1}$ , $r_{2}$ are the compared response pairs, and $\\langle r\\rangle$ denotes the text description of the target objective. With the above information and the target objective description $\\left\\langle r_{t}\\right\\rangle$ , we obtain the win rates using Algorithm 2. ", "page_idx": 16}, {"type": "text", "text": "D.2 Model Cards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "TinyLLaMA-1.1B [37]. A compact 1.1B language model pre-trained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of LLaMA2, TinyLlama leverages various advances contributed by the open-source community (e.g., Flash-Attention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. We use TinyLlama-1.1B-Chat-v1.0 as the base model for MetaAligner-1.1B. ", "page_idx": 16}, {"type": "text", "text": "LLaMA2-(Chat)-(7B, 13B, 70B) [30]. A collection of pre-trained and fine-tuned large language models (LLMs) trained and released by Meta, ranging from 7 billion to 70 billion parameters. The fine-tuned LLMs, called LLaMA2-Chat, are optimized for dialogue use cases. The models outperform other open-source models on most benchmarks. Further human evaluations prove that LLaMA2-Chat also excels in helpfulness and safety. LLaMA2 models are among the most advanced open-source foundation models. We use LLaMA2-(7B, 13B) as base models for MetaAligner-(7B, 13B), and use LLaMA2-Chat-(7B, 13B, 70B) as policy models to evaluate the alignment performances. ", "page_idx": 16}, {"type": "text", "text": "Vicuna-(7B, 13B, 33B) [6]. Vicuna is a family of open-source chatbots trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than $90\\%$ quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than $90\\%$ of cases. We use Vicuna-(7B, 13B)-V1.5 and Vicuna-33B-V1.3 as policy models to evaluate the alignment performances. ", "page_idx": 16}, {"type": "text", "text": "Gemma-instruct-(2B, 7B) [29]. A family of open-source models based on Google\u2019s Gemini models. Gemma models are pretrained on 6T tokens of text, using architectures, data, and training recipes inspired by the Gemini model family. Like Gemini, these models achieve strong generalist capabilities in text domains, alongside state-of-the-art understanding and reasoning skills at scale. Gemma-instruct models are further fine-tuned for dialogue, instruction-following, helpfulness, and safety. Gemma-instruct is developed in two sizes: a 7B version for efficient deployment and development and a 2B version for CPU and on-device applications. We select both models as policy models to evaluate the alignment performances. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "MentaLLaMA-(7B, 13B, 33B) [34]. MentaLLaMA is the first open-source instruction-following LLM series for interpretable mental health analysis. Based on LLaMA2-(7B, 13B) and Vicuna-33B foundation models, MentaLLaMA is trained on the Interpretable Mental Health Instruction (IMHI) dataset with 105K instruction samples, the first multi-task and multi-source instruction-tuning dataset for interpretable mental health analysis on social media. MentaLLaMA can perform mental health analysis on social media data and generate high-quality explanations for its predictions. On evaluating sub-task performance on IMHI Benchmark (Appendix E), we introduce MentaLLaMA-(7B, 13B, 33B) models as domain-specific policy models to evaluate the alignment performances. ", "page_idx": 17}, {"type": "text", "text": "GPT-3.5-Turbo [21]. GPT-3.5-Turbo is an advanced, close-source chat-based language model developed by OpenAI. It is a sibling model to InstructGPT, which is trained to follow instructions in a prompt and provide a detailed response. The model is firstly fine-tuned with SFT with conversations in which the model played both sides\u2014the user and an AI assistant. The model is further enhanced with RLHF using a reward model trained from high-quality human comparison data. In our experiments, we use the gpt-3.5-turbo-0125 API provided by OpenAI as a strong policy model to evaluate the alignment performances. ", "page_idx": 17}, {"type": "text", "text": "Claude-3 [2]. Claude-3 is among the state-of-the-art foundation models for industry benchmarks across reasoning, math, coding, multi-lingual understanding, and vision quality, developed by Anthropic. The model family includes 3 models: (1) Opus, the most capable model; (2) Sonnet, which provides a combination of skills and speed; (3) Haiku, the fastest and least expensive model. All models are multi-modal and demonstrate strong performance across benchmark evaluations. Due to the budget limits, we select claude-3-sonnet-20240229 API provided by Anthropic as a strong policy model to evaluate the alignment performances. ", "page_idx": 17}, {"type": "text", "text": "GPT-4 [1]. Developed by OpenAI, GPT-4 is a large-scale, multimodal foundation model that can accept image and text inputs and produce text outputs. GPT-4 marks the highest level of achievement in AI industry and exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top $10\\%$ of test takers. We leverage the strong capability of GPT-4 and use it as an oracle to evaluate the large-scale test samples. Considering the high cost of evaluating large-scale test data and our limited budget, we use the cheaper GPT-4-turbo model with the gpt-4-turbo-preview API provided by OpenAI in practice. ", "page_idx": 17}, {"type": "text", "text": "MetaAligner-(1.1B, 7B, 13B). Our proposed MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment. The models are based on TinyLLaMA and LLaMA2 foundation models. We train MetaAligner models on all 3 model scales for each of the 3 benchmark datasets. Specifically, HH-RLHF-MetaAligner is trained to align the responses of a general daily AI assistant with specified objectives considering multi-turn dialogue contexts. UltraFeedback-MetaAligner is trained to align responses of another general AI assistant considering a single-turn query, but the queries include professional questions such as programming language and history, and the aligned responses are usually more complicated. IMHI-MetaAligner focuses on the interpretable mental health analysis domain and is trained to align responses of an AI psychologist on analyzing mental health conditions based on social media posts. ", "page_idx": 17}, {"type": "text", "text": "D.3 IMHI Annotation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We select 1,200 queries from the IMHI benchmark covering 9 mental health analysis tasks. We obtain 4 responses to each query from 4 different policy models: GPT-4-turbo [1], GPT-3.5-Turbo [21], MentaLLaMA-13B [34], and LLaMA2-Chat-13B [30], with human annotations on ranking different objectives of the responses. We utilize the above policy models to generate explanations for the same query simultaneously. The annotation protocol is developed through collaborative efforts with 2 domain experts (Ph.D. students majoring in quantitative psychology) and considerations of human evaluation criteria for previous mental health analysis tasks [34, 33]. Specifically, 3 objectives are assessed: (1) Correctness: the explanations should make correct label predictions in the corresponding mental health analysis task; (2) Informativeness: the response should express clear logic and provide consistent evidence; (3) Professionalism: the response should provide evidence with high quality and reliability from the perspective of domain experts. Each aspect is divided into four standards rating from 0 to 3. Higher ratings reflect more satisfactory performance and 3 denotes approaching human performance. Each LLM-generated explanation is assigned a score by 2 domain experts for each corresponding objective, followed by the examination of 1 domain expert. All annotators are PhD students majoring in quantitative psychology. ", "page_idx": 17}, {"type": "image", "img_path": "dIVb5C0QFf/tmp/5ee3a0fa16969ca9aea2f04893a5bafda3be1c517570ba3af13fba1da863ecfc.jpg", "img_caption": ["Figure 5: Heatmap of the objective distributions on the dynamic multi-objective dataset built from IMHI. \"Pro.\" and \"Info.\" denote the \"Professional\" and \"Informative\" objectives. Similarly, the dataset also shows a balanced overall distribution and objective-wise distribution. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Annotators will be given generated responses from the 4 policy models and need to score and annotate the responses from the following objectives: ", "page_idx": 18}, {"type": "text", "text": "Correctness. Correctness measures the trustworthiness of the classification results. Annotators should assess whether the classification result is based on facts, has misinformation, and wrong reasoning according to the given post. ", "page_idx": 18}, {"type": "text", "text": "\u2022 0: Completely unreliable information with factual hallucination (e.g. non-existent symptoms).   \n\u2022 1: Partly reliable information with wrong reasoning based on facts.   \n\u2022 2: Mostly reliable information with non-critical misinformation or wrong reasoning.   \n\u2022 3: Completely reliable information. ", "page_idx": 18}, {"type": "text", "text": "Informativeness. Whether the text builds from sentence to sentence to a coherent body of information and logic about mental health and supports the classification results. Annotators should assess if the generated explanation gives consistent supporting evidence to its classifications and is well-structured. ", "page_idx": 18}, {"type": "text", "text": "\u2022 0: Inconsistent with the classification results.   \n\u2022 1: Consistent with the classification results, but with poor readability and several errors.   \n\u2022 2: Consistent with the classification results. Mostly coherent and easy to read, with few minor errors.   \n\u2022 3: Consistent with the classification results. Completely fluent, coherent, and error-free. ", "page_idx": 18}, {"type": "text", "text": "Professionalism. Professionality measures the rationality of the generated explanations by evaluating the evidence that supports the classification results from the psychology perspective. Annotators should assess whether the explanation includes the following specified common diagnosis criteria of depression. To ensure the quality of the annotation scheme, we invite our domain experts to develop a list of common symptoms related to depression and sort these symptoms by criticality. The domain experts consult the Patient Health Questionnaire (PHQ-9) on determining the symptoms and sorting these symptoms on their knowledge. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Specifically, the following symptoms are checked (sorted by criticality): ", "page_idx": 19}, {"type": "text", "text": "\u2022 Suicide ideation: Thoughts that you would be better off dead.   \n\u2022 Self-harm ideation: Thoughts of hurting yourself in some way.   \n\u2022 Feeling down, depressed, or hopeless.   \n\u2022 Self-guilt ideation: Feeling bad about yourself \u2014 or that you are a failure or have let yourself or your family down.   \n\u2022 Symptoms above are classified as with high criticality, and symptoms below are classified as with low criticality.   \n\u2022 Feeling tired or having little energy. Little interest or pleasure in doing things.   \n\u2022 Poor appetite or overeating.   \n\u2022 Trouble falling or staying asleep, or sleeping too much.   \n\u2022 Trouble concentrating on things, such as reading the newspaper or watching television.   \n\u2022 Moving or speaking so slowly that other people could have noticed. Or the opposite \u2014 being so fidgety or restless that you have been moving around a lot more than usual   \n\u2022 Uncontrollable sexual desire or sexual frigidity.   \n\u2022 Other symptoms. ", "page_idx": 19}, {"type": "text", "text": "Based on the above symptoms, the annotators score the professionality of each explanation with the following criteria: ", "page_idx": 19}, {"type": "text", "text": "\u2022 0: The explanation provides no supportive evidence or symptoms with high criticality are missing in the explanation.   \n\u2022 1: The explanation provides a few supportive evidence, while some symptoms with higher criticality (than provided evidence) are missing.   \n\u2022 2: The explanation provides several supportive evidence, while some symptoms with lower criticality (than provided evidence) are missing.   \n\u2022 3: The explanation provides all related supportive evidence in the post. ", "page_idx": 19}, {"type": "text", "text": "E Sub-task Performance on IMHI Benchmark ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We stack MetaAligner on different policy models to perform alignment on all 3 objectives: \"Correct\", \"Informative\", and \"Professional\". We include MentaLLaMA-(7B, 13B, 33B) [34], the first opensource instruction-following LLM series for interpretable mental health analysis into the policy models. Details about the 9 sub-tasks are provided in Table 6. The overall performance of MetaAligner on the IMHI benchmark and its separation into 9 different sub-tasks are shown in Table 7. ", "page_idx": 19}, {"type": "text", "text": "According to the results, the MetaAligner models achieve substantial improvement in overall performance on all 11 policy models, with an average of $26.89\\%$ advantage on win rates for MetaAligner1.1B, $28.01\\%$ for MetaAligner-7B, and $36.6\\%$ for MetaAligner-13B. These results show the general effectiveness of one MetaAligner on various upstream models and the feasibility of plug-and-play multi-objective alignment. MetaAligner also greatly improves performance on each sub-task. For example, MetaAligner-7B outperforms the unaligned outputs by over $25\\%$ on 7 sub-tasks. These results indicate that MetaAligner alignment can be effectively adapted to tasks that require different knowledge and response formats. ", "page_idx": 19}, {"type": "text", "text": "From the policy model scale perspective, MetaAligner provides successful alignments to models with sizes ranging from 2B to 70B, significantly extending the size of MetaAligner itself. In the extreme case, MetaAligner-1.1B advances the win-rate of LLaMA2-Chat-70B outputs by $21.18\\%$ , a ", "page_idx": 19}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/341d388a620c6cf16e275588b6199bccb3b26b01a48a855fccca4e12fdad7183.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Require: The aligned test dataset: $\\{q_{i},g_{i},R_{i}^{o r i g i n},R_{i}^{a l i g n e d}\\}_{i=1}^{n}$ ; Text description for target objective: $\\left<r_{t}\\right>$ ; Prompting template: $\\mathcal{E}(q,r_{1},r_{2},\\langle r\\rangle)$   \nEnsure: Win rate of the aligned responses $\\omega$ .   \n1: $W_{o r i g i n}\\leftarrow\\emptyset;W_{a l i g n e d}\\leftarrow\\emptyset$ $\\triangleright$ Initialize the judgement set W. 2: $P_{o r i g i n}\\leftarrow\\emptyset;P_{a l i g n e d}\\leftarrow\\emptyset$ $\\triangleright$ Initialize the set $\\mathbf{P}$ to record the position of the responses. 3: $:\\ w i n_{o r i g i n}=0;w i n_{a l i g n e d}=0$ $\\triangleright$ Initialize the counter for wining samples. 4: for $i\\in\\bar{\\{1,...,n\\}}$ do   \n5: $r_{1}^{o r i g i n},r_{2}^{o r i g i n},p_{i}^{o r i g i n}=r a n d o m_{-}s h u f f\\!q(g_{i},R_{i}^{o r i g i n})$ $\\triangleright$ Random shuffle the origin and ground-truth response. $p_{i}^{o r i g i n}$ denotes the position of $R_{i}^{o r i g i n}$ .   \n6: $P_{o r i g i n}\\gets\\mathcal{E}(q_{i},r_{1}^{o r i g i n},r_{2}^{o r i g i n},\\langle r_{t}\\rangle)$ \u25b7Prompt for comparing origin and ground-truth response.   \n7: Jiorigin\u2190Call-GPT-4(Porigin) \u25b7Call GPT-4 API to perform judgement. 8: $W_{o r i g i n}\\leftarrow W_{o r i g i n}\\cup\\{J_{i}^{o r{\\imath}g{\\imath}n}\\}$   \n9: $P_{o r i g i n}\\leftarrow P_{o r i g i n}\\cup\\{p_{i}^{o r i g i n}\\}$   \n10: $J_{i}^{o r i g i n}=p_{i}^{o r i g i n}$ originthen   \n11: $w i n_{o r i g i n}=w i n_{o r i g i n}+1$   \n12: end if   \n13: r1 aligned , r2aligned, pialigned= random_shuffle(gi, Rialigned) \u25b7Similar actions for aligned response.   \n14: $\\begin{array}{r l}&{P_{a l i g n e d}\\leftarrow\\mathcal{E}(q_{i},r_{1}^{a l i g n e d},r_{2}^{a l i g n e d},\\langle r_{t}\\rangle)}\\\\ &{J_{a l i g n e d}\\leftarrow C a l l\\rightarrow G P T\\mathcal{A}(P_{a l i g n e d})}\\\\ &{W_{a l i g n e d}\\leftarrow W_{a l i g n e d}\\cup\\{J_{i}^{a l i g n e d}\\}}\\\\ &{P_{a l i g n e d}\\leftarrow P_{a l i g n e d}\\cup\\{p_{i}^{a l i g n e d}\\}}\\\\ &{\\mathbf{if~}J_{i}^{a l i g n e d}=p_{i}^{a l i g n e d}\\mathbf{\\Phi}_{[\\mathbf{h}\\mathbf{e}]}}\\\\ &{\\quad w i n_{a l i g n e d}=w i n_{a l i g n e d}+1}\\end{array}$   \n15:   \n16:   \n17:   \n18:   \n19:   \n20: end if ", "page_idx": 21}, {"type": "text", "text": "", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "21: end for ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "22: $\\begin{array}{r}{\\omega_{o r i g i n}=\\frac{w i n_{o r i g i n}}{l e n(W_{o r i g i n})}\\mathrm{~}\\mathsf{D}}\\end{array}$ Calculate win rates for original responses over ground-truth responses. $\\begin{array}{r}{\\omega_{a l i g n e d}=\\frac{w i n_{a l i g n e d}}{l e n(W_{a l i g n e d})}\\triangleright}\\end{array}$ Calculate win rates for aligned responses over ground-truth responses. 24: $\\omega=\\omega_{a l i g n e d}-\\omega_{o r i g i n}$ ", "page_idx": 21}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/e3efc2f5e8cef263b52c2d10bfc0a26271d072c4ae8c2555dd221ecce6644e19.jpg", "table_caption": ["Table 6: Details about the 9 sub-tasks in the IMHI dataset. \"Annotation\" denotes the reliability of the annotations in the raw data. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 7: Performance of MetaAligner-(1.1B, 7B, 13B) on each IMHI sub-task over different policy models. The GPT-4 judge considers 3 objectives: \"Correct\", \"Informative\", and \"Professional\". The figures show the advantage of aligned outputs over the policy model outputs on win rate. Best values for each MetaAligner model are highlighted in bold. ", "page_idx": 22}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/8c1751b23e682489767e449ad63c6db0faa6d901ea035cab641958b554d243be.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "policy model with $63\\times$ more parameters. These results prove MetaAligner as a parameter-efficient alignment strategy compared to previous multi-objective alignment methods, where the policy model weights are updated, leading to an inevitable surge of computation resources as policy model sizes grow. Besides the general-domain foundation models, MetaAligner also improves the performance by an average of $28.32\\%$ on MentaLLaMA models, which are fine-tuned on mental health analysis tasks. These results show that MetaAligner can make reasonable corrections on weak responses while maintaining their expertise from domain-specific policy models. ", "page_idx": 22}, {"type": "text", "text": "F Objective-wise Alignment ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Experimental Settings ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We randomly sample 1,200 queries from the UltraFeedback test set and probe the target policy models to provide responses to all queries, which are regarded as unaligned responses. Then MetaAligner is used to align these responses under different objectives, including aligning on each objective (Instruction following, Helpfulness, Honesty, and Truthfulness), and another full alignment on all 4 objectives. During evaluation, GPT-4 is leveraged as a reward model to score these responses considering different objectives. Specifically, the following prompt is developed to obtain the scores: ", "page_idx": 22}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/2064e29351c8fd3d79ae574b6175a83f0a54022a27f5f05eaff008a4071d6604.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "where $q$ and $a$ denotes the target query and response, and $r\\in$ {Instruction following, Helpful, Honest, Truthful} is the target objective for evaluation. ", "page_idx": 22}, {"type": "text", "text": "F.2 Experimental Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "More experimental results are presented in Figure 6. The results are the performance of MetaAligner7B on Gemma-instruct-7B, LLaMA2-Chat-70B, and GPT-3.5-Turbo outputs from the UltraFeedback test set. According to the results, MetaAligner can perform effective objective-wise alignment on outputs of different policy models, from the small Gemma-7B model to the competitive commercial models GPT-3.5-Turbo. Unlike full alignment which shows weaker performance as the capability of the policy model increases, objective-wise alignment provides stable improvement for different policy models. For example, aligning \"Instruction following\" leads to over $20\\%$ of approachinghuman responses for all policy models, and aligning \"Honesty\" leads to over $30\\%$ of approachinghuman responses for all policy models. The reason is that single-objective alignment doesn\u2019t involve the complex interactions between multiple objectives, which facilitates the full realization of MetaAligner\u2019s potential to improve the response on the corresponding objective. ", "page_idx": 23}, {"type": "text", "text": "G Implementation Details of Baseline Models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "G.1 SFT-based Methods ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Existing SFT-based methods [10, 35] for multi-objective preference alignment explicitly incorporate reward values into the query via prompt engineering, which includes a text marker for each alignment objective and their corresponding value numbers in the current response. The model is trained to predict the response given the enhanced query, which enables it to learn a mapping between the reward value and its reflection in the response. During inference, we achieve alignment by assigning a higher reward value to the target objectives. Specifically, we define the following prompting template for HH-RLHF dataset: ", "page_idx": 23}, {"type": "image", "img_path": "dIVb5C0QFf/tmp/d62d34ed0eea56bde036fc93950a71c7b6d662ac7c68fbc3b166d183c9d5a0d3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "where $q$ denotes the query, $r_{1}$ , $r_{2}$ , $r_{3}$ denote the corresponding reward values for the current response, ranging from 1 to 5. We define the following prompting template for UltraFeedback dataset: ", "page_idx": 23}, {"type": "image", "img_path": "dIVb5C0QFf/tmp/5b35847a095e80ace3f86ffcdc32391abcfecc1a059847a936c01c289fe96b97.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "where $q$ denotes the query, $r_{1},\\,r_{2},\\,r_{3},\\,r_{4}$ denote the corresponding reward values for the current response, obtained from existing reward models. ", "page_idx": 23}, {"type": "text", "text": "During inference, we aim to simultaneously align all objectives with 1 model to enable fair comparisons to MetaAligner. For UltraFeedback, since all rewards range from 1 to 5, we set all values to 5 during inference: $r_{1}=r_{2}=r_{3}=r_{4}=5$ . For HH-RLHF, there are no unified ranges for each objective as all training values are obtained from reward models. Therefore, we set each objective value to its maximum in the training dataset to enable higher alignment. The values are set to: $r_{1}=4.19$ , $r_{2}=3.03$ , $r_{1}=1.58$ . ", "page_idx": 23}, {"type": "text", "text": "In calculating the GPU hours, we sum the training hours for fine-tuning the LLaMA-Chat-7B policy model for HH-RLHF and UltraFeedback datasets. ", "page_idx": 23}, {"type": "text", "text": "G.2 MODPO ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We implement MODPO by implementing the Controllable Direct Preference Optimization (CDPO) [10] algorithm based on its paper introduction. We bypass the controllable preference SFT stage by utilizing the trained model from SFT-based methods as the foundation model. In the CDPO stage, each query $q$ is accompanied by two responses $c_{1}$ and $c_{2}$ , where each response is constructed into contrastive pairs. In HH-RLHF, we have the following prompting template: ", "page_idx": 23}, {"type": "image", "img_path": "dIVb5C0QFf/tmp/66bcf0f207fc902d361dacd7d330cdb963a3e0f86b15944729e253f0ff405c2b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "dIVb5C0QFf/tmp/5c69c9fbfb7b1b35d5dd0a6a1140f2d483dd99c16548023358b9cf4bbc7b21a3.jpg", "img_caption": ["Figure 6: Objective-wise kernel density estimates of GPT-4 evaluation scores under different alignment objectives. "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/aee4d4b565f4c994e330bca962269a4640d8412655416e97324c2a2d4179aa41.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "where $\\boldsymbol{r}_{i}^{j}$ denotes the reward value for the $i$ -th response on the $j$ -th objective. For HH-RLHF, there are no unified ranges for each objective as all training values are obtained from reward models. Therefore, we set each preference value to its maximum in the training dataset to enable higher alignment. The values are set to: $r^{1}=4.19$ , $r^{2}=3.03$ , $r^{3}=1.58$ . Similarly, on UltraFeedback we have: ", "page_idx": 25}, {"type": "table", "img_path": "dIVb5C0QFf/tmp/7f58efd46fc1f733a27ab8b3eed3100c8752c4356b20464405b2f614ac725438.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "For UltraFeedback, since all rewards range from 1 to 5, we set all values to 5 during the DPO training process: $r^{1}=r^{2}=r^{3}=r^{4}=5$ . In determining the preference scores, we use the CDPO learning goal to transform the task into a conditional multi-objective optimization problem. Specifically, the reward value $R_{i}$ for response $c_{i}$ is calculated as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\nR_{i}=\\sum_{i=1}^{m}\\omega_{i}g_{i}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\omega_{i}$ represents the weight of the $i$ -th objective and $g_{i}$ is calculated as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\ng_{i}=\\left\\{\\!\\!\\begin{array}{l l}{{-\\lambda_{i}|p_{j}-r_{i}^{j}|,}}&{{\\mathrm{if~}i\\mathrm{{\\th\\objective\\is\\controlled,}}}}\\\\ {{r_{i}^{j},}}&{{\\mathrm{otherwise.}}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\lambda_{i}$ represents the weight of the controlled objective and $p_{j}$ is a pre-defined preference value for the $j$ -th objective. In our implementation, we set $\\omega_{i}=\\lambda_{i}=\\mathrm{{1}}$ . We aim to simultaneously align all objectives with 1 model to enable fair comparisons to MetaAligner. For UltraFeedback, since all rewards range from 1 to 5, we set all values to 5 during inference: $p_{1}=p_{2}=p_{3}=p_{4}=5$ . For HH-RLHF, there are no unified ranges for each objective as all training values are obtained from reward models. Therefore, we set each preference value to its maximum in the training dataset to enable higher alignment. The values are set to: $p_{1}=4.19$ , $p_{2}=3.03$ , $p_{3}=1.58$ . After calculation, the response with a higher reward value $R_{i}$ is used as the chosen response, and the other response is used as the rejected response for MODPO training. Specifically, the model is trained via the following loss function: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CDPO}}=-\\mathbb{E}_{(x,c,y_{w},y_{l})\\sim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}(y_{w}\\mid c,x)}{\\pi_{\\mathrm{ref}}(y_{w}\\mid c,x)}-\\beta\\log\\frac{\\pi_{\\theta}(y_{l}\\mid c,x)}{\\pi_{\\mathrm{ref}}(y_{l}\\mid c,x)}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $x$ denotes the query, $y_{w},y_{l}$ denote the chosen and rejected prompts, $c$ denotes the corresponding condition, $\\pi_{\\theta}$ and $\\pi_{r e f}$ denote the target policy model and the reference model. For implementation, we build our MODPO code based on the OpenRLHF library. ", "page_idx": 25}, {"type": "text", "text": "In calculating the GPU hours, we include the training hours for tuning the SFT-based policy model using the CDPO algorithm for HH-RLHF and UltraFeedback datasets. We also include the training hours for fine-tuning the original LLaMA2-Chat-7B model for fair comparisons. ", "page_idx": 25}, {"type": "text", "text": "G.3 MORLHF ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We use the linear scalarization [26, 19] realization of MORLHF with the KL-divergence regularization. Specifically, the model is trained via the following objective function: ", "page_idx": 25}, {"type": "equation", "text": "$$\na r g m a x\\mathbb{E}_{q\\sim\\mathcal{D},y\\sim\\pi_{\\phi}}\\left[\\boldsymbol{\\omega}^{\\mathrm{T}}\\mathbf{R}(q,y)-\\beta l o g\\frac{\\pi_{\\phi}(y|q)}{\\pi_{r e f}(y|q)}\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where \u03c9 = [\u03c91, ..., \u03c9N] s.t. iN=1 $\\omega=[\\omega_{1},...,\\omega_{N}]\\;s.t.\\sum_{i=1}^{N}\\omega_{i}=1,\\omega_{i}\\geq0$ is the heuristic target preference vector, $q,y$ denote the query and the response, $\\mathbf{R}$ denotes the reward functions for the target objectives. In implementing the MORLHF algorithm, we first train a reward model for each objective based on random samples from $50\\%$ of the training data. Following the reward models we used in building the HH-RLHF dataset, we select the GPT2-large as foundation models for the reward model, and optimize the reward models using the following pair-wise loss functions: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r m}=-l o g(\\sigma(R_{c}-R_{r}-m a r g i n))\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\sigma$ denotes the Sigmoid function, $R_{c}$ and $R_{r}$ denote the reward output of the chosen response and the rejected response, and margin denotes the margin loss for the corresponding response pairs when multiple responses are ranked, such as in UltraFeedback. Secondly, we fine-tune the LLaMA2- Chat-7B policy model with the highest-ranked responses from the HH-RLHF and UltraFeedback datasets to obtain sub-optimal starting points for RLHF. The SFT training process is formalized as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a r g m a x\\operatorname{\\mathbb{E}}_{(q,y)\\sim\\mathcal{D}}\\left[P_{\\pi_{\\phi}}(y|q)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $q$ and $y$ denote the query and its corresponding highest-ranked response. Thirdly, following most works in RLHF, we leverage the PPO algorithm [25] to enable parameterized optimization of the policy model. In linear scalarization, we set $\\omega_{1}=\\bar{\\omega}_{2}=\\ldots=\\omega_{N}\\overset{\\cdot}{=}\\frac{1}{N}$ . For implementation, we build our MORLHF code based on the OpenRLHF library. ", "page_idx": 26}, {"type": "text", "text": "In calculating the GPU hours, we include the training hours for all reward models in HH-RLHF and UltraFeedback datasets, with a sum of seven reward model training processes. We also include the training hours for fine-tuning the original LLaMA2-Chat-7B model for reaching the sub-optimal starting points. Finally, the PPO training hours for HH-RLHF and UltraFeedback are included in the GPU hours. ", "page_idx": 26}, {"type": "text", "text": "G.4 Self-Refinement ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We include a prompt engineering-based self-refinement method as a baseline method to further demonstrate the effectiveness of MetaAligner. Specifically, this approach involves obtaining an initial response from the policy model and then prompting the same model to further refine its own output. For the second stage, we utilize the same prompting strategies as MetaAligner, which is detailed in Appendix C.2. However, this method often requires aligner models with strong in-context learning capabilities, leading to high inference costs due to larger model sizes or expensive commercial models. Therefore, we select LLaMA2-Chat-70B, a strong policy model as the target policy model for self-refinement. As self-refinement does not involve any training procedures, we do not report its GPU hours as other methods. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: all claims accurately reflect the paper\u2019s contributions and scope and are supported by experimental results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The limitations are discussed in Appendix A. They will be moved back to the main body once extra pages are allowed. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: the paper does not include theoretical results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: all experimental setting are provided in Appendix D. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: we release all codes, trained models, and part of the training data to facilitate the reproduction of our results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: all experimental setting are provided in Appendix D. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We mostly used GPT-4 as judges to rate the responses, a trustworthy but costly AI system. Repeating these evaluation processes or the LLM-based tuning algorithms would be too expensive and time-consuming. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: details about computational resources are provided in Appendix D. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: our research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: the broader impacts are discussed in Appendix A. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: the safeguards are discussed in Appendix A. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: all used assets (datasets, source codes) are properly cited and their licenses are listed in Appendix A. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: the proposed models, datasets, and codes are provided with documentation. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: the research didn\u2019t involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: the research didn\u2019t involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}]