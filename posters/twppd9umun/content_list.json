[{"type": "text", "text": "Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jie $\\mathbf{M}\\mathbf{a}^{\\dagger1^{*}}$ , Min $\\mathbf{H}\\mathbf{u}^{\\dagger1,4}$ , Pinghui Wang1, Wangchun $\\mathrm{\\mathbf{Sun}^{1}}$ , Lingyun Song2, Hongbin Pei1, Jun Liu3, Youtian Du1 ", "page_idx": 0}, {"type": "text", "text": "1 MOE KLINNS Lab, Xi\u2019an Jiaotong University, China   \n2 School of Computer Science, Northwestern Polytechnical University, China   \n3 School of Computer Science and Technology, Xi\u2019an Jiaotong University, China 4 China Mobile System Integration Co. \u2020 Equal Contribution \\* Corresponding Author ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs. Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness. Furthermore, current datasets may not provide a precise diagnostic for these methods. To tackle these challenges, firstly, we propose a novel dataset, MUSIC-AVQA-R, crafted in two steps: rephrasing questions within the test split of a public dataset (MUSIC-AVQA) and subsequently introducing distribution shifts to split questions. The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions. Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning. Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of $9.32\\%$ . Extensive ablation experiments are conducted on the two datasets mentioned to analyze the component effectiveness within the debiasing strategy. Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset. We also conduct experiments combining various baselines with our proposed strategy on two datasets to verify its plug-and-play capability. Our dataset and code are available at https://github.com/reml-group/MUSIC-AVQA-R. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Humans possess the extraordinary capacity to seamlessly integrate auditory and visual cues, effectively establishing a cohesive relationship between visual and auditory stimuli [1]. Audio-Visual Question Answering (AVQA) [2\u20135, 3] seeks to enable intelligent systems to acquire this capability and produce answers based on provided natural language questions. It requires the system to learn high-order interaction representations of the concepts encompassed with audio, video, and language modalities. As is known to us [6\u20138], the high-level reasoning ability of the system mainly relies on large-scale data that does not contain harmful biases or statistical regularities. ", "page_idx": 0}, {"type": "text", "text": "Nevertheless, completely avoiding the negative bias in datasets seems challenging. Previous studies [9\u201312] in visual and extractive QA have investigated the bias from the perspective of changing answer distributions and human-in-the-loop adversarial attacks. Drawing inspiration from these works, several open questions are proposed for the AVQA task, concerning model evaluations and model designs. ", "page_idx": 0}, {"type": "image", "img_path": "twpPD9UMUN/tmp/64276f5daa67907403811741dd580c13c7518b3a3b0da16ebe597364bc289c85.jpg", "img_caption": ["Figure 1: The question in current AVQA datasets is generated by a limited set of predefined templates, which may not be in line with the real-world scenario. Our findings indicate that existing methods [5, 1] such as STG [4] are not robust, which may be attributed to excessive bias learning, such as memorizing statistical regularities between critical question words and answers. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Question 1: have existing datasets comprehensively measured model robustness? The questions in the current AVQA dataset [13, 5, 14, 3] are generated by a limited set of predefined templates, such as the 33 templates in the MUSIC-AVQA dataset [4]. Fig. 1 shows the samples in the training and test split, which are produced using a predefined template. The observed difference mainly stems from a single word, leading to a limited vocabulary size of only 93 words. This has the potential to deviate from real-world scenarios. Moreover, current datasets cannot reflect the performance on rare or less common samples, which is an important indicator for evaluating model robustness [15, 16]. ", "page_idx": 1}, {"type": "text", "text": "Question 2: have existing methods overcome the data bias? We found that existing methods [5, 1, 17, 18] such as STG [4] are brittle for the question with rare answers. This may be attributed to memorizing the statistical regularity between critical question words and answers, such as the connection between \u201cIs\u201d, \u201cPlaying\u201d, and \u201cYes\u201d. Specifically, the experimental result [4] shows that STG achieves an accuracy of $54.09\\%$ on the test split of MUSIC-AVQA only given questions. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we present the development of a novel dataset called MUSIC-AVQA-R, which aims to address the first question precisely. The dataset complements MUSIC-AVQA [4] and provides a more refined diagnostic for current AVQA methods. To preserve the inherent bias, we maintain the original training and validation splits of the MUSIC-AVQA dataset. In contrast, we employ a human-machine collaboration mechanism to rephrase the question in the test split. This ensures diverse and natural question forms while remarkably expanding the number of questions from 9,129 to 211,572. We introduce a distribution shift based on answer distributions of specific question types. This allows us to measure performance on both frequent (in-distribution) and rare (out-of-distribution) data simultaneously. ", "page_idx": 1}, {"type": "text", "text": "To tackle the second question, we propose a robust framework that applies a Multifaceted Cycle Collaborative Debiasing (MCCD) strategy. Specifically, the strategy introduces a novel optimization objective, which enlarges the distribution difference between uni-modal (question, audio, and video) and multi-modal logit. By doing so, our model becomes less prone to learning biases from individual modalities. Intuitively, we cannot choose the correct answer based on only one modality. Hence, MCCD employs cycle guidance to constrain the logit distribution of each modality, thereby promoting the similarity of uni-modal logit distribution. The experimental results demonstrate that our framework yields significant improvements on both datasets, with a particularly notable enhancement of $9.32\\%$ observed on the MUSIC-AVQA-R dataset. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are fourfold: (1) We propose a novel dataset MUSIC-AVQA-R and a set of respective evaluation metrics. This enables us to thoroughly evaluate the reasoning behavior of AVQA models and characterize their generalization capabilities in in- and out-of-distribution scenarios. (2) We present an AVQA architecture that incorporates the MCCD strategy to overcome training biases. To the best of our knowledge, this is the first work to systematically explore biases in the AVQA task from model evaluations as well as model designs. (3) We conduct extensive experiments on MUSIC-AVQA and MUSIC-AVQA-R to verify the effectiveness and superiority of our proposed architecture and debiasing strategy. (4) We evaluate 13 recent multimodal QA methods on the proposed dataset and show their limited ability to generalize not only in-distribution scenarios but also in out-of-distribution situations. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Model Robustness Evaluation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Despite the notable achievements of QA datasets [19\u201323, 3], they suffer from biases, resulting in incomplete evaluations. In recent years, numerous studies have tackled this issue from various perspectives [24\u201327]. ", "page_idx": 2}, {"type": "text", "text": "One avenue of research [12, 28, 29] reorganizes existing datasets, thereby making the distribution between training and testing splits significantly different or even reversed. The reorganized datasets reflect the performance in the out-of-distribution situation but lack measurement in the in-distribution scenario. To this end, GQA-OOD [30] introduces the distribution shift in both the validation and test splits to assess visual QA models in both scenarios simultaneously. Nevertheless, the number of questions in the GQA-OOD test split is only 2,796, which may not reflect the real generalization ability of visual QA models due to the presence of a limited number of testing samples [31]. Inspired by the adversarial attack, another line of works [32, 33] regard the dataset construction as a game played by two parties: a human annotator and a well-trained model. Only samples generated by humans that successfully attack the model are incorporated into the dataset. In addition, there exists another line of work [14] that complements videos and questions to obtain balanced training data. ", "page_idx": 2}, {"type": "text", "text": "Different from the mentioned works, our dataset, MUSIC-AVQA-R, not only prioritizes question diversity but also considers the volume of test samples. This enhances the precision and comprehensiveness of robustness evaluation. Moreover, we recognize the formidable challenge of obtaining completely pure training data. As such, we opt to retain the inherent bias present in both the training and validation splits. Our primary objective is to inspire the community to enhance model robustness through the implementation of debiasing strategies, rather than striving for balanced training data. Remarkably, to the best of our knowledge, our dataset is the first AVQA dataset explicitly designed for robustness evaluation. ", "page_idx": 2}, {"type": "text", "text": "2.2 Bias Dependency Elimination ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A variety of debiasing QA methods [34\u201337] have been proposed to overcome bias memorization. These methods can be divided into four classes [24]: ensemble learning, data augmentation, contrastive learning, and answer re-ranking. ", "page_idx": 2}, {"type": "text", "text": "Ensemble learning methods [38, 28, 39, 6, 40, 41] typically leverage a combination of a bias learner and a vanilla QA model to comprehensively predict answers. Data augmentation methods [42\u201345] generate additional question-answer pairs to balance the data distribution. Based on the positive and negative sample generation, contrastive learning-based methods [46\u201348] strive to learn an embedding space where similar sample pairs are closely clustered while disparate ones are distinctly separated. Consequently, the vanilla QA method is optimized jointly through contrastive and QA losses. Answer re-ranking methods [34, 49\u201352] primarily focus on reordering the answers predicted by the vanilla QA model to enhance context comprehension, such as vision grounding. ", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, COCA [53] is the only work to mitigate the bias learning in the AVQA task, which first employs causal regularization to intervene bias-irrelevant causal effects and then introspects predictions. Unlike the mentioned works, which only consider language biases, our method considers audio, vision, language biases, and their collaboration. The proposed MCCD strategy features plug-and-play capability, enhancing the debiasing potential of baseline methods. ", "page_idx": 2}, {"type": "text", "text": "3 Dataset Creation and Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce the first dataset, MUSIC-AVQA-R, to evaluate the robustness of AVQA models. The construction of this dataset involves two key processes: rephrasing and splitting. The former involves the rephrasing of questions in the test split of MUSIC-AVQA, and the latter is dedicated to the categorization of questions into frequent (head) and rare (tail) subsets. ", "page_idx": 3}, {"type": "text", "text": "3.1 Rephrasing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The questions within the existing dataset [5, 4] are formulated using a restricted collection of predefined templates. To augment diversity and reality, we employ a rephrasing tool1 to rephrase each question 25 times. To ensure the rephrasing quality, three annotators participate in a verification process where their consensus through voting is required. They are all senior students in the field of information science, with one specializing in computer science and the other two in automation. Their extensive professional background equips them with the ability to assess whether the above rephrasing fulfills the requirement. Rephrasings are incorporated into the dataset only when two or more individuals validate the quality of the modifications. According to the statistics, $92.4\\%$ of rephrasings pass this validation, and the Fleiss Kappa value used to measure vote consistency is 0.839. Please see details in Table 4 of Appendix B. These results strongly suggest an exceptionally high quality of the rephrasing efforts. Fig. 2 illustrates the distribution of rephrased questions based on their initial three words. We see that our rephrasing questions have various formats, and the comparison between the two datasets is shown in Fig. 6 of Appendix B. The vocabulary size of our dataset is 465, which is $\\mathbf{5x}$ larger than MUSIC-AVQA. These results indicate that our dataset is more in line with the real-world scenario. Furthermore, an expansion of the question count within the test split has been implemented, escalating from 9,129 to 211,572 questions. This substantial increase in the volume of test samples enhances the precision of evaluations for AVQA methods. ", "page_idx": 3}, {"type": "text", "text": "3.2 Splitting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To provide a precise diagnostic for AVQA models, we introduce a distribution shift based on answer distributions of specific question types, following [30]. Guided by this distribution, we categorize rephrased questions into head and tail, enabling the assessment of in-distribution and outof-distribution performance, respectively. We also utilize the overall performance to assess the model effectiveness on the entire test split. ", "page_idx": 3}, {"type": "text", "text": "Specifically, to characterize the distribution shift, we first utilize the annotation for question types, including \u201cExistential\u201d, \u201cLocation\u201d, \u201cCounting\u201d, \u201cComparative\u201d, and \u201cTemporal\u201d, to group questions. Fig. 3(a) illustrates the answer distribution of the \u201cTemporal\u201d questions within the AVQA task. We see that the answer presents a long-tailed distribution. The distribution of other types is given in Appendix B. It is essential to note that MUSIC-AVQA encompasses three tasks: audio QA, visual QA, and AVQA. ", "page_idx": 3}, {"type": "text", "text": "Next, we characterize the answer balance using Shannon ent py, expressed as $\\begin{array}{r}{H(A)=-\\sum_{i=1}^{N}p(a_{i})\\log p(a_{i})}\\end{array}$ , where $H(A)$ is the e y of an answer $A$ for a certain question type, $N$ is the number of answer classes, and $p(a_{i})$ probability of answer class $i$ . It is important to note that the entropy depends on the number of answer classes, which exhibits significant variability across different question groups. To facilitate meaningful comparisons, we normalize $H(A)$ of each group by $\\begin{array}{r}{\\log(N)\\colon\\bar{H}(A)\\ =\\ \\frac{H(A)}{\\log(N)}}\\end{array}$ , with $\\log(N)$ representing the entropy of a uniform distribution of size $N$ . Refer to Appendix C for detailed proof. Thus, the normalized entropy ${\\bar{H}}(A)$ indicates the proximity of the distribution $H(A)$ to a ", "page_idx": 3}, {"type": "image", "img_path": "twpPD9UMUN/tmp/6f72e61e5d0a6004c7ae7bf8ec782767ff81681e99326efe8064ca117a04e9b4.jpg", "img_caption": ["Figure 2: Distribution of rephrasing questions based on the first three words. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "twpPD9UMUN/tmp/83c6a1141420b789d842c003a31121e99c2ac48f67dbacb2b527df477378c672.jpg", "img_caption": ["(a) Answer distributions of \u201cTemporal\u201d questions in the AVQA task. (b) Statistics of head and tail samples. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Statistics visualization for MUSIC-AVQA-R. $\\mu(a)$ is the average number of answers in a group. The dark color on the right denotes the number of head samples, while the light-colored area denotes that of tail samples. ", "page_idx": 4}, {"type": "text", "text": "uniform distribution. We preserve the group with a normalized entropy below a threshold of 0.9, which aims at selecting imbalanced groups. ", "page_idx": 4}, {"type": "text", "text": "Finally, we categorize the samples into head and tail classes. We define the tail class as class $i$ with $|a_{i}|\\leq1.2\\mu(a)$ following [30], where $\\left|a_{i}\\right|$ represents the number of samples belonging to answer class $i$ , and $\\mu(a)$ denotes the average sample count for a group. Consequently, the tail samples are rare, while the head samples are more prevalent within a group. Fig. 3(b) illustrates the statistics of head and tail samples across various groups within each task. ", "page_idx": 4}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To mitigate bias learning, we propose a robust AVQA architecture that integrates a multifaceted cycle collaborative debiasing strategy. Fig. 4 illustrates an overview of our proposed architecture. It first learns the uni-modal and multimodal representations using a pre-trained model. Then, the architecture utilizes distinct bias learners to capture uni-modal biases. Finally, a collaborative debiasing strategy is leveraged to magnify the disparity between fusion logit and bias logit, obtained based on multimodality and uni-modality representations, respectively. Meanwhile, a cycle guidance mechanism is employed to maintain the similarity between bias logit. The aforementioned procedure is only carried out in the test split of MUSIC-AVQA. Consequently, our proposed dataset, MUSIC-AVQA-R, allows for a more precise and comprehensive evaluation of models handling data biases. ", "page_idx": 4}, {"type": "text", "text": "Uni-modal Embedding. Given an AVQA sample comprising a video sequence and a corresponding question, we initially partition the sequence, consisting of visual and audio tracks, into $T$ nonoverlapping pairs of visual and audio segments, denoted as $\\{V_{t},A_{t}\\}_{t=1}^{T}$ , where each segment spans one second. Subsequently, a distinct embedding layer is employed to acquire uni-modal embeddings. Specifically, we employ a pre-trained VGGish model [54] with fixed parameters, which is a VGG-like audio processing network, to obtain an audio embedding vector. For video embedding vectors, we employ a pre-trained ResNet-18 with fixed parameters on the frames. The VisualBert model [55] is applied to obtain a word-level question embedding vector. To ensure dimension matching, distinct linear layers are applied to the aforementioned vectors, resulting in uni-modal embeddings $\\mathbf{A}_{i}^{\\mathrm{e}},\\mathbf{V}_{i}^{\\mathrm{e}}\\in\\mathbb{R}^{T\\times7\\tilde{\\mathrm{6}}8}$ and $\\mathbf{Q}_{i}^{\\mathrm{e}}\\in\\mathbb{R}^{l\\times768}$ , where $l$ is the question length. ", "page_idx": 4}, {"type": "text", "text": "Uni- and Multi-modal Representation. We leverage VisualBert to obtain both uni-modal and multimodal representations, represented as ${\\bf A}_{i}^{\\mathrm{c}}$ , $\\mathbf{V}_{i}^{\\mathrm{c}}$ , $\\mathbf{Q}_{i}^{\\mathrm{c}}$ , $\\bar{\\mathbf{M}}_{i}^{\\mathrm{c}}\\in\\mathbb{R}^{768}$ . In the case of uni-modal learning, we exclusively input the aforementioned uni-modal embeddings to VisualBert. For multi-modal learning, we treat question embeddings as queries, concatenate video and audio embeddings as context, and leverage VisualBert to perform multi-modal interaction. Then, we apply a linear projection on the representation to obtain the multi-modality logit $\\hat{\\mathbf{y}}_{i}^{\\mathrm{m}}\\in\\mathbb{R}^{42}$ , where 42 denotes the number of possible answers. ", "page_idx": 4}, {"type": "text", "text": "Uni-modal Bias Learning. AVQA may involve various harmful uni-modal biases, encompassing biases associated with audio, video, and language, respectively. To capture these uni-modal biases, we utilize a bias learner that takes only one of the three modalities as input. Specifically, distinct non-linear multi-layer perceptron layers serve as the learners, producing the corresponding logit $\\hat{\\mathbf{y}}_{i}^{\\mathrm{a}},\\hat{\\mathbf{y}}_{i}^{\\mathrm{v}},\\hat{\\mathbf{y}}_{i}^{\\mathrm{q}}\\in\\mathbb{R}^{42}$ on the answer space. It should be noted that these bias learners are removed during the testing stage. ", "page_idx": 4}, {"type": "image", "img_path": "twpPD9UMUN/tmp/afbe6492b397499351dbecd1489d05e79c3137adfc3a085c63a30ad25459243c.jpg", "img_caption": ["Figure 4: Robust AVQA architecture to overcome bias learning. Our MCCD strategy is plug-and-play, allowing seamless integration with other AVQA methods. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Collaborative Debiasing. To eliminate bias learning, we propose a multifaceted cycle collaborative debiasing (MCCD) strategy. It first reduces the bias impact from multiple views by enlarging the dissimilarity between uni-modal and multi-modal logit. This discrepancy enlargement ${\\mathcal{L}}_{\\mathrm{d}}$ is implemented by the joint inverse distance: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{d}}=\\frac{\\alpha}{3K}\\sum_{i=1}^{K}\\left(\\frac{1}{d_{i}^{\\mathrm{a}}}+\\frac{1}{d_{i}^{\\mathrm{v}}}+\\frac{1}{d_{i}^{\\mathrm{q}}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $K$ is the batch size, $\\alpha$ is used to balance optimization, $d_{i}^{\\mathrm{a}}$ denotes the Euclidean distance between audio logit and multi-modality logit, $d_{i}^{\\mathrm{v}}$ represents the distance between video logit and multi-modality logit, $d_{i}^{\\mathrm{q}}$ is the distance between question logit and multi-modality logit, and $\\epsilon=1e{-5}$ is added to the denominator to avoid division by zero. ", "page_idx": 5}, {"type": "text", "text": "Intuitively, relying solely on one modality for answer prediction may result in similar logit distributions. Therefore, MCCD employs cycle guidance to constrain the distribution of uni-modal logit. This guidance $\\mathcal{L}_{\\mathrm{c}}$ is implemented by the Kullback\u2013Leibler divergence: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal L_{\\mathrm c}=\\frac{\\beta}{3}\\left(\\mathcal L_{\\mathrm{qa}}+\\mathcal L_{\\mathrm{av}}+\\mathcal L_{\\mathrm{vq}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta$ is the factor to control weight, $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{qa}}=\\frac{1}{K}\\sum_{i=1}^{K}\\hat{\\mathbf{y}}_{i}^{\\mathrm{q}}\\left(\\log\\hat{\\mathbf{y}}_{i}^{\\mathrm{q}}-\\log\\hat{\\mathbf{y}}_{i}^{\\mathrm{a}}\\right)}\\end{array}$ denotes the relative entropy between the question $\\hat{\\mathbf{y}}_{i}^{\\mathrm{q}}$ and audio logit $\\hat{\\mathbf{y}}_{i}^{\\mathrm{a}}$ , $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{av}}\\,=\\,\\frac{1}{K}\\sum_{i=1}^{K}\\hat{\\mathbf{y}}_{i}^{\\mathrm{a}}\\left(\\log\\hat{\\mathbf{y}}_{i}^{\\mathrm{a}}-\\log\\hat{\\mathbf{y}}_{i}^{\\mathrm{v}}\\right)}\\end{array}$ is the relative entropy between the audio $\\hat{\\mathbf{y}}_{i}^{\\mathrm{a}}$ and video logit $\\hat{\\mathbf{y}}_{i}^{\\mathrm{v}}$ , and $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{vq}}=\\frac{1}{K}\\sum_{i=1}^{K}\\hat{\\mathbf{y}}_{i}^{\\mathrm{v}}\\left(\\log\\hat{\\mathbf{y}}_{i}^{\\mathrm{v}}-\\log\\hat{\\mathbf{y}}_{i}^{\\mathrm{q}}\\right)}\\end{array}$ represents the relative entropy between the video $\\hat{\\mathbf{y}}_{i}^{\\mathrm{v}}$ and question logit $\\hat{\\mathbf{y}}_{i}^{\\mathrm{q}}$ . ", "page_idx": 5}, {"type": "text", "text": "Finally, we utilize the summation of $\\mathcal{L}_{\\mathrm{d}},\\;\\mathcal{L}_{\\mathrm{c}}$ and $\\mathcal{L}_{\\mathrm{a}}$ to optimize the parameters of our method. $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{a}}=-\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{y}_{i}^{\\mathrm{f}}\\log\\hat{\\mathbf{y}}_{i}^{\\mathrm{f}}}\\end{array}$ is the loss of answer prediction that is regarded as a multi-classification problem, where $\\mathbf{y}_{i}^{\\mathrm{{f}}},\\hat{\\mathbf{y}}_{i}^{\\mathrm{{f}}}$ denote the one-hot answer label and logit of multi-modality fusion, respectively. The training details are shown in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Dataset and Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "MUSIC-AVQA [4], which contains training, validation, and testing splits with 31,927, 4,568, and 9,129 QA pairs, is developed by gathering questions for 9,288 musical performances. The questions are produced by a limited set of pre-defined templates. The videos, sourced from YouTube, include solo performances, ensembles of the same instruments, and ensembles of different instruments. This dataset consists of three tasks: audio QA, visual QA, and AVQA. The standard accuracy is used to evaluate model performance on the mentioned tasks. To comprehensively assess model robustness, we conduct rephrasing and splitting on the test split, expanding the question count from 9,129 to 211,572. Owing to the introduction of distribution shift, our proposed dataset provides three metrics: head accuracy, tail accuracy, and overall accuracy, to evaluate models precisely. The test split comparison between MUSIC-AVQA and MUSIC-AVQA-R is shown in Appendix D.1. We can see that the latter exhibits a larger test sample space. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "twpPD9UMUN/tmp/c8f0f9d1319f8ea8daf6f53016b977d174cf65e5b94f69f44058f572b57979ac.jpg", "table_caption": ["Table 1: Experimental results $(\\%)$ on the MUSIC-AVQA test split. EXIST, LOC, CNT, COMP, and TEMP, which are question types, denote \u201cExistential\u201d, \u201cLocation\u201d, \u201cCounting\u201d, \u201cComparative\u201d, and \u201cTemporal\u201d, respectively. Avg. denotes the average accuracy. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "During the data pre-processing stage, audio and video are sampled at rates of $16\\;\\mathrm{kHz}$ and 1 fps, respectively. In the uni-modal embedding module, we employ ResNet-18 and VGGish to obtain 512-dimensional and 128-dimensional embeddings of the visual and audio segments, respectively. In the uni-modal bias learning module, the hidden layer size of the bias learner is set to 768. In the collaborative debiasing module, we set the factors $\\alpha$ and $\\beta$ to $_{1e-2}$ and $3e{-1}$ for optimization equilibrium, respectively. During the training stage, the initial learning rate is set to $3e{-5}$ , decaying by 0.5 every 20 epochs. The maximum epoch and batch size are set to 150 and 64, respectively. We use the Adam optimizer to train our architecture and save the model that achieves the best performance on the validation split. The experiments for all methods, except LAVisH, are conducted using a single NVIDIA Tesla V100 GPU. The experiment for LAVisH is run on two NVIDIA Tesla A100 GPUs. The other details are shown in Appendix D.2. ", "page_idx": 6}, {"type": "text", "text": "5.3 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We select 14 previous state-of-the-art multi-modal QA methods as baselines to verify the effectiveness of the proposed architecture and investigate the robustness of these methods. Audio QA methods: FCNLSTM [56], and CONVLSTM [56]. Visual QA methods: GRU [57], BiLSTM Attn [58], HCAttn [59], and MCAN [60]. Video QA methods: HME [61], PSAC [62], and HCRN [63]. AVQA methods: AVSD [13], LAViT [5], STG [4], COCA [53] and LAVisH [1]. We abstain from reassessing COCA on the MUSIC-AVQA-R dataset due to its lack of publicly available code. The baseline introductions are shown in Appendix D.3. Due to the particularly slow computation speed of STG, we do not conduct experiments with STG $^+$ MCCD. Due to computing power limitations, we reevaluate LAVisH with a batch size of 2. ", "page_idx": 6}, {"type": "table", "img_path": "twpPD9UMUN/tmp/b853e3b2c2b1822ca59976d3d2f9d5ac8150b9e52ee65d892c59c5b03b1d5e58.jpg", "table_caption": ["Table 2: Experimental results $(\\%)$ on the MUSIC-AVQA-R test split. The question types, such as CNT and COMP, are introduced in Table 1. H and $\\mathrm{T}$ denote the head and tail accuracy. There is no publicly available code for COCA. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.4 Comparison on MUSIC-AVQA ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments on the MUSIC-AVQA test split to validate the effectiveness of the proposed architecture. The results are presented in Table 1. All methods, except LAVisH, utilize ResNet-18 to encode visual features, whereas LAVisH employs stronger models such as ViT [64] or Swin [65] to acquire visual representations. We first analyze the comparison under the same visual encoding conditions. Notably, compared with COCA, our architecture obtains significant improvements of $4.53\\%$ and $4\\%$ in the audio and visual QA tasks, respectively. It also achieves the best performance in the AVQA task. Furthermore, our architecture obtains a new state-of-the-art result of $72.20\\%$ on the whole question. It is worth mentioning that visual QA methods, like GRU, and video QA methods, such as HME, also exhibit competitive results in the AVQA task, despite lacking one modality as input. We also observe that LAVisH, proposed based on STG, introduces trainable parameters into robust visual encoders, thereby achieving superior results compared to methods employing weaker visual encoders. The results on this dataset can demonstrate the efficacy of these methods to some extent. However, MUSIC-AVQA lacks refined and precise evaluations due to its inherent shortcomings that are introduced in Section 1. Consequently, it is insufficient to evaluate these methods only on this dataset. ", "page_idx": 7}, {"type": "text", "text": "5.5 Robustness Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments on the MUSIC-AVQA-R test split to explore the robustness of the aforementioned methods. Their released codes are employed to conduct this experiment. The results are presented in Table 2. Several crucial insights arise when combining the results from this table with those from Table 1. Firstly, audio QA methods, such as CONVLSTM, showcase competitive robustness and even achieve the highest tail accuracy on EXIST questions within the AVQA task. Secondly, the visual QA method MCAN demonstrates noteworthy robustness by obtaining the second-best performance on the test split. Thirdly, the video QA baseline experiences a relatively significant performance degradation, with PSAC, for instance, declining by $16.45\\%$ . Notably, HCRN exhibits the lowest performance on both datasets, indicating its poor robustness. Furthermore, the results of AVQA baselines lag behind other types of QA methods like HME, suggesting that their strong performance on the MUSIC-AVQA dataset may rely on memorizing statistical regularities between input modalities and answers. Ultimately, our architecture outperforms others on the test split. Beneftiing from the MCCD strategy, it attains the highest head (in-distribution setting) and tail (out-of-distribution setting) results across various question types, providing further evidence of its superior robustness. We also show the overall accuracy of these methods on each type of question. Please see the details in Appendix D.4. It can be seen that our architecture achieves the best overall accuracy on each type of question. ", "page_idx": 7}, {"type": "text", "text": "To validate the plug-and-play capability of MCCD, we conduct extensive experiments using the baselines+MCCD on the aforementioned datasets. The results are presented in Tables 1 and 2. We observe that MCCD consistently improves performance across most methods on MUSIC-AVQA (9 out of 13) and MUSIC-AVQA-R (11 out of 13), respectively. This underscores the robust debiasing capability of MCCD in a plug-and-play manner. ", "page_idx": 8}, {"type": "text", "text": "5.6 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To verify the debiasing effectiveness of MCCD, we conduct extensive experiments on both the test split of MUSIC-AVQA and MUSIC-AVQAR. The results are shown in Table 3. Firstly, we validate the contribution of the component within multifaceted debiasing. It can be seen that removing the component will lead to an overall performance improvement in some aspects of MUSIC-AVQA while resulting in a significant decrease in our dataset. This observa", "page_idx": 8}, {"type": "text", "text": "Table 3: Ablation results $(\\%)$ on the test split of MUSICAVQA and our dataset. AQA and VQA denote audio QA, and visual QA, respectively. di(#)is the distance between the (#) logit and the multi-modality logit. MD: multifaceted debiasing. CG: cycle guidance. ", "page_idx": 8}, {"type": "table", "img_path": "twpPD9UMUN/tmp/0b7a9f9e5794dfe6834411494d31be474c83c2086be79fb17c239220cc30056e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "tion strongly supports the debiasing efficacy of these components. Secondly, we verify the overall contribution of the multifaceted debiasing. It can be seen that the performance decrease of $0.72\\%$ and $2.15\\%$ occurs in both datasets, respectively. Finally, we validate the contribution of cycle guidance. We see that this model variant obtains the best performance on the MUSIC-AVQA dataset. However, there was a noticeable performance degradation in our proposed dataset. In summary, each component plays a distinctive role in the debiasing process, which is further demonstrated by the performance degradation on the head and tail samples. ", "page_idx": 8}, {"type": "text", "text": "5.7 Sensitivity and Qualitative Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We employ the control variable method to perform a sensitivity analysis on the weight-controlling factors of the MCCD strategy. The results are presented in the left part of Fig. 5. Our findings indicate stable optimization across various settings, except for the case with $\\alpha=0.008$ and $\\beta=0.3$ . Upon conducting further experimental analysis, we identify the issue as originating from the model\u2019s failure to converge. Moreover, we visualize the attention weight on the uniformly sampled audio and video frames to qualitatively analyze the debiasing capability of our method. The visualization, displayed in the right part of Fig. 5, reveals that crucial audio and video frames for QA consistently receive significant attention, both in in- and out-of-distribution settings. This further demonstrates that our method predicts answers through the grounding capabilities of audio and vision rather than relying on bias learning. More cases are shown in Appendix D.5. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Limitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We are the first to investigate bias learning in the AVQA task from model evaluation and design aspects. On the one hand, we construct a new dataset, MUSIC-AVQA-R, which evaluates the performance on the head, tail, and overall samples, providing a precise measure of model robustness. On the other hand, we introduce a robust architecture employing the MCCD strategy to mitigate bias learning. Extensive experiments demonstrate the effectiveness of our architecture and the plug-andplay debiasing capability of MCCD. Furthermore, we reevaluate previous multi-modal QA methods on our proposed dataset, revealing their poor robustness. ", "page_idx": 8}, {"type": "text", "text": "Due to constraints imposed by MUSIC-AVQA, the answer space of our dataset is limited, comprising only 42 classes, and answer lengths are typically confined to a single word. This deviation from real-world scenarios is noteworthy. Concerning model designs, for a fair comparison with baselines, we do not select large generative models to be backbones. However, compared with the answer classification, it may be more useful to generate answers for the AVQA task. ", "page_idx": 8}, {"type": "image", "img_path": "twpPD9UMUN/tmp/89d2637f2a7ab62ed7a3259d013d854ef9650d502ce3682d3a94871ec0666e7c.jpg", "img_caption": ["Figure 5: Sensitivity and qualitative analysis. $\\alpha$ and $\\beta$ are the weight-controlling factors in the MCCD strategy. We visualize attention weights on the uniformly sampled audio and video frames. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Key Research and Development Program of China (2021YFB1715600), the National Natural Science Foundation of China (U22B2019, 62477037, 62450005, 62437002, 62306229, 62293553), the Natural Science Basic Research Program of Shaanxi (2023-JC-YB-593), the Youth Innovation Team of Shaanxi Universities \u201cMulti-modal Data Mining and Fusion\u201d, the Shaanxi Undergraduate and Higher Education Teaching Reform Research Program (23BY195), the Youth Talent Support Program of Shaanxi Science and Technology Association (20240113), the Xi\u2019an Jiaotong University-China Mobile Communications Group Co., Ltd. Digital Government Joint Institute, and the China Postdoctoral Science Foundation (2024M752585). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Y.-B. Lin, Y.-L. Sung, J. Lei, M. Bansal, and G. Bertasius, \u201cVision transformers are parameter-efficient audio-visual learners,\u201d in CVPR, 2023, pp. 2299\u20132309. [Online]. Available: https://doi.org/10.1109/CVPR52729.2023.00228   \n[2] H. Alamri, V. Cartillier, A. Das, J. Wang, A. Cherian, I. Essa, D. Batra, T. K. Marks, C. Hori, P. Anderson et al., \u201cAudio visual scene-aware dialog,\u201d in CVPR, 2019, pp. 7558\u20137567. [Online]. Available: http://openaccess.thecvf.com/content_CVPR_2019/html/Alamri_Audio_Visual_ Scene-Aware_Dialog_CVPR_2019_paper.html   \n[3] P. Yang, X. Wang, X. Duan, H. Chen, R. Hou, C. Jin, and W. Zhu, \u201cAVQA: A dataset for audio-visual question answering on videos,\u201d in ACM MM, 2022, pp. 3480\u20133491. [Online]. Available: https://doi.org/10.1145/3503161.3548291   \n[4] G. Li, Y. Wei, Y. Tian, C. Xu, J.-R. Wen, and D. Hu, \u201cLearning to answer questions in dynamic audio-visual scenarios,\u201d in CVPR, 2022, pp. 19 108\u201319 118. [Online]. Available: https://doi.org/10.1109/CVPR52688.2022.01852   \n[5] H. Yun, Y. Yu, W. Yang, K. Lee, and G. Kim, \u201cPano-AVQA: Grounded audio-visual question answering on $360^{\\circ}$ videos,\u201d in CVPR, 2021, pp. 2031\u20132041. [Online]. Available: https://doi.org/10.1109/ICCV48922.2021.00204   \n[6] Z. Wen, G. Xu, M. Tan, Q. Wu, and Q. Wu, \u201cDebiased visual question answering from feature and sample perspectives,\u201d in NeurIPS, 2021, pp. 3784\u20133796. [Online]. Available: https:// proceedings.neurips.cc/paper/2021/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html   \n[7] M. Vatsa, A. Jain, and R. Singh, \u201cAdventures of trustworthy vision-language models: A survey,\u201d in AAAI, 2024, pp. 22 650\u201322 658. [Online]. Available: https: //doi.org/10.1609/aaai.v38i20.30275   \n[8] S. M. Hall, F. Gon\u00e7alves Abrantes, H. Zhu, G. Sodunke, A. Shtedritski, and H. R. Kirk, \u201cVisogender: A dataset for benchmarking gender bias in image-text pronoun resolution,\u201d in NeurIPS, 2024. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/ c93f26b1381b17693055a611a513f1e9-Abstract-Datasets_and_Benchmarks.html   \n[9] Y. Li, B. Hu, F. Zhang, Y. Yu, J. Liu, Y. Chen, and J. Xu, \u201cA multi-modal debiasing model with dynamical constraint for robust visual question answering,\u201d in Findings of ACL, 2023, pp. 5032\u20135045. [Online]. Available: https://doi.org/10.18653/v1/2023.findings-acl.311   \n[10] A. Ravichander, J. Stacey, and M. Rei, \u201cWhen and why does bias mitigation work?\u201d in Findings of EMNLP, 2023, pp. 9233\u20139247. [Online]. Available: https: //aclanthology.org/2023.findings-emnlp.619   \n[11] J. Miller, K. Krauth, B. Recht, and L. Schmidt, \u201cThe effect of natural distribution shift on question answering models,\u201d in ICML, 2020, pp. 6905\u20136916. [Online]. Available: http://proceedings.mlr.press/v119/miller20a.html   \n[12] A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi, \u201cDon\u2019t just assume; look and answer: Overcoming priors for visual question answering,\u201d in CVPR, 2018, pp. 4971\u20134980. [Online]. Available: http://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_ Just_Assume_CVPR_2018_paper.html   \n[13] I. Schwartz, A. G. Schwing, and T. Hazan, \u201cA simple baseline for audiovisual scene-aware dialog,\u201d in CVPR, 2019, pp. 12 548\u201312 558. [Online]. Available: http://openaccess.thecvf.com/content_CVPR_2019/html/Schwartz_A_Simple_Baseline_ for_Audio-Visual_Scene-Aware_Dialog_CVPR_2019_paper.html   \n[14] X. Liu, Z. Dong, and P. Zhang, \u201cTackling data bias in music-avqa: Crafting a balanced dataset for unbiased question-answering,\u201d in WACV, 2024, pp. 4478\u20134487. [Online]. Available: https://doi.org/10.1109/WACV57701.2024.00442   \n[15] X. Zhang, F. Zhang, and C. Xu, \u201cNext-ood: Overcoming dual multiple-choice VQA biases,\u201d IEEE TPAMI, vol. 46, no. 4, pp. 1913\u20131931, 2024. [Online]. Available: https://doi.org/10.1109/TPAMI.2023.3269429   \n[16] B. Zhu, K. Tang, Q. Sun, and H. Zhang, \u201cGeneralized logit adjustment: Calibrating fine-tuned models by removing label bias in foundation models,\u201d in NeurIPS, 2023. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/ cbe1fd3136e0f049bb8bc104231ccb99-Abstract-Conference.html   \n[17] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, \u201cImageBind: One embedding space to bind them all,\u201d in CVPR, 2023, pp. 15 180\u201315 190. [Online]. Available: https://doi.org/10.1109/CVPR52729.2023.01457   \n[18] H. Zhang, X. Li, and L. Bing, \u201cVideo-LLaMA: An instruction-tuned audio-visual language model for video understanding,\u201d in EMNLP (Demos), 2023, pp. 543\u2013553. [Online]. Available: https://aclanthology.org/2023.emnlp-demo.49   \n[19] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \u201cSquad: $100{,}000{+}$ questions for machine comprehension of text,\u201d in EMNLP, 2016, pp. 2383\u20132392. [Online]. Available: https://doi.org/10.18653/v1/d16-1264   \n[20] Y. Goyal, T. Khot, A. Agrawal, D. Summers-Stay, D. Batra, and D. Parikh, \u201cMaking the v in VQA matter: Elevating the role of image understanding in visual question answering,\u201d IJCV, vol. 127, pp. 398\u2013414, 2019. [Online]. Available: https://doi.org/10.1007/s11263-018-1116-0   \n[21] D. A. Hudson and C. D. Manning, \u201cGQA: A new dataset for real-world visual reasoning and compositional question answering,\u201d in CVPR, 2019, pp. 6700\u20136709. [Online]. Available: http://openaccess.thecvf.com/content_CVPR_2019/html/Hudson_GQA_A_New_ Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html   \n[22] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, \u201cZero-shot video question answering via frozen bidirectional language models,\u201d in NeurIPS, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022, pp. 124\u2013141. [Online]. Available: http://papers.nips.cc/paper_files/paper/2022/hash/ 00d1f03b87a401b1c7957e0cc785d0bc-Abstract-Conference.html   \n[23] Y. Li, W. Li, and L. Nie, \u201cMmcoqa: Conversational question answering over text, tables, and images,\u201d in ACL, 2022, pp. 4220\u20134231. [Online]. Available: https: //doi.org/10.18653/v1/2022.acl-long.290   \n[24] J. Ma, P. Wang, D. Kong, Z. Wang, J. Liu, H. Pei, and J. Zhao, \u201cRobust visual question answering: Datasets, methods, and future challenges,\u201d IEEE TPAMI, vol. 46, no. 8, pp. 5575\u20135594, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2307.11471   \n[25] C. Kervadec, C. Wolf, G. Antipov, M. Baccouche, and M. Nadri, \u201cSupervising the transfer of reasoning patterns in vqa,\u201d in NeurIPS, vol. 34, 2021, pp. 18 256\u201318 267. [Online]. Available: https://proceedings.neurips.cc/paper/2021/hash/ 9766527f2b5d3e95d4a733fcfb77bd7e-Abstract.html   \n[26] S. Ramakrishnan, A. Agrawal, and S. Lee, \u201cOvercoming language priors in visual question answering with adversarial regularization,\u201d in NeurIPS, 2018, pp. 1548\u20131558. [Online]. Available: https://proceedings.neurips.cc/paper/2018/hash/ 67d96d458abdef21792e6d8e590244e7-Abstract.html   \n[27] C. Zheng, H. Zhou, F. Meng, J. Zhou, and M. Huang, \u201cLarge language models are not robust multiple choice selectors,\u201d in ICLR, 2023. [Online]. Available: https: //doi.org/10.48550/arXiv.2309.03882   \n[28] M. Ko, J. Lee, H. Kim, G. Kim, and J. Kang, \u201cLook at the first sentence: Position bias in question answering,\u201d in EMNLP, 2020, pp. 1109\u20131121. [Online]. Available: https://doi.org/10.18653/v1/2020.emnlp-main.84   \n[29] C. Dancette, R. Cadene, D. Teney, and M. Cord, \u201cBeyond question-based biases: Assessing multimodal shortcut learning in visual question answering,\u201d in ICCV, 2021, pp. 1574\u20131583. [Online]. Available: https://doi.org/10.1109/ICCV48922.2021.00160   \n[30] C. Kervadec, G. Antipov, M. Baccouche, and C. Wolf, \u201cRoses are red, violets are blue... but should VQA expect them to?\u201d in CVPR, 2021, pp. 2776\u20132785. [Online]. Available: https://openaccess.thecvf.com/content/CVPR2021/html/Kervadec_Roses_Are_Red_ Violets_Are_Blue..._but_Should_VQA_Expect_CVPR_2021_paper.html   \n[31] F. E. Harrell Jr, K. L. Lee, and D. B. Mark, \u201cMultivariable prognostic models: Issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors,\u201d Statistics in medicine, vol. 15, no. 4, pp. 361\u2013387, 1996. [Online]. Available: https://onlinelibrary.wiley.com/doi/epdf/10.1002/%28SICI%291097-0258% 2819960229%2915%3A4%3C361%3A%3AAID-SIM168%3E3.0.CO%3B2-4   \n[32] S. Sheng, A. Singh, V. Goswami, J. Magana, T. Thrush, W. Galuba, D. Parikh, and D. Kiela, \u201cHuman-adversarial visual question answering,\u201d in NeurIPS, 2021, pp. 20 346\u201320 359. [Online]. Available: https://proceedings.neurips.cc/paper/2021/hash/ aa97d584861474f4097cf13ccb5325da-Abstract.html   \n[33] L. Li, J. Lei, Z. Gan, and J. Liu, \u201cAdversarial VQA: A new benchmark for evaluating the robustness of VQA models,\u201d in ICCV, 2021, pp. 2042\u20132051. [Online]. Available: https://doi.org/10.1109/ICCV48922.2021.00205   \n[34] J. Wu and R. J. Mooney, \u201cSelf-critical reasoning for robust visual question answering,\u201d in NeurIPS, 2019, pp. 8604\u20138614. [Online]. Available: https://proceedings.neurips.cc/paper/2019/ hash/33b879e7ab79f56af1e88359f9314a10-Abstract.html   \n[35] W. Xu, Q. Liu, S. Wu, and L. Wang, \u201cCounterfactual debiasing for fact verification,\u201d in ACL, 2023, pp. 6777\u20136789. [Online]. Available: https://doi.org/10.18653/v1/2023.acl-long.374   \n[36] C. Tsirigotis, J. Monteiro, P. Rodr\u00edguez, D. V\u00e1zquez, and A. C. Courville, \u201cGroup robust classification without any group information,\u201d in NeurIPS, 2023. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/ b0d9ceb3d11d013e55da201d2a2c07b2-Abstract-Conference.html   \n[37] D. Esiobu, X. Tan, S. Hosseini, M. Ung, Y. Zhang, J. Fernandes, J. Dwivedi-Yu, E. Presani, A. Williams, and E. Smith, \u201cRobbie: Robust bias evaluation of large generative language models,\u201d in EMNLP, 2023, pp. 3764\u20133814. [Online]. Available: https://doi.org/10.18653/v1/2023.emnlp-main.230   \n[38] R. Cadene, C. Dancette, H. Ben-younes, M. Cord, and D. Parikh, \u201cRUBi: Reducing unimodal biases for visual question answering,\u201d in NeurIPS, 2019, pp. 841\u2013852. [Online]. Available: https://proceedings.neurips.cc/paper/2019/hash/ 51d92be1c60d1db1d2e5e7a07da55b26-Abstract.html   \n[39] Y. Niu and H. Zhang, \u201cIntrospective distillation for robust question answering,\u201d in NeurIPS, 2021, pp. 16 292\u201316 304. [Online]. Available: https://proceedings.neurips.cc/paper/2021/hash/ 878d5691c824ee2aaf770f7d36c151d6-Abstract.html   \n[40] J. Cho, D. Kim, H. Ryu, and I. S. Kweon, \u201cGenerative bias for robust visual question answering,\u201d in CVPR, 2023, pp. 11 681\u201311 690. [Online]. Available: https: //doi.org/10.1109/CVPR52729.2023.01124   \n[41] J. Ma, P. Wang, Z. Wang, D. Kong, M. Hu, T. Han, and J. Liu, \u201cAdaptive loose optimization for robust question answering,\u201d arXiv preprint arXiv:2305.03971, 2023. [Online]. Available: https://arxiv.org/pdf/2305.03971   \n[42] G. Kv and A. Mittal, \u201cReducing language biases in visual question answering with visually-grounded question encoder,\u201d in ECCV, 2020, pp. 18\u201334. [Online]. Available: https://doi.org/10.1007/978-3-030-58601-0_2   \n[43] E. Abbasnejad, D. Teney, A. Parvaneh, J. Shi, and A. v. d. Hengel, \u201cCounterfactual vision and language learning,\u201d in CVPR, 2020, pp. 10 044\u201310 054. [Online]. Available: https://openaccess.thecvf.com/content_CVPR_2020/html/Abbasnejad_Counterfactual_ Vision_and_Language_Learning_CVPR_2020_paper.html   \n[44] L. Chen, Y. Zheng, and J. Xiao, \u201cRethinking data augmentation for robust visual question answering,\u201d in ECCV, 2022, pp. 95\u2013112. [Online]. Available: https://doi.org/10.1007/ 978-3-031-20059-5_6   \n[45] D. Teney, E. Abbasnejad, and A. van den Hengel, \u201cUnshuffling data for improved generalization in visual question answering,\u201d in ICCV, 2021, pp. 1417\u20131427. [Online]. Available: https://doi.org/10.1109/ICCV48922.2021.00145   \n[46] Z. Liang, W. Jiang, H. Hu, and J. Zhu, \u201cLearning to contrast the counterfactual samples for robust visual question answering,\u201d in EMNLP, 2020, pp. 3285\u20133292. [Online]. Available: https://doi.org/10.18653/v1/2020.emnlp-main.265   \n[47] X. Zhu, Z. Mao, C. Liu, P. Zhang, B. Wang, and Y. Zhang, \u201cOvercoming language priors with self-supervised learning for visual question answering,\u201d in IJCAI, 2021, pp. 1083\u20131089. [Online]. Available: https://doi.org/10.24963/ijcai.2020/151   \n[48] Q. Si, Y. Liu, F. Meng, Z. Lin, P. Fu, Y. Cao, W. Wang, and J. Zhou, \u201cTowards robust visual question answering: Making the most of biased samples via contrastive learning,\u201d in Findings of EMNLP, 2022, pp. 6650\u20136662. [Online]. Available: https://doi.org/10.18653/v1/2022.findings-emnlp.495   \n[49] C. Jing, Y. Wu, X. Zhang, Y. Jia, and Q. Wu, \u201cOvercoming language priors in VQA via decomposed linguistic representations,\u201d in AAAI, 2020, pp. 11 181\u201311 188. [Online]. Available: https://doi.org/10.1609/aaai.v34i07.6776   \n[50] R. Shrestha, K. Kafle, and C. Kanan, \u201cA negative case analysis of visual grounding methods for VQA,\u201d in ACL, 2020, pp. 8172\u20138181. [Online]. Available: https: //doi.org/10.18653/v1/2020.acl-main.727   \n[51] I. Gat, I. Schwartz, A. Schwing, and T. Hazan, \u201cRemoving bias in multi-modal classifiers: Regularization by maximizing functional entropies,\u201d in NeurIPS, 2020, pp. 3197\u20133208. [Online]. Available: https://proceedings.neurips.cc/paper/2020/hash/ 20d749bc05f47d2bd3026ce457dcfd8e-Abstract.html   \n[52] Q. Si, Z. Lin, M. yu Zheng, P. Fu, and W. Wang, \u201cCheck it again: Progressive visual question answering via visual entailment,\u201d in ACL, 2021, pp. 4101\u20134110. [Online]. Available: https://doi.org/10.18653/v1/2021.acl-long.317   \n[53] M. Lao, N. Pu, Y. Liu, K. He, E. M. Bakker, and M. S. Lew, \u201cCOCA: Collaborative causal regularization for audio-visual question answering,\u201d in AAAI, 2023, pp. 12 995\u201313 003. [Online]. Available: https://doi.org/10.1609/aaai.v37i11.26527   \n[54] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in ICASSP, 2017, pp. 776\u2013780. [Online]. Available: https://doi.org/10.1109/ICASSP.2017.7952261   \n[55] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, \u201cVisualBert: A simple and performant baseline for vision and language,\u201d arXiv preprint arXiv:1908.03557, 2019. [Online]. Available: http://arxiv.org/abs/1908.03557   \n[56] H. M. Fayek and J. Johnson, \u201cTemporal reasoning via audio question answering,\u201d TASLP, vol. 28, pp. 2283\u20132294, 2020. [Online]. Available: https://doi.org/10.1109/TASLP.2020.3010650   \n[57] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, \u201cVQA: Visual question answering,\u201d in ICCV, 2015, pp. 2425\u20132433. [Online]. Available: https://doi.org/10.1109/ICCV.2015.279   \n[58] P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao, and B. Xu, \u201cAttention-based bidirectional long short-term memory networks for relation classification,\u201d in ACL, 2016, pp. 207\u2013212. [Online]. Available: https://doi.org/10.18653/v1/p16-2034   \n[59] J. Lu, J. Yang, D. Batra, and D. Parikh, \u201cHierarchical question-image co-attention for visual question answering,\u201d in NeurIPS, 2016, pp. 289\u2013297. [Online]. Available: https://proceedings. neurips.cc/paper/2016/hash/9dcb88e0137649590b755372b040afad-Abstract.html   \n[60] Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian, \u201cDeep modular co-attention networks for visual question answering,\u201d in CVPR, 2019, pp. 6281\u20136290. [Online]. Available: http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Deep_Modular_ Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.html   \n[61] C. Fan, X. Zhang, S. Zhang, W. Wang, C. Zhang, and H. Huang, \u201cHeterogeneous memory enhanced multimodal attention model for video question answering,\u201d in CVPR, 2019, pp. 1999\u20132007. [Online]. Available: http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_ Heterogeneous_Memory_Enhanced_Multimodal_Attention_Model_for_Video_Question_ Answering_CVPR_2019_paper.html   \n[62] X. Li, J. Song, L. Gao, X. Liu, W. Huang, X. He, and C. Gan, \u201cBeyond RNNs: Positional self-attention with co-attention for video question answering,\u201d in AAAI, 2019, pp. 8658\u20138665. [Online]. Available: https://doi.org/10.1609/aaai.v33i01.33018658   \n[63] T. M. Le, V. Le, S. Venkatesh, and T. Tran, \u201cHierarchical conditional relation networks for video question answering,\u201d in CVPR, 2020, pp. 9972\u20139981. [Online]. Available: https://openaccess.thecvf.com/content_CVPR_2020/html/Le_Hierarchical_Conditional_ Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.html   \n[64] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2020. [Online]. Available: https://openreview.net/forum?id=YicbFdNTTy   \n[65] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao, Z. Zhang, L. Dong et al., \u201cSwin transformer v2: Scaling up capacity and resolution,\u201d in CVPR, 2022, pp. 12 009\u201312 019. [Online]. Available: https://doi.org/10.1109/CVPR52688.2022.01170 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Model Training and Testing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The details of model training are shown in Algorithm 1, where $L$ denotes the number of training samples, and $n_{\\mathrm{b}}$ is the batch size. In the test stage, the bias learner is removed. ", "page_idx": 14}, {"type": "table", "img_path": "twpPD9UMUN/tmp/42aab157ec9723d9829b45170ed7627a37af116faece67376ee3fc8f587fed15.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Dataset Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We make statistics on the rephrasings, as depicted in Table 4. Any rephrasing that garners fewer than one vote will be disregarded. It is evident that the overwhelming majority of the rephrased questions received three favorable votes. Fig. 6 shows the distribution of questions in MUSIC-AVQA and ", "page_idx": 14}, {"type": "text", "text": "Table 4: Statistics of rephrasing consistency. Positive and Negative denote whether the annotator agrees with the rephrasing or not. ", "page_idx": 14}, {"type": "table", "img_path": "twpPD9UMUN/tmp/5a2f05dce408234280785b246cfb1c6596dee12e14cdbc0b67fc8f342d318a39.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "MUSIC-AVQA-R based on the first three words. It is evident that there are notably more entries within each circle in the left figure compared to the right figure. This suggests that the diversity of our dataset is higher than MUSIC-AVA. ", "page_idx": 14}, {"type": "text", "text": "We visualize the answer distribution of specific types of questions. Fig. 7, 8, and 9 show the visualization of the AVQA, audio QA and visual QA tasks, respectively. We can see that all of the answer distributions are long-tail. This further demonstrates the necessity of head and tail sample splitting. Specifically, for the \u201cLocation\u201d type in the AVQA task, we can see that the number of \u201ccongas\u201d is far less than that of \u201cyes\u201d. It is noteworthy that, for question types featuring only two possible answers, we classify questions with lower frequencies as tail samples and those with higher frequencies as head samples. For instance, for the \u201cExistential\u201d questions in the AVQA task, we consider the questions with the answer \u201cno\u201d as tail samples. ", "page_idx": 14}, {"type": "image", "img_path": "twpPD9UMUN/tmp/28887cbb1a7ce565c4fd225b3c8b726f9ac41b55fb7f8f15b0578c8d86cc478e.jpg", "img_caption": ["Figure 6: Distribution visualization of questions based on the first three words. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "twpPD9UMUN/tmp/cfe4b0a298eddfa7abfd043fd0df5824e955ddef8ae4b8a1f868220922274453.jpg", "img_caption": ["Figure 7: Answer distributions of specific types of questions in the AVQA task. $\\mu(a)$ is the average number of answers in a group. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "twpPD9UMUN/tmp/7050b9bb277795a7ae5579633a71bd3ac74439d619db3a8343f09eb07c48e8cd.jpg", "img_caption": ["Figure 8: Answer distributions of specific types of questions in the audio QA task. $\\mu(a)$ is the average number of answers in a group. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "twpPD9UMUN/tmp/426c8ee7da5b769060ff535726c02113e86ade540776b74c824f786a639bea44.jpg", "img_caption": ["Figure 9: Answer distributions of specific types of questions in the visual QA task. $\\mu(a)$ is the average number of answers in a group. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.1 Ethics Statement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "MUSIC-AVQA [4] has undergone meticulous pre-processing tailored for academic research purposes. We ensure the absence of any information that discloses the names or uniquely identifies individuals, as well as avoiding any offensive content. We only perform rephrasing and splitting for the questions within the dataset to develop MUSIC-AVQA-R, which preserves its inherent characteristics. ", "page_idx": 16}, {"type": "text", "text": "B.2 Question Comparison ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We select questions from both the head and tail splits to demonstrate the diversity of our dataset, as illustrated in Figures 10 and 11. Due to the use of pre-defined templates, the questions in the train and test splits of MUSIC-AVQA differ by only a single word. In contrast, the questions in our dataset exhibit a variety of formats, which better reflect real-world scenarios. Additionally, our dataset encompasses a larger test space compared to MUSIC-AVQA. ", "page_idx": 16}, {"type": "text", "text": "Predefined Question Template in MUSIC-AVQA:   \n$\\spadesuit$ Which is the musical instrument that sounds at the same time as the <Object>? QuestioninMUSIC-AVQA:   \nWhich is the musical instrument that sounds at the same time as the pipa? Rephrased Question in MUSIC-AVQA-R (Head Split):   \n$\\checkmark$ Which is the musical instrument that sounds at the same time as the pipa? $\\checkmark$ Which musical instrument plays simultaneously with the pipa?   \n$\\checkmark$ What musical instrument is playing in unison with the pipa?   \n$\\checkmark$ What musical piece is playing in the background as the pipa is being played? $\\checkmark$ What musical instrument is playing concurrently with the pipa?   \n$\\checkmark$ What musical instrument is being played when the pipa is being played? $\\checkmark$ What musical piece is being played simultaneously with the pipa?   \n$\\checkmark$ When the pipa is being played, what musical composition is being performed? $\\checkmark$ What musical work is being performed when the pipa is being played?   \n$\\checkmark$ The pipa is being played, but what musical composition is that?   \n$\\checkmark$ When playing the pipa what musical composition is being performed?   \n$\\checkmark$ What instrument is playing simultaneously with the pipa?   \n$\\checkmark$ What other instrument is playing at the same time as the pipa?   \n$\\checkmark$ What musical instrument plays simultaneously with the pipa?   \n$\\checkmark$ What is the musical instrument that produces sound in tandem with the pipa? $\\checkmark$ What musical instrument produces sound simultaneously with the pipa?   \n\u221a   \n.... ", "page_idx": 17}, {"type": "text", "text": "Figure 10: Question Comparison between MUSIC-AVQA and MUSIC-AVQA-R. The rephrased question comes from the head split of our dataset. The questions in our dataset feature diverse formats, which more accurately reflect real-world scenarios. ", "page_idx": 17}, {"type": "text", "text": "C Proof ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Given a probability density function $f(x)$ that conforms to a uniform distribution with size $N$ , its entropy $H(X)$ can be calculated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H(X)=-\\displaystyle\\sum_{i=1}^{N}p(x_{i})\\cdot\\log_{2}p(x_{i}),}\\\\ &{~~\\boldsymbol{p}(x_{i})=f(x_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $p(x_{i})$ is the probability of $X=x_{i}$ . ", "page_idx": 17}, {"type": "text", "text": "In a uniform distribution, each probability $p(x_{i})$ is the same, i.e., $\\begin{array}{r}{p(x_{i})=f(x_{i})=\\frac{1}{N}}\\end{array}$ , so ", "page_idx": 17}, {"type": "equation", "text": "$$\nH(X)=-\\sum_{i=1}^{N}{\\frac{1}{N}}\\cdot\\log_{2}\\left({\\frac{1}{N}}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, bring the $\\textstyle{\\frac{1}{N}}$ term outside the summation: ", "page_idx": 17}, {"type": "equation", "text": "$$\nH(X)=-\\frac{1}{N}\\sum_{i=1}^{N}\\log_{2}\\left(\\frac{1}{N}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thirdly, move the negative sign inside the logarithm: ", "page_idx": 17}, {"type": "equation", "text": "$$\nH(X)={\\frac{1}{N}}\\sum_{i=1}^{N}\\log_{2}(N),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, combine $\\scriptstyle{\\frac{1}{N}}$ with the summation: ", "page_idx": 17}, {"type": "equation", "text": "$$\nH(X)=\\log_{2}(N).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "twpPD9UMUN/tmp/0ba6fa33990600cef5fe4804d4202c0407737e276ef409d1f47b34cb6c933eb9.jpg", "img_caption": ["Figure 11: Question Comparison between MUSIC-AVQA and MUSIC-AVQA-R. The rephrased question comes from the tail split of our dataset. The questions in our dataset feature diverse formats, which more accurately reflect real-world scenarios. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Dataset Comparison ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The test split comparison between MUSIC-AVQA and MUSIC-AVQA-R is shown in Table 5. We can see that our proposed dataset exhibits a larger test sample space. This can provide a more precise evaluation for the model robustness. ", "page_idx": 18}, {"type": "text", "text": "Table 5: Test split comparison between MUSIC-AVQA and MUSIC-AVQA-R. EXIST, LOC, CNT, COMP, and TEMP, which are question types, denote \u201cExistential\u201d, \u201cLocation\u201d, \u201cCounting\u201d, \u201cComparative\u201d, and \u201cTemporal\u201d, respectively. ", "page_idx": 18}, {"type": "table", "img_path": "twpPD9UMUN/tmp/896a66d4d24002e06b097ae54d8d9d0d0644c334c4fc34048977a0dd94fa9b03.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.2 Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The number of trainable parameters of our model is 117M. In the default settings, our model training takes about 20 hours. We initialize the seed for both Numpy and Torch to 42. The other details can be found in our uploaded code. ", "page_idx": 18}, {"type": "text", "text": "D.3 Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The audio QA baselines are as follows. ", "page_idx": 18}, {"type": "text", "text": "\u2022 FCNLSTM2 employs a fully convolutional network and LSTM to initially learn the representations of audio and questions separately. Subsequently, it projects the concatenated features of both into the answer space.   \n\u2022 CONVLSTM is a variant of FCNLSTM, incorporating five convolutional blocks identical to VGGNet for acquiring a variable-sized representation of audio. ", "page_idx": 19}, {"type": "text", "text": "The visual QA baselines are as follows. ", "page_idx": 19}, {"type": "text", "text": "\u2022 GRU (dubbed \u201cdeeper $\\mathrm{LSTM+Norm\\I^{\\circ}}$ in the published paper) is a simple baseline that first uses VGGNet and LSTM to encode the images and questions and then maps the concatenated features of them into the answer space.   \n\u2022 BiLSTM Attn is an attention-based bi-directional LSTM network, which was often used in previous relation classification.   \n\u2022 $\\mathbf{HCAttn}^{3}$ is a hierarchical co-attention method that employs question- and image-guided attention to reason on images and questions, respectively.   \n\u2022 $\\mathbf{MCAN^{4}}$ is a deep modular co-attention network comprised of cascaded modular co-attention layers, where attention is implemented through multi-head attention in Transformers. ", "page_idx": 19}, {"type": "text", "text": "The video QA baselines are as follows. ", "page_idx": 19}, {"type": "text", "text": "\u2022 $\\mathbf{H}\\mathbf{M}\\mathbf{E}^{5}$ is a heterogeneous memory-enhanced multimodal attention model that can effectively learn global context information from appearance and motion features. \u2022 PSAC6 employs positional self-attention block to model the dependency between question words and video frames, respectively. It utilizes a co-attention mechanism to perform multi-modal interaction. \u2022 $\\mathbf{HCRN}^{7}$ is a hierarchical conditional relation network that embeds video input at various granularities, encompassing frames, short clips, and entire video levels. ", "page_idx": 19}, {"type": "text", "text": "The AVQA baselines are as follows. ", "page_idx": 19}, {"type": "text", "text": "\u2022 $\\mathbf{AVSD}^{8}$ is a simple but effective audio-visual dialog method. It initially encodes the input modalities separately and subsequently feeds their fused features into a LSTM to generate answers.   \n\u2022 $\\mathbf{LAViT^{9}}$ is a spatial AVQA framework that utilizes three distinct Transformer blocks to perform interaction between input modalities.   \n\u2022 $\\mathbf{S}\\mathbf{T}\\mathbf{G}^{10}$ associates particular visual locations with audio to conduct spatial grounding. Based on this, audio and visual features of key timestamps are further emphasized through question queries for temporal grounding.   \n\u2022 $\\mathbf{LAVisH^{11}}$ , based on STG, incorporates trainable parameters into powerful visual encoders such as ViT and Swin. ", "page_idx": 19}, {"type": "text", "text": "D.4 Reevaluation on MUSIC-AVQA-R ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 6 illustrates the overall accuracy for specific types of questions. Notably, our architecture surpasses all baselines across every question type. Importantly, our architecture achieves the highest performance in all three tasks, underscoring the idea that the additional modality serves as a valuable complement. For instance, the audio modality may enhance AVQA models in the video QA task. ", "page_idx": 19}, {"type": "text", "text": "2https://github.com/facebookresearch/daqa   \n3https://github.com/jiasenlu/HieCoAttenVQA   \n4https://github.com/MILVLG/mcan-vqa   \n5https://github.com/fanchenyou/HME-VideoQA   \n6https://github.com/lixiangpengcs/PSAC   \n7https://github.com/thaolmk54/hcrn-videoqa   \n8https://github.com/idansc/simple-avsd   \n9https://github.com/hs-yn/PanoAVQA   \n10https://github.com/GeWu-Lab/MUSIC-AVQA   \n11https://github.com/GenjiB/LAVISH ", "page_idx": 19}, {"type": "text", "text": "Table 6: Experimental results on the MUSIC-AVQA-R test split. The numerical values represent the overall accuracy for specific types of questions. EXIST, LOC, CNT, COMP, and TEMP are question types, representing \u201cExistential\u201d, \u201cLocation\u201d, \u201cCounting\u201d, \u201cComparative\u201d, and \u201cTemporal\u201d, respectively. ", "page_idx": 20}, {"type": "table", "img_path": "twpPD9UMUN/tmp/f99157b70bd777c97ee15f19121d64a9e51cb5c3e74fb47afe90b246cbb8f08f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.5 Case Study ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "twpPD9UMUN/tmp/f0bc668f9d072d4655119443ec0873988825212d09c6fea7b2721a4f2b05cf62.jpg", "img_caption": ["Figure 12: Attention weight visualization on the uniformly sampled audio and video frames. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "We visualize attention weight on extra uniformly sampled audio and video frames to qualitatively analyze the debiasing capability. In Fig. 12, the visualization is presented for a more extensive range of head and tail samples. We can see that our method can focus on the key audio and video frames for QA simultaneously in both in- and out-of-distribution settings. For instance, in the upper head and tail case, our method demonstrates high attention to various instruments, leading to accurate answers for \u201ccounting\u201d questions. This serves as additional evidence supporting the debiasing effectiveness of our proposed MCCD strategy, highlighting its substantial contribution to improving model robustness. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have made clear claims about our contribution and scope in Section Abstract and 1. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have discussed the limitations of our work not only from the model design but also from the model evaluation in Section 6. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Section 3.2, we claim that $\\log N$ is the entropy of a uniform distribution.   \nWe have provided a detailed proof in Appendix C. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have provided the implementation and training details in Section 5.1, 5.2, Appendix A, B, and D. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have uploaded our code and dataset in the supplemental material. The detailed \"readme\" file is also uploaded. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have described the training and test details in Section 5.1, 5.2, Appendix A, B, and D.2. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We re-evaluate 13 multi-modal QA methods, conducting the statistical significance experiment may be computationally and timely expensive. For example, running one epoch for STG takes almost 12 hours, and the maximum number of epochs is set to 80. To the best of our knowledge, all methods adopt seed fixing, which undoubtedly enhances the reproducibility of the experiments and the credibility of the results. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided the detailed description in Section 5.2, Appendix D.2, and D.3. ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work conforms to the NeurIPS Code of Ethics. We have provided the reason in Appendix B.1. ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided analyses in Appendix B.1. ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Our model is not generative. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have adhered to the mentioned licenses and terms of use for all the assets used in the paper, and the original creators or owners have been properly credited or mentioned in Section 5.3, and Appendix D.3. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have uploaded a detailed \"readme\" file in the supplementary material. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided crowdsourcing descriptions in Section 3.1 and Appendix B. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our paper does not contain such risks. ", "page_idx": 24}]