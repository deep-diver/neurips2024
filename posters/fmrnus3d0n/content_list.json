[{"type": "text", "text": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yijun Yang1,2,\u2217 Ruiyuan Gao1, Xiao $\\mathbf{Yang^{2\\dagger}}$ , Jianyuan Zhong1, Qiang $\\mathbf{X}\\mathbf{u}^{1}$ ", "page_idx": 0}, {"type": "text", "text": "1The Chinese University of Hong Kong, 2Tsinghua University {yjyang,rygao,jyzhong,qxu}@cse.cuhk.edu.hk,{yangyj16,yangxiao19}@tsinghua.org.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in Text-to-Image models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-ForWork contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GUARDT2I, a novel moderation framework that adopts a generative approach to enhance Text-to-Image models\u2019 robustness against adversarial prompts. Instead of making a binary classification, GUARDT2I utilizes a large language model to conditionally transform text guidance embeddings within the Text-to-Image models into natural language for effective adversarial prompt detection, without compromising the models\u2019 inherent performance. Our extensive experiments reveal that GUARDT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios. Our framework is available at https://github.com/cure-lab/GuardT2I. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent advancements in Text-to-Image (T2I) models, such as Midjourney[6], Leonardo.Ai[3], DALL\u00b7E 3[10], and others[26, 33, 37, 28, 20, 35], have significantly facilitated the generation of highquality images from textual prompts, as demonstrated in Fig. 1 (a). As the widespread application of T2I models continues, concerns about their misuse have become increasingly prominent [38, 29, 45, 27, 47, 46, 41, 8]. In response, T2I service providers have implemented defensive strategies. However, sophisticated adversarial prompts that appear innocuous to humans can manipulate these models to produce explicit Not-Safe-for-Work (NSFW) content, such as pornography, violence, and political sensitivity [29, 45, 46, 38], raising significant safety challenges, as illustrated in Fig. 1 (b). ", "page_idx": 0}, {"type": "text", "text": "Existing defensive methods for T2I models can be broadly classified into two categories: training interference and post-hoc content moderation. Training interference focuses on removing inappropriate concepts during the training process through techniques like dataset filtering [10, 28] or fine-tuning to forget NSFW concepts [12, 15]. While effective in suppressing NSFW generation, these methods often compromise image quality in normal use cases and remain vulnerable to adversarial attacks [42]. On the other hand, post-hoc content moderation methods, such as OpenAI-Moderation and SafetyChecker, maintain the synthesis quality therefore being widely used in T2I services [6, 3, 10]. These methods rely on text or image classifiers to identify and block malicious prompts or generated content. However, they struggle to effectively defend against adversarial prompts, as reported in [45, 46]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we introduce a new defensive framework called GUARDT2I, specifically designed to protect T2I models from adversarial prompts. Our key observation is that although adversarial prompts (as shown in Fig. 1 (b)) may have noticeable visual differences compared to explicit prompts, they still contain the same underlying semantic information within the T2I model\u2019s latent space. Therefore, we approach the defense against adversarial prompts as a generative task and harness the power of the large language model (LLM) to effectively handle the semantic meaning embedded in implicit adversarial prompts. Specifically, we modify LLM to a conditional LLM, c\u00b7LLM, and fine-tune the c\u00b7LLM to \u201ctranslate\u201d the latent representation of prompts back to plain texts, which can reveal the real intention of the user. For legitimate prompts, as shown in Fig. 1 (c), GUARDT2I tries to reconstruct the input prompt, as shown in Fig. 1 (c)\u2019s Prompt Interpretation. For adversarial prompts, instead of reconstructing the input prompt, GUARDT2I would generate the prompt interpretation conform to the underlying semantic meaning of the adversarial prompt whenever possible, as demonstrated in Fig. 1 (d). Consequently, by estimating the similarity between the input and the synthetic prompt interpretation, we can identify adversarial prompts. ", "page_idx": 0}, {"type": "image", "img_path": "FMrNus3d0n/tmp/239da73f2e5fbbb5f0ce4de61a42ea2bc021bebd0e31c9ddb7cd7c332fd0ef99.jpg", "img_caption": ["Figure 1: Overview of GUARDT2I. GuardT2I can effectively halt the generation process of adversarial prompts to avoid NSFW generations, without compromising normal prompts or increasing inference time. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "GUARDT2I accomplishes defense without altering the original T2I models. This ensures that the performance and generation qualities of the T2I models remain intact. Additionally, GUARDT2I operates in parallel with the T2I models, thereby imposing no additional inference latency during normal usage. Moreover, GUARDT2I has the capability to halt the diffusion steps of malicious prompts at an early stage, which helps to reduce computational costs. ", "page_idx": 1}, {"type": "text", "text": "Overall, the contributions of this work include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To the best of our knowledge, GUARDT2I is the first generative paradigm defensive framework specifically designed for T2I models. Through the transformation of latent variables from T2I models into natural language, our defensive framework not only demonstrates exceptional generalizability across various adversarial prompts, but also provide decision-making interpretation.   \n\u2022 We propose a conditional LLM $.c{\\cdot}L L M)$ to \u201ctranslate\u201d the latent back to plain text, coupled with bi-level parsing methods for prompt moderation.   \n\u2022 We perform extensive evaluations for GUARDT2I against various malicious attacks, including rigorous adaptive attacks, where attackers have full knowledge of GUARDT2I and try to deceive it for NSFW syntheses. ", "page_idx": 1}, {"type": "text", "text": "Experimental results demonstrate that GUARDT2I outperforms baselines, such as Microsoft Azure [2], Amazon AWS Comprehend [2], and OpenAI-Moderation [23, 19], by a large margin, particularly when facing adaptive attacks. Furthermore, our in-depth analysis reveals that the adaptive adversarial prompts that can bypass GUARDT2I tend to have much-weakened synthesis quality. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Adversarial Prompts ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion-based T2I models, trained on extensive internet-sourced datasets, are adept at producing vibrant and creative imagery [36, 26, 6]. However, the lack of curation in these datasets leads to generations of NSFW content by the models [38, 27]. Such content may encompass depictions of violence, pornography, bullying, gore, political sensitivity, racism [27]. Currently, such risk mainly comes from two types of adversarial prompts, i.e., manually and automatically generated ones. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Manually Crafted Attacking Prompts. Schramowski et al. [38] amass a collection of handwritten adversarial prompts, referred to as $I2P$ , from various online communities. These prompts not only lead to the generation of NSFW content but also eschew explicit NSFW keywords. Furthermore, Rando et al. [29] reverse-engineer the safety fliters of a popular T2I model, Stable Diffusion [33]. By adding extraneous text, which effectively deceived the model\u2019s safety mechanisms. ", "page_idx": 2}, {"type": "text", "text": "Automatically Generated Adversarial Prompts. Researchers propose adversarial attack algorithms to automatically construct adversarial prompts for T2I models to induce NSFW contents [38, 45, 46, 41] or functionally disable the T2I models [18]. For instance, by considering the existence of safety prompt filters, SneakyPrompt [46] \u201cjailbreak\u201d T2I models for NSFW images with reinforcement learning strategies. MMA-Diffusion [45] presents a gradient-based attacking method, and showcases current defensive measures in commercial T2I services, such as Midjourney [6] and Leonardo.Ai [3], can be bypassed in the black-box attack way. ", "page_idx": 2}, {"type": "text", "text": "2.2 Defensive Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Model Fine-tuning techniques target at developing harmless T2I models. Typically, they involve concept-erasing solutions [12, 15, 38], which change the weights of existing T2I models [12, 15] or the inference guidance [12, 38] to eliminate the generation capability of inappropriate content. Although their concepts are meaningful, currently, their methods are not practical. For one thing, the deleterious effects they are capable of mitigating are not comprehensive, because they can only eliminate harmful content that has clear definitions or is exemplified by enough images, and their methods lack scalability. For another, their methods inadvertently affect the quality of benign image generation [48, 16, 38]. Due to these drawbacks, current T2I online services [6, 3] and open-sourced models [33, 26] seldom consider this kind of method. ", "page_idx": 2}, {"type": "text", "text": "Post-hoc Content Moderators refer to content moderators applied on top of T2I systems. The moderation can be applied to images or prompts. Image-based moderators, like safety checkers in SD [1, 30], operate on the syntheses to detect and censor NSFW elements. They suffer from significant inference costs because they take the output from T2I models as input. Prompt-based moderators refer to prompt fliters to prevent the generation of harmful content. Due to its lower cost and higher accuracy compared to image-based ones, ", "page_idx": 2}, {"type": "table", "img_path": "FMrNus3d0n/tmp/76e4507cec4b863efdfa49a0b39308a2c8faec593115426f47b3275f3efc0dc0.jpg", "table_caption": ["Table 1: Comparison of our generative defensive approach with existing classification-based ones. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "currently, these technologies are extensively employed by online services, such as Midjourney [6] and Leonardo.Ai [3]. More examples in this category include OpenAI\u2019s Moderation API [23], Detoxify [13] and NSFW-Text-Classifier [21]. ", "page_idx": 2}, {"type": "text", "text": "Note that most existing content moderators treat content moderation as a classification task, which necessitates extensive amounts of meticulously labeled data and operate in a black-box manner [19]. Therefore, they fail to adapt to unseen/customized NSFW concepts, as summarized in Tab. 1 and lack interpretability of the decision-making process, not to mention advanced adversarial prompt threats [45, 46, 38]. By contrast, in this paper, we take a generative perspective to build GUARDT2I, which is more generalizable to various NSFW content and provides interpretation. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Overview. As illustrated in Fig. 2 (a), T2I models rely on a text encoder, $\\tau(\\cdot)$ , to convert a user\u2019s prompt $\\mathbf{p}$ into a guidance embedding e, defined by $\\mathbf{e}=\\tau(\\mathbf{p})\\in\\mathbb{R}^{d}$ . This embedding effectively dictates the semantic content of the image produced by the diffusion model [22]. We have observed that an adversarial prompt, denoted as $\\mathbf{p}_{\\mathrm{adv}}$ , which may appear benign or nonsensical to humans, can contain the same underlying semantic information within the T2I model\u2019s latent space as an explicit prompt does, leading the diffusion model to generate NSFW content. ", "page_idx": 2}, {"type": "text", "text": "This observation has motivated us to introduce the concept of Prompt Interpretation (see Fig. 2 (b)) in order to convert the implicit guidance embedding $\\mathbf{e}$ into plain text. By moderating the Prompt ", "page_idx": 2}, {"type": "image", "img_path": "FMrNus3d0n/tmp/8459b1879ae265883522f41c4859e98d596913a16745a733935e6ff5d2d210f6.jpg", "img_caption": ["Figure 2: The Workflow of GUARDT2I against Adversarial Prompts. (a) GUARDT2I halts the generation process of adversarial prompts. (b) Within GUARDT2I, the c\u00b7LLM translates the latent guidance embedding e into natural language, accurately reflecting the user\u2019s intent. (c) A double-folded generation parse detects adversarial prompts. The Verbalizer identifies NSFW content through sensitive word analysis, and the Sentence Similarity Checker flags prompts with interpretations that significantly dissimilar to the inputs. (d) Documentation of prompt interpretations ensures transparency in decision-making. $\\star$ aims to avoid offenses. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Interpretation, we can easily identify adversarial prompts (see Fig. 2 (c)). To be specific, when given a guidance embedding for a normal prompt, as depicted in Fig. 1 (c), the GUARDT2I model accurately reconstructs the input prompt with slight variations. However, when encountering an adversarial prompt\u2019s guidance embedding, like the one shown in Fig. 2 (b), the generated prompt interpretation will differ significantly from the original input and may contain explicit NSFW words, e.g. \u201csex\u201d, and \u201cfuck\u201d, which can be easily distinguished. Furthermore, the generated prompt interpretation enhances decision-making transparency, as illustrated in Fig. 2 (d). ", "page_idx": 3}, {"type": "text", "text": "Text Generation with c\u00b7LLM. Translating the latent representation e back to plain text presents a significant challenge due to the implicitness of latents. To resolve this issue, we approach it as a conditional generation problem and incorporate crossattention modules to pre-trained LLMs, resulting in a conditional LLM $\\left\\langle c{\\cdot}L L M\\right\\rangle$ to fulflil this conditional generation task. To be specific, we employ a decoderonly architecture, comprising of $L$ stacked transformer layers, as outlined in Fig. 3, and insert crossattention layers in each transformer block. These cross-attention layers receive the guidance embedding e as the query and utilize the scaled dot product attention mechanism to calculate the attention score [43], as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Attention}(\\mathbf{Q}=\\mathbf{e},\\mathbf{K},\\mathbf{V})=\\mathrm{softmax}\\left({\\frac{\\mathbf{e}\\mathbf{K}^{T}}{\\sqrt{d}}}\\right)\\cdot\\mathbf{V}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, the output from the final layer of the c\u00b7LLM is projected through a linear projection layer into the token space and translated to text. ", "page_idx": 3}, {"type": "text", "text": "To fine-tune c\u00b7LLM, we curate a sub-dataset sourced from the LAION-COCO dataset [40], as the training set, denoted as $\\mathcal{D}$ . It is important to note that the source dataset $\\mathcal{D}$ should be unfiltered, meaning it naturally contains both Safe-For-Work (SFW) and NSFW prompts. This deliberate inclusion enables the resulting c\u00b7LLM, trained on this dataset, to acquire knowledge about NSFW concepts and potentially generate NSFW prompts in natural language.3 We input the prompt $\\mathbf{p}$ from $\\mathcal{D}$ into the text encoder of T2I models, yielding the corresponding guidance embedding, expressed as $\\mathbf{e}=\\tau(\\mathbf{p})\\in\\mathbb{R}^{d}$ (see Fig. 3). The resulting dataset, comprising pairs of guidance embeddings and their corresponding prompts $(\\mathbf{e},\\mathbf{p})$ , is named the Mapped Guidance Embedding Dataset, $\\mathcal{D}_{e}$ , and serves in the training of $\\mathrm{\\bfc}$ \u00b7LLM. ", "page_idx": 3}, {"type": "image", "img_path": "FMrNus3d0n/tmp/59b848129f05af55f662303627c6381aeb1350c09e3d8724fb80bea319a1ce81.jpg", "img_caption": ["Figure 3: Architecture of c\u00b7LLM. T2I\u2019s text guidance embedding e is fed to c\u00b7LLM through the multi-head cross attention layer\u2019s query entry. L indicates the total number of transformer blocks. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "For a given training sample $(\\mathbf{e}_{i},\\mathbf{p}_{i})$ from $\\mathcal{D}_{e}$ , c\u00b7LLM is tasked with generating a sequence of interpreted prompt tokens $\\hat{\\mathbf{y}}=(\\hat{y}_{1},\\hat{y}_{2},...,\\hat{y}_{n})$ conditioned on the T2I\u2019s guidance embedding e. The challenges arise from potential information loss during the compression of e, and the discrepancy between the LLM\u2019s pre-training tasks and the current conditional generation task. These challenges may hinder the decoder\u2019s ability to accurately reconstruct the target prompt $\\mathbf{p}$ using only $\\mathbf{e}$ , as illustrated in Fig. 3. To address this issue, we employ teacher forcing [44] training technique, wherein the c\u00b7LLM is fine-tuned with both $\\mathbf{e}$ and the ground truth prompt p. We parameterize the c\u00b7LLM by $\\theta$ , and our optimization goal focuses on minimizing the cross-entropy (CE) loss at each prompt token position $t$ , conditioned upon the guidance embedding e. By denoting the token sequence of prompt $\\mathbf{p}$ as $\\mathbf{y}=(y_{1},y_{2},...,y_{n})$ the loss function can be depicted as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C E}(\\boldsymbol{\\theta})=-\\sum_{t=1}^{n}l o g(p_{\\boldsymbol{\\theta}}(\\hat{y}_{t}|y_{0},y_{1},...,y_{t-1};\\mathbf{e})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $y_{0}$ indicates the special $<\\;B O S\\;>$ begin of sentence token. The underlying concept of the aforementioned objective Eq. (2) aims to tune $\\mathrm{^c}$ \u00b7LLM to minimize the discrepancy between the predicted token sequence $\\hat{\\mathbf{y}}$ and the target token sequence $\\mathbf{y}$ . Teacher forcing ensures that the model is exposed to the ground truth prompt $\\mathbf{p}$ at each step of the generation, thereby conditioning the model to predict the next token in the sequence more accurately [44, 9, 43]. The approach is grounded in the concept that a well-optimized model, through minimizing ${\\mathcal{L}}_{C E}(\\theta)$ , will produce an output probability distribution $p_{\\theta}(\\cdot|y_{0},y_{1},...,y_{t-1};\\mathbf{e})\\in\\mathbb{R}^{|V|}$ , where $\\vert V\\vert$ represents the size of the vocabulary codebook, which closely matches the one-hot encoded target token $y_{t}$ , thereby enhancing the fidelity and coherence of the generated prompt interpretations [44, 9, 43, 17]. ", "page_idx": 4}, {"type": "text", "text": "A Double-folded Generation Parse Detects Adversarial Prompts. After revealing the true intent of input prompts with plain text, in this step, we introduce a bi-level parsing mechanism including Verbalizer and Sentence Similarity Checker to detect malicious prompts. ", "page_idx": 4}, {"type": "text", "text": "Firstly, Verbalizer, $V(\\cdot,S)$ , as a simple and direct moderation method, is used to check either the Prompt Interpretation contains any explicit words, e.g. \u201cfuck\u201d, as illustrated in Fig. 2 (c). Here, $\\boldsymbol{S}$ denotes a developer-defined NSFW word list. Notably, $\\boldsymbol{S}$ is adaptable, allowing real-time updates to include emerging NSFW words, while maintaining the system\u2019s effectiveness against evolving threats. ", "page_idx": 4}, {"type": "text", "text": "In addition, we utilize the Sentence Similarity Checker to examine the similarity in text space. For a benign prompt, its Prompt Interpretation is expected to be identical to the itself, indicating high similarity during inference. In contrast, adversarial prompts reveal the obscured intent of the attacker, resulting in significant discrepancy with the original prompt. We measure this discrepancy using an established sentence similarity model [32], flagging low similarity ones as potentially malicious. ", "page_idx": 4}, {"type": "image", "img_path": "FMrNus3d0n/tmp/5d0f038562b14620cd4865d535ffd4b0e41e98db7c6291826f2ae2ff461e0058.jpg", "img_caption": ["Figure 4: Workflow of Sentence Similarity Checker. (a) Normal Prompt: In the case of a normal prompt, its prompt interpretation closely aligns with the original prompt, resulting in a SFW decision. (b) Adversarial Prompt: Conversely, for an adversarial prompt, its prompt interpretation significantly differs from the original prompt both, therefore be identified. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Resistance to Adaptive Attacks. GUARDT2I demonstrates considerable robustness even under adaptive attacks. To deceive both T2I and GUARDT2I simultaneously, the adversarial prompts must appear nonsensical yet retain similar semantic content in T2I\u2019s latent space, while also resembling their prompt interpretation to bypass GUARDT2I. This requirement creates confilcting optimization directions: while adaptive attacks aim for prompts that differ visually from explicit ones, GUARDT2I requires similarity in prompt interpretation and absence of explicit NSFW words. Consequently, increasing GUARDT2I\u2019s bypass rate leads to a reduced NSFW generation rate by the T2I model, making it challenging for adaptive attackers to circumvent GUARDT2I effectively. ", "page_idx": 4}, {"type": "table", "img_path": "FMrNus3d0n/tmp/7e5e2a8d7b90329ef408ad60e2f0ca4b001cf34fbd8e8d72ce0284d34f77b4d8.jpg", "table_caption": ["Table 2: Comparison with baselines. Bolded values are the highest performance. The underlined italicized values are the second highest performance. \\* indicates human-written adversarial prompts. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Training Dataset. LAION-COCO [40] represents a substantial dataset comprising 600M highquality captions that are paired with publicly sourced web images. This dataset encompasses a diverse range of prompts, including both standard and NSFW content, mirroring real-world scenarios. We use a subset of LAION-COCO consisting of 10M randomly sampled prompts to fine-tune our c\u00b7LLM. ", "page_idx": 5}, {"type": "text", "text": "Test Adversarial Prompt Datasets. I2P [38] comprises $4.7\\mathrm{k}$ hand-crafted adversarial prompts. These prompts can guide T2Is towards NSFW syntheses, including self-harm, violence, shocking content, hate, harassment, sexual content, and illegal activities. We further extract 200 sexual-themed prompts from I2P to form the I2P-sexual adversarial prompt dataset. SneakyPrompt [46], Ring-A-Bell [42], P4D [11] , and MMA-Diffusion [45] generate adversarial prompts automatically, we directly employ their released benchmark for evaluation. ", "page_idx": 5}, {"type": "text", "text": "Target Model. We employ Stable Diffusion v1.5 [7], a popular open-source T2I model, as the target model of our evaluation. This model has been selected due to its extensive adoption within the community and its foundational influence on subsequent commercial T2I models [3, 26, 25, 6, 4]. ", "page_idx": 5}, {"type": "text", "text": "Implementation. Our GUARDT2I comprises three components: Verbalizer, Sentence Similarity Checker, and c\u00b7LLM. Verbalizer operates based on predefined 25 NSFW words. We utilize the offthe-shelf Sentence-transformer [32], to function as the Sentence Similarity Checker. We implement c\u00b7LLM with 24 transformer blocks. Its initial weights are sourced from [34]. Please refer to Appendix for more detailed implementation. Note that GUARDT2I as an LLM-based solution, also follows the scaling law [14], one can implement GUARDT2I with other types of pre-trained LLMs and text similarity models, based on real scenarios. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We employ both commercial moderation API models and popular open-source moderators as baselines. OpenAI Moderation [23, 19] classifies five type NSFW themes, including sexual content, hateful content, violence, self-harm, and harassment. If any of these categories are flagged, the prompt is rejected [19]. Microsoft Azure Content Moderator [5], as a classifier-based API moderator, focuses on sexually explicit and offensive NSFW themes. AWS Comprehend [2] treats NSFW prompt detection as a binary classification task. If the model classifies the prompt as toxic, it is rejected. NSFW-text-classifier [21] is an open-source binary NSFW classifier. Detoxity [13] is capable of detecting four types of inappropriate prompts, including pornography content, threats, insults, and identity-based hate. ", "page_idx": 5}, {"type": "image", "img_path": "FMrNus3d0n/tmp/06ab450ff006242550a8912420d7cfcee96d2813b7eac1212366d382d200b775.jpg", "img_caption": ["Figure 5: ROC curves of our GUARDT2I and baselines against various adversarial prompts. The black line represents the GUARDT2I model\u2019s consistent and high AUROC scores across different thresholds. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "FMrNus3d0n/tmp/0a6610b3eb2ed79ae786f9fc145c74be9c5fcfdb5da8583c81a3488f8ee42e57.jpg", "table_caption": ["Table 3: Normal Use Case Results. Bolded values are the highest performance. The underlined italicized values are the second highest performance. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "SLD [38] and ESD [11] are concept-erasing methods, which are designed to reduce the probability of NSFW generation. Therefore, we use the Attack Success Rate (ASR) as our evaluation metric. For GuardT2I, we set the threshold at FPR $@5\\%$ , a common adaptation. As a concept-erasing method, ESD [11] only removes a single NSFW concept, \u201cnudity\u201d, by fine-tuning the T2I model. This limitation means it fails to mitigate other NSFW themes such as violence, self-harm, and illegal content. Consequently, our evaluation focuses solely on \u201cadult content\u201d. All implementations of the baseline models and the tested adversarial prompts are released by their original papers. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. Rejecting adversarial prompts is a detection task, for which we employ standard metrics including AUROC, AUPRC, and FPR $@$ TPR95. These metrics are used to evaluate GUARDT2I and baseline models, in line with established practices in [27, 19]. Higher values of AUROC and AUPRC signify superior performance, whereas a lower FPR $@$ TPR95 value is preferable. Due to space limitation, detailed explanations of these metrics are provided in Appendix. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tab 2 presents a comprehensive evaluation of the proposed GUARDT2I moderator in comparison with several baseline methods across multiple adversarial prompt datasets. The results demonstrate that GUARDT2I consistently outperforms existing approaches in key performance metrics. Specifically, GUARDT2I achieves the highest average AUROC of $98.36\\%$ and the highest average AUPRC of $98.51\\%$ , surpassing all baseline methods, including OpenAI-Moderation, Microsoft Azure, AWS Comprehend, NSFW-text-classifier, and Detoxify. Furthermore, GUARDT2I exhibits superior effectiveness in minimizing false positives and attack success rates, attaining an average FPR $@$ TPR95 of $19.26\\%$ and an average ASR of ${\\bf8.75\\%}$ , both of which are significantly lower than those of the compared baselines. The reduced standard deviations across these metrics $(\\pm3.15\\$ for AUROC, $\\pm4.35$ for AUPRC, and $\\pm17.14$ for FPR $@$ TPR95) further indicate the robustness and consistency of GUARDT2I\u2019s performance. These findings collectively highlight the superior capability of GUARDT2I in effectively moderating adversarial prompts, ensuring both high detection accuracy and resilience against various attack strategies. ", "page_idx": 6}, {"type": "text", "text": "GUARDT2I causes little impact on normal use cases. Tab. 2\u2019s FPR $@$ TPR95 results corroborate GUARDT2I is harmless to normal prompts, demonstrating a significantly lower FPR of $18.39\\%$ , which is $89.23\\%$ lower than the top-performing baseline average. This metric is critical in practical scenarios where high FPR can frustrate user experience. Moreover, we evaluate the performance of GUARDT2I using the FID [24] and CLIP-Score [24] metrics to assess image quality and text alignment in Tab. 3. We compared our approach to the concept-erasing defense methods ESD [12] and SLD [38], which aim to reduce the probability of generating NSFW images. Additionally, we reported the average Attack Success Rate (ASR) to indicate the effectiveness of the defense methods. ", "page_idx": 6}, {"type": "text", "text": "Table 4: Paired prompt and interpretation of GUARDT2I for normal and adversarial prompts demonstration. Our prompt interpretation can faithfully reflect the malicious intent of the adversarial prompt by revealing sensitive words. Verb. is the short for GUARDT2I\u2019s Verbalizer and Sim. indicates the sentence similarity score predicted by GUARDT2I\u2019s Sentence Similarity Checker, the higher the similar. ", "page_idx": 7}, {"type": "table", "img_path": "FMrNus3d0n/tmp/3d49682018f8a95294a8eaa5923eb0fc4db74d4ab84c4d3385769d74b3e47668.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Generalizability against Various Adversarial Prompts. GUARDT2I demonstrates strong and consistent results across varying thresholds, as showcased by the black ROC curve in Fig. 5. Taking the OpenAI Moderation as a point of comparison, it performs exceptionally well on SneakyPrompt, achieving an AUROC of $98.50\\%$ (red curve in Fig. 5 (a)), but drops to $73.02\\%$ on MMA-Diffusion, as indicated by the red curve in Fig. 5 (b). This performance gap is due to OpenAI Moderation\u2019s fixed decision boundaries, making it less adaptable to unfamiliar prompts. In contrast, GUARDT2I operates generatively, analyzing each prompt for similarities or NSFW words, thereby offering more accurate and adaptable responses to diverse adversarial prompts. ", "page_idx": 7}, {"type": "image", "img_path": "FMrNus3d0n/tmp/4dd0067c4f4f15674fdff4fb6ee3ff07c3cb94f68227d474898dadcd6ec80461.jpg", "img_caption": ["Figure 6: AUROC comparison over various NSFW themes. Our GUARDT2I, benefitting from the generalization capabilities of the LLM, stably exhibits decent performance under a wide range of NSFW threats. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Generalizability against Diverse NSFW Concepts. As can be seen in Fig. 6, GUARDT2I consistently achieves AUROC scores exceeding $90\\%$ across I2P\u2019s five NSFW themes, indicating consistently high performance. In contrast, baselines exhibit significant performance fluctuations when faced with different NSFW themes. This inconsistency mainly stems from these models being trained on limited NSFW datasets, which hampers their ability to generalize to unseen NSFW themes. On the other hand, our proposed GUARDT2I model, which leverages c\u00b7LLM, benefits from unsupervised training on large-scale language datasets. This approach equips it with a broad understanding of diverse concepts, thereby enhancing its generalization capabilities across different NSFW themes. ", "page_idx": 7}, {"type": "text", "text": "Interpretability. The prompt interpretations generated by GUARDT2I, as illustrated in Tab. 4, serve a dual purpose: to facilitate the detection of adversarial prompts and contribute to the interpretability of the pass or reject decision due to their inherent readability. As demonstrated in Tab. 4\u2019s upper section, when presented with a normal prompt, our GUARDT2I model showcases its proficiency in reconstructing the original prompt based on the associated T2I\u2019s latent guidance embeddings. In the context of adver", "page_idx": 7}, {"type": "image", "img_path": "FMrNus3d0n/tmp/a25427b05b73dbf6ad505ac07be03b5d6afbf6f5078ed797e4a44dc447254f28.jpg", "img_caption": ["Figure 7: Word clouds of adversarial prompts [45], and their prompt interpretations. GUARDT2I can effectively reveal the concealed malicious intentions of attackers. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "sarial prompts, the significance of prompt interpretations becomes even more pronounced. As illustrated in Tab. 4\u2019s lower section, GUARDT2I interprets adversarial prompts\u2019 corresponding text guidance embedding into readable sentences. These sentences, which serve as prompt interpretations, can reveal the actual intention of the attacker. As analyzed in Fig. 7, the original adversarial prompts\u2019 prominent words seem safe for work, while after being parsed by our GUARDT2I we can get their actual intentions. The ability to provide interpretability is a distinctive feature of GUARDT2I, distinguishing it from classifier-based methods that typically lack such transparency. This capability not only differentiates GUARDT2I but also adds significant value by shedding light on the decision-making process, offering developers of T2I a deeper understanding. ", "page_idx": 8}, {"type": "text", "text": "4.3 Evaluation on Adaptive Attacks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Considering attackers have complete knowledge of both T2I and GUARDT2I, we modify the most recent MMA-Diffusion adversarial attack [45], which provides a flexiable gradient-based optimization flow to attack T2I models, by adding an additional term to attack GUARDT2I, as depicted in Eq. (3), to perform adaptive attacks. ", "page_idx": 8}, {"type": "equation", "text": "$$\nL_{a d a p t i v e}=(1-\\alpha)\\cdot L_{T2I}+\\alpha\\cdot L_{G u a r d T2I},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $L_{T2I}$ is the original attack loss proposed by MMA-Diffusion, which steers T2I model towards generating NSFW content. Besides, $L_{G u a r d T2I}$ is the loss function from GUARDT2I\u2019s Sentence Similarity Checker, which can attack GUARDT2I by optimizing with gradients, and $\\alpha$ is a hyperparameter to trade off two items. ", "page_idx": 8}, {"type": "text", "text": "The experiments are performed on a NVIDIA-A800-(80G) GPU with the default attack settings of MMA-Diffusion. We sample 100 NSFW prompts from MMA-Diffusion\u2019s dataset, and report the results with various $\\alpha$ in Tab. 5, where\u201cGUARDT2I Bypass Rate\u201d indicates the percentage of adaptive prompts that bypass GUARDT2I. \u201cT2I NSFW Content Rate\u201d represents the percentage of bypassed prompts that result in the T2I generating NSFW content. Therefore, the \u201cAdaptive Attack Success Rate\u201d is calculated as \u201cGUARDT2I Bypass Rate\u201d $\\times$ \u201cT2I NSFW Content Rate\u201d. Following [45], a synthesis is considered NSFW, once it can trigger the NSFW detector nested in Stable Diffusion [7]. ", "page_idx": 8}, {"type": "text", "text": "The results show that adaptive attacks on the entire system are challenging due to conflicting optimization directions. Specifically, $L_{T2I}$ aims to find prompts that appear different and malicious semantic according to the embeddings of T2I. On the other hand, GUARDT2I requires any bypassed prompts to stay close to their semantics according to the embeddings of T2I models. As a result, an increase in the \u201cGUARDT2I Bypass Rate\u201d leads to a decrease in the \u201cT2I NSFW Generation Rate\u201d, and vice versa. Therefore, even for adaptive attackers, evading GUARDT2I becomes difficult, with an overall \u201cAttack Success Rate\u201d no higher than $16\\%$ . In a sanity check with doubled attack iterations (1000, ${\\sim}30\\$ minutes per adv. prompt), the highest \u201cAdaptive Attack Success Rate\u201d observed is $24\\%$ . By contrast, that of Safety Checker is higher than $85.48\\%$ as reported by [45]. Moreover, qualitative results show that the successful adversarial prompts trend to degrade the synthesis quality, as illustrated in Fig. 8, weakening the threat posed by adaptive attacks. To strengthen GUARDT2I\u2019s robustness, developers can set a more strict threshold. If some users are still concerned about moving to GUARDT2I from the alternative moderators then they can use both in parallel. ", "page_idx": 8}, {"type": "image", "img_path": "FMrNus3d0n/tmp/f921672d0ab2f3ef66a257d17ce6c81037bef0043812b35735c57c51a16e6c3f.jpg", "img_caption": ["Figure 8: Syntheses generated by successful adaptive attack prompts. Adaptive adversarial prompts that can bypass GUARDT2I tend to have muchweakened synthesis quality. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Tab. 6 explores the roles of two key components in GUARDT2I: Verbalizer and Sentence Similarity Checker. Verbalizer shows variable effectiveness across different adversarial prompts, indicating its limited capacity to handle complex cases independently. As a complementary, Sentence Similarity Checker consistently achieves high AUROC scores above $91\\%$ , demonstrating its ability to discern subtle differences between prompts effectively. Combining both components results in the highest performance, highlighting a synergistic effect. The Verbalizer analyzes the linguistic structure, while the Sentence Similarity Checker assesses semantic coherence, together providing a comprehensive defense against adversarial prompts. ", "page_idx": 8}, {"type": "table", "img_path": "FMrNus3d0n/tmp/d39c8d27494d1591710f727f6bebf28eb8e19a4b0aeaa327b8cc899d74894094.jpg", "table_caption": ["Table 5: Adaptive Attack Results on GUARDT2I with Various Adaptive Attack Weight "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "FMrNus3d0n/tmp/d53d02112015dc52da43911f160a0837bc3ace3089aade4b7a9a735a2b143c18.jpg", "table_caption": ["Table 6: Ablation Study on Verbalizer and Sentence Similarity Checker. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "table", "img_path": "FMrNus3d0n/tmp/c59bde665685899398f9af99407634e03936c95b6dd2e62b1dff60a914fea63c.jpg", "table_caption": ["Table 7: Comparison of Model Parameters and Inference Times on NVIDIA-A800 "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Failure Case Analysis. We analyze two types of failure cases involving both false negatives and false positives. As shown in Fig. 9 (a), a false negative occurred when an adversarial prompt [38] led to the generation of unauthorized T2I content about Trump, mistakenly classified as normal. To prevent such errors, we can enrich Verbalizer by including specific keywords like \u201cDonald Trump.\u201d ", "page_idx": 9}, {"type": "image", "img_path": "FMrNus3d0n/tmp/0bce40fbd73ee31326abf19a816b54dbd69351e432d3eb6629aac6a9ed74a49d.jpg", "img_caption": ["Figure 9: Failure cases of GUARDT2I. (a) Fake news of the famous individual. (b) GUARDT2I alarms rarely used terminology. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In addition, we have observed that GUARDT2I occasionally suffers from false alarms due to the rare appearance of certain terminologies. However, the rare terminology is either difficult for T2I model to depict, as demonstrated in Fig. 9 (b), making the false alarm less harmful. ", "page_idx": 9}, {"type": "text", "text": "Computational Cost. Tab. 7 compares the computational costs of GUARDT2I and the image classifier-based post-hoc SafetyChecker [1]. GUARDT2I operates in parallel with T2I, allowing for an immediate cessation of the generation process upon detection of harmful messages. As long as GuardT2I\u2019s inference speed is faster than the image generation speed of the T2I model, it does not introduce additional latency from the user\u2019s perspective. In contrast, SafetyChecker requires a full diffusion process of 50 iterations to classify NSFW content, making it significantly less efficient. Particularly in the presence of an adversarial prompt, GUARDT2I responds approximately 300 times faster than SafetyChecker. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "By adopting a generative approach, GUARDT2I enhances the robustness of T2I models against adversarial prompts, mitigating the potential misuse for generating NSFW content. Our proposed GUARDT2I offers the capability to track and measure the prompts of T2I models, ensuring compliance with safety standards. Furthermore, it provides fine-grained control that accommodates diverse adversarial prompt threats. Unlike traditional classification methods, GUARDT2I leverages the c\u00b7LLM to transform text guidance embeddings within T2I models into natural language, enabling effective detection of adversarial prompts without compromising T2I models\u2019 inherent performance. Through extensive experiments, we have demonstrated that GUARDT2I outperforms leading commercial solutions such as OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios. And show decent robustness against adaptive attacks. We firmly believe that our interpretable GUARDT2I model can contribute to the development of safer T2I models, promoting responsible behavior in real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by General Research Fund of Hong Kong Research Grants Council (RGC) under Grant No. 1420352, the Research Matching Grant Scheme under Grant (No. 7106937, 8601130, and 8601440), and the NSFC Projects (No. 92370124, and 62076147). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Safety Checker nested in Stable Diffusion. https://huggingface.co/CompVis/ stable-diffusion-safety-checker, 2023. 3, 10 [2] AWS Comprehend. https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html,   \n2024. 2, 6 [3] Leonardo.Ai. https://leonardo.ai/, 2024. 1, 3, 6 [4] Lexica. https://lexica.art/, 2024. 6 [5] Microsoft Azure Content Moderator. https://learn.microsoft.com/zh-cn/azure/ai-services/ content-moderator/api-reference, 2024. 6 [6] Midjourney. https://midjourney.com/, 2024. 1, 2, 3, 6 [7] Stable Diffusion V1.5 checkpoint. https://huggingface.co/runwayml/stable-diffusion-v1-5? text $=$ chi+venezuela+drogenius, 2024. 6, 9, 10 [8] Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang, Zhan Qin, Zhibo Wang, and Kui Ren. SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution. arXiv preprint arXiv:2309.14122, 2023. 1 [9] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of the International Conference on Learning Representations, 2015.   \n5 [10] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. https://cdn.openai.com/papers/dall-e-3.pdf,   \n2023. 1 [11] Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, and Wei-Chen Chiu. Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. arXiv preprint arXiv:2309.06135, 2023. 6, 7 [12] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing Concepts from Diffusion Models. arXiv preprint arXiv:2303.07345, 2023. 1, 3, 6, 7 [13] Laura Hanu and Unitary team. Detoxify. https://github.com/unitaryai/detoxify, 2020. 3, 6 [14] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 6, 15 [15] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating Concepts in Text-to-Image Diffusion Models. arXiv preprint arXiv:2303.13516, 2023. 1, 3 [16] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, Minguk Kang, Taesung Park, Jure Leskovec, Jun-Yan Zhu, Li Fei-Fei, Jiajun Wu, Stefano Ermon, and Percy Liang. Holistic Evaluation of Text-To-Image Models. arXiv preprint arXiv:2311.04287, 2023. 3 [17] Haoran Li and Wei Lu. Mixed Cross Entropy Loss for Neural Machine Translation. In Proceedings of the International Conference on Machine Learning, pages 6425\u20136436, 2021. 5 [18] Han Liu, Yuhao Wu, Shixuan Zhai, Bo Yuan, and Ning Zhang. Riatig: Reliable and imperceptible adversarial text-to-image generation with natural prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20585\u201320594, June 2023. 3 [19] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 15009\u201315018, 2023. 2,   \n3, 6, 7 [20] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2021. 1   \n[22] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In Proceedings of the International Conference on Machine Learning, pages 16784\u201316804, 2022. 3   \n[23] OpenAI. Moderation overview. https://platform.openai.com/docs/guides/moderation/ overview, 2023. 2, 3, 6   \n[24] I. Pavlov, A. Ivanov, and S. Stafievskiy. Text-to-Image Benchmark: A benchmark for generative models. https://github.com/boomb0om/text2image-benchmark, September 2023. Version 0.1.0. 7   \n[25] PlaygroundAI. Playground. https://playgroundai.com/, 2023. 6   \n[26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. arXiv preprint arXiv:2307.01952, 2023. 1, 2, 3, 6, 10   \n[27] Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, and Yang Zhang. Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models. arXiv preprint arXiv:2305.13873, 2023. 1, 3, 7   \n[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv preprint arXiv:2204.06125, 2022. 1   \n[29] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tram\u00e8r. Red-Teaming the Stable Diffusion Safety Filter. arXiv preprint arXiv:2210.04610, 2022. 1, 3   \n[30] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tram\u00e8r. Red-Teaming the Stable Diffusion Safety Filter. arXiv preprint arXiv:2210.04610, 2022. 3   \n[31] Sashank Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018. 15   \n[32] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2019. 5, 6, 15   \n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10674\u201310685, 2022. 1, 3, 14   \n[34] Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 8:264\u2013280, 2020. 6, 15   \n[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023. 1   \n[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022. 2   \n[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. In Proceedings of the Advances in Neural Information Processing Systems, 2022. 1   \n[38] Patrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, and Kristian Kersting. Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22522\u201322531, 2023. 1, 3, 6, 7, 10, 17   \n[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An Open Large-scale Dataset for Training Next Generation Image-text Models. In Proceedings of the Advances in Neural Information Processing Systems, 2022. 15   \n[40] Christoph Schuhmann, Andreas K\u00f6pf, Theo Coombes, Richard Vencu, Benjamin Trom, and Romain Beaumont. LAION-COCO. https://laion.ai/blog/laion-coco/, 2022. 4, 6   \n[41] Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models? arXiv preprint arXiv:2310.10012, 2023. 1, 3   \n[42] Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Ring-a-bell! how reliable are concept removal methods for diffusion models? In The Twelfth International Conference on Learning Representations, 2023. 1, 6   \n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Proceedings of the Advances in Neural Information Processing Systems, pages 5998\u20136008, 2017. 4, 5, 15   \n[44] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1989. 5   \n[45] Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Tsung-Yi Ho, Nan Xu, and Qiang Xu. MMA-Diffusion: MultiModal Attack on Diffusion Models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 3, 6, 8, 9, 10   \n[46] Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao. Sneakyprompt: Jailbreaking text-to-image generative models. In Proceedings of the IEEE Symposium on Security and Privacy, 2024. 1, 3, 6, 10   \n[47] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. arXiv preprint arXiv:2310.11868, 2023. 1   \n[48] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. arXiv preprint arXiv:2310.11868, 2023. 3 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This supplementary material provides additional details and results that are not included in the main paper due to page limitations. The following items are included in this supplementary material. ", "page_idx": 13}, {"type": "text", "text": "A Preliminaries of Diffusion-based Text-to-Image Model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Text-guided Stable Diffusion Models. Stable Diffusion (SD) models [33], a subclass of diffusion models, streamline text-guided diffusion and denoising processes in the latent space, thereby boosting efficiency. ", "page_idx": 13}, {"type": "text", "text": "During training, the initial image $x_{0}$ and prompt $\\mathbf{p}$ are encoded into latent spaces using $\\mathcal{E}(\\cdot)$ and $\\tau(\\cdot)$ respectively, resulting in $z_{0}=\\mathcal{E}(x_{0})$ and guidance embedding, $\\mathbf{e}=\\tau(\\mathbf{p})$ . Noise is incrementally introduced across $T$ diffusion steps, generating a series of samples $z_{1},...,z_{T}$ through $z_{t+1}=a_{t}z_{t}+$ $b_{t}\\epsilon_{t}$ , where $\\epsilon_{t}$ follows a Gaussian distribution. Ideally, with a large $T$ , the final $z_{T}$ approximates $\\mathcal{N}(0,1)$ . ", "page_idx": 13}, {"type": "text", "text": "This property allows us to generate latent vectors for images by starting with Gaussian noise $z_{T}\\sim$ $\\mathcal{N}(0,1)$ and gradually reducing noise. To achieve this, we train a neural network, $\\epsilon_{\\theta}$ , implemented as an Unet in SD, which predicts $z_{t+1}$ based on the input $z_{t}$ . For prompt guidance, the prompt embedding $\\mathbf{e}$ is injected as an condition to run conditional diffusion steps, $\\epsilon_{\\theta}(z_{t}|\\tau(\\mathbf{p}))$ . Additionally, by replacing the prompt with a null prompt $\\mathcal{Q}$ with a fixed probability, the model can generate images unconditionally. The denoising diffusion model is trained by minimizing the following loss function: ", "page_idx": 13}, {"type": "equation", "text": "$$\nL(\\theta)=\\mathbb{E}_{t,z_{0}=\\mathcal{E}(x_{0}),\\epsilon\\sim\\mathcal{N}(0,1)}[|\\epsilon-\\epsilon_{\\theta}(z_{t+1},t|\\tau(\\mathbf{p})|_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "During the inference phase, the latent noise is extrapolated in two directions: towards $\\epsilon(z_{t}|\\tau(p))$ and away from $\\epsilon(z_{t}|\\mathcal{D})$ . This process is carried out as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\theta}(z_{t}|\\tau(\\mathbf{p}))=\\epsilon_{\\theta}(z_{t}|\\tau(\\boldsymbol{\\mathcal{D}}))+g\\cdot(\\epsilon_{\\theta}(z_{t}|\\tau(\\mathbf{p}))-\\epsilon_{\\theta}(z_{t}|\\tau(\\boldsymbol{\\mathcal{D}}))),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $g$ indicates guidance scale, typically $g\\,>\\,1$ . Subsequently, the image decoder, $\\mathcal{D}(\\cdot)$ , will decode the latent image embedding to an image. ", "page_idx": 13}, {"type": "text", "text": "B Inference Workflow of GUARDT2I", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 1 Inference Workflow of GUARDT2I   \nInput: T2I\u2019s prompt embedding e from original prompt p, c\u00b7LLM (\u00b7); Verbalizer $V(\\cdot,S)$ with NSFW word list $s$ ; Text similarity checker $S i m(\\cdot,\\cdot)$ and threshold $s$   \nOutput: Early stop diffusion process $/$ Accept the input prompt 1: ${\\bf p}_{I}=\\mathrm{c}\\cdot\\mathrm{LLM}$ (e)   \n2: if $V(\\mathbf{p}_{I},S)$ then   \n3: Early Stop: NSFW Prompt Detected   \n4: else if $S i m({\\bf p},{\\bf p}_{I})<s$ then 5: Early Stop: Adv. Prompt Detected 6: else   \n7: Accept: Normal Prompt 8: end if ", "page_idx": 13}, {"type": "text", "text": "C Evaluation Metric ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "AUROC: The AUROC metric measures the ability of our model to discriminate between adversarial and normal prompts. It quantifies the trade-off between the TPR and the FPR, providing an overall assessment of the model\u2019s performance across different thresholds. ", "page_idx": 13}, {"type": "text", "text": "AUPRC: The AUPRC metric focuses on the precision-recall trade-off, providing a more detailed evaluation. ", "page_idx": 13}, {"type": "text", "text": "FPR $\\mathbf{\\mathcal{Q}T P R95\\%}$ : $\\vec{\\bigtriangledown}\\mathrm{PR}\\ @\\mathrm{TPR95}\\%$ quantifies the proportion of false positives (incorrectly identified as adversarial examples) when the model correctly identifies $95\\%$ of the true positives (actual adversarial prompts). A lower FPR $@$ TPR95 value is desirable, as it indicates that the model can maintain high accuracy in detecting adversarial examples with fewer mistakes. This metric is particularly important in commercial scenarios where frequent false alarms are unacceptable. Note that FPR $@$ TPR95 provides a specific slice of the ROC curve at a high-recall threshold. Developers have the flexibility to adjust the threshold to achieve desired performance based on specific application scenarios. ", "page_idx": 14}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Settings of the target Stable Diffusion model. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the target SDv1.5 model, we set the guidance scale to 7.5, the number of inference steps to 50, and the image size to $512\\times512$ , 4 syntheses per prompt, throughout evaluations. ", "page_idx": 14}, {"type": "text", "text": "D.2 Hardware platform. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We conduct our training and main experiments on the NVIDIA RTX4090 GPU with 24GB of memory. For adaptive attack and computational cost evaluation, we conduct experiments on the NVIDIA A800 GPU with $80\\:\\mathrm{GB}$ of memory. ", "page_idx": 14}, {"type": "text", "text": "D.3 Implementation details of GUARDT2I. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our GUARDT2I comprises three primary components: Verbalizer, Sentence Similarity Checker and c\u00b7LLM. The Verbalizer operates based on predefined 25 NSFW words; details of which can be found in the Tab. A-1. We utilize an off-the-shelf sentence-transformer checkpoint [32], to function as the Sentence Similarity Checker. The architecture diagram of c\u00b7LLM is introduced in Fig. 3, which is a transformer-based model consisting of 24 standard transformer blocks [43], each with 1024 hidden dimensions. This model is initialized from a publicly available checkpoint [34], pre-trained on an extensive text corpus [39]. We fine-tune c\u00b7LLM using the Adam optimizer [31] with a learning rate of $1\\times10^{-5}$ , and a batch size of 1024 for 50 epochs, using around 768 GPU hours on NVIDIA4090. Note that, GUARDT2I as a LLM-based solution, also follows the scaling law [14], one can implement GUARDT2I with other type of pretrained LLMs and more powerful text similarity models, according to the real-world hardware resource budgets. ", "page_idx": 14}, {"type": "text", "text": "D.4 NSFW Word List of Verbalizer. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table A-1 provides a comprehensive compilation of NSFW sensitive words utilized in the Verbalizer component of our GUARDT2I framework. For our specific use case, we primarily focus on pornographic-related words. It is crucial to emphasize that this list is not fixed and developers can dynamically adjust it to suit specific application scenarios in real-time. This customizable feature ensures the Verbalizer remains adaptable and responsive to the distinct requirements and sensitivities of diverse contexts. By leveraging this flexibility, developers can effectively tailor the Verbalizer to guard T2I models. ", "page_idx": 14}, {"type": "text", "text": "E Additional Failure Case Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We have expanded on the failure case visualizations discussed in Section 5 and have provided an enlarged version in Figure A-1. This figure offers a more detailed illustration of the specific instances where our system encountered challenges and produced undesired outputs. By thoroughly examining these failure cases, we gain valuable insights into the areas that require improvement and refinement in our approach. ", "page_idx": 14}, {"type": "text", "text": "To address these challenges, we propose two promising solutions. Firstly, enriching the Verbalizer with specific keywords, such as the example of incorporating the term \"Trump and Thanos\" can enhance the system\u2019s ability to handle these failures. Secondly, employing an active learning technique can further improve GUARDT2I\u2019s performance by iteratively learning from and adapting to the feedback received from these failure cases. ", "page_idx": 14}, {"type": "table", "img_path": "FMrNus3d0n/tmp/376d52e43f5890283171a066c90ba661aaa4038a43aba7fb84a602214a711393.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "FMrNus3d0n/tmp/61df1e666f15fc63778d3ffe0f9b95d4d875e2ff8921ef7e5e4faa11acb72a92.jpg", "img_caption": ["Figure A-1: Additional failure case analysis. Upper section: The adversarial prompt [38] generates shocking content (fake news about Trump/Thanos) but is mistakenly flagged as a normal prompt. Lower section: GUARDT2I occasionally produces false alarms due to the reconstruction of rarely used terminology (see bolded words), resulting in false positives. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The abstract and introduction highly align with the proposed method and the contributions claimed in the introduction section is supported by experimental results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Please refer to Section 5. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Statements related to theoretical materials are well-cited and introduced. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Model architecture is introduced in Section 3, and the training and implementation details can be found in Section 4.1, Appendix D and Appendix D.4. Codes and model checkpoints will be publicly released upon acceptance. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: Codes and model checkpoints will be publicly released upon acceptance. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Training and implementation details can be found in Section 4.1, Appendix D. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our GUARDT2I significantly outperforms the baselines, surpassing the level of influence caused by randomness. Due to limited computational resources, we only fixed the random seed during evaluation. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We use NVIDIA 4090 for training and main evaluations (Appendix D). For adaptive attacks, since it requires more VRAM, around 65 GB, we conduct the experiments on NVIDIA A800 (80G) see Section 4.3. For the time of execution, we report it in Tab. 7. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We keep anonymization in submission. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: As a defensive framework, our GUARDT2I contributes to the safe use of T2I models, therefore having positive societal impacts as introduced in 1 instead of negative ones. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We perform adaptive attacks on our GUARDT2I to show the worst case, as described in Sec. 4.3. The results indicate that GUARDT2I exhibits decent robustness even under rigorous adaptive attacks. Furthermore, adaptive adversarial prompts that can bypass GUARDT2I tend to have much weakened synthesis quality, as illustrated in Fig. 8. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The employed datasets and models are all available for research purposes and are well cited. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper alone is self-contained. Moreover, codes and models will be released upon acceptance. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: All showcase images are model-generated. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: All showcase images are model-generated. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]