[{"figure_path": "FMrNus3d0n/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of GUARDT2I. GuardT2I can effectively halt the generation process of adversarial prompts to avoid NSFW generations, without compromising normal prompts or increasing inference time.", "description": "This figure illustrates how GuardT2I works.  Panel (a) shows a standard prompt generating a safe image; (b) shows an adversarial prompt generating NSFW content.  (c) shows how GuardT2I processes a normal prompt, successfully reconstructing it without altering image quality or speed. (d) shows how GuardT2I handles an adversarial prompt; the prompt is interpreted, the NSFW content is identified, and generation is halted.", "section": "1 Introduction"}, {"figure_path": "FMrNus3d0n/figures/figures_3_1.jpg", "caption": "Figure 2: The Workflow of GUARDT2I against Adversarial Prompts. (a) GUARDT2I halts the generation process of adversarial prompts. (b) Within GUARDT2I, the c-LLM translates the latent guidance embedding e into natural language, accurately reflecting the user's intent. (c) A double-folded generation parse detects adversarial prompts. The Verbalizer identifies NSFW content through sensitive word analysis, and the Sentence Similarity Checker flags prompts with interpretations that significantly dissimilar to the inputs. (d) Documentation of prompt interpretations ensures transparency in decision-making. aims to avoid offenses.", "description": "This figure illustrates the workflow of GUARDT2I in handling adversarial prompts.  It shows how GUARDT2I intercepts the generation process and uses a conditional Language Model (c-LLM) to translate the latent embedding into natural language. This interpretation is then analyzed by a two-stage parsing system: the Verbalizer checks for explicit NSFW words, and the Sentence Similarity Checker compares the interpretation to the original prompt to detect discrepancies indicative of adversarial intent.  The final decision (reject or accept) and the reasoning behind it are documented to maintain transparency. ", "section": "3 Method"}, {"figure_path": "FMrNus3d0n/figures/figures_3_2.jpg", "caption": "Figure 3: Architecture of c\u00b7LLM. T2I's text guidance embedding e is fed to c-LLM through the multi-head cross attention layer's query entry. L indicates the total number of transformer blocks.", "description": "This figure shows the architecture of the conditional large language model (c-LLM) used in GUARDT2I.  The c-LLM takes the text guidance embedding from the text-to-image model as input. This embedding is processed through multiple transformer layers incorporating a cross-attention mechanism that allows the c-LLM to condition its text generation on the input embedding.  The output of the c-LLM is a plain text interpretation of the original prompt.", "section": "3 Method"}, {"figure_path": "FMrNus3d0n/figures/figures_4_1.jpg", "caption": "Figure 1: Overview of GUARDT2I. GuardT2I can effectively halt the generation process of adversarial prompts to avoid NSFW generations, without compromising normal prompts or increasing inference time.", "description": "This figure illustrates how GUARDT2I works.  Panel (a) shows a normal prompt leading to a high-quality image synthesis. Panel (b) shows how an adversarial prompt (that looks innocuous to humans) can lead to NSFW content generation.  Panels (c) and (d) demonstrate GUARDT2I's intervention: it maintains high-quality syntheses for normal prompts and intercepts the generation process for adversarial prompts, providing an explanation.  GUARDT2I does this without adding inference time for typical prompts.", "section": "1 Introduction"}, {"figure_path": "FMrNus3d0n/figures/figures_6_1.jpg", "caption": "Figure 5: ROC curves of our GUARDT2I and baselines against various adversarial prompts. The black line represents the GUARDT2I model's consistent and high AUROC scores across different thresholds.", "description": "This figure shows the Receiver Operating Characteristic (ROC) curves for GUARDT2I and several baseline methods (OpenAI Moderation, Microsoft Azure, AWS Comprehend, NSFW Text Classifier, and Detoxify) when tested against various types of adversarial prompts (SneakyPrompt, MMA-Diffusion, I2P-Sexual, and I2P).  The y-axis represents the True Positive Rate (TPR), and the x-axis represents the False Positive Rate (FPR).  A higher AUROC score (area under the curve) indicates better performance.  GUARDT2I consistently outperforms the baselines across all four adversarial datasets, demonstrating its superior ability to detect adversarial prompts.", "section": "4.2 Main Results"}, {"figure_path": "FMrNus3d0n/figures/figures_7_1.jpg", "caption": "Figure 6: AUROC comparison over various NSFW themes. Our GUARDT2I, benefitting from the generalization capabilities of the LLM, stably exhibits decent performance under a wide range of NSFW threats.", "description": "This figure compares the Area Under the ROC Curve (AUROC) scores achieved by GUARDT2I and several baseline methods across five different NSFW themes: Violence, Self-harm, Hate, Shocking, and Illegal.  It visually demonstrates GUARDT2I's superior generalizability and consistent high performance (AUROC scores above 90% across all themes) compared to the fluctuating performance of baseline methods, which are significantly less robust to diverse NSFW content categories. The stable performance of GUARDT2I is attributed to its generative approach and the use of a Large Language Model (LLM), enhancing its ability to adapt to and correctly classify various types of NSFW content.", "section": "4.2 Main Results"}, {"figure_path": "FMrNus3d0n/figures/figures_7_2.jpg", "caption": "Figure 7: Word clouds of adversarial prompts [45], and their prompt interpretations. GUARDT2I can effectively reveal the concealed malicious intentions of attackers.", "description": "This figure shows a comparison of word clouds generated from adversarial prompts and their corresponding interpretations by GUARDT2I.  The left word cloud (a) highlights the seemingly innocuous words used in adversarial prompts that are designed to bypass safety filters of text-to-image models. The right word cloud (b) displays the words extracted by GUARDT2I from its prompt interpretation of the same inputs. The difference reveals GUARDT2I's capability to uncover the underlying NSFW intent of adversarial prompts. The highlighted words in (b) emphasize the explicit content that was implicitly expressed in the original adversarial prompts.", "section": "4.3 Evaluation on Adaptive Attacks"}, {"figure_path": "FMrNus3d0n/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of GUARDT2I. GuardT2I can effectively halt the generation process of adversarial prompts to avoid NSFW generations, without compromising normal prompts or increasing inference time.", "description": "This figure demonstrates GUARDT2I's effectiveness in handling adversarial prompts.  It shows four scenarios: (a) High-quality image generation from a standard prompt; (b) NSFW image generation from an adversarial prompt designed to produce inappropriate content; (c) GUARDT2I maintaining high-quality image generation and speed from a standard prompt; and (d) GUARDT2I successfully halting the generation process of an adversarial prompt before NSFW content is produced.  GUARDT2I's ability to detect and mitigate adversarial prompts without sacrificing performance is highlighted.", "section": "1 Introduction"}, {"figure_path": "FMrNus3d0n/figures/figures_9_1.jpg", "caption": "Figure A-1: Additional failure case analysis. Upper section: The adversarial prompt [38] generates shocking content (fake news about Trump/Thanos) but is mistakenly flagged as a normal prompt. Lower section: GUARDT21 occasionally produces false alarms due to the reconstruction of rarely used terminology (see bolded words), resulting in false positives.", "description": "This figure presents two examples of GUARDT2I's failures. The first shows a false negative where an adversarial prompt about Trump and Thanos generated an image related to them, but the system incorrectly classified it as a normal prompt. The second shows a false positive where a normal prompt mentioning uncommon words led to the system incorrectly classifying it as an adversarial prompt.  This highlights limitations in the system's ability to accurately identify certain types of adversarial prompts and the challenges in differentiating between genuinely malicious and unusually phrased prompts.", "section": "Appendix"}, {"figure_path": "FMrNus3d0n/figures/figures_16_1.jpg", "caption": "Figure 1: Overview of GUARDT2I. GuardT2I can effectively halt the generation process of adversarial prompts to avoid NSFW generations, without compromising normal prompts or increasing inference time.", "description": "This figure illustrates how GUARDT2I works.  Panel (a) shows the high-quality images generated by a text-to-image model using normal prompts. Panel (b) demonstrates that adversarial prompts, seemingly innocuous to humans, can lead to NSFW content generation.  Panels (c) and (d) showcase GUARDT2I's ability to successfully filter adversarial prompts by transforming the latent representation of these prompts back into plain text, which reveals their true, potentially harmful, intent without affecting the model's performance on legitimate prompts.", "section": "1 Introduction"}]