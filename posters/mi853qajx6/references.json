{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper is foundational to the field of large language models and introduced the concept of few-shot learning, a critical aspect in the development and use of LLMs."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduced Llama 2, a significant open-source LLM that has been extensively used in the research community, making it crucial to the study of prompt engineering and LLM robustness."}, {"fullname_first_author": "Jiasheng Gu", "paper_title": "Robustness of learning from task instructions", "publication_date": "2022-12-03", "reason": "This paper directly addresses the sensitivity of LLMs to prompt variations, a central theme of the current paper, making it highly relevant to the discussion of prompt robustness."}, {"fullname_first_author": "Moran Mizrahi", "paper_title": "State of what art? a call for multi-prompt llm evaluation", "publication_date": "2024-01-01", "reason": "This paper also directly addresses the issue of prompt robustness in LLMs, proposing a multi-prompt evaluation framework that is relevant to the methodology used in the current study."}, {"fullname_first_author": "Jiuding Sun", "paper_title": "Evaluating the zero-shot robustness of instruction-tuned language models", "publication_date": "2023-06-11", "reason": "This study directly investigates the robustness of instruction-tuned LLMs, a key area of focus in the current paper, providing important context and insights into the challenges of prompt engineering."}]}