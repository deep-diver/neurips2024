{"references": [{"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational in establishing the capabilities of large language models as few-shot learners, directly impacting the development of LLM-based agents."}, {"fullname_first_author": "Achiam, J.", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This is a technical report on GPT-4, a highly influential multimodal large language model that directly informs the research and development of LLM agents."}, {"fullname_first_author": "Bai, J.", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "publication_date": "2023-08-12", "reason": "This paper introduces Qwen-VL, a large vision-language model serving as a significant advancement in multimodal capabilities which directly impacts the application area of this paper."}, {"fullname_first_author": "Zhang, L.", "paper_title": "Mobile-Agent: Autonomous multi-modal mobile device agent with visual perception", "publication_date": "2024-01-16", "reason": "This work is directly related to this paper as it focuses on mobile device operation agents, building a foundation for this paper's advancement."}, {"fullname_first_author": "Wang, W.", "paper_title": "Cogvlm: Visual expert for pretrained language models", "publication_date": "2023-11-03", "reason": "This paper introduces CogVL, a visual-language model that enhances the capabilities of pretrained language models, which is highly relevant to the task of mobile device operation assistance."}]}