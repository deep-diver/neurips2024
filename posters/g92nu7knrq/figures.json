[{"figure_path": "g92nu7knRq/figures/figures_1_1.jpg", "caption": "Figure 1: Upper: Overview of Decoupled-head method. Multi-Head attention (MHA) has equal query, key and value heads. Grouped-Query attention (GQA) instead shares single key and value heads for each group of query heads. Decoupled-Head attention (DHA) shares key heads and value heads for different groups of query heads in different layers. Lower: GQA Initialization: Heads are mean pooled into a single head; DHA Initialization: DHA search head grouping and progressively fuse heads to maintain parameter functions.", "description": "This figure illustrates the difference between three attention mechanisms: Multi-Head Attention (MHA), Grouped-Query Attention (GQA), and Decoupled-Head Attention (DHA).  The upper part shows the architectural differences, highlighting how query, key, and value heads are arranged and shared in each method. MHA has independent heads, GQA groups query heads and shares key/value heads within each group, and DHA allows for different key/value head sharing patterns across different layers. The lower part explains the initialization process for GQA and DHA. GQA uses simple mean pooling for initialization, whereas DHA employs a more sophisticated approach that searches for groups of similar heads and progressively fuses them to preserve functionality.", "section": "1 Introduction"}, {"figure_path": "g92nu7knRq/figures/figures_2_1.jpg", "caption": "Figure 2: Visualization of the similarity between heads within the MHA of LLaMA2-7B model at the 0th layer (a) and the 21st layer (b). Details in Appendix E.1. Key heads and value heads exhibit decoupled distributions.", "description": "This figure visualizes the similarity between heads in the Multi-Head Attention (MHA) mechanism of the LLaMA2-7B model.  Two heatmaps are shown, one for layer 0 and another for layer 21. Each heatmap represents the pairwise similarity between different heads, with warmer colors indicating higher similarity. The figure highlights that the distribution of similar head clusters (groups of highly similar heads) is different across layers and between key heads and value heads, showing a decoupled distribution. This observation motivates the design of the Decoupled-Head Attention (DHA) mechanism which allocates different numbers of key heads and value heads at different layers.", "section": "3 Observation"}, {"figure_path": "g92nu7knRq/figures/figures_3_1.jpg", "caption": "Figure 3: (a) Model loss with heads proportions in linear fusion. (b) Layer Redundancy of the query, key, value head parameter matrices in the LLaMA2-7B model MHA.", "description": "The figure shows two subfigures. Subfigure (a) presents a line graph illustrating the relationship between loss and head ratios during a 4-head fusion process, demonstrating the impact of fusing similar parameters on model performance. Subfigure (b) displays a bar chart that depicts the redundancy of query, key, and value head parameters across 32 layers of a large language model, revealing variability in redundancy across layers and between key/value pairs. These findings highlight the potential for model compression by selectively fusing similar heads across different layers and parameter types.", "section": "3 Observation"}, {"figure_path": "g92nu7knRq/figures/figures_4_1.jpg", "caption": "Figure 4: An illustration of DHA. First, we reconstruct the a single head forward as a linear combination of multiple heads' forward with proportions w, grouping heads with similar functions based on multi-step optimization. Next, we initialize and optimize the fusion operators. indicates the optimization narrows the distance between proportions w. Finally, we fuse heads within groups and continued pre-training DHA model.", "description": "This figure illustrates the three stages of the Decoupled-Head Attention (DHA) model transformation from Multi-Head Attention (MHA) checkpoints. The first stage, Dependence Search, identifies groups of similar heads using multi-step optimization. The second stage, In-Group Head Fusion, initializes and optimizes fusion operators to linearly combine heads within each group, aiming to minimize the difference in functionality between the original heads and the fused head. The final stage, Post-Process, fuses the heads within groups and performs continued pre-training of the DHA model. The figure also shows the fusion loss calculation, which measures the difference between the original MHA heads and the fused DHA heads.", "section": "4 Method"}, {"figure_path": "g92nu7knRq/figures/figures_8_1.jpg", "caption": "Figure 8: Allocation of key-value head budgets with 32 of LLaMA2-7B MHA (Left) or the transformed layers in DHA-7B-25% after 240 step Search. DHA-7B-25% DHA (Right).", "description": "This figure shows a comparison of key and value head budget allocation in a 32-layer LLaMA2-7B model using the Multi-Head Attention (MHA) mechanism (left) versus the Decoupled-Head Attention (DHA) mechanism with a 25% head budget (right). The DHA model was obtained after 240 steps of the search process.  The bar chart on the left illustrates the original even distribution of key and value heads across all layers in the MHA model. In contrast, the bar chart on the right shows the adaptive allocation of heads in the DHA model, where layers are assigned a different number of key and value heads based on the fusion loss calculation in the search stage.  This adaptive allocation reflects the varying degree of redundancy found across different layers within the MHA model's parameters.", "section": "5.3 Analysis"}, {"figure_path": "g92nu7knRq/figures/figures_22_1.jpg", "caption": "Figure 1: Upper: Overview of Decoupled-head method. Multi-Head attention (MHA) has equal query, key and value heads. Grouped-Query attention (GQA) instead shares single key and value heads for each group of query heads. Decoupled-Head attention (DHA) shares key heads and value heads for different groups of query heads in different layers. Lower: GQA Initialization: Heads are mean pooled into a single head; DHA Initialization: DHA search head grouping and progressively fuse heads to maintain parameter functions.", "description": "This figure illustrates the architecture of three different attention mechanisms: Multi-Head Attention (MHA), Grouped-Query Attention (GQA), and Decoupled-Head Attention (DHA).  The top part shows the general structure of each method, highlighting the different ways that query (Q), key (K), and value (V) heads are used.  MHA has independent heads for Q, K, and V. GQA shares K and V heads across groups of Q heads.  DHA shares K and V heads across different groups of Q heads, with the grouping potentially varying between layers. The bottom part displays how model initialization differs between GQA and DHA.  GQA uses simple mean pooling to initialize its parameters while DHA employs a more sophisticated approach involving head grouping and progressive fusion to retain parametric knowledge from the original MHA checkpoint.", "section": "1 Introduction"}, {"figure_path": "g92nu7knRq/figures/figures_25_1.jpg", "caption": "Figure 1: Upper: Overview of Decoupled-head method. Multi-Head attention (MHA) has equal query, key and value heads. Grouped-Query attention (GQA) instead shares single key and value heads for each group of query heads. Decoupled-Head attention (DHA) shares key heads and value heads for different groups of query heads in different layers. Lower: GQA Initialization: Heads are mean pooled into a single head; DHA Initialization: DHA search head grouping and progressively fuse heads to maintain parameter functions.", "description": "This figure illustrates the architecture of three attention mechanisms: Multi-Head Attention (MHA), Grouped-Query Attention (GQA), and Decoupled-Head Attention (DHA).  The upper part shows a comparison of how query, key, and value heads are structured and shared in each mechanism. MHA has independent heads for each of these, GQA shares key and value heads within groups of query heads, and DHA dynamically shares key and value heads across different layers and groups of query heads. The lower part shows how each model is initialized. GQA uses mean pooling, while DHA searches for similar heads and fuses them to maintain functionality.", "section": "1 Introduction"}, {"figure_path": "g92nu7knRq/figures/figures_26_1.jpg", "caption": "Figure 1: Upper: Overview of Decoupled-head method. Multi-Head attention (MHA) has equal query, key and value heads. Grouped-Query attention (GQA) instead shares single key and value heads for each group of query heads. Decoupled-Head attention (DHA) shares key heads and value heads for different groups of query heads in different layers. Lower: GQA Initialization: Heads are mean pooled into a single head; DHA Initialization: DHA search head grouping and progressively fuse heads to maintain parameter functions.", "description": "This figure illustrates the architecture of three different attention mechanisms: Multi-Head Attention (MHA), Grouped-Query Attention (GQA), and Decoupled-Head Attention (DHA).  The upper part shows a comparison of how query, key, and value heads are structured and shared across the three methods.  MHA has independent heads for query, key, and value. GQA shares key and value heads across multiple query heads. DHA is more flexible and shares key and value heads differently across layers. The lower part shows the initialization process for GQA and DHA. GQA uses a mean pooling method to create a single head from a group of heads, while DHA uses a more sophisticated process to progressively search for and fuse similar heads.", "section": "1 Introduction"}, {"figure_path": "g92nu7knRq/figures/figures_27_1.jpg", "caption": "Figure 1: Upper: Overview of Decoupled-head method. Multi-Head attention (MHA) has equal query, key and value heads. Grouped-Query attention (GQA) instead shares single key and value heads for each group of query heads. Decoupled-Head attention (DHA) shares key heads and value heads for different groups of query heads in different layers. Lower: GQA Initialization: Heads are mean pooled into a single head; DHA Initialization: DHA search head grouping and progressively fuse heads to maintain parameter functions.", "description": "This figure illustrates the differences between three attention mechanisms: Multi-Head Attention (MHA), Grouped-Query Attention (GQA), and Decoupled-Head Attention (DHA).  The top part shows the architectural differences, highlighting how MHA uses separate key, query, and value heads for each attention head, GQA shares key and value heads for a group of query heads, and DHA shares them adaptively for different groups of query heads across layers. The bottom part shows the initialization process for GQA (mean pooling) and DHA (progressive head fusion).", "section": "1 Introduction"}]