{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, which is foundational to many large language models (LLMs) and is directly relevant to the core topic of this paper."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Fast transformer decoding: One write-head is all you need", "publication_date": "2019-11-00", "reason": "This paper proposed a significant improvement to the efficiency of Transformer decoding, a key challenge addressed in this paper's method."}, {"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-00", "reason": "This is a direct precursor to the current work, proposing a method for creating efficient multi-query attention models, and its limitations are addressed in the current paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA 2: Open Foundation and Fine-Tuned Chat Models", "publication_date": "2023-07-00", "reason": "LLaMA 2 is the large language model used as the basis for this paper's experiments, making it a critical component of the research."}, {"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-00-00", "reason": "Knowledge distillation is a technique used in model compression; this foundational paper on the topic is relevant to the current work's goal of efficient model design."}]}