[{"type": "text", "text": "Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sujai Hiremath Jacqueline Maasch Mengxiao Gao Cornell Tech Cornell Tech Tsinghua University sh2583@cornell.edu jam887@cornell.edu gaomx21@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Promit Ghosal University of Chicago promit@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Kyra Gan Cornell Tech kyragan@cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning the unique directed acyclic graph corresponding to an unknown causal model is a challenging task. Methods based on functional causal models can identify a unique graph, but either suffer from the curse of dimensionality or impose strong parametric assumptions. To address these challenges, we propose a novel hybrid approach for global causal discovery in observational data that leverages local causal substructures. We first present a topological sorting algorithm that leverages ancestral relationships in linear structural equation models to establish a compact top-down hierarchical ordering, encoding more causal information than linear orderings produced by existing methods. We demonstrate that this approach generalizes to nonlinear settings with arbitrary noise. We then introduce a nonparametric constraint-based algorithm that prunes spurious edges by searching for local conditioning sets, achieving greater accuracy than current methods. We provide theoretical guarantees for correctness and worst-case polynomial time complexities, with empirical validation on synthetic data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Causal graphical models compactly represent the data generating processes (DGP) of complex systems, including physical, biological, and social domains. Access to the true causal graph or its substructures can offer mechanistic insights [31, 13] and enable downstream causal inference, including effect estimation [11, 34, 2, 8, 16, 35]. In practice, the true causal graph is often unknown, and can be challenging to assume using domain knowledge. In such limited-knowledge settings, we can instead rely on causal discovery algorithms that learn the causal graph from observational data in a principled, automated manner [41, 7]. ", "page_idx": 0}, {"type": "text", "text": "Traditional approaches to causal discovery infer causal relationships either through conditional independence relations (PC [40]) or goodness-of-fti measures (GES [3], GRaSP [12]). While these discovery methods provide flexibility by avoiding assumptions over the functional form of the DGP, they are generally worst-case exponential in time complexity and learn Markov equivalence classes (MEC) rather than unique directed acyclic graphs (DAGs) [19]. Therefore, additional modeling assumptions are often necessary for time-efficient and accurate global discovery. ", "page_idx": 0}, {"type": "text", "text": "Certain parametric assumptions can enable recovery of the unique ground truth DAG, e.g., assuming a particular functional causal model (FCM) [48]. Under the additive noise model (ANM), we obtain unique identifiability by assuming linear causal mechanisms with non-Gaussian noise distributions [37, 36] or nonlinear causal functions with arbitrary noise [10]. Under the independent additive noise assumption, the causal parents of a variable are statistically independent of its noise term. For this class of models, discovery often entails regressing the variable of interest against its hypothesized parent set and testing for marginal independence between this set and the residual term [25]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Current FCM approaches to global causal discovery trade off between two main issues, suffering from either 1) strong parametric assumptions over the noise or functional form (or both) or 2) the use of high-dimensional nonparametric regressions, which require large sample sizes for reliable estimation and do not scale to large graphs. In addition, current FCM-based methods are ill-suited for causal discovery in sparse causal graphs, a setting that characterizes many high-dimensional applications (e.g., analysis of genetic data in healthcare applications) [6, 47, 46, 15]. ", "page_idx": 1}, {"type": "text", "text": "Contributions We propose a hybrid causal discovery approach to graph learning that combines functional causal modeling with constraint-based discovery. We depart from previous methods by characterizing conditions that allow us to search for and exploit local, rather than global, causal relationships between vertices. These local relationships stem from root vertices: this motivates a top-down, rather than bottom-up, approach. Thus, we learn the topological sort and discover true edges starting from the roots rather than the leaves, as in existing methods [25, 30, 21]. This approach leverages sparsity in both the ordering phase and edge discovery phase to reduce the size of conditioning sets, as well as the number of high-dimensional regressions. We summarize our major contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a topological ordering algorithm LHTS for linear non-Gaussian ANMs that exploits local ancestor-descendent relationships to obtain a compact hierarchical sort.   \n\u2022 We introduce a topological ordering algorithm NHTS for nonlinear ANMs that exploits local parent-child relationships to run fewer high-dimensional regressions than traditional methods, achieving lower sample complexity.   \n\u2022 We introduce a constraint-based algorithm ED that nonparametrically prunes spurious edges from a discovered topological ordering, leveraging local properties of causal edges to use smaller conditioning sets than traditional sparse regression techniques.   \n\u2022 We achieve accurate causal discovery in synthetic data, outperforming baseline methods. ", "page_idx": 1}, {"type": "text", "text": "Organization After describing the preliminaries in Section 2, we introduce the linear problem setting in Section 3, establishing the connection between ancestral relationships and causal active paths and introducing a linear hierarchical topological sorting algorithm (LHTS). Next, we extend our method to the nonlinear setting in Section 4 by establishing the connection between parental relationships and active causal paths, introducing a nonlinear hierarchical topological sorting algorithm (NHTS). In section 5, we establish a sufficient conditioning set for determining edge relations and introduce an efficient edge discovery algorithm (ED). We then test LHTS, NHTS and ED in synthetic experiments in Section 6. To conclude, we discuss future work that might generalize our approach to full ANMs. ", "page_idx": 1}, {"type": "text", "text": "Related Work Our work is related to two kinds of discovery methods that explicitly leverage the topological structure of DAGs: 1) permutation-based approaches, and 2) FCM-based approaches. ", "page_idx": 1}, {"type": "text", "text": "The original permutation-based approach SP [27] searches over the space of variable orderings to find permutations that induce DAGs with minimal edge counts. Authors in [39] introduce greedy variants of SP (such as GSP) that maintain asymptotic consistency; GRaSP [12] relaxes the assumptions of prior methods to obtain improvements in accuracy. These methods highlight the importance of using permutations for efficient causal discovery, but generally suffer from the need to bound search runtime with heuristics, poor sample efficiency in high dimensional settings, and are unable to recover a unique topological ordering or DAG ([22]). ", "page_idx": 1}, {"type": "text", "text": "On the other hand, the recent stream of FCM-based approaches decompose graph learning into two phases: 1) learning the topological sort, i.e., inferring a causal ordering of the variables; and 2) edge discovery, i.e., identifying edges consistent with the causal ordering [38, 25, 1, 30, 21, 32, 20]. ", "page_idx": 1}, {"type": "text", "text": "The literature on topological ordering algorithms for ANMs is organized along the types of parametric assumptions made on both the functional forms and noise distributions of the underlying DGP. Early approaches like ICA-LiNGAM [37] and DirectLiNGAM [38] focus on learning DAGs generated by linear functions and non-Gaussian noise terms. Recent work leverages score matching to obtain the causal ordering in settings with nonlinear functions and Gaussian noise: SCORE [30] and DAS [21] exploit particular variance properties, while DiffAN estimates the score function with a diffusion model [32]. NoGAM [20] generalizes the score-matching procedure of SCORE to nonlinear causal mechanisms with arbitrary noise distributions. RESIT [25] leverages residual independence results in nonlinear ANMs to identify topological orderings when the noise distribution is arbitrary. NoGAM and RESIT both rely on high-dimensional nonparametric regression. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Once a topological ordering is obtained, spurious edges are pruned. Works that are agnostic to the distribution of noise often use a parametric approach, implementing either a form of sparse regression (e.g., Lasso regression [18]) or a version of additive hypothesis testing with generalized additive models (GAMs) [17] (e.g., CAM-pruning [1]). RESIT [25] provides edge pruning in nonlinear ANMs, relying again on high-dimensional nonparametric regression. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We focus on structural equation models (SEMs) represented as DAGs. These graphs describe the causal relationships between variables, where an edge $x_{i}\\to x_{j}$ implies that $x_{i}$ has a direct causal influence on $x_{j}$ . Let $G=(V,E)$ be a DAG on $|V|=d$ vertices, where $E$ represents directed edges. To define pairwise relationships between vertices, we let ${\\mathrm{Ch}}(x_{i})$ denote the children of $x_{i}$ such that $x_{j}\\in\\operatorname{Ch}(x_{i})$ if and only if $x_{i}\\to x_{j}$ , and $\\mathrm{Pa}(x_{i})$ denote the parents of $x_{i}$ such that $x_{j}\\in{\\mathrm{Pa}}(x_{i})$ if and only if $x_{j}\\to x_{i}$ . Similarly, let $\\operatorname{An}(x_{i})$ denote the ancestors of $x_{i}$ such that $x_{j}\\in\\operatorname{An}(x_{i})$ if and only if there exists a directed path $x_{j}\\ \\mathrm{~--~}x_{i}$ , and ${\\mathrm{De}}(x_{i})$ denote the descendants of $x_{i}$ such that $x_{j}\\in\\mathsf{D e}(x_{i})$ if and only if there exists a directed path $x_{i}\\mathrm{~--~}x_{j}$ . Vertices can be classified based on the totality of their pairwise relationships: $x_{i}$ is a root if and only if $\\mathrm{Pa}(x_{i})=\\emptyset$ , a leaf if and only if $\\operatorname{Ch}(x_{i})=\\emptyset$ , an isolated vertex if $x_{i}$ is both a root and a leaf, and an intermediate vertex otherwise. See an illustrative DAG in Figure 1. Vertices can also be classified in terms of triadic relationships: $x_{i}$ is a confounder of $x_{j},x_{k}$ if and only if $x_{i}\\in\\mathrm{An}(x_{j})\\cap\\mathrm{An}(x_{k})$ ; a mediator of $x_{j}$ to $x_{k}$ if and only if $x_{i}\\in\\mathsf{D e}(x_{j})\\cap\\mathsf{A n}(\\dot{x_{k}})$ ; and a collider between $x_{j}$ and $x_{k}$ if and only if $x_{i}\\in\\mathsf{D e}(x_{j})\\cap\\mathsf{D e}(x_{k})$ . ", "page_idx": 2}, {"type": "text", "text": "Undirected paths that transmit causal information between vertices $x_{j},x_{k}$ can be differentiated into frontdoor and backdoor paths [42]. A frontdoor path is a directed path $x_{j}\\;\\cdots\\rightarrow\\;\\cdot\\;\\cdot\\;\\cdot\\;\\rightarrow\\;x_{k}$ that starts with an edge out of $x_{j}$ , and ends with an edge into $x_{k}$ . A backdoor path is a path $x_{j}\\ \\gets\\mathrm{~\\cdot~\\cdot~\\cdot~\\to~}x_{k}$ that starts with an edge into $x_{j}$ , and ends with an edge into $x_{k}$ . Paths that start and end with an edge out of $x_{j}$ and $x_{k}$ $(x_{k}\\;\\xrightarrow{\\;-\\;+\\;\\;.\\;.\\;\\;+\\;-\\;}x_{k})$ do not transmit causal information between $x_{j},x_{k}$ . ", "page_idx": 2}, {"type": "text", "text": "Paths between two vertices are further classified, relative to a vertex set $\\mathbf{Z}$ , as either active or inactive [42]. A path between vertices $x_{j},x_{k}$ is active relative to $\\mathbf{Z}$ if every node on the path is active relative to $\\mathbf{Z}$ . Vertex $x_{i}\\in V$ is active on path relative to $\\mathbf{Z}$ if one of the following holds: 1) $x_{i}\\in\\mathbf{Z}$ and $x_{i}$ is a collider, 2) $x_{i}\\not\\in{\\bf Z}$ and $x_{i}$ is not a collider, 3) $x_{i}\\not\\in{\\bf Z}$ , $x_{i}$ is a collider, but $\\operatorname{De}(x_{i})\\cap\\mathbf{Z}\\neq\\emptyset$ . An inactive path is simply a path that is not active. Throughout the rest of the paper, we will describe causal paths as active or inactive with respect to $\\mathbf{Z}=\\emptyset$ unless otherwise specified. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Topological orderings). Consider a given DAG $G\\,=\\,(V,E)$ . A topological sort (linear order) is a mapping $\\pi:V\\,\\rightarrow\\,\\{0,1,\\dots,|V|\\}$ , such that if $x_{i}\\,\\in\\,P a(x_{j})$ , then $x_{i}$ appears before $x_{j}$ in the sort $\\pi$ : $:\\pi(x_{i})<\\pi(x_{j})$ . A hierarchical sort (between a partial and linear order) is a mapping $\\pi_{L}:V\\to\\{0,1,\\dots,|V|\\}$ , such that if $'P a(x_{i})=\\emptyset$ , then $\\pi_{L}(x_{i})=0$ , and if $P a(x_{i})\\neq\\emptyset$ , then $\\pi_{L}(x_{i})$ equals the maximum length of the longest directed path from each root vertex to $x_{i}$ , i.e., $\\pi_{L}(x_{i})=1+\\operatorname*{max}\\left\\{\\pi_{L}(x_{j}):x_{j}\\in P a(x_{i})\\right\\}$ . ", "page_idx": 2}, {"type": "text", "text": "We note that the hierarchical sort is unique, and that it coincides with a topological sort when the number of layers equals $\\vert V\\vert$ , i.e., the DAG is complete. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.2 (ANMs). ANMs $I^{g}J$ are a popular general class of structural equation models defined over a DAG $G$ with ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{i}=f_{i}(P a(x_{i}))+\\varepsilon_{i},\\forall x_{i}\\in V,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f_{i}s$ are arbitrary functions and $\\varepsilon_{i}s$ are independent arbitrary noise distributions. ", "page_idx": 2}, {"type": "text", "text": "This model implicitly assumes the causal Markov condition and acyclicity; we adopt the aforementioned assumptions, as well as faithfulness [41]. ", "page_idx": 2}, {"type": "text", "text": "3 Linear setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first restrict our attention to ANMs that feature only linear causal functions $f$ , known as Linear Non-Gaussian Acyclic causal Models (LiNGAMs). Following [38], we note that a LiNGAM can be represented as a $d\\times d$ adjacency matrix $B=\\{b_{i j}\\}$ , where $b_{i j}$ is the coefficient from $x_{j}$ to $x_{i}$ Note that, for any topological ordering $\\pi$ of a LiNGAM, if $\\pi(x_{j})\\overset{.}{<}\\pi(x_{i})$ , then $b_{j i}=0$ . Thus, each $x_{i}\\in V$ admits the following representation: $\\begin{array}{r}{x_{i}=\\sum_{\\pi(x_{j})<\\pi(x_{i})}b_{i j}x_{j}+\\varepsilon_{i}}\\end{array}$ . ", "page_idx": 2}, {"type": "image", "img_path": "xnmm1jThkv/tmp/539a98670bb4f5bc58c946e762126dec0d2df00848333a581c0dd36e528121b5.jpg", "img_caption": ["Figure 1: Illustrative DAG, where $x_{1}$ is a root, $x_{3}$ is a leaf, $x_{3}\\in\\mathbf{Ch}(x_{2}),x_{3}\\in\\mathbf{De}(x_{1})$ . "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "xnmm1jThkv/tmp/1e543c3eb4fedcb1b596fd17545574fed3de64340307513dedbd06d8dc6e83d9.jpg", "img_caption": ["Figure 2: Enumeration of active causal path relation types between a pair of nodes $x_{i}$ and $x_{j}$ . Dashed arrows indicate ancestorship. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Identifiability Identifiability conditions for LiNGAMs [37] primarily concern the distribution of errors $\\varepsilon_{i}$ : under Gaussianity, distinct linear DGPs can admit the same joint distribution, making them impossible to distinguish. Shimizu et al. [37] generalize this intuition with independent component analysis (ICA) [4] to provide a multivariate identifiability condition for LiNGAMs (see Appendix A.1). In this section, we adopt the aforementioned condition. ", "page_idx": 3}, {"type": "text", "text": "Ancestral Relations and Active Causal Paths We first establish the connection between ancestral relationships and active causal paths. We exhaustively enumerate and define the potential pairwise causal ancestral path relations in Figure 2 and Lemma 3.1 (proof in Appendix A.2): ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1 (Active Causal Ancestral Path Relation Enumeration). Each pair of distinct nodes $x_{i},x_{j}\\in V$ can be in one of four possible active causal ancestral path relations: $A P I$ ) no active path exists between $x_{i},x_{j};A P2)$ there exists an active backdoor path between $x_{i},x_{j}$ , but there is no active frontdoor path between them; $A P3$ ) there exists an active frontdoor path between $x_{i},x_{j}$ , but there is no active backdoor path between them; $A P4$ ) there exists an active backdoor path between $x_{i},x_{j}$ , and there exists an active frontdoor path between them. ", "page_idx": 3}, {"type": "text", "text": "Next, in Lemma 3.2, we summarize the connection between causal paths and ancestral relationships (proof in Appendix A.3): ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.2. The ancestral relationship between a pair of distinct nodes $x_{i},x_{j}\\in V$ can be expressed using active causal path relations: $x_{i},x_{j}$ are not ancestrally related if and only if they are in AP1 or $A P2$ relation; and $x_{i},x_{j}$ are ancestrally related if and only if they are in AP3 or in AP4 relation. ", "page_idx": 3}, {"type": "text", "text": "The active causal ancestral path relation of a pair of nodes $x_{i},x_{j}$ that are not ancestrally related can be determined through marginal independence testing and sequential univariate regressions as illustrated in Lemmas 3.3 and 3.4 (proofs in Appendices A.4, A.5): ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.3 (AP1). Vertices $x_{i},x_{j}$ are in AP1 relation if and only if $x_{i}$ \u22a5\u22a5 $x_{j}$ . ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.4 (AP2). Let $M$ be the set of mutual ancestors between a pair of vertices $x_{i}$ and $x_{j}$ , i.e., $M=A n(x_{i})\\cap A n(x_{j})$ . Let $x_{i}^{M},x_{j}^{M}$ be the result of sequentially regressing all mutual ancestors in $M$ out of $x_{i},x_{j}$ with univariate regressions, in any order. Then, let $r_{i}^{j}$ be the residual of $x_{j}^{M}$ regressed on $x_{i}^{M}$ , and $r_{j}^{i}$ be the residual of $x_{i}^{M}$ regressed on $x_{j}^{M}$ . Suppose $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ . Then, $x_{i},x_{j}$ are in $A P2$ relation if and only if $r_{i}^{j}$ \u22a5\u22a5 $x_{i}^{M}$ and $r_{j}^{i}\\perp\\!\\!\\!\\perp x_{j}^{M}$ . ", "page_idx": 3}, {"type": "text", "text": "If a pair of nodes $x_{i},x_{j}$ is ancestrally related, fully ascertaining their ancestral relation involves discerning between the ancestor and descendent. As illustrated in Lemmas 3.5 and 3.6 (proofs in Appendices A.6, A.7), this can be determined through marginal independence testing after sequential univariate regressions with respect to the mutual ancestor set. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.5 (AP3). Let $r_{i}^{j}$ be the residual of the $x_{j}$ regressed on $x_{i}$ , and $\\boldsymbol{r}_{j}^{i}$ be the residual of $x_{i}$ regressed on $x_{j}$ . Vertices $x_{i},x_{j}$ are in $A P3$ relation if and only if $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ and one of the following holds: 1) $x_{i}$ \u22a5\u22a5 $r_{i}^{j}$ and $x_{j}$ \u0338\u22a5\u22a5 $\\boldsymbol{r}_{j}^{i},$ , corresponding to $x_{i}\\,\\in\\,A n(x_{j})$ , or 2) $x_{i}$ \u0338\u22a5\u22a5 $r_{i}^{j}$ and $\\boldsymbol{x}_{j}\\perp\\boldsymbol{\\perp}\\boldsymbol{r}_{j}^{i}$ , corresponding to $x_{j}\\in A n(x_{i})$ . ", "page_idx": 3}, {"type": "table", "img_path": "xnmm1jThkv/tmp/19d5587f1dd214275f4466a57ac5c78b8d9bef5b2d54577d2875d35a2f4f4532.jpg", "table_caption": ["Algorithm 1 LHTS: Linear Hierarchical Topological Sort "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Lemma 3.6 (AP4). Let $M$ , $r_{i}^{j},r_{j}^{i},x_{i}^{M},x_{j}^{M}$ be as defined in Lemma 3.4. Suppose $x_{i},x_{j}$ not in $A P3$ relation. Then, $x_{i},x_{j}$ are in $A P4$ relation if and only if $x_{i}\\neq.x_{j}$ and one of the following holds: $r_{i}^{j}\\,\\perp\\!\\!\\!\\perp x_{i}^{M}$ and $r_{j}^{i}\\downarrow^{j}\\ x_{j}^{M}$ corresponding to $x_{i}\\,\\in\\,A n(x_{j})$ , or 2) $r_{i}^{j}\\ \\#\\ x_{i}^{M}$ and $r_{j}^{i}\\,\\perp\\!\\!\\!\\perp x_{j}^{M}$ corresponding to $x_{j}\\in A n(x_{i})$ . ", "page_idx": 4}, {"type": "text", "text": "Linear Hierarchical Topological Sort We propose LHTS in Algorithm 1 to leverage the above results to discover a hierarchical topological ordering. Marginal independence tests and pairwise regressions are used to identify active causal ancestral path relations between pairs of vertices. ", "page_idx": 4}, {"type": "text", "text": "LHTS discovers all pairwise active causal ancestral path relations: Stage 1 discovers all AP1 relations using marginal independence tests; Stage 2 discovers all AP3 relations by detecting when pairwise regressions yield an independent residual in only one direction; Stage 3 iteratively discovers all AP2 and AP4 relations through marginal independence tests and pairwise sequential regressions involving mutual ancestors. Note that Stage 2 can be viewed as a special case of Stage 3, where the mutual ancestor set is empty. The process utilizes proofs provided in Appendices A.4, A.6, A.5, and A.7, respectively. Stage 4 uses the complete set of ancestral relations to build the final hierarchical topological sort via the subroutine AS (Algorithm 4 in Appendix A.8) by recursively peeling off vertices with no unsorted ancestors. We show the correctness of Algorithm 1 in Theorem 3.7 (proof in Appendix A.9) and subroutine $A S$ (proof in Appendix A.8), as well as the worst case time complexity in Theorem 3.8 (proof in Appendix A.10). We provide a walk-through of LHTS in Appendix A.11. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.7. Given a graph $G$ , Algorithm 1 asymptotically finds a correct hierarchical sort of $G$ . Theorem 3.8. Given n samples of $d$ vertices generated by a LiNGAM, the worst case runtime complexity of Algorithm 1 is upper bounded by $\\breve{O}(d^{3}n^{2})$ . ", "page_idx": 4}, {"type": "text", "text": "LHTS can be seen as a generalization of DirectLiNGAM [38], where they recursively identify root nodes as vertices that have either AP1 or AP3 relations with every remaining vertex at each step. The next lemma, Lemma 3.9 (proof in Appendix A.12) implies that LHTS discovers all root vertices in Stage 2, obtaining the first layer in the hierarchical sort. LHTS extends DirectLiNGAM by discovering AP2 and AP4 relations, allowing us to recover a compact hierarchical sort. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.9. A vertex is a root vertex if and only if it has AP1 or AP3 relations with all other vertices. ", "page_idx": 4}, {"type": "text", "text": "4 Nonlinear setting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we develop a version of Algorithm 1 for the nonlinear ANM (Eq. (1)) setting. Rather than detecting ancestor-descendant relationships, we outline the connection between active causal paths and parent-child relationships. We assume the unique identifiability of the nonlinear ANM [25], and provide the conditions in Appendix B.1. ", "page_idx": 4}, {"type": "image", "img_path": "xnmm1jThkv/tmp/81c33515b5ff51da9fdb7d56f4166b992c8d3623f029148f1ca3b0c75dc0d34b.jpg", "img_caption": ["Figure 3: Enumeration of the potential active causal paths among a fixed variable $x_{j}$ , one of its potential parents $x_{i}$ , and set $C\\,=\\,{\\mathrm{PA}}(x_{j})\\,\\backslash\\,x_{i}$ . Solid arrows denote parenthood relations, and undirected dashed connections indicate the existence of active paths. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Nonlinear Topological Sort In the linear setting, we determined ancestral relations through a sequence of pairwise regressions that led to independent residuals. However, a naive extension of this method into the nonlinear setting would fail, as regressions yield independent residuals under different conditions in the nonlinear case. For clarity, we demonstrate how LHTS fails to correctly recover causal relationships in an exemplary 3-node DAG with nonlinear causal mechanisms. ", "page_idx": 5}, {"type": "text", "text": "Consider a DAG $G$ with three vertices $x_{1},x_{2},x_{3}$ , where $x_{1}\\rightarrow x_{3},x_{2}\\rightarrow x_{3}$ . The functional causal relationships are nonlinear, given by $x_{1}=\\varepsilon_{1},x_{2}=\\varepsilon_{2},x_{3}=x_{1}x_{2}+\\varepsilon_{3}$ , where the $\\varepsilon_{i}\\mathbf{s}$ are mutually independent noise variables. We focus on whether LHTS can recover the parent-child relationship between $x_{1}$ and $x_{3}$ . LHTS finds that the relationship between $x_{1},x_{3}$ is unknown in Stage 1. In Stage 2, LHTS runs pairwise regressions between $x_{1},x_{3}$ but incorrectly concludes that $x_{1},x_{3}$ are not in $A P3$ relation because neither pairwise regression provides an independent residual; both parents of $x_{3}$ must be included in the covariate set for an independent residual to be recovered. ", "page_idx": 5}, {"type": "text", "text": "To handle nonlinear causal relationships, we shift our focus to searching for a different set of local substructures: the connection between active causal parental paths and the existence of parent-child relationships. We will use the existence of specific parent-child relationships to first obtain a superset of root vertices, then prune away non-roots. Once all root vertices are identified, we build the hierarchical topological sort through nonparametric regression, layer by layer. ", "page_idx": 5}, {"type": "text", "text": "Given a vertex $x_{j}$ and one of its potential parents $x_{i}$ , we first provide an enumeration of all potential active casual parental path types between them with respect to the set $C=\\mathrm{PA}(x_{j})\\setminus x_{i}$ (which could potentially be the empty set) in Figure 3 and Lemma 4.1 (proof in Appendix B.2). ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.1 (Active Causal Parental Path Relation Enumeration). Let $x_{i}$ , $x_{j}\\in V$ be a pair of distinct nodes, where $x_{i}$ is one of the potential parents of $x_{j}$ . Let $C\\,=\\,P A{\\bigl(}x_{j}{\\bigr)}\\,\\backslash\\,x_{i}$ . There are in total four possible types of active causal parental path relations between $x_{i}$ and $x_{j}$ with respect to $C$ : $P P I$ ) no active path exists between $x_{i}$ and $x_{j}$ , and no active path exists between $x_{i}$ and $C;P P2$ ) $x_{i}$ directly causes $x_{j}$ $\\tau_{i}\\to x_{j},$ ), and no active path exists between $x_{i}$ and $C;P P3_{\\!}$ ) $x_{i}$ directly causes $x_{j}$ $(x_{i}\\to x_{j},$ ), and there exists an active path between $x_{i}$ and $C;\\,P P4_{\\sim}$ ) $x_{i}$ and $x_{j}$ are not directly causally related, and there exists an active path between $x_{i}$ and $C$ . ", "page_idx": 5}, {"type": "text", "text": "Next, for a pair of distinct vertices $x_{i},x_{j}\\in V$ , we establish the connection between pairwise independence properties and active causal parental path relations in Lemma 4.2 (proof in Appendix B.3). This allows us to reduce the cardinality of the potential pairs of vertices under consideration in the later stages of the algorithm. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.2 (Non-PP1). Vertices $x_{i},x_{j}\\in V$ are not in PP1 relation if and only if $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ ", "page_idx": 5}, {"type": "text", "text": "In Lemma 4.3, we show that all pairs of vertices that are in PP2 relation can be identified through local nonparametric regressions (proof in B.4). ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.3 (PP2). Let $x_{i},x_{j}\\in V$ , $P_{i j}=\\{x_{k}\\in V:x_{k}$ \u22a5\u22a5 $x_{i},x_{k}$ \u0338\u22a5\u22a5 $x_{j}\\}$ , $r_{i}^{j}$ be the residual of $x_{j}$ nonparametrically regressed on $x_{i}$ , and $r_{i,P}^{j}$ be the residual of $x_{j}$ nonparametrically regressed on $x_{i}$ and all $x_{k}\\in P_{i j}$ . Suppose $x_{i}$ and $x_{j}$ are not in PP1 relation. Then, $x_{i}$ and $x_{j}$ are in $P P2$ relation $i f$ and only if one of the following holds: $^{\\,l}$ ) $x_{i}$ \u22a5\u22a5 $r_{i}^{j}\\;o r\\,2,$ ) $x_{i}$ \u22a5\u22a5 $r_{i,P}^{j}$ . ", "page_idx": 5}, {"type": "text", "text": "Condition 1) of Lemma 4.3 is relevant when $C=\\varnothing$ : in this case, pairwise regression identifies $x_{i}$ as a parent of $x_{j}$ . Condition 2) is relevant when $C\\neq\\emptyset$ : we leverage the independence of $x_{i}$ from the rest of $x_{j}$ \u2019s parents to generate the set $P_{i j}$ , a set that contains $C$ , but does not contain any of $x_{j}$ \u2019s descendants. If an independent residual were to be recovered by nonparametrically regressing $x_{j}$ onto $x_{i}$ and $P_{i j}$ , we identify $x_{i}$ as a parent of $x_{j}$ . ", "page_idx": 5}, {"type": "table", "img_path": "xnmm1jThkv/tmp/cb3f4205543e8280d0d343d2d920ecb1ddb744c6394e416eb444f3ae1ee101a9.jpg", "table_caption": ["Algorithm 2 NHTS: Nonlinear Hierarchical Topological Sort "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Let $W$ be the set of all parent vertices that are in PP2 relation with at least one vertex, i.e., the union of $x_{i}$ satisfying either condition of Lemma 4.3. We now show that all non-isolated root vertices are contained in $W$ , and they can be differentiated from non-roots in $W$ that are also in PP2 relations. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.4 (Roots). All non-isolated root vertices are contained in $W$ . In addition, $x_{i}\\in W$ is $a$ root vertex if and only if $^{\\,l}$ ) $x_{i}$ is not a known descendant of any $x_{j}\\in W$ , and 2) for each $x_{j}\\in W$ , either a) $x_{i}$ \u22a5\u22a5 $x_{j},\\,b)$ $x_{j}$ is in $P P2$ relation to $x_{i}$ , i.e., $x_{j}\\in C h(x_{i})$ , or c) there exists a child of $x_{i}$ , denoted by $x_{k}$ , that cannot be $d\\sb{\\l}$ -separated from $x_{j}$ given $x_{i}$ , i.e., $x_{j}$ \u0338\u22a5\u22a5 $x_{k}|x_{i}$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.4 relies on the following intuition: for any non-isolated root vertex $x_{i}$ and descendent $x_{k}\\,\\in\\,\\operatorname{De}(x_{i})$ , there exists a child of $x_{i}$ that 1) lies on a directed path between $x_{i}$ and $x_{k}$ , and 2) is in PP2 relation with $x_{i}$ . Vertex $x_{i}$ will fail to ${\\bf d}\\cdot$ -separate this specific child from the marginally dependent non-root descendent.1 On the other hand, non-roots in $W$ must ${\\mathrm{d}}\\cdot$ -separate the children they are in PP2 relation to from all marginally dependent roots: this asymmetry allows non-roots to be pruned. See Appendix B.5 for a detailed proof. ", "page_idx": 6}, {"type": "text", "text": "We propose a method, Algorithm 2, that leverages the above results to discover a hierarchical topological ordering: we first use the above lemmas to identify the roots, then use nonparametric regression to discover the topological sort. ", "page_idx": 6}, {"type": "text", "text": "In Stage 1, we discover all pairs $x_{i},x_{j}$ not in PP1 relation by testing for marginal dependence; in Stage 2, we leverage Lemma 4.3 to find the vertex pairs that are in PP2 relations, a superset of the root vertices; in Stage 3 we prune non-roots by finding they d-seperate their children from at least one other vertex in the superset (Lemma 4.4); in Stage 4 we identify vertices in the closest unknown layer by regressing them on sorted nodes and finding independent residuals. We show the correctness of Algorithm 2 in Theorem 4.5 (proof in Appendix B.6), and the worst case time complexity in Theorem 4.6 (proof in Appendix B.7). We provide a walk-through of Algorithm 2 in Appendix B.8. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.5. Given a graph $G$ , Algorithm 2 asymptotically finds a correct hierarchical sort of $G$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6. Given n samples of d vertices generated by a identifiable nonlinear ANM, the worst case runtime complexity of Algorithm 2 is upper bounded by $O(d^{3}n^{3})$ . ", "page_idx": 6}, {"type": "text", "text": "The number of nonparametric regressions run by NHTS in each step is actually inversely related to the size of the covariate sets, while the number of regressions in each step of RESIT and NoGAM are directly proportionate to the covariate set size. We provide a formal analysis of the reduction in complexity for the worst case (fully connected DAG) in Theorem 4.7 (proof in Appendix B.9): ", "page_idx": 6}, {"type": "image", "img_path": "xnmm1jThkv/tmp/24c0b1aae5f0d95e170e07d95a0a2d6f0e187b248200e1c93912aca35be796f3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "xnmm1jThkv/tmp/28ee265df4c86b733bda619159546887f001049523bda598d05b16cb85876e86.jpg", "table_caption": ["Figure 4: DAG corresponding to Lemma 5.1, which tests whether $x_{i}\\in\\mathbf{P}\\mathrm{a}(x_{j})$ (i.e., whether the red arrow exists). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Theorem 4.7. Consider a fully connected DAG $G=(V,E)$ with nonlinear ANM. Let $d:=|V|$ . Let $n_{k}^{\\mathrm{NHTS}}$ be the number of nonparametric regressions with covariate set size $k\\in[d-2]$ run by NHTS when sorting $V$ ; we similarly define $n_{k}^{\\mathrm{RESIT}}$ and $n_{k}^{\\mathrm{NoGAM}}$ respectively. Then, nNHTS= d \u2212k, and $n_{k}^{\\mathrm{RESIT}}=n_{k}^{\\mathrm{NoGAM}}=k+1$ nkNoGAM= k + 1. This implies that for all k > d2, nkN $n_{k}^{\\mathrm{NHTS}}<n_{k}^{\\mathrm{RESIT}}=n_{k}^{\\mathrm{NoGAM}}$ . ", "page_idx": 7}, {"type": "text", "text": "5 Edge discovery ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Parent selection from a topological ordering $\\pi$ via regression is traditionally a straightforward task in the infinite sample setting: for each vertex $x_{i}$ , $\\pi$ establishes $J_{i}=\\{x_{k}:\\pi(x_{k})<\\bar{\\pi}(x_{i})\\}$ , a superset of $x_{i}$ \u2019s parents that contains none of $x_{i}$ \u2019s descendants. The general strategy for pruning $\\mathrm{Pa}(x_{i})$ from $J_{i}$ is to regress $x_{i}$ on $J_{i}$ and check which $x_{k}\\in J_{i}$ are relevant predictors. The key issue is that $J_{i}$ grows large in high-dimensional graphs: current edge pruning methods either make strong parametric assumptions or suffer in sample complexity. Lasso and GAM methods impose linear and additive models, failing to correctly identify parents in highly nonlinear settings. RESIT assumes a more general nonlinear ANM, but requires huge sample sizes and oracle independence tests for accurate parent set identification: authors in [25] confirm this, saying \"despite [our] theoretical guarantee[s], RESIT does not scale well to a high number of nodes.\" ", "page_idx": 7}, {"type": "text", "text": "We propose an entirely nonparametric constraint-based method that uses a local conditioning set $Z_{i j}$ to discover whether $x_{i}\\in\\mathbf{P}\\mathrm{a}(x_{j})$ , rather than $J_{i}$ , outperforming previous methods by relaxing parametric assumptions and conditioning on fewer variables. The following lemma outlines a sufficient condition for determining whether an edge exists between two vertices (proof in Appendix C.1), visualized in Figure 4. ", "page_idx": 7}, {"type": "text", "text": "Lemma 5.1 (Parent Discovery). Let $\\pi$ be a topological ordering, $x_{i},x_{j}$ such that $\\pi(x_{i})<\\pi(x_{j})$ . Let $Z_{i j}=C_{i j}\\cup M_{i j}$ , where $C_{i j}=\\{x_{k}:x_{k}\\in P a(x_{i}),x_{k}\\not\\perp x_{j}\\}$ , $M_{i j}\\stackrel{.}{=}\\{x_{k}:x_{k}\\in P a(x_{j}),\\pi(x_{i})<$ $\\pi(x_{k})<{\\bar{\\pi}}(x_{j})\\}$ . Then, $x_{i}\\to x_{j}\\iff x_{i}$ \u0338\u22a5\u22a5 $x_{j}|Z_{i j}$ . ", "page_idx": 7}, {"type": "text", "text": "The intuition is that to determine whether $x_{i}\\to x_{j}$ , instead of conditioning on all potential ancestors of $x_{j}$ , it suffices to condition on potential confounders of $x_{i},x_{j}\\ (C_{i j})$ and potential mediators between $x_{i}$ and $x_{j}\\;(M_{i j})$ . This renders all backdoor and frontdoor paths inactive, except the frontdoor path corresponding to a potential direct edge from $x_{i}$ to $x_{j}$ . ", "page_idx": 7}, {"type": "text", "text": "Edge Discovery We propose an algorithm that leverages the above results to discover the true edges admitted by any topological ordering by running the described conditional independence test in a specific ordering. We give the implementation for pruning a linear topological sort here, but it can be generalized to a hierarchical version (see Appendix C.2). We show the correctness of Algorithm 3 in Theorem 5.2 (proof in Appendix C.3). ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. Given a correct linear topological ordering of $G$ , Algorithm 3 asymptotically finds correct parent sets $P A(x_{i}),\\forall x_{i}\\in G$ . ", "page_idx": 7}, {"type": "text", "text": "The key insight is to check each potential parent-offspring relation using the conditional independence test $x_{i}$ \u22a5\u22a5 $x_{j}|Z_{i j}$ such that previous steps in the algorithm obtain all vertices in both $C_{i j}$ and $M_{i j}$ . ", "page_idx": 7}, {"type": "text", "text": "We first fix a vertex $x_{j}$ whose parent set we want to discover. We then check if vertices ordered before $x_{j}$ are parents of $x_{j}$ in reverse order, starting with the vertex immediately previous to $x_{j}$ in the ordering. This process starts at the beginning of $\\pi$ , meaning we discover parent-offspring relations from root to leaf (see Appendix C.3 for a detailed walk-through and proof). We show the worst case time complexity of Algorithm 3 in Theorem 5.3 (proof in Appendix C.4). ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.3. Given n samples of d vertices generated by a model corresponding to a DAG G, the runtime complexity of ED is upper bounded by $O(d^{2}n^{3})$ . ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Setup Methods2 are evaluated on 20 DAGs in each trial. The DAGs are randomly generated with the Erdos-Renyi model [5]; the probability of an edge is set such that the average number of edges in each $d$ -dimensional DAG is $d$ . Gaussian, Uniform, or Laplace noise is used as the exogenous error. In experiments for linear topological sorting methods, we use linear causal mechanisms to generate the data; in experiments for nonlinear topological sorting methods (Figure 6) and edge pruning algorithms (Figure 7), we use quadratic causal mechanisms to generate the data. Existing ANM methods are prone to exploiting artifacts that are more common in simulated ANMs than real-world data [28, 29], inflating their performance on synthetic DAGs and leaving real-world applicability an open question. To reduce concerns about such artifacts, data were generated with $R^{2}$ -sortability less than 0.7 [29], and standardized to zero mean and unit variance [28]. ", "page_idx": 8}, {"type": "text", "text": "Metrics $A_{t o p}$ is equal to the percentage of edges that can be recovered by the returned topological ordering (an edge cannot be recovered if a child is sorted before a parent). We note that $A_{t o p}$ is a normalized version of the topological ordering divergence $D_{t o p}$ defined in [30]. Edge pruning algorithms return a list of predicted parent sets for each vertex: $\\begin{array}{r}{\\bar{F_{1}}=2\\frac{\\mathrm{Precision}\\cdot\\mathrm{Recall}}{\\mathrm{Precision}+\\mathrm{Recall}}}\\end{array}$ measures the performance of these predictions. ", "page_idx": 8}, {"type": "text", "text": "Linear Topological Sorts Figure 5 demonstrates the performance of our linear topological ordering algorithm, LHTS, in comparison with the benchmark algorithms, DirectLiNGAM [38] and $R^{2}$ -Sort [29]. $R^{2}$ -Sort is a heuristic sorting algorithm that exploits artifacts common in simulated ANMs; both benchmarks are agnostic to the noise distribution. We observe that both DirectLiNGAM and LHTS significantly outperform $R^{2}$ -sort. LHTS demonstrates asymptotic correctness in Figure 5(c), achieving near-perfect $A_{t o p}$ at $n=2000$ . However, LHTS has consistently lower $A_{t o p}$ than DirectLiNGAM in Figure 5(a). On the other hand, LHTS encodes more causal information: the orderings produced by LHTS in Figure 5(b) had roughly $\\sim70\\%$ fewer layers than the orderings produced by DirectLiNGAM, reducing the size of potential parent sets $J$ by identifying many non-causal relationships. ", "page_idx": 8}, {"type": "image", "img_path": "xnmm1jThkv/tmp/1b4a9c990ac03557e722f7c067b7a5be6adbb73b859f19071523a9bd92d3587b.jpg", "img_caption": ["Figure 5: Performance of LHTS on synthetic data. Top row: $n=500$ with varying dimension $d$ . Bottom row: $d=10$ with varying sample size $n$ . See Appendix D.1 for runtime results. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Nonlinear Topological Sorts Figure 6 illustrates the performance of our nonlinear topological sorting algorithm. We take GES [3], GRaSP [12], GSP [39], DirectLiNGAM [38], NoGAM [20] , and $R^{\\breve{2}}$ -Sort [29] as baseline comparators that are all agnostic to the noise distribution. We excluded PC and RESIT since in general they perform much worse than baseline methods [20]. We note that as DirectLiNGAM, NoGAM, and NHTS are FCM-based methods, they each return a unique topological ordering; however, as GES, GRaSP, and GSP are scoring-based methods [7], they return only a MEC. All topological orderings contained within an MEC satisfy every conditional independence constraint in the data, and therefore are all equally valid. To enable a fair comparison, we randomly select one ordering permitted by an outputted MEC for evaluation. We note that NHTS outperformed all baselines, achieving the highest median $A_{t o p}$ in all trials. Furthermore, as expected from Theorem 4.6, NHTS ran up to $4\\times$ faster than NoGAM (see Appendix D.2). We provide additional experiments over DAGs with increasing density in Appendix D.3. ", "page_idx": 9}, {"type": "image", "img_path": "xnmm1jThkv/tmp/5188dfbad6a029e39a5e480a332ae9b7a4dadda465b1cc570ffdf9c30b56b540.jpg", "img_caption": ["Figure 6: Performance of NHTS on synthetic data, $n=300$ , dimension $d=10$ , with varying error distributions: Gaussian, Laplace, Uniform (left, middle, right). See Appendix D.2 for runtime results. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Edge Pruning Figure 7 illustrates the performance of our edge pruning algorithm ED. We take covariate hypothesis testing with GAMs (CAM-pruning [1]), Lasso regression, and RESIT as baseline comparators that are all agnostic to the noise distribution. All algorithms were given correct linear topological sorts: ED significantly outperformed all baselines, with the highest median $F_{1}$ score in all trials. ED was slower than Lasso, but was significantly faster than the other nonlinear edge pruning algorithms, CAM-pruning and RESIT. RESIT was excluded from higher-dimensional tests due to runtime issues. The poor performance of baseline methods highlights the need for a sample efficient nonparametric method for accurate causal discovery of nonlinear DGPs. We provide additional experiments in settings with increasing density and varying noise distributions in Appendix D.4. ", "page_idx": 9}, {"type": "image", "img_path": "xnmm1jThkv/tmp/db7dd7df05ac44a8d300ce7ac16947188adc4c307d4836027f882d2a77be2d96.jpg", "img_caption": ["Figure 7: Performance of ED on synthetic data, uniform noise. Left, middle: $n=500$ with varying dimension $d$ . Right: $d=10$ with varying sample size $n$ . See Appendix D.5 for runtime results. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Discussion In this paper we developed novel global causal discovery algorithms by searching for and leveraging local causal relationships. We improved on previous topological ordering methods by running fewer regressions, each with lower dimensionality, producing hierarchical topological sorts. Additionally, we improved on previous edge pruning procedures by introducing a nonparametric constraint-based method that conditions on far fewer variables to achieve greater recovery of parent sets. We tested our methods on robustly generated synthetic data, and found that both our nonlinear sort NHTS and edge pruning algorithm ED significantly outperformed baselines. Future work includes extending the topological sorting algorithms to the full ANM setting with both linear and nonlinear functions, simultaneously exploiting both ancestor-descendent and parent-child relations, as well as adapting our approach to handle various forms of unmeasured confounding. Additionally, we aim to develop statistical guarantees of sample complexity for our methods, extending previous results [50] derived in the setting of nonlinear ANMs with Gaussian noise. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] B\u00fchlmann, P., Peters, J., and Ernest, J. CAM: Causal additive models, high-dimensional order search and penalized regression. The Annals of Statistics, 2014. URL http://arxiv.org/ abs/1310.1533.   \n[2] Cheng, D., Li, J., Liu, L., Yu, K., Duy Le, T., and Liu, J. Toward Unique and Unbiased Causal Effect Estimation From Data With Hidden Variables. IEEE Transactions on Neural Networks and Learning Systems, 2023. URL https://ieeexplore.ieee.org/document/9674196/.   \n[3] Chickering, D. M. Learning Equivalence Classes of Bayesian Network Structures. Journal of Machine Learning Research, 2002. URL https://arxiv.org/abs/1302.3566.   \n[4] Comon, P. Independent component analysis, a new concept? Signal Processing, 1994. URL https://hal.science/hal-00417283/document.   \n[5] Erdos, P. and Renyi, A. On the evolution of random graphs. Publicationes Mathematicae, 1960. URL https://publi.math.unideb.hu/load_doi.php?pdoi $=$ 10_5486_PMD_ 1959_6_3_4_12.   \n[6] Gan, K., Jia, S., and Li, A. Greedy approximation algorithms for active sequential hypothesis testing. Advances in Neural Information Processing Systems, 2021. URL https://dl.acm. org/doi/proceedings/10.5555/3540261.   \n[7] Glymour, C., Zhang, K., and Spirtes, P. Review of Causal Discovery Methods Based on Graphical Models. Frontiers in Genetics, 2019. URL https://www.frontiersin.org/ article/10.3389/fgene.2019.00524/full.   \n[8] Gupta, S., Childers, D., and Lipton, Z. C. Local Causal Discovery for Estimating Causal Effects. In Proceedings of the 2nd Conference on Causal Learning and Reasoning (CLeaR), 2023. URL http://arxiv.org/abs/2302.08070.   \n[9] Hoyer, P. O. and Hyttinen, A. Bayesian discovery of linear acyclic causal models. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence, 2009. URL https://arxiv. org/abs/1205.2641.   \n[10] Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., and Sch\u00f6lkopf, B. Nonlinear causal discovery with additive noise models. In Advances in Neural Information Processing Systems, 2008. URL https://dl.acm.org/doi/10.5555/2981780.2981866.   \n[11] Hoyer, P. O., Shimizu, S., Kerminen, A. J., and Palviainen, M. Estimation of causal effects using linear non-Gaussian causal models with hidden variables. International Journal of Approximate Reasoning, 2008. URL https://linkinghub.elsevier.com/retrieve/pii/ S0888613X08000212.   \n[12] Lam, W.-Y., Andrews, B., and Ramsey, J. Greedy Relaxations of the Sparsest Permutation Algorithm. Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence, 2022. URL https://proceedings.mlr.press/v180/lam22a/lam22a.pdf.   \n[13] Lee, J. J., Srinivasan, R., Ong, C. S., Alejo, D., Schena, S., Shpitser, I., Sussman, M., Whitman, G. J., and Malinsky, D. Causal determinants of postoperative length of stay in cardiac surgery using causal graphical learning. The Journal of Thoracic and Cardiovascular Surgery, 2022. URL https://linkinghub.elsevier.com/retrieve/pii/S002252232200900X.   \n[14] Li, A., Lee, J., Montagna, F., Trevino, C., and Ness, R. Dodiscover: Causal discovery algorithms in Python., 2022. URL https://github.com/py-why/dodiscover.   \n[15] Linder, J., Koplik, S. E., Kundaje, A., and Seelig, G. Deciphering the impact of genetic variation on human polyadenylation using aparent2. Genome biology, 2022. URL https: //pubmed.ncbi.nlm.nih.gov/36335397/.   \n[16] Maasch, J., Pan, W., Gupta, S., Kuleshov, V., Gan, K., and Wang, F. Local discovery by partitioning: Polynomial-time causal discovery around exposure-outcome pairs. In Proceedings of the 40th Conference on Uncertainy in Artificial Intelligence, 2024. URL https://arxiv. org/abs/2310.17816.   \n[17] Marra, G. and Wood, S. Practical variable selection for generalized additive models. Computational Statistics Data Analysis, 2023. URL https://www.sciencedirect.com/science/ article/abs/pii/S0167947311000491.   \n[18] Marra, G. and Wood, S. Regression Shrinkage and Selection via the Lasso. Computational Statistics Data Analysis, 2023. URL https://www.sciencedirect.com/science/article/ abs/pii/S0167947311000491.   \n[19] Montagna, F., Mastakouri, A. A., Eulig, E., Noceti, N., Rosasco, L., Janzing, D., Aragam, B., and Locatello, F. Assumption violations in causal discovery and the robustness of score matching. In 37th Conference on Neural Information Processing Systems, 2023. URL http: //arxiv.org/abs/2310.13387.   \n[20] Montagna, F., Noceti, N., Rosasco, L., Zhang, K., and Locatello, F. Causal Discovery with Score Matching on Additive Models with Arbitrary Noise. Proceedings of the 2nd Conference on Causal Learning and Reasoning, 2023. URL http://arxiv.org/abs/2304.03265.   \n[21] Montagna, F., Noceti, N., Rosasco, L., Zhang, K., and Locatello, F. Scalable Causal Discovery with Score Matching. Proceedings of the 2nd Conference on Causal Learning and Reasoning, 2023. URL http://arxiv.org/abs/2304.03382.   \n[22] Niu, W., Gao, Z., Song, L., and Li, L. Comprehensive Review and Empirical Evaluation of Causal Discovery Algorithms for Numerical Data, 2024. URL https://arxiv.org/abs/ 2407.13054.   \n[23] Pearl, J., Glymour, M., and Jewell, N. P. Causal inference in statistics: a primer. Wiley, 2016. URL https://www.datascienceassn.org/sites/default/files/CAUSAL% 20INFERENCE%20IN%20STATISTICS.pdf.   \n[24] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 2011. URL https://jmlr.csail.mit.edu/papers/volume12/ pedregosa11a/pedregosa11a.pdf.   \n[25] Peters, J., Mooij, J., Janzing, D., and Sch\u00f6lkopf, B. Causal Discovery with Continuous Additive Noise Models. Journal of Machine Learning Research, 2014. URL http://arxiv.org/abs/ 1309.6779.   \n[26] Ramos-Carreno, C. and Torrecilla, J. dcor: Distance correlation and energy statistics in python. SoftwareX, 2023. URL https://www.sciencedirect.com/science/article/ pii/S2352711023000225.   \n[27] Raskutti, G. and Uhler, C. Learning directed acyclic graphs based on sparsest permutations. STAT, 2013. URL https://arxiv.org/abs/1307.0366.   \n[28] Reisach, A., Seiler, C., and Weichwald, S. Beware of the simulated dag! causal discovery benchmarks may be easy to game. Advances in Neural Information Processing Systems, 2021. URL https://arxiv.org/abs/2102.13647.   \n[29] Reisach, A. G., Tami, M., Seiler, C., Chambaz, A., and Weichwald, S. A Scale-Invariant Sorting Criterion to Find a Causal Order in Additive Noise Models. 37th Conference on Neural Information Processing Systems, 2023. URL http://arxiv.org/abs/2303.18211.   \n[30] Rolland, P., Cevher, V., Kleindessner, M., Russel, C., Scholkopf, B., Janzing, D., and Locatello, F. Score Matching Enables Causal Discovery of Nonlinear Additive Noise Models. In Proceedings of the 39th International Conference on Machine Learning, 2022. URL https://arxiv.org/abs/2203.04413.   \n[31] Runge, J., Bathiany, S., Bollt, E., Camps-Valls, G., Coumou, D., Deyle, E., Glymour, C., Kretschmer, M., Mahecha, M. D., Mu\u00f1oz-Mar\u00ed, J., van Nes, E. H., Peters, J., Quax, R., Reichstein, M., Scheffer, M., Sch\u00f6lkopf, B., Spirtes, P., Sugihara, G., Sun, J., Zhang, K., and Zscheischler, J. Inferring causation from time series in Earth system sciences. Nature Communications, 2019. URL http://www.nature.com/articles/s41467-019-10105-3.   \n[32] Sanchez, P., Liu, X., O\u2019Neil, A. Q., and Tsaftaris, S. A. Diffusion models for causal discovery via topological ordering. Proceedings of the 39th International Conference on Machine Learning, 2023. URL https://arxiv.org/abs/2210.06201.   \n[33] Seabold, S. and Perktold, J. statsmodels: Econometric and statistical modeling with python. In 9th Python in Science Conference, 2010. URL https://www.statsmodels.org/stable/ index.html.   \n[34] Shah, A., Shanmugam, K., and Ahuja, K. Finding Valid Adjustments under Non-ignorability with Minimal DAG Knowledge. Proceedings of the 25th International Conference on Artificial Intelligence and Statistics, 2022. URL http://arxiv.org/abs/2106.11560.   \n[35] Shah, A., Shanmugam, K., and Kocaoglu, M. Front-door adjustment beyond markov equivalence with limited graph knowledge. Advances in Neural Information Processing Systems, 2023. URL https://arxiv.org/abs/2306.11008.   \n[36] Shimizu, S. Lingam: Non-Gaussian Methods for Estimating Causal Structures. Behaviormetrika, 2014. URL http://link.springer.com/10.2333/bhmk.41.65.   \n[37] Shimizu, S., Hoyer, P. O., Hyvarinen, A., and Kerminen, A. A Linear Non-Gaussian Acyclic Model for Causal Discovery. Journal of Machine Learning Research, 2006. URL https: //dl.acm.org/doi/10.5555/1248547.1248619.   \n[38] Shimizu, S., Inazumi, T., Sogawa, Y., Hyvarinen, A., Kawahara, Y., Washio, T., Hoyer, P. O., Bollen, K., and Hoyer, P. Directlingam: A direct method for learning a linear non-gaussian structural equation model. Journal of Machine Learning Research, 2011. URL https://dl. acm.org/doi/10.5555/1953048.2021040.   \n[39] Solus, L., Wang, Y., and Uhler, C. Consistency guarantees for greedy permutation-based causal inference algorithms. Biometrika, 2021. URL https://academic.oup.com/biomet/ article-abstract/108/4/795/6062392?redirectedFrom $\\lvert=$ fulltext.   \n[40] Spirtes, P. An Anytime Algorithm for Causal Inference. Proceedings of Machine Learning Research, 2001. URL https://proceedings.mlr.press/r3/spirtes01a/spirtes01a. pdf.   \n[41] Spirtes, P. and Zhang, K. Causal discovery and inference: concepts and recent methodological advances. Applied Informatics, 2016. URL https://applied-informatics-j. springeropen.com/articles/10.1186/s40535-016-0018-x.   \n[42] Spirtes, P., Glymour, C., and Scheines, R. Causation, Prediction, and Search. Springer New York, 2000. URL http://link.springer.com/10.1007/978-1-4612-2748-9.   \n[43] Squires, C. Graphical Model Learning, 2021. URL https://github.com/uhlerlab/ graphical_model_learning.   \n[44] Steeg, G. Non-parametric Entropy Estimation Toolbox, 2014. URL https://github.com/ gregversteeg/NPEET.   \n[45] Takashi, I., Mayumi, I., Yan, Z., Takashi Nicholas, M., and Shohei, S. Python package for causal discovery based on lingam. Journal of Machine Learning Research, 2023. URL https://www.jmlr.org/papers/volume24/21-0321/21-0321.pdf.   \n[46] Wang, S. K., Nair, S., Li, R., Kraft, K., Pampari, A., Patel, A., Kang, J. B., Luong, C., Kundaje, A., and Chang, H. Y. Single-cell multiome of the human retina and deep learning nominate causal variants in complex eye diseases. Cell genomics, 2022. URL https://www. sciencedirect.com/science/article/pii/S2666979X22001069.   \n[47] Wong, A. K., Sealfon, R. S., Theesfeld, C. L., and Troyanskaya, O. G. Decoding disease: from genomes to networks to phenotypes. Nature Reviews Genetics, 2021. URL https: //www.nature.com/articles/s41576-021-00389-x.   \n[48] Zhang, K. and Hyvarinen, A. On the Identifiability of the Post-Nonlinear Causal Model. Uncertainty in Artificial Intelligence, 2009. URL https://arxiv.org/pdf/1205.2599. [49] Zheng, Y., Huang, B., Chen, W., Ramsey, J., Gong, M., Cai, R., Shimizu, S., Spirtes, P., and Zhang, K. Causal-learn: Causal discovery in python. Journal of Machine Learning Research,   \n2024. URL https://arxiv.org/abs/2307.16405. [50] Zhu, Z., Locatello, F., and Cevher, V. Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/   \n0a3dc35a2391cabcb59a6b123544e3db-Paper-Conference.pdf. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Assumptions, Proofs, and Walkthrough for LHTS ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Identifiability in Linear Setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As an example [25]: suppose $X,N$ are normally distributed, $X~\\bot\\bot~N$ , and $Y\\;=\\;a X\\,+\\,N$ . If \u00afa =a2Va Va ra(rX()X+)\u03c32 \u0338= a1 and N\u00af = X \u2212\u00afaY , then N\u00af \u22a5\u22a5Y and the following DGP also fits the same distribution of $X,Y,N$ : $X=\\bar{a}Y+\\bar{N}$ . To generalize the intuition that non-Gaussianity could lead to identifiability, [37] use independent component analysis [4] to provide the following theorem: ", "page_idx": 14}, {"type": "text", "text": "Theorem A.1. Assume a linear SEM with graph $G$ , where $\\begin{array}{r}{x_{i}\\;=\\;\\sum_{k(j)<k(i)}b_{i j}x_{j}\\,+\\,\\varepsilon_{i},\\forall j\\;=\\;}\\end{array}$ $1,\\cdot\\cdot,d$ , where all $\\varepsilon_{j}$ are jointly independent and non-Gaussian distributed. Additionally, for each $j\\in\\{1,\\ldots,d\\}$ we require $b_{i j}\\neq0$ for all $j\\in P a(x_{i})$ . Then, $G$ is identifiable from the distribution. ", "page_idx": 14}, {"type": "text", "text": "We assume that the identifiability conditions described in Theorem A.1 hold throughout Section 3. ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 3.1 (Active Causal Ancestral Path Relation Enumeration). Each pair of distinct nodes xi, $,x_{j}\\in V$ can be in one of four possible active causal ancestral path relations: $A P I$ ) no active path exists between $x_{i},x_{j};A P2)$ there exists an active backdoor path between $x_{i},x_{j}$ , but there is no active frontdoor path between them; $A P3$ ) there exists an active frontdoor path between $x_{i},x_{j}$ , but there is no active backdoor path between them; $A P4_{\\cdot}$ ) there exists an active backdoor path between $x_{i},x_{j}$ and there exists an active frontdoor path between them. ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 3.1. For a pair of nodes $x_{i}$ and $x_{j}$ , either $x_{i}\\perp\\!\\!\\!\\perp x_{j}$ or $x_{i}\\not\\perp x_{j}$ . If the former, then $x_{i},x_{j}$ are in AP1 relation. Suppose the latter is true: $x_{i}\\neq x_{j}$ implies $\\exists$ at least one active path between $x_{i},x_{j}$ . Active paths can either be backdoor or frontdoor paths: therefore either a frontdoor path exists, a backdoor path exists, or both a frontdoor and backdoor path exist. Thus, $x_{i},x_{j}$ are either in AP2, AP3, or AP4 relation. ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 3.2. The ancestral relationship between a pair of distinct nodes $x_{i},x_{j}\\in V$ can be expressed using active causal path relations: $x_{i},x_{j}$ are not ancestrally related if and only if they are in AP1 or $A P2$ relation; and $x_{i},x_{j}$ are ancestrally related if and only if they are in AP3 or in AP4 relation. ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 3.2. Note, $x_{i},x_{j}$ are ancestrally related if and only if there exists an active frontdoor path between them. The conclusion follows immediately. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 3.3 (AP1). Vertices $x_{i},x_{j}$ are in AP1 relation if and only if $x_{i}$ \u22a5\u22a5 $x_{j}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 3.3. Note, $x_{i}\\perp\\!\\!\\!\\perp x_{j}$ if and only if there does not exist an active causal path between them. The conclusion follows immediately. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.5 Proof of Lemma 3.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 3.4 (AP2). Let $M$ be the set of mutual ancestors between a pair of vertices $x_{i}$ and $x_{j}$ , i.e., M = An(xi) \u2229An(xj). Let xiM , xj be the result of sequentially regressing all mutual ancestors in $M$ out of $x_{i},x_{j}$ with univariate regressions, in any order. Then, let $\\boldsymbol{r}_{i}^{j}$ be the residual of xjM regressed on $x_{i}^{M}$ , and $r_{j}^{i}$ be the residual of $x_{i}^{M}$ regressed on $x_{j}^{M}$ . Suppose $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ . Then, $x_{i},x_{j}$ are in $A P2$ relation if and only if $r_{i}^{j}$ \u22a5\u22a5 $x_{i}^{M}$ and $r_{j}^{i}\\perp\\!\\!\\!\\perp x_{j}^{M}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 3.4. Under an identifiable LiNGAM, and given a linear topological ordering $k$ : $V\\mapsto\\mathbb{R}$ , $x_{i}$ and $x_{j}$ admit the following representation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{i}=\\sum_{k(m)<k(i)}\\alpha_{i m}\\varepsilon_{m}+\\varepsilon_{i}},}\\\\ {{\\displaystyle x_{j}=\\sum_{k(m)<k(j)}\\alpha_{j m}\\varepsilon_{m}+\\varepsilon_{j}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\varepsilon_{m}$ are jointly independent noise terms and non-Gaussian distributed, following from Theorem A.1. Additionally, for each $m:k(m)<k(i)$ we require $\\alpha_{i m}\\neq0$ for all $m\\in P a(x_{i})$ . Similarly for $\\alpha_{j m}$ . ", "page_idx": 15}, {"type": "text", "text": "We first show the forward direction. Suppose $x_{i},x_{j}$ are in AP2 relation. Note, $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ . Let $\\overline{{M}}$ be the complement of $M$ , i.e., ${\\overline{{M}}}=V\\setminus M$ . Then, $\\exists$ the following decomposition of $x_{i},x_{j}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{i}=\\sum_{x_{m}\\in{\\cal M}}\\alpha_{i m}\\varepsilon_{m}+\\sum_{x_{m}\\in{\\overline{{{\\cal M}}}}\\cap\\mathrm{An}(x_{i})}\\alpha_{i m}\\varepsilon_{m}+\\varepsilon_{i}},}\\\\ {{\\displaystyle x_{j}=\\sum_{x_{m}\\in{\\cal M}}\\alpha_{j m}\\varepsilon_{m}+\\sum_{x_{m}\\in{\\overline{{{\\cal M}}}}\\cap\\mathrm{An}(x_{j})}\\alpha_{j m}\\varepsilon_{m}+\\varepsilon_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, the xiM , x jMhave the general form ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i}^{M}=\\displaystyle\\sum_{x_{m}\\in Y}\\beta_{i m}\\varepsilon_{m}+\\varepsilon_{i},}\\\\ &{x_{j}^{M}=\\displaystyle\\sum_{x_{m}\\in Z}\\beta_{j m}\\varepsilon_{m}+\\varepsilon_{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $Y\\subseteq{\\overline{{M}}}\\cap\\operatorname{An}(x_{i})),Z\\subseteq{\\overline{{M}}}\\cap\\operatorname{An}(x_{j}))$ . Note, $Y\\cap Z=\\emptyset$ . As $\\varepsilon_{i}$ are all mutually independent, this implies that rij \u22a5\u22a5rji. ", "page_idx": 15}, {"type": "text", "text": "We now show the reverse direction. Suppose $x_{i}\\not\\perp x_{j},\\:r_{i}^{j}\\:\\perp\\:x_{i}^{M}$ , and $r_{j}^{i}\\ \\perp\\!\\!\\!\\perp x_{j}^{M}$ . Note that $x_{i}\\not\\perp x_{j}\\implies x_{i},x_{j}$ are not in AP1 relation. Suppose for contradiction that $x_{i},x_{j}$ are in AP3 or AP4 relation, where $x_{i}\\in\\mathsf{A n}(x_{j})$ (WLOG). Then $\\exists$ a frontdoor path between $x_{i},x_{j}$ , WLOG $x_{i}\\in{\\mathrm{An}}(x_{j})$ . Regressing the descendent on the ancestor will always result in a dependent residual, even after projecting out the influence of the mutual ancestors. Therefore, xjM \u0338\u22a5\u22a5rji, leading to a contradiction. Therefore, $x_{i},x_{j}$ must be in AP2 relation. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.6 Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 3.5 (AP3). Let $r_{i}^{j}$ be the residual of the $x_{j}$ regressed on $x_{i}$ , and $\\boldsymbol{r}_{j}^{i}$ be the residual of $x_{i}$ regressed on $x_{j}$ . Vertices $x_{i},x_{j}$ are in $A P3$ relation if and only if $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ and one of the following holds: 1) $x_{i}$ \u22a5\u22a5 $\\boldsymbol{r}_{i}^{j}$ and $x_{j}$ \u0338\u22a5\u22a5 $r_{j}^{i}$ , corresponding to $x_{i}\\,\\in\\,A n(x_{j})$ , or 2) $x_{i}$ \u0338\u22a5\u22a5 $\\boldsymbol{r}_{i}^{j}$ and $x_{j}$ \u22a5\u22a5 $\\boldsymbol{r}_{j}^{i}$ , corresponding to $x_{j}\\in A n(x_{i})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 3.5. We first show the forwards direction. Suppose $x_{i},x_{j}$ are in $A P3$ relation. Note an active path exists between $x_{i},x_{j}$ , so $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ . WLOG, let $x_{i}\\in\\mathbf{A}\\mathfrak{n}(\\bar{x}_{j})$ . Then, $x_{i},x_{j}$ admit the following representation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{i}=x_{i}}}\\\\ {{\\displaystyle x_{j}=b_{j i}x_{i}+\\sum_{x_{m}\\in\\mathrm{An}(x_{j})\\backslash x_{i}}\\alpha_{j m}x_{m}+\\varepsilon_{j}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note, as there does not exist a backdoor path between $x_{i},x_{j}$ , we have $x_{i}$ \u22a5\u22a5 $x_{m}\\forall x_{m}\\in\\mathrm{An}(x_{j})\\setminus x_{i}$ . Therefore, $r_{i}^{j}\\ \\perp\\perp x_{i}$ . Note that $x_{j}\\,\\in\\,{\\mathrm{De}}(x_{i})$ : pairwise regression will leave $r_{j}^{i}\\nrightarrow x_{j}$ as this an example of reverse causality, a special case of endogeneity [23]. ", "page_idx": 15}, {"type": "text", "text": "We now show the reverse direction. WLOG, suppose $x_{i},x_{j}$ satisfy condition 1) in Lemma 3.5. As $x_{i}\\neq x_{j}$ , they cannot be in AP1 relation. Suppose for contradiction that they are in AP2 or AP4 relation. Then, $\\exists$ a backdoor path between $x_{i},x_{j}$ , and a confounding $x_{z}$ on that backdoor path. As $x_{z}$ confounds $x_{i},x_{j}$ and is not adjusted for in a pairwise regression, we have $x_{i}$ \u0338\u22a5\u22a5 $r_{i}^{j},x_{j}$ \u0338\u22a5\u22a5 $\\boldsymbol{x}_{j}^{i}$ , a contradiction. Therefore, $x_{i},x_{j}$ must be in AP2 relation. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.7 Proof of Lemma 3.6 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 3.6 (AP4). Let $M$ , $r_{i}^{j},r_{j}^{i},x_{i}^{M},x_{j}^{M}$ be as defined in Lemma 3.4. Suppose $x_{i},x_{j}$ not in AP3 relation. Then, $x_{i},x_{j}$ are in $A P4$ relation if and only if $x_{i}\\neq.x_{j}$ and one of the following holds: $r_{i}^{j}\\,\\perp\\!\\!\\!\\perp x_{i}^{M}$ and $r_{j}^{i}\\downarrow^{j}\\ x_{j}^{M}$ corresponding to xi \u2208An(xj), or 2) rij \u0338\u22a5\u22a5xiMand rji \u22a5\u22a5xjM corresponding to $x_{j}\\in A n(x_{i})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3.6. We first show the forward direction. Suppose $x_{i},x_{j}$ are in AP4 relation. Note that an active path exists between $x_{i},x_{j}$ , so $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ . WLOG, suppose $x_{i},\\in\\mathrm{An}(x_{j})$ . Note, $x_{i}^{M},x_{j}^{M}$ are the result of projecting mutual ancestors $M$ out of $x_{i},x_{j}$ . Therefore, xiM , xjM admit the following representation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{i}^{M}=\\sum_{x_{m}\\in\\overline{{{M}}}\\cap\\mathrm{An}(x_{i})}}\\displaystyle\\alpha_{i m}\\varepsilon_{m}+\\varepsilon_{i}\\hfill}}\\\\ {{\\displaystyle x_{j}^{M}=\\alpha_{j i}\\varepsilon_{i}+\\sum_{x_{m}\\in\\overline{{{M}}}\\cap\\mathrm{An}(x_{j})}}\\alpha_{j m}\\varepsilon_{m}+\\varepsilon_{j}\\hfill}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $\\overline{{M}}\\cap\\mathbf{An}(x_{i})\\cap(\\overline{{M}}\\cap\\mathbf{An}(x_{j})\\cup x_{j})=\\emptyset.$ . Therefore, $\\boldsymbol{r}_{i}^{j}$ \u22a5\u22a5 $x_{i}^{M}$ . Note that $\\varepsilon_{i}\\in$ both $x_{i}^{M},x_{j}^{M}$ , therefore $r_{j}^{i}\\neq{x_{j}^{M}}$ . ", "page_idx": 16}, {"type": "text", "text": "We now show the reverse direction. Suppose condition 1) holds, i.e., $x_{i}\\neq x_{j}$ and WLOG $\\boldsymbol{r}_{i}^{j}$ \u22a5\u22a5 $x_{i}^{M},r_{j}^{i}\\nbot x_{j}^{M}$ . Suppose for contradiction that $x_{i},x_{j}$ not in AP4 relation. They cannot be in AP1 relation because $x_{i}\\neq x_{j}$ . They cannot be in AP2 relation because $\\boldsymbol{x}_{j_{.}}^{M}\\neq\\boldsymbol{f}_{j}^{i}$ . By assumption they are not in AP3 relation. Therefore, we have a contradiction, therefore they are in AP4 relation. ", "page_idx": 16}, {"type": "text", "text": "A.8 Ancestor Sort ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "xnmm1jThkv/tmp/7eff8fcd4ebd6d98515d45154454720d152714f4b95f20a24eb77b0347ab098c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Overview In each iteration, this algorithm uses the set of ancestral relations to identify which vertices have no ancestors amongst the vertices that have yet to be sorted. It peels those nodes off, adding them as a layer to the hierarchical topological sort. First the roots are peeled off, then the next layer, and so on. ", "page_idx": 16}, {"type": "text", "text": "Proof of Correctness ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The input to the algorithm is an ancestor table $A R S$ cataloging all ancestral relations for every pair of nodes in an unknown DAG $G$ . Let $H$ be the minimum number of layers $(H-1$ is the length of the longest directed path in the graph) necessary in a valid hierarchical topological sort of $G$ . ", "page_idx": 16}, {"type": "text", "text": "1. $H=1$ . None of the nodes have ancestors, so all nodes are added to layer 1 in Stage 1. 2. $H=2$ . Nodes with no ancestors are added to layer 1 in Stage 1, and then removed from the remaining variables in Stage 2. As $H=2$ , the longest directed path is 1, so all nodes in the remaining variables must have no ancestors in the remaining variables (otherwise, this would make the longest directed path 2). The nodes are added to layer 2 in Stage 1, and removed from the remaining variables in Stage 2. Remaining variables is empty, so now the order is correctly returned. ", "page_idx": 17}, {"type": "text", "text": "Inductive Assumption For any graph $G$ with $H<k$ , Hierarchical Topological Sort will return a minimal hierarchical topological ordering. ", "page_idx": 17}, {"type": "text", "text": "We now show that hierarchical topological sort now yields a minimal hierarchical topological ordering for $G$ where $H=k$ . ", "page_idx": 17}, {"type": "text", "text": "1. In the first iteration of Stage 1, nodes with no ancestors will be added to layer 1, then removed from the remaining variables in Stage 2.   \n2. At this point, note that we can consider the induced subgraph formed by the nodes left in remaining, $G^{\\prime}$ . The minimal hierarchical topological ordering that represents $G^{\\prime}$ must have $k-1$ layers. It cannot be greater than $k-1$ , because $G^{\\prime}$ is a subgraph of $G$ and we removed nodes in layer 1 of, reducing the maximal path length by 1. It cannot be less than $k-1$ , because that would imply that $k$ was not the minimal number of layers needed to represent $G$ .   \n3. By Inductive Assumption, Hierarchical Topological sort will return the a correct minimal hierarchical topological ordering for the induced subgraph $G^{\\prime}$ .   \n4. Appending the layer 1 and the sort produced by $G^{\\prime}$ yields the full hierarchical topological sort for $G$ . ", "page_idx": 17}, {"type": "text", "text": "Inductive assumption is satisfied for $H=k$ , so for a graph $G$ with arbitrary $H$ , the algorithm recovers the correct hierarchical topological sort. ", "page_idx": 17}, {"type": "text", "text": "A.9 Proof of Correctness for Linear Hierarchical Topological Sort ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem A.2. 3.7 Given a graph $G$ , Algorithm 1 asymptotically finds a correct hierarchical sort of $G$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. The goal is to show that all ancestral relations between distinct pair of nodes $x_{i},x_{j}\\in V$ determined by LHTS in Stages 1, 2, and 3 are correct. By A.8, Stage 4 will return a valid hierarchical topological sort given fully identified ancestral relations. ", "page_idx": 17}, {"type": "text", "text": "Stage 1 identifies $x_{i},x_{j}$ as in AP1 relaton if and only if $x_{i}\\perp\\!\\!\\!\\perp x_{j}$ : it follows by Lemma 3.3 that vertices in AP1 relation are correctly identified, and vertices in AP2, AP3 and AP4 relation are not identified. ", "page_idx": 17}, {"type": "text", "text": "Stage 2 identifies $x_{i},x_{j}$ unidentified in Stage 1 as in AP3 relation if and only if after pairwise regression, one of the residuals is dependent, and the other is independent: it follows by Lemma 3.5 vertices in AP3 relation are correctly identified, and vertices in AP2 or AP4 relation are not identified. ", "page_idx": 17}, {"type": "text", "text": "Consider a hierarchical topological sort of DAG $G\\,\\pi_{H}$ , with $h$ layers. Note, Layer 1 of $\\pi_{H}$ consists of root nodes. Ancestral relations between root nodes and all other nodes are of type AP1 and AP3, and were discovered in Stage 1 and Stage 2. ", "page_idx": 17}, {"type": "text", "text": "We induct on layers to show that Stage 3 recovers all ancestral relations of type AP2 and AP4, one layer at a time in each iteration. ", "page_idx": 17}, {"type": "text", "text": "Base Iterations (1,2) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. All ancestral relationships are known for nodes in layer 1 of $\\pi_{H}$ , therefore all mutual ancestors of layer 2 and every lower layer are known. Then, by Lemma 3.4 and Lemma 3.6 ancestral relations of type AP2 and AP3 between vertices in layer 2 and all lower layers are discovered in iteration 1.   \n2. All ancestral relationships are known for nodes in layer 1 and 2 of $\\pi_{H}$ , therefore all mutual ancestors of layer 3 and every lower layer are known. Then, by Lemma 3.4 and Lemma 3.6 ancestral relations of type AP2 and AP3 between vertices in layer 2 and all lower layers are discovered in iteration 2. ", "page_idx": 18}, {"type": "text", "text": "Iteration $k-1$ , Inductive Assumption We have recovered all ancestral relationships of nodes in layers 0 to $k-1$ . ", "page_idx": 18}, {"type": "text", "text": "1. As ancestral relationships of nodes in layers 0 to $k-1$ are known, mutual ancestors of vertices in layer $k$ and every lower layer are known, so by Lemma 3.4 and 3.4 all ancestral relations of type AP2 and AP4 between vertices in layer $k$ and every lower layer are discovered. ", "page_idx": 18}, {"type": "text", "text": "Iteration $k-1$ Inductive Assumption is satisfied for iteration $k$ , therefore we recover all ancestral relations of type AP2 and AP4 and 4) for nodes in layers 0 to $k$ . Thus, for a DAG with an arbitrary number of layers, Stage 3 recovers all ancestral relations of type AP2 and AP4. ", "page_idx": 18}, {"type": "text", "text": "A.10 Time Complexity Proof for Linear Hierarchical Topological Sort ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. In Stage 1, LHTS runs $O(d^{2})$ marginal independence tests that each have $O(n^{2})$ complexity. In each step for Stage 2 and Stage 3, LHTS runs $O(\\dot{d}^{2})$ marginal independence tests each with $O(n^{\\dot{2}})$ complexity. In the worse case of a fully connected DAG, there are $\\dot{d}^{2}$ steps in total, across Stage 2 and Stage 3: this is because in each step one layer of the layered sort DAG is identified, and a fully connected DAG has $d$ layers. Therefore, the overall sample complexity of LHTS is $O(d^{3}n^{2})$ . ", "page_idx": 18}, {"type": "text", "text": "A.11 Walk-through of LHTS ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The following diagram illustrates each stage of LHTS on an exemplary 5-node DAG: ", "page_idx": 18}, {"type": "image", "img_path": "xnmm1jThkv/tmp/3c26d38afe8e389e4d888d186450888481df24e50179197eace21d2e2b8ebe12.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "All vertices are dependent on each other, so no AP1 relations are discovered in Stage 1. In Stage 2, vertex A is discovered in AP2 relation to all vertices, and vertex D is discovered in AP2 relation to vertex E. In Stage 3, vertices B and C are discovered to be in AP3 relation to each other; then, we discover vertex B in AP4 relation to vertex D, and vertex C in AP4 relation to vertex D. Therefore, in Stage 4, we recover the topological sort after running the AS subroutine. ", "page_idx": 18}, {"type": "text", "text": "A.12 Proof of Lemma 3.9: Root Active Causal Ancestral Path Relations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 3.9. A vertex is a root vertex if and only if it has AP1 or AP3 relations with all other vertices. ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 3.9. We first show the forward direction. If $x_{i}$ is a root, then $\\mathrm{De}(x_{i})=\\emptyset$ . Therefore, it cannot have a backdoor path between it and any other vertex, therefore it must be in either $A P1$ or $A P3$ relation with other vertices. ", "page_idx": 19}, {"type": "text", "text": "We now show the reverse direction. If $x_{i}$ is in AP1 or AP3 (as an ancestor) relations with all other nodes, it cannot have any parents. Therefore, $x_{i}$ is a root. ", "page_idx": 19}, {"type": "text", "text": "B Assumptions, Proofs, and Walkthrough for NHTS ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 Identifiability in Nonlinear Setting ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Following the style of [19], we first observe that the following condition guarantees that the observed distribution of a pair of variables $x_{i},x_{j}$ can only be generated by a unique ANM: ", "page_idx": 20}, {"type": "text", "text": "Condition B.1 (Hoyer & Hyttinen [9]). Given a bivariate model $x_{i}=\\varepsilon_{i},x_{j}=f_{j}(x_{i})\\!+\\!\\varepsilon_{j}$ generated according to (1), we call the SEM an identifiable bivariate ANM if the triple $(f_{i},p_{\\varepsilon_{i}},p_{\\varepsilon_{j}})$ does not solve the differential equation $\\begin{array}{r}{k^{\\prime\\prime\\prime}=k^{\\prime\\prime}(-\\frac{g^{\\prime\\prime\\prime}f^{\\prime}}{g^{\\prime\\prime}}+\\frac{f^{\\prime\\prime}}{f^{\\prime}})-2g^{\\prime\\prime}f^{\\prime\\prime}f^{\\prime}+g^{\\prime}f^{\\prime\\prime\\prime}+\\frac{g^{\\prime}g^{\\prime\\prime\\prime}f^{\\prime\\prime}f^{\\prime}}{g^{\\prime\\prime}}-\\frac{g^{\\prime}(f^{\\prime\\prime})^{2}}{f^{\\prime}},}\\end{array}$ for all $x_{i},x_{j}$ such that $f^{\\prime}(x_{i})g^{\\prime\\prime}(x_{j}-f_{j}(x_{i}))\\stackrel{\\circ}{\\neq}0$ , where $p_{\\varepsilon_{i}},p_{\\varepsilon_{j}}$ are the density of $\\varepsilon_{i},\\varepsilon_{j}$ , $f=f_{j},k=$ $\\log p_{\\varepsilon_{i}},g=p_{\\varepsilon_{j}}$ . The arguments $x_{j}-f_{j}(x_{i}),x_{i}$ and $x_{i}$ of $g,k$ and $f$ respectively, are removed for readability. ", "page_idx": 20}, {"type": "text", "text": "There is a generalization of this condition to the multivariate nonlinear ANM proved by [25]: ", "page_idx": 20}, {"type": "text", "text": "Theorem B.2. (Peters et al. [25]). An ANM corresponding to $D A G G$ is identifiable $i f\\forall x_{j}\\in V,x_{i}\\in$ $P a(x_{j})$ and all sets $S\\subseteq V$ with $P a(x_{j})\\setminus\\{i\\}\\subseteq S\\subseteq{\\overline{{D e(j)}}}\\setminus\\{x_{i},x_{j}\\},$ , $\\exists\\,X_{S}$ with positive joint density such that the triple $\\Big(f_{j}(P a(j)\\backslash\\{x_{i}\\},x_{i}),p_{x_{i}|X_{s}},p_{\\varepsilon_{j}}\\Big)$ satisfies Condition B.1, and $f_{j}$ are non-constant in all arguments. ", "page_idx": 20}, {"type": "text", "text": "We assume that the identifiability conditions described in Theorem B.2 hold throughout Section 4. ", "page_idx": 20}, {"type": "text", "text": "B.2 Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma 4.1 (Active Causal Parental Path Relation Enumeration). Let $x_{i}$ , $x_{j}\\in V$ be a pair of distinct nodes, where $x_{i}$ is one of the potential parents of $x_{j}$ . Let $C\\,=\\,P A{\\bigl(}x_{j}{\\bigr)}\\,\\backslash\\,x_{i}$ . There are in total four possible types of active causal parental path relations between $x_{i}$ and $x_{j}$ with respect to $C$ : $P P I$ ) no active path exists between $x_{i}$ and $x_{j}$ , and no active path exists between $x_{i}$ and $C;P P2_{\\circ}$ ) $x_{i}$ directly causes $x_{j}$ $(x_{i}\\to x_{j},$ ), and no active path exists between $x_{i}$ and $C;P P3$ ) $x_{i}$ directly causes $x_{j}$ $(x_{i}\\to x_{j},$ ), and there exists an active path between $x_{i}$ and $C;\\,P P4_{\\prime}$ ) $x_{i}$ and $x_{j}$ are not directly causally related, and there exists an active path between $x_{i}$ and $C$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Either $x_{i}$ \u22a5\u22a5 $x_{j}$ or $x_{i}\\neq x_{j}$ : if the former, then $x_{i},x_{j}$ are in PP1 relation. Suppose the latter is true. Then, either $x_{i}$ \u22a5\u22a5 $C$ or $x_{i}\\neq C$ : if the former, then $x_{i},x_{j}$ are in PP2 relation. Suppose the latter is true. Then, either $x_{i}\\in\\mathbf{P}\\mathbf{A}(x_{j})$ or $x_{i}\\not\\in{\\bf P A}(x_{j})$ . If the former, then $x_{i},x_{j}$ are in PP3 relation, and if the latter, then $x_{i},x_{j}$ are in PP4 relation. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "B.3 Proof of Lemma 4.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma 4.2 (Non-PP1). Vertices $x_{i},x_{j}\\in V$ are not in PP1 relation if and only if $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. We first show the forward direction. If $x_{i},x_{j}$ are not in PP1 relation, then either $x_{i}\\in\\mathbf{P}\\mathrm{a}(x_{j})$ or there exists an active causal path between $x_{i},x_{j}$ ; therefore, $x_{i}\\not\\perp x_{j}$ . ", "page_idx": 20}, {"type": "text", "text": "$x_{i}$ $x_{j}$ $x_{i},x_{j}$ therefore they cannot be in PP1 relation. ", "page_idx": 20}, {"type": "text", "text": "B.4 Proof of Lemma 4.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma 4.3 (PP2). Let $x_{i},x_{j}\\in V$ , $P_{i j}=\\{x_{k}\\in V:x_{k}$ \u22a5\u22a5 $x_{i},x_{k}$ \u0338\u22a5\u22a5 $x_{j}\\}$ , $r_{i}^{j}$ be the residual of $x_{j}$ nonparametrically regressed on $x_{i}$ , and $r_{i,P}^{j}$ be the residual of $x_{j}$ nonparametrically regressed on $x_{i}$ and all $x_{k}\\in P_{i j}$ . Suppose $x_{i}$ and $x_{j}$ are not in PP1 relation. Then, $x_{i}$ and $x_{j}$ are in PP2 relation $i f$ and only if one of the following holds: $^{\\,l}$ $)\\;x_{i}\\perp\\!\\!\\!\\perp r_{i}^{j}\\;o r\\,2)\\;x_{i}\\perp\\!\\!\\!\\perp r_{i,P}^{j}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Note, there exists two sub cases of PP2 relation: PP2a) $x_{i}$ is the only parent of $x_{j}$ , i.e. $|C|=0$   \nand PP2b) $x_{i}$ is not the only parent of $x_{j}$ , i.e., $|C|>0$ . ", "page_idx": 20}, {"type": "text", "text": "We first show the forward direction. Suppose $x_{i},x_{j}$ are in PP2a relation. Then, as the ", "page_idx": 21}, {"type": "equation", "text": "$$\nx_{j}=f_{j}(x_{i})+\\varepsilon_{j}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$x_{i}$ \u22a5\u22a5 $\\boldsymbol{r}_{i}^{j}$ . Suppose that $x_{i},x_{j}$ are in a PP2b relation. Consider $P_{i j}$ : note that $x_{i}$ \u22a5\u22a5 $C$ , and $x_{j}\\neq C$ , $C\\subseteq P_{i j}$ . Note that $\\mathrm{De}(x_{j})\\not\\perp x_{i}$ , therefore $\\mathrm{De}(x_{j})\\notin P_{i j}$ . Therefore, $P_{i j}$ contains all parents of $x_{j}$ , and excludes all descendent of $x_{j}$ . Then, as ", "page_idx": 21}, {"type": "equation", "text": "$$\nx_{j}=f_{j}(x_{i},C)+\\varepsilon_{j}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we have $x_{i}\\perp\\!\\!\\!\\perp r_{j_{P}}^{i}$ ", "page_idx": 21}, {"type": "text", "text": "We now show the reverse direction. Suppose $x_{i}\\perp\\!\\!\\!\\perp r_{i}^{j}$ . Note by assumption $x_{i},x_{j}$ are not in PP1 relation. Suppose for contradiction that $x_{i},x_{j}$ are in PP3 or PP4 relation. Then, there must exist at least one active path between $x_{i},x_{j}$ that goes through $C$ . If $\\exists$ a backdoor path from $x_{i},x_{j}$ , then at least one vertex in $C$ is a confounder of $x_{i},x_{j}$ , and thus $x_{i}\\not\\perp r_{i}^{j}$ ; therefore, there must exist a frontdoor path between $x_{i},x_{j}$ through $C$ , with at least one mediator $x_{k}$ . Note that $x_{k}$ is dependent on both $x_{i}$ and $x_{j}$ , and therefore is excluded from $P_{i j}$ . The exclusion of $x_{k}$ introduces omitted variable bias [23], and thus $x_{i}$ \u0338\u22a5\u22a5 $r_{i_{P}}^{j}$ , leading to contradiction. Thus, $x_{i},x_{j}$ are in PP2 relation. ", "page_idx": 21}, {"type": "text", "text": "B.5 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 4.4 (Roots). All non-isolated root vertices are contained in $W$ . In addition, $x_{i}\\in W$ is $a$ root vertex if and only if 1) $x_{i}$ is not a known descendant of any $x_{j}\\in W$ , and 2) for each $x_{j}\\in W$ , either a) $x_{i}$ \u22a5\u22a5 $x_{j},\\,b):$ $x_{j}$ is in $P P2$ relation to $x_{i}$ , i.e., $x_{j}\\in C h(x_{i})$ , or c) there exists a child of $x_{i}$ , denoted by $x_{k}$ , that cannot be $d\\sb{\\l}$ -separated from $x_{j}$ given $x_{i}$ , i.e., $x_{j}$ \u0338\u22a5\u22a5 $x_{k}|x_{i}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. We first show that all non-isolated root vertices are contained in $W$ . Note, if a root $x_{i}$ is not isolated, it has at least one child $x_{j}$ . By definition, $x_{i}$ has no parents, so there cannot exist a backdoor path between $x_{i},x_{j}$ . Consider $\\bar{x_{j}}\\in\\mathsf{C h}(x_{i})$ that is in a hierarchical layer closest to $x_{j}$ . Note that there cannot exist an active path from $x_{i}$ to $\\mathrm{Pa}(x_{j})\\setminus x_{i}$ , because that would imply that $x_{j}$ is not the child of $x_{i}$ that is in the closest hierarchical layer. Therefore, $x_{i},x_{j}$ must be in PP2 relation, and $x_{i}\\in W$ . ", "page_idx": 21}, {"type": "text", "text": "We now show the forward direction. Suppose $x_{i}$ is a root vertex. Then, $x_{i}$ has no parents, so it cannot be the descendent of any vertex, let alone any vertex in $W$ : Condition 1) is satisfied. For each $x_{j}\\in W$ , there either exists an active path between $x_{i},x_{j}$ , or there does not. If there does not exist an active path, then $x_{i}\\perp\\!\\!\\!\\perp x_{j}$ , satisfying Condition a). If there does exist an active path, then as $x_{i}$ is a root, $x_{j}\\in\\mathsf{D e}(x_{i})$ : the active path must be a frontdoor path. Consider the vertex $x_{k}\\in\\operatorname{Ch}(x_{j})$ that is in a hierarchical layer closest to $x_{j}$ , AND is along the active frontdoor path from $x_{i}$ to $x_{j}$ . Note that $x_{i}$ is in PP2 relation with $x_{k}$ . If $x_{k}=x_{j}$ , then Condition b) is satisfied. If $x_{k}\\neq x_{j}$ , then we note that there is an active frontdoor path between $x_{k}$ and $x_{j}$ . Therefore, $x_{k},x_{j}$ are dependent even after conditioning on the mutual ancestor $x_{i}$ , i.e., $x_{j}$ \u0338\u22a5\u22a5 $x_{k}|x_{i}$ : Condition c) is satisfied. Therefore, Condition 2) is always satisfied. ", "page_idx": 21}, {"type": "text", "text": "We now show the reverse direction. Suppose Condition 1) and Condition 2) are satisfied for $x_{j}\\in W$ . Suppose for contradiction that $x_{j}$ is not a root. Note that $\\exists x_{i}\\,\\in\\,W$ such that $x_{i}$ is a root and $x_{j}\\,\\in\\,{\\mathrm{De}}(x_{i})$ . Consider any vertex $x_{k}\\in\\mathsf{C h}(x_{j})$ such that $x_{j}$ is in PP2 relation to $x_{k}$ . Note that all active paths between $x_{i}$ and $x_{k}$ must be frontdoor paths, and $x_{j}$ must lie along all these active frontdoor paths: otherwise, this would contradict that $x_{j}$ and $x_{k}$ are in PP2 relation. Therefore, $x_{i}$ \u22a5\u22a5 $x_{k}|x_{j}$ . This contradicts Condition 2c), therefore Condition 1) and Condition 2) cannot hold true for $x_{i}\\in W$ that is not a root. ", "page_idx": 21}, {"type": "text", "text": "B.6 Proof of Correctness for Nonlinear Hierarchical Topological Sort ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem 4.5. Given a graph $G$ , Algorithm 2 asymptotically finds a correct hierarchical sort of $G$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. This proof can be broken into two parts: we first show 1) all root vertices are identified after Stage 3, then we show 2) that Stage 4 recovers a correct hierarchical topological sort given the root vertices. ", "page_idx": 22}, {"type": "text", "text": "Note that Stage 1 identifies $x_{i},x_{j}$ as not in PP1 relation if and only if $x_{i}$ \u0338\u22a5\u22a5 $x_{j}$ : it follows by Lemma 4.2 that these identifications are correct. ", "page_idx": 22}, {"type": "text", "text": "Note that Stage 2 identifies the set $x_{i}\\,\\in\\,W$ if and only if $\\exists x_{j}$ such that either 1) $x_{i}$ \u22a5\u22a5 $\\boldsymbol{r}_{i}^{j}$ or 2) $x_{i}\\perp\\!\\!\\perp r_{i,P}^{j}$ : it follows by Lemma 4.3 that these identifications are correct. ", "page_idx": 22}, {"type": "text", "text": "Note that Stage 3 explicitly uses a condition from Lemma 4.4 to find all non-isolated root vertices - it is therefore correct. Note that all isolated root vertices were identified in Stage 1. Therefore, all roots are identified. ", "page_idx": 22}, {"type": "text", "text": "Consider a hierarchical topological sort of DAG $G\\,\\pi_{H}$ : suppose the maximal directed path in $G$ has size $h$ . Note that layer 1 of $\\pi_{H}$ consists of root nodes, and therefore has been identified. We note the following observation: when $x_{i}$ is nonparametrically regressed on all $x_{k}\\in\\pi_{H}$ to obtain residual $r_{i}$ , if $\\exists$ an ancestor of $x_{i}$ not in $\\pi_{H}$ , then $r_{i}$ will be dependent on at least one $x_{i}\\in\\pi_{H}$ due to omitted variable bias [23]. ", "page_idx": 22}, {"type": "text", "text": "We now induct on the layers of $\\pi_{H}$ to show that Stage 4 correctly recovers all layers. ", "page_idx": 22}, {"type": "text", "text": "Base Iterations (1,2) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. All roots are identified, so layer 1 of $\\pi_{H}$ is identified. ", "page_idx": 22}, {"type": "text", "text": "2. By observation, only $x_{i}$ in layer 2 of $\\pi_{H}$ will have independent residuals $r_{i}$ after nonparametric regression on $x_{i}\\in\\pi_{H}$ , so layer 2 is correctly identified. ", "page_idx": 22}, {"type": "text", "text": "Iteration $k-1$ , Inductive Assumption We have recovered all layers of $\\pi_{H}$ up to layer $k-1$ . ", "page_idx": 22}, {"type": "text", "text": ". By observation, only $x_{i}$ in layer $k$ of $\\pi_{H}$ will have independent residuals $r_{i}$ after nonparametric regression on $x_{i}\\in\\pi_{H}$ , so layer $k$ is correctly identified. ", "page_idx": 22}, {"type": "text", "text": "Iteration $k-1$ Inductive Assumption is satisfied for iteration $k$ , therefore we recover all layers of $\\pi_{H}$ from 1 to $k$ . Thus, for a DAG with an arbitrary number of layers, Stage 4 recovers the full hierarchical topological sort. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "B.7 Time Complexity for NHTS ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Theorem 4.6. Given n samples of $d$ vertices generated by a identifiable nonlinear ANM, the worst case runtime complexity of Algorithm 2 is upper bounded by $O(d^{3}n^{3})$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. In Stage 1, NHTS runs $O(d^{2})$ marginal independence tests that each have $O(n^{3})$ complexity. In Stage 2, NHTS runs $O(d^{2})$ nonparametric regressions and $O(d^{2})$ marginal independence tests, each of which have $O(n^{3})$ complexity. In Stage 3 NHTS runs at most $O(\\check{d}^{\\check{2}})$ conditional independence tests, each of which has $\\bar{O(n^{3})}$ complexity. In the worst case of a fully connected DAG, NHTS goes through $O(d)$ steps in Stage 4: in each step of Stage 4, NHTS runs $O(d)$ nonparametric regressions and $O(d^{2})$ marginal independence tests, each of which has $O(n^{3})$ complexity. Therefore, the overall sample complexity of NHTS is $O(d^{3}n^{3})$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "B.8 Walk-through of NHTS ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The following diagram illustrates each stage of NHTS on an exemplary 5-node DAG: ", "page_idx": 22}, {"type": "text", "text": "In Stage 1, we discover that none of the vertices are in PP1 relations. In Stage 2, we discover that vertex A is in PP2 relation to vertices B and C, and vertex D is in PP2 relation to vertex E: therefore, A and D are our potential roots. In Stage 3, we find that vertex D d-separates its child, vertex E, from vertex A: therefore A is the root vertex. In Stage 4, we regress the unsorted vertices onto A, finding that B,C are independent of the residual in the first round, then D, then E in the last round. ", "page_idx": 22}, {"type": "image", "img_path": "xnmm1jThkv/tmp/bc3d3b3c1b269a43dcde7af53dacbcc52bdda26a31b6ef99c9d4f08a3360eb76.jpg", "img_caption": ["B.9 Time Complexity of NHTS vs RESIT, NoGAM "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Theorem 4.7. Consider a fully connected DAG $G=(V,E)$ with nonlinear ANM. Let $d:=|V|$ . Let $n_{k}^{\\mathrm{NHTS}}$ be the number of nonparametric regressions with covariate set size $k\\in[d-2]$ run by NHTS when sorting $V$ ; we similarly define $n_{k}^{\\mathrm{RESIT}}$ Tand nkN $n_{k}^{\\mathrm{NoGAM}}$ respectively. Then, nNHTS= d \u2212k, and $n_{k}^{\\mathrm{RESIT}}=n_{k}^{\\mathrm{NoGAM}}=k+1$ nkNoGAM= k + 1. This implies that for all k > d2, nkN $n_{k}^{\\mathrm{NHTS}}<n_{k}^{\\mathrm{RESIT}}=n_{k}^{\\mathrm{NoGAM}}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. RESIT and NoGAM both identify leaf vertices in an iterative fashion, regressing each unsorted vertex on the rest of the unsorted vertices; RESIT tests the residual for independence with the covariate set while NoGAM uses the residual for score matching. Therefore, the number of regressions run in both methods in each step equals one plus the covariate set size. Therefore, when the covariate set size is $k>\\frac{d}{2}$ , there are $k+1$ regressions run. ", "page_idx": 23}, {"type": "text", "text": "In the case of a fully directed graph, the first stage of NHTS only runs pairwise regressions with empty conditioning sets. After the first stage, NHTS regresses each unsorted vertex onto all sorted vertices, finding vertices with independent residuals. Therefore, the number of regressions run is equal to $d$ minus the size of the covariate set. Therefore, when the covariate set size is $\\textstyle{\\bar{k}}>{\\frac{d}{2}}$ , there are $d-k$ regressions run. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C Proofs and Walkthrough for ED ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 Proof of Lemma ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 5.1 (Parent Discovery). Let \u03c0 be a topological ordering, $x_{i},x_{j}$ such that $\\pi(x_{i})<\\pi(x_{j})$ . Let $Z_{i j}=C_{i j}\\cup M_{i j}$ , where $C_{i j}=\\{x_{k}:x_{k}\\in P a(x_{i}),x_{k}\\not\\perp x_{j}\\},M_{i j}\\stackrel{}{=}\\{x_{k}:x_{k}\\in P a(x_{j}),\\pi(x_{i})<\\pi(x_{j})\\}.$ $\\pi(x_{k})<{\\bar{\\pi}}(x_{j})\\}$ . Then, $x_{i}\\to x_{j}\\iff x_{i}$ \u0338\u22a5\u22a5 $x_{j}|Z_{i j}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Note, $C_{i,j}$ blocks all backdoor paths between $x_{i},x_{j}$ and $M_{i,j}$ blocks all frontdoor paths between $x_{i},x_{j}$ , except the direct edge. ", "page_idx": 24}, {"type": "text", "text": "We first show the forward direction. If xi \u0338\u22a5\u22a5xj|Zi,j, there must be an direct edge between xi, xj, and as k(i) < k(j), we have xi \u2192xj. ", "page_idx": 24}, {"type": "text", "text": "We now show the reverse direction. If $x_{i}\\to x_{j}$ , then there does not exist a conditioning set that makes $x_{i}$ \u22a5\u22a5 $x_{j}$ , which implies $x_{i}$ \u0338\u22a5\u22a5 $x_{j}|Z_{i,j}$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C.2 Hierarchical Version of Edge Discovery ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The current implementation of Edge Discovery takes a linear hierarchical topological sort as input; this is equivalent to a hierarchical topological sort where every layer contains only one vertex. To generalize ED to a hierarchical sort, we simply adjust how the algorithm loops over the sort, and which vertices are included in which conditioning sets $C_{i j},M_{i j}$ . We give an example of how the latter would change: suppose the algorithm is checking whether $x_{i}\\in\\operatorname{Pa}(x_{j})$ where $x_{i}$ is in layer 2 $(\\pi_{L}(x_{i})=2)$ and $x_{j}$ is layer 6 $(\\pi_{L}(x_{j})=6)$ . $M_{i j}$ would be equal to the vertices who are parents of $x_{j}$ that are in layers that are between layer 2 and 6: i.e. $M_{i j}$ equals $x_{k}\\,\\in\\,\\operatorname{Pa}(x_{j})$ such that $2\\,<\\,^{\\prime}\\pi_{L}(x_{k})\\,<\\,6$ . $C_{i j}$ , would stay the same, just being equal to $\\mathrm{Pa}(x_{i})$ . Now, to generalize the looping part, notice that the linear version of ED loops over indices of the linear topological sort. The hierarchical version of Edge Discovery would loop over and within layers of the topological sort. We give an example of this: suppose the algorithm has found all parents of vertices in layer 4 or lower: the next step would be to find the parents of vertices in layer 5. The algorithm sets any vertex in layer 5 as $x_{j}$ , and first finds all parents of $x_{j}$ in the immediately preceding layer, layer 4. The algorithm then finds all parents of $x_{j}$ in the layer preceding layer 4, layer 3. This is essentially the same approach as the linear version, except the size of the conditioning sets and number of conditional tests run is far lower, as the layers provide extra knowledge, limiting which variables can be parents or confounders of other variables. ", "page_idx": 24}, {"type": "text", "text": "C.3 Edge Discovery ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "xnmm1jThkv/tmp/eac636a8efda0faf008fe606678583e0c22016542ec7e56becf2d4400a7a0799.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "We first provide a walk-through of ED on an exemplary 5-node DAG: ", "page_idx": 24}, {"type": "text", "text": "We first use a topological ordering algorithm to obtain a sort from the DAG: the sort corresponds to a fully connected DAG, from which we will prune spurious edges. In the first two iterations we find that $A\\rightarrow B$ , but $B\\neq C$ , as $B\\perp\\!\\!\\!\\perp C|A$ . In the third iteration we find that $B\\rightarrow D,C\\rightarrow D$ , but $A\\ne D$ , as $A$ \u22a5\u22a5 $D|B,C$ . In the final iteration, we find that $D\\rightarrow E$ , but $A\\nrightarrow E,B\\nrightarrow E,C\\nrightarrow E$ , given $A\\perp E|D,B\\perp E|D,C\\perp E|D$ . Therefore, we recover the correct set causal edges, removing all spurious edges. We now present a statement of correctness and proof for ED: ", "page_idx": 25}, {"type": "text", "text": "Theorem 5.2. Given a correct linear topological ordering of $G$ , Algorithm 3 asymptotically finds correct parent sets $P A(x_{i}),\\forall x_{i}\\in G$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Correctness ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Background We are given a linear topological sort $\\pi(V)=[x_{1},x_{2},.\\ldots,x_{d}]$ (where $x_{1}$ has no parents). For $x_{a}$ that appears before $x_{b}$ , let $Z_{a,b}=C_{a,b}\\cup M_{a,b}$ , where $C_{a,b}$ is the set of potential confounders of $x_{a},x_{b}$ , and $M_{a,b}$ is the set of known mediators of $x_{a},x_{b}$ (defined precisely in Lemma 5.1). Let $x$ \u22a5\u22a5 $y|$ \u00b7 be the value of a conditional independence test between $x$ and $y$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Base Case Iterations (1,2,3,4) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Finding parents of $x_{1}$ : by the topological sort, $x_{1}$ has no parents. ", "page_idx": 25}, {"type": "text", "text": "2. Finding parents of $x_{2}$ : As $x_{1}$ is the only possible parent of $x_{2}$ , there are no possible confounders or mediators between $x_{1}$ and $x_{2}$ , so we initialize $Z_{1,2}~=~\\emptyset$ , then by $L3$ $x_{1}\\not\\perp x_{2}|Z_{1,2}=x_{1}$ \u0338\u22a5\u22a5 $x_{2}\\iff x_{1}\\to x_{2}$ . Thus, we recover all possible edges between $[x_{1},x_{2}]$ . ", "page_idx": 25}, {"type": "text", "text": "3. Finding parents of $x_{3}$ : first we check whether $x_{2}\\to x_{3}$ , then we check whether $x_{1}\\to x_{3}$ . From iteration 2, we know all edges between $[x_{1},x_{2}]$ . ", "page_idx": 25}, {"type": "text", "text": "Case 1: if $x_{1}\\to x_{2}$ , then it is a possible confounder. Therefore, we initialize $C_{2,3}=x_{1}$ . By topological sort there is no mediator between $x_{2}$ and $x_{3}$ , therefore we initialize $M_{2,3}=\\varnothing$ Then by Lemma $5.1\\;x_{2}$ \u0338\u22a5\u22a5 $x_{3}|Z_{2,3}=x_{2}\\ {\\underline{{\\mathcal{H}}}}\\ x_{3}|x_{1}\\iff x_{2}\\to x_{3}$ . If an edge between $x_{2}$ and $x_{3}$ exists, initialize $M_{1,3}$ accordingly. Note, $x_{1}$ has no parents, so we initialize $C_{1,3}=\\varnothing$ . Then, by Lemma $5.1\\;x_{1}$ \u0338\u22a5\u22a5 $x_{3}|Z_{1,3}\\iff x_{1}\\to x_{3}$ . We recover all possible edges between $[x_{1},x_{2},x_{3}]$ . ", "page_idx": 25}, {"type": "text", "text": "Case 2: if $x_{1}\\neq x_{2}$ , there are no possible confounders between $x_{2}$ and $x_{3}$ . Therefore, we initialize $C_{2,3}=\\varnothing$ . By topological sort there is no mediator between $x_{2}$ and $x_{3}$ , therefore we initialize $M_{2,3}=\\emptyset$ . Then by Lemma $5.1\\;x_{2}$ \u0338\u22a5\u22a5 $x_{3}|Z_{2,3}=x_{2}$ \u0338\u22a5\u22a5 $x_{3}\\iff x_{2}\\to x_{3}$ Note, $x_{1}$ has no parents, so we initialize $C_{1,3}\\;=\\;\\emptyset$ . Note, as $x_{1}\\ \\neq\\ x_{2}$ , there are no possible mediators between $x_{1}$ and $x_{3}$ : we initialize $M_{1,3}\\,=\\,\\emptyset$ . Then, by Lemma 5.1 $x_{1}$ \u0338\u22a5\u22a5 $x_{3}|Z_{1,3}\\,=\\,x_{1}$ \u0338\u22a5\u22a5 $x_{3}\\leftrightarrow$ $x_{1}\\rightarrow\\,x_{3}$ . We recover all possible edges between $[x_{1},x_{2},x_{3}]$ . ", "page_idx": 25}, {"type": "text", "text": "4. Finding parents of $x_{4}$ : first we check whether $x_{3}\\rightarrow\\,x_{4}$ , then whether $x_{2}\\rightarrow\\,x_{4}$ , then whether $x_{1}\\to x_{4}$ . From iteration 3, we know all edges between $[x_{1},x_{2},x_{3}]$ . ", "page_idx": 25}, {"type": "text", "text": "Case 1: if $[x_{1},x_{2},x_{3}]$ has no edges, then no node causes $x_{3}$ directly or indirectly, therefore we initialize $C_{3,4}=\\varnothing$ . There are no possible mediators between $x_{3}$ and $x_{4}$ , so $M_{3,4}=\\emptyset$ Therefore, by Lemma $5.1\\ x_{3}$ \u0338\u22a5\u22a5 $x_{4}|Z_{3,4}\\,=\\,x_{3}\\,\\;\\not\\Pi\\,\\;x_{4}\\,\\Longleftrightarrow\\,\\;x_{3}\\,\\rightarrow\\,x_{4}$ . As $[x_{1},x_{2},x_{3}]$ has no edges, no node causes $x_{2}$ directly or indirectly, therefore we initialize $C_{2,4}=\\bar{\\varnothing}$ . Note, $x_{2}\\neq x_{3}$ , so there are no possible mediators between $x_{2}$ and $x_{4}$ , so we initialize $M_{2,4}\\,=\\,\\emptyset$ . Then, by Lemma $5.1~x_{2}$ \u0338 $\\mathbb{L}\\ x_{4}|Z_{2,4}=x_{2}\\ \\mathbb{\\mathcal{L}}\\ x_{4}\\iff x_{2}\\to x_{4}$ . Note, $x_{1}$ has no parents, so we initialize $C_{1,4}\\,=\\,\\emptyset$ . As, $x_{1}$ does not cause $x_{2}$ or $x_{3}$ there are no possible mediators between $x_{1}$ and $x_{4}$ , therefore we initialize $M_{1,4}=\\emptyset$ . Then, by Lemma $5.1\\ x_{1}$ \u0338\u22a5\u22a5 $x_{4}|Z_{1,4}=x_{1}$ \u0338\u22a5\u22a5 $x_{4}\\iff x_{1}\\to x_{4}$ . We recover all possible edges between $[x_{1},x_{2},x_{3},x_{4}]$ . ", "page_idx": 25}, {"type": "text", "text": "Case 2: if $x_{1}\\to x_{2}$ is the only edge between the nodes $[x_{1},x_{2},x_{3}]$ , then no node causes $x_{3}$ directly or indirectly, therefore $C_{3,4}\\,=\\,\\emptyset$ . By topological sort there is no mediator between $x_{3},x_{4}$ , so we initialize $M_{3,4}=\\emptyset$ . Then, by Lemma $5.1\\ x_{3}$ \u0338\u22a5\u22a5 $x_{4}|Z_{3,4}=x_{3}\\not\\mid$ $\\bot\\ x_{4}\\iff x_{3}\\to x_{4}$ . As $x_{1}\\to x_{2}$ , we initialize $C_{2,4}=x_{1}$ . As $x_{2}\\neq x_{3}$ , there are no possible mediators between $x_{2}$ and $x_{4}$ , so we initialize $M_{2,4}=\\emptyset$ . Then, by Lemma 5.1 $x_{2}\\not\\perp x_{4}|Z_{2,4}=x_{2}$ \u0338\u22a5\u22a5 $x_{4}|x_{1}\\iff x_{2}\\to x_{4}$ . If an edge exists between $x_{2}$ and $x_{4}$ , we initialize $M_{1,4}$ accordingly. As $x_{1}$ has no parents, we initialize $C_{1,4}=\\varnothing$ . Then, by Lemma $5.1\\;x_{1}$ \u0338\u22a5\u22a5 $x_{4}|Z_{1,4}\\iff x_{1}\\to x_{4}$ . We recover all possible edges between $[x_{1},x_{2},x_{3},x_{4}]$ . ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Case 3: if $x_{1}\\rightarrow x_{3}$ is the only edge between the nodes $[x_{1},x_{2},x_{3}]$ , then $x_{1}$ is the only potential confounder of $x_{3}$ and $x_{4}$ so we initialize $C_{3,4}\\,=\\,x_{1}$ . There are no possible mediators between $x_{3}$ and $x_{4}$ , so we initialize $M_{3,4}\\,=\\,\\emptyset$ . Then, by Lemma $5.1\\ x_{3}\\ \\mathscr{L}$ $\\textnormal{\\downarrow}x_{4}|Z_{3,4}\\,=\\,x_{3}$ \u0338\u22a5\u22a5 $x_{4}|x_{1}\\iff x_{3}\\,\\to\\,x_{4}$ . Note, $x_{2}$ has no parents, so we initialize $C_{2,4}\\,=\\,\\emptyset$ . Note, $x_{2}\\ \\neq\\ x_{3}$ , so we initialize $M_{2,4}\\,=\\,\\emptyset$ . Then, by Lemma $5.1\\ x_{2}$ \u0338\u22a5\u22a5 $x_{4}|Z_{2,4}=x_{2}\\ \\underline{{{\\mathcal{M}}}}\\ x_{4}\\iff x_{2}\\to x_{4}$ . If an edge exists between $x_{3}$ and $x_{4}$ , we initialize $M_{1,4}$ accordingly. As $x_{1}$ has no parents, we initialize $C_{1,4}\\,=\\,\\emptyset$ . Then, by Lemma 5.1 $x_{1}$ \u0338\u22a5\u22a5 $x_{4}|Z_{1,4}\\iff x_{1}\\to x_{4}$ . We recover all possible edges between $[x_{1},x_{2},x_{3},x_{4}]$ . ", "page_idx": 26}, {"type": "text", "text": "Case 4: if $x_{1}\\rightarrow x_{2},x_{2}\\rightarrow x_{3}$ are the only edges between nodes $[x_{1},x_{2},x_{3}]$ , then $x_{1}$ and $x_{2}$ cause $x_{3}$ either indirectly or directly. Therefore, we initialize $C_{3,4}=\\{x_{1},x_{2}\\}$ . There are no possible mediators between $x_{3}$ and $x_{4}$ , so we initialize $M_{3,4}=\\varnothing$ . Then, by Lemma $5.1\\ x_{3}$ \u0338\u22a5\u22a5 $x_{4}|Z_{3,4}=x_{3}$ \u0338\u22a5\u22a5 $x_{4}|x_{1},x_{2}\\iff x_{3}\\to x_{4}$ . If an edge exists between $x_{3}$ and $x_{4}$ , we initialize $M_{2,4}$ accordingly. Note, $x_{1}$ is a parent of $x_{2}$ , so we initialize $C_{2,4}=x_{1}$ Then, by Lemma $5.1\\;x_{2}$ \u0338\u22a5\u22a5 $x_{4}|Z_{2,4}\\iff x_{2}\\to x_{4}$ . If an edge exists between $x_{3}$ and $x_{4}$ , and/or between $x_{2}$ and $x_{4}$ , initialize $M_{1,4}$ accordingly. Note, $x_{1}$ has no parents, so initialize $C_{1,4}=\\varnothing$ . Then by Lemma $5.1\\ x_{1}$ \u0338\u22a5\u22a5 $x_{4}|Z_{1,4}\\iff x_{1}\\to x_{4}$ . We recover all possible edges between $[x_{1},x_{2},x_{3},x_{4}]$ . ", "page_idx": 26}, {"type": "text", "text": "Case 5: if $x_{1}\\rightarrow x_{3},x_{2}\\rightarrow x_{3}\\mathrm{are}$ the only edges between nodes $[x_{1},x_{2},x_{3}]$ , then $x_{1}$ and $x_{2}$ cause $x_{3}$ directly. Therefore, we initialize $C_{3,4}\\,=\\,\\{x_{1},x_{2}\\}$ . There are no possible mediators between $x_{3}$ and $x_{4}$ , so we initialize $M_{3,4}=\\emptyset$ . Then, by $L3\\;x_{3}$ \u0338\u22a5\u22a5 $x_{4}|Z_{3,4}=$ $x_{3}$ \u0338\u22a5\u22a5 $x_{4}|x_{1},x_{2}\\quad\\Longleftrightarrow\\quad x_{3}\\;\\to\\;x_{4}$ . If an edge exists between $x_{3}$ and $x_{4}$ , we initialize $M_{2,4}$ accordingly. Note, $x_{2}$ has no parents, so we initialize $C_{2,4}=\\varnothing$ . Then,by $L3\\;x_{2}$ \u0338\u22a5\u22a5 $x_{4}|Z_{2,4}\\iff x_{2}\\to x_{4}$ . If an edge exists between $x_{3}$ and $x_{4}$ , initialize $M_{1,4}$ accordingly. Note, $x_{1}$ has no parents, so initialize $C_{1,4}=\\varnothing$ . Then by $L3\\;x_{1}$ \u0338\u22a5\u22a5 $x_{4}|Z_{1,4}\\iff x_{1}\\to x_{4}$ We recover all possible edges between $[x_{1},x_{2},x_{3},x_{4}]$ . ", "page_idx": 26}, {"type": "text", "text": "Case 6: if $x_{1}\\,\\to\\,x_{2},x_{1}\\,\\to\\,x_{3},x_{2}\\,\\to\\,x_{3}$ are the only edges between nodes $[x_{1},x_{2},x_{3}]$ , then $x_{1}$ and $x_{2}$ cause $x_{3}$ directly. Therefore, we initialize $\\boldsymbol{C}_{3,4}=\\{x_{1},x_{2}\\}$ . There are no possible mediators between $x_{3}$ and $x_{4}$ , so we initialize $M_{3,4}=\\emptyset$ . Then, by Lemma 5.1 $x_{3}$ \u0338\u22a5\u22a5 $x_{4}|Z_{3,4}\\,=\\,x_{3}$ \u0338\u22a5\u22a5 $x_{4}|x_{1},x_{2}\\;\\;\\Longleftrightarrow\\;\\;x_{3}\\;\\rightarrow\\;x_{4}$ . If an edge exists between $x_{3}$ and $x_{4}$ , we initialize $M_{2,4}$ accordingly. Note, $x_{1}$ is a parent of $x_{2}$ , so we initialize $C_{2,4}=x_{1}$ . Then, by Lemma $5.1\\;x_{2}$ \u0338\u22a5\u22a5 $x_{4}|Z_{2,4}\\iff x_{2}\\to x_{4}$ . If an edge exists between $x_{3}$ and $x_{4}$ , and/or between $x_{2}$ and $x_{4}$ , initialize $M_{1,4}$ accordingly. Note, $x_{1}$ has no parents, so initialize $C_{1,4}=\\varnothing$ . Then by Lemma $5.1\\ x_{1}$ \u0338\u22a5\u22a5 $x_{4}|Z_{1,4}\\iff x_{1}\\to x_{4}$ . We recover all possible edges between $[x_{1},x_{2},x_{3},x_{4}]$ . ", "page_idx": 26}, {"type": "text", "text": "5. Finding parents of $x_{5}$ : first we check whether $x_{4}\\to x_{5}$ , then whether $x_{3}\\rightarrow\\,x_{5}$ , then whether $x_{2}\\to x_{5}$ , then whether $x_{1}\\rightarrow x_{5}$ . ", "page_idx": 26}, {"type": "text", "text": "Iteration $k-1$ Inductive Assumption We have recovered the edges between $[x_{1},\\ldots,x_{k-1}]$ . ", "page_idx": 26}, {"type": "text", "text": "Iteration $k$ We now find all nodes in $[x_{1},\\ldots,x_{k-1}]$ that cause $x_{k}$ (which yields all edges between $\\left[x_{1},\\ldots,x_{k}\\right])$ . ", "page_idx": 26}, {"type": "text", "text": "Base Case Sub-Iteration (1,2) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. We first check whether $x_{k-1}\\ \\to\\ x_{k}$ . We initialize the potential confounders $C_{k-1,k}$ ${\\mathrm{using}}[x_{1},\\ldots,x_{k-1}]$ . The set of mediators $M_{k-1,k}$ is empty by the topological sort. Then, by Lemma 5.1 $x_{k-1}\\to x_{k}\\iff x_{k-1}$ \u0338\u22a5\u22a5 $x_{k}|Z_{k-1,k}$ . ", "page_idx": 26}, {"type": "text", "text": "2. We now check whether $x_{k-2}\\ \\to\\ x_{k}$ . We initialize the potential confounders $C_{k-1,k}$ using $[x_{1},\\ldots,x_{k-2}]$ . Only $x_{k-1}$ can be a mediator: we know whether $x_{k-2}$ causes $x_{k-1}$ , and in our previous step we found whether $x_{k-1}$ causes $x_{k}$ . Thus, we initialize $M_{k-2,k}$ accordingly. Thus, by Lemma $5.1\\ x_{k-2}\\rightarrow x_{k}\\iff x_{k-2}$ \u0338\u22a5\u22a5 $x_{k}\\vert Z_{k-2,k}$ . . ", "page_idx": 27}, {"type": "text", "text": "Sub-Iteration $j$ Inductive Assumption We have recovered edges between $[x_{j},\\dots,x_{k}]$ ", "page_idx": 27}, {"type": "text", "text": "Sub-Iteration $j-1$ We now find if $x_{j-1}$ causes $x_{k}$ . ", "page_idx": 27}, {"type": "text", "text": ". For node $x_{j-1}$ where $1\\,\\leq\\,j\\,-\\,1\\,<\\,k$ , we obtain $C_{j-1,k}$ by Iteration $k\\mathrm{~-~}1$ Inductive Assumption and $M_{j-1,k}$ by Sub-Iteration $j$ Inductive Assumption. Then, by Lemma 5.1 $x_{j-1}\\to x_{k}\\iff x_{j-1}\\ {\\underline{{\\mathbb{1}}}}\\ x_{k}|Z_{j-1,k}$ . ", "page_idx": 27}, {"type": "text", "text": "Sub-Iteration $j$ Inductive Assumption is satisfied for $j\\mathrm{~-~}1$ , therefore we recover all nodes in $[x_{1},\\ldots,x_{k-1}]$ that cause $x_{k}$ . This satisfies Iteration $k-1$ Inductive Assumption for $k$ , which means we recover all edges between $[x_{1},\\ldots,x_{k}]$ . Thus, for a topological sort of arbitrary length, the algorithm recovers all possible edges. ", "page_idx": 27}, {"type": "text", "text": "C.4 Time Complexity for Edge Discovery ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 5.3. Given n samples of $d$ vertices generated by a model corresponding to a DAG G, the runtime complexity of ED is upper bounded by $O(d^{2}n^{3})$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. ED checks for the existence of every edge permitted by a topological sort $\\pi$ by running one conditional independence test that has complexity $\\dot{O}(n^{3})$ . In the worst case, there are $\\dot{O}(d^{2})$ possible edges, so the overall complexity is $O(d^{2}n^{\\bar{3}})$ . \u53e3 ", "page_idx": 27}, {"type": "text", "text": "D Additional Experiments and Runtimes ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "xnmm1jThkv/tmp/6c44624cc810cf4cae36c45c92cded8caa5d7a14b79380df6d075caa1127308a.jpg", "table_caption": ["D.1 Runtimes for Linear Topological Sort ", "Figure 10: Runtimes for linear topological sorts, left: top row; right: bottom row, see Figure 5. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.2 Runtimes for Nonlinear Topological Sort ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "xnmm1jThkv/tmp/344dd7cf7178131404ce364e9cc7cd71f31aca576724cd2021249eef7684431c.jpg", "table_caption": ["Figure 11: Runtimes for nonlinear topological sorts, see Figure 6. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.3 Topological Sorts on Nonlinear Data ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "xnmm1jThkv/tmp/b614fc5bbc629fbb11201bfbf1e13a2ecc71f3000cfea0ffd3b39b251a87c199.jpg", "img_caption": ["Figure 12: Performance of NHTS on data generated with Gaussian, Laplace, or Uniform noise (left, middle, right columns), the average number of edges set to $2d,3d,$ , or $4d$ (top, middle, bottom rows). "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "In figure 12 we provide additional experiments for nonlinear topological sorts $[d=10$ , $n=300]$ ); we see that NHTS maintains superior performance even as the density of the underlying graph increases, although the performance gap decreases, especially for laplacian noise, as the graph becomes denser. ", "page_idx": 28}, {"type": "image", "img_path": "xnmm1jThkv/tmp/0624ce7890e981580cb101bcf01d8bf894f4fe1eedc22185a0b84dc748ada50d.jpg", "img_caption": ["Figure 13: Performance of ED on data generated with Gaussian, Laplace, or Uniform noise (left, middle, right columns), the average number of edges set to $2d,3d,$ , or $4d$ (top, middle, bottom rows). "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "In figure 13 we provide additional experiments for edge pruning methods $d=20$ , $n=300]$ ); we see that ED generally maintains superior performance as the noise distribution is varied and density is increased, although the performance gap decreases for all noise distributions as the graph becomes denser. ", "page_idx": 29}, {"type": "table", "img_path": "xnmm1jThkv/tmp/133ee0981c551d84720a28ee7535a5db3232f1e722562f50b481e8e9eef6f492.jpg", "table_caption": ["D.5 Runtimes for Edge Pruning ", "Figure 14: Runtimes for edge pruning: tables correspond to the graphs in Figure 7, from left to right. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "All tests were done in Python. All runtimes were computed locally on an Apple M2 Pro Chip, 16 Gb of RAM, with no parallelization. ", "page_idx": 30}, {"type": "text", "text": "E.1 Topological Sort for LiNGAM ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "DirectLiNGAM was imported from the lingam [45] package, $R^{2}$ \u2212sort was imported from the CausalDisco [28, 29] package, and LHTS was implemented using the Sklearn [24] package. All assets used have a CC-BY 4.0 license. We follow [38] to generate the data for Figure 5, using linear causal mechanisms with randomly drawn coefficient values, plus independent uniform noise. Data is standardized to remove shortcuts [28]. Cutoff values for independence tests were set to $\\alpha=0.05$ for all methods. ", "page_idx": 30}, {"type": "text", "text": "E.2 Topological Sort for Nonlinear ANM ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "GES and GRaSP were imported from the causal-learn [49] package; GSP was imported from the graphical_model_learning [43] package. DirectLiNGAM was imported from the lingam package [45]. NoGAM was imported from the dodiscover [14] package. $R^{2}$ \u2212sort was imported from the CausalDisco package. NHTS and LoSAM were implemented using the kernel ridge regression (KRR) function from the Sklearn package, used independence tests from either the causal-learn package or the dcor [26] package, and a mutual information estimator from the npeet [44] package. All assets used have a CC-BY 4.0 license. We follow [16] to generate the data used for Figure 6 and Figure 12, using quadratic causal mechanisms with randomly drawn coefficient values, plus independent gaussian, laplace or uniform noise. Features generated with quadratic mechanisms were standardized after being generated to remove shortcuts [28] and to prevent the quadratic mechanisms from driving all values close to 0 (ensuring stability). Note that, to enable a fair comparison between NHTS and other topological ordering methods, we implement a version of NHTS that returns a linear topological sort, rather than a hierarchical topological sort, by adding only one vertex to the sort in each iteration of its sorting procedure. For NHTS, in Stage 2 we used KRR with polynomial kernel, $\\alpha=1$ , degree $=3$ , $\\mathrm{coef0}=1$ , and in Stage 4 we used KRR with RBF kernel, $\\alpha=0.1$ , $\\gamma=0.01$ . Cutoff values for independence tests were set to $\\alpha=0.05$ for all methods, no cross validation was allowed for any method. Otherwise, default settings were used for all baselines. ", "page_idx": 30}, {"type": "text", "text": "E.3 Edge Pruning ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lasso and RESIT were implemented using the sklearn package, hypothesis testing with GAMs was implemented using Bsplines and GLMGam from the statsmodel [33] package. Independence tests used either the causal-learn package or the dcor package. All assets used have a CC-BY 4.0 license. We follow [16] to generate the data used for Figure 7 and Figure 13, using quadratic causal mechanisms with randomly drawn coefficient values, plus independent uniform, gaussian, or laplace noise. Features generated with quadratic mechanisms were standardized after being generated to remove shortcuts [28] and to prevent the quadratic mechanisms from driving all values close to 0 (ensuring stability). Cutoff values for independence tests were set to $\\alpha=0.05$ for all methods. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We address all claims made in the abstract and contributions section throughout the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We clearly outline the assumptions and identifiability conditions needed for our methods to hold, only claiming aysymptotic correctness when appropriate. See Definition 2.2, Appendix A.1 and Appendix B.1. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Each theoretical result includes the necessary assumptions, and links to a correct proof in the Appendix. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Experimental procedures are described in both Section 6 and Appendix E, and code is released on github. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Code for the algorithms and data generation is provided in the supplemental material and github link. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: See Section 6, Appendix E, and the supplemental material or github link. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Error bars for experiments for linear topological sorts and edge pruning methods in the main text are standard deviations. Error bars for nonlinear topological sorts and edge pruning experiments not in the main text are whiskers on a boxplot (1.5 times the IQR). ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See Appendix E. ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All parts of the Code of Ethics were followed. ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper addresses a foundational problem in the field of causal discovery, an issue not fundamentally tied to any specific set of applications. ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] Justification: See Appendix E. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No new assets introduced in the paper. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Not applicable. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Not applicable. ", "page_idx": 32}]