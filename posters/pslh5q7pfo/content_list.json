[{"type": "text", "text": "Active preference learning for ordering items in- and out-of-sample ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Herman Bergstr\u00f6m\u2217 ", "page_idx": 0}, {"type": "text", "text": "Emil Carlsson\u2217\u2020", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chalmers University of Technology and University of Gothenburg hermanb@chalmers.se ", "page_idx": 0}, {"type": "text", "text": "Sleep Cycle AB Chalmers University of Technology and University of Gothenburg ", "page_idx": 0}, {"type": "text", "text": "Devdatt Dubhashi Chalmers University of Technology and University of Gothenburg ", "page_idx": 0}, {"type": "text", "text": "Fredrik D. Johansson Chalmers University of Technology and University of Gothenburg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning an ordering of items based on pairwise comparisons is useful when items are difficult to rate consistently on an absolute scale, for example, when annotators have to make subjective assessments. When exhaustive comparison is infeasible, actively sampling item pairs can reduce the number of annotations necessary for learning an accurate ordering. However, many algorithms ignore shared structure between items, limiting their sample efficiency and precluding generalization to new items. It is also common to disregard how noise in comparisons varies between item pairs, despite it being informative of item similarity. In this work, we study active preference learning for ordering items with contextual attributes, both in- and out-of-sample. We give an upper bound on the expected ordering error of a logistic preference model as a function of which items have been compared. Next, we propose an active learning strategy that samples items to minimize this bound by accounting for aleatoric and epistemic uncertainty in comparisons. We evaluate the resulting algorithm, and a variant aimed at reducing model misspecification, in multiple realistic ordering tasks with comparisons made by human annotators. Our results demonstrate superior sample efficiency and generalization compared to non-contextual ranking approaches and active preference learning baselines. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The success of supervised learning is built on annotating items at great volumes with small error. For subjective assessments, however, assigning a value from an arbitrary rating scale can be difficult and prone to inconsistencies, causing many to favor preference feedback from pairwise comparisons (Yannakakis and Mart\u00ednez, 2015; Christiano et al., 2017; Ouyang et al., 2022; Zhu et al., 2023). Preference feedback is sufficient to learn an ordering of items (F\u00fcrnkranz and H\u00fcllermeier, 2003), but for $n$ items, there are ${\\mathcal{O}}(n^{2})$ possible pairs of items to compare. A common solution is to use crowd-sourcing (Chen et al., 2013; Yang et al., 2021; Larkin et al., 2022), but many tasks require domain expertise, making annotations expensive to collect. This is the case in the field of medical imaging, where annotations require trained radiologists (Phelps et al., 2015; Jang et al., 2022; Lid\u00e9n et al., 2024; T\u00e4rn\u00e5sen and Bergstr\u00f6m, 2023). So, how can we learn the best ordering possible from a limited number of comparisons? ", "page_idx": 0}, {"type": "text", "text": "Classically, this problem is solved by active learning, sampling comparisons based on preference feedback and estimated item scores (Herbrich et al., 2006; Maystre and Grossglauser, 2017; Heckel et al., 2018). However, consider a radiologist who wants to quantify the expression of a disease in a collection of $\\Chi$ -ray images. Purely preference-based algorithms utilize only the outcomes of comparisons but ignore the contents of the X-rays, which can reveal similarities between items and inform an ordering strategy. Moreover, the set we want to order is often larger than the set of items observed during training\u2014we may want to rank new X-rays in relation to previous ones. This cannot be solved by learning per-item scores alone. As an alternative, active learning for classification can be used to fit a map from pairs of item contexts $x_{i},x_{j}$ (e.g., the contents of images) to the comparison $i\\ \\succ\\!?\\ \\ j$ , that can be applied to old and new items alike (Houlsby et al., 2011; Qian et al., 2015). However, as we show in Section 4, learning this map to recover a complete ordering is distinct from the tasks preference learning is commonly used for, and existing algorithms lack theoretical justification for this application. Moreover, formal results for related problems, such as contextual bandits or reinforcement learning (Das et al., 2024; Filippi et al., 2010; Zhu et al., 2023; Bengs et al., 2022), do not translate directly to effective active sampling criteria for ordering. There is a small body of work on learning a contextual model to recover the complete ordering (Jamieson and Nowak, 2011; Ailon, 2011) but these either assume noiseless preference feedback or that the noise is unrelated to the similarity of items, which is unrealistic for subjective assessments. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We propose using a contextual logistic preference model to support efficient insample ordering and generalization to new items. Our analysis yields the first bound on the expected ordering error achievable given a collected set of comparisons (Section 4). This result justifies an active sampling principle that accounts for both epistemic and aleatoric uncertainty which we implement in a greedy deterministic algorithm called GURO (Section 5). We further propose a hybrid variant of the contextual preference model, compatible with GURO as well as existing sampling strategies, that overcomes model misspecification by adding per-item parameters (Section 5.1). We evaluate GURO and baseline algorithms in four diverse ordering tasks, three of which utilize comparisons performed by human annotators (Section 6). Our sampling strategy compares favorably to active preference learning baselines, and our hybrid model benefits both GURO and other sampling criteria, achieving the low variance of contextual models and the low bias of fitting per-item parameters. This results in faster convergence in-sample, better generalization to new items, and efficient continual learning when new items are added. ", "page_idx": 1}, {"type": "text", "text": "2 Ordering items with active preference learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our goal is to learn an ordering of items $\\mathcal{T}$ according to an unobserved score $y_{i}~\\in~\\mathbb{R}$ , defined for each item $\\textit{i}\\in\\textit{\\mathcal{T}}$ . The ground-truth ordering of $\\mathcal{T}$ is determined by a comparison function $\\pi_{i j}:=\\mathbb{1}[y_{i}>y_{j}]$ , where $\\pi_{i j}=1$ indicates that $i$ ranks higher than $j$ . We assume there are no ties. ", "page_idx": 1}, {"type": "text", "text": "We define the ordering error $R_{\\mathbb{Z}}(h)$ of a learned comparison function $h:\\mathcal{T}\\times\\mathcal{T}\\rightarrow\\{0,1\\}$ as the frequency of pairwise inversions under a uniform distribution of item pairs, ", "page_idx": 1}, {"type": "equation", "text": "$$\nR_{\\mathbb{Z}}(h)=\\frac{2}{n(n-1)}\\sum_{i\\neq j\\in\\mathbb{Z}}\\mathbb{1}[h(i,j)\\neq\\pi_{i j}]\\;,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $n=|{\\mathcal{T}}|$ . This error is equivalent to the normalized Kendall\u2019s Tau distance (Kendall, 1948). ", "page_idx": 1}, {"type": "text", "text": "Hypotheses $h$ are learned from preference feedback\u2014noisy pairwise comparisons $C_{i j}\\in\\{0,1\\}$ for items $(i,j)$ related to their score, for example, provided by human annotators. $C_{i j}=1$ indicates that an annotator perceived that item $i$ has a higher score than $j$ , i.e., that they prefer $i$ over $j$ . Our goal is to minimize the ordering error $R_{\\mathrm{{Z}}}(h)$ for a fixed budget $T\\geq1$ of adaptively chosen comparisons. ", "page_idx": 1}, {"type": "text", "text": "We are interested in contextual problems, where each item $\\textit{i}\\in\\mathcal{T}$ is endowed with item-specific attributes $x_{i}\\in\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ . As we will see, this permits more sample-efficient ordering and learning algorithms that can order items out-of-sample, trained on comparisons of a subset of items $\\mathcal{T}_{D}\\subseteq\\mathcal{T}$ and generalizing to ${\\mathcal{T}}\\setminus{\\mathcal{T}}_{D}$ . Ordering algorithms based only on preference feedback cannot solve this problem since observed comparisons are uninformative of new items. ", "page_idx": 1}, {"type": "text", "text": "Our active preference learning scenario proceeds as follows: 1) A learner is given an annotation budget $T$ , a pool of items $\\mathcal{T}_{D}\\subseteq\\mathcal{T}$ and item attributes $x_{i}$ for $i\\in\\mathcal{T}_{D}$ . 2) Over rounds $t=1,...,T$ , the learner requests a comparison of two items $i_{t},j_{t}\\,\\in\\,\\mathcal{T}_{D}$ according to a sampling criterion and receives noisy binary preference feedback $c_{t}\\,\\sim\\,p(C_{i j})$ , independently of previous comparisons. 3) After $T$ rounds, the learner returns a comparison function $\\bar{h}:\\mathcal{T}\\times\\dot{\\mathcal{T}}\\rightarrow\\mathbf{\\bar{\\Omega}}\\{0,1\\}$ . We denote the history of accumulated observations until and including time $t$ by $D_{t}=((i_{1},\\dot{j}_{1},c_{1}),...,(i_{t},j_{t},c_{t}))$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We assume that comparisons $C_{i j}$ follow a logistic model applied to the difference between item scores, $p(C_{i j}\\,=\\,1)\\,\\stackrel{\\cdot}{=}\\,\\sigma(y_{i}\\,-\\,y_{j}^{\\cdot})$ , the so-called Bradley-Terry model (Bradley and Terry, 1952), which assumes linear stochastic transitivity (Oliveira et al., 2018). Throughout, $\\sigma(z)\\,=\\,1/(1+$ $e^{-z}$ ) and $\\dot{\\sigma}(z)$ its derivative at $z$ . Specifically, we study the case where $y_{i}$ is a linear function of item attributes, $y_{i}\\,=\\,\\theta_{*}^{\\top}x_{i}$ , with $\\theta_{*}\\,\\in\\,\\mathbb{R}^{d}$ the ground-truth coefficients. Thus, comparisons are determined by a logistic regression model applied to the attribute difference vector $z_{i j}:=x_{i}-x_{j}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\np(C_{i j}=1)=\\sigma(\\boldsymbol{\\theta}_{*}^{\\top}z_{i j})\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We face two kinds of uncertainty when actively learning the model in (2): epistemic and aleatoric. Epistemic uncertainty, or model uncertainty, is the uncertainty about the true parameter $\\boldsymbol{\\theta}_{*}$ , while aleatoric uncertainty is the irreducible uncertainty about labels due to noisy annotation. ", "page_idx": 2}, {"type": "text", "text": "3 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Active Preference Learning: Preference learning (F\u00fcrnkranz and H\u00fcllermeier, 2003; Chu and Ghahramani, 2005) is related to the problem of learning to rank (Burges et al., 2005; Busse et al., 2012). When using adaptively chosen comparisons it may be posed as an active learning or bandit problem (Brinker, 2004; Long et al., 2010; Silva et al., 2014; Ling et al., 2020). Non-contextual active learners, such as TrueSkill (Herbrich et al., 2006; Minka et al., 2018), Hamming-LUCB (Heckel et al., 2018), and Probe-Rank (Lou et al., 2022) produce in-sample preference orderings, but must be updated if new items are to be ranked. Contextual algorithms, such as BALD (Houlsby et al., 2011), mitigate this by exploiting item structure and Kirsch and Gal (2022) show that many recently proposed contextual active learning strategies may be unified in a framework based on Fisher information. Similarly, methods have been proposed to recover a linear preference model by adaptively sampling paired comparisons (Qian et al., 2015; Massimino and Davenport, 2021; Canal et al., 2019). Still, this setting differs from ours in that we emphasize recovering the full ordering, not perfectly estimating the parameters. While it is true that knowing the parameters is sufficient to order the list, reducing uncertainty for all parameters equally will likely be wasteful (see Section 4). Ailon (2011) offer guarantees for ordering using contextual features in the noiseless setting, while Jamieson and Nowak (2011) analyze the setting where noise is unrelated to item similarity. ", "page_idx": 2}, {"type": "text", "text": "Bandits: Bandit algorithms with relative or dueling feedback (Yue and Joachims, 2009; Bengs et al., 2021; Yan et al., 2022) also learn from pairwise comparisons, and have been proposed both in contextual (Dud\u00edk et al., 2015) and non-contextual settings (Yue et al., 2012) to minimize regret or identify top- $k$ items. Bengs et al. (2022) proposed CoLSTIM, a contextual dueling bandit for regret minimization under linear stochastic transitivity, matching (2), and Di et al. (2023) gave varianceaware regret bounds for this setting. However, algorithms that find the top- $\\cdot k$ items, such as pure exploration bandits (Fang, 2022; Jun et al., 2021), can be arbitrarily bad at learning a full ordering (see Appendix D). Related are also George and Dimitrakakis (2023) who learn Kemeny rankings in non-contextual dueling bandits, and Wu et al. (2023b) who minimize Borda regret. Zhu et al. (2023) studies the problem of estimating a preference model from offline data. Our analysis uses techniques from logistic bandits (Filippi et al., 2010; Li et al., 2017; Faury et al., 2020; Kveton et al., 2020). ", "page_idx": 2}, {"type": "text", "text": "RLHF: Preference learning is commonly used when training large language models through reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022; Ouyang et al., 2022; Wu et al., 2023a). In this line of work, Zhu et al. (2023) provide guarantees on the sample complexity of learning a preference model from offline data. They leverage similar tools from statistical learning and bandits as we do. In contrast to their work, we provide sampling strategies for the online setting. Mehta et al. (2023) consider active learning for RLHF in a dueling bandit framework where the goal is to optimize a contextual version of the Borda regret. Concurrent work by Mukherjee et al. (2024) and Das et al. (2024) studies a similar problem, as we do here, in the RLHF setting but with the objective to identify an optimal policy in a contextual bandit with dueling feedback. In contrast to their objective, we are interested in recovering the ordering of items. Das et al. (2024) use similar bandit techniques as we do, and their selection criterion, when adapted for ordering, corresponds to our NormMin baseline (see Section 6). ", "page_idx": 2}, {"type": "text", "text": "4 Which comparisons result in a good ordering? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We give an upper bound on the ordering error $R_{\\mathrm{{Z}}}(h)$ for a hypothesis $h$ fit using a set of $T$ comparisons. The bound is retrospective, attempting to answer the question \u201cif we collect comparisons $D_{T}$ , how good is our resulting model at ordering the items in $\\mathcal{T V}$ . In Section 5, we use insights from the result to design an active learning algorithm. ", "page_idx": 3}, {"type": "text", "text": "We restrict our analysis to the logistic model in (2) and denote by $R(\\theta)\\equiv R_{\\mathrm{{Z}}}(h_{\\theta})$ the risk of the hypothesis defined by $h_{\\theta}(i,j)=\\mathbb{1}[\\theta^{\\top}z_{i j}>0]$ . Recall that $z_{i j}=x_{i}-x_{j}$ for $i,j\\in\\mathcal{T}$ , and define $z_{t}\\equiv z_{i_{t}j_{t}}$ as the difference between attributes for the pair of items selected at round $t$ . Let $\\theta_{t}$ be the maximum-likelihood estimate (MLE) fit to $t$ rounds of feedback, $D_{t}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{\\theta}_{t}=\\arg\\operatorname*{max}_{\\boldsymbol{\\theta}}\\sum_{s=1}^{t}\\left(c_{s}\\log\\sigma(\\boldsymbol{\\theta}^{\\top}{\\boldsymbol{z}}_{s})+(1-c_{s})(1-\\sigma(\\boldsymbol{\\theta}^{\\top}{\\boldsymbol{z}}_{s}))\\right)~.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $\\Delta_{i j}~>~0$ lower bound the margin of comparison, $|\\sigma(z_{i j}^{\\top}\\theta_{*})\\,-\\,1/2|\\;>\\;\\Delta_{i j}$ for all $i,j\\ \\in\\ Z$ and define $\\begin{array}{r}{\\Delta_{*}\\,=\\,\\operatorname*{min}_{i\\neq j}\\Delta_{i j}/|i-j|}\\end{array}$ . Next, let $\\begin{array}{r}{\\mathbf{H}_{t}(\\boldsymbol{\\theta})\\,:=\\,\\sum_{s=1}^{t}\\dot{\\sigma}(z_{s}^{\\top}\\boldsymbol{\\theta})z_{s}z_{s}^{\\top}}\\end{array}$ be the Hessian of the negative log-likelihood of observations at time $t$ under (2), given the parameter $\\theta$ , also known as observed Fisher information. We define $\\tilde{\\mathbf{H}}_{t}(\\theta)\\,:=\\,\\scriptstyle\\frac{1}{t}\\mathbf{H}_{t}(\\theta)$ . For a square matrix $V$ , we define $\\|x\\|_{V}={\\sqrt{x^{\\top}V x}}$ . We make the following assumptions for our analysis: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. $\\boldsymbol{\\theta}_{*}$ satisfies $\\lVert{\\boldsymbol{\\theta}}_{*}\\rVert_{2}\\leq S$ for some $S>0$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. $\\forall i\\in{\\mathcal{T}}$ , we have $\\|x_{i}\\|_{2}\\leq Q$ for $Q>0$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 3. ${\\bf H}_{T}(\\theta_{T})$ and $\\mathbf{H}_{T}(\\theta_{*})$ have full rank and minimum eigenvalues larger than $\\lambda_{0}>0$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 implies that $\\theta_{*}$ lies in some ball with radius $S$ and cannot have unbounded coefficients. Assumption 2 states that there exists an upper bound on the norm of the feature vectors. This assumption is trivially satisfied whenever we have a finite set of data points. Both assumptions 1 and 2 are standard in the bandit literature and only required for our analysis. Assumption 3 is naturally satisfied for sufficiently large $T$ by any sampling strategy with support on $d$ linearly independent vectors, or can be ensured by allowing for a burn-in phase of $d$ samples in the beginning of an adaptive strategy. Assumption 3 ensures the uniqueness of $\\theta_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "We start by stating the following concentration result for the deviation of $\\sigma(z_{i j}^{\\top}\\theta_{T})$ from the true probability $\\sigma(z_{i j}^{\\top}\\theta_{*})$ . The proof of Lemma 1 is found in Appendix $\\mathbf{C}$ and builds on results for optimistic algorithms in logistic multi-armed bandits (Filippi et al., 2010; Faury et al., 2020). ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 (Concentration Lemma). Define, for all pairs of items $i,j\\in\\mathcal{Z}$ , and any $\\Delta>0$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha_{i j}(\\Delta):=\\exp\\left(\\frac{-\\Delta^{2}T}{8d C_{1}(\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{T})\\|z_{i j}\\|_{\\dot{\\mathbf{H}}_{T}^{-1}(\\theta_{T})})^{2}}\\right),\\ \\ \\ \\beta_{i j}(\\Delta):=\\exp\\left(\\frac{-\\Delta T}{d C_{1}\\|z_{i j}\\|_{\\dot{\\mathbf{H}}_{T}^{-1}(\\theta_{T})}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, i $\\dot{\\mathbf{\\xi}}\\alpha:=\\alpha_{i j}(\\Delta),\\beta:=\\beta_{i j}(\\Delta)$ and $\\begin{array}{r}{\\alpha,\\beta\\leq\\frac{1}{4d T}}\\end{array}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nP\\left(|\\sigma(z_{i j}^{\\top}\\theta_{*})-\\sigma(z_{i j}^{\\top}\\theta_{T})|>\\Delta\\right)\\le2d T\\left(\\alpha+\\beta\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$C_{1}$ depends on $S,\\lambda_{0},Q$ from Assumptions $_{l-3}$ (see Appendix $C$ for definition and proof). ", "page_idx": 3}, {"type": "text", "text": "The concentration result in Lemma 1 is verifiable (given by observables) since the upper bound depends only on the maximum likelihood estimate $\\theta_{T}$ at time $T$ , not on $\\boldsymbol{\\theta}_{*}$ . We present a sharper, unverifiable bound in Appendix $\\mathbf{C}$ which instead depends on $\\boldsymbol{\\theta}_{*}$ but does not suffer from the explicit scaling with $d$ in the definitions of $\\alpha$ and $\\beta$ . The bound in Lemma 1 can also be expressed in terms of $\\mathbf{H}_{T}^{-1}(\\bar{\\boldsymbol{\\theta}}_{T})$ by using the equality $\\begin{array}{r}{\\lVert z_{i j}\\rVert_{{\\mathbf{H}}_{T}^{-1}(\\theta_{T})}^{2}=\\frac{1}{T}\\lVert z_{i j}\\rVert_{{\\tilde{\\mathbf{H}}}_{T}^{-1}(\\theta_{T})}^{2}.}\\end{array}$ . As long as our sampling strategy ensures that the minimum eigenvalue of $\\tilde{\\mathbf{H}}_{t}(\\theta_{t})$ does not tend to zero, i.e., the strategy is strongly consistent (Chen et al., 1999), we have $\\alpha_{i j}(\\Delta_{i j})\\,\\sim\\,\\exp[-\\Delta_{i j}^{2}T/(\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{T})^{2}\\lVert z_{i j}\\rVert_{\\tilde{\\mathbf{H}}_{T}^{-1}(\\theta_{T})}^{2})]$ and $\\beta_{i j}(\\Delta_{i j})\\sim\\exp[-\\Delta_{i j}T/\\|z_{i j}\\|_{\\tilde{\\mathbf{H}}_{T}^{-1}(\\theta_{T})}^{2}]$ . Since $\\Delta_{i j}^{2}\\,<\\,\\Delta_{i j}\\,<\\,1/2$ by definition, we can view $\\alpha$ as the first-order term and $\\beta$ as the second-order term of our bound. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Greedy Uncertainty Reduction for Ordering (GURO), [BayesGURO] ", "page_idx": 4}, {"type": "text", "text": "Require: Training items $\\mathcal{Z}_{D}$ , attributes $\\mathbf{X}=\\{x_{i}\\}_{i\\in\\mathbb{Z}_{d}}$   \n1: Initialize $\\theta_{0}$   \n2: for $t=1,...,T$ do   \n3: Draw $\\left(i_{t},j_{t}\\right)$ based on $\\theta_{t}$ according to (5) [or (7)]   \n4: Observe $c_{t}$ from noisy comparison (annotator)   \n5: $D_{t}=D_{t-1}\\cup\\left\\{i_{t},j_{t},c_{t}\\right)\\right\\}$   \n6: $\\theta_{t}=\\mathrm{MLE}(D_{t})$ according to (3) [or $\\theta_{t}=\\mathrm{MAP}(D_{t})$ as in (11) in the Appendix]   \n7: end for   \n8: Return $h_{T}$ ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 formally captures the intuition that it should be easier to sort when annotations contain little noise, i.e., $\\dot{\\sigma}(z_{i j}^{\\top}\\bar{\\theta}_{T})$ is small. Especially, we observe $\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{T})\\approx0_{\\cdot}$ for pairs where $\\Delta_{i j}$ is sufficiently large, causing the first-order term to vanish, leaving us with the faster decaying second-order term $\\beta$ . Lemma 1 also tells us that the hardest pairs to guarantee a correct ordering for are the ones with both high aleatoric uncertainty under the MLE model, e.g., where annotators disagree or labels are noisy, captured by $\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{T})$ , as well as high epistemic uncertainty captured by $\\lVert z_{i j}\\rVert_{\\tilde{\\mathbf{H}}_{T}^{-1}(\\theta_{T})}$ . ", "page_idx": 4}, {"type": "text", "text": "A direct consequence of Lemma 1 is the following bound on the ordering error of $h_{\\theta_{T}}$ over $\\mathcal{T}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[R(\\theta_{T})]\\leq\\sum_{i\\neq j}\\frac{2\\operatorname*{min}\\left\\{2d T\\left(\\alpha_{i j}(\\Delta_{i j})+\\beta_{i j}(\\Delta_{i j})\\right),1\\right\\}}{n(n-1)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The right-hand side in the above inequality can be bounded further by utilizing that $\\Delta_{i j}\\geq|i-j|\\Delta_{*}$ .   \nTogether with Markov\u2019s inequality, this yields the following bound on $P(R(\\theta_{T})\\geq\\epsilon)$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Upper bound on the ordering error). ", "page_idx": 4}, {"type": "text", "text": "Let $\\alpha_{*}\\ :=\\ \\operatorname*{max}_{i\\neq j}\\alpha_{i j}(\\Delta_{*})$ and $\\beta_{*}\\ :=\\ \\operatorname*{max}_{i\\neq j}\\beta_{i j}(\\Delta_{*})$ , with $\\alpha,\\beta$ from Lemma $^{\\,l}$ . Then, for $\\begin{array}{r}{\\alpha_{*},\\beta_{*}\\leq\\frac{1}{4d T}}\\end{array}$ and any $\\epsilon\\in(0,1)$ , the ordering error $R(\\theta_{T})$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\cal P}(R(\\theta_{T})\\geq\\epsilon)\\leq\\frac{4d T}{\\epsilon n}\\left(\\left(\\alpha_{*}^{-1}-1\\right)^{-1}+\\left(\\beta_{*}^{-1}-1\\right)^{-1}\\right)\\approx\\frac{4d T}{\\epsilon n}\\left(\\alpha_{*}+\\beta_{*}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha_{*}$ and $\\beta_{*}$ decay exponentially with $T$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 suggests that the probability of $R(\\theta_{T})\\;\\geq\\;\\epsilon$ decays exponentially with a rate that depends on the quantities maxi,j \u03c3\u02d9(zi\u22a4j\u03b8T )\u2225zij\u2225H\u02dct\u22121(\u03b8T ) and maxi,j \u2225zij\u22252H\u02dc\u22121(\u03b8T ). Both quantities are random variables that depend on the particular sampling strategy that yields $\\mathbf{H}_{t}$ . Focusing on the leading term, $\\operatorname*{max}_{i,j}\\dot{\\sigma}\\big(z_{i j}^{\\dag}\\theta_{T}\\big)\\lVert z_{i j}\\rVert_{\\tilde{\\mathbf{H}}_{t}^{-1}(\\theta_{T})}$ , Theorem 1 suggests that an active learner should gather data to minimize this quantity and obtain the smallest possible bound. The factor $\\lVert z_{i j}\\rVert_{\\tilde{\\mathbf{H}}_{T}^{-1}(\\theta_{T})}^{2}$ is the weighted norm of $z_{i j}$ w.r.t. the inverse of the observed Fisher information (cf. Kirsch and Gal (2022)). It controls the shape of the confidence ellipsoid around $\\theta_{T}$ and the width of the confidence interval around $\\theta_{T}^{\\top}z_{i j}$ . The leading term in Theorem 1 re-scales this quantity with aleatoric noise under the MLE estimate $\\theta_{T}$ . This suggests that higher epistemic (model) certainty is needed in directions with high aleatoric uncertainty\u2014where item similarity increases noise in comparisons. ", "page_idx": 4}, {"type": "text", "text": "In Appendix C.3, we comment on i) generalizations to regularized preference models, ii) applications to generalized linear models with other link functions, iii) lower bounds on the ordering error, and iv) an algorithm-specific upper bound. ", "page_idx": 4}, {"type": "text", "text": "5 Greedy uncertainty reduction for ordering (GURO) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We present an active preference learning algorithm based on greedy minimization of the bound in Theorem 1, called GURO. We begin with fully contextual preference models of the form $\\sigma(\\theta^{\\top}z_{i j})$ and return in Section 5.1 to parameterization variants to reduce the effects of model misspecification. ", "page_idx": 4}, {"type": "text", "text": "The main component of the bound in Theorem 1 to be controlled by an active learner is the term ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i,j\\in\\mathcal{T}}\\;\\dot{\\sigma}\\big(z_{i j}^{\\top}\\theta_{T}\\big)\\|z_{i j}\\|_{\\mathbf{H}_{T}^{-1}(\\theta_{T})}\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which represents the highest uncertainty in the comparison of any items $i,j\\in\\mathcal{T}$ under the model $\\theta_{T}$ . A smaller value of (4) yields a smaller bound and a stronger guarantee. Recall that, for any $t=1,...,T,\\theta_{t}$ is the MLE estimate of the ground-truth parameter $\\theta_{*}$ with respect to the observed history $D_{t}$ . Both factors in (4) are determined by the sampling strategy that yielded the item pairs $\\left(i_{t},j_{t}\\right)$ in $D_{T}$ and, therefore, ${\\mathbf{H}}_{T}$ and $\\theta_{T}$ (the results of comparisons $c_{i j}$ are outside the control of the algorithm, but $z_{i j}$ are known). ", "page_idx": 5}, {"type": "text", "text": "Direct minimization of (4), for a subset $\\mathcal{Z}_{D}$ , is not feasible without access to comparisons $c_{i j}$ and their likelihood under $\\theta_{T}$ . Instead, we adopt a greedy, alternating approach: In each round, a) a single pair is sampled for comparison by maximizing (4) under the current model estimate, and b) $\\theta_{t}$ is recomputed based on $D_{t}$ . Specifically, at $t=1,...,T$ , we sample, ", "page_idx": 5}, {"type": "equation", "text": "$$\ni_{t},j_{t}=\\underset{i,j\\in\\mathbb{Z}_{D},i\\neq j}{\\arg\\operatorname*{max}}\\ \\dot{\\sigma}(z_{i j}^{\\top}\\theta_{t-1})\\|z_{i j}\\|_{\\mathbf{H}_{t-1}^{-1}(\\theta_{t-1})}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We refer to this sampling criterion as Greedy Uncertainty Reduction for Ordering (GURO), since it reduces the uncertainty of $\\theta_{t}$ in the direction of $z_{i j}$ . To see this, consider the change of $\\mathbf{H}_{t}(\\theta_{t})$ after a single play of $i_{t},j_{t}$ . The Sherman-Morrison formula (Sherman and Morrison, 1950) yields, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{H}_{t}^{-1}(\\theta_{t-1})=\\mathbf{H}_{t-1}^{-1}(\\theta_{t-1})-\\dot{\\sigma}(z_{t}^{\\top}\\theta_{t-1})\\frac{\\mathbf{H}_{t-1}^{-1}(\\theta_{t-1})z_{t}z_{t}^{\\top}\\mathbf{H}_{t-1}^{-1}(\\theta_{t-1})}{1+\\dot{\\sigma}(z_{t}^{\\top}\\theta_{t-1})\\lVert z_{t}\\rVert_{\\mathbf{H}_{t-1}^{-1}(\\theta_{t-1})}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $z_{t}~:=~z_{i_{t}j_{t}}$ . With $\\xi$ as the second term in (6), it holds for all $i\\,<\\,j\\,\\in\\,{\\cal Z}$ , with $\\mathbf{H}_{t-1}\\,=$ $\\mathbf{H}_{t-1}(\\theta_{t-1})$ , that $||z_{i j}||_{\\mathbf{H}_{t}^{-1}(\\theta_{t-1})}^{2}=||z_{i j}||_{\\mathbf{H}_{t-1}^{-1}}^{2}-||z_{i j}||_{\\xi}^{2}\\leq||z_{i j}||_{\\mathbf{H}_{t-1}^{-1}}^{2}$ . The inequality is strict for the pair $i_{t},j_{t}$ in (5). As $\\theta_{t}$ converges to $\\theta_{*}$ , this pair becomes representative of the maximizer of (4) provided there is no major systematic discrepancy between $\\mathcal{T}_{D}$ and $\\mathcal{T}$ . ", "page_idx": 5}, {"type": "text", "text": "Surprisingly, GURO can also be justified from a Bayesian analysis. Consider a Bayesian model of the parameter $\\theta$ with $p(\\theta)$ the prior belief and $p(\\theta\\mid\\dot{D}_{t})$ the posterior after observing the preference feedback in $D_{t}$ . A natural active learning strategy is to sample items $i_{t},j_{t}$ for which the model preference is highly uncertain under the posterior distribution, ", "page_idx": 5}, {"type": "equation", "text": "$$\ni_{t},j_{t}=\\underset{i,j\\in\\mathbb{Z}_{D},i<j}{\\arg\\operatorname*{max}}\\ \\hat{\\mathbb{V}}_{\\theta|D_{t-1}}[\\sigma(\\theta^{\\top}z_{i j})]\\;,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\mathbb{V}}_{\\theta|D_{t-1}}\\big[\\sigma(\\theta^{\\top}z_{i j})\\big]$ is a finite-sample estimate of the variance in predictions, computed by sampling from the posterior. In Appendix B.3, we show that the first-order Taylor expansion of the true variance is equal to the GURO criterion. Hence, we refer to sampling according to (7) as BayesGURO. Unlike GURO, BayesGURO can incorporate prior knowledge through $p(\\theta)$ and benefits from controlled stochasticity through the empirical estimate $\\hat{\\mathbb{V}}$ , which makes it appropriate for batched algorithms\u2014a deterministic criterion would construct batches of a single item pair. Both GURO and BayesGURO are presented in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Computational Complexity: Running the algorithms requires $O(n^{2})\\,$ operations each iteration to evaluate the sampling criteria (Equation 5 or 7) on all possible pairs, a problem shared by many active preference learning algorithms (Qian et al., 2015; Canal et al., 2019; Houlsby et al., 2011). A way of mitigating this computational complexity is to, at each time step, sample a fixed number of comparisons and only evaluate on these, similar to the approach taken in Canal et al. (2019). When only looking at a sample of $m\\ll n^{2}$ pairs, the complexity is reduced to $O(m)$ . While making $m$ too small can hurt the sample complexity, we describe in Appendix E how we implemented this sub-sampling strategy to speed up computations in one of our experiments and observed no noticeable change in performance. Lastly, we want to highlight that in many realistic scenarios, the computational burden pales in comparison to the time it takes to query an annotator. ", "page_idx": 5}, {"type": "text", "text": "5.1 Preference models for in- and out-of-sample ordering ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our default preference model $h(i,j)=\\mathbb{1}[f(i,j)>0]$ is based on a fully contextual scoring function ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{\\theta}(x_{i},x_{j})=\\theta^{\\top}(x_{i}-x_{j})\\;,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "fit with a logistic likelihood $\\sigma(f(i,j))\\,\\approx\\,p(C_{i j}\\,=\\,1)$ . The model\u2019s strength is that the variance in its estimates grows with $d$ , but not with $n\\,=\\,|{\\mathcal{T}}|$ , often resulting in quicker convergence than non-contextual methods for moderate dimension $d$ (see, e.g., Figure $2c$ ). The fully contextual model also generalizes to unseen items as long as the attributes for $\\mathcal{T}_{D}$ span attributes observed for $\\mathcal{T}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The limitations of a fully contextual model are model misspecification (error due to the functional form), and noise (error due to $C$ not being fully determined by $X$ ). The former can be mitigated by applying the linear model to a representation function $\\phi:\\mathcal{X}\\rightarrow\\mathbb{R}^{d^{\\prime}}$ , $f_{\\theta}(x_{i},x_{j})=\\theta^{\\top}(\\phi(x_{i})-$ $\\phi(x_{j}))$ . A good representation $\\phi$ , e.g., from a foundation model, can mitigate misspecification and admit different input modalities. As demonstrated in Figure 5 in the Appendix, even a representation pre-trained for a different task can perform much better than a random initialization.2 ", "page_idx": 6}, {"type": "text", "text": "Noise due to insufficiencies in $X$ cannot be mitigated by a representation $\\phi(x)$ ; If annotators consistently compare items based on features $U$ not included in $X$ , no function $h(X_{i},X_{j})$ can perfectly order the items. However, for in-sample ordering of $\\mathcal{Z}_{D}$ , adding per-item parameters $\\zeta_{i}\\in\\mathbb{R}$ to the scoring function, one for each item $i\\in\\mathcal{T}_{D}$ , can mitigate both misspecification and noise, ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{\\theta,\\zeta}(x_{i},x_{j})=\\theta^{\\top}(\\phi(x_{i})-\\phi(x_{j}))+(\\zeta_{i}-\\zeta_{j})\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We call this a hybrid model and apply it in \u201cGURO Hybrid\u201d and baselines in experiments. The term $\\zeta_{i}-\\zeta_{j}$ can correct the residual of the fully contextual model, which is small if a) the context captures the most relevant information about the ordering, and b) the functional form $\\theta^{\\top}\\phi(x_{i})$ is nearly wellspecified. Using $\\zeta_{i}-\\zeta_{j}$ alone is sufficient in-sample, but has high variance (the dimension is $n$ instead of $d$ ) and poor generalization $\\zeta_{i}$ are unknown for items $i\\not\\in\\mathcal{T}_{D}.$ ). In practice, we use L2 regularization to prevent the model from learning an arbitrary $\\theta$ by using the full expressivity of $\\zeta_{i}$ (see Appendix E for details). Empirically, our hybrid models exhibit the best of both worlds: When $\\phi$ is poor, the model recovers and competes with non-contextual models (Figure 5); when $\\phi$ is good, convergence matches fully contextual models (Figure 2). ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate GURO (Algorithm 1) and GURO Hybrid (see Section 5.1) in four image ordering tasks, one with logistic (synthetic) preference feedback, and three tasks based on real-world feedback from human annotators3.We provide a synthetic experiment in Appendix E.2 that includes empirical estimates of the bound in Theorem 1. The experiments include five diverse baseline algorithms, described next. BALD (Houlsby et al., 2011) is a priori the strongest baseline since it is a contextual active learning algorithm, unlike the others. Its selection criterion greedily maximizes the decrease in posterior entropy, which amounts to reducing the epistemic uncertainty and includes a term to downplay the influence of aleatoric uncertainty. This is not always beneficial, as suggested by our analysis in Section 4, since learners may require several comparisons of high-uncertainty pairs to get the order right. CoLSTIM (Bengs et al., 2022) is a contextual bandit algorithm, developed for regret minimization and is not expected to perform well here. It is included to illustrate the mismatch between regret minimization and our setting. ", "page_idx": 6}, {"type": "text", "text": "TrueSkill (Herbrich et al., 2006; Graepel, 2012) is a non-contextual skill-rating system that models the score of each item as a Gaussian distribution, disregarding item attributes, and has been adopted in various works to score items based on subjective pairwise comparisons (Larkin et al., 2022; Naik et al., 2014; Sartori et al., 2015). We use the sampling rule from Hees et al. (2016), designed for ordering. Finally, we include Uniform sampling, and to illustrate the importance of accounting for aleatoric uncertainty, we use a version of GURO called NormMin that ignores the $\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{t})$ term and plays the pair maximizing $\\lVert z_{i j}\\rVert_{\\mathbf{H}_{t}^{-1}(\\theta_{t})}$ , i.e., it minimizes the second-order term in Lemma 1. NormMin corresponds to the selection criterion in the concurrent work Das et al. (2024), adapted to our problem of finding the correct ordering. We refer the reader to Appendix E.2 for a detailed comparison where NormMin performs significantly worse than Uniform on certain problem instances, and Appendix E for details regarding the implementation and the choice of hyperparameters for GURO, BayesGURO, and baselines. ", "page_idx": 6}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/158cde646c0b8325c3c1c67e762d3fefb36962d5f7ea11b0a574679954d9027e.jpg", "img_caption": ["Figure 1: X-RayAge. Performance of active sampling strategies when comparisons are simulated using a logistic model according to (2). In-sample Kendall\u2019s Tau distance $R_{I_{D}}$ on 200 images (left) and generalization error $R_{I_{E}}-R_{I_{D}}$ for models trained on 150 images and evaluated on 150 images from a different distribution (right). All results are averaged over 100 different random seeds. ", "(a) Mean $R_{I_{D}}$ with 1-sigma error region. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/f4dcaba42176711bb3b8e20aeb065a1e74980a79d2f4dd0af6a95f16cffd169c.jpg", "img_caption": ["(b) Mean generalization error $(95\\%$ CI) "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.1 Ordering $\\mathbf{X}$ -ray images under the logistic model ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our first task (X-RayAge) is to order $\\Chi$ -ray images based on perceived age (Ieki et al., 2022) where the preference feedback follows a (well-specified) logistic model. We base this experiment on the data from the Kaggle competition \u201dX-ray Age Prediction Challenge\u201d (Felipe Kitamura, 2023) which contains more than 10 000 de-identified chest X-rays, along with the person\u2019s true age. Features were extracted using the 121-layer DenseNet in the TorchXrayVision package (Cohen et al., 2022) followed by PCA projection, resulting in 35 features. A ridge regression model, $\\theta_{*}$ , was fit to the true age ( $\\bar{R^{2}}\\approx0.\\dot{67})$ ). During active learning, feedback is drawn from $p(C_{i j}=1)=\\sigma\\left(\\theta_{*}^{\\top}z_{i,j}\\cdot\\lambda\\right)$ , where $\\lambda$ (set to 0.1) controls the noise level. We only include the fully contextual models here since they are well-specified by design, meaning $\\mathcal{T}$ can be ordered using only contextual features. ", "page_idx": 7}, {"type": "text", "text": "In the first setting, we sub-sample $200~\\mathrm{X}$ -ray images uniformly at random from the full set. A ground-truth ordering of these elements is derived using the learned linear model. Figure 1a shows the ordering error over 2 000 iterations. GURO and BayesGURO perform similarly, both better than the baselines. BALD starts off converging about as fast as GURO, but plateaus, most likely as a result of actively avoiding comparisons with high aleatoric uncertainty\u2014pairs where annotators disagree in their preferences. The poor performance of CoLSTIM highlights the discrepancy between regret minimization and recovering a complete ordering. ", "page_idx": 7}, {"type": "text", "text": "In the second setting, we evaluate how well the algorithms generalize to new items. First, we sample $300\\,\\mathrm{X}.$ -ray images from the full dataset. Next, we split these into two sets, with one $(I_{D})$ containing the youngest $50\\%$ and the other $(I_{E})$ the oldest $5\\bar{0}\\%$ . The algorithms were then trained to order the list containing the younger subjects, but were simultaneously evaluated on how well they could sort the list containing the older subjects. The continuously measured difference in ordering error evaluated on $I_{E}$ and $I_{D}$ are presented in Figure 1b. While all algorithms are worse at ordering items in $I_{E}$ , GURO and BayesGURO achieve the lowest average difference. Together with Figure 1a, this means that our proposed algorithms achieved the best in-sample and out-of-sample orderings. For completeness, the in-sample performance of algorithms in the generalization experiment in Figure 1b are included in Appendix E.2. ", "page_idx": 7}, {"type": "text", "text": "6.2 Ordering items with human preference data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we evaluate our algorithm on three publicly available datasets to study the algorithms\u2019 performance when preference feedback comes from human annotators (see Table 1 for an overview, detailed information of datasets in Appendix E.1). The datasets are IMDB-WIKI-SbS (Pavlichenko and Ustalov, 2021), where annotators have stated which of two people appear older, ImageClarity (Zhang et al., 2016), where modified versions of the same image have been compared according to the level of distortion, as well as the extended WiscAds dataset (Carlson and Montgomery, 2017), where labels correspond to which political advertisement is perceived as more negative toward an ", "page_idx": 7}, {"type": "text", "text": "Table 1: Datasets with preference feedback from annotators. Pretrained models are ResNet34 (He et al., 2016), all-mpnet-base-v2 (Reimers and Gurevych, 2019), and FaceNet (Schroff et al., 2015). ", "page_idx": 8}, {"type": "table", "img_path": "PSLH5q7PFo/tmp/b0b94cbddab43b16bcdee1e3f5c3b8c09bcbd0a7a2af48b8a5b00cdf07535eb6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/b93967de22adc8809115be869480f05984889ffd1758c32ecf0d246d06aec222.jpg", "img_caption": ["Figure 2: The empirical error $\\hat{R}_{D^{\\prime}}(h)$ on a holdout comparison set $D^{\\prime}$ when comparisons are made by human annotators. The plots are averaged over 100 (a,b) or 10 (c,d) seeds, and the shaded area represents one standard deviation above and below the mean. For every seed, $10\\%$ of comparisons were used for the holdout set. In (d) we initially order a list $\\mathcal{Z}_{D}$ of $3\\ 000$ images. After 10 000 comparisons the remaining 3 072 images, ${\\mathcal{T}}\\backslash{\\mathcal{T}}_{D}$ , are added. ", "(c) IMDB-WIKI-SbS. , . ", "(d) IMDB-WIKI-SbS. $n=3\\,000$ (6 072). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "opponent. In all datasets, pairs of items were sampled uniformly for annotation. For each experiment, we construct a feature vector $\\phi(x_{i})\\in\\mathbb{R}^{d}$ for all $n$ items using a pre-trained embedding model followed by PCA, applied to reduce computational complexity. We restrict algorithms to only query pairs for which an annotation exists and remove the annotation from the pool once queried. In cases where multiple annotations exist for the same pair, the feedback is chosen randomly among these. ", "page_idx": 8}, {"type": "text", "text": "The images in the ImageClarity dataset have been constructed to have an objective ground truth ordering but this is not the case for WiscAds or IMDB-WIKI-SbS. As the ground-truth ordering is generally unknown also in real-world applications, we evaluate methods by the error on a heldout set of comparisons $D^{\\prime}$ , $\\begin{array}{r}{\\hat{R}_{D^{\\prime}}(h)=\\frac{1}{|D^{\\prime}|}\\sum_{(i,j,c)\\in D^{\\prime}}\\mathbb{1}[h(i,j)\\neq c]}\\end{array}$ . This serves as an empirical analog of Kendall\u2019s Tau distance and a minimizer of $\\hat{R}_{D^{\\prime}}(h)$ will minimize $R_{\\mathbb{Z}}(h)$ for sufficiently large $D^{\\prime}$ , but will not converge toward 0 since there is inherent noise in annotations. This metric makes no assumptions on the ground truth ordering unlike the alternative approach of fitting an ordering to all available comparisons, see e.g., Maystre and Grossglauser (2017). In Appendix E.2, we show results for the latter that highlight the limitations of estimating a \u201cground-truth\u201d ordering, as well as the similar results when measuring the distance to the objective ground-truth ordering of the ImageClarity dataset. The longest trajectory (single seed) for any algorithm took less than 35hrs to complete on one core of an Intel Xeon Gold 6130 CPU and required at most 10 GB of memory. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "In all experiments, we compare fully contextual (8) and hybrid (9) versions of GURO, BALD, and Uniform, as well as TrueSkill. The results of each experiment can be seen in Figure 2. Figure 2a shows that the ImageClarity dataset is the easiest to order using contextual (non-hybrid) features. This is expected, as features relevant to the level of distortion are low-level. In this case, the choice of adaptive strategy has a modest impact on the ordering error. Figures 2b and 2c highlight the differences between modeling strategies. The fully contextual algorithms initially improve rapidly, achieving a rough ordering of the items, before plateauing and not making any real improvements. This indicates that the features are informative enough to roughly order the list, but insufficient for retrieving a more granular ordering. The non-contextual TrueSkill converges at a much slower pace but keeps improving steadily throughout. Perhaps most interesting are the hybrid algorithms, which seemingly reap the benefits of both methods, improving as quickly as the contextual methods, but avoiding the plateau. In fact, in Figure 5 in the Appendix we show that the hybrid models perform comparably to TrueSkill even when features are completely uninformative. ", "page_idx": 9}, {"type": "text", "text": "The limitations of BALD are most noticeable in the fully contextual case, where it plateaus at a higher error compared to GURO and Uniform. This is however not as prominent when we use BALD in conjunction with our hybrid model, likely a result of the increased dimensionality of the model causing BALD Hybrid to attribute more of the observed errors to model uncertainty. While this initially causes the algorithm to avoid fewer comparisons that are subject to aleatoric uncertainty, the final iterations in Figure 2c suggest that BALD Hybrid can still run into this issue given enough samples. In all experiments, GURO and GURO Hybrid perform better than or similar to our baselines, never worse. Additionally, Figures 2b and 2c showcase how our hybrid model can increase performance when used with existing sampling strategies, such as BALD or Uniform. ", "page_idx": 9}, {"type": "text", "text": "The final experiment, visible in Figure 2d, is a few-shot scenario where after some time, additional images are added to the pool of items. IMDB-WIKI-SbS was used as it contained the highest number of both images and comparisons. The initial pool consists of 3 000 images sampled from the dataset. After 10 000 steps, the remaining 3 072 images were added to the pool. The results again emphasize the differences between our three types of models; the increase in error of the fully contextual model is very slight, likely a result of added samples being drawn from the same distribution. For TrueSkill, the error increases drastically as a result of the algorithm not having seen these items before and having no way of generalizing the results of previous comparisons to them. Lastly, the hybrid algorithms seem to be moderately affected. The error increases as the model has not yet tuned any of the added per-item parameters, but the extent is much smaller than for TrueSkill as the model can provide a rough ranking of the out-of-sample elements using the contextual features. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have demonstrated the benefits of utilizing contextual features in active preference learning to efficiently order a list of items. Empirically, this leads to quicker convergence, compared to noncontextual methods, and allows algorithms to generalize out-of-sample. We derived an upper bound on the ordering error and used it to design an active sampling strategy that outperforms or matches baselines on realistic image and text ordering tasks. Both theoretical and empirical results highlight the benefit of accounting for noise in comparisons when learning from human annotators. ", "page_idx": 9}, {"type": "text", "text": "The optimality of our sampling strategy remains an open question. A future direction is to derive a lower bound on the ordering error, and prove an\u2014ideally matching\u2014algorithm-specific upper bound. However, constructing upper bounds for related fixed-budget tasks is an open problem (Qin, 2022). Moreover, motivated by the annotation setting, our focus has been on reducing sample complexity and we leave it to future work to explore potential linear approximations of the sampling criteria and other trade-offs between sample complexity and computational complexity. Further, our approach can potentially be improved by performing representation learning throughout the learning process. Finally, our experiments are constrained to a limited amount of already-collected (offline) human preference data, causing different algorithms to select disproportionately similar comparisons. Future work should evaluate the strategies in an online setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "FDJ and HB are supported by Swedish Research Council Grant 2022-04748. FDJ is also supported in part by the Wallenberg AI, Autonomous Systems and Software Program founded by the Knut and ALice Wallenberg Foundation. EC and DD are supported by Chalmers AI Research Centre (CHAIR). The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant agreement no. 2022-06725. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Nir Ailon. Active learning ranking from pairwise preferences with almost optimal query complexity. Advances in Neural Information Processing Systems, 24, 2011. ", "page_idx": 10}, {"type": "text", "text": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli TranJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022. ", "page_idx": 10}, {"type": "text", "text": "Viktor Bengs, R\u00f3bert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke H\u00fcllermeier. Preference-based online learning with dueling bandits: A survey. The Journal of Machine Learning Research, 22 (1):278\u2013385, 2021.   \nViktor Bengs, Aadirupa Saha, and Eyke H\u00fcllermeier. Stochastic contextual dueling bandits under linear stochastic transitivity models. In International Conference on Machine Learning, pages 1764\u20131786. PMLR, 2022.   \nChristopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning. Springer, 2006.   \nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \nKlaus Brinker. Active learning of label ranking functions. In Proceedings of the twenty-first international conference on Machine learning, page 17, 2004.   \nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning, pages 89\u201396, 2005.   \nLudwig M. Busse, Morteza Haghir Chehreghani, and Joachim M. Buhmann. The information content in sorting algorithms. In 2012 IEEE International Symposium on Information Theory Proceedings, pages 2746\u20132750, 2012. doi: 10.1109/ISIT.2012.6284021.   \nGregory Canal, Andy Massimino, Mark Davenport, and Christopher Rozell. Active embedding search via noisy paired comparisons. In International Conference on Machine Learning, pages 902\u2013911. PMLR, 2019.   \nDavid Carlson and Jacob M. Montgomery. A Pairwise Comparison Framework for Fast, Flexible, and Reliable Human Coding of Political Texts. American Political Science Review, 111(4):835\u2013 843, November 2017. ISSN 0003-0554, 1537-5943.   \nKani Chen, Inchi Hu, and Zhiliang Ying. Strong consistency of maximum quasi-likelihood estimators in generalized linear models with fixed and adaptive designs. The Annals of Statistics, 27(4): 1155\u20131163, 1999.   \nXi Chen, Paul N. Bennett, Kevyn Collins-Thompson, and Eric Horvitz. Pairwise ranking aggregation in a crowdsourced setting. In Proceedings of the sixth ACM international conference on Web search and data mining, WSDM \u201913, pages 193\u2013202, New York, NY, USA, February 2013. Association for Computing Machinery. ISBN 978-1-4503-1869-3. doi: 10.1145/2433396.2433420.   \nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \nWei Chu and Zoubin Ghahramani. Preference learning with gaussian processes. In Proceedings of the 22nd international conference on Machine learning, pages 137\u2013144, 2005.   \nJoseph Paul Cohen, Joseph D. Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo Guarrera, Matthew P Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, and Hadrien Bertrand. TorchXRayVision: A library of chest X-ray datasets and models. In Medical Imaging with Deep Learning, 2022.   \nNirjhar Das, Souradip Chakraborty, Aldo Pacchiano, and Sayak Ray Chowdhury. Provably sample efficient rlhf via active preference optimization. arXiv preprint arXiv:2402.10500, 2024.   \nVictor H. de la Pe\u00f1a, Michael J. Klass, and Tze Leung Lai. Self-normalized processes: exponential inequalities, moment bounds and iterated logarithm laws. The Annals of Probability, 32(3):1902 \u2013 1933, 2004. doi: 10.1214/009117904000000397.   \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \nQiwei Di, Tao Jin, Yue Wu, Heyang Zhao, Farzad Farnoud, and Quanquan Gu. Variance-aware regret bounds for stochastic contextual dueling bandits. arXiv preprint arXiv:2310.00968, 2023.   \nMiroslav Dud\u00edk, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual dueling bandits. In Conference on Learning Theory, pages 563\u2013587. PMLR, 2015.   \nBoli Fang. Fixed-budget pure exploration in multinomial logit bandits. In International Joint Conference on Artificial Intelligence, 2022.   \nLouis Faury, Marc Abeille, Cl\u00e9ment Calauz\u00e8nes, and Olivier Fercoq. Improved optimistic algorithms for logistic bandits. In International Conference on Machine Learning, pages 3052\u20133060. PMLR, 2020.   \nPaulo Kuriki Felipe Kitamura, Lilian Mallagoli. Spr x-ray age prediction challenge, 2023.   \nSarah Filippi, Olivier Cappe, Aur\u00e9lien Garivier, and Csaba Szepesv\u00e1ri. Parametric bandits: The generalized linear case. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc., 2010.   \nJohannes F\u00fcrnkranz and Eyke H\u00fcllermeier. Pairwise preference learning and ranking. In European conference on machine learning, pages 145\u2013156. Springer, 2003.   \nAur\u00e9lien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In Conference on Learning Theory, pages 998\u20131027. PMLR, 2016.   \nAnne-Marie George and Christos Dimitrakakis. Eliciting kemeny rankings, 2023.   \nThore Graepel. Score-based bayesian skill learning. In Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD-12), January 2012.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "770\u2013778. IEEE, 2016. ", "page_idx": 11}, {"type": "text", "text": "Reinhard Heckel, Max Simchowitz, Kannan Ramchandran, and Martin Wainwright. Approximate ranking from pairwise comparisons. In International Conference on Artificial Intelligence and Statistics, pages 1057\u20131066. PMLR, 2018. ", "page_idx": 12}, {"type": "text", "text": "J\u00f6rn Hees, Benjamin Adrian, Ralf Biedert, Thomas Roth-Berghofer, and Andreas Dengel. Tssort: Probabilistic noise resistant sorting. arXiv preprint arXiv:1606.05289, 2016. ", "page_idx": 12}, {"type": "text", "text": "Ralf Herbrich, Tom Minka, and Thore Graepel. Trueskill\u2122: a bayesian skill rating system. Advances in neural information processing systems, 19, 2006. ", "page_idx": 12}, {"type": "text", "text": "Neil Houlsby, Ferenc Husz\u00e1r, Zoubin Ghahramani, and M\u00e1t\u00e9 Lengyel. Bayesian Active Learning for Classification and Preference Learning, December 2011. arXiv:1112.5745 [cs, stat]. ", "page_idx": 12}, {"type": "text", "text": "Hirotaka Ieki, Kaoru Ito, Mike Saji, Rei Kawakami, Yuji Nagatomo, Kaori Takada, Toshiya Kariyasu, Haruhiko Machida, Satoshi Koyama, Hiroki Yoshida, Ryo Kurosawa, Hiroshi Matsunaga, Kazuo Miyazawa, Kouichi Ozaki, Yoshihiro Onouchi, Susumu Katsushika, Ryo Matsuoka, Hiroki Shinohara, Toshihiro Yamaguchi, Satoshi Kodera, Yasutomi Higashikuni, Katsuhito Fujiu, Hiroshi Akazawa, Nobuo Iguchi, Mitsuaki Isobe, Tsutomu Yoshikawa, and Issei Komuro. Deep learning-based age estimation from chest X-rays indicates cardiovascular prognosis. Communications Medicine, 2(1):1\u201312, December 2022. ISSN 2730-664X. doi: 10.1038/s43856-022-00220-6. Number: 1 Publisher: Nature Publishing Group. ", "page_idx": 12}, {"type": "text", "text": "Kevin G Jamieson and Robert Nowak. Active ranking using pairwise comparisons. Advances in neural information processing systems, 24, 2011. ", "page_idx": 12}, {"type": "text", "text": "Ikbeom Jang, Garrison Danley, Ken Chang, and Jayashree Kalpathy-Cramer. Decreasing annotation burden of pairwise comparisons with human-in-the-loop sorting: Application in medical image artifact rating. arXiv preprint arXiv:2202.04823, 2022. ", "page_idx": 12}, {"type": "text", "text": "Kwang-Sung Jun, Lalit Jain, Blake Mason, and Houssam Nassif. Improved confidence bounds for the linear logistic model and applications to bandits. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5148\u20135157. PMLR, 18\u201324 Jul 2021. ", "page_idx": 12}, {"type": "text", "text": "Maurice George Kendall. Rank correlation methods. Griffin, 1948. ", "page_idx": 12}, {"type": "text", "text": "Andreas Kirsch and Yarin Gal. Unifying approaches in active learning and active sampling via fisher information and information-theoretic quantities. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. Expert Certification.   \nBranislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and Craig Boutilier. Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence and Statistics, pages 2066\u20132076. PMLR, 2020.   \nAndrew Larkin, Ajay Krishna, Lizhong Chen, Ofer Amram, Ally R. Avery, Glen E. Duncan, and Perry Hystad. Measuring and modelling perceptions of the built environment for epidemiological research using crowd-sourcing and image-based deep learning models. Journal of Exposure Science & Environmental Epidemiology, 32(6):892\u2013899, November 2022. ISSN 1559-064X. doi: 10.1038/s41370-022-00489-8. Number: 6 Publisher: Nature Publishing Group.   \nTor Lattimore and Csaba Szepesv\u00e1ri. Bandit Algorithms. Cambridge University Press, 2020. doi: 10.1017/9781108571401.   \nLihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In International Conference on Machine Learning, pages 2071\u20132080. PMLR, 2017.   \nMats Lid\u00e9n, Antoine Spahr, Ola Hjelmgren, Simone Bendazzoli, Josefin Sundh, Magnus Sk\u00f6ld, G\u00f6ran Bergstr\u00f6m, Chunliang Wang, and Per Thunberg. Machine learning slice-wise whole-lung CT emphysema score correlates with airway obstruction. European Radiology, 34(1):39\u201349, January 2024. ISSN 1432-1084. doi: 10.1007/s00330-023-09985-3.   \nSuiyi Ling, Jing Li, Anne Flore Perrin, Zhi Li, Luk\u00e1\u0161 Krasula, and Patrick Le Callet. Strategy for boosting pair comparison and improving quality assessment accuracy. arXiv preprint arXiv:2010.00370, 2020.   \nBo Long, Olivier Chapelle, Ya Zhang, Yi Chang, Zhaohui Zheng, and Belle Tseng. Active learning for ranking through expected loss optimization. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 267\u2013274, 2010.   \nHao Lou, Tao Jin, Yue Wu, Pan Xu, Quanquan Gu, and Farzad Farnoud. Active ranking without strong stochastic transitivity. Advances in neural information processing systems, 35:297\u2013309, 2022.   \nAndrew K Massimino and Mark A Davenport. As you like it: Localization via paired comparisons. Journal of Machine Learning Research, 22(186):1\u201339, 2021.   \nLucas Maystre and Matthias Grossglauser. Just sort it! a simple and effective approach to active preference learning. In International Conference on Machine Learning, pages 2344\u20132353. PMLR, 2017.   \nViraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, and Willie Neiswanger. Sample efficient reinforcement learning from human feedback via active exploration. arXiv preprint arXiv:2312.00267, 2023.   \nTom Minka, Ryan Cleven, and Yordan Zaykov. Trueskill 2: An improved bayesian skill rating system. Technical Report, 2018.   \nSubhojyoti Mukherjee, Anusha Lalitha, Kousha Kalantari, Aniket Deshmukh, Ge Liu, Yifei Ma, and Branislav Kveton. Optimal design for human feedback, 2024.   \nNikhil Naik, Jade Philipoom, Ramesh Raskar, and C\u00e9sar Hidalgo. Streetscore-predicting the perceived safety of one million streetscapes. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 779\u2013785, 2014.   \nIFD Oliveira, S Zehavi, and O Davidov. Stochastic transitivity: Axioms and models. Journal of Mathematical Psychology, 85:25\u201335, 2018.   \nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.   \nNikita Pavlichenko and Dmitry Ustalov. Imdb-wiki-sbs: An evaluation dataset for crowdsourced pairwise comparisons. CoRR, abs/2110.14990, 2021.   \nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \nAndrew S. Phelps, David M. Naeger, Jesse L. Courtier, Jack W. Lambert, Peter A. Marcovici, Javier E. Villanueva-Meyer, and John D. MacKenzie. Pairwise comparison versus Likert scale for biomedical image assessment. AJR. American journal of roentgenology, 204(1):8\u201314, 2015. ISSN 0361-803X. doi: 10.2214/ajr.14.13022.   \nLi Qian, Jinyang Gao, and HV Jagadish. Learning user preferences by adaptive pairwise comparison. Proceedings of the VLDB Endowment, 8(11):1322\u20131333, 2015.   \nChao Qin. Open problem: Optimal best arm identification with fixed-budget. In Conference on Learning Theory, pages 5650\u20135654. PMLR, 2022.   \nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ abs/1908.10084.   \nPaat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395\u2013411, 2010.   \nAadirupa Saha. Optimal algorithms for stochastic contextual preference bandits. Advances in Neural Information Processing Systems, 34:30050\u201330062, 2021.   \nAadirupa Saha and Akshay Krishnamurthy. Efficient and optimal algorithms for contextual dueling bandits under realizability. In International Conference on Algorithmic Learning Theory, pages 968\u2013994. PMLR, 2022.   \nAndreza Sartori, Victoria Yanulevskaya, Almila Akdag Salah, Jasper Uijlings, Elia Bruni, and Nicu Sebe. Affective analysis of professional and amateur abstract paintings using statistical analysis and art theory. ACM Transactions on Interactive Intelligent Systems (TiiS), 5(2):1\u201327, 2015.   \nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Unified Embedding for Face Recognition and Clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 815\u2013823, June 2015. doi: 10.1109/CVPR.2015.7298682. arXiv:1503.03832 [cs].   \nJack Sherman and Winifred J. Morrison. Adjustment of an Inverse Matrix Corresponding to a Change in One Element of a Given Matrix. The Annals of Mathematical Statistics, 21(1):124 \u2013 127, 1950. doi: 10.1214/aoms/1177729893.   \nRodrigo M Silva, Marcos A Gon\u00e7alves, and Adriano Veloso. A two-stage active learning method for learning to rank. Journal of the Association for Information Science and Technology, 65(1): 109\u2013128, 2014.   \nMax Simchowitz, Kevin Jamieson, and Benjamin Recht. The simulator: Understanding adaptive sampling in the moderate-confidence regime. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 1794\u20131834. PMLR, 07\u201310 Jul 2017. URL https://proceedings. mlr.press/v65/simchowitz17a.html.   \nAnkita Singh and Shayok Chakraborty. Deep active learning with relative label feedback: An application to facial age estimation. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2021.   \nHanna T\u00e4rn\u00e5sen and Herman Bergstr\u00f6m. Rank based annotation system for supervised learning in medical imaging. Master\u2019s thesis, Chalmers University of Technology, 2023.   \nDmitry Ustalov, Nikita Pavlichenko, and Boris Tseitlin. Learning from Crowds with Crowd-Kit. Journal of Open Source Software, 9(96):6227, 2024. ISSN 2475-9066. doi: 10.21105/joss.06227.   \nTianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, and Jiantao Jiao. Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment, 2023a.   \nYue Wu, Tao Jin, Hao Lou, Farzad Farnoud, and Quanquan Gu. Borda regret minimization for generalized linear dueling bandits. arXiv preprint arXiv:2303.08816, 2023b.   \nPan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu. Neural contextual bandits with deep representation and shallow exploration. In International Conference on Learning Representations, 2022.   \nXinyi Yan, Chengxi Luo, Charles LA Clarke, Nick Craswell, Ellen M Voorhees, and Pablo Castells. Human preferences as dueling bandits. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 567\u2013577, 2022.   \nMiao Yang, Ge Yin, Yixiang Du, and Zhiqiang Wei. Pair comparison based progressive subjective quality ranking for underwater images. Signal Processing: Image Communication, 99:116444, 11 2021. ISSN 09235965. doi: 10.1016/j.image.2021.116444.   \nGeorgios N. Yannakakis and H\u00e9ctor P. Mart\u00ednez. Ratings are overrated! Frontiers in ICT, 2, July 2015. doi: 10.3389/fict.2015.00013.   \nYisong Yue and Thorsten Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1201\u20131208, 2009.   \nYisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The $\\boldsymbol{\\mathrm{k}}$ -armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):1538\u20131556, 2012.   \nXiaohang Zhang, Guoliang Li, and Jianhua Feng. Crowdsourced top- $\\cdot\\mathbf{k}$ algorithms: an experimental evaluation. Proceedings of the VLDB Endowment, 9(8):612\u2013623, April 2016. ISSN 2150-8097. doi: 10.14778/2921558.2921559. URL https://dl.acm.org/doi/10.14778/ 2921558.2921559.   \nBanghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise or $\\mathrm{k}$ -wise comparisons. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 43037\u201343067. PMLR, 23\u201329 Jul 2023. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "PSLH5q7PFo/tmp/e60d4c9327e7cfef9d1ea12f86721e0f9aecd327d79db441969ca6f69b59f9ef.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Algorithms ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 MLE estimator for logistic regression ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The log-likelihood $L_{t}(\\theta)$ of data $D_{t}\\,=\\,\\{(i_{s},j_{s},c_{s})\\}_{s=1}^{t}$ , with $z_{s}\\,=\\,x_{i_{s}}\\,-\\,x_{j_{s}}$ , under a logistic regression model with parameters $\\theta$ is defined by ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{t}(\\theta)=\\sum_{s=1}^{t}\\left(c_{s}\\log\\sigma(\\theta^{\\top}z_{s})+(1-c_{s})(1-\\sigma(\\theta^{\\top}z_{s}))\\right)\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The maximum likelihood estimator (MLE) at time $t$ is the parameters ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\theta_{t}=\\arg\\operatorname*{max}_{\\theta}L_{t}(\\theta)\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The regularized estimator with ridge $\\slash\\ell_{2}$ penalty with parameter $\\lambda$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\theta_{t}^{R}=\\arg\\operatorname*{min}_{\\theta}-L_{t}(\\theta)+\\lambda\\|\\theta\\|_{2}^{2}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.2 Bayesian estimator for logistic regression ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "$\\theta_{B,t}$ is the MAP estimate of $\\theta$ at time $t$ according to the log likelihood ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\theta_{B,t}=\\operatorname*{arg\\,max}_{\\theta}\\ln{p(\\theta\\mid D_{t})},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln p(\\theta\\mid D_{t})=-\\ \\frac{1}{2}(\\theta-\\theta_{B,0})^{\\top}\\mathbf{H}_{B,0}^{-1}(\\theta-\\theta_{B,0})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\displaystyle\\sum_{t}c_{t}\\ln(\\sigma(z_{i_{t},j_{t}}^{\\top}\\theta))+(1-c_{t})\\ln(1-\\sigma(z_{i_{t},j_{t}}^{\\top}\\theta))+c o n s t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The hessian at time $t$ is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{H}_{B,t}=\\mathbf{H}_{B,0}+\\sum_{(i,j)\\in D_{t}}\\dot{\\sigma}(z_{i,j}^{\\top}\\theta_{B,t})z_{i,j}z_{i,j}^{\\top}=\\mathbf{H}_{B,0}+\\mathbf{H}_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, if priors $\\theta_{B,0}=\\mathbf{0}$ and $\\mathbf{H}_{B,0}^{-1}=I_{d}$ are used, the log likelihood boils down to: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ln p(\\theta\\mid D_{t})=-\\,\\frac{1}{2}||\\theta||_{2}^{2}+\\sum_{t}c_{t}\\ln(\\sigma(z_{i_{t},j_{t}}^{\\top}\\theta))+(1-c_{t})\\ln(1-\\sigma(z_{i_{t},j_{t}}^{\\top}\\theta))+c o n s t.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that the MAP estimate will be the same as the MLE estimate with ridge regularisation in the frequentist setting. Similarly, the Hessian becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{H}_{B,t}=\\mathbf{H}_{t}+I_{d}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Sequential updates are also possible in the Bayesian setting by using your current estimates as the new priors. Note that this will give slightly different results, as the calculation of $\\mathbf{H}_{B,t}$ depends on the current estimate of \u03b8B,t. ", "page_idx": 17}, {"type": "text", "text": "B.3 Stochastic Bayesian uncertainty reduction (BayesGURO) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We describe BayesGURO, a Bayesian sampling criterion, closely related to GURO. Consider a Bayesian model of the parameter $\\theta$ with $p(\\theta)$ the prior belief and $p(\\theta\\mid D_{t})$ the posterior after observing the preference feedback in $D_{t}$ . A natural strategy for learning more about the ordering of $\\mathcal{T}$ is to sample items $i_{t},j_{t}$ based on an estimate of the posterior variance of predictions for their comparison, ", "page_idx": 17}, {"type": "equation", "text": "$$\ni_{t},j_{t}=\\underset{i,j\\in\\mathbb{Z}_{D},i<j}{\\arg\\operatorname*{max}}\\,\\,\\hat{\\mathbb{V}}_{\\theta|D_{t-1}}[\\sigma(\\theta^{\\top}z_{i j})]\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, $\\hat{\\mathbb{V}}_{\\theta\\mid D_{t}}[\\sigma(\\theta^{T}z_{i j})]$ is an estimate of the variance of probabilities $\\sigma(\\theta^{T}z_{i j})$ , computed from finite samples drawn from the posterior of $\\theta$ . Estimating the variance in this way both i) allows for tractable implementation, and ii) induces controlled stochasticity in the selection of item pairs. This can be useful in batched learning settings so that multiple pairs can be sampled within the same batch. A deterministic criterion would return the same item pair every time until $\\theta$ is updated. We refer to the sampling criterion in (7) as BayesGURO. ", "page_idx": 17}, {"type": "text", "text": "For the logistic model considered in Section 4, using Laplace approximation with a Normal prior $\\mathcal{N}(0,\\mathbf{H}_{B,0}^{-1})$ on $\\theta$ , the Bayesian criterion in (7) is related to the GURO sampling criterion in (5) through the first-order Taylor expansion of the variance: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Psi_{\\theta|D_{t}}(\\sigma(\\theta^{\\top}z_{i j}))\\approx(\\dot{\\sigma}(\\mathbb{E}_{\\theta|D_{t}}[\\theta^{\\top}z_{i j}]))^{2}\\Psi_{\\theta|D_{t}}[\\theta^{\\top}z_{i j}]=(\\dot{\\sigma}(\\theta_{B,t}^{\\top}z_{i j})||z_{i j}||_{\\mathbf{H}_{B,t}^{-1}(\\theta_{B,t})})^{2}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(wfuhretrhee $\\theta_{B,t}$ s icsr itbheed  MinA PA pespteinmdaitxe  oBf. $\\theta$ .a t Ttihmuse, $t$ t ao nad $\\mathbf{H}_{B,t}$ risd etrh ea pHpersosxiiamn aatdijouns,t efdo rb ay  tlahreg per inour mcboevra rioaf npcoe $\\mathbf{H}_{B,0}^{-1}$ samples, the GURO and BayesGURO active learning criteria are equivalent, save for the influence of the prior. In practice, we find that the Bayesian variant lends itself well to sequential updates of the posterior. The choice of prior $p(\\theta)$ , which could be useful under strong domain knowledge, and the stochasticity of using few posterior samples to approximate $\\mathbb{V}$ make the two criteria distinct. ", "page_idx": 18}, {"type": "text", "text": "B.4 Uniform sampling ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The uniform sampling algorithm is given in Algorithm 2. The corresponding Bayesian version replaces line 5 with the MAP estimate. ", "page_idx": 18}, {"type": "table", "img_path": "PSLH5q7PFo/tmp/701ffc2f2cbf73349085cf75f31e7563f74ee9432daea144748707fa2cbe13b2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.5 BALD ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Algorithm 3 BALD bandit   \nRequire: Training items $\\mathcal{Z}_{D}$ , attributes $\\mathbf{X}=\\{x_{i}\\}_{i\\in\\mathbb{Z}_{d}}$   \n1: Initialize $\\theta_{B,0}=\\mathbf{0}$ , $\\mathbf{H}_{B,0}=\\lambda^{-1}I$   \n2: for $t=1,...,T$ do   \n3: Draw $\\begin{array}{r}{(i_{t},j_{t})=\\arg\\operatorname*{max}_{i,j}H[y_{\\cdot}\\mid z_{i,j},D_{t-1}]-\\mathbb{E}_{\\theta\\sim p(\\theta\\mid D_{t-1})}[H[y\\mid z_{i,j},\\theta]]}\\end{array}$   \n4: Observe $c_{t}$ from noisy comparison (annotator)   \n5: $D_{t}=D_{t-1}\\cup\\left\\{i_{t},j_{t},c_{t}\\right)\\right\\}$   \n6: Let $\\theta_{t}=\\mathrm{MAP}(D_{t})$   \n7: Update $\\begin{array}{r}{\\mathbf{H}_{B,t}\\leftarrow\\mathbf{H}_{B,0}+\\sum_{(i,j)\\in D_{t}}\\dot{\\sigma}(z_{i,j}^{\\top}\\theta_{t})z_{i,j}z_{i,j}^{\\top}}\\end{array}$   \n8: end for   \n9: Return $h_{T}$ ", "page_idx": 18}, {"type": "text", "text": "Where the posterior is calculated as in Appendix B.2 and $H[y\\mid z_{i,j},D_{t-1}]-\\mathbb{E}_{\\theta\\sim p(\\theta|D_{t-1})}[H[y\\mid z_{i,j},\\theta]]$ is approximated as in Appendix B.5.1. ", "page_idx": 18}, {"type": "text", "text": "B.5.1 Deriving the BALD sampling criterion ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The BALD criteria formalized using our notation becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underset{i,j}{\\arg\\operatorname*{max}}\\;H[y\\mid z_{i,j},D_{t}]-\\mathbb{E}_{\\theta\\sim p(\\theta|D_{t})}[H[y\\mid z_{i,j},\\theta]],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $H$ represents Shannon\u2019s entropy ", "page_idx": 18}, {"type": "equation", "text": "$$\nh(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first term of the equation becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\nH[y\\mid z_{i j},D_{t}]=h(\\operatorname*{Pr}(y\\mid z_{i,j},D_{t}))=h\\left(\\int\\operatorname*{Pr}(y\\mid z_{i,j},\\theta)\\operatorname*{Pr}(\\theta\\mid D_{t})d\\theta\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here $\\operatorname*{Pr}(y\\mid z_{i,j},D_{t})$ is the predictive distribution for our Bayesian logistic regression model. As covered in Bishop and Nasrabadi (2006, Chapter 4), this expectation cannot be evaluated analytically but can be approximated using the probit function $\\Phi$ ; ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(y\\mid z_{i j},D_{t})\\approx\\Phi\\left(\\frac{\\theta_{t}^{\\top}z_{i,j}}{\\sqrt{\\lambda^{-2}+||z_{i j}||_{\\mathbf{H}_{t}^{-1}}^{2}}}\\right)\\approx\\sigma\\left(\\frac{\\theta_{t}^{\\top}z_{i,j}}{\\sqrt{1+\\frac{\\pi||z_{i j}||_{\\mathbf{H}_{t}^{-1}(\\theta_{*})}^{2}}{8}}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, the term $\\mathbb{E}_{\\theta\\sim p(\\theta|D_{t})}\\big[H[y\\mid z_{i,j},\\theta]\\big]$ must be calculated. The true definition is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim p(\\theta|D_{t})}[H[y\\mid z_{i,j},\\theta]]=\\int h(\\sigma(\\theta^{\\top}z_{i,j}))\\mathcal{N}(\\theta\\mid\\theta_{t},\\mathbf{H}_{t}^{-1})d\\theta.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To make this a one variable integral, let $X=\\theta^{\\top}z_{i,j}$ define a new random variable. Since $\\theta\\sim\\mathcal{N}(\\theta_{t},\\mathbf{H}_{t}^{-1})$ , and $z_{i,j}$ is just a constant vector, we know that $X$ will follow a univariate normal distribution $\\textit{X}\\sim$ $\\mathcal{N}(\\boldsymbol{\\theta}_{t}^{\\top}\\boldsymbol{z}_{i,j},||\\boldsymbol{z}_{i j}||_{\\mathbf{H}_{t}^{-1}}^{2})$ . This allows us to rewrite the integral as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int h(\\sigma(\\theta^{T}\\mathbf{z}))\\mathcal{N}(\\theta\\mid\\theta_{t},\\mathbf{H}_{t}^{-1})d\\theta=\\int h(\\sigma(x))\\mathcal{N}(\\theta_{t}^{\\top}z_{i,j},\\|z_{i j}\\|_{\\mathbf{H}_{t}^{-1}}^{2})d x.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "However, this integral has no closed form solution. Instead we perform the same strategy as in Houlsby et al. (2011) and do a Taylor expansion of $\\ln h(\\sigma(\\boldsymbol{\\theta}^{\\top}\\mathbf{z}))$ . The third-order Taylor expansion gives us ", "page_idx": 19}, {"type": "equation", "text": "$$\nh(\\sigma(x))\\approx\\exp{\\left(-{\\frac{x^{2}}{8\\ln{2}}}\\right)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Inserting this, the term can be approximated as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int h(\\sigma(x))N(x\\mid\\theta_{t}^{\\top}z_{i,j},\\lVert z_{i j}\\rVert_{\\mathbf{H}_{t}^{-1}}^{2})d x\\approx\\int\\exp{\\left(-\\frac{x^{2}}{8\\ln{2}}\\right)}\\mathcal{N}(x\\mid\\theta_{t}^{\\top}z_{i,j},\\lVert z_{i j}\\rVert_{\\mathbf{H}_{t}^{-1}}^{2})d x}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{C}{\\sqrt{\\lVert z_{i j}\\rVert_{\\mathbf{H}_{t}^{-1}}^{2}+C^{2}}}\\exp{\\left(-\\frac{(\\theta_{t}^{\\top}z_{i,j})^{2}}{2(\\lVert z_{i j}\\rVert_{\\mathbf{H}_{t}^{-1}}^{2}+C^{2})}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C=\\sqrt{4\\ln{2}}$ . Finally, we arrive at an estimation of the objective function we wish to maximize: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H[y\\mid z_{i,j},D_{t}]-\\mathbb{E}_{\\theta\\sim p(\\theta|D_{t})}[H[y\\mid z_{i,j},\\theta]]\\approx h\\left(\\sigma\\left(\\frac{\\theta_{t}^{\\top}z_{i,j}}{\\sqrt{1+\\frac{\\pi}{8}\\left||z_{i}|\\right|_{\\mathbf{H}_{t}^{-1}}^{2}}}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\ \\frac{C}{\\sqrt{||z_{i j}||_{\\mathbf{H}_{t}^{-1}}^{2}+C^{2}}}\\exp\\left(-\\frac{(\\theta_{t}^{\\top}z_{i,j})^{2}}{(||z_{i j}||_{\\mathbf{H}_{t}^{-1}}^{2}+C^{2})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C Proofs of Lemma 1 and Theorem 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. We now proceed to bound ", "page_idx": 20}, {"type": "equation", "text": "$$\nP\\left(|\\sigma(z_{i j}^{\\top}\\theta_{t})-\\sigma(z_{i j}^{\\top}\\theta_{*})|>\\Delta\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From the self-concordant property of logistic regression we have (Faury et al., 2020) ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\sigma(z_{i j}^{\\top}\\theta_{t})-\\sigma(z_{i j}^{\\top}\\theta_{*})|\\leq\\dot{\\sigma}(z_{i j}\\top\\theta_{t})|z_{i j}^{\\top}(\\theta_{t}-\\theta_{*})|+\\frac{1}{4}|z_{i j}^{\\top}(\\theta_{t}-\\theta_{*})|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We will prove a high probability bound on the event ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\dot{\\sigma}(z_{i j}\\top\\theta_{t})|z_{i j}^{\\top}(\\theta_{t}-\\theta_{*})|+\\frac{1}{4}|z_{i j}^{\\top}(\\theta_{t}-\\theta_{*})|^{2}\\leq\\Delta.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Directly trying to bound the LHS in Equation 13 will result in a rather messy expression. Instead, we define the events ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathcal E_{1}:=\\left\\lbrace\\dot{\\sigma}(z_{i j}\\top\\theta_{t})|z_{i j}^{\\top}(\\theta_{t}-\\theta_{*})|\\leq\\frac{\\Delta}{2}\\right\\rbrace}\\\\ {\\displaystyle\\mathcal E_{2}:=\\left\\lbrace\\frac{1}{4}|z_{i j}^{\\top}(\\theta_{t}-\\theta_{*})|^{2}\\leq\\frac{\\Delta}{2}\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Clearly $\\mathcal{E}_{1}\\left\\cup\\mathcal{E}_{2}\\right.$ implies the expression in Equation 13. Assume we have bounds on the complement of these events, $P\\left(\\mathcal{E}_{1}^{c}\\right)\\leq\\alpha$ and $P\\left(\\mathcal{E}_{2}^{c}\\right)\\leq\\beta$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P\\left(|\\sigma(z_{i j}^{\\top}\\theta_{t})-\\sigma(z_{i j}^{\\top}\\theta_{*})|>\\Delta\\right)\\le\\alpha+\\beta+\\alpha\\beta}\\\\ {\\le2\\alpha+2\\beta.\\qquad\\qquad\\qquad\\qquad\\le2\\alpha+2\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now proceed to bound the probability of these complements separately. ", "page_idx": 20}, {"type": "text", "text": "Step 1. Relating $\\theta_{t}$ to $\\boldsymbol{\\theta}_{*}$ : The first challenge in our analysis to is relate $\\theta_{*}$ and $\\theta_{t}$ . In contrast to linear regression, where we have a closed-form expression for $\\theta_{t}$ , there is no analytical solution for $\\theta_{t}$ given a set of observation. However, we know that $\\theta_{t}$ is the MLE, corresponding to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta_{t}=\\operatorname*{arg\\,max}_{\\theta}L_{t}(\\theta)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\nL_{t}(\\theta)=\\sum_{s=1}^{t}c_{s}\\log\\sigma\\left(z_{s}^{\\top}\\theta\\right)+\\left(1-c_{s}\\right)\\log\\left(1-\\sigma\\left(z_{s}^{\\top}\\theta\\right)\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}L_{t}(\\theta)=\\sum_{s=1}^{t}c_{s}z_{s}-\\underbrace{\\sum_{s=1}^{t}\\sigma\\left(z_{s}^{\\top}\\theta\\right)z_{s}}_{g_{t}(\\theta)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and hence $\\begin{array}{r}{g_{t}(\\theta_{t})=\\sum_{s=1}^{t}c_{s}z_{s}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "A standard trick in logistic bandits (Filippi et al., 2010; Faury et al., 2020; Jun et al., 2021) is to relate $\\theta_{*}-\\theta_{t}$ to $g_{t}\\big(\\theta_{*}\\big)-g_{t}\\big(\\theta_{t}\\big)$ . Especially, the following equality is due to the mean-value theorem (see Filippi et al. (2010)) ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{t}(\\theta_{*})-g_{t}(\\theta_{t})=\\mathbf{H}_{t}(\\theta^{\\prime})\\left(\\theta_{*}-\\theta_{t}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\theta^{\\prime}$ is some convex combination of $\\theta_{\\ast},\\theta_{t}$ . Note that $\\mathbf{H}_{t}(\\theta^{\\prime})$ has full rank ", "page_idx": 20}, {"type": "text", "text": "Using Equation 14 yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|z_{i j}^{\\top}\\left(\\boldsymbol{\\theta}_{*}-\\boldsymbol{\\theta}_{t}\\right)\\right|=\\left|z_{i j}^{\\top}\\mathbf{H}_{t}^{-1}(\\boldsymbol{\\theta}^{\\prime})\\left(g_{t}(\\boldsymbol{\\theta}_{*})-g_{t}(\\boldsymbol{\\theta}_{t})\\right)\\right|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, since $\\begin{array}{r}{g_{t}(\\theta_{t})=\\sum_{s=1}^{t}c_{s}z_{s}}\\end{array}$ , due to $\\nabla_{\\theta}L_{t}(\\theta_{t})=0$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{t}(\\theta_{t})-g_{t}(\\theta_{*})=\\sum_{s=1}^{t}\\underbrace{\\Big(c_{s}-\\sigma\\left(z_{s}^{\\top}\\theta_{*}\\right)\\Big)}_{\\epsilon_{s}}z_{s}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\epsilon_{s}$ is a sub-Gaussian random variable with mean 0 and variance $\\nu_{s}^{2}:=\\dot{\\sigma}\\left(z_{s}^{\\top}\\theta_{*}\\right)$ . We define ", "page_idx": 21}, {"type": "equation", "text": "$$\nS_{t}:=\\sum_{s=1}^{t}\\epsilon_{s}z_{s}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We now have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|z_{i j}^{\\top}\\left(\\theta_{\\ast}-\\theta_{t}\\right)\\right|=\\left|z_{i j}^{\\top}\\mathbf{H}_{t}^{-1}(\\theta^{\\prime})S_{t}\\right|\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and Lemma 10 in Faury et al. (2020) states that $\\mathbf{H}_{t}^{-1}(\\boldsymbol{\\theta}^{\\prime})\\preccurlyeq(1+2S)\\mathbf{H}_{t}^{-1}(\\boldsymbol{\\theta}_{\\ast})$ where $||\\theta_{*}||_{2}\\leq S$ . Hence, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|z_{i j}^{\\top}\\left(\\theta_{*}-\\theta_{t}\\right)\\right|\\leq\\left(1+2S\\right)\\left|z_{i j}^{\\top}\\mathbf{H}_{t}^{-1}(\\theta_{*})S_{t}\\right|\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Step 2. Tail bound for vector-valued martingales: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We will now prove an upper bound on the probability that $\\left|z_{i j}^{\\top}\\mathbf{H}_{t}^{-1}(\\boldsymbol{\\theta}_{*})S_{t}\\right|$ deviates much from a certain threshold. This step is based on the proof of Lemma 1 in Filippi et al. (2010) which itself is based on a derivation of a concentration inequality in Rusmevichientong and Tsitsiklis (2010). The difference compared to Filippi et al. (2010) is that we work with the Hessian $\\mathbf{H}_{t}(\\theta_{*})$ instead of the design matrix for linear regression $\\begin{array}{r}{V_{t}=\\bar{\\sum_{s}}x_{s}x_{s}^{\\top}}\\end{array}$ . This require us to construct a slightly different martingale. ", "page_idx": 21}, {"type": "text", "text": "Let $A$ and $B$ are two random variables such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left\\{\\gamma A-\\frac{\\gamma^{2}}{2}B^{2}\\right\\}\\right]\\leq1,\\forall\\gamma\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then due to Corollary 2.2 in de la Pe\u00f1a et al. (2004) it holds that $\\forall a\\ge\\sqrt{2}$ and $b>0$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nP\\left(|A|\\geq a{\\sqrt{(B^{2}+b)\\left(1+{\\frac{1}{2}}\\log\\left({\\frac{B^{2}}{b}}+1\\right)\\right)}}\\right)\\leq\\exp\\left\\{{\\frac{-a^{2}}{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\eta\\in\\mathbb{R}^{d}$ and consider the process ", "page_idx": 21}, {"type": "equation", "text": "$$\nM_{t}^{\\gamma}(\\theta_{*},\\eta):=\\exp\\left\\{\\gamma\\eta^{\\top}S_{t}-\\gamma^{2}||\\eta||_{\\mathbf{H}_{t}(\\theta_{*})}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We will now proceed to prove that $M_{t}^{\\gamma}(\\theta,\\eta)$ is a non-negative super martingale satisfying Equation 15. Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma\\eta^{\\top}S_{t}-\\gamma^{2}\\vert\\vert\\eta\\vert\\vert_{\\mathbf H_{t}(\\theta_{*})}^{2}=\\sum_{s=1}^{t}\\underbrace{\\left(\\gamma\\eta^{\\top}z_{s}\\epsilon_{s}-\\dot{\\sigma}(\\theta^{\\top}z_{s})\\gamma^{2}\\left(\\eta^{\\top}z_{s}\\right)^{2}\\right)}_{F_{s}}=\\sum_{s=1}^{t}F_{s}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Further we use the fact that $\\epsilon_{s}$ is sub-Gaussian with parameter $\\nu_{s}$ , .i.e, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\{\\lambda\\epsilon_{s}\\}\\right]\\leq\\exp\\left\\{\\nu_{s}^{2}\\lambda^{2}\\right\\},\\forall\\lambda>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $D_{s-1}$ denote the observations up until time $s$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp\\{F_{s}\\}\\mid D_{s-1}\\right]=\\mathbb{E}\\left[\\exp\\left\\{\\underbrace{\\gamma\\eta^{\\top}z_{s}}_{\\lambda}\\epsilon_{s}\\right\\}\\right]\\exp\\left\\{-\\underbrace{\\dot{\\sigma}(\\theta_{t}^{\\top}z_{s})}_{\\nu_{s}^{2}}\\gamma^{2}\\left(\\eta^{\\top}z_{s}\\right)^{2}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp\\left\\{\\nu_{s}^{2}\\lambda^{2}\\right\\}\\exp\\left\\{-\\nu_{s}^{2}\\lambda^{2}\\right\\}=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This also implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[M_{t}^{\\gamma}(\\theta_{*},\\eta)\\ |\\ D_{t-1}\\right]\\leq M_{t-1}^{\\gamma}(\\theta_{*},\\eta)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and $M_{t}^{\\gamma}(\\theta_{*},\\eta)$ is a super-martingale satisfying ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left\\{\\gamma\\eta^{\\top}S_{t}-\\gamma^{2}||\\eta||_{\\mathbf{H}_{t}(\\theta_{*})}^{2}\\right\\}\\right]\\leq1,\\forall\\gamma\\geq0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and we can apply the results of de la Pe\u00f1a et al. (2004). ", "page_idx": 21}, {"type": "text", "text": "We now follow the last step of the proof of Lemma 1 in Filippi et al. (2010). We let $\\begin{array}{r}{a=\\sqrt{2\\log\\frac{1}{\\delta}}}\\end{array}$ for some $\\delta\\in(0,1/e)$ and let $b=\\lambda_{0}\\|\\eta\\|_{2}^{2}$ . We have with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\eta^{\\top}S_{t}|\\leq\\sqrt{2\\log\\frac{1}{\\delta}}\\sqrt{\\|\\eta\\|_{\\mathbf{H}_{t}(\\theta_{*})+\\lambda_{0}\\|\\eta\\|_{2}^{2}}^{2}\\left(1+\\frac{1}{2}\\log\\left(1+\\frac{\\|\\eta\\|_{\\mathbf{H}_{t}(\\theta_{*})}^{2}}{\\lambda_{0}\\|\\eta\\|_{2}^{2}}\\right)\\right)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Rearanging and using the fact that $\\lambda_{0}||\\eta||_{2}^{2}\\leq\\|\\eta\\|_{\\mathbf{H}_{t}(\\theta_{*})}^{2}\\leq t\\|\\eta\\|_{2}$ yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\eta^{\\top}S_{t}|\\leq\\rho(\\lambda_{0})||\\eta||_{\\mathbf{H}_{t}(\\theta_{*})}\\sqrt{2\\log\\frac{t}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\rho$ is defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho(\\lambda_{0})=\\sqrt{3+2\\log\\left(1+\\frac{4Q^{2}}{\\lambda_{0}}\\right)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We take $M_{t}$ to be a matrix such that $M_{t}^{2}=\\mathbf{H}_{t}(\\theta_{*})$ and note that for any $\\tau>0$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nP\\left(||S_{t}||_{\\mathbf{H}_{t}^{-1}(\\theta_{*})}^{2}\\geq d\\tau^{2}\\right)\\leq\\sum_{i=1}^{d}P\\left(\\left|S_{t}^{\\top}M_{t}^{-1}e_{i}\\right|\\geq\\tau\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $e_{i}$ is the i:th unit vector. Equation 18 with $\\eta=M_{t}^{-1}e_{i}$ together with $||M_{t}^{-1}e_{i}||_{\\mathbf{H}_{t}(\\theta_{*})}=1$ yield that the following holds with with probability at least $1-\\delta$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n||S_{t}||_{\\mathbf{H}_{t}^{-1}(\\theta_{*})}\\leq\\rho(\\lambda_{0})\\sqrt{2d\\log t}\\sqrt{\\log\\frac{d}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Step 3. (Unverifiable) High-probability bounds on ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ . ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We now have enough machinery to state high-probability bounds for our two events. These bounds will be unverifiable in the sense that the depend on the true parameter $\\theta_{*}$ which is not known to us during runtime. We derive verifiable bounds in the next step of the proof. ", "page_idx": 22}, {"type": "text", "text": "Recall that $\\mathbf{H}_{t}^{-1}(\\theta_{*})$ is symmetric. We apply Equation 18 with $\\eta\\,=\\,{\\bf H}_{t}^{-1}(\\theta_{*})z_{i j}$ and $\\alpha\\,>\\,0$ in place of $\\delta$ . First, we note that $\\|\\mathbf{H}_{t}^{-1}(\\theta_{*})z_{i j}\\|_{\\mathbf{H}_{t}(\\theta_{*})}=\\|z_{i j}\\|_{\\mathbf{H}_{t}^{-1}(\\theta_{*})}$ which implies with probability at least $1-\\alpha$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|z_{i j}^{\\top}\\mathbf{H}_{t}^{-1}(\\theta_{*})S_{t}\\right|=\\left|S_{t}^{\\top}\\mathbf{H}_{t}^{-1}(\\theta_{*})z_{i j}\\right|\\leq\\rho(\\lambda_{0})\\|z_{i j}\\|_{\\mathbf{H}_{t}^{-1}(\\theta_{*})}\\sqrt{2\\log\\frac{t}{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We solve for smallest possible $\\alpha\\in(0,1/e)$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n(1+2S)\\rho(\\lambda_{0})\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{*})||z_{i j}||_{\\mathbf{H}_{t}^{-1}(\\theta_{*})}\\sqrt{2\\log\\frac{t}{\\alpha}}\\leq\\frac{\\Delta}{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Rearanging yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\alpha\\leq\\exp\\left\\lbrace\\frac{-\\Delta^{2}}{8\\rho^{2}(\\lambda_{0})(1+2S)^{2}\\left(\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{*})||z_{i j}||_{\\mathbf{H}_{t}^{-1}(\\theta_{*}))}\\right)^{2}}+\\log T\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For ${\\mathcal{E}}_{2}$ and the bound on its probability, $\\beta>0$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{4}|z_{i j}^{\\top}(\\theta_{t}-\\theta_{*})|^{2}\\leq\\frac{1}{2}(1+2S)^{2}||z_{i j}||_{\\mathbf{H}_{t}^{-1}(\\theta_{*})}^{2}\\rho^{2}(\\lambda_{0})\\log\\frac{t}{\\beta}\\leq\\frac{\\Delta}{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta\\leq\\exp\\left\\{\\frac{-\\Delta}{\\rho^{2}(\\lambda_{0})(1+2S)^{2}\\left(\\left||z_{i j}\\right|\\right|_{\\mathbf{H}_{t}^{-1}(\\theta_{*}))}\\right)^{2}}+\\log T\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that both Equation 21 and Equation 22 are under the assumption that the RHS satisfy $<1/e$ since this is required in order to apply the results of de la Pe\u00f1a et al. (2004). As we discuss in the main text, these quantities are approaching zero as $O(T e^{-T})$ , ignoring various constants, for reasonable sampling strategies and will satisfy this condition eventually. ", "page_idx": 22}, {"type": "text", "text": "Step 4. (Verifiable) High-probability bounds on ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ . ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The bounds in the previous step depend on the true parameter $\\boldsymbol{\\theta}_{*}$ which we do not have access to in practise. We again use Lemma 10 of Faury et al. (2020) together with Cauchy-Schwartz ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|z_{i j}(\\boldsymbol{\\theta}_{*}-\\boldsymbol{\\theta}_{t})|=\\left|z_{i j}^{\\top}\\mathbf{H}_{t}^{-1/2}(\\boldsymbol{\\theta}^{\\prime})\\mathbf{H}_{t}^{1/2}(\\boldsymbol{\\theta}^{\\prime})\\boldsymbol{S}_{t}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq(1+2S)||z_{i j}||_{\\mathbf{H}_{t}^{-1}(\\boldsymbol{\\theta}_{t})}||S_{t}||_{\\mathbf{H}_{t}^{-1}(\\boldsymbol{\\theta}_{*})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using Equation 19 we have with probability at last $1-\\alpha$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n(1+2S)\\sigma(z_{i j}^{\\top}\\theta_{*})||z_{i j}||_{{\\mathbf{H}}_{t}^{-1}(\\theta_{t})}||S_{t}||_{{\\mathbf{H}}_{t}^{-1}(\\theta_{*})}\\leq(1+2S)\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{*})||z_{i j}||_{{\\mathbf{H}}_{t}^{-1}(\\theta_{t})}\\rho(\\lambda_{0})\\sqrt{2d\\log t}\\sqrt{\\log\\frac{d}{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We solve for smallest $\\alpha\\in(1/e)$ such that Equation 23 is smaller than $\\Delta_{i j}/2$ . This yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\alpha\\leq\\exp\\left\\lbrace\\frac{-\\Delta^{2}}{8d\\rho^{2}(\\lambda_{0})(1+2S)^{2}\\left(\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{T})||z_{i j}||_{{\\mathbf H}_{t}^{-1}(\\theta_{T}))}\\right)^{2}}+\\log d T\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Same steps for $\\beta$ yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\beta\\leq\\exp\\left\\{\\frac{-\\Delta}{d\\rho^{2}(\\lambda_{0})(1+2S)^{2}\\left(\\left||z_{i j}|\\right|_{\\mathbf{H}_{t}^{-1}(\\theta_{T}))}\\right)^{2}}+\\log d T\\right\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For brevity, define $C_{1}=\\rho^{2}(\\lambda_{0})(1+2S)^{2}$ . ", "page_idx": 23}, {"type": "text", "text": "Using the definition of $\\tilde{\\mathbf{H}}_{t}$ yields the statement of Lemma 1. ", "page_idx": 23}, {"type": "text", "text": "C.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. We let $i\\succ j$ denote that $i$ is preferred to $j$ . W.l.o.g assume $1\\succ2\\succ\\dots\\succ n$ . The key observation is thatfor any $i$ and $j$ such that $i<j$ it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta_{i,j}>(j-i)\\Delta_{*}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If we get the wrong relation between $i,j$ then $\\sigma(z_{i j}^{\\top}\\theta_{*})-\\sigma(z_{i j}^{\\top}\\theta_{T})>(j-i)\\Delta_{*}$ . Lemma 1 implies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\sigma(z_{i j}^{\\top}\\theta_{*})-\\sigma(z_{i j}^{\\top}\\theta_{T})>(j-i)\\Delta)\\leq d T(\\underbrace{\\exp\\left\\{\\frac{-(j-i)\\Delta^{2}}{8d\\rho^{2}(\\lambda_{0})(1+2S)^{2}\\left(\\dot{\\sigma}(z_{i j}^{\\top}\\theta_{T})\\|z_{i j}\\|_{H_{t}^{-1}(\\theta_{T}))}\\right)^{2}}\\right\\}}_{\\alpha_{i j}^{j-i}}}\\\\ &{\\qquad+\\underbrace{\\exp\\left\\{\\frac{-(j-i)\\Delta}{d\\rho(\\lambda_{0})(1+2S)^{2}\\left(\\left|\\vert z_{i j}\\right\\vert_{H_{t}^{-1}(\\theta_{T}))}\\right)^{2}}\\right\\}}_{\\beta_{i j}^{j-i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $R(\\theta_{T})$ be the ordering error of the $_n$ items. Then, under a uniform distribution over items we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[R(\\theta_{T})]\\leq\\frac{4d T}{n(n-1)}\\left(\\underbrace{\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}\\alpha_{i j}^{j-i}}_{A}+\\underbrace{\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}\\beta_{i j}^{j-i}}_{B}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$A$ and $B$ will be upper bounded using the same argument. We now upper bound sum $A$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{x t}\\,\\alpha_{*}:=\\exp\\left\\lbrace\\frac{-\\Delta_{*}^{2}}{8d C_{1}\\operatorname*{max}_{i,j}\\,\\delta(z_{i j}^{\\top}\\theta_{T})||z_{i j}||_{\\mathbf{H}_{t}^{-1}(\\theta_{*})}^{2}}\\right\\rbrace\\mathrm{then}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad A\\le\\displaystyle\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}\\alpha_{*}^{j-i}\\le(n-1)\\left(\\displaystyle\\sum_{j=0}^{n}\\alpha_{*}^{j}-1\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le(n-1)\\left(\\displaystyle\\frac{1}{1-\\alpha_{*}}-1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This follows from the definition of $\\delta_{1,*}$ and properties of the geometric sum. It is easy to see that ${\\frac{1}{1-e^{-x}}}-1=$ $\\frac{1}{e^{x}-1}$ . Hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{4d T}{n(n-1)}A\\leq\\frac{4d T}{n}\\left(\\alpha_{*}^{-1}-1\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $B$ we perform the same steps with $\\begin{array}{r}{\\beta_{*}:=\\exp\\left\\lbrace\\frac{-\\Delta_{*}}{d C_{1}\\operatorname*{max}_{i,j}||z_{i j}||_{\\mathbf{H}_{t}^{-1}(\\theta_{*})}^{2}}\\right\\rbrace}\\end{array}$ to get $\\frac{4d T}{n(n-1)}A\\leq\\frac{4d T}{n}\\left(\\beta_{*}^{-1}-1\\right)^{-1}.$ ", "page_idx": 24}, {"type": "text", "text": "Combing yields and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R(\\theta_{T})\\right]\\leq\\frac{4d T}{n}\\left(\\left(\\alpha_{*}^{-1}-1\\right)^{-1}+\\left(\\beta_{*}^{-1}-1\\right)^{-1}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Markov\u2019s inequality we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nP(R(\\theta_{T})\\ge\\epsilon)\\le\\frac{4d T}{\\epsilon n}\\left(\\left(\\alpha_{*}^{-1}-1\\right)^{-1}+\\left(\\beta_{*}^{-1}-1\\right)^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.3 Extensions of current theory ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Regularized estimators. In our analysis in Section 4, we have assumed that $\\theta_{T}$ is the maximum likelihood estimate and that ${\\bf H}(\\theta_{T})$ has full rank. This can be relaxed by considering $\\ell_{2}$ (Ridge) regularization where $\\theta_{\\lambda_{0},T}$ is the optimum of the regularized log-likelihood with regularization $\\lambda_{0}\\mathbf{I}$ and ${\\bf H}_{\\lambda_{0}}(\\theta_{\\lambda_{0},T})\\;=\\;$ $\\begin{array}{r}{\\sum_{s=1}^{T}\\dot{\\sigma}(z_{s}^{\\top}\\theta_{\\lambda_{0},T})z_{s}z_{s}^{\\top}+\\lambda_{0}\\mathbf{I}}\\end{array}$ . The same machinery used to prove Lemma 1 (Filippi et al., 2010; Faury et al., 2020) can be applied to this regularized version with small changes to the final bound. ", "page_idx": 24}, {"type": "text", "text": "Generalized linear models. It is also possible to derive similar results for generalized linear models with other link functions, $\\mu(z_{i j}^{\\top}\\theta_{*})$ , by using the general inequality $\\mathbf{H}(\\theta)\\geq\\kappa^{-1}\\mathbf{V}$ with $\\begin{array}{r}{\\mathbf{V}=\\sum_{s=1}^{T}z_{s}z_{s}^{\\top}}\\end{array}$ and $\\kappa\\geq$ $\\begin{array}{r}{1/\\operatorname*{min}_{z_{i j}}\\,\\dot{\\mu}(z_{i j}^{\\top}\\theta_{*})}\\end{array}$ . We conjecture that this will yield a scaling of $\\sim\\exp(-\\Delta^{2}T/\\kappa)$ where, unfortunately, $\\kappa$ might be very large. For a more thorough discussion on the dependence on $\\kappa$ in generalized linear bandits, see Lattimore and Szepesv\u00e1ri (2020, Chapter 19). ", "page_idx": 24}, {"type": "text", "text": "Lower and algorithm-specific upper bounds on the ordering error. A worst-case lower bound on the ordering error can be constructed in the fixed-confidence setting, where the goal is to minimize the number of comparisons until a correct ordering is found with a given confidence, by following Garivier and Kaufmann (2016). This involves defining the set of alternative models $\\operatorname{Alt}(\\theta_{*})$ which differs from $\\theta_{*}$ in their induced ordering of $\\mathcal{Z}$ . The bound is then constructed by optimizing the frequency of comparisons of each pair of items so that such alternative models are distinguished as much as possible from the true parameter. We have left this result out of the paper as we find it uninformative in the regime when the number of comparisons is small, (see Simchowitz et al. (2017) for a discussion on the limitations of these asymptotic results in the standard bandit setting). Constructing a lower bound for our fixed-budget setting, of learning as good an ordering as possible with a fixed number of comparisons, is much more challenging. The fixed-confidence result yields $a$ bound for the fixed-budget case (Garivier and Kaufmann, 2016), but constructing either a tight lower bound or a tight algorithm-specific upper bound is an open problem (Fang, 2022). ", "page_idx": 24}, {"type": "text", "text": "D Comparison with regret minimization ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Bengs et al. (2022) considered a problem formulation where the goal is to learn a parameter $\\theta$ which determines the utility $Y_{i,t}$ for a set of arms $i=1,...,n$ as a function of observed context vectors $x_{i,t}$ in a sequence of rounds $t=1,...,T$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\nY_{i,t}=\\boldsymbol{\\theta}^{\\top}\\boldsymbol{X}_{i,t}\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The probability that item $i$ is preferred over $j$ (denoted $i\\succ j$ ) in round $t$ is decided through a comparison function $F$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(i\\succ j\\mid X_{i,t},X_{j,t})=F(Y_{i,t}-Y_{j,t})\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The goal in their setting is to, in each round, select two items $\\left(i_{t},j_{t}\\right)$ so that their maximum (or average) utility is as close as possible to the utility of the best item. The expected regret in their average-utility setting is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Re_{B S H}=\\mathbb{E}[\\sum_{t=1}^{T}2Y_{i_{t}^{*},t}-Y_{i_{t},t}-Y_{j_{t},t}]\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proposition 1 (Informal). An algorithm which achieves minimal regret in the setting of Bengs et al. (2022) can perform arbitrarily poorly in our setting. ", "page_idx": 25}, {"type": "text", "text": "Proof. The optimal choice of arm pair in the BSH setting is the optimal and next-optimal arm $(i_{t}^{*},i_{t}^{\\prime})$ such that $i_{t}^{*}\\;\\succ\\;i_{t}^{\\prime}\\;\\succ\\;j$ for any other arms $j$ . Assume that the ordering of all other arms $j$ is determined by a feature $X_{j,t}(k)$ but that $X_{i_{t}^{*},t}(k)\\,=\\,X_{i_{t}^{\\prime},t}(k)$ . Then, no knowledge will be gained about arms other than the top 2 choices under the BSH regret. As the number of arms grows larger, the error in our setting grows as well. ", "page_idx": 25}, {"type": "text", "text": "Saha (2021) study the same average-utility regret setting and give a lower bound under Gumbel noise. Saha and Krishnamurthy (2022) investigated where there is a computationally efficient algorithm that achieves the derived optimality guarantee. ", "page_idx": 25}, {"type": "text", "text": "E Experiment details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For BayesGURO and BALD, the posterior $p(\\theta\\mid D_{t})$ is estimated using the Laplace approximation as described in Bishop and Nasrabadi (2006, Chapter 4). With this approximation, the covariance matrix is the same as the inverse of the Hessian of the log-likelihood. For both methods, the priors $\\boldsymbol{\\theta}_{B,0}=\\mathbf{0}^{d}$ and $\\mathbf{H}_{B,0}^{-1}=I_{d}$ were used, and sequential updates were performed every iteration. The sample criterion for BALD under a logistic model is given in Appendix B.5.1. For BayesGURO, 50 posterior samples were used to estimate $\\hat{\\mathbb{V}}_{\\theta\\mid D_{t}}[\\sigma(\\theta^{T}z_{i j})]$ for every $z_{i j}$ . The hybrid algorithms follow the same structure with the added constraint that each per-item parameter $\\zeta_{i}$ is independent of other parameters. This allows for efficient updates of $\\mathbf{H}_{B,t}^{-1}$ by using sparsity in the covariance. ", "page_idx": 26}, {"type": "text", "text": "GURO, CoLSTIM, and Uniform use LogisticRegression from Scikit-learn (Pedregosa et al., 2011) with default Ridge regularization $C=1$ ) and the lbfgs optimizer. The former two updates $\\theta_{t}$ every iteration using the full history, $D_{t}$ in all experiments except for IMDB-WIKI-SbS, where GURO updates $\\theta_{t}$ every 25th iteration. This caused no noticeable change in performance as GURO still updates $\\mathbf{H}_{t}^{-1}$ every iteration using the ShermanMorrison formula. Note that when using the Sherman-Morrison formula in practice, you only get an estimate of $\\mathbf{H}_{t}^{-1}(\\theta_{t})$ since previous versions have been calculated using older estimates of $\\theta$ . This method for approximating the inverse hessian is covered in Bishop and Nasrabadi (2006, Chapter 5) and when we compared it to calculating $\\mathbf{H}_{t}^{-1}(\\theta_{t})$ from scratch every iteration we observed that the methods performed equally. The design matrix for CoLSTIM is updated as in Bengs et al. (2022): the confidence width $c_{1}$ was chosen to be $\\sqrt{d\\log(T)}$ , and the perturbed values were generated using the standard Gumbel distribution. ", "page_idx": 26}, {"type": "text", "text": "To increase computational efficiency for the large IMDB-WIKI-SbS dataset, the hybrid algorithms did not evaluate all $\\sim\\,100\\ 000$ comparisons at every time step. Instead, a subset of $5\\ 000$ comparisons was first sampled, and the highest-scoring pair in this set was chosen. This resulted in a large speed-up and no noticeable change in performance during evaluation. ", "page_idx": 26}, {"type": "text", "text": "E.1 Datasets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "ImageClarity Data available at https://dbgroup.cs.tsinghua.edu.cn/ligl/crowdtopk. This dataset contained differently distorted versions of the same image. To extract relevant features, we used a ResNet34 model (He et al., 2016) that had been pre-trained on Imagenet (Deng et al., 2009). After PCA projection feature dimensionality was reduced to $d\\,=\\,63$ . The dataset consisted of 100 images and 27 730 comparisons. Since the type of distortion is the same for all images, the dataset has a true ordering with regards to the strength of the distortion applied. ", "page_idx": 26}, {"type": "text", "text": "WiscAdds Data available at https://dataverse.harvard.edu/dataset.xhtml?persistentId= doi:10.7910/DVN/0ZRGEE (license: CC0 1.0). The WiscAdds dataset, containing 935 political texts, has been extended with 9 528 pairwise comparisons by Carlson and Montgomery (2017). In comparisons, annotators have stated which of two texts has a more negative tone toward a political opponent. To extract general features from the text, sentences were embedded using the pre-trained all-mpnet-base-v2 model from the Sentence-Transformers library (Reimers and Gurevych, 2019). After applying PCA to the sentence embeddings, each embedding had a dimensionality of $d=162$ . ", "page_idx": 26}, {"type": "text", "text": "IMDB-WIKI-SbS Data available at https://github.com/Toloka/IMDB-WIKI-SbS (license: CC BY). IMDB-WIKI-SbS consists of close-up images of actors of different ages. For each comparison, the label corresponds to which of two people appears older. The complete dataset consists of 9 150 images and 250 249 comparisons, but images that were grayscale or had a resolution lower than $160\\times160$ were removed, resulting in 6 072 images and 110 349 comparisons. We extract features from each image using the Inception-ResNet implemented in FaceNet (Schroff et al., 2015) followed by PCA, resulting in $d=75$ features per image. ", "page_idx": 26}, {"type": "text", "text": "E.2 Additional figures ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "X-RayAge ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To highlight the importance of the first-order term in Lemma 1, we evaluated NormMin on the same $\\Chi$ -ray ordering task as in Figure 1a. The results, shown in Figure 3a, indicate that not only does the algorithm perform worse than GURO, but is seemingly also outperformed by a uniform sampling strategy. Furthermore, for completeness, we include Figure 3b which shows the in-sample error, $R_{I_{D}}$ , during the generalization experiment. ", "page_idx": 26}, {"type": "text", "text": "Synthetic Example and Illustration of Upper Bound ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this setting, 100 synthetic data points were generated. Each data point consisted of 10 features, where the feature values were sampled according to a standard normal distribution. The true model, $\\theta_{*}$ , was generated by sampling each value uniformly between $-3$ and 3. The pairwise comparison feedback was simulated the same way as in Section 6.1, with $\\lambda=0.5$ . The upper bound of the probability that $R(\\theta_{t})\\geq0.2$ was calculated every iteration according to Theorem 1. Each algorithm was run for 2000 comparisons, updating every 10th, the results of which can be seen in Figure 4. We observe in Figure 4b that our greedy algorithms are seemingly the fastest at minimizing the upper bound. The order of performance follows the same trend as in the experiments of Section 6. ", "page_idx": 26}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/114805293e2bfe360bb3d8204506430412c9dd6f220c291abe6d9f737f3616e9.jpg", "img_caption": ["(a) X-RayAge. NormMin included in the experiment shown in Figure 1a. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/6eca2f0540d97ba3df4397859cda6b1b5d9500fc2aa5fb613fe93131580aec03.jpg", "img_caption": ["Figure 3: Additional figures from the $\\mathbf{X}$ -RayAge experiment. ", "(b) X-RayAge. The in-sample error $R_{I_{D}}$ for the generalization experiment performed in Figure 1b. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/a38c86c00fe2f40345817d25d58dcd818869243ca0e64c88e5c5dd9a814ffb15.jpg", "img_caption": ["(a) The risk $R(\\theta_{t})$ , defined as the normalized Kendall\u2019s tau distance between estimated and true orderings. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/4a960de09fa88b82fac87a5f3ce9d708f568f3563d42ac5436d2f5b3aa71a86c.jpg", "img_caption": ["(b) The probability that the frequency of pairwise inversions is $\\ge20\\%$ after every comparison, according to (1). "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 4: The loss (left) along with the upper bound (right) when ordering a list of size 100 in a synthetic environment. The results have been averaged over 50 seeds. ", "page_idx": 27}, {"type": "text", "text": "Randomly initialized representation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "As discussed in Section 6.2, the performance of our contextual approach will depend on the quality of the representations. To underscore the practical usefulness of our algorithms, we have performed the same experiment as in Figure 2c, but this time the model used to extract image features was untrained (i.e., the weights were random). As to be expected, the results, shown in Figure 5, demonstrate that the fully contextual algorithms have no real way of ordering the items according to these uninformative features. However, GURO Hybrid performs similarly to TrueSkill, despite model misspecification. This is promising, since you may not know in advance how informative the extracted features will be for the target ordering task. ", "page_idx": 27}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/542a7a82d145485a85628385815fb77abaaaa1b56b0c53f901ca009cab4d22fd.jpg", "img_caption": ["Figure 5: IMDB-WIKI-SbS. The same experiment as presented in Figure. 2c, but the model used for feature extraction is untrained. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "ImageClarity ground truth ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The ImageClarity dataset consists of multiple versions of the same image, with the same distortion applied to it to varying degrees. Due to this artificial construction, the pairwise comparisons should, given enough samples, reflect the magnitudes of the applied distortions. In Figure 6 we perform the same experiment as in Figure 2a, but instead of evaluating on a holdout comparison set, we measure the distance to the ground-truth ordering. The overall results are very similar, although we do see a slight increase in the performance of contextual algorithms compared to the non-contextual TrueSkill. ", "page_idx": 28}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/392608888404c10b93d1e5d8378443cb9e455449fdb977608b8582874cacdb78.jpg", "img_caption": ["Figure 6: ImageClarity. Same experiment as in Figure 2a, but now measuring the distance to the ground-truth ordering. Averaged over 25 seeds along with the 1-sigma error region. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Ground truth ordering using the Bradley-Terry model ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "An alternate approach to evaluate ordering quality is to estimate a \u201dground-truth\u201d ordering by applying the popular Bradley-Terry (BT) model (Bradley and Terry, 1952) to all available comparisons. We used the CrowdKit library (Ustalov et al., 2024) to find the MLE scores for each item and ordered the elements accordingly. In Figure 7 we run the same experiments as in Figure 2, but instead measure the distance to the constructed BT ordering. The overall trends remain, but for (b) and (c) there is a slight shift for the later iterations. More specifically we see non-contextual TrueSkill eventually overtaking the contextual algorithms. ", "page_idx": 28}, {"type": "text", "text": "The issue is that algorithms with orderings closer to the maximum likelihood estimate of the BT model will be favored. To exemplify this we use the ImageClarity dataset since it contains the largest number of comparisons relative to the number of items. We sample $1\\;000$ comparisons and let this be the collection that is available to the algorithms. We further construct two target orderings, one from the BT estimate using the sampled subset of comparisons, and a second, more probable ordering, from the BT estimate using all 27 730 available comparisons. Figure 8 shows the distance between the GURO and TS algorithms and the different target orderings, where dashed lines indicate the distance to the ordering generated using the full list of comparisons. If we only look at the distance to the ordering produced using our subset of comparisons, TrueSkill seemingly outperforms GURO after about 350 comparisons. However, if we instead measure the distance to the more probable ordering, we see that GURO converges toward a lower distance. Note that these are the same orderings, evaluated against different targets. This is likely the effect we observe in Figure 7b and c, but not in Figure 7a as a result of the high amount of comparisons available to us. ", "page_idx": 28}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/2bc17ed321768008b5f9ddc9d6677cf3c2b50a13b48a56bfc6c29e87233dee8b.jpg", "img_caption": ["Figure 7: The same experiment as presented in Figure. 2c, but we instead measure the distance to a ground-truth estimated using all available comparisons. ", "(c) IMDB-WIKI-SbS. $n=6\\:072$ , $d=75$ . "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "image", "img_path": "PSLH5q7PFo/tmp/6ad505f123905bf63e78f1744b8aeb372d16f9dd7d58128191fb48e9f393e4d7.jpg", "img_caption": ["Figure 8: ImageClarity. The same experiment as presented in Figure 2a, but we instead measure the distance to target orderings that correspond to the maximum likelihood estimate of the BT model using different numbers of comparisons. The dashed lines show the distance to the BT estimate using all 27 730 comparisons. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In the introduction, we clearly state where in the paper each contribution can be found. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: In Section 7 we highlight limitations such as real data experiments not being performed in an online setting and the lack of optimality guarantees for our algorithm. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model wellspecification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We clearly define our problem setting in Section 2, including relevant assumptions. Additionally, in Section 4 we list the three assumptions that our analysis depends on, and include the complete proofs in Appendix C. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. ", "page_idx": 30}, {"type": "text", "text": "\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Among our main contributions are the GURO and BayesGURO sampling strategies, which we not only offer algorithmic descriptions of but also provide the code for. We also describe our experimental setup in Section 6 and Appendix E including hyperparameters and pre-trained models used for feature extraction. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We provide the code in a publicly available repository. We have also provided links from which the datasets can downloaded. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. ", "page_idx": 31}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide experimental details, including splits and hyperparameters, in Section 6.2 as well as Appendix E. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Section 6 we either provide 1-sigma error bars or the $95\\%$ confidence interval, explicitly stating which in the figure captions. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Section 6.2 we state the compute resources used for the most demanding experiments. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have read and ensured that our work conforms to the ethical guidelines stipulated. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: In the introduction, we describe our motivating example of annotating medical images. Enabling quantitative analysis of these images could be highly beneficial for medical research. We do however not see a direct path from our active sampling criterion to a negative application. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The algorithms we provide do not pose any particular risk for misuse. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We cite the authors of all datasets we use according to their wishes in Section 6. We further include URL\u2019s in Appendix E.1 along with licenses for all available datasets except ImageClarity, for which we could not find the relevant information. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not introduce any new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have not performed any crowdsourcing experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 34}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We did not perform any experiments with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]