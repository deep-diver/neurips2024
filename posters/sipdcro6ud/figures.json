[{"figure_path": "siPdcro6uD/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison between our proposed approach and the mainstream REC/RES architectures.", "description": "This figure compares the architecture of the proposed OneRef model with existing REC/RES architectures.  It shows that existing models typically employ separate modality-dependent encoders (vision and language) with complex fusion mechanisms and often include decoders. In contrast, OneRef utilizes a unified modality-shared encoder, eliminating the need for complex fusion and interaction modules, resulting in a more efficient and concise architecture.", "section": "1 Introduction"}, {"figure_path": "siPdcro6uD/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of our multimodal Mask Referring Modeling (MRefM) paradigm, which includes Referring-aware mask image modeling and Referring-aware mask language modeling.", "description": "This figure illustrates the Mask Referring Modeling (MRefM) paradigm.  MRefM consists of two components: Referring-aware Mask Image Modeling (Referring MIM) and Referring-aware Mask Language Modeling (Referring MLM).  Referring MIM uses a referring-aware dynamic image masking strategy to mask image patches, focusing on the referred region. Both Referring MIM and Referring MLM reconstruct not only modality-related content but also cross-modal referring content.  The model uses a shared multi-head self-attention one-tower encoder to process both visual and textual features before separate MIM and MLM heads. The output of Referring MIM reconstructs the masked visual content and the visual target-relation score.  The output of Referring MLM reconstructs the masked textual content and the semantic target-relation score. This unified approach models the referential relationship between images and text.", "section": "3 Methodology"}, {"figure_path": "siPdcro6uD/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration of the referring-based grounding and segmentation transfer.", "description": "This figure illustrates the OneRef model's architecture for both referring expression comprehension (REC) and referring expression segmentation (RES) tasks.  In (a), the REC task, the unified modality-shared encoder processes both image and text features.  A referring-based REC head, using a similarity mask (Msim) and constraint, predicts the bounding box (B) of the referred object. In (b), the RES task, a similar process occurs. The key difference is that after the similarity mask, a deconvolution upsamples the feature map and a referring-based RES head uses bilinear interpolation to produce a fine-grained segmentation mask (Mseg) of the object. The results in both tasks are directly regressed using the unified visual-linguistic feature space and referring-aware modeling.", "section": "3.4 Referring-based grounding and segmentation transfer"}, {"figure_path": "siPdcro6uD/figures/figures_8_1.jpg", "caption": "Figure 4: Illustrations of random masking (MAE) [27], block-wise masking (BEiT) [4], and our referring-aware dynamic masking.  \u03b1 denotes the entire masking ratio, while \u03b2 and \u03b3 denote the masking ratio beyond and within the referred region.", "description": "This figure compares three different image masking strategies: random masking (MAE), block-wise masking (BEiT), and the proposed referring-aware dynamic masking.  The referring-aware dynamic masking focuses on the referred region in the image, applying a higher masking ratio (\u03b3) within that area and a lower ratio (\u03b2) to the surrounding context.  This approach aims to improve the model's ability to focus on the relevant part of the image when processing referring expressions.", "section": "3 Methodology"}, {"figure_path": "siPdcro6uD/figures/figures_19_1.jpg", "caption": "Figure 5: The reconstruction of the visual target-relation score s<sub>vt</sub> \u2208 R<sup>N<sub>v</sub></sup><sup>\u00d74</sup>. (x, y) represents the coordinates of a general image patch, and P is the patch size. By slicing the predicted score, four masks can be derived. The score represents the spatial distance and the relative size between the current patch region and the referred region.", "description": "This figure illustrates how the visual target-relation score is calculated and used in the Referring MIM module.  The score is a 4-dimensional vector representing the horizontal and vertical distances from the center of the referred region, as well as the ratio of patch width and height to the referred region's width and height.  This allows the model to explicitly encode the spatial relationship between image patches and the referred region, improving accuracy by providing a more comprehensive understanding of both visual and textual information.", "section": "3.2 Referring-aware mask image modeling"}, {"figure_path": "siPdcro6uD/figures/figures_22_1.jpg", "caption": "Figure 2: Illustration of our multimodal Mask Referring Modeling (MRefM) paradigm, which includes Referring-aware mask image modeling and Referring-aware mask language modeling.", "description": "This figure illustrates the architecture of the Mask Referring Modeling (MRefM) paradigm.  MRefM consists of two main components: Referring-aware Mask Image Modeling (Referring MIM) and Referring-aware Mask Language Modeling (Referring MLM).  Referring MIM takes masked image patches as input and reconstructs them while also predicting a visual target-relation score indicating the distance between each patch and the grounding region. This uses a referring-aware dynamic image masking strategy where the referred region has a higher masking ratio. Referring MLM takes masked text tokens as input and reconstructs them alongside a semantic target-relation score showing the relationship between each word and the referred image region. Both modules reconstruct modality-related content and cross-modal referring content. These components work together within a modality-shared one-tower transformer encoder to enable direct regression of referring results.", "section": "3 Methodology"}, {"figure_path": "siPdcro6uD/figures/figures_23_1.jpg", "caption": "Figure 3: Illustration of the referring-based grounding and segmentation transfer.", "description": "This figure shows the architecture for the referring-based grounding and segmentation transfer tasks.  The left side shows the referring expression comprehension task.  An image and expression text are input to a unified modality-shared encoder.  The output is a bounding box. The right side shows the referring expression segmentation task; an image and expression text are input to the same encoder and the output is a segmentation mask.", "section": "3.4 Referring-based grounding and segmentation transfer"}, {"figure_path": "siPdcro6uD/figures/figures_23_2.jpg", "caption": "Figure 2: Illustration of our multimodal Mask Referring Modeling (MRefM) paradigm, which includes Referring-aware mask image modeling and Referring-aware mask language modeling.", "description": "This figure illustrates the Mask Referring Modeling (MRefM) paradigm proposed in the paper.  MRefM is a novel approach that enhances the referring capabilities of the BEiT-3 model. It consists of two main components: Referring-aware Mask Image Modeling (Referring MIM) and Referring-aware Mask Language Modeling (Referring MLM).  Referring MIM reconstructs masked visual features using visual tokens after the dot product operation with the aggregated text token, thereby incorporating the visual target-relation score.  Referring MLM reconstructs masked language content using text tokens after the dot product operation with the aggregated visual token, adding the semantic target-relation score. A key feature is the use of a referring-aware dynamic image masking strategy that focuses masking on regions relevant to the referring expression, instead of random masking. The entire process takes place within a unified modality-shared one-tower transformer encoder.", "section": "3 Methodology"}]