{"importance": "This paper is crucial because it presents **OneRef**, a novel and efficient framework for visual grounding and referring segmentation tasks. Its minimalist design, built upon a modality-shared one-tower transformer, simplifies existing complex architectures. The **Mask Referring Modeling (MRefM)** paradigm enhances the model's ability to capture nuanced referential relationships, leading to state-of-the-art performance across multiple datasets. This work opens new avenues for future research in unified multimodal models and efficient transfer learning.", "summary": "OneRef: Unified one-tower model surpasses existing methods in visual grounding and segmentation by leveraging a novel Mask Referring Modeling paradigm.", "takeaways": ["OneRef, a unified one-tower model, simplifies existing complex architectures for visual grounding and segmentation.", "The novel Mask Referring Modeling (MRefM) paradigm improves the model's understanding of referential relationships between images and text.", "OneRef achieves state-of-the-art performance on multiple datasets, demonstrating its effectiveness and efficiency."], "tldr": "Existing visual grounding and referring segmentation methods often rely on bulky transformer-based fusion and various interaction technologies.  They also struggle to capture nuanced image-text relationships. This leads to complex architectures with many parameters and intricate processes.  The current mask visual language modeling (MVLM) techniques fail to fully capture the needed nuanced referential relationships.\nOneRef addresses these issues with a minimalist, modality-shared one-tower transformer, unifying visual and linguistic feature spaces.  It introduces a novel MVLM paradigm called Mask Referring Modeling (MRefM), including referring-aware mask image and language modeling, and a dynamic image masking strategy. OneRef's unified architecture enables direct regression of results without complex techniques, achieving state-of-the-art performance on various grounding and segmentation benchmarks.", "affiliation": "Institute of Automation, Chinese Academy of Sciences", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "siPdcro6uD/podcast.wav"}