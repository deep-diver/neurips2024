[{"figure_path": "VXJVNdmXO4/tables/tables_8_1.jpg", "caption": "Table 1: Test Error of Data Valuation Methods. We compared the test mean squared error on the buyer test point on a synthetic Gaussian-distributed data and four medical datasets: MIMIC, RSNA, Fitzpatrick17K, and DrugLib. The subheading denotes the number of seller training data available for that experiment, and \u201cN/A\u201d denotes that the method exceeded runtime constraints for the experiment. We optimize a separate random sample of training and validation data for each buyer and average over 100 buyers. Bolded values indicate the best-performing method and underlined values denote the second-best-performing method.", "description": "This table presents a comparison of the mean squared error achieved by different data valuation methods across various datasets (synthetic Gaussian, MIMIC, RSNA, Fitzpatrick, DrugLib) and varying amounts of seller training data (1k, 100k, 1k, 35k, 12k, 15k, 3.5k, respectively).  The results are averaged over 100 buyer test points.  The table highlights the relative performance of various methods, notably showing that DAVED consistently achieves the lowest mean squared error.", "section": "5 Experiments"}, {"figure_path": "VXJVNdmXO4/tables/tables_8_2.jpg", "caption": "Table 1: Test Error of Data Valuation Methods. We compared the test mean squared error on the buyer test point on a synthetic Gaussian-distributed data and four medical datasets: MIMIC, RSNA, Fitzpatrick17K, and DrugLib. The subheading denotes the number of seller training data available for that experiment, and \u201cN/A\u201d denotes that the method exceeded runtime constraints for the experiment. We optimize a separate random sample of training and validation data for each buyer and average over 100 buyers. Bolded values indicate the best-performing method and underlined values denote the second-best-performing method.", "description": "This table presents a comparison of the mean squared error (MSE) achieved by various data valuation methods on five datasets: synthetic Gaussian data and four real-world medical datasets (MIMIC, RSNA, Fitzpatrick17K, DrugLib).  The table shows the average MSE over 100 buyer test points for different budget sizes.  The results highlight the performance of DAVED (both single-step and multi-step variants) in comparison to existing methods, demonstrating its effectiveness in various settings.", "section": "5 Experiments"}, {"figure_path": "VXJVNdmXO4/tables/tables_18_1.jpg", "caption": "Table 1: Test Error of Data Valuation Methods. We compared the test mean squared error on the buyer test point on a synthetic Gaussian-distributed data and four medical datasets: MIMIC, RSNA, Fitzpatrick17K, and DrugLib. The subheading denotes the number of seller training data available for that experiment, and \u201cN/A\u201d denotes that the method exceeded runtime constraints for the experiment. We optimize a separate random sample of training and validation data for each buyer and average over 100 buyers. Bolded values indicate the best-performing method and underlined values denote the second-best-performing method.", "description": "This table compares the test mean squared error of several data valuation methods on various datasets, including synthetic Gaussian data and real medical datasets such as MIMIC-III, RSNA Pediatric Bone Age, Fitzpatrick17K, and DrugLib.  It shows the performance of different methods under varying amounts of seller training data and highlights the best-performing method for each scenario.", "section": "Comparing Performance on Homogeneous Costs"}, {"figure_path": "VXJVNdmXO4/tables/tables_18_2.jpg", "caption": "Table 1: Test Error of Data Valuation Methods. We compared the test mean squared error on the buyer test point on a synthetic Gaussian-distributed data and four medical datasets: MIMIC, RSNA, Fitzpatrick17K, and DrugLib. The subheading denotes the number of seller training data available for that experiment, and \u201cN/A\u201d denotes that the method exceeded runtime constraints for the experiment. We optimize a separate random sample of training and validation data for each buyer and average over 100 buyers. Bolded values indicate the best-performing method and underlined values denote the second-best-performing method.", "description": "This table compares the performance of different data valuation methods on various datasets in terms of the mean squared error (MSE). The datasets include a synthetic Gaussian dataset and four medical datasets (MIMIC, RSNA, Fitzpatrick17K, and DrugLib).  The table reports the MSE for different budget sizes, highlighting the best-performing methods and illustrating the impact of data scarcity.", "section": "Comparing Performance on Homogeneous Costs"}, {"figure_path": "VXJVNdmXO4/tables/tables_19_1.jpg", "caption": "Table 1: Test Error of Data Valuation Methods. We compared the test mean squared error on the buyer test point on a synthetic Gaussian-distributed data and four medical datasets: MIMIC, RSNA, Fitzpatrick17K, and DrugLib. The subheading denotes the number of seller training data available for that experiment, and \u201cN/A\u201d denotes that the method exceeded runtime constraints for the experiment. We optimize a separate random sample of training and validation data for each buyer and average over 100 buyers. Bolded values indicate the best-performing method and underlined values denote the second-best-performing method.", "description": "This table compares the performance of different data valuation methods (including DAVED) on various datasets (synthetic and medical).  It reports the mean squared error (MSE) for different budget sizes. The table highlights the superior performance of DAVED across different datasets and budget sizes.", "section": "5 Experiments"}]