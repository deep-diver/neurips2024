[{"figure_path": "UXEo3uNNIX/figures/figures_1_1.jpg", "caption": "Figure 1: Architecture of Coupled Mamba.", "description": "The figure illustrates the architecture of the Coupled Mamba model, which is a multi-modal fusion model based on state space models.  It shows N layers, each comprising M Coupled Mamba blocks (one for each modality).  Each block receives the sequence data of multiple modalities as input, aggregates the states, and transitions to the next time step's state for each modality.  Pooling layers are used to adapt the output for downstream tasks.  The model integrates multiple modalities via a state transition scheme dependent on previous time steps' states from its own chain and neighboring chains.", "section": "1 Introduction"}, {"figure_path": "UXEo3uNNIX/figures/figures_3_1.jpg", "caption": "Figure 2: Coupling Mamba receives input  X<sub>t\u22121</sub>, and performs internal state switching and output through three key parameter matrices, where B, C and S are respectively represented as the input matrix, output matrix and state transfer matrix. The hidden states are summed across modalities and used for state transition input to generate next time states. The state is propagated sequentially in time.", "description": "This figure illustrates the Coupled Mamba model's architecture.  It shows how the model receives multi-modal inputs (X<sub>t-1</sub>) at time step t-1. These inputs are processed through three key matrices (B, C, S) representing input, output, and state transition respectively. Notably, the hidden states (h) from all modalities are summed before being used to generate the next time step's hidden states (h<sub>t</sub>). This process continues sequentially over time.", "section": "3 Coupled State Space Model"}, {"figure_path": "UXEo3uNNIX/figures/figures_9_1.jpg", "caption": "Figure 3: GPU usage comparison", "description": "This figure compares the GPU memory usage of Coupled Mamba and Cross Attention methods with varying sequence lengths. It shows that Coupled Mamba uses significantly less GPU memory than Cross Attention, especially as the sequence length increases.  The Y-axis represents GPU memory usage in GB, while the X-axis shows the sequence length. The graph visually demonstrates the memory efficiency advantage of Coupled Mamba.", "section": "4 Experiment"}, {"figure_path": "UXEo3uNNIX/figures/figures_9_2.jpg", "caption": "Figure 4: Inference speed comparison", "description": "This figure compares the inference speed of Coupled Mamba and Cross Attention models across varying sequence lengths.  The results demonstrate that Coupled Mamba consistently shows faster inference times than Cross Attention, and the difference in speed becomes more pronounced as the sequence length increases.", "section": "4 Experiment"}]