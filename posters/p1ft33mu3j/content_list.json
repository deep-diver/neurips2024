[{"type": "text", "text": "Linear Transformers are Versatile In-Context Learners ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Max Vladymyrov Google Research mxv@google.com ", "page_idx": 0}, {"type": "text", "text": "Johannes von Oswald Google, Paradigms of Intelligence Team jvoswald@google.com ", "page_idx": 0}, {"type": "text", "text": "Mark Sandler Google Research sandler@google.com ", "page_idx": 0}, {"type": "text", "text": "Rong Ge Duke University rongge@cs.duke.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided incontext during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that each layer of a linear transformer maintains a weight vector for an implicit linear regression problem and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We analyze this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, driving breakthroughs across various domains and serving as a foundation for powerful models (Anil et al., 2023; Achiam et al., 2023; Team et al., 2023; Jiang et al., 2023). However, despite their widespread success, the mechanisms that drive their performance remain an active area of research. A key component of their success is attributed to in-context learning (ICL, Brown et al., 2020) \u2013 an emergent ability of transformers to make predictions based on information provided within the input sequence itself, without explicit parameter updates. ", "page_idx": 0}, {"type": "text", "text": "Recently, several papers (Garg et al., 2022; Aky\u00fcrek et al., 2022; von Oswald et al., 2023a) have suggested that ICL might be partially explained by an implicit meta-optimization of the transformers that happens on input context (aka mesa-optimization Hubinger et al., 2019). They have shown that transformers with linear self-attention layers (aka linear transformers) trained on linear regression tasks can internally implement gradient-based optimization. ", "page_idx": 0}, {"type": "text", "text": "Specifically, von Oswald et al. (2023a) demonstrated that linear transformers can execute iterations of an algorithm similar to the gradient descent algorithm (which they call $\\mathrm{GD^{++}}$ ), with each attention layer representing one step of the algorithm. Later, Ahn et al. (2023); Zhang et al. (2023) further characterized this behavior, showing that the learned solution is a form of preconditioned GD, and this solution is optimal for one-layer linear transformers. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we continue to study linear transformers trained on linear regression problems. We prove that each layer of every linear transformer maintains a weight vector for an underlying linear regression problem. Under some restrictions, the algorithm it runs can be interpreted as a complex variant of preconditioned gradient descent with momentum-like behaviors. ", "page_idx": 1}, {"type": "text", "text": "While maintaining a linear regression model (regardless of the data) might seem restrictive, we show that linear transformers can discover powerful optimization algorithms. As a first example, we prove that in case of $\\mathrm{GD^{++}}$ , the preconditioner results in a second order optimization algorithm. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we demonstrate that linear transformers can be trained to uncover even more powerful and intricate algorithms. We modified the problem formulation to consider mixed linear regression with varying noise levels1 (inspired by Bai et al., 2023). This is a harder and non-trivial problem with no obvious closed-form solution, since it needs to account for various levels of noise in the input. ", "page_idx": 1}, {"type": "text", "text": "Our experiments with two different noise variance distributions (uniform and categorical) demonstrate the remarkable flexibility of linear transformers. Training a linear transformer in these settings leads to an algorithm that outperforms $\\mathrm{GD^{++}}$ as well as various baselines derived from the exact closedform solution of the ridge regression. We discover that this result holds even when training a linear transformer with diagonal weight matrices. ", "page_idx": 1}, {"type": "text", "text": "Through a detailed analysis, we reveal key distinctions from $\\mathrm{GD^{++}}$ , including momentum-like term and adaptive rescaling based on the noise levels. ", "page_idx": 1}, {"type": "text", "text": "Our findings contribute to the growing body of research where novel, high-performing algorithms have been directly discovered through the reverse-engineering of transformer weights. This work expands our understanding of the implicit learning capabilities of attention-based models and highlights the remarkable versatility of even simple linear transformers as in-context learners. We demonstrate that transformers have the potential to discover effective algorithms that may advance the state-of-the-art in optimization and machine learning in general. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section we introduce notations for linear transformers, data, and type of problems we consider. ", "page_idx": 1}, {"type": "text", "text": "2.1 Linear transformers and in-context learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given input sequence $e_{1},e_{2},...,e_{n}\\in\\mathbb{R}^{d+1}$ , a single head in a linear self-attention layer is usually parameterized by four matrices, key $W_{K}$ , query $W_{Q}$ , value $W_{V}$ and projection $W_{P}$ . The output of the non-causal layer at position $i$ is $e_{i}+\\Delta e_{i}$ where $\\bar{\\Delta}e_{i}$ is computed as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta e_{i}=W_{P}\\left(\\sum_{j=1}^{n}\\langle W_{Q}e_{i},W_{K}e_{j}\\rangle W_{V}e_{j}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Equivalently, one can use parameters $P=W_{P}W_{V}$ and $Q=W_{K}^{\\top}W_{Q}$ , and the equation becomes ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta e_{i}=\\sum_{j=1}^{n}(e_{j}^{\\top}Q e_{i})P e_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Multiple heads $(P_{1},Q_{1}),(P_{2},Q_{2}),...,(P_{h},Q_{h})$ simply sum their effects ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta e_{i}=\\sum_{k=1}^{H}\\sum_{j=1}^{n}(e_{j}^{\\top}Q_{k}e_{i})P_{k}e_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We define a linear transformer as a multi-layer neural network composed of $L$ linear self-attention layers parameterized by $\\theta=\\{Q_{k}^{l},P_{k}^{l}\\}$ for $k=1\\ldots H,l=1\\ldots L$ . To isolate the core mechanisms, we consider a simplified decoder-only architecture, excluding MLPs and LayerNorm components. This architecture was also used in previous work (von Oswald et al., 2023a; Ahn et al., 2023). ", "page_idx": 1}, {"type": "text", "text": "We consider two versions of linear transformers: FULL with the transformer parameters represented by full matrices and DIAG, where the parameters are restricted to diagonal matrices only. ", "page_idx": 1}, {"type": "text", "text": "Inspired by von Oswald et al. (2023a), in this paper we consider regression data as the token sequence. Each token $e_{i}=(x_{i},y_{i})\\in\\mathbb{R}^{d+1}$ consists of a feature vector $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{d}$ and its corresponding output $y_{i}\\in\\mathbb{R}$ . Additionally, we append a query token $e_{n+1}=(x_{t},0)$ to the sequence, where $x_{t}\\in\\mathbb{R}^{d}$ represents test data. The goal of in-context learning is to predict $y_{t}$ for the test data $x_{t}$ . We constrain the attention to only focus on the first $n$ tokens of the sequence so that it ignores the query token. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We use $(x_{i}^{l},y_{i}^{l})$ to denote the $i$ -th token in the transformer\u2019s output at layer $l$ . The initial layer is simply the input: $\\bar{(x_{i}^{0},y_{i}^{0})}=(x_{i},y_{i})$ . For a model with parameters $\\theta$ , we read out the prediction by taking the negative2 of the last coordinate of the final token in the last layer as $\\hat{y}_{\\theta}(\\{e_{1},...,e_{n}\\},e_{n+1})=-y_{n+1}^{L}$ . ", "page_idx": 2}, {"type": "text", "text": "Let\u2019s also define the following notation to be used throughout the paper ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\Sigma=\\sum_{i=1}^{n}\\boldsymbol{x}_{i}(\\boldsymbol{x}_{i})^{\\top};}&{\\displaystyle\\alpha=\\sum_{i=1}^{n}y_{i}\\boldsymbol{x}_{i};}\\\\ {\\displaystyle\\Sigma^{l}=\\sum_{i=1}^{n}x_{i}^{l}(x_{i}^{l})^{\\top};}&{\\displaystyle\\alpha^{l}=\\sum_{i=1}^{n}y_{i}^{l}x_{i}^{l};}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\lambda=\\sum_{i=1}^{n}(y_{i})^{2}}\\\\ {\\displaystyle\\lambda^{l}=\\sum_{i=1}^{n}(y_{i}^{l})^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2.2 Noisy regression model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As a model problem, we consider data generated from a noisy linear regression model. For each input sequence $\\tau$ , we sample a ground-truth weight vector $w_{\\tau}\\sim N(0,I)$ , and generate $n$ data points as $\\bar{x_{i}}\\sim N(0,I)$ and $y_{i}=\\langle w_{\\tau},x_{i}\\rangle+\\xi_{i}$ , with noise $\\xi_{i}\\sim N(0,\\sigma_{\\tau}^{2})$ . ", "page_idx": 2}, {"type": "text", "text": "Note that each sequence can have different ground-truth weight vectors $w_{\\tau}$ , but every data point in the sequence shares the same $w_{\\tau}$ and $\\sigma_{\\tau}$ . The query is generated as $x_{t}\\sim N(0,I)$ and $y_{t}=\\bar{\\langle w_{\\tau},x_{t}\\rangle}$ (since the noise is independent, whether we include noise in $y_{q}$ will only be an additive constant to the final objective). ", "page_idx": 2}, {"type": "text", "text": "We further define an ordinary least square (OLS) loss as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{\\mathrm{OLS}}(w)=\\sum_{i=1}^{n}\\left(y_{i}-\\langle w,x_{i}\\rangle\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The OLS solution is $w^{*}:=\\Sigma^{-1}\\alpha$ with residuals $r_{i}:=y_{i}-\\left\\langle w^{*},x_{i}\\right\\rangle$ . ", "page_idx": 2}, {"type": "text", "text": "In the presence of noise $\\sigma_{\\tau},\\,w^{*}$ in general is not equal to the ground truth $w_{\\tau}$ . For a known noise level $\\sigma_{\\tau}$ , the best estimator for $w_{\\tau}$ is provided by ridge regression: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{\\mathtt{R R}}(w)=\\sum_{i=1}^{n}\\left(y_{i}-\\langle w,x_{i}\\rangle\\right)^{2}+\\sigma_{\\tau}^{2}\\|w\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with solution $w_{\\sigma^{2}}^{*}:=\\left(\\Sigma+\\sigma_{\\tau}^{2}I\\right)^{-1}\\alpha$ . Of course, in reality the variance of the noise is not known and has to be estimated from the data. ", "page_idx": 2}, {"type": "text", "text": "2.3 Fixed vs. mixed noise variance problems ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider two different problems within the noisy linear regression framework. ", "page_idx": 2}, {"type": "text", "text": "Fixed noise variance. In this scenario, the variance $\\sigma_{\\tau}$ remains constant for all the training data. Here, the in-context loss is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nL(\\theta)=\\underset{w_{\\tau}\\sim N(0,I)}{\\mathbb{E}}\\left[(\\hat{y}_{\\theta}(\\{e_{1},...,e_{n}\\},e_{n+1})-y_{t})^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $e_{i}=(x_{i},y_{i})$ and $y_{i}=\\left\\langle w_{\\tau},x_{i}\\right\\rangle+\\xi_{i}$ . This problem was initially explored by Garg et al. (2022). Later, von Oswald et al. (2023a) have demonstrated that a linear transformer (6) converges to a form of a gradient descent solution, which they called $\\mathrm{GD^{++}}$ . We define this in details later. ", "page_idx": 2}, {"type": "text", "text": "Mixed noise variance. In this case, the noise variance $\\sigma_{\\tau}$ is drawn from some fixed distribution $p(\\sigma_{\\tau})$ for each sequence. The in-context learning loss becomes: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(\\theta)=\\underset{w_{\\tau}\\sim N(0,I)}{\\mathbb{E}}\\left[(\\hat{y}_{\\theta}(\\{e_{1},...,e_{n}\\},e_{n+1})-y_{t})^{2}\\right].}\\\\ {x_{i}{\\sim}N(0,I)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {\\xi_{i}{\\sim}N(0,\\sigma_{\\tau}^{2})\\qquad\\qquad\\quad}\\\\ {\\sigma_{\\tau}{\\sim}p(\\sigma_{\\tau})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In other words, each training sequence $\\tau$ has a fixed noise level $\\sigma_{\\tau}$ , but different training sequences have different noise levels sampled from a specified distribution $p(\\sigma_{\\tau})$ . This scenario adds complexity because the model must predict $w_{\\tau}$ for changing noise distribution, and the optimal solution likely would involve some sort of noise estimation. We have found that empirically, $\\mathrm{GD^{++}}$ fails to model this noise variance and instead converges to a solution which can be interpreted as a single noise variance estimate across all input data. ", "page_idx": 3}, {"type": "text", "text": "3 Related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In-context Learning as Gradient Descent Our work builds on research that frames in-context learning as (variants of) gradient descent (Aky\u00fcrek et al., 2022; von Oswald et al., 2023a). For 1-layer linear transformer, several works Zhang et al. (2023); Mahankali et al. (2023); Ahn et al. (2023) characterized the optimal parameters and training dynamics. More recent works extended the ideas to auto-regressive models (Li et al., 2023; von Oswald et al., 2023b) and nonlinear models (Cheng et al., 2023). Fu et al. (2023) noticed that transformers perform similarly to second-order Newton methods on linear data, for which we give a plausible explanation in Theorem 5.1. ", "page_idx": 3}, {"type": "text", "text": "In-context Learning in LLMs There are also many works that study how in-context learning works in pre-trained LLMs (Kossen et al., 2023; Wei et al., 2023; Hendel et al., 2023; Shen et al., 2023). Due to the complexity of such models, the exact mechanism for in-context learning is still a major open problem. Several works (Olsson et al., 2022; Chan et al., 2022; Aky\u00fcrek et al., 2024) identified induction heads as a crucial mechanism for simple in-context learning tasks, such as copying, token translation and pattern matching. ", "page_idx": 3}, {"type": "text", "text": "Other theories for training transformers Other than the setting of linear models, several other works (Garg et al., 2022; Tarzanagh et al., 2023; Li et al., 2023; Huang et al., 2023; Tian et al., 2023a,b) considered optimization of transformers under different data and model assumptions. Wen et al. (2023) showed that it can be difficult to interpret the \u201calgorithm\u201d performed by transformers without very strong restrictions. ", "page_idx": 3}, {"type": "text", "text": "Mixed Linear Models Several works observed that transformers can achieve good performance on a mixture of linear models (Bai et al., 2023; Pathak et al., 2023; Yadlowsky et al., 2023). While these works show that transformers can implement many variants of model-selection techniques, our result shows that linear transformers solve such problems by discovering interesting optimization algorithm with many hyperparameters tuned during the training process. Such a strategy is quite different from traditional ways of doing model selection. Transformers are also known to be able to implement strong algorithms in many different setups (Guo et al., 2023; Giannou et al., 2023). ", "page_idx": 3}, {"type": "text", "text": "Effectiveness of linear and kernel-like transformers A main constraint on transformer architecture is that it takes $O(N^{2})$ time for a sequence of length $N$ , while for a linear transformer this can be improved to $O(N)$ . Mirchandani et al. (2023) showed that even linear transformers are quite powerful for many tasks. Other works (Katharopoulos et al., 2020; Wang et al., 2020; Schlag et al., 2021; Choromanski et al., 2020) uses ideas similar to kernel/random features to improve the running time to almost linear while not losing much performance. ", "page_idx": 3}, {"type": "text", "text": "4 Linear transformers maintain linear regression model at every layer ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "While large, nonlinear transformers can model complex relationship, we show that linear transformers are restricted to maintaining a linear regression model based on the input, in the sense that the $l$ -th layer output is always a linear function of the input with latent (and possibly nonlinear) coefficients. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.1. Suppose the output of $a$ linear transformer at l-th layer is $(x_{1}^{l},y_{1}^{l}),(x_{2}^{l},y_{2}^{l}),...,\\hat{(x_{n}^{l},y_{n}^{l})},(x_{t}^{l},y_{t}^{l})$ , then there exists matrices $M^{l}$ , vectors $u^{l},w^{l}$ and scalars $a^{l}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i}^{l+1}=M^{l}x_{i}+y_{i}u^{l},}\\\\ &{y_{i}^{l+1}=a^{l}y_{i}-\\langle w^{l},x_{i}\\rangle,}\\end{array}\\qquad\\qquad\\qquad\\begin{array}{r l}&{x_{t}^{l+1}=M^{l}x_{t},}\\\\ &{y_{t}^{l+1}=-\\langle w^{l},x_{t}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that $M^{l},\\,u^{l},w^{l}$ and $a^{l}$ are not linear in the input, but this still poses restrictions on what the linear transformers can do. For example we show that it cannot represent a quadratic function: ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. Suppose the input to a linear transformer is $(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})$ where $x_{i}\\,\\sim\\,N(0,I)$ and $y_{i}\\,=\\,w^{\\top}x_{i},$ , let the $l$ -th layer output be $(x_{1}^{l},y_{1}^{l}),(x_{2}^{l},y_{2}^{l}),...,(x_{n}^{l},y_{n}^{l})$ and let $y^{l}=(y_{1}^{l},...,y_{n}^{l})$ and $y^{*}=(x_{1}(1)^{2},x_{2}(1)^{2},...,x_{n}(1)^{2})$ (here $x_{i}(1)$ is just the first coordinate of $x_{i}$ ), then when $n\\gg d$ with high probability the cosine similarity of $y^{\\ast}$ and $y^{l}$ is at most $0.1$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 implies that the output of linear transformer can always be explained as linear combinations of input with latent weights $a^{l}$ and $w^{l}$ . The matrices $M^{l}$ , vectors $u^{l},\\dot{w}^{l}$ and numbers $a^{l}$ are not linear and can in fact be quite complex, which we characterize below: ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.3. In the setup of Theorem 4.1, if we let ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\left(\\begin{array}{c c}{{A^{l}}}&{{b^{l}}}\\\\ {{(c^{l})^{\\top}}}&{{d^{l}}}\\end{array}\\right):=}}\\\\ {{\\displaystyle\\sum_{k=1}^{h}\\left[P_{k}^{l}\\displaystyle\\sum_{j=1}^{n}\\left(\\left(\\begin{array}{c}{{x_{j}^{l}}}\\\\ {{y_{j}^{l}}}\\end{array}\\right)((x_{j}^{l})^{\\top},y_{j}^{l})\\right)Q_{k}^{l}\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then one can recursively compute matrices $M^{l}$ , vectors $u^{l},w^{l}$ and numbers $a^{l}$ for every layer using ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M^{l+1}=(I+A^{l})M^{l}+b^{l}(w^{l})^{\\top}}\\\\ &{\\;u^{l+1}=(I+A^{l})u^{l}+a^{l}b^{l}}\\\\ &{\\;a^{l+1}=(1+d^{l})a^{l}+\\langle c^{l},u^{l}\\rangle}\\\\ &{\\;w^{l+1}=(1+d^{l})w^{l}-(M^{l})^{\\top}c^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with the init. condition $a^{0}=1,w^{0}=0,M^{0}=I,u^{0}=0.$ ", "page_idx": 4}, {"type": "text", "text": "The updates to the parameters are complicated and nonlinear, allowing linear transformers to implement powerful algorithms, as we will later see in Section 5. In fact, even with diagonal $P$ and $Q$ , they remain flexible. The updates in this case can be further simplified to a more familiar form: ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.4. In the setup of Theorem 4.1 with diagonal parameters, $u^{l},w^{l}$ are updated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u^{l+1}=(I-\\Lambda^{l})u^{l}+\\Gamma^{l}\\Sigma\\left(a^{l}w^{\\ast}-w^{l}\\right);}\\\\ &{w^{l+1}=(1+s^{l})w^{l}-\\Pi^{l}\\Sigma(a^{l}w^{\\ast}-w^{l})-\\Phi^{l}u^{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here $\\Lambda^{l},\\Gamma^{l},s^{l},\\Pi^{l},\\Phi^{l}$ are matrices and numbers that depend on $M^{l},u^{l},a^{l},w^{l}$ in Lemma 4.3. ", "page_idx": 4}, {"type": "text", "text": "Note that $\\Sigma\\left(a^{l}w^{*}-w^{l}\\right)$ is (proportional to) the gradient of a linear model $\\begin{array}{r}{f(w^{l})=\\sum_{i=1}^{n}(a^{l}y_{i}-}\\end{array}$ $\\langle w^{l},x_{i}\\rangle)^{2}$ . This makes the updates similar to a gradient descent with momentum: ", "page_idx": 4}, {"type": "equation", "text": "$$\nu^{l+1}=(1-\\beta)u^{l}+\\nabla f(w^{l});w^{l+1}=w^{l}-\\eta u^{l}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Of course, the formula in Lemma 4.4 is still much more complicated with matrices in places of $\\beta$ and $\\eta$ , and also including a gradient term for the update of $w$ . ", "page_idx": 4}, {"type": "text", "text": "5 Power of diagonal attention matrices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although linear transformers are constrained, they can solve complex in-context learning problems. Empirically, we have found that they are able to very accurately solve linear regression with mixed noise variance (7), with final learned weights that are very diagonal heavy with some low-rank component (see Fig. 4). Surprisingly, the final loss remains remarkably consistent even when their $Q$ and $P$ matrices (3) are diagonal. Here we will analyze this special case and explain its effectiveness. ", "page_idx": 4}, {"type": "text", "text": "Since the elements of $x$ are permutation invariant, a diagonal parameterization reduces each attention heads to just four parameters: ", "page_idx": 5}, {"type": "equation", "text": "$$\nP_{k}^{l}=\\left(\\begin{array}{c c}{{p_{x,k}^{l}I}}&{{0}}\\\\ {{0}}&{{p_{y,k}^{l}}}\\end{array}\\right);\\quad Q_{k}^{l}=\\left(\\begin{array}{c c}{{q_{x,k}^{l}I}}&{{0}}\\\\ {{0}}&{{q_{y,k}^{l}}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It would be useful to further reparametrize the linear transformer (3) using: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\omega_{x x}^{l}=\\sum_{k=1}^{H}p_{x,k}^{l}q_{x,k}^{l},\\;\\;\\;\\omega_{x y}^{l}=\\sum_{k=1}^{H}p_{x,k}^{l}q_{y,k}^{l},}\\\\ {\\omega_{y x}^{l}=\\sum_{k=1}^{H}p_{y,k}^{l}q_{x,k}^{l},\\;\\;\\;\\omega_{y y}^{l}=\\sum_{k=1}^{H}p_{y,k}^{l}q_{y,k}^{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This leads to the following diagonal layer updates: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i}^{l+1}=x_{i}^{l}+\\omega_{x x}^{l}\\Sigma^{l}x_{i}^{l}+w_{x y}^{l}y_{i}^{l}\\alpha^{l}}\\\\ &{x_{t}^{l+1}=x_{t}^{l}+\\omega_{x x}^{l}\\Sigma^{l}x_{t}^{l}+w_{x y}^{l}y_{t}^{l}\\alpha^{l}}\\\\ &{y_{i}^{l+1}=y_{i}^{l}+\\omega_{y x}^{l}\\langle\\alpha^{l},x_{i}^{l}\\rangle+\\omega_{y y}^{l}y_{i}^{l}\\lambda^{l},}\\\\ &{y_{t}^{l+1}=y_{t}^{l}+\\omega_{y x}^{l}\\langle\\alpha^{l},x_{t}^{l}\\rangle+\\omega_{y y}^{l}y_{t}^{l}\\lambda^{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Four variables \u03c9lxx, \u03c9lxy, \u03c9lyx, \u03c9ly represent information flow between the data and the labels across layers. For instance, the term controlled by $\\omega_{x x}^{l}$ measures information flow from $x^{l}$ to $x^{l+1}$ , $\\omega_{y x}^{l}$ measures the flow from $x^{l}$ to $y^{l+1}$ and so forth. Since the model can always be captured by these 4 variables, having many heads does not significantly increase its representation power. When there is only one head the equation $\\omega_{x x}^{l}\\omega_{y y}^{l}=\\omega_{x y}^{l^{-}}\\omega_{y x}^{l}$ is always true, while models with more than one head do not have this limitation. However empirically even models with one head is quite powerful. ", "page_idx": 5}, {"type": "text", "text": "5.1 $\\mathbf{GD}^{++}$ and least squares solver ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "$\\mathrm{GD^{++}}$ , introduced in von Oswald et al. (2023a), represents a linear transformer that is trained on a fixed noise variance problem (6). It is a variant of a diagonal linear transformer, with all the heads satisfying $q_{y,k}^{l}=0$ . Dynamics are influenced only by $\\omega_{x x}^{\\widetilde{l}}$ and $\\omega_{y x}^{l}$ , leading to simpler updates: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{x_{i}^{l+1}=\\left(I+\\omega_{x x}^{l}\\Sigma^{l}\\right)x_{i}^{l}}}\\\\ {{y_{i}^{l+1}=y_{i}^{l}+\\omega_{y x}^{l}\\langle\\alpha^{l},x_{i}^{l}\\rangle.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The update on $x$ acts as preconditioning and the update on $y$ performs gradient descent on the data. ", "page_idx": 5}, {"type": "text", "text": "While existing analysis by Ahn et al. (2023) has not yielded fast convergence rates for $\\mathrm{GD^{++}}$ , we show here that it is actually a second-order optimization algorithm for the least squares problem (4): ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.1. Given $(x_{1},y_{1}),...,(x_{n},y_{n}),(x_{t},0)$ where $\\Sigma$ has eigenvalues in the range $[\\nu,\\mu]$ with a condition number $\\kappa=\\nu/\\mu$ . Let $w^{*}$ be the optimal solution to least squares problem (4), then there exists hyperparameters for $G D^{++}$ algorithm that outputs $\\hat{y}$ with accuracy $|\\hat{y}-\\langle x_{t},w^{*}\\rangle|\\leq\\epsilon\\|x_{t}\\|\\|w^{*}\\|$ in $l=O(\\log\\kappa{+}\\log\\log1/\\epsilon)$ steps. In particular that implies there exists an $l$ -layer linear transformer that can solve this task. ", "page_idx": 5}, {"type": "text", "text": "The convergence rate of $O(\\log\\log1/\\epsilon)$ is typically achieved only by second-order algorithms such as Newton\u2019s method. ", "page_idx": 5}, {"type": "text", "text": "5.2 Understanding $\\omega_{y y}$ : adaptive rescaling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "If a layer only has $\\omega_{y y}^{l}\\neq0$ , it has a rescaling effect. The amount of scaling is related to the amount of noise added in a model selection setting. The update rule for this layer is: ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{i}^{l+1}=\\left(1+\\omega_{y y}^{l}\\lambda^{l}\\right)y_{i}^{l}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This rescales every $y$ by a factor that depends on $\\lambda^{l}$ . When $\\omega_{y y}^{l}<0$ , this shrinks of the output based on the norm of $y$ in the previous layer. This is useful for the mixed noise variance problem, as ridge regression solution scales the least squares solution by a factor that depends on the noise level. ", "page_idx": 5}, {"type": "text", "text": "Specifically, assuming $\\Sigma\\,\\approx\\,\\mathbb{E}[\\Sigma]\\,=\\,n I$ , the ridge regression solution becomes $\\begin{array}{r}{w_{\\sigma^{2}}^{*}\\approx\\frac{n}{n+\\sigma^{2}}w^{*}}\\end{array}$ , which is exactly a scaled version of the OLS solution. Further, when noise is larger, the scaled factor is smaller, which agrees with the behavior of a negative \u03c9yy. ", "page_idx": 5}, {"type": "text", "text": "We can show that using adaptive scaling $\\omega_{y y}$ even a 2-layer linear transformer can be enough to solve a simple example of categorical mixed noise variance problem $\\sigma_{\\tau}\\in\\{\\sigma_{1},\\sigma_{2}\\}$ and $n\\to\\infty$ : ", "page_idx": 5}, {"type": "image", "img_path": "p1ft33Mu3J/tmp/7e27d5e8e26bea4c27bd88e5d0b6333779cb90ec11dc27b27b2fe91ff0847571.jpg", "img_caption": ["Figure 1: In-context learning performance for noisy linear regression problem across models with different number of layers and $\\sigma_{m a x}$ for $\\sigma_{\\tau}\\sim U(0,\\sigma_{m a x})$ . Each marker corresponds to a separately trained model with a given number of layers. Models with diagonal attention weights (DIAG) match those with full attention weights (FULL). Models specialized on a fixed noise $(\\mathrm{GD}^{++})$ perform poorly, similar to a Ridge Regression solution with a constant noise (CONSTRR). Among the baselines, only tuned exact Ridge Regression solution (TUNEDRR) is comparable with linear transformers. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Theorem 5.2. Suppose the input to the transformer is $(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n}),(x_{q},0)$ , where $\\begin{array}{r}{x_{i}\\sim N(0,\\frac{1}{n}I)}\\end{array}$ , $y_{i}\\,=\\,w^{\\top}x_{i}+\\xi_{i}$ . Here $\\xi_{i}\\sim N(0,\\sigma^{2})$ is the noise whose noise level $\\sigma$ can take one of two values: $\\sigma_{1}\\,\\,o r\\,\\sigma_{2}$ . Then as $n$ goes to $+\\infty$ , there exists a set of parameters for two-layer linear transformers such that the implicit $w^{2}$ of the linear transformer converges to the optimal ridge regression results (and the output of the linear transformer is $-\\langle w^{2},x_{q}\\rangle)$ . Further, the first layer only has $\\omega_{y x}$ being nonzero and the second layer only has $\\omega_{y y}$ being nonzero. ", "page_idx": 6}, {"type": "text", "text": "5.3 Understanding $\\omega_{x y}$ : adapting step-sizes ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The final term in the diagonal model, $\\omega_{x y}$ , has a more complicated effect. Since it changes only the $x$ -coordinates, it does not have an immediate effect on $y$ . To understand how it influences the $y$ we consider a simplified two-step process, where the first step only has $\\omega_{x y}\\neq0$ and the second step only has $\\omega_{y x}\\neq0$ (so the second step is just doing one step of gradient descent). In this case, the first layer will update the $x_{i}$ \u2019s as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i}^{1}=x_{i}+y_{i}\\omega_{x y}\\displaystyle\\sum_{j=1}^{n}y_{j}x_{j}}\\\\ &{\\quad=x_{i}+\\omega_{x y}y_{i}\\displaystyle\\sum_{j=1}^{n}(\\langle w^{*},x_{j}\\rangle+r_{j})x_{j}}\\\\ &{\\quad=x_{i}+\\omega_{x y}y_{i}\\Sigma w^{*}}\\\\ &{\\quad=x_{i}+\\omega_{x y}(\\langle w^{*},x_{i}\\rangle+r_{i})\\Sigma w^{*}}\\\\ &{\\quad=(I+\\omega_{x y}\\Sigma w^{*}(w^{*})^{\\top})x_{i}+\\omega_{x y}r_{i}\\Sigma w^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "There are two effects of the $\\omega_{x y}$ term, one is a multiplicative effect on $x_{i}$ , and the other is an additive term that makes $x$ -output related to the residual $r_{i}$ . The multiplicative step in $x_{i}$ has an unknown preconditioning effect. For simplicity we assume the multiplicative term is small, that is: ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{i}^{1}\\approx x_{i}+\\omega_{x y}r_{i}\\Sigma w^{*};\\quad x_{t}^{1}\\approx x_{t}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The first layer does not change $y$ , so $y_{t}^{1}=y_{t}$ and $y_{i}^{1}=y_{i}$ . For this set of $x_{i}$ , we can write down the output on $y$ in the second layer as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{y_{t}^{2}}=y_{t}+\\omega_{y x}\\displaystyle\\sum_{i=1}^{n}y_{i}(x_{i}^{1})^{\\top}x_{t}}\\\\ &{\\quad\\approx y_{t}+\\omega_{y x}\\displaystyle[\\sum_{i=1}^{n}y_{i}x_{i}+\\omega_{x y}\\displaystyle\\sum_{i=1}^{n}y_{i}r_{i}\\Sigma w^{*}]x_{t}}\\\\ &{\\quad=y_{t}+\\omega_{y x}(1+\\omega_{x y}\\displaystyle\\sum_{i=1}^{n}r_{i}^{2})(\\Sigma w^{*})^{\\top}x_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "p1ft33Mu3J/tmp/5e74c8b520e76e737b779d5f584ffdf11b7311fab0d926947be11922f844da05.jpg", "img_caption": ["Figure 2: Per-variance proflie of models behavior for uniform noise variance $\\sigma_{\\tau}\\sim U(0,\\sigma_{m a x})$ . Top two rows: 7-layer models with varying $\\sigma_{m a x}$ . Bottom row: models with varying numbers of layers, fixed $\\sigma_{m a x}=5$ . In-distribution noise is shaded gray. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Here we used the properties of residual $r_{i}$ (in particular $\\textstyle\\sum_{i}y_{i}x_{i}=\\Sigma w^{*}$ , and $\\textstyle\\sum_{i}y_{i}r_{i}=\\sum_{i}r_{i}^{2})$ . Note that $(\\Sigma{w^{*}})^{\\top}x_{t}$ is basically what a gradient descent step on the original input should do. Therefore effectively, the two-layer network is doing gradient descent, but the step size is the product of $-\\omega_{y x}$ and $(1+\\dot{\\omega}_{x y}\\sum_{i}r_{i}^{2})$ . The factor $(1+\\omega_{x y}\\sum_{i}r_{i}^{2})$ depends on the level of noise, and when \u03c9 ${}_{x y},\\omega_{y x}<0$ , the effective step size is smaller when there is more noise. This is especially helpful in the model selection problem, because intuitively one would like to perform early-stopping (small step sizes) when the noise is high. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we investigate the training dynamics of linear transformers when trained with a mixed noise variance problem (7). We evaluate three types of single-head linear transformer models: ", "page_idx": 7}, {"type": "text", "text": "\u2022 FULL. Trains full parameter matrices.   \n\u2022 DIAG. Trains diagonal parameter matrices (10).   \n\u2022 $\\mathrm{GD^{++}}$ . An even more restricted diagonal variant defined in (11). ", "page_idx": 7}, {"type": "text", "text": "For each experiment, we train each linear transformer modifications with a varying number of layers (1 to 7) using using Adam optimizer for 200 000 iterations with a learning rate of 0.0001 and a batch size of 2 048. In some cases, especially for a large number of layers, we had to adjust the learning rate to prevent stability issues. We report the best result out of 5 runs with different training seeds. We used $N=20$ in-context examples in $D=10$ dimensions. We evaluated the algorithm using $100\\,000$ novel sequences. All the experiments were done on a single H100 GPU with 80GB of VRAM. It took on average 4\u201312 hours to train a single algorithm, however experimenting with different weight decay parameters, better optimizer and learning rate schedule will likely reduce this number dramatically. ", "page_idx": 7}, {"type": "text", "text": "We use adjusted evaluation loss as our main performance metric. It is calculated by subtracting the oracle loss from the predictor\u2019s loss. The oracle loss is the closed-form solution of the ridge regression loss (5), assuming the noise variance $\\sigma_{\\tau}$ is known. The adjusted evaluation loss allows for direct model performance comparison across different noise variances. This is important because higher noise significantly degrades the model prediction. Our adjustment does not affect the model\u2019s optimization process, since it only modifies the loss by an additive constant. ", "page_idx": 7}, {"type": "text", "text": "Baseline estimates. We evaluated the linear transformer against a closed-form solution to the ridge regression problem (5). We estimated the noise variance $\\sigma_{\\tau}$ using the following methods: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Constant Ridge Regression (CONSTRR). The noise variance is estimated using a single scalar value for all the sequences, tuned separately for each mixed variance problem. ", "page_idx": 7}, {"type": "image", "img_path": "p1ft33Mu3J/tmp/f6b4e69fcc587b297c972d319758fda111749d77a35d0070422efe60369122f8.jpg", "img_caption": ["Figure 3: In-context learning performance for noisy linear regression across models with varying number of layers for conditional noise variance $\\sigma_{\\tau}\\in\\{1,3\\}$ and $\\sigma_{\\tau}\\in\\{1,3,5\\}$ . Top: loss for models with various number of layers and per-variance proflie for models with 7 layers. Bottom: Per-variance profile of the model across different numbers of layers. In-distribution noise is shaded gray. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "\u2022 Adaptive Ridge Regression (ADARR). Estimate the noise variance via unbiased estimator (Cherkassky & Ma, 2003) \u03c3e2st =n1\u2212d jn=1(yj \u2212y\u02c6j)2, where y\u02c6j represents the solution to the ordinary least squares (4), found in a closed-form.   \n\u2022 Tuned Adaptive Ridge Regression (TUNEDRR). Same as above, but after the noise is estimated, we tuned two additional parameters to minimize the evaluation loss: (1) a max. threshold value for the estimated variance, (2) a multiplicative adjustment to the noise estimator. These values are tuned separately for each problem. ", "page_idx": 8}, {"type": "text", "text": "Notice that all the baselines above are based on ridge regression, which is a closed-form, non-iterative solution. Thus, they have an algorithmic advantage over linear transformers that do not have access to matrix inversion. These baselines help us gauge the best possible performance, establishing an upper bound rather than a strictly equivalent comparison. ", "page_idx": 8}, {"type": "text", "text": "A more faithful comparison to our method would be an iterative version of the ADARR that does not use matrix inversion. Instead, we can use gradient descent to estimate the noise and the solution to the ridge regression. However, in practice, this gradient descent estimator converges to ADARR only after $\\approx100$ iterations. In contrast, linear transformers typically converge in fewer than 10 layers. ", "page_idx": 8}, {"type": "text", "text": "We consider two choices for the distribution of $\\sigma_{\\tau}$ : ", "page_idx": 8}, {"type": "text", "text": "\u2022 Uniform. $\\sigma_{\\tau}\\sim U(0,\\sigma_{m a x})$ drawn from a uniform distribution bounded by $\\sigma_{m a x}$ . We tried multiple scenarios with $\\sigma_{m a x}$ ranging from 0 to 7. \u2022 Categorical. $\\sigma_{\\tau}\\in S$ chosen from a discrete set $S$ . We tested $S=\\{1,3\\}$ and $S=\\{1,3,5\\}$ . ", "page_idx": 8}, {"type": "text", "text": "Our approach generalizes the problem studied by Bai et al. (2023), who considered only categorical variance selection and show experiments only with two $\\sigma_{\\tau}$ values. ", "page_idx": 8}, {"type": "text", "text": "Uniform noise variance. For the uniform noise variance, Fig. 1 shows that FULL and DIAG achieve comparable performance across different numbers of layers and different $\\sigma_{m a x}$ . On the other hand, $\\mathrm{GD}^{\\bar{+}+}$ converges to a higher value, closely approaching the performance of the CONSTRR baseline. ", "page_idx": 8}, {"type": "text", "text": "As $\\sigma_{m a x}$ grows, linear transformers show a clear advantage over the baselines. With 4 layers, they outperform the closed-form solution ADARR for $\\sigma_{m a x}=4$ and larger. Models with 5 or more layers match or exceed the performance of TUNEDRR. ", "page_idx": 8}, {"type": "text", "text": "The top of Fig. 2 offers a detailed perspective on performance of 7-layer models and the baselines. Here, we computed per-variance profiles across noise variance range from 0 to $\\sigma_{m a x}+1$ . We can see that poor performance of $\\mathrm{GD^{++}}$ comes from its inability to estimate well across the full noise variance range. Its performance closely mirrors to CONSTRR, suggesting that $\\mathrm{GD^{++}}$ under the hood might also be estimating a single constant variance for all the data. ", "page_idx": 8}, {"type": "text", "text": "ADARR perfectly estimates problems with no noise, but struggles more as noise variance increases. TUNEDRR slightly improves estimation by incorporating $\\sigma_{m a x}$ into its tunable parameters, yet its prediction suffers in the mid-range. FULL and DIAG demonstrate comparable performance across all noise variances. While more research is needed to definitively confirm or deny their equivalence, we believe that these models are actually not identical despite their similar performance. ", "page_idx": 8}, {"type": "image", "img_path": "p1ft33Mu3J/tmp/e166c33e3c20f3e4d666490883d9e4bcec8d2cd104e8600433711e26186d6e61.jpg", "img_caption": ["Figure 4: Weights for 4 layer linear transformer with FULL parametrization trained with categorical noise $\\sigma_{\\tau}\\in\\{1,3\\}$ . Top: weights for $Q^{l}$ matrix, bottom: weights for $P^{l}$ matrix. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "At the bottom of Fig. 2 we set the noise variance to $\\sigma_{m a x}=5$ and display a per-variance profile for models with varying layers. Two-layer models for FULL and DIAG behave akin to $\\mathrm{GD^{++}}$ , modeling only a single noise variance in the middle. However, the results quickly improve across the entire noise spectrum for 3 or more layers. In contrast, $\\mathrm{GD^{++}}$ quickly converges to a suboptimal solution. ", "page_idx": 9}, {"type": "text", "text": "Categorical noise variance. Fig. 3 shows a notable difference between DIAG and FULL models for categorical noise variance $\\sigma_{\\tau}\\'\\in\\{1,3\\}$ . This could stem from a bad local minima, or suggest a fundamental difference between the models for this problem. Interestingly, from per-variance profliing we see that DIAG extrapolates better for variances not used for training, while FULL, despite its lower in-distribution error, performs worse on unseen variances. Fig. 4 shows learned weights of the 4 layer linear transformer with FULL parametrization. The weights are very diagonal heavy, potentially with some low-rank component. ", "page_idx": 9}, {"type": "text", "text": "For $\\sigma_{\\tau}\\in\\{1,3,5\\}$ , examining the per-variance profile at the bottom of Fig. 3 reveals differences in their behaviors. FULL exhibits a more complex per-variance profile with more fluctuations than the diagonal model, suggesting greater representational capacity. Surprisingly, it did not translate to better loss results compared to DIAG. ", "page_idx": 9}, {"type": "text", "text": "For easy comparison, we compile the results of all methods and baselines in Table 1 in the Appendix. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our research reveals the surprising ability of linear transformers to tackle challenging in-context learning problems. We show that each layer maintains an implicit linear regression model, akin to a complex variant of preconditioned gradient descent with momentum-like behavior. ", "page_idx": 9}, {"type": "text", "text": "Remarkably, when trained on noisy linear regression problems with unknown noise variance, linear transformers not only outperform standard baselines but also uncover a sophisticated optimization algorithm that incorporates noise-aware step-size adjustments and rescaling. This discovery highlights the potential of linear transformers to automatically discover novel optimization algorithms when presented with the right problems, opening exciting avenues for future research, including automated algorithm discovery using transformers and generalization to other problem domains. ", "page_idx": 9}, {"type": "text", "text": "While our findings demonstrate the impressive capabilities of linear transformers in learning optimization algorithms, we acknowledge limitations in our work. These include the focus on simplified linear models, analysis of primarily diagonal attention matrices, and the need for further exploration into the optimality of discovered algorithms, generalization to complex function classes, scalability with larger datasets, and applicability to more complex transformer architectures. We believe these limitations present valuable directions for future research and emphasize the need for a deeper understanding of the implicit learning mechanisms within transformer architectures. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Nolan Miller and Andrey Zhmoginov for their valuable suggestions and feedback throughout the development of this project. Part of this work was done while Rong Ge was visiting Google Research. Rong Ge\u2019s research is supported in part by NSF Award DMS-2031849 and CCF-1845171 (CAREER). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nKwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023.   \nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.   \nEkin Aky\u00fcrek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-Context language learning: Architectures and algorithms. arXiv preprint arXiv:2401.12973, 2024.   \nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.   \nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nStephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022.   \nXiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent to learn non-linear functions in context. arXiv preprint arXiv:2312.06528, 2023.   \nVladimir Cherkassky and Yunqian Ma. Comparison of model selection for regression. Neural computation, 15(7):1691\u20131714, 2003.   \nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.   \nDeqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086, 2023.   \nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.   \nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.   \nTianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do transformers learn in-context beyond simple functions? a case study on learning with representations. arXiv preprint arXiv:2310.10616, 2023.   \nRoee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. arXiv preprint arXiv:2310.15916, 2023.   \nYu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.   \nEvan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from learned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820, 2019.   \nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 5156\u20135165. PMLR, 2020.   \nJannik Kossen, Tom Rainforth, and Yarin Gal. In-context learning in large language models learns label relationships but is not conventional learning. arXiv preprint arXiv:2307.12375, 2023.   \nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pp. 19565\u201319594. PMLR, 2023.   \nArvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023.   \nSuvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023.   \nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.   \nReese Pathak, Rajat Sen, Weihao Kong, and Abhimanyu Das. Transformers can optimally learn regression mixture models. arXiv preprint arXiv:2311.08362, 2023.   \nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355\u20139366. PMLR, 2021.   \nLingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent? arXiv preprint arXiv:2310.08540, 2023.   \nDavoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \nYuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023a.   \nYuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023b.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nJohannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 35151\u201335174. PMLR, 2023a.   \nJohannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023b.   \nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.   \nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.   \nKaiyue Wen, Yuchen Li, Bingbin Liu, and Andrej Risteski. Transformers are uninterpretable with myopic methods: a case study with bounded dyck grammars. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nSteve Yadlowsky, Lyric Doshi, and Nilesh Tripuraneni. Pretraining data mixtures enable narrow model selection capabilities in transformer models. arXiv preprint arXiv:2311.00871, 2023.   \nRuiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs from Sections 4 and 5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We first give the proof for Theorem 4.1. In the process we will also prove Lemma 4.3, as Theorem 4.1 follows immediately from an induction based on the lemma. ", "page_idx": 13}, {"type": "text", "text": "Proof. We do this by induction. At $l\\,=\\,0$ , it\u2019s easy to check that we can set $a^{(0)}\\,=\\,1,w^{(0)}\\,=$ $0,\\bar{M}^{(0)}=I,u^{(0)}=\\bar{0}$ . ", "page_idx": 13}, {"type": "text", "text": "Suppose this is true for some layer $l$ , if the weights of layer $l$ are $(P_{1}^{l},Q_{1}^{l}),...,(P_{k}^{l},Q_{k}^{l})$ for $k$ heads, at output of layer $l+1$ we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{l}{x_{i}^{l+1}}\\\\ {y_{i}^{l+1}}\\end{array}\\right)=\\left(\\begin{array}{l}{x_{i}^{l}}\\\\ {y_{i}^{l}}\\end{array}\\right)+\\sum_{k=1}^{p}\\left[P_{k}^{l}\\sum_{j=1}^{n}\\left(\\left(\\begin{array}{l}{x_{j}^{l}}\\\\ {y_{j}^{l}}\\end{array}\\right)((x_{j}^{l})^{\\top},y_{j}^{l})\\right)Q_{k}^{l}\\right]\\left(\\begin{array}{l}{x_{i}^{l}}\\\\ {y_{i}^{l}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that the same equation is true for $i=n+1$ just by letting $y_{n+1}=0$ . Let the middle matrix has the following structure: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{l l}{A}&{b}\\\\ {c^{\\top}}&{d}\\end{array}\\right):=\\sum_{k=1}^{p}\\left[P_{k}^{l}\\sum_{j=1}^{n}\\left(\\left(\\begin{array}{l}{x_{j}^{l}}\\\\ {y_{j}^{l}}\\end{array}\\right)((x_{j}^{l})^{\\top},y_{j}^{l})\\right)Q_{k}^{l}\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then one can choose the parameters of the next layer as in Lemma 4.3 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M^{l+1}=(I+A)M^{l}+b(w^{l})^{\\top}}\\\\ &{\\;u^{l+1}=(I+A)u^{l}+a^{l}b}\\\\ &{\\;a^{l+1}=(1+d)a^{l}+\\langle c,u^{l}\\rangle}\\\\ &{\\;w^{l+1}=(1+d)w^{l}-(M^{l})^{\\top}c.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "One can check that this choice satisfies (12). ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This lemma is in fact a corollary of Lemma 4.3. We first give a more detailed version which explicitly state the unknown matrices $\\Lambda^{l^{\\prime}},\\Gamma^{l},\\Pi^{l},\\Phi^{l}$ : ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. In the setup of Theorem 4.1 with diagonal parameters (9), one can recursively compute matrices $u^{l},w^{l}$ using the following formula ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u^{l+1}=\\left((1+\\omega_{x y}^{l}(a^{l})^{2}\\rho)I+\\omega_{x x}^{l}\\Sigma^{l}\\right)u^{l}}\\\\ &{\\qquad+a^{l}\\omega_{x y}^{l}\\left(M^{l}+a^{l}u^{l}(w^{*})^{\\top}\\right)\\Sigma\\left(a^{l}w^{*}-w^{l}\\right),}\\\\ &{w^{l+1}=(1+\\omega_{y y}^{l}\\lambda^{l})w^{l}}\\\\ &{\\qquad-\\omega_{y x}^{l}(M^{l})^{\\top}(M^{l}+a^{l}u^{l}(w^{*})^{\\top})\\Sigma(a^{l}w^{*}-w^{l})}\\\\ &{\\qquad-a^{l}\\rho\\omega_{y x}^{l}(M^{l})^{\\top}u^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\textstyle\\rho=\\sum_{i=1}^{n}r_{i}^{2}$ and initial conditions $a^{0}=1,w^{0}=0,M^{0}=I,u^{0}=0$ ", "page_idx": 13}, {"type": "text", "text": "Proof. First, we compute the following matrix that appeared in Lemma 4.3 for the specific diagonal case: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\begin{array}{c c}{A^{l}}&{b^{l}}\\\\ {(c^{l})^{\\top}}&{d^{l}}\\end{array}\\right)=\\displaystyle\\sum_{k=1}^{p}\\left[P_{k}^{l}\\displaystyle\\sum_{j=1}^{n}\\left(\\left(\\begin{array}{c}{x_{j}^{l}}\\\\ {y_{j}^{l}}\\end{array}\\right)((x_{j}^{l})^{\\top},y_{j}^{l})\\right)Q_{k}^{l}\\right],}\\\\ &{\\qquad=\\left(\\begin{array}{c c}{\\omega_{x x}^{l}\\Sigma^{l}}&{\\omega_{x y}^{l}\\alpha^{l}}\\\\ {\\omega_{y x}^{l}(\\alpha^{l})^{\\top}}&{\\omega_{y y}^{l}\\lambda^{l}}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This implies that $A^{l}=\\omega_{x x}^{l}\\Sigma^{l},b^{l}=\\omega_{x y}^{l}\\alpha^{l},c^{l}=\\omega_{y x}^{l}\\alpha^{l}$ and $d^{l}=\\omega_{y y}^{l}\\lambda^{l}$ . Next we rewrite $\\alpha^{l}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha^{l}=\\displaystyle\\sum_{i=1}^{n}y^{l}x^{l}}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{n}(a^{l}y_{i}-\\langle w^{l},x_{i}\\rangle)(M^{l}x_{i}+y_{i}u^{l})}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{n}(a^{l}r_{i}+\\langle a^{l}w^{*}-w^{l},x_{i}\\rangle)((M^{l}+a^{l}u^{l}(w^{*})^{\\top})x_{i}+r_{i}u^{l})}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{n}\\langle a^{l}w^{*}-w^{l},x_{i}\\rangle(M^{l}+a^{l}u^{l}(w^{*})^{\\top})x_{i}+\\displaystyle\\sum_{i=1}^{n}a^{l}r_{i}^{2}u^{l}}\\\\ &{\\quad=(M^{l}+a^{l}u^{l}(w^{*})^{\\top})\\Sigma(a^{l}w^{*}-w^{l})+a^{l}\\rho u^{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here the first step is by Theorem 4.1, the second step replaces $y_{i}$ with $\\langle w^{*},x_{i}\\rangle+r_{i}$ , the third step uses the fact that $\\textstyle\\sum_{i=1}^{n}r_{i}x_{i}=0$ to get rid of the cross terms. ", "page_idx": 14}, {"type": "text", "text": "The remaining proof just substitutes the formula for $\\alpha^{l}$ into Lemma 4.3. ", "page_idx": 14}, {"type": "text", "text": "Now Lemma A.1 implies Lemma 4.4 immediately by setting $\\Lambda^{l}~=~-\\omega_{x y}^{l}(a^{l})^{2}\\rho I\\,-\\,\\omega_{x x}^{l}\\Sigma^{l}$ , $\\Gamma^{l}\\,=\\,a^{l}\\omega_{x y}^{l}\\left(M^{l}+a^{l}u^{l}(w^{\\ast})^{\\top}\\right)$ , $s^{l}\\,=\\,t u\\omega_{y y}\\lambda^{l}$ , $\\Pi^{l}\\,=\\,\\omega_{y x}^{l}(M^{l})^{\\top}(M^{l}+a^{l}u^{\\bar{l}}(w^{*})^{\\top})$ and $\\Phi^{l}\\,=$ $a^{l}\\rho\\omega_{y x}^{l}(M^{l})^{\\top}$ . ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof for Theorem 4.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. By Theorem 4.1, we know $y_{i}^{l}=\\langle w^{l},x_{i}\\rangle$ for some $w^{l}$ . When $n\\gg d$ , with high probability the norm of $y^{l}$ is on the order of $\\Theta({\\sqrt{n}})\\|w^{l}\\|$ , and the norm of $y^{\\ast}$ is $\\Theta({\\sqrt{n}})$ . Therefore we only need to bound the correlation. The correlation is equal to ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\langle y^{*},y^{l}\\rangle|=|w_{1}^{l}\\sum_{i=1}^{n}x_{1}^{3}+\\sum_{i=1}^{n}x_{i}(1)^{2}\\sum_{j=2}^{d}w_{j}^{l}x_{i}(j)|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We know with high probability $\\textstyle|\\sum_{i=1}^{n}x_{i}^{3}|=O({\\sqrt{n}})$ because $\\mathbb{E}[x_{i}^{3}]=0$ . The second term can be written as $\\langle w^{l},v\\rangle$ where $v$ is a vector whose coordinates\u221a are $v_{1}=0$ and $\\begin{array}{r}{v_{j}=\\sum_{i=1}^{n}x_{i}(1)^{2}x_{i}(j)}\\end{array}$ for $2\\leq j\\leq d$ , therefore with high probability $\\|v\\|=O({\\sqrt{n d}})$ . Therefore, with high probability the cosine similarity is at most ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{|\\langle y^{*},y^{l}\\rangle|}{\\|y^{*}\\|\\|y^{l}\\|}=O(1)\\frac{|\\langle y^{*},y^{l}\\rangle|}{n\\|w^{l}}}\\\\ &{\\qquad\\qquad=O(1)\\frac{|w_{1}^{l}\\sum_{i=1}^{n}x_{1}^{3}+\\sum_{i=1}^{n}x_{i}(1)^{2}\\sum_{j=2}^{d}w_{j}^{l}x_{i}(j)|}{n\\|w^{l}}}\\\\ &{\\qquad\\qquad\\leq O(1)\\frac{|w_{1}^{l}||\\sum_{i=1}^{n}x_{1}^{3}|+|w^{l}|\\|v\\|}{n\\|w^{l}}}\\\\ &{\\qquad\\qquad\\leq O(1)\\frac{\\sqrt{n}+\\sqrt{n d}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $n\\gg d$ this can be made smaller than any fixed constant. ", "page_idx": 14}, {"type": "text", "text": "A.4 Proof for Theorem 5.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section we prove Theorem 5.1 by finding hyperparameters for $\\mathrm{GD^{++}}$ algorithm that solves least squares problems with very high accuracy. The first steps in the construction iteratively makes the data $x_{i}$ \u2019s better conditioned, and the last step is a single step of gradient descent. The proof is based on several lemma, first we observe that if the data is very well-conditioned, then one-step gradient descent solves the problem accurately: ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. Given $(x_{1},y_{1}),...,(x_{n},y_{n})$ where $\\textstyle\\Sigma:=\\sum_{i=1}^{n}x_{i}x_{i}^{\\top}$ has eigenvalues between 1 and $1+\\epsilon$ . Let $\\begin{array}{r}{\\boldsymbol{w}^{*}:=\\arg\\operatorname*{min}_{\\boldsymbol{w}}\\sum_{i=1}^{n}(y_{i}-\\langle\\boldsymbol{w},\\boldsymbol{x}_{i}\\rangle)^{2}}\\end{array}$ be the optimal least squares solution, then $\\hat{w}=$ $\\textstyle\\sum_{i=1}^{n}y_{i}x_{i}$ satisfies $\\lVert\\boldsymbol{\\hat{w}}-\\boldsymbol{w^{*}}\\rVert\\leq\\epsilon\\lVert\\boldsymbol{w^{*}}\\rVert$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. We can write $y_{i}=\\left\\langle x_{i},w^{*}\\right\\rangle+r_{i}$ . By the fact that $w^{*}$ is the optimal solution we know $r_{i}$ \u2019s satisfy $\\textstyle\\sum_{i=1}^{n}r_{i}x_{i}=0$ . Therefore $\\begin{array}{r}{\\hat{w}=\\sum_{i=1}^{n}y_{i}x_{i}=\\sum_{i=1}^{n}\\langle x_{i},w^{*}\\rangle x_{i}\\stackrel{\\cdot}{=}\\Sigma w^{*}}\\end{array}$ . This implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{w}-w^{*}\\|=\\|(\\Sigma-I)w^{*}\\|\\leq\\|\\Sigma-I\\|\\|w^{*}\\|\\leq\\epsilon\\|w^{*}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next we show that by applying just the preconditioning step of $\\mathrm{GD^{++}}$ , one can get a well-conditioned $x$ matrix very quickly. Note that the $\\Sigma$ matrix is updated as $\\Sigma\\,\\leftarrow\\,(I\\mathrm{~-~}\\gamma\\Sigma)\\Sigma(I\\mathrm{~-~}\\gamma\\Sigma)$ , so an eigenvalue of $\\lambda$ in the original $\\Sigma$ matrix would become $\\lambda(1-\\gamma\\lambda)^{2}$ . The following lemma shows that this transformation is effective in shrinking the condition number ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. Suppose $\\nu/\\mu\\,=\\,\\kappa\\,\\geq\\,1.1,$ , then there exists an universal constant $c<1$ such that choosing $\\gamma\\nu=1/3$ implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{\\lambda\\in[\\nu,\\mu]}\\lambda(1-\\gamma\\lambda)^{2}}{\\operatorname*{min}_{\\lambda\\in[\\nu,\\mu]}\\lambda(1-\\gamma\\lambda)^{2}}\\leq c\\kappa.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, $i f\\nu/\\mu=\\kappa\\leq1+\\epsilon$ where $\\epsilon\\leq0.1$ , then choosing $\\gamma\\nu=1/3$ implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{\\lambda\\in[\\nu,\\mu]}\\lambda(1-\\gamma\\lambda)^{2}}{\\operatorname*{min}_{\\lambda\\in[\\nu,\\mu]}\\lambda(1-\\gamma\\lambda)^{2}}\\leq1+2\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The first claim shows that one can reduce the condition number by a constant factor in every step until it\u2019s a small constant. The second claim shows that once the condition number is small $(1+\\epsilon)$ , each iteration can bring it much closer to 1 (to the order of $1+O(\\epsilon^{2}))$ . ", "page_idx": 15}, {"type": "text", "text": "Now we prove the lemma. ", "page_idx": 15}, {"type": "text", "text": "Proof. First, notice that the function $f(x)~=~x(1-\\gamma x)^{2}$ is monotonically nondecreasing for $x\\in[0,\\nu]$ if $\\gamma\\nu=1/3$ (indeed, it\u2019s derivative $f^{\\prime}(x)=(1-\\gamma x)(1-3\\gamma x)$ is always nonnegative). Therefore, the max is always achieved at $x=\\nu$ and the min is always achieved at $x=\\mu$ . The new ratio is therefore ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\nu(1-\\gamma\\nu)^{2}}{\\mu(1-\\gamma\\mu)^{2}}=\\kappa\\frac{4/9}{(1-1/3\\kappa)^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $\\kappa\\geq1.1$ the ratio $\\frac{4/9}{(1\\!-\\!1/3\\kappa)^{2}}$ is always below $\\frac{4/9}{(1-1/3.3)^{2}}$ which is a constant bounded away from 1. ", "page_idx": 15}, {"type": "text", "text": "When $\\kappa=1+\\epsilon<1.1$ , we can write down the RHS in terms of $\\epsilon$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\nu(1-\\gamma\\nu)^{2}}{\\mu(1-\\gamma\\mu)^{2}}=\\kappa\\frac{4/9}{(1-1/3\\kappa)^{2}}=(1+\\epsilon)(1+\\frac{1}{2}(1-\\frac{1}{1+\\epsilon}))^{-2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that by the careful choice of $\\gamma$ , the RHS has the following Taylor expansion: ", "page_idx": 15}, {"type": "equation", "text": "$$\n(1+\\epsilon)(1+\\frac{1}{2}(1-\\frac{1}{1+\\epsilon}))^{-2}=1+\\frac{3\\epsilon^{2}}{4}-\\frac{5\\epsilon^{3}}{4}+O(\\epsilon^{4}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "One can then check the RHS is always upperbounded by $2\\epsilon^{2}$ when $\\epsilon<0.1$ . ", "page_idx": 15}, {"type": "text", "text": "With the two lemmas we are now ready to prove the main theorem: ", "page_idx": 15}, {"type": "text", "text": "Proof. By Lemma A.3 we know in $O(\\log\\kappa+\\log\\log1/\\epsilon)$ iterations, by assigning $\\kappa$ in the way of Lemma A.3 one can reduce the condition number of $x$ to $\\kappa^{\\prime}\\leq1+\\epsilon/2\\kappa$ (we chose $\\epsilon/2\\kappa$ here to give some slack for later analysis). ", "page_idx": 15}, {"type": "text", "text": "Let $\\Sigma^{\\prime}$ be the covariance matrix after these iterations, and $\\nu^{\\prime},\\mu^{\\prime}$ be the upper and lowerbound for its eigenvalues. The data $x_{i}$ \u2019s are transformed to a new data $x_{i}^{\\prime}\\;=\\;M x_{i}$ for some matrix ", "page_idx": 15}, {"type": "image", "img_path": "p1ft33Mu3J/tmp/f9970ab1c1c08f15fb35a9bffae4de3b537433cc73a78ebfdcce8d1fdbfe28d5.jpg", "img_caption": ["Figure 5: Linear transformer models show a consistent decrease in error per layer when trained on data with mixed noise variance $\\sigma_{\\tau}\\sim U(0,5)$ . The error bars measure variance over 5 training seeds. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "$M$ . Let $M\\,=\\,A\\Sigma^{-1/2}$ , then since $M^{\\prime}\\,=\\,A A^{\\top}$ we know $A$ is a matrix with singular values between $\\sqrt{\\mu^{\\prime}}$ and $\\sqrt{\\nu^{\\prime}}$ . The optimal solution $(w^{*})^{\\prime}=M^{-\\top}w^{*}$ has norm at most $\\sqrt{\\nu}/\\sqrt{\\mu^{\\prime}}\\|w^{*}\\|$ . Therefore by Lemma A.2 w\u221ae know the one-step gradient step with $\\begin{array}{r}{\\hat{w}\\;=\\;\\sum_{i=1}^{n}\\frac{1}{\\mu^{\\prime}}{\\dot{y}}_{i}x_{i}}\\end{array}$ satisfy $\\|\\hat{w}-(w^{*})^{\\prime}\\|\\leq(\\kappa^{\\prime}-1)\\sqrt{\\nu}/\\sqrt{\\mu^{\\prime}}\\|w^{*}\\|$ . The test data $x_{t}$ is also transformed to $x_{t}^{\\prime}=A\\Sigma^{-1/2}x_{t}$ , and the algorithm outputs $\\langle\\hat{w},x_{t}^{\\prime}\\rangle$ , so the error is at most $\\sqrt{\\nu}\\|w^{*}\\|*\\|x_{t}^{\\prime}\\|\\leq(\\kappa^{\\prime}-1)\\sqrt{\\kappa}\\sqrt{\\kappa^{\\prime}}\\|w^{*}\\|*\\|x_{t}\\|$ . By the choice of $\\kappa^{\\prime}$ we can check that RHS is at most $\\epsilon\\|w^{*}\\|\\|x_{t}\\|$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.5 Proof of the Theorem 5.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The key observation here is that when $\\begin{array}{r l r}{\\operatorname*{lim}_{n\\to\\infty}\\sum_{i=1}^{n}\\stackrel{\\bullet}{x}_{i}x_{i}^{\\top}}&{=}&{I}\\end{array}$ . Therefore the ridge regression solutions converge to $n\\_\\rightarrow\\_\\infty$ , under the assumptions we have $\\begin{array}{r l}{w_{\\sigma^{2}}^{*}}&{{}=}\\end{array}$ $\\textstyle{\\frac{1}{1+\\sigma^{2}}}\\sum_{i=1}^{n}y_{i}x_{i}$ and the desired output is $\\langle w_{\\sigma^{2}}^{*},x_{q}\\rangle$ . ", "page_idx": 16}, {"type": "text", "text": "By the calculations before, we know after the first-layer, the implicit $w$ is $\\begin{array}{r}{w^{1}=\\omega_{y x}\\sum_{i=1}^{n}y_{i}x_{i}}\\end{array}$ . As long as $\\omega_{y x}$ is a constant, when $n\\to\\infty$ we know $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}(y_{i}^{1})^{2}=\\sigma^{2}$ (as the part of $y$ that depend on $x$ is negligible compared to noise), therefore the output of the second layer satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\nw^{2}=(1+n\\sigma^{2}\\omega_{y y})w^{1}=(1+n\\sigma^{2}\\omega_{y y})\\omega_{y x}\\sum_{i=1}^{n}y_{i}x_{i}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, as long as we choose $\\omega_{y x}$ and $\\omega_{y y}$ to satisfy $\\begin{array}{r}{(1+n\\sigma^{2}\\omega_{y y})\\omega_{y x}=\\frac{1}{1+\\sigma^{2}}}\\end{array}$ when $\\sigma=\\sigma_{1}$ or $\\sigma_{2}$ (notice that these are two linear equations on $\\omega_{y x}$ and $n\\omega_{y x}\\omega_{y y}$ , so they always have a solution), then we have $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}w^{2}=w_{\\sigma^{2}}^{*}}\\end{array}$ for the two noise levels. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B More experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we provide results of additional experiments that did not make it to the main text. ", "page_idx": 16}, {"type": "text", "text": "Fig. 6 shows an example of unadjusted loss. Clearly, it is virtually impossible to compare the methods across various noise levels this way. ", "page_idx": 16}, {"type": "text", "text": "Fig. 7 shows per-variance profile of intermediate predictions of the network of varying depth. It appears that $\\bar{\\mathrm{GD}}^{++}$ demonstrates behavior typical of GD-based algorithms: early iterations model higher noise (similar to early stopping), gradually converging towards lower noise predictions. DIAG exhibits this patter initially, but then dramatically improves, particularly for lower noise ranges. Intriguingly, FULL displays the opposite trend, first improving low-noise predictions, followed by a decline in higher noise prediction accuracy, especially in the last layer. ", "page_idx": 16}, {"type": "text", "text": "Finally, Table 1 presents comprehensive numerical results for our experiments across various mixed noise variance models. For each model variant (represented by a column), the best-performing result is highlighted in bold. ", "page_idx": 16}, {"type": "image", "img_path": "p1ft33Mu3J/tmp/a7e3b1e8d7e738b26ef325d0676f8c12761f6ccece047a2ce673c2326cccdb28.jpg", "img_caption": ["Figure 6: Example of unadjusted loss given by directly minimizing (7). It is pretty hard to see variation between comparable methods using this loss directly. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "p1ft33Mu3J/tmp/b928de014c028f6c6e83297782a9e4a7bc493e2437ec4b5a4a0ae8c34639fa2f.jpg", "table_caption": [], "table_footnote": ["Table 1: Adjusted evaluation loss for models with various number of layers with uniform noise variance $\\sigma_{\\tau}\\sim U(0,\\sigma_{m a x})$ . We highlight in bold the best results for each problem setup (i.e. each column). "], "page_idx": 17}, {"type": "image", "img_path": "p1ft33Mu3J/tmp/df27cee42ac605f3ae267dfab24dfcbaf4815b17810ebb8cc58caa5872281931.jpg", "img_caption": ["Figure 7: Layer by layer prediction quality for different models with $\\sigma_{\\tau}\\sim U(0,5)$ . The error bars measure std over 5 training seeds. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction accurately state the paper\u2019s main claims: that linear transformers can learn optimization algorithms, that they maintain an implicit linear regression model at each layer, and that they can handle noisy linear regression problems with varying noise levels, surpassing many reasonable baselines. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We acknowledge that the analysis is limited to linear transformers and doesn\u2019t consider the effects of non-linearities in larger models. We also mentioned in the paper that the algorithm discovered for mixed noise variance is not proven to be optimal, and the analysis focuses primarily on the diagonal attention case. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide the full set of assumptions and complete proofs for all theoretical results. The proofs are detailed and appear in the Appendix, which is clearly referenced in the main text. Each theorem and lemma is numbered, and all assumptions are clearly stated. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We believe that we provide sufficient details on data generation, model variants, training parameters, and evaluation metrics to allow anyone to replicate the core experiments and validate the main claims. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 20}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] , ", "page_idx": 21}, {"type": "text", "text": "Justification: We talk about synthetic data only, which is very easy to generate. The code, while net released at this stage, will be released after the paper\u2019s acceptance. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide sufficient detail on the training and test procedures. We specify the optimizer, learning rate, batch size, number of iterations, and how hyperparameters were tuned (including adjustming the the learning rate for stability). The data generation process is also detailed, along with the evaluation metrics and baselines used. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Some figures (like Figs. 2 and 6) include the errorbars, however, the main figures (Figs. 1,3,5) are reported as best of of 5 training seeds. We have acknowledged that some runs have stability issues which result in models stuck in a high energy region of the loss. These outliers would affect the mean significantly, since our loss is already at $10^{-2}$ level. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We disclose all the relevant information on the compute resources that we used to produce the results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper\u2019s research focuses on theoretical analysis and simulated experiments on linear regression, which doesn\u2019t present any obvious ethical concerns as outlined in the NeurIPS Code of Ethics. The study doesn\u2019t involve human subjects, sensitive data, or potential real-world harms that would require further scrutiny. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work focuses primarily on the theoretical analysis of linear transformers and their ability to learn optimization algorithms in a simplified setting. As such, it does not directly translate into specific applications with immediate societal impacts. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work does not involve the release of any trained models or datasets, as it primarily focuses on theoretical analysis and simulated experiments. Therefore, safeguards for responsible release are not applicable in this context. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our work does not utilize any existing code, data, or pre-trained models. All experiments were conducted using synthetic data and models implemented specifically for this study. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This work does not introduce any new assets such as code, data, or pre-trained models. We focus on theoretical analysis and use synthetically generated data for our experiments, so we are not releasing any assets alongside this paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This research does not involve any crowdsourcing or experiments with human subjects. Our study is purely analytical and based on simulated data. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our research does not involve any human participants. The study is purely analytical and experimental, utilizing simulated data and not involving any interaction or data collection from human subjects. Therefore, IRB approval or considerations for participant risks are not applicable. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]