[{"figure_path": "WSsht66fbC/tables/tables_6_1.jpg", "caption": "Table 1: Performance of different algorithms on the Safety Benchmarks. The first 7 environments represent the hard constraint case. The remaining environments illustrate the soft constraint case, with values in brackets indicating the cost threshold. Each algorithm is run for 6 independent seeds. (orange) and (blue) indicate the best performance in the known costs and inferred costs settings, respectively. Algorithms with a cost violation (C.V) rate below 1% are deemed to have equal performance in terms of safety.", "description": "This table presents a comparison of the performance of various reinforcement learning algorithms on safety benchmark tasks.  It shows the return and cost violation rate for each algorithm across several environments, distinguishing between those with hard constraints (Cmax=0) and soft constraints (Cmax>0).  The best-performing algorithm with known and inferred costs are highlighted.  Algorithms with very low cost violation rates are considered equivalent in terms of safety.", "section": "5.1 Experiment Setup"}, {"figure_path": "WSsht66fbC/tables/tables_8_1.jpg", "caption": "Table 2: Comparison of PPOLag performance when trained with the underlying task cost function versus the transferred cost function. Results are averaged over three independent seeds.", "description": "This table compares the performance of the PPO-Lagrangian algorithm trained using the true underlying cost function against a version trained using a cost function inferred from a different agent (cost transfer).  The results, averaged over three independent seeds, show return and cost violation rate for both scenarios in two environments: Doggo Circle (trained from Point Circle source) and Doggo Goal (trained from Point Goal source). It demonstrates the effectiveness of cost transfer by showing comparable performance with the true cost function.", "section": "5.3 Cost Transfer"}, {"figure_path": "WSsht66fbC/tables/tables_16_1.jpg", "caption": "Table 1: Performance of different algorithms on the Safety Benchmarks. The first 7 environments represent the hard constraint case. The remaining environments illustrate the soft constraint case, with values in brackets indicating the cost threshold. Each algorithm is run for 6 independent seeds. (orange) and (blue) indicate the best performance in the known costs and inferred costs settings, respectively. Algorithms with a cost violation (C.V) rate below 1% are deemed to have equal performance in terms of safety.", "description": "This table presents a comparison of the performance of different reinforcement learning algorithms on various safety benchmark environments.  It shows the return and cost violation rate for each algorithm across multiple environments, categorized into hard and soft constraint cases. The table highlights the best performance achieved with known cost functions and compares it to the performance using inferred cost functions.  Algorithms with less than 1% cost violation are considered equally safe.", "section": "5.1 Experiment Setup"}, {"figure_path": "WSsht66fbC/tables/tables_18_1.jpg", "caption": "Table 1: Performance of different algorithms on the Safety Benchmarks. The first 7 environments represent the hard constraint case. The remaining environments illustrate the soft constraint case, with values in brackets indicating the cost threshold. Each algorithm is run for 6 independent seeds. (orange) and (blue) indicate the best performance in the known costs and inferred costs settings, respectively. Algorithms with a cost violation (C.V) rate below 1% are deemed to have equal performance in terms of safety.", "description": "This table presents a comparison of the performance of different reinforcement learning algorithms on various safety benchmark environments.  It shows the return and cost violation rate for each algorithm across multiple environments, distinguishing between hard and soft constraint settings.  The table highlights the best performance achieved with known and inferred cost functions, indicating the effectiveness of the proposed method (RLSF) in learning safe policies.", "section": "5.1 Experiment Setup"}]