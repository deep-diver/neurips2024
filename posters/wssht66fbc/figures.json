[{"figure_path": "WSsht66fbC/figures/figures_7_1.jpg", "caption": "Figure 1: Cost Violation rate of different algorithms in the Driver environment. Each algorithm is run for 6 independent seeds, with the curves representing the mean and the shaded regions indicating the standard error.", "description": "This figure compares the performance of RLSF, PPO, and PPOLag in three different self-driving scenarios within the Safety Driver environment.  The x-axis represents the number of training steps (in millions), and the y-axis shows the cost violation rate. Shaded areas represent the standard error across 6 independent runs for each algorithm.  The figure demonstrates the ability of RLSF to achieve a low cost violation rate comparable to PPOLag (which uses the known cost function) while significantly outperforming PPO (which doesn't incorporate cost).", "section": "5.2 Cost Inference across various tasks"}, {"figure_path": "WSsht66fbC/figures/figures_8_1.jpg", "caption": "Figure 2: Comparison of different sampling and scheduling schemes. Results are averaged over 3 independent seeds. The proposed sampling method generates on average \u2248 1950 queries, hence for fair comparison the other methods were given a budget of 2000 queries.", "description": "This figure compares the performance of different sampling methods for collecting feedback from an evaluator in a reinforcement learning setting.  Five methods are compared: entropy-uniform, entropy-decreasing, random-decreasing, random-uniform, and the novel method proposed in the paper.  The x-axis represents training steps (in millions), the left y-axis shows the average return, and the right y-axis shows the cost violation rate.  The proposed novel method achieves comparable performance to other methods, with significantly lower cost violation rates and a far lower number of total queries.  The shaded areas represent standard error across three independent seeds.", "section": "5.4 Effect of Novelty Sampling"}, {"figure_path": "WSsht66fbC/figures/figures_9_1.jpg", "caption": "Figure 3: Comparing the inferred cost to the true cost.", "description": "This figure compares the inferred cost function learned by the RLSF algorithm with the actual cost function for two different environments: SafetyPointGoal and SafetyPointCircle.  The plots show the mean cost over multiple runs, with shaded regions indicating the standard error. In SafetyPointGoal, the inferred cost closely tracks the true cost. However, in SafetyPointCircle, there is a noticeable overestimation bias in the inferred cost, especially in the earlier stages of training, which gradually decreases over time.", "section": "5.5 Analyzing the Inferred Cost function"}, {"figure_path": "WSsht66fbC/figures/figures_9_2.jpg", "caption": "Figure 1: Cost Violation rate of different algorithms in the Driver environment. Each algorithm is run for 6 independent seeds, with the curves representing the mean and the shaded regions indicating the standard error.", "description": "This figure displays the cost violation rates of various reinforcement learning algorithms (RLSF, PPO, PPOLag) across three different driving scenarios within the Driver environment. The x-axis represents the number of steps (in millions), and the y-axis represents the cost violation rate. Each algorithm is evaluated across six independent seeds, with the mean cost violation rate displayed as a line and the shaded region around it indicating the standard error of the mean. This visualization allows comparison of the safety performance of the algorithms over time in complex driving environments.", "section": "5.2 Cost Inference across various tasks"}, {"figure_path": "WSsht66fbC/figures/figures_15_1.jpg", "caption": "Figure 5: Driver Environments", "description": "This figure shows three different scenarios in the Driver environment used in the paper.  The scenarios include a blocked road (SafeDriverBlocking), overtaking on a two-lane highway (SafeDriverTwoLanes), and a lane change maneuver (SafeDriverLaneChange). Each scenario presents unique challenges for an autonomous driving agent in terms of safety and efficient navigation.", "section": "5.1 Experiment Setup"}, {"figure_path": "WSsht66fbC/figures/figures_17_1.jpg", "caption": "Figure 7: Training curves depicting the performance of different algorithms. The curves portray the mean returns, with shaded regions indicating the standard error. Parallel lines represents the best-performing run.", "description": "This figure shows the training curves for different reinforcement learning algorithms across various benchmark environments. Each curve represents the average return of an algorithm over multiple independent runs, with shaded areas showing the standard error.  The parallel lines indicate the best-performing run achieved for each algorithm. This visualization helps to compare the learning progress and final performance of different algorithms in various constrained reinforcement learning tasks.", "section": "5 Experiments"}, {"figure_path": "WSsht66fbC/figures/figures_18_1.jpg", "caption": "Figure 8: Performance of algorithms in different driving scenarios. Each algorithm is run for 6 independent seeds, curves represent the mean and shaded regions represent the standard error. Normalized return is given by the formula r -rPPO where rPPO is the return of PPO and rrandom is the return of a random policy.", "description": "This figure compares the performance of different reinforcement learning algorithms in three self-driving scenarios: lane change, overtaking, and blocked road.  The algorithms are RLSF (the authors' method), PPO (a standard RL algorithm), and PPOLag (a constrained RL algorithm).  The graph shows the average return (reward accumulated) and cost violation rate (how often the algorithm violated safety constraints) over multiple runs with independent random seeds. The shaded area around the lines represents the standard error, indicating variability across runs.  The normalized return is calculated relative to a random policy. The figure demonstrates RLSF's superior performance, especially in maintaining safety (low cost violation rate).", "section": "5.2 Cost Inference across various tasks"}, {"figure_path": "WSsht66fbC/figures/figures_19_1.jpg", "caption": "Figure 7: Training curves depicting the performance of different algorithms. The curves portray the mean returns, with shaded regions indicating the standard error. Parallel lines represents the best-performing run.", "description": "This figure compares the performance of different reinforcement learning algorithms across various benchmark environments.  Each algorithm's performance is shown as a curve, representing the mean return over multiple independent runs. The shaded regions indicate the standard error, providing a measure of the variability in the results.  Horizontal lines mark the return of a known optimal algorithm, offering a baseline for comparison. The figure displays the algorithms' ability to achieve high returns in different tasks.", "section": "5 Experiments"}, {"figure_path": "WSsht66fbC/figures/figures_19_2.jpg", "caption": "Figure 10: Model prediction accuracy (mean \u00b1 standard error) in the surrogate task (averaged over 3 seeds) with trajectory level feedback. Predictions are made on the next 50k steps following 250k training steps.", "description": "This figure displays the model's accuracy in a surrogate task using trajectory-level feedback. The accuracy is measured for the next 50,000 steps after 250,000 training steps.  It shows a comparison of the model's performance on novel versus non-novel trajectories. The results indicate that the model is less accurate on novel trajectories, which supports the use of a novelty-based sampling mechanism.", "section": "5.4 Effect of Novelty Sampling"}, {"figure_path": "WSsht66fbC/figures/figures_19_3.jpg", "caption": "Figure 6: Implicit decreasing schedule observed when following novelty based sampling across different environments. The results are averaged over 6 independent seeds.", "description": "This figure shows the implicit decreasing schedule of queries observed when using novelty-based sampling across different Safety Gym environments.  The x-axis represents the training steps (in 1e6), and the y-axis represents the fraction of total trajectories queried for feedback. Each colored line represents a different environment, and the shaded area around each line represents the standard error across 6 independent seeds. The graph shows that the fraction of trajectories queried decreases as training progresses, indicating that the novelty-based sampling mechanism effectively reduces the number of queries needed over time.", "section": "5.4 Effect of Novelty Sampling"}, {"figure_path": "WSsht66fbC/figures/figures_20_1.jpg", "caption": "Figure 11: Costs incurred by a policy after 500K and 20M training interactions over a randomly sampled trajectory in the Point Goal environment.", "description": "This figure shows the costs incurred by a policy in the Point Goal environment after 500,000 and 20,000,000 training interactions, respectively, over a randomly sampled trajectory. The plots visually depict the cost values over time steps for both interaction counts. It helps to understand how the cost function behaves differently over various stages of training.", "section": "D.6 Ablation on the size of the feedback buffer"}, {"figure_path": "WSsht66fbC/figures/figures_21_1.jpg", "caption": "Figure 12: Comparison of state level and trajectory level feedback elicitation.", "description": "This figure compares the performance of the RLSF algorithm using state-level feedback (k=1, meaning feedback is collected for each state) versus trajectory-level feedback (k=500 or k=1000, meaning feedback is collected for segments of trajectories).  The top row shows the results for the SafetyPointCircle environment. The bottom row shows results for the SafetyPointGoal environment. Each subplot shows both return and cost violation rate or cost over training steps. The plots demonstrate the trade-off between efficiency (fewer feedback requests with trajectory-level feedback) and performance (better performance often achieved with state-level feedback).", "section": "5.2 Cost Inference across various tasks"}, {"figure_path": "WSsht66fbC/figures/figures_22_1.jpg", "caption": "Figure 13: Impact of the size of the feedback buffer on performance.", "description": "This figure displays the effect of varying the size of the feedback buffer on the performance of RLSF.  It shows training curves for return and cost violation rate for different buffer sizes (100k, 1M, and 4M) across the SafetyPointCircle and SafetyPointGoal environments.  The results indicate that smaller buffers (100k) lead to more conservative policies, while larger buffers (4M) achieve better performance.", "section": "5. Experiments"}, {"figure_path": "WSsht66fbC/figures/figures_22_2.jpg", "caption": "Figure 14: Norm of the gradient when optimising the MLE loss v/s the proposed surrogate loss with trajectory level feedback.", "description": "This figure compares the gradient norms during the optimization of the maximum likelihood estimation (MLE) loss and the proposed surrogate loss.  The surrogate loss is designed to address challenges in optimizing the MLE loss, particularly when dealing with long trajectory segments where the probability of a safe trajectory approaches zero, leading to unstable gradients.  The plot shows that the surrogate loss produces more stable gradients during training, allowing for smoother convergence and improved performance.", "section": "D.7 Need for the surrogate loss"}]