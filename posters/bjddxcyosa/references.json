{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that is extensively used and benchmarked in the current paper."}, {"fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft coco: Common objects in context", "publication_date": "2014-09-06", "reason": "This paper introduces the COCO dataset, which provides the image data used to create the benchmark in the current paper."}, {"fullname_first_author": "Tristan Thrush", "paper_title": "Winoground: Probing vision and language models for visio-linguistic compositionality", "publication_date": "2022-06-01", "reason": "This paper introduces Winoground, a benchmark used for comparison and evaluation in the current paper, highlighting its importance in the field."}, {"fullname_first_author": "Xiaohua Zhai", "paper_title": "Sigmoid loss for language image pre-training", "publication_date": "2023-10-01", "reason": "This paper introduces a new loss function for vision-language pre-training that is compared to the methods used in the current paper."}, {"fullname_first_author": "Sivan Doveh", "paper_title": "Teaching structured vision & language concepts to vision & language models", "publication_date": "2023-06-01", "reason": "This paper addresses the limitations of existing benchmarks and proposes a new approach for teaching structured vision and language concepts which is compared with the methods in this paper."}]}