[{"type": "text", "text": "Query-Efficient Correlation Clustering with Noisy Oracle ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuko Kuroki CENTAI Institute Turin, Italy yuko.kuroki@centai.eu ", "page_idx": 0}, {"type": "text", "text": "Atsushi Miyauchi CENTAI Institute Turin, Italy atsushi.miyauchi@centai.eu ", "page_idx": 0}, {"type": "text", "text": "Francesco Bonchi CENTAI Institute, Turin, Italy Eurecat, Barcelona, Spain bonchi@centai.eu ", "page_idx": 0}, {"type": "text", "text": "Wei Chen   \nMicrosoft Research   \nBeijing, China   \nweic@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the weighted similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We introduce two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given a set $V=[n]$ of $n$ objects and a pairwise similarity measure s $\\colon(\\O_{2}^{V})\\to[0,1]$ (where $\\binom{V}{2}$ is the set of unordered pairs of elements of $V$ , and the value closer to 1 means higher similarity), the goal of Correlation Clustering [7] is to cluster the objects so that, to the best possible extent, similar objects are put in the same cluster and dissimilar objects are put in different clusters. Assuming that cluster identifiers are represented by natural numbers, a clustering $\\mathcal{C}$ can be represented as a function $\\ell:V\\to\\mathbb{N}$ , where each cluster is a maximal set of objects sharing the same label. The objective is to minimize the following cost: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathrm{cost}_{\\mathrm{s}}(\\ell)=\\sum_{\\stackrel{(x,y)\\in\\binom{V}{2},}{\\ell(x)=\\ell(y)}}(1-\\mathrm{s}(x,y))+\\sum_{\\stackrel{(x,y)\\in\\binom{V}{2},}{\\ell(x)\\neq\\ell(y)}}\\mathrm{s}(x,y).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "The intuition underlying the above problem definition is that if two objects $x$ and $y$ are dissimilar, expressed by a small value of $\\operatorname{s}(x,y)$ , yet they are assigned to the same cluster, we should incur a high cost. Conversely, if $\\operatorname{s}(x,y)$ is high, indicating that $x$ and $y$ are very similar, but they are assigned to different clusters, we should also incur a high cost. ", "page_idx": 0}, {"type": "text", "text": "Two key features make correlation clustering quite suitable in real-world applications. Firstly, it does not require the number of clusters as part of the input; instead, it automatically finds the optimal number, performing model selection. Secondly, it only requires the pairwise information without assuming any specific structure of the data. This reasonably eliminates the need for domain knowledge about complex data. Correlation clustering has been applied to a wide range of problems across various domains, including duplicate detection and similarity joins [34, 46], spam detection [12, 72], co-reference resolution [66], biology [9, 14], image segmentation [54], social network analysis [13], and clustering aggregation [42]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Correlation clustering is NP-hard even in the simplest formulations [7, 75], and minimizing the cost function in (1) is APX-hard [20]; thus, we cannot expect a polynomial-time approximation scheme. Nevertheless, there are a number of constant-factor approximation algorithms for various settings [1, 3, 7, 20, 22, 30\u201332]. For the formulation of (1), Ailon et al. [3] presented KwikCluster, a simple 5-approximation algorithm. The algorithm randomly picks a pivot $v\\in V$ and constructs a cluster by taking all the vertices similar to $v$ ; then, the algorithm removes the cluster and repeats the process until $V$ is fully clustered. The simplicity and theoretical guarantees of KwikCluster have produced a lot of variations in different scenarios [13, 29, 61, 69, 76, 79]. ", "page_idx": 1}, {"type": "text", "text": "In practice, preparing the similarity function involves costly measurements. Given $n$ items to be clustered, $\\Theta^{\\mathbf{\\!\\scriptscriptstyleA}}{(n^{2})}$ similarity computations are needed to prepare the input to correlation clustering algorithms. Moreover, computing the similarity $\\operatorname{s}(x,y)$ might have additional expenses (e.g., human effort or financial resources) besides the mere computational cost. To mitigate these issues, some query-efficient methods have been proposed based on the active learning framework [11, 15, 40]. In this framework, the similarity function is initially unknown but an oracle that returns the true similarity in $\\{0,1\\}$ for a pair of objects is sequentially queried. In particular, these studies provided a randomized algorithm that, given a budget $T$ of queries, attains a solution whose expected cost is at most $\\begin{array}{r l}{3\\cdot\\mathrm{OPT}+\\mathcal{O}\\big(\\frac{n^{3}}{T}\\big)}\\end{array}$ , where OPT is the optimal value of the problem. ", "page_idx": 1}, {"type": "text", "text": "However, the above methods for query-efficient correlation clustering have significant limitations. Indeed, all the aforementioned works [11, 15, 40] only consider the binary similarity of $\\{0,1\\}$ , while the similarity between two objects are often non-binary in real-world scenarios. For example, in biological sciences, protein-protein interaction networks are commonly analyzed, where the strength of the interactions among proteins is represented as a real-valued similarity [68]. As another example, in entity resolution, i.e., a task central to data integration [80], real-valued similarity is used to indicate the likelihood of matches of objects instead of binary decisions. Therefore, allowing the similarity to be real-valued in the interval $[0,1]$ would be more practical and flexible. Furthermore, the above works assume the access to the strong oracle that returns the true value of $\\operatorname{s}(x,y)$ $\\leftrightharpoons0\\rightleftharpoons$ or 1), while evaluating $\\operatorname{s}(x,y)$ might be inherently noisy, due to error-prone experiments, noisy measurements, or biased judgments. In the above first example the strength of the interactions among proteins is often measured based on biological experiments involving unavoidable noise, while in the second example the likelihood of matches of objects is usually obtained based on biased human judgements. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we focus on the challenging scenario where (i) the underlying similarity measure can take any real value in $[0,1]$ rather than being binary, and (ii) we can only query a noisy oracle that provides inaccurate evaluations of the weighted similarity $\\operatorname{s}(x,y)$ . The goal of this paper is to devise clustering algorithms that perform as few queries on $\\operatorname{s}(x,y)$ as possible to an oracle that returns noisy answers to $\\operatorname{s}(x,y)$ . In pursuit of this goal, we introduce two novel formulations based on multi-armed bandits problems, both of which achieve a reasonable trade-off between the number of queries to the oracle and the quality of solutions. ", "page_idx": 1}, {"type": "text", "text": "While our problem formulations are novel, recent prior work has explored related issues. Silwal et al. [76] proposed a practical model using the strong oracle along with a cheaper but inaccurate oracle. Their algorithm achieves a cost of $3\\cdot{\\bar{\\mathrm{OPT}}}+\\epsilon n^{\\bar{2}}$ using $n+\\mathcal{O}(\\frac{\\gamma}{\\epsilon})$ queries to the strong oracle, where $\\gamma>0$ is the error level of noisy oracle and $\\epsilon>0$ is the additive error. However, they still focus on the binary similarity and there is no guarantee on the query upper bound for the noisy oracle. Unlike theirs, our models are designed to handle the weighted similarity and do not rely on any strong oracle. Aronsson and Chehreghani [4, 5] studied a non-persistent noise model where the oracle returns the true value of $\\operatorname{s}(x,y)$ with probability $1-\\gamma$ and a noisy value otherwise. Their algorithm handles a general weighted similarity but provides neither query complexity nor approximation guarantee. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we study the problem of query-efficient correlation clustering with noisy oracles, where the similarity function s : $\\bar{\\left(\\!\\!\\begin{array}{l}{V}\\\\ {2}\\end{array}\\!\\!\\right)}\\rightarrow\\bar{\\left[0,\\bar{1}\\right]}$ is initially unknown, and only noisy feedback instead of the true similarity $\\operatorname{s}(x,y)$ is observed when querying a pair of objects $(x,y)$ . In this scenario, it is desired to achieve a reasonable trade-off between the number of queries to the oracle and the cost of clustering. To this end, we introduce two formulations of online learning problems rooted in the paradigm of Pure Exploration of Combinatorial Multi-Armed Bandits (PE-CMAB). In the fixed confidence setting (Problem 1), given a confidence level $\\delta\\in(0,1)$ , the learner aims to find a well-approximate solution with probability at least $1-\\delta$ while minimizing the number of queries required to determine the output. Conversely, in the fixed budget setting (Problem 2), given a querying budget $T$ , the learner aims to maximize the probability that the output is a well-approximate solution. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 For Problem 1, we design KC-FC (Algorithm 1), which effectively combines threshold bandits with KwikCluster. We prove that given confidence level $\\delta\\,\\in\\,(0,1)$ , KC-FC finds a solution whose expected cost is at most $5\\cdot\\mathrm{OPT}+\\epsilon$ with probability at least $1-\\delta$ , where OPT is the optimal value of the problem, and provide the upper bound of the number of queries (Theorem 1). \u2022 We design KC-FB (Algorithm 3) for Problem 2, which adaptively determines the number of queries for each pair of objects based on KwikCluster. We prove that the error probability of the expected cost being worse than $5\\cdot\\mathrm{OPT}+\\epsilon$ decreases exponentially with budget $T$ (Theorem 2). \u2022 We empirically validate our theoretical findings by demonstrating that KC-FC and KC-FB outperform baseline methods in terms of the sample complexity and cost of clustering, respectively. ", "page_idx": 2}, {"type": "text", "text": "It is worth noting that our approximation guarantees in Theorems 1 and 2 match the approximation ratio 5 of KwikCluster [3], where $\\mathrm{~s~}\\colon\\bar{\\binom{V}{2}~}\\rightarrow[0,1]$ is known in advance, up to the additive error $\\epsilon>0$ . These results are not achievable using existing PE-CMAB algorithms due to the NP-hardness of correlation clustering. In the standard PE-CMAB, a learner aims to identify the best action that maximizes the linear reward from the combinatorial decision set ${\\mathcal{D}}\\subseteq2^{[m]}$ with $m$ -base arms. Existing algorithms for PE-CMAB (e.g., [23, 25, 35, 52, 82]) rely on the assumption that the offline problem is polynomial-time solvable. Redesigning them to obtain a well-approximate solution while running efficiently is quite challenging, as the exact optimization of the offilne problem is crucial to achieving statistical validity and a correctness guarantee for the output. Ours are the first polynomialtime algorithms that work for the case of PE-CMAB where the underlying offline optimization is NP-hard, filling a critical gap in existing PE-CMAB algorithms, which is of independent interest. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Correlation clustering with noisy input. The bulk of the literature on noisy correlation clustering (see Section 4.6 of Bonchi et al. [10]) considers the binary similarity and assumes that there is the ground-truth clustering but some of the $\\operatorname{s}(x,y)$ are wrong: they are 0 instead of 1, or vice versa. The seminal work by Bansal et al. [7] and Joachims and Hopcroft [49] provided the bounds on the error with which correlation clustering recovers the ground truth under a simple probabilistic model over graphs. Mathieu and Schudy [64] studied the model starting from an arbitrary partition of the $n$ elements into clusters, where $\\operatorname{s}(x,y)$ is perturbed independently with probability $p$ , and a more general model with the adversary. They proposed an algorithm that achieves some approximation ratio and manages to approximately recover the ground truth. Chen et al. [28] extended the framework to sparse Erd\u02ddos\u2013R\u00e9nyi random graphs and obtained an algorithm that conditionally recovers the ground truth. Finally, Makarychev et al. [62] overcame some limitations of Mathieu and Schudy [64] and Chen et al. [28]; they assumed very little about the observations and gave two approximation algorithms. Unlike the above models, ours is based on online learning with an unknown distribution with mean of $\\operatorname{s}(x,y)$ , which is in general not binary, and does not assume any ground-truth clustering. ", "page_idx": 2}, {"type": "text", "text": "Combinatorial multi-armed bandits. Multi-Armed Bandit (MAB) is a classical decision-making model [57, 59, 73]: There are $m$ possible actions (called arms), whose expected reward $\\mu_{i}$ for each $i\\,\\in\\,[m]$ is unknown. At each round, a learner chooses an arm to pull and observes a stochastic reward sampled from an unknown probability distribution. The most popular objective is to minimize the cumulative regret [16, 18]. Another popular objective is to identify the arm with the maximum expected reward. This problem, called the Best Arm Identification (BAI) or Pure Exploration (PE) in MAB, has also received much attention [6, 8, 17, 21, 24, 37, 38, 41, 48, 53]. The model of Combinatorial Multi-Armed Bandits (CMAB) is a generalization of MAB [19, 26], where an interested subset of arms forms a certain combinatorial structure such as a spanning tree, matching, or path. Since its introduction by Chen et al. [25], the study of PE-CMAB has been actively pursued in various settings [23, 35, 36, 47, 50, 52, 55, 56, 67, 78, 82]. Notably, Gullo et al. [44] addressed regret minimization for correlation clustering by adapting UCB-type algorithms. However, regret minimization in CMAB is quite different from pure exploration framework when working with approximation oracles (i.e., offline approximation algorithms) for solving NP-hard problems. For regret minimization, we can incorporate approximation oracles with the UCB framework, consistent with the optimization under uncertainty principle (e.g., [26, 27, 81]). However, in pure exploration, the lack of uniqueness of $\\alpha$ -approximate solutions makes it difficult to determine the stopping condition in the FC setting. In the FB setting, the Combinatorial Successive Accept Reject algorithm proposed by Chen et al. [25] iteratively solves the so-called Constrained Oracle problem, which is often NP-hard, as later addressed in Du et al. [35]. We anticipate a similar NP-hard problem in correlation clustering, requiring a different approach. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Other clustering settings. Ailon et al. [2] and Saha and Subramanian [74] studied correlation clustering with same-cluster queries, where all similarities of $\\binom{V}{2}$ are known in advance and their query is further allowed to access the optimal clustering. Our setting differs significantly as we are interested in the case where similarities are unknown and only noisy similarity values are received rather than same-cluster queries. Finally, it is worth mentioning that Xia and Huang [83] and Gupta et al. [45] proposed a MAB approach for clustering reconstruction with noisy same-cluster queries [58, 65, 70, 71, 77]. However, this clustering reconstruction problem does not directly offer any algorithmic result for correlation clustering. The detailed comparison is deferred to Appendix A. ", "page_idx": 3}, {"type": "text", "text": "2 Problem statements ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here we formally define our formulations of PE-CMAB for correlation clustering. Our problem instances are characterized by $(V,\\mathrm{s})$ , where $V\\,=\\,[n]$ is the set of elements to be clustered and $\\mathrm{s}:\\binom{V}{2}\\to[0,1]$ is the pairwise similarity function, which is unknown to the learner. Define the set of unordered pairs as $E=\\binom{V}{2}$ with $m:=|E|$ . ", "page_idx": 3}, {"type": "text", "text": "At each round $t=1,2,\\ldots,$ a learner will pull (i.e., query) one arm (i.e., pair of elements in $V$ ) from action space $E\\,=\\,\\binom{V}{2}$ based on past observations. After pulling $e\\in E$ , the learner can observe the random feedback $X_{t}(e)$ , which is independently sampled from an unknown distribution such as Bernoulli or $R$ -sub-Gaussian with unknown mean $\\mathrm{\\dot{s}}(e)\\in[0,1]$ .1 After some exploration rounds, the learner must identify a well-approximate solution. Let $\\mathrm{OPT}(\\mathrm{s})$ be the optimal value of the offline problem minimizing the cost function (1) and let ${\\mathcal{C}}_{\\mathrm{out}}$ be the output by an algorithm. For $\\alpha\\geq1$ and $\\epsilon>0$ , we say $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ to be an $(\\alpha,\\epsilon)$ -approximate solution if $\\mathrm{cost}_{\\mathrm{s}}(\\mathcal{C}_{\\mathrm{out}})\\leq\\alpha\\cdot\\mathrm{OPT}(\\mathrm{s})+\\epsilon$ . We study the following two formulations: Fixed Confidence (FC) and Fixed Budget (FB) settings. ", "page_idx": 3}, {"type": "text", "text": "Problem 1 (Fixed confidence setting). Let $\\alpha\\geq1$ . Given a confidence level $\\delta\\in(0,1)$ and additive error $\\epsilon>0$ , the learner aims to guarantee that the output $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ is an $(\\alpha,\\epsilon)$ -approximate solution with probability at least $1-\\delta$ . The evaluation metric of an algorithm is the sample complexity, i.e., the number of queries to the oracle the learner uses. ", "page_idx": 3}, {"type": "text", "text": "Problem 2 (Fixed budget setting). Let $\\alpha\\geq1$ . Given a querying budget $T$ and additive error $\\epsilon>0$ , the learner aims to maximize the probability that the output $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ is an $(\\alpha,\\epsilon)$ -approximate solution. ", "page_idx": 3}, {"type": "text", "text": "Note that the case of $\\alpha=1$ corresponds to the standard PE-CMAB formulations. However, as the offilne problem minimizing the cost function (1) is APX-hard [20], we cannot expect any polynomialtime algorithm that can handle $\\alpha=1$ in the above formulations. ", "page_idx": 3}, {"type": "text", "text": "3 Fixed confidence setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we design KC-FC (Algorithm 1) for Problem 1, built on a novel combination of KwikCluster (detailed in Algorithm 4 in Appendix B) and techniques of threshold bandits. The key idea of the proposed method is to first identify pairs with seemingly high similarity, which are then passed to KwikCluster to produce a high-quality clustering. ", "page_idx": 3}, {"type": "text", "text": "Input : Confidence level $\\delta$ , set $V$ of $n$ objects, and error $\\epsilon$   \n1 $E_{1}\\leftarrow E$ , $V_{1}\\leftarrow V$ , $r\\gets1$ , and $\\mathcal{C}_{\\mathrm{out}}\\gets\\emptyset$ ;   \n2 Compute $\\widehat{G}_{\\epsilon^{\\prime}}$ by TB-HS (Algorithm 2) with $\\epsilon^{\\prime}=\\frac{\\epsilon}{12m}$ ;   \n3 Define $\\widehat{\\Gamma}(v):=\\{u\\in V:\\{u,v\\}\\in\\widehat{G}_{\\epsilon^{\\prime}}\\}$ ;   \n4 while $|V_{r}|>0$ do   \n5 Pick a pivot $p_{r}\\in V_{r}$ uniformly at random;   \n6 $\\mathcal{C}_{\\mathrm{out}}\\leftarrow\\mathcal{C}_{\\mathrm{out}}\\cup\\{C_{r}\\}$ , where $C_{r}:=(\\{p_{r}\\}\\cup\\hat{\\Gamma}(p_{r}))\\cap V_{r}$ ;   \n7 $V_{r+1}\\gets V_{r}\\setminus C_{r}$ and $r\\gets r+1$ ;   \n8 return ${\\mathcal{C}}_{\\mathrm{out}}$ ", "page_idx": 4}, {"type": "text", "text": "For the first phase, we leverage one of the variants of MAB, called the threshold bandits [51, 60, 63], which is defined as follows: Given a confidence level $\\delta$ and $m$ -arms, the learner must return the set of good arms, i.e., arms whose expected rewards are greater than a given threshold $\\theta>0$ , as soon as possible, and stops when the learner believes that there is no remaining good arm, w.p. at least $1-\\delta$ . TB-HS (detailed in Algorithm 2) is our key procedure, which is designed for identifying seemingly high similarity pairs. Note that, if we naively use the existing algorithm by Kano et al. [51] for threshold bandits where the set of arms is $E=\\textstyle\\binom{\\bar{V}}{2}$ and the threshold is $\\theta=0.5$ , the algorithm is not even guaranteed to terminate; the resulting sample complexity becomes infinitely large if $\\mathbf{s}(e)=\\mathbf{s}(e^{\\prime})$ for different $e,e^{\\prime}\\in E$ or if there exists $e\\in E$ with $\\mathrm{s}(e)=0.5$ , which may frequently happen in practice. Our strategy to avoid such an unbounded sample complexity is to allow TB-HS to misidentify pairs of elements with similarity close to 0.5, taking advantage of the fact that the output accuracy can be guaranteed despite such misidentification. ", "page_idx": 4}, {"type": "text", "text": "Algorithm details. Let $\\widehat{\\mathrm{s}}_{t}(e)$ be the empirical mean of the similarity for each pair $e\\in E$ kept at round $t$ . Let $N_{t}(e)$ be the number of queries of $e\\ \\in\\ E$ that has been pulled by the end of round $t$ . TB-HS maintains the confidence bound defined as $\\begin{array}{r}{\\mathrm{rad}_{t}(e):=\\sqrt{\\frac{\\log(4m N_{t}(e)^{2}/\\delta)}{2N_{t}(e)}}}\\end{array}$ log(4mNt(e)2/\u03b4)for each $e\\in E$ . The arm selection at round $t$ is based on the Lower-Confidence-Bound (LCB) score, i.e., $\\mathtt{s}_{t}(e):=\\widehat{\\mathrm{s}}_{t}(e)\\!-\\!\\operatorname{rad}_{t}(e)$ and the Upper-Confidence-Bound (UCB) score, i.e., $\\overline{{\\mathrm{s}}}_{t}(e):=\\widehat{\\mathrm{s}}_{t}(e)\\!+\\!\\mathrm{rad}_{t}(e)$ . We pull  t he arm $\\hat{e}_{t}^{g}$ with the highest LCB (line 5) and the arm $\\hat{e}_{t}^{b}$ with the lowest UC B  (line 6). Then $\\hat{e}_{t}^{g}$ will be added to $\\widehat{G}_{\\epsilon}$ if its LCB is no less than $0.5-\\epsilon$ , and $\\hat{e}_{t}^{b}$ will be added to $\\widehat{B}_{\\epsilon}$ if its UCB is no greater than $0.5+\\epsilon$ . TB-HS continues this procedure until every $e\\in E$ is added to either $\\widehat{G}_{\\epsilon}$ or $\\widehat{B}_{\\epsilon}$ . Our main algorithm KC-FC invokes TB-HS to compute $\\widehat{G}_{\\epsilon^{\\prime}}$ with parameter $\\epsilon^{\\prime}=\\frac{\\epsilon}{12m}$ . Then it carries out KwikCluster using the predicted similarity by $\\widehat{G}_{\\epsilon^{\\prime}}$ as follows. Until an unclustered element exists, it picks one pivot element $p_{r}$ uniformly at ran dom, builds a cluster $C_{r}$ around it by adding those among the unclustered elements that seemingly have high similarity with a pivot $p_{r}$ (based on $\\widehat{G}_{\\epsilon^{\\prime}}$ ), and removes all the elements in $C_{r}$ from the list of unclustered elements. ", "page_idx": 4}, {"type": "text", "text": "Analysis. For a given $\\epsilon\\ \\in\\ (0,0.5)$ , we define the following sets, which appear only in the theoretical analysis and are unknown to the learner: $E_{[0.5\\pm\\epsilon]}\\,:=\\,\\{e\\,\\in\\,E\\,:\\,|\\bar{0}.5-\\mathrm{s}(e)|\\,\\le\\,\\epsilon\\}$ , $E_{(0.5+\\epsilon,1]}\\;:=\\;\\{e\\;\\in\\;E\\,:\\,\\mathrm{s}(e)\\,>\\,0.5+\\epsilon\\}$ , and $E_{[0,0.5-\\epsilon)}\\,:=\\,\\{e\\,\\in\\,E\\,:\\,\\mathrm{s}(e)\\,<\\,0.5\\,-\\,\\epsilon\\}$ . For $\\epsilon\\in(0,0.5)$ , we introduce the definition of the gaps that characterize our sample complexity: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\Delta}_{e,\\epsilon}:=\\left(\\Delta_{e}+\\operatorname*{min}\\left\\{\\epsilon-\\Delta_{\\operatorname*{min}},\\,\\frac{\\epsilon}{2}\\right\\}\\right)\\mathrm{~for~}e\\in[m],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Delta_{e}:=|\\mathrm{s}(e)-0.5|$ for $e\\in[m]$ and $\\begin{array}{r}{\\Delta_{\\operatorname*{min}}:=\\operatorname*{min}_{e\\in[m]}\\Delta_{e}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Now we present our theorem, guaranteeing that KC-FC finds a $(5,\\epsilon)$ -approximate solution with high probability and provides an upper bound of the number of queries, i.e., the sample complexity. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Given a confidence level $\\delta\\in(0,1)$ and additive error $\\epsilon>0$ , KC-FC (Algorithm 1) guarantees that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[\\mathrm{cost.}(\\mathcal{C}_{\\mathrm{out}})\\leq5\\cdot\\mathrm{OPT}(\\mathrm{s})+\\epsilon]\\geq1-\\delta,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Input :Set $E$ of $m$ -arms and confidence level $\\delta$ ", "page_idx": 5}, {"type": "text", "text": "1 $\\widehat{G}_{\\epsilon}^{'}\\leftarrow\\emptyset$ and $\\widehat{B}_{\\epsilon}\\leftarrow\\emptyset$ ;   \n2  Pull each $e\\in E$ once to initialize empirical mean $\\widehat{\\mathrm{s}}_{m}(e)$ , $t\\leftarrow m$ , and $E_{t}\\gets E$ ;   \n3 Compute radt(e) := $\\begin{array}{r}{\\mathrm{rad}_{t}(e):=\\sqrt{\\frac{\\log\\left(4m N_{t}(e)^{2}/\\delta\\right)}{2N_{t}(e)}}}\\end{array}$ log(4mNt(e)2/\u03b4)for e \u2208E;   \n4 while $|E_{t}|>0$ do   \n5 Pull $\\hat{e}_{t}^{g}:=\\mathrm{argmax}_{e\\in E_{t}}(\\widehat{\\mathbf{s}}_{t}(e)-\\mathrm{rad}_{t}(e))$ once;   \n6 Pul $\\begin{array}{r}{\\hat{e}_{t}^{b}:=\\operatorname*{argmin}_{e\\in E_{t}}(\\widehat{\\mathbf{s}}_{t}(e)+\\operatorname{rad}_{t}(e))}\\end{array}$ once;   \n7 Update $\\widehat{\\mathrm{s}}_{t}$ and $\\mathrm{rad}_{t}$ for $\\hat{e}_{t}^{g}$ and $\\hat{e}_{t}^{b}$ ;   \n8 if $\\mathtt{S}_{t}(\\hat{e}_{t}^{g}):=\\widehat{\\mathtt{s}}_{t}(\\hat{e}_{t}^{g})-\\mathrm{rad}_{t}(\\hat{e}_{t}^{g})\\geq0.5-\\epsilon$ then   \n9 Add $\\hat{e}_{t}^{g}$ to good arms, i.e., $\\widehat{G}_{\\epsilon}\\leftarrow\\widehat{G}_{\\epsilon}\\cup\\{\\widehat{e}_{t}^{g}\\}$ , and delete $\\hat{e}_{t}^{g}$ from $E_{t}$ ;   \n10 if $\\overline{{\\mathrm{s}}}_{t}(\\hat{e}_{t}^{b}):=\\widehat{\\mathrm{s}}_{t}(\\hat{e}_{t}^{b})+\\mathrm{rad}_{t}(\\hat{e}_{t}^{b})\\leq0.5+\\epsilon$ then   \n11 Add $\\hat{e}_{t}^{b}$ to bad arms, i.e., $\\widehat{B}_{\\epsilon}\\leftarrow\\widehat{B}_{\\epsilon}\\cup\\{\\widehat{e}_{t}^{b}\\}$ , and delete $\\hat{e}_{t}^{b}$ from $E_{t}$ ;   \n12 $E_{t+2}\\leftarrow E_{t}$ ;   \n13 $t\\gets t+2$ ;   \n$\\widehat{G}_{\\epsilon}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "and letting $\\epsilon^{\\prime}=\\frac{\\epsilon}{12m}$ , the sample complexity $T$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\sum_{e\\in E}\\frac{1}{\\tilde{\\Delta}_{e,\\epsilon^{\\prime}}^{2}}\\log\\left(\\frac{n}{\\tilde{\\Delta}_{e,\\epsilon^{\\prime}}^{2}\\delta}\\log\\left(\\frac{n}{\\tilde{\\Delta}_{e,\\epsilon^{\\prime}}^{2}\\delta}\\right)\\right)+\\frac{n^{2}}{\\operatorname*{max}\\left\\{\\Delta_{\\operatorname*{min}},\\frac{\\epsilon^{\\prime}}{2}\\right\\}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Furthermore, KC-FC runs in time polynomial in $n$ ", "page_idx": 5}, {"type": "text", "text": "Proof Sketch. For the outputs $\\widehat{G}_{\\epsilon}$ and $\\widehat{B}_{\\epsilon}$ of TB-HS (Algorithm 2) with parameters $\\epsilon\\in(0,0.5)$ and $\\delta\\,\\in\\,(0,1)$ , by using the Hoef fding in equality and the procedure of TB-HS (lines 8 and 10), it is easy to see that $E_{(0.5+\\epsilon,1]}\\subseteq{\\widehat{G}}_{\\epsilon}$ and $E_{[0,0.5-\\epsilon)}\\subseteq\\widehat{B}_{\\epsilon}$ w.p. at least $1-\\delta$ . Consider the similarity functions : E \u2192 [0, 1] such that for each e \u2208 E,s(e) = s(e) if e \u2208 E[0,0.5\u2212\u03f5) \u222aE(0.5+\u03f5,1], and $\\widetilde{\\mathrm{s}}_{e}$ otherwise, where $\\widetilde{\\mathrm{s}}_{e}$ is an arbitrary value that satisfies $|\\mathrm{s}(e)-\\widetilde{\\mathrm{s}}_{e}|<2\\epsilon$ . Noticing that KCFC  c orresponds to Kwi k Cluster associated with a certain choice of s (i.e., $\\widetilde{\\mathrm{s}}_{e}$ for $e\\,\\in\\,E_{[0.5\\pm\\epsilon]})$ , we can show that $\\mathbb{E}[\\mathrm{cost}_{\\mathrm{s}}(\\mathcal{C}_{\\mathrm{out}})]\\le5\\cdot\\mathrm{OPT}(\\mathrm{s})+12\\epsilon|E_{[0.5\\pm\\epsilon]}|$ for the output $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ , providing the approximation guarantee. The rest of the proof requires the analysis of the upper bound of the number of queries that TB-HS used to stop. This can be done based on a prior analysis of threshold bandits [51], while carefully handling $\\epsilon>0$ . The complete proof for analysis is given in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "For the time complexity, each iteration of sub-routine TB-HS takes ${\\mathcal{O}}(m)$ steps in a naive implementation or amortized ${\\mathcal{O}}(\\log T)$ steps if we manage arms using two heaps corresponding to LCB/UCB values, and the other procedure in KC-FC runs in time polynomial in $n$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Comparison with existing PE-CMAB methods in the FC setting. Existing methods for PECMAB (e.g., [23, 25, 35, 82]) are limited by their reliance on the polynomial-time solvability of the underlying offline problem. If we use an efficient approximation algorithm in those existing methods, their stopping conditions no longer have a guarantee of the quality of the output. Specifically, such existing methods use the LUCB-type strategy, and its stopping condition requires the exact computation of the empirical best solution and the second empirical best solution to check if the current estimation is enough or not. When we only have an approximate oracle (i.e., approximation algorithm), such existing stopping conditions are no longer valid, and the algorithm is not guaranteed to stop. In contrast, KC-FC runs in time polynomial in $n$ while ensuring sample complexity and approximation guarantee. We also note that $\\Delta_{e}$ , the distance between $\\mathbf{s}(e)$ and 0.5, interestingly characterizes our sample complexity, as we show that the learning task boils down to identifying $E_{(0.5+\\epsilon^{\\prime},1]}$ and $E_{[0,0.5-\\epsilon^{\\prime})}$ thanks to the behavior of KC-FC \u2013 they leverage the property that by accurately estimating the mean of the base arms (i.e., pairs of elements), we can maintain the approximation guarantee of KwikCluster in the offline setting with small additive error. ", "page_idx": 5}, {"type": "text", "text": "Statistical efficiency. In the noise-free setting, $\\binom{n}{2}$ queries are sufficient, while in the noisy setting, there is even no trivial upper bound on the sample complexity to achieve some desired approximation guarantee (e.g., our $(5,\\epsilon)$ -approximation). Note that the value of $\\tilde{\\Delta}_{e,\\epsilon}$ defined in (2) always has the following lower bound: $\\tilde{\\Delta}_{e,\\epsilon}=\\Delta_{e}+\\epsilon/2\\left(>0\\right)$ if $\\epsilon/2\\ge\\Delta_{\\mathrm{min}}$ holds and $\\tilde{\\Delta}_{e,\\epsilon}=\\Delta_{e}+\\epsilon-\\Delta_{\\mathrm{min}}\\geq$ $\\epsilon\\ (>0)$ otherwise. Therefore, our sample complexity $T$ given in Theorem 1 is always bounded, contrasting existing results for threshold bandits [51]. The naive sampling algorithm (Uniform-FC in Appendix E) requires $\\begin{array}{r}{O\\big(\\frac{n^{6}}{\\epsilon^{2}}\\log\\frac{n}{\\delta}\\big)}\\end{array}$ samples to achieve the $(5,\\epsilon)$ -approximation w.p. at least $1-\\delta$ . KC-FC achieves a much better sample complexity than Uniform-FC, as $\\sum_{e\\in E}\\tilde{\\Delta}_{e,{\\epsilon^{\\prime}}}^{-2}=$ $\\begin{array}{r}{\\sum_{e\\in E}(\\Delta_{e}+\\frac{\\epsilon^{\\prime}}{2})^{-2}\\ll\\frac{n^{6}}{\\epsilon^{2}},}\\end{array}$ when $\\begin{array}{r}{\\Delta_{\\mathrm{min}}\\,\\le\\,\\frac{\\epsilon^{\\prime}}{2}\\,\\ll\\,\\Delta_{e}}\\end{array}$ for most $e\\,\\in\\,E$ , which is often the case in practice. To the best of our knowledge, lower bounds on the sample complexity related to PE-CMAB are known only for the following settings: (i) the time complexity of algorithms can be exponential, or (ii) the underlying offline problem is assumed to be polynomial-time solvable and to have the unique correct (namely optimal) solution [25, 35, 39]. Deriving an effective lower bound on the number of samples required to guarantee an approximate solution is particularly challenging because it necessitates dealing with multiple correct solutions [33], while most existing approaches rely on the uniqueness of the correct solution. Evaluating the necessity of the second term $\\frac{n^{2}}{\\operatorname*{max}\\left\\{\\Delta_{\\operatorname*{min}},\\frac{\\epsilon^{\\prime}}{2}\\right\\}^{2}}$ and investigating a lower bound for our case are crucial and remain important future work. However, it is worth noting that the additional term is independent of a dominating term involving log \u03b41 . ", "page_idx": 6}, {"type": "text", "text": "Remark. If we utilize TB-HS within the loop (Algorithm 5 in Appendix B), the algorithm achieves $(5,\\epsilon)$ -approximation guarantee with probability at least $1-\\delta$ , and the sample complexity $T$ is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\sum_{r=1}^{k}\\left(\\sum_{e\\in I_{V_{r}}(p_{r})}\\frac{1}{\\tilde{\\Delta}_{e,e_{r}^{\\prime}}^{2}}\\log\\left(\\frac{n}{\\tilde{\\Delta}_{e,e_{r}^{\\prime}}^{2}\\delta}\\log\\left(\\frac{n}{\\tilde{\\Delta}_{e,e_{r}^{\\prime}}^{2}\\delta}\\right)\\right)+\\frac{|V_{r}|}{\\operatorname*{max}(\\Delta_{\\operatorname*{min},r},\\frac{\\epsilon_{r}^{\\prime}}{2})^{2}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $k$ is the total number of loops in Algorithm 5, $\\epsilon_{r}^{\\prime}:=\\epsilon/(12|I_{V_{r}}(p_{r})|)$ , $I_{V_{r}}(p_{r})\\subseteq E$ represents the set of pairs between the pivot $p_{r}$ selected in phase $r$ and its neighbors in $V_{r}$ , and $\\Delta_{\\operatorname*{min},r}:=$ $\\mathrm{min}_{e\\in I_{V_{r}}(p_{r})}\\,\\Delta_{e}$ . When $k\\ll n$ , the above sample complexity can be better than that of Theorem 1. However, it should be noted that the symbols related to $r$ and the total number of loops $k$ , especially instance-dependent gaps $\\tilde{\\Delta}_{e,\\epsilon_{r}^{\\prime}}$ , are all random variables. In contrast, the current Theorem 1 does not contain any random variables. Specifically, the significant term related to $\\log\\delta^{-1}$ is characterized by the gap $\\bar{\\Delta}_{e,\\epsilon}$ or $\\Delta_{e}$ , which represents the distance from 0.5 and not a random variable. ", "page_idx": 6}, {"type": "text", "text": "4 Fixed budget setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we investigate Problem 2 and design KC-FB (Algorithm 3). KC-FB is inspired by the successive reject algorithm [6] and exploits KwikCluster to determine the number of queries for each pair adaptively. ", "page_idx": 6}, {"type": "text", "text": "Algorithm. KC-FB proceeds in at most $n$ phases and maintains the subset of elements $V_{r}\\subseteq V$ in each phase $r\\,\\in\\,[n]$ starting with $V_{1}\\,=\\,V$ . We denote the set of pairs that can be formed with $v$ in $V_{r}$ by $I_{V_{r}}(v)\\;:=\\;\\{\\{v,u\\}\\;\\in\\;{\\binom{V_{r}}{2}}\\;:\\;u\\;\\in\\;V_{r}\\}$ . In each phase $r$ , the algorithm chooses the pivot $p_{r}$ uniformly at random from $V_{r}$ , and pulls each $e\\,\\in\\,I_{V_{r}}(p_{r})$ for appropriately determined $\\tau_{r}$ times. Based on the empirical mean $\\begin{array}{r}{\\widehat{\\mathrm{s}}_{r}(\\bar{e}):=\\sum_{k=1}^{\\tau_{r}}X_{k}(e)/\\tau_{r}}\\end{array}$ for each $e\\,\\in\\,I_{V_{r}}(p_{r})$ , it finds one cluster $C_{r}\\;=\\;\\{p_{r}\\}\\cup\\Gamma_{V_{r}}(p_{r},\\widehat{\\mathrm{s}}_{r})$ ,  w here $\\Gamma_{V_{r}}(\\overline{{{p}}}_{r}^{\\,\\cdot\\,}\\!,\\stackrel{\\cdot\\,}{\\mathrm{s}_{r}})\\;:=\\;\\{u\\;\\in\\;V_{r}\\;:\\,\\widehat{\\mathrm{s}}_{r}(p_{r},u)\\;>\\;0.5\\}$ , and updates $V_{r+1}\\gets V_{r}\\setminus C_{r}$ . This procedure will be continued until $|V_{r}|=0$ and finally the algorithm outputs ${\\mathcal{C}}_{\\mathrm{out}}$ consisting of all clusters computed. Updating the number of pulls $\\tau_{r}$ (line 8) is a key to prove the statistical property. Intuitively, $\\tau_{r}$ represents a pre-fixed budget of queries when $e\\in\\bar{\\left(\\ V_{r}\\right)}$ would be pulled: In the initial phase, we allocate $\\tau_{1}:=\\lfloor T/m\\rfloor$ to each $e\\in\\left({V_{1}\\atop2}\\right)$ . Notice that the surplus, the sum of the pre-fixed budgets of pairs that have been removed without being queried, is $\\tau_{1}\\cdot\\left(|{\\binom{V_{1}}{2}}|-|{\\binom{V_{2}}{2}}|-(|V_{1}|-1)\\right)$ , because the number of pairs that have been removed in this phase is $|\\left(\\begin{array}{l}{V_{1}}\\\\ {2}\\end{array}\\right)|-|\\left(\\begin{array}{l}{V_{2}}\\\\ {2}\\end{array}\\right)|$ , and among those pairs, the number of pairs that have been actually pulled by the algorithm is $\\left(|V_{1}|-1\\right)$ . This surplus is additionally redistributed equally to each $e\\in\\left({V_{2}\\atop2}\\right)$ . This will be also done for the remaining phases $r=2,\\ldots,n$ . ", "page_idx": 6}, {"type": "text", "text": "Algorithm 3 KwikCluster with Fixed Budget (KC-FB) ", "page_idx": 7}, {"type": "text", "text": "Input : Budget $T>0$ , set $V$ of $n$ objects, additive error $\\epsilon$ 1 $V_{1}\\leftarrow V$ , $r\\gets1$ , $\\tau_{1}\\gets\\lfloor T/m\\rfloor$ , and $\\mathcal{C}_{\\mathrm{out}}\\leftarrow\\emptyset$ ; 2 while $|V_{r}|>0$ do ", "page_idx": 7}, {"type": "text", "text": "3 Pick a pivot $p_{r}\\in V_{r}$ uniformly at random;   \n4 Pull each $e\\in I_{V_{r}}(p_{r})$ for $\\tau_{r}$ times and observe random feedback $\\{X_{k}(e)\\}_{k=1}^{\\tau_{r}}$ ;   \n5 Compute empirical mean $\\begin{array}{r}{\\widehat{\\mathrm{s}}_{r}(e)=\\sum_{k=1}^{\\tau_{r}}X_{k}(e)/\\tau_{r}}\\end{array}$ for each $e\\in I_{V_{r}}(p_{r})$ ;   \n6 $\\mathcal{C}_{\\mathrm{out}}\\,\\{-\\,\\mathcal{C}_{\\mathrm{out}}\\,\\bar{\\cup}\\,\\{C_{r}\\}}$ whe r e $C_{r}:=\\{p_{r}\\}\\cup\\Gamma_{V_{r}}(p_{r},\\widehat{\\mathbf{s}}_{r})$ ;   \n7 $V_{r+1}\\gets V_{r}\\setminus C_{r}$ ;   \n8 $\\begin{array}{r}{\\tau_{r+1}\\longleftarrow\\tau_{r}+\\left\\lfloor\\frac{\\tau_{r}\\cdot(|\\binom{V_{r}}{2}|-|\\binom{V_{r+1}}{2}|-(|V_{r}|-1))}{|\\binom{V_{r+1}}{2}|}\\right\\rfloor\\mathrm{and}\\;r\\leftarrow r+1;}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "9 return ${\\mathcal{C}}_{\\mathrm{out}}$ ", "page_idx": 7}, {"type": "text", "text": "Analysis. The following theorem states that KC-FB outputs a well-approximate solution with high probability. The proof of Theorem 2 is deferred to Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. For $\\epsilon>0$ , define the minimal gap $\\Delta_{\\mathrm{min},\\epsilon}$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{e\\in E}{\\operatorname*{min}}\\operatorname*{max}\\left\\{\\frac{\\epsilon}{6\\operatorname*{max}\\{1,|E_{[0.5\\pm\\epsilon]}|\\}},\\Delta_{e}\\right\\}}&{\\mathrm{for}\\,\\epsilon\\in(0,0.5),}\\\\ {\\underset{e\\in E}{\\operatorname*{min}}\\operatorname*{max}\\left\\{\\frac{\\epsilon}{6m},\\Delta_{e}\\right\\}}&{\\mathrm{for}\\,\\epsilon\\ge0.5,}\\end{array}\\quad w h e r e\\,\\Delta_{e}=|\\mathrm{s}(e)-0.5|\\,\\,\\,(\\forall e\\in E).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then, KC-FB (Algorithm 3) uses at most $T$ queries to output $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ that satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\mathbb{E}[\\cos\\ t_{\\mathrm{out}})]\\leq5\\cdot\\mathrm{OPT}(\\mathrm{s})+\\epsilon]\\geq1-\\delta\\:\\mathrm{~for~}\\delta\\leq2n^{3}\\exp\\left(-\\frac{2T\\Delta_{\\operatorname*{min},\\epsilon}^{2}}{n^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Assuming that each query takes $\\mathcal{O}(1)$ time, the time complexity of $K C{\\cdot}F B$ is $O(T+n^{2})$ . ", "page_idx": 7}, {"type": "text", "text": "Proof Sketch. We can show the random event $\\operatorname*{Pr}{\\[\\bigcap_{r=1}^{n}\\mathcal{E}_{r}]}$ occurs with high probability, where $\\mathcal{E}_{r}\\;:=\\;\\{\\forall e\\in I_{V_{r}}(p_{r})$ , $|\\mathrm{s}(e)-\\widehat{\\mathrm{s}}_{r}(e)|<\\operatorname*{max}\\left\\{\\epsilon,\\Delta_{e}^{\\cdot}\\right\\}\\}$ for each phase $r\\,\\in\\,[n]$ (See Lemma 8 in Appendix D.1). Under the assu mption of such estimation success event $\\textstyle{\\bigcap_{r=1}^{n^{-}}{\\mathcal{E}}_{r}}$ , by utilizing the unique feature of KwikCluster, we can maintain the approximation guarantee of KwikCluster in the noise-free setting up to additive error (See Lemma 9 in Appendix D.2). Simply combining these lemmas with adjusted parameter \u03f5\u2032 \u2208(0, 0.5), defined as6 max{1, |\u03f5E[0.5\u00b1\u03f5]|} if \u03f5 < 0.5 and6\u03f5m otherwise, will conclude the proof (See Appendix D.3 for details). \u53e3 ", "page_idx": 7}, {"type": "text", "text": "The parameter $\\delta\\in(0,1)$ in (3) represents the error probability of $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ being worse than any $(5,\\epsilon)$ - approximate solution, and it decays exponentially to the querying budget $T$ . A larger parameter $\\Delta_{\\mathrm{min},\\epsilon}$ provides the better guarantee; KC-FB performs better when the similarity function clearly expresses similarity $(+1)$ or dissimilarity $(-1)$ , as $\\operatorname*{min}_{e\\in E}\\Delta_{e}$ tends to be large. ", "page_idx": 7}, {"type": "text", "text": "To evaluate the significance of our results, we analyze the uniform sampling algorithm (Uniform-FB in Appendix E); Uniform-FB queries each $e\\in E$ uniformly $\\lfloor T/m\\rfloor$ times to obtain $\\widehat{\\mathbf{s}}(e)$ , and then applies any $\\alpha$ -approximation algorithm to instance $(V,{\\widehat{\\mathbf{s}}})$ of the offilne problem mini m izing (1). We see that the error probability that the output is not an $(\\alpha,\\epsilon)$ -approximate solution is bounded by $\\begin{array}{r}{\\mathcal{O}\\left(n^{2}\\exp\\left(-\\frac{T\\epsilon^{\\bar{2}}}{\\alpha^{2}n^{6}}\\right)\\right)}\\end{array}$ . In contrast, KC-FB adaptively allocates the budget to the remaining pairs, which enables us to query essential pairs of elements, i.e., pairs whose estimated similarity values affect the behavior of cluster construction, more times than $\\lfloor\\bar{T}/m\\rfloor$ . This leads to a better performance in the cost of clustering in practice (see Section 5). ", "page_idx": 7}, {"type": "text", "text": "Comparison with existing PE-CMAB methods in the FB setting. In the literature of PE-CMAB, the FB setting presents even more computational challenges and a scarcity of theoretical results. The current state-of-the-art algorithms [6, 25, 36] suffer from one or more of the following issues: (i) inability to handle a partition structure in correlation clustering, (ii) requiring exponential running time, and (iii) lacking any approximation guarantees when the underlying problem is NP-hard. By leveraging the properties of KwikCluster, our approach ensures the polynomial-time complexity of $O(\\bar{T}+n^{2})$ while guaranteeing that the probability of obtaining a well-approximate solution exponentially increases with the budget $T$ , along with instance-dependent analysis. ", "page_idx": 7}, {"type": "text", "text": "5 Experimental evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate the performance of our proposed algorithms, KC-FC and KC-FB, using various datasets, providing empirical evidence to support our theoretical findings. ", "page_idx": 8}, {"type": "text", "text": "Datasets. We use publicly-available realworld graphs presented in Table 1. In the FC setting, to observe the behavior of the sample complexity with respect to the hidden minimum gap $\\Delta_{\\mathrm{min}}$ in (2), we generate our instances as follows. For each graph, we vary the lower bound on $\\Delta_{\\mathrm{min}}$ , which we denote by $\\mathrm{LB}_{\\Delta_{\\mathrm{min}}}$ , in $\\{0.10,0.15,0.20,\\dots,0.50\\}$ . For each pair ", "page_idx": 8}, {"type": "table", "img_path": "WRCFuoiz1h/tmp/dcee7900a0e543ddf8d515fd8418a14b525e6eb84c3cd16aff89bd055f0a794a.jpg", "table_caption": ["Table 1: Real-world graphs used in our experiments. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "of vertices $u,v$ , we set $\\mathrm{s}(\\bar{u,v})=\\mathrm{uniform}[0.5+\\mathrm{LB}_{\\Delta_{\\mathrm{min}}},1]$ if $u,v$ have an edge in the graph, and $\\mathrm{s}(u,v)\\,=\\,\\mathrm{uniform}[0,0.5\\,-\\,\\mathrm{LB}_{\\Delta_{\\mathrm{min}}}]$ otherwise, where $\\mathrm{uniform}[a,b]$ is the value drawn from the interval $[a,b]$ uniformly at random. On the other hand, in the FB setting, we employ a more realistic setting: For each graph, our problem instance is generated by embedding the vertices into a $d$ -dimensional Euclidean space using node2vec [43], obtaining a vector $\\mathrm{vec}(\\bar{v})\\,\\in\\,\\mathbb{R}^{d}$ for each vertex $v$ . Specifically, we used the publicly-available Python module of node2vec2 with default parameter settings (particularly $d=64$ ). Then, define the unknown similarity of each pair of vertices $u,v$ as $\\begin{array}{r}{\\mathrm{s}(\\bar{u^{,}}v)\\stackrel{^{\\cdot}}{=}\\frac{\\mathrm{sim}_{\\mathrm{cos}}(\\bar{\\mathrm{vec}}(u),\\mathrm{vec}(v))-\\mathrm{min}_{-}\\mathrm{cos}}{\\mathrm{max}_{-}\\mathrm{cos}-\\mathrm{min}_{-}\\mathrm{cos}}\\in\\left[0,1\\right]}\\end{array}$ , where min_cos and max_cos are the minimal and maximal cosine similarities, respectively, among all pairs of vertices. We note that max_cos $>$ min_cos holds for all instances. In all experiments, noisy feedback when querying a pair $e\\in E$ is generated by a Bernoulli distribution with mean $\\mathrm{s}(e)$ . ", "page_idx": 8}, {"type": "text", "text": "Baselines. We compare our methods with Uniform-FC in the FC setting and Uniform-FB in the FB setting, whose pseudocode and full analysis are given in Appendix E. Uniform-FC pulls each $e\\in E$ for \u2308182m2 $\\Biggl[\\frac{\\bar{1}8m^{2}}{\\epsilon^{2}}\\log\\frac{\\bar{2}m}{\\delta}\\Biggr]$ times and employs KwikCluster with respect to the empirical similarity, while Uniform-FB is its adaption to the FB setting. Moreover, we compare the cost of clustering of our algorithms with that of KwikCluster having access to the unknown (true) similarity, which is regarded as the stronger baseline than other KwikCluster-based methods for the binary case [11, 15, 40, 76]. ", "page_idx": 8}, {"type": "text", "text": "Machine and code. The experiments were performed on a machine with Apple M1 Chip and 16 GB RAM. The code was written in Python 3, which is available online.3 ", "page_idx": 8}, {"type": "text", "text": "Performance of KC-FC. We evaluate the performance of algorithms in terms of not only the c\u221aost of clustering but also the sample compl\u221aexity. In both KC-FC and Uniform-FC, we set $\\epsilon=\\sqrt{n}$ allowing each element to make only $\\bar{1/\\sqrt{n}}$ mistakes, and $\\delta=0.01$ following a standard choice in PE-MAB. Taking into account the limited scalability of the algorithms, we only use the instances with $n<1{,}000$ . In particular, as will be shown later, Uniform-FC requires a large number of samples, which makes the algorithm prohibitive even for quite small instances. Therefore, we do not run the algorithm and just report the sample complexity, which can be calculated without running it. For each $\\mathrm{LB}_{\\Delta_{\\mathrm{min}}}$ , we run both KC-FC and KwikCluster having access to the unknown similarity 100 times and report the average value and the standard deviation. ", "page_idx": 8}, {"type": "image", "img_path": "WRCFuoiz1h/tmp/a71b214fea966e7c4099493153fcc539efc51c9fc169796f67483d239555f7e9.jpg", "img_caption": ["Figure 1: Sample complexity of KC-FC & Uniform-FC. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The results are depicted in Figures 1 and 2. As can be seen, the sample complexity of KC-FC is much smaller than that of Uniform-FC. In fact, the sample complexity of Uniform-FC makes the algorithm prohibitive even for very small instances. Moreover, consistent with the theoretical analysis, as (the lower bound $\\mathrm{LB}_{\\Delta_{\\mathrm{min}}}$ on) $\\Delta_{\\mathrm{min}}$ increases, the sample complexity of KC-FC becomes smaller. This desirable property is not possessed by Uniform-FC. Remarkably, looking at Figure 2, we see that KC-FC outputs a clustering whose quality is comparable with that of KwikCluster having access to the unknown similarity. ", "page_idx": 8}, {"type": "image", "img_path": "WRCFuoiz1h/tmp/bd350321f981e8b1c480488b37ccfb6b3aa2e0b3abf764d776a5fc74bcd6b966.jpg", "img_caption": ["Figure 2: Cost of clustering of KC-FC & KwikCluster having the access to the unknown similarity. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "WRCFuoiz1h/tmp/b00674118a8ca556fc8d892c7576d2c1042514e446473da9c08bec149386537c.jpg", "table_caption": ["Table 2: Cost of clustering of KC-FB & baselines $(n\\geq1{,}000)$ "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Performance of KC-FB. Here we evaluate the performance of KC-FB. For small instances with $n<1{,}000$ ,F owre l avragrey $T$ sitna $\\{n^{2.1},n^{2.2},\\dotsb,n^{3.0}\\}$ waen df ixo fcoor sst coafl acbliulsittey.r iFnog r weiatchh r iensspteacnt cteo  atnhde $T$ $n\\geq1{,}000$ $T=n^{2.2}$   \n$T$ , we run both KC-FB and Uniform-FB 100 times and report the average value and the standard deviation. As KwikCluster having access to the unknown similarity is independent of $T$ , we just run it 100 times for each instance. ", "page_idx": 9}, {"type": "image", "img_path": "WRCFuoiz1h/tmp/b577f47f443ce95473358c44b30f4d5b0866cd5623684809a88ee8dc526f2aa6.jpg", "img_caption": ["Figure 3: Cost of clustering of KC-FB & baselines $\\mathit{\\Omega}_{\\leftmoon}<1{,}000)$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The results are shown in Figure 3 and Table 2. As can be seen, KC-FB outperforms the baseline method Uniform-FB. In fact, for all instances and almost all values of $T$ , KC-FB outputs a better clustering than that of Uniform-FB. We can see that this superiority comes from the fact that KC-FB estimates the unknown similarity better than Uniform-FB thanks to its sophisticated sampling strategy. Indeed, KwikCluster having access to the unknown similarity showcases the best performance, verifying the importance of the precise estimation of the unknown similarity. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We studied the online learning problems of correlation clustering, where the similarity function is initially unknown and only noisy feedback is observed. For the FC setting, we devised KC-FC and proved the upper bound of the number of queries required to find a clustering whose cost is at most $5\\cdot\\mathrm{OPT}+\\epsilon$ with high probability. For the FB setting, we devised KC-FB and showed that the error probability of the expected cost being worse than $5\\cdot\\mathrm{OPT}+\\epsilon$ decays exponentially with budget $T$ . Importantly, our algorithms are the first examples of PE-CMAB with NP-hard offline problems. One future work, yet a significant challenge, is to derive information-theoretic lower bounds of PE-CMAB in the case where the offline problem is NP-hard. Investigating other variants of correlation clustering or exploring the case where the variance of random feedback differs across pairs, namely heteroscedastic noise, would also be worthwhile directions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work of Yuko Kuroki is supported by Japan Science and Technology Agency (JST) Strategic Basic Research Programs PRESTO \u201cR&D Process Innovation by AI and Robotics: Technical Foundations and Practical Applications\u201d grant number JPMJPR24T2, and was partially supported by Microsoft Research Asia and JST Strategic Basic Research Programs ACT-X grant number JPMJAX200E while she was at The University of Tokyo. The authors would like to thank the anonymous reviewers for their insightful comments and useful feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ahmadian, S., Epasto, A., Kumar, R., and Mahdian, M. (2020). Fair correlation clustering. In Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS), pages 4195\u20134205.   \n[2] Ailon, N., Bhattacharya, A., and Jaiswal, R. (2018). Approximate correlation clustering using same-cluster queries. In Proc. Latin American Symp. on Theoretical Informatics (LATIN), pages 14\u201327.   \n[3] Ailon, N., Charikar, M., and Newman, A. (2008). Aggregating inconsistent information: Ranking and clustering. Journal of the ACM, 55(5):1\u201327.   \n[4] Aronsson, L. and Chehreghani, M. H. (2024a). Correlation clustering with active learning of pairwise similarities. Transactions on Machine Learning Research.   \n[5] Aronsson, L. and Chehreghani, M. H. (2024b). Effective acquisition functions for active correlation clustering.   \n[6] Audibert, J.-Y., Bubeck, S., and Munos, R. (2010). Best arm identification in multi-armed bandits. In Proc. Conf. on Learning Theory (COLT), pages 41\u201353.   \n[7] Bansal, N., Blum, A., and Chawla, S. (2004). Correlation clustering. Machine Learning, 56(1-3):89\u2013113.   \n[8] Barrier, A., Garivier, A., and Stoltz, G. (2023). On best-arm identification with a fixed budget in non-parametric multi-armed bandits. In Proc. Int. Conf. on Algorithmic Learning Theory (ALT), pages 136\u2013181.   \n[9] Ben-Dor, A., Shamir, R., and Yakhini, Z. (1999). Clustering gene expression patterns. Journal of Computational Biology, 6(3/4):281\u2013297.   \n[10] Bonchi, F., Garc\u00eda-Soriano, D., and Gullo, F. (2022). Correlation Clustering, volume 12 of Synthesis Lectures on Data Mining and Knowledge Discovery. Morgan & Claypool Publishers.   \n[11] Bonchi, F., Garc\u00eda-Soriano, D., and Kutzkov, K. (2013a). Local correlation clustering. arXiv preprint arXiv:1312.5105.   \n[12] Bonchi, F., Garc\u00eda-Soriano, D., and Liberty, E. (2014). Correlation clustering: From theory to practice. In Proc. ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD).   \n[13] Bonchi, F., Gionis, A., Gullo, F., Tsourakakis, C., and Ukkonen, A. (2015). Chromatic correlation clustering. ACM Transactions on Knowledge Discovery from Data, 9(4):1\u201324.   \n[14] Bonchi, F., Gionis, A., and Ukkonen, A. (2013b). Overlapping correlation clustering. Knowledge and Information Systems, 35(1):1\u201332.   \n[15] Bressan, M., Cesa-Bianchi, N., Paudice, A., and Vitale, F. (2019). Correlation clustering with adaptive similarity queries. In Proc. Conf. on Advances in Neural Information Processing Systems (NeurIPS), pages 12510\u201312519.   \n[16] Bubeck, S. and Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends\u00ae in Machine Learning, 5:1\u2013122.   \n[17] Bubeck, S., Munos, R., and Stoltz, G. (2011). Pure exploration in finitely-armed and continuousarmed bandits. Theoretical Computer Science, 412(19):1832 \u2013 1852.   \n[18] Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, Learning, and Games. Cambridge University Press.   \n[19] Cesa-Bianchi, N. and Lugosi, G. (2012). Combinatorial bandits. Journal of Computer and System Sciences, 78(5):1404\u20131422.   \n[20] Charikar, M., Guruswami, V., and Wirth, A. (2005). Clustering with qualitative information. Journal of Computer and System Sciences, 71(3):360\u2013383.   \n[21] Chaudhuri, A. R. and Kalyanakrishnan, S. (2019). Pac identification of many good arms in stochastic multi-armed bandits. In Proc. Int. Conf. on Machine Learning (ICML), pages 991\u20131000.   \n[22] Chawla, S., Makarychev, K., Schramm, T., and Yaroslavtsev, G. (2015). Near optimal LP rounding algorithm for correlation clustering on complete and complete $k$ -partite graphs. In Proc. ACM Symp. on Theory of Computing (STOC), pages 219\u2013228.   \n[23] Chen, L., Gupta, A., and Li, J. (2016a). Pure exploration of multi-armed bandit under matroid constraints. In Proc. Conf. on Learning Theory (COLT), pages 647\u2013669.   \n[24] Chen, L. and Li, J. (2015). On the optimal sample complexity for best arm identification. arXiv preprint arXiv:1511.03774.   \n[25] Chen, S., Lin, T., King, I., Lyu, M. R., and Chen, W. (2014a). Combinatorial pure exploration of multi-armed bandits. In Proc. Conf. on Advances in Neural Information Processing Systems (NIPS), pages 379\u2013387.   \n[26] Chen, W., Wang, Y., and Yuan, Y. (2013). Combinatorial multi-armed bandit: General framework and applications. In Proc. Int. Conf. on Machine Learning (ICML), pages 151\u2013159.   \n[27] Chen, W., Wang, Y., Yuan, Y., and Wang, Q. (2016b). Combinatorial multi-armed bandit and its extension to probabilistically triggered arms. Journal of Machine Learning Research, 17(50):1\u201333.   \n[28] Chen, Y., Jalali, A., Sanghavi, S., and Xu, H. (2014b). Clustering partially observed graphs via convex optimization. Journal of Machine Learning Research, 15(1):2213\u20132238.   \n[29] Chierichetti, F., Dalvi, N., and Kumar, R. (2014). Correlation clustering in MapReduce. In Proc. ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD), page 641\u2013650.   \n[30] Cohen-Addad, V., Lee, E., Li, S., and Newman, A. (2023). Handling correlated rounding error via preclustering: A 1.73-approximation for correlation clustering. In Proc. IEEE Symp. on Foundations of Computer Science (FOCS), pages 1082\u20131104.   \n[31] Cohen-Addad, V., Lee, E., and Newman, A. (2022). Correlation clustering with Sherali-Adams. In Proc. IEEE Symp. on Foundations of Computer Science (FOCS), pages 651\u2013661.   \n[32] Davies, S., Moseley, B., and Newman, H. (2023). Fast combinatorial algorithms for min max correlation clustering. In Proc. Int. Conf. on Machine Learning (ICML), pages 7205\u20137230.   \n[33] Degenne, R. and Koolen, W. M. (2019). Pure exploration with multiple correct answers. In Proc. Conf. on Advances in Neural Information Processing Systems (NeurIPS), pages 14591\u20131460.   \n[34] Demaine, E. D., Emanuel, D., Fiat, A., and Immorlica, N. (2006). Correlation clustering in general weighted graphs. Theoretical Computer Science, 361(2-3):172\u2013187.   \n[35] Du, Y., Kuroki, Y., and Chen, W. (2021a). Combinatorial pure exploration with bottleneck reward function. In Proc. Conf. on Advances in Neural Information Processing Systems (NeurIPS), pages 23956\u201323967.   \n[36] Du, Y., Kuroki, Y., and Chen, W. (2021b). Combinatorial pure exploration with full-bandit or partial linear feedback. In Proc. AAAI Conf. on Artificial Intelligence (AAAI), pages 7262\u20137270.   \n[37] Even-Dar, E., Mannor, S., and Mansour, Y. (2002). PAC bounds for multi-armed bandit and Markov decision processes. In Proc. Conf. on Learning Theory (COLT), pages 255\u2013270.   \n[38] Even-Dar, E., Mannor, S., and Mansour, Y. (2006). Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 7:1079\u20131105.   \n[39] Fiez, T., Jain, L., Jamieson, K. G., and Ratliff, L. (2019). Sequential experimental design for transductive linear bandits. In Proc. Conf. on Advances in Neural Information Processing Systems (NeurIPS), pages 10667\u201310677.   \n[40] Garc\u00eda-Soriano, D., Kutzkov, K., Bonchi, F., and Tsourakakis, C. E. (2020). Query-efficient correlation clustering. In Proc. The Web Conference, pages 1468\u20131478.   \n[41] Garivier, A. and Kaufmann, E. (2016). Optimal best arm identification with fixed confidence. In Proc. Conf. on Learning Theory (COLT), pages 998\u20131027.   \n[42] Gionis, A., Mannila, H., and Tsaparas, P. (2007). Clustering aggregation. ACM Transactions on Knowledge Discovery from Data, 1(1).   \n[43] Grover, A. and Leskovec, J. (2016). Node2vec: Scalable feature learning for networks. In Proc. ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD), page 855\u2013864.   \n[44] Gullo, F., Mandaglio, D., and Tagarelli, A. (2023). A combinatorial multi-armed bandit approach to correlation clustering. Data Mining and Knowledge Discovery, 37:1\u201362.   \n[45] Gupta, S., W J Staar, P., and de Sainte Marie, C. (2024). Clustering items from adaptively collected inconsistent feedback. In Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS), pages 604\u2013612.   \n[46] Hassanzadeh, O., Chiang, F., Lee, H. C., and Miller, R. J. (2009). Framework for evaluating clustering algorithms in duplicate detection. Proc. the VLDB Endowment (PVLDB), 2(1):1282\u2013 1293.   \n[47] Huang, W., Ok, J., Li, L., and Chen, W. (2018). Combinatorial pure exploration with continuous and separable reward functions and its applications. In Proc. Int. Joint Conf. on Artificial Intelligence (IJCAI), pages 2291\u20132297.   \n[48] Jamieson, K., Malloy, M., Nowak, R., and Bubeck, S. (2014). lil\u2019UCB: An optimal exploration algorithm for multi-armed bandits. In Proc. Conf. on Learning Theory (COLT), pages 423\u2013439.   \n[49] Joachims, T. and Hopcroft, J. E. (2005). Error bounds for correlation clustering. In Proc. Int. Conf. on Machine Learning (ICML), pages 385\u2013392. ACM.   \n[50] Jourdan, M., Mutn\u00fd, M., Kirschner, J., and Krause, A. (2021). Efficient pure exploration for combinatorial bandits with semi-bandit feedback. In Proc. Int. Conf. on Algorithmic Learning Theory (ALT), pages 805\u2013849.   \n[51] Kano, H., Honda, J., Sakamaki, K., Matsuura, K., Nakamura, A., and Sugiyama, M. (2019). Good arm identification via bandit feedback. Machine Learning, 108:721\u2013745.   \n[52] Katz-Samuels, J., Jain, L., Karnin, Z., and Jamieson, K. (2020). An empirical process approach to the union bound: Practical algorithms for combinatorial and linear bandits. In Proc. Conf. on Advances in Neural Information Processing Systems (NeurIPS).   \n[53] Kaufmann, E., Capp\u00e9, O., and Garivier, A. (2016). On the complexity of best-arm identification in multi-armed bandit models. Journal of Machine Learning Research, 17:1\u201342.   \n[54] Kim, S., Nowozin, S., Kohli, P., and Yoo, C. D. (2011). Higher-order correlation clustering for image segmentation. In Proc. Conf. on Advances in Neural Information Processing Systems (NIPS), pages 1530\u20131538.   \n[55] Kuroki, Y., Miyauchi, A., Honda, J., and Sugiyama, M. (2020a). Online dense subgraph discovery via blurred-graph feedback. In Proc. Int. Conf. on Machine Learning (ICML), pages 5522\u20135532.   \n[56] Kuroki, Y., Xu, L., Miyauchi, A., Honda, J., and Sugiyama, M. (2020b). Polynomial-time algorithms for multiple-arm identification with full-bandit feedback. Neural Computation, 32(9):1733\u2013 1773.   \n[57] Lai, T. L. and Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4\u201322.   \n[58] Larsen, K. G., Mitzenmacher, M., and Tsourakakis, C. E. (2020). Clustering with a faulty oracle. In Proc. The Web Conference, page 2831\u20132834.   \n[59] Lattimore, T. and Szepesv\u00e1ri, C. (2020). Bandit Algorithms. Cambridge University Press.   \n[60] Locatelli, A., Gutzeit, M., and Carpentier, A. (2016). An optimal algorithm for the thresholding bandit problem. In Proc. Int. Conf. on Machine Learning (ICML), pages 1690\u20131698.   \n[61] Makarychev, K. and Chakrabarty, S. (2023). Single-pass pivot algorithm for correlation clustering. Keep it simple! In Proc. Conf. on Advances in Neural Information Processing Systems (NeurIPS).   \n[62] Makarychev, K., Makarychev, Y., and Vijayaraghavan, A. (2015). Correlation clustering with noisy partial information. In Proc. Conf. on Learning Theory (COLT), pages 1321\u20131342.   \n[63] Mason, B., Jain, L., Tripathy, A., and Nowak, R. (2020). Finding all $\\epsilon$ -good arms in stochastic bandits. In Proc. Conf. on Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 20707\u201320718.   \n[64] Mathieu, C. and Schudy, W. (2010). Correlation clustering with noisy input. In Proc. ACM-SIAM Symp. on Discrete Algorithms (SODA), pages 712\u2013728.   \n[65] Mazumdar, A. and Saha, B. (2017). Clustering with noisy queries. In Proc. Conf. on Advances in Neural Information Processing Systems (NIPS), page 5790\u20135801.   \n[66] McCallum, A. and Wellner, B. (2005). Conditional models of identity uncertainty with application to noun coreference. In Proc. Conf. on Advances in Neural Information Processing Systems (NIPS), pages 905\u2013912.   \n[67] Nakamura, S. and Sugiyama, M. (2024). Thompson sampling for real-valued combinatorial pure exploration of multi-armed bandit. In Proc. AAAI Conf. on Artificial Intelligence (AAAI).   \n[68] Nepusz, T., Yu, H., and Paccanaro, A. (2012). Detecting overlapping protein complexes in protein-protein interaction networks. Nature Methods, 9(5):471\u2013472.   \n[69] Pan, X., Papailiopoulos, D., Oymak, S., Recht, B., Ramchandran, K., and Jordan, M. I. (2015). Parallel correlation clustering on big graphs. In Proc. Conf. on Advances in Neural Information Processing Systems (NIPS), pages 82\u201390.   \n[70] Peng, P. and Zhang, J. (2021). Towards a query-optimal and time-efficient algorithm for clustering with a faulty oracle. In Proc. Conf. on Learning Theory (COLT), pages 3662\u20133680.   \n[71] Pia, A. D., Ma, M., and Tzamos, C. (2022). Clustering with queries under semi-random noise. In Proc. Conf. on Learning Theory (COLT), pages 5278\u20135313.   \n[72] Ramachandran, A., Feamster, N., and Vempala, S. (2007). Filtering spam with behavioral blacklisting. In Proc. ACM Conf. on Computer and Communications Security (CCS), pages 342\u2013351.   \n[73] Robbins, H. (1952). Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58:527\u2013535.   \n[74] Saha, B. and Subramanian, S. (2019). Correlation Clustering with Same-Cluster Queries Bounded by Optimal Cost. In Proc. European Symp. on Algorithms (ESA), pages 81:1\u201381:17.   \n[75] Shamir, R., Sharan, R., and Tsur, D. (2004). Cluster graph modification problems. Discrete Applied Mathematics, 144(1-2):173\u2013182.   \n[76] Silwal, S., Ahmadian, S., Nystrom, A., McCallum, A., Ramachandran, D., and Kazemi, S. M. (2023). Kwikbucks: Correlation clustering with cheap-weak and expensive-strong signals. In Proc. Int. Conf. on Learning Representations (ICLR).   \n[77] Tsourakakis, C. E., Mitzenmacher, M., Larsen, K. G., B\u0142asiok, J., Lawson, B., Nakkiran, P., and Nakos, V. (2020). Predicting positive and negative links with noisy queries: Theory & practice.   \n[78] Tzeng, R.-C., Wang, P.-A., Proutiere, A., and Lu, C.-J. (2023). Closing the computationalstatistical gap in best arm identification for combinatorial semi-bandits. In Proc. Conf. on Advances in Neural Information Processing Systems (NeurIPS).   \n[79] Veldt, N. (2022). Correlation clustering via strong triadic closure labeling: Fast approximation algorithms and practical lower bounds. In Proc. Int. Conf. on Machine Learning (ICML), pages 22060\u201322083.   \n[80] Wang, J., Kraska, T., Franklin, M. J., and Feng, J. (2012). Crowder: Crowdsourcing entity resolution. Proc. the VLDB Endowment (PVLDB), 5(11):1483\u20131494.   \n[81] Wang, Q. and Chen, W. (2017). Improving regret bounds for combinatorial semi-bandits with probabilistically triggered arms and its applications. In Proc. Conf. on Advances in Neural Information Processing Systems (NeurIPS), page 1161\u20131171.   \n[82] Wang, S. and Zhu, J. (2022). Thompson sampling for (combinatorial) pure exploration. In Proc. Int. Conf. on Machine Learning (ICML), pages 23470\u201323483.   \n[83] Xia, J. and Huang, Z. (2022). Optimal clustering with noisy queries via multi-armed bandit. In Proc. Int. Conf. on Machine Learning (ICML), pages 24315\u201324331. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Additional comparison with other clustering models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Cluster recovering with noisy same-cluster queries. Another line of studies has focused on clustering reconstruction with noisy same-cluster queries, which was first proposed by Mazumdar and Saha [65] and further investigated by Larsen et al. [58], Peng and Zhang [70], Pia et al. [71], Tsourakakis et al. [77] and Xia and Huang [83]. In this model, given a set of $n$ elements, the goal is to recover the underlying ground-truth $k$ -clustering by asking pairwise queries to an oracle, which tells us if the two elements belong to the same cluster, but whose answer is correct only with probability $\\textstyle{\\frac{1}{2}}+{\\frac{\\delta}{2}}$ . Recently, Gupta et al. [45] first considered a noisy and inconsistent oracle, in contrast to a consistent oracle that returns the same answer when queried. Although this clustering reconstruction problem shares the intuition with correlation clustering, there are also important differences which do not allow to transfer algorithmic results from one to the other. Firstly, in correlation clustering the input information might be inconsistent (e.g., $a$ is very similar to $b$ , which is very similar to $c$ , but $a$ is not similar to $c$ ), instead in clustering reconstruction this is not possible: if $a$ is in the same cluster of $b$ and $b$ is in the same cluster of $c$ , then $a$ is in the same cluster of $c$ . Secondly, the aim is to reconstruct the exact underlying clustering, while we aim at minimizing the cost function in (1). Lastly and more importantly, the number of clusters $k$ is part of the input to the problem, while in correlation clustering it is unknown. Therefore, the theoretical results and techniques for solving the clustering reconstruction problem cannot be directly applied to correlation clustering. ", "page_idx": 15}, {"type": "text", "text": "Query-based correlation clustering. As fully discussed in the main text, our work lies in queryefficient correlation clustering, for which we utilize the methodology of PE-CMAB. Table 3 summarizes how existing query-based settings differ from our attempt. Note that the oracle in Aronsson and Chehreghani [4, 5] is assumed to return the true value of $\\operatorname{s}(x,y)$ with probability $1-\\gamma$ and a noisy value with probability $\\gamma$ , which is different from our models; we only observe a random variable independently sampled from an unknown distribution with mean $\\mathrm{s}(\\dot{x},y)\\in[0,1]$ . While one might consider using majority voting on repeated queries to handle noise when the underlying distribution is Bernoulli, this approach lacks any approximation guarantees and query complexity bounds. Moreover, for $R$ -sub-Gaussian noise, majority voting is not well-defined. Instead, using sample mean estimates, as done in PE-MAB methods, is standard. Our approach leverages these principles, ensuring a $(5,\\epsilon)$ -approximation guarantee with fewer queries and statistical guarantees. ", "page_idx": 15}, {"type": "table", "img_path": "WRCFuoiz1h/tmp/6b48ebdf45a2a0650c4fa73f5021d8ca173fc02fabd32ef43f9721f013c00345.jpg", "table_caption": ["Table 3: Different problem settings in correlation clustering with queries "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Pseudocode of KwikCluster and Algorithm 5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We detail the pseudocode of KwikCluster [3] in Algorithm 4. The approximation guarantee of KwikCluster is 5 for the weighted similarity case (and 3 for the restricted binary similarity case). We also detail the pseudocode of Algorithm 5, which sequentially uses TB-HS at each phase in the framework of KC-FC. ", "page_idx": 15}, {"type": "text", "text": "Input : Set $V$ of $n$ objects, and similarity function s   \n1 $\\mathcal{C}\\gets\\emptyset$ ;   \n2 while $|V|>0$ do   \n3 Pick a pivot $p\\in V$ uniformly at random;   \n4 $\\mathcal{C}\\leftarrow\\mathcal{C}\\cup\\{C_{p}\\}$ where $C_{p}:=\\{p\\}\\cup\\{u\\in V:\\mathrm{s}(p,u)>0.5\\}$ ;   \n5 $V\\leftarrow V\\setminus C_{p}^{\\;\\top}$ ; ", "page_idx": 16}, {"type": "text", "text": "6 return $\\mathcal{C}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 5 KC-FC variant with sequential use of TB-HS ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input : Confidence level $\\delta$ , set $V$ of $n$ objects, and error $\\epsilon$   \n7 $E_{1}\\leftarrow E$ , $V_{1}\\leftarrow V$ , $r\\gets1$ , and $\\mathcal{C}_{\\mathrm{out}}\\gets\\emptyset$ ;   \n8 while $|V_{r}|>0$ do   \n9 Pick a pivot $p_{r}\\in V_{r}$ uniformly at random;   \n10 Let $I_{V_{r}}(p_{r})\\subseteq E$ be the set of pairs between the pivot $p_{r}$ and its neighbors in $V_{r}$ ;   \n11 $\\epsilon_{r}^{\\prime}:=\\epsilon/(12|I_{V_{r}}(p_{r})|)$ ;   \n12 Compute $\\widehat{G}_{\\epsilon^{\\prime}}^{\\left(r\\right)}$ by TB-HS (Algorithm 2) with the input of error $\\epsilon_{r}^{\\prime}$ , confidence level $\\delta/n$ , and the   \nset of pa irs $I_{V_{r}}(p_{r})$ ;   \n13 Define $\\widehat{\\Gamma}^{(r)}(v):=\\{u\\in V:\\{u,v\\}\\in\\widehat{G}_{\\epsilon^{\\prime}}^{(r)}\\}$ ;   \n14 $\\mathcal{C}_{\\mathrm{out}}\\leftarrow\\mathcal{C}_{\\mathrm{out}}\\cup\\{C_{r}\\}$ , where $C_{r}:=(\\{p_{r}\\}\\cup\\widehat{\\Gamma}^{(r)}(p_{r}))\\cap V_{r}$ ;   \n15 $V_{r+1}\\gets V_{r}\\setminus C_{r}$ and $r\\gets r+1$ ;   \n16 return ${\\mathcal{C}}_{\\mathrm{out}}$ ", "page_idx": 16}, {"type": "text", "text": "C Analysis of KC-FC ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide a complete proof of Theorem 1 in Section 3. In particular, Lemma 4 guarantees the accuracy of the subroutine TB-HS, and based on this, Lemma 5 assures the $(5,\\epsilon)$ - approximation using the properties of KwikCluster. Moreover, Lemma 6 establishes an upper bound crucial for the sample complexity via novel analysis dependent on $\\epsilon$ and $\\Delta_{\\mathrm{min}}$ (Lemma 7). Finally, by combining the sample complexity required by subroutine TB-HS (Lemma 6) and the output guarantee of KC-FC (Lemma 5), Theorem 1 is demonstrated. ", "page_idx": 16}, {"type": "text", "text": "C.1 Basic lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first introduce the Hoeffding inequality, which will be frequently used in our proof. Note that we consider Bernoulli distribution for the sake of simplicity, but our results would carry on for $R$ -sub-Gaussian distribution by simply adjusting the Hoeffding inequality for the case accordingly. ", "page_idx": 16}, {"type": "text", "text": "Lemma 1 (Hoeffding inequality for bounded random variables). Let $X_{1},\\ldots,X_{k}$ be $k$ independent random variables such that, $\\mathbb{E}[X_{i}]=\\mu$ and $a\\leq X_{i}\\leq b$ for each $i\\in[k]$ . Let $\\textstyle{\\bar{X}}\\,=\\,{\\frac{1}{k}}\\sum_{i=1}^{k}X_{i}$ denote the average of these random variables. Then, for any $\\lambda>0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[{\\bar{X}}\\leq\\mu-\\lambda\\right]\\leq\\exp\\left(-{\\frac{2k\\lambda^{2}}{(b-a)^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The next lemma presents the probability that some random event happens, which will be used later. ", "page_idx": 16}, {"type": "text", "text": "Lemma 2. Let $\\widehat{\\mathrm{s}}_{e,k}$ be the empirical mean of the rewards when e has been pulled $k$ times. For each $e\\in[m]$ and $k$ ,  d efine the random event $\\mathcal{E}_{e,k}$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{E}_{e,k}:=\\left\\{|\\mathrm{s}(e)-\\widehat{\\mathrm{s}}_{e,k}|<\\sqrt{\\frac{\\log(4m k^{2}/\\delta)}{2k}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\mathcal{E}_{k}$ be the random event that for all $e\\in[m]$ , the random event $\\mathcal{E}_{e,k}$ happens. Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\bigcap_{k=1}^{\\infty}{\\mathcal{E}}_{k}\\right]\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 2. We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\bigcup_{k=1}^{\\infty}-\\mathcal{E}_{e,k}\\right]=\\sum_{k=1}^{\\infty}\\operatorname*{Pr}\\left[\\,|\\mathrm{s}(e)-\\widehat{\\mathrm{s}}_{e,k}|\\geq\\sqrt{\\frac{\\log(4m k^{2}/\\delta)}{2k}}\\right]\\leq\\sum_{k=1}^{\\infty}\\frac{\\delta}{2m k^{2}}=\\frac{\\pi^{2}\\delta}{12m}\\leq\\frac{\\delta}{m},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality follows from the Hoeffding inequality (Lemma 1). Therefore, by taking the union bound, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\bigcap_{k=1}^{\\infty}\\mathcal{E}_{k}\\right]\\geq1-\\operatorname*{Pr}\\left[\\bigcup_{e\\in[m]}\\bigcup_{k=1}^{\\infty}\\mathcal{E}_{e,k}\\right]\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2 Approximation guarantee ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide the following lemmas for guaranteeing the quality of the output. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. Let $\\epsilon\\in(0,0.5)$ and $\\delta\\in(0,1)$ . TB-HS (Algorithm 2) with parameter $\\epsilon,\\delta$ outputs, with probability at least $1-\\delta$ , $\\widehat{G}_{\\epsilon}$ and $\\widehat{B}_{\\epsilon}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{s}(e)\\geq0.5-\\epsilon\\,\\mathrm{~for~every~}e\\in\\widehat{G}_{\\epsilon},}\\\\ &{\\mathrm{s}(e)\\leq0.5+\\epsilon\\,\\mathrm{~for~every~}e\\in\\widehat{B}_{\\epsilon},}\\\\ &{\\widehat{G}_{\\epsilon}\\cup\\widehat{B}_{\\epsilon}=E,\\;\\widehat{G}_{\\epsilon}\\cap\\widehat{B}_{\\epsilon}=\\emptyset.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 3. The proof is almost straightforward by the procedure of the algorithm, line 8 and 10, as follows. By Lemma 2, we have $\\begin{array}{r}{\\bar{\\operatorname*{Pr}}\\left[\\bigcap_{k=1}^{\\infty}\\mathcal{E}_{k}\\right]\\stackrel{!}{\\geq}1-\\delta}\\end{array}$ . Now we assume that $\\bigcap_{k=1}^{\\infty}{\\mathcal{E}}_{k}$ happens. Let $t\\,>\\,0$ be the stopping time, where every $e\\ \\in\\ E$ has been added to either $\\widehat{G}_{\\epsilon}$ or $\\widehat{B}_{\\epsilon}^{}$ . For $e\\in\\widehat{G}_{\\epsilon}$ , from the stopping condition and the random event $\\bigcap_{k=1}^{\\infty}{\\mathcal{E}}_{k}$ , it is easy to see that $\\mathrm{s}(e)\\geq\\widehat{\\mathrm{s}}_{t^{\\prime}}(e)-\\mathrm{rad}_{t^{\\prime}}(e)\\geq0.5-\\epsilon$ , where $t^{\\prime}$ denotes the round that arm $e$ was added to $\\widehat{G}_{\\epsilon}$ . For $e\\in\\widehat{B}_{\\epsilon}$ , it is also easy to see that $\\mathrm{s}(e)\\leq\\widehat{\\mathrm{s}}_{t^{\\prime}}(e)+\\mathrm{rad}_{t^{\\prime}}(e)\\leq0.5+\\epsilon$ , where $t^{\\prime}$ denotes the round that arm $e$ was added to $\\widehat{B}_{\\epsilon}$ . The third condition is obvious from the stopping condition of the algorithm. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 4. Let $\\epsilon\\in(0,0.5)$ and $\\delta\\in(0,1)$ . Let $\\widehat{G}_{\\epsilon}$ and $\\widehat{B}_{\\epsilon}$ be the output of TB-HS (Algorithm 2) with parameters $\\epsilon,\\delta$ . Then, with probability at  least $1-\\delta$ , we have that $(i)$ every $e\\in E_{(0.5+\\epsilon,1]}\\iota$ is included in $\\widehat{G}_{\\epsilon}$ , and (ii) every $e\\in E_{[0,0.5-\\epsilon)}$ is included in $\\widehat{B}_{\\epsilon}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 4. We have $\\operatorname*{Pr}\\left[\\bigcap_{k=1}^{\\infty}\\mathcal{E}_{k}\\right]\\geq1-\\delta$ by Lemma 2 again, and we assume that $\\bigcap_{k=1}^{\\infty}{\\mathcal{E}}_{k}$ happens. Consider any $e\\in E_{(0.5+\\epsilon,1]}$ . Suppose that $e$ is not included in $\\widehat{G}_{\\epsilon}$ . Then, from Lemma 3, we see that $e\\in\\widehat{B}_{\\epsilon}$ , and thus $\\mathrm{s}(e)\\leq0.5+\\epsilon$ , which contradicts the fact that $e\\in E_{(0.5+\\epsilon,1]}$ . Therefore, $e$ is included in $\\widehat{G}_{\\epsilon}$ . Similarly, consider any $e\\in E_{[0,0.5-\\epsilon)}$ . Suppose that $e$ is not included in $\\widehat{B}_{\\epsilon}$ . Then, from Lemma 3, we see that $e\\in\\widehat{G}_{\\epsilon}$ , and thus $\\mathbf{s}(e)\\geq0.5-\\epsilon$ , which contradicts the fact that $e\\in E_{[0,0.5-\\epsilon)}$ . Therefore, $e$ is included in $\\widehat{B}_{\\epsilon}$ . ", "page_idx": 17}, {"type": "text", "text": "Based on Lemma 4, we prove the following key lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5 (Approximation guarantee). Let $\\epsilon\\,\\in\\,(0,0.5)$ and $\\delta\\,\\in\\,(0,1)$ . With probability at least $1-\\delta$ , the output $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ of KC-FC (Algorithm 1), where subroutine $_{T B}$ -HS (Algorithm 2) is invoked with parameters $\\epsilon,\\delta$ , is $a$ $(5,12\\epsilon|E_{[0.5\\pm\\epsilon]}|)$ -approximate solution for instance $(V,\\mathrm{s})$ of the offline problem minimizing (1). ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 5. By Lemma 4, we have $E_{(0.5+\\epsilon,1]}\\subseteq\\widehat{G}_{\\epsilon}$ and $E_{[0,0.5-\\epsilon)}\\subseteq\\widehat{B}_{\\epsilon}$ w.p. at least $1-\\delta$ . Construct the similarity function $\\widetilde{\\mathrm{s}}:E\\to[0,1]$ such that for each $e\\in E$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{s}}(e)=\\left\\{\\!\\!\\begin{array}{l l}{\\mathbf{s}(e)}&{\\mathrm{if~}e\\in E_{[0,0.5-\\epsilon)}\\cup E_{(0.5+\\epsilon,1]},}\\\\ {\\widetilde{\\mathbf{s}}_{e}}&{\\mathrm{otherwise,}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\widetilde{\\mathrm{s}}_{e}$ is an arbitrary value that satisfies $|\\mathrm{s}(e)-\\widetilde{\\mathrm{s}}_{e}|<2\\epsilon$ . Consider running KwikCluster with the similaritys. Let $\\ensuremath{\\mathcal{C}^{\\prime}}_{\\mathrm{out}}$ be the output of this algorithm. Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{cost}_{\\mathrm{s}}(\\mathcal{C}_{\\mathrm{out}}^{\\prime})]<\\mathbb{E}[\\mathrm{cost}_{\\widetilde{\\mathrm{s}}}(\\mathcal{C}_{\\mathrm{out}}^{\\prime})]+2\\epsilon|E_{[0.5\\pm\\epsilon]}|}\\\\ &{\\leq5\\cdot\\mathrm{OPT}(\\widetilde{\\mathrm{s}})+2\\epsilon|E_{[0.5\\pm\\epsilon]}|}\\\\ &{<5\\left(\\mathrm{OPT}(\\mathrm{s})+2\\epsilon|E_{[0.5\\pm\\epsilon]}|\\right)+2\\epsilon|E_{[0.5\\pm\\epsilon]}|}\\\\ &{=5\\cdot\\mathrm{OPT}(\\mathrm{s})+12\\epsilon|E_{[0.5\\pm\\epsilon]}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Noticing that KC-FC corresponds to the above algorithm associated with a certain choice of $\\widetilde{\\mathrm{s}}$ (i.e., $\\widetilde{\\mathrm{s}}_{e}$ for $e\\in E_{[0.5\\pm\\epsilon]})$ , we have the lemma. ", "page_idx": 18}, {"type": "text", "text": "C.3 Sample complexity analysis and proof of Theorem 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We prove the following main lemma to evaluate the sample complexity of TB-HS. Let $m_{\\mathrm{g}}$ be the number of pairs (i.e., arms) whose similarity is no less than the threshold 0.5. Without loss of generality, we assume that $E=[m]$ indexed as $\\mathrm{s}(1)\\geq\\cdots\\geq\\mathrm{s}(m_{\\mathrm{g}})\\geq0.5>\\mathrm{s}(m_{\\mathrm{g}}+1)\\geq\\cdots\\geq$ $\\bar{\\mathbf{s}}(m)$ in whole analysis. ", "page_idx": 18}, {"type": "text", "text": "Lemma 6 (Sample complexity). The upper bound of the sample complexity of TB-HS (Algorithm 2) with parameters $\\epsilon\\in(0,0.5)$ and $\\delta\\in(0,1)$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\nT=\\mathcal{O}\\left(\\sum_{e\\in E}\\frac{1}{\\tilde{\\Delta}_{e,\\epsilon}^{2}}\\log\\left(\\frac{\\sqrt{m/\\delta}}{\\tilde{\\Delta}_{e,\\epsilon}^{2}}\\log\\left(\\frac{\\sqrt{m/\\delta}}{\\tilde{\\Delta}_{e,\\epsilon}^{2}}\\right)\\right)+\\frac{m}{\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prove Lemma 6, we begin with the following lemma. Recall that $\\tilde{\\Delta}_{e,\\epsilon}$ and $\\Delta_{\\mathrm{min}}$ are defined by (2). ", "page_idx": 18}, {"type": "text", "text": "Lemma 7. Let $\\epsilon\\in(0,0.5)$ and $\\delta\\in(0,1)$ . Define ", "page_idx": 18}, {"type": "equation", "text": "$$\nk_{e}:=\\frac{1}{\\tilde{\\Delta}_{e,\\epsilon}^{2}}\\log\\left(\\frac{4\\sqrt{m/\\delta}}{\\tilde{\\Delta}_{e,\\epsilon}^{2}}\\log\\left(\\frac{5\\sqrt{m/\\delta}}{\\tilde{\\Delta}_{e,\\epsilon}^{2}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\begin{array}{r}{L e t\\,\\underline{{\\mathrm{s}}}_{e,k}:=\\widehat{\\mathrm{s}}_{e,k}-\\sqrt{\\frac{\\log(4m k^{2}/\\delta)}{2k}}}\\end{array}$ , and $\\begin{array}{r}{\\overline{{\\mathrm{s}}}_{e,k}:=\\widehat{\\mathrm{s}}_{e,k}+\\sqrt{\\frac{\\log\\left(4m k^{2}/\\delta\\right)}{2k}}}\\end{array}$ , where $\\widehat{\\mathrm{s}}_{e,k}$ is the empirical mean of the rewards when $e$ has been pulled $k$ times. If $k\\geq k_{e}$ holds, then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[\\mathrm{s}_{e,k}\\leq0.5-\\epsilon]\\leq\\exp(-2k\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}),\\ \\forall e\\in[m_{g}],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\overline{{\\mathrm{s}}}_{e,k}\\geq0.5+\\epsilon]\\leq\\exp(-2k\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}),\\ \\forall e\\in[m]\\backslash\\left[m_{g}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It also holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{k=1}^{\\infty}\\mathbb{1}\\big[\\mathrm{s}_{e,k}\\le0.5-\\epsilon\\big]\\right]\\le k_{e}+\\frac{1}{2\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}},\\ \\forall e\\in[m_{g}],}\\\\ &{\\mathbb{E}\\left[\\displaystyle\\sum_{k=1}^{\\infty}\\mathbb{1}\\big[\\bar{\\mathrm{s}}_{e,k}\\ge0.5+\\epsilon\\big]\\right]\\le k_{e}+\\frac{1}{2\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}},\\ \\forall e\\in[m]\\setminus[m_{g}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 7. Suppose that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{\\log(4m k^{2}/\\delta)}{2k}}\\le\\Delta_{e}-\\operatorname*{max}\\left\\{\\Delta_{\\operatorname*{min}}-\\epsilon,-\\frac{\\epsilon}{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, for each $e\\in[m_{g}]$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}[\\mathrm{s}_{e,k}\\le0.5-\\epsilon]=\\mathrm{Pr}\\left[\\widehat{\\mathrm{s}}_{e,k}-\\mathrm{s}(e)\\le-\\Delta_{e}-\\epsilon+\\sqrt{\\frac{\\log\\left(4m k^{2}/\\delta\\right)}{2k}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\le\\mathrm{Pr}\\left[\\widehat{\\mathrm{s}}_{e,k}-\\mathrm{s}(e)\\le-\\Delta_{e}-\\epsilon+\\Delta_{e}-\\operatorname*{max}\\left\\{\\Delta_{\\operatorname*{min}}-\\epsilon,-\\frac{\\epsilon}{2}\\right\\}\\right]}\\\\ &{\\qquad\\qquad=\\mathrm{Pr}\\left[\\widehat{\\mathrm{s}}_{e,k}-\\mathrm{s}(e)\\le-\\operatorname*{max}\\left\\{\\Delta_{\\operatorname*{min}},\\frac{\\epsilon}{2}\\right\\}\\right]}\\\\ &{\\qquad\\qquad\\le\\exp\\left(-2k\\operatorname*{max}\\left\\{\\Delta_{\\operatorname*{min}},\\frac{\\epsilon}{2}\\right\\}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality follows from the Hoeffding equality (Lemma 1). Now we show, via a similar analysis of Lemma 2 in Kano et al. [51], that for $k\\geq k_{e}$ , it indeed holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{\\log(4m k^{2}/\\delta)}{2k}}\\le\\Delta_{e}-\\operatorname*{max}\\left\\{\\Delta_{\\operatorname*{min}}-\\epsilon,\\,-\\frac\\epsilon2\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\begin{array}{r}{c_{e}:=\\left(\\Delta_{e}+\\operatorname*{min}\\left\\{\\epsilon-\\Delta_{\\operatorname*{min}},\\frac{\\epsilon}{2}\\right\\}\\right)^{2}}\\end{array}$ for simplicity, Then we can rewrite $k\\geq k_{e}$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\nk=\\frac{1}{c_{e}}\\log\\frac{4t\\sqrt{m/\\delta}}{c_{e}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $t\\geq\\log\\frac{5\\sqrt{m/\\delta}}{c_{e}}>1$ . Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\cfrac{\\log(4m k^{2}/\\delta)}{2k}}\\leq\\Delta_{\\varepsilon}+\\operatorname*{min}\\left\\{\\varepsilon-\\Delta_{\\operatorname*{min}},\\,\\frac{\\varepsilon}{2}\\right\\}}\\\\ &{\\Leftrightarrow\\log(4m k^{2}/\\delta)\\leq2\\varepsilon,k}\\\\ &{\\Leftrightarrow\\log\\left(\\frac{4m\\left(\\log\\left(\\frac{4\\sqrt{m/\\delta}}{\\varepsilon}\\right)\\right)^{2}}{\\sqrt{\\varepsilon_{\\phi}^{2}\\delta}}\\right)\\leq\\log\\left(\\frac{16^{2}m}{\\sqrt{\\varepsilon_{\\phi}^{2}\\delta}}\\right)}\\\\ &{\\Leftrightarrow\\log\\left(\\frac{4(\\sqrt{m/\\delta})}{c_{\\phi}}\\right)\\leq2t}\\\\ &{\\Leftrightarrow t-1+\\log\\left(\\frac{4\\sqrt{m/\\delta}}{c_{\\phi}}\\right)\\leq2t}\\\\ &{\\Leftrightarrow\\log\\left(\\frac{4\\sqrt{m/\\delta}}{c_{\\phi}\\cdot c_{\\phi}}\\right)\\leq t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where e is the base of natural logarithms and $\\log t\\leq t-1$ is used. Therefore, $t\\geq\\log{\\frac{5\\sqrt{m/\\delta}}{c_{e}}}$ is sufficient to fulfill (5). ", "page_idx": 19}, {"type": "text", "text": "The second statement of Lemma 7 can easily be shown by adapting the proof of Lemma 3 in Kano et al. [51]. For each $e\\in[m_{g}]$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left[\\sum_{k=1}^{\\infty}[{\\bf{[s_{c},k\\bar{\\;}{\\leq}0.5-\\epsilon]}}\\right]\\leq\\mathbb{E}\\left[\\sum_{k=1}^{k_{c}}+\\sum_{k=1}^{\\infty}\\mathbb{I}\\left[{{\\bf{\\tilde{s}}}_{e,k}\\leq0.5-\\epsilon}\\right]\\right]}}\\\\ &{}&{\\leq k_{e}+\\sum_{k=1}^{\\infty}\\mathrm{Pr}[{\\bf{s}}_{e,k}\\leq0.5-\\epsilon]\\quad}\\\\ &{}&{\\leq k_{e}+\\sum_{k=1}^{\\infty}\\exp(-2k\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2})}\\\\ &{}&{\\leq k_{e}+\\frac{1}{\\mathrm{e}^{\\mathrm{2max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}-1}}\\;-1}\\\\ &{}&{\\leq k_{e}+\\frac{1}{2\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $e\\,\\in\\,[m]\\,\\setminus\\,[m_{g}]$ , we omit the proof, as the analysis is essentially the same as the case for $e\\in[m_{g}]$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma $\\theta.$ . Let $\\boldsymbol{a}(t)\\in\\left(\\begin{array}{l}{V}\\\\ {2}\\end{array}\\right)$ denote the selected pair (i.e., arm) by the algorithm in round $t$ . Then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T=\\displaystyle\\sum_{t=1}^{\\infty}1[a(t)\\in[m],t\\le T]}\\\\ &{\\quad=\\displaystyle\\sum_{t=1}^{\\infty}1[a(t)\\in[m_{y}],t\\le T]+\\displaystyle\\sum_{t=1}^{\\infty}1[a(t)\\in[m]\\backslash[m_{y}],t\\le T]}\\\\ &{\\quad\\le\\displaystyle\\sum_{t=1}^{\\infty}1[a(t)\\in[m_{y}]]+\\displaystyle\\sum_{t=1}^{\\infty}1[a(t)\\in[m]\\backslash[m_{y}]]}\\\\ &{\\quad\\le\\displaystyle\\sum_{t=1}^{\\infty}\\sum_{s\\in[m_{x}]=1}^{1}1[a(t)\\in\\times]+\\displaystyle\\sum_{s\\in[m]}\\sum_{|m_{t}|=1}^{\\infty}1[a(t)=\\epsilon]}\\\\ &{\\quad=\\displaystyle\\sum_{t\\le m_{y}\\mid t=1}^{\\infty}\\sum_{l=1}^{\\infty}1[a(t)=\\epsilon,N_{t}(\\epsilon)=k]+\\displaystyle\\sum_{\\epsilon\\in[m]\\backslash[m_{y}]}\\sum_{l=1}^{\\infty}\\sum_{k=1}^{\\infty}1[a(t)=\\epsilon,N_{t}(\\epsilon)=k]}\\\\ &{\\quad\\le\\displaystyle\\sum_{t\\le m_{y}\\mid t=1}^{\\infty}\\left[\\sum_{i=1}^{\\infty}[a(t)=\\epsilon,N_{t}(\\epsilon)=k]\\right]+\\displaystyle\\sum_{\\epsilon\\in[m]\\backslash[m_{i}]\\backslash[m_{i}]}\\sum_{l=1}^{\\infty}\\left[\\sum_{i=1}^{\\infty}[a(t)=\\epsilon,N_{t}(\\epsilon)=k\\right]\\right]}\\\\ &{\\quad\\le\\displaystyle\\sum_{t\\le m_{y}\\mid t=1}^{\\infty}\\left[\\sum_{i=1}^{\\infty}[a(t)=\\epsilon,N_{t}(\\epsilon)=k]\\right]+\\displaystyle\\sum_{\\epsilon\\in[m]\\backslash[m_{i}]\\backslash[m_{i}]}\\sum_{l=1}^{\\infty}\\left[\\sum_{i=1}^{\\infty}[a(t)=\\epsilon,N_{t}(\\epsilon)=k\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the third inequality follows from the fact that event $\\{a(t)=e,N_{t}(e)=k\\}$ occurs for at most one $t\\in\\mathbb{N}$ . For $e\\in[m_{g}]$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\mathbb{1}\\bigg[\\bigcup_{t=1}^{\\infty}\\{a(t)=e,N_{t}(e)=k\\}\\bigg]\\leq\\mathbb{E}\\left[\\sum_{k=1}^{\\infty}\\mathbb{1}\\big[\\mathrm{s}_{e,k}\\leq0.5-\\epsilon\\big]\\right]\\leq k_{e}+\\frac{1}{2\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality follows from Lemma 7. ", "page_idx": 20}, {"type": "text", "text": "Similarly, for each $e\\in[m]\\setminus[m_{g}]$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\mathbb{1}\\left[\\bigcup_{t=1}^{\\infty}\\{a(t)=e,N_{t}(e)=k\\}\\right]\\leq\\mathbb{E}\\left[\\sum_{k=1}^{\\infty}\\mathbb{1}\\big[\\bar{\\mathrm{s}}_{e,k}\\geq0.5+\\epsilon\\big]\\right]\\leq k_{e}+\\frac{1}{2\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, by combining the above, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nT\\le\\sum_{e\\in[m]}k_{e}+\\frac{m}{2\\operatorname*{max}\\{\\Delta_{\\operatorname*{min}},\\epsilon/2\\}^{2}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 1. Finally, we are ready to complete the proof of Theorem 1. In KC-FC, TB-HS is run with parameter $\\epsilon^{\\prime}=\\frac{\\epsilon}{12m}$ and confidence $\\delta$ . Therefore, by Lemma 5 for $\\epsilon^{\\prime},\\delta$ , we have the approximation guarantee: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{cost}_{\\mathrm{s}}(\\mathcal{C}_{\\mathrm{out}})]\\le5\\cdot\\mathrm{OPT}(\\mathrm{s})+12\\epsilon^{\\prime}|E_{[0.5\\pm\\epsilon^{\\prime}]}|\\le5\\cdot\\mathrm{OPT}(\\mathrm{s})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The sample complexity of KC-FC is equal to that of TB-HS with parameters $\\epsilon^{\\prime},\\delta$ , which is given by Lemma 6 for $\\epsilon^{\\prime},\\delta$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "D Analysis of KC-FB ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we prove Theorem 2 in Section 4. ", "page_idx": 20}, {"type": "text", "text": "D.1 Basic analysis of some random event and its occurrence probability ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The following lemma states that $\\widehat{\\mathrm{s}}_{r}$ for phase $r\\in[n]$ is well-estimated with high probability. The proof is almost straightforward from the Hoeffding inequality (Lemma 1) and union bounds. ", "page_idx": 20}, {"type": "text", "text": "Lemma 8. Let $\\epsilon\\in(0,0.5)$ . Given a phase $r\\in[n]$ , we define the random event ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{E}_{r}:=\\left\\{\\forall e\\in I_{V_{r}}(p_{r}),\\,\\left\\lvert\\mathrm{s}(e)-\\widehat{\\mathbf{s}}_{r}(e)\\right\\rvert<\\operatorname*{max}\\left\\{\\epsilon,\\Delta_{e}\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\bigcap_{r=1}^{n}{\\mathcal{E}}_{r}\\right]\\geq1-2n^{3}\\exp\\left(-{\\frac{2T\\operatorname*{min}_{e\\in E}\\operatorname*{max}\\left\\{\\epsilon,\\Delta_{e}\\right\\}^{2}}{n^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We first evaluate $\\operatorname*{Pr}\\left[\\left|\\widehat{\\mathbf{s}}_{r}(e)-\\mathbf{s}(e)\\right|\\geq\\operatorname*{max}\\left\\{\\epsilon,\\Delta_{e}\\right\\}\\right]$ for a fixed phase $r\\in[n]$ and $e\\in I_{V_{r}}(p_{r})$ . Each $e\\in I_{V_{r}}(p_{r})$ has been pu lled at least $\\lfloor T/m\\rfloor$ times because the initial budget for the pair was set to $\\tau_{1}=\\lfloor T/m\\rfloor$ and the budget has not decreased in the later iterations. Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}\\left[\\left|\\widehat{\\mathbf s}_{r}(e)-\\mathbf s(e)\\right|\\geq\\operatorname*{max}\\left\\{\\epsilon,\\Delta_{e}\\right\\}\\right]=\\mathrm{Pr}\\left[\\left|\\displaystyle\\sum_{k=1}^{\\tau_{r}}X_{k}(e)/\\tau_{r}-\\mathbf s(e)\\right|\\geq\\operatorname*{max}\\left\\{\\epsilon,\\Delta_{e}\\right\\}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathrm{Pr}\\left[\\left|\\displaystyle\\sum_{k=1}^{\\tau_{r}}X_{k}(e)/\\tau_{r}-\\mathbf s(e)\\right|\\geq\\operatorname*{max}\\left\\{\\epsilon,\\Delta_{e}\\right\\}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\exp\\left(-2\\tau_{r}\\operatorname*{max}\\left\\{\\epsilon,\\Delta_{e}\\right\\}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\frac{2T\\operatorname*{max}\\left\\{\\epsilon,\\Delta_{e}\\right\\}^{2}}{m}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second inequality follows from Lemma 1. Taking a union bound for $r\\,\\in\\,[n]$ and all $e\\in I_{V_{r}}(p_{r})$ , we further have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{Pr}\\left[\\circled{\\hat{\\tau}}\\|\\xi_{r}\\right]\\geq1-\\frac{\\displaystyle\\sum_{n=1}^{\\infty}\\sum_{i=1}^{\\infty}\\operatorname*{Pr}_{i}\\big[\\mathrm{Pr}_{i}(\\phi_{i})-\\mathrm{s}(e)\\big]\\geq\\operatorname*{max}\\{\\phi_{i},\\mathrm{\\Delta}_{\\mathrm{a}}\\}\\right]}\\\\ &{\\geq1-\\displaystyle\\sum_{m=1}^{\\infty}\\sum_{i\\in\\mathcal{N}_{r}\\times\\mathcal{C}_{i,i+1}}\\operatorname*{Pr}_{i}\\big[\\mathrm{Pr}_{i}(e)-\\mathrm{s}(e)\\big]\\geq\\operatorname*{max}\\{\\epsilon,\\Delta_{\\mathrm{c}}\\}\\right]}\\\\ &{\\geq1-\\displaystyle\\sum_{n=1}^{\\infty}\\sum_{i\\in\\mathcal{N}_{r}\\times\\mathcal{C}_{i,i}}\\int_{\\mathcal{N}_{r}(\\phi_{i})}2\\mathrm{cos}\\,\\bigg(\\frac{-2\\mathrm{Tmax}\\,\\big[\\epsilon,\\Delta_{\\mathrm{c}}\\big]^{2}}{m}\\bigg)}\\\\ &{=1-\\displaystyle\\sum_{n=1}^{\\infty}2|V_{\\mathrm{t}}||I_{\\mathcal{N}_{r}}(\\mathrm{p}_{i})|\\exp\\left(-\\frac{2T\\operatorname*{max}\\{\\phi_{i}\\leq\\lambda_{\\mathrm{max}}^{2}\\,\\big[\\phi_{i},\\lambda_{\\mathrm{max}}^{2}\\big]\\right)}{m}\\right.}\\\\ &{=1-\\displaystyle\\sum_{n=1}^{\\infty}2|V_{\\mathrm{t}}|(\\mathrm{Pr}_{i}|-1)\\exp\\left(-\\frac{2T\\operatorname*{min}_{i\\in\\mathcal{N}_{r}\\times\\mathcal{C}_{i,i}}\\{\\epsilon,\\Delta_{\\mathrm{c}}\\}^{2}}{m}\\right)}\\\\ &{\\geq1-\\displaystyle\\sum_{n=1}^{\\infty}2^{\\lambda_{1}}\\big\\langle\\mathrm{Pr}_{i}^{2}\\mathrm{exp}\\left(-\\frac{T\\operatorname*{max}\\{\\phi_{i},\\lambda_{\\mathrm{max}}^{2}\\,\\big[\\phi_{i},\\lambda_{\\mathrm{m}}\\big]^{2}\\right)}{m}\\right.}\\\\ &{=1-2\\alpha^{3}\\exp\\left(-\\frac{T\\operatorname*{min}_{i\\in\\mathcal{N}_{r}}\\mathrm{Emax}\\,\\{\\phi_{i},\\lambda_{\\mathrm{m}}^{2}\\}}{m}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the third inequality follows from (7). ", "page_idx": 21}, {"type": "text", "text": "D.2 Theoretical guarantee of the output ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Next we prove a key lemma that provides the theoretical guarantee of the output ${\\mathcal{C}}_{\\mathrm{out}}$ of KC-FB. Lemma 9. Let $\\epsilon\\in(0,0.5)$ . Under the assumption that $\\bigcap_{r=1}^{n}\\mathcal{E}_{r}$ happens, the output $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ of KC-FB is a $(5,6\\epsilon|E_{[0.5\\pm\\epsilon]}|)$ -approximate solution for instance $(V,\\mathrm{s})$ of the offline problem minimizing (1). ", "page_idx": 21}, {"type": "text", "text": "Proof. Let ${\\widehat{E}}\\subseteq E$ be the set of pairs that have been pulled in the algorithm. For $e=\\{u,v\\}\\in\\widehat{E}$ , let $r_{e}$ be the phase, in which either $u$ or $v$ is selected as a pivot. Construct the weight $\\widetilde{\\mathrm{s}}:E\\to[0,1]$ such ", "page_idx": 21}, {"type": "text", "text": "that for each $e\\in E$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{s}}(e)=\\left\\{\\widehat{\\mathbf{s}}_{r_{e}}(e)\\quad\\mathrm{if~}e\\in E_{[0.5\\pm\\epsilon]}\\cap\\widehat{E},\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consider running KwikCluster (Algorithm 4) with the similaritys while respecting the selection of pivots $p_{r}$ of KC-FB, that is, in the $r$ -th iteration, the algorithm se l ects the pivot $p_{r}$ if it exists. In the first iteration, the algorithm can select the pivot $p_{1}$ and construct the cluster $\\{p_{1}\\}\\cup\\Gamma_{V_{1}}(p_{1},\\widetilde{\\mathrm{s}})$ . By the definition ofs and the assumption of the lemma, we have $\\Gamma_{V_{1}}(p_{1},\\widetilde{\\bf s})=\\Gamma_{V_{1}}(p_{1},\\widehat{\\bf s}_{1})$ . In fac t, for any element $u$ in $V_{1}$ (except for $p_{1}$ ), we see that $\\widetilde{\\mathrm{s}}(p_{1},u)>0.5$ if and o nly if $\\widehat{\\mathrm{s}}(p_{1},u)>0.5$ . Therefore, the cluster produced is exactly the same as $C_{1}$ in KC-FB. In the second i t eration, the algorithm can select $p_{2}$ because $p_{2}$ was not contained in the cluster of the first iteration, and by applying the same argument as above, we see that the cluster of this iteration is exactly the same as $C_{2}$ in KC-FB. The later iterations can be handled in the same way. Therefore, we see that the output of the above algorithm coincides with that of KC-FB. ", "page_idx": 22}, {"type": "text", "text": "Then it suffices to show that the output of the above algorithm has the desired approximation guarantee. Let $\\mathcal{C}_{\\mathrm{{out}}}^{\\prime}$ be the output of the above algorithm. Recalling that KC-FB picks pivot $p_{t}$ uniformly at random, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{cost}_{\\mathrm{s}}(\\mathcal{C}_{\\mathrm{out}}^{\\prime})]\\leq\\mathbb{E}[\\mathrm{cost}_{\\tilde{\\mathrm{s}}}(\\mathcal{C}_{\\mathrm{out}}^{\\prime})]+\\epsilon|E_{[0.5\\pm\\epsilon]}|}\\\\ &{\\leq5\\cdot\\mathrm{OPT}(\\tilde{\\mathrm{s}})+\\epsilon|E_{[0.5\\pm\\epsilon]}|}\\\\ &{\\leq5\\left(\\mathrm{OPT}(\\mathrm{s})+\\epsilon|E_{[0.5\\pm\\epsilon]}|\\right)+\\epsilon|E_{[0.5\\pm\\epsilon]}|}\\\\ &{=5\\cdot\\mathrm{OPT}(\\mathrm{s})+6\\epsilon|E_{[0.5\\pm\\epsilon]}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first and third inequalities follow from the fact that $\\mathrm{s}(e)$ and $\\widetilde{\\mathrm{s}}(e)$ may be different only for $e\\in E_{[0.5\\pm\\epsilon]}\\;(\\cap\\;\\widehat{E})$ and the difference there is at most $\\epsilon$ from the assumption of the lemma. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "D.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 2. For \u03f5 > 0, define \u03f5\u2032 \u2208(0, 0.5) as 6 max{1, |\u03f5E[0.5\u00b1\u03f5]|} if $\\epsilon<0.5$ and $\\frac{\\epsilon}{6m}$ otherwise. By Lemma 8 for $\\epsilon^{\\prime}$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\bigcap_{r=1}^{n}\\mathcal{E}_{r}^{\\prime}\\right]\\geq1-2n^{3}\\exp\\left(-\\frac{2T\\operatorname*{min}_{e\\in E}\\operatorname*{max}\\left\\{\\epsilon^{\\prime},\\Delta_{e}\\right\\}^{2}}{n^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathcal{E}_{r}^{\\prime}$ is the random event for phase $r\\in\\{1,\\ldots,n\\}$ that is defined by (6) with $\\epsilon^{\\prime}$ . Therefore, using Lemma 9 for $\\epsilon^{\\prime}$ , we can see that the output $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ of KC-FB is a $(5,\\epsilon)$ -approximate solution for instance $(V,\\mathrm{s})$ of the offline problem minimizing (1) w.p. at least $\\begin{array}{r}{1-2n^{3}\\exp\\left(-\\frac{2T\\operatorname*{min}_{e\\in E}\\operatorname*{max}\\left\\{\\epsilon^{\\prime},\\Delta_{e}\\right\\}^{2}}{n^{2}}\\right).}\\end{array}$ Finally, we can easily confirm that KC-FB does not exceed the given budget $T$ due to the algorithm procedure of line 8, which concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "E Uniform sampling algorithms ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here we provide the complete description and analysis of the naive uniform-sampling algorithms for both the FC setting (Algorithm 6) and the FB setting (Algorithm 7). Note that in the FC setting, no feasible stopping conditions are known from previous studies to guarantee that the output is an approximate solution, even with uniform or arbitrary sampling strategies. Therefore existing analysis of uniform sampling given in Chen et al. [25] is not applicable to our case with offline optimization being NP-hard. ", "page_idx": 22}, {"type": "text", "text": "First, we show a basic analysis of the cost of clustering when the estimate $\\widehat{\\mathrm{s}}$ is close to the unknown similarity s. ", "page_idx": 22}, {"type": "text", "text": "Lemma 10. Let $\\epsilon\\in(0,0.5)$ . Assume that $|\\mathrm{s}(e)-\\widehat{\\mathbf{s}}(e)|\\leq\\epsilon$ for every $e\\in E$ . Let ${\\mathcal{C}}_{\\mathrm{out}}$ be the output of any $\\alpha$ -approximation algorithm for instance $(V,{\\widehat{\\mathbf{s}}})$ of the offline problem minimizing (1). Then $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ is an $(\\alpha,(\\alpha+1)\\epsilon m)$ -approximate solution for instance $(V,\\mathrm{s})$ of the offline problem. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 6 Uniform sampling in the FC setting (Uniform-FC) ", "page_idx": 23}, {"type": "text", "text": "Input : Set $V$ of $n$ objects, confidence level $\\delta$ , additive error $\\epsilon>0$ $\\begin{array}{r}{T(e)\\gets\\lceil\\frac{(\\alpha+1)^{2}m^{2}}{2\\epsilon^{2}}\\log\\frac{2m}{\\delta}\\rceil}\\end{array}$ for each $e\\in E$ ;   \n2 Sample each $e\\in E$ for $T(e)$ times and compute empirical mean $\\widehat{\\mathrm{s}}(e)$ ;   \n3 $\\hat{\\mathcal{C}}\\gets$ solution of an approximation algorithm for instance $(V,{\\widehat{\\mathbf{s}}})$ of the offline problem minimizing (1); Algorithm 7 Uniform sampling in the FB setting (Uniform-FB) Input : Set $V$ of $n$ objects, budget $T$   \n1 Sample each $e\\in E$ for $\\lfloor T/m\\rfloor$ times and compute the empirical mean $\\widehat{\\mathrm{s}}(e)$ ;   \n2 $\\hat{\\mathcal{C}}\\gets$ solution of an approximation algorithm for instance $(V,{\\widehat{\\mathbf{s}}})$ of the offline problem minimizing (1); ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Proof. We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{cost}_{\\mathrm{s}}(\\mathcal{C}_{\\mathrm{out}})]\\leq\\mathbb{E}[\\mathrm{cost}_{\\mathrm{\\widehat{s}}}(\\mathcal{C}_{\\mathrm{out}})]+\\epsilon m}\\\\ &{\\leq\\alpha\\cdot\\mathrm{OPT}(\\widehat{\\mathrm{s}})+\\epsilon m}\\\\ &{\\leq\\alpha\\left(\\mathrm{OPT}(\\mathrm{s})+\\epsilon m\\right)+\\epsilon m}\\\\ &{=\\alpha\\cdot\\mathrm{OPT}(\\mathrm{s})+(\\alpha+1)\\epsilon m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next we evaluate the performance of Algorithm 6. ", "page_idx": 23}, {"type": "text", "text": "Proposition 1. Given a confidence level $\\delta\\in(0,1)$ and an additive error $\\epsilon\\in(0,0.5)$ , the uniform sampling algorithm with an $\\alpha$ -approximation oracle for the $F C$ setting (Algorithm $^{6}$ ) outputs $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ that satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[\\mathrm{cost_{s}}(\\mathcal{C}_{\\mathrm{out}})\\leq\\alpha\\cdot\\mathrm{OPT(s)}+\\epsilon\\right]\\geq1-\\delta,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the upper bound of the number of samples is ", "page_idx": 23}, {"type": "equation", "text": "$$\nT=\\mathcal{O}\\left(\\frac{\\alpha^{2}n^{6}}{\\epsilon^{2}}\\log\\frac{2n^{2}}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. As the algorithm samples each $e\\in E$ for $T(e)$ times, by the Hoeffding inequality (Lemma 1), it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\vert{\\widehat{\\mathbf{s}}}(e)-\\mathbf{s}(e)\\vert\\geq{\\frac{\\epsilon}{(\\alpha+1)m}}\\right]\\leq2\\exp\\left(-{\\frac{2T(e)\\epsilon^{2}}{(\\alpha+1)^{2}m^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\begin{array}{r}{T(e)\\ge\\frac{(\\alpha+1)^{2}m^{2}}{2\\epsilon^{2}}\\log\\frac{2m}{\\delta}}\\end{array}$ gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{2T(e)\\epsilon^{2}}{(\\alpha+1)^{2}m^{2}}\\right)\\leq\\frac{\\delta}{2m}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, by taking a union bound, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[|{\\widehat{\\mathbf{s}}}(e)-\\mathbf{s}(e)|<{\\frac{\\epsilon}{(\\alpha+1)m}},\\,\\forall e\\in E\\right]\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Lemma 10 for $\\begin{array}{r}{\\epsilon:=\\frac{\\epsilon}{(\\alpha+1)m}}\\end{array}$ , we see that $\\ensuremath{\\mathcal{C}_{\\mathrm{out}}}$ is an $(\\alpha,\\epsilon)$ -approximate solution for instance $(V,\\mathrm{s})$ of the offline problem minimizing (1) w.p. at least $1-\\delta$ , as desired. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "The next proposition evaluates the performance of Algorithm 7. ", "page_idx": 23}, {"type": "text", "text": "Proposition 2. Given a sampling budget $T$ and additive error $\\epsilon\\in(0,0.5)$ , the uniform sampling algorithm with an $\\alpha$ -approximation oracle for the $F B$ setting (Algorithm 7) outputs ${\\mathcal{C}}_{\\mathrm{out}}$ that satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[\\mathrm{cost}_{\\mathrm{s}}(\\mathcal{C}_{\\mathrm{out}})>\\alpha\\cdot\\mathrm{OPT}(\\mathrm{s})+\\epsilon\\right]=\\mathcal{O}\\left(n^{2}\\exp\\left(-\\frac{T\\epsilon^{2}}{\\alpha^{2}n^{6}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. As $e\\in E$ has been pulled at least $\\lfloor{\\frac{T}{m}}\\rfloor$ times, by Lemma 1, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[|{\\widehat{\\mathbf{s}}}(e)-{\\mathbf{s}}(e)|\\geq{\\frac{\\epsilon}{(\\alpha+1)m}}\\right]\\leq2\\exp\\left(-{\\frac{2T\\epsilon^{2}}{(\\alpha+1)^{2}m^{3}}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking a union bound for all $e\\in E$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\bigg[\\vert\\widehat{\\mathrm{s}}(e)-\\mathrm{s}(e)\\vert<\\frac{\\epsilon}{(\\alpha+1)m},\\,\\forall e\\in E\\bigg]\\geq1-2\\sum_{e\\in E}\\exp\\left(-\\frac{2T\\epsilon^{2}}{(\\alpha+1)^{2}m^{3}}\\right)}\\\\ {\\geq1-2m\\exp\\left(-\\frac{2T\\epsilon^{2}}{(\\alpha+1)^{2}m^{3}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Lemma 10, when for all $e\\in E$ $\\textstyle\\bar{\\iota},\\,\\vert\\widehat{\\mathrm{s}}(e)-\\mathrm{s}(e)\\vert<\\frac{\\epsilon}{(\\alpha+1)m}$ , we have $\\mathrm{cost}_{\\mathrm{s}}(\\mathcal{C}_{\\mathrm{out}})\\leq\\alpha\\cdot\\mathrm{OPT}(\\mathrm{s})+\\epsilon,$ , which concludes the proof. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction (Section 1) reflect the paper\u2019s contributions and scope including theoretical results and its importance in the literature as well as empirical evidences. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We have discussed in detail the limitations of our work, specifically the absence of lower bounds for our novel formulations, and additionally provided auxiliary analysis by devising naive algorithms. We also included future directions in Section 6. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All theorems (Theorem 1 and Theorem 2), formulas, and proofs are numbered and cross-referenced. Assumptions are clearly stated in Section 2. Complete proofs are included in the appendix (Appendix C for Theorem 1, Appendix D for Theorem 2, and Appendix E for analysis of baselines). Short sketch provided in Section 3 is complemented by Appendix C. All theorems and lemmas are properly referenced in the proofs. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: As detailed in Section 5, we fully disclosed all the information needed to reproduce the main experimental results of the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 26}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not provide open access to the data and code as its primary focus is on theoretical contributions, with experiments included to support the theoretical findings. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Although our proposed method does not involve training or hyperparameters, all relevant parameters for problem setup, such as the values of \u03f5, $\\delta$ , and $T$ , as well as all parameters related to instance generation, are detailed in Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The experimental results reported in Section 5 confidence intervals or standard deviation as well as averages. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: As in Section 5, the paper provides sufficient information on the computer resources needed to reproduce the experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work is regarded as a theoretical study from an algorithmic perspective.   \nThe authors believe this will not lead to any negative social impact. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The original papers that produced the code package or dataset are properly credited in Section 5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]