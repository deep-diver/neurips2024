[{"figure_path": "dQmEIwRw16/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of our EM algorithm to Projected Gradient Descent (PGD). \u03b7 is the step size. For K = 2, 71 ~ 73 are 1, 10 and 20 respectively. For K = 20 and K = 200, \u03b71 ~ \u03b73 are 0.1, 1 and 5 respectively. Higher step size leads to divergence of PGD.", "description": "The table compares the performance of the proposed EM algorithm and the Projected Gradient Descent (PGD) method for optimizing pseudo-labels (y) in the self-labeling clustering loss function.  It shows the running time per iteration and to convergence, as well as the number of iterations required for convergence, for different values of K (number of clusters) and different step sizes (\u03b7) used in PGD. The results demonstrate the superior efficiency of the EM algorithm, especially for larger values of K.", "section": "4 Our Self-labeling Loss and EM"}, {"figure_path": "dQmEIwRw16/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of different methods on clustering with fixed features extracted from Resnet-50. The numbers are the average accuracy and the standard deviation over trials. We use the 20 coarse categories for CIFAR100 similarly to others.", "description": "This table presents the performance comparison of different clustering methods on four benchmark datasets (STL10, CIFAR10, CIFAR100-20, MNIST).  The methods compared include Kmeans, MIGD, SeLa, MIADM, and the proposed method ('Our').  ResNet-50 is used to extract features, and the evaluation metric is average accuracy with standard deviation across multiple trials.  CIFAR100 uses 20 coarse categories for consistency with other related works.", "section": "5.1 Clustering with Fixed Features"}, {"figure_path": "dQmEIwRw16/tables/tables_8_1.jpg", "caption": "Table 3: Quantitative comparison of discriminative clustering-based classification methods with simultaneous feature training from the scratch. The network architecture is VGG-4. We reuse the code published by [17, 1, 15] and use our improved implementation of [16] (also for other tables).", "description": "This table compares the performance of several discriminative clustering-based classification methods on four benchmark datasets (STL10, CIFAR10, CIFAR100-20, and MNIST).  The methods are trained from scratch using a VGG-4 network architecture, and the results shown are the accuracy (ACC) with standard deviations obtained from multiple runs. The table highlights the superior performance of the proposed method compared to other established techniques.", "section": "5.2 Deep Clustering"}, {"figure_path": "dQmEIwRw16/tables/tables_8_2.jpg", "caption": "Table 4: Quantitative comparison using network ResNet-18. The most related work MIADM (5) is also highlighted in all tables.", "description": "The table compares the performance of different deep clustering methods on three datasets (CIFAR10, CIFAR100-20, and STL10) using ResNet-18 as the network architecture.  The methods compared are SCAN, IMSAT, MIADM, and the proposed method.  The results are shown in terms of ACC (Accuracy), NMI (Normalized Mutual Information), and ARI (Adjusted Rand Index). The proposed method achieves the best or close-to-best performance across all three datasets and metrics, highlighting its effectiveness.", "section": "5.2 Deep Clustering"}, {"figure_path": "dQmEIwRw16/tables/tables_8_3.jpg", "caption": "Table 5: Quantitative results for semi-supervised classification on STL10 and CIFAR10 using ResNet18. The numbers 0.1, 0.05 and 0.01 correspond to different ratio of labels used for supervision. \"Only seeds\" means we only use standard cross-entropy loss on seeds for training.", "description": "This table presents the results of semi-supervised classification experiments on STL10 and CIFAR10 datasets using the ResNet18 architecture.  Different ratios of labeled data (0.1, 0.05, 0.01) were used, comparing the performance of the proposed method against other state-of-the-art semi-supervised classification techniques.  The \"Only seeds\" row shows results using only the standard cross-entropy loss on labeled data, while the other rows show results incorporating the proposed self-labeling approach.", "section": "5.3 Semi-supervised Classification"}, {"figure_path": "dQmEIwRw16/tables/tables_14_1.jpg", "caption": "Table 1: Comparison of our EM algorithm to Projected Gradient Descent (PGD). \u03b7 is the step size. For K = 2, 71 ~ 73 are 1, 10 and 20 respectively. For K = 20 and K = 200, \u03b71 ~ \u03b73 are 0.1, 1 and 5 respectively. Higher step size leads to divergence of PGD.", "description": "This table compares the performance of the proposed EM algorithm with the Projected Gradient Descent (PGD) method for optimizing the pseudo-labels y in the self-labeling loss function.  It shows the running time and number of iterations required for convergence with different step sizes (\u03b7) and numbers of clusters (K).  The results demonstrate that the EM algorithm significantly outperforms PGD in terms of efficiency, especially when dealing with a larger number of clusters.", "section": "4 Our Self-labeling Loss and EM"}, {"figure_path": "dQmEIwRw16/tables/tables_15_1.jpg", "caption": "Table 6: Using fixed features extracted from Resnet-50.", "description": "This table presents the clustering accuracy results obtained using features extracted from a pre-trained ResNet-50 model.  The results are shown for four different datasets (STL10, CIFAR10, CIFAR100-20, and MNIST).  Two rows (a and b) are provided, likely representing slightly different experimental runs or variations in the process. The numbers represent the average accuracy and standard deviation over multiple trials.  This demonstrates the performance of the proposed method on a standard benchmark task using existing pre-trained features.", "section": "5.1 Clustering with Fixed Features"}, {"figure_path": "dQmEIwRw16/tables/tables_15_2.jpg", "caption": "Table 7: With simultaneous feature training from the scratch. The network architecture is VGG-4.", "description": "This table presents the results of deep clustering experiments using the VGG-4 network architecture where both features and clustering are learned simultaneously.  The results, showing accuracy with standard deviations, are given for four datasets: STL10, CIFAR10, CIFAR100-20, and MNIST. This setup contrasts with Table 6, which used pre-trained ResNet-50 features.", "section": "5.2 Deep Clustering"}, {"figure_path": "dQmEIwRw16/tables/tables_15_3.jpg", "caption": "Table 8: Network architecture summary. s: stride; p: padding; K: number of clusters. The first column is used on MNIST [25]; the second one is used on CIFAR10/100 [43]; the third one is used on STL10 [8]. Batch normalization is also applied after each Conv layer. ReLu is adopted for non-linear activation function.", "description": "This table details the architecture used for the deep clustering experiments in the paper.  It shows three different architectures, one for each of the datasets MNIST, CIFAR-10/100, and STL-10,  Each architecture is composed of convolutional and max pooling layers followed by a linear layer. The table specifies the number of channels, stride, padding, and kernel size for each layer.  Batch normalization and ReLU activation functions are used.", "section": "E Experiments"}]