[{"figure_path": "AF32GbuupC/tables/tables_3_1.jpg", "caption": "Table 1: Time consumption of 200 episodes training (sec.) of baseline w/ and w/o MP (only consider feed-forward and -backward).", "description": "This table presents the time it took to train different models for 200 episodes using two different methods: one with message passing (GNN) and one without (PeerMLP). The results are broken down by dataset (CoraFull, DBLP, ogbn-A) and model type (GNN, PeerMLP). It shows that PeerMLP is significantly faster than GNN but might not generalize well.", "section": "3.2 FGSAM"}, {"figure_path": "AF32GbuupC/tables/tables_6_1.jpg", "caption": "Table 2: Accuracy and Time consumption on the baseline with different optimizer. The best and the runner-up are denoted as boldface and underlined, respectively. '5N3K' denotes 5-way 3-shot setting. Time consumption of 200 episodes of training (sec., only consider forward-backward) is also shown.", "description": "This table presents the accuracy and training time of several state-of-the-art (SOTA) models for few-shot node classification (FSNC) using different optimizers: Adam, SAM, FGSAM, and FGSAM+.  The results are shown for different dataset settings (5-way 3-shot, 5-way 5-shot, 10-way 3-shot, and 10-way 5-shot) across three datasets: CoraFull, DBLP, and ogbn-arXiv.  The table highlights the best performing model for each setting using bold text and the second-best performing model using underlined text.  It also provides the training time for 200 episodes, focusing solely on the feedforward and backpropagation steps.", "section": "5.2 Evaluation on Real-World Datasets"}, {"figure_path": "AF32GbuupC/tables/tables_6_2.jpg", "caption": "Table 2: Accuracy and Time consumption on the baseline with different optimizer. The best and the runner-up are denoted as boldface and underlined, respectively. '5N3K' denotes 5-way 3-shot setting. Time consumption of 200 episodes of training (sec., only consider forward-backward) is also shown.", "description": "This table presents the accuracy and training time of several state-of-the-art models using different optimizers (Adam, SAM, FGSAM, and FGSAM+) under various few-shot learning settings (5-way 3-shot, 5-way 5-shot, 10-way 3-shot, and 10-way 5-shot).  It compares the performance of these optimizers across three different datasets: CoraFull, DBLP, and ogbn-arXiv.  The time reported is for 200 training epochs and only includes the forward and backward passes. The best and second-best accuracies are highlighted in bold and underlined, respectively.", "section": "5.2 Evaluation on Real-World Datasets"}, {"figure_path": "AF32GbuupC/tables/tables_8_1.jpg", "caption": "Table 2: Accuracy and Time consumption on the baseline with different optimizer. The best and the runner-up are denoted as boldface and underlined, respectively. '5N3K' denotes 5-way 3-shot setting. Time consumption of 200 episodes of training (sec., only consider forward-backward) is also shown.", "description": "This table presents the accuracy and training time of several baseline models (Meta-GCN, AMM-GNN, GPN, TENT) using different optimizers (Adam, SAM, FGSAM, FGSAM+).  The results are shown for various few-shot learning settings (5-way 3-shot, 5-way 5-shot, 10-way 3-shot, 10-way 5-shot) across three datasets (CoraFull, DBLP, ogbn-arXiv). The table highlights the best-performing models and optimizers in terms of accuracy and training time efficiency.", "section": "5.2 Evaluation on Real-World Datasets"}, {"figure_path": "AF32GbuupC/tables/tables_14_1.jpg", "caption": "Table 5: Statistics of evaluation datasets", "description": "This table presents the statistics of three benchmark node classification datasets used in the paper's experiments.  For each dataset (CoraFull, DBLP, and ogbn-arXiv), it shows the number of nodes, edges, features, and classes. It also provides the class split used for training, validation, and testing, showing the percentage of nodes allocated to each set.", "section": "5.1 Experiment Settings"}, {"figure_path": "AF32GbuupC/tables/tables_14_2.jpg", "caption": "Table 2: Accuracy and Time consumption on the baseline with different optimizer. The best and the runner-up are denoted as boldface and underlined, respectively. '5N3K' denotes 5-way 3-shot setting. Time consumption of 200 episodes of training (sec., only consider feed-forward and -backward) is also shown.", "description": "This table presents the accuracy and training time for different optimizers (Adam, SAM, FGSAM, FGSAM+) applied to various GNN models (Meta-GCN, AMM-GNN, GPN, TENT) on three datasets (CoraFull, DBLP, ogbn-arXiv) with 5-way 3-shot and 5-way 5-shot settings. The best and second-best results are highlighted.", "section": "5.2 Evaluation on Real-World Datasets"}, {"figure_path": "AF32GbuupC/tables/tables_15_1.jpg", "caption": "Table 2: Accuracy and Time consumption on the baseline with different optimizer. The best and the runner-up are denoted as boldface and underlined, respectively. '5N3K' denotes 5-way 3-shot setting. Time consumption of 200 episodes of training (sec., only consider feed-forward and -backward) is also shown.", "description": "This table presents the accuracy and training time of several state-of-the-art models (Meta-GCN, AMM-GNN, GPN, TENT) using different optimizers (Adam, SAM, FGSAM, FGSAM+) in the 5-way 3-shot setting.  The results are presented for three different datasets (CoraFull, DBLP, ogbn-arXiv).  The table shows that FGSAM and FGSAM+ generally achieve higher accuracy compared to Adam and SAM, while FGSAM+ achieves faster training speed than Adam in most cases.", "section": "5.2 Evaluation on Real-World Datasets"}, {"figure_path": "AF32GbuupC/tables/tables_16_1.jpg", "caption": "Table 2: Accuracy and Time consumption on the baseline with different optimizer. The best and the runner-up are denoted as boldface and underlined, respectively. \u20185N3K\u2019 denotes 5-way 3-shot setting. Time consumption of 200 episodes of training (sec., only consider feed-forward and -backward) is also shown.", "description": "This table presents the accuracy and training time of various models (Meta-GCN, AMM-GNN, GPN, TENT) using different optimizers (Adam, SAM, FGSAM, FGSAM+) on three datasets (CoraFull, DBLP, ogbn-arXiv) under different few-shot learning settings (5N3K, 5N5K, 10N3K, 10N5K). The results show that FGSAM and FGSAM+ generally outperform Adam and SAM, particularly in terms of training time.", "section": "5.2 Evaluation on Real-World Datasets"}, {"figure_path": "AF32GbuupC/tables/tables_16_2.jpg", "caption": "Table 2: Accuracy and Time consumption on the baseline with different optimizer. The best and the runner-up are denoted as boldface and underlined, respectively. '5N3K' denotes 5-way 3-shot setting. Time consumption of 200 episodes of training (sec., only consider feed-forward and -backward) is also shown.", "description": "This table presents the accuracy and training time of several state-of-the-art models for few-shot node classification on three datasets (CoraFull, DBLP, ogbn-arXiv) under different optimization methods (Adam, SAM, FGSAM, FGSAM+).  The results are shown for various few-shot settings (5-way 3-shot, 5-way 5-shot, 10-way 3-shot, 10-way 5-shot).  The training times are also provided for 200 episodes of training, focusing only on the forward and backward passes, showcasing the computational efficiency of the proposed methods.", "section": "5.2 Evaluation on Real-World Datasets"}, {"figure_path": "AF32GbuupC/tables/tables_16_3.jpg", "caption": "Table 10: Benchmark datasets statistics for node classification", "description": "This table presents the statistics of nine real-world benchmark datasets used for node classification in the paper.  For each dataset, it provides the number of nodes, edges, classes, features, and the homophily score H(G). The homophily score is a measure of how likely nodes of the same class are to be connected. Datasets include Cora, Citeseer, Pubmed, Chameleon, Squirrel, Actor, Cornell, Texas, and Wisconsin.", "section": "E.1 Statistics Of Benchmark Datasets In Node Classification"}, {"figure_path": "AF32GbuupC/tables/tables_17_1.jpg", "caption": "Table 11: Performance of different update interval k.", "description": "This table presents the performance of GPN with FGSAM+ using different update intervals (k).  It shows that as k increases, the accuracy decreases, but the training time also decreases, indicating a trade-off between accuracy and training efficiency.  The results are shown for CoraFull and DBLP datasets, showcasing the impact of the update interval on both accuracy and training time.", "section": "5.5 Ablation Studies"}, {"figure_path": "AF32GbuupC/tables/tables_17_2.jpg", "caption": "Table 12: Performance of prompt-based FSNC on Citeseer.", "description": "This table presents the results of prompt-based few-shot node classification (FSNC) on the Citeseer dataset.  It compares the performance of the baseline method, ProG [32], against the proposed FGSAM+ method.  The comparison is shown for both 3-shot and 5-shot settings, with accuracy (acc) and F1-score (F1) reported for each.", "section": "5.6 Additional Task on Conventional Node Classification"}]