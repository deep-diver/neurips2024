[{"figure_path": "AF32GbuupC/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of average accuracy and training time across datasets on different GNNs. The closer to the top left corner, the better.", "description": "This figure compares the performance of several GNNs (Meta-GCN, AMM-GNN, GPN, TENT) optimized using different methods (Adam, SAM, FGSAM, FGSAM+).  It visualizes the trade-off between accuracy and training time. Each point represents a GNN trained with a specific optimization algorithm on a specific dataset. The x-axis shows the training time relative to the Adam optimizer (100% representing the training time of Adam), and the y-axis represents the average accuracy achieved on various datasets. The top left corner represents the ideal scenario\u2014high accuracy and low training time. The figure suggests that FGSAM+ achieves a good balance between these two aspects, outperforming other methods in many cases.", "section": "Abstract"}, {"figure_path": "AF32GbuupC/figures/figures_2_1.jpg", "caption": "Figure 2: (a): Loss landscape visualization of GNN across tasks and optimizers. (b): Loss of GNN, MLP and its PeerMLP on the test set over the training process. In these experiments, MLP and PeerMLP share the same weight space as GNN but are trained without message-passing.", "description": "This figure visualizes the loss landscape and loss curve for GNNs, MLPs, and PeerMLPs across different tasks (node classification and few-shot node classification) and optimizers. The loss landscapes illustrate the difference in the complexity of the loss function for each setting. The loss curves show the test loss over the training process, highlighting the faster convergence and improved generalization of PeerMLP and FGSAM compared to GNNs with standard optimizers.", "section": "3 Methodology"}, {"figure_path": "AF32GbuupC/figures/figures_4_1.jpg", "caption": "Figure 3: Left (a): The solid line indicates that the gradient is computed on the corresponding model, while the dashed line indicates the opposite. Right (b): The difference of gradients (i.e., ||gt+1\u2212gt||2). It can be seen that gv and gg change much slower than gs and gh across the training process, thus can be reused in the intermediate steps.", "description": "This figure visualizes the framework of FGSAM and FGSAM+ and shows the difference of gradients across the training process for both algorithms.  The left panel (a) shows that the solid line represents gradients calculated on the corresponding model. The dashed line represents the gradient computed on the opposite model, while the right panel (b) shows how the differences in gradients change much slower than gs and gh. Therefore, gv and gg can be reused in intermediate steps to save computational cost.", "section": "3.2 FGSAM"}, {"figure_path": "AF32GbuupC/figures/figures_4_2.jpg", "caption": "Figure 3: Left (a): The solid line indicates that the gradient is computed on the corresponding model, while the dashed line indicates the opposite. Right (b): The difference of gradients (i.e., ||g<sub>t+1</sub>-g<sub>t</sub>||<sub>2</sub>). It can be seen that g<sub>v</sub> and g<sub>g</sub> change much slower than g<sub>s</sub> and g<sub>h</sub> across the training process, thus can be reused in the intermediate steps.", "description": "This figure visualizes the framework of FGSAM and FGSAM+ and shows the difference of gradients across the training process for different components. The left panel (a) shows a schematic of the two algorithms, with solid lines representing gradients computed on the corresponding model and dashed lines representing the opposite. The right panel (b) shows that the differences in gradients (g<sub>v</sub> and g<sub>g</sub>) change much slower compared to others (g<sub>s</sub> and g<sub>h</sub>), indicating that they can be reused in intermediate steps for computational efficiency.", "section": "3.2 FGSAM"}, {"figure_path": "AF32GbuupC/figures/figures_7_1.jpg", "caption": "Figure 4: Performance of GPN trained by Adam and FGSAM+ with different settings. Left: Results with various hidden channels. Middle Left: Results with various model depths. Middle Right: Results with features perturbed by noise of varying standard deviations. Right: Results with edges subjected to various noise ratios.", "description": "This figure presents the results of experiments conducted to evaluate the performance of Graph Prototypical Networks (GPN) trained using two different optimizers: Adam and FGSAM+. The experiments were designed to investigate the impact of various hyperparameters on the model's performance.  Four different aspects were examined:\n\n1. **Number of Hidden Channels:** The leftmost plot shows how accuracy changes with varying numbers of hidden channels in the GPN architecture.\n2. **Number of Layers:** The next plot illustrates the effect of the number of layers in the GPN model on accuracy.\n3. **Noisy Features:** The third plot explores the impact of adding Gaussian noise to the input features (with varying standard deviations) on the model's accuracy.\n4. **Noisy Edges:** The final plot examines the influence of randomly adding edges to the graph (with various ratios of noisy edges to the original number of edges) on the GPN's accuracy.\n\nEach plot compares the accuracy achieved using Adam against the accuracy achieved using FGSAM+, allowing for a direct comparison of their performance under different conditions.", "section": "5 Experiments"}, {"figure_path": "AF32GbuupC/figures/figures_8_1.jpg", "caption": "Figure 5: Training loss curves related to different p across optimizers.", "description": "The figure shows training loss curves for different values of hyperparameter p (radius of the lp ball) across various optimizers: Adam, SAM, and FGSAM+.  It illustrates how the training loss converges differently depending on the optimizer and the value of p.  Specifically, it demonstrates that SAM struggles to converge with higher p values, while FGSAM+ consistently achieves convergence. This highlights FGSAM+'s ability to mitigate the issues associated with imbalanced adversarial games in SAM-like methods.", "section": "Additional Study"}, {"figure_path": "AF32GbuupC/figures/figures_12_1.jpg", "caption": "Figure 3: Left (a): The solid line indicates that the gradient is computed on the corresponding model, while the dashed line indicates the opposite. Right (b): The difference of gradients (i.e., ||gt+1-gt||2). It can be seen that gv and gg change much slower than gs and gh across the training process, thus can be reused in the intermediate steps.", "description": "This figure shows two plots. Plot (a) visualizes the framework of FGSAM and FGSAM+. The solid lines represent the gradient being computed on the corresponding model (GNN or MLP), while the dashed lines indicate the opposite. Plot (b) displays curves showing the difference in gradients (||gt+1-gt||2) during training. The figure demonstrates that the gradient variation for gv and gg is significantly slower than that for gs and gh, supporting the claim that gv and gg can be reused to enhance computational efficiency in intermediate training steps.", "section": "Methodology"}]