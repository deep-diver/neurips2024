[{"type": "text", "text": "Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating Few-Shot Node Classification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yihong Luo1,2\u2217& Yuhan Chen3\u2217, Siya ${\\bf{Q}}{\\bf{i u}}^{1,2}$ , Yiwei Wang4,5, Chen Zhang6, Yan Zhou6, Xiaochun $\\mathbf{Cao^{7\\dagger}}$ , Jing Tang2,1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1 The Hong Kong University of Science and Technology 2 The Hong Kong University of Science and Technology (Guangzhou) 3 School of Computer Science and Engineering, Sun Yat-sen University 4 University of California, Merced 5 University of California, Los Angeles 6 Createlink Technology School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) have shown superior performance in node classification. However, GNNs perform poorly in the Few-Shot Node Classification (FSNC) task that requires robust generalization to make accurate predictions for unseen classes with limited labels. To tackle the challenge, we propose the integration of Sharpness-Aware Minimization (SAM)\u2014a technique designed to enhance model generalization by finding a flat minimum of the loss landscape\u2014into GNN training. The standard SAM approach, however, consists of two forward-backward steps in each training iteration, doubling the computational cost compared to the base optimizer (e.g., Adam). To mitigate this drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware Minimization (FGSAM), that integrates the rapid training of Multi-Layer Perceptrons (MLPs) with the superior performance of GNNs. Specifically, we utilize GNNs for parameter perturbation while employing MLPs to minimize the perturbed loss so that we can find a flat minimum with good generalization more efficiently. Moreover, our method reutilizes the gradient from the perturbation phase to incorporate graph topology into the minimization process at almost zero additional cost. To further enhance training efficiency, we develop $\\mathrm{FGSAM+}$ that executes exact perturbations periodically. Extensive experiments demonstrate that our proposed algorithm outperforms the standard SAM with lower computational costs in FSNC tasks. In particular, our FGSAM $^+$ as a SAM variant offers a faster optimization than the base optimizer in most cases. In addition to FSNC, our proposed methods also demonstrate competitive performance in the standard node classification task for heterophilic graphs, highlighting the broad applicability. The code is available at https://github.com/draym28/FGSAM_NeurIPS24. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) have received significant interest in recent years due to their powerful ability in various graph learning tasks, e.g., node classification. Numerous GNNs have been developed accordingly [14, 19, 34]. Despite their successes, GNNs, like traditional neural networks, tend to be over-parameterized, often requiring extensive labeled data for training to ensure generalization. However, in real-world networks, many node classes have few labeled instances, which can lead to GNNs overfitting, resulting in poor generalization in these limited labeled classes. Recently, an increasing amount of research is focusing on developing superior GNNs, e.g., Meta-GCN [41], AMM-GNN [36], GPN [7] and TENT [37], for Few-Shot Node Classification (FSNC) which aims to classify nodes from new classes with limited labelled instances. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Intuitively, training GNNs for FSNC requires robust model generalization ability for recognizing unseen classes from a small number of labelled examples. Motivated by the success of the recently proposed Sharpness-Aware Minimization (SAM) for improving models\u2019 generalization in the vision domain [11], we suggest incorporating SAM into training GNNs for addressing FSNC tasks. The core idea of SAM is to perturb the model parameters to find flat minima of the loss landscape, thereby making the model more generalizable. However, a key drawback of SAM is that it requires executing two forward-backward steps to complete one optimization step, resulting in twice the time consumption compared to general optimizers like Adam. Some works [8, 9, 22] have been proposed to accelerate SAM, but none of them are crafted for graphs, i.e., not leveraging the graph properties for accelerating SAM. ", "page_idx": 1}, {"type": "text", "text": "This paper mainly focuses on efficient GNN training in FSNC scenarios by leveraging SAM for improving the generalization of GNNs on unseen classes. To tackle the high training cost issue of SAM, we utilize the connection between GNNs and MLPs\u2014GNNs discarding Message-Passing (MP) are equivalent to MLPs with faster training and worse performance in general\u2014to accelerate training. Specifically, we propose Fast Graph Sharpness-Aware Minimization (FGSAM) that uses GNNs for perturbing parameters and employs MLPs (i.e., GNNs discarding MP) to minimize perturbed training loss. This speeds up training at ", "page_idx": 1}, {"type": "image", "img_path": "AF32GbuupC/tmp/4a7cf71d29af90c1512662fc8976b80cffe13904a72067a98def783ecffbd2a6.jpg", "img_caption": ["Figure 1: Comparison of average accuracy and training time across datasets on different GNNs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The closer to the top left corner, the better. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "the cost of dropping graph topology information during minimizing the perturbed loss. Interestingly, we find that the gradient computed in parameter perturbation can be reused when minimizing loss to explicitly reintroduce topology information with negligible extra cost. Moreover, we can add back MP during inference to improve performance. To further reduce the computational cost, we propose FGSAM+ which conducts an exact FGSAM-update at every $k$ steps. As shown in Fig. 1, empirical results in FSNC tasks show that our proposed FGSAM and $\\mathrm{FGSAM+}$ methods outperform both Adam and SAM, and meanwhile $\\mathrm{FGSAM+}$ is even faster than Adam. In addition, we evaluate the proposed methods in node classification, showing strong results, especially in heterophilic graphs which are known to be challenging for GNNs [6, 29]. This indicates that our proposed methods can effectively improve the GNN\u2019s generalization capability for better performance. ", "page_idx": 1}, {"type": "text", "text": "The contributions of this paper can be summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We study the application of SAM in FSNC tasks.   \n\u2022 We propose FGSAM that improves generalization in an efficient way by leveraging GNNs for sharpness-aware perturbation parameters and employing MLPs to expedite training.   \n\u2022 We further propose an enhanced version named $\\mathrm{FGSAM+}$ , which conducts the actual FGSAM at every $k$ steps and approximates it in the intermediate steps.   \n\u2022 We demonstrate strong empirical results of the proposed methods across tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Graph Neural Networks. Let $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ denotes an undirected graph, $\\boldsymbol{\\mathcal{V}}=\\{v_{i}\\}_{i=1}^{n}$ is the node set and $\\mathcal{E}\\subseteq\\mathcal{V}\\times\\mathcal{V}$ is the edge set. $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ is the adjacency matrix. Let $\\mathbf{X}=\\{\\pmb{x}_{i}\\}_{i=1}^{n}\\in\\mathbb{R}^{n\\times d_{0}}$ be the initial node feature matrix, where $d_{0}$ is the initial dimension, and $\\mathbf{Y}\\,=\\,\\{\\pmb{{y}}_{i}\\}_{i=1}^{n}\\,\\in\\,\\mathbb{R}^{n\\times C}$ denotes the ground-truth node label matrix, where $C$ denotes the number of classes and $\\pmb{y}_{i}$ is the one-hot encoding of node $v_{i}$ \u2019s label $y_{i}$ . Let $\\mathbf{H}^{(L)}$ be the output of the last layer of an $L$ -layer GCN, the prediction probability matrix $\\hat{\\mathbf{Y}}=\\mathrm{softmax}\\left(\\mathbf{H}^{(L)}\\right)$ is the final output of node classification. ", "page_idx": 1}, {"type": "text", "text": "Few-Shot Node Classification. In the FSNC task, the entire set of node classes $\\mathcal{C}$ can be divided into two disjoint subsets: base classes set $\\mathcal{C}_{\\mathrm{base}}$ and novel classes set $\\mathcal{C}_{\\mathrm{novel}}$ , such that $\\mathcal{C}=\\mathcal{C}_{\\mathrm{base}}\\cup\\mathcal{C}_{\\mathrm{novel}}$ and $\\mathcal{C}_{\\mathrm{base}}\\cap\\mathcal{C}_{\\mathrm{novel}}=\\emptyset$ . There are sufficient labeled nodes in $\\mathcal{C}_{\\mathrm{base}}$ , while there are only a limited number of labeled nodes in $\\mathcal{C}_{\\mathrm{novel}}$ . FSNC task aims to learn a model using the sufficient labeled nodes from $\\mathcal{C}_{\\mathrm{base}}$ , enabling it to accurately predict unlabeled nodes (i.e., query nodes $\\mathcal{Q}$ ) in $\\mathcal{C}_{\\mathrm{novel}}$ , with limited labeled instances (i.e., support nodes $\\boldsymbol{S}$ ) from $\\mathcal{C}_{\\mathrm{novel}}$ . ", "page_idx": 1}, {"type": "image", "img_path": "AF32GbuupC/tmp/370b3211b4d04930c740d10fe6f402e6337f6f86ef2761b1a0b96947df0674ba.jpg", "img_caption": ["Figure 2: (a): Loss landscape visualization of GNN across tasks and optimizers. (b): Loss of GNN, MLP and its PeerMLP on the test set over the training process. In these experiments, MLP and PeerMLP share the same weight space as GNN but are trained without message-passing. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Sharpness-Aware Minimization (SAM). SAM [11] is an effective method to improve model\u2019s generalization. Let $D_{\\mathrm{tr}}=\\{(x_{i},{\\pmb y}_{i})\\}_{i=1}^{n}$ be the training dataset, following distribution $\\mathcal{D}$ . Given a model parameterized by $\\pmb{w}$ and a commonly used loss function (e.g., cross-entropy loss) $\\ell$ , instead of directly minimizing training loss $\\begin{array}{r}{\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(\\pmb{\\dot{w}})=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(\\pmb{x}_{i},\\pmb{\\dot{y}}_{i};\\pmb{\\dot{w}})}\\end{array}$ , SAM aims to minimize the population loss $\\mathcal{L}_{\\mathcal{D}}(\\pmb{w})=\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}}[\\ell(\\pmb{x},\\pmb{y};\\pmb{w})]$ by minimizing the vanilla training loss as well as the loss sharpness (i.e., find parameters whose neighbors within the $\\ell_{p}$ ball also have low training loss $\\mathcal{L}_{\\mathcal{D}_{\\mathrm{tr}}}$ ) as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle w^{*}=\\arg\\operatorname*{min}\\Big\\{\\operatorname*{max}_{\\|\\epsilon\\|_{p}\\leq\\rho}\\left[\\mathcal{L}_{\\mathcal{D}_{\\mathrm{u}}}({\\bf w}+\\epsilon)-\\mathcal{L}_{\\mathcal{D}_{\\mathrm{u}}}({\\bf w})\\right]+\\mathcal{L}_{\\mathcal{D}_{\\mathrm{u}}}({\\bf w})+\\lambda\\|{\\bf w}\\|_{2}^{2}\\Big\\}}}\\\\ {{\\displaystyle=\\arg\\operatorname*{min}\\Big\\{\\operatorname*{max}_{\\|\\epsilon\\|_{p}\\leq\\rho}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{u}}}({\\bf w}+\\epsilon)+\\lambda\\|{\\pmb w}\\|_{2}^{2}\\Big\\},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\rho$ is the radius of the $\\ell_{p}$ ball, and $p\\geq0$ (usually $p=2$ ). In this way, the model can converge to flat minima in loss landscape $(w^{*})$ , making the model more generalizable [11]. For efficiency, SAM applies first-order Taylor expansion and classical dual norm problem to obtain the approximation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}=\\rho\\frac{\\nabla_{w}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w)}{\\|\\nabla_{w}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w)\\|}\\approx\\arg\\operatorname*{max}_{\\|\\epsilon\\|_{p}\\leq\\rho}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w+\\epsilon).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Finally, SAM computes the gradient w.r.t. perturbed model $\\mathbf{\\nabla}w+\\hat{\\epsilon}$ for update $\\mathbf{\\nabla}w$ in Eq. (1): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla_{w}\\operatorname*{max}_{\\|\\epsilon\\|_{p}\\leq\\rho}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w+\\epsilon)\\approx\\nabla_{w}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w+\\hat{\\epsilon})\\approx\\nabla_{w}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w)|_{w+\\hat{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Additional Related Works. The effectiveness of SAMs and its variants have been widely verified in computer vision area [1, 8, 9, 11, 20, 22, 42]. Specifically, LookSAM [22] speeds up the SAM by periodically conducting exact perturbation, and Sharp-MAML [1] firstly focusing on meta-learning tasks. However, there is limited work on developing SAM for graphs. WT-AWP [38] is the first SAM-like work that applied to GNN and gives a theoretical analysis of generalization bound on graphs. Compared to these works, our proposed FGSAM is crafted for graphs by its unique property, enabling the first SAM-like algorithm that can be faster than the base optimizer. Our work also shares some similarities with existing works [16, 39] that explore the connection between GNNs and MLPs. However, they attributed the claim that introducing MP to MLP can improve performance during evaluation to the powerful generalization ability of MP. In contrast, we prove that for the linear case with synthetic graphs, whether there is MP or not, both will converge to the same optimal solution, taking a solid step toward understanding the underlying reasons. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we propose Fast Graph Sharpness-Aware Minimization (FGSAM), an efficient version of SAM for GNNs, aiming to reduce the training time when using SAM in FSNC tasks while improving model\u2019s generalization. ", "page_idx": 2}, {"type": "text", "text": "3.1 Motivating Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "SAMs are a series of new general training scheme used to improve the model\u2019s generalization, thus it is intuitive to use SAM in FSNC tasks. However, there is no work studying how to apply SAM to FSNC tasks. So our first question is: Q1: Can SAM benefit few-shot node classification tasks? ", "page_idx": 2}, {"type": "table", "img_path": "AF32GbuupC/tmp/03d25e742659ffaf743e277eeea1111ea7f3cf1f637b2fc86e21f494071ff966.jpg", "table_caption": ["Table 1: Time consumption of 200 episodes training (sec.) of baseline w/ and w/o MP (only consider feed-forward and -backward). "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "A key property of FSNC is that the GNNs need to be generalized to unseen classes (i.e., novel classes), and the GNNs often converge to a relatively low loss on the training set, but the final performance depends on the GNNs\u2019 generalization ability. To demonstrate this intuitively, we plot the GNN\u2019s loss landscape of novel classes under the FSNC setting and of the test set under the NC setting (Fig. 2a), following previous work [21]. The loss landscape of GNN under the FSNC setting is sharp and not smooth, with many local minima, in contrast to the flat and smooth loss landscape of GNN under the NC setting. This to some extent indicates that the FSNC setting poses a greater challenge to GNNs, which is consistent with our prior knowledge. Hence, applying SAM-like techniques can intuitively improve the generalization of GNN and enhance its performance. ", "page_idx": 3}, {"type": "text", "text": "However, another problem arises: training GNN on FSNC is already slow, and the core drawback of SAM is that it requires twice the training cost compared to Adam or SGD. Q2: Can we find a way to reduce the SAM training cost based on GNN properties? ", "page_idx": 3}, {"type": "text", "text": "It is well known that the training speed of GNNs is slower than MLPs, mainly due to the notorious MP that causes significant time consumption, yet MP is essential for improving GNN performance. Removing the MP from GNNs $f_{\\mathrm{gnn}}(\\{\\mathbf{X},\\mathbf{A}\\};\\mathbf{w})$ turns them into MLPs $f_{\\mathrm{mlp}}(\\mathbf{X};\\pmb{w})$ , which is an intriguing connection. As shown in Tab. 1 and Fig. 2b, MLPs without the burden of MP demonstrate a substantial training time advantage under the same settings as GNNs and can achieve nearly the same performance as GNNs on the training set, however, they perform significantly worse on the test set, revealing their poor generalization performance. ", "page_idx": 3}, {"type": "text", "text": "Inspired by previous work [16], it is appealing to remove MP during training, but reintroduce it in inference (PeerMLP). Although reintroducing MP after training can improve the performance, it still cannot surpass GNNs\u2019 (Fig. 2b). This may be because of the lack of graph topology information in training. Hence, we propose minimizing training loss on PeerMLPs but minimizing the sharpness according to GNNs, implicitly incorporating the graph topology information in training. This allows the model to quickly converge to the vicinity of local minima and further converge to flat GNN local minima through a GNN\u2019s sharpness-aware approach. By doing so, we not only introduce SAM to enhance the model\u2019s generalization ability and the information w.r.t graph topology but also leverage the intriguing connection between MLPs and GNNs to improve training speed. ", "page_idx": 3}, {"type": "text", "text": "3.2 FGSAM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We elaborate our proposed method Fast Graph Sharpness-Aware Minimization (FGSAM). For the ease of reference, Fig. 3a visualizes the framework of FGSAM, so does to its enhanced version FGSAM $^+$ . There are two forward-backward steps in the FGSAM-update. ", "page_idx": 3}, {"type": "text", "text": "Step 1: Graph sharpness-aware perturbation. The first forward-backward step is served for computing the maximum perturbation $\\hat{\\epsilon}$ (Eq. (2)), where we propose to perturb parameters with MP (GNN), i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}=\\rho\\frac{g^{\\mathrm{gm}}}{\\|g^{\\mathrm{gm}}\\|}=\\rho\\frac{\\nabla_{w}\\mathcal{L}_{\\mathcal{G}}(\\boldsymbol{w};f_{\\mathrm{gm}})}{\\|\\nabla_{w}\\mathcal{L}_{\\mathcal{G}}(\\boldsymbol{w};f_{\\mathrm{gm}})\\|}=\\rho\\frac{\\nabla_{w}\\mathcal{L}(f_{\\mathrm{gm}}(\\mathcal{G};\\boldsymbol{w}),\\mathbf{Y})}{\\|\\nabla_{w}\\mathcal{L}(f_{\\mathrm{gmn}}(\\mathcal{G};\\boldsymbol{w}),\\mathbf{Y})\\|}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Step 2: Minimizing perturbed loss. We propose to minimize the perturbed loss by removing the MP (PeerMLP) to speed up training, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbf{w}^{*}=\\operatorname*{arg\\,min}_{w}\\mathcal{L}_{\\mathbf{X}}(\\mathbf{w}+\\hat{\\epsilon};f_{\\mathrm{mlp}})=\\operatorname*{arg\\,min}_{w}\\mathcal{L}\\big(f_{\\mathrm{mlp}}(\\mathbf{X};w+\\hat{\\epsilon}),\\mathbf{Y}\\big)}\\quad}&{}\\\\ &{=\\underset{w}{\\arg\\operatorname*{min}}\\,\\mathcal{L}\\big(f_{\\mathrm{gm}}(\\hat{\\mathcal{G}}=\\{\\mathbf{X},\\mathbf{I}\\};w+\\hat{\\epsilon}),\\mathbf{Y}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is clear that minimizing the loss on PeerMLPs is equivalent to minimizing the loss on GNNs ignoring the topology information. As demonstrated in Sec. 3.1, intuitively the proposed approach can make model convergence near the local minima easily due to the connection between MLPs and GNNs, and perturbing parameters with MP can find the good flat minima of GNNs (see Fig. 2a). ", "page_idx": 3}, {"type": "text", "text": "Reintroducing Graph Topology in Minimization with Free Lunch. While reintroducing the MP in evaluation can improve performance, its absence during the minimization process may result in ", "page_idx": 3}, {"type": "image", "img_path": "AF32GbuupC/tmp/a9f1143c70afb78e13dd8dd3cad15b9d026ba4ba22a1e8177b9c375303d94b91.jpg", "img_caption": ["(a) Visualization of FGSAM and FGSAM+ "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "AF32GbuupC/tmp/db79d861641e9e99a233980bfceb1f59699f176525ff106246e5edfcc069b595.jpg", "img_caption": ["(b) Gradient difference curves "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Left (a): The solid line indicates that the gradient is computed on the corresponding model, while the dashed line indicates the opposite. Right (b): The difference of gradients (i.e., $\\lVert g_{t+1}-g_{t}\\rVert_{2})$ . It can be seen that $\\scriptstyle g_{v}$ and $\\scriptstyle g_{\\mathcal{G}}$ change much slower than $\\pmb{g}_{s}$ and $g_{h}$ across the training process, thus can be reused in the intermediate steps. ", "page_idx": 4}, {"type": "text", "text": "sub-optimal results. Incorporating MP directly into the minimization is computationally expensive, leading us to employ MLP to minimize the perturbed loss. Fortuitously, the gradient w.r.t. MP is computed during the perturbation step, offering an opportunity for computational savings. We propose to capitalize on the already available gradient information from the first step by reusing it in the optimization procedure, as formalized in the following optimization target: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{w}^{*}=\\arg\\operatorname*{min}_{\\pmb{w}}\\big\\{\\lambda\\times\\mathcal{L}_{\\mathcal{G}}(\\pmb{w};f_{\\mathrm{gnn}})+\\mathcal{L}_{{\\bf X}}(\\pmb{w}+\\hat{\\epsilon};f_{\\mathrm{mlp}})\\big\\},\\quad\\lambda\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This formulation implies that the computational cost of involving MP in the optimization is mitigated since the forward and backward passes are precomputed in the initial step. Thus, we effectively integrate graph topology into the minimization process almost without incurring additional computational expense, akin to receiving a free lunch. See detailed FGSAM in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Adaptation to MAML Models. Model-Agnostic Meta-Learning (MAML) [10] is widely used in FSNC tasks [7, 37], involving two separate update steps in one MAML-update: i) pre-training for learning task-relevant knowledge, and ii) meta-update for task-irrelevant update. This is different from standard gradient descent. Hence for integrating the FGSAM into the MAML models, we propose treating the MAML-update process as a single entity, and applying the FGSAM-update only once simplifies the implementation. This contrasts with the Sharp-MAML [1], where the SAM-update is applied separately in the two stages. ", "page_idx": 4}, {"type": "text", "text": "3.3 FGSAM+ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although the training time of FGSAM can be largely faster than na\u00efve SAM by ignoring the MP in minimizing perturbed loss, it still requires a full forward-backward step of GNN, which makes our approach need an extra computation cost for a forward-backward step of PeerMLP, compared to the base optimizer. ", "page_idx": 4}, {"type": "text", "text": "Fortunately, the forward-backward step of GNN is mainly for perturbing parameters in FGSAM, thus we can further reduce the training time while maintaining performance, by employing FGSAM-update at every $k$ step (i.e., perturb parameters at every $k$ step) and reusing the preserved gradients from parameters perturbation into the intermediate steps [22]. Eq. (3) can be rewritten as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{w}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w)|_{w+\\hat{\\epsilon}}\\approx\\nabla_{w}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w+\\hat{\\epsilon})\\approx\\nabla_{w}\\Big[\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w)+\\rho\\|\\nabla_{w}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(w)\\|\\Big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this way, SAM-gradient $\\pmb{g}_{s}$ is composed by the vanilla gradient $\\nabla_{\\pmb{w}}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{tr}}}(\\pmb{w})$ and the gradient of the $\\ell_{2}$ -norm of vanilla gradient $\\nabla_{\\pmb{w}}||\\nabla_{\\pmb{w}}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{tr}}}(\\pmb{w})||$ . ", "page_idx": 4}, {"type": "text", "text": "This suggests that SAM-gradient $g_{s}\\ =\\ \\nabla_{w}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{tr}}}(w)|_{w+\\hat{\\epsilon}}$ can be divided into two orthogonal parts [22]: $g_{h}$ (in the direction of vanilla gradient $\\pmb{g}\\;=\\;\\nabla_{\\pmb{w}}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{t}}}(\\pmb{w})$ ) is used to minimize the loss value, and flatness-gradient $\\scriptstyle g_{v}$ is used to adjust the updates towards a flat region. $S_{0}\\,_{g h}$ and $\\scriptstyle g_{v}$ can be easily obtained if $\\pmb{g}_{s}$ and $\\textbf{\\textit{g}}$ are given: ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{h}=\\|g_{s}\\|\\cos\\theta{\\frac{g}{\\|g\\|}}=\\|g_{s}\\|{\\frac{g_{s}\\cdot g}{\\|g_{s}\\|\\|g\\|}}{\\frac{g}{\\|g\\|}}{\\frac{g}{\\|g\\|}}{\\frac{g}{\\|g\\|}},\\ \\ g_{v}=g_{s}-g_{h},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta$ is the angle between $\\pmb{g}_{s}$ and $g_{h}$ . As illustrated in [22], $\\scriptstyle g_{v}$ changes much slower than $\\pmb{g}_{s}$ and $g_{h}$ , thus we can compute and preserve $\\scriptstyle g_{v}$ at every $k$ steps, and reuse it to approximate $\\pmb{g}_{s}$ in intermediate steps. ", "page_idx": 4}, {"type": "text", "text": "However, in our case, there exists a clear gap between the model used for perturbing (GNN) and for minimizing (PeerMLP). This is different from the approach in [22], which uses the same model for both. Thus we use an extra PeerMLP forward-backward step to get another $g^{\\mathrm{mlp}}$ for computing $\\scriptstyle g_{v}$ to reduce the gap: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{g}^{\\mathrm{mlp}}=\\nabla_{w}\\mathcal{L}(f_{\\mathrm{mlp}}(\\mathbf{X};w))=\\nabla_{w}\\mathcal{L}(w;f_{\\mathrm{mlp}}),\\ \\ \\pmb{g}_{s}=\\nabla_{w}\\mathcal{L}(w;f_{\\mathrm{mlp}})|_{w+\\hat{\\epsilon}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the $\\hat{\\epsilon}$ is obtained by perturbing parameters with MP Eq. (4), $\\pmb{g}_{s}$ and $g^{\\mathrm{mlp}}$ are obtained without MP, thus there still exists a gap. ", "page_idx": 5}, {"type": "text", "text": "Moreover, since we reintroduce graph topology (Eq. (6)) in minimization, we propose to further use the extra PeerMLP step to reuse graph topology for better performance. Specifically, we conduct the gradient w.r.t. topology information by projection as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{g}_{\\mathcal{G}}=\\pmb{g}^{\\mathrm{gnn}}-\\lVert\\pmb{g}^{\\mathrm{gnn}}\\rVert\\cos(\\theta^{\\prime})\\frac{\\pmb{g}^{\\mathrm{mlp}}}{\\lVert\\pmb{g}^{\\mathrm{mlp}}\\rVert},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\theta^{\\prime}$ is the angle between ${\\pmb g}^{\\mathrm{gnn}}$ and $g^{\\mathrm{mlp}}$ . This can be reused in a similar way as $\\scriptstyle g_{v}$ when approximating FGSAM-update in the intermediate steps. We further conduct experiments to verify whether the $_{g g}$ and $\\scriptstyle g_{v}$ will change slowly so that they can be reused for speed up in our approach. We plot the change of ${\\bf{\\mathit{g}}}_{s},{\\bf{\\mathit{g}}}_{h},{\\bf{\\mathit{g}}}_{v}$ and $\\scriptstyle g_{\\mathcal{G}}$ (Fig. 3b) and the results show that the projected gradient both $\\scriptstyle g_{v}$ and $_{g g}$ on parameters perturbed with MP shows a much more stable pattern and slower changes than $\\pmb{g}_{s}$ and $g_{h}$ , indicating the feasibility of updating $\\scriptstyle g_{v}$ and $\\scriptstyle g_{\\mathcal{G}}$ every $k$ steps and reusing it for the intermediate steps. We present the detailed $\\mathbf{FGSAM+}$ in Algorithm 1 in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Since we need an extra PeerMLP forward-backward step at every $k$ step, the overall computation cost of our approach, $\\mathrm{FGSAM+}$ , will be $\\scriptstyle{\\frac{1}{k}}\\times$ the computation cost of GNNs plus $\\bigl(1+\\frac{1}{k}\\bigr)\\times$ the computation cost of MLPs on average. ", "page_idx": 5}, {"type": "text", "text": "4 Analysis of Toy Case ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we employ the Contextual Stochastic Block Model (CSBM) to analyze why minimizing perturbed training loss without MP can work to some extent, which is the underlying mechanism of FGSAM. The CSBM has been widely used to analyze of the properties of GNN [25, 26]. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we focus on a CSBM model that contains $K$ distinct classes $c_{1},c_{2},\\ldots,c_{K}$ . The nodes within the resulting graphs are grouped into $n$ non-overlapping sets $C_{1},C_{2},\\dots,C_{K}$ , each set representing one of the $K$ classes. The generation of edges is governed by a probability $p$ within the same class and a probability $q$ between different classes. For any given node $i$ , we sample its initial features $\\mathbf{\\boldsymbol{x}}_{i}\\in\\dot{\\mathbb{R}}^{l}$ from a Gaussian distribution denoted by $\\pmb{x}_{i}\\overset{\\cdot}{\\sim}\\mathcal{N}(\\pmb{\\mu},\\mathbf{I})$ , where the mean $\\pmb{\\mu}=\\pmb{\\mu}_{k}\\in\\mathbb{R}^{l}$ corresponds to node $i$ belonging to set $C_{K}$ , and $k$ is an element of $\\{1,2,\\ldots,K\\}$ . Furthermore, the condition $||\\pmb{\\mu}_{i}-\\pmb{\\mu}_{j}||_{2}=D$ holds true for all $i,j$ belonging to $\\{1,2,\\ldots,K\\}$ , with $D$ being a positive constant. Graphs that arise from this specified CSBM model are referred to as $K$ -classes CSBM. After applying a MP operation, the resultant features for node $i$ are denoted by $\\boldsymbol{h}_{i}$ . ", "page_idx": 5}, {"type": "text", "text": "The neighborhood label distribution $\\mathcal{D}_{i}$ of node $i$ is a K-dimensions vector, where $D_{i}[j]=\\mathbb{I}(i\\in$ $C_{j})p+\\bar{(}1-\\mathbb{I}(i\\in C_{j}))q$ . Based on the neighborhood label distribution, consider the MP operation as $\\begin{array}{r}{\\pmb{h}_{i}\\,=\\,\\frac{1}{d e g(i)}\\sum_{j\\in\\mathcal{N}(i)}\\pmb{x}_{i}}\\end{array}$ , we have: $\\begin{array}{r}{h_{i}\\sim\\mathcal{N}\\left(\\frac{(p-q)\\mu_{k}+q K\\bar{\\mu}}{p+(K-1)q},\\frac{{\\bf I}}{d e g(i)}\\right)}\\end{array}$ (pp\u2212+q)(\u00b5Kk\u2212+1q)qK \u00b5\u00af,degI(i) , where i \u2208Ck and \u00b5\u00af = j=K1 \u00b5j. Based on the distribution of hi and xi, we can obtain following theorem: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 (The effectiveness of removing MP in minimization). Consider a $K$ -classes CSBM, the optimal linear classifiers for both original features $\\pmb{x}_{i}$ and filtered features $h_{i}$ are the same. ", "page_idx": 5}, {"type": "text", "text": "Detailed proof is in Appendix C. The theorem tells us that under the linear case, whether the MP layer is used or not, the optimal decision bound is the same. Hence, this encourages us to learn the weight of transformation layers without MP to speed up training. However, the real graph is more complex and we do not use a linear classifier, thus we propose to perform the graph sharpness-aware perturbation which implicitly involves the information of neighbors. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We verify the effectiveness of our proposed FGSAM and FGSAM $^+$ in this section. We first conduct experiments to demonstrate that our proposed algorithms achieve better performance compared to SAM which requires twice the training time. Then we show that our proposed algorithms can achieve faster training speed compared to base optimizers (e.g., Adam). Next, we also conduct extra studies and an extra task to show the robustness and potential applications of our proposed algorithms. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Accuracy and Time consumption on the baseline with different optimizer. The best and the runner-up are denoted as boldface and underlined, respectively. \u20185N3K\u2019 denotes 5-way 3-shot setting. Time consumption of 200 episodes of training (sec., only consider forward-backward) is also shown. ", "page_idx": 6}, {"type": "table", "img_path": "AF32GbuupC/tmp/e248700408021e3275a8e3d7648851b90ae20d797b7405a231ddb8d6b5088477.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "AF32GbuupC/tmp/50081c1af4f13eb5dd378278b03b843a6da46a34022961201813a99c195ad7da.jpg", "table_caption": ["Table 3: Comparison between SAM variants regarding accuracy and time consumption (10N3K) "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Experiment Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Baseline. We evaluate our proposed FGSAM and $\\mathrm{FGSAM+}$ on SOTA models. The existing models can be divided into two main categories: MAML and non-MAML methods. Two representative models are selected from each category, respectively, as baselines for evaluation (Meta-GCN [41] and AMM-GNN [36] for MAML models, and GPN [7] and TENT [37] for non-MAML models). ", "page_idx": 6}, {"type": "text", "text": "Datasets. We conduct evaluations on three widely used real-world benchmark node classification datasets: CoraFull [5], DBLP and ogbn-arXiv [17], and we use the train/val/test split as in [33] and [23]. The comprehensive statistics of datasets are shown in Tab. 5 in Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We implement our model by PyTorch [28] and conduct experiments on an RTX-3090Ti. We use Optuna [2] to search the hyper-parameters for each setting. See Appendix D.2 for detailed FSNC learning protocol. ", "page_idx": 6}, {"type": "text", "text": "5.2 Evaluation on Real-World Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The results of different models across datasets are summarized in Tab. 2. All the models share a 2- layers architecture with 16 hidden channels. It can be seen that our proposed algorithms FGSAM and FGSAM $^+$ provide better performance than Adam in most cases, and provide comparable performance with SAM. These results support our claim that FGSAM and $\\mathrm{FGSAM+}$ can find local minima with better generalization properties. Note that message-passing is only used in perturbing parameters, not involved in parameters update (i.e., MLPs). The results further indicate that implicitly involving graph topology in training can make PeerMLPs outperform GNNs. See Appendix D.3 for details. ", "page_idx": 6}, {"type": "text", "text": "5.3 Time Consumption ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To demonstrate the training speed advantage of our proposed algorithm, we summarize the training time for different models using various optimization methods across three datasets (Tab. 2). The results indicate that our proposed algorithm FGSAM demonstrates only a slight increase in training cost compared to Adam in most cases. Furthermore, our enhanced version FGSAM+ outperforms Adam in terms of speed in the majority of scenarios. It is worth mentioning that our proposed algorithms achieve superior or comparable performance when compared to both Adam and SAM. See Appendix D.4 for detailed results. ", "page_idx": 6}, {"type": "text", "text": "Limitation. For models composed of many non-GNN components (e.g., TENT), the training time on FGSAM $^+$ may be still longer than that on Adam, since it is hardly further reduced. ", "page_idx": 6}, {"type": "image", "img_path": "AF32GbuupC/tmp/ea125116a869595318f44f1bc6e07602337f92d3a6a424b2443dc8668db0a37d.jpg", "img_caption": [], "img_footnote": ["Figure 4: Performance of GPN trained by Adam and $\\mathrm{FGSAM+}$ with different settings. Left: Results with various hidden channels. Middle Left: Results with various model depths. Middle Right: Results with features perturbed by noise of varying standard deviations. Right: Results with edges subjected to various noise ratios. "], "page_idx": 7}, {"type": "text", "text": "5.4 Comparison of the Variants of SAM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Training with Different Optimizer. We compare the performance of Meta-GNN and GPN training with different variants of SAM, including original SAM, ESAM [8], LookSAM [22], AE-SAM [18], our proposed FGSAM and $\\mathrm{FGSAM+}$ (Tab. 3). We observe an anomalous phenomenon where ESAM, as an efficient variant of SAM, actually trains slower than SAM. This is because ESAM sorts the sample losses and selects a suitable subset at each iteration, an operation that is negligible for image tasks; however, for graph tasks, since GNNs are relatively smaller, the proportion of time consumed by the sorting step is significant, leading to an increase in training time. As shown in Tab. 3, our proposed method greatly reduces the training time, based on the relationship between GNN and MLP, while maintaining and even achieving superior performance, compared to other optimizers, indicating ours\u2019 high efficiency and effectiveness. ", "page_idx": 7}, {"type": "text", "text": "The Impact of Perturbing Parameters with Message-Passing. A key point of our work is that we perform parameter perturbation using GNNs, while PeerMLPs (i.e., without message-passing) are used to minimize the perturbed loss. This is significantly distinct from previous SAM methods which shared the same model for both parameter perturbation and loss minimization. So a natural question arises: to what extent does our approach benefit from performing parameter perturbation using GNNs? We thus compare our approach to PeerMLPs training with Adam and vanilla SAM. Note that message-passing would be reintroduced during validation and test. From Tab. 3, although the training time of PeerMLPs is shorter than that of GNNs, GNNs outperform their PeerMLPs in most cases. Despite that using PeerMLPs can accelerate the training of GNNs, the topology information is still very important for learning node representations. Thus our proposed FGSAM $^+$ is a better solution, achieving a better trade-off between efficiency and performance. ", "page_idx": 7}, {"type": "text", "text": "5.5 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We further verify the consistent effectiveness of our method compared to Adam across different settings regarding model implementation and graph property. Due to the computational resource restriction, all experiments here were conducted using GPN on the CoraFull with the 5-way 3-shot setting. We provide additional experiments (e.g., the effect of update interval $k$ ) in the Appendix E. ", "page_idx": 7}, {"type": "text", "text": "The Impact of Network Structure. Here we investigate the effect of hidden dimension and the number of layers on the performance (on the left of Fig. 4). GPN with Adam requires a higher hidden dimension (128) to achieve relatively high accuracy, whereas GPN with FGSAM+ can attain SOTA even with a small hidden dimension (16). With respect to the number of layers, GPN with $\\mathrm{FGSAM+}$ consistently performs better within the range of $1{\\sim}8$ compared to GPN with Adam, demonstrating the effectiveness of our proposed method (middle left of Fig. 4). ", "page_idx": 7}, {"type": "text", "text": "The Impact of Noisy Features and Edges. Here we investigate the effect of randomly adding Gaussian noise to features and randomly adding edges during testing (on the middle right and the right of Fig. 4). Specifically, for noisy features, we randomly add Gaussian noise with varying standard deviations to the node features. Meanwhile, for noisy edges, we uniformly and randomly introduce additional edges into the original structure. The results show that GPN with FGSAM+ method can still achieve relatively high performance, compared to GPN with Adam. These results effectively verify the robustness of our proposed method. ", "page_idx": 7}, {"type": "text", "text": "5.6 Additional Task on Conventional Node Classfication ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our proposed FGSAM $^+$ also has the potential to be extended to other domains. To demonstrate this, we evaluate the performance of the $\\mathrm{FGSAM+}$ on the standard node classification task on both homophilic and heterophilic graphs. For homophilic graphs, we utilize three well-established citation networks: Cora, Citeseer, and Pubmed [12, 31]. For heterophilic graphs, we include page-page networks from Wikipedia, specifically the Chameleon and Squirrel datasets [30], actor-network, namely Actor [29], and web pages networks, namely Cornell, Texas and Wisconsin [29]. See Appendix E.1 for statistics of these datasets. We use data splits $(48\\%/32\\%/20\\%)$ provided by [29], and set $k=2$ for $\\mathrm{FGSAM+}$ . We select three representative baselines, namely the classical GCN [19], GAT [34] with learnable MP operation, and GraphSAGE [15] with complex MP operation, to demonstrate the effectiveness of FGSAM and FGSAM+. ", "page_idx": 7}, {"type": "table", "img_path": "AF32GbuupC/tmp/61ee13938eba4159553523c5112f5cbd5397c433d841cd30fbf3959848e51a3e.jpg", "table_caption": ["Table 4: Results on nine real-world node classification benchmark datasets: Mean accuracy $(\\%)$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "As shown in Tab. 4, both FGSAM and $\\mathrm{FGSAM+}$ generally outperform Adam and SAM across base models, indicating the potential wide application of our method. We observed that the proposed method achieves greater improvement on heterophilic graphs compared to homophilic graphs, and heterophilic graphs are generally considered more challenging. This indicates that our method can effectively enhance the generalization capability of GNNs. We also provide additional experiments of integrating $\\mathrm{FGSAM+}$ with prompt-based FSNC [32] in the Appendix E.3. ", "page_idx": 8}, {"type": "text", "text": "5.7 Additional Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We observe that both FGSAM and $\\mathrm{FGSAM+}$ generally outperform the standard SAM across tasks (FSNC and standard node classification). This is an interesting finding, as our FGSAM and $\\mathrm{FGSAM+}$ algorithm remove message-passing during the minimization of the perturbed loss, which is expected to hurt performance. We attribute these counterintuitive results to the mitigation of the imbalance adversarial game. The training process of SAM-like algorithms entails an adversarial game similar to that in Generative Adversarial Nets (GANs) [13]. Prior studies [3, 4, 27] have demonstrated that imbalanced adversarial games in GANs can give rise to worse results. Both FGSAM and $\\mathrm{FGSAM+}$ employ distinct models for perturbation and minimization, which can help alleviate the extent of imbalance. These factors may explain the observed performance discrepancies among the compared algorithms. To verify the explanation, we conduct experiments varying the hyper-parameter $\\rho$ . Specifically, we graphically illustrate the comparative training loss of SAM and $\\mathrm{FGSAM+}$ over a range of $\\rho$ values in Fig. 5, which reveals that while SAM struggles to converge with higher $\\rho$ values, FGSAM $^+$ consistently achieves convergence. Moreover, it is established that a higher $\\rho$ value is conducive to a tighter generalization bound, suggesting that a larger $\\rho$ could potentially enhance performance. Consequently, $\\mathrm{FGSAM+}$ is capable of mitigating the imbalanced games issue and tolerating a larger $\\rho$ , which contributes to its enhanced performance. ", "page_idx": 8}, {"type": "image", "img_path": "AF32GbuupC/tmp/fdc54b2822236b61e444a857033dfa9c9148bc9ecc53535381672fa6b1e1063f.jpg", "img_caption": ["Figure 5: Training loss curves related to different $\\rho$ across optimizers. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we study the application of Sharpness-Aware Minimization (SAM) in FSNC to improve model\u2019s generalization, since the key for FSNC is to generalize the model to unseen samples. In order to alleviate the heavy computation cost of SAM, we utilize the connection between MLPs and GNNs and use MLPs to accelerate the training of GNNs. However, the low generalization and lack of using graph topology of MLPs also limit its performance. Hence we propose to apply GNNs to perturb parameters for generalization and use MLPs to minimize the perturbed training loss for conducting the proposed FGSAM. Moreover, we reuse the GNN gradient in perturbation in minimization for better including topology information. We further reduce the training time by conducting exact FGSAM update at every $k$ steps and approximate FGSAM\u2019s gradient with reusing information in the intermediate steps. Finally, the extensive experiments demonstrate the effectiveness and efficiency of our proposed methods. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jing Tang\u2019s work is partially supported by National Key R&D Program of China under Grant No. 2023YFF0725100, by the National Natural Science Foundation of China (NSFC) under Grant No. 62402410 and U22B2060, by National Language Commission under Grant No. WT145-39, by Guangdong Basic and Applied Basic Research Foundation under Grant No. 2023A1515110131, by Guangzhou Municipal Science and Technology Bureau under Grant No. 2023A03J0667 and 2024A04J4454, and by Createlink Technology Co., Ltd. Xiaochun Cao\u2019s work is supported in part by National Natural Science Foundation of China (No. 62411540034), in part by Shenzhen Science and Technology Program (Grant No. KQTD20221101093559018). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Momin Abbas, Quan Xiao, Lisha Chen, Pin-Yu Chen, and Tianyi Chen. Sharp-maml: Sharpnessaware model-agnostic meta learning. In International conference on machine learning, pages 10\u201332. PMLR, 2022.   \n[2] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2623\u20132631, 2019.   \n[3] Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial networks. In International Conference on Learning Representations, 2017.   \n[4] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214\u2013223. PMLR, 2017.   \n[5] Aleksandar Bojchevski and Stephan G\u00fcnnemann. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. In ICLR, 2018.   \n[6] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. In International Conference on Learning Representations. https://openreview. net/forum, 2021.   \n[7] Kaize Ding, Jianling Wang, Jundong Li, Kai Shu, Chenghao Liu, and Huan Liu. Graph prototypical networks for few-shot learning on attributed networks. In CIKM, 2020.   \n[8] Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent YF Tan. Efficient sharpness-aware minimization for improved training of neural networks. arXiv preprint arXiv:2110.03141, 2021.   \n[9] Jiawei Du, Daquan Zhou, Jiashi Feng, Vincent Tan, and Joey Tianyi Zhou. Sharpness-aware training for free. Advances in Neural Information Processing Systems, 35:23439\u201323451, 2022.   \n[10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017.   \n[11] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.   \n[12] Lise Getoor. Query-driven active surveying for collective classification. 2012.   \n[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[14] Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V Chawla. Few-shot graph learning for molecular property prediction. In Proceedings of the web conference 2021, pages 2559\u20132567, 2021.   \n[15] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NIPS, 2017.   \n[16] Xiaotian Han, Tong Zhao, Yozen Liu, Xia Hu, and Neil Shah. Mlpinit: Embarrassingly simple gnn training acceleration with mlp initialization. arXiv preprint arXiv:2210.00102, 2022.   \n[17] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In NeurIPS, 2020.   \n[18] Weisen Jiang, Hansi Yang, Yu Zhang, and James Kwok. An adaptive policy to employ sharpnessaware minimization. In The Eleventh International Conference on Learning Representations, 2022.   \n[19] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[20] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning, pages 5905\u20135914. PMLR, 2021.   \n[21] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018.   \n[22] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable sharpness-aware minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12360\u201312370, 2022.   \n[23] Yonghao Liu, Mengyu Li, Ximing Li, Fausto Giunchiglia, Xiaoyue Feng, and Renchu Guan. Few-shot node classification on attributed networks with graph meta-learning. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval, pages 471\u2013481, 2022.   \n[24] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In Ying Ding, Jie Tang, Juan F. Sequeda, Lora Aroyo, Carlos Castillo, and Geert-Jan Houben, editors, Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023, pages 417\u2013428. ACM, 2023.   \n[25] Sitao Luan, Chenqing Hua, Minkai Xu, Qincheng Lu, Jiaqi Zhu, Xiao-Wen Chang, Jie Fu, Jure Leskovec, and Doina Precup. When do graph neural networks help with node classification: Investigating the homophily principle on node distinguishability. arXiv preprint arXiv:2304.14274, 2023.   \n[26] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural networks? arXiv preprint arXiv:2106.06134, 2021.   \n[27] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2794\u20132802, 2017.   \n[28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[29] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. arXiv preprint arXiv:2002.05287, 2020.   \n[30] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-Scale Attributed Node Embedding. Journal of Complex Networks, 9(2), 2021.   \n[31] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. Ai Magazine, 2008.   \n[32] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 2120\u20132131. ACM, 2023.   \n[33] Zhen Tan, Song Wang, Kaize Ding, Jundong Li, and Huan Liu. Transductive linear probing: A novel framework for few-shot node classification. In Learning on Graphs Conference, pages 4\u20131. PMLR, 2022.   \n[34] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[35] Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 2020.   \n[36] Ning Wang, Minnan Luo, Kaize Ding, Lingling Zhang, Jundong Li, and Qinghua Zheng. Graph few-shot learning with attribute matching. In Proceedings of the 29th ACM International Conference on Information and Knowledge Management, 2020.   \n[37] Song Wang, Kaize Ding, Chuxu Zhang, Chen Chen, and Jundong Li. Task-adaptive few-shot node classification. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1910\u20131919, 2022.   \n[38] Yihan Wu, Aleksandar Bojchevski, and Heng Huang. Adversarial weight perturbation improves generalization in graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10417\u201310425, 2023.   \n[39] Chenxiao Yang, Qitian Wu, Jiahua Wang, and Junchi Yan. Graph neural networks are inherently good generalizers: Insights by bridging GNNs and MLPs. In The Eleventh International Conference on Learning Representations, 2023.   \n[40] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016.   \n[41] Fan Zhou, Chengtai Cao, Kunpeng Zhang, Goce Trajcevski, Ting Zhong, and Ji Geng. Meta-gnn: On few-shot node classification in graph meta-learning. In CIKM, 2019.   \n[42] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training. arXiv preprint arXiv:2203.08065, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Potential Broader Impact ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 12}, {"type": "text", "text": "B Algorithm ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Algorithm 1 Training with FGSAM and $\\mathrm{FGSAM+}$ ", "text_level": 1, "page_idx": 12}, {"type": "image", "img_path": "AF32GbuupC/tmp/a931c35e3504f87ecd3094e8b97d6dc53dd0b74c1959e41621870c9f72dbb5b7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "C Proof ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The linear classifier for K-classification problems can be formulated as $\\textstyle{\\frac{K(K-1)}{2}}$ binary classification problems. ", "page_idx": 12}, {"type": "text", "text": "Hence we study the classification between class $C_{o}$ and $C_{p}$ without loss of generality. ", "page_idx": 12}, {"type": "text", "text": "The distribution of original features from different classes follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{i}\\sim\\mathcal{N}\\left(\\pmb{\\mu}_{o},\\pmb{I}\\right),i\\in C_{o}}\\\\ {\\pmb{x}_{i}\\sim\\mathcal{N}\\left(\\pmb{\\mu}_{p},\\pmb{I}\\right),i\\in C_{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The distribution of filtered features from different classes follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{i}\\sim\\mathcal{N}\\left(\\frac{\\left(p-q\\right)\\mu_{o}+q K\\bar{\\mu}}{p+\\left(K-1\\right)q},\\frac{\\mathbf{I}}{d e g\\left(i\\right)}\\right),\\quad i\\in C_{o},}\\\\ &{h_{i}\\sim\\mathcal{N}\\left(\\frac{\\left(p-q\\right)\\mu_{p}+q K\\bar{\\mu}}{p+\\left(K-1\\right)q},\\frac{\\mathbf{I}}{d e g\\left(i\\right)}\\right),\\quad i\\in C_{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For simplicity, we denote \u00b5\u02dc0 = (pp\u2212+q)(\u00b5Ko\u2212+1q)qK \u00b5\u00afand \u00b5\u02dcp = (pp\u2212+q)(\u00b5Kp\u2212+1q)qK \u00b5\u00af. ", "page_idx": 13}, {"type": "text", "text": "Following [26], the optimal classifier of original features constructs a decision bound $\\mathcal{P}=\\{\\pmb{x}|\\pmb{w}^{T}\\pmb{x}-$ $\\boldsymbol{w^{T}b}\\}$ , where $\\begin{array}{r}{w\\,=\\,\\frac{\\mu_{o}-\\mu_{p}}{2}/||\\frac{\\mu_{o}-\\mu_{p}}{2}||,\\,b\\,=\\,\\frac{\\mu_{o}+\\mu_{p}}{2}}\\end{array}$ . Similarly, the optimal classifier of filtered features constructs a decision bound $\\mathcal{P}^{\\prime}\\,=\\,\\{h|w^{\\prime T}h-w^{\\prime T}b\\}$ , where $\\begin{array}{r}{{\\pmb w}^{\\prime}\\,=\\,\\frac{\\tilde{\\mu}_{o}-\\tilde{\\mu}_{p}}{2}/\\vert\\vert\\frac{\\tilde{\\mu}_{o}-\\tilde{\\mu}_{p}}{2}\\vert\\vert}\\end{array}$ b\u2032 = \u00b5\u02dco+\u00b5\u02dcp. ", "page_idx": 13}, {"type": "text", "text": "And we have $\\begin{array}{r}{\\tilde{\\pmb{\\mu}}_{o}-\\tilde{\\pmb{\\mu}}_{p}=\\frac{p-q}{p+(K-1)q}\\left(\\pmb{\\mu}_{o}-\\pmb{\\mu}_{p}\\right)}\\end{array}$ , hence we have $\\boldsymbol{w}=\\boldsymbol{w}^{\\prime}$ . Then we verify whether $\\pmb{w}^{T}\\pmb{b}=\\pmb{w}^{\\prime T}\\pmb{b}^{\\prime}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w^{T}b^{\\prime}=w^{T}\\left(\\frac{\\hat{\\mu}_{n}+\\hat{\\mu}_{n}}{2}\\right)}\\\\ &{\\quad=w^{T}\\frac{1}{2}\\left(\\frac{(p-q)\\,|\\boldsymbol{\\mu}_{n}+q|V\\hat{\\boldsymbol{\\mu}}_{n}}{p+(K-1)\\,q}+\\frac{(p-q)\\,|\\boldsymbol{\\mu}_{n}+q|K\\hat{\\boldsymbol{\\mu}}_{n}}{p+(K-1)\\,q}\\right)}\\\\ &{\\quad=w^{T}\\left(\\frac{1}{2}\\left(\\mu_{n}+\\mu_{n}\\right)+(1-\\lambda)\\,\\hat{\\boldsymbol{\\mu}}\\right)}\\\\ &{\\quad=w^{T}\\bigg(\\frac{\\lambda}{2}\\left(\\mu_{n}+\\mu_{n}\\right)}\\\\ &{\\qquad\\qquad+(1-\\lambda)\\,\\frac{1}{2}\\left(\\mu-\\mu_{n}+\\mu-\\mu_{n}+\\mu_{n}\\right)\\bigg)}\\\\ &{\\quad=w^{T}\\left(\\frac{1}{2}\\left(\\mu_{n}+\\mu_{n}\\right)+(1-\\lambda)\\left(\\mu-\\frac{\\mu_{n}+\\mu_{n}}{2}\\right)\\right)}\\\\ &{\\quad=w^{T}\\left(\\frac{1}{2}\\left(\\mu_{n}+\\mu_{n}\\right)\\right)+(1-\\lambda)\\,w^{T}\\left(\\mu-\\frac{\\mu_{n}+\\mu_{n}}{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{\\lambda=\\frac{p-q}{p+(K-1)q}}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "Then we show $\\begin{array}{r}{\\left(\\pmb{\\mu}_{o}-\\pmb{\\mu}_{p}\\right)^{T}\\left(\\bar{\\pmb{\\mu}}-\\frac{\\pmb{\\mu}_{o}+\\pmb{\\mu}_{p}}{2}\\right)=\\mathbf{0}.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "From $||\\pmb{\\mu}_{i}-\\pmb{\\mu}_{j}||_{2}=D$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n||\\pmb{\\mu}_{o}-\\bar{\\pmb{\\mu}}||_{2}=||\\pmb{\\mu}_{p}-\\bar{\\pmb{\\mu}}||_{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which gives: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\left(\\mu_{o}-\\bar{\\mu}\\right)^{T}\\left(\\mu_{o}-\\bar{\\mu}\\right)=\\left(\\mu_{p}-\\bar{\\mu}\\right)^{T}\\left(\\mu_{p}-\\bar{\\mu}\\right)}}\\\\ {{\\mu_{o}^{T}\\mu_{o}-2\\mu_{o}^{T}\\bar{\\mu}+\\bar{\\mu}^{T}\\bar{\\mu}=\\mu_{p}^{T}\\mu_{o}-2\\mu_{p}^{T}\\bar{\\mu}+\\bar{\\mu}^{T}\\bar{\\mu}}}\\\\ {{\\mu_{o}^{T}\\mu_{o}-2\\mu_{o}^{T}\\bar{\\mu}=\\mu_{p}^{T}\\mu_{o}-2\\mu_{p}^{T}\\bar{\\mu}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mu_{o}-\\mu_{p}\\right)^{T}\\bigg(\\Bar{\\mu}-\\frac{\\mu_{o}+\\mu_{p}}{2}\\bigg)}\\\\ &{=\\mu_{o}^{T}\\Bar{\\mu}-\\mu_{o}^{T}\\frac{\\mu_{o}+\\mu_{p}}{2}-\\mu_{p}^{T}\\Bar{\\mu}+\\mu_{p}^{T}\\frac{\\mu_{o}+\\mu_{p}}{2}}\\\\ &{=\\mu_{o}^{T}\\Bar{\\mu}-\\frac{1}{2}\\mu_{o}^{T}\\mu_{o}-\\bigg(\\mu_{p}^{T}\\Bar{\\mu}-\\frac{1}{2}\\mu_{p}^{T}\\mu_{p}\\bigg)}\\\\ &{=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining Eq. (13) and Eq. (16), we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb w}^{\\prime T}{\\pmb b}^{\\prime}={\\pmb w}^{T}{\\pmb b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which means $\\mathcal{P}=\\mathcal{P}^{\\prime}$ . ", "page_idx": 13}, {"type": "text", "text": "This completes the proof. ", "page_idx": 13}, {"type": "table", "img_path": "AF32GbuupC/tmp/78162132e62f595422b54fc46a20bb4d0186d0258306521b86abc641bf92fbfe.jpg", "table_caption": ["Table 5: Statistics of evaluation datasets "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "AF32GbuupC/tmp/cbc7591e26fe46e1f9f84676dca9f7cefa010f1f2d61eaccf0b524637d60a31b.jpg", "table_caption": ["Table 6: Hyper-parameters Search Space. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Experiments details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Datasets Description ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 CoraFull is an extension of the prevalent dataset \u2018Cora\u2019 [40], a citation network dataset. On this graph, nodes represent papers and edges represent citation links. The nodes are labeled on the paper topics. Node attributes are obtained using bag-of-words for the title and abstract of the paper. ", "page_idx": 14}, {"type": "text", "text": "\u2022 DBLP is also a citation network, where nodes represent papers and edges represent the citation between papers. Specifically, the node attributes are generated by the abstract and the node labels are based on the paper venues. ", "page_idx": 14}, {"type": "text", "text": "\u2022 ogbn-arXiv is a citation network among all Computer Science arXiv papers based on MAG [35]. Node represent papers and edges are citations links. The node attributes are obtained using skip-gram on abstract of papers. The nodes are labeled by the subject area. ", "page_idx": 14}, {"type": "text", "text": "D.2 Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Specifically, we implement our model by PyTorch [28] and conduct experiments on 24GB Nvidia RTX3090Ti, according to the training protocol Algorithm 2. Repeat number $R=5$ , patience $P=10$ , SAM update interval $k=2$ , validation interval $I=10$ , validation number $V=20$ , test number $W\\,=\\,100$ . For MAML models max epochs $T\\,=\\,500$ , and for non-MAML model max epochs $T=1000$ . We evaluate our method under various settings, i.e., $N=\\{5,10\\}$ , $K=\\{3,5\\}$ , but we set $N=\\{2,5\\}$ for Coauthor-CS dataset. We use Optuna [2] for hyper-parameters searching for all models with various optimizers, the search space is shown in Tab. 6. ", "page_idx": 14}, {"type": "text", "text": "Note that we further split $\\mathcal{C}_{\\mathrm{base}}$ into two disjoint class set: training class set $\\mathcal{C}_{\\mathrm{tr}}$ and validation class set $\\mathcal{C}_{\\mathrm{val}}$ , such that $\\mathcal{C}_{\\mathrm{base}}\\,=\\,\\mathcal{C}_{\\mathrm{tr}}\\cup\\mathcal{C}_{\\mathrm{val}}$ and $\\mathcal{C}_{\\mathrm{tr}}\\cap\\mathcal{C}_{\\mathrm{val}}=\\emptyset$ . Overall, we use $\\mathcal{C}_{\\mathrm{tr}}$ and $\\mathcal{C}_{\\mathrm{val}}$ for train and validation in the meta-training stage, respectively, and use $\\mathcal{C}_{\\mathrm{novel}}$ for meta-test. We split $\\mathcal{C}$ into $\\mathcal{C}_{\\mathrm{tr}}$ , $\\mathcal{C}_{\\mathrm{val}}$ and $\\mathcal{C}_{\\mathrm{novel}}$ according to the class split ratio in Tab. 5. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Training Protocol of FSNC Task ", "page_idx": 15}, {"type": "text", "text": "Require: $\\mathcal{G}$ , $\\mathcal{C}_{\\mathrm{tr}}$ , $\\mathcal{C}_{\\mathrm{val}}$ , $\\mathcal{C}_{\\mathrm{novel}}$ , repeat number $R$ , max epochs $T$ , patience $P$ , validation interval $I$ , validation number $V$ , test number $W$ .   \nEnsure: A trained model $\\hat{f}$ , model\u2019s accuracy $\\hat{s}$ . Initialize $f$ , $s\\gets\\{\\}$ . # repeat R times for $r\\gets0$ to $R-1$ do Initialize $s_{\\mathrm{best}}\\leftarrow0,\\,s_{\\mathrm{test}}\\leftarrow\\{\\},\\,p\\leftarrow0;$ # meta-training for $t\\leftarrow0$ to $T-1$ do # training Sample training task $\\boldsymbol{\\mathcal{T}_{t}}=\\{S_{t},\\boldsymbol{\\mathcal{Q}}_{t}\\}$ from $\\mathcal{C}_{\\mathrm{tr}}$ ; Optimize model $f$ on $\\mathcal{T}_{t}$ ; # validation if $t\\%I=0$ then Sample $V$ validation tasks ${\\mathcal{T}}_{\\mathrm{val}}$ from $\\mathcal{C}_{\\mathrm{val}}$ ; Compute mean accuracy $s_{\\mathrm{val}}\\;\\mathrm{on}\\tau_{\\mathrm{val}}$ by $f$ ; if $s_{\\mathrm{val}}>s_{\\mathrm{best}}$ then $s_{\\mathrm{best}}\\leftarrow s_{\\mathrm{val}}$ , $p\\gets0$ ; else $p\\gets p+1;$ ; end if # early-stop if $p=P$ then break; end if end if end for # meta-test Sample $W$ test tasks $\\mathcal{T}_{\\mathrm{test}}$ from $\\mathcal{C}_{\\mathrm{novel}}$ ; Compute mean accuracy $s_{\\mathrm{test}}$ on these tasks using model $f$ ; $s=s\\cup s_{\\mathrm{test}}$ ; end for $\\hat{f}\\leftarrow\\bar{f},\\hat{s}\\leftarrow\\mathrm{mean}(s).$ ", "page_idx": 15}, {"type": "table", "img_path": "AF32GbuupC/tmp/6566eb0de8a68e57fcd718dee12a1278bb73d617c83c5a84a0fa376f1b6ef81e.jpg", "table_caption": ["Table 7: Accuracy on the baseline with different optimizer. \u20185N3K\u2019 denotes 5-way 3-shot setting. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D.3 Evaluation Results with Standard Deviation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Tab. 7 and Tab. 8 we present the detailed results of Tab. 2 and Tab. 7 with standard deviation, respectively. ", "page_idx": 15}, {"type": "table", "img_path": "AF32GbuupC/tmp/a22fb61f21208a0bbc56ae45dce7b34850886e47cc8a50c7ed7b6874bf69d78e.jpg", "table_caption": ["Table 8: Results on nine real-world node classification benchmark datasets: Mean accuracy $(\\%)$ . The best results are denoted as boldface. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 9: Time consumption comparison. The results stands for the time (sec.) consumed in 200 episodes training (only consider the feed-forward and -backward). ", "page_idx": 16}, {"type": "table", "img_path": "AF32GbuupC/tmp/573ab24d2884bd533e711123383bd29c08086137d6a54bca2177edc4e6e1ac33.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.4 The Full Results of Time Consumption ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we present the detailed results of training time consumption of different optimizers across various datasets. Tab. 9 indicates that our proposed algorithm FGSAM demonstrates only a slight increase in training cost compared to Adam in most cases. Furthermore, our enhanced version FGSAM $^+$ outperforms Adam in terms of speed in the majority of scenarios. It is worth mentioning that our proposed algorithms achieve superior or comparable performance when compared to both Adam and SAM. ", "page_idx": 16}, {"type": "text", "text": "As mentioned before, for models composed of many non-GNN components (e.g. TENT), the training time on FGSAM+ may be still longer than that on Adam, since it is hardly further reduced. ", "page_idx": 16}, {"type": "text", "text": "E Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Statistics Of Benchmark Datasets In Node Classification ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "AF32GbuupC/tmp/a24a0fc046ad58dc6692010a0bfb8690cf2920f22c405e8ca3d614d6fc9d65af.jpg", "table_caption": ["Table 10: Benchmark datasets statistics for node classification "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "AF32GbuupC/tmp/43750067ede6e7aadfc86527d6e14387d0ee327ce404b36c2413e2c83afd70eb.jpg", "table_caption": ["Table 11: Performance of different update interval $k$ "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "AF32GbuupC/tmp/432c2865cbfc1a9aacd9c7782d1c635e1a778b658192ae7a6c6f1d5249f2c483.jpg", "table_caption": ["Table 12: Performance of prompt-based FSNC on Citeseer. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E.2 The Effect of Update Interval $k$ in FGSAM+ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we study the effect of update interval $k$ in $\\mathrm{FGSAM+}$ . It can be observed from Tab. 11 that as $k$ increases, the performance decreases, but meanwhile training time also decreases. This indicates that the possibility of choosing $\\boldsymbol{\\mathrm{k}}$ to achieve a better trade-off between performance and efficiency. We note that the performance drop with increasing $\\boldsymbol{\\mathrm{k}}$ seems to be larger compared to LookSAM [22] in computer vision tasks. This indicates the importance of the perturbation step in FGSAM+, as it not only introduces information about flat minima, but also incorporates neighbor information in training. Therefore, we recommend setting $k=2$ as the prior optimal update interval to avoid large information loss. ", "page_idx": 17}, {"type": "text", "text": "E.3 Integrating with Prompt-Based FSNC ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Recently, there are many prompt-based methods [24, 32] have been developed, showing promising performance in FSNC. Hence, we investigate how our method performs in such a prompt-based FSNC task. Note that under this setting, the proposed method is used in prompt tuning instead of training. As shown in Tab. 12, our method improves the baseline [32] with a remarkable margin. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We have discussed some limitations about the proposed FGSAM $\\cdot+$ in Sec. 5.3. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have provided assumptions and proofs in the main body and appendix. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Necessary information is provided ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Data is publicly available and code is also available. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Detailed are provided in the paper (main body and appendix). Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Due to the page limit, the standard deviation is provided in the appendix (Tab. 7). ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The information is provided in Sec. 5.1 and Sec. 5.3. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We confirm that we have adhered to the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We have discussed the potential broader impacts in Appendix A. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: There are no high-risk issues in our model trained for FSNC tasks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have cited all the related papers, the papers of models and datasets we use. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We will submit the code we used along with the supplement materials. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]