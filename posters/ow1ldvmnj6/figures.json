[{"figure_path": "OW1ldvMNJ6/figures/figures_0_1.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure shows a comparison of image generation results between the baseline model SDXL and the proposed model CoMat.  For several different text prompts, both models generated images.  The figure demonstrates that CoMat generates images that are more closely aligned with the text prompt than SDXL.  This highlights CoMat's improved ability to accurately reflect the input text in the generated image.", "section": "Abstract"}, {"figure_path": "OW1ldvMNJ6/figures/figures_1_1.jpg", "caption": "Figure 2: Visualization of token activation and attention map. We compare the tokens' attention activation value and attention map before and after applying our methods. Our method improves token activation and encourages the missing concept 'gown' to appear. Furthermore, the attention map of the attribute token 'red' better aligns with its region in the image.", "description": "This figure compares the attention mechanism of the SDXL model and the CoMat-SDXL model. The left panel shows the generated images from SDXL and CoMat-SDXL. The middle panel shows a bar chart illustrating the attention activation values for different tokens (red, cap, gown, guitar) in the text prompt. The right panel displays the attention maps for the token 'red' in both models. The results show that CoMat-SDXL improves token activation and better aligns the attention map with the corresponding regions in the generated image.", "section": "1 Introduction"}, {"figure_path": "OW1ldvMNJ6/figures/figures_3_1.jpg", "caption": "Figure 3: We showcase the results of our CoMat-SDXL compared with other state-of-the-art models. CoMat-SDXL consistently generates more faithful images.", "description": "This figure compares the image generation results of CoMat-SDXL with three other state-of-the-art text-to-image models (SDXL, Playground v2, PixArt-Alpha) on four different prompts. Each row shows the images generated by each model for the same prompt. The figure demonstrates that CoMat-SDXL consistently generates images that are more aligned with the textual descriptions compared to the other models.", "section": "Method"}, {"figure_path": "OW1ldvMNJ6/figures/figures_4_1.jpg", "caption": "Figure 4: Overview of CoMat. The text-to-image diffusion model (T2I-Model) first generates an image according to the text prompt. Then the image is sent to the concept activation module and attribute concentration module to compute the loss for fine-tuning the online T2I-Model.", "description": "This figure provides a detailed overview of the CoMat framework, which is an end-to-end diffusion model fine-tuning strategy designed to improve text-to-image alignment. The process starts with a text prompt fed into the original text-to-image diffusion model, which generates an initial image. This image is then processed through two key modules: the concept activation module and the attribute concentration module. The concept activation module leverages an image-to-text model to identify missing or underrepresented concepts in the generated image, allowing the model to refine its understanding.  The attribute concentration module helps to correctly map attributes from the text to specific regions within the generated image. The outputs from these modules then guide the fine-tuning of the online text-to-image model, improving its overall alignment with the original text prompt.", "section": "Method"}, {"figure_path": "OW1ldvMNJ6/figures/figures_6_1.jpg", "caption": "Figure 4: Overview of CoMat. The text-to-image diffusion model (T2I-Model) first generates an image according to the text prompt. Then the image is sent to the concept activation module and attribute concentration module to compute the loss for fine-tuning the online T2I-Model.", "description": "This figure shows the overall framework of CoMat, a text-to-image diffusion model fine-tuning strategy. It starts with a text prompt that is fed into the original text-to-image diffusion model (T2I-Model). This model generates an image based on the prompt.  The generated image then goes through two key modules: the concept activation module and the attribute concentration module. These modules calculate a loss function that is used to fine-tune an online version of the T2I model, improving its ability to accurately reflect the text prompt in the generated image.  The figure shows the flow of data through these components and their interactions.", "section": "4 Method"}, {"figure_path": "OW1ldvMNJ6/figures/figures_8_1.jpg", "caption": "Figure 6: Visualization of the effectiveness of the proposed modules. CA contributes to the existence of objects mentioned in the prompts. AC further guides the attention of the attributes to focus on their corresponding objects.", "description": "This figure shows examples of images generated by different models. The first column shows the baseline model, which sometimes misses objects or attributes mentioned in the prompt. The second column shows the model with concept activation, which improves the generation of objects. The third column shows the model with both concept activation and attribute concentration, which further improves the alignment of attributes with their corresponding objects. The results show the effectiveness of the proposed modules for aligning text and image concepts.", "section": "5.4 Ablation Study"}, {"figure_path": "OW1ldvMNJ6/figures/figures_9_1.jpg", "caption": "Figure 7: Visualization result of the effectiveness of the Fidelity Preservation module (FP) and Mixed Latent (ML) strategy.", "description": "This figure visualizes how the Fidelity Preservation (FP) module and Mixed Latent (ML) strategy in CoMat affect the generated image quality. The left two images show a blue envelope and a white stamp. The images on the right show a black swan and a white lake. In both cases, the model without FP and ML generates misshaped objects, while CoMat with FP and ML generates more aligned images with better quality.", "section": "F More Qualitative Results"}, {"figure_path": "OW1ldvMNJ6/figures/figures_17_1.jpg", "caption": "Figure 8: User preference study results.", "description": "The figure shows the results of a user preference study comparing SDXL and CoMat-SDXL.  Participants were asked to choose which model produced better image quality and better text-image alignment based on a set of prompts.  The bar chart visualizes the percentage of participants who preferred each model for each aspect.", "section": "B Additional Results and Analysis"}, {"figure_path": "OW1ldvMNJ6/figures/figures_17_2.jpg", "caption": "Figure 4: Overview of CoMat. The text-to-image diffusion model (T2I-Model) first generates an image according to the text prompt. Then the image is sent to the concept activation module and attribute concentration module to compute the loss for fine-tuning the online T2I-Model.", "description": "This figure shows a diagram of the CoMat model, highlighting its key components. The text-to-image diffusion model generates an initial image based on the given text prompt. This image is then fed into two modules: the concept activation module and the attribute concentration module. These modules calculate a loss function that is used to fine-tune the online T2I model, improving its ability to generate images that accurately reflect the input text prompt. The diagram visually represents the workflow of CoMat, illustrating how the different components interact and contribute to improved text-to-image alignment.", "section": "4 Method"}, {"figure_path": "OW1ldvMNJ6/figures/figures_18_1.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure shows image generation results from the SDXL model and the proposed CoMat model.  Four text prompts are used, and for each prompt, the image generated by each model is shown side-by-side. The images illustrate the improved alignment between the text prompt and generated image achieved with the CoMat method compared to the SDXL baseline. The superior performance of CoMat in faithfully representing the details and relationships described in the prompts is evident.", "section": "Abstract"}, {"figure_path": "OW1ldvMNJ6/figures/figures_23_1.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure shows a comparison between images generated by the state-of-the-art text-to-image diffusion model SDXL and the proposed model CoMat.  Four different text prompts are used, and for each prompt, there are two generated images: one from SDXL and one from CoMat.  The purpose is to demonstrate that CoMat produces images that are more closely aligned with the descriptions given in the text prompts. The same random seed was used to generate both pairs to ensure fair comparison.", "section": "Abstract"}, {"figure_path": "OW1ldvMNJ6/figures/figures_23_2.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure shows a comparison of image generation results between the state-of-the-art text-to-image diffusion model SDXL and the proposed method CoMat.  Four different text prompts are used, and for each, the images generated by both models are displayed side-by-side. The images generated by CoMat demonstrate a significant improvement in alignment with the text prompts, showing that CoMat successfully addresses the misalignment problem often seen in text-to-image diffusion models. The same random seed was used for all image generations to ensure a fair comparison.", "section": "Introduction"}, {"figure_path": "OW1ldvMNJ6/figures/figures_23_3.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure shows examples of images generated by the state-of-the-art text-to-image diffusion model SDXL and the proposed model CoMat.  The images were generated from the same text prompts to highlight the improvement in alignment between the generated image and the input text prompt. CoMat produces images that more closely match the descriptions given in the text prompts than SDXL.", "section": "Introduction"}, {"figure_path": "OW1ldvMNJ6/figures/figures_23_4.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure shows a comparison of images generated by the state-of-the-art text-to-image diffusion model SDXL and the proposed method CoMat.  For each of four different text prompts, the figure displays the output of both SDXL and CoMat.  The differences highlight CoMat's improved ability to generate images that accurately reflect the details and relationships described in the text prompt. The use of the same random seed for both models allows for a direct comparison based solely on the models' abilities to interpret the text instructions.", "section": "Abstract"}, {"figure_path": "OW1ldvMNJ6/figures/figures_23_5.jpg", "caption": "Figure 3: We showcase the results of our CoMat-SDXL compared with other state-of-the-art models. CoMat-SDXL consistently generates more faithful images.", "description": "This figure presents a comparison of image generation results between the proposed CoMat-SDXL model and three other state-of-the-art text-to-image models (SDXL, Playground v2, and PixArt-Alpha).  For each model, four example images are displayed, each generated from a different text prompt. The figure aims to demonstrate that CoMat-SDXL produces images that are more closely aligned with the corresponding text prompts than the other models, indicating a superior ability to generate faithful images.", "section": "Method"}, {"figure_path": "OW1ldvMNJ6/figures/figures_23_6.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure showcases the results of the SDXL model and the proposed CoMat model when generating images from the same set of text prompts. It visually demonstrates that the CoMat model significantly improves the alignment between the text prompt and the generated image compared to the SDXL model.", "section": "Introduction"}, {"figure_path": "OW1ldvMNJ6/figures/figures_24_1.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure showcases examples of images generated by the state-of-the-art text-to-image diffusion model SDXL and the proposed method CoMat.  The top row shows SDXL struggling to correctly interpret and generate images according to the descriptive prompts. The bottom row illustrates CoMat's improved ability to accurately reflect the textual input in the generated images. The same random seed was used for both models in each pair of images, highlighting CoMat's superior performance.", "section": "Abstract"}, {"figure_path": "OW1ldvMNJ6/figures/figures_25_1.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure shows a comparison of images generated by the baseline text-to-image diffusion model (SDXL) and the proposed method (CoMat).  Four different text prompts were used, and for each prompt, both models generated images. The figure demonstrates that CoMat produces images which are more faithful to the descriptions in the text prompts compared to SDXL, highlighting the effectiveness of the proposed method in improving text-image alignment.", "section": "Abstract"}, {"figure_path": "OW1ldvMNJ6/figures/figures_26_1.jpg", "caption": "Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.", "description": "This figure showcases the limitations of current text-to-image diffusion models in aligning generated images with text prompts.  It compares images generated by the SDXL model with those generated by the authors' proposed CoMat model, using the same random seed for each prompt. The CoMat model produces images that more accurately reflect the content of the text prompts, highlighting its improved ability to align text and image.", "section": "Abstract"}]