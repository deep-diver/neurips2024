[{"type": "text", "text": "CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dongzhi Jiang1,2, Guanglu Song2, Xiaoshi $\\mathbf{W}\\mathbf{u}^{1}$ , Renrui Zhang1,3, Dazhong Shen3, Zhuofan Zong1,2, $\\mathbf{Y}\\mathbf{u}\\,\\mathbf{Liu}^{2\\boxtimes}$ , Hongsheng Li1,3,4 ", "page_idx": 0}, {"type": "text", "text": "1CUHK MMLab, 2SenseTime Research, 3Shanghai AI Laboratory, 4CPII under InnoHK {dzjiang, wuxiaoshi, zhangrenrui}@link.cuhk.edu.hk songguanglu@sensetime.com {dazh.shen, zongzhuofan, liuyuisanai}@gmail.com hsli@ee.cuhk.edu.hk ", "page_idx": 0}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/2fbabb812bb9859d197ddb427f3221155dfadff68a92b0b9e6d6e1896ec6aea7.jpg", "img_caption": ["Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL [49]. Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. We break down the problem into two causes: concept ignorance and concept mismapping. To tackle the two challenges, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with the imageto-text concept matching mechanism. Firstly, we introduce a novel image-totext concept activation module to guide the diffusion model in revisiting ignored concepts. Additionally, an attribute concentration module is proposed to map the text conditions of each entity to its corresponding image area correctly. Extensive experimental evaluations, conducted across three distinct text-to-image alignment benchmarks, demonstrate the superior efficacy of our proposed method, CoMatSDXL, over the baseline model, SDXL [49]. We also show that our method enhances general condition utilization capability and generalizes to the long and complex prompt despite not specifically training on it. The code is available at https://github.com/CaraJ7/CoMat. ", "page_idx": 0}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/38b5457a0d6b33f7b3feae01cd99e0379ad3c237518a23babc58a57066345700.jpg", "img_caption": ["Figure 2: Visualization of token activation and attention map. We compare the tokens\u2019 attention activation value and attention map before and after applying our methods. Our method improves token activation and encourages the missing concept \u2018gown\u2019 to appear. Furthermore, the attention map of the attribute token \u2018red\u2019 better aligns with its region in the image. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The area of text-to-image generation has witnessed considerable progress with the introduction of diffusion models [23, 52, 51, 56, 59, 76] recently. These models have demonstrated remarkable performance in creating high-fidelity and diverse images based on textual prompts. However, it still remains challenging for these models to faithfully align with the prompts, especially for the complex ones. For example, as shown in Fig. 1, current state-of-the-art open-sourced model SDXL [49] fails to generate entities or attributes mentioned in the prompts, e.g., the feathers made of lace and dwarfs in the top row. Additionally, it fails to understand the relationship in the prompt. In the middle row of Fig. 1, it mistakenly generates a Victorian gentleman and a quilt with a river on it. ", "page_idx": 1}, {"type": "text", "text": "We break down this misalignment problem into two causes: concept ignorance and concept mismapping. The concept ignorance problem is caused by the diffusion model\u2019s omission of certain concepts in the text prompt. Even though the concept token is activated, the diffusion model often fails to map it to the correct area in the image, which is termed the concept mismapping problem. Actually, the misalignment originally stems from the training paradigm of the text-to-image diffusion models: Given the text condition $c$ and the paired image $x$ , the training process aims to learn the conditional distribution $p(x|c)$ . However, the text condition only serves as additional information for the denoising loss. Without explicit guidance in learning each concept in the text, the diffusion model could easily fail to understand the concepts in the prompt correctly. ", "page_idx": 1}, {"type": "text", "text": "Recently, to alleviate the misalignment, various works have proposed to incorporate linguistics prior [53, 6] to heuristically address the concept omission or concept mismapping problem. However, a specific design is required for each type of misalignment problem. Other works use the Large Language Model (LLM) [41, 77] to split the prompt into single entities and generate each of them. Although this method promotes the congruence between the image\u2019s global structure and the text prompt, it still suffers from local misalignment of the single entity. Hence, we ask the question: $\\pmb{I s}$ there a universal solution to address various global and local misalignment problems? ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose CoMat, an end-to-end fine-tuning strategy to enhance the prompt understanding and following by a novel image-to-text matching mechanism. Concept Activation module is proposed to address the concept ignorance problem. Given the generated image $\\hat{x}$ conditioning on the prompt $c$ , we seek to model and maximize the posterior probability $p(c|\\hat{\\boldsymbol x})$ using a pre-trained image-to-text model. In contrast to regarding the textual prompt merely as a condition, as performed in the pre-training phase of the diffusion model, our approach incorporates the condition as a supervisory signal during the training process. Thanks to the proficiency of the image-to-text model in concept matching, whenever a particular concept is absent from the generated image, the diffusion model is steered to incorporate it within the image generation process. The guidance forces the diffusion model to revisit the ignored conditions and attend more to them. As an illustrative example shown in Fig. 2, the ignored concept in the image (e.g., the gown) possesses low attention activation values. After applying our method, we observe increased attention activation of each key concept, contributing to the aligned image. In addition, considering the catastrophic forgetting issue arising from the new optimization objective, we also introduce a novel fidelity preservation module and mixed latent strategy to preserve the generation capability of the diffusion model. As for the concept mismapping problem, we find it especially prevails among the attributes of the objects. Hence, the Attribute Concentration module is introduced to promote both positive and negative mapping. We match the concept of attribute tokens in the text prompt to the generated image, with the insight that the attribute tokens should only be activated within its entity\u2019s area. Since the concept is a general term for a variety of features, our method can address both global structures and local details. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "As an end-to-end method, no extra overhead is introduced during inference. We also show that our method is composable with methods leveraging external knowledge. Our contributions are summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose CoMat, a text-to-image diffusion model fine-tuning strategy to effectively enhance the condition utilization capability by explicitly addressing the condition ignorance and incorrect condition mapping problem.   \n\u2022 We introduce the concept activation module equipped with fidelity preservation and mixed latent strategy to facilitate concept generation and attribute concentration module to foster correct concept mapping from text to image.   \n\u2022 Extensive quantitative and qualitative comparisons with baseline models indicate that our method significantly improves the text-image alignment in various scenarios, including object existence, attribute binding, relationship, and complex prompts. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recently, text-to-image diffusion models [56, 52, 65, 66] have become extremely trending, but they have also brought many new challenges [28, 61]. Among them, the text-to-image alignment problem has gained much attention. The problem is defined as the incoherence between the prompts and the generated images, which involves multiple aspects including existence, attribute binding, relationship, etc. Recent methods address the problem mainly in three ways. ", "page_idx": 2}, {"type": "text", "text": "Attention-based methods [6, 53, 47, 70, 2, 40] aim to modify or add restrictions on the attention map in the attention module in the UNet. This type of method often requires a heuristic design for each misalignment problem. ", "page_idx": 2}, {"type": "text", "text": "Planning-based methods first obtain the image layouts, either from the input of the user [39, 11, 32, 74, 15] or the generation of the Large Language Models (LLM) [48, 77, 69], and then produce aligned images conditioned on the layout. In addition, a few works propose to further refine the image with other vision expert models [55, 72, 71, 77]. Although such method splits a compositional prompt into single objects, it does not resolve the inaccuracy of the downstream diffusion model and still suffers from incorrect attribute binding problems. Besides, it exerts nonnegligible costs during inference. ", "page_idx": 2}, {"type": "text", "text": "Moreover, some works aim to enhance the alignment using feedback from image understanding models. [28, 62] fine-tune the diffusion model with well-aligned generated images chosen by the VQA model [35] to strategically bias the generation distribution. Other works propose to optimize the diffusion models in an online manner. [17, 4] introduce RL fine-tuning for generic rewards. As for differentiable reward, [14, 75, 73] propose to backpropagate the reward function gradient through the denoising process. Other works like [46] enhance the prompt encoding to foster better alignment. Similar to our work, [18] also proposes to leverage image captioning models. We discuss the difference between their method with ours in Appendix C. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We implement our method on the leading text-to-image diffusion model, Stable Diffusion [56], which belongs to the family of latent diffusion models (LDM). In the training process, a normally distributed noise $\\epsilon$ is added to the original latent code $z_{0}$ with a variable extent based on a timestep $t$ sampling from $\\{1,...,T\\}$ . Then, a denoising function $\\epsilon_{\\theta}$ , parameterized by a UNet backbone, is trained to predict the noise added to $z_{0}$ with the text prompt $\\mathcal{P}$ and the current latent $z_{t}$ as the input. Specifically, the text prompt is first encoded by the CLIP [50] text encoder $W$ , then incorporated into the denoising function $\\epsilon_{\\theta}$ by the cross-attention mechanism. Concretely, for each cross-attention layer, the latent and text embedding is linearly projected to query $Q$ and key $K$ , respectively. The cross-attention map A(i) \u2208Rh\u00d7w\u00d7l is calculated as A(i) = Softmax( Q(i)(\u221aKd(i))T) , where $i$ is the index of head. $h$ and $w$ are the resolution of the latent, $l$ is the token length for the text embedding, and $d$ is the feature dimension. $A_{i,j}^{n}$ denotes the attention score of the token index $n$ at the position $(i,j)$ . The denoising loss in diffusion models\u2019 training is formally expressed as: ", "page_idx": 2}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/73af460874dcf3d0463fbfbbdbbd2a609e4441ccfba09c91bfcc8ed4909c9663.jpg", "img_caption": ["Figure 3: We showcase the results of our CoMat-SDXL compared with other state-of-the-art models. CoMat-SDXL consistently generates more faithful images. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{LDM}}=\\mathbb{E}_{z_{0},t,p,\\epsilon\\sim\\mathcal{N}(0,I)}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(z_{t},t,W(\\mathcal{P})\\right)\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For inference, one draws a noise sample $z_{T}\\sim\\mathcal{N}(0,I)$ , and then iteratively uses $\\epsilon_{\\theta}$ to estimate the noise and compute the next latent sample. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The overall framework of our method is shown in Fig. 4. In Section 4.1, we first illustrate the concept activation module. Following this, we detail how we maintain the generation capability of the diffusion model by the fidelity preservation module and mixed latent strategy. Subsequently, in Section 4.2, we introduce the attribute concentration module for promoting attribute binding, and then we integrate the two components for joint learning. ", "page_idx": 3}, {"type": "text", "text": "4.1 Concept Activation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As noted in Section 1, the diffusion model occasionally exhibits little attention on certain concepts, and the corresponding concept is therefore missing in the image, which we termed as the condition ignorance problem. To address this, our key insight is to add supervision on the generated image to detect the missing concepts. We achieve this by leveraging the image understanding ability of an image-to-text model, which can accurately identify concepts not present in the generated image based on the given text prompt. With the image-to-text model\u2019s supervision, the diffusion model is compelled to revisit text tokens to search for ignored condition information and assign more significance to the previously overlooked text concepts for better text-image alignment. Concretely, given a prompt $\\mathcal{P}$ with word tokens $\\{w_{1},w_{2},\\dots,w_{L}\\}$ , we first generate an image $\\mathcal{T}$ with the denoising function $\\epsilon_{\\theta}$ after $T$ denoising steps. Then, a frozen image-to-text model $\\mathcal{C}$ is used to score the alignment between the prompt and the image in the form of log-likelihood. The scoring capability of $\\mathcal{C}$ comes with the image-to-text models\u2019 training nature. These models are trained for the image captioning task with the negative loglikelihood loss, i.e., the model needs to maximize the probability of generating the caption given the corresponding image. Therefore, whenever the generated image does not align with the text prompt, the model will output a low log-likelihood. Our training objective aims to minimize the negative of the log-likelihood, denoted as $\\mathcal{L}_{i2t}$ : ", "page_idx": 3}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/9f894586cbcfc8fc120ed7d4cdc042551866dd5ff85e680a03d85119db53d33c.jpg", "img_caption": ["Figure 4: Overview of CoMat. The text-to-image diffusion model (T2I-Model) first generates an image according to the text prompt. Then the image is sent to the concept activation module and attribute concentration module to compute the loss for fine-tuning the online T2I-Model. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{i2t}}=-\\log(p_{\\mathcal{C}}(\\mathcal{P}|\\mathcal{Z}(\\mathcal{P};\\epsilon_{\\theta})))=-\\sum_{i=1}^{L}\\log(p_{\\mathcal{C}}(w_{i}|\\mathcal{Z},w_{1:i-1})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Besides, it is also important to note that the concepts in the image include a broad field. This method provides a universal solution to various misalignment problems like object existence, complex relationships, etc. Finally, to conduct the gradient update through the whole iterative denoising process, we follow [73] to fine-tune the denoising network $\\epsilon_{\\theta}$ , which ensures the training effectiveness and efficiency by simply stopping the gradient of the denoising network input. ", "page_idx": 4}, {"type": "text", "text": "However, since this fine-tuning process is purely piloted by the knowledge from the image-to-text model, the diffusion model could quickly overfti to the image-to-text model, lose its original capability, and produce deteriorated images, as shown in Fig. 7. To address this hacking issue, we introduce a novel fidelity preservation module and a mixed latent training strategy to preserve the generation ability of the diffusion model and guide the learning process. ", "page_idx": 4}, {"type": "text", "text": "Fidelity Preservation. We propose a novel adversarial loss that uses a discriminator to differentiate between images generated by pre-trained and fine-tuned diffusion models. Instead of using real-world images as the real data input for the discriminator, we use images generated by the original pre-trained diffusion model. This choice is based on the significant gap that still exists between the images generated by the original diffusion model and real-world images. Simply aligning the distribution of images generated by the fine-tuned diffusion model with that of real-world images would pose an undesired challenge for the learning process. For the discriminator $\\mathcal{D}_{\\phi}$ , we initialize it with the pre-trained UNet in the Stable Diffusion model. The choice is motivated by the fact that the pre-trained UNet shares similar knowledge with the online training model and ftis well with the input domain. In our practice, this also enables the adversarial loss to be directly calculated in the latent space instead of the image space. Concretely, given a single text prompt, we employ the original diffusion model and the online training model to respectively generate image latent $\\hat{z}_{0}$ and $\\hat{z}_{0}^{\\prime}$ . The adversarial loss is then computed as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a d v}=\\log\\left(\\mathcal{D}_{\\phi}\\left(\\hat{z}_{0}\\right)\\right)+\\log\\left(1-\\mathcal{D}_{\\phi}\\left(\\hat{z}_{0}^{\\prime}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We aim to fine-tune the online model to minimize this adversarial loss, while concurrently training the discriminator to maximize it. ", "page_idx": 5}, {"type": "text", "text": "Mixed Latent Strategy. Besides, we inject information from real-world images to guide the learning process. Specifically, in addition to the latents starting from pure noise (marked as black in Fig. 4), we obtain the noisy real latents by adding noise on a real-world image at a random timestep $\\tau$ (marked as \u2018Noisy GT\u2019). We jointly denoise these two types of latents and calculate the loss given by the image-to-text model. The intuition is that, since the noisy real latent is a perturbed version of the real-world image, which is well aligned with its prompt, this provides a shortcut for the diffusion model to directly reconstruct the original image. This guidance can not only smooth the optimization process, but also prohibits the gradient from simply hacking the image-to-text model and encourages the diffusion model to generate an image both aligned with the prompt and of high fidelity. More illustration is included in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "4.2 Attribute Concentration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Except for paying enough attention to the concept, the diffusion model must also map the concepts correctly on the image. As we dive into the generation process by visualizing the token attention activation map, we find that, for the attribute token, even though it is activated, it fails to attend to the correct area in the image and still causes the misalignment, e.g., \u2018yellow\u2019 in Fig. 5. Hence, we introduce the attribute concentration module to encourage the positive and discourage the negative concept mapping of attributes. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we first extract all the entities $\\{e_{1},..,e_{N}\\}$ in the prompts. An entity can be defined as a tuple of a noun $n_{i}$ and its attributes $a_{i}$ , i.e., $e_{i}\\,=\\,(n_{i},a_{i})$ , where both $n_{i}$ and $a_{i}$ are the sets of one or multiple tokens. We employ spaCy\u2019s transformer-based dependency parser [24] to parse the prompt to find all entity nouns, and then collect all attributes for each noun. A predefined set of nouns is established for filtering, including nouns that are abstract (e.g., scene, atmosphere, language), difficult to identify their area (e.g., sunlight, noise, place), or describe the background (e.g., morning, bathroom, party). Given all the selected nouns, we use them to prompt an open vocabulary segmentation model, Grounded-SAM [55], to find their corresponding regions as a binary mask $\\{\\breve{M}^{1},...,M^{N}\\}$ . It is worth emphasizing that, to guarantee the segmentation accuracy, we only use the nouns of entities, excluding their associated attributes, as prompts for segmentation, considering the diffusion model could likely ignore the attribute or assign a wrong one to the object. Taking the \u2018suitcase\u2019 object in Fig 5 as an example, the model ignored the \u2018purple\u2019 attribute. Consequently, if the prompt \u2019purple suitcase\u2019 is given to the segmentor, it will fail to identify the entity\u2019s region. These inaccuracies can lead to a cascade of errors in the following process. ", "page_idx": 5}, {"type": "text", "text": "We add supervision to promote the diffusion model to map the entity tokens to the positive area, i.e., the entity area, and not to attend to the negative area, i.e., the other area: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\mathcal{L}_{\\mathrm{pos}}}}&{{=}}&{{\\displaystyle-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{M_{u,v}^{i}=1}\\left(\\alpha\\sum_{k\\in n_{i}\\cup a_{i}}\\left(\\frac{A_{u,v}^{k}}{\\sum_{x,y}A_{x,y}^{k}}\\right)+\\beta\\frac{\\log(\\sum_{k\\in n_{i}\\cup a_{i}}A_{u,v}^{k})}{|A|}\\right),}}\\\\ {{\\mathcal{L}_{\\mathrm{neg}}}}&{{=}}&{{\\displaystyle-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{M_{u,v}^{i}=0}\\left(\\underbrace{\\alpha\\sum_{k\\in n_{i}\\cup a_{i}}\\left(\\frac{-A_{u,v}^{k}}{\\sum_{x,y}A_{x,y}^{k}}\\right)}_{\\mathrm{region-lenel}}+\\beta\\underbrace{\\frac{\\log(1-\\sum_{k\\in n_{i}\\cup a_{i}}A_{u,v}^{k})}{\\|A\\|}}_{\\mathrm{pixel-tenel}}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $|A|$ is the number of pixels on the attention map, $\\alpha$ and $\\beta$ are two scaling factors, and $M^{i}$ should be resized to the resolution for each attention map $A$ . The loss function covers the level of regions and pixels. Take the $\\mathcal{L}_{\\mathrm{pos}}$ for example. We restrict the attention of each entity tokens $e_{i}$ only activated inside the positive region by the region-level loss. We further restrict each pixel in the positive region to only attend to entity tokens by the pixel-level loss. We take into account the scenario where certain objects in the prompt do not appear in the generated image due to misalignment. In this case, the negative loss of pixels is still valid. When the mask is entirely zero, it signifies that none of the pixels should attend to the missing entity tokens in the current image. ", "page_idx": 5}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/c8a998d53bf2781410c44dcb2822fb3c4a3959ffe2a4952b7c8c7f2ac43b3f36.jpg", "img_caption": ["Figure 5: Overview of Attribute Concentration. Given a prompt, we first generate an image and record the cross-attention map for each token. We then identify regions of each entities in the prompt using the segmentation model. Finally, we optimize for the consistency between the entity attention map and its respective area in the image by encouraging positive and discouraging negative mapping. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Finally, we combine the image-to-text model loss, adversarial loss and attribute concentration loss to build up our training objectives for the online diffusion model as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{\\mathrm{i2t}}+\\mathcal{L}_{\\mathrm{pos}}+\\mathcal{L}_{\\mathrm{neg}}+\\lambda\\mathcal{L}_{\\mathrm{adv}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda$ are scaling factors to balance the loss. We provide the pseudocode for the loss computation process in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1: T2I-CompBench result. The best score is in blue , with the second-best score in green ", "page_idx": 6}, {"type": "table", "img_path": "OW1ldvMNJ6/tmp/eafb179427544e599e63654ce0468753c5d173b967464fcac869ab0fe2e01e83.jpg", "table_caption": ["Base Model Settings. We mainly implement our method on SDXL [56] for all experiments, and we also evaluate our method on Stable Diffusion v1.5 [56] (SD1.5). For the captioning model, we choose "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "BLIP [35] fine-tuned on COCO [42] image-caption data. We adopt the pre-trained UNet of SD1.5 as the discriminator in the fidelity preservation module. More training details are in Appendix E.1. ", "page_idx": 7}, {"type": "text", "text": "Dataset. Since the prompt to the diffusion model needs to be challenging enough to lead to missing concepts, we directly utilize the training data or text prompts provided in existing text-to-image alignment benchmarks. Specifically, the training data includes the training set provided in T2ICompBench [28], all the data from HRS-Bench [3], and 5,000 prompts randomly chosen from ABC-6K [20]. Altogether, these amount to around 20,000 text prompts. Note that the training set composition can be freely adjusted according to the ability targeted to improve. The text-image pairs used in the mixed latent strategy are from the training set of COCO [42]. ", "page_idx": 7}, {"type": "text", "text": "Benchmarks. We evaluate our method on three text-image alignment benchmarks and follow their default settings. T2I-CompBench [28] comprises 6,000 compositional text prompts evaluating 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions). TIFA [27] uses pre-generated question-answer pairs and a VQA model to evaluate the generation results with 4,000 diverse text prompts and 25,000 questions across 12 categories. DPG-Bench [26] composes 1065 dense prompts with an average token length of 83.91. The prompt depicts a much more complex scenario with diverse objects and adjectives. ", "page_idx": 7}, {"type": "table", "img_path": "OW1ldvMNJ6/tmp/c2919d70813df86c5875a49718ef00c461d4f69bc8e59b095664fa893bea2a3f.jpg", "table_caption": ["Table 2: TIFA and DPG-Bench results. ", "Table 3: FID-10K result. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Quantitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our methods with our baseline models: SD1.5 and SDXL, and two state-of-the-art open-sourced text-to-image models: PixArt- $\\alpha$ [7] and Playground-v2 [33]. ", "page_idx": 7}, {"type": "text", "text": "T2I-CompBench. The evaluation result is shown in Table 1. Note that we cannot reproduce results reported in some relevant works [7, 28] due to the evolution of the evaluation code. All our shown results are based on the latest code released in $\\mathrm{GitHub^{1}}$ . We observe significant gains in all six sub-categories compared with our baseline models. With our methods, SD1.5 can even achieve better or comparable results compared with PixArt- $\\alpha$ and Playground-v2. Our CoMat-SDXL demonstrates the best performance regarding attribute binding, spatial relationships, and complex compositions. ", "page_idx": 7}, {"type": "text", "text": "TIFA. We show the results in TIFA in Table 2. Our CoMat-SDXL achieves the best performance with an improvement of 1.6 scores compared to SDXL. Besides, CoMat significantly enhances SD1.5 by 7.4 scores, which largely surpasses PixArt- $\\alpha$ . ", "page_idx": 7}, {"type": "text", "text": "DPG-Bench. The results in DPG-Bench is shown in Table 2. Although we do not train our model on dense prompts and can only accept 77 tokens, similar to Stable Diffusion, our method successfully generalizes to this more complex scenario and brings significant improvement to the baseline model. ", "page_idx": 7}, {"type": "text", "text": "5.3 Qualitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Fig. 3 presents a side-by-side comparison between CoMat-SDXL and other state-of-the-art diffusion models. We observe these models exhibit inferior condition utilization ability compared with CoMatSDXL. Prompts in Fig. 3 all possess concepts that are contradictory to real-world phenomena. All the three compared models stick to the original bias and choose to ignore the unrealistic content (e.g., waterfall cascading from a teapot, transparent violin, robot penguin, and waterfall of liquid gold), which causes misalignment. However, by training to faithfully align with the conditions in the prompt, CoMat-SDXL follows the unrealistic conditions and provides well-aligned images. The user study result and more visualization result is detailed in Appendix B.1 and F.2. ", "page_idx": 7}, {"type": "text", "text": "Table 4: Impact of concept activation and attribute concentration. \u2018CA\u2019 and \u2018AC\u2019 denote concept activation and attribute concentration respectively. ", "page_idx": 8}, {"type": "table", "img_path": "OW1ldvMNJ6/tmp/0e81e363c4d90f533c99e03ba64ee1822b3675a9133dc3b87860fb7513e07d20.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "OW1ldvMNJ6/tmp/8caf998e65b022ab3487c4ab603f5e493c9ffd1660933ee7e245b357ab20743a.jpg", "table_caption": ["Table 5: The impact of different image-to-text models. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effectiveness of Concept Activation and Attribute Concentration. In Table 4, we show the T2I-CompBench result aiming to identify the effectiveness of the concept activation and attribute concentration modules. We find that the concept activation module accounts for major gains to the baseline model. On top of that, the attribute concentration module brings further improvement to all six sub-categories in T2I-CompBench. We show the qualitative effectiveness in Fig. 6. ", "page_idx": 8}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/fc6d4b0904d31bbfcee69e6c8484a223cb213b905d7081bbcb5ad7136491f209.jpg", "img_caption": ["Figure 6: Visualization of the effectiveness of the proposed modules. CA contributes to the existence of objects mentioned in the prompts. AC further guides the attention of the attributes to focus on their corresponding objects. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Design of Fidelity Preservation and Mixed Latent. We examine the photorealism of generated images to evaluate the generation capability. We calculate the FID [22] score using 10K data from the COCO validation set. As shown in Table 3 and Fig. 7, without any preservation method, the diffusion model only tries to hack the image-to-text model and loses its original generation ability with an increase of FID score from 16.69 to 19.02. Besides, inputting the latent generated by the original diffusion model performs better than the latent of real-world images. As for the discriminator architecture, the UNet is superior to a pre-trained DINO [5] which even interferes the training process. Finally, the Mixed Latent (ML) strategy further enhances the generated image quality. ", "page_idx": 8}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/07691b48ff46c12a7db8e95f8d3c06f14f6efc55dc706d668e186491623626c9.jpg", "img_caption": ["Figure 7: Visualization result of the effectiveness of the Fidelity Preservation module (FP) and Mixed Latent (ML) strategy. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Different Image-to-text Models. We show the T2I-CompBench results with different image captioning models in Table 5. We find that all three image-to-text models can boost the performance of the diffusion model with our framework, where BLIP achieves the best performance. We provide more analysis on the choice of the image-to-text models in Appendix B.3. ", "page_idx": 9}, {"type": "text", "text": "5.5 Robustness Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[16] introduces an automated method to discover prompts that induce misalignment in Stable Diffusion models. We evaluate this attack method on SD1.5 and CoMat-SD1.5 using both short and long prompts. ", "page_idx": 9}, {"type": "text", "text": "For 1,000 ImageNet-1K classes, we generate 20 samples per class using the attack method and measure the success rate - defined as the proportion of generated images that could be mistakenly classified by a visual classifier. Table 6 shows that CoMat-SD1.5 exhibits lower attack success rates for both prompt lengths, demonstrating enhanced alignment robustness compared to the base model. ", "page_idx": 9}, {"type": "table", "img_path": "OW1ldvMNJ6/tmp/78e2fb636532762d8de91793c2bbf1a62c02f1ea9c9af967fd8f4a45626719e3.jpg", "table_caption": ["Table 6: Success rate of prompt attack. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "How to more effectively incorporate Multimodal Large Language Models (MLLMs) into text-toimage diffusion models by our proposed method requires more exploration. MLLM possesses state-of-the-art image-text understanding capability in addition to image captioning. We will focus on leveraging MLLMs to provide finer-grained guidance to the diffusion model in our future work. In addition, the attribute concentration module cannot assign attributes to multiple same-name objects, such as an Asian girl with an Indian girl, the segmentation model cannot differentiate two girls and therefore cannot assign attributes. As for the training cost, since our method needs the diffusion model to perform the whole inference process, the training time is extended. Our future direction will be to accelerate the training process. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose CoMat, an end-to-end diffusion model fine-tuning strategy equipped with image-to-text concept matching. We identify the two causes of the misalignment problem and propose two key components to explicitly address them. The concept activation module leverages an image-to-text model to supervise the generated image and find out the ignored condition information. It also integrates the fidelity preservation module and mixed latent strategy to maintain the generation capability. Besides, we introduce the attribute concentration module to address the attribute mismapping issue. Through extensive experiments, we have demonstrated that CoMat largely outperforms its baseline model and even surpasses commercial products in multiple aspects. We hope our work can inspire future work on the cause of the misalignment and the solution to it. ", "page_idx": 9}, {"type": "text", "text": "Acknoledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This project is funded in part by National Key RD Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)\u2019s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)   \n[2] Agarwal, A., Karanam, S., Joseph, K., Saxena, A., Goswami, K., Srinivasan, B.V.: A-star: Test-time attention segregation and retention for text-to-image synthesis. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2283\u20132293 (2023)   \n[3] Bakr, E.M., Sun, P., Shen, X., Khan, F.F., Li, L.E., Elhoseiny, M.: Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 20041\u201320053 (2023)   \n[4] Black, K., Janner, M., Du, Y., Kostrikov, I., Levine, S.: Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301 (2023)   \n[5] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9650\u20139660 (2021) [6] Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., Cohen-Or, D.: Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG) 42(4), 1\u201310 (2023)   \n[7] Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., Li, Z.: Pixart- $\\alpha$ : Fast training of diffusion transformer for photorealistic text-to-image synthesis (2023)   \n[8] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 (2023)   \n[9] Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., et al.: Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330 (2024)   \n[10] Chen, L., Wei, X., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Lin, B., Tang, Z., et al.: Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325 (2024)   \n[11] Chen, M., Laina, I., Vedaldi, A.: Training-free layout control with cross-attention guidance. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 5343\u20135353 (2024)   \n[12] Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)   \n[13] Cho, J., Hu, Y., Baldridge, J., Garg, R., Anderson, P., Krishna, R., Bansal, M., Pont-Tuset, J., Wang, S.: Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation. In: ICLR (2024)   \n[14] Clark, K., Vicol, P., Swersky, K., Fleet, D.J.: Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400 (2023)   \n[15] Dahary, O., Patashnik, O., Aberman, K., Cohen-Or, D.: Be yourself: Bounded attention for multi-subject text-to-image generation. arXiv preprint arXiv:2403.16990 (2024)   \n[16] Du, C., Li, Y., Qiu, Z., Xu, C.: Stable diffusion is unstable. Advances in Neural Information Processing Systems 36 (2024)   \n[17] Fan, Y., Watkins, O., Du, Y., Liu, H., Ryu, M., Boutilier, C., Abbeel, P., Ghavamzadeh, M., Lee, K., Lee, K.: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems 36 (2024)   \n[18] Fang, G., Jiang, Z., Han, J., Lu, G., Xu, H., Liang, X.: Boosting text-to-image diffusion models with fine-grained semantic rewards. arXiv preprint arXiv:2305.19599 (2023)   \n[19] Feng, W., He, X., Fu, T.J., Jampani, V., Akula, A., Narayana, P., Basu, S., Wang, X.E., Wang, W.Y.: Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032 (2022)   \n[20] Feng, W., He, X., Fu, T.J., Jampani, V., Akula, A., Narayana, P., Basu, S., Wang, X.E., Wang, W.Y.: Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032 (2022)   \n[21] Guo, Z., Zhang, R., Zhu, X., Tong, C., Gao, P., Li, C., Heng, P.A.: Sam2point: Segment any 3d as videos in zero-shot and promptable manners. arXiv preprint arXiv:2408.16768 (2024)   \n[22] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems 30 (2017)   \n[23] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems 33, 6840\u20136851 (2020)   \n[24] Honnibal, M., Montani, I.: spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing (2017), to appear   \n[25] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)   \n[26] Hu, X., Wang, R., Fang, Y., Fu, B., Cheng, P., Yu, G.: Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135 (2024)   \n[27] Hu, Y., Liu, B., Kasai, J., Wang, Y., Ostendorf, M., Krishna, R., Smith, N.A.: Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 20406\u201320417 (2023)   \n[28] Huang, K., Sun, K., Xie, E., Li, Z., Liu, X.: T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems 36, 78723\u201378747 (2023)   \n[29] Jiang, D., Zhang, R., Guo, Z., Wu, Y., Lei, J., Qiu, P., Lu, P., Chen, Z., Song, G., Gao, P., et al.: Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959 (2024)   \n[30] Jiao, Y., Chen, S., Jie, Z., Chen, J., Ma, L., Jiang, Y.G.: Lumen: Unleashing versatile visioncentric capabilities of large multimodal models. arXiv preprint arXiv:2403.07304 (2024)   \n[31] Kim, W., Son, B., Kim, I.: Vilt: Vision-and-language transformer without convolution or region supervision. In: International conference on machine learning. pp. 5583\u20135594. PMLR (2021)   \n[32] Kim, Y., Lee, J., Kim, J.H., Ha, J.W., Zhu, J.Y.: Dense text-to-image generation with attention modulation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7701\u20137711 (2023)   \n[33] Li, D., Kamko, A., Sabet, A., Akhgari, E., Xu, L., Doshi, S.: Playground v2, [https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic] (https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic)   \n[34] Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., Li, C.: Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895 (2024)   \n[35] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: International conference on machine learning. pp. 12888\u201312900. PMLR (2022)   \n[36] Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems 34, 9694\u20139705 (2021)   \n[37] Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., et al.: Oscar: Object-semantics aligned pre-training for vision-language tasks. In: Computer Vision\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXX 16. pp. 121\u2013137. Springer (2020)   \n[38] Li, Y., Tian, W., Jiao, Y., Chen, J., Jiang, Y.G.: Eyes can deceive: Benchmarking counterfactual reasoning abilities of multi-modal large language models. arXiv preprint arXiv:2404.12966 (2024)   \n[39] Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., Lee, Y.J.: Gligen: Open-set grounded text-to-image generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22511\u201322521 (2023)   \n[40] Li, Y., Keuper, M., Zhang, D., Khoreva, A.: Divide & bind your attention for improved generative semantic nursing. arXiv preprint arXiv:2307.10864 (2023)   \n[41] Lian, L., Li, B., Yala, A., Darrell, T.: Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv preprint arXiv:2305.13655 (2023)   \n[42] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740\u2013755. Springer (2014)   \n[43] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural information processing systems 36 (2024)   \n[44] Liu, N., Li, S., Du, Y., Torralba, A., Tenenbaum, J.B.: Compositional visual generation with composable diffusion models. In: European Conference on Computer Vision. pp. 423\u2013439. Springer (2022)   \n[45] Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems 32 (2019)   \n[46] Ma, B., Zong, Z., Song, G., Li, H., Liu, Y.: Exploring the role of large language models in prompt encoding for diffusion models. arXiv preprint arXiv:2406.11831 (2024)   \n[47] Meral, T.H.S., Simsar, E., Tombari, F., Yanardag, P.: Conform: Contrast is all you need for high-fidelity text-to-image diffusion models. arXiv preprint arXiv:2312.06059 (2023)   \n[48] Phung, Q., Ge, S., Huang, J.B.: Grounded text-to-image synthesis with attention refocusing. arXiv preprint arXiv:2306.05427 (2023)   \n[49] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M\u00fcller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023)   \n[50] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748\u20138763. PMLR (2021)   \n[51] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents. arxiv 2022. arXiv preprint arXiv:2204.06125 (2022)   \n[52] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zeroshot text-to-image generation. In: International conference on machine learning. pp. 8821\u20138831. Pmlr (2021)   \n[53] Rassin, R., Hirsch, E., Glickman, D., Ravfogel, S., Goldberg, Y., Chechik, G.: Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment. Advances in Neural Information Processing Systems 36 (2024)   \n[54] Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., R\u00e4dle, R., Rolland, C., Gustafson, L., et al.: Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024)   \n[55] Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J., Huang, X., Chen, Y., Yan, F., et al.: Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159 (2024)   \n[56] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684\u201310695 (2022)   \n[57] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. pp. 234\u2013241. Springer (2015)   \n[58] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 22500\u201322510 (2023)   \n[59] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems 35, 36479\u201336494 (2022)   \n[60] Shao, H., Qian, S., Xiao, H., Song, G., Zong, Z., Wang, L., Liu, Y., Li, H.: Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. arXiv preprint arXiv:2403.16999 (2024)   \n[61] Shen, D., Song, G., Xue, Z., Wang, F.Y., Liu, Y.: Rethinking the spatial inconsistency in classifier-free diffusion guidance. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9370\u20139379 (2024)   \n[62] Sun, J., Fu, D., Hu, Y., Wang, S., Rassin, R., Juan, D.C., Alon, D., Herrmann, C., van Steenkiste, S., Krishna, R., et al.: Dreamsync: Aligning text-to-image generation with image understanding feedback. arXiv preprint arXiv:2311.17946 (2023)   \n[63] Sun, Z., Shen, S., Cao, S., Liu, H., Li, C., Shen, Y., Gan, C., Gui, L.Y., Wang, Y.X., Yang, Y., et al.: Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525 (2023)   \n[64] Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490 (2019)   \n[65] Wang, F.Y., Huang, Z., Bergman, A.W., Shen, D., Gao, P., Lingelbach, M., Sun, K., Bian, W., Song, G., Liu, Y., Li, H., Wang, X.: Phased consistency model (2024), https://arxiv.org/ abs/2405.18407   \n[66] Wang, F.Y., Wu, X., Huang, Z., Shi, X., Shen, D., Song, G., Liu, Y., Li, H.: Be-your-outpainter: Mastering video outpainting through input-specific adaptation. In: European Conference on Computer Vision. pp. 153\u2013168. Springer (2025)   \n[67] Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., Wang, L.: Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100 (2022)   \n[68] Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O.K., Singhal, S., Som, S., et al.: Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442 (2022)   \n[69] Wang, Z., Xie, E., Li, A., Wang, Z., Liu, X., Li, Z.: Divide and conquer: Language models can plan and self-correct for compositional text-to-image generation. arXiv preprint arXiv:2401.15688 (2024)   \n[70] Wang, Z., Sha, Z., Ding, Z., Wang, Y., Tu, Z.: Tokencompose: Grounding diffusion with token-level supervision. arXiv preprint arXiv:2312.03626 (2023)   \n[71] Wen, S., Fang, G., Zhang, R., Gao, P., Dong, H., Metaxas, D.: Improving compositional text-to-image generation with large vision-language models. arXiv preprint arXiv:2310.06311 (2023)   \n[72] Wu, T.H., Lian, L., Gonzalez, J.E., Li, B., Darrell, T.: Self-correcting llm-controlled diffusion models. arXiv preprint arXiv:2311.16090 (2023)   \n[73] Wu, X., Hao, Y., Zhang, M., Sun, K., Song, G., Liu, Y., Li, H.: Deep reward supervisions for tuning text-to-image diffusion models (2024)   \n[74] Xie, J., Li, Y., Huang, Y., Liu, H., Zhang, W., Zheng, Y., Shou, M.Z.: Boxdiff: Text-toimage synthesis with training-free box-constrained diffusion. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7452\u20137461 (2023)   \n[75] Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems 36 (2024)   \n[76] Xue, Z., Song, G., Guo, Q., Liu, B., Zong, Z., Liu, Y., Luo, P.: Raphael: Text-to-image generation via large mixture of diffusion paths. Advances in Neural Information Processing Systems 36 (2024)   \n[77] Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., Cui, B.: Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv preprint arXiv:2401.11708 (2024)   \n[78] Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., Wu, Y.: Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917 (2022)   \n[79] Yuksekgonul, M., Bianchi, F., Kalluri, P., Jurafsky, D., Zou, J.: When and why vision-language models behave like bags-of-words, and what to do about it? In: The Eleventh International Conference on Learning Representations (2022)   \n[80] Zhang, G., Qu, S., Liu, J., Zhang, C., Lin, C., Yu, C.L., Pan, D., Cheng, E., Liu, J., Lin, Q., et al.: Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327 (2024)   \n[81] Zhang, J., Jiao, Y., Chen, S., Chen, J., Jiang, Y.G.: Eventhallusion: Diagnosing event hallucinations in video llms. arXiv preprint arXiv:2409.16597 (2024)   \n[82] Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., Qiao, Y.: Llama-adapter: Efficient fine-tuning of language models with zero-init attention. ICLR 2024 (2023)   \n[83] Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.W., Gao, P., et al.: Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624 (2024)   \n[84] Zhang, R., Jiang, Z., Guo, Z., Yan, S., Pan, J., Dong, H., Gao, P., Li, H.: Personalize segment anything model with one shot. ICLR 2024 (2023)   \n[85] Zhang, R., Wei, X., Jiang, D., Zhang, Y., Guo, Z., Tong, C., Liu, J., Zhou, A., Wei, B., Zhang, S., et al.: Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739 (2024)   \n[86] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)   \n[87] Zong, Z., Ma, B., Shen, D., Song, G., Shao, H., Jiang, D., Li, H., Liu, Y.: Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046 (2024)   \n[88] Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Wang, J., Wang, L., Gao, J., Lee, Y.J.: Segment everything everywhere all at once. Advances in Neural Information Processing Systems 36 (2024) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we first detail the mixed latent training strategy and then provide the pseudocode for a single loss computation step. ", "page_idx": 16}, {"type": "text", "text": "The mixed latent strategy contains two types of latents in the fine-tuning procedure, i.e., the latent starting from the pure noise and the noisy latent from the GT Images. ", "page_idx": 16}, {"type": "text", "text": "Latent starting from the pure noise This serves as the main branch in our pipeline. Our fine-tuning process shares the same procedure to generate an image as the diffusion model does in the inference time. We uniformly sample $K$ steps from all the inference steps to enable the gradient. Therefore, the latent is sampled from the pure noise ${\\mathcal{N}}(0,I)$ . We iteratively denoise it to obtain the generated image. The image is then used to calculate the $\\mathcal{L}_{i2t}$ and $\\mathcal{L}_{a d v}$ loss. It is also sent to the segmentation model to provide the object mask for computing the $\\mathcal{L}_{p o s}$ and $\\mathcal{L}_{n e g}$ . The latent starting from the pure noise corresponds to the upper left part in Fig. 4. Please refer to [73] for how to receive the gradient from the loss. ", "page_idx": 16}, {"type": "text", "text": "Noisy latent from the GT Images We also aim to inject information from the GT images to stabilize the fine-tuning process. We randomly sample a timestamp $\\tau$ from a pre-defined range $[T_{1},T_{2}]$ . Then we obtain $x_{\\tau}$ by adding the timestamped noise $\\epsilon_{\\tau}$ on the latent of the GT Image $x_{0}$ . We also iteratively denoise this noisy GT latent to get $\\scriptstyle{\\hat{x}}_{0}$ as we do for the latents starting from the pure noise. This $\\scriptstyle{\\hat{x}}_{0}$ is only used to calculate the $\\mathcal{L}_{i2t}$ loss. The latent starting from the noisy GT corresponds to the bottom left part in Fig. 4. ", "page_idx": 16}, {"type": "text", "text": "The pseudocode for a single loss computation step for the online T2I-Model is described below. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 A single loss computation step for the online T2I-Model during fine-tuning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: Text prompt $\\mathcal{P}$ , GT Image $\\mathcal{T}_{g}$ , GT Prompt $\\mathcal{P}_{g}$ , original T2I-Model $\\epsilon_{p r e}$ , online T2I-Model $\\epsilon_{\\theta}$ ,   \npre-trained I2T-Model $\\mathcal{C}$ , discriminator $\\mathcal{D}_{\\phi}$ , segmentation model $\\boldsymbol{S}$ , timestep range $[T_{1},T_{2}]$ , timestep   \n$\\tau$ , attention map $A$ , scaler $\\lambda;[;]$ denotes concatenate   \n1: $x_{T},\\xi\\sim\\mathcal{N}(0,I)$   \n2: $\\tau\\sim\\mathrm{Uniform}[T_{1},T_{2}]$   \n3: x\u03c4 = AddNoise(Ig, \u03be, \u03c4)   \n4: ${\\hat{\\mathcal{T}}},A=0$ GenerateImage $(\\epsilon_{\\theta},x_{T},\\mathcal{P})$   \n5: ${\\hat{\\mathcal{L}}}_{g},\\_=$ GenerateImage $(\\epsilon_{\\theta},x_{\\tau},\\mathcal{P}_{g})$   \n6: $\\mathcal{L}_{i2t}=\\mathrm{ComputeI2TLoss}(\\mathcal{C},\\left[\\hat{\\mathcal{Z}};\\hat{\\mathcal{Z}}_{g}\\right],[\\mathcal{P};\\mathcal{P}_{g}])$   \n7: $\\hat{\\mathcal{T}}_{p r e},\\underline{{{\\underline{{\\mathbf{\\Pi}}}}}}=$ GenerateImage(\u03f5pre, xT , P)   \n8: $\\mathcal{L}_{a d v}=\\mathrm{ComputeAdvLoss}(\\mathcal{D}_{\\phi},\\hat{\\mathcal{L}},\\hat{\\mathcal{Z}}_{p r e})$   \n9: $\\mathcal{L}_{p o s},\\mathcal{L}_{n e g}=\\mathrm{ComputeAttLoss}(S,\\dot{\\mathcal{L}},\\mathcal{P},A)$   \n10: $\\mathcal{L}=\\mathcal{L}_{i2t}+\\mathcal{L}_{p o s}+\\mathcal{L}_{n e g}+\\lambda\\mathcal{L}_{a d v}$   \nOutput: Training loss for the online T2I-Model $\\mathcal{L}$ ", "page_idx": 16}, {"type": "text", "text": "B Additional Results and Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 User preference study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We randomly select 100 prompts from DSG1K [13] and use them to generate images with SDXL [56] and our method (CoMat-SDXL). We ask 5 participants to assess both the image quality and text-image alignment. Human raters are asked to select the superior respectively from the given two synthesized images, one from SDXL, and another from our CoMat-SDXL. For fairness, we use the same random seed for generating both images. The voting results are summarised in Fig. 8. Our CoMat-SDXL greatly enhances the alignment between the prompt and the image without sacrificing the image quality. ", "page_idx": 16}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/d752ca936bcd499600f3ae07ce5c7a960c8e9530b4b0c3eb19a5b1748318f590.jpg", "img_caption": ["Figure 8: User preference study results. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Composability with planning-based methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Since our method is an end-to-end fine-tuning strategy, we demonstrate its flexibility in the integration with other planning-based methods, where combining our method also yields superior performance. RPG [77] is a planning-based method utilizing Large Language Model (LLM) to generate the description and subregion for each object in the prompt. We refer the reader to the original paper for details. We employ SDXL and our CoMat-SDXL as the base model used in [77] respectively. As shown in Fig. 9, even though the layout for the generated image is designed by LLM, SDXL still fails to faithfully generate the single object aligned with its description, e.g., the wrong mat color and the missing candle. Although the planning-based method generates the layout for each object, it is still bounded by the base model\u2019s condition following capability. Combining our method can therefore perfectly address this issue and further enhance alignment. ", "page_idx": 17}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/16e594b58f683fd00daf13c5309958a25e55be8eefb68b3b3dbb42ed5f248872.jpg", "img_caption": ["Figure 9: Pipeline for integrating CoMat-SDXL with planning-based method. CoMat-SDXL correctly generates the green mat in the upper row and the tall white candle in the bottom row. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 How to choose an image-to-text model? ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide a further analysis of the varied performance improvements observed with different image-to-text models, as shown in Table 5 of the main text. ", "page_idx": 18}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/67635db738721fbef18e700402df560b44477be4f14ec07e815ab6cd5342912a.jpg", "img_caption": ["Figure 10: Examples for the three core sensitivities. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "For an image-to-text model to be valid for the concept activation module, it should be able to tell whether each concept in the prompt appears and appears correctly. We construct a test set to evaluate this capability of the image-to-text model. Intuitively, given an image, a qualified image-to-text model should be sensitive enough to the prompts that faithfully describe it against those that are incorrect in certain aspects. We study three core demands for an image-to-text model: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Attribute sensitivity. The image-to-text model should distinguish the noun and its corresponding attribute. The corrupted caption is constructed by switching the attributes of the two nouns in the prompt.   \n\u2022 Relation sensitivity. The image-to-text model should distinguish the subject and object of a relation. The corrupted caption is constructed by switching the subject and object.   \n\u2022 Quantity sensitivity. The image-to-text model should distinguish the quantity of an object. Here we only evaluate the model\u2019s ability to tell one from many. The corrupted caption is constructed by turning singular nouns into plural or otherwise. ", "page_idx": 18}, {"type": "text", "text": "We assume that they are the basic requirements for an image-to-text model model to provide valid guidance for the diffusion model. Besides, we also choose images from two domains: real-world images and synthetic images. For real-world images, we randomly sample 100 images from the ARO benchmark [79]. As for the synthetic images, we use the pre-trained SD1.5 [56] and SDXL [49] to generate 100 images according to the prompts in T2ICompBench [28]. These selections make up for the 200 images in our test data. We show the examples in Fig. 10. ", "page_idx": 18}, {"type": "text", "text": "For the sensitivity score, we compare the difference between the alignment score (i.e., log-likelihood) of the correct and corrupted captions for an image. Given the correct caption $\\mathcal{P}$ and corrupted caption $\\mathcal{P}^{\\prime}$ corresponding to image $\\mathcal{T}$ , we compute the sensitivity score $\\boldsymbol{S}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\nS=\\frac{\\log(p_{\\mathcal{C}}(\\mathcal{P}|\\mathcal{Z}))-\\log(p_{\\mathcal{C}}(\\mathcal{P}^{\\prime}|\\mathcal{Z}))}{|\\log(p_{\\mathcal{C}}(\\mathcal{P}|\\mathcal{Z}))|}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we take the mean value of all the images in the test set. The result is shown in Table 7. The rank of the sensitivity score aligns with the rank of the gains brought by the image-to-text model model shown in the main text. Hence, except for the parameters, we argue that sensitivity is also a must for an image-to-text model to function in the concept activation module. ", "page_idx": 18}, {"type": "table", "img_path": "OW1ldvMNJ6/tmp/a5cba2319a57fc524ff146f58a4ca10b3b58950c5f551e2b37739f50d2ef484c.jpg", "table_caption": ["Table 7: Statistics of image-to-text models. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C More Related Work ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The image-to-text model in the main text refers to the models capable of image captioning. Previous image captioning models are pre-trained on various vision and language tasks (e.g., image-text matching, (masked) language modeling) [37, 45, 31, 64], then fine-tuned with image captioning tasks [12]. Various model architectures have been proposed [68, 78, 36, 35, 67]. BLIP [35] takes a fused encoder architecture, while GIT [67] adopts a unified transformer architecture. Recently, multimodal large language models (MLLMs) have been flourishing [43, 86, 82, 1, 87, 60, 30]. Empowered by the strong language ability of large language models (LLMs) [82, 80], MLLMs are capable of various vision-language tasks like detailed image captioning [43, 86, 63, 8], visual question answering [38, 83, 85, 9, 29], etc. LLaVA [43, 34, 34] is one of the representative MLLMs. When prompted properly, it can generate elaborate image captions. ", "page_idx": 19}, {"type": "text", "text": "Similar to our work, [18] proposes to caption the generated images and optimize the coherence between the produced captions and text prompts. Although an image-to-text model is also involved, they fail to provide detailed guidance. It has been shown that the generated captions are prone to omit key concepts and involve undesired added features [27]. Besides, the method leverages a pre-trained text encoder to compute the similarity between the prompt and generated caption, which further causes information to be missed during text encoding. All these designs lead the optimization target to be vague and sub-optimal. ", "page_idx": 19}, {"type": "text", "text": "C.1 vs. Differentiable Reward Method ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Similarity: Our method is inspired by the technique introduced in the differentiable reward method to perform gradient update. ", "page_idx": 19}, {"type": "text", "text": "Difference: (1) Reward Model. Our method is the first to leverage an image-to-text model to perform image captioning on the generated image and compute the loss on the caption. (2) No fidelity preservation. The current differentiable reward method ignores the aspect of preserving the generation capability if not training against a reward of image quality. Our method introduces a novel fidelity preservation module, which utilizes a discriminator with similar knowledge to preserve the generation capability. This greatly alleviates the reward hacking problem introduced by only training with the differentiable reward method. (3) No guidance from real-world image. The current differentiable reward method all starts from pure noise. Since our method is optimizing for alignment, we can incorporate real-world image-text pairs to guide the optimization process. With our mixed latent strategy, the latent starting from the noise is conditioned on the difficult prompt to promote alignment, while the latent starting from the noisy GT image is used to prohibit the diffusion model from overfitting to the image-to-text model. ", "page_idx": 19}, {"type": "text", "text": "C.2 vs. TokenCompose [70] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Similarity: Both [70] and our method incorporates the object mask to guide the attention of the diffusion model. ", "page_idx": 19}, {"type": "text", "text": "Difference: (1) Limited and inferior optimizing target. [70] merely focuses on optimizing the consistency between the noun mask and the object mask. However, as shown in Fig. 5, the attention mask of the noun (the \u2018bear\u2019 token) has already aligned well with the object mask. Optimizing for this consistency is inferior. On the other hand, our method focuses on a much broader area, i.e., entity tokens, which consist of nouns and their various associated attributes. We also find that the consistency between the attributes and the object mask bears very little similarity, which should be paid more attention. (2) No negative concept mapping. Since the training data of [70] is the real-world image-text pairs, all the nouns in the prompt show up in the image. However, this prohibits the model from learning in a negative way, i.e., if the entity is not on the image, none of the pixel should be activated by this token. Our method leverages images generated by the diffusion model. The entity missing is common. The model obtains the chance to learn in a negative way. (3) No difficult training data. Another issue caused by training with image-text pairs is that the training data may be of a common scenario, which is easier to learn. Since our method does not need real-world images and only starts from the noise and text prompt, this enables a more efficient training process. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "C.3 vs. Class-specific Prior Preservation Loss [58] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Similarity: Both the class-specific prior preservation loss (CPP Loss) [58] and our proposed fidelity preservation module (FP) share the similar high-level idea of preserving the generation quality while fine-tuning the diffusion models. ", "page_idx": 20}, {"type": "text", "text": "Difference: (1) Target task and preserve domain. [58] seeks to personalize image generation for specific objects. While the introduced CPP Loss primarily maintains generative capabilities within a narrow domain\u2014specifically, the object class present in the training data\u2014our proposed FP module operates within the context of text-image alignment. FP aims to preserve general generative capabilities by computing adversarial loss across the entire training dataset, encompassing a diverse range of text prompts. (2) Methodology. Since the training data of [58] finetunes the diffusion model with the pretraining loss, i.e., the squared error denoising loss on a certain timestamp. CPP Loss follows its form. In contrast, our fine-tuning procedure simulates the inference process of the diffusion model to conduct a full-step inference. We aim to directly supervise the generated image to achieve the training-test alignment. Therefore, we propose the novel FP module to leverage a discriminator to adversarially preserve its quality. The applied discriminator is also updated along with the fine-tuning process, enabling finer control of the image quality. ", "page_idx": 20}, {"type": "text", "text": "D Future Work ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We believe our work can also be applied in the text-to-video diffusion models. With the introduction of various MLLMs handling videos [81, 10] and video segmentation models [54, 21], both our concept activation and attribute concentration modules could be used for text-video-alignment training. ", "page_idx": 20}, {"type": "text", "text": "E Experimental Setup ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Implementation Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Training Details. In our method, we inject LoRA [25] layers into the UNet of the online training model and discriminator and keep all other components frozen. For both SDXL and SD1.5, we train 2,000 iters on 8 NVIDIA A100 GPUS. We use a local batch size of 6 for SDXL and 4 for SD1.5. We choose Grounded-SAM [55] from other open-vocabulary segmentation models [84, 88]. The DDPM [23] sampler with 50 steps is used to generate the image for both the online training model and the original model. In particular, we follow [73] and only enable gradients in 5 steps out of those 50 steps, where the attribute concentration module would also be operated. Besides, to speed up training, we use training prompts to generate and save the generated latents of the pre-trained model in advance, which are later input to the discriminator during fine-tuning. ", "page_idx": 21}, {"type": "text", "text": "Training Resolutions. We observe that training SDXL is very slow due to the large memory overhead at $1024\\times1024$ . However, SDXL is known to generate low-quality images at resolution $512\\times512$ This largely affects the image understanding of the image-to-text model. So we first equip the training model with better image generation capability at $512\\times512$ . We use our training prompts to generate $1024\\times1024$ images with pre-trained SDXL. Then we resize these images to $512\\times512$ and use them to fine-tune the UNet of SDXL for 100 steps, after which the model can already generate high-quality $512\\times512$ images. We continue to implement our method on the fine-tuned UNet. ", "page_idx": 21}, {"type": "text", "text": "Training Layers for Attribute Concentration. Following [70], only cross-attention maps in the middle blocks and decoder blocks are used to compute the loss. ", "page_idx": 21}, {"type": "text", "text": "Hyperparameters Settings. We provide the detailed training hyperparameters in Table 8. ", "page_idx": 21}, {"type": "text", "text": "Table 8: CoMat training hyperparameters for SD1.5 and SDXL ", "page_idx": 21}, {"type": "table", "img_path": "OW1ldvMNJ6/tmp/e1a4a319db954a58e2392b3b632b3256aabf96beb28081d3fe1057d74d4bdb07.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "F More Qualitative Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Effectiveness of the Fidelity Preservation module (FP) and Mixed Latent (ML) strategy ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We visualize the effectiveness of how we preserve the generation capability of the diffusion model in Fig. 7. As shown in the figure, without any preservation technique, the diffusion model generates misshaped envelopes and swans. With the FP and ML applied, the diffusion model generates images aligned with the prompt and without artifacts. ", "page_idx": 22}, {"type": "text", "text": "F.2 Comparison with the baseline model ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We showcase more comparison results between our method with the baseline model in Fig. 11 to 14. Fig. 11 shows the generation results with long and complex prompts. Fig. 12 to 14 shows that our method solves various problems of misalignment, including object missing, incorrect attribute binding, incorrect relationship, inferior prompt understanding. ", "page_idx": 22}, {"type": "text", "text": "CoMat-SDXL ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "SDXL ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "CoMat-SDXL ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/7b2133c17f4e4df4a0fb05cc571a77020d9e54013b548c9a833c06ad14760ef0.jpg", "img_caption": ["In the middle of a cozy room with a vintage charm, a circular wooden dining table takes the stage, its surface adorned with a decorative vase and a few scattered books. The room's warmth is maintained by an oldfashioned radiator humming steadily in the corner, a testament to its long service. As dusk approaches, the waning sunlight softly permeates the space through a window with a delicate frost pattern, casting a gentle glow that enhances the room's rustic ambiance. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/935e010f94e373c51c61b173b0efbed1cff519cf00778c42533d2e1e21184978.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A dining room setting showcasing an unusually large red bell pepper with a shiny, slightly wrinkled texture, prominently placed beside a diminutive golden medal with a red ribbon on a polished wooden dining table. The pepper's vibrant hue contrasts with the medal's gleaming surface. The scene is composed in natural light, highlighting the intricate details of the pepper's surface and the reflective quality of the medal. ", "page_idx": 23}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/a877a6442b84db838e3cef9d40006df5bf8d70464a2d54c74708ec377323f831.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Inside a dimly lit room, the low luminance emanates from a bedside lamp casting a soft glow upon the nightstand. There lies a travel magazine, its pages open to a vivid illustration of a car driving along a picturesque landscape. Positioned next to the image is a light pink toothbrush, its bristles glistening in the ambient light. Beside the magazine, the textured fabric of the bedspread is just discernible, contributing to the composed and quiet scene. ", "page_idx": 23}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/0994f493b860bdb897dd156a285e718aad4af18d45ef2d142733a7c48ed63b39.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A brightly colored hot air balloon with vibrant stripes of red, yellow, and   \nblue hangs in the clear sky, its large round shape contrasting against the fluffy white clouds. Below it, a sleek black scooter with red accents   \nspeeds along a concrete pathway, its rider leaning forward in a hurry. The balloon moves at a leisurely pace, starkly contrasting with the frenetic energy of the scooter's rapid movement on the ground. ", "page_idx": 23}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/b6f176961e5f126fba712818d763c02c18ce4a1294e044079c335faff263ef97.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "On a reflective metallic table, there is a brightly colored handbag featuring a floral pattern next to a freshly sliced avocado, its green flesh and brown pit providing a natural contrast to the industrial surface. The table is set for lunch, with silverware and a clear glass water bottle positioned neatly beside the avocado. The juxtaposition of the colorful fashion accessory and the rich texture of the avocado creates a striking visual amidst the midday meal setting. ", "page_idx": 23}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/535c46043f2d853b0318159190c3c7cfd89e456b3a6a33cbd7f0a05fd303a1a6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A deep red rose with plush petals sits elegantly coiled atop an ivory, intricately patterned lace napkin. The napkin rests on a rustic wooden table that contributes to the charming garden setting. As the late evening sun casts a warm golden hue over the area, the shadows of surrounding foliage dance gently around the rose, enhancing the romantic ambiance. Nearby, the green leaves of the garden plants provide a fresh and verdant backdrop to the scene. ", "page_idx": 23}, {"type": "text", "text": "Figure 11: More Comparisons between SDXL and CoMat-SDXL on complex prompts. All pairs are generated with the same random seed. ", "page_idx": 23}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/1cac176f4e20c1f313077ea15cf151c8dd9ce0861573065ebba1bcf3079ace37.jpg", "img_caption": ["Figure 12: More Comparisons between SDXL and CoMat-SDXL. All pairs are generated with the same random seed. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/4f9c904c9f6a83cb118d4716070184426462087ff7897e6a04d3e237e3928059.jpg", "img_caption": ["Figure 13: More Comparisons between SDXL and CoMat-SDXL. All pairs are generated with the same random seed. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "OW1ldvMNJ6/tmp/965569ee98e542c69d90142c0faea489cc5cfe1328317b6ccd3ea64ed2414673.jpg", "img_caption": ["Figure 14: More Comparisons between SDXL and CoMat-SDXL. All pairs are generated with the same random seed. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction accurately reflect this paper\u2019s contributions and scope. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We discuss the limitations of the work in Appendix ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Please see Section Experiments and Section Appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We will provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Please see Section Experiments and Section Appendix. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We conduct experiments only once and report the accuracy of the best model, and it would be too computationally expensive to conduct the pre-training multiple times. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Please see Section Appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Please see Section Appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Please see Section Experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Please see supplementary material. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]