[{"figure_path": "OoOCoZFVK3/tables/tables_12_1.jpg", "caption": "Table 1: Hyperparameters in IMDB Review", "description": "This table details the hyperparameters used for fine-tuning GPT-2 models on the IMDB Review dataset using both the single-agent Proximal Policy Optimization (PPO) and the proposed Cooperative Multi-Agent Reinforcement Learning (CORY) methods. It lists hyperparameters such as learning rate, number of epochs, batch size, gradient accumulation steps, initial KL coefficient, discount factor, GAE parameter, gradient clipping range, value clipping range, value loss coefficient, and the period of role exchange (only applicable to CORY). These settings are crucial for achieving the balance between maximizing task reward and minimizing KL divergence during the training process.", "section": "A.1 Hyperparameters"}, {"figure_path": "OoOCoZFVK3/tables/tables_12_2.jpg", "caption": "Table 2: Hyperparameters in GSM8K", "description": "This table lists the hyperparameters used in the GSM8K experiments for both PPO and CORY.  It shows the values for parameters like learning rate, batch size, gradient accumulation steps, and iterations.  Noteworthy is the addition of the `Period of role exchange (TREx)` parameter, specific to the CORY method, indicating the frequency of role swapping between the two LLM agents during training.  The consistent settings for many parameters across both methods highlights a controlled experimental design, aiming to isolate the impact of the CORY methodology.", "section": "A.1 Hyperparameters"}, {"figure_path": "OoOCoZFVK3/tables/tables_16_1.jpg", "caption": "Table 3: Examples of IMDB Review. GPT2-Large is fine-tuned with PPO and CORY respectively.", "description": "This table shows three examples of IMDB movie review snippets. For each snippet, the original GPT2-Large generated text, the PPO fine-tuned model generated text, and the CORY fine-tuned model generated text are compared.  The goal is to generate positive sentiment continuations of the given snippets. The table highlights the differences in the generated texts by the different models, illustrating the impact of the fine-tuning methods on the model's sentiment generation capability.  Specifically, it demonstrates how CORY avoids distribution collapse, generating more consistently positive and grammatically correct sentences compared to PPO.", "section": "4.1 Subjective Rewards on IMDB Review"}, {"figure_path": "OoOCoZFVK3/tables/tables_17_1.jpg", "caption": "Table 3: Examples of IMDB Review. GPT2-Large is fine-tuned with PPO and CORY respectively.", "description": "This table shows examples of IMDB reviews generated by GPT-2 Large models fine-tuned using PPO and CORY.  It highlights the differences in the generated text, illustrating that the CORY approach results in more positive and grammatically correct sentences compared to PPO.", "section": "4.1 Subjective Rewards on IMDB Review"}, {"figure_path": "OoOCoZFVK3/tables/tables_18_1.jpg", "caption": "Table 3: Examples of IMDB Review. GPT2-Large is fine-tuned with PPO and CORY respectively.", "description": "This table presents examples of movie review sentences generated by GPT-2 Large models fine-tuned using both Proximal Policy Optimization (PPO) and the proposed Coevolving with the Other You (CORY) method.  The goal is to complete the provided sentence snippet in a positive direction.  The table demonstrates the differences in the generated text resulting from each fine-tuning approach, highlighting the impact of the method on text generation quality and sentiment.", "section": "4.1 Subjective Rewards on IMDB Review"}]