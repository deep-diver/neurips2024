[{"type": "text", "text": "Statistical-Computational Trade-offs for Density Estimation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anders Aamand Alexandr Andoni Justin Y. Chen University of Copenhagen Columbia University MIT aamand@mit.edu andoni@cs.columbia.edu justc@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Piotr Indyk Shyam Narayanan Sandeep Silwal MIT Citadel Securities\u2217 UW-Madison ndyk@mit.edu shyam.s.narayanan@gmail.com silwal@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Haike Xu   \nMIT   \nhaikexu@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the density estimation problem defined as follows: given $k$ distributions $p_{1},\\ldots,p_{k}$ over a discrete domain $[n]$ , as well as a collection of samples chosen from a \u201cquery\u201d distribution $q$ over $[n]$ , output $p_{i}$ that is \u201cclose\u201d to $q$ . Recently [1] gave the first and only known result that achieves sublinear bounds in both the sampling complexity and the query time while preserving polynomial data structure space. However, their improvement over linear samples and time is only by subpolynomial factors. ", "page_idx": 0}, {"type": "text", "text": "Our main result is a lower bound showing that, for a broad class of data structures, their bounds cannot be significantly improved. In particular, if an algorithm uses $O(n/\\log^{c}k)$ samples for some constant $c>0$ and polynomial space, then the query time of the data structure must be at least $k^{1-O(1)/\\log\\log k}$ , i.e., close to linear in the number of distributions $k$ . This is a novel statistical-computational trade-off for density estimation, demonstrating that any data structure must use close to a linear number of samples or take close to linear query time. The lower bound holds even in the realizable case where $q=p_{i}$ for some $i$ , and when the distributions are flat (specifically, all distributions are uniform over half of the domain $[n])$ . We also give a simple data structure for our lower bound instance with asymptotically matching upper bounds. Experiments show that the data structure is quite efficient in practice. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The general density estimation problem is defined as follows: given $k$ distributions $p_{1},\\ldots,p_{k}$ over a domain $[n]^{2}$ , build a data structure which when queried with a collection of samples chosen from a \u201cquery\u201d distribution $q$ over $[n]$ , outputs $p_{i}$ that is \u201cclose\u201d to $q$ . An ideal data structure reports the desired $p_{i}$ quickly given the samples (i.e., has fast query time), uses few samples from $q$ (i.e., has low sampling complexity) and uses little space. ", "page_idx": 0}, {"type": "text", "text": "In the realizable case, we know that $q$ is equal to one of the distributions $p_{j}$ , $1\\le j\\le k$ , and the goal is to identify a (potentially different) $p_{i}$ such that $\\lVert q-p_{i}\\rVert_{1}\\leq\\epsilon$ for an error parameter $\\epsilon>0$ . In the more general agnostic case, $q$ is arbitrary and the goal is to report $p_{i}$ such that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\|q-p_{i}\\|_{1}\\leq C\\cdot\\operatorname*{min}_{j}\\|q-p_{j}\\|_{1}+\\epsilon\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "for some constant $C>1$ and error parameter $\\epsilon>0$ . The problem is essentially that of non-parametric learning of a distribution $q\\in{\\mathcal{F}}$ , where the family $\\mathcal{F}=\\bar{\\{}p_{1},}\\ldots...\\,p_{k}\\}$ has no structure whatsoever. Its statistical complexity was understood already in [17], and the more modern focus has been on the structured case (when $\\mathcal{F}$ has additional properties). Surprisingly, its computational complexity is still not fully understood. ", "page_idx": 1}, {"type": "text", "text": "Due to its generality, density estimation is a fundamental problem with myriad applications in statistics and learning distributions. For example, the framework provides essentially the best possible sampling bounds for mixtures of Gaussians [10, 18, 12]. The framework has also been studied in private [8, 7, 13, 15] and low-space [4] settings. ", "page_idx": 1}, {"type": "table", "img_path": "PtD4aZPzcR/tmp/028a3f0811a44ddb813d9b914940a24901096b98d40c1e02d3837068479322f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Table 1: Prior work and our results. For simplicity the results stated only for the realizable case, constant $\\epsilon>0$ , and with $O(\\cdot)$ factors suppressed. The bound of [1] (row 6 of the table) is stated as in Theorem 3.1 of that paper. However, by adjusting the free parameters, their algorithm can be easily generalized to use $n/s$ samples for $n/\\bar{s}\\,>\\,\\bar{n7}\\mathrm{polylog}(\\bar{n})$ , resulting in a query time of $O(n+k^{1-\\Omega(\\epsilon^{2})/s})$ . Note that the term $1-1/s$ in their bound results in a larger exponent than $1-1/\\log s$ in our upper bound. Furthermore, our algorithm is correct as long as $n/s\\,\\dot{\\gg}\\,\\log k/\\varepsilon^{2}$ which is the information theoretic lower bound. ", "page_idx": 1}, {"type": "text", "text": "Table 1 summarizes known results as well as our work. As seen in the table, the data structures are subject to statistical-computational trade-offs. On one hand, if the query time is not a concern, logarithmic in $k$ samples are sufficient [17, 11]. On the other hand, if the sampling complexity is not an issue, then one can use the algorithm of [16] to learn the distribution $\\hat{q}$ such that $\\lVert\\hat{q}-q\\rVert_{1}\\leq\\epsilon/2$ , and then deploy standard approximate nearest neighbor search algorithms with sublinear query time, e.g., from [14]. Unfortunately both of these extremes require either linear (in $n$ ) sample complexity, or linear (in $k$ ) query time time. Thus, achieving the best performance with respect to one metric resulted in the worst possible performance on the other metric. ", "page_idx": 1}, {"type": "text", "text": "The first and only known result that obtained non-trivial improvements to both the sampling complexity and the query time is due to a recent work of [1]. Their improvements, however, were quite subtle: the sample complexity was improved by a sub-logarithmic factor, while the query time was improved by a sub-polynomial factor of k1/(log k)1/4= 2log(k)3/4. ", "page_idx": 1}, {"type": "text", "text": "This result raises the question of whether further improvements are possible. In particular, [1] asks: To what extent can our upper bounds of query and sample complexity be improved? What are the computational-statistical tradeoffs between the sample complexity and query time? These are the questions that we address in this paper. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Lower bound: We give the first limit to the best possible tradeoff between the query time and sampling complexity, demonstrating a novel statistical-computational tradeoff for a fundamental problem. To our knowledge, this is the first statistical-computational trade-off for a data structures problem\u2013if we allow for superpolynomial space, logarithmic sampling and query complexity is possible. As in [5, 6, 9, 3], we focus on data structures that operate in the so-called list-ofpoints model3, which captures all bounds depicted in Table 1. Suppose that the query time of the data structure is of the form $k^{\\rho_{q}}$ . We show that, if the data structure is allowed polynomial space $k n\\!+\\!k^{1+\\rho_{u}}$ and uses $n/s$ samples, then the query time exponent $\\rho_{q}$ must be at least $1\\bar{-}O(\\rho_{u})/\\bar{\\log{s}}$ Therefore, if we set $s=\\log^{c}k$ for some $c>0$ , as in [1], then the query time of the data structure must be at at least $k^{1-O(1)/\\log\\log k}$ . That is, the query time can be improved over linear time in $k$ by at most a factor of $k^{O(1)/\\log\\log k}=2^{O(\\log k/\\log\\log k)}$ . This shows that it is indeed not possible to improve the linear query time by a polynomial factor while keeping the sampling complexity below $n/\\log^{c}n$ , for any constant $c>0$ . ", "page_idx": 1}, {"type": "image", "img_path": "PtD4aZPzcR/tmp/b5bc631eaeb603479083e692456c061cdc9d7fc67f802ea017a946285f3294de.jpg", "img_caption": ["Figure 1: Left: Trade-off between $1/s$ (samples as a fraction of $n$ ) and the query time exponent $\\rho_{q}$ for our algorithm for half-uniform distributions (solid green curve), the algorithm by [1] for general distributions (dashed green curve), our analytic lower bound (solid red curve), and a numerical evaluation of the bound from Theorem 3.2 (dashed black curve). We have fixed the space parameter $\\rho_{u}=1/2$ . The plots illustrate the asymptotic behaviour proven in Theorem 3.1 and Theorem 4.2 that as $s\\to\\infty$ , $\\bar{\\rho}_{q}=1-\\Theta(1/\\log s)$ both in the lower bound and for our algorithm for half-uniform distributions. Right: The same plot zoomed in to the upper left corner with $1/s$ on log-scale. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our lower bound instance falls within the realizable case and in the restricted setting where the data and query distributions are \u201chalf-uniform\u201d, i.e. each distribution is uniform over half of the universe $[n]$ . Note that showing a lower bound under these restrictions automatically extends to the agnostic case with general distributions, as the former is a special case of the latter. ", "page_idx": 2}, {"type": "text", "text": "Our construction takes lower bounds by [3] for set similarity search as a starting point. We adapt their lower bound to our setting in which queries are formed via samples from a distribution. The resulting lower bound is expressed via a complicated optimization problem and does not yield a closed-form statistical-computational trade-off. One of our technical contributions is solve this optimization problem for a regime of interest to get our explicit lower bound. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Upper bound: We complement the lower bound by demonstrating a data structure for our hard instances (in the realizable case with half-uniform distributions) achieving sampling and query time bounds asymptotically matching those of the lower bound. We note that the existence of such an algorithm essentially follows from [3]. However, the algorithms presented there are quite complex. In contrast, our algorithm can be viewed as a \u201cbare-bones\u201d version of their approach, and as a result it is simple and easy to implement. To demonstrate the last point, we present an empirical evaluation of the algorithm on the synthetic data set from [1], and compare it to the algorithm from that paper, as well as a baseline tailored to half-uniform distributions. Our faster algorithm achieves over ${\\bf6}\\times$ reduction in the number of operations needed to correctly answer 100 random queries. In Figure 1, we illustrate the trade-off between the number of samples and the query time exponent $\\rho_{q}$ in our upper and lower bounds. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Open Questions: The direct question left open by our work is whether there exists a data structure whose upper bounds for general distributions match our lower bounds (note we give matching upper bounds for half-uniform distributions). [1] give an algorithm for the general case, but with a worse trade-off than that described by our lower bound. More generally, are there other data structures problems for which one can show statistical-computational tradeoffs between the trifecta of samples, query time, and space? ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries and Roadmap for the Lower Bound ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "First we introduce helpful notation used throughout the paper. ", "page_idx": 3}, {"type": "text", "text": "Notation: We use ${\\mathrm{Bern}}(p)$ to denote the Bernoulli $(p)$ distribution and $\\mathrm{Poi}(\\lambda)$ to denote the $\\mathrm{Poisson}(\\lambda)$ distribution. For a discrete distribution $f:X\\to\\mathbb{R}$ , we use $s u p p(f)=\\left\\{x\\in X:f(x)\\neq\\right.$ $0\\}$ to denote $f$ \u2019s support and $|s u p p(f)|$ to denote the size of its support. We use $f^{n}$ to denote the tensor product of $n$ identical distribution $f$ . We call a distribution $f$ half-uniform if it is a uniform distribution on its support $T$ with $|T|=n/2$ . For a binary distribution $P$ supported on $\\{0,1\\}$ with a random variable $x\\sim P$ , we sometimes explicitly write $P=\\Big[\\mathbb{P}[x=1]\\Big]$ . Similarly, for a joint distribution $P Q$ over $\\{0,1\\}^{2}$ with $(x,y)\\sim P Q$ , we write $P Q=\\left[\\mathbb{P}[x=1,y=1]\\quad\\mathbb{P}[x=1,y=0]\\right].$ For a vector $\\boldsymbol{x}\\in\\mathbb{R}^{n}$ , we use $x[i]$ to denote its $i$ -th coordinate. We use $\\begin{array}{r}{d(p||q)\\;=\\;p\\log\\frac{p}{q}\\;+}\\end{array}$ (1 \u2212p) log 11\u2212\u2212qp and $\\begin{array}{r}{D(P||Q)=\\sum_{p\\in P}p\\log{\\frac{p}{q}}}\\end{array}$ to denote the standard KL-divergence over a binary distribution or a general discrete distribution. KL divergence $D(P||Q)$ is only finite when $s u p p(P)\\subseteq$ $s u p p(Q)$ , also denoted as $P\\ll Q$ . All logarithms are natural. ", "page_idx": 3}, {"type": "text", "text": "We now introduce the main problem which we use to prove our statistical-computational lower bound.   \nWe state a version which generalizes half-uniform distributions. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Uniform random density estimation problem). For a universe $U\\,=\\,\\{0,1\\}^{n}$ , we generate the following problem instance: ", "page_idx": 3}, {"type": "text", "text": "1. A dataset $P$ is constructed by sampling $k$ uniform distributions, where for each uniform distribution $p\\in P$ , every element $i\\in[n]$ is contained in $p$ \u2019s support with probability $w_{u}$ . 2. Fix a distribution $p^{*}\\in P$ , take $\\begin{array}{r}{\\mathrm{Poi}\\left(\\frac{|s u p p(p^{*})|}{s\\cdot w_{u}}\\right)}\\end{array}$ samples from $p^{*}$ and get a query set $q$ . 3. The goal of the data structure is to preprocess $P$ such that when given the query set $q$ , it recovers the distribution $p^{*}$ . ", "page_idx": 3}, {"type": "text", "text": "We denote this problem as $\\mathrm{URDE}(w_{u},s)$ . URDE abbreviates Uniform Random Density Estimation. The name comes from the fact that the data set distributions are uniform over randomly generated supports. In Section 3, we prove a lower bound for URDE by showing that a previously studied \u2018hard\u2019 problem can be reduced to URDE. The previously studied hard problem is the GapSS problem. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Random GapSS problem [3]). For a universe $U=\\{0,1\\}^{n}$ and parameters $0<w_{q}<$ $w_{u}<1$ , let distribution $P_{U}=\\mathrm{Bern}(w_{u})^{n}$ , $P_{Q}=\\mathrm{Bern}(w_{q})^{n}$ , and $P_{Q U}=\\left[{\\omega_{u}}^{w_{q}}{w_{q}}\\;\\;\\;\\;\\;{0}_{w u}\\right]^{r}$ . A random $\\mathrm{GapSS}(w_{u},w_{q})$ problem is generated by the following steps: ", "page_idx": 3}, {"type": "text", "text": "1. A dataset $P\\subseteq U$ is constructed by sampling $k$ points where $p\\sim P_{U}$ for all $p\\in P$ . 2. A dataset point $p^{*}\\in P$ is fixed and a query point $q$ is sampled such that $(q,p^{*})\\sim P_{Q U}$ . 3. The goal of the data structure is to preprocess $P$ such that it recovers $p^{*}$ when given the query point $q$ . ", "page_idx": 3}, {"type": "text", "text": "We denote this problem as random $\\mathrm{GapSS}(w_{u},w_{q})$ . GapSS abbreviates Gap Subset Search. To provide some intuition about how GapSS relates to URDE, let us denote the data set $P=\\{p_{1},\\dots,p_{k}\\}$ . Then the $p_{i}\\in\\{0,1\\}^{n}$ can naturally be viewed as $k$ independently generated random subsets of $[n]$ . For each $i$ , $p_{i}$ includes each element of $[n]$ with probability $w_{u}$ . The query point $q$ can similarly be viewed as a random subset of $[n]$ including each element with probability $w_{q}$ , but it is correlated with some fixed $p^{*}\\in P$ . Namely, $p^{*}$ and $q$ are generated according to the join distribution $P_{Q U}$ (with the right marginal distributions $P_{Q}$ and $P_{U}$ ) such $q$ a subset of $p^{*}$ . The goal in GapSS is to identify $p^{*}$ given $q$ . This intuition is formalized in Section 3. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Our main goal is to study the asymptotic behavior of algorithms with sublinear samples, or specifically, the query time and memory trade-off when only sublinear samples are available, so all our theorems assume the setting that both the support size $n$ and the number of samples $k$ goes to infinity and $n\\ll k\\leq\\mathrm{poly}(n)$ . Sublinear samples mean that $\\textstyle{\\frac{1}{s}}<o(1)$ as $n$ goes to infinity. ", "page_idx": 4}, {"type": "text", "text": "Our lower bound extend and generalize lower bounds for GapSS in the \u2018List-of-points\u2019 model. Thus, the lower bound we provide for URDE is also in the \u201cList-of-points\u201d model defined below (slightly adjusted from the original definition in [5] to our setting). The model captures a large class of data structures for retrieval problems such as partial match and nearest neighbor search: where one preprocesses a dataset $P$ to answer queries $q$ that can \u201cmatch\u201d a point in the dataset. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.3 (List-of-points model). Fix a universe $Q$ of queries, a universe $U$ of dataset points, as well as a partial predicate $\\phi:Q\\times S\\rightarrow\\{0,1,\\bot\\}$ . We first define the following $\\phi$ -retrieval problem: preprocess a dataset $P\\subseteq U$ so that given a query $q\\in Q$ such that there exist some $p^{*}\\in P$ with $\\bar{\\phi(q,p^{*})}=1$ and $\\phi(q,p)=0$ on all $\\bar{p^{\\bigstar}}\\in P\\setminus\\bar{\\{p^{*}\\}}$ , we must report $p^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "Then a list-of-points data structure solving the above problem is as follows: ", "page_idx": 4}, {"type": "text", "text": "1. We fix (possibly random) sets $A_{i}\\subseteq U$ , for $1\\leq i\\leq m$ ; and with each possible query point $q\\in Q$ , we associate a (random) set of indices $I(q)\\subseteq[m]$ ;   \n2. For the given dataset $P\\subset U$ , we maintain $m$ lists of points $L_{1},L_{2},...,L_{m}$ , where $L_{i}=P\\cap A_{i}$ .   \n3. On query $q\\in Q$ , we scan through lists $L_{i}$ where $i\\in I(q)$ , and check whether there exists some $p\\in L_{i}$ with $\\phi(q,p)=1$ . If it exists, return $p$ . ", "page_idx": 4}, {"type": "text", "text": "The data structure succeeds, for a given $q\\in Q,p^{*}\\in P$ with $\\phi(q,p^{*})=1$ , if there exists $i\\in I(q)$ such that $p^{*}\\,\\in\\,L_{i}$ . The total space is defined by $\\begin{array}{r}{S\\,=\\,m+\\sum_{i\\in[m]}|L_{i}|}\\end{array}$ and the query time by $\\begin{array}{r}{T=|I(q)|+\\sum_{i\\in I(q)}|L_{i}|}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "To see how the lower bound model relates to URDE, in our setting, the $\\cdot\\phi$ -retrieval problem\u2019 is the URDE problem: $U$ is the set of random half-uniform distributions, $Q$ is the family of query samples, and $\\phi(q,p)$ is 1 if the samples $q$ were drawn from the distribution $p$ , and 0 otherwise. (The $\\perp$ case corresponds to an \u201capproximate\u201d answer, considering by the earlier papers; but we define URDE problem directly to not have approximate solutions.) ", "page_idx": 4}, {"type": "text", "text": "We use the list-of-points model as it captures all known \u201cdata-independent\u201d similarity search data structures, such as Locality-Sensitive Hashing [14]. In principle, a lower bound against this model does not rule out data-dependent hashing approaches. However, these have been useful only for datasets which are not chosen at random. In particular, [5] conjecture that data-dependency doesn\u2019t help on random instances, which is the setting of our theorems. ", "page_idx": 4}, {"type": "text", "text": "3 Lower bounds for random half-uniform density estimation problem ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we formalize our lower bound. The main theorem of the section is the following. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Lower bound for URDE). If a list-of-points data structure solves the URDE $\\left({\\frac{1}{2}},s\\right)$ using time $O(k^{\\rho_{q}})$ and space $O(k^{1+\\rho_{u}})$ , and\u03c1 succeeds with probability at least 0.99, then for sufficiently large s, \u03c1q \u22651 \u2212s1\u2212log 12\u2212o(1) \u2212log su\u22121. ", "page_idx": 4}, {"type": "text", "text": "To prove Theorem 3.1, our starting point is the following result of [3] that provides a lower-bound for the random GapSS problem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 (Lower bound for random GapSS, [3]). Consider any list-of-points data structure for solving the random $\\mathrm{GapSS}(w_{u},w_{q})$ problem on $k$ points, which uses expected space $O(k^{1+\\rho_{u}})$ , has expected query time $O(k^{\\rho_{q}-o_{k}(1)})$ , and succeeds with probability at least 0.99. Then for every $\\alpha\\in[0,1],$ , we have that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha\\rho_{q}+(1-\\alpha)\\rho_{u}\\geq\\operatorname*{inf}_{t_{q},t_{u}\\in[0,1]}{F(t_{u},t_{q})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad v h e r e\\ F(t_{u},t_{q})\\,=\\,\\alpha\\frac{D(T||P)-d(t_{q}||w_{q})}{d(t_{u}||w_{u})}+(1-\\alpha)\\frac{D(T||P)-d(t_{u}||w_{u})}{d(t_{u}||w_{u})},\\;P\\,=\\,\\left[\\!\\!\\begin{array}{l l}{w_{u}}&{\\!\\!\\!0}\\\\ {w_{u}-w_{q}}&{\\!\\!\\!1-w_{u}}\\end{array}\\!\\!\\right]}\\\\ &{\\qquad\\qquad\\mathrm{arg~inf}\\qquad\\!D(T||P).}\\\\ &{\\qquad\\qquad\\mathrm{g}_{X\\sim T^{[X]}}\\!\\!\\left[\\!\\frac{t_{q}}{t_{u}}\\!\\!\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our proof strategy is to first give a reduction from the the GapSS problem to the URDE problem. Note that the URDE problem involves a statistical step where we receive samples from an unknown distribution (our query). On the other hand, the query of GapSS is a specified vector, rather than a distribution, with no ambiguity. Our reduction bridges this and shows that GapSS is a \u2018strictly easier\u2019 problem than URDE. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 (Reduction from random GapSS to URDE). If a list-of-points data structure solves the URDE $(w_{u},s)$ problem of size $k$ in Definition 2.1 using time $O(k^{\\rho_{q}})$ and space $O(k^{1+\\rho_{u}})$ , and succeeds with probability at least 0.99, then there exists a list-of-points data structure solving the GapSS $(w_{u},w_{q})$ problem for $w_{q}=w_{u}\\left(1-e^{\\frac{-1}{s\\cdot w_{u}}}\\right)$ using space $O(k^{1+\\rho_{u}})$ and time $O(k^{\\rho_{q}}\\!+\\!w_{q}\\!\\cdot\\!n)$ , and succeeds with probability at least 0.99. ", "page_idx": 5}, {"type": "text", "text": "Proof. We provide a reduction from random $\\mathrm{GapSS}(w_{u},w_{q})$ to $\\mathrm{URDE}(w_{u},s)$ with $\\begin{array}{r l}{s}&{{}=}\\end{array}$ wu log(\u221211\u2212wwqu ). Specifically, for each instance (P1, p1\u2217, q1) generated from GapSS(wu, wq) in Definition 2.2, we will construct an instance $(P_{2},p_{2}^{*},q_{2})$ generated from $\\mathrm{URDE}(w_{u},s)$ satisfying Definition 2.1 for some $s$ . ", "page_idx": 5}, {"type": "text", "text": "For each point $p_{1}\\in P_{1}$ , it is straightforward to construct a corresponding uniform distribution $p_{2}$ supported on those coordinates where $p_{1}[i]=1$ . Then let\u2019s construct $q_{2}$ from $q_{1}$ . Recall that for each $i\\in U$ with $p_{1}^{*}[i]=1$ , we have $q_{1}[i]=0$ with probability $1-\\frac{w_{q}}{w_{u}}$ , in which case we add no element $i$ to $q_{2}$ . If $q_{1}[i]=1$ , we add $\\begin{array}{r}{\\mathrm{Poi}_{+}\\left(\\frac{1}{s\\cdot w_{u}}\\right)}\\end{array}$ copies of element $i$ to $q_{2}$ where $\\begin{array}{r}{\\mathbb{P}\\left[\\mathrm{Poi}_{+}(\\lambda)=x\\right]=\\frac{\\mathbb{P}\\left[\\mathrm{Poi}(\\lambda)=x\\right]}{\\mathbb{P}\\left[\\mathrm{Poi}(\\lambda)>0\\right]}}\\end{array}$ for any $x\\,>\\,0$ . By setting wu log(\u221211\u2212wwqu ), we have P Poi s\u00b71wu = 0 = 1 \u2212wwqu . Thus for each element $i$ in $p_{2}^{*}$ , the number of its appearances in $q_{2}$ exactly follows the distribution $\\textstyle\\mathrm{Poi}({\\frac{1}{s\\cdot w_{u}}})$ . According to the property of the Poisson distribution, uniformly sampling Poi |susp\u00b7pw(up2\u2217)| elements from a set of size $|s u p p(p_{2}^{*})|$ is equivalent to sampling each element $\\textstyle\\mathrm{Poi}({\\frac{1}{s\\cdot w_{u}}})$ times. Therefore, the constructed instance $(P_{2},p_{2}^{*},q_{2})$ is an instance of $\\mathrm{URDE}(w_{u},s)$ , as stated in Definition 2.1. Equivalently, we have the relationship $w_{q}=w_{u}\\left(1-e^{\\frac{-1}{s\\cdot w_{u}}}\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "Hence we complete our reduction from GapSS(Definition 2.2) to URDE (Definition 2.1). ", "page_idx": 5}, {"type": "text", "text": "To get the desired space-time trade-off in the sublinear sample regime, which means $s\\rightarrow\\infty$ (or equivalently $w_{q}\\to0$ ), and to get an interpretable analytic bound, we need to develop upon the lower bound in Theorem 3.2. This requires explicitly solving the optimization problem in Theorem 3.2. Proving Theorem 3.4 (proof in Appendix 3) is the main technical contribution of the paper. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4 (Explicit lower bound for random GapSS instance). Consider any list-of-points data structure for solving the random GapSS $\\left({\\textstyle{\\frac{1}{2}}},w_{q}\\right)$ which has expected space $O(k^{\\mathrm{i}+\\rho_{u}})$ , uses expected query time $O\\left(k^{\\rho_{q}-o(1)}\\right)$ , and succeeds with probability at least 0.99. Then we have the following lower bound for sufficiently small $w_{q}$ $\\begin{array}{r}{v_{q}\\colon\\rho_{q}\\geq1-w_{q}^{1-\\log2-o(1)}+\\frac{\\rho_{u}}{1+\\log w_{q}},}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Applying our reduction to the random GapSS lower bound above allows us to prove our main theorem. ", "page_idx": 5}, {"type": "text", "text": "Proof of Theorem 3.1. According to the reduction given in Theorem 3.3 from $\\mathrm{GapSS}(w_{u},w_{q})$ to $\\mathrm{URDE}(w_{u},s)$ where $\\begin{array}{r}{w_{q}=w_{u}\\left(1-e^{\\frac{-1}{w_{u}s}}\\right)\\geq\\frac{1}{s}}\\end{array}$ . We can apply the lower bound in Theorem 3.4 and get the desired lower bound. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Remark 3.5. Note that in $\\mathrm{URDE}\\left({\\frac{1}{2}},s\\right)$ , the distributions are uniform over random subsets of expected size $n/2$ and the query set is generated by taking $\\mathrm{Poi}\\left(\\frac{2|s u p p(p^{*})|}{s}\\right)$ samples from one of them $p^{*}$ . This is not quite saying that the query complexity is $n/s$ . However, by standard concentration bounds, from the Poisson sample, we can simulate sampling with a fixed number of samples $n/s-\\tilde{O}(\\sqrt{n/s})=n/s(1-o(1))$ with high probability, and so, any algorithm using this fixed number of samples must have the same lower bound on $\\rho_{q}$ as in Theorem 3.1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 A simple algorithm for half-uniform density estimation problem ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present a simple algorithm for a special case of the density estimation problem when the input distributions are half-uniform. The algorithm also works for the related $\\begin{array}{r}{\\mathrm{URDE}\\big(\\frac{1}{2},s\\big)}\\end{array}$ problem of Theorem 3.1. A distribution $p$ over $[n]$ is half-uniform if there exists $T\\subset[n]$ with $|T|=n/2$ such that $p[i]=2/n$ if $i\\in T$ and 0 otherwise. The problem we consider in this section is: ", "page_idx": 6}, {"type": "text", "text": "Definition 4.1 (Half-uniform density estimation problem; $\\mathrm{HUDE}(s,\\varepsilon))$ . For a domain $[n]$ , integer $k$ , $\\varepsilon>0$ , and $s>0$ , we consider the following data structure problem. ", "page_idx": 6}, {"type": "text", "text": "1. A dataset $P$ of $k$ distributions $p_{1},\\ldots,p_{k}$ over $[n]$ which are half-uniform over subsets $T_{1},\\ldots,T_{k}\\subset[n]$ each of size $n/2$ is given.   \n2. We receive a query set $q$ consisting of $n/s$ samples from an unknown distribution $p_{i^{*}}~\\in~P$ satisfying that $\\|p_{i^{*}}-p_{j}\\|\\geq\\varepsilon$ for $j\\neq i^{*}$ .   \n3. The goal of the data structure is to preprocess $P$ such that when given the query set $q$ , it recovers the distribution $p_{i^{*}}$ with probability at least 0.99. ", "page_idx": 6}, {"type": "text", "text": "This problem is related to the $\\mathrm{URDE}(1/2,s)$ problem in Theorem 3.1. Indeed, with high probability, an instance of $\\mathrm{URDE}(1/2,s)$ consists of almost half-uniform distributions with support size $n/2\\pm$ $O({\\sqrt{n\\log k}})$ . Moreover, two such distributions $p_{i},p_{j}$ have $\\|p_{i}-p_{j}\\|_{1}=(1\\pm O(\\sqrt{(\\log k)/n}))$ with high probability. Thus, an instance of $\\mathrm{URDE}(1/2,s)$ is essentially an instance of $\\mathrm{HUDE}(s,1)$ . ", "page_idx": 6}, {"type": "text", "text": "To solve $\\mathrm{HUDE}(s,\\varepsilon)$ , we can essentially apply the similarity search data structure of [3] querying it with the set $Q$ consisting of all elements that were sampled at least once. This approach obtains the optimal trade-off between $\\rho_{u}$ and $\\rho_{q}$ (at least in the List-of-points model). The contribution of this section is to provide a very simple alternative algorithm with a slightly weaker trade-off between $\\rho_{u}$ and $\\rho_{q}$ . Section 5 evaluates the simplified algorithm experimentally. Our main theorem is: ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2. Suppose n and $k$ are polynomially related, $s\\geq2$ , and that s is such that4 $\\begin{array}{r}{\\frac{n}{s}\\geq C\\frac{\\log k}{\\varepsilon^{2}}}\\end{array}$ for a sufficiently large constant $C$ . Let $\\varepsilon>0$ and $\\rho_{u}>0$ be given. There exists a data structure for the $H U D E(s,\\varepsilon)$ problem using space $O(k^{1+\\rho_{u}}+n k)$ and with query time $O\\left(k^{1-\\frac{\\varepsilon\\rho_{u}}{2\\log(2s)}}+n/s\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "Let us compare the upper bound of Theorem 4.2 to the lower bound in Theorem 3.1. While Theorem 4.2 is stated for half-uniform distributions, its proof is easily modified to work for the $\\mathrm{URDE}(1/2,s)$ problem where the support size is random. Then $\\varepsilon\\,=\\,1\\,-\\,O(1)$ and as $s\\rightarrow\\infty$ , $\\scriptstyle{\\frac{2}{1-e^{-2/s}}}\\;=\\;s(1+o(1))$ . Thus, the query time exponent in Theorem 4.2 is $\\begin{array}{r}{\\rho_{q}=1\\!-\\!(1\\!+\\!o(1)))\\frac{\\log(2)\\rho_{u}}{\\log s}}\\end{array}$ $\\mathrm{URDE}(1/2,s)$ is exactly the hard instance in Theorem 3.1, and so we know that any algorithm must have $\\begin{array}{r}{\\rho_{q}\\geq1-(1+o(1))\\frac{\\rho_{u}}{\\log s}}\\end{array}$ as $s\\to\\infty$ . Asymptotically, our algorithm therefore gets the right logarithmic dependence on $s$ but with a leading constant of instead of 1. ", "page_idx": 6}, {"type": "text", "text": "Next we define the algorithm. Let $\\ell$ and $L$ be parameters which we will also specify shortly. During preprocessing, our algorithm samples $L$ subsets $S_{1},...,S_{L}$ of $[n]$ each of size $\\ell$ independently and uniformly at random. For each $i\\in L$ , it stores a set $A_{i}$ of all indices $j\\in[k]$ such that $S_{i}\\subset T_{j}$ , namely the indices of the distributions $p_{j}$ which contain $S_{i}$ in their support. See Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "During the query phase, we receive $n/s$ samples from $p=p_{i^{*}}$ for some unknown $i^{*}\\in[k]$ . Our algorithm first forms the subset $Q\\subset[n]$ consisting of all elements that were sampled at least once. Note that $|Q|\\leq n/s$ as elements can be sampled several times. The algorithm proceeds in two steps. First, it goes through the $L$ sets $S_{1},...,S_{L}$ until it finds an $i$ such that $S_{i}\\subset Q$ . Second, it scans through the indices $j\\in A_{i}$ . For each such $j$ it samples a set $U_{j}$ one element at a time from $Q$ . It stops this sampling at the first point of time where either $U_{j}\\not\\subseteq T_{j}$ or $\\begin{array}{r}{|U_{j}|=C\\frac{\\log n}{\\varepsilon}}\\end{array}$ for a sufficiently ", "page_idx": 6}, {"type": "table", "img_path": "PtD4aZPzcR/tmp/f180ee73c4f3cd8e575c50037f041e61734a4c10c9afa5fb218c517affd7635a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "1: Input: Half uniform distributions $\\{p_{i}\\}_{i=1}^{k}$ over $[n]$ with support sets $\\{T_{i}\\}_{i=1}^{k}$ , data structure   \n$(S_{i},A_{i})_{i\\in[L]}\\leftarrow\\mathtt{P r e p r o c e s s i n g}(\\{p_{i}\\}_{i=1}^{k})$ , and $n/s$ samples from query distribution $p=p_{i^{*}}$ .   \n2: Output: A distribution $p_{j}$ .   \n3: procedure DENSITY-ESTIMATION $\\{p_{i}\\}_{i=1}^{k})$   \n4: $Q\\leftarrow\\{i\\in[n]\\mid i$ appeared in the $n/s$ samples}   \n5: for $i=1$ to $L$ do   \n6: if $S_{i}\\subset Q$ then   \n7: for $j\\in A_{i}$ do   \n8: $U_{j}\\gets$ sample from $Q$ of size $C^{\\frac{\\log n}{\\varepsilon}}$ for a large constant $C$ .   \n9: if $U_{j}\\subset T_{j}$ then   \n10: Return: pj ", "page_idx": 7}, {"type": "text", "text": "large constant $C$ . In the first case, it concludes that $p_{j}$ is not the right distribution and proceeds to the next element of $A_{j}$ and in the latter case, it returns the distribution $p_{j}$ as the answer to the density estimation problem. See Algorithm 2. We defer the formal proofs to Appendix B. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We test our algorithm in Section 4 experimentally on datasets of half-uniform distributions as in [1] and corresponding to our study in Sections 3 and 4. Given parameters $k$ and $n$ , the input distributions are $k$ distributions each uniform over a randomly chosen $n/2$ elements. ", "page_idx": 7}, {"type": "text", "text": "Algorithms We compare two main algorithms: an implementation of the algorithm presented in Section 4 which we refer to as the Subset algorithm and a baseline for half-uniform distributions which we refer to as the Elimination algorithm. The Subset algorithm utilizes the same techniques as that presented in Section 4 but with some slight changes geared towards practical usage. We do not compare to the \u201cFastTournament\u201d of [1] since it was computationally prohibitive; see Remark C.1. ", "page_idx": 7}, {"type": "text", "text": "The Subset algorithm picks $L$ subsets of size $\\ell$ and preprocesses the data by constructing a dictionary mapping subsets to the distributions whose support contains that subset. When a query arrives, scan through the $L$ subset until we find one that is contained in the support of the query. We then restrict ourselves to solving the problem over the set of distributions mapped to by that subset and run Elimination. The Elimination algorithm goes through the samples one at a time. It starts with a set of distributions which is the entire input in general or a subset of the input when called as a subroutine of the Subset algorithm. To process a sample, the Elimination algorithm removes from consideration all distributions which do not contain that element in its support. When a single distribution remains, the algorithm returns that distribution as the solution. As the input distributions are random half-uniform distributions, we expect to throw away half of the distributions at each step (other than the true distribution) and terminate in logarithmically in size of the initial set of distribution steps. ", "page_idx": 7}, {"type": "text", "text": "Experimental Setup Our experiments compare the Subset and Elimination algorithms while varying several problem parameters: the number of distributions $k$ , the domain size $n$ , the number of samples $S$ (for simplicity, we use this notation rather than $n/s$ samples as in the rest of the paper), and the size of subsets $\\ell$ used by the Subset algorithm. While we vary one parameter at a time, the others are set to a default of $k\\,=\\,50000$ , $n=500$ , $S\\,=\\,50$ , $l\\,=\\,3$ . Given these parameters, we evaluate the Subset algorithm and the Elimination algorithm on 100 random queries where each query corresponds to picking one of the input distributions as the true distribution to draw samples from. ", "page_idx": 7}, {"type": "image", "img_path": "PtD4aZPzcR/tmp/ae066ed28f51d6e099ecd47bfeda9d97706004d138a49b0791e45642800fe63c.jpg", "img_caption": ["Figure 2: Comparison of efficiency of the Subset (Ours) and Elimination algorithms as (a): the number of distributions $k$ varies. Other parameters are set to $n\\,=\\,500$ , $S\\,=\\,50,\\ell\\,=\\,3$ . (b): the domain size $n$ varies. Other parameters are set to $k=50000,S=50,\\ell=3.$ . (c): the number of samples $S$ varies. Other parameters are set to $k\\,=\\,50000$ , $n=500,\\ell=3$ . (d): the subset size $\\ell$ varies. Other parameters are set to $k=50000,n=500,S=50$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In all settings we test, the Elimination algorithm achieves $100\\%$ accuracy on these queries, which is to be expected as long as the number of samples is sufficently more than the $\\log_{2}k$ . There is a remaining free parameter, which is the number of subsets $L$ used in the Subset algorithm. We start with a modest value of $L=200$ and increase $L$ by a factor of 1.5 repeatedly until the Subset algorithm also achieves $100\\%$ accuracy on the queries (in reality, it\u2019s failure probability will likely still be greater than that of the Elimination algorithm). The results we report correspond to this smallest value of $L$ for which the algorithm got all the queries correct. ", "page_idx": 8}, {"type": "text", "text": "For both algorithms, we track the average number of operations as well as the execution time of the algorithms (not counting preprocessing). A single operation corresponds to a membership query of checking whether a given distribution/sample contains a specified element in its support which is the main primitive used by both algorithms. We use code from [1] as a basis for our setup. ", "page_idx": 8}, {"type": "text", "text": "Results For all parameter settings we test, the number of operations per query by our Subset algorithm is significantly less than those required by Elimination algorithm, up to a factor of more than 6x. The average query time (measured in seconds) shows similar improvements for the Subset algorithm though for some parameter settings, it takes more time than the Elimination algorithm. While, in general, operations and time are highly correlated, these instances where they differ may depend on the specific Python data structures used to implement the algorithms, cache efficiency, or other computational factors. ", "page_idx": 8}, {"type": "text", "text": "As the number of distributions $k$ increases, Figure 2a shows that both time and number of operations scale linearly. Across the board, our Subset algorithm outperforms the Elimination algorithm baseline and exhibits a greater improvement as $k$ increases. On the other hand, as the domain size increases in Figure 2b, the efficiency of the Subset algorithm degrades while the Elimination algorithm maintains its performance. This is due to the fact that for larger domains, more subsets are needed in order to correctly answer all queries, leading to a greater runtime. ", "page_idx": 8}, {"type": "text", "text": "In Figure 2c, we see that across the board, as we vary the number of samples, the Subset algorithm has a significant advantage over the Elimination algorithm in query operations and time. Finally, Figure 2d shows that for subset size $\\ell\\in\\{2,3,4\\}$ , the Subset algorithm experiences a significant improvement over the Elimination algorithm. But for $\\ell=5$ , the improvement (at least in terms of time) suddenly disappears. For this setting, that subset size requires many subsets in order to get high accuracy, leading to longer running times. ", "page_idx": 9}, {"type": "text", "text": "Overall, on flat distributions for a variety of parameters, our algorithm has significant benefits even over a baseline tailored for this case. The good performance of the Subset algorithm corresponds with our theory and validates the contribution of providing a simple algorithm for density estimation in this hard setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments: This work was supported by the Jacobs Presidential Fellowship, the Mathworks Fellowship, the NSF TRIPODS program (award DMS-2022448) and Simons Investigator Award; also supported in part by NSF (CCF2008733) and ONR (N00014-22-1-2713). Justin Chen is supported by an NSF Graduate Research Fellowship under Grant No. 174530. Shyam Narayanan is supported by an NSF Graduate Fellowship and a Google Fellowship. Anders Aamand was supported by the DFF-International Postdoc Grant 0164-00022B and by the VILLUM Foundation grants 54451 and 16582. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Anders Aamand, Alexandr Andoni, Justin Y Chen, Piotr Indyk, Shyam Narayanan, and Sandeep Silwal. Data structures for density estimation. In International Conference on Machine Learning, 2023.   \n[2] Jayadev Acharya, Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Maximum selection and sorting with adversarial comparators. The Journal of Machine Learning Research, 19(1):2427\u20132457, 2018.   \n[3] Thomas D Ahle and Jakob BT Knudsen. Subsets and supermajorities: Optimal hashing-based set similarity search. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 728\u2013739. IEEE, 2020.   \n[4] Maryam Aliakbarpour, Mark Bun, and Adam Smith. Hypothesis selection with memory constraints. Advances in Neural Information Processing Systems, 36, 2024.   \n[5] Alexandr Andoni, Thijs Laarhoven, Ilya Razenshteyn, and Erik Waingarten. Optimal hashingbased time-space trade-offs for approximate near neighbors. In Proceedings of the twenty-eighth annual ACM-SIAM symposium on discrete algorithms, pages 47\u201366. SIAM, 2017.   \n[6] Alexandr Andoni, Huy L Nguyen, Aleksandar Nikolov, Ilya Razenshteyn, and Erik Waingarten. Approximate near neighbors for general symmetric norms. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 902\u2013913, 2017.   \n[7] Mark Bun, Gautam Kamath, Thomas Steinke, and Steven Z Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019.   \n[8] Cl\u00e9ment L Canonne, Gautam Kamath, Audra McMillan, Adam Smith, and Jonathan Ullman. The structure of optimal private tests for simple hypotheses. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 310\u2013321, 2019.   \n[9] Tobias Christiani and Rasmus Pagh. Set similarity search beyond minhash. In Proceedings of the 49th annual ACM SIGACT symposium on theory of computing, pages 1094\u20131107, 2017.   \n[10] Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms for proper learning mixtures of gaussians. In Conference on Learning Theory, pages 1183\u20131213. PMLR, 2014.   \n[11] Luc Devroye and G\u00e1bor Lugosi. Combinatorial methods in density estimation. Springer series in statistics. Springer, 2001.   \n[12] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high-dimensions without the computational intractability. SIAM Journal on Computing, 48(2):742\u2013864, 2019.   \n[13] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. In Conference on Learning Theory, pages 1785\u20131816. PMLR, 2020.   \n[14] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604\u2013613, 1998.   \n[15] Gautam Kamath, Vikrant Singhal, and Jonathan Ullman. Private mean estimation of heavy-tailed distributions. In Conference on Learning Theory, pages 2204\u20132235. PMLR, 2020.   \n[16] Sudeep Kamath, Alon Orlitsky, Dheeraj Pichapati, and Ananda Theertha Suresh. On learning distributions from their samples. In Conference on Learning Theory, pages 1066\u20131100. PMLR, 2015.   \n[17] Henry Scheffe. A Useful Convergence Theorem for Probability Distributions. The Annals of Mathematical Statistics, 18(3):434\u2013438, 1947.   \n[18] Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimalsample estimators for spherical gaussian mixtures. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Appendix: Omitted Proofs of Section 3 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Theorem 3.4 (Explicit lower bound for random GapSS instance). Consider any list-of-points data structure for solving the random GapSS $\\left({\\textstyle{\\frac{1}{2}}},w_{q}\\right)$ which has expected space $O(k^{\\mathrm{i}+\\rho_{u}})$ , uses expected query time $O\\left(k^{\\rho_{q}-o(1)}\\right)$ , and succeeds with probability at least 0.99. Then we have the following lower bound for sufficiently small $w_{q}$ : $\\begin{array}{r}{\\rho_{q}\\geq1-w_{q}^{1-\\log2-o(1)}+\\frac{\\rho_{u}}{1+\\log w_{q}}.}\\end{array}$ ", "page_idx": 11}, {"type": "text", "text": "Proof. Our proof proceeds by explicitly calculating the lower bound given in Theorem 3.2 when $w_{u}=$ $\\frac{1}{2}$ and $w_{q}$ approaches 0. Recall that Theorem 3.2 states that if a list-of-points data structure solves $\\mathrm{GapSS}(w_{u},w_{q})$ for $k$ points uses expected space $k^{1+\\rho_{u}}$ , and has expected query time $k^{\\rho_{q}-o_{k}(1)}$ , then for every $\\dot{\\alpha}\\in[0,1]$ , we have that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\alpha\\rho_{q}+(1-\\alpha)\\rho_{u}\\geq\\operatorname*{inf}_{\\stackrel{t_{q},t_{u}\\in[0,1]}{t_{u}\\neq w_{u}}}\\left(\\alpha\\frac{D(T||P)-d(t_{q}||w_{q})}{d\\left(t_{u}||w_{u}\\right)}+(1-\\alpha)\\frac{D(T||P)-d(t_{u}||w_{u})}{d(t_{u}||w_{u})}\\right)\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $P=\\left[w_{u}-w_{q}\\quad1-w_{u}\\right]$ and T = arg inf D(T||P). T \u226aP E [X] X\u223c ", "page_idx": 11}, {"type": "text", "text": "We denote the fraction in the right hand side of Equation 1 as $\\boldsymbol{F}(t_{q},t_{u})$ . Our goal is to provide a lower bound in the case $w_{u}=1/2$ . ", "page_idx": 11}, {"type": "text", "text": "First, notice that to satisfy $T\\ll P$ (i.e. $s u p p(T)\\;\\subseteq\\;s u p p(P))$ and $\\underset{X\\sim T}{\\mathbb{E}}[X]\\,=\\,\\left[{t_{q}}\\right]$ , the only available choice is $T=\\left[t_{q}\\!\\!\\!\\begin{array}{c c}{{t_{q}}}&{{0}}\\\\ {{t_{u}-t_{q}}}&{{1-t_{u}}}\\end{array}\\right]$ . Plugging this in and expanding $\\boldsymbol{F}(t_{u},t_{q})$ , we get $F(t_{u},t_{q})=\\frac{(t_{u}-t_{q})\\log\\frac{t_{u}-t_{q}}{w_{u}-w_{q}}+\\alpha\\cdot d(t_{u}||w_{u})-t_{u}\\log\\frac{t_{u}}{w_{u}}-\\alpha\\cdot d(t_{q}||w_{q})+t_{q}\\log\\frac{t_{q}}{w_{q}}}{d(t_{u}||w_{u})}.$ (2) ", "page_idx": 11}, {"type": "text", "text": "For $w_{u}\\,=\\,1/2$ fixed, and for fixed $w_{q},\\alpha,t_{u}\\,\\ne\\,1/2$ , we can consider $F$ as a function of only $t_{q}$ . Because $F$ is a continuously differentiable function in terms of $t_{q}$ , the infimum of $F$ (for fixed $w_{u},w_{q},\\alpha,t_{u})$ can only be achieved either when $\\partial F/\\partial t_{q}=0$ , or at the boundary points ${{t}_{q}}=0$ or $t_{q}\\rightarrow t_{u}^{-}$ ). ", "page_idx": 11}, {"type": "text", "text": "We first consider the case when the partial derivative is 0 and handle the endpoint cases later. Calculating the partial derivative of $F$ with respect to $t_{q}$ gives us $\\begin{array}{r}{\\frac{\\partial F}{\\partial t_{q}}\\,=\\,\\log\\frac{{t_{q}}^{\\star}}{w_{q}}-\\log\\frac{{t_{u}}-{t_{q}}}{w_{u}-{w_{q}}}\\,-}\\end{array}$ $\\begin{array}{r}{\\alpha\\log{\\frac{t_{q}(1-w_{q})}{w_{q}(1-t_{q})}}}\\end{array}$ . When $\\begin{array}{r}{\\frac{{\\partial}F}{{\\partial}t_{q}}=0}\\end{array}$ , we must have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{t_{u}-t_{q}}{w_{u}-w_{q}}=\\left(\\frac{t_{q}}{w_{q}}\\right)^{1-\\alpha}\\left(\\frac{1-t_{q}}{1-w_{q}}\\right)^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Plugging in the relation in (3) and $\\begin{array}{r}{w_{u}=\\frac{1}{2}}\\end{array}$ , we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma(t_{u},t_{q})=\\alpha+\\frac{(t_{u}-t_{q})\\log\\frac{t_{u}-t_{q}}{w_{u}-w_{q}}-t_{u}\\log\\frac{t_{u}}{w_{u}}-\\alpha\\cdot{d(t_{q}||w_{q})}+t_{q}\\log\\frac{t_{u}}{w_{u}}}{d(t_{u}||w_{u})}}\\\\ &{\\qquad\\qquad=\\alpha+\\frac{t_{u}\\left(\\log\\frac{t_{u}-t_{q}}{w_{u}-w_{q}}-\\log\\frac{t_{u}}{w_{u}}\\right)-t_{q}\\log\\frac{t_{u}-t_{q}}{w_{u}-w_{q}}-\\alpha\\cdot{d(t_{q}||w_{q})}+t_{q}\\log\\frac{t_{q}}{w_{u}}}{d(t_{u}||w_{u})}}\\\\ &{\\qquad\\qquad=\\alpha+\\frac{t_{u}\\left(\\log\\frac{t_{u}-t_{u}}{w_{u}-w_{q}}-\\log\\frac{t_{u}}{w_{u}}\\right)-t_{q}\\log\\frac{t_{u}-t_{u}}{w_{u}}+(1-\\alpha)\\cdot t_{q}\\log\\frac{t_{u}}{w_{u}}-\\alpha\\cdot(1-t_{q})\\log\\frac{t_{u}}{w_{u}}}{d(t_{u}||w_{u})}}\\\\ &{\\qquad\\qquad=\\alpha+\\frac{t_{u}\\left(\\log\\frac{t_{u}-t_{q}}{w_{u}-w_{q}}-\\log\\frac{t_{u}}{w_{u}}\\right)-\\alpha\\log\\frac{t_{u}}{w_{u}}}{d(t_{u}||w_{u})}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(\\mathrm{Puging~in~equation~3})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\;\\;\\;\\;d(t_{u}||w_{u})}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "By Lemma A.1, if we set \u03b1 = 1 +log1 wq for $w_{q}$ is sufficiently small, we have $F(t_{u},t_{q})\\ge\\alpha-$ wq1\u2212log 2\u2212o(1), uniformly over tu, tq. Next, let\u2019s check the boundary cases. For tq = 0, Lemma A.8 proves that $F(t_{u},0)\\geq\\alpha-w_{q}^{1-o(1)}\\geq\\alpha-w_{q}^{1-\\log2-o(1)}$ wq1\u2212log 2\u2212o(1). For tq \u2192tu\u2212 , because \u2202\u2202tF is continuous for $0\\leq t_{q}<t_{u}$ and $\\operatorname*{lim}_{t_{q}\\to t_{u}^{-}}\\frac{\\partial F}{\\partial t_{q}}=+\\infty$ , the infimum of $F$ cannot be achieved when $t_{q}\\rightarrow t_{u}^{-}$ . ", "page_idx": 12}, {"type": "text", "text": "Thus, for wu = 1/2, any fixed wq sufficiently small, \u03b1 = 1 +log1 wq , and any fixed $t_{u}\\neq1/2$ and the infimum of $F$ across $0\\leq t_{q}<t_{u}$ is at least wq1\u2212log 2\u2212o(1), where the o(1) term goes to 0 as wq goes to 0, uniformly across $t_{u},t_{q}$ . So in fact, $F(t_{u},t_{q})\\geq\\alpha-w_{q}^{1-\\log2-o(1)}$ wq1\u2212log 2\u2212o(1)uniformly across tu, tq. Applying this bound back to our original inequality in 1 gives us the desired bound. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Our goal is to now bound the fraction in the final step of the proof of Theorem 3.4. The following lemma bound this fraction. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1. Fix any constant $\\delta>0$ . Suppose $w_{q}<1$ is smaller than a sufficiently small constant c = c\u03b4 that only depends on \u03b4, wu = 1/2, and \u03b1 = 1 +log1 wq . Suppose these parameters satisfy the relation given in Equation 3. Then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t_{q},t_{u}\\in[0,1]}\\frac{t_{u}\\left(\\log\\left(1-\\frac{t_{q}}{t_{u}}\\right)+\\log\\frac{1}{1-2w_{q}}\\right)-\\alpha\\log\\frac{1-t_{q}}{1-w_{q}}}{d(t_{u}||w_{u})}\\geq-w_{q}^{1-\\log2-\\delta}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Equivalently, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{t_{q},t_{u}\\in[0,1]}}\\frac{t_{u}\\left(\\log\\left(1-\\frac{t_{q}}{t_{u}}\\right)+\\log\\frac{1}{1-2w_{q}}\\right)-\\alpha\\log\\frac{1-t_{q}}{1-w_{q}}}{d(t_{u}||w_{u})}\\geq-w_{q}^{1-\\log2-o(1)}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for sufficiently small $w_{q}{_{}}$ , where $o(1)$ denotes a term that uniformly goes to 0 as $w_{q}\\to0$ (regardless of $t_{q},t_{u})$ . ", "page_idx": 12}, {"type": "text", "text": "In the rest of this section, we use $o(1)$ to denote any term that goes to 0 as $w_{q}\\,\\to\\,0$ , uniformly over $t_{q},t_{u}$ . We will prove some auxiliary lemmas before proving Lemma A.1. We first define the following function $H(x)$ . ", "page_idx": 12}, {"type": "text", "text": "Definition A.2. For a value $x,H(x):=x\\log(2x)+(1-x)\\log(2(1-x)).$ ", "page_idx": 12}, {"type": "text", "text": "It is clear that $H(x)$ is only defined when $0<x<1$ . Moreover, we note the following basic property and provide its proof for completeness. ", "page_idx": 12}, {"type": "text", "text": "Proposition A.3. For any $x\\in(0,1),\\,2(\\frac{1}{2}-x)^{2}\\leq H(x)\\leq16(\\frac{1}{2}-x)^{2}.$ ", "page_idx": 12}, {"type": "text", "text": "iPsr $\\begin{array}{r}{\\dot{H^{\\prime\\prime}}(x)\\,=\\,\\frac{1}{x}\\,\\dot{+}\\,\\frac{1}{1-x}\\,=\\,\\frac{1}{x-x^{2}}}\\end{array}$ .t $H(1/2)=0$ $0\\,<\\,x\\,<\\,1$ d, $x-x^{2}\\leq1/4$ $H^{\\prime}(1/2)=0$ , Msoo $H^{\\prime\\prime}(x)\\geq4$ efcoorn adl ld $x$ r.i vTathiuvse, $H^{\\prime\\prime}(x)\\geq2({\\frac{1}{2}}-x)^{2}$ . ", "page_idx": 12}, {"type": "text", "text": "Next, we have that $\\textstyle x-x^{2}\\geq{\\frac{3}{16}}$ for $x\\in[1/4,3/4]$ , which means $H^{\\prime\\prime}(x)\\leq{\\frac{16}{3}}$ for $x\\in[1/4,3/4]$ . Thus, $\\begin{array}{r}{H(x)\\leq\\frac{8}{3}(\\frac{1}{2}-x)^{2}}\\end{array}$ for $x\\in[1/4,3/4]$ . Since the first derivative $H^{\\prime}(x)=\\log(x)-\\log(1-x)$ is negative for $x<\\textstyle{\\frac{1}{2}}$ and positive for $x>\\textstyle{\\frac{1}{2}}$ , this means $H(x)$ is maximized as $x$ approaches either 0 or 1. But the limits equal $\\log2$ , so $H(x)\\leq\\log2$ for all $x$ . Since $({\\frac{1}{2}}-x)^{2}\\geq{\\frac{1}{16}}$ for all $1>x>\\frac{3}{4}$ or $\\begin{array}{r}{0<x<\\frac{1}{4}}\\end{array}$ , this means for any such $x$ , $H(x)\\leq\\log2\\leq16\\log2\\cdot({\\frac{1}{2}}-x)^{2}\\leq16\\cdot({\\frac{1}{2}}-x)^{2}$ . So in either case, $H(x)\\leq16({\\frac{1}{2}}-x)^{2}$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "We are now ready to prove Lemma A.1. ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma A.1. For simplicity of notation, let $\\boldsymbol{x}_{\\mathit{\\Pi}}=t_{q}$ . Recalling the value of $\\alpha,\\,w_{u}$ , and Equation 3, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\nt_{u}=x+\\left(\\frac{x}{w_{q}}\\right)^{1-\\alpha}\\cdot\\left(\\frac{1-x}{1-w_{q}}\\right)^{\\alpha}\\cdot\\left(\\frac{1}{2}-w_{q}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now we denote the fraction in the lemma statement as $b(x)/a(x)$ and we note ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{a=H(t_{u})=t_{u}\\cdot\\log\\left(2t_{u}\\right)+\\left(1-t_{u}\\right)\\cdot\\log\\left(2\\left(1-t_{u}\\right)\\right),}}\\\\ {{b=-\\alpha\\cdot\\log\\left(\\displaystyle\\frac{1-x}{1-w_{q}}\\right)+t_{u}\\cdot\\left(\\log\\left(1-\\displaystyle\\frac{x}{t_{u}}\\right)+\\log\\left(\\displaystyle\\frac{1}{1-2w_{q}}\\right)\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From Proposition A.4, it suffices to check the following cases: ", "page_idx": 13}, {"type": "text", "text": "In Case 1, $x^{*}$ is such that $x\\in(0,x^{*})$ is the regime of $x$ such that $a$ and $b$ are well defined. Proposition A.4 further states that $x^{*}$ only depends on $w_{q}$ and wq1\u2212log 2\u2212o(1)as wq \u21920. ", "page_idx": 13}, {"type": "text", "text": "The cases are handled in Lemmas A.5, A.6, and A.7, respectively. Combining them proves Lemma A.1. ", "page_idx": 13}, {"type": "text", "text": "Proposition A.4. Suppose that $\\begin{array}{r}{0<w_{q}<\\frac{1}{3}}\\end{array}$ and $0\\leq x\\leq1$ . Then, the regime of $x$ such that $a,b$ are well defined is $x\\in(0,x^{*})$ , where $0<x^{*}<1$ only depends on $w_{q}$ and $x^{*}\\leq w_{q}^{1-\\log2-o(1)}\\ a s$ $w_{q}\\to0$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We must have that $0<t_{u}<1$ , so that $a=H(t_{u})$ is well-defined. If $x=0$ , $t_{u}=0$ , and if $x=1$ , then $\\begin{array}{r}{\\log\\left(\\frac{1-x}{1-w_{q}}\\right)}\\end{array}$ isn\u2019t defined, so $b$ isn\u2019t defined. If $0<x<1.$ , $t_{u}$ is always positive, so $a$ being well-defined is equivalent to $1-t_{u}>0$ , which is equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\n1-x>\\left(\\frac{x}{w_{q}}\\right)^{1-\\alpha}\\cdot\\left(\\frac{1-x}{1-w_{q}}\\right)^{\\alpha}\\cdot\\left(\\frac{1}{2}-w_{q}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We can rearrange this as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\frac{1-x}{x}\\right)^{-1/(\\log w_{q})}=\\left(\\frac{1-x}{x}\\right)^{1-\\alpha}>\\frac{1/2-w_{q}}{w_{q}^{1-\\alpha}(1-w_{q})^{\\alpha}}=\\frac{e(1/2-w_{q})}{(1-w_{q})^{1+1/(\\log w_{q})}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We can again rearrange this as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1-x}{x}>C(w_{q}):=\\left(\\frac{e(1/2-w_{q})}{(1-w_{q})^{1+1/(\\log w_{q})}}\\right)^{-\\log w_{q}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $C(w_{q})$ is positive if $w_{q}<1/3$ . This is equivalent to is equivalent to $\\begin{array}{r}{0<x<\\frac{1}{C(w_{q})+1}}\\end{array}$ . So, we can set =C(w1q)+1. Thus, a is well defined if and only if 0 < x < x\u2217, where x\u2217\u2208(0, 1) clearly holds. Moreover, if wq < 1/3, then log11\u2212\u2212wxq and log1\u221212w are well-defined, and $t_{u}>x>0$ so $\\begin{array}{r}{\\log\\left(1-\\frac{x}{t_{u}}\\right)}\\end{array}$ is also well-defined. In summary, $a,b$ are well defined if and only if $0<x<x^{*}=$ $\\textstyle{\\frac{1}{1+C(w_{q})}}$ ", "page_idx": 13}, {"type": "text", "text": "Finally, we bound $x^{*}$ for $w_{q}$ sufficiently small. Note that $(1-w_{q})^{1+1/(\\log w_{q})}=1+o(1)$ and $1/2-$ $w_{q}=1/2-o(1)$ $\\begin{array}{r}{C(w_{q})=\\left(\\frac{e}{2}\\cdot(1\\pm o(1))\\right)^{-\\log{w_{q}}}=(e/2)^{-\\log{w_{q}}\\cdot(1\\pm o(1))}=w_{q}^{\\log{2}-1\\pm o(1)}.}\\end{array}$ $C(w_{q})\\geq w_{q}^{\\log2-1+o(1)}$ $x^{*}\\leq w_{q}^{1-\\log2-o(1)}$ ", "page_idx": 13}, {"type": "text", "text": "Lemma A.5. Suppose that $w_{q}$ is sufficiently small, and $0<x\\le w_{q}^{1.01}$ wq or , wq0.99\u2264x < x\u2217. Then, $\\begin{array}{r}{\\frac{b}{a}\\geq-w_{q}^{1-\\log2-o(1)}}\\end{array}$ as $w_{q}\\to0$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Since $x$ is strictly positive, we can write $x=w_{q}^{1+\\gamma}$ , for some real $\\gamma$ with $|\\gamma|\\geq0.01$ . Then, $\\begin{array}{r}{\\frac{x}{w_{q}}=w_{q}^{\\gamma}}\\end{array}$ , so $\\left(\\frac{x}{w_{q}}\\right)^{1-\\alpha}=w_{q}^{-\\gamma/\\log w_{q}}=e^{-\\gamma}$ q\u2212\u03b3/ log wq = e\u2212\u03b3. Finally, since x = o(1) and wq = o(1), this means $t_{u}=x+e^{-\\gamma}\\cdot\\left(\\frac{1}{2}\\pm o(1)\\right)=0.5\\cdot e^{-\\gamma}\\pm o(1)\\cdot(e^{-\\gamma}+1)$ . Since $|\\gamma|>0.01$ , this implies that, for $w_{q}$ sufficiently small, $|t_{u}-0.5|\\geq\\Omega(1)$ , which means $a=H(t_{u})=\\Omega(1)$ by Proposition A.3. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "We can also bound $b$ as follows. Note that $\\begin{array}{r}{\\left|\\log\\left(\\frac{1-x}{1-w_{q}}\\right)\\right|=O(x+w_{q})}\\end{array}$ , so $\\begin{array}{r}{-\\alpha\\cdot\\log\\left(\\frac{1-x}{1-w_{q}}\\right)\\geq}\\end{array}$ $-O(x+w_{q})$ . Next, note that for $w_{q}$ sufficiently small (and thus $x<x^{*}$ is also sufficiently small), since $\\begin{array}{r}{0<\\alpha,1-\\alpha<1,\\left(\\frac{1-x}{1-w_{q}}\\right)^{\\alpha}\\geq0.5}\\end{array}$ , and $\\left(\\frac{x}{w_{q}}\\right)^{1-\\alpha}\\geq x^{1-\\alpha}\\geq x$ . Also, $\\textstyle{\\frac{1}{2}}-w_{q}\\geq{\\frac{1}{3}}$ . Thus, $t_{u}\\geq x+x\\cdot{\\frac{1}{2}}\\cdot{\\frac{1}{3}}\\geq{\\frac{7}{6}}\\cdot x$ . Therefore, $\\begin{array}{r}{0\\leq\\frac{x}{t_{u}}\\leq\\frac{6}{7}}\\end{array}$ , which means $\\begin{array}{r}{\\left|\\log\\left(1-\\frac{x}{t_{u}}\\right)\\right|\\le O(x/t_{u})}\\end{array}$ . Thus, $\\begin{array}{r}{\\left|t_{u}\\cdot\\left(\\log\\left(1-\\frac{x}{t_{u}}\\right)+\\log\\left(\\frac{1}{1-2w_{q}}\\right)\\right)\\right|\\leq O(t_{u}\\cdot(x/t_{u}+w_{q}))=\\dot{O}(x+w_{q})}\\end{array}$ . Thus, $|b|\\leq O(x\\!+\\!w_{q})$ . Since $a=\\Omega(1)$ is positive, and $b\\geq-O(x+w_{q})$ , this means that ${\\textstyle{\\frac{b}{a}}}\\geq-O(x+w_{q})$ . Since we know that $x<x^{*}\\leq w_{q}^{1-\\log2-o(1)}$ q1\u2212log 2\u2212o(1), this implies that ba \u2265\u2212wq1 $\\begin{array}{r}{\\frac{b}{a}\\geq-w_{q}^{1-\\log2-o(1)}}\\end{array}$ \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma A.6. Suppose that $\\begin{array}{r}{w_{q}^{1.01}<x<(1+\\frac{1}{\\log w_{q}})w_{q},}\\end{array}$ , or $\\begin{array}{r}{(1-\\frac{1}{\\log w_{q}})w_{q}<x<w_{q}^{0.99}}\\end{array}$ . Then, $\\frac{b}{a}\\geq-w_{q}^{0.99-o(1)}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. We again write $x=w_{q}^{1+\\gamma}$ , where this time $|\\gamma|<0.01$ . Since either $\\begin{array}{r}{x>(1-\\frac{1}{\\log w_{q}})w_{q}}\\end{array}$ or $\\begin{array}{r}{x<(1+\\frac{1}{\\log w_{q}})w_{q}}\\end{array}$ , this means that $\\begin{array}{r}{|\\gamma|\\geq\\Omega\\left(\\frac{1}{(\\log w_{q})^{2}}\\right)}\\end{array}$ . Then, we can write ", "page_idx": 14}, {"type": "equation", "text": "$$\nt_{u}=O(w_{q}^{0.99})+w_{q}^{-\\gamma/(\\log{w_{q}})}\\cdot\\left(\\frac{1}{2}\\pm O(w_{q}^{0.99})\\right)=\\frac{e^{-\\gamma}}{2}\\pm O(w_{q}^{0.99})=\\frac{1}{2}-\\Theta(\\gamma),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "since $\\begin{array}{r}{0.01>|\\gamma|\\ge\\Omega\\left(-\\frac{1}{\\log{w_{q}}}\\right)}\\end{array}$ so the $w_{q}^{0.99}$ term is negligible compared to $\\gamma$ . So, $a=H(t_{u})=$ $\\Theta(\\gamma^{2})\\geq\\Omega\\left(1/(\\log w_{q}\\right)^{\\!4}\\right)$ , by Proposition A.3. ", "page_idx": 14}, {"type": "text", "text": "Next, to bound $b$ , note that $\\begin{array}{r}{\\left|\\log\\left(\\frac{1-x}{1-w_{q}}\\right)\\right|\\,=\\,O(x+w_{q})\\,\\le\\,O(w_{q}^{0.99})}\\end{array}$ . Also, since $\\begin{array}{r}{t_{u}\\,=\\,\\frac{e^{-\\gamma}}{2}\\pm}\\end{array}$ $O(w_{q}^{0.99})$ , this means that $t_{u}\\,=\\,\\Theta(1)$ , so  tu \u00b7 log 1 \u2212txu + log 1\u221212wq    \u2264O(x + wq) = $O(w_{q}^{0.99})$ . Overall, $|b|\\leq O(w_{q}^{0.99})$ , so $\\frac{b}{a}\\geq-w_{q}^{0.99-o(1)}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma A.7. Suppose that $\\begin{array}{r}{(1+\\frac{1}{\\log w_{q}})w_{q}\\leq x\\leq(1-\\frac{1}{\\log w_{q}})w_{q}}\\end{array}$ . Then, $\\begin{array}{r}{\\frac{b}{a}\\geq-w_{q}^{1-o(1)}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Suppose that x = (1 + \u03b2)wq, where |\u03b2| \u2264\u2212log1 wq . We will look at $t_{u}$ from a more fine grained perspective. ", "page_idx": 14}, {"type": "text", "text": "Note that $\\begin{array}{r l}&{\\left(\\frac{x}{w_{q}}\\right)^{1-\\alpha}\\,=\\,(1+\\beta)^{-1/(\\log w_{q})}\\,=\\,e^{-(\\beta\\pm O(\\beta^{2}))/(\\log w_{q})}\\,=\\,1-\\frac{\\beta}{\\log w_{q}}\\pm O\\left(\\frac{\\beta^{2}}{\\log w_{q}}\\right).}\\\\ &{\\left(\\frac{1-x}{\\pm\\frac{\\beta}{\\tau}}\\right)^{\\alpha}=\\left(1-\\frac{\\beta w_{q}}{\\pm\\frac{\\beta}{\\tau}}\\right)^{\\alpha}=1-\\frac{\\beta w_{q}}{\\pm\\frac{\\beta}{\\tau}}\\cdot\\alpha\\pm O(\\beta^{2}w_{\\circ}^{2}).\\,\\mathrm{Thus}.}\\end{array}$ Moreover, $\\begin{array}{r}{\\left(\\frac{1-x}{1-w_{q}}\\right)^{\\alpha}=\\left(1-\\frac{\\beta w_{q}}{1-w_{q}}\\right)^{\\alpha}=1-\\frac{\\beta w_{q}}{1-w_{q}}\\cdot\\alpha\\pm O(\\beta^{2}w_{q}^{2})}\\end{array}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(\\frac{x}{w_{q}}\\right)^{1-\\alpha}\\cdot\\left(\\frac{1-x}{1-w_{q}}\\right)^{\\alpha}=1-\\frac{\\beta}{\\log w_{q}}-\\frac{\\beta w_{q}}{1-w_{q}}\\cdot\\alpha\\pm O\\left(\\frac{\\beta^{2}}{\\log w_{q}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, we can write ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{t_{u}=w_{q}+\\beta w_{q}+\\left(1-\\frac{\\displaystyle\\beta}{\\displaystyle\\log w_{q}}-\\frac{\\displaystyle\\beta w_{q}}{\\displaystyle1-w_{q}}\\cdot\\alpha\\pm O\\left(\\frac{\\displaystyle\\beta^{2}}{\\displaystyle\\log w_{q}}\\right)\\right)\\cdot\\left(\\frac{\\displaystyle1}{\\displaystyle2}-w_{q}\\right)}}\\\\ {{\\phantom{=}=\\frac{\\displaystyle1}{\\displaystyle2}+\\beta w_{q}-\\beta\\cdot\\left(\\frac{\\displaystyle1}{\\displaystyle\\log w_{q}}+\\frac{w_{q}}{\\displaystyle1-w_{q}}\\cdot\\alpha\\right)\\cdot\\left(\\frac{\\displaystyle1}{\\displaystyle2}-w_{q}\\right)\\pm O\\left(\\frac{\\displaystyle\\beta^{2}}{\\displaystyle\\log w_{q}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $\\begin{array}{r}{t_{u}=\\frac{1}{2}\\!-\\!\\Theta(\\beta/\\log{w_{q}})}\\end{array}$ , since the other terms in (5) are all negligible compared to $\\beta/\\log{w_{q}}$ , which means that $a=\\Theta(\\beta^{2}/(\\log w_{q})^{2})$ by Proposition A.3. ", "page_idx": 14}, {"type": "text", "text": "To bound $b$ , we will need the more fine-grained approximation of $t_{u}$ from (5). Indeed, note that $\\begin{array}{r}{\\log\\left(\\frac{1-x}{1-w_{q}}\\right)=\\log\\left(1-\\frac{\\beta w_{q}}{1-w_{q}}\\right)=-\\frac{\\beta w_{q}}{1-w_{q}}\\dot{\\pm}O(\\beta^{2}\\dot{w}_{q}^{2})}\\end{array}$ , and $\\begin{array}{r}{t_{u}\\cdot\\left(\\log\\left(1-\\frac{x}{t_{u}}\\right)+\\log\\left(\\frac{1}{1-2w_{q}}\\right)\\right)=}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "$\\begin{array}{r}{t_{u}\\cdot\\log\\left(\\frac{t_{u}-x}{t_{u}(1-2w_{q})}\\right)=t_{u}\\cdot\\log\\left(1+\\frac{2w_{q}t_{u}-x}{t_{u}(1-2w_{q})}\\right).}\\end{array}$ Note that $2w_{q}t_{u}=2(1/2-\\Theta(\\beta/\\log w_{q}))w_{q}=$ $w_{q}-\\Theta(\\beta\\cdot w_{q}/\\log w_{q})$ . Thus, $2w_{q}t_{u}-x=\\Theta(\\beta\\cdot w_{q})$ . Moreover, $t_{u}(1-2w_{q})=\\Theta(1)$ . Therefore, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathrm{}}_{u}\\cdot\\log\\left(1+{\\frac{2w_{q}t_{u}-x}{t_{u}(1-2w_{q})}}\\right)=t_{u}\\cdot\\left({\\frac{2w_{q}t_{u}-x}{t_{u}(1-2w_{q})}}\\pm O\\left({\\frac{2w_{q}t_{u}-x}{t_{u}(1-2w_{q})}}\\right)^{2}\\right)={\\frac{2w_{q}t_{u}-x}{1-2w_{q}}}\\pm O(\\beta^{2}w_{q}^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\nb=\\alpha\\cdot\\frac{\\beta w_{q}}{1-w_{q}}+\\frac{2w_{q}t_{u}-x}{1-2w_{q}}\\pm O(\\beta^{2}w_{q}^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the more fine-grained approximation of $t_{u}$ , we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{2}w_{q}t_{u}-x=w_{q}+2\\beta w_{q}^{2}-w_{q}\\beta\\left(\\frac{1}{\\log{w_{q}}}+\\frac{w_{q}}{1-w_{q}}\\cdot\\alpha\\right)\\cdot(1-2w_{q})-(w_{q}+\\beta w_{q})\\pm O(\\beta^{2}w_{q})}\\\\ &{\\qquad\\qquad=(2\\beta w_{q}^{2}-\\beta w_{q})-w_{q}\\beta\\left(\\frac{1}{\\log{w_{q}}}+\\frac{w_{q}}{1-w_{q}}\\cdot\\alpha\\right)\\cdot(1-2w_{q})\\pm O(\\beta^{2}w_{q})}\\\\ &{\\qquad=-w_{q}\\beta(1-2w_{q})-w_{q}\\beta\\left(\\frac{1}{\\log{w_{q}}}+\\frac{w_{q}}{1-w_{q}}\\cdot\\alpha\\right)\\cdot(1-2w_{q})\\pm O(\\beta^{2}w_{q})}\\\\ &{\\qquad=w_{q}\\beta\\cdot\\left(-1-\\frac{1}{\\log{w_{q}}}-\\frac{w_{q}}{1-w_{q}}\\cdot\\alpha\\right)\\cdot(1-2w_{q})\\pm O(\\beta^{2}w_{q})}\\\\ &{\\qquad=-w_{q}\\beta\\cdot\\left(\\frac{\\alpha}{1-w_{q}}\\right)\\cdot(1-2w_{q})\\pm O(\\beta^{2}w_{q}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last line uses the fact that $\\begin{array}{r}{\\alpha=1+\\frac{1}{\\log w_{q}}}\\end{array}$ . So, ", "page_idx": 15}, {"type": "equation", "text": "$$\nb=\\alpha\\cdot\\frac{\\beta w_{q}}{1-w_{q}}-w_{q}\\beta\\cdot\\frac{\\alpha}{1-w_{q}}\\pm O(\\beta^{2}w_{q})=\\pm O(\\beta^{2}w_{q}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{|\\frac{b}{a}|\\leq O(w_{q}\\cdot(\\log w_{q})^{2})\\leq w_{q}^{1-o(1)}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Finally we check the endpoint cases of Theorem 3.4. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.8. For F(tu, tq) defined in Equation 2, and for \u03b1 = 1 +log1 wq , $F(t_{u},0)\\geq\\alpha-w_{q}^{1-o(1)}$ where $o(1)$ goes to 0 as $w_{q}$ goes to $0$ , uniformly over $t_{u}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. For $t_{q}=0$ , we can calculate that ", "page_idx": 15}, {"type": "equation", "text": "$$\nF=\\frac{t_{u}\\log\\left(\\frac{t_{u}}{1/2-w_{q}}\\right)-t_{u}\\log(2t_{u})-\\alpha d(t_{q}||w_{q})}{d(t_{u}||1/2)}=\\alpha+\\frac{\\alpha\\log(1-w_{q})-t_{u}\\log(1-2w_{q})}{d(t_{u}||1/2)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let us first consider the term $\\alpha\\log(1-w_{q})-t_{u}\\log(1-2w_{q})$ . If we were to set $t_{u}=1/2$ , this equals ", "page_idx": 15}, {"type": "text", "text": "$\\left(1+\\frac{1}{\\log w_{q}}\\right)\\cdot\\log(1-w_{q})-\\frac{1}{2}\\log(1-2w_{q})=\\log(1-w_{q})-\\frac{1}{2}\\log(1-2w_{q})+\\frac{1}{\\log w_{q}}\\cdot\\log(1-w_{q})=-\\frac{1}{2}\\log(1-w_{q}),$ wq \u00b1O(wq2). log wq ", "page_idx": 15}, {"type": "text", "text": "For sufficiently small $w_{q}$ , $|\\log(1-2w_{q})|\\leq3w_{q}$ , which means that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha\\log(1-w_{q})-t_{u}\\log(1-2w_{q})=-\\frac{w_{q}}{\\log w_{q}}\\pm O(w_{q}^{2}+|t_{u}-1/2|\\cdot w_{q}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that \u2212log qwq is positive, since $\\log{w_{q}}$ is negative. If $w_{q}$ is sufficiently small and $|t_{u}-1/2|\\leq$ $-\\frac{c}{\\log w_{q}}$ for some sufficiently small constant $c$ , then the term $O(w_{q}^{2}+|t_{u}-1/2|\\cdot w_{q})$ is smaller than \u2212logw qwq , which means \u03b1 log(1 \u2212wq) \u2212tu log(1 \u22122wq) \u22650. Because d(tu||1/2) is nonnegative, this means $F\\,\\geq\\,\\alpha\\,\\geq\\,\\alpha\\,-\\,w_{q}^{1-o(1)}$ . Alternatively, if $|t_{u}\\,-\\,1/2|\\,\\,\\geq\\,-\\frac{c}{\\log w_{q}}$ , then we still have $\\alpha\\log(1-w_{q})-t_{u}\\log(1-2w_{q})\\geq-O(w_{q}^{2}+|t_{u}-1/2|\\cdot w_{q})\\geq-O(w_{q})$ , and by Proposition A.3, $\\begin{array}{r}{d(t_{u}||1/2)=H(t_{u})=\\Theta((t_{u}-1/2)^{2})\\ge\\Omega\\left(\\left(\\frac{1}{\\log w_{q}}\\right)^{2}\\right)}\\end{array}$ . So, $F\\geq\\alpha-O\\left(w_{q}\\cdot(\\log w_{q})^{2}\\right)\\geq$ $\\alpha-w_{q}^{1-o(1)}$ wq1\u2212o(1). So, in either case, we have that F(tu, 0) \u2265\u03b1 \u2212wq1\u2212 o(1), where the o(1) term does not depend on $t_{u}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B Appendix: Omitted proofs of section 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma B.1. The expected space usage of Algorithm 1 is $O(L\\ell+L k2^{-\\ell}+n k)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. The algorithm has to store each of the $L$ sets $S_{i}$ which requires space $O(L\\ell)$ . Furthermore, it needs to store the sets $A_{i}$ which each have expected size $O(2^{-\\ell}k)$ . Indeed, the probability that a random subset of size $\\ell$ is contained in a given $T_{j}$ is at most $2^{-\\ell}$ . Finally, the algorithm needs to store the sets $T_{j}$ which takes $O(n k)$ space. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma B.2. The expected query time of Algorithm 2 is $\\begin{array}{r}{O(L\\ell+\\frac{k}{\\varepsilon}(1-\\varepsilon/2)^{\\ell}+\\frac{n}{s})}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. First, the algorithm forms the set $Q$ which takes $O(n/s)$ time. Then, the algorithm goes over the $L$ sets $S_{i}$ until it finds an $i$ such that $S_{i}\\subset Q$ . This takes time $O(L\\ell)$ . Next, the algorithm goes through the indices $j\\in A_{i}$ . For each such $j$ , it samples the set $U_{j}$ one element at a time checking if $U_{j}\\,\\subset\\,T_{j}$ . Let us first bound the expected size of $A_{i}$ . We clearly have that $i^{*}\\in A_{i}$ . Indeed, $S_{i}\\subset Q\\subset T_{i^{*}}$ and $A_{i}$ lists all the $j$ such that $S_{i}\\subset T_{j}$ . Now for a $j\\in[k]$ with $j\\neq i^{*}$ , the assumption that $\\|p_{j}-p_{i^{*}}\\|_{1}>\\varepsilon$ , gives that $\\begin{array}{r}{|T_{j}\\cap T_{i^{*}}|\\leq n(\\frac{1}{2}-\\frac{\\varepsilon}{4})}\\end{array}$ . As the sampling of $S_{1},...,S_{L}$ and $Q$ are independent, we can view $S_{i}$ as a random size- $\\bar{\\ell}$ subset of $T_{i^{*}}$ . In particular, the probability that $S_{i}\\subset T_{j}$ can be upper bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\frac{|T_{j}\\cap T_{i^{*}}|}{|T_{i^{*}}|}\\right)^{\\ell}\\leq\\left(\\frac{n\\left(1/2-\\varepsilon/4\\right)}{n/2}\\right)^{\\ell}=(1-\\varepsilon/2)^{\\ell}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and so, the expected size of $\\left|A_{i}\\right|$ is at most $k(1-\\varepsilon/2)^{\\ell}$ . Finally, using the assumption that $\\|p_{i^{*}}-$ $p_{j}\\|_{1}>\\varepsilon$ for $j\\neq i^{*}$ , and that $n/s\\gg(\\log k)/\\varepsilon^{2}$ , by a standard concentration bound, it holds for any $j\\neq i^{*}$ that $|Q\\cap T_{j}|\\leq|Q|(1-\\varepsilon/4)$ with high probability in $k$ . In particular, for $j\\neq i^{*}$ , we only expect to include $O(1/\\varepsilon)$ samples in $U_{j}$ before observing that $U_{j}\\subsetneq T_{j}$ . In conclusion, the expected query time is $\\begin{array}{r}{O(\\frac{n}{s}+L\\ell+\\frac{k}{\\varepsilon}(1-\\varepsilon/2)^{\\ell})}\\end{array}$ , as desired. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma B.3. Let $\\begin{array}{r}{L=C\\left(\\frac{2}{1-e^{-2/s}}\\right)^{\\ell}}\\end{array}$ for a sufficiently large constant $C$ . Assume that $L=k^{O(1)}$ .   \nThen Algorithm 2 returns $i^{*}$ with probability at least 0.99. ", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r}{\\ell=O(\\frac{\\log k}{\\log s})=O(\\frac{\\log n}{\\log s})}\\end{array}$ .k eWd et hreatc $\\ell=O(\\log k)=O(\\log n)$ , and further, for $s>10$ , it holds that ", "page_idx": 16}, {"type": "text", "text": "Note that by standard concentration bounds, it holds with high probability in $k$ that $Q\\subset T_{j}$ only for $j=i^{*}$ . In fact, as in the previous proof, for all $j\\neq i^{*}$ , $|Q\\cap T_{j}|\\leq|Q|(1-\\varepsilon/4)$ with high probability in $k$ . In particular, this implies that the algorithm with high probability never returns a $j\\neq i^{*}$ . Indeed, by a union bound, the probability of this happening is at most ", "page_idx": 16}, {"type": "equation", "text": "$$\nk\\operatorname*{Pr}[U_{j}\\subset T_{j}]\\leq k(1-\\varepsilon/4)^{|U_{j}|}=k(1-\\varepsilon/4)^{C\\log n/\\varepsilon}\\leq k n^{-C/4}\\leq n^{-10},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used that $k$ and $n$ are polynomially related and $C$ is sufficiently large. In order for the algorithm to succeed, it therefore suffices to show that there exists an $i\\in[L]$ such that $S_{i}\\subset Q$ . In that case, the algorithm will indeed return $p_{i^{*}}$ with high probability. ", "page_idx": 16}, {"type": "text", "text": "The expected size of $Q$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[|Q|]=\\frac{n}{2}\\left(1-\\left(1-\\frac{2}{n}\\right)^{n/s}\\right)\\ge\\frac{n}{2}\\left(1-e^{-2/s}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and by a standard application of Azuma\u2019s inequality, it holds with high probability in $n$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|Q|={\\frac{n}{2}}\\left(1-e^{-2/s}\\right)-O((n/s)^{0.51}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that the probability that a single set $S_{i}$ is contained in a fixed set $Q$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\prod_{i=0}^{\\ell-1}\\left({\\frac{|Q|-i}{n-i}}\\right)\\geq\\left({\\frac{|Q|-\\ell}{n}}\\right)^{\\ell}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the bounds on $\\ell$ in the beginning of the proof, we find that $\\ell\\ll O((n/s)^{0.51})$ . In particular, conditioning on the high probability event of Equation (6), the probability in Equation (7) is at least, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\frac{1-e^{-2/s}}{2}-O\\left(\\frac{1}{n^{0.49}s^{0.51}}\\right)\\right)^{\\ell}\\geq c\\left(\\frac{1-e^{-2/s}}{2}\\right)^{\\ell},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some constant $c>0$ . The probability that no $S_{i}$ is contained in $Q$ is at most ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(1-c\\left(\\frac{1-e^{-2/s}}{2}\\right)^{\\ell}\\right)^{L}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We thus choose $\\begin{array}{r}{L=\\frac{5}{c}\\left(\\frac{2}{1-e^{-2/s}}\\right)^{\\ell}}\\end{array}$ to ensure that this probability is at most $e^{-5}<1/100$ and the result follows. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "We can now prove our main theorem of Section 4. ", "page_idx": 17}, {"type": "text", "text": "Theorem 4.2. Suppose n and $k$ are polynomially related, $s\\geq2$ , and that $s$ is such that5 $\\begin{array}{r}{\\frac{n}{s}\\geq C\\frac{\\log k}{\\varepsilon^{2}}}\\end{array}$ for a sufficiently large constant $C$ . Let $\\varepsilon>0$ and $\\rho_{u}>0$ be given. There exists a data structure for the $H U D E(s,\\varepsilon)$ problem using space $O(k^{1+\\rho_{u}}+n k)$ and with query time $O\\left(k^{1-\\frac{\\varepsilon\\rho_{u}}{2\\log(2s)}}+n/s\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 4.2. Let us pick $L=k^{\\rho_{u}}$ . We further choose $\\ell$ such that $\\begin{array}{r}{L=C\\left(\\frac{2}{1-e^{-2/s}}\\right)^{\\ell}}\\end{array}$ for some sufficiently large constant $C$ as in Lemma B.3. In particular, $\\ell\\leq\\rho_{u}\\log(k)$ , so we obtain that the space bound from Lemma B.1 is $O(k^{1+\\rho_{u}}+n k)$ . On the other hand, the bound on the query time in Lemma B.2 is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\supset\\left(k^{\\rho_{u}}\\log k+\\frac{k}{\\varepsilon}(1-\\varepsilon/2)^{\\ell}+n/s\\right)=O\\left(k^{\\rho_{u}}\\log k+\\frac{1}{\\varepsilon}k^{1+\\rho_{u}\\log(1-\\frac{\\varepsilon}{2})/\\log\\left(\\frac{2}{1-e^{-2/s}}\\right)}+n/s\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as desired. Finally, by the choice of $L$ and $\\ell$ , it follows by Lemma B.3 that the algorithm returns $i^{*}$ with probability at least 0.99. ", "page_idx": 17}, {"type": "text", "text": "Note that while vastly simplified from the expression of Theorem 3.2 from [3], the expression for the query time in Theorem 4.2 is unwieldy. For a simplified version of the theorem, one can note that $\\textstyle\\bar{\\log(1-\\frac{\\varepsilon}{2})}\\leq-\\varepsilon/2$ and for $s\\geq2$ , we have that $\\frac{\\varepsilon}{1-e^{-2/s}}\\leq2s$ , resulting in the claimed bound. ", "page_idx": 17}, {"type": "text", "text": "Remark B.4. Theorem 4.2 is stated for the promise problem of Definition 4.1 where all distributions $p\\in P$ with $p\\neq p_{i^{*}}$ have $\\|p-p_{i^{*}}\\|_{1}\\geq\\varepsilon$ . However, even if this condition is not met, we can still guarantee to return a distribution $p_{j}$ such that $\\|p_{j}-p^{i_{*}}\\|\\leq\\varepsilon$ with probability 0.99 and only a slight increase in the query time. Indeed, as in the proof of Lemma B.3 as long as there exists an $S_{i}\\subset Q$ , the list $A_{i}$ will contain $p_{i^{*}}$ . Moreover, as in the proof of that lemma, the algorithm will never return a $p_{j}$ with $\\|p_{j}-p_{i^{*}}\\|>\\varepsilon$ . Thus it will either return $p_{i^{*}}$ when it encounters it in the list $A_{i}$ , or a distribution $p_{j}$ with $\\|p_{j}-p_{i^{*}}\\|\\leq\\varepsilon$ . The only difference is that now the bound on the number of samples included in $U_{j}$ in Lemma B.2 becomes $O(\\log n/\\varepsilon)$ instead of $O(1/\\varepsilon)$ with a corresponding blow up by a $\\log n$ factor in the query time. ", "page_idx": 17}, {"type": "text", "text": "C Remark about experimental setting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Remark C.1. The prior work of [1] also tested their \u201cFastTournament\u201d algorithm on random halfuniform distributions. That algorithm works for the general problem and not just flat distributions, but its generality makes it is much less efficient for the special case we consider. From their experiments, in the setting where $k=8192$ , $n=500$ , and $S=50$ , the best setting of parameters achieves less than $98\\%$ accuracy using more than 400000 operations (their notion of operation corresponds to querying the probability mass at a specific index for two distributions while our notion of operation corresponds to querying whether a specific index is in the support of a single distribution/sample). For a comparable setting of $k\\,=\\,10000$ , $n=500$ , $S=50$ , our algorithm uses fewer then 20000 operations, more than a $20\\times$ improvement. In addition to having much better query time than the general algorithm, our algorithm has subquadratic preprocessing time of $\\bar{O(k L\\ell)}$ while the tournament-based algorithm requires $O(k^{2}n)$ time which is prohibitive for the parameter settings we test. For these reasons, we restrict our comparisons to our Subset algorithm and the Elimination algorithm baseline, both tailored for the half-uniform setting. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All theorem statements have full proofs. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We lay out the assumptions underlying our results and discuss our bounds in comparison to prior work in the introduction. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All theoretical claims have full proofs. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Full experimental details given in Section 5. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Full experimental details given in Section 5. Code is attached in the supplementary material. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Full experimental details given in Section 5. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We do not report error bars but run experiments over 100 randomly chosen queries to boost significance. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Full experimental details given in Section 5. The experiments are not computationally intensive for the parameter regime we study. ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper adheres to the code of ethics. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: The work is theoretical in nature and we do not envision any societal impacts. ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: No new data or models were generated as part of this project. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes, we use open-source code from [1]. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: The experimental code is a lightweight modification of an existing code base as a proof-of-concept for our simple upper bound, so we do not add extensive documentation. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: No human subjects. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: No human subjects. ", "page_idx": 20}]