[{"figure_path": "npoHt6WV1F/tables/tables_7_1.jpg", "caption": "Table 1: Restricted-access scenario: Transfer results on CIFAR-10 with 1.5% bit-error rate", "description": "This table presents the results of the transferability experiment performed under a restricted-access scenario.  The experiment evaluated the performance of NeuralFuse trained on a surrogate model and then transferred to various target models (ResNet18, ResNet50, VGG11, VGG16, VGG19). The table shows the clean accuracy (CA), perturbed accuracy (PA), clean accuracy with NeuralFuse (CA (NF)), perturbed accuracy with NeuralFuse (PA (NF)), and the recovery percentage (RP) for each combination of surrogate model, target model, and bit error rate (0.5% and 1%).  The results demonstrate the transferability and robustness of NeuralFuse.", "section": "4.3 Performance Evaluation, Restricted-access Scenario (Transferability)"}, {"figure_path": "npoHt6WV1F/tables/tables_7_2.jpg", "caption": "Table 2: Energy saving (%) by NeuralFuse for 30 combinations of base models and generators.", "description": "This table shows the energy savings achieved by using NeuralFuse with various base models (ResNet18, ResNet50, VGG11, VGG16, and VGG19) and NeuralFuse generators (ConvL, ConvS, DeConvL, DeConvS, UNetL, and UNetS).  The energy savings are calculated at a 1% bit-error rate.  The values represent the percentage reduction in dynamic memory access energy when using NeuralFuse compared to the bit-error-free (nominal) voltage.", "section": "4.4 Energy/Accuracy Tradeoff"}, {"figure_path": "npoHt6WV1F/tables/tables_8_1.jpg", "caption": "Table 3: The efficiency ratio for all NeuralFuse generators.", "description": "This table presents the efficiency ratio for different NeuralFuse generators. The efficiency ratio is calculated as the recovery percentage in perturbed accuracy divided by the NeuralFuse's parameter count.  It compares the efficiency ratios of all NeuralFuse generators trained on CIFAR-10, considering different base models (ResNet18, ResNet50, VGG11, VGG16, and VGG19) and bit error rates (0.5% and 1%). This allows for a comparison of the effectiveness of different generator architectures in terms of accuracy recovery per parameter.", "section": "4.5 Model Size and NeuralFuse Efficiency"}, {"figure_path": "npoHt6WV1F/tables/tables_13_1.jpg", "caption": "Table 4: Model architecture for both Convolution-based and Deconvolution-based generators. Each ConvBlock consists of a Convolution (kernel = 3 \u00d7 3, padding = 1, stride = 1), a Batch Normalization, and a ReLU layer. Each DeConvBlock consists of a Deconvolution (kernel = 4 \u00d7 4, padding = 1, stride = 2), a Batch Normalization, and a ReLU layer.", "description": "This table details the architecture of the Convolution-based and Deconvolution-based NeuralFuse generators. It specifies the layers, including ConvBlocks (Convolution, Batch Normalization, ReLU), MaxPool, UpSample, and DeConvBlocks (Deconvolution, Batch Normalization, ReLU) layers for both large (L) and small (S) versions of each generator type.  The #CHs column indicates the number of channels for each layer. The table helps in understanding the structural differences and complexities of the different generator designs.", "section": "Implementation Details"}, {"figure_path": "npoHt6WV1F/tables/tables_13_2.jpg", "caption": "Table 1: Restricted-access scenario: Transfer results on CIFAR-10 with 1.5% bit-error rate", "description": "This table presents the results of transfer learning experiments conducted on the CIFAR-10 dataset using the NeuralFuse framework in a restricted access scenario.  The experiments involved training NeuralFuse generators (ConvL and UNetL) on a source model (SM) with a 1.5% bit error rate (BER) and then transferring them to various target models (TM) with different bit error rates (0.5% and 1%). The table shows the clean accuracy (CA), perturbed accuracy (PA), clean accuracy with NeuralFuse (CA(NF)), perturbed accuracy with NeuralFuse (PA(NF)), and the recovery percentage (RP) for each combination of source and target models and bit error rates.  The RP indicates the percentage improvement in perturbed accuracy achieved by NeuralFuse.", "section": "4.3 Performance Evaluation, Restricted-access Scenario (Transferability)"}, {"figure_path": "npoHt6WV1F/tables/tables_14_1.jpg", "caption": "Table 6: The total weights memory access calculated by SCALE-SIM.", "description": "This table presents the total number of weight memory accesses for different base models (ResNet18, ResNet50, VGG11, VGG16, VGG19) and NeuralFuse generators (ConvL, ConvS, DeConvL, DeConvS, UNetL, UNetS).  The values are calculated using the SCALE-SIM simulator.  The total weight memory access (T.W.M.A) for each model is shown, providing a quantitative measure of memory usage related to model weights.", "section": "4.4 Energy/Accuracy Tradeoff"}, {"figure_path": "npoHt6WV1F/tables/tables_14_2.jpg", "caption": "Table 6: The total weights memory access calculated by SCALE-SIM.", "description": "This table presents the total number of weight memory accesses (T.W.M.A.) for different base models (ResNet18, ResNet50, VGG11, VGG16, VGG19) and NeuralFuse generators (ConvL, ConvS, DeConvL, DeConvS, UNetL, UNetS) calculated using the SCALE-SIM simulator.  The T.W.M.A. values are crucial for determining the overall energy consumption of the models in the context of SRAM access energy.", "section": "C NeuralFuse's Energy/Accuracy Tradeoff"}, {"figure_path": "npoHt6WV1F/tables/tables_15_1.jpg", "caption": "Table 2: Energy saving (%) by NeuralFuse for 30 combinations of base models and generators.", "description": "This table presents the energy savings achieved by using NeuralFuse with various base models (ResNet18, ResNet50, VGG11, VGG16, and VGG19) and generators (ConvL, ConvS, DeConvL, DeConvS, UNetL, and UNetS).  The energy savings are calculated as the percentage reduction in dynamic memory access energy at low voltage with a 1% bit error rate, compared to the nominal voltage without bit errors. The table shows the effectiveness of NeuralFuse in reducing energy consumption while maintaining accuracy.", "section": "4.4 Energy/Accuracy Tradeoff"}, {"figure_path": "npoHt6WV1F/tables/tables_16_1.jpg", "caption": "Table 9: The Inference Latency of base model and base model with NeuralFuse.", "description": "This table shows the inference latency of base models and base models with NeuralFuse in milliseconds (ms). The latency is measured for different models (ResNet18 and VGG19) on two datasets (CIFAR-10 and ImageNet-10) and with different NeuralFuse generators (ConvL, ConvS, DeConvL, DeConvS, UNetL, and UNetS). The numbers in parentheses indicate the increase in latency compared to the base model for each generator.", "section": "D Inference Latency of NeuralFuse"}, {"figure_path": "npoHt6WV1F/tables/tables_17_1.jpg", "caption": "Table 10: Experimental results based on \u03bb value choosing. The results show that \u03bb = 5 can balance the tradeoff between clean accuracy and perturbed accuracy.", "description": "This table presents the results of experiments conducted to determine the optimal value of the hyperparameter \u03bb in the NeuralFuse training objective function.  The objective function balances the importance of maintaining accuracy on clean (unperturbed) inputs versus accuracy on inputs perturbed by simulated bit errors due to low voltage.  Different values of \u03bb were tested on ResNet18 and VGG19 models, and the clean accuracy (CA), perturbed accuracy (PA), and the percentage accuracy recovery (RP) provided by NeuralFuse are reported.  The results show that \u03bb = 5 provides a good balance between clean and perturbed accuracy.", "section": "Ablation Studies"}, {"figure_path": "npoHt6WV1F/tables/tables_17_2.jpg", "caption": "Table 11: Performance of the universal input perturbation (UIP) trained by EOPM on CIFAR-10 pre-trained ResNet18.", "description": "This table presents the results of an experiment comparing the performance of NeuralFuse against a simple baseline method called Universal Input Perturbation (UIP).  The experiment uses a CIFAR-10 pre-trained ResNet18 model and evaluates performance under different bit error rates (BER). The table shows the clean accuracy (CA), perturbed accuracy (PA), and the recovery percentage (RP) for both NeuralFuse and UIP.  The RP metric indicates the improvement in perturbed accuracy achieved by each method relative to the baseline.", "section": "4.3 Performance Evaluation, Restricted-access Scenario (Transferability)"}, {"figure_path": "npoHt6WV1F/tables/tables_18_1.jpg", "caption": "Table 1: Restricted-access scenario: Transfer results on CIFAR-10 with 1.5% bit-error rate", "description": "This table presents the results of the transferability experiment conducted under the restricted access scenario on the CIFAR-10 dataset with a 1.5% bit error rate.  It shows the performance of different base models (ResNet18, ResNet50, VGG11, VGG16, VGG19) when using NeuralFuse generators trained on a surrogate model (ResNet18 or VGG19) at 1.5% bit-error rate. The results are compared with and without NeuralFuse at 0.5% and 1% bit-error rates. The table includes clean accuracy (CA), perturbed accuracy (PA), clean accuracy with NeuralFuse (CA(NF)), perturbed accuracy with NeuralFuse (PA(NF)), and recovery percentage (RP) for each model and generator combination.", "section": "4.3 Performance Evaluation, Restricted-access Scenario (Transferability)"}, {"figure_path": "npoHt6WV1F/tables/tables_19_1.jpg", "caption": "Table 1: Restricted-access scenario: Transfer results on CIFAR-10 with 1.5% bit-error rate", "description": "This table presents the results of transfer learning experiments on CIFAR-10, where NeuralFuse models were trained on a source model with a 1.5% bit error rate and then tested on various target models with different bit error rates (0.5% and 1%). The table shows the clean accuracy (CA), perturbed accuracy (PA), and the recovery percentage (RP) for each combination.  It demonstrates NeuralFuse's ability to transfer knowledge learned from a high bit-error-rate setting to achieve accuracy improvements in lower bit-error-rate scenarios. Different source (SM) and target (TM) models (ResNet18, ResNet50, VGG11, VGG16, VGG19) and generator types (ConvL, UNetL) are evaluated.", "section": "4.3 Performance Evaluation, Restricted-access Scenario (Transferability)"}, {"figure_path": "npoHt6WV1F/tables/tables_20_1.jpg", "caption": "Table 14: Testing accuracy under 0.5% of random bit error rate on ImageNet-10.", "description": "This table presents the results of testing the accuracy of various pre-trained models (ResNet18, ResNet50, VGG11, VGG16, VGG19) on the ImageNet-10 dataset under a 0.5% random bit error rate.  The models were tested both with and without NeuralFuse, and for each model, several different NeuralFuse generator architectures were applied (ConvL, ConvS, DeConvL, DeConvS, UNetL, UNetS). The table shows the clean accuracy (CA), the perturbed accuracy (PA), the accuracy with NeuralFuse (CA (NF)), the perturbed accuracy with NeuralFuse (PA (NF)), and the recovery percentage (RP).  The RP metric indicates the improvement in accuracy achieved by NeuralFuse in mitigating the impact of bit errors.", "section": "H.3 ImageNet-10"}, {"figure_path": "npoHt6WV1F/tables/tables_21_1.jpg", "caption": "Table 15: Testing accuracy (%) under 1%, 0.5% and 0.35% of random bit error rate on CIFAR-100.", "description": "This table presents the results of testing the accuracy of various models (ResNet18, ResNet50, VGG11, VGG16, VGG19) on the CIFAR-100 dataset under different bit error rates (1%, 0.5%, 0.35%).  For each model and bit error rate, the table shows the clean accuracy (CA), the perturbed accuracy (PA), the accuracy after applying NeuralFuse (CA (NF), PA (NF)), and the percentage recovery (RP).  The results demonstrate the effectiveness of NeuralFuse in improving model accuracy under various bit error conditions.", "section": "F.4 CIFAR-100"}, {"figure_path": "npoHt6WV1F/tables/tables_22_1.jpg", "caption": "Table 1: Restricted-access scenario: Transfer results on CIFAR-10 with 1.5% bit-error rate", "description": "This table presents the results of the transferability experiment in the restricted access scenario on the CIFAR-10 dataset using a 1.5% bit error rate.  It shows the clean accuracy (CA), perturbed accuracy (PA), and the recovery percentage (RP) achieved by NeuralFuse with various combinations of source models (SM) used for training the generators, target models (TM) used for testing, and the bit error rate (BER). The table aims to evaluate the performance of NeuralFuse in situations where access to the base model is restricted.", "section": "4.3 Performance Evaluation, Restricted-access Scenario (Transferability)"}, {"figure_path": "npoHt6WV1F/tables/tables_23_1.jpg", "caption": "Table 17: Transfer results on GTSRB: NeuralFuse trained on SM with 1.5% BER", "description": "This table presents the results of transfer learning experiments on the GTSRB dataset using NeuralFuse.  The model was trained on a source model (SM) with a 1.5% bit error rate (BER).  The table shows the clean accuracy (CA), perturbed accuracy (PA), and the recovery percentage (RP) for different target models (TM) and bit error rates (0.5% and 1%).  The results are shown separately for two different NeuralFuse generators: ConvL and UNetL.  It demonstrates the transferability of NeuralFuse trained at one bit error rate to other models and BERs.", "section": "G.2 GTSRB"}, {"figure_path": "npoHt6WV1F/tables/tables_23_2.jpg", "caption": "Table 16: Transfer results on CIFAR-10: NeuralFuse trained on SM with 1% BER", "description": "This table presents the results of transfer learning experiments on the CIFAR-10 dataset.  The NeuralFuse model was initially trained on a source model (SM) with a 1% bit error rate (BER).  The table shows the performance of the trained NeuralFuse model when applied to various target models (TM) at different BERs (0.5% and 1%). The columns show the source model, target model, BER, clean accuracy (CA), perturbed accuracy (PA), NeuralFuse clean accuracy (CA (NF)), NeuralFuse perturbed accuracy (PA (NF)), and the recovery percentage (RP).  The RP indicates the improvement in accuracy achieved by applying the NeuralFuse model, relative to the perturbed accuracy without NeuralFuse.", "section": "G.1 CIFAR-10"}, {"figure_path": "npoHt6WV1F/tables/tables_24_1.jpg", "caption": "Table 16: Transfer results on CIFAR-10: NeuralFuse trained on SM with 1% BER", "description": "This table presents the results of transfer learning experiments on the CIFAR-10 dataset.  The experiment uses NeuralFuse, a model-agnostic module, trained on a source model (SM) with a 1% bit-error rate (BER).  The table shows how well NeuralFuse, trained in this way, transfers to different target models (TM) operating under 0.5% and 1% BER.  The results are shown in terms of clean accuracy (CA), perturbed accuracy (PA), clean accuracy with NeuralFuse (CA (NF)), perturbed accuracy with NeuralFuse (PA (NF)), and the recovery percentage (RP) of PA (NF) compared to PA. This demonstrates the transferability of NeuralFuse trained on a single model and BER to other models and BERs. ", "section": "G.1 CIFAR-10"}, {"figure_path": "npoHt6WV1F/tables/tables_24_2.jpg", "caption": "Table 1: Restricted-access scenario: Transfer results on CIFAR-10 with 1.5% bit-error rate", "description": "This table presents the results of transfer learning experiments conducted in a restricted-access scenario.  NeuralFuse, a model-agnostic module, was trained on a surrogate model (SM) with a 1.5% bit-error rate (BER) and then transferred to various target models (TM) for testing.  The table shows the clean accuracy (CA), perturbed accuracy (PA) before and after applying NeuralFuse (CA(NF), PA(NF)), and the recovery percentage (RP) for different target models under 0.5% and 1% BER. The results demonstrate the transferability and effectiveness of NeuralFuse in access-limited settings.", "section": "4.3 Performance Evaluation, Restricted-access Scenario (Transferability)"}, {"figure_path": "npoHt6WV1F/tables/tables_25_1.jpg", "caption": "Table 1: Restricted-access scenario: Transfer results on CIFAR-10 with 1.5% bit-error rate", "description": "This table presents the results of transfer learning experiments conducted in a restricted-access scenario on the CIFAR-10 dataset with a 1.5% bit error rate.  It shows the performance of NeuralFuse generators trained on different source models (SM) applied to various target models (TM) under different bit error rates (BER). The table includes the clean accuracy (CA), perturbed accuracy (PA), clean accuracy with NeuralFuse (CA (NF)), perturbed accuracy with NeuralFuse (PA (NF)), and the recovery percentage (RP) which quantifies the improvement in accuracy due to NeuralFuse.  The goal is to evaluate the transferability of NeuralFuse trained in one setting to other models and bit error rates.", "section": "4.3 Performance Evaluation, Restricted-access Scenario (Transferability)"}, {"figure_path": "npoHt6WV1F/tables/tables_25_2.jpg", "caption": "Table 22: Reduced-precision Quantization and with 0.5% BER on CIFAR-10 pre-trained models.", "description": "This table presents the results of an experiment evaluating the impact of reduced-precision quantization on model accuracy, in the presence of a 0.5% bit error rate (BER) due to low voltage.  It compares the clean accuracy (CA), perturbed accuracy (PA), and the accuracy recovery percentage (RP) achieved by NeuralFuse using different numbers of bits for quantization (from 8 bits down to 2 bits).  The table shows the performance of two NeuralFuse generator architectures (ConvL and UNetL, both trained with a 1% BER) on two different base models (ResNet18 and VGG19).", "section": "H.1 CIFAR-10"}, {"figure_path": "npoHt6WV1F/tables/tables_26_1.jpg", "caption": "Table 22: Reduced-precision Quantization and with 0.5% BER on CIFAR-10 pre-trained models.", "description": "This table presents the results of an experiment evaluating the performance of NeuralFuse on CIFAR-10 pre-trained models under reduced-precision quantization and a 0.5% bit error rate.  It shows the clean accuracy (CA), perturbed accuracy (PA), and the accuracy recovery percentage (RP) achieved by NeuralFuse (ConvL and UNetL) for different bit quantization levels (8-2 bits).  The data demonstrates NeuralFuse's resilience to both low-precision and bit errors.", "section": "H.1 CIFAR-10"}, {"figure_path": "npoHt6WV1F/tables/tables_27_1.jpg", "caption": "Table 24: Reduced-precision Quantization and with 0.5% BER on ImageNet-10 pre-trained models.", "description": "This table presents the results of an experiment evaluating the impact of reduced-precision quantization on model accuracy, in the presence of a 0.5% bit error rate due to low voltage.  It shows clean accuracy (CA), perturbed accuracy (PA), and the accuracy recovered by NeuralFuse (CA(NF), PA(NF)) for different numbers of bits used for quantization.  The recovery percentage (RP) is also provided, indicating the effectiveness of NeuralFuse in mitigating the negative impact of both reduced precision and bit errors.  Two different NeuralFuse generator architectures, ConvL and UNetL, are compared.", "section": "H.3 ImageNet-10"}, {"figure_path": "npoHt6WV1F/tables/tables_28_1.jpg", "caption": "Table 25: Performance of the generator trained by adversarial training under K flip bits on ResNet18 with CIFAR-10. The results show that the generator trained by adversarial training cannot achieve high accuracy against bit errors under a 1% bit error rate.", "description": "This table presents the results of an experiment evaluating the performance of a generator trained using adversarial training.  The experiment used ResNet18 on the CIFAR-10 dataset. The goal was to assess the generator's ability to maintain high accuracy when facing bit errors (simulated by flipping bits in the model's weights). The table shows that the adversarial training approach was not successful in achieving this, demonstrating a limitation in the method for addressing the impact of random bit errors.", "section": "Additional Experiments on Adversarial Training"}, {"figure_path": "npoHt6WV1F/tables/tables_28_2.jpg", "caption": "Table 26: Performance of NeuralFuse trained with robust CIFAR-10 pre-trained PreAct ResNet18. The results show that NeuralFuse can be used together with a robust model and further improve perturbed accuracy under both 1% and 0.5% BER", "description": "This table presents the results of applying NeuralFuse to a robust model (PreAct ResNet18) pre-trained on CIFAR-10.  It evaluates the performance of different NeuralFuse generator architectures (ConvL, ConvS, DeConvL, DeConvS, UNetL, UNetS) at both 1% and 0.5% bit error rates (BER). The table shows the clean accuracy (CA), perturbed accuracy (PA), and the percentage recovery (RP) achieved by NeuralFuse for each generator architecture and BER.  The RP values indicate how much accuracy was recovered by NeuralFuse compared to the PA without NeuralFuse.", "section": "Additional Experiments on Adversarial Training"}]