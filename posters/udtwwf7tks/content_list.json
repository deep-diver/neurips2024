[{"type": "text", "text": "Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ashwin Ramachandran1\u2217 Vaibhav Raj2\u2217 Indrayumna Roy2 Soumen Chakrabarti2 Abir De2 1UC San Diego 2IIT Bombay ashwinramg@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "{vaibhavraj, indraroy15, soumen, abir}@cse.iitb.ac.in ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph retrieval based on subgraph isomorphism has several real-world applications such as scene graph retrieval, molecular fingerprint detection and circuit design. Roy et al. [35] proposed IsoNet, a late interaction model for subgraph matching, which first computes the node and edge embeddings of each graph independently of paired graph and then computes a trainable alignment map. Here, we present $\\mathrm{IsoNet++}$ , an early interaction graph neural network (GNN), based on several technical innovations. First, we compute embeddings of all nodes by passing messages within and across the two input graphs, guided by an injective alignment between their nodes. Second, we update this alignment in a lazy fashion over multiple rounds. Within each round, we run a layerwise GNN from scratch, based on the current state of the alignment. After the completion of one round of GNN, we use the last-layer embeddings to update the alignments, and proceed to the next round. Third, $\\mathrm{IsoNet++}$ incorporates a novel notion of node-pair partner interaction. Traditional early interaction computes attention between a node and its potential partners in the other graph, the attention then controlling messages passed across graphs. In contrast, we consider node pairs (not single nodes) as potential partners. Existence of an edge between the nodes in one graph and non-existence in the other provide vital signals for refining the alignment. Our experiments on several datasets show that the alignments get progressively refined with successive rounds, resulting in significantly better retrieval performance than existing methods. We demonstrate that all three innovations contribute to the enhanced accuracy. Our code and datasets are publicly available at https://github.com/structlearning/isonetpp. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In graph retrieval based on subgraph isomorphism, the goal is to identify a subset of graphs from a corpus, denoted $\\{G_{c}\\}$ , wherein each retrieved graph contains a subgraph isomorphic to a given query graph $G_{q}$ . Numerous real-life applications, e.g., molecular fingerprint detection [6], scene graph retrieval [16], circuit design [29] and frequent subgraph mining [43], can be formulated using subgraph isomorphism. Akin to other retrieval systems, the key challenge is to efficiently score corpus graphs against queries. ", "page_idx": 0}, {"type": "text", "text": "Recent work on neural graph retrieval [1, 2, 11, 22, 23, 35, 31, 46] has shown significant promise. Among them, Lou et al. [23, Neuromatch] and Roy et al. [35, IsoNet] focus specifically on subgraph isomorphism. They employ graph neural networks (GNNs) to obtain embeddings of query and corpus graphs and compute the relevance score using a form of order embedding [39]. In addition, IsoNet also approximates an injective alignment between the query and corpus graphs. These two models operate in a late interaction paradigm, where the representations of the query and corpus graphs are computed independent of each other. In contrast, GMN [22] is a powerful early interaction network for graph matching, where GNNs running on $G_{q}$ and $G_{c}$ interact with each other at every layer. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Conventional wisdom suggests that early interaction is more accurate (even if slower) than late interaction, but GMN was outperformed by IsoNet. This is because of the following reasons. (1) GMN does not explicitly infer any alignment between $G_{q}$ and $G_{c}$ . The graphs are encoded by two GNNs that interact with each other at every layer, mediated by attentions from each node in one graph on nodes in the other. These attentions are functions of node embeddings, so they change from layer to layer. While these attentions may be interpreted as approximate alignments, they induce at best non-injective mappings between nodes. (2) In principle, one wishes to propose a consistent alignment across all layers. However, GMN\u2019s attention based \u2018alignment\u2019 is updated in every layer. (3) GMN uses a standard GNN that is known to be an over-smoother [36, 40]. Due to this, the attention weights (which depend on the over-smoothed node representations) also suffer from oversmoothing. These limitations raise the possibility of a third approach based on early interaction networks, enabled with explicit alignment structures, that have the potential to outperform both GMN and IsoNet. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We present $\\mathrm{IsoNet++}$ , an early interaction network for subgraph matching that maintains a chain of explicit, iteratively refined, injective, approximate alignments between the two graphs. ", "page_idx": 1}, {"type": "text", "text": "Early interaction GNNs with alignment refinement We design early interaction networks for scoring graph pairs, that ensure the node embeddings of one graph are influenced by both its paired graph and the alignment map between them. In contrast to existing works, we model alignments as an explicit \u201cdata structure\u201d. An alignment can be defined between either nodes or edges, thus leading to two variants of our model: $\\mathrm{IsoNet++}$ (Node) and $\\mathrm{IsoNet++}$ (Edge). Within IsoNet++, we maintain a sequence of such alignments and refine them using GNNs acting on the two graphs. These alignments mediate the interaction between the two GNNs. In our work, we realize the alignment as a doubly stochastic approximation to a permutation matrix, which is an injective mapping by design. ", "page_idx": 1}, {"type": "text", "text": "Eager or lazy alignment updates In our work, we view the updates to the alignment maps as a form of gradient-based updates in a specific quadratic assignment problem or asymmetric GromovWasserstein (GW) distance minimization [30, 41]. The general form of $\\mathrm{IsoNet++}$ allows updates that proceed lockstep with GNN layers (eager layer-wise updates), but it also allows lazy updates. Specifically, IsoNet++ can perform $T$ rounds of updates to the alignment, each round including $K$ layers of GNN message passing. During each round, the alignment is held fixed across all propagation layers in GNN. At the end of each round, we update the alignment by feeding the node embeddings into a neural Gumbel-Sinkhorn soft permutation generator [10, 26, 37]. ", "page_idx": 1}, {"type": "text", "text": "Node-pair partner interaction between graphs The existing remedies to counter oversmoothing [8, 33, 40] entail extra computation; but they may be expensive in an early-interaction setting. Existing early interaction models like [22] perform node partner interaction; interactions are constrained to occur between a node and it\u2019s partner, the node in the paired graph aligned with it. Instead, we perform node-pair partner interaction; the interaction is expanded to include the node-pairs (or edges) in the paired graph that correspond to node-pairs containing the node. Consequently, the embedding of a node not only depends on nodes in the paired graph that align with it, but also captures signals from nodes in the paired graph that are aligned with its neighbors. ", "page_idx": 1}, {"type": "text", "text": "Experiments The design components of $\\mathrm{IsoNet++}$ and their implications are subtle \u2014 we report on extensive experiments that tease out their effects. Our experiments on real world datasets show that, $\\scriptstyle\\mathrm{IsoNet++}$ outperforms several state-of-the-art methods for graph retrieval by a substantial margin. Moreover, our results suggest that capturing information directly from node-pair partners can improve representation learning, as compared to taking information only from node partner. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation Given graph $G=(V,E)$ , we use $\\operatorname{nbr}(u)$ to denote the neighbors of a node $u\\in V$ . We use $u\\rightarrow v$ to indicate a message flow from node $u$ to node $v$ . Given a set of corpus graphs $C=\\{G_{c}\\}$ and a query graph $G_{q}$ , we denote $y(G_{c}\\,|\\,G_{q})$ as the binary relevance label of $G_{c}$ for $G_{q}$ . Motivated by several real life applications like substructure search in molecular graphs [12], object search in scene graphs [16], and text entailment [20], we consider subgraph isomorphism to significantly influence the relevance label, similar to previous works [23, 35]. Specifically, $\\bar{y}(G_{c}\\,|\\,\\bar{G_{q}})=1$ when $G_{q}$ is a subgraph of $G_{c}$ , and 0 otherwise. We define $C_{q+}\\subseteq C$ as the set of corpus graphs that are relevant to $G_{q}$ and set $C_{q-}\\,=\\,C\\backslash C_{q+}$ . Mildly overloading notation, we use $_{P}$ to indicate a \u2018hard\u2019 (0/1) permutation matrix or its \u2018soft\u2019 doubly-stochastic relaxation. $B_{n}$ denotes the set of all $n\\times n$ doubly stochastic matrices, and $\\Pi_{n}$ denotes the set of all $n\\times n$ permutation matrices. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "IsoNet [35] Given a graph $G=(V,E)$ , IsoNet uses a GNN, which initializes node representations $\\{h_{0}(u):u\\in V\\}$ using node-local features. Then, messages are passed between neighboring nodes in $K$ propagation layers. In the $k$ th layer, a node $u$ receives messages from its neighbors, aggregates them, and then combines the result with its state after the $(k-1)$ th layer: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{h}_{\\boldsymbol{k}}(\\boldsymbol{u})=\\operatorname{comb}_{\\boldsymbol{\\theta}}\\big(\\boldsymbol{h}_{\\boldsymbol{k}-1}(\\boldsymbol{u}),\\sum_{\\boldsymbol{v}\\in\\mathfrak{n b r}(\\boldsymbol{u})}\\left\\{\\operatorname{msg}_{\\boldsymbol{\\theta}}(\\boldsymbol{h}_{\\boldsymbol{k}-1}(\\boldsymbol{u}),\\boldsymbol{h}_{\\boldsymbol{k}-1}(\\boldsymbol{v}))\\right\\}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\operatorname*{msg}_{\\theta}(\\cdot)$ and $\\operatorname{comb}_{\\theta}(\\cdot,\\cdot)$ are suitable networks with parameters collectively called $\\theta$ . Edges may also be featurized and influence the messages that are aggregated [24]. The node representations at the final propagation layer $K$ can be collected into the matrix $H=\\{h_{K}(u)\\,|\\,u\\in\\,\\mathbf{\\bar{V}}\\}$ . Given a node $u\\in G_{q}$ and a node $\\Bar{u^{\\prime}}\\in G_{c}$ , we denote the embeddings of $u$ and $u^{\\prime}$ after the propagation layer $k$ as $h_{k}^{(q)}(u)$ and $h_{k}^{(c)}(u^{\\prime})$ respectively. $H^{(q)}$ and $H^{(c)}$ denote the $K$ th-layer node embeddings of $G_{q}$ and $G_{c}$ , collected into matrices. Note that, here the set of vectors $H^{(q)}$ and $H^{(c)}$ do not dependent on $G_{c}$ and $G_{q}$ . In the end, IsoNet compares these embeddings to compute the distance $\\Delta(\\bar{G}_{c}\\,|\\,G_{q})$ , which is inversely related to $\\hat{y}(G_{c}\\,|\\,G_{q})$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta(G_{c}\\,|\\,G_{q})=\\sum_{u,i}\\mathrm{{ReLU}}[H^{(c)}-P H^{(q)}][u,i]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Since subgraph isomorphism entails an asymmetric relevance, we have: $\\Delta(G_{c}\\,|\\,G_{q})\\neq\\Delta(G_{q}\\,|\\,G_{c})$ . IsoNet also proposed another design of $\\Delta$ , where it replaces the node embeddings with edge embeddings and node alignment matrix with edge alignment matrix in Eq. (2). ", "page_idx": 2}, {"type": "text", "text": "In an early interaction network, $H^{(q)}$ depends on $G_{c}$ and $H^{(c)}$ depends on $G_{q}$ for any given $(G_{q},G_{c})$ pair. Formally, one should write $H^{(q\\,|\\,c)}$ and $H^{(c\\mid q)}$ instead of $H^{(q)}$ and $H^{(c)}$ respectively for an early interaction network, but for simplicity, we will continue using $H^{(q)}$ and $H^{(c)}$ . ", "page_idx": 2}, {"type": "text", "text": "Our goal Given a set of corpus graphs $C=\\{G_{c}\\,|\\,c\\in[|C|]\\}$ , our high-level goal is to build a graph retrieval model so that, given a query $G_{q}$ , it can return the corpus graphs $\\left\\{G_{c}\\bar{\\right\\}$ which are relevant to $G_{q}$ . To that end, we seek to develop (1) a GNN-based early interaction model, and (2) an appropriate distance measure $\\Delta(\\cdot\\,|\\,\\cdot)$ , so that $\\Delta(H^{(c)}\\mid H^{(q)})$ is an accurate predictor of $y(G_{c}\\,|\\,G_{q})$ , at least to the extent that $\\Delta(\\cdot|\\cdot)$ is effective for ranking candidate corpus graphs in response to a query graph. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed early-interaction GNN with multi-round alignment refinement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first write down the subgraph isomorphism task as an instance of the quadratic assignment problem (QAP) or the Gromov-Wasserstein (GW) distance optimization task. Then, we design IsoNet $^{++}$ , by building upon this formulation. ", "page_idx": 2}, {"type": "text", "text": "3.1 Subgraph isomorphism as Gromov-Wasserstein distance optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "QAP or GW formulation with asymmetric cost We are given a graph pair $G_{q}$ and $G_{c}$ padded with appropriate number of nodes to ensure $|V_{q}|=|V_{c}|=n$ (say). Let their adjacency matrices be $A_{q},A_{c}\\in\\left\\{0,1\\right\\}^{n\\times n}$ . Consider the family of hard permutation matrices $P\\in\\Pi_{n}$ where $P[u,u^{\\prime}]=1$ indicates $u\\in V_{q}$ is \u201cmatched\u201d to $u^{\\prime}\\in V_{c}$ . Then, $G_{q}$ is a subgraph of $G_{c}$ , if for some permutation matrix $_{P}$ , the matrix $A_{q}$ is covered by $P A_{c}P^{\\top}$ , i.e., for each pair $(u,v)$ , whenever we have $A_{q}[u,v]=1$ , we will also have $P A_{c}{P}^{\\top}[u,v]=1$ . This condition can be written as $A_{q}\\leq P A_{c}P^{\\top}$ . We can regard a deficit in coverage as a cost or distance: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{cost}(P;A_{q},A_{c})=\\sum_{u\\in[n],v\\in[n]}\\left[\\left(A_{q}-P A_{c}P^{\\top}\\right)_{+}\\right][u,v]}\\\\ &{\\qquad\\qquad\\qquad=\\sum_{u,v\\in[n]}\\sum_{u^{\\prime},v^{\\prime}\\in[n]}(A_{q}[u,v]-A_{c}[u^{\\prime},v^{\\prime}])_{+}\\;P[u,u^{\\prime}]\\;P[v,v^{\\prime}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $[\\cdot]_{+}=\\operatorname*{max}\\left\\{\\cdot,0\\right\\}$ is the ReLU function, applied elementwise. The function $\\mathrm{cost}(P;A_{q},A_{c})$ can be driven down to zero using a suitable choice of $_{P}$ iff $G_{q}$ is a subgraph of $G_{c}$ . This naturally suggests the relevance distance ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta(G_{c}\\,|\\,G_{q})=\\operatorname*{min}_{P\\in\\Pi_{n}}\\mathrm{cost}(P;A_{q},A_{c})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Xu et al. [41] demonstrate that this QAP is a realization of the Gromov-Wassterstein distance minimization in a graph setting. ", "page_idx": 2}, {"type": "image", "img_path": "udTwwF7tks/tmp/432bba44e31b9203623a271526d8dbbe6fb01b6e62d53e8ec86ffc82f4f1ea6d.jpg", "img_caption": ["Figure 1: Overview of IsoNet++. Panel (a) shows the pipeline of $\\mathrm{IsoNet++}$ . Given a graph pair $\\bar{(G_{q},G_{c})}$ , we execute $T$ rounds, each consisting of $K$ GNN layer propagations. After a round $t$ , we use the node embeddings to update the node alignment $P\\,=\\,P_{t}$ from its previous estimate $P=P_{t-1}$ . Within each round $t\\in[T]$ , we compute the node embeddings of $G_{q}$ by gathering signals from $G_{c}$ and vice-versa, using GNN embeddings in the previous round and the node-alignment map $P_{t}$ . The alignment $P_{t}$ remains consistent across all propagation layers $k\\in[K]$ and is updated at the end of round $t$ . Panel (b) shows our proposed node pair partner interaction in IsoNet+ $^{+}$ (Node). When computing the message value of the node pair $(u,v)$ , we also feed the node embeddings of the partners $u^{\\prime}$ and $v^{\\prime}$ in addition to the embeddings of the pairs $(u,v)$ , where $u^{\\prime}$ and $v^{\\prime}$ is approximately aligned with $u$ and $v$ , respectively (when converted to soft alignment, $u^{\\prime},v^{\\prime}$ need not be neighbors). Panel (c) shows the node pair partner interaction in $\\scriptstyle\\mathrm{IsoNet++}$ (Edge). In contrast to IsoNet++ (Node), here we feed the information from the message value of the partner pair $(u^{\\prime},v^{\\prime})$ instead of their node embeddings into the message passing network . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Updating $_{P}$ with projected gradient descent As shown in Benamou et al. [3], Peyr\u00e9 et al. [30], Xu et al. [41], one approach is to first relax $_{P}$ into a doubly stochastic matrix, which serves as a continuous approximation of the discrete permutation, and then update it using projected gradient descent (PGD). Here, the soft permutation $P_{t-1}$ is updated to $P_{t}$ at time-step $t$ by solving the following linear optimal transport (OT) problem, regularized with the entropy of $\\{{\\bar{P}}[u,{\\bar{v}}]\\mid u,v\\,\\,{\\bar{\\in}}\\,[n]\\}$ with a temperature $\\tau$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{t}\\gets\\underset{P\\in B_{n}}{\\mathrm{arg\\,min}}\\,\\mathrm{Trace}\\left(P^{\\top}\\nabla_{P}\\mathrm{cost}(P;A_{q},A_{c})\\big|_{P=P_{t-1}}\\right)+\\tau\\sum_{u,v}P[u,v]\\cdot\\log P[u,v].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Such an OT problem is solved using the iterative Sinkhorn-Knopp algorithm [10, 37, 26]. Similar to other combinatorial optimization problems on graphs, a QAP (4) does not capture the coverage cost in the presence of dense node or edge features, where two nodes or edges may exhibit graded degrees of similarity represented by continuous values. Furthermore, the binary values of the adjacency matrices result in inadequate gradient signals in $\\nabla_{P}\\mathrm{{cost}}(\\cdot)$ . Additionally, the computational bottleneck of solving a fresh OT problem in each PGD step introduces a significant overhead, especially given the large number of pairwise evaluations required in typical learning-to-rank setups. ", "page_idx": 3}, {"type": "text", "text": "3.2 Design of IsoNet+ $^{-+}$ (Node) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Building upon the insights from the above GW minimization (3) and the successive refinement step (6), we build $\\mathrm{IsoNet++}$ (Node), the first variant of our proposed early interaction model. ", "page_idx": 3}, {"type": "text", "text": "Node-pair partner interactions between graphs For simpler exposition, we begin by describing a synthetic scenario, where $_{P}$ is a hard node permutation matrix, which induces the alignment map as a bijection $\\pi:V_{q}\\rightarrow V_{c}$ , so that $\\pi(a)=b$ if $P[a,b]=1$ . We first initialize layer $k=0$ embeddings as $h_{0}^{(q)}(u)\\,=\\,\\mathrm{Init}_{\\theta}(\\mathrm{feature}(u))$ using a neural network $\\operatorname{Init}\\!\\theta$ . (Throughout, $h_{k}^{(c)}(u)$ are treated likewise.) Under the given alignment map $\\pi$ , a simple early interaction model would update the node embeddings as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{k+1}^{(q)}(u)=\\mathrm{comb}_{\\theta}\\left(h_{k}^{(q)}(u),\\ \\sum_{v\\in\\mathrm{nbr}(u)}\\mathrm{msg}_{\\theta}(h_{k}^{(q)}(u),h_{k}^{(q)}(v)),\\ h_{k}^{(c)}(\\pi(u))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the above expression, the update layer uses representation of the partner node $u^{\\prime}\\in V_{c}$ during the message passing step, to compute $h_{k+1}^{(q)}(u)$ , the embedding of node $u\\in V_{q}$ . Li et al. [22] use a similar update protocol, by approximating $\\begin{array}{r}{\\pmb{h}_{k}^{(c)}(\\pi(u))=\\sum_{u^{\\prime}\\in V_{c}}a_{u^{\\prime}\\to u}^{(k)}\\pmb{h}_{k}^{(c)}(u^{\\prime})}\\end{array}$ , where $a_{u^{\\prime}\\to u}^{(k)}$ is the kth layer attention from u \u2208Vq to potential partner u\u2032 \u2208Vc, with u\u2032\u2208Vc a(uk\u2032)\u2192u . Instead of regarding only nodes as potential partners, $\\mathrm{IsoNet++}$ will regard node pairs as partners. Given $(u,v)\\in E_{q}$ , the partners $(\\pi(\\bar{u}),\\pi(v))\\bar{\\in}E_{c}$ should then greatly influence the intensity of assimilation of $h_{k}^{(c)}(u^{\\prime})$ into $h_{k+1}^{(c)}(u)$ . The first key innovation in IsoNet++ is to replace (7) to recognize and implement this insight: ", "page_idx": 3}, {"type": "image", "img_path": "udTwwF7tks/tmp/b2a15064ef4be8d2fc75c4945cb957fcf71d111faa0c69737e10f0e48cac59d8.jpg", "img_caption": ["Figure 2: Illustration of the three interaction modes. IsoNet has no/late interaction between $\\ensuremath{\\boldsymbol{h}}^{(q)}$ and $\\pmb{h}^{(c)}$ . $\\mathrm{IsoNet++}$ and GMN allow interaction between the representations of the query and corpus nodes. Under node pair interaction, the individual node embeddings $\\ensuremath{\\boldsymbol{h}}^{(q)}$ are used for message passing directly, thereby exposing them only to their neighbors. In the corresponding $\\operatorname{comb}_{\\theta}$ step, nodes interact only with their respective partners, therefore missing out on information from the partners of its neighbors. However, under node pair partner interaction, the representation of a node is combined with that of its partner(s) first, using the inter $\\mathbf{\\nabla}\\theta$ block to obtain $\\bar{z^{(q)}}$ (12), which is used for message passing. Thus, when interacting with its neighbors, a node also gets information from the partners of its neighbors. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{k+1}^{(q)}(u)=\\mathrm{comb}_{\\theta}\\left([h_{k}^{(q)}(u),h_{k}^{(c)}(\\pi(u))],\\right.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {\\left.\\sum_{v\\in\\mathfrak{n v}(u)}\\operatorname*{msg}_{\\theta}\\left([h_{k}^{(q)}(u),h_{k}^{(c)}(\\pi(u))],[h_{k}^{(q)}(v),h_{k}^{(c)}(\\pi(v))]\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Embeddings $h_{k+1}^{(c)}(u^{\\prime})$ for nodes $u^{\\prime}\\in V_{c}$ are updated likewise in a symmetric manner. The network $\\mathrm{msg}_{\\theta}$ is provided embeddings from partners $\\pi(u),\\pi(v)$ of $u,v\\,\\in\\,V_{q}$ \u2014 this allows $h_{k+1}^{(\\bullet)}(u)$ to capture information from all nodes in the paired graph, that match with the $(k+1)$ -hop neighbors of $u$ . We schematically illustrate the interaction between the paired graphs in IsoNet, GMN and IsoNet++ in Figure 2. ", "page_idx": 4}, {"type": "text", "text": "Multi-round lazy refinement of node alignment In reality, we are not given any alignment map $\\pi$ . This motivates our second key innovation beyond prior models [1, 22, 23, 35], where we decouple GNN layer propagation from updates to $_{P}$ . To achieve this, IsoNet+ $^+$ (Node) executes $T$ rounds, each consisting of $K$ layer propagations in both GNNs. At the end of each round $t$ , we refine the earlier alignment $P_{t-1}$ to the next estimate $P_{t}$ , which will be used in the next round. Henceforth, we will use the double subscript $t,k$ instead of the single subscript $k$ as in traditional GNNs. We denote the node embeddings at layer $k$ and round $t$ by $\\pmb{h}_{t,k}^{(q)}(u),\\pmb{h}_{t,k}^{(c)}(u^{\\prime})\\in\\mathbb{R}^{\\mathrm{dim}_{h}}$ for $u\\in V_{q}$ and $u^{\\prime}\\in V_{c}$ which are (re-)initialized with node features $h_{t,0}^{\\bullet}$ for each round $t$ . We gather these into matrices ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{t,k}^{(q)}=[h_{t,k}^{(q)}(u)\\,|\\,u\\in V_{q}]\\in\\mathbb{R}^{n\\times\\dim_{h}}\\quad\\mathrm{and}\\quad H_{t,k}^{(c)}=[h_{t,k}^{(c)}(u^{\\prime})\\,|\\,u^{\\prime}\\in V_{c}]\\in\\mathbb{R}^{n\\times\\dim_{h}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$_{P}$ no longer remains an oracular hard permutation matrix, but becomes a doubly stochastic matrix indexed by rounds, written as $P_{t}$ . At the end of round $t$ , a differentiable aligner module takes ${\\cal H}_{t,K}^{(q)}$ and H(c) as inputs and outputs a doubly stochastic node alignment (relaxed permutation) matrix $P_{t}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{t}=\\mathrm{NodeAlignerRefinement}_{\\phi}\\left(H_{t,K}^{(q)},H_{t,K}^{(c)}\\right)}\\\\ &{\\quad=\\mathrm{GumbelSinkhorn}\\left(\\mathrm{LRL}_{\\phi}(H_{t,K}^{(q)})\\,\\mathrm{LRL}_{\\phi}(H_{t,K}^{(c)})^{\\top}\\right)\\in\\mathcal{B}_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the above expression, GumbelSinkhorn $(\\bullet)$ performs iterative Sinkhorn normalization on the input matrix added with Gumbel noise [26]; $\\mathrm{LRL}_{\\phi}$ is a neural module consisting of two linear layers with a ReLU activation after the first layer. As we shall see next, $P_{t}$ is used to gate messages flowing aTchreo ssso fftr oalmi gonnme egnrta tios  tkhee pott fhreor zdeunr ifnogr  trhoeu nddu $t+1$ ,n  i.oef. ,a lwl hlialye ecros minp rutoiunng $\\bar{H}_{t+1,1:K}^{(q)}$ $H_{t+1,1:K}^{(c)}$ $P_{t}$ $t+1$ $P_{t}[u,u^{\\prime}]$ interpreted as the probability that $u$ is assigned to $u^{\\prime}$ , which naturally requires that $P_{t}$ should be row-equivariant (column equivariant) to the shuffling of the node indices of $G_{q}\\left(G_{c}\\right)$ . As shown in Appendix D, the above design choice (11) ensures this property. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Updating node representation using early-interaction GNN Here, we describe the early interaction GNN for the query graph $G_{q}$ . The GNN on the corpus graph $G_{c}$ follows the exact same design and is deferred to Appendix E.1. In the initial round $t=1$ ), since there is no prior alignment estimate $P_{t=0}$ , we employ the traditional late interaction GNN (1) to compute all layers $H_{1,1:K}^{(q\\bar{)}}$ and $\\pmb{H}_{1,1:K}^{(c)}$ separately. These embeddings are then used to estimate $P_{t=1}$ using Eq. (11). For subsequent rounds $\\mathit{\\Omega}_{t}>1\\mathit{\\Omega}_{\\mathit{c}}$ ), given embeddings $\\pmb{H}_{t,1:K}^{(q)}$ , and the alignment estimate matrix $P_{t}$ , we run an early interaction GNN from scratch. We start with a fresh initialization of the node embeddings as before; i.e., $h_{t+1,0}^{(q)}(u)=\\operatorname{Init}_{\\theta}(\\operatorname{feature}(u))$ . For each subsequent propagation layer $k+1$ ( $k\\in[0,K-1])$ , we approximate (8) as follows. We read previous-round, same-layer embeddings $h_{t,k}^{(c)}(u^{\\prime})$ of nodes $u^{\\prime}$ from the other graph $G_{c}$ , incorporate the alignment strength $P_{t}[u,u^{\\prime}]$ , and aggregate these to get an intermediate representation of $u$ that is sensitive to $P_{t}$ and $G_{c}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{z}_{t+1,k}^{(q)}(u)=\\mathrm{inter}_{\\theta}\\left(\\boldsymbol{h}_{t+1,k}^{(q)}(u),\\sum_{u^{\\prime}\\in V_{c}}\\boldsymbol{h}_{t,k}^{(c)}(u^{\\prime})P_{t}[u,u^{\\prime}]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\operatorname{inter}_{\\theta}$ is a neural network that computes interaction between the graph pairs; $z_{t+1,k}^{(q)}(u)$ provides a soft alignment guided representation of $[h_{k}^{(q)}(u),h_{k}^{(c)}(\\pi(u))]$ in Eq. (8), which can be relaxed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t+1,k+1}^{(q)}(u)=\\mathrm{comb}_{\\theta}\\left(z_{t+1,k}^{(q)}(u),\\sum_{v\\in\\mathrm{nbr}(u)}\\mathrm{msg}_{\\theta}(z_{t+1,k}^{(q)}(u),z_{t+1,k}^{(q)}(v))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the above expression, we explicitly feed $z_{t+1,k}^{(q)}(v),v\\in\\mathsf{n b r}(u)$ in the $\\mathrm{msg}_{\\theta}$ network, capturing embeddings of nodes in the corpus $G_{c}$ aligned with the neighbors of node $u\\in V_{q}$ in $h_{t+1,k+1}^{(q)}(u)$ This allows the model to perform node-pair partner interaction. Instead, if we were to feed only $h_{t+1,k}^{(q)}(u)$ into the $\\mathrm{msg}_{\\theta}$ network, then it would only perform node partner interaction. In this case, the computed embedding for $u$ would be based solely on signals from nodes in the paired graph that directly correspond to $u$ , therefore missing additional context from other neighbourhood nodes. ", "page_idx": 5}, {"type": "text", "text": "dDiissttaanncte aosf  a asliogftn dmisetnatnce Fbientawlleye,n  atth teh see te $T$ tahned $\\Delta(G_{c}\\,|\\,G_{q})$ $H_{T,K}^{(q)}=[h_{T,K}^{(q)}(u)\\,|\\,u\\in V_{q}]$ ${\\pmb{H}}_{T,K}^{(c)}=$ $[h_{T,K}^{(c)}(u^{\\prime})\\,|\\;u^{\\prime}\\in V_{c}]$ , measured as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{\\theta,\\phi}(G_{c}\\,|\\,G_{q})=\\sum_{u}\\sum_{d}\\mathrm{ReLU}(H_{T,K}^{(q)}[u,d]-(P_{T}H_{T,K}^{(c)})[u,d])}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our focus is on graph retrieval applications. It is unrealistic to assume direct supervision from a gold alignment map $P^{*}$ . Instead, training query instances are associated with pairwise preferences between two corpus graphs, in the form $\\langle G_{q},G_{c+},G_{c-}\\rangle$ , meaning that, ideally, we want $\\Delta_{\\theta,\\phi}(G_{c-}|G_{q})\\geq$ $\\gamma+\\Delta_{\\theta,\\phi}\\big(G_{c+}|G_{q}\\big)$ , where $\\gamma>0$ is a margin hyperparameter. This suggests a minimization of the standard hinge loss as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\theta,\\phi}\\sum_{q\\in Q}\\sum_{c+\\in C_{q+},c-\\in C_{q-}}[\\gamma+\\Delta_{\\theta,\\phi}(G_{c+}\\,|\\,G_{q})-\\Delta_{\\theta,\\phi}(G_{c-}\\,|\\,G_{q})]_{+}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This loss is back-propagated to train model weights $\\theta$ in $\\cosh\\!{\\mathrm{b}}_{\\theta}$ , $\\operatorname{inter}\\theta$ , $\\mathrm{msg}_{\\theta}$ and weights $\\phi$ in the Gumbel-Sinkhorn network. ", "page_idx": 5}, {"type": "text", "text": "Multi-layer eager alignment variant Having set up the general multi-round framework of $\\scriptstyle\\mathrm{IsoNet++}$ , we introduce a structurally simpler variant that updates $_{P}$ eagerly after every layer, eliminating the need to re-initialize node embeddings every time we update $_{P}$ . The eager variant retains the beneftis of node-pair partner interactions, while ablating $\\mathrm{IsoNet++}$ toward GMN. Updating $_{P}$ via Sinkhorn iterations is expensive compared to a single GNN layer. In practice, we see a non-trivial tradeoff between computation cost, end task accuracy, and the quality of our injective alignments, depending on the value of $K$ for eager updates, and the values $(T,K)$ for lazy updates. Formally, $P_{k}$ is updated across layers as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{P}_{k}=\\mathrm{NodeAlignerRefinement}_{\\phi}\\left({H}_{k}^{(q)},{H}_{k}^{(c)}\\right)}\\\\ &{\\quad=\\mathrm{GumbelSinkhorn}\\left(\\mathrm{LRL}_{\\phi}({H}_{k}^{(q)})\\operatorname{LRL}_{\\phi}({H}_{k}^{(c)})^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We update the GNN embeddings, layerwise, as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{k}^{(q)}(u)=\\operatorname{inter}_{\\theta}\\Big(h_{k}^{(q)}(u),\\sum_{u^{\\prime}\\in V_{c}}h_{k}^{(c)}(u^{\\prime})P_{k}[u,u^{\\prime}]\\Big),}\\\\ &{h_{k+1}^{(q)}(u)=\\mathrm{comb}_{\\theta}\\left(z_{k}^{(q)}(u),\\sum_{v\\in\\mathrm{nbr}(u)}\\mathrm{msg}_{\\theta}(z_{k}^{(q)}(u),z_{k}^{(q)}(v))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Analysis of computational complexity Here, will compare the performance of IsoNet (Node) [35] with multi-layer $\\mathrm{IsoNet++}$ (Node) and multi-round IsoNet+ $^+$ (Node) for graphs with $\\vert V\\vert$ nodes. For multi-layer IsoNet $^{++}$ (Node) and IsoNet (Node), we assume $K$ propagation steps and for multi-round IsoNet+ $^{-+}$ (Node), $T$ rounds, each with $K$ propagation steps. ", "page_idx": 6}, {"type": "text", "text": "\u2014IsoNet (Node): The total complexity is $O(|V|^{2}+K|E|)$ , computed as follows: (1) Initialization of layer embeddings at layer $k=0$ takes $O(|V|)$ time. (2) The node representation computation incurs a complexity of $\\left.\\begin{array}{r}{\\partial(|E|)}\\end{array}\\right.$ for each message passing step since it aggregates node embeddings across all neighbors. (3) The computation of $_{P}$ takes $O(|\\bar{V}|^{2})$ time. ", "page_idx": 6}, {"type": "text", "text": "\u2014Multi-layer eager $t s o N e t++\\ (N o d e)$ : The total complexity is ${\\cal O}(K|V|^{2}+K|E|+K|V|^{2})\\,=$ $O(K|V|^{2})$ , computed as follows: (1) Initialization (layer $k\\;=\\;0$ ) takes $O(|V|)$ time. (2) The computation of intermediate embeddings $z^{(\\bullet)}$ (Eq. 18) involves the evaluation of the expression $\\begin{array}{r}{\\sum_{u^{\\prime}\\in V_{c}}h_{k}^{(\\bullet)}(u^{\\prime})P_{k}[u,u^{\\prime}]}\\end{array}$ and hence admits a complexity of $O(|V|)$ for each node per layer. The total complexity for $K$ steps and $\\vert V\\vert$ nodes is thus $O(K|V|^{2})$ . (3) Next, for each node in every layer, we compute $h_{k+1}^{(\\bullet)}$ (Eq. 19) which gathers messages $z^{(\\bullet)}$ from all its neighbors, contributing a total complexity of $O(K|E|)$ . (4) Finally, we update $P_{k}$ for each layer which has a complexity of $O(K|V|^{2})$ . ", "page_idx": 6}, {"type": "text", "text": "\u2014Multi-round $I s o N e t+(N o d e).$ : Here, the key difference from the multi-layer version above is that the doubly stochastic matrix $P_{t}$ from round $t$ is used to compute $_{\\textit{z}}$ and the $K$ -step-GNN runs in each of the $T$ rounds. This multiplies the complexity of steps 2 and 3 with $T$ , raising it to $O(K T|V|^{2}+K T|E|)$ . Matrix $P_{t}$ is updated a total of $T$ times, which changes the complexity of step 4 to $O(T|V|^{2})$ . Hence, the total complexity is $O(K T|V|^{2}+T|V|^{2}+K\\bar{T}|E|)=O(\\mathop{K T}|\\dot{V}|^{2})$ ", "page_idx": 6}, {"type": "text", "text": "Hence, the complexity of IsoNet is $O(|V|^{2}+K|E|)$ , multi-layer IsoNet++ is $O(K|V|^{2})$ and multiround $\\scriptstyle\\mathrm{IsoNet++}$ is $O(K T|V|^{2})$ . This increased complexity of the latter comes with the benefit of a significant performance boost, as our experiments suggest. ", "page_idx": 6}, {"type": "text", "text": "3.3 Extension of IsoNet $^{++}$ (Node) to IsoNet++ (Edge) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now extend $\\mathrm{IsoNet++}$ (Node) to $\\scriptstyle\\mathrm{IsoNet++}$ (Edge) which uses explicit edge alignment for interaction across GNN and relevance distance surrogate. ", "page_idx": 6}, {"type": "text", "text": "Multi-round refinement of edge alignment In IsoNet+ $^{+}$ (Edge), we maintain a soft edge permutation matrix $\\boldsymbol{S}$ which is frozen at $S=S_{t-1}$ within each round $t\\in[T]$ and gets refined after every round $t$ as $S_{t-1}\\to S_{t}$ . Similar to IsoNet++ (Node), within each round $t$ , GNN runs from scratch: it propagates messages across layers $k\\in[K]$ and $S_{t-1}$ assists it to capture cross-graph signals. Here, in addition to node embeddings $h_{t,k}^{(\\bullet)}$ , we also use edge embeddings ${\\pmb{m}}_{t,k}^{(q)}(\\boldsymbol{e}),\\;{\\pmb{m}}_{t,k}^{(c)}(\\boldsymbol{e}^{\\prime})\\in\\mathbb{R}^{\\mathrm{dim}_{m}}$ at each layer $k$ and each round $t$ , which capture the information about the subgraph $k\\leq K$ hop away from the edges $e$ and $e^{\\prime}$ . Similar to Eq. (9), we define $M_{t,k}^{(q)}=[m_{t,k}^{(q)}(e)]_{e\\in E_{q}}$ , and $M_{t,k}^{(c)}=[m_{t,k}^{(c)}(e^{\\prime})]_{e^{\\prime}\\in E_{c}}$ $M_{t,0}^{(\\bullet)}$ are initialized using the features of the nodes connected by the edges, and possibly local edge features. Given the embeddings $M_{t,K}^{\\left(q\\right)}$ ) and M t(,cK) computed at the end of round $t$ , an edge aligner module (EdgeAlignerRefinement $\\mathbf{\\nabla}_{\\phi}\\big(\\bullet\\big)$ ) takes these embedding matrices as input and outputs a soft edge permutation matrix $S_{t}$ , similar to the update of $P_{t}$ in Eq. (11). ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{t}=\\mathrm{EdgeAlignerRefinement}_{\\phi}\\left(M_{t,K}^{(q)},M_{t,K}^{(c)}\\right)}\\\\ &{\\quad=\\mathrm{GumbelSinkhorn}(\\mathrm{LRL}_{\\phi}(M_{t,K}^{(q)})\\mathrm{~LRL}_{\\phi}(M_{t,K}^{(c)})^{\\top})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, M (\u2022) are appropriately padded to ensure that they have the same number of rows. ", "page_idx": 6}, {"type": "text", "text": "Edge alignment-induced early interaction GNN For $t=1$ , we start with a late interaction model using vanilla GNN (1) and obtain ${\\cal S}_{t=1}$ using Eq. (21). Having computed the edge embeddings $m_{t,1:K}^{(\\bullet)}(\\bullet)$ and node embeddings $h_{t,1:K}^{(\\bullet)}(\\bullet)$ upto round $t$ , we compute $S_{t}$ and use it to build a fresh early interaction GNN for round $t+1$ . To this end, we adapt the GNN guided by $P_{t}$ in Eqs. (12)\u2013 (13),to the GNN guided by $S_{t}$ . We overload the notations for neural modules and different embedding vectors from $\\mathrm{IsoNet++}$ (Node), whenever their roles are similar. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Starting with the same initialization as in IsoNet+ $^+$ (Node), we perform the cross-graph interaction guided by the soft edge permutation matrix $S_{t}$ , similar to Eq. (12). Specifically, we use the embeddings of edges $\\{e^{\\prime}=\\bar{(u^{\\prime},v^{\\prime})}\\}\\in E_{c}$ , computed at layer $k$ at round $t$ , which share soft alignments with an edge $e=(u,v)\\in E_{q}$ , to compute $z_{t+1,k}^{(q)}(e)$ and $z_{t+1,k}^{(q)}(e^{\\prime})$ as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t+1,k}^{(q)}(e)=\\operatorname{inter}_{\\theta}\\left(m_{t+1,k}^{(q)}(e),\\sum_{e^{\\prime}\\in E_{c}}m_{t,k}^{(c)}(e^{\\prime})S_{t}[e,e^{\\prime}]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Finally, we update the node embeddings $h_{t+1,k+1}^{(\\bullet)}$ for propagation layer $k+1$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t+1,k+1}^{(q)}(u)=\\mathrm{comb}_{\\theta}\\left(h_{t+1,k}^{(q)}(u),\\sum_{a\\in\\mathfrak{n b r}(u)}\\mathrm{msg}_{\\theta}(h_{t+1,k}^{(q)}(u),h_{t+1,k}^{(q)}(a),z_{t+1,k}^{(q)}((u,a)))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In this case, we perform the cross-graph interaction at the edge level rather than the node level. Hence, $\\mathrm{msg}_{\\theta}$ acquires cross-edge signals separately as $z_{t+1,k}^{(\\bullet)}$ . $h_{t+1,k+1}^{(\\bullet)}$ $z_{t+1,k+1}^{(\\bullet)}$ update $m_{t+1,k+1}^{(\\bullet)}$ as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m_{t+1,k+1}^{(q)}\\big((u,v)\\big)=\\mathrm{msg}_{\\theta}\\left(h_{t+1,k+1}^{(q)}(u),h_{t+1,k+1}^{(q)}(v),z_{t+1,k}^{(q)}((u,v))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Likewise, we develop $m_{t+1,k+1}^{(c)}$ for corpus graph $G_{c}$ . Note that $m_{t+1,k+1}^{(q)}((u,v))$ captures signals not only from the matched pair $(u^{\\prime},v^{\\prime})$ , but also signals from the nodes in $G_{c}$ which share correspondences with the neighbor nodes of $u$ and $v$ . Finally, we pad zero vectors to $[m_{T,K}^{(q)}(e)]_{e\\in E_{q}}$ and $[m_{T,K}^{(c)}(e^{\\prime})]_{e^{\\prime}\\in E_{c}}$ to build the matrices $M_{T,K}^{(q)}$ and M T(c,)K with same number of rows, which are finally used to compute the relevance distance ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{\\theta,\\phi}(G_{c}\\,|\\,G_{q})=\\sum_{u}\\sum_{d}\\mathrm{ReLU}(M_{T,K}^{(q)}[e,d]-(S_{T}M_{T,K}^{(c)})[e,d]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We report on a comprehensive evaluation of $\\mathrm{IsoNet++}$ on six real datasets and analyze the efficacy of the key novel design choices. In Appendix G, we provide results of additional experiments. ", "page_idx": 7}, {"type": "text", "text": "4.1 Experimental setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets We use six real world datasets in our experiments, viz., AIDS, Mutag, PTC-FM (FM), PTC-FR (FR), PTC-MM (MM) and PTC-MR (MR), which were also used in [27, 35]. Appendix F provides the details about dataset generation and their statistics. ", "page_idx": 7}, {"type": "text", "text": "State-of-the-art baselines We compare our method against eleven state-of-the-art methods, viz., (1) GraphSim [2] (2) GOTSim [11], (3) SimGNN [1], (4) EGSC [31], (5) H2MN [45], (6) Neuromatch [23], (7) GREED [32], (8) GEN [22], (9) GMN [22] (10) IsoNet (Node) [35], and (11) IsoNet (Edge) [35]. Among them, Neuromatch, GREED, IsoNet (Node) and IsoNet (Edge) apply asymmetric hinge distances between query and corpus embeddings for $\\Delta(G_{c}\\,|\\,G_{q})$ , specifically catered towards subgraph matching, similar to our method in Eqs. (14) and (25). GMN and GEN use symmetric Euclidean distance between their (whole-) graph embeddings $\\pmb{g}^{(q)}$ (for query) and $\\pmb{g}^{(c)}$ (for corpus) as $||\\pmb{g}^{(q)}-\\pmb{g}^{(c)}||$ in their paper [22], which is not suitable for subgraph matching and therefore, results in poor performance. Hence, we change it to $\\Delta(G_{c}\\,|\\,G_{q})=[{\\pmb g}^{(q)}-{\\pmb g}^{(c)}]_{+}$ . The other methods first compute the graph embeddings, then fuse them using a neural network and finally apply a nonlinear function on the fused embeddings to obtain the relevance score. ", "page_idx": 7}, {"type": "text", "text": "Training and evaluation protocol Given a fixed corpus set $C$ , we split the query set $Q$ into $60\\%$ training, $15\\%$ validation and $25\\%$ test set. We train all the models on the training set by minimizing a ranking loss (15). During the training of each model, we use five random seeds. Given a test query $q^{\\prime}$ , we rank the corpus graphs $C$ in the decreasing order of $\\Delta_{\\theta,\\phi}{\\left(G_{c}\\,|\\,G_{q^{\\prime}}\\right)}$ computed using the trained model. We evaluate the quality of the ranking by measuring Average Precision (AP) and $\\mathrm{HITS}\\mathcal{@20}$ , described in Appendix F. Finally, we report mean average precision (MAP) and mean $\\mathrm{HITS}@20$ , across all the test queries. By default, we set the number of rounds $T=3$ , the number of propagation layers in GNN $K\\,=\\,5$ . In Appendix F, we discuss the baselines, hyperparameter setup and the evaluation metrics in more detail. ", "page_idx": 7}, {"type": "table", "img_path": "udTwwF7tks/tmp/f72a6677de09a8604edac61593b5536c4fe4abc98ba77b88a95f5205a55f6804.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Comparison of the two variants of IsoNet+ $^{\\cdot+}$ (IsoNet+ $^{\\cdot+}$ (Node) and IsoNet+ $^+$ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean $\\mathrm{HITS}\\mathcal{Q}20$ . In all cases, we used $60\\%$ training, $15\\%$ validation and $25\\%$ test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for $\\mathrm{IsoNet++}$ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.) ", "page_idx": 8}, {"type": "table", "img_path": "udTwwF7tks/tmp/f552bd05209139ec8f2c639808ef995e467804a76880f111e32726e03057dad5.jpg", "table_caption": [], "table_footnote": ["Table 4: Lazy multi-round vs. eager multi-layer. First (Last) two rows report MAP for $\\mathrm{IsoNet++}$ (Node) (IsoNet+ $^{+}$ (Edge)). Green shows the best method. "], "page_idx": 8}, {"type": "table", "img_path": "udTwwF7tks/tmp/564864c8742bfee7500df8e3b60353dc867e326630089cf0d662a9aabc0a1914.jpg", "table_caption": [], "table_footnote": ["Table 5: Node partner vs. node pair partner interaction. First (Last) two rows report MAP for multi-round (multilayer) update. Green shows the best method. "], "page_idx": 8}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Comparison with baselines First, we compare IsoNet+ $^+$ (Node) and $\\mathrm{IsoNet++}$ (Edge) against all the baselines, across all datasets. In Table 3, we report the results. The key observations are as follows: (1) IsoNet+ $^+$ (Node) and IsoNet+ $^+$ (Edge) outperform all the baselines by significant margins across all datasets. IsoNet $^{++}$ (Edge) consistently outperforms $\\mathrm{IsoNet++}$ (Node). This is because edge alignment allows us to compare the graph pairs more effectively than node alignment. A similar effect was seen for IsoNet (Edge) vs. IsoNet (Node) [35]. (2) Among all state-of-the-art competitors, IsoNet (Edge) performs the best followed by IsoNet (Node). Similar to us, they also use edge and node alignments respectively. However, IsoNet does not perform any interaction between the graph pairs and the alignment is computed once only during the computation of $\\Delta(G_{c}\\,|\\,G_{q})$ . This results in modest performance compared to $_\\mathrm{IsoNet++}$ . (3) GMN uses \u201cattention\u201d to estimate the alignment between graph pairs, which induces a non-injective mapping. Therefore, despite being an early interaction model, it is mostly outperformed by IsoNet, which uses injective alignments. ", "page_idx": 8}, {"type": "text", "text": "Lazy vs. eager updates In lazy multi-round updates, the alignment matrices remain unchanged across all propagation layers and are updated only after the GNN completes its $K$ -layer message propagations. To evaluate its effectiveness, we compare it against the eager multi-layer update (described at the end of Section 3.2), where the GNN executes its $K$ -layer message propagations only once; the alignment map is updated across $K$ layers; and, the alignment at $k$ th layer is used to compute the embeddings at $(k+1)\\mathrm{th}$ layer. In Table 4, we compare the performance in terms MAP, which shows that lazy multi-round updates significantly outperform multi-layer updates. ", "page_idx": 8}, {"type": "text", "text": "Node partner vs. node-pair partner interaction To understand the benefits of node-pair partner interaction, we contrast $\\mathrm{IsoNet++}$ (Node) against another variant of our method, which performs node partner interaction rather than node pair partner interaction, similar to Eq. (7). For lazy multi-round updates, we compute the embeddings as follows: ", "page_idx": 8}, {"type": "text", "text": "$\\begin{array}{r}{\\mathsf{\\Pi}_{t+1,k+1}^{\\mathsf{(q)}}(u)=\\mathrm{comb}_{\\theta}(h_{t+1,k}^{(q)}(u),\\,\\sum_{v\\in\\mathfrak{a b v}(u)}\\mathrm{msg}_{\\theta}(h_{t,k}^{(q)}(u),h_{t,k}^{(q)}(v)),\\,\\sum_{u^{\\prime}\\in V_{c}}P_{t}[u,u^{\\prime}]h_{t,k}^{(c)}(u^{\\prime}))}\\end{array}$ For eager multi-layer updates, we compute the embeddings as: ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{k+1}^{(q)}(u)=\\mathrm{comb}_{\\theta}(h_{k}^{(q)}(u),\\ \\sum_{v\\in\\mathrm{nbr}(u)}\\mathrm{msg}_{\\theta}(h_{k}^{(q)}(u),h_{k}^{(q)}(v)),\\ \\sum_{u^{\\prime}\\in V_{c}}P_{k}[u,u^{\\prime}]h_{k}^{(c)}(u^{\\prime}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "image", "img_path": "udTwwF7tks/tmp/9663a9d5cb9b2636777ea90c1312d76aac8eb7d957cfbcea15255730d7117cf6.jpg", "img_caption": ["Figure 6: Empirical probability density of similarity between the estimated alignments and the true alignments $P^{*},S^{*}$ for both multi-round and multi-layer update strategies across different stages of updates $t$ for multi-round and $k$ for multi-layer), for AIDS. Similarity is measured using $p(\\bar{\\mathrm{Tr}}(P_{t}^{\\top}\\bar{P^{*}}))$ ), $p(\\operatorname{Tr}(S_{t}^{\\top}S^{*}))$ for multi-round lazy updates and $p(\\mathrm{Tr}(P_{k}^{\\top}P^{*}))$ ), $p\\big(\\mathrm{Tr}(S_{k}^{\\top}S^{\\ast})\\big)$ for multi-layer eager updates. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 5 summarizes the results, which shows that IsoNet+ $^+$ (Node) (node partner pair) performs significantly better than Node partner for both multi-round lazy updates (top-two rows) and multi-layer eager updates (bottom tow rows). ", "page_idx": 9}, {"type": "text", "text": "Quality of injective alignments Next we compare between multi-round and multi-layer update strategies in terms of their ability to refine the alignment matrices, as the number of updates of these matrices increases. For multi-round (layer) updates, we instrument the alignments $P_{t}$ and $S_{t}$ ( $P_{k}$ and $S_{k}$ ) for different rounds $t\\,\\in\\,[T]$ (layers $k\\,\\in\\,[K];$ . Specifically, we look into the distribution of the similarity between the learned alignments $P_{t},S_{t}$ and the correct alignments $P^{*},S^{*}$ (using combinatorial routine), measured using the inner products $\\mathrm{Tr}(P_{t}^{\\top}P^{*})$ and $\\overline{{\\mathrm{Tr}}}(S_{t}^{\\top}S^{*})$ for different $t$ . Similarly, we compute $\\mathrm{Tr}(P_{k}^{\\top}P^{*})$ and $\\mathrm{Tr}(S_{k}^{\\top}S^{*})$ for different $k\\,\\in\\,[K]$ . Figure 6 summarizes the results, which shows that (1) as $t$ or $k$ increases, the learned alignments become closer to the gold alignments; (2) multi-round updates refine the alignments approximately twice as faster than the multi-layer variant. The distribution of $\\mathrm{Tr}(P_{t}^{\\top}P^{*})$ at $t=1$ in multi-round strategy is almost always close to $\\mathrm{Tr}(P_{k}^{\\top}P^{*})$ for $k=2$ . Note that, our aligner networks learn to refine the $P_{t}$ and $S_{t}$ through end-to-end training, without using any form of supervision from true alignments or the gradient computed in Eq. (6). ", "page_idx": 9}, {"type": "text", "text": "Accuracy-inference time trade-off Here, we analyze the accuracy and inference time trade-off. We vary $T$ and $K$ for IsoNet++\u2019s lazy multi-round variant, and vary $K$ for $\\mathrm{IsoNet++^{\\circ}}\\mathrm{:}$ s eager multi-layer variant and for GMN. Figure 7 summarizes the results. Notably, the eager multi-layer variant achieves the highest accuracy for $K=8$ on the AIDS dataset, despite the known issue of oversmoothing in GNNs for large $K$ . This unexpected result may be due to our message passing components, which involve terms like $\\textstyle\\sum_{u^{\\prime}}P[u,u^{\\prime}]h(u^{\\prime})$ , effectively acting as a convolution between alignment scores and embedding vect ors. This likely enables $_{P}$ to function as a filter, countering the oversmoothing effect. ", "page_idx": 9}, {"type": "image", "img_path": "udTwwF7tks/tmp/f155fb59aeec224a373b18212a6a5c10a345926887ff1b5d681cdf78f6664918.jpg", "img_caption": ["Figure 7: Trade-off between MAP and inference time (batch size $_{=128}$ ). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce IsoNet++ as an early-interaction network for estimating subgraph isomorphism. IsoNet $^{++}$ learns to identify explicit alignments between query and corpus graphs despite having access to only pairwise preferences and not explicit alignments during training. We design a graph neural network (GNN) that uses an alignment estimate to propagate messages, then uses the GNN\u2019s output representations to refine the alignment. Experiments across several datasets confirm that alignment refinement is achieved over several rounds. Design choices such as using node-pair partner interaction (instead of node partner) and lazy updates (over eager) boost the performance of our architecture, making it the state-of-the-art in subgraph isomorphism based subgraph retrieval. We also demonstrate the accuracy v/s inference time trade offs for IsoNet++, which show how different knobs can be tuned to utilize our models under regimes with varied time constraints. ", "page_idx": 9}, {"type": "text", "text": "This study can be extended to graph retrieval problems which use different graph similarity measures, such as maximum common subgraph or graph edit distance. Extracting information from node-pairs is effective and can be widely used to improve GNNs working on multiple graphs at once. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Indradyumna acknowledges Qualcomm Innovation Fellowship, Abir and Soumen acknowledge grants from Amazon, Google, IBM and SERB. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Y. Bai, H. Ding, S. Bian, T. Chen, Y. Sun, and W. Wang. Simgnn: A neural network approach to fast graph similarity computation. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 384\u2013392, 2019.   \n[2] Y. Bai, H. Ding, K. Gu, Y. Sun, and W. Wang. Learning-based efficient graph similarity computation via multi-scale convolutional set matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3219\u20133226, 2020.   \n[3] J.-D. Benamou, G. Carlier, M. Cuturi, L. Nenna, and G. Peyr\u00e9. Iterative bregman projections for regularized transportation problems. SIAM Journal on Scientific Computing, 37(2):A1111\u2013 A1138, 2015. [4] Q. Berthet, M. Blondel, O. Teboul, M. Cuturi, J.-P. Vert, and F. Bach. Learning with differentiable pertubed optimizers. Advances in neural information processing systems, 33:9508\u20139519, 2020.   \n[5] L. Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb. com/. Software available from wandb.com. [6] A. Cereto-Massagu\u00e9, M. J. Ojeda, C. Valls, M. Mulero, S. Garcia-Vallv\u00e9, and G. Pujadas. Molecular fingerprint similarity search in virtual screening. Methods, 71:58\u201363, 2015.   \n[7] D. Chen, L. O\u2019Bray, and K. Borgwardt. Structure-aware transformer for graph representation learning. ICML, 2022.   \n[8] E. Cohen-Karlik, A. B. David, and A. Globerson. Regularizing towards permutation invariance in recurrent models. In NeurIPS. Curran Associates Inc., 2020. ISBN 9781713829546. URL https://arxiv.org/abs/2010.13055.   \n[9] L. P. Cordella, P. Foggia, C. Sansone, and M. Vento. A (sub) graph isomorphism algorithm for matching large graphs. IEEE transactions on pattern analysis and machine intelligence, 26(10): 1367\u20131372, 2004.   \n[10] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26:2292\u20132300, 2013.   \n[11] K. D. Doan, S. Manchanda, S. Mahapatra, and C. K. Reddy. Interpretable graph similarity computation via differentiable optimal alignment of node embeddings. pages 665\u2013674, 2021.   \n[12] H.-C. Ehrlich and M. Rarey. Systematic benchmark of substructure search in molecular graphsfrom ullmann to vf2. Journal of Cheminformatics, 4:1\u201317, 2012.   \n[13] X. Gao, B. Xiao, D. Tao, and X. Li. A survey of graph edit distance. Pattern Analysis and applications, 13(1):113\u2013129, 2010.   \n[14] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263\u20131272. PMLR, 2017.   \n[15] A. Hagberg, P. Swart, and D. S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008.   \n[16] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. Shamma, M. Bernstein, and L. Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3668\u20133678, 2015.   \n[17] N. Karalias and A. Loukas. Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs. Advances in Neural Information Processing Systems, 33: 6659\u20136672, 2020.   \n[18] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[19] J. Kotary, F. Fioretto, P. Van Hentenryck, and B. Wilder. End-to-end constrained optimization learning: A survey. arXiv preprint arXiv:2103.16378, 2021.   \n[20] A. Lai and J. Hockenmaier. Learning to predict denotational probabilities for modeling entailment. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 721\u2013730, 2017. URL https://www.aclweb.org/anthology/E17-1068.pdf.   \n[21] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.   \n[22] Y. Li, C. Gu, T. Dullien, O. Vinyals, and P. Kohli. Graph matching networks for learning the similarity of graph structured objects. In International conference on machine learning, pages 3835\u20133845. PMLR, 2019. URL https://arxiv.org/abs/1904.12787.   \n[23] Z. Lou, J. You, C. Wen, A. Canedo, J. Leskovec, et al. Neural subgraph matching. arXiv preprint arXiv:2007.03092, 2020.   \n[24] D. Marcheggiani and I. Titov. Encoding sentences with graph convolutional networks for semantic role labeling. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1506\u20131515, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics. doi: 10.18653/ v1/D17-1159. URL https://aclanthology.org/D17-1159.   \n[25] B. McFee and G. R. G. Lanckriet. Partial order embedding with multiple kernels. In International Conference on Machine Learning, 2009. URL https://api.semanticscholar.org/ CorpusID:699292.   \n[26] G. Mena, D. Belanger, S. Linderman, and J. Snoek. Learning latent permutations with gumbelsinkhorn networks. arXiv preprint arXiv:1802.08665, 2018. URL https://arxiv.org/pdf/ 1802.08665.pdf.   \n[27] C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and M. Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020. URL www.graphlearning.io.   \n[28] R. Myers, R. Wison, and E. R. Hancock. Bayesian graph edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(6):628\u2013635, 2000.   \n[29] M. Ohlrich, C. Ebeling, E. Ginting, and L. Sather. Subgemini: Identifying subcircuits using a fast subgraph isomorphism algorithm. In Proceedings of the 30th International Design Automation Conference, pages 31\u201337, 1993.   \n[30] G. Peyr\u00e9, M. Cuturi, and J. Solomon. Gromov-wasserstein averaging of kernel and distance matrices. In International conference on machine learning, pages 2664\u20132672. PMLR, 2016.   \n[31] C. Qin, H. Zhao, L. Wang, H. Wang, Y. Zhang, and Y. Fu. Slow learning and fast inference: Efficient graph similarity computation via knowledge distillation. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.   \n[32] R. Ranjan, S. Grover, S. Medya, V. Chakaravarthy, Y. Sabharwal, and S. Ranu. Greed: A neural framework for learning graph distance functions. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, November 29-Decemer 1, 2022, 2022.   \n[33] I. Roy, A. De, and S. Chakrabarti. Adversarial permutation guided node representations for link prediction. In AAAI Conference, 2021. URL https://arxiv.org/abs/2012.08974.   \n[34] I. Roy, S. Chakrabarti, and A. De. Maximum common subgraph guided graph retrieval: Late and early interaction networks. In NeurIPS, 2022. URL https://openreview.net/forum? id=COAcbu3_k4U.   \n[35] I. Roy, V. S. Velugoti, S. Chakrabarti, and A. De. Interpretable neural subgraph matching for graph retrieval. In AAAI Conference, 2022. URL https://indradyumna.github.io/pdfs/ IsoNet_main.pdf.   \n[36] T. K. Rusch, M. M. Bronstein, and S. Mishra. A survey on oversmoothing in graph neural networks. Preprint, 2023. URL https://arxiv.org/abs/2303.10993.   \n[37] R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343\u2013348, 1967.   \n[38] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[39] I. Vendrov, R. Kiros, S. Fidler, and R. Urtasun. Order-embeddings of images and language. arXiv preprint arXiv:1511.06361, 2015. URL https://arxiv.org/pdf/1511.06361.   \n[40] F. Wenkel, Y. Min, M. Hirn, M. Perlmutter, and G. Wolf. Overcoming oversmoothness in graph convolutional networks via hybrid scattering networks, 2022. URL https://arxiv.org/ abs/2201.08932.   \n[41] H. Xu, D. Luo, H. Zha, and L. C. Duke. Gromov-wasserstein learning for graph matching and node embedding. In International conference on machine learning, pages 6932\u20136941. PMLR, 2019.   \n[42] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.   \n[43] X. Yan, P. S. Yu, and J. Han. Graph indexing: A frequent structure-based approach. In Proceedings of the 2004 ACM SIGMOD international conference on Management of data, pages 335\u2013346, 2004.   \n[44] Z. Zeng, A. K. Tung, J. Wang, J. Feng, and L. Zhou. Comparing stars: On approximating graph edit distance. Proceedings of the VLDB Endowment, 2(1):25\u201336, 2009.   \n[45] Z. Zhang, J. Bu, M. Ester, Z. Li, C. Yao, Z. Yu, and C. Wang. H2mn: Graph similarity learning with hierarchical hypergraph matching networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 2274\u20132284, 2021.   \n[46] W. Zhuo and G. Tan. Efficient graph similarity computation with alignment regularization. Advances in Neural Information Processing Systems, 35:30181\u201330193, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval (Appendix) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We find two limitations of our method each of which could form the basis of detailed future studies. ", "page_idx": 13}, {"type": "text", "text": ". Retrieval systems greatly benefit from the similarity function being hashable. This can improve the inference time multi-fold while losing very little, if at all any, performance, making the approach ready for production environments working under tight time constraints. The design of a hash function for an early interaction network like ours is unknown and seemingly difficult. In fact, such a hashing procedure is not known even for predecessors like IsoNet (Edge) or GMN, and this is an exciting future direction. ", "page_idx": 13}, {"type": "text", "text": "2. Our approach does not explicitly differentiate between nodes or edges that may belong to different classes. This can be counterproductive when there exist constraints that prevent the alignment of two nodes or edges with different labels. While the network is designed to process node and edge features, it might not be enough to rule out alignments that violate the said constraint. Such constraints could also exist for node-pairs, such as in knowledge graphs with hierarchical relationships between entity types, and are not taken into account by our model. Extending our work to handle such restrictions is an interesting problem to consider. ", "page_idx": 13}, {"type": "text", "text": "B Related work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we discuss different streams of work that are related to and have influenced the study. ", "page_idx": 13}, {"type": "text", "text": "B.1 Graph Representation Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Graph neural networks (GNN) [14, 22, 21, 18, 42, 38] have emerged as a widely applicable approach for graph representation learning. A graph neural network computes the embedding of a node by aggregating the representations of its neighbors across $K$ steps of message passing, effectively combining information from $K$ -hop neighbors. GNNs were first used for graph similarity computation by Li et al. [22], who enriched the architecture with attention to predict isomorphism between two graphs. Attention acts as a mechanism to transfer information from the representation of one graph to that of the other, thus boosting the performance of the approach. Chen et al. [7] enriched the representation of graphs by capturing the subgraph around a node effectively through a structure aware transformer architecture. ", "page_idx": 13}, {"type": "text", "text": "B.2 Differentiable combinatorial solvers ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We utilize a differentiable gadget to compute an injective alignment, which is a doubly stochastic matrix. The differentiability is crucial to the training procedure as it enables us to backpropagate through the alignments. The GumbelSinkhorn operator, which performs alternating normalizations across rows and columns, was first proposed by Sinkhorn and Knopp [37] and later used for the Optimal Transport problem by Cuturi [10]. Other methods to achieve differentiability include adding random noise to the inputs to discrete solvers [4] and designing probabilistic loss functions [17]. A compilation of such approaches towards constrained optimization on graphs through neural techniques is presented in [19]. ", "page_idx": 13}, {"type": "text", "text": "B.3 Graph Similarity Computation and Retrieval ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Several different underlying measures have been proposed for graph similarity computation, including full graph isomorphism [22], subgraph isomorphism [23, 35], graph edit distance (GED) [2, 11, 13, 28, 44] and maximum common subgraph (MCS) [2, 11, 34]. Bai et al. [2] proposed GraphSim towards the GED and MCS problems, using convolutional neural network based scoring on top of graph similarity matrices. GOTSim [11] explicitly computes the alignment between the two graphs by studying the optimal transformation cost. GraphSim [2] utilizes both graph-level and node-level signals to compute a graph similarity score. NeuroMatch [23] evaluates, for each node pair across the two graphs, if the neighborhood of one node is contained in the neighborhood of another using order embeddings [25]. GREED [32] proposed a Siamese graph isomorphism network, a late interaction model to tackle the GED problem and provided supporting theoretical guarantees. Zhang et al. [45] propose an early interaction model, using hypergraphs to learn higher order node similarity. Each hypergraph convolution contains a subgraph matching module to learn cross graph similarity. Qin et al. [31] trained a slower attention-based network on multi-level features from a GNN and distilled its knowledge into a faster student model. Roy et al. [35] used the GumbelSinkhorn operator as a differentiable gadget to compute alignments in a backpropagation-friendly fashion and also demonstrated the utility of computing alignments for edges instead of nodes. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "C Broader Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This work can be directly applied to numerous practical applications, such as drug discovery and circuit design, which are enormously beneficial for the society and continue to garner interest from researchers and practitioners worldwide. The ideas introduced in this paper have beneftited from and can benefit the information retrieval community as well, beyond the domain of graphs. However, malicious parties could use this technology for deceitful purposes, such as identifying and targeting specific social circles on online social networks (which can be represented as graphs). Such pros and cons are characteristic of every scientific study and the authors consider the positives to far outweigh the negatives. ", "page_idx": 14}, {"type": "text", "text": "D Network architecture of different components of IsoNet++ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "$\\scriptstyle\\mathrm{IsoNet++}$ models consist of three components - an encoder, a message-passing network and a node/edge aligner. We provide details about each of these components below. For convenience, we represent a linear layer with input dimension $a$ and output dimension $b$ as $\\operatorname{Linear}(a,b)$ and a linear-ReLU-linear network with Linear $(a,b)$ , Linear $(b,c)$ layers with ReLU activation in the middle as $\\mathrm{LRL}(a,b,c)$ . ", "page_idx": 14}, {"type": "text", "text": "D.1 Encoder ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The encoder transforms input node/edge features before they are fed into the message-passing network. For models centred around node alignment like IsoNet+ $^+$ (Node), the encoder refers to $\\operatorname{Init}_{\\theta}$ and is implemented as a Linear $(1,10)$ layer. The edge vectors are not encoded and passed as-is down to the message-passing network. For edge-based models like IsoNet++ (Edge), the encoder refers to both $\\mathrm{Init}_{\\theta,\\mathrm{node}}$ and $\\mathrm{Init}_{\\theta,\\mathrm{edge}}$ , which are implemented as Linear $(1,10)$ and Linear $(1,20)$ layers respectively. ", "page_idx": 14}, {"type": "text", "text": "D.2 GNN ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Within the message-passing framework, we use node embeddings of size $\\mathrm{dim}_{h}\\,=\\,10$ and edge embeddings of size $\\dim_{m}=20$ . We specify each component of the GNN below. ", "page_idx": 14}, {"type": "text", "text": "\u2022 $\\operatorname{inter}_{\\theta}$ combines the representation of the current node/edge $(h_{\\bullet})$ with that from the other graph, which are together fed to the network by concatenation. For node-based and edgebased models, it is implemented as LRL(20, 20, 10) and $\\mathrm{LRL}(40,40,20)$ networks respectively. In particular, we ensure that the input dimension is twice the size of the output dimension, which in turn equals the intermediate embedding dimension $\\dim(z)$ . \u2022 $\\mathrm{msg}_{\\theta}$ is used to compute messages by combining intermediate embeddings $z_{\\bullet}$ of nodes across an edge with the representation of that edge. For node-based models, the edge vector is a fixed vector of size 1 while the intermediate node embeddings $z_{\\bullet}$ are vectors of dimension 10, resulting in the network being a Linear(21, 20) layer. For edge-based models, the edge embedding is the $\\mathbf{\\nabla}m$ vector of size 20 which requires $\\mathrm{msg}_{\\theta}$ to be a Linear(40, 20) layer. Note that the message-passing network is applied twice, once to the ordered pair $\\overline{{(u,v)}}$ and then to $(v,u)$ and the outputs thus obtained are added up. This is to ensure node order invariance for undirected edges by design. \u2022 $\\operatorname{comb}_{\\theta}$ combines the representation of a node $z_{\\bullet}$ with aggregated messages received by it from all its neighbors. It is modelled as a GRU where the node representation (of size 10) is the initial hidden state and the aggregated message vector (of size 20) is the only element of an input sequence which updates the hidden state to give us the final node embedding $h_{\\bullet}$ . ", "page_idx": 14}, {"type": "text", "text": "D.3 Node aligner ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The node aligner takes as input two sets of node vectors H(q) \u2208 Rn\u00d710 and H(c) \u2208 Rn\u00d710 representing $G_{q}$ and $G_{c}$ respectively. $n$ refers to the number of nodes in the corpus graph (the query graph is padded to meet this node count). We use $\\mathrm{LRL}_{\\phi}$ as a LRL(10, 16, 16) network (refer Eq. 11). ", "page_idx": 15}, {"type": "text", "text": "D.4 Edge aligner ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The design of the edge aligner is similar to the node aligner described above in Section D.3, except that its inputs are sets of edge vectors $M^{(q)}\\in\\mathbb{R}^{e\\times20}$ and $M^{(c)}\\in\\mathbb{R}^{e\\times20}$ . $e$ refers to the number of edges in the corpus graph (the query graph is padded to meet this edge count). We use $\\mathrm{LRL}_{\\phi}$ as a LRL(20, 16, 16) network (refer Eq. 21). ", "page_idx": 15}, {"type": "text", "text": "D.5 GumbelSinkhorn operator ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The GumbelSinkhorn operator consists of the following operations - ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{0}=\\exp(D_{\\mathrm{in}}/\\tau)}\\\\ &{D_{t+1}=\\mathrm{RowNorm}\\left(\\mathrm{ColumnNorm}(D_{t})\\right)}\\\\ &{D_{\\mathrm{out}}=\\displaystyle\\operatorname*{lim}_{t\\rightarrow\\infty}D_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The matrix $D_{\\mathrm{out}}$ obtained after this set of operations will be a doubly-stochastic matrix. The input $D_{\\mathrm{in}}$ in our case is the matrix containing the dot product of the node/edge embeddings of the query and corpus graphs respectively. $\\tau$ represents the temperature and is fixed to 0.1 in all our experiments. ", "page_idx": 15}, {"type": "text", "text": "Theorem Equation 11 results in a permutation matrix that is row-equivariant (column-) to the shuffling of nodes in $G_{q}\\left(G_{c}\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof To prove the equivariance of Eq. 11, we need to show that given a shuffilng (permutation) of query nodes Z \u2208\u03a0n which modifies the node embedding matrix to Z H\u02d9t(,qK), t he resulting output of said equation would change to $Z P_{t}$ . Below, we consider any matrices with $Z$ in the suffix as being an intermediate expression in the computation of NodeAlignerRefinement $_\\phi(Z H_{t,K}^{(q)},H_{t,K}^{(c)})$ . ", "page_idx": 15}, {"type": "text", "text": "It is easy to observe that the operators $\\mathrm{LRL}_{\\phi}$ (a linear-ReLU-linear network applied to a matrix), RowNorm, ColumnNorm and element-wise exponentiation (exp), division are all permutationequivariant since a shuffling of the vectors fed into these will trivially result in the output vectors getting shuffled in the same order. Thus, we get the following sequence of operations ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{\\mathrm{in},Z}=\\mathrm{LRL}_{\\phi}(Z H_{t,K}^{(q)})\\,\\mathrm{LRL}_{\\phi}(H_{t,K}^{(c)})^{\\top}=Z\\cdot\\mathrm{LRL}_{\\phi}(H_{t,K}^{(q)})\\,\\mathrm{LRL}_{\\phi}(H_{t,K}^{(c)})^{\\top}D_{\\mathrm{in}}=Z D_{\\mathrm{in}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$D_{0,Z}$ equals $\\exp(D_{\\mathrm{in},Z}/\\tau)$ , which according to above equation would lead to $D_{0,Z}=Z D_{0}$ . We can then inductively show using Eq. 27 and the equivariance of row/column normalization, assuming the following holds till $t$ , that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1,Z}=\\mathrm{RowNorm}\\left(\\mathrm{ColumnNorm}(D_{t,Z})\\right)=\\mathrm{RowNorm}\\left(\\mathrm{ColumnNorm}(Z D_{t})\\right)}\\\\ &{=\\mathrm{RowNorm}\\left(Z\\cdot\\mathrm{ColumnNorm}(D_{t})\\right)=Z\\cdot\\mathrm{RowNorm}\\left(\\mathrm{ColumnNorm}(D_{t})\\right)=Z D_{t+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The above equivariance would also hold in the limit, resulting in the doubly stochastic matrix $D_{\\mathrm{out},Z}=Z D_{\\mathrm{out}}$ , which concludes the proof. \u25a0 ", "page_idx": 15}, {"type": "text", "text": "A similar proof can be followed to show column equivariance for a shuffling in the corpus nodes. ", "page_idx": 15}, {"type": "text", "text": "E Variants of our models and GMN, used in the experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Multi-round refinement of IsoNet+ $^+$ (Node) for the corpus graph ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 Initialize: ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{0}^{(c)}(u^{\\prime})=\\operatorname{Init}_{\\theta}(\\operatorname{feature}(u^{\\prime})),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Update the GNN embeddings as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{t+1,k}^{(c)}(u^{\\prime})=\\operatorname{inter}_{\\theta}\\Big(h_{t+1,k}^{(c)}(u^{\\prime}),\\sum_{u\\in V_{q}}h_{t,k}^{(q)}(u)P_{t}^{\\top}[u^{\\prime},u]\\Big),}\\\\ &{h_{t+1,k+1}^{(c)}(u^{\\prime})=\\mathrm{comb}_{\\theta}\\left(z_{t+1,k}^{(c)}(u^{\\prime}),\\sum_{v^{\\prime}\\in\\mathfrak{n b r}(u^{\\prime})}\\operatorname*{msg}_{\\theta}(z_{t+1,k}^{(c)}(u^{\\prime}),z_{t+1,k}^{(c)}(v^{\\prime}))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "E.2 Eager update for IsoNet+ $^+$ (Edge) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 Initialize: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{0}^{(q)}(u)=\\mathrm{Init}_{\\theta,\\mathrm{node}}(\\mathrm{feature}(u)),}\\\\ {\\pmb{m}_{0}^{(q)}(e)=\\mathrm{Init}_{\\theta,\\mathrm{edge}}(\\mathrm{feature}(e)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u2022 The edge alignment is updated across layers. $\\ensuremath{\\boldsymbol{S}}_{0}$ is set to a matrix of zeros. For $k>0$ , the following equation is used: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{k}=\\mathrm{EdgeAlignerRefinement}_{\\phi}\\left(M_{k}^{(q)},M_{k}^{(c)}\\right)}\\\\ &{\\quad=\\mathrm{GumbelSinkhorn}\\left(\\mathrm{LRL}_{\\phi}(M_{k}^{(q)})\\operatorname{LRL}_{\\phi}(M_{k}^{(c)})^{\\top}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u2022 We update the GNN node and edge embeddings as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{k}^{(q)}(e)=\\operatorname*{inter}_{\\theta}\\Big(m_{k}^{(q)}(e),\\sum_{e^{\\prime}\\in E_{c}}m_{k}^{(c)}(e^{\\prime})S_{k}[e,e^{\\prime}]\\Big),}\\\\ &{h_{k+1}^{(q)}(u)=\\operatorname{comb}_{\\theta}\\Big(h_{k}^{(q)}(u),\\sum_{a\\in\\mathrm{nbr}(u)}\\operatorname*{msg}_{\\theta}(h_{k}^{(q)}(u),h_{k}^{(q)}(a),z_{k}^{(q)}((u,a)))\\Big)}\\\\ &{m_{k+1}^{(q)}((u,v))=\\operatorname*{msg}_{\\theta}(h_{k+1}^{(q)}(u),h_{k+1}^{(q)}(v),z_{k}^{(q)}((u,v)))}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "E.3 Node partner (with additional MLP) variant of IsoNet $^{++}$ (Node) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we update node embeddings as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t+1,k+1}^{(q)}(u)=\\mathrm{comb}_{\\theta}\\left(z_{t+1,k}^{(q)}(u),\\sum_{v\\in\\mathrm{nbr}(u)}\\underbrace{\\mathrm{msg}_{\\theta}(h_{t+1,k}^{(q)}(u),h_{t+1,k}^{(q)}(v))}_{z_{\\mathrm{~is~replaced}\\;\\mathrm{with}\\;h}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, $z_{t+1,k}^{(q)}(u)$ is computed as Eq. (12), where inter $\\mathrm{~\\,~}^{\\!}\\theta$ is an MLP network. In contrast to Eq. (13), here, $z_{t+1,k}^{(q)}(u),z_{t+1,k}^{(q)}(v)$ are not fed into the message passing layer. Hence, in the message passing layer, we do not capture the signals from the partners of $u$ and $v$ in $G_{c}$ . Only signals from partners of $u$ are captured through $z_{t+1,k}^{(q)}(u)$ in the first argument. ", "page_idx": 16}, {"type": "text", "text": "E.4 Node pair partner (msg only) variant of IsoNet+ $^+$ (Node) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We change the GNN update equation as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{t+1,k+1}^{(q)}(u)=\\mathrm{comb}_{\\theta}\\left(\\underbrace{h_{t+1,k}^{(q)}(u)}_{z\\mathrm{~is~replaced~with~}h},\\sum_{v\\in\\mathrm{nbr}(u)}\\mathrm{msg}_{\\theta}(z_{t+1,k}^{(q)}(u),z_{t+1,k}^{(q)}(v))\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Node pair partner interaction takes place because, we feed $_{z}$ from Eq. (12) into the message passing layer. However, we use $^h$ in the first argument, instead of $_{z}$ . ", "page_idx": 16}, {"type": "text", "text": "F Additional details about experimental setup ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use six datasets from the TUDatasets collection [27] for benchmarking our methods with respect to existing baselines. Lou et al. [23] devised a method to sample query and corpus graphs from the graphs present in these datasets to create their training data. We adopt it for the task of subgraph matching. In particular, we choose a node $u\\in G$ as the center of a Breadth First Search (BFS) and run the algorithm till $|V|$ nodes are traversed, where the range of $|V|$ is listed in Table 8 (refer to the Min and Max columns for $\\vert V_{q}\\vert$ and $|V_{c})$ . This process is independently performed for the query and corpus splits (with different ranges for graph size) to obtain 300 query graphs and 800 corpus graphs. The set of query graphs is split into train, validation and test splits of 180 $(60\\%)$ , 45 $(15\\%)$ and 75 $(25\\%)$ graphs respectively. Ground truth labels are computed for each query-corpus graph pair using the VF2 algorithm [9, 15, 23] implemented in the Networkx library. Various statistics about the datasets are listed in Table 8. pairs $(y)$ denotes the number of pairs in the dataset with gold label $y$ , where $y\\in\\{0,1\\}$ . ", "page_idx": 17}, {"type": "table", "img_path": "udTwwF7tks/tmp/fd8aba8af52d320646080d8eb9db98549bbbfb8d84db016624f96b0561b6936b.jpg", "table_caption": [], "table_footnote": ["Table 8: Statistics for the 6 datasets borrowed from the TUDatasets collection [27] "], "page_idx": 17}, {"type": "text", "text": "F.2 Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "GraphSim, GOTSim, SimGNN, Neuromatch, GEN, GMN, IsoNet (Node), IsoNet (Edge): We utilized the code from official implementation of [35] 1. Some for loops were vectorized to improve the running time of GMN. ", "page_idx": 17}, {"type": "text", "text": "EGSC: The official implementation 2 is refactored and integrated into our code.   \nH2MN: We use the official code from 3.   \nGREED: We use the official code from 4. The model is adapted from the graph edit distance (GED) task to the subgraph isomorphism task, using a hinge scoring layer. ", "page_idx": 17}, {"type": "text", "text": "The number of parameters involved in all models (our methods and baselines) are reported in Table 9. ", "page_idx": 17}, {"type": "table", "img_path": "udTwwF7tks/tmp/4515d0324fbc47bf06d53b1c7015065f24ab9426cd084b5801dc4e444d95d830.jpg", "table_caption": [], "table_footnote": ["Table 9: Number of parameters for all models used in comparison "], "page_idx": 17}, {"type": "text", "text": "F.3 Calculation of Metrics: Mean Average Precision (MAP), HITS $@\\mathbf{K}$ , Precision $@\\mathbf{K}$ and Mean Reciprocal Rank (MRR) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given a ranked list of corpus graphs $C=\\{G_{c}\\}$ for a test query $G_{q}$ , sorted in the decreasing order of $\\Delta_{\\theta,\\phi}(G_{c}|G_{q})$ , let us assume that the $c_{+}^{\\mathrm{th}}$ relevant graph is placed at position $\\mathrm{pos}(c_{+})\\in\\{1,...,|C|\\}$ in the ranked list. Then Average Precision (AP) is computed as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{AP}(q)=\\frac{1}{|C_{q+}|}\\sum_{c_{+}\\in[|C_{q+}|]}\\frac{c_{+}}{\\mathrm{pos}(c_{+})}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Mean average precision is defined as $\\textstyle\\sum_{q\\in Q}\\operatorname{AP}(q)/|Q|$ . ", "page_idx": 18}, {"type": "text", "text": "Precision $\\begin{array}{r}{\\ @K(q)\\ =\\ \\frac{1}{K}\\,{:}\\ }\\end{array}$ # relevant graphs corresponding to $G_{q}$ till rank $K$ . Finally we report the mean of Precision $@K(q)$ across queries. ", "page_idx": 18}, {"type": "text", "text": "Reciprocal rank or $\\operatorname{RR}(q)$ is the inverse of the rank of the topmost relevant corpus graph corresponding to $G_{q}$ in the ranked list. Mean reciprocal rank (MRR) is average of $\\operatorname{RR}(q)$ across queries. ", "page_idx": 18}, {"type": "text", "text": "$\\mathrm{HITS}@K$ for a query $G_{q}$ is defined as the fraction of positively labeled corpus graphs that appear before the $K^{\\mathrm{th}}$ negatively labeled corpus graph. Finally, we report the average of $\\mathrm{HIYS}@K$ across queries. ", "page_idx": 18}, {"type": "text", "text": "Note that $\\mathrm{HITS}\\mathcal{Q}\\mathrm{K}$ is a significantly aggressive metric compared to Precision $@\\,\\mathrm{K}$ and MRR, as can be seen in Tables 12 and 13. ", "page_idx": 18}, {"type": "text", "text": "F.4 Details about hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All models were trained using early stopping with MAP score on the validation split as a stopping criterion. For early stopping, we used a patience of 50 with a tolerance of $10^{-4}$ . We used the Adam optimizer with the learning rate as $10^{-\\dot{3}}$ and the weight decay parameter as $5\\cdot10^{-4}$ . We set batch size to 128 and maximum number of epochs to 1000. ", "page_idx": 18}, {"type": "text", "text": "Seed Selection and Reproducibility Five integer seeds were chosen uniformly at random from the range $\\lbrack0,10^{4}]$ resulting in the set $\\{1704,4929,7366,7474,7762\\}$ . $\\mathrm{IsoNet++}$ (Node), GMN and IsoNet (Edge) were trained on each of these 5 seeds for all 6 datasets. Note that these seeds do not control the training-dev-test splits but only control the initialization. Since the overall problem is non-convex, in principle, one should choose the best initial conditions leading to local minima. Hence, for all models, we choose the best seed, based on validation MAP score, is shown in Table 10. ", "page_idx": 18}, {"type": "table", "img_path": "udTwwF7tks/tmp/7cd12aa68c8fc20ea4b0278173aebf42fd56cc24b304c39b03a6448cd8ab0e0b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 10: Best seeds for all models. For IsoNet (Edge), GMN and IsoNet++ (Node), these are computed based on MAP score on the validation split at convergence. For other models, the identification occurs after 10 epochs of training. ", "page_idx": 18}, {"type": "text", "text": "IsoNet+ $^{\\cdot+}$ (Edge) and all ablations on top of $\\mathrm{IsoNet++}$ (Node) were trained using the best seeds for IsoNet+ $^{+}$ (Node) (as in Tables 4, 5 and 16). Ablations of GMN were trained with the best GMN seeds. For baselines excluding IsoNet (Edge), models were trained on all 5 seeds for 10 epochs and the MAP scores on the validation split were considered. Full training with early stopping was resumed only for the best seed per dataset. This approach was adopted to reduce the computational requirements for benchmarking. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Margin Selection For GraphSim, GOTSim, SimGNN, Neuromatch, GEN, GMN and IsoNet (Edge), we use the margins determined by Roy et al. [35] for each dataset. For IsoNet (Node), the margins prescribed for IsoNet (Edge) were used for standardization. For IsoNet+ $^+$ (Node), IsoNet++ (Edge) and ablations, a fixed margin of 0.5 is used. ", "page_idx": 19}, {"type": "text", "text": "Procedure for baselines EGSC, GREED, H2MN: They are trained on five seeds with a margin of 0.5 for 10 epochs and the best seed is chosen using the validation MAP score at this point. This seed is also used to train a model with a margin of 0.1 for 10 epochs. The better of these models, again using MAP score on the validation split, is identified and retrained till completion using early stopping. ", "page_idx": 19}, {"type": "table", "img_path": "udTwwF7tks/tmp/496ae33f25ee8236e45ddd72e36300a072bdf0dcae22e0042aa78db34bf2fc59.jpg", "table_caption": [], "table_footnote": ["Table 11: Best margin for baselines used in comparison. "], "page_idx": 19}, {"type": "text", "text": "F.5 Software and Hardware ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All experiments were run with Python 3.10.13 and PyTorch 2.1.2. $\\mathrm{IsoNet++}$ (Node), IsoNet++ (Edge), GMN, IsoNet (Edge) and ablations on top of these were trained on Nvidia RTX A6000 (48 GB) GPUs while other baselines like GraphSim, GOTSim etc. were trained on Nvidia A100 (80 GB) GPUs. ", "page_idx": 19}, {"type": "text", "text": "As an estimate of training time, we typically spawn 3 training runs of IsoNet $\\vdash+$ (Node) or IsoNet++ (Edge) on one Nvidia RTX A6000 GPU, each of which takes 300 epochs to conclude on average, with an average of 6-12 minutes per epoch. This amounts to 2 days of training. Overloading the GPUs by spawning 6 training runs per GPU increases the training time marginally to 2.5 days. ", "page_idx": 19}, {"type": "text", "text": "Additionally, we use wandb [5] to manage and monitor the experiments. ", "page_idx": 19}, {"type": "text", "text": "F.6 License ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "GEN, GMN, GOTSim, GREED and EGSC are available under the MIT license, while SimGNN is public under the GNU license. The licenses for GraphSim, H2MN, IsoNet (Node), IsoNet (Edge), Neuromatch could not be identified. The authors were unable to identify the license of the TUDatasets repository [27], which was used to compile the 6 datasets used in this paper. ", "page_idx": 19}, {"type": "text", "text": "G Additional experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "G.1 Comparison against baselines ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Tables 12 and 13, we report the Mean Average Precision (MAP), HITS $@20$ , MRR and Precision $@20$ scores for several baselines as well as the four approaches discussed in our paper - multi-layer and multi-round variants of $\\mathrm{IsoNet++}$ (Node) and $\\mathrm{IsoNet++}$ (Edge). Multi-round $\\scriptstyle\\mathrm{IsoNet++}$ (Edge) outperforms all other models with respect to all metrics, closely followed by multi-round $\\mathrm{IsoNet++}$ (Node) and multi-layer $\\mathrm{IsoNet++}$ (Edge) respectively. Among the baselines, IsoNet (Edge) is the best-performing model, closely followed by IsoNet (Node) and GMN. ", "page_idx": 20}, {"type": "text", "text": "For MRR, Precision $@\\,20$ , the comparisons are less indicative of the significant boost in performance obtained by $\\mathrm{IsoNet++}$ , since these are not aggressive metrics from the point of view of information retrieval. ", "page_idx": 20}, {"type": "table", "img_path": "udTwwF7tks/tmp/26cfbff5185ef0a570687f3ed6117d1aa9dceb2eb629c80e66c47eb67ccdc895.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "udTwwF7tks/tmp/71997080c8e5290cf2e288c77496dfec0fb6540d716ce4c885730853acbf2cc2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 12: Replication of Table 3 with standard error. Comparison of the two variants of $\\mathrm{IsoNet++}$ $\\mathrm{(IsoNet++}$ (Node) and IsoNet+ $^{\\cdot+}$ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision MAP and $\\mathrm{HITS}@20$ In all cases, we used $60\\%$ training, $15\\%$ validation and $25\\%$ test sets. The first five methods apply a neural network on the fused graph-pair representations. The next six methods apply asymmetric hinge distance between the query and corpus embeddings similar to our method. The numbers with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for $\\mathrm{IsoNet++}$ (Edge) across FM, MM and MR are verified to be not exactly same, but they take the same value until the third decimal). ", "page_idx": 20}, {"type": "table", "img_path": "udTwwF7tks/tmp/b905d114e78ab0f6087f1c7efd5dee96b7ac16a8f19d9580c3f953846b6e5364.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "udTwwF7tks/tmp/524e75c7d93c6d5b70889c6198e8ae34c3846e0a6429b40fbe7ef9a31bb35532.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 13: MRR and Precision $@20$ of corresponding models from Table 3 with standard error. Comparison of the two variants of $\\mathrm{IsoNet++}$ $\\mathrm{(IsoNet++}$ (Node) and $\\mathrm{IsoNet++}$ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms MRR and Precision $@20$ . In all cases, we used $60\\%$ training, $15\\%$ validation and $25\\%$ test sets. The first five methods apply a neural network on the fused graph-pair representations. The next six methods apply asymmetric hinge distance between the query and corpus embeddings similar to our method. The numbers with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. ", "page_idx": 21}, {"type": "text", "text": "G.2 HITS $@20$ , MRR and Precision $@20$ for multi-round IsoNet+ $^+$ and multi-layer IsoNet++ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 14 compares multi-round and multi-layer $\\mathrm{IsoNet++}$ with respect to different metrics. We observe that multi-round $\\mathrm{IsoNet++}$ outperforms multi-layer IsoNet++ by a significant margin when it comes to all metrics, both when the models are node-based or edge-based. This reinforces the observations from MAP scores noted earlier in Table 4. Note that a minor exception occurs for MRR but the scores are already so close to 1 that this particular metric can be discounted and our key observation above still stands. ", "page_idx": 21}, {"type": "text", "text": "G.3 Refinement of alignment matrix across rounds and layers in multi-round IsoNet+ $^+$ and multi-layer IsoNet++ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The node (edge) alignment calculated after round $t$ is denoted as $P_{t}$ $(S_{t})$ . We accumulate such alignments across multiple rounds. This also includes $P_{T}$ $(S_{T})$ which is used to compute the relevance distance in Eq. 14 (Eq. 25). We wish to compare the predicted alignments with ground truth alignments. We expect our final alignment matrix $P_{t}$ $\\left(S_{t}\\right)$ to be one of them. We determine the closest ground truth matrices $P^{*}$ and $S^{*}$ by computing maxP $\\mathrm{Tr}(P_{T}^{\\top}P)$ and $\\operatorname*{max}_{S}\\operatorname{Tr}(S_{T}^{\\top}S)$ for $\\mathrm{IsoNet++}$ (Node) and $\\mathrm{IsoNet++}$ (Edge) respectively. We now use the closest ground-truth alignment $P^{*}$ , to compute $\\mathrm{Tr}(P_{t}^{\\top}P^{*})$ for $t\\,\\in\\,[T]$ . For each $t$ , we plot a histogram with bin width 0.1 that denotes the density estimate $\\mathrm{p}(\\mathrm{Tr}(P_{t}^{\\top}P^{*}))$ . The same procedure is adopted for edges, with $S^{*}$ used instead of $P^{*}$ . The histograms are depicted in Figure 15. We observe that the plots shift rightward with increasing $t$ . The frequency of graph pairs with misaligned $P_{t}$ $(S_{t})$ decreases with rounds $t$ while that with well-aligned $P_{t}$ $(S_{t})$ increases. ", "page_idx": 21}, {"type": "table", "img_path": "udTwwF7tks/tmp/9d61b468c0c51340b19fd8bccd20f821cfd9ef256eeec00432bf2532f76b3c1a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Here, we also study alignments obtained through multi-layer refinement. We adopt the same procedure as in Section G.3. One key difference is that the node/edge alignments are computed after every layer $k$ and are accumulated across layers $k\\in[K]$ . In Figure 15, we observe that the plots, in general, shift rightward with increasing $k$ . The frequency of graph pairs with misaligned $P_{t}$ $(S_{t})$ decreases with rounds $k$ while that with well-aligned $P_{k}$ $(S_{k})$ increases. ", "page_idx": 22}, {"type": "text", "text": "G.4 Comparison across alternatives of multi-layer IsoNet+ $^{\\cdot+}$ (Node) and multi-round IsoNet $^{++}$ (Node) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Table 16, we compare different alternatives to the multi-round and multi-layer variants of IsoNet++ (Node). In particular, we consider four alternatives - Node partner (equation shown in Section 4), Node partner (with additional MLP) [Appendix E.3], Node pair partner (msg only) [Appendix E.4] and IsoNet $^{++}$ (Node). We observe that for all metrics, IsoNet $^{++}$ (Node) and Node pair partner (msg only) dominate the other alternatives in most cases. This highlights the importance of node pair partner interaction for determining the subgraph isomorphism relationship between two graphs. For the multi-round variant, IsoNet+ $^+$ (Node) outperforms Node pair partner (msg only) in four of the datasets and is comparable / slightly worse in the other two. Once again, comparisons based on MRR break down because it does not cause a strong differentiation between the approaches. ", "page_idx": 22}, {"type": "image", "img_path": "udTwwF7tks/tmp/cdb5be16dc4957ac5cd6fddc8ffa80a5f48383c376cf2c6394da5978ecc78245.jpg", "img_caption": ["Figure 15: Similar to Figure 6, we plot empirical probability density of $p(\\mathrm{Tr}(P_{t}^{\\top}P^{*}))$ and $p(\\mathrm{Tr}(S_{t}^{\\top}S^{\\ast}))$ for different values of $t$ lazy multi round updates and $p(\\mathrm{Tr}(P_{k}^{\\top}P^{*}))$ ) and $p(\\mathrm{Tr}(S_{k}^{\\top}S^{*}))$ for different values of $k$ for eager multi layer updates. The first (last) two plots in the left (right) of each row are for multi-round IsoNet $^{-+}$ (Node) (multi-round IsoNet $^{++}$ (Edge)). "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "udTwwF7tks/tmp/758c79c842c6743ad610f488d301017d0fedfc2e0fa08f2c2373354c595bd207.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "G.5 Comparison of GMN with IsoNet+ $^+$ alternative for multi-layer and multi-round ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In Table 17, we modify the GMN architecture to include node pair partner interaction in the messagepassing layer. Based on the reported metrics, we observe that there is no substantial improvement upon including information from node pairs in GMN, which is driven by a non-injective mapping (attention). This indicates that injectivity of the doubly stochastic matrix in our formulation is crucial towards the boost in performance obtained from node pair partner interaction as well. ", "page_idx": 25}, {"type": "table", "img_path": "udTwwF7tks/tmp/9893b209f86d88d7337cfe18e1c0cf7434bf98d16865827192e798023cbf8031.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 17: Effect of node pair partner interaction in GMN. The tables compare GMN with its IsoNet++ alternative. The first table reports MAP values, the second table reports $\\mathrm{HIT}\\mathbb{S}\\ @20$ values, the third table reports MRR values and the fourth table reports Precision $@20$ . In each table, the first two rows report metrics for multi-layer refinement and the second two rows report metrics for multi-round refinement. Rows colored green and yellow indicate the best and second best methods according to the respective metrics. ", "page_idx": 25}, {"type": "text", "text": "G.6 Variation of IsoNet+ $^{+}$ (Node) and IsoNet+ $^+$ (Edge) with different $T$ and $K$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we analyze the accuracy and inference time trade-off of multi-round lazy and multilayer eager variants of $\\scriptstyle\\mathrm{IsoNet++}$ (Node) and IsoNet+ $^{\\cdot+}$ (Edge). In the following tables, we show the MAP and inference time. Additionally, we also analyze the trade-off of GMN and IsoNet (Edge). The $T,K$ parameters for different models are so chosen that they can be compared against each other while fixing the inference time to be roughly similar. For instance, multi-round lazy $\\mathrm{IsoNet++}$ (Node) with $T=5,K=5$ maps to multi-layer eager IsoNet+ $^{\\cdot+}$ (Node) with $K=8$ , allowing for a direct comparison of performance without caring much about different compute. Note that in below tables, models are listed in order of increasing inference time (i.e. increasing $K$ or $T$ ). ", "page_idx": 26}, {"type": "text", "text": "In tables 19 and 20, we show variations for multi-round lazy IsoNet+ $^+$ (Node) for fixed $T$ and fixed $K$ respectively. We observe that with fixed $T$ , increasing $K$ from 5 to 10 doesn\u2019t improve the model significantly. For fixed $K$ , performance (in terms of MAP) improves notably when increasing $T$ from 3 to 5. ", "page_idx": 26}, {"type": "text", "text": "In table 21, we show variations for multi-layer eager IsoNet+ $^+$ (Node) for varying $K$ . We observe that except for a drop at $K=7$ , the performance of the model improves as we increase $K$ . In fact, at $K=8$ , the performance is surprisingly good, even outperforming the similarly timed $T=5,K=5$ variant of lazy multi-round IsoNet $^{++}$ (Node) on both AIDS and Mutag. ", "page_idx": 26}, {"type": "text", "text": "In tables 22 and 23, we compare variants of multi-round lazy IsoNet $^{++}$ (Edge) with fixed $T$ and fixed $K$ respectively. We observe that when $T$ is fixed and $K$ is increased, the gain is marginal. We observe a significant gain When $K$ is fixed and $T$ is increased from 3 to 4. ", "page_idx": 26}, {"type": "text", "text": "In table 24, we study the trade-off for multi-layer eager IsoNet+ $^+$ (Edge) for varying $K$ . We observe that with increasing $K$ , the performance continues to improve and peaks at $K=8$ . Note that even at this $K$ , the performance of multi-layer eager IsoNet+ $^+$ (Edge) is worse than a similarly timed variant $(T=5,K=5)$ of multi-round IsoNe $^{\\mathrel{++}}$ (Edge). ", "page_idx": 26}, {"type": "text", "text": "In table 25, we show variations for GMN for varying $K$ . We observe marginal gains while increasing $K$ . From $K=10$ to $K=12$ , the performance drops. ", "page_idx": 26}, {"type": "text", "text": "In table 26, we show how performance varies for IsoNet (Edge) for varying $K$ . We observe that the model does not improve with increasing $K$ . ", "page_idx": 26}, {"type": "image", "img_path": "udTwwF7tks/tmp/a7c6dd8837f89e713488920b391d5e9e17eb3b83c93b822b1fdd543cf29c2ce4.jpg", "img_caption": ["Figure 18: Trade off between MAP and inference time (batch size $_{\\mathord{=}128}$ ). "], "img_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "udTwwF7tks/tmp/ff0da977f95a3625599f02dd8a880f253abd35526cb2b15cb7e38b60407a2def.jpg", "table_caption": [], "table_footnote": ["Table 19: MAP and inference time trade-off of variants of multi-round lazy IsoNet+ $^{\\cdot+}$ (Node) with fixed $T$ . Rows colored green indicate the best $K$ according to the MAP score. "], "page_idx": 27}, {"type": "table", "img_path": "udTwwF7tks/tmp/0b0f3991af0d587fae9afe9ee4a73aae4c1a6c08146a2a63b8c25359dce18751.jpg", "table_caption": [], "table_footnote": ["Table 20: MAP and inference time trade-off of variants of multi-round lazy IsoNet $^{++}$ (Node) with fixed $K$ . Rows colored green and yellow indicate the best and second best $T$ according to the MAP score. "], "page_idx": 27}, {"type": "table", "img_path": "udTwwF7tks/tmp/2eb825fff254533d9924c5406c279394862bebf5b18643ef24983e833a74faba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "udTwwF7tks/tmp/ec8ca9d9c296d3afbc1610fb701541b1a144ed5be932b40f02c089be096915a5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 21: MAP and inference time trade-off of variants of multi-layer eager $\\mathrm{IsoNet++}$ (Node) with increasing $K$ . Rows colored green and yellow indicate the best and second best $K$ according to the MAP score. ", "page_idx": 27}, {"type": "table", "img_path": "udTwwF7tks/tmp/5bce2d302c60db2ba03210860ec83ce9c4f583ffb388d4e7b397bb2dde47e6e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "udTwwF7tks/tmp/0b06eb5af3fbc27f3d4635ca2cad9539e9d39fc91ecce2bfb56966a702f2ceb2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 22: MAP and inference time trade-off of variants of multi-round lazy $\\mathrm{IsoNet++}$ (Edge) with fixed $T$ .   \nRows colored green indicate the best $K$ according to the MAP score. ", "page_idx": 27}, {"type": "table", "img_path": "udTwwF7tks/tmp/be283aa717912f8d4ac4b2f0ec288e67bc1f2f9261887149b8b5a7187b552832.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "udTwwF7tks/tmp/37e296e9f6b8510eb645fafbc90caec13a38cef5dfeb10238900629b6b0577dd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 23: MAP and inference time trade-off of variants of multi-round lazy IsoNet $^{++}$ (Edge) with fixed $K$ Rows colored green and yellow indicate the best and second best $T$ according to the MAP score. ", "page_idx": 27}, {"type": "table", "img_path": "udTwwF7tks/tmp/bd569ec2d6358a93475fd779f7d50ffac057243e186d72cb3e099b1ecdf1b0fd.jpg", "table_caption": [], "table_footnote": ["Table 24: MAP and inference time trade-off of variants of multi-layer eager $\\mathrm{IsoNet++}$ (Edge) with increasing $K$ . Rows colored green and yellow indicate the best and second best $K$ according to the MAP score. "], "page_idx": 28}, {"type": "table", "img_path": "udTwwF7tks/tmp/0f63bb9b07b585664189357c79f179ec2461bd1ee21da55f8e027643543642b6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "udTwwF7tks/tmp/52b46623f1a6ca5e726156cc2a4b4912556920ba97d2c4c239712b9b1cfe57ef.jpg", "table_caption": [""], "table_footnote": ["Table 25: MAP and inference time trade-off of variants of GMN with increasing $K$ . Rows colored green and yellow indicate the best and second best $K$ according to the MAP score. "], "page_idx": 28}, {"type": "table", "img_path": "udTwwF7tks/tmp/6c1f5766425911f591d44251a120578f5c8248ea59e1c41628757c1d86b1a6d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 26: MAP and inference time trade-off of variants of IsoNet (Edge) with increasing $K$ . Rows colored green and yellow indicate the best and second best $T$ according to the MAP score. ", "page_idx": 28}, {"type": "text", "text": "G.7 Contribution of refining alignment matrix in inference time ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In GMN, computing the embeddings of nodes after the message passing step at each layer dominates the inference time. However, in the case of IsoNet+ $^+$ models, we observe the refinement of the alignment matrix at each layer or round to also be time-intensive. In table 27, we show the contribution of embedding computation and matrix updates to the total inference time. The updates to $P$ constitute the largest share of inference time for multi-layer variants. This can be attributed to the refinement of $P$ after every message passing step, equaling the frequency of embedding computation. In the case of multi-round variants, both embedding computation and updates to $P$ contribute almost equally since $P$ is refined only at the end of each round, after several layers of message passing alongwith embedding computation. ", "page_idx": 29}, {"type": "table", "img_path": "udTwwF7tks/tmp/2baebd2287d33400813cdb3e632d3350cf9e05a333d511008682ec8fe47f64fe.jpg", "table_caption": ["Table 27: Inference time contribution of embedding computation and matrix updates by multi-layer and multi-round IsoNet $^{++}$ (Node) and IsoNet++ (Edge) models. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "G.8 Transfer ability of learned models ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we evaluate the transfer ability of each trained model across datasets. In table 28, we report the Mean Average Precision (MAP) scores for models trained using the AIDS and Mutag datasets respectively evaluated on all six datasets. We observe that despite a zero-shot transfer from one of the datasets to all others, variants of $\\mathrm{IsoNet++}$ show the best accuracy. ", "page_idx": 30}, {"type": "table", "img_path": "udTwwF7tks/tmp/94a81a1b9d303912226266523f68e561988be351b00cc002f3c4b5dced4c6b41.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Section 1.1 discusses the paper\u2019s contributions. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Appendix A discusses the limitations of our work. Details about computational efficiency are included in the main paper as well as in Appendix G.6, expressed explicitly as the running time of each approach. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper includes one theorem, which is noted and proved in Appendix D.5. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. ", "page_idx": 31}, {"type": "text", "text": "\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The paper introduces new architectures and the designs of each of these are discussed in the main paper under Sections 3.2, 3.3 and Appendices D, E. Hyperparameters, training procedure, hardware and random seeds for all experiments are noted in Appendix F. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes]   \nJustification: Uploaded in github. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not ", "page_idx": 32}, {"type": "text", "text": "including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Important parameters that are required to understand and appreciate the results are noted as and when required. Hyperparameters, training procedure, hardware and random seeds for all experiments are noted in Appendix F. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Appendix G.1 includes standard error for the metrics reported in the paper, which includes Mean Average Precision (MAP) and $\\mathrm{HITS}@20$ , computed across each query graph in the test split. The section also reports the method of calculation of the standard error. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Types of GPUs and inference time are reported in Appendices F.5 and G.6 respectively. Other details about the training setting are mentioned point-wise in Appendix F. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The authors have studied the ethics guidelines and find the work to conform well to them.   \nGuuideli   \n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Justification: Broader societal impacts (both positive and negative) are discussed in Appendix C.   \nGuidelines:   \n\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The datasets used in the paper are derived from public graph datasets widely used by researchers and the authors are not aware of any risks for misuse posed by them. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The related work and datasets applicable to this work have been cited in Appendices B and F.1 respectively. Best effort was made by the authors to determine the licenses of the datasets and existing architectures, and are included in Appendix F.6. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Details of the models and datasets are discussed in Appendices D and F.1 respectively. Datasets used in the paper are publicly accessible. The code is submitted with the paper as a supplementary item and is anonymized & well-documented. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The experiments performed in this paper do not involve human subjects.   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "ustification: The experiments performed in this paper do not involve human subjects.   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]