[{"figure_path": "4U18ZoRXTD/figures/figures_1_1.jpg", "caption": "Figure 1: Sound propagation point patterns between a listener (blue sphere) and emitter (yellow sphere) captured by our AV-GS. Notice the points outside the propagation path (points behind the speaker, points behind rigid walls). Please note we slice the scene into half along the y-axis (omitting the points from the ceiling) in order to facilitate better visibility.", "description": "This figure shows the distribution of Gaussian points learned by the AV-GS model for sound propagation between a listener and a speaker. The points are color-coded to represent their contribution to the sound propagation, with points closer to the direct path between the listener and speaker having higher contributions. The figure also highlights points outside of the direct sound path, such as those behind the speaker or reflecting off walls, demonstrating AV-GS's ability to model indirect sound propagation.", "section": "1 Introduction"}, {"figure_path": "4U18ZoRXTD/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our proposed AV-GS. Our model is comprised of a 3D Gaussian Splatting model G, an acoustic field network F and an audio binauralizer B. We first train G to capture the scene geometry information. Next, we construct an audio-focused point representation Ga, with the location X and audio-guidance parameter a initialized by the pre-trained G. Then the acoustic field network F is used to process the a parameters for all the Gaussian points in the vicinity of the listener and the sound source (in the 3D space). The output from F is finally used to condition the audio binauralizer B, which transforms the mono audio to binaural audio w.r.t the listener and sound source location.", "description": "This figure shows the overall architecture of the proposed AV-GS model.  It illustrates the three main components: a 3D Gaussian Splatting model (G) for capturing scene geometry, an acoustic field network (F) for processing audio-guidance parameters, and an audio binauralizer (B) for transforming mono audio into binaural audio. The figure details the flow of information and processing steps, from initial scene representation to final binaural audio output.", "section": "3 Method"}, {"figure_path": "4U18ZoRXTD/figures/figures_6_1.jpg", "caption": "Figure 3: In the presence of (a) complex geometry, and (b) meaningless views, AV-NeRF, when compared to our AV-GS makes errors in binaural synthesis. For both scenarios we showcase the corresponding listener view, used by AV-NeRF, as well as the learned holistic scene representation that is used by AV-GS, and hence unaffected by both scenarios.", "description": "This figure compares the performance of AV-NeRF and AV-GS in scenarios with complex geometry and when the listener's view is uninformative.  It shows that AV-NeRF, which relies on the listener's view, produces errors in these challenging situations, whereas AV-GS, using its learned holistic scene representation, remains accurate. The figure highlights the differences in the generated binaural audio and scene representations between the two methods.", "section": "4.3 Results"}, {"figure_path": "4U18ZoRXTD/figures/figures_8_1.jpg", "caption": "Figure 4: Ablation on the size of vicinity w.r.t the listener and sound source position. Percentile-k denotes the top k % points nearest to the listener and sound source are considered in computing the scene context. (a) RGB color (from G), (b) learned a (from Ga), (c) 5% percentile vicinity, (d) 25% percentile vicinity.", "description": "This figure shows the ablation study on the size of vicinity for the listener and sound source.  The subfigures (a) to (d) show the effect of different percentile values (5%, 10%, 15%, 20%, 25%) on the selection of points within the vicinity for the listener and speaker. Subfigure (a) displays RGB color from the 3D Gaussian splatting model G (scene geometry). Subfigure (b) illustrates the learned audio-guidance parameters (a) from the audio-focused point representation Ga. Subfigures (c) and (d) represent the 5% and 25% percentile vicinity respectively, visualizing the distribution of points considered for binaural audio synthesis based on proximity to the listener and speaker.", "section": "4.4 Ablation study"}, {"figure_path": "4U18ZoRXTD/figures/figures_12_1.jpg", "caption": "Figure 5: Distance aware audio rendering. As the distance from the sound source increases, the amplitude of the synthesized binaural audio decreases. Yellow sphere - location of the sound source, blue sphere - location of the listener.", "description": "This figure shows how the proposed AV-GS model handles distance awareness in audio synthesis.  As the listener (blue sphere) moves further from the sound source (yellow sphere), the amplitude of the generated binaural audio decreases, demonstrating the model's ability to realistically simulate sound propagation based on distance.", "section": "A.1 Direction and distance awareness showcased by AV-GS"}, {"figure_path": "4U18ZoRXTD/figures/figures_13_1.jpg", "caption": "Figure 6: Direction aware audio rendering. Relative to the viewing direction of the listener w.r.t to the sound source AV-GS synthesizes binaural audios with varying amplitude levels in the left and right audio channel. The arrow on the listener (blue sphere) represents the viewing direction, and the yellow sphere depicts the sound source.", "description": "This figure shows how AV-GS handles directionality in audio synthesis.  The top row shows the 3D scene representation with the listener's position, the sound source, and the direction the listener is facing. The bottom row shows the resulting binaural audio waveforms for the left and right channels.  In (a), the left channel is louder because the listener is facing towards the source; in (b), the right channel is louder because the sound is to the listener's right.", "section": "A.1 Direction and distance awareness showcased by AV-GS"}, {"figure_path": "4U18ZoRXTD/figures/figures_13_2.jpg", "caption": "Figure 7: Overview of the binauralizer module, B (with modifications from [17]).", "description": "This figure shows the architecture of the binauralizer module B, which is a crucial component in the AV-GS model. The binauralizer takes the mono audio and transforms it into binaural audio using learned scene context, listener's position, and sound source position. It consists of several components, including STFT for short-time Fourier transform, MLPs for multilayer perceptrons, and inverse STFT to generate the final audio output. The figure also includes details on the input and output of each component, making it easier to understand how the module processes the audio signals to produce realistic and immersive sounds.", "section": "A.2 Binauralizer"}, {"figure_path": "4U18ZoRXTD/figures/figures_14_1.jpg", "caption": "Figure 7: Overview of the binauralizer module, B (with modifications from [17]).", "description": "This figure shows the architecture of the binauralizer module B which is adapted from [17] with modifications to input the learned scene context from acoustic field network F. The main components are two Multilayer Perceptrons (MLPs), each having four linear layers with additional residual connections.  The width of each linear layer is set to 128 for the RWAVS dataset and 256 for the SoundSpaces dataset.  ReLU activation is used for all layers except the last one, which uses a Sigmoid function. The first MLP takes the listener's position (x, y) and the frequency f as input.  A coordinate transformation projects the listener's direction into a high-frequency space. The output of the second MLP is a mixture mask and a difference mask, both scalars.  The process is repeated for all frequencies f \u2208 [0, F] to obtain the complete masks mm and md. For the SoundSpaces dataset, the mixture mask is removed, and the impulse response is predicted directly for a corresponding time input.", "section": "3.1.2 Audio Binauralizer"}]