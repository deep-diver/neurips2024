[{"figure_path": "jFWl9EWZ7z/tables/tables_5_1.jpg", "caption": "Table 1: Quantitative comparison of F1@0.5 on the Multi3DRefer [52] val set.", "description": "This table presents a quantitative comparison of the F1@0.5 score on the Multi3DRefer validation set.  The F1@0.5 score is a metric used to evaluate the performance of multi-object 3D grounding models. The table compares the performance of the proposed D-LISA method against several state-of-the-art baselines.  The comparison is broken down by several sub-categories, reflecting different levels of difficulty in the grounding task (ZT w/o D, ZT w/D, ST w/o D, ST w/D, MT, All).  The results show that D-LISA outperforms the existing baselines by a significant margin, particularly in the more challenging scenarios.", "section": "4 Experiments"}, {"figure_path": "jFWl9EWZ7z/tables/tables_7_1.jpg", "caption": "Table 2: Acc@0.5 of different methods on the ScanRefer dataset [8]. For joint models indicated by *, the best grounding performance with extra captioning training data is reported.", "description": "This table presents a quantitative comparison of various methods on the ScanRefer dataset for single-object 3D grounding.  The metric used is Acc@0.5 (accuracy at Intersection over Union threshold of 0.5). Results are shown separately for three categories of object instances: Unique, Multiple, and All (combination of Unique and Multiple).  For methods marked with an asterisk (*), the reported results are the best achieved when using additional captioning training data. The table highlights the performance of the proposed D-LISA method in comparison to existing state-of-the-art methods.", "section": "4.2 Single-object 3D grounding"}, {"figure_path": "jFWl9EWZ7z/tables/tables_7_2.jpg", "caption": "Table 3: Grounding accuracy of different methods on Nr3D dataset [2].", "description": "This table presents a quantitative comparison of the performance of various methods on the Nr3D dataset for single-object 3D grounding.  It breaks down the accuracy (Acc@0.5) across different subsets of the dataset categorized by difficulty (Easy, Hard), viewpoint dependency (View-Dep, View-Indep), and overall performance.  The table allows for a comparison of D-LISA against several state-of-the-art baselines, highlighting the relative strengths and weaknesses of each approach under various conditions.", "section": "4.2 Single-object 3D grounding"}, {"figure_path": "jFWl9EWZ7z/tables/tables_8_1.jpg", "caption": "Table 4: Ablation study of proposed modules on Multi3DRefer dataset. 'LIS.', 'DBP.' and 'DMR.' stands for 'Language informed spatial fusion', \u2018Dynamic box proposal', and 'Dynamic multi-view renderer' respectively.", "description": "This table presents the ablation study results for the proposed D-LISA model. It shows the impact of each module (Language-informed spatial fusion, Dynamic box proposal, and Dynamic multi-view renderer) on the overall performance. By comparing different rows, we can see how each module contributes to the improvements in the F1@0.5 score across various sub-categories of the Multi3DRefer dataset.", "section": "4.3 Ablation studies"}, {"figure_path": "jFWl9EWZ7z/tables/tables_9_1.jpg", "caption": "Table 5: Computational cost for proposed modules during inference.", "description": "This table presents a breakdown of the computational cost, measured in FLOPs (floating-point operations) and inference time (in seconds), for each module of the proposed D-LISA model.  It compares the computational cost of the baseline modules (detector, multi-view renderer, and fusion) with their dynamic counterparts. This allows for a quantitative analysis of the computational overhead introduced by the dynamic components. The table shows that the dynamic box proposal and multi-view renderer modules add minimal computational cost compared to their baseline versions.  The dynamic components improve model accuracy while keeping the overall model efficient.", "section": "4.3 Ablation studies"}, {"figure_path": "jFWl9EWZ7z/tables/tables_13_1.jpg", "caption": "Table 1: Quantitative comparison of F1@0.5 on the Multi3DRefer [52] val set.", "description": "This table presents a quantitative comparison of the proposed D-LISA model against other state-of-the-art methods on the Multi3DRefer dataset.  The F1@0.5 score, a common metric for evaluating object detection, is used to measure the performance of each model. The dataset is divided into various subcategories representing different complexities of scenes, including zero targets with or without distractors, single targets with or without distractors, and multiple targets. Each method's F1 score is presented for each subcategory, allowing for a detailed comparison of model performance across diverse scenarios.", "section": "4.1 Multi-object 3D grounding"}, {"figure_path": "jFWl9EWZ7z/tables/tables_13_2.jpg", "caption": "Table A2: Ablation studies on question types on Multi3DRefer dataset. \u2018LIS.', \u2018DBP.\u2019 and \u2018DMR.\u2019 stands for \u2018Language informed spatial fusion\u2019, \u2018Dynamic box proposal\u2019, and \u2018Dynamic multi-view renderer\u2019 respectively. F1@0.5 results are reported.", "description": "This table presents the ablation study results on different question types (Spatial, Color, Texture, Shape) by removing one module at a time (LIS, DBP, DMR).  The F1@0.5 scores are reported for each ablation setting to demonstrate the individual contributions of each module to the overall performance.  The baseline is when all modules are included.", "section": "A1 Additional multi-object grounding results"}, {"figure_path": "jFWl9EWZ7z/tables/tables_13_3.jpg", "caption": "Table A3: Ablation studies on the filtering threshold \\(\\tau_f\\). F1@0.5 results are reported.", "description": "This ablation study investigates the effect of varying the filtering threshold  (\\(\\tau_f\\)) on the model's performance. The results, measured by F1@0.5, demonstrate that a threshold of 0.5 yields the best performance, suggesting an optimal balance between inclusiveness and selectivity in the dynamic box proposal module.", "section": "A1 Additional multi-object grounding results"}, {"figure_path": "jFWl9EWZ7z/tables/tables_14_1.jpg", "caption": "Table 1: Quantitative comparison of F1@0.5 on the Multi3DRefer [52] val set.", "description": "This table presents a quantitative comparison of the F1@0.5 scores achieved by different methods on the Multi3DRefer validation set.  The F1@0.5 score is a common metric for evaluating the performance of object detection and grounding models.  The table shows the performance of several state-of-the-art methods, including 3DVG-Trans, D3Net, 3DJCG, M3DRef-CLIP (with and without NMS), and the proposed D-LISA method, across different categories of evaluation (ZT w/o D, ZT w/D, ST w/o D, ST w/D, MT, and All). This allows for a comprehensive comparison of the various approaches and highlights the improvements achieved by the proposed method.", "section": "4.1 Multi-object 3D grounding"}, {"figure_path": "jFWl9EWZ7z/tables/tables_14_2.jpg", "caption": "Table 2: Acc@0.5 of different methods on the ScanRefer dataset [8]. For joint models indicated by *, the best grounding performance with extra captioning training data is reported.", "description": "This table presents a quantitative comparison of various methods on the ScanRefer dataset, focusing on the accuracy at an Intersection over Union (IoU) threshold of 0.5 (Acc@0.5).  It breaks down the results across different categories: 'Unique' (target object has a unique semantic class in the scene), 'Multiple' (target object has the same semantic class as other objects in the scene), and 'All' (the overall average). The table also distinguishes between validation and test set results.  The asterisk (*) indicates that some methods have achieved their best results using supplementary caption training data.", "section": "4.2 Single-object 3D grounding"}, {"figure_path": "jFWl9EWZ7z/tables/tables_15_1.jpg", "caption": "Table 3: Grounding accuracy of different methods on Nr3D dataset [2].", "description": "This table presents a comparison of different methods' performance on the Nr3D dataset for single-object 3D grounding.  The dataset is divided into subsets based on difficulty (Easy, Hard) and viewpoint dependence (View-Dep, View-Indep).  The accuracy is measured as the percentage of correctly identified objects.  The table allows for a comparison of D-LISA's performance against other state-of-the-art methods across various difficulty levels and viewpoints.", "section": "4.2 Single-object 3D grounding"}, {"figure_path": "jFWl9EWZ7z/tables/tables_15_2.jpg", "caption": "Table A8: Comparison of language guided spatial attention methods on Multi3DRefer dataset.", "description": "This table presents an ablation study comparing the performance of two different spatial attention mechanisms: the standard spatial self-attention mechanism and the proposed language-informed spatial attention (LISA) mechanism.  The comparison is done across different subcategories of the Multi3DRefer dataset, allowing assessment of their impact on various scene complexities (i.e., presence or absence of distractors with the same semantics). The results showcase the effectiveness of LISA in improving grounding performance.", "section": "A3 Additional results for LISA"}, {"figure_path": "jFWl9EWZ7z/tables/tables_16_1.jpg", "caption": "Table A8: Comparison of language guided spatial attention methods on Multi3DRefer dataset.", "description": "This table compares the performance of two different spatial attention mechanisms: the Spatial Self-Attention from ViL3DRef and the proposed Language-Informed Spatial Attention (LISA).  The comparison is done across different sub-categories of the Multi3DRefer dataset (ZT w/o D, ZT w/D, ST w/o D, ST w/D, MT, All) which represent varying levels of complexity in the task. The F1@0.5 metric is used to evaluate performance across these sub-categories, showing a slight improvement by the proposed LISA method.", "section": "A3 Additional results for LISA"}]