[{"type": "text", "text": "Generative Subspace Adversarial Active Learning for Outlier Detection in Multiple Views of High-dimensional Tabular Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Outlier detection in high-dimensional tabular data is an important task in data min  \n2 ing, essential for many downstream tasks and applications. Existing unsupervised   \n3 outlier detection algorithms face one or more problems, including inlier assumption   \n4 (IA), curse of dimensionality (CD), and multiple views (MV). To address these   \n5 issues, we introduce Generative Subspace Adversarial Active Learning (GSAAL),   \n6 a novel approach that uses a Generative Adversarial Network with multiple ad  \n7 versaries. These adversaries learn the marginal class probability functions over   \n8 different data subspaces, while a single generator in the full space models the entire   \n9 distribution of the inlier class. GSAAL is specifically designed to address the MV   \n10 limitation while also handling the IA and CD, making it the only method to address   \n11 all three. We provide a mathematical formulation of MV, theoretical guarantees   \n12 for the training, and scalability analysis for GSAAL. Our extensive experiments   \n13 demonstrate the effectiveness and scalability of GSAAL, highlighting its superior   \n14 performance compared to other popular OD methods, especially in MV scenarios. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Outlier detection (OD), a fundamental and widely recognized issue in data mining, involves the   \n17 identification of anomalous or deviating data points within a dataset. Outliers are typically defined   \n18 as low-probability occurrences within a population [41, 19]. In the absence of access to the true   \n19 probability distribution of the data points, OD algorithms rely on constructing a scoring function.   \n20 Points with higher scores are more likely to be outliers. Existing unsupervised OD algorithms have   \n21 one or more of the following problems, in high-dimensional tabular data scenarios.   \n22 \u2022 The inlier assumption (IA): OD algorithms often make assumptions about what constitutes   \n23 an inlier, which can be challenging to verify and validate [30].   \n24 \u2022 The curse of dimensionality (CD): As the dimensionality of data increases, the challenge of   \n25 identifying outliers intensifies, decreasing the effectiveness of certain OD algorithms [2]   \n26 \u2022 Multiple Views (MV): Outliers are often only visible in certain \"views\" of the data and are   \n27 hidden in the full space of original features [31]   \n29 The inlier assumption poses a challenge to algorithms that assume a standard profile of the inlier   \n30 data. For example, angle-based algorithms like ABOD [24] assume that inliers have other inliers   \n31 at all angles. Similarly, neighbor-based algorithms like kNN [34] assume that inliers have other   \n32 neighboring points nearby. These assumptions influence the scoring as it measures the degree to   \n33 which a sample deviates from this assumed norm. Consequently, the performance of these algorithms   \n34 may degrade if these assumptions do not hold [30]. This means that a general OD method should not   \n35 make any inlier assumptions.   \n36 The curse of dimensionality [2] refers to the decrease in the relative proximity of data points as the   \n37 number of dimensions increases. Simply put, with high dimensionality, the distance between any pair   \n38 of points becomes similar, regardless of whether none, one, or both of the points in a pair are outliers.   \n39 This is particularly problematic for OD algorithms that rely on distances or on identifying neighbors   \n40 to detect outliers, such as density- (e.g., LOF [3]), neighbor- (e.g., kNN [34]), and cluster-based (e.g.,   \nSVDD [1, Chapter 2]) OD algorithms.   \n42 Multiple Views refers to the phenomenon that certain complex correlations between features are only   \n43 observable in some feature subspaces [31]. As detailed in [1], this occurs when the dataset contains   \n44 additional irrelevant features, making some outliers only detectable in certain subspaces. In scenarios   \n45 where multiple subspaces contain different interesting structures, this problem is exacerbated. It then   \n46 becomes increasingly difficult to explain the variability of a data point based solely on its behavior in   \n47 a single subspace [23]. This problem can occur regardless of the dimensionality of the dataset if the   \n48 number of points is insufficient to capture a complex correlation structure.   \nThe following example illustrates the three problems described above ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/1597c4513341645856385b2f1420318d69fe545b60e941a006f450e520837de6.jpg", "img_caption": ["Figure 1: Scatterplots of the dataset from example 1. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "50 Example 1 (Effect of MV, IA and CD). Consider the random variables $\\mathbf{x}_{1},\\mathbf{x}_{2}$ and $\\mathbf{x}_{3}$ , where $\\mathbf{x}_{1}$ and   \n51 $\\mathbf{x}_{2}$ are highly correlated and $\\mathbf{x}_{3}$ is Gaussian noise. Figure 1 plots datasets with 20, 100 and 1000   \n52 realizations of $({\\bf x}_{1},{\\bf x}_{2},{\\bf x}_{3})$ . It also contains the classification boundaries from both a locality-based   \n53 method (green) and a cluster-based method (red) in the subspace. The cluster-based detector ftited in   \n54 the full $3D$ space fails to detect the outlier shown in the figure (red cross). However, the outlier is   \n55 always detected in the $2D$ subspace, as we can see. Once we increase the number of samples over   \n56 $n=1000$ , the cluster-based method detects the outlier in the full space (MV). On the contrary, the   \n57 locality-based method could not detect the outlier in any tested scenario $(M V+I A)$ . If we increase   \n58 the dimensionality by adding more features consisting of noise, no method can detect the outlier in   \n59 the full space $(M V+I A+C D,$ .   \n60 We are interested in tackling outlier detection whenever a population exhibits MV, like [31, 23, 25]   \n61 and as showcased in [1]. Particularly, the goal of this paper is to propose the first outlier detection   \n62 method that explicitly addresses IA, CD, and MV simultaneously.   \n63 As we will explain in the next section, we build on Generative Adversarial Active Learning   \n64 (GAAL) [44], a widely used approach for outlier detection [30, 17, 39]. It involves training a   \n65 Generative Adversarial Network (GAN) to mimic the distribution of outlier data, and it enhances   \n66 the discriminator\u2019s performance through active learning [38], leveraging the GAN\u2019s data generation   \n67 capability. GAAL methods avoid IA [30] and use the multi-layered structure of the GAN to overcome   \n68 the curse of dimensionality [33]. However, they often miss important subspaces, leading to MV.   \n69 Challenges. Training multiple GAN-based models in individual subspaces is not trivial. (1) The   \n70 joint training of generators and discriminators in GANs requires careful monitoring to determine   \n71 the optimal stopping point, a task that becomes daunting for large ensembles. (2) The generation of   \n72 difficult-to-detect points in a subspace remains hard [40]. (3) While several authors have proposed   \n73 multi-adversarial architectures for GANs [11, 5], none of them address adversaries tailored to   \n74 subspaces composed of feature subsets. Furthermore, these methods may not be suitable for GAAL   \n75 since they do not have convergence guarantees for detectors, as we will explain.   \n76 Contributions. (1) We propose GSAAL (Generative Subspace Adversarial Active Learning), a   \n77 novel GAAL method that uses multiple adversaries to learn the marginal inlier probability functions   \n78 in different data subspaces. Each adversary focuses on a single subspace. Simultaneously, we train   \n79 a single generator in the full space to approximate the entire distribution of the inlier class. All   \n80 networks are trained end-to-end, avoiding the ensembling problem. (2) To our knowledge, we give   \n81 the first mathematical formulation of the \u201cmultiple views\u201d problem. We used it to show the ability of   \n82 GSAAL to mitigate the MV problem. (3) We formulate the novel optimization problem for GSAAL   \n83 and give convergence guarantees of each discriminator to the marginal distribution of its respective   \n84 subspace. We also analyze the worst-case complexity of the method. (4) In extensive experiments we   \n85 compare GSAAL with multiple competitors. GSAAL was the only method capable of consistently   \n86 detecting anomalous data under MV. Furthermore, on 22 popular benchmark datasets for the one-class   \n87 classification task, GSAAL demonstrated SOTA-level performance and was orders of magnitude   \n88 faster in inference than its best competitors. (5) Our code is publicly available.1   \n89 Paper outline: Section 2 reviews related work, Section 3 contains the theoretical results for our method,   \n90 Section 4 features our experimental results, and Section 5 concludes and addresses limitations. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "table", "img_path": "qLtLQ4KUCq/tmp/19cb656fceec6ca655a8119d32396d8b076207701df92b296586b974cd986f32.jpg", "table_caption": ["Table 1: Families of OD methods with the limitations they address. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "91 2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "92 This section is a brief overview of popular unsupervised outlier detection methods for tabular data   \n93 related to our approach. We categorize them based on their ability to address the specific limitations   \n94 outlined above. Table 1 is a comparative summary. Further comments about OD in other data types   \n95 can be found in the appendix.   \n96 Classical Methods Conventional outlier detection approaches, such as distance-based strategies   \n97 like LOF and KNN, angle-based techniques like ABOD, and cluster-based methods like SVDD,   \n98 rely on specific assumptions on the behavior of inlier data. They use a scoring function to measure   \n99 deviations from this assumed norm. These methods face the inlier assumption limitation by definition.   \n00 For example, local methods that assume isolated outliers fail when several outlying samples fall   \n01 together. In addition, many classical methods, which rely on measuring distances, are susceptible to   \n102 the curse of dimensionality. Both limitations impair the effectiveness of these methods [30].   \n103 Subspace Methods Subspace-based methods [25] operate in lower-dimensional subspaces formed   \n104 by subsets of features. They effectively counteract the curse of dimensionality by focusing on   \n105 identifying so-called \u201csubspace outliers\u201d [22]. These outliers, which are prevalent in high-dimensional   \n106 datasets with many correlated features, are often elusive to conventional non-subspace methods [29,   \n107 31]. However, existing subspace methods inherently operate on specific assumptions on the nature of   \n108 anomalies in each subspace they explore, and thus face the inlier assumption limitation.   \n109 Generative Methods A common strategy to mitigate the IA and CD limitations is to reframe the   \n110 task as a classification task using self-supervision. A prevalent self-supervised technique, particularly   \n111 for tabular data, is the generation of artificial outliers [13, 30]. This method involves distinguishing   \n112 between actual training data and artificially generated data drawn from a predetermined \u201creference   \n113 distribution\u201d. [21] showed that by approximating the class probability of being a real sample, one   \n114 approximates the probability function of being an inlier. One then uses this approximation as a   \n115 scoring function [30]. However, it is not easy to find the right reference distribution, and a poor   \n116 choice can affect OD by much [21].   \n117 A first approach to this challenge proposed the use of na\u00efve reference distributions by uniformly   \n118 generating data in the space. This approach showed promising results in low-dimensional spaces but   \n119 failed in high dimensions due to the curse of dimensionality [21]. Other approaches, such as assuming   \n120 parametric distributions for inlier data [1, Chapter 2] or directly generating in susbpaces [12], can   \n121 avoid CD when the parametric assumptions are met. Methods that generate in the subspaces can   \n122 model the subspace behavior, additionally tackling the MV limitation. However, these last two   \n123 approaches do not address the IA limitation, as they make specific assumptions about the behavior of   \n124 the inlier data.   \n125 Generative Adversarial Active Learning According to [21], the closer the reference distribution   \n126 is to the inlier distribution, the better the final approximation to the inlier probability function will   \n127 be. Hence, recent developments in generative methods have focused on learning the reference   \n128 distribution in conjunction with the classifier. A key approach is the use of Generative Adversarial   \n129 Networks (GANs), where the generator converges to the inlier distribution [15]. The most common   \n130 approaches for this are GAAL-based methods [30, 17, 39]. These methods differentiate themselves   \n131 from other GANs for OD by training the detectors using active learning after normal convergence of   \n132 the GAN [36, 10]. The architecture of GAAL inherently addresses the curse of dimensionality, as   \n133 GANs can incorporate layers designed to manage high-dimensional data [33]. In practice, GAAL  \n134 based methods outperformed all their competitors in their original work. However, they overlook the   \n135 behavior of the data in subspaces and therefore may be susceptible to MV.   \n136 Our method, GSAAL, incorporates several subspace-focused detectors into GAAL. These detectors   \n137 approximate the marginal inlier probability functions of their subspaces. Thus, GSAAL effectively   \n138 addresses MV while inheriting GAAL\u2019s ability to overcome IA and CD limitations. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "139 3 Our Method: GSAAL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "140 We first formalize the notion of data exhibiting multiple views. We then use it to design our   \n141 outlier detection method, GSAAL, and give convergence guarantees. Finally, we derive the runtime   \n142 complexity of GSAAL. All the proofs and extra derivations can be found in the technical appendix. ", "page_idx": 3}, {"type": "text", "text": "143 3.1 Multiple Views ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "144 Several authors [1, 31, 23, 25, 29] have observed that at times the variability of the data can only be   \n145 explained from its behavior in some subspaces. Researchers variably call this problem \u201cthe subspace   \n146 problem\u201d [1, 25] or \u201cmultiple views of the data\u201d [22, 31]. Previous research has largely focused on   \n147 practical scenarios, leaving aside the need for a formal definition. In response, we propose a unifying   \n148 definition of \u201cmultiple views\u201d that provides a foundation for developing methods to address this   \n149 challenge effectively.   \n150 The problem \u201cmultiple views\u201d of data (MV) arises from two different effects. First, it involves the   \n151 ability to understand the behavior of a random vector $\\mathbf{x}$ by examining lower-dimensional subsets of   \n152 its components $(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d})$ . Second, it stems from the challenge of insufficient data to obtain an   \n153 effective scoring function in the full space of $\\mathbf{x}$ . As Example 1 shows, combining these two effects   \n154 obscures the behavior of the data in the full space. Hence, methods not considering subspaces when   \n155 building their scoring function may have issues detecting outliers under MV. The next definition   \n156 formalizes the first effect.   \n157 Definition 1 (myopic distribution). Consider a random vector $\\mathbf{x}:\\Omega\\longrightarrow\\mathbb{R}^{d}$ and $D i a g_{d\\times d}(\\{0,1\\})$ ,   \n158 the set of diagonal binary matrices without the identity. If there exists a random matrix $\\mathbf{u}:\\Omega\\longrightarrow$   \n159 $D i a g_{d\\times d}(\\{0,1\\})$ , such that   \n160 we say that the distribution of $\\mathbf{x}$ is myopic to the views of $\\mathbf{u}.$ . Here, $x$ and ux are realizations of $\\mathbf{x}$   \n161 and ux, and $p_{\\mathbf{x}}$ and $p_{\\mathbf{ux}}$ are the pdfs of $\\mathbf{x}$ and ux.   \n162 It is clear that, under MV, using $p_{\\mathbf{ux}}$ to build a scoring function instead of $p_{\\mathbf{x}}$ mitigates the effects.   \n163 This comes as the subspaces selected by $\\mathbf{u}$ are smaller in dimensionality. Hence it should take fewer   \n164 samples to approximate the pdf of ux. The difficulty is that it is not yet clear how to approximate   \n165 $p_{\\mathbf{ux}}$ . The following proposition elaborates on a way to do so. It states that by averaging a collection   \n166 of marginal distributions of $\\mathbf{x}$ in the subspaces given by realizations of $\\mathbf{u}$ , one can approximate the   \n167 distribution of $p_{\\mathbf{ux}}$ .   \n168 Proposition 1. Let x and $\\mathbf{u}$ be as before with $p_{\\mathbf{x}}$ myopic to the views of u. Consider a set of   \n169 independent realizations of u: $\\{u_{i}\\}_{i=1}^{k}$ . Then $\\begin{array}{r}{\\frac{1}{k}\\sum_{i}p_{u_{i}{\\bf x}}(u_{i}x)}\\end{array}$ is an unbiased statistic for $p_{\\mathbf{ux}}(u x)$ .   \n170 MV appears when there is a lack of data, and its distribution is myopic. To improve OD under MV,   \n171 one can exploit the distribution myopicity to model $\\mathbf{x}$ in the subspaces, where less data is sufficient.   \n172 Proposition 1 gives us a way to do so, by approximating $p_{\\mathbf{ux}}$ . In this way, under myopicity, this also   \n173 approximates $p_{\\mathbf{x}}$ , avoiding MV. Our method, GSAAL, exploits these derivations, as we explain next. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "174 3.2 GSAAL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "175 GAAL methods tackle IA by being agnostic to outlier definition and mitigate CD through the use of   \n176 multilayer neural networks [30, 28, 33]. GAAL methods have two steps:   \n177 1. Training of the GAN. Train the GAN consisting of one generator $\\mathcal{G}$ and one detector $\\mathcal{D}$ using   \n178 the usual min-max optimization problem as in [15].   \n179 2. Training of the detector through active learning. After convergence, $\\mathcal{G}$ is fixed, and $\\mathcal{D}$   \n180 continues to train. This last step is an active learning procedure with [44]. Following [21],   \n181 ${\\mathcal{D}}(x)$ now approximates the pdf of the training data $p_{\\mathbf{x}}$ .   \n182 After Step 2, the detector converges to $p_{\\mathbf{x}}$ . However, our goal is to approximate $p_{\\mathbf{x}}$ by exploiting   \n183 a supposed myopicity of the distribution. We extend GAAL methods to also address MV in what   \n184 follows. The following theorem adapts the objective function of the GAN to the subspace case and   \n185 gives guarantees that the detectors converge to the marginal pdfs used in Proposition 1:   \n186 Theorem 1. Consider x and u as in the previous definition, with x a realization of $\\mathbf{x}$ and $\\{u_{i}\\}_{i}$ a set   \n187 of realizations of u. Consider a generator $\\mathcal{G}:z\\in Z\\longmapsto\\mathcal{G}(z)\\in\\mathbb{R}^{d}$ and $\\{\\mathcal{D}_{i}\\}$ , $i=1,\\ldots,k$ , a set   \n188 of detectors such as $\\mathcal{D}_{i}:u_{i}x\\in S_{i}\\subset\\mathbb{R}^{d}\\longmapsto\\mathcal{D}_{i}(u_{i}x)\\in[0,1]$ . $Z$ is an arbitrary noise space where   \n189 $\\mathcal{G}$ randomly samples from. Consider the following optimization problem ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{G}}{\\mathrm{min}}\\underset{\\mathcal{D}_{i},\\,\\forall i}{\\mathrm{max}}\\sum_{i}V(\\mathcal{G},\\mathcal{D}_{i})=}\\\\ &{\\underset{\\mathcal{G}}{\\mathrm{min}}\\underset{\\mathcal{D}_{i},\\,\\forall i}{\\mathrm{max}}\\sum_{i}\\mathbb{E}_{u_{i}\\mathbf{x}}\\log\\mathcal{D}_{i}(u_{i}x)+\\mathbb{E}_{z}\\log\\left(1-\\mathcal{D}_{i}\\left(u_{i}\\mathcal{G}(z)\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "190 where each addend $V(\\mathcal{G},\\mathcal{D}_{i})$ is the binary cross entropy in each subspace. Under these conditions,   \n191 the following holds:   \n192 $i$ ) Each detector in optimum is $\\begin{array}{r}{D_{i}^{*}(u_{i}x)=\\frac{1}{2},\\forall x}\\end{array}$ . Thus, in optimum $V(\\mathcal{G},\\mathcal{D}_{i})=-\\log(4),\\forall i$ .   \n193 $i i]$ ) Each individual $\\mathcal{D}_{i}$ converges to $D_{i}^{*}(u_{i}\\bar{x})=p_{u_{i}x}(u_{i}x)$ after trained in Step 2 of a GAAL   \n119945 iii) $\\begin{array}{r}{\\widehat{D}^{*}(x)=\\frac{1}{k}\\sum_{i=1}^{k}{D}_{i}^{*}(u_{i}\\mathbf{x})}\\end{array}$ approximates $p_{\\mathbf{ux}}(u x)$ . If $p_{\\mathbf{x}}$ is myopic, $\\mathcal{D}^{*}(x)$ also approxi  \n196 mates $p_{\\mathbf{x}}(x)$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "197 Using Theorem 1 we can extend the GAAL methods to the subspace case: ", "page_idx": 4}, {"type": "text", "text": "198 1. Training the GAN. Train a GAN with one generator $\\mathcal{G}$ and multiple detectors $\\{\\mathcal{D}_{i}\\}$ with   \n199 Equation (2) as the objective function. The training of each detector stops when the loss   \n200 reaches its value with the optimum in Statement $(i)$ .   \n201 2. Training of the $k$ detectors by active learning. Train each $\\mathcal{D}_{i}$ as in Step 2 of a regular GAAL   \n202 method using $\\mathcal{G}$ . By Statement $(i i)$ of the Theorem, each $\\mathcal{D}_{i}$ will approximate $p_{u_{i}\\mathbf{x}}$ . By   \n203 Statement (iii), $\\begin{array}{r}{\\mathcal{D}(x)=\\frac{1}{k}\\sum_{i=1}^{k}\\mathcal{D}_{i}\\big(u_{i}\\mathbf{x}\\big)}\\end{array}$ will approximate $p_{\\mathbf{x}}$ under the myopicity of the   \n204 data.   \n205 We call this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL).   \n206 The appendix contains the pseudo-code for GSAAL. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "207 3.3 Complexity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "208 In this section, we focus on studying the theoretical complexity of GSAAL. We study both its usability   \n209 for training and, more importantly, for inference.   \n210 Theorem 2. Consider our GS\u221aAAL method with generator $\\mathcal{G}$ and detectors $\\{{\\mathcal{D}}_{i}\\}_{i=1}^{k}$ , each with four   \n211 fully connected hidden layers, $\\sqrt{n}$ nodes in the detectors and $d$ in the generator. Let $D$ be the training   \n212 data for GSAAL, with n data points and $d$ features. Then the following holds:   \n213 i) Time complexity of training is $\\mathcal{O}(E_{D}\\cdot n\\cdot(k\\cdot n+d^{2}))$ ). $E_{D}$ is an unknown complexity   \n214 variable depicting the unique epochs to convergence for the network in dataset $D$ .   \n215 ii) Time complexity of single sample inference is in ${\\mathcal{O}}(k\\cdot n)$ , with $k$ the number of detectors   \n216 used.   \n217 The linear inference times make GSAAL particularly appealing in situations where the model can be   \n218 trained once for each dataset, like one-class classification. We build on this particular strength in the   \n219 following section. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "220 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "221 This section presents experiments with GSAAL. We will outline the experimental setting, and examine   \n222 the handling of \u201cmultiple views\u201d in GSAAL and other OD methods. We then evaluate GSAAL\u2019s   \n223 performance against various OD methods and investigate its scalability. The appendix includes a   \n224 study on the sensitivity to the number of detectors, IA experiments, an ablaition study and extra   \n225 competitors evaluated in the real world datasets. System specifications are included in the appendix. ", "page_idx": 5}, {"type": "text", "text": "226 4.1 Experimental Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "227 This section has three parts: First, we describe the synthetic and real data for the outlier detection   \n228 experiments. Then, we describe the configuration of GSAAL. Finally, we present our competitors. ", "page_idx": 5}, {"type": "text", "text": "229 4.1.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "230 Synthetic. We constructed synthetic datasets, each containing two correlated features, $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ ,   \n231 along with 58 independent features $\\mathbf{x}_{j}$ , $j=3,\\ldots,60$ consisting of Gaussian noise. This approach   \n232 simulates datasets that exhibit the MV property by adding irrelevant features into a pair of highly   \n233 correlated variables. We detail the methodology and all correlation patterns in the technical appendix.   \n234 Real. We selected 22 real-world tabular datasets for our experiments from [19]. The selection   \n235 criteria included datasets with less than 10,000 data points, more than 10 outliers, and more than 15   \n236 features, focusing on high-dimensional data while keeping the runtime (of competing OD methods)   \n237 tractable. Table 2a contains the summary of the datasets. For datasets with multiple versions, we chose   \n238 the first in alphanumeric order. Details about each dataset are available in the original source [19]. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "239 4.1.2 Network Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "240 Structure. Unless stated otherwise, GSAAL uses the following network architecture. It consists of   \n241 four fully connected la\u221ayers with ReLu activation functions used in the generator and the detectors.   \n242 Each layer in $k\\,=\\,2\\sqrt{d}$ detectors has $\\sqrt{n}$ nodes, where $n$ and $d$ are the number of data points   \n243 and features in the training set, respectively. This configuration ensures linear inference time. The   \n244 generator has $d$ nodes in each layer, a standard in GAAL approaches, which ensures polynomial   \n245 training times. We assumed $\\mathbf{u}$ to be distributed uniformly across all subspaces. Therefore, we   \n246 obtained each subspace for the detectors by drawing uniformly from the set of all subspaces.   \n247 Training. Like other GAAL methods [30, 44], we train the generator $\\mathcal{G}$ together with all the   \n248 detectors $\\mathcal{D}_{i}$ until the loss of $\\mathcal{G}$ stabilizes. Then we train each detector $\\mathcal{D}_{i}$ until convergence with   \n249 $\\mathcal{G}$ fixed. To automate this process, we introduce an early stopping criterion: Training stops when a   \n250 detector\u2019s loss approaches the theoretical optimum $(-\\log(4))$ , see statement $(i i)$ of Theorem 1. For   \n251 consistency across experiments, training parameters remain fixed unless otherwise noted. Specifically,   \n252 the learning rates of the detectors and the generator are 0.01 and 0.001, respectively. We use minibatch   \n253 gradient descent [14] optimization, with a batch size of 500. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "qLtLQ4KUCq/tmp/d236f89fd19c012738878d399d5f2e2c5a14cb253d8431d99ad2f41effc209e1.jpg", "table_caption": ["(a) Real-world datasets converted to tabular if needed ", "Table 2: Real-world datasets and Competitors ", "(b) Competitors "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "254 4.1.3 Competitors ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "255 We selected popular and accessible methods from each category, as summarized in Table 2b, guided   \n256 by related work. We excluded generative methods with uniform distributions because they prove   \n257 ineffective for large datasets [21]. We could not include a generative method with subspace behavior   \n258 due to operational issues with the most relevant method in this class, [12], caused by its outdated   \n259 repository. We used the recommended parameters for all methods, as usual in OD [19].   \n260 We used the pyod [43] library to access all competitors except MO-GAAL. We used MO-GAAL   \n261 from its original source and implemented our method GSAAL in keras [6]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "262 4.2 Effect of Multiple Views on Outlier Detection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "263 To demonstrate the effectiveness of GSAAL under MV, we use synthetic datasets. Visualizing the   \n264 outlier scoring function in a 60-dimensional space is challenging, so we project it into the $\\mathbf{x}_{1}-\\mathbf{x}_{2}$   \n265 subspace. A method adept at handling MV should have a boundary that accurately reflects the $\\mathbf{x}_{1}$ and   \n266 $\\mathbf{x}_{2}$ dependency structure. We first generate a synthetic dataset $\\dot{D}^{\\mathrm{synth}}$ as described in section 4.1.1   \n267 and train the OD model. Using this model, we compute the scores for the points $(x_{1},x_{2},0,\\ldots,0)$   \n268 and visualize the level curves on the $\\mathbf{x}_{1}-\\mathbf{x}_{2}$ plane.   \n269 Figure 2 shows results for selected datasets and competitors, which are detailed in the Appendix. It   \n270 shows the level curves and decision boundaries (dashed lines) of the methods. Notably, our model   \n271 effectively detects correlations in the right subspace. To quantify this, we generated outliers in the   \n272 subspace of interest and extra inliers. We tested the one-class classification performance of each   \n273 method in 10 different MV datasets. On average, GSAAL managed to obtain 0.70 AUC, while the   \n274 second-best performer (IForest) did not surpass a random classifier \u20140.49 AUC. All results and   \n275 further details can be found in section B.2 in the appendix. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "276 4.3 One-class Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "277 This section evaluates GSAAL on a one-class classification task [37]. First, we study the effectiveness   \n278 of GSAAL on real data. Then, we investigate the scalability of GSAAL in practical scenarios. ", "page_idx": 6}, {"type": "text", "text": "279 4.3.1 Real-world Performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "280 We perform the outlier detection experiments on real datasets. Specifically, we take on the task of   \n281 one-class classification, where the goal is to detect outliers by training only on a collection of inliers   \n282 [19]. To evaluate the performance of OD methods, we use AUC as it is robust to test data imbalance,   \n283 a common issue in OD tasks . The procedure is as follows:   \n284 1. Split the dataset $D$ into a training set $D^{\\mathrm{train}}$ containing $80\\%$ of the inliers from $D$ , and a test   \n285 set $D^{\\mathrm{test}}$ containing the remaining inliers and all outliers.   \n286 2. Train an outlier detection model with $D^{\\mathrm{train}}$ and evaluate its performance on $D^{\\mathrm{test}}$ with ROC   \n287 AUC.   \n288 To save space, we moved the detailed AUC results to the appendix; showing that GSAAL obtained   \n289 the lowest median rank \u2014see Figure 10 in the appendix. Although other subspace methods tend to   \n290 perform better with irrelevant attributes [29, 25], they did not outperform classical OD methods on   \n291 average in our experiments. Notably, ABOD, the second-best method in our experiments, performed   \n292 poorly in the MV tests (Section 4.2).   \n293 For statistical comparisons, we use the Conover-Iman post hoc test for pairwise comparisons be  \n294 tween multiple populations [7]. It is superior to the Nemenyi test due to its improved type I error   \n295 boundings [8]. Conover-Iman test requires a preliminary positive result from a multiple population   \n296 comparison test, for which we employ the Kruskal-Wallis test [26].   \n297 Table 3 shows the test results. In each cell, $\"+\"$ indicates that the method in the row has a significantly   \n298 lower median rank than the method in the column, while \u2018\u2212\u2019 indicates a significantly higher median   \n299 rank. One symbol indicates p-values $\\leq0.15$ and two symbols indicate ${\\bf p}$ -values $\\leq0.05$ . A blank   \n300 indicates no significant difference. The table shows that GSAAL is superior to most of its competitors.   \n301 Our method does not significantly outperform the classical methods ABOD and kNN. However, these   \n302 methods struggle to detect structures in subspaces, showing their inadequacy in dealing with the MV   \n303 limitation, see Section 4.2.   \n304 Overall, the results support GSAAL\u2019s superiority in outlier detection tasks involving multiple views.   \n305 Additionally, they establish our method as the leading GAAL option for One-class classification ", "page_idx": 6}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/8645b755872ca9309f1f8124ce04f1844d6dcb60266a0eba4511f5247becd9cb.jpg", "img_caption": ["Figure 2: GSAAL finds classification boundaries for datasets banana and star under MV. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "qLtLQ4KUCq/tmp/902fd33e1fb7654b51ce2c917ad9e39e9316a2a3a7ccd4d4fc90706e6525bd5c.jpg", "table_caption": ["Table 3: Results of the Conover-Iman test for pairwise comparisons of the rankings. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "306 4.3.2 Scalability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "307 In section 3.3, we derived that the inference time of GSAAL scales linearly with the number of   \n308 training points if the number of detectors $k$ is fixed, while it does not depend on the number of   \n309 features $d$ . This is in contrast to other methods, in particular LOF, KNN, and ABOD, which have   \n310 quadratic runtimes in $d$ [3, 24]. We now validate this experimentally. The procedure is as follows:   \n311 1. Generate datasets $D_{\\mathrm{train}}$ and $D_{\\mathrm{test}}$ consisting of random points. $|D_{\\mathrm{test}}|=10^{6}$ .   \n312 2. Train an OD method using $D_{\\mathrm{train}}$ and record the inference time over $D_{\\mathrm{test}}$ .   \n313 Following the result of the sensitivity study in our appendix, we fixed $k=30$ . Figure 3a plots the   \n314 inference time of a single data point as a function of the number of features when $|D_{t r a i n}|=500$   \n315 Figure 3b plots the inference time as a function of the number of points in $D_{\\mathrm{train}}$ , for a fixed number of   \n316 100 features. Both figures confirm our complexity derivations and show that GSAAL is particularly   \n317 well-suited for large datasets. ", "page_idx": 7}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/54122a3797522b607e94eb657b4a7df6f4656e62bfac6d03a10b57de4818a3a0.jpg", "img_caption": ["Figure 3: Plots of different performance metrics for scalability "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "318 5 Limitations & Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "319 5.1 Limitations and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "320 In section 4 we randomly selected subspaces for training the detectors in GSAAL, i.e. we took   \n321 a uniform distribution of u. This was already sufficient to demonstrate the highly competitive   \n322 performance of our method. In practice, this assumption seemed to perform well for our experiments.   \n323 However, GSAAL can work with any subspace search strategy to obtain the distribution of u, for   \n324 example, the methods exploiting multiple views [23, 22]. We have not included them in this paper   \n325 due to the lack of an official implementation. In the future, we plan to benchmark various subspace   \n326 search methods in GSAAL.   \n327 Next, GSAAL is limited to tabular data, since the \u201cmultiple views\u201d problem has only been observed   \n328 for this data type. The mathematical formulation of MV in section 3 does not exclude unstructured   \n329 data. The difficulty lies in identifying good search strategies for u for non-tabular data, which remains   \n330 an open question [18]. However, depending on the type of unstructured data, extending GSAAL to   \n331 work with it is not immediate. Therefore, building a method that exploits the theoretical derivations   \n332 of GSAAL for structured data is future work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "333 5.2 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "334 Unsupervised outlier detection (OD) methods rely on a scoring function to distinguish inliers from   \n335 outliers, since the true probability function that generated the dataset is usually unavailable in practice.   \n336 However, they face one or more of the following problems \u2014 Inlier Assumption (IA), Curse of   \n337 Dimensionality (CD), or Multiple Views (MV). In this article, we have proposed the first mathematical   \n338 formulation of MV, which allows for a better understanding of how to solve this occurrence. Using   \n339 this formulation, we developed GSAAL, which is the first OD approach that solves MV, CD, and IA.   \n340 In short, GSAAL is a generative adversarial network with a generator and multiple detectors ftited in   \n341 the subspaces to find outliers not visible in the full space. In our experiments on 27 different datasets,   \n342 we demonstrated the usefulness of GSAAL, in particular, its ability to deal with MV and its superior   \n343 performance on OD tasks with real datasets. In addition, we have shown that GSAAL can scale up to   \n344 deal with high-dimensional data, which is not the case for our most competent competitors. These   \n345 results confirm GSAAL\u2019s ability to deal with data exhibiting MV and its usability in any practical   \n346 scenario involving large datasets. ", "page_idx": 8}, {"type": "text", "text": "347 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "348 [1] C. C. Aggarwal. Outlier Analysis. Springer International Publishing, Cham, 2017. ", "page_idx": 8}, {"type": "text", "text": "349 [2] R. Bellman. Dynamic programming. Princeton, New Jersey: Princeton University Press. XXV,   \n350 342 p. (1957)., 1957.   \n351 [3] M. M. Breunig, H. Kriegel, R. T. Ng, and J. Sander. LOF: identifying density-based local   \n352 outliers. In SIGMOD Conference, pages 93\u2013104. ACM, 2000.   \n353 [4] G. O. Campos, A. Zimek, J. Sander, R. J. G. B. Campello, B. Micenkov\u00e1, E. Schubert, I. Assent,   \n354 and M. E. Houle. On the evaluation of unsupervised outlier detection: measures, datasets, and   \n355 an empirical study. Data Mining and Knowledge Discovery, 30(4):891\u2013927, Jul 2016.   \n356 [5] J. Choi and B. Han. Mcl-gan: Generative adversarial networks with multiple specialized   \n357 discriminators. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,   \n358 editors, Advances in Neural Information Processing Systems, volume 35, pages 29597\u201329609.   \n359 Curran Associates, Inc., 2022.   \n360 [6] F. Chollet et al. Keras. https://keras.io, 2015.   \n361 [7] W. Conover and R. Iman. Multiple-comparisons procedures. informal report. Technical report,   \n362 Los Alamos National Laboratory (LANL), Feb. 1979.   \n363 [8] W. J. W. J. Conover. Practical nonparametric statistics / W.J. Conover. Wiley series in   \n364 probability and statistics. Applied probability and statistics section. Wiley, New York ;, third   \n365 edition. edition, 1999.   \n366 [9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional   \n367 transformers for language understanding. In North American Chapter of the Association for   \n368 Computational Linguistics, 2019.   \n369 [10] J. Donahue, P. Kr\u00e4henb\u00fchl, and T. Darrell. Adversarial feature learning. In International   \n370 Conference on Learning Representations, 2017.   \n371 [11] I. Durugkar, I. M. Gemp, and S. Mahadevan. Generative multi-adversarial networks. ArXiv,   \n372 abs/1611.01673, 2016.   \n373 [12] C. D\u00e9sir, S. Bernard, C. Petitjean, and L. Heutte. One class random forests. Pattern Recognition,   \n374 46(12):3490\u20133506, 2013.   \n375 [13] R. El-Yaniv and M. Nisenson. Optimal single-class classification strategies. In B. Sch\u00f6lkopf,   \n376 J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems, volume 19.   \n377 MIT Press, 2006.   \n378 [14] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http:   \n379 //www.deeplearningbook.org.   \n380 [15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and   \n381 Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence,   \n382 and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27.   \n383 Curran Associates, Inc., 2014.   \n384 [16] A. Goodge, B. Hooi, S.-K. Ng, and W. S. Ng. Lunar: Unifying local outlier detection methods   \n385 via graph neural networks. ArXiv, abs/2112.05355, 2021.   \n386 [17] J. Guo, Z. Pang, M. Bai, P. Xie, and Y. Chen. Dual generative adversarial active learning.   \n387 Applied Intelligence, 51(8):5953\u20135964, Aug 2021.   \n388 [18] N. Gupta, D. Eswaran, N. Shah, L. Akoglu, and C. Faloutsos. Lookout on time-evolving graphs:   \n389 Succinctly explaining anomalies from any detector, 2017.   \n390 [19] S. Han, X. Hu, H. Huang, M. Jiang, and Y. Zhao. Adbench: Anomaly detection benchmark. In   \n391 S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in   \n392 Neural Information Processing Systems, volume 35, pages 32142\u201332159. Curran Associates,   \n393 Inc., 2022.   \n394 [20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE   \n395 Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2015.   \n396 [21] K. Hempstalk, E. Frank, and I. H. Witten. One-class classification by combining density and   \n397 class probability estimation. In W. Daelemans, B. Goethals, and K. Morik, editors, Machine   \n398 Learning and Knowledge Discovery in Databases, pages 505\u2013519, Berlin, Heidelberg, 2008.   \n399 Springer Berlin Heidelberg.   \n400 [22] F. Keller, E. Muller, and K. Bohm. Hics: High contrast subspaces for density-based outlier   \n401 ranking. In 2012 IEEE 28th International Conference on Data Engineering, pages 1037\u20131048,   \n402 2012.   \n403 [23] F. Keller, E. M\u00fcller, A. Wixler, and K. B\u00f6hm. Flexible and adaptive subspace search for   \n404 outlier analysis. In Proceedings of the 22nd ACM International Conference on Information &   \n405 Knowledge Management, CIKM \u201913, page 1381\u20131390, New York, NY, USA, 2013. Association   \n406 for Computing Machinery.   \n407 [24] H. Kriegel, M. Schubert, and A. Zimek. Angle-based outlier detection in high-dimensional data.   \n408 In KDD, pages 444\u2013452. ACM, 2008.   \n409 [25] H.-P. Kriegel, P. Kr\u00f6ger, E. Schubert, and A. Zimek. Outlier detection in axis-parallel subspaces   \n410 of high dimensional data. In T. Theeramunkong, B. Kijsirikul, N. Cercone, and T.-B. Ho, editors,   \n411 Advances in Knowledge Discovery and Data Mining, pages 831\u2013838, Berlin, Heidelberg, 2009.   \n412 Springer Berlin Heidelberg.   \n413 [26] W. H. Kruskal. A nonparametric test for the several sample problem. The Annals of Mathemati  \n414 cal Statistics, 23(4):525\u2013540, 1952.   \n415 [27] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436\u2013444, May 2015.   \n416 [28] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Poczos. Mmd gan: Towards deeper   \n417 understanding of moment matching network. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,   \n418 R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing   \n419 Systems, volume 30. Curran Associates, Inc., 2017.   \n420 [29] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation forest. In 2008 Eighth IEEE International   \n421 Conference on Data Mining, pages 413\u2013422, 2008.   \n422 [30] Y. Liu, Z. Li, C. Zhou, Y. Jiang, J. Sun, M. Wang, and X. He. Generative adversarial active   \n423 learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data   \n424 Engineering, 32(8):1517\u20131528, 2020.   \n425 [31] E. M\u00fcller, I. Assent, P. Iglesias, Y. M\u00fclle, and K. B\u00f6hm. Outlier ranking via subspace analysis   \n426 in multiple views of the data. In 2012 IEEE 12th International Conference on Data Mining,   \n427 pages 529\u2013538, 2012.   \n428 [32] B. Perozzi, L. Akoglu, P. Iglesias S\u00e1nchez, and E. M\u00fcller. Focused clustering and outlier   \n429 detection in large attributed graphs. In Proceedings of the 20th ACM SIGKDD International   \n430 Conference on Knowledge Discovery and Data Mining, KDD \u201914, page 1346\u20131355, New York,   \n431 NY, USA, 2014. Association for Computing Machinery.   \n432 [33] T. Poggio, A. Banburski, and Q. Liao. Theoretical issues in deep networks. Proceedings of the   \n433 National Academy of Sciences, 117(48):30039\u201330045, 2020.   \n434 [34] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient algorithms for mining outliers from large   \n435 data sets. In Proceedings of the 2000 ACM SIGMOD International Conference on Management   \n436 of Data, SIGMOD \u201900, page 427\u2013438, New York, NY, USA, 2000. Association for Computing   \n437 Machinery.   \n438 [35] L. Ruff, R. Vandermeulen, N. Goernitz, L. Deecke, S. A. Siddiqui, A. Binder, E. M\u00fcller, and   \n439 M. Kloft. Deep one-class classification. In J. Dy and A. Krause, editors, Proceedings of the   \n440 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine   \n441 Learning Research, pages 4393\u20134402. PMLR, 10\u201315 Jul 2018.   \n442 [36] T. Schlegl, P. Seeb\u00f6ck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised   \n443 anomaly detection with generative adversarial networks to guide marker discovery. In M. Ni  \n444 ethammer, M. Styner, S. Aylward, H. Zhu, I. Oguz, P.-T. Yap, and D. Shen, editors, Information   \n445 Processing in Medical Imaging, pages 146\u2013157, Cham, 2017. Springer International Publishing.   \n446 [37] N. Seliya, A. Abdollah Zadeh, and T. M. Khoshgoftaar. A literature review on one-class   \n447 classification and its potential applications in big data. Journal of Big Data, 8(1):122, Sep 2021.   \n448 [38] B. Settles. Active learning literature survey. 2009.   \n449 [39] S. Sinha, S. Ebrahimi, and T. Darrell. Variational adversarial active learning. In Proceedings of   \n450 the IEEE/CVF International Conference on Computer Vision, pages 5972\u20135981, 2019.   \n451 [40] G. Steinbuss and K. B\u00f6hm. Hiding outliers in high-dimensional data spaces. International   \n452 Journal of Data Science and Analytics, 4(3):173\u2013189, Nov 2017.   \n453 [41] H. Wang, M. J. Bah, and M. Hammad. Progress in outlier detection techniques: A survey. IEEE   \n454 Access, 7:107964\u2013108000, 2019.   \n455 [42] H. Xu, G. Pang, Y. Wang, and Y. Wang. Deep isolation forest for anomaly detection. IEEE   \n456 Transactions on Knowledge and Data Engineering, 35(12):12591\u201312604, 2023.   \n457 [43] Y. Zhao, Z. Nasrullah, and Z. Li. Pyod: A python toolbox for scalable outlier detection. Journal   \n458 of Machine Learning Research, 20(96):1\u20137, 2019.   \n459 [44] J.-J. Zhu and J. Bento. Generative adversarial active learning. arXiv preprint arXiv:1702.07956,   \n460 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "461 A Theoretical Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "462 In this appendix, we will include all the proofs of the included theorems and propositions. Addition  \n463 ally, we also extend all non-experimental sections with relevant information for the experimental   \n464 appendix. ", "page_idx": 12}, {"type": "text", "text": "465 A.1 Previous Remarks ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "466 Before starting to prove our main results, it is important to add a remark about our notation in this   \n467 article. Whenever we denote ux, we mean the operation resulting in the following vector: $\\mathbf{u}(\\omega)\\mathbf{x}(\\omega)$ .   \n468 Thus, ux is a random vector following its distribution $p_{\\mathbf{ux}}$ . However, it is important to remark that   \n469 $u x$ , and therefore, also $u_{i}\\mathbf{x}$ , does not state the usual matrix-vector multiplication. What we mean by   \n470 $u x$ is the operation $U\\times_{M}x$ , where $U$ stands for the range-complete version of $u$ and $\\times_{M}$ the usual   \n471 matrix multiplication. This means that whenever we write $u x$ we are considering the projection of $x$   \n472 into the subspace of the features selected in $u$ . This means that $u_{i}\\mathbf{x}$ is the random vector composed   \n473 of the features selected by $u_{i}$ , and therefore, $p_{u_{i}\\mathbf{x}}(u_{i}x)$ denotes subsequent marginal pdf of $\\mathbf{x}$ . We   \n474 do not state this in the main text as it functionally does not change anything of our derivations, and   \n475 simply works as a notation. The only important remarks stemming from this fact are the following:   \n476 1. $p_{\\mathbf{x}}(u_{i}x)=p_{\\mathbf{x}}(\\pi_{u_{i}}(x))$ , where $\\pi_{u_{i}}$ denotes the projection of a point $x$ into the subspace of   \n477 $u_{i}$ . Therefore, we can write $p_{\\mathbf{x}}(u_{i}x)=p_{u_{i}\\mathbf{x}}(u_{i}x)$ .   \n478 2. The operator as stated before is not distributive. This is trivial, as given u a random matrix as   \n479 in definition 1, $(1_{d}-{\\bf u}){\\bf x}$ is defined properly, as $1_{d}-\\mathbf{u}\\in D i a g(\\{0,1\\})$ . However, $\\mathbf{x}-\\mathbf{ux}$   \n480 denotes the vector subtraction between two vectors with different dimensionality.   \n481 While not important to understand the following proofs and the derivations from the main text,   \n482 understanding this is crucial for anyone seeking to work with these definitions. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "483 A.2 Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "484 We will reformulate all of the statements for completion before introducing each proof. ", "page_idx": 12}, {"type": "text", "text": "485 Proposition 2. Let x and u be as before with $p_{\\mathbf{x}}$ myopic to the views of u. Consider a set of   \n486 independent realizations of u: $\\{u_{i}\\}_{i=1}^{k}$ , a realization of $\\mathbf{x},\\;x_{:}$ , and a realization of ux, ux. Then   \n487 $\\begin{array}{r}{\\frac{1}{k}\\sum_{i}p_{u_{i}{\\bf x}}(u_{i}x)}\\end{array}$ is a statistic for $p_{\\mathbf{ux}}(u x)$ . ", "page_idx": 12}, {"type": "text", "text": "488 Proof. Consider $\\mathbf{x}$ and $\\mathbf{u}$ as in the statement. Recall the law of total probabilities: ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{\\mathbf{ux}}(u x)=\\mathbb{E}_{\\mathbf{u}}\\left(p_{\\mathbf{ux}|\\mathbf{u}=u^{\\prime}}(u x|u^{\\prime})\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "489 By taking the definition of $\\mathbf{u}$ and the myopicity, it is trivial that: ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{{\\bf u x}|{\\bf u}=u^{\\prime}}(u x|u^{\\prime})=p_{u^{\\prime}{\\bf x}}(u^{\\prime}x)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "490 for $u^{\\prime}$ such that $p_{\\mathbf{u}}(u^{\\prime})\\neq0$ . ", "page_idx": 12}, {"type": "text", "text": "491 Then, by definition of marginal probability and expectation, we have that: ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{{\\bf u x}}(u x)=\\sum_{i=1}^{N}p_{\\bf u}(u_{i})p_{u_{i}{\\bf x}}(u_{i}x),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "492 as $\\mathbf{u}$ is discrete with finite set of occurrences of size $N$ . Thus, we can approximate   \n493 $\\begin{array}{r}{\\sum_{i=1}^{N}p_{\\mathbf{u}}(u_{i})p_{u_{i}\\mathbf{x}}(u_{i}x))}\\end{array}$ by $\\begin{array}{r}{\\frac{1}{k}\\sum_{i}p_{u_{i}\\mathbf{x}}}\\end{array}$ with $u_{i}$ independent samples of $\\mathbf{u}$ .   \n494 Theorem 3. Consider x and u as in the previous definition, with x a realization of $\\mathbf{x}$ and $\\{u_{i}\\}_{i}$ a set   \n495 of realizations of u. Consider a generator $\\mathcal{G}:z\\in Z\\longmapsto\\mathcal{G}(z)\\in\\mathbb{R}^{d}$ and $\\{\\mathcal{D}_{i}\\}$ , $i=1,\\ldots,k$ , a set   \n496 of detectors such as $\\mathcal{D}_{i}:u_{i}x\\in S_{i}\\subset\\mathbb{R}^{d}\\longmapsto\\mathcal{D}_{i}(u_{i}x)\\in[0,1]$ . $Z$ is an arbitrary noise space where   \n497 $\\mathcal{G}$ randomly samples from. Consider the following objective function ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{G}}{\\mathrm{min}}\\underset{\\mathcal{D}_{i},\\,\\forall i}{\\mathrm{max}}\\sum_{i}V(\\mathcal{G},\\mathcal{D}_{i})=}\\\\ &{\\underset{\\mathcal{G}}{\\mathrm{min}}\\underset{\\mathcal{D}_{i},\\,\\forall i}{\\mathrm{max}}\\sum_{i}\\mathbb{E}_{u_{i}\\mathbf{x}}\\log\\mathcal{D}_{i}(u_{i}x)+\\mathbb{E}_{z}\\log\\left(1-\\mathcal{D}_{i}\\left(u_{i}\\mathcal{G}(z)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "498 Under these conditions, the following holds: ", "page_idx": 12}, {"type": "text", "text": "500 $i i^{\\cdot}$ ) Each individual $\\mathcal{D}_{i}$ converges to $D_{i}^{*}(u_{i}x)=p_{u_{i}x}(u_{i}x)$ after trained in Step 2 of a GAAL   \n501 method.   \n502 iii) $\\begin{array}{r}{\\mathcal{D}^{*}(x)=\\frac{1}{k}\\sum_{i=1}^{k}\\mathcal{D}_{i}^{*}(u_{i}\\mathbf{x})}\\end{array}$ approximates $p_{\\mathbf{ux}}(u x)$ . If $p_{\\mathbf{x}}$ is myopic, $\\mathcal{D}^{*}(x)$ also approxi  \n503 mates $p_{\\mathbf{x}}(x)$ .   \n504 Proof. This proof will follow mainly the results in [15], adapted for our case. We will first derivative   \n505 two general results that we are going to use to immediately prove $(i),(i i)$ and $(i i i)$ . First, consider   \n506 the objective function ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i}V(\\mathcal{G},\\mathcal{D}_{i})=\\sum_{i}\\mathbb{E}_{u_{i}\\mathbf{x}\\sim p_{u_{i}\\mathbf{x}}}\\log(\\mathcal{D}_{i}(u_{i}x))+}}\\\\ &{}&{\\mathbb{E}_{\\mathbf{z}\\sim p_{\\mathbf{z}}}(1-\\log(\\mathcal{D}_{i}(u_{i}\\mathcal{G}(z)))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "507 where $\\mathbf{z}$ is the random vector used by $\\mathcal{G}$ to sample from the noise space $Z$ . We will write $\\mathbb{E}_{\\mathbf{x}},\\mathbb{E}_{\\mathbf{z}}$ and   \n508 $\\mathbb{E}_{u_{i}\\mathbf{x}}$ instead of $\\mathbb{E}_{\\mathbf{x}\\sim p_{\\mathbf{x}}},\\mathbb{E}_{\\mathbf{z}\\sim p_{\\mathbf{z}}}$ and $\\mathbb{E}_{u_{i}\\mathbf{x}\\sim p_{u_{i}\\mathbf{x}}}$ as an abuse of notation. ", "page_idx": 13}, {"type": "text", "text": "509 The problem is, then, to optimize: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathcal{G}}\\operatorname*{max}_{\\mathcal{D}_{i},\\;\\forall i}\\sum_{i}V(\\mathcal{G},\\mathcal{D}_{i}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "510 Fixing $\\mathcal{G}$ and maximizing for all $\\mathcal{D}_{i}$ , each detector individually maximizes $V(\\mathcal{G},\\mathcal{D}_{i})$ . Let us try to   \n511 obtain the optimal of each $\\mathcal{D}_{i}$ with a fixed $\\mathcal{G}$ . First, we write: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V(\\mathcal{G},\\mathcal{D}_{i})=\\displaystyle\\int_{u_{i}x}p_{u_{i}\\mathbf{x}}(u_{i}x)\\log\\mathcal{D}_{i}(u_{i}x)d u_{i}x+}\\\\ {\\displaystyle\\int_{z}p_{\\mathbf{z}}(z)\\log(1-\\mathcal{D}_{i}(u_{i}\\mathcal{G}(z)))d z.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "512 As $\\mathcal{G}$ uses $\\mathbf{z}$ to sample from its sample distribution $p_{\\mathcal{G}}(x)$ , we can rewrite the second addent, like in   \n513 [15], as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V(\\mathcal{G},\\mathcal{D}_{i})=\\displaystyle\\int_{u_{i}x}p_{u_{i}{\\mathbf{x}}}(u_{i}x)\\log\\mathcal{D}_{i}(u_{i}x)d u_{i}x+}\\\\ &{\\qquad\\qquad\\qquad\\displaystyle\\int_{u_{i}x}p_{\\mathcal{G}}(u_{i}x)\\log(1-\\mathcal{D}_{i}(u_{i}x))d u_{i}x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "514 Aggregating both integrals, we have a function of the type $f(t)\\,=\\,a\\log(t)+b\\log(1-t)$ , with   \n515 $a,\\bar{b}\\in\\bar{\\mathbb{R}}-\\bar{\\{0\\}}$ . We know that $f(t)$ obtains its optimum in $\\textstyle t={\\frac{a}{a+b}}$ . As $f(t)\\in\\mathbb{R}^{+}$ , $V(\\mathcal{G},\\mathcal{D}_{i})$ obtains   \n516 its optimum for a given $\\mathcal{G}$ in: ", "page_idx": 13}, {"type": "equation", "text": "$$\nD_{i}^{*}(u_{i}x)=\\frac{p_{u_{i}x}(u_{i}x)}{p_{u_{i}{\\bf x}}(u_{i}x)+p_{\\mathcal{G}}(u_{i}x)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "517 Let us now consider the following function ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\mathcal{G})=\\displaystyle\\sum_{i}\\operatorname*{max}_{\\mathcal{D}_{i},\\;\\forall i}(\\mathcal{G},\\mathcal{D}_{i})}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{i}\\mathbb{E}_{u_{i}\\mathbf{x}}\\log\\frac{p_{u_{i}\\mathbf{x}}(u_{i}x)}{p_{u_{i}\\mathbf{x}}(u_{i}x)+p_{\\mathcal{G}}(u_{i}x)}+}\\\\ &{\\quad\\quad\\mathbb{E}_{u_{i}\\mathbf{x}\\sim p_{\\mathcal{G}}}\\log\\frac{p_{\\mathcal{G}}(u_{i}x)}{p_{u_{i}\\mathbf{x}}(u_{i}x)+p_{\\mathcal{G}}(u_{i}x)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "518 This is known in Game Theory as the cost function of player \u201c $\\scriptstyle{\\mathcal{G}}$ \u201d in the null-sum game defined by   \n519 the min max optimization problem. [15] refers to it as the virtual training criterion of the GAN. The   \n520 adversarial game defined by (4) reaches an equilibrium (and thus, the min max problem an optimum)   \n521 whenever $C(\\mathcal{G})$ is minimized. We will study the value of $\\mathcal{G}$ in such equilibrium and use it, together   \n522 with (5), to prove the statements. ", "page_idx": 13}, {"type": "text", "text": "523 Rewriting $C(\\mathcal{G})$ it is clear that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C(\\mathcal{G})=\\displaystyle\\sum_{i}K L\\left(p_{u_{i}\\mathbf{x}(u_{i}x)}||\\frac{p_{u_{i}\\mathbf{x}}(u_{i}x)+p_{\\mathcal{G}}(u_{i}x)}{2}\\right)}\\\\ {+\\,K L\\left(p_{\\mathcal{G}}(u_{i}x)||\\frac{p_{u_{i}\\mathbf{x}}(u_{i}x)+p_{\\mathcal{G}}(u_{i}x)}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "524 This expression corresponds to that of a sum of multiple binary cross entropies between a population   \n525 coming from $p_{u_{i}\\mathbf{x}}$ and from $p_{\\mathcal{G}}$ projected by $u_{i}$ . Therefore, as we know, we can rewrite: ", "page_idx": 14}, {"type": "equation", "text": "$$\nC(G)=\\sum_{i}2J S D(p_{u_{i}\\mathbf{x}(u_{i}x)}\\|p_{\\mathcal{G}}(u_{i}x)),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "526 with $J S D$ the Jensen-Shannon divergence. Since $J S D(s\\|r)\\,\\in\\,[0,\\log(2))$ , it is clear that $C(\\mathcal{G})$   \n527 obtains its minimum only whenever ", "page_idx": 14}, {"type": "equation", "text": "$$\np\\varsigma(u_{i}x)=p_{u_{i}\\mathbf{x}}(u_{i}x),\\forall\\forall x^{2};\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "528 and for all $i\\in\\{1,\\ldots,k\\}$ . ", "page_idx": 14}, {"type": "text", "text": "529 Knowing $\\mathcal{G}$ and $\\mathcal{D}_{i}$ in the optimum for all $i$ , we can prove the statements above: ", "page_idx": 14}, {"type": "text", "text": "530 (i) As $p_{\\mathcal{G}}(u_{i}x)=p_{u_{i}{\\bf x}}(u_{i}x)$ for almost all $x$ , in the optimum of (4), it is immediate that: ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{i}(u_{i}x)=\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "531 i.e., the detectors cannot differentiate between the real training data and the synthetic data of the   \n532 generator. If one employs the numerically stable version of each $V(\\mathcal{G},\\mathcal{D}_{i})$ (equivalent to the   \n533 numerically stable version of the binary cross entropy [6]), it is trivial to see that ", "page_idx": 14}, {"type": "equation", "text": "$$\nV^{\\mathrm{stable}}(\\mathcal{G},\\mathcal{D}_{i})=\\log(2).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "534 (ii) After optimizing (4), training each $D_{i}$ individually with $\\mathcal{G}$ fixed, is the equivalent of building a   \n535 two-class classifier distinguishing between the artificial class generated by $p_{\\mathcal{G}}(\\bar{u}_{i}x)=p_{u_{i}{\\bf x}}(u_{i}x)$ and   \n536 the real data coming from $p_{u_{i}\\mathbf{x}}(u_{i}x)$ . By [21], the resulting two-class classifier would be such as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{i}(u_{i}x)=p_{u_{i}{\\bf x}}(u_{i}x).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "537 (iii) By proposition 2 and statement $(i i),\\textstyle{\\frac{1}{k}}\\sum_{i}D_{i}^{*}(u_{i}x)$ is an estimator for $p_{\\mathbf{ux}}(u x)$ . By myopicity,   \n538 it is also of $p_{\\mathbf{x}}(x)$ . \u53e3   \n539 Theorem 4. Giving our GSAA\u221aL method with generator $\\mathcal{G}$ and detectors $\\{{\\mathcal{D}}_{i}\\}_{i=1}^{k}$ , each with four   \n540 fully connected hidden layers, $\\sqrt{n}$ nodes in the detectors and $d$ in the generator, we obtain that:   \n541 i) The training time complexity is bounded with $\\mathcal{O}(E_{D}\\cdot n\\cdot(k\\cdot n+d^{2}))$ , for a dataset $D$ with   \n542 n training samples and $d$ features. $E_{D}$ is an unknown complexity variable depicting the   \n543 unique epochs to convergence for the network in dataset $D$ .   \n544 ii) The single sample inference time complexity is bounded with $O(k\\cdot n)$ , with $k$ the number of   \n545 detectors used.   \n546 Proof. An evaluation of a neural network is composed of two steps, the backpropagation, and the   \n547 fowardpass steps. While training the network requires both, inference requires only a fowardpass.   \n548 Therefore, we will first prove $(i i)$ and will build upon it to prove $(i)$ .   \n549 (ii). GSAAL consists of a generator and $k$ detectors. Single point inference consists of a single   \n550 fowardpass of all the detectors. We will first prove the general complexity of a fowardpass of a   \n551 general fully connected 4 layer network and will use it to derive all the other complexities. Let us   \n552 consider three weight matrices $W_{j i}$ , $W_{h j}$ and $W_{l h}$ each between two layers, with $j,i,h$ and $l$ being   \n553 the number of nodes in each. Therefore, $W_{j i}$ denotes a matrix with $j$ rows and $i$ columns, and so   \n554 on. Now, let us consider $x_{i1}$ the datapoint after passing the input layer. Lastly, without any loss of   \n555 generality, consider $f$ to be the activation function for all layers. This way, the forward pass of a   \n556 single detector can be written as: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\nc_{l1}=f\\left(W_{l h}f\\left(W_{h j}f\\left(W_{j i}x_{i1}\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "557 We will study the complexity in the first layer and use it to derive the complexity of the others.   \n558 $A_{j1}=W_{j i}x_{i1}$ is a simple matrix-vector multiplication that we know to be $\\mathcal{O}(j\\cdot i)$ atmost. Then, as   \n559 $f$ is an activation function, $f(A_{j1})$ is equivalent to writing $f_{j1}\\odot A_{j1}$ , with $\\odot$ being the element-wise   \n560 multiplication. Thus, $f\\left(W_{j i}x_{i1}\\right)$ is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{O}(j\\cdot i+j)=\\mathcal{O}(j\\cdot(i+1))=\\mathcal{O}(j\\cdot i).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "561 Doing this for all layers, we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{O}(l\\cdot h+k\\cdot j+j\\cdot i).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "562 As all layers have $\\sqrt{n}$ nodes, ", "page_idx": 15}, {"type": "equation", "text": "$$\nO(3n)=O(n).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "563 As we have $k$ detectors, the complexity for a fowardpass of all detectors, and thus, for a single sample   \n564 inference of GSAAL is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{O}}(k\\cdot n).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "565 (i). A backpropagation step has the same complexity as an inference step on all training samples.   \n566 As we have $n$ training samples, this then becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\nO(k\\cdot n^{2})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "567 for the detectors. As the training consists of multiple epochs, we will write ", "page_idx": 15}, {"type": "equation", "text": "$$\nO(E_{D}\\cdot k\\cdot n^{2}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "568 with $E_{D}$ being the number of epochs needed for convergence for the training data set $D$ . As the   \n569 training consists of both backpropagation and fowardpass steps on all training samples, the total   \n570 training time complexity for all detectors is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{O}}(E_{D}\\cdot k\\cdot n^{2}+k\\cdot n^{2})={\\mathcal{O}}(E_{D}\\cdot k\\cdot n^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "571 As we also need to consider the generator, we will use equation 8 to derive both steps on the generator.   \n572 As the generator is also a fully connected 4-layer network, with all layers having $d$ nodes, the   \n573 complexity for a single fowardpass is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{O}}(d^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "574 As during training one generates $n$ samples during each fowardpass: ", "page_idx": 15}, {"type": "equation", "text": "$$\nO(n\\cdot d^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "575 Now, on each backpropagation pass the network calculates the backpropagation error for each   \n576 generated sample, thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\nO(n\\cdot d^{2})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "577 is also the time complexity for the backpropagation step of the generator. Considering all $E_{D}$ epochs   \n578 and both backpropagation and fowardpass steps of the generator and all the detectors, the time   \n579 complexity of GSAAL\u2019s training is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{O}(E_{D}\\cdot k\\cdot n^{2}+E_{D}\\cdot n\\cdot d^{2})=\\mathcal{O}(E_{D}\\cdot n\\cdot(k\\cdot n+d^{2}))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "580 ", "page_idx": 15}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/aefecb1213e0e5efe1ebc4228a566e54643217fd644695046389656359917188.jpg", "img_caption": ["Figure 4: Difference in statistical distance between two populations. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "581 A.3 Related Work (extension) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "582 Deep Outlier Detection for other data types. Outlier detection is also very popular in different   \n583 data types, especially in unstructured data [42, 16, 36, 35, 32]. Due to the complexity of the data they   \n584 are used for, deep methods are the main approach employed for this task. The main difference with   \n585 the other deep methods introduced for tabular data, is that the deep architecture in the later targets   \n586 mainly CD. For unstructured data types, like images or natural language, is the complexity of the data   \n587 that drives the architecture. For example, to treat image data, multiple linear layers do not suffice,   \n588 complex layers like convolutional or residual layers are employed for this [27].   \n589 Although popular, most deep methods have limited to no use at all in tabula data in their original   \n590 articles. However, some have appeared in the literature of tabular data as competitors [36, 35]. We   \n591 identified the most common for our task in related articles and benchmarks, and included them as an   \n592 extension of our main experiments in sections B.2 and B.3. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "593 A.4 Multiple Views (extension) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "594 In this section we extend the derivations in section 3.1 by providing an example of a myopic   \n595 distribution:   \n596 Example 2 (Myopic distribution). Consider a x like in example 1. Here, it is clear that $\\mathbf{x_{1}},\\mathbf{x_{2}}\\bot\\mathbf{x_{3}}$ .   \n597 Consider, then, u such that: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{u}:\\{1\\}\\longrightarrow\\{d i a g(1,1,0)\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "598 To test whether $p_{\\mathbf{x}}$ is myopic, we employed a simple test utilizing a statistical distance (MMD with   \n599 the identity kernel) between $p_{\\mathbf{x}}$ and $p_{\\mathbf{ux}}$ . This way, $i f M{\\hat{M}}D(p_{\\mathbf{x}}{\\bar{\\|}}p_{\\mathbf{ux}})=0,$ , it would be clear that the   \n600 equality holds. As a control measure, we also calculated the same distance for a different population   \n601 $\\mathbf{x}^{\\prime}$ , where $\\mathbf{x}_{3}\\,=\\,\\mathbf{x}_{1}^{2}$ . We have plotted the results in image $\\mathcal{I},$ , where Population $^{\\,l}$ refers to x and   \n602 Population 2 to $\\mathbf{x}^{\\prime}$ . As we can see, we do obtain a positive result in the test of myopicity for x and a   \n603 negative one for $\\mathbf{x}^{\\prime}$ . ", "page_idx": 16}, {"type": "text", "text": "604 A.5 GSAAL (extension) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "605 We now extend the results from section 3.2 by providing the pseudocode for the training of our   \n606 method. It is important to consider that, while theorem 3 formulates the optimization problem   \n607 in terms of the neural networks $\\mathcal{G}$ and $\\{\\boldsymbol{D}_{i}\\}_{i}$ , in practice this will not be the case. Instead, we   \n608 will consider the optimization in terms of their weights, $\\Theta_{\\mathcal{G}}$ and $\\Theta_{\\mathcal{D}_{i}}$ . Therefore, in practice, the   \n609 convergence into an equilibrium will be limited by the capacity of the networks themselves [14].   \n610 We considered the optimization to follow minibatch-stochastic gradient descent [14]. To consider   \n611 any other minibatch-gradient method it will suffice to perform the necessary transformations to the   \n612 gradients.   \n613 The pseudocode is located in Algorithm 1. As it is the training for the method, it takes both   \n614 the parameters for the method and the training. In this case, epochs refers to the total number   \n615 of epochs we will train in total, while stop_epoch marks the epoch where we start step 2 of the   \n616 GAAL training. Lines 1-3 initialize both the detectors in their subspaces and the generator with ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 GSAAL training   \nRequire: Data set $D$ , Number of Discriminators $\\kappa$ , u, epochs, stop_epoch   \n1: Initialize Generator $\\mathcal{G}$ {# $d$ is the dimensionality of $D$ }   \n2: $\\{u_{i}\\}_{i=1}^{\\kappa}\\gets\\mathsf{D R A W F R O M u}(\\kappa)$   \n3: Initialize Discriminators $\\{\\mathcal{D}_{i}\\}_{i=1}^{\\kappa}$ with unique subspaces $\\{u_{i}\\}_{i=1}^{\\kappa}$   \n4: for epoch \u2208 $\\bar{\\mathbf{\\xi}}\\{1,...,e p o c h\\dot{s}\\}$ do   \n5: for batch $\\in\\{1,...,b a t c h e s\\}$ do   \n6: noise $\\leftarrow$ Random noise $\\bar{z}^{(1)},...,z^{(m)}$ from $Z$   \n7: $d a t a\\gets$ Draw current batch $\\bar{x}^{(1)},...,x^{(m)}$   \n8: for $j\\in\\{1...k\\}$ do   \n9: Update $\\mathcal{D}_{j}$ by ascending the stochastic gradient: $\\begin{array}{r l}{\\lefteqn{\\nabla_{\\Theta_{\\mathcal{D}_{j}}}\\frac{1}{m}\\sum_{i=1}^{m}\\log(\\mathcal{D}_{j}(u_{j}x^{(i)}))+}\\quad}&{{}}\\end{array}$ $\\log(1-\\mathcal{D}_{j}\\big(u_{j}\\mathcal{G}(z^{(i)})\\big))$   \n10: end for   \n11: if epoch $<$ stop_epoch then   \n12: Update $\\mathcal{G}$ by descending the stochastic gradient: $\\begin{array}{r}{\\nabla_{\\Theta_{G}}\\frac{1}{k}\\sum_{j=1}^{k}\\frac{1}{m}\\sum_{i=1}^{m}\\log(1\\;-\\;}\\end{array}$ $\\mathcal{D}_{j}\\big(\\mathcal{G}(z^{(i)})\\big)\\big)$   \n13: end if   \n14: end for   \n15: end for ", "page_idx": 17}, {"type": "table", "img_path": "qLtLQ4KUCq/tmp/9e2a4f5dd0ab590d8945491b26958a7374c98ac043828313b3bc4b23ac397327.jpg", "table_caption": ["Table 4: Different outliers generated for the experiments. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "617 random weight matrices $\\Theta_{\\mathcal{D}_{i}}$ and $\\Theta_{\\mathcal{G}}$ . Lines 4-13 correspond to the normal GAN training loop   \n618 across multiple epochs, referred to as step 1 of a GAAL method, if epoch $<$ stop_epoch. Here   \n619 we proceed with training each detector and the generator using their gradients. Lines 8-10 update   \n620 each detector by ascending its stochastic gradient, while line 11 updates the generator by descending   \n621 its stochastic gradient. After the normal GAN training, we start the active learning loop [30] once   \n622 $e p o c h\\ge s t o p\\_e p o c h$ . The only difference with the regular GAN training is that $\\mathcal{G}$ remains fixed, i.e.,   \n623 we do not descend using its gradient. This allows us to additionally train the detectors and, in case of   \n624 equilibrium of step 1, converge to the desired marginal distributions as derived in theorem 3. ", "page_idx": 17}, {"type": "text", "text": "625 B Experimental Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "626 In this section, we will include a supplementary experiment testing the IA condition for completion,   \n627 the sensibility experiments, and an ablation study. Additionally, we extended both main experimental   \n628 studies featured in the main text. All of the code for the extra experiments, as well as for all   \n629 experiments in the main text, can be found in our remote repository3. Our experiments used a RTX   \n630 3090 GPU and an AMD EPYC 7443p CPU running Python in Ubuntu 22.04.3 LTS. Deep neural   \n631 network methods were trained on the GPU and inferred on the CPU; shallow methods used only the   \n632 CPU. ", "page_idx": 17}, {"type": "text", "text": "633 B.1 Effects of Inlier Assumptions on Outlier Detection ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "634 GAAL methodologies are capable of dealing with the inlier assumption by learning the correct inlier   \n635 distribution $p_{\\mathbf{x}}$ without any assumption [30]. While this should also extend to our methodology, we   \n636 will study experimentally whether this condition holds in practice. To do so, as one cannot identify   \n637 beforehand whether a method is going to fail due to IA, we will generate synthetic datasets. This will   \n638 allow us to generate outliers that we know to follow from a specific IA, ensuring that failure comes   \n639 from the anomalies themselves. We will include all of the code in the code repository. To generate   \n640 the synthetic datasets we follow:   \n641   \n642   \n643   \n644   \n645   \n646   \n647   \n648   \n649 ", "page_idx": 17}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/39b79918a52b4cc1bfbb738538ecf5076eec65910accf73e2d41471c71c10d29.jpg", "img_caption": ["Figure 5: 2D-example of the different types of anomalies we generate using the method summarized in table 4. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/59364347f33941cb94bdd8bfc56bb7f298b91a918851650bc023eb6cacc23dac.jpg", "img_caption": ["Figure 6: AUCs of the different methods in the IA experiments. From left to right: Local (blue), Angle (orange) and Cluster (green). "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "1. Generate $D$ , a population of 2000 inliers following some distribution $F$ in $\\mathbb{R}^{20}$ .   \n2. Select an outlier detection method $M$ with some assumption about the normality of the data and fit it using $D$ . We will call such $M$ as the reference model for the generation.   \n3. Generate 400 outliers by sampling on $\\mathbb{R}^{20}$ uniformly and keeping only those points $o$ such that $M(o)\\,=\\,1$ (i.e., they are detected as outliers). We will write $\\dot{O}^{D}$ to refer to such a collection of points.   \n4. Repeat step 3 10 times, to obtain $O_{1}^{D},\\ldots,O_{10}^{D}$ .   \n5. Sample out $20\\%$ of the points in $D$ . The remainder $80\\%$ will be stored in $D^{\\mathrm{train}}$ , and the other $20\\%$ in $D_{1}^{\\mathrm{test}},\\ldots,D_{10}^{\\mathrm{test}}$ together with each $O_{i}^{D}$ . ", "page_idx": 18}, {"type": "text", "text": "650 These steps were repeated 4 times with different $F$ , to create 4 different training sets and 40 different   \n651 testing sets, corresponding to a total of 40 different datasets employed per model $M$ selected in step   \n652 2. As we used 3 different reference models, we have a total of 120 different datasets employed in   \n653 this experiment alone. In particular, the models used for this are collected in table 4. The table   \n654 contains the name of the outlier type, the description of the IA taken to generate them, and a brief   \n655 description of how the outliers should look. Column $M$ contains the method employed to generate   \n656 each, these being $L O F$ , $A B O D$ , and the same inlier distribution as $D$ , but with multiple shifted   \n657 means $\\mu_{i}$ and with a significantly lower amount of points $n$ . A visualization of how these outliers   \n658 would look with 2 features is located in figure 5. To study how different methods behave when   \n659 detecting these outliers, we have performed the same experiments as in section 4.3, but with these   \n660 synthetic datasets. Figure 6 gathers all the AUCs of a method in 3 boxplots, one for each outlier type   \n61 in each training set. Additionally, we grouped all based on the IA and assigned a similar color for   \n662 all of them. We have done this for the classical OD methods LOF, ABOD, and kNN, besides our   \n663 method GSAAL. We cropped the image below 0.45 in the $y$ axis as we are not interested in results   \n664 below a random classifier. As we can see, classical methods seem to correctly detect outliers for   \n665 an outlier type that verifies its IA. However, whenever we introduce outliers behaving outside of   \n666 their IA, the performance hit is significant. Notoriously, it appears that none of them had trouble   \n667 detecting the Local and Angle outlier type. regardless of their IA. This can be easily explained by   \n668 those outliers types being similar, as we can see in figure 5. On the other hand, GSAAL manages to   \n669 have a significant detection rate regardless of the outlier type. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "670 B.2 Effects of Multiple Views on Outlier Detection (extension) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "671 In this section, we will include a brief description of the generation process for the datasets used in   \n672 section 4.2. We will also perform the same experiment as in section 4.2 for all methods showcased in   \n673 the main text and additional datasets. The datasets were generated by the following formulas: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Banana. Given $\\theta\\in[0,\\pi]$ we have $\\mathbf{x}=\\sin(\\theta)+U(0,0.1)$ and $\\mathbf{y}=\\sin(\\theta)^{3}+U(0,0.1)$ .   \n75 \u2022 Spiral. Given $\\theta\\,\\in\\,[0,4\\pi]$ and $r\\,\\in\\,(0,1)$ , we have $\\mathbf{x}\\,=\\,r\\cos(\\theta)+U(0,0.1)$ and $\\textbf{y}=$   \n6 $r\\sin(\\theta)$ . \u2022 Star. Given $\\theta\\in[0,2\\pi]$ and $r\\in\\{r\\in\\mathbb{R}|r=\\sin(5\\theta);r\\geq0,1,0.4\\}$ , we have $\\mathbf{x}=r\\cos(\\theta)+$   \n8 $U(0,0.1)$ and $\\mathbf{y}=r\\sin(\\theta)+U(0,0.1)$ .   \n9 \u2022 Circle. Given $\\theta\\in[0,2\\pi]$ , we have $\\mathbf{x}=\\cos(\\theta)+U(0,0.1)$ and $\\mathbf{y}=\\sin(\\theta)+U(0,0.1)$ .   \n0 \u2022 $L$ . Given $x_{1}\\,=\\,N(0,0.1),x_{2}\\,=\\,U(0,5),y_{1}\\,=\\,U(-5,0)$ , and $y_{2}\\,=\\,N(0,0.1)$ ; we have $\\mathbf{x}=\\mathsf{c o n c a t}(x_{1},x_{2})$ and $\\mathbf{y}=\\mathsf{c o n c a t}(y_{1},y_{2})$ . ", "page_idx": 19}, {"type": "text", "text": "682 We considered $N(0,0.1)$ to denote a random normal realization with $\\mu\\,=\\,0$ and $\\sigma^{2}\\,=\\,0.1$ , and   \n683 $U(a,b)$ to denote a uniform realization in the $[a,b]$ interval.   \n684 Figure 7 contains all images from the MV experiment. We employed the default parameters for all   \n685 methods in this experiments. We did that as those were the employed parameters in our real world   \n686 experiments. Additonally, the choice of parameter did not impact the outcome of the experiment   \n687 much. Our remote repository includes extra images for every competitor with multiple parameters   \n688 for comparison. We do not have any new insight beyond the ones exposed in the main article. Note   \n689 that we have included all methods but SOD. The reason was that SOD failed to execute for datasets   \n690 Star, Spiral, and Circle.   \n691 Additionally, we added competitors from outside of our related work that will later be used in section   \n692 B.3. In particular, we employed LUNAR, DIF and DeepSVDD with default parameters. We included   \n693 extra images in our remote repository with multiple parameters for the deep competitors as well. The   \n694 method AnoGAN was not included due to it failing in datasets Star, Spiral and Circle. Their results   \n695 can be seen in Figure 8. As it also happened our main competitors, some of the extra competitors were   \n696 capable of detecting the data structure in very sparse occasions. However they remained incapable to   \n697 properly describe a boundary consistently. The only method that was sensible enough in all datasets   \n698 was GSAAL.   \n699 In order to quantify this, we tested the ability of all methods to perform one-class classification in   \n700 each dataset. As outliers, we used white noise in the $\\mathbf{x}_{1}-\\mathbf{x}_{2}$ subspace. Additionally, we created two   \n701 extra datasets greatly different from the rest, $X$ and wave: ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "702 ", "page_idx": 19}, {"type": "text", "text": "703 ", "page_idx": 19}, {"type": "text", "text": "\u2022 $X$ . Given $x_{1}=x_{2}=U(-1,1)$ and $y_{1}=x_{1}+U(0,0.1),y_{2}=x_{2}+U(0,0.1)$ ; we have $\\mathbf{x}=\\mathsf{c o n c a t}(x_{1},x_{2})$ and $\\mathbf{y}=\\mathsf{c o n c a t}(y_{1},y_{2}).$ .. \u2022 Wave. Given $\\theta\\in[0,4\\pi]$ , we have $\\mathbf{x}=\\theta$ and $\\mathbf{y}=\\sin(x)+U(0,0.1)$ . ", "page_idx": 19}, {"type": "text", "text": "704 ", "page_idx": 19}, {"type": "text", "text": "705 We will also use them as outleirs, for a total of 15 different datasets. We also generated extra inliers   \n706 in each test set. We gathered the AUC results in Figure 9. As we can see, all other methods struggel   \n707 to come ahead of the random classifier, marked with a dashed line. The only method well above that   \n708 is GSAAL. ", "page_idx": 19}, {"type": "text", "text": "709 B.3 One-class Classification (extension) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "710 As we noted in Section 4, we obtained our benchmark datasets from [19], a benchmark study for   \n711 One-class classification methods in tabular data. Some of the datasets featured in the study, and   \n712 also in our experiments, were obtained from embedding image or text data using a pre-trained NN   \n713 (ResNet [20] and BERT [9], respectively). We shunt the interested reader into [19] for additional   \n714 information. Additionally, we found discrepancies between the versions of the datasets in the study   \n715 of [4] and [19]. We utilized the version of those datasets featured in [4] for our experiments due   \n716 to popularity. This affected the datasets Arrhythmia, Annthyroid, Cardiotocography, InternetAds,   \n717 Ionosphere, SpamBase, Waveform, WPBC and Hepatitis. Figure 10 summarizes the ranks from the   \n718 one-class experiments in section 4.3. Table 5 summarizes the AUC results from our experiments. As   \n719 mentioned in section A.3, we also included extra methods outside of our related work. Particularly,   \n720 we added deep versions tailored to image data of previously included methods \u2014DeepSVDD [35]   \n721 and Deep Isolation Forest [42] (DIF)\u2014 and others that extend some types of outlier detectors into   \n722 image and text data \u2014LUNAR [16], as an extension of Locality-based classical methods, and   \n723 AnoGAN [36], as an extension of Generative methods. For their parameters, we employed the   \n724 recommended ones for LUNAR and DIF, and trained the models the same way that the authors did   \n725 in their articles. As for DeepSVDD and AnoGAN, as they do not have any recommended way of   \n726 training nor hyperparameters, we performed a grid search for their training parameters and kept the   \n727 best result. We used all of their official implementations4. All deep methods (including MO-GAAL   \n728 and GSAAL) were trained multiple times with the same train set and their results were averaged to   \n729 account for initialization.   \n730 Additionally, we gathered all extra deep methods and performed the same statistical analysis as in   \n731 section 4.3. We also included MO GAAL besides GSAAL for completion. SO GAAL, the single   \n732 generator version of MO GAAL was not included, even if popular in the related literature. The   \n733 reason is that authors in [30] showed that MO GAAL constantly outperforms SO GAAL in the outlier   \n734 detection task. Results are included in table 6, gathered after a positive Kruskal-Wallis test. As we can   \n735 see, GSAAL outperform almost all competitors except LUNAR (the most recent method). However,   \n736 LUNAR is incapable to detect change in the subspaces as GSAAL does, see section B.2. Therefore,   \n737 regardless of considering the tabular related work, or the more generalist deep methods, GSAAL   \n738 still can outperform most competitors in the field. Additionally, for those that GSAAL performs   \n739 similar to, we showed that we are more sensible to changes in subspaces. This fact makes GSAAL   \n740 the preferred option for One-class classification under MV. ", "page_idx": 19}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/0c4df127daae6bad9dfc5f4bef4fd8694ec7bc9650de1c3fdf29bd0ad7917a9a.jpg", "img_caption": ["Figure 7: Projected classification boundaries for the datasets in section 4.2 and the extra datasets. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/0ba30e571a9a532e90985cabe5327823cc2c116a9f9de129972cd1d7eb27113c.jpg", "img_caption": ["Figure 8: Projected classification boundaries of the competitors outside of our related work. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/cebc92065402d37a617f5217ddab87c3373437041529380143f91ab041ffc9f5.jpg", "img_caption": ["Figure 9: AUC results in the MV datasets. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/5c857e30a0b8210734e8462d402ea2e114a0e867c8fb400bf95dcce572d0429f.jpg", "img_caption": ["Figure 10: Boxplots of the ranks used for the Conover-Iman experiment in section 4.3. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "qLtLQ4KUCq/tmp/1c84d5f503a59bbc83817e0d5c210dbafb68b78e713d66bea56e2a22d2a6fa01.jpg", "img_caption": ["Figure 11: Performance of the detector with different values of $k$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "741 B.4 Parameter Sensibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "742 We now explore the effect of the number of detectors in GSAAL, $k$ , by repeating the previous   \n743 experiments with varying $k$ . Figure 11a plots the median AUC for different $k$ values, showing a   \n744 stabilization \u221aat larger $k$ . Next, Figure 11b compares the results with a fixed $k=30$ and the default   \n745 value $k=2\\sqrt{d}$ used in the previous experiments; there is no large difference in either the AUC or the   \n746 ranks. We also found that the results in Table 3 remain almost the same if one sets $k=30$ . So we   \n747 recommend fixing $k=30$ , which makes GSAAL very suitable for high-dimensional data. ", "page_idx": 22}, {"type": "text", "text": "748 B.5 Ablation study ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "749 Lastly, we also performed an ablation study for GSAAL. We identify two critical components in our   \n750 method, the subspace nature of our detectors, and the multiple detectors used. Table 7 contains a   \n751 summary of the included features in each considered configuration. We will compare the performance   \n752 of all the different configurations of GSAAL.   \n753 We will employ, once again, the Conover-Iman test to compare the performance of all configuration   \n754 in a statistically sound way. Table 8 contains the results of the ablation experiment. As expected, our   \n755 fully configured method significantly outperformed all of the others. This further confirms that the   \n756 performance increase over our competitors comes directly from tackling the MV problem. ", "page_idx": 22}, {"type": "table", "img_path": "qLtLQ4KUCq/tmp/41de4dd1800c3543c5a0c20ff9cc183ce832ff559487918928f91fcf3e243490.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "qLtLQ4KUCq/tmp/cc5d8e3827610cb49cd8a388dd0729cdf77519c6d7ba9c65dc88680b6e41f63e.jpg", "table_caption": ["Table 6: Results of the Conover-Iman test for all the Deep methods. ", ""], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "qLtLQ4KUCq/tmp/ec62ff825806b10584a628a0cb2d6563e75152c9f1eee9c9b7efe5917aefcaee.jpg", "table_caption": ["Table 7: Summary of the included components in the ablation study. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "table", "img_path": "qLtLQ4KUCq/tmp/8295c9f868979b327a63d006a95f1c1dbb274ae978c98f81d7d98e92e86c97b5.jpg", "table_caption": ["Table 8: Results of the Connover-Iman test for the ablation study. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "757 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "759 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n760 paper\u2019s contributions and scope?   \n761 Answer: [Yes]   \n762 Justification: sections 3 for the theoretical claims, 4.2 for the MV claims, and 4.3 for the   \n763 real world performance claims.   \n764 Guidelines:   \n765 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n766 made in the paper.   \n767 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n768 contributions made in the paper and important assumptions and limitations. A No or   \n769 NA answer to this question will not be perceived well by the reviewers.   \n770 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n771 much the results can be expected to generalize to other settings.   \n772 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n773 are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "74 2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "9 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n80 the paper has limitations, but those are not discussed in the paper.   \n1 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n2 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n3 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n84 model well-specification, asymptotic approximations only holding locally). The authors   \n5 should reflect on how these assumptions might be violated in practice and what the   \n86 implications would be.   \n7 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n88 only tested on a few datasets or with a few runs. In general, empirical results often   \n9 depend on implicit assumptions, which should be articulated.   \n90 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n1 For example, a facial recognition algorithm may perform poorly when image resolution   \n92 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n93 used reliably to provide closed captions for online lectures because it fails to handle   \n94 technical jargon.   \n95 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n96 and how they scale with dataset size.   \n97 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n98 address problems of privacy and fairness.   \n99 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n0 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n01 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n2 judgment and recognize that individual actions in favor of transparency play an impor  \n3 tant role in developing norms that preserve the integrity of the community. Reviewers   \n4 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "805 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "806 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n807 a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "821 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 4 includes all details about our experimental setup (competitors, datasets, experiments & training). Section A in the appendix includes the pseudo-code as well ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "861 5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "862 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n863 tions to faithfully reproduce the main experimental results, as described in supplemental   \n864 material?   \n865 Answer: [Yes]   \n866 Justification: We include our GitHub (anonymized for the double-blind phase).   \n867 Guidelines:   \n868 \u2022 The answer NA means that paper does not include experiments requiring code.   \n869 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n870 public/guides/CodeSubmissionPolicy) for more details.   \n871 \u2022 While we encourage the release of code and data, we understand that this might not be   \n872 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n873 including code, unless this is central to the contribution (e.g., for a new open-source   \n874 benchmark).   \n875 \u2022 The instructions should contain the exact command and environment needed to run to   \n876 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n877 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n878 \u2022 The authors should provide instructions on data access and preparation, including how   \n879 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n880 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n881 proposed method and baselines. If only a subset of experiments are reproducible, they   \n882 should state which ones are omitted from the script and why.   \n883 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n884 versions (if applicable).   \n885 \u2022 Providing as much information as possible in supplemental material (appended to the   \n886 paper) is recommended, but including URLs to data and code is permitted.   \n887 6. Experimental Setting/Details   \n888 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n889 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n890 results?   \n891 Answer: [Yes]   \n892 Justification: We explain our processes for one-class classification in section 4.3. Hyper  \n893 parameters, as well as optimizers, are included in section 4.1. Additionally, our remote   \n894 repository contains the full details.   \n895 Guidelines:   \n896 \u2022 The answer NA means that the paper does not include experiments.   \n897 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n898 that is necessary to appreciate the results and make sense of them.   \n899 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n900 material.   \n901 7. Experiment Statistical Significance   \n902 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n903 information about the statistical significance of the experiments?   \n904 Answer: [Yes]   \n905 Justification: We utilized a statistical test to study the significance of all of our performance   \n906 results \u2014see tables 3, 6, 8. We also extensively used boxplots of all AUC results to visualize   \n907 our performance in different scenarios \u2014see figures 6, 9, 10, 11.b.   \n908 Guidelines:   \n909 \u2022 The answer NA means that the paper does not include experiments.   \n910 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n911 dence intervals, or statistical significance tests, at least for the experiments that support   \n912 the main claims of the paper.   \n913 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n914 example, train/test split, initialization, random drawing of some parameter, or overall   \n915 run with given experimental conditions).   \n916 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n917 call to a library function, bootstrap, etc.)   \n918 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n919 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n920 of the mean.   \n921 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n922 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n923 of Normality of errors is not verified.   \n924 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n925 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n926 error rates).   \n927 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n928 they were calculated and reference the corresponding figures or tables in the text.   \n929 8. Experiments Compute Resources   \n930 Question: For each experiment, does the paper provide sufficient information on the com  \n931 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n932 the experiments?   \n933 Answer: [Yes]   \n934 Justification: See the beginning of section B   \n935 Guidelines:   \n936 \u2022 The answer NA means that the paper does not include experiments.   \n937 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n938 or cloud provider, including relevant memory and storage.   \n939 \u2022 The paper should provide the amount of compute required for each of the individual   \n940 experimental runs as well as estimate the total compute.   \n941 \u2022 The paper should disclose whether the full research project required more compute   \n942 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n943 didn\u2019t make it into the paper).   \n944 9. Code Of Ethics   \n945 Question: Does the research conducted in the paper conform, in every respect, with the   \n946 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n947 Answer: [Yes]   \n948 Justification: We reviewed the NeurIPS Code of Ethics and found no violation.   \n949 Guidelines:   \n950 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n951 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n952 deviation from the Code of Ethics.   \n953 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n954 eration due to laws or regulations in their jurisdiction).   \n955 10. Broader Impacts   \n956 Question: Does the paper discuss both potential positive societal impacts and negative   \n957 societal impacts of the work performed?   \n958 Answer: [Yes]   \n959 Justification: In sections, 1 & 5 we go through the importance of outlier detection in   \n960 many fields, particularly for our use-case. Our positive impact on society consists of the   \n961 improvement of the tasks where outlier detection is needed.   \n962 Guidelines:   \n963 \u2022 The answer NA means that there is no societal impact of the work performed.   \n964 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n965 impact or why the paper does not address societal impact.   \n966 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n967 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n968 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n969 groups), privacy considerations, and security considerations.   \n970 \u2022 The conference expects that many papers will be foundational research and not tied   \n971 to particular applications, let alone deployments. However, if there is a direct path to   \n972 any negative applications, the authors should point it out. For example, it is legitimate   \n973 to point out that an improvement in the quality of generative models could be used to   \n974 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n975 that a generic algorithm for optimizing neural networks could enable people to train   \n976 models that generate Deepfakes faster.   \n977 \u2022 The authors should consider possible harms that could arise when the technology is   \n978 being used as intended and functioning correctly, harms that could arise when the   \n979 technology is being used as intended but gives incorrect results, and harms following   \n980 from (intentional or unintentional) misuse of the technology.   \n981 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n982 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n983 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n984 feedback over time, improving the efficiency and accessibility of ML).   \n985 11. Safeguards   \n986 Question: Does the paper describe safeguards that have been put in place for responsible   \n987 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n988 image generators, or scraped datasets)?   \n989 Answer: [NA]   \n990 Justification: We do not identify any risks.   \n991 Guidelines:   \n992 \u2022 The answer NA means that the paper poses no such risks.   \n993 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n994 necessary safeguards to allow for controlled use of the model, for example by requiring   \n995 that users adhere to usage guidelines or restrictions to access the model or implementing   \n996 safety filters.   \n997 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n998 should describe how they avoided releasing unsafe images.   \n999 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1000 not require this, but we encourage authors to take this into account and make a best   \n1001 faith effort.   \n1002 12. Licenses for existing assets   \n1003 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1004 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1005 properly respected?   \n1006 Answer: [Yes]   \n1007 Justification: We include URLs and citations for all dataset selections, packages, and   \n1008 methods.   \n1009 Guidelines:   \n1010 \u2022 The answer NA means that the paper does not use existing assets.   \n1011 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1012 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1013 URL.   \n1014 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1015 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1016 service of that source should be provided. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Justification: We include the documentation of our implementation in the repository. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "56 Question: Does the paper describe potential risks incurred by study participants, whether   \n57 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n58 approvals (or an equivalent approval/review based on the requirements of your country or   \n59 institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]