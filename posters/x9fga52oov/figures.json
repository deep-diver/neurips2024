[{"figure_path": "X9Fga52OOv/figures/figures_1_1.jpg", "caption": "Figure 1: Results of Short and Long Videos. The first row of each case shows 16-frame videos generated using short video diffusion models (LaVie [1] and VideoCrafter2 [2]). Directly extending these models to longer videos, like those with 128 frames, preserves temporal consistency but lacks fine spatial-temporal details. In contrast, our proposed FreeLong adapts short video diffusion models to create consistent long videos with high fidelity.", "description": "This figure compares the results of generating short (16 frames) and long (128 frames) videos using two different approaches.  The first row shows videos generated by directly extending pre-trained short video diffusion models (LaVie and VideoCrafter).  While temporally consistent, these long videos lack detail. The second row shows videos produced by the proposed method, FreeLong.  FreeLong improves the generation of long videos by maintaining the temporal consistency while greatly enhancing the fine spatial-temporal details, resulting in higher fidelity video.", "section": "1 Introduction"}, {"figure_path": "X9Fga52OOv/figures/figures_3_1.jpg", "caption": "Figure 2: Ratio of short video SNR on high/low frequency to different long videos. Our findings reveal that: (a) When direct extend short video diffusion model to generate long videos, the SNR of high-frequency components in the space-time frequency domain degrades significantly as video length increases. (b) In the spatial frequency domain, the SNR of high-frequency components decreases even more substantially, resulting in the over-smoothing of each frame. (c) Conversely, in the temporal frequency domain, the SNR of high-frequency components increases significantly, introducing temporal flickering.", "description": "This figure presents a frequency analysis of long videos generated by directly extending short video diffusion models.  It shows that extending these models to generate longer videos leads to a decrease in high-frequency spatial components and an increase in high-frequency temporal components.  The SNR (signal-to-noise ratio) of high-frequency components decreases significantly as video length increases in the space-time and spatial domains. This results in a loss of detail and over-smoothing. Conversely, in the temporal domain, the SNR of high-frequency components increases, leading to temporal flickering. This observation motivates the approach used in FreeLong to balance these frequency components.", "section": "3 Observation and Analysis"}, {"figure_path": "X9Fga52OOv/figures/figures_3_2.jpg", "caption": "Figure 3: Temporal Attention Visualization. We visualize the temporal attention by average across all layers and time steps from LaVie [1] and VideoCrafter [2]. The attention maps for 16-frame videos exhibit a diagonal-like pattern, indicating a high correlation with adjacent frames, which helps preserve high-frequency details and motion patterns when generating new frames. In contrast, attention maps for longer videos are less structured, such as 128 frames, making the model struggle to identify and attend to the relevant information across distant frames. This lack of structure in the attention maps results in the distortion of high-frequency components of long videos, which results in the degradation of fine spatial-temporal details.", "description": "This figure visualizes the temporal attention mechanisms in LaVie and VideoCrafter models for generating videos of different lengths (16, 64, and 128 frames).  The visualization shows that the attention maps for short videos (16 frames) have a clear diagonal pattern, signifying strong correlations between adjacent frames and contributing to the preservation of fine details. In contrast, for longer videos (64 and 128 frames), the attention maps become less structured, indicating that the model has difficulty capturing long-range temporal dependencies. This lack of structure leads to the distortion of high-frequency components, ultimately degrading the quality of the generated videos.", "section": "3 Observation and Analysis"}, {"figure_path": "X9Fga52OOv/figures/figures_4_1.jpg", "caption": "Figure 4: Overview of FreeLong. FreeLong facilitates consistent and high-fidelity video generation using SpectralBlend Temporal Attention (SpectralBlend-TA). SpectralBlend-TA effectively blends low-frequency global video features with high-frequency local video features through a two-step process: local-global attention decoupling and spectral blending. Local video features are obtained by masking temporal attention to concentrate on fixed-length adjacent frames, while global temporal attention encompasses all frames. During spectral blending, 3D FFT projects features into the frequency domain, where high-frequency local components and low-frequency global components are merged. The resulting blended feature, transformed back to the time domain via IFFT, is then utilized in the subsequent block for refined video generation.", "description": "FreeLong uses SpectralBlend Temporal Attention to generate high-fidelity and consistent long videos.  It decouples local and global attention, applies a frequency filter to blend low-frequency global features and high-frequency local features, and uses the resulting blended feature in iterative denoising.", "section": "4 FreeLong: Training-free Long Video Generation"}, {"figure_path": "X9Fga52OOv/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative Comparison. Results from LaVie [1] and VideoCrafter [2] are presented. Direct videos exhibit consistent frames, but they appear over-smoothed. FreeNoise and the sliding-window approach struggle to capture global consistency effectively. Our FreeLong method achieves consistent long video generation while maintaining high fidelity, preserving crucial details and textures across the entire sequence.", "description": "This figure shows a comparison of long video generation results using different methods: Direct sampling, Sliding Window, FreeNoise, and FreeLong.  It demonstrates that FreeLong generates videos that maintain both consistency and high fidelity by effectively blending low-frequency global features and high-frequency local features, while the other methods fail to achieve either or both qualities. Two examples are shown: a yacht passing under a bridge and a woman sitting near a fire. For each example, 16 frames from each approach are displayed, visually showcasing the qualitative differences.", "section": "5.2 Quantitative Comparison"}, {"figure_path": "X9Fga52OOv/figures/figures_7_1.jpg", "caption": "Figure 6: Ablation Study. Global features and low-frequency components of global features ensure consistency but degrade fidelity. Local features and high-frequency local features maintain spatial-temporal details but lack temporal consistency. Directly adding global and local features degrades fidelity. Our method achieves both high fidelity and temporal consistency.", "description": "This ablation study visualizes the effects of using only global features, only local features, a direct combination of global and local features, low-frequency components of global features, high-frequency components of local features, and the combined approach of FreeLong on video generation.  The results show that global features alone maintain consistency but lose detail, while local features maintain detail but lose consistency. Combining them directly also results in poor quality. Only FreeLong's method, by selectively combining low-frequency global and high-frequency local features, manages to achieve both high fidelity and temporal consistency.", "section": "5.4 Ablation Studies"}, {"figure_path": "X9Fga52OOv/figures/figures_8_1.jpg", "caption": "Figure 7: Results of Multi-Prompt Video Generation. Our method ensures coherent visual continuity and motion consistency across different video segments.", "description": "This figure showcases the effectiveness of FreeLong in generating videos from multiple prompts.  Each row represents a different scene described by a sequence of prompts.  The generated video smoothly transitions between the scenes, demonstrating the model's ability to maintain visual coherence and motion consistency despite the changes in description.", "section": "5.5 Multi-Prompt Video Generation"}, {"figure_path": "X9Fga52OOv/figures/figures_9_1.jpg", "caption": "Figure 8: Longer Video Generation. FreeLong scales to generate videos longer than 128 frames (e.g., 512 frames), maintaining both temporal consistency and high fidelity throughout the entire sequence. This demonstrates that our method scales well with increasing video lengths, addressing the challenges associated with generating long continuous content without significant degradation in quality.", "description": "This figure shows the results of generating videos longer than 128 frames using the FreeLong method.  It demonstrates the method's ability to maintain both temporal consistency (smooth transitions) and high fidelity (visual quality) even with significantly increased video length.  Four examples are provided, each showing a sequence of frames from a short video (128 frames) to a much longer video (512 frames).  The consistent quality across these varying lengths highlights FreeLong's scalability for long video generation.", "section": "5.6 Longer Video Generation"}, {"figure_path": "X9Fga52OOv/figures/figures_14_1.jpg", "caption": "Figure 9: More Long Video Generation Results.", "description": "This figure shows more examples of long videos generated using FreeLong. Each row represents a different video, showing frames from 10, 40, 70, 100, and 120 frames.  The videos showcase FreeLong's ability to generate high-fidelity long videos across a variety of scenes and subjects.  The results demonstrate the temporal consistency and visual quality of FreeLong, preserving details and natural motion over extended durations.", "section": "A.4 More Qualitative Results"}, {"figure_path": "X9Fga52OOv/figures/figures_15_1.jpg", "caption": "Figure 10: Generalization to Other Base Models. FreeLong can be easily integrated into various video diffusion frameworks by replacing the temporal attention with SpectralBlend-TA, enabling these models to generate consistent long videos with high fidelity.", "description": "This figure demonstrates the adaptability of FreeLong to different base video diffusion models. By simply replacing the temporal attention mechanism with FreeLong's SpectralBlend-TA, various models (Modelscope, ZeroScope, Animatediff, OpenSora) successfully generate long, consistent videos with high fidelity, showcasing FreeLong's model-agnostic nature and effectiveness.", "section": "A.5 Generalization to Other Base Models"}]