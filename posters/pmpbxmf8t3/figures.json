[{"figure_path": "pMPBxMf8T3/figures/figures_1_1.jpg", "caption": "Figure 1: An illustration comparing training from aggregated data versus from heterogeneous data. The left example resembles the case where the model is trained on complete datasets, resulting in a stable spurious signal that the model tends to fit. The right example simulates a two-environment case where the spurious signal changes at each step. This oscillation creates a contraction effect, preventing the model from fitting the spurious signal.", "description": "This figure compares the training results of two different SGD approaches: PooledSGD (left) and HeteroSGD (right). PooledSGD trains the model on a complete dataset, leading to the model fitting both core and spurious signals. In contrast, HeteroSGD trains the model sequentially on data from different environments. The changing spurious signals across environments introduce oscillations that prevent the model from fitting spurious signals and allows it to converge towards the invariant solution.", "section": "Main Results"}, {"figure_path": "pMPBxMf8T3/figures/figures_8_1.jpg", "caption": "Figure 1: An illustration comparing training from aggregated data versus from heterogeneous data. The left example resembles the case where the model is trained on complete datasets, resulting in a stable spurious signal that the model tends to fit. The right example simulates a two-environment case where the spurious signal changes at each step. This oscillation creates a contraction effect, preventing the model from fitting the spurious signal.", "description": "This figure compares the training process of a model using aggregated data versus heterogeneous data. The left panel shows the training on complete datasets, leading to a stable spurious signal that the model fits.  The right panel shows a two-environment setting where the spurious signal fluctuates at each step, preventing the model from learning it. This highlights the implicit bias of heterogeneity towards invariance learning.", "section": "Main Results"}, {"figure_path": "pMPBxMf8T3/figures/figures_8_2.jpg", "caption": "Figure 3: The left figure shows that the heterogeneity facilitates us to eliminate the spurious signal and learn the invariance. The right figure shows that both true and spurious signals flow up when M is small, and when M \u2265 0.05, the spurious signal is eliminated.", "description": "This figure compares the performance of the proposed HeteroSGD algorithm under different levels of heterogeneity (M) in a multi-environment matrix sensing problem.  The left panel shows the trajectories of the true signal, spurious signal, and error terms over iterations. When the heterogeneity is high (M=6.0), the algorithm successfully separates the invariant true signal from the varying spurious signal, showing the effect of heterogeneity in preventing the model from fitting spurious signals. When heterogeneity is low (M=1.0), both true and spurious signals increase, illustrating the lack of sufficient heterogeneity to drive implicit invariance learning. The right panel summarizes the average final signal value attained as a function of heterogeneity. A phase transition is observed around M=5.0, where the spurious signal is effectively suppressed with higher heterogeneity.", "section": "Simulations"}, {"figure_path": "pMPBxMf8T3/figures/figures_9_1.jpg", "caption": "Figure 3: The left figure shows that the heterogeneity facilitates us to eliminate the spurious signal and learn the invariance. The right figure shows that both true and spurious signals flow up when M is small, and when M \u2265 0.05, the spurious signal is eliminated.", "description": "This figure compares the impact of heterogeneity (M) and learning rate (\u03b7) on signal recovery in a multi-environment matrix sensing problem. The left panel shows that with sufficient heterogeneity, the spurious signal is suppressed while the true signal is recovered.  The right panel shows that a higher learning rate is needed to eliminate the spurious signal when heterogeneity is low.", "section": "Simulations"}, {"figure_path": "pMPBxMf8T3/figures/figures_9_2.jpg", "caption": "Figure 1: An illustration comparing training from aggregated data versus from heterogeneous data. The left example resembles the case where the model is trained on complete datasets, resulting in a stable spurious signal that the model tends to fit. The right example simulates a two-environment case where the spurious signal changes at each step. This oscillation creates a contraction effect, preventing the model from fitting the spurious signal.", "description": "This figure compares the training results of two different methods: PooledSGD (left) and HeteroSGD (right). PooledSGD trains on data from all environments at once, leading to the model fitting spurious signals. In contrast, HeteroSGD trains sequentially on data from each environment, causing oscillations in the spurious signal that prevent overfitting and allowing the model to learn the invariant core signal.", "section": "Main Results"}, {"figure_path": "pMPBxMf8T3/figures/figures_29_1.jpg", "caption": "Figure 6: These figures shows that when the batch size is small, the trajectory will be far away from A* and A* + A, suggesting that the algorithm is not stable in this regime.", "description": "This figure shows the result of applying PooledSGD with different step sizes (\u03b7 = 0.01, 0.05, 0.1) and a batch size of 80. The left panel displays the Frobenius norm of the difference between the estimated matrix (U<sup>T</sup>U) and the true invariant matrix (A*).  The right panel shows the Frobenius norm of the difference between the estimated matrix (U<sup>T</sup>U) and the sum of the true invariant matrix (A*) and the average spurious matrix (\u0100). The plots demonstrate that when the batch size is small, the algorithm's trajectory deviates significantly from both the true invariant matrix and the sum of the true invariant matrix and the average spurious matrix, indicating instability.", "section": "B.2 The Failure of Pooled Stochastic Gradient Descent"}]