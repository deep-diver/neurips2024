[{"figure_path": "cknAewsBhD/figures/figures_1_1.jpg", "caption": "Figure 1: An overview of the proposed EGSST framework. The EGSST is an event-based, lightweight, and efficient framework designed for rapid object detection in event data. A graph is constructed from the data and divided into K connected subgraphs. These subgraphs are fed into a Graph Convolutional Network (GCN) [22] and a SSM. The GCN processes the subgraphs that are not removed to produce a global Graph Feature Map, which preserves both spatial and temporal information. The SSM assesses the dynamics of the entire graph and outputs a dynamic feature indicator, which includes the dynamics of each subgraph and the aggregated dynamics obtained through a Graph Attention Network (GAT) [23]. The TAC is activated based on the output from the SSM to enhance focus on the temporal dimension or to feed the graph feature maps directly into a Multi-scale Linear ViT [24]. Finally, a detection head, such as RT-DETR [25] or YOLOX [26], is employed to generate prediction outputs.", "description": "This figure provides a detailed overview of the EGSST framework, illustrating the flow of event data through various modules.  It begins with graph construction from the event stream, followed by processing via a GCN and SSM. The SSM assesses object dynamics and informs the TAC, which either focuses on temporal data or feeds data directly to a Linear ViT. Finally, a detection head produces the predictions.", "section": "3 Methodology"}, {"figure_path": "cknAewsBhD/figures/figures_4_1.jpg", "caption": "Figure 2: Dynamic Visualization of the SSM. Each image is generated from 10,000 event points, causing slight blurring. However, connected subgraphs effectively filter out background noise, preserving only relevant objects. (a) The scene shows low relative dynamic, hence the distinction is not pronounced. (b) The truck on the right accelerates to overtake, while the car on the left moves slower, making the truck's relative values significantly higher.", "description": "This figure shows a comparison of original event data and the output of the Spatiotemporal Sensitivity Module (SSM). The SSM processes 10,000 events to identify and highlight dynamic regions.  Subfigure (a) displays a scene with low relative dynamics, showing limited difference between the original and processed data. Subfigure (b) depicts a more dynamic scene with a faster-moving truck overtaking a slower car.  The SSM effectively highlights the faster-moving vehicle by assigning it higher relative dynamic values, showcasing the module's ability to filter noise and highlight relevant motion.", "section": "3.3 Spatiotemporal Sensitivity Module (SSM)"}, {"figure_path": "cknAewsBhD/figures/figures_6_1.jpg", "caption": "Figure 3: The flowchart of the Multi-scale Linear ViT. This diagram shows the stages of the Multi-scale Linear ViT, with the removable Enhanced CNN and Activated TAC modules. The Enhanced CNN processes input data through convolutional and normalization layers before passing it to the ViT stages. Activated TACs at Stage 2 and Stage 4 optimize temporal processing and balance efficiency. The data is then sent to the detection head for final object detection.", "description": "This figure shows the architecture of the Multi-scale Linear Vision Transformer (ViT) used in the EGSST framework.  It details the flow of data through an enhanced CNN (Convolutional Neural Network) for initial feature extraction, followed by four stages of the ViT, each incorporating linear attention mechanisms.  Activated Temporal Activation Controllers (TACs) are integrated at stages 2 and 4 to adaptively manage the temporal processing based on the dynamic features. Finally, a detection head processes the output to provide object detection predictions.", "section": "3 Methodology"}, {"figure_path": "cknAewsBhD/figures/figures_9_1.jpg", "caption": "Figure 4: Prediction Results. Due to the low accumulated event count, the visualizations appear somewhat blurred. Nevertheless, our model effectively identifies objects within these sparse events, demonstrating its robustness and efficacy.", "description": "This figure shows the results of object detection on two datasets (Gen1 and 1Mpx). The images display detected objects with bounding boxes labeled 'car' or 'pedestrian'. The low event count leads to some blurring, but the model still accurately identifies objects, showcasing its robustness and efficacy.", "section": "4 Experiments"}]