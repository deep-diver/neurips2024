[{"figure_path": "CAdBTYBlOv/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers on several datasets.  The results show that the time spent on the linear system solver is the most significant portion of the overall training time.  Using the pathwise gradient estimator is faster than using the standard estimator.  Additionally, initializing the solver with the solution from the previous step (warm starting) further reduces the runtime.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_2_1.jpg", "caption": "Figure 2: Marginal likelihood optimisation for iterative GPs.", "description": "This figure illustrates the three-level hierarchical structure of marginal likelihood optimisation for iterative Gaussian processes. The outer loop uses an optimizer (like L-BFGS or Adam) to maximize the marginal likelihood.  The gradient estimator (e.g., Hutchinson trace estimator) computes the gradient of the marginal likelihood, which requires solving systems of linear equations.  The inner loop utilizes a linear system solver (e.g., conjugate gradients) to obtain approximate solutions to these linear systems.", "section": "2.1 Hierarchical View of Marginal Likelihood Optimisation for Iterative Gaussian Processes"}, {"figure_path": "CAdBTYBlOv/figures/figures_3_1.jpg", "caption": "Figure 3: On the POL and ELEVATORS datasets, the pathwise estimator results in a lower RKHS distance (12) between solver initialisation and solution, as predicted by theory (14,15) (left). This results in fewer AP iterations until reaching the tolerance (left middle). When using the standard estimator, the initial distance follows the top eigenvalue of H\u00b9 (right middle), which is strongly related to the noise precision (right). The latter tends to increase during marginal likelihood optimisation when fitting the data. The effects are greater on POL due to the higher noise precision.", "description": "This figure compares the pathwise and standard estimators in terms of the initial distance to the solution in RKHS, the number of iterations to reach the tolerance, and the top eigenvalue of the inverse kernel matrix. The pathwise estimator demonstrates a smaller initial distance and fewer iterations, especially noticeable on the POL dataset with higher noise precision.", "section": "Pathwise Estimation of Marginal Likelihood Gradients"}, {"figure_path": "CAdBTYBlOv/figures/figures_4_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "The figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers across different datasets.  It shows that the linear system solver is the most time-consuming part of the process.  The pathwise gradient estimator significantly reduces the runtime compared to the standard estimator. Further runtime improvements are observed when using warm starting (initializing the solver with the solution from the previous step).", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_4_2.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers (Conjugate Gradients, Alternating Projections, Stochastic Gradient Descent) on three datasets (POL, ELEV, BIKE).  The hatched areas represent the proportion of total runtime spent on the linear solver itself, illustrating its dominant role.  The results show that the pathwise gradient estimator significantly reduces the time compared to the standard estimator.  Furthermore, warm-starting the solver (initializing it with the solution from the previous iteration) provides additional runtime improvements for both estimators.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_5_1.jpg", "caption": "Figure 6: Two-dimensional cross-sections along top eigendirections of the inner-loop quadratic objective after 20 marginal likelihood steps on the POL dataset. The current solution is placed at the origin of coordinates (left and middle). Warm starting significantly reduces the initial root-mean-square RKHS distance to the solution throughout marginal likelihood optimisation (right).", "description": "This figure visualizes the effect of warm starting on linear system solvers. It shows two-dimensional cross-sections of the quadratic objective function's landscape for both standard initialization (at zero) and warm starting (using the previous solution) after 20 optimization steps.  The left and middle panels illustrate the position of the initial point and solution for both methods. The right panel shows how warm starting reduces the initial distance to the solution over multiple optimization steps. The results demonstrate that warm starting leads to faster convergence of the solvers.", "section": "4 Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/figures/figures_5_2.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "The figure compares the relative runtimes of different hyperparameter optimization methods using three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) on three different datasets. The results show that the pathwise gradient estimator and warm starting the linear system solver significantly reduce the runtime compared to the standard methods.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_6_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "The figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers across multiple datasets.  It demonstrates that the time spent on the linear system solver is the primary contributor to the overall training time.  Using a pathwise gradient estimator significantly reduces the runtime compared to the standard estimator.  Furthermore, warm-starting the linear system solver (initializing it with the previous solution) provides additional speed-ups.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_7_1.jpg", "caption": "Figure 9: Relative residual norms of the probe vector linear systems at each marginal likelihood step on the POL dataset when solving until the tolerance or a maximum number of solver epochs is reached. Increasing the compute budget generally reduces the residual norm. Given the same compute budget, the pathwise estimator reaches lower residual norms than the standard estimator. Adding warm starts further reduces the residual norm for both estimators. However, the final test log-likelihood does not always match the residual norm. Surprisingly, good predictive performance can be obtained even if the residual norm is much higher than the tolerance \u03c4 = 0.01.", "description": "This figure shows the relative residual norms of the probe vectors for the linear systems at each marginal likelihood optimization step on the POL dataset. The results are shown for different methods (standard, pathwise, standard + warm start, pathwise + warm start) and maximum numbers of solver epochs (10, 20, 30, 40, 50). The figure demonstrates that increasing the compute budget (i.e., number of solver epochs) generally reduces the residual norm, and that the pathwise estimator and warm starting further improve the residual norm. Interestingly, good predictive performance is observed even when the residual norm is significantly higher than the tolerance (0.01).", "section": "Solving Linear Systems on a Limited Compute Budget"}, {"figure_path": "CAdBTYBlOv/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers across different datasets.  The results highlight that the linear system solver's runtime significantly impacts the overall training time. The pathwise gradient estimator consistently outperforms the standard estimator by reducing solver runtime, and warm starting further enhances the performance by leveraging previous solutions to initialize the solver.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_22_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers across several datasets.  The main takeaway is that the pathwise gradient estimator and warm starting significantly reduce the time spent on linear system solves, which is the dominant factor in the total training time.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_23_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers on three different datasets.  It shows that the time spent on the linear system solver is the dominant factor in the total training time. The pathwise gradient estimator significantly reduces the runtime compared to the standard estimator, and warm-starting the solver further improves performance.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_24_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers across several datasets.  The main observation is that the time spent on the linear solver is the dominant factor in the total training time. The figure shows that using the pathwise gradient estimator, along with warm starting the solver (initializing it with the solution from the previous step), significantly reduces the runtime compared to the standard method without warm starting.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_26_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers across several datasets.  The results show that the linear system solver is the dominant factor in the overall training time. The pathwise gradient estimator significantly reduces runtime compared to the standard estimator. Furthermore, warm-starting the solver with the previous solution provides additional runtime improvements for both estimators.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_26_2.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers across several datasets.  It shows that the time spent on the linear system solver is the dominant factor in overall runtime.  The pathwise gradient estimator offers significant speedups compared to the standard estimator.  Furthermore, warm-starting the solver (using the solution from the previous step) yields additional improvements in computational efficiency.", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_27_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "The figure compares the relative runtimes of different hyperparameter optimization methods using three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) across three different datasets.  The main observation is that the time spent on the linear system solver dominates the overall training time.  The pathwise gradient estimator significantly reduces runtime compared to the standard estimator. Further improvements are obtained by initializing the solver with the solution from the previous step (warm start).", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_27_2.jpg", "caption": "Figure 9: Relative residual norms of the probe vector linear systems at each marginal likelihood step on the POL dataset when solving until the tolerance or a maximum number of solver epochs is reached. Increasing the compute budget generally reduces the residual norm. Given the same compute budget, the pathwise estimator reaches lower residual norms than the standard estimator. Adding warm starts further reduces the residual norm for both estimators. However, the final test log-likelihood does not always match the residual norm. Surprisingly, good predictive performance can be obtained even if the residual norm is much higher than the tolerance \u03c4 = 0.01.", "description": "This figure shows how the residual norms of probe vectors in linear systems behave when solving until a tolerance or a maximum number of epochs is reached.  The results are shown for different solvers (CG, AP, SGD) and for two gradient estimators (standard, pathwise), with and without warm starting. It shows that increasing the computation budget reduces the residual norm, and that the pathwise estimator with warm starting achieves the lowest norms.  Surprisingly, good prediction performance can be achieved even when the norms are substantially larger than the target tolerance.", "section": "Solving Linear Systems on a Limited Compute Budget"}, {"figure_path": "CAdBTYBlOv/figures/figures_28_1.jpg", "caption": "Figure 9: Relative residual norms of the probe vector linear systems at each marginal likelihood step on the POL dataset when solving until the tolerance or a maximum number of solver epochs is reached. Increasing the compute budget generally reduces the residual norm. Given the same compute budget, the pathwise estimator reaches lower residual norms than the standard estimator. Adding warm starts further reduces the residual norm for both estimators. However, the final test log-likelihood does not always match the residual norm. Surprisingly, good predictive performance can be obtained even if the residual norm is much higher than the tolerance \u03c4 = 0.01.", "description": "This figure shows the relative residual norms of probe vectors in linear systems for different solvers (CG, AP, SGD) at each marginal likelihood optimization step on the POL dataset.  The experiment compares the standard and pathwise estimators, with and without warm starts, under varying computational budgets (maximum solver epochs).  The results demonstrate that increasing the compute budget generally lowers residual norms.  The pathwise estimator consistently achieves lower residual norms than the standard estimator for the same budget. Warm starting further improves performance. Interestingly, while lower residual norms are generally better, the figure also shows that good predictive performance can be maintained even with higher-than-tolerance residual norms.", "section": "Solving Linear Systems on a Limited Compute Budget"}, {"figure_path": "CAdBTYBlOv/figures/figures_28_2.jpg", "caption": "Figure 9: Relative residual norms of the probe vector linear systems at each marginal likelihood step on the POL dataset when solving until the tolerance or a maximum number of solver epochs is reached. Increasing the compute budget generally reduces the residual norm. Given the same compute budget, the pathwise estimator reaches lower residual norms than the standard estimator. Adding warm starts further reduces the residual norm for both estimators. However, the final test log-likelihood does not always match the residual norm. Surprisingly, good predictive performance can be obtained even if the residual norm is much higher than the tolerance \u03c4 = 0.01.", "description": "This figure compares the relative residual norms (the difference between the actual and estimated solutions) of linear systems for three different solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) using different configurations (standard, pathwise, warm start, and pathwise + warm start) on the POL dataset.  The x-axis represents the marginal likelihood steps, and the y-axis shows the relative residual norm.  The figure demonstrates that increasing the computational budget (allowing more solver epochs) reduces the residual norm across all configurations. The pathwise estimator consistently achieves lower residual norms compared to the standard estimator for the same computational budget.  Furthermore, incorporating warm starts further improves the residual norms, but surprisingly, this doesn't always directly translate to higher test log-likelihood.", "section": "Solving Linear Systems on a Limited Compute Budget"}, {"figure_path": "CAdBTYBlOv/figures/figures_29_1.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers on several datasets.  The hatched areas represent the proportion of total runtime spent on the linear system solver, highlighting its dominance in overall training time.  The results show that the pathwise gradient estimator is faster than the standard estimator, and that warm-starting (using the previous solution to initialize the solver) further improves speed for both estimators. ", "section": "1 Introduction"}, {"figure_path": "CAdBTYBlOv/figures/figures_29_2.jpg", "caption": "Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.", "description": "This figure compares the relative runtimes of different hyperparameter optimization methods using various linear system solvers across several datasets.  The results show that the linear system solver is the most time-consuming part of the process.  The pathwise gradient estimator significantly speeds up this process compared to the standard estimator, and further improvements are achieved by using warm start initialization. The improvements are substantial and evident across all three solvers (conjugate gradient, alternating projections, stochastic gradient descent).", "section": "1 Introduction"}]