[{"type": "text", "text": "Phased Consistency Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fu-Yun Wang1 Zhaoyang Huang2 Alexander William Bergman3,6 Dazhong Shen4 Peng Gao4 Michael Lingelbach3,6 Keqiang Sun1 Weikang Bian1 Guanglu Song5 Yu Liu4 Xiaogang Wang1 Hongsheng Li1,4,7 ", "page_idx": 0}, {"type": "text", "text": "1MMLab, CUHK 2Avolution AI 3Hedra 4Shanghai AI Lab 5Sensetime Research 6Stanford University 7CPII under InnoHK ", "page_idx": 0}, {"type": "text", "text": "{fywang@link, xgwang@ee, hsli@ee}.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Consistency Models (CMs) have made significant progress in accelerating the generation of diffusion models. However, their application to high-resolution, text-conditioned image generation in the latent space remains unsatisfactory. In this paper, we identify three key flaws in the current design of Latent Consistency Models (LCMs). We investigate the reasons behind these limitations and propose Phased Consistency Models (PCMs), which generalize the design space and address the identified limitations. Our evaluations demonstrate that PCMs outperform LCMs across 1\u201316 step generation settings. While PCMs are specifically designed for multi-step refinement, they achieve comparable 1-step generation results to previously state-of-the-art specifically designed 1-step methods. Furthermore, we show the methodology of PCMs is versatile and applicable to video generation, enabling us to train the state-of-the-art few-step text-to-video generator. Our code is available at https://github.com/G-U-N/Phased-Consistency-Model. ", "page_idx": 0}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/6607e8bec5efb88a210bdf0bb57872c379ab7db5adc1d2e94538ac9b237629f7.jpg", "img_caption": ["Figure 1: PCMs: Towards stable and fast image and video generation. "], "img_footnote": [], "page_idx": 0}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/4c5d87bfb66f97a653def693b289bb7c5d0cceb95f02158a81b513a41c23af4d.jpg", "img_caption": ["Figure 2: Summative motivation. We observe and summarize three crucial limitations for (latent) consistency models, and generalize the design space, well tackling all these limitations. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion models [20, 15, 57, 68] have emerged as the dominant methodology for image synthesis [45, 41, 7] and video synthesis [22, 53, 52, 63]. These models have shown the ability to generate highquality and diverse samples conditioned on varying signals. At their core, diffusion models rely on an iterative evaluation to generate new samples. This iterative evaluation trajectory models the probability flow ODE (PF-ODE) [56, 57] that transforms an initial normal distribution to a target real data distribution. However, the iterative nature of diffusion models makes the generation of new samples time-intensive and resource-consuming. To address this challenge, consistency models [56, 11, 63, 55] have emerged to reduce the number of iterative steps required to generate samples. These models work by training a model that enforces the self-consistency [56] property: any point along the same PF-ODE trajectory shall be mapped to the same solution point. These models have been extended to high-resolution text-to-image synthesis with latent consistency models (LCMs) [35]. Despite the improvements in efficiency and the ability to generate samples in a few steps, the sample quality of such models is still limited. ", "page_idx": 1}, {"type": "text", "text": "We show that the current design of LCMs is flawed, causing inevitable drawbacks in controllability, consistency, and efficiency during image sampling. Fig. 2 illustrates our observations of LCMs. The limitations are listed as follows: ", "page_idx": 1}, {"type": "text", "text": "(1) Consistency. Due to the specific consistency property, CMs can only use the purely stochastic multi-step sampling algorithm, which assumes that the accumulated noise variable in each generative step is independent and causes varying degrees of stochasticity for different inferencestep settings. As a result, we can find inconsistency among the samples generated with the same seeds in different inference steps. ", "page_idx": 1}, {"type": "text", "text": "(2) Controllability. Even though diffusion models can adopt classifier-free guidance (CFG) [16] in a wide range of values (i.e. 2\u201315), equipped with weights of LCMs, they can only accept values of CFG within range of 1\u20132. Larger values of CFG would cause the exposure problem. This brings difficulty for the hyper-parameter selection. Additionally, we find that LCMs are insensitive to the negative prompt. As shown in the figure, LCMs still generate black dogs even when the negative prompt is set to \u201cblack dog\". Both phenomenon reduce the controllability on generation. We show the reason behind this is the CFG-augmented ODE solver adopted in the consistency distillation stage. ", "page_idx": 1}, {"type": "text", "text": "(3) Efficiency. LCMs tend to generate much inferior samples at the few-step settings, especially in less than 4 inference steps, which limits the sampling efficiency. We argue that the reason lies in the traditional L2 loss or the Huber loss used in the LCMs procedure, which is insufficient for the fine-grained supervision in few-step settings. ", "page_idx": 1}, {"type": "text", "text": "To this end, we propose Phased Consistency Models (PCMs), which can tackle the discussed limitations of LCMs and are easy to train. Specifically, instead of mapping all points along the ODE trajectory to the same solution, PCMs phase the ODE trajectory into several sub-trajectories and only enforce the self-consistency property on each sub-trajectory. Therefore, PCMs can sample samples along the solution points of different sub-trajectories in a deterministic manner without error accumulation. As an example shown in Fig. 3, we train PCMs with two sub-trajectories, with three edge points including $\\mathbf{x}_{T},\\,\\mathbf{x}_{0}$ , and $\\mathbf{x}_{\\lfloor T/2\\rfloor}$ selected. Thereby, we can achieve 2-step deterministic sampling (i.e., $\\mathbf{x}_{T}\\rightarrow\\mathbf{x}_{\\lfloor T/2\\rfloor}\\rightarrow\\mathbf{x}_{0}$ ) for generation. Moreover, the phased nature of PCMs leads to an additional advantage. For distillation, we can choose to use a normal ODE solver without the CFG alternatively in the consistency distillation stage, which is not viable in LCMs. As a result, PCMs can optionally use larger values of CFG for inference and be more responsive to the negative prompt. Additionally, to improve the sample quality for few-step settings, we propose an adversarial loss in the latent space for more fine-grained supervision. ", "page_idx": 1}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/4e9f500153a936e9a00d8899c59cdea995bec4d114a96372bc15304c2c071ad4.jpg", "img_caption": ["Figure 3: (Left) Illustrative comparison of diffusion models [15], consistency models [56], consistency trajectory models [21], and our phased consistency model. (Right) Simplified visualization of the forward SDE and reverse-time PF-ODE trajectories. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To conclude, we dive into the design of (latent) consistency models, analyze the reasons for their unsatisfactory generation characteristics, and propose effective strategies for tackling these limitations. We validate the effectiveness of PCMs on widely recognized image generation benchmarks with stable diffusion v1-5 (0.9 B) [45] and stable diffusion XL (3B) [41] and video generation benchmarks following AnimateLCM [63]. Vast experimental results show the effectiveness of PCMs. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models define a forward conditional probability path, with a general representation of $\\alpha_{t}\\mathbf{x}_{0}+\\sigma_{t}\\epsilon$ for intermediate distribution $\\mathbb{P}_{t}(\\mathbf{x}|\\mathbf{x}_{0})$ conditioned on $\\mathbf{x}_{0}\\sim\\mathbb{P}_{0}$ , which is equivalent to the stochastic differential equation $\\mathrm{d}\\mathbf{x}_{t}=f_{t}\\mathbf{x}_{t}\\mathrm{d}t+g_{t}\\mathrm{d}\\pmb{w}_{t}$ with $\\pmb{w}_{t}$ denoting the standard Winer process, $\\begin{array}{r}{f_{t}=\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}_{t}}}\\end{array}$ and $\\begin{array}{r}{g_{t}^{2}=\\frac{\\bar{\\mathrm{d}}\\sigma_{t}^{2}}{\\mathrm{d}t}\\!-\\!2\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t}\\sigma_{t}^{2}}\\end{array}$ . There exists a deterministic flow for reversing the transition, which is represented by the probability flow ODE (PF-ODE) $\\mathrm{d}\\mathbf{x}=\\left[f_{t}-g_{t}^{2}/2\\nabla_{\\mathbf{x}}\\log\\mathbb{P}_{t}(\\mathbf{x})\\right]\\mathrm{d}t.$ . In standard diffusion training, a neural network is typically learned to estimate the score of marginal distribution $\\mathbb{P}_{t}(\\mathbf{x}_{t})$ , which is equivalent to $\\begin{array}{r}{s_{\\phi}(\\mathbf{x},\\dot{t})\\overset{\\cdot}{\\approx}\\nabla_{\\mathbf{x}}\\log\\mathbb{P}_{t}(\\mathbf{x})=\\mathbb{E}_{\\mathbf{x}_{0}\\sim\\mathbb{P}(\\mathbf{x}_{0}|\\mathbf{x})}\\left[\\nabla_{\\mathbf{x}}\\log\\mathbb{P}_{t}(\\mathbf{x}|\\mathbf{x}_{0})\\right]}\\end{array}$ . Substitue the $\\nabla_{\\mathbf{x}}\\log\\mathbb{P}_{t}(x)$ with $\\boldsymbol{s}_{\\phi}(\\boldsymbol{x},t)$ , and we get the empirical PF-ODE. Famous solvers including DDIM [54], DPM-solver [33], Euler and Heun [20] can be generally perceived as the approximation of the PF-ODE with specific orders and forms of diffusion for sampling in finite discrete inference steps. However, when the number of discrete steps is too small, they face inevitable discretization errors. ", "page_idx": 2}, {"type": "text", "text": "Consistency models [56], instead of estimating the score of marginal distributions, learn to directly predict the solution point of ODE trajectory by enforcing the self-consistency property: all points at the same ODE trajectory map to the same solution point. To be specific, given a ODE trajectory $\\{\\mathbf{x}_{t}\\}_{t\\in[\\epsilon,T]}$ , the consistency models $f_{\\theta}(\\cdot,t)$ learns to achieve $f_{\\theta}(\\mathbf{x}_{t},t)=\\mathbf{x}_{\\epsilon}$ by enforcing $f_{\\theta}(\\mathbf{x}_{t},t)=f_{\\theta}(\\bar{\\mathbf{x}_{t^{\\prime}}},\\bar{t^{\\prime}})$ for all $t,t^{\\prime}\\in[\\epsilon,T]$ . A boundary condition $f_{\\theta}(\\mathbf{x}_{\\epsilon},\\epsilon)=\\mathbf{x}_{\\epsilon}$ is set to guarantee the successful convergence of consistency training. During the sampling procedure, we first sample from the initial distribution $\\mathbf{x}_{T}\\,\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . Then, the final sample can be generated/refined by alternating denoising and noise injection steps, i.e, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{\\epsilon}^{\\tau_{k}}=f_{\\theta}(\\hat{\\tau_{k}},\\tau_{k}),\\;\\;\\hat{\\mathbf{x}}_{\\tau_{k-1}}=\\hat{\\mathbf{x}}_{\\epsilon}^{\\tau_{k}}+\\pmb{\\eta},\\;\\;\\pmb{\\eta}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\{\\tau_{k}\\}_{k=1}^{K}$ are selected time points in the ODE trajectory and $\\tau_{K}=T$ . Actually, because only one solution point is used to conduct the consistency model, it is inevitable to introduce stochastic error, i.e., $\\eta$ , for multiple sampling procedures. ", "page_idx": 2}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/2d144338578b8961ca03c11ef1d65e7f8c9f8a1d27dfb14bafda98698c636f21.jpg", "img_caption": ["Figure 4: Training paradigm of PCMs. \u2018?\u2019 means optional usage. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Recently, Consistency trajectory models (CTMs) [21] propose a more flexible framework. specifically, CTMs $f_{\\theta}(\\cdot,t,s)$ learns to achieve $f_{\\theta}(\\mathbf{x}_{t},t,s)=f(x_{t},t,s;\\phi)$ by enforcing $\\bar{f_{\\theta}}(\\mathbf{x}_{t},t,\\bar{s})=$ $f_{\\theta}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},s)$ with $s\\,\\leq\\,\\operatorname*{min}(t,t^{\\prime})$ and $t,t^{\\prime}\\,\\in\\,[\\epsilon,T]$ . However, the learning objectives of CTMs are redundant, including many trajectories that will never be applied for inference. More specifically, if we split the continuous time trajectory into $N$ discrete points, diffusion models learn $\\bar{\\cal O}(N)$ objectives (i.e., each point learns to move to its adjacent point), consistency models learn $\\mathcal{O}(N)$ objectives (i.e., each point learns to move to solution point), and consistency trajectory models learn $\\mathcal{O}(N^{2})$ objectives (i.e., each point learns to move to all the other points in the trajectory). Hence, except for the current timestep embedding, CTMs should additionally learn a target timestep embedding, which is not comprised of the design space of diffusion models. ", "page_idx": 3}, {"type": "text", "text": "Different from the above approaches, PCMs can be optimized efficiently and support deterministic sampling without additional stochastic error. Overall, Fig. 3 illustrates the difference in training and inference processes among diffusion models, consistency models, consistency trajectory models, and phased consistency models. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce the technical details of PCMs, which overcome the limitations of LCMs in terms of consistency, controllability, and efficiency. Consistency: Specifically, we first introduce the main framework of PCMs, consisting of definition, parameterization, the distillation objective, and the sampling procedure in Sec. 3.1. In particular, by enforcing the self-consistency property in multiple ODE sub-trajectories respectively, PCMs can support deterministic sampling to preserve image consistency with varying inference steps. Controllability: Secondly, in Sec. 3.2, to improve the controllability of text guidance, we revisit the potential drawback of the guided distillation adopted in LCMs, and propose to optionally remove the CFG for consistency distillation. Efficiency: Thirdly, in Sec. 3.3, to further improve inference efficiency, we introduce an adversarial consistency loss to enforce the modeling of data distribution, which facilitates 1-step generation. ", "page_idx": 3}, {"type": "text", "text": "3.1 Main Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Definition. For a solution trajectory of a diffusion model $\\{\\mathbf{x}_{t}\\}_{t\\in[\\epsilon,T]}$ following the PF-ODE, we split the trajectory into multiple sub-trajectories with hyper-defined edge timesteps $s_{0},s_{1},\\ldots,s_{M}$ , where $s_{0}=\\epsilon$ and $s_{M}\\,=\\,T$ . The $M$ sub-trajectories can be represented as $\\left\\{\\mathbf{x}_{t}\\right\\}_{t\\in\\left[s_{m},s_{m+1}\\right]}$ with $m=0,1,\\ldots,M-1$ . We treat each sub-trajectory as an independent CM and define the consistency function as $f^{m}:(\\mathbf{x}_{t},t)\\rightarrow\\mathbf{x}_{s_{m}}$ , $t\\in\\left[s_{m},s_{m+1}\\right]$ . We learn $\\pmb{f}_{\\theta}^{m}$ to estimate $\\pmb{f}$ by enforcing the self-consistency property on each sub-trajectory that its outputs are consistent for arbitrary pairs on the same sub-trajectory. Namely, $\\pmb{f}^{m}(\\mathbf{x}_{t},t)\\,=\\,\\pmb{f}^{m}(\\mathbf{x}_{t^{\\prime}},t^{\\prime})$ for all $t,t^{\\prime}\\,\\in\\,\\left[s_{m},s_{m+1}\\right]$ . Note that the consistency function is only defined on the sub-trajectory. However, for sampling, it is necessary to define a transition from timestep $T$ (i.e., $s_{M}$ ) to $\\epsilon$ (i.e., $s_{0}$ ). Thereby, we defined $f^{m,m^{\\prime}}=f^{m^{\\prime}}\\left(\\cdot\\cdot\\cdot f^{m-2}\\left(f^{m-1}\\left(f^{m}(\\mathbf{x}_{t},t),s_{m}\\right),s_{m-1}\\right)\\cdot\\cdot\\cdot\\,,s_{m^{\\prime}}\\right)$ that transforms any point $\\mathbf{x}_{t}$ on $m$ -th sub-trajectory to the solution point of $m^{\\prime}$ -th trajectory. ", "page_idx": 3}, {"type": "text", "text": "Parameterization. Following the definition, the corresponding consistency function of each subtrajectory should satisfy boundary condition $f^{m}(\\mathbf{x}_{s_{m}},s_{m})=\\mathbf{x}_{s_{m}}$ , which is crucial for guaranteeing the successful training of consistency models. To satisfy the boundary condition, we typically need to explicitly parameterize $f_{\\theta}^{m}(\\mathbf{x},t)$ as $f_{\\theta}^{m}(\\mathbf{x}_{t},t)=c_{\\mathrm{skip}}^{m}(t)\\mathbf{x}_{r}+c_{\\mathrm{out}}^{m}(t)F_{\\theta}(\\mathbf{x}_{t},t,s_{m})$ , where $\\dot{c}_{\\mathrm{skip}}^{m}(t)$ gradually increases to 1 and $c_{\\mathrm{out}}^{m}(t)$ gradually decays to 0 from timestep $s_{m+1}$ to $s_{m}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Another important thing is how to parameterize $F_{\\theta}(\\mathbf{x}_{t},t,s)$ , basically it should be able to indicate the target prediction at timestep $s$ given the input $\\mathbf{x}$ at timestep $t$ . Since we build upon the epsilon prediction models, we hope to maintain the epsilon-prediction learning target. ", "page_idx": 4}, {"type": "text", "text": "For the above-discussed PF-ODE, there exists an exact solution [33] from timestep $t$ to $s$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{x}_{s}=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}+\\alpha_{s}\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\sigma_{t_{\\lambda}(\\lambda)}\\nabla\\log\\mathbb{P}_{t_{\\lambda}(\\lambda)}(\\mathbf{x}_{t_{\\lambda}(\\lambda)})\\mathrm{d}\\lambda\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\lambda_{t}=\\ln{\\frac{\\alpha_{t}}{\\sigma_{t}}}}\\end{array}$ and $t_{\\lambda}$ is a inverse function with $\\lambda_{t}$ . The equation shows that the solution of the ODE from $t$ to $s$ is the scaling of $\\mathbf{x}_{t}$ and the weighted sum of scores. Given a epsilon prediction diffusion network $\\epsilon_{\\phi}(\\mathbf{x},t)$ , we can estimate the solution as $\\begin{array}{r}{\\mathbf{x}_{s}=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}\\!-\\!\\alpha_{s}\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\epsilon_{\\phi}(\\mathbf{x}_{t_{\\lambda}(\\lambda)},t_{\\lambda}(\\lambda))\\mathrm{d}\\lambda}\\end{array}$ . However, note that the solution requires knowing the epsilon prediction at each timestep between $t$ and $s$ , but consistency modes need to predict the solution with only $\\mathbf{x}_{t}$ available with single network evaluation. Thereby, we parameterize the $F_{\\theta}(\\mathbf{x},t,s)$ as following, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{x}_{s}=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}-\\alpha_{s}\\hat{\\epsilon}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t},t)\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\mathrm{d}\\lambda\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "One can show that the parameterization has the same format with DDIM $\\begin{array}{r}{\\mathbf{x}_{s}=\\alpha_{s}(\\frac{\\mathbf{x}_{t}-\\sigma_{t}\\pm\\pmb{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)}{\\alpha_{t}})+}\\end{array}$ $\\sigma_{s}\\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)$ (see Theorem 3). But here we clarify that the parameterization has an intrinsic difference from DDIM. DDIM is the first-order approximation of solution ODE, which works because we assume the linearity of the score in small intervals. This causes the DDIM to degrade dramatically in few-step settings since the linearity is no longer satisfied. Instead, our parameterization is not approximation but exact solution learning. The learning target of $\\hat{\\epsilon}_{\\theta}(\\mathbf{x},t)$ is no more the scaled score $\\bar{-\\sigma}_{t}\\nabla_{\\mathbf{x}}\\log\\mathbb{P}_{t}(\\mathbf{x})$ (which epsilon-prediction diffusion models learn to estimate) but $\\frac{\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\epsilon_{\\phi}(\\mathbf{x}_{t_{\\lambda}(\\lambda)},t_{\\lambda}(\\lambda))\\mathrm{d}\\lambda}{\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\mathrm{d}\\lambda}$ . Actually, we can define the parameterization in other formats, but we find this format is simple and has a small gap between the original diffusion models. The parameterization of $F_{\\theta}$ also allows for a better property that we can drop the introduced $c_{\\mathrm{skip}}^{m}$ and $c_{\\mathrm{out}}^{m}$ in consistency models to ease the complexity of the framework. Note that following Eq. 3, we can get $F_{\\theta}(\\mathbf{x}_{s_{m}},s_{m},s_{m})\\,=$ $\\begin{array}{r}{\\frac{\\alpha_{s_{m}}}{\\alpha_{s_{m}}}\\mathbf{x}_{s_{m}}\\,-0=\\mathbf{x}_{s_{m}}.}\\end{array}$ . Therefore, the boundary condition is already satisfied. Hence, we can simply define $f_{\\theta}^{m}(\\mathbf{x},t)=F_{\\theta}(\\mathbf{x},t,s_{m})$ . This parameterization also aligns with several previous diffusion distillation techniques [46, 3, 74] utilizing DDIM format, building a deep connection with previous distillation methods. The difference is that we provide a more fundamental explanation of the meaning and learning objective of parameterizations. ", "page_idx": 4}, {"type": "text", "text": "Phased consistency distillation objective. Denote the pre-trained diffusion models as $s_{\\phi}(\\mathbf{x},t)=$ $-\\,\\frac{\\epsilon_{\\phi}(\\mathbf{x},t)}{\\sigma_{t}}$ , which induces an empirical PF-ODE. We firstly discretize the whole trajectory into $N$ sub-intervals with $N+1$ discrete timesteps from $[\\epsilon,T]$ , which we denote as $t_{0}\\;=\\;\\epsilon\\;<\\;t_{1}\\;<$ $t_{2}\\,<\\,\\cdot\\cdot\\,<\\,t_{N}\\,=\\,T$ . Typically $N$ should be sufficiently large to make sure the ODE solver approximates the ODE trajectory correctly. Then we sample $M+1$ timesteps as edge timesteps $\\dot{s_{0}}^{-}=t_{0}<s_{1}<s_{2}<\\cdots<s_{M}=t_{N}\\in\\{\\dot{t}_{i}\\}_{i=0}^{N}$ to split the ODE trajectory into $M$ sub-trajectories. Each sub-trajectory $[s_{i},s_{i+1}]$ consists of the set of sub-intervals $\\{[t_{j},t_{j+1}]\\}_{t_{j}\\geq s_{i},t_{j+1}\\leq s_{i+1}}$ . ", "page_idx": 4}, {"type": "text", "text": "Here we define $\\Phi(\\mathbf{x}_{t_{n+k}},t_{n+k},t_{n};\\phi)$ as the $k$ -step ODE solver that approximate $\\mathbf{x}_{t_{n}}^{\\phi}$ from ${\\bf x}_{t_{n+k}}$ on the same sub-trajectory following Equation 2, namely, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}=\\Phi(\\mathbf{x}_{t_{n+k}},t_{n+k},t_{n};\\phi).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Following CMs [56], we set $k=1$ to minimize the ODE solver cost. The training loss is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{PCM}}(\\pmb{\\theta},\\pmb{\\theta}^{-};\\phi)=\\mathbb{E}_{\\mathbb{P}(m),\\mathbb{P}(n|m),\\mathbb{P}(\\mathbf{x}_{t_{n+1}}|n,m)}\\left[\\lambda(t_{n})d\\left(\\pmb{f}_{\\pmb{\\theta}}^{m}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\pmb{f}_{\\pmb{\\theta}^{-}}^{m}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{P}(m)\\,:=\\,\\mathrm{uniform}(\\{0,1,\\dots,M-1\\})$ , $\\mathbb{P}(n|m)\\,:=\\,\\mathrm{uniform}(\\{n+1|t_{n+1}\\,\\le\\,s_{m+1},t_{n}\\,\\ge$ $s_{m}\\}$ ), $\\mathbb{P}(\\mathbf{x}_{t_{n+1}}|n,m)=\\mathbb{P}_{t_{n+1}}(\\mathbf{x})$ , and $\\pmb{\\theta}^{-}=\\mu\\pmb{\\theta}^{-}+(1-\\mu)\\pmb{\\theta}$ . ", "page_idx": 4}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/c3e9d4c9f4bd91a00db8986a356a332a230705b8456b364873f3b9d55a61c011.jpg", "img_caption": ["Figure 5: Qualitative Comparison. Our method achieves top-tier performance. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We show that when $\\mathcal{L}_{\\mathrm{PCM}}(\\theta,\\theta^{-};\\phi)=0$ and local errors of ODE solvers uniformly bounded by ${\\mathcal{O}}((\\Delta t)^{p+1})$ , the solution estimation error within the arbitrary sub-trajectory $f_{\\theta}^{m}(\\mathbf{x}_{t_{n}},t_{n})$ is bounded by $O((\\Delta t)^{p})$ in Theorem 1. Additionally, the solution estimation error across any sets of subtrajectories (i.e., the error between $f_{\\theta}^{m,m^{\\prime}}(\\mathbf{x}_{t_{n}},t_{n})$ and $\\mathbf{\\boldsymbol{f}}^{m,m^{\\prime}}(\\mathbf{\\boldsymbol{x}}_{t_{n+1},t_{n+1}};\\boldsymbol{\\phi}))$ is also bounded by $O((\\Delta t)^{p})$ in Theorem 2. ", "page_idx": 5}, {"type": "text", "text": "Sampling. For a given initial sample at timestep $t$ which belongs to the sub-trajectory $\\left[s_{m},s_{m+1}\\right]$ , we can support deterministic sampling following the definition of $\\pmb{f}^{m,0}$ . Previous work [21, 67] reveals that introducing a certain degree of stochasticity might lead to better generation quality. We show that our sampling method can also introduce randomness through a simple modification, which we discuss at Sec. IV.1 ", "page_idx": 5}, {"type": "text", "text": "3.2 Guided Distillation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For convenience, we take the epsilon-prediction format with text conditions $^c$ for the following discussion. The consistency model is denoted as $\\epsilon_{\\theta}(\\mathbf{x}_{t},t,c)$ , and the diffusion model is denoted as $\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c)$ . A commonly applied strategy for text-conditioned diffusion models is classifier-free guidance (CFG) [16, 51, 1]. At training, $^c$ is randomly substituted with null text embedding $\\mathcal{Q}$ . At each inference step, the model computes $\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c)$ and $\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c_{\\mathrm{neg}})$ simultaneously, and the actual prediction is the linear combination of them. Namely, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c,c_{\\mathrm{neg}};w)=\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c_{\\mathrm{neg}})+w(\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c)-\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c_{\\mathrm{neg}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $w$ controlling the strength and $c_{\\mathrm{neg}}$ can be set to $\\mathcal{Q}$ or text embeddings of unwanted characteristics. A noticeable phenomenon is that diffusion models with this strategy can not generate content with good quality without using CFG. That is, the empirical ODE trajectory induced with pure $\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c)$ deviates away from the ODE trajectory to real data distribution. Thereby, it is necessary to apply CFG-augmented prediction $\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c,c_{\\mathrm{neg}};w)$ for ODE solvers. Recall that we have shown that the consistency learning target of the consistency model $\\epsilon_{\\theta}(\\mathbf{x}_{t},t,c)$ is the weighted sum of epsilon prediction on the trajectory. Thereby, we have $\\epsilon_{\\theta}(\\mathbf{x}_{t},t,c)\\propto\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c,\\mathcal{O};w)$ , for all $t^{\\prime}\\leq t$ . On this basis, if we additionally apply the CFG for the consistency models, we can prove that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\epsilon_{\\theta}(\\mathbf{x}_{t},t,c,c_{\\mathrm{neg}};w^{\\prime})\\propto w w^{\\prime}(\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c)-\\epsilon_{\\phi}^{\\mathrm{nerge}}))+\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c_{\\mathrm{neg}})\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where \u03f5\u03d5 $\\epsilon_{\\phi}^{\\mathrm{merge}}=(1-\\alpha)\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c_{\\mathrm{neg}})+\\alpha\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset)$ and $\\begin{array}{r}{\\alpha=\\frac{\\left(w-1\\right)}{w w^{\\prime}}}\\end{array}$ (See Theorem 4). This equation indicates that applying CFG $w^{\\prime}$ to the consistency models trained with CFG-augmented ODE solver confined with $w$ , is equivalent to scaling the prediction of original diffusion models by $w^{\\prime}w$ , which explains the the exposure problem. We can also observe that the epsilon prediction with negative prompts is diluted by the prediction with null text embedding, which reveals that the impact of negative prompts is reduced. ", "page_idx": 5}, {"type": "table", "img_path": "mtBmKqyqGS/tmp/9eed46fe788f550e6ba5aa50cd3a6e1a36999115a5fd39c232601d55a105b7ad.jpg", "table_caption": ["Table 1: Comparison of FID-SD and FID-CLIP with Stable Diffusion v1-5 based methods under different steps. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We ask the question: Is it possible to conduct consistency distillation with diffusion models trained for CFG usage without applying CFG-augmented ODE solver? Our finding is that it is not applicable for original CMs but works well for PCMs especially when the number of sub-trajectory $M$ is large. We empirically find that $M=4$ is sufficient for successful training. As we discussed, the text-conditioned diffusion models trained for CFG usage fail at achieving good generation quality when removing CFG for inference. That is, target data distribution induced by PF-ODE with $\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c)$ has a large distribution distance to real data distribution. Therefore, ftiting the spoiled ODE trajectory is only to make the generation quality bad. In contrast, when phasing the whole ODE trajectory into several sub-trajectories, the negative influence is greatly alleviated. On one hand, the starting point xsm+1 of sub-trajectories is replaced by adding noise to the real data. On the other hand, the distribution distance between the distribution of solution points $\\mathbf{x}_{s_{m}}$ of sub-trajectories and real data distribution $\\mathbb{P}_{s_{m}}^{\\mathrm{data}}$ at the same timestep is much smaller proportional to the noise level introduced. To put it straightforwardly, even though the distribution gap between the real data and the samples generated from $\\epsilon_{\\phi}(\\mathbf{x}_{t},t,c)$ is large, adding noise to them reduce the gap. Thereby, we can optionally train a consistency model whether supporting larger values of CFG or not. ", "page_idx": 6}, {"type": "text", "text": "3.3 Adversarial Consistency Loss ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We introduce an adversarial loss to enforce the distribution consistency, which greatly improves the generation quality in few-step settings. For convenience, we introduce an additional symbol $\\mathcal{T}_{t\\rightarrow s}$ which represents a flow from $\\mathbb{P}_{t}$ to $\\mathbb{P}_{s}$ . Let $T_{t\\rightarrow s}^{\\phi}$ , $\\mathcal{T}_{t\\rightarrow s}^{\\theta}$ be the transition mapping following the ODE trajectory of pre-trained diffusion and our consistency models. Additionally, let $\\tau_{s\\rightarrow t}^{-}$ be the distribution transition following the forward process SDE (adding noise). The loss function is defined as the following ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{PCM}}^{a d v}(\\pmb{\\theta},\\pmb{\\theta}^{-};\\pmb{\\phi},m)=D\\left(\\mathcal{T}_{s_{m}\\rightarrow s}^{-}\\mathcal{T}_{t_{n+k}\\rightarrow s_{m}}^{\\pmb{\\theta}}\\mathcal{H}^{\\mathbb{P}}t_{n+k}\\left\\|\\mathcal{T}_{s_{m}\\rightarrow s}^{-}\\mathcal{T}_{t_{n}\\rightarrow s_{m}}^{\\pmb{\\theta}^{-}}\\mathcal{T}_{t_{n+1}\\rightarrow t_{n}}^{\\phi}\\mathcal{H}^{\\mathbb{P}}t_{n+1}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\#$ is the pushforward operator, and $D$ is the distribution distance metric. To penalize the distribution distance, we apply the GAN-style training paradigm. To be specific, as shown in Fig. 4, for the sampled ${\\bf x}_{t_{n+k}}$ and the $\\mathbf{x}_{t_{n}}^{\\phi}$ solved through the pre-trained diffusion model $\\phi$ , we first compute their predicted solution point $\\tilde{\\mathbf{x}}_{s_{m}}=f_{\\theta}^{m}(\\mathbf{x}_{t_{n+k}},t_{n+k})$ and $\\hat{\\mathbf{x}}_{s_{m}}=f_{\\theta^{-}}(\\mathbf{x}_{t_{n}}^{\\phi},t_{n})$ . Then we randomly add noise to $\\tilde{\\mathbf{x}}_{s_{m}}$ and $\\hat{\\mathbf{x}}_{s_{m}}$ to obtain $\\tilde{\\mathbf{x}}_{s}$ and $\\hat{\\mathbf{x}}_{s}$ with randomly sampled $s\\in\\Tilde{[}s_{m},s_{m+1}\\Tilde{]}$ . We optimize the adversarial loss between $\\tilde{\\mathbf{x}}_{s}$ and $\\hat{\\mathbf{x}}_{s}$ . Specifically, the $\\mathcal{L}_{\\mathrm{PCM}}^{a d v}$ can be re-written as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{PCM}}^{a d v}(\\pmb{\\theta},\\pmb{\\theta}^{-};\\phi,m)=\\mathrm{ReLU}(1+\\mathcal{D}(\\widetilde{\\bf x}_{s},s,c))+\\mathrm{ReLU}(1-\\mathcal{D}(\\hat{\\bf x}_{s},s,c)),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathrm{{ReLU}}(x)\\,=\\,x$ if $x\\,>\\,0$ else $\\mathrm{ReLU}(x)\\,=\\,0.$ , $\\mathcal{D}$ is the discriminator, $^c$ is the image conditions (e.g., text prompts), and the loss is updated in a min-max manner [12]. Therefore the eventual optimization objective is $\\mathcal{L}_{\\mathrm{PCM}}+\\lambda\\mathcal{L}_{\\mathrm{PCM}}^{a d v}$ with $\\lambda$ as a hyper-parameter controlling the trade-off of distribution consistency and instance consistency. We adopt for all training settings. However, we hope to clarify that the adversarial loss has an intrinsic difference from the GAN. The GAN training aims to align the training data distribution and the generation distribution of the model. That iisn,t $D({\\bar{T}}_{t_{n+k}\\to s_{m}}^{\\theta}{\\#}{\\bar{\\mathbb{P}}}_{t_{n+k}}\\|\\mathbb{P}_{s_{m}})$ lI anl sToh ceoorveerma g5e,  two ez serhoo. wY teht aitn  cTohnesiosrteenmc 6y,  pwroe psehrtoyw i tsh eatn fcoorcmebdi,n ionugr standard GAN with consistency distillation is a flawed design when considering the pre-trained data distribution and distillation data distribution mismatch. Its loss will be non-zero when the self-consistency property is achieved, thus corrupting the consistency distillation learning. Our experimental results also verify our statement (Fig. 7). ", "page_idx": 6}, {"type": "table", "img_path": "mtBmKqyqGS/tmp/f77e322f5aff72b345d8abff49baacd7c93643106c04d91f3be9b4fdda30a923.jpg", "table_caption": ["Table 2: One-step generation comparison on Stable Diffusion v1-5 "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset. Training dataset: For image generation, we train all models on the CC3M [5] dataset. For video generation, we train the model on WebVid-2M [2]. Evaluation dataset: For image generation, we evaluate the performance on the COCO-2014 [28] following the 30K split of karpathy. We also evaluate the performance on the CC12M with our randomly chosen 30K split. For video generation, we evaluate with the captions of UCF-101 [58]. ", "page_idx": 7}, {"type": "text", "text": "Backbones. We verify the text-to-image generation based on Stable Diffusion v1-5 [45] and Stable Diffusion XL [41]. We verify the text-to-video generation following the design of AnimateLCM [63] with decoupled consistency distillation. ", "page_idx": 7}, {"type": "text", "text": "Evaluation metrics. Image: We report the FID [14] and CLIP score [43] of the generated images and the validation 30K-sample splits. Following [8, 47], we also compute the FID with CLIP features (FID-CLIP). Note that, all baselines and our method focus on distilling the knowledge from the pre-trained diffusion models for acceleration. Therefore, we also compute the FID of all baselines and the generated images of original pre-trained diffusion models including Stable Diffusion v1-5 and Stable Diffusion XL (FID-SD). Video: For video generation, we evaluate the performance from three perspectives: the CLIP Score to measure the text-video alignment, the CLIP Consistency to measure the inter-frame consistency of the generated videos, the Flow Magnitude to measure the motion magnitude of the generated videos with Raft-Large [59]. ", "page_idx": 7}, {"type": "text", "text": "4.2 Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Comparison methods. We compare PCM with Stable Diffusion v1-5 to methods including Stable Diffusion v1-5 [45], InstaFlow [31], LCM [35], CTM [21], TCD [74] and SD-Turbo [49]. We compare PCM with Stable Diffusion XL to methods including Stable Diffusion XL [41], CTM [21], SDXL-Lightning [27], SDXL-Turbo [49], and LCM [35]. We apply the \u2018Ours\u2019 and \u2018Ours\\*\u2019 to denote our methods trained with CFG-augmented ODE solver or not. We only report the performance of \u2018Ours\\*\u2019 with more than 4 steps which aligns with our claim that it is only possible when phasing the ODE trajectory into multiple sub-trajectories. For video generation, we compare with DDIM [54], DPM [33], and AnimateLCM [63]. ", "page_idx": 7}, {"type": "text", "text": "Qualitative comparison. We evaluate our model and comparison methods with a diverse set of prompts in different inference steps. The results are listed in Fig. 5. Our method shows clearly the top performance in both image visual quality and text-image alignment across 1\u201316 steps. ", "page_idx": 7}, {"type": "text", "text": "Quantitative comparison. One-step generation: We show the one-step generation results comparison of methods based on Stable Diffusion v1-5 and Stable Diffusion XL in Table 2 and Table 5, respectively. Notably, PCM consistently surpasses the consistency model-based methods including LCM and CTM by a large margin. Additionally, it achieves comparable or even superior to the state-of-the-art GAN-based (SD-Turbo, SDXL-Turbo, SDXL-Lightning) or Rectified-Flow-based (InstaFlow) one-step generation methods. Note that InstaFlow applies the LIPIPS [72] loss for training and SDXL-Turbo can only generate $512\\times512$ resolution images, therefore it is easy for them to obtain higher scores. Multi-step generation: We report the FID changes of different methods on COCO-30K and CC12M-30K in Table 1 and Table 3. \u2018Ours\u2019 and \u2018Ours\\*\u2019 achieve the best or second-best performance in most cases. It is notably the gap of performance between our methods and other baselines becoming large as the timestep increases, which indicates the phased nature of our methods supports more powerful multi-step sampling ability. Video generation: We show the quantitative comparison of video generation in Table 4, our model achieves consistent superior performance. The 1-step and 2-step generation results of DDIM and DPM are very noisy, therefore it is meaningless to evaluate their Flow Magnitude and CLIP Consistency. ", "page_idx": 7}, {"type": "table", "img_path": "mtBmKqyqGS/tmp/bd388d7289a7c93c2d93b6a6037b30cb44a29914e37fd921e6432e48f877bc37.jpg", "table_caption": ["Table 3: Comparison of FID-SD on CC12M-30K with Stable Diffusion XL. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "mtBmKqyqGS/tmp/1b158d29a65f136970973b5e81d5b0a89327f0ee295dcfbbd192c675fd709393.jpg", "table_caption": ["Table 4: Quantitative comparison for video generation. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "mtBmKqyqGS/tmp/faf7d158f8a1d76e018511ea52673f67e52d7a76674416c8e9c565f65f6cb8ad.jpg", "table_caption": ["Table 5: One-step and two-step generation comparison on Stable Diffusion XL "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Human evaluation metrics. To more comprehensively reflect the performance of phased consistency models, we conduct a thorough evaluation using human aesthetic preference metrics, encompassing 1\u201316 steps. This assessment employs well-regarded metrics, including HPSv2 (HPS) [66], PickScore (PICKSCORE) [23], and Laion Aesthetic Score (AES) [50], to benchmark our method against all comparative baselines. As shown in Table 6 and Table 7, across all evaluated settings, our method consistently achieves either superior or comparable results, with a marked performance advantage over the consistency model baseline LCM, demonstrating its robustness and appeal across diverse human-centric evaluation criteria. We conduct a human preference ablation study on the proposed adversarial consistency loss, with the results presented in Table 8. The inclusion of adversarial consistency loss consistently enhances human evaluation metrics across different inference steps. ", "page_idx": 8}, {"type": "table", "img_path": "mtBmKqyqGS/tmp/1374ff15a55724739a9c0a0c78fe06d30108cb638d743a7aa80dbdf9379f91b2.jpg", "table_caption": ["Table 6: Aesthetic evaluation on SD v1-5. ", "Table 7: Aesthetic evaluation on SDXL "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "mtBmKqyqGS/tmp/02da00853328a9fa98bb1df004b5981a4d904f4de948257e8bc3247d81eded6b.jpg", "table_caption": ["Table 8: Aesthetic ablation study on the adversarial consistency loss. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Sensitivity to negative prompt. To show the comparison of sensitivity to negative prompt between our model tuned without CFG-augmented ODE solver and LCM. We provide an example of a prompt and negative prompt to GPT-4o and ask it to generate 100 pairs of prompts and their corresponding negative prompts. For each prompt, we generate 10 images. We first generate images without using the negative prompt to show the positive prompt alignment comparison. Then we generate images with positive and negative prompts. We compute the CLIP score of generated images and the prompts and negative prompts. Fig. 6 shows that we not only achieve better prompt alignment but are much more sensitive to negative prompts. ", "page_idx": 8}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/cefbac315b5a8455c7a290fe5cccff4ea16d24023bfee7257670df468d06056a.jpg", "img_caption": ["Figure 9: Ablation study on the adversarial consistency design. (Left) Replacing latent discriminator with DINO causes detail loss. (Right) Replacing our adversarial loss with normal GAN loss causes training conflicted objectives and instability. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Consistent generation ability. Consistent generation ability under different inference steps is valuable in practice for multistep refinement. We compute the average CLIP similarity between the 1-step generation and the 16-step generation for each method. As shown in the rightmost column of Table 2 and Table 5, our method achieves significantly better consistent generation ability. ", "page_idx": 9}, {"type": "text", "text": "Adversarial consistency design and its effectiveness. We show the ablation study on the adversarial consistency loss design and its effectiveness. From the architecture level of discriminator, we compare the latent discriminator shared from the teacher diffusion model and the pixel discriminator from pre-trained DINO [4]. Note that DINO is trained with 224 resolutions, therefore we should resize the generation results and feed them into DINO. We find this could make the generation results fail at details as shown in the left of Fig. 9. From the adversarial loss, we compare our adversarial loss to the normal GAN loss. We find normal GAN loss causes the training to be unstable and corrupts the generation results, which aligns with our previous analysis. For its effectiveness, we compare the FID-CLIP and FID scores with the adversarial loss or without the adversarial loss under different inference steps. Fig. 7 shows that it greatly improves the FID scores in the low-step regime and gradually coverage to similar performance of our model without using the adversarial loss as the step increases. ", "page_idx": 9}, {"type": "text", "text": "Randomness for sampling. Fig. 8 illustrates the influence of the randomness introduced in sampling as Eq. 44. The figure shows that introducing a certain of randomness in sampling may help to alleviate unrealistic objects or shapes. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations and Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Despite being able to generate high-quality images and videos in a few steps, we find that when the number of steps is very low, especially with only one step, the generation quality is unstable. The model may produce structural errors or blurry images. Fortunately, we discover that this phenomenon can be mitigated through multi-step refinement. In conclusion, in this paper, we observe the defects in latent consistency models. We summarize these defects on three levels, analyze their causes, and generalize the design framework to address these defects. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This project is funded in part by National Key R&D Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)\u2019s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. arXiv preprint arXiv:2403.17377, 2024.   \n[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In International Conference on Computer Vision, pages 1728\u20131738, 2021.   \n[3] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023.   \n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In International Conference on Computer Vision, 2021.   \n[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual $12\\mathrm{m}$ : Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.   \n[6] Ricky TQ Chen and Yaron Lipman. Riemannian flow matching on general geometries. arXiv preprint arXiv:2302.03660, 2023.   \n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.   \n[8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.   \n[9] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024.   \n[10] Zhengyang Geng, Ashwini Pokle, and J Zico Kolter. One-step diffusion distillation via deep equilibrium models. Advances in Neural Information Processing Systems, 36, 2024.   \n[11] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and J Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024.   \n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.   \n[13] Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024.   \n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017.   \n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. ddpm. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020. ", "page_idx": 10}, {"type": "text", "text": "[16] Jonathan Ho and Tim Salimans. cfg. arXiv preprint arXiv:2207.12598, 2022. ", "page_idx": 11}, {"type": "text", "text": "[17] Minghui Hu, Jianbin Zheng, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, and Tat-Jen Cham. One more step: A versatile plug-and-play module for rectifying diffusion schedule flaws and enhancing low-frequency controls. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7331\u20137340, June 2024.   \n[18] Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, and Hongsheng Li. Comat: Aligning text-to-image diffusion model with image-to-text concept matching. arXiv preprint arXiv:2404.03653, 2024.   \n[19] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. arXiv preprint arXiv:2405.05967, 2024.   \n[20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. edm. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[21] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023.   \n[22] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with diffusion models. In Advances in Neural Information Processing Systems, 2021.   \n[23] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:36652\u201336663, 2023.   \n[24] Fei Kong, Jinhao Duan, Lichao Sun, Hao Cheng, Renjing Xu, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, and Kaidi Xu. Act-diffusion: Efficient adversarial consistency training for one-step diffusion models, 2024.   \n[25] Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models, 2024.   \n[26] Sangyun Lee, Yilun Xu, Tomas Geffner, Giulia Fanti, Karsten Kreis, Arash Vahdat, and Weili Nie. Truncated consistency models. arXiv preprint arXiv:2410.14895, 2024.   \n[27] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024.   \n[28] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In European Conference on Computer Vision, volume 8693, pages 740\u2013755. Springer, 2014.   \n[29] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \n[30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \n[31] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In International Conference on Learning Representations, 2023.   \n[32] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024.   \n[33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n[34] Guanxing Lu, Zifeng Gao, Tianxing Chen, Wenxun Dai, Ziwei Wang, and Yansong Tang. Manicm: Real-time 3d diffusion policy via consistency model for robotic manipulation. arXiv preprint arXiv:2406.01586, 2024.   \n[35] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023.   \n[36] Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu Liu. Exploring the role of large language models in prompt encoding for diffusion models, 2024.   \n[37] Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Wenbing Zhu, Jiangning Zhang, Hao Chen, Mingmin Chi, and Yabiao Wang. Osv: One step is enough for high-quality image to video generation. arXiv preprint arXiv:2409.11367, 2024.   \n[38] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Conference on Computer Vision and Pattern Recognition, pages 14297\u201314306, 2023.   \n[39] Zachary Novack, Ge Zhu, Jonah Casebeer, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J Bryan. Presto! distilling steps and layers for accelerating music generation. arXiv preprint arXiv:2410.05167, 2024.   \n[40] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, October 2023.   \n[41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.   \n[42] Aaditya Prasad, Kevin Lin, Jimmy Wu, Linqi Zhou, and Jeannette Bohg. Consistency policy: Accelerated visuomotor policies via consistency distillation, 2024.   \n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Learning Representations, pages 8748\u20138763. PMLR, 2021.   \n[44] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024.   \n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[46] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.   \n[47] Axel Sauer, Kashyap Chitta, Jens M\u00fcller, and Andreas Geiger. Projected gans converge faster. Advances in Neural Information Processing Systems, 34:17480\u201317492, 2021.   \n[48] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In International Conference on Learning Representations, pages 30105\u201330118. PMLR, 2023.   \n[49] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023.   \n[50] Christoph Schuhmann. Laion-aesthetics. https://laion.ai/blog/laion-aesthetics/, 2022. Accessed: 2023-11-10.   \n[51] Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, and Yu Liu. Rethinking the spatial inconsistency in classifier-free diffusion guidance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9370\u20139379, 2024.   \n[52] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pages 1\u201311, 2024.   \n[53] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022.   \n[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[55] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023.   \n[56] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \n[57] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[58] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402, 2012.   \n[59] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow, 2020.   \n[60] Cunzheng Wang, Ziyuan Guo, Yuxuan Duan, Huaxia Li, Nemo Chen, Xu Tang, and Yao Hu. Target-driven distillation: Consistency distillation with target timestep selection and decoupled guidance. arXiv preprint arXiv:2409.01347, 2024.   \n[61] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023.   \n[62] Fu-Yun Wang, Zhengyang Geng, and Hongsheng Li. Stable consistency tuning: Understanding and improving consistency models. arXiv preprint arXiv:2410.18958, 2024.   \n[63] Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Accelerating the animation of personalized diffusion models and adapters with decoupled consistency learning. arXiv preprint arXiv:2402.00769, 2024.   \n[64] Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, and Hongsheng Li. Be-your-outpainter: Mastering video outpainting through inputspecific adaptation. In European Conference on Computer Vision, pages 153\u2013168. Springer, 2025.   \n[65] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024.   \n[66] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023.   \n[67] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola. Restart sampling for improving generative processes, 2023.   \n[68] Yilun Xu, Shangyuan Tong, and Tommi Jaakkola. Stable target field for reduced variance score estimation in diffusion models. arXiv preprint arXiv:2302.00670, 2023.   \n[69] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. Advances in Neural Information Processing Systems, 36, 2024.   \n[70] Tianwei Yin, Micha\u00ebl Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6613\u20136623, 2024.   \n[71] Yuanhao Zhai, Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Chung-Ching Lin, David Doermann, Junsong Yuan, and Lijuan Wang. Motion consistency model: Accelerating video diffusion with disentangled motion-appearance distillation. arXiv preprint arXiv:2406.06890, 2024.   \n[72] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Conference on Computer Vision and Pattern Recognition, June 2018.   \n[73] Long Zhao, Sanghyun Woo, Ziyu Wan, Yandong Li, Han Zhang, Boqing Gong, Hartwig Adam, Xuhui Jia, and Ting Liu. epsilon-vae: Denoising as visual decoding. arXiv preprint arXiv:2410.04081, 2024.   \n[74] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024.   \n[75] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "I Related Works 1 ", "page_idx": 15}, {"type": "text", "text": "I.1 Diffusion Models I.2 Consistency Models . . 2 ", "page_idx": 15}, {"type": "text", "text": "II Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "II.1 Phased Consistency Distillation 2   \nII.2 Parameterization Equivalence . . . 4   \nII.3 Guided Distillation 4   \nII.4 Distribution Consistency Convergence . 6 ", "page_idx": 15}, {"type": "text", "text": "III Discussions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "III.1 Numerical Issue of Parameterization 7   \nIII.2 Why CTM Needs Target Timestep Embeddings? 8   \nIII.3 Contributions . 8 ", "page_idx": 15}, {"type": "text", "text": "IV Sampling 9 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "IV.1 Introducing Randomness for Sampling . . . 9   \nIV.2 Improving Diversity for Generation 9 ", "page_idx": 15}, {"type": "text", "text": "V Societal Impact 9 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "V.1 Positive Societal Impact . 9   \nV.2 Negative Societal Impact . . 10   \nV.3 Safeguards . . 10 ", "page_idx": 15}, {"type": "text", "text": "VI Implementation Details 10 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "VI.1 Training Details 10   \nVI.2 Pseudo Training Code . . 10   \nVI.3 Decoupled Consistency Distillation For Video Generation . 11   \nVI.4 Discriminator Design of Image Generation and Video Generation . 11 ", "page_idx": 15}, {"type": "text", "text": "VIIMore Generation Results. 12 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "I Related Works ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "I.1 Diffusion Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Diffusion models [15, 57, 20, 73] have gradually become the dominant foundation models in image synthesis. Many works have greatly explored the nature of diffusion models [29, 6, 57, 22] and generalize/improve the design space of diffusion models [54, 20, 22]. Some works explore the model architecture for diffusion models [7, 40, 9, 75]. Some works scale up the diffusion models for text-conditioned synthesis or real-world applications [45, 41, 52, 64, 18, 36, 69, 17, 61]. Some works explore the sampling acceleration methods, including scheduler-level [20, 33, 54] or traininglevel [38, 56]. The formal ones are basically to explore better approximation of the PF-ODE [33, 54]. ", "page_idx": 15}, {"type": "text", "text": "The latter are mainly distillation methods [38, 46, 30, 65, 70, 10] or initializing diffusion weights for GAN training [49, 27, 39, 19]. ", "page_idx": 16}, {"type": "text", "text": "I.2 Consistency Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The consistency model is a new family of generative models [11, 56, 55, 62, 26, 32] supporting fast and high quality generation. It can be trained either by distillation or direct training without teacher models. Improved techniques even allow consistency training excelling the performance of diffusion training [55]. Consistency trajectory model [21] proposes to learn the trajectory consistency, providing a more flexible framework. Some works combine [24] consistency training with GAN [24] for better training efficiency. Some works adopt continuous consistency models and achieve excelling performance [11, 62, 32]. Some works apply the idea of consistency model to language model [25] and policy learning [42, 34]. Some works extends the application scope of consistency models for text-conditioned image generation [35, 44, 74] and text-conditioned video generation [63, 37, 71, 60]. ", "page_idx": 16}, {"type": "text", "text": "We notice that a recent work multistep consistency models [13] also proposes to splitting the ODE trajectory into multi-parts for consistency learning, and here we hope to credit their work for the valuable exploration. However, our work is principally different from multistep consistency models. Since they do not open-source their code and weights, we clarify the difference between our work and multistep consistency models based on the details of their technical report. Firstly, they didn\u2019t provide explicit model definitions and boundary conditions nor theoretically show the error bound to prove the soundness of their work. Yet in our work, we believe we have given an explicit definition of important components and theoretically shown the soundness of important techniques applied. For example, they claimed using DDIM for training and inference, while in our work, we have shown that although we can also optionally parameterize as the DDIM format, there\u2019s an intrinsic difference between that parameterization and DDIM (i.e., the difference between exact solution learning and first-order approximation). The aDDIM and invDDIM as highlighted in their pseudo code have no relation to PCM. Secondly, they are mainly for the unconditional or class-conditional generation, yet we aim for text-conditional generation in large models and dive into the influence of classifier-free guidance in consistency distillation. Besides, we introduce an adversarial loss that aligns well with consistency learning and improves the generation results in few-step settings. We recommend readers to read their work for better comparison. ", "page_idx": 16}, {"type": "text", "text": "II Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "II.1 Phased Consistency Distillation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The following is an extension of the original proof of consistency distillation for phased consistency distillation. ", "page_idx": 16}, {"type": "text", "text": "Theorem 1. For arbitrary sub-trajectory $[s_{m},s_{m+1}]$ . let $\\begin{array}{r}{\\Delta t_{m}:=\\operatorname*{max}_{t_{n},t_{n+1}\\in[s_{m},s_{m+1}]}\\left\\lbrace\\left|t_{n+1}-\\right.\\right.}\\end{array}$ $t_{n}|\\}$ , and $\\pmb{f}^{m}(\\cdot,\\cdot;\\phi)$ be the target phased consistency function induced by the pre-trained diffusion model (empirical $P F{\\cdot}O D E_{\\l}$ ). Assume the $\\pmb{f}_{\\pmb{\\theta}}^{m}$ is $L$ -Lipschitz and the ODE solver has local error uniformly bounded by $\\mathcal{O}((t_{n+1}-t_{n})^{p+1})$ with $p\\geq1$ . Then, if $\\mathcal{L}_{P C M}(\\pmb{\\theta},\\pmb{\\theta};\\phi)=0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{n},t_{n+1}\\in[s_{m},s_{m+1}],\\mathbf{x}}\\|f_{\\theta}^{m}(\\mathbf{x},t_{n})-f^{m}(\\mathbf{x},t_{n})\\|=\\mathcal{O}((\\Delta t_{m})^{p}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. From the condition $\\mathcal{L}_{\\mathrm{PCM}}(\\theta,\\theta;\\phi)=0$ , for any ${\\bf x}_{t_{n}}$ and $t_{n},t_{n+1}\\in\\left[s_{m},s_{m+1}\\right]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{f}_{\\pmb{\\theta}}^{m}(\\mathbf{x}_{t_{n+1},t_{n+1}})\\equiv\\pmb{f}_{\\pmb{\\theta}}^{m}(\\hat{\\pmb{x}}_{t_{n}}^{\\phi},t_{n})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Denote $e_{n}^{m}:=f_{\\theta}^{m}(\\mathbf{x}_{t_{n}},t_{n})-f^{m}(\\mathbf{x}_{t_{n}},t_{n};\\phi)$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{n+1}^{m}=f_{\\theta}^{m}(\\mathbf{x}_{t_{n+1}},t_{n+1})-f^{m}(\\mathbf{x}_{t_{n+1}},t_{n+1};\\phi)}\\\\ &{\\qquad=f_{\\theta}^{m}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f^{m}(\\mathbf{x}_{t_{n}},t_{n};\\phi)}\\\\ &{\\qquad=f_{\\theta}^{m}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m}(\\mathbf{x}_{t_{n}},t_{n})+f_{\\theta}^{m}(\\mathbf{x}_{t_{n}},t_{n})-f^{m}(\\mathbf{x}_{t_{n}},t_{n};\\phi)}\\\\ &{\\qquad=f_{\\theta}^{m}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m}(\\mathbf{x}_{t_{n}},t_{n})+e_{n}^{m}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Considering that $\\pmb{f}_{\\pmb{\\theta}}^{m}$ is $L$ -Lipschitz, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|e_{n+1}^{m}\\|_{2}=\\|e_{n}^{m}+f_{\\theta}^{m}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m}(\\mathbf{x}_{t_{n}},t_{n})\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\|e_{n}^{m}\\|_{2}+\\|f_{\\theta}^{m}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m}(\\mathbf{x}_{t_{n}},t_{n})\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\|e_{n}^{m}\\|_{2}+L\\|\\mathbf{x}_{t_{n}}-\\mathbf{x}_{t_{n}}^{\\phi}\\|_{2}}\\\\ &{\\qquad=\\|e_{n}^{m}\\|_{2}+L\\cdot\\mathcal{O}((t_{n+1}-t_{n})^{p+1})}\\\\ &{\\qquad\\qquad\\leq\\|e_{n}^{m}\\|_{2}+L(t_{n+1}-t_{n})\\cdot\\mathcal{O}((\\Delta t_{m})^{p})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Besides, due to the boundary condition, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{e_{s_{m}}^{m}=f_{\\theta}^{m}(\\mathbf{x}_{s_{m}},s_{m})-f^{m}(\\mathbf{x}_{s_{m}},s_{m};\\phi)}}\\\\ &{}&{=\\mathbf{x}_{s_{m}}-\\mathbf{x}_{s_{m}}}\\\\ &{}&{=0}\\end{array}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|e_{n+1}^{m}\\|_{2}\\le\\|e_{s_{m}}^{m}\\|_{2}+L\\cdot\\mathcal{O}((\\Delta t)^{p})\\sum_{t_{i},t_{i+1}\\in[s_{m},s_{m+1}]}t_{i+1}-t_{i}}\\\\ &{\\qquad\\qquad=0+L\\cdot\\mathcal{O}((\\Delta t)^{p})\\cdot(s_{m+1}-s_{m})}\\\\ &{\\qquad\\qquad=\\mathcal{O}((\\Delta t_{m})^{p})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem 2. For arbitrary set of sub-trajectories $\\{[s_{i},s_{i+1}]\\}_{i=m}^{m^{\\prime}}$ . Let $\\begin{array}{r l}{\\Delta t_{m}}&{{}:=}\\end{array}$ $\\begin{array}{r}{\\operatorname*{max}_{t_{n},t_{n+1}\\in[s_{m},s_{m+1}]}\\{|t_{n+1}\\!-\\!t_{n}|\\},\\Delta t_{m,m^{\\prime}}:=\\operatorname*{max}_{i\\in[m^{\\prime},m]}\\{\\Delta t_{i}\\}}\\end{array}$ , and $\\pmb{f}^{m,m^{\\prime}}(\\cdot,\\cdot;\\phi)$ be the target phased consistency function induced by the pre-trained diffusion model (empirical $P F{\\cdot}O D E_{\\l}$ ). Assume the $\\pmb{f}_{\\pmb{\\theta}}^{m,m^{\\prime}}$ is $L$ -Lipschitz and the ODE solver has local error uniformly bounded by $O((t_{n+1}{-}t_{n})^{p+1})$ with $p\\geq1$ . Then, $i f\\,\\mathcal{L}_{P C M}(\\pmb{\\theta},\\pmb{\\theta};\\pmb{\\phi})=0$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{n}\\in[s_{m},s_{m+1}),\\mathbf{x}}\\|f_{\\theta}^{m,m^{\\prime}}(\\mathbf{x},t_{n})-f^{m,m^{\\prime}}(\\mathbf{x},t_{n})\\|=\\mathcal{O}((\\Delta t_{m,m^{\\prime}})^{p}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. From the condition $\\mathcal{L}_{\\mathrm{PCM}}(\\theta,\\theta;\\phi)=0$ , for any ${\\bf x}_{t_{n}}$ and $t_{n},t_{n+1}\\in\\left[s_{m},s_{m+1}\\right]$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{f}_{\\pmb{\\theta}}^{m}(\\mathbf{x}_{t_{n+1},t_{n+1}})\\equiv\\pmb{f}_{\\pmb{\\theta}}^{m}(\\hat{\\pmb{x}}_{t_{n}}^{\\phi},t_{n})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Denote $e_{n}^{m,m^{\\prime}}:=f_{\\pmb\\theta}^{m,m^{\\prime}}(\\mathbf x_{t_{n}},t_{n})-f^{m,m^{\\prime}}(\\mathbf x_{t_{n}},t_{n};\\pmb\\phi)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{n+1}^{m,m^{\\prime}}=f_{\\pmb{\\theta}}^{m,m^{\\prime}}(\\mathbf{x}_{t_{n+1}},t_{n+1})-f^{m,m^{\\prime}}(\\mathbf{x}_{t_{n+1}},t_{n+1};\\phi)}\\\\ &{\\qquad=f_{\\pmb{\\theta}}^{m,m^{\\prime}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f^{m,m^{\\prime}}(\\mathbf{x}_{t_{n}},t_{n};\\phi)}\\\\ &{\\qquad=f_{\\pmb{\\theta}}^{m,m^{\\prime}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\pmb{\\theta}}^{m,m^{\\prime}}(\\mathbf{x}_{t_{n}},t_{n})+f_{\\pmb{\\theta}}^{m,m^{\\prime}}(\\mathbf{x}_{t_{n}},t_{n})-f^{m,m^{\\prime}}(\\mathbf{x}_{t_{n}},t_{n};\\phi)}\\\\ &{\\qquad=f_{\\pmb{\\theta}}^{m,m^{\\prime}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\pmb{\\theta}}^{m,m^{\\prime}}(\\mathbf{x}_{t_{n}},t_{n})+e_{n}^{m,m^{\\prime}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Considering that $\\pmb{f}_{\\pmb{\\theta}}^{m}$ is $L$ -Lipschitz, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert e_{n+1}^{m,m^{\\prime}}\\Vert_{2}=\\Vert e_{n}^{m,m^{\\prime}}+f_{\\theta}^{m,m^{\\prime}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m,m^{\\prime}}(\\mathbf{x}_{t_{n}},t_{n})\\Vert_{2}}&{}\\\\ &{\\qquad\\leq\\Vert e_{n}^{m,m^{\\prime}}\\Vert_{2}+\\Vert f_{\\theta}^{m,m^{\\prime}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m,m^{\\prime}}(\\mathbf{x}_{t_{n}},t_{n})\\Vert_{2}}\\\\ &{\\qquad\\leq\\Vert e_{n}^{m,m^{\\prime}}\\Vert_{2}+L\\Vert f_{\\theta}^{m,m^{\\prime}+1}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m,m^{\\prime}+1}(\\mathbf{x}_{t_{n}},t_{n})\\Vert_{2}}\\\\ &{\\qquad\\leq\\Vert e_{n}^{m,m^{\\prime}}\\Vert_{2}+L^{2}\\Vert f_{\\theta}^{m,m^{\\prime}+2}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m,m^{\\prime}+2}(\\mathbf{x}_{t_{n}},t_{n})\\Vert_{2}}\\\\ &{\\qquad\\leq\\quad\\vdots}\\\\ &{\\leq\\Vert e_{n}^{m,m^{\\prime}}\\Vert_{2}+L^{m-m^{\\prime}}\\Vert f_{\\theta}^{m}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m}(\\mathbf{x}_{t_{n}},t_{n})\\Vert_{2}}\\\\ &{\\qquad\\leq\\Vert e_{n}^{m,m^{\\prime}}\\Vert_{2}+L^{m-m^{\\prime}+1}\\Vert\\mathbf{x}_{t_{n}}-\\mathbf{x}_{t_{n}}^{\\phi}\\Vert_{2}}\\\\ &{\\qquad=\\Vert e_{n}^{m,m^{\\prime}}\\Vert_{2}+L^{m-m^{\\prime}+1}\\cdot\\mathcal{O}((t_{n+1}-t_{n})^{p+1})}\\\\ &{\\qquad\\leq\\Vert e_{n}^{m,m^{\\prime}}\\Vert_{2}+L^{m-m^{\\prime}+1 \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|e_{n+1}^{m,m^{\\prime}}\\|_{2}\\le\\|e_{s_{m}}^{m,m^{\\prime}}\\|_{2}+L^{m-m^{\\prime}+1}\\cdot\\mathcal{O}((\\Delta t)^{p})\\sum_{t_{i},t_{i+1}\\in[s_{m},s_{m+1}]}t_{i+1}-t_{i}}}\\\\ &{}&{=\\|e_{s_{m}}^{m,m^{\\prime}}\\|_{2}+L^{m-m^{\\prime}+1}\\cdot\\mathcal{O}((\\Delta t)^{p})\\cdot(s_{m+1}-s_{m})}\\\\ &{}&{=\\|e_{s_{m}}^{m,m^{\\prime}}\\|_{2}+\\mathcal{O}((\\Delta t_{m})^{p})}\\\\ &{}&{=\\|e_{s_{m}}^{m-1,m^{\\prime}}\\|_{2}+\\mathcal{O}((\\Delta t_{m})^{p})}\\end{array},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Where the last equation is due to the boundary condition $f_{\\theta}^{m,m^{\\prime}}(\\mathbf{x}_{s_{m}},s_{m})$ $\\begin{array}{r}{f_{\\theta}^{m-1,m^{\\prime}}(f_{\\theta}^{m}(\\mathbf{x}_{s_{m}},s_{m}),s_{m})=\\mathbf{f}_{\\theta}^{m-1,m^{\\prime}}(\\mathbf{x}_{s_{m}},s_{m}).}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Thereby, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|e_{n+1}^{m,m^{\\prime}}\\|_{2}\\leq\\|e_{s_{m}-1}^{m-1,m^{\\prime}}\\|_{2}+\\mathcal{O}((\\Delta t_{m})^{p})}\\\\ &{\\qquad\\leq\\|e_{m-2}^{m-2,m^{\\prime}}\\|_{2}+\\mathcal{O}((\\Delta t_{m})^{p})+\\mathcal{O}((\\Delta t_{m-1})^{p})}\\\\ &{\\qquad\\leq}\\\\ &{\\qquad\\leq\\|e_{s_{m}-1}^{m^{\\prime}}\\|+\\displaystyle\\sum_{i=m}^{m^{\\prime}+1}\\mathcal{O}((\\Delta t_{i})^{p})}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{i=m}^{m^{\\prime}}\\mathcal{O}((\\Delta t_{i})^{p})}\\\\ &{\\qquad\\leq\\sum_{i=m}^{m}\\mathcal{O}((\\Delta t_{i})^{p})}\\\\ &{\\qquad\\leq(m-m^{\\prime}+1)\\mathcal{O}((\\Delta t_{m,m^{\\prime}})^{p})}\\\\ &{\\qquad=\\mathcal{O}((\\Delta t_{m,m^{\\prime}})^{p})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "II.2 Parameterization Equivalence ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We show that the parameterization of Equation 3 is equal to the DDIM inference format [33, 54]. ", "page_idx": 18}, {"type": "text", "text": "Theorem 3. Define $\\begin{array}{r}{F_{\\theta}(\\mathbf{x}_{t},t,s)=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}-\\alpha_{s}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\mathrm{d}\\lambda,}\\end{array}$ then the parameterization has the same format of DDI $\\begin{array}{r}{M\\,\\mathbf{x}_{s}=\\alpha_{s}\\left(\\frac{\\mathbf{x}_{t}-\\sigma_{t}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)}{\\alpha_{t}}\\right)+\\sigma_{s}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\theta}(\\mathbf{x}_{t},t,s)=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}-\\alpha_{s}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda_{s}}\\mathrm{d}\\lambda}\\\\ &{\\phantom{\\frac{(x_{t},t)}{\\alpha_{t}}}=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}-\\alpha_{s}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\left(e^{-\\lambda_{t}}-e^{-\\lambda_{s}}\\right)}\\\\ &{\\phantom{\\frac{(x_{t},t)}{\\alpha_{t}}}=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}-\\alpha_{s}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)(\\frac{\\sigma_{t}}{\\alpha_{t}}-\\frac{\\sigma_{s}}{\\alpha_{s}})}\\\\ &{\\phantom{\\frac{(x_{t},t)}{\\alpha_{t}}}=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}-\\frac{\\alpha_{s}\\sigma_{t}}{\\alpha_{t}}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)+\\sigma_{s}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)}\\\\ &{\\phantom{\\frac{(x_{t},t)}{\\alpha_{t}}}=\\alpha_{s}\\left(\\frac{\\mathbf{x}_{t}-\\sigma_{t}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)}{\\alpha_{t}}\\right)+\\sigma_{s}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "II.3 Guided Distillation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We show the relationship of epsilon prediction of consistency models trained with guided distillation and diffusion models when considering the classifier-free guidance. ", "page_idx": 18}, {"type": "text", "text": "Theorem 4. Assume the consistency model $\\epsilon_{\\theta}$ is trained by consistency distillation with the teacher diffusion model $\\epsilon_{\\phi}$ , and the ODE solver is augmented with CFG value w and null text embedding $\\mathcal{Q}$ . Let $^c$ and $c_{n e g}$ be the prompt and negative prompt applied for the inference of consistency model. Then, if the ODE solver is perfect and the $\\mathcal{L}_{P C M}(\\pmb{\\theta},\\pmb{\\theta})=0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\epsilon_{\\theta}(\\mathbf{x},t,c,c_{n e g};w^{\\prime})\\propto w w^{\\prime}\\left[\\epsilon_{\\phi}(\\mathbf{x},t,c)-((1-\\frac{w-1}{w w^{\\prime}})\\epsilon_{\\phi}(\\mathbf{x},t,c_{n e g})+\\frac{w-1}{w w^{\\prime}}\\epsilon_{\\phi}(\\mathbf{x},t,\\emptyset))\\right]}\\\\ {\\displaystyle\\;+\\,\\epsilon_{\\phi}(\\mathbf{x},t,c_{n e g})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. If the ODE solver is perfect, that means the empirical PF-ODE is exactly the PF-ODE of the training data. Then considering Theorem 1 and Theorem 2, it is apparent that the consistency model will fit the PF-ODE. To show that, considering the case in Theorem 1, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{e_{n+1}^{m}=f_{\\theta}^{m}(\\mathbf x_{t_{n+1}},t_{n+1})-f^{m}(\\mathbf x_{t_{n+1}},t_{n+1};\\phi)}\\\\ {=f_{\\theta}^{m}(\\hat{\\mathbf x}_{t_{n}}^{\\phi},t_{n})-f^{m}(\\mathbf x_{t_{n}},t_{n};\\phi)}\\\\ {=f_{\\theta}^{m}(\\hat{\\mathbf x}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m}(\\mathbf x_{t_{n}},t_{n})+f_{\\theta}^{m}(\\mathbf x_{t_{n}},t_{n})-f^{m}(\\mathbf x_{t_{n}},t_{n};\\phi)}\\\\ {=f_{\\theta}^{m}(\\hat{\\mathbf x}_{t_{n}}^{\\phi},t_{n})-f_{\\theta}^{m}(\\mathbf x_{t_{n}},t_{n})+e_{n}^{m}}\\\\ {\\overset{(i)}{=}f_{\\theta}^{m}(\\mathbf x_{t_{n}},t_{n})-f_{\\theta}^{m}(\\mathbf x_{t_{n}},t_{n})+e_{n}^{m}}\\\\ {=e_{n}^{m}}\\\\ {=\\vdots}\\\\ {=e_{s}^{m}}\\\\ {=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(i)$ is because the ODE solver is perfect. Considering our parameterization in Equation 3, then it should be equal to the exact solution in Equation 2. That is, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}-\\alpha_{s}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\mathrm{d}\\lambda=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}+\\alpha_{s}\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\sigma_{t_{\\lambda}(\\lambda)}\\nabla\\log\\mathbb{P}_{t_{\\lambda}(\\lambda)}(\\mathbf{x}_{t\\lambda(\\lambda)})\\mathrm{d}\\lambda}\\\\ &{\\qquad\\qquad\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\mathrm{d}\\lambda=\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\epsilon_{\\phi}(\\mathbf{x}_{t\\lambda(\\lambda)},t_{\\lambda}(\\lambda))\\mathrm{d}\\lambda}\\\\ &{\\qquad\\qquad\\qquad\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)=\\frac{\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\epsilon_{\\phi}(\\mathbf{x}_{t\\lambda(\\lambda)},t_{\\lambda}(\\lambda))\\mathrm{d}\\lambda}{\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\mathrm{d}\\lambda}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we have the epsilon prediction is weighted integral of diffusion-based epsilon prediction on the trajectory. Therefore, it is apparent that, for any $t^{\\prime}\\leq t$ and $t^{\\prime}$ on the same sub-trajectory with $t$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\propto\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When considering the text-conditioned generation and the ODE solver being augmented with the CFG value $w$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,c)\\propto\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\mathcal{O})+w(\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c)-\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\mathcal{O}))\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If we additionally apply the classifier-free guidance to the consistency models with negative prompt embedding $c_{\\mathrm{n}e g}$ and CFG value $w^{\\prime}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,c,\\mathrm{c}_{\\mathrm{neg}};w^{\\prime})}\\\\ &{=\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,c_{\\mathrm{neg}})+w^{\\prime}(\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,c)-\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,c_{\\mathrm{neg}}))}\\\\ &{\\propto\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset)+w(\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c)-\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset))}\\\\ &{+w^{\\prime}\\{[\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset)+w(\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c)-\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset))]}\\\\ &{-\\left[\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset)+w(\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c)-\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset))\\right]\\}}\\\\ &{=\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset)+w(\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c_{\\mathrm{neg}})-\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset))+w w^{\\prime}(\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c)-\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c_{\\mathrm{neg}}))}\\\\ &{=w w^{\\prime}(\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c)-((1-\\alpha)\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c_{\\mathrm{neg}})+\\alpha\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},\\emptyset)))+\\epsilon_{\\phi}(\\mathbf{x}_{t^{\\prime}},t^{\\prime},c_{\\mathrm{neg}})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "II.4 Distribution Consistency Convergence ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We firstly show that when $\\mathcal{L}_{\\mathrm{PCM}}=0$ is achieved, our distribution consistency loss will also converge to zero. Then, we additionally show that, when considering the pre-train data distribution and distillation data distribution mismatch, combining GAN is a flawed design. The loss is still non-zero even when the self-consistency is achieved, thus corrupting the training. ", "page_idx": 20}, {"type": "text", "text": "Theorem 5. Denote the data distribution applied for consistency distillation phased is $\\mathbb{P}_{0}$ . And considering that the forward conditional probability path is defined by $\\alpha_{t}\\mathbf{x}_{0}+\\sigma_{t}\\pmb{\\epsilon}_{}$ , we further define the intermediate distribution $\\begin{array}{r}{\\mathbb{P}_{t}(\\mathbf{x})\\stackrel{{}}{=}(\\mathbb{P}_{0}(\\frac{\\mathbf{x}}{\\alpha_{t}})\\stackrel{{}}{\\cdot}\\frac{\\hat{\\mathbf{\\xi}}}{\\alpha_{t}})*\\mathcal{N}(0,\\bar{\\sigma_{t}})}\\end{array}$ . Similarly, we denote the data distribution applied for pretraining the diffusion model is $\\mathbb{P}_{0}^{p r e t r a i n}(\\mathbf{x})$ and the intermediate distribution following forward process are $\\begin{array}{r}{\\mathbb{P}_{t}^{p r e t r a i n}(\\mathbf{x})=(\\mathbb{P}_{t}^{p r e t r a i n}(\\mathbf{x})=(\\mathbb{P}_{0}^{p r e t r a i n}(\\frac{\\mathbf{x}}{\\alpha_{t}})\\cdot\\frac{1}{\\alpha_{t}})*\\mathcal{N}(0,\\sigma_{t}))}\\end{array}$ . This is rmeuacsho nlaarbglee rs idnactea sceutrsr ecnot mlapragree dd itfof utshioosne  mofo dceolnss iasrtee tnycpyi cdaislltiyl ltartaiionne.d  Awnidt,h  wmeu cdhe nmotoer et hree sfolouwrc $T_{t\\to s}^{\\phi}$ $\\mathcal{T}_{t\\rightarrow s}^{\\theta}$ , and $\\mathcal{T}_{t\\rightarrow s}^{\\phi^{\\prime}}$ correspond to our consistency model, pre-trained diffusion model, and the PF-ODE of the data distribution used for consistency distillation, respectively. Additionally, let $\\tau_{s\\rightarrow t}^{-}$ be the distribution transition following the forward process SDE (adding noise). Then, if the $\\mathcal{L}_{P C M}=0,$ , for arbitrary sub-trajectory $[s_{m},s_{m+1}]$ , we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{P C M}^{a d v}(\\pmb{\\theta},\\pmb{\\theta};\\phi,m)=D\\left(\\mathcal{T}_{s_{m}\\rightarrow s}^{-}\\mathcal{T}_{t_{n+1}\\rightarrow s_{m}}^{\\pmb{\\theta}}\\#\\mathbb{P}_{t_{n+1}}\\Big|\\Big|\\mathcal{T}_{s_{m}\\rightarrow s}^{-}\\mathcal{T}_{t_{n}\\rightarrow s_{m}}^{\\pmb{\\theta}}\\mathcal{T}_{t_{n+1}\\rightarrow t_{n}}^{\\phi}\\#\\mathbb{P}_{t_{n+1}}\\right)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Firstly, considering that the forward process $\\tau_{s\\rightarrow t}^{-}$ is equivalent to scaling the original variables and then performing convolution operations with $\\bar{\\mathcal{N}}(\\bar{\\mathbf{0}_{,\\,\\sigma_{s\\rightarrow t}^{2}}}\\mathbf{\\bar{I}})$ . Therefore, as long as ", "page_idx": 20}, {"type": "equation", "text": "$$\nD\\left(\\mathcal{T}_{t_{n+1}\\rightarrow s_{m}}^{\\theta}\\#\\mathbb{P}_{t_{n+1}}\\Big|\\Big|\\mathcal{T}_{t_{n}\\rightarrow s_{m}}^{\\theta}\\mathcal{T}_{t_{n+1}\\rightarrow t_{n}}^{\\phi}\\#\\mathbb{P}_{t_{n+1}}\\right)=0\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D\\left(\\mathcal{T}_{s_{m}\\rightarrow s}^{-}\\mathcal{T}_{t_{n+1}\\rightarrow s_{m}}^{\\theta}\\#\\mathbb{P}_{t_{n+1}}\\Big|\\Big|\\mathcal{T}_{s_{m}\\rightarrow s}^{-}\\mathcal{T}_{t_{n}\\rightarrow s_{m}}^{\\theta}\\mathcal{T}_{t_{n+1}\\rightarrow t_{n}}^{\\phi}\\#\\mathbb{P}_{t_{n+1}}\\right)=0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From the condition $\\mathcal{L}_{\\mathrm{PCM}}(\\theta,\\theta;\\phi)\\,=\\,0$ , for any $\\mathbf{x}_{t_{n}}\\,\\in\\,\\mathbb{P}_{t_{n}}$ and $\\mathbf{x}_{t_{n+1}}\\in\\mathbb{P}_{t_{n+1}}$ and $t_{n},t_{n+1}\\ \\in$ $[s_{m},s_{m+1}]$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{f}_{\\pmb{\\theta}}^{m}(\\mathbf{x}_{t_{n+1},t_{n+1}})\\equiv\\pmb{f}_{\\pmb{\\theta}}^{m}(\\hat{\\pmb{x}}_{t_{n}}^{\\phi},t_{n})\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which induces that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{t_{n+1}\\rightarrow s_{m}}^{\\theta}\\#\\mathbb{P}_{t_{n+1}}\\equiv T_{t_{n}\\rightarrow s_{m}}^{\\theta}T_{t_{n+1}\\rightarrow t_{n}}^{\\phi}\\#\\mathbb{P}_{t_{n+1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we show that if $\\mathcal{L}_{P C M}(\\pmb{\\theta},\\pmb{\\theta};\\pmb{\\phi})=0$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{PCM}}^{a d v}(\\theta,\\theta;\\phi)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Theorem 6. Denote the data distribution applied for consistency distillation phased is $\\mathbb{P}_{0}$ . And considering that the forward conditional probability path is defined by $\\alpha_{t}\\mathbf{x}_{0}+\\sigma_{t}\\pm$ , we further define the intermediate distribution $\\begin{array}{r}{\\mathbb{P}_{t}(\\mathbf{x})\\stackrel{{}}{=}(\\mathbb{P}_{0}(\\frac{\\mathbf{x}}{\\alpha_{t}})\\stackrel{{}}{\\cdot}\\frac{\\cdot}{\\alpha_{t}})*\\mathcal{N}(0,\\sigma_{t})}\\end{array}$ . Similarly, we denote the data distribution applied for pretraining the diffusion model is $\\mathbb{P}_{0}^{p r e t r a i n}(\\mathbf{x})$ and the intermediate distribution following forward process are $\\begin{array}{r}{\\mathbb{P}_{t}^{p r e t r a i n}(\\mathbf{x})=(\\mathbb{P}_{t}^{p r e t r a i n}(\\mathbf{x})=(\\mathbb{P}_{0}^{p r e t r a i n}(\\frac{\\mathbf{x}}{\\alpha_{t}})\\cdot\\frac{1}{\\alpha_{t}})*\\mathcal{N}(0,\\sigma_{t}))}\\end{array}$ . This is reasonable since current large diffusion models are typically trained with much more resources on much larger datasets compared to those of consistency distillation. And, we denote the flow $T_{t\\to s}^{\\phi}$ , $\\mathcal{T}_{t\\rightarrow s}^{\\theta}$ e,  adnatd $\\mathcal{T}_{t\\rightarrow s}^{\\phi^{\\prime}}$ r icbourtrieosnp ounsde dt of ooru rc ocnosnissitsetnencyc yd imstoidlleal,t iporne,- trreasipneecdt idvieffluy.s ioTnh emn,o dife lt,h ae O, fDoEr $\\mathcal{L}_{P C M}=0,$ arbitrary sub-trajectory $[s_{m},s_{m+1}]$ , we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal L_{P C M}^{a d v}(\\pmb{\\theta},\\pmb{\\theta};\\phi,m)=D\\left(\\mathcal T_{t_{n+1}\\rightarrow s_{m}}^{\\pmb{\\theta}}\\#\\mathbb P_{t_{n+1}}\\Big|\\Big|\\mathbb P_{s_{m}}\\right)\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. From the condition $\\mathcal{L}_{\\mathrm{PCM}}(\\theta,\\theta;\\phi)=0$ , for any $\\mathbf{x}_{t_{n}}\\in\\mathbb{P}_{t_{n}}$ and $\\mathbf{x}_{t_{n+1}}\\in\\mathbb{P}_{t_{n+1}}$ and $t_{n},t_{n+1}\\in$ $[s_{m},s_{m+1}]$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{f}_{\\pmb{\\theta}}^{m}(\\mathbf{x}_{t_{n+1},t_{n+1}})\\equiv\\pmb{f}_{\\pmb{\\theta}}^{m}(\\hat{\\pmb{x}}_{t_{n}}^{\\phi},t_{n})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which induces that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{t_{n+1}\\to s_{m}}^{\\pmb\\theta}\\#\\mathbb{P}_{t_{n+1}}\\equiv T_{t_{n+1}\\to s_{m}}^{\\phi}\\#\\mathbb{P}_{t_{n+1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Besides, since $\\mathcal{T}_{t\\rightarrow s}^{\\phi^{\\prime}}$ corresponds to the PF-ODE of the data distribution used for consistency distillation, we can rewrite ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}_{s_{m}}\\equiv T_{t_{n+1}\\rightarrow s_{m}}^{\\phi^{\\prime}}\\#\\mathbb{P}_{t_{n+1}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D\\left(\\mathcal{T}_{t_{n+1}\\rightarrow s_{m}}^{\\theta}\\#\\mathbb{P}_{t_{n+1}}\\Big\\lVert\\mathbb{P}_{s_{m}}\\right)=D\\left(\\mathcal{T}_{t_{n+1}\\rightarrow s_{m}}^{\\phi}\\#\\mathbb{P}_{t_{n+1}}\\Big\\lVert T_{t_{n+1}\\rightarrow s_{m}}^{\\phi^{\\prime}}\\#\\mathbb{P}_{t_{n+1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since we know that $\\mathbb{P}_{0}^{p r e t r a i n}\\neq\\mathbb{P}_{0}$ . Therefore, there exists $m$ and $n$ achieving the strict inequality. Specifically, if we set $s_{m}=\\epsilon$ and $t_{n+1}=T$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nD\\left(T_{T\\rightarrow\\epsilon}^{\\phi}\\#\\mathbb{P}_{T}\\Big\\lVert T_{T\\rightarrow\\epsilon}^{\\phi^{\\prime}}\\#\\mathbb{P}_{T}\\right)=D\\left(\\mathbb{P}_{0}^{p r e t r a i n}\\Big\\rVert\\mathbb{P}_{0}\\right)>0\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "III Discussions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "III.1 Numerical Issue of Parameterization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Even though our above-discussed parameterization is theoretically sound, it poses a numerical issue when applied to the epsilon-prediction-based models, especially for the one-step generation. To be specific, for the one-step generation, we are required to predict the solution point $\\mathbf{x}_{\\mathrm{0}}$ with $\\mathbf{x}_{t}$ from the self-consistency property of consistency models. With epsilon-prediction models, the formula can represented as the following ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{x}_{0}=\\frac{\\mathbf{x}_{t}-\\sigma_{t}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)}{\\alpha_{t}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "However, when inference, we should choose $t=T$ since it is the noisy timestep that is closest to the normal distribution. For the DDPM framework, there should be $\\sigma_{T}\\approx1$ and $\\alpha_{T}\\approx0$ to make the noisy timestep $T$ as close to the normal distribution as possible. For instance, the noise schedulers applied by Stable Diffusion v1-5 and Stable Diffusion XL all have the $\\alpha_{T}=0.068265$ . Therefore, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{x}_{0}=\\frac{\\mathbf{x}_{T}-\\sigma_{T}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)}{\\alpha_{T}}\\approx14.64(\\mathbf{x}_{T}-\\sigma_{T}\\epsilon_{\\theta}(\\mathbf{x}_{t},t))\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This indicates that we will multiply over $10\\times$ to the model prediction. In our experiments, we find the SDXL is more influenced by this issue, tending to produce artifact points or lines. ", "page_idx": 21}, {"type": "text", "text": "Generally speaking, using the $\\mathbf{x}0$ -prediction and v-prediction can well solve these issues. It is obvious for the $\\mathbf{x}0$ -prediction. For the v-prediction, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{0}=\\alpha_{t}\\mathbf{x}_{t}-\\sigma_{t}\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "However, since the diffusion models are trained with the epsilon-prediction, transforming them into the other prediction formats takes additional computation resources and might harm the performance due to the relatively large gaps among different prediction formats. ", "page_idx": 21}, {"type": "text", "text": "We solve this issue through a simple way that we set a clip boundary to the value of $\\alpha_{t}$ . Specifically, we apply ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{x}_{0}=\\frac{\\mathbf{x}_{t}-\\sigma_{t}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)}{\\operatorname*{max}\\{\\alpha_{t},0.5\\}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "III.2 Why CTM Needs Target Timestep Embeddings? ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Consistency trajectory model (CTM), as we discussed, proposes to learn how \u2018move\u2019 from arbitrary points on the ODE trajectory to arbitrary other points. Thereby, the model should be aware of \u2018where they are\u2019 and \u2018where they target\u2019 for precise prediction. ", "page_idx": 22}, {"type": "text", "text": "Basically, they can still follow our parameterization with an additional target timestep embedding. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{x}_{s}=\\frac{\\alpha_{s}}{\\alpha_{t}}\\mathbf{x}_{t}-\\alpha_{s}\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,s)\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\mathrm{d}\\lambda\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In this design, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,s)=\\frac{\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\epsilon_{\\phi}(\\mathbf{x}_{t_{\\lambda}(\\lambda)},t_{\\lambda}(\\lambda))\\mathrm{d}\\lambda}{\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\mathrm{d}\\lambda}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, we show that dropping the target timestep embedding $s$ is a flawed design. ", "page_idx": 22}, {"type": "text", "text": "Assume we hope to predict the results at timestep $s^{\\prime}$ and timestep $s^{\\prime\\prime}$ from timestep $t$ and sample $\\mathbf{x}_{t}$ , where $s^{\\prime}<s^{\\prime\\prime}$ . ", "page_idx": 22}, {"type": "text", "text": "If we hope to learn both $\\mathbf{x}_{s^{\\prime}}$ and $\\mathbf{x}_{s^{\\prime\\prime}}$ correctly, without the target timestep embedding, we must have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)=\\frac{\\int_{\\lambda_{t}}^{\\lambda_{s^{\\prime}}}e^{-\\lambda}\\epsilon_{\\phi}\\left(\\mathbf{x}_{t_{\\lambda}(\\lambda)},t_{\\lambda}(\\lambda)\\right)\\mathrm{d}\\lambda}{\\int_{\\lambda_{t}}^{\\lambda_{s^{\\prime}}}e^{-\\lambda}\\mathrm{d}\\lambda}}\\\\ {\\hat{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)=\\frac{\\int_{\\lambda_{t}}^{\\lambda_{s^{\\prime\\prime}}}e^{-\\lambda}\\epsilon_{\\phi}\\left(\\mathbf{x}_{t_{\\lambda}(\\lambda)},t_{\\lambda}(\\lambda)\\right)\\mathrm{d}\\lambda}{\\int_{\\lambda_{t}}^{\\lambda_{s^{\\prime\\prime}}}e^{-\\lambda}\\mathrm{d}\\lambda}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This will lead to the conclusion that all the segments $[t,s]$ on the ODE trajectory have the same weighted epsilon prediction, which violates the truth, ignoring the dynamic variations and dependencies specific to each segments. ", "page_idx": 22}, {"type": "text", "text": "III.3 Contributions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here we re-emphasize the key components of PCM and summarize the contributions of our work. ", "page_idx": 22}, {"type": "text", "text": "The motivation of our work is to accelerate the sampling of high-resolution text-to-image and textto-video generation with consistency models training paradigm. Previous work, latent consistency model (LCM), tried to replicate the power of the consistency model in this challenging setting but did not achieve satisfactory results. We observe and analyze the limitations of LCM from three perspectives and propose PCM, generalizing the design space and tackling all these limitations. ", "page_idx": 22}, {"type": "text", "text": "At the heart of PCM is to phase the whole ODE trajectory into multiple phases. Each phase corresponding to a sub-trajectory is treated as an independent consistency model learning objective. We provide a standard definition of PCM and show that the optimal error bounds between the prediction of trained PCM and PF-ODE are upper-bounded by ${\\bar{O}}((\\Delta t)^{p})$ . The phasing technique allows for deterministic multi-step sampling, ensuring consistent generation results under different inference steps. ", "page_idx": 22}, {"type": "text", "text": "Additionally, we provide a deep analysis of the parameterization when converting pre-trained diffusion models into consistency models. From the exact solution format of PF-ODE, we propose a simple yet effective parameterization and show that it has the same formula as first-order ODE solver DDIM. However, we show that the parameterization has a natural difference from the DDIM ODE solver. That is the difference between exact solution learning and score estimation. Besides, we point out that even though the parameterization is theoretically sound, it poses numerical issues when building the one-step generator. We propose a simple yet effective threshold clip strategy for the parameterization. ", "page_idx": 22}, {"type": "text", "text": "We introduce an innovative adversarial loss, which greatly boosts the generation quality at a low-step regime. We implement the loss in a GAN style. However, we show that our method has an intrinsic difference from traditional GAN. We show that our introduced adversarial loss will converge to zero when the self-consistency property is achieved. However, the GAN loss will still be non-zero when considering the distribution distance between the pre-trained dataset for diffusion training and the distillation dataset for consistency distillation. We investigate the structures of discriminators and compare the latent-based discriminator and pixel-based discriminator. Our finding is that applying the latent-based visual backbone of pre-trained diffusion U-Net makes the discriminator design simple and can produce better visual results compared to pixel-based discriminators. We also implement a similar discriminator structure for the text-to-video generation with a temporal inflated U-Net. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "For the setting of text-to-image generation and text-to-video generation, classifier-free guidance (CFG) has become an important technique for achieving better controllability and generation quality. We investigate the influence of CFG on consistency distillation. And, to our knowledge, we, for the first time, point out the relations of consistency model prediction and diffusion model prediction when considering the CFG-augmented ODE solver (guided distillation). We show that this technique causes the trained consistency models unable to use large CFG values and be less sensitive to the negative prompt. ", "page_idx": 23}, {"type": "text", "text": "We achieve state-of-the-art few-step text-to-image generation and text-to-video generation with only 8 A 800 GPUs, indicating the advancements of our method. ", "page_idx": 23}, {"type": "text", "text": "IV Sampling ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "IV.1 Introducing Randomness for Sampling ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For a given initial sample at timestep $t$ belonging to the sub-trajectory $[s_{m},s_{m+1}]$ , we can support deterministic sampling according to the definition of $\\pmb{f}^{m,0}$ . ", "page_idx": 23}, {"type": "text", "text": "However, previous work [21, 67] reveals that introducing a certain degree of stochasticity can lead to better generation results. We also observe a similar trade-off phenomenon in our practice. Therefore, we reparameterize the $F_{\\theta}(\\mathbf{x},t,s)$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\alpha_{s}(\\frac{\\mathbf{x}_{t}-\\sigma_{t}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)}{\\alpha_{t}})+\\sigma_{s}(\\sqrt{r}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)+\\sqrt{(1-r)}\\epsilon),\\quad\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By controlling the value of $r\\,\\in\\,[0,1]$ , we can determine the stochasticity for generation. With $r=1$ , it is pure deterministic sampling. With $r=0$ , it degrades to pure stochastic sampling. Note that, introducing the random $\\epsilon$ will make the generation results be away from the target induced by the diffusion models. However, since $\\epsilon_{\\theta}$ and $\\epsilon$ all follows the normal distribution, we have $\\sqrt{r}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)+\\sqrt{(1-r)}\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and thereby the predicted $\\mathbf{x}_{s}$ should still follow the same distribution $\\mathbb{P}_{s}$ approximately. ", "page_idx": 23}, {"type": "text", "text": "IV.2 Improving Diversity for Generation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our another observation is the generation diversity with guided distillation is limited compared to original diffusion models. This is due to that the consistency models are distilled with a relatively large CFG value for guided distillation. The large CFG value, though known for enhancing the generation quality and text-image alignment, will degrade the generation diversity. We explore a simple yet effective strategy by adjusting the CFG values. $w^{\\prime}\\epsilon_{\\theta}(\\bar{\\mathbf{x}},t,c)+(1-w^{\\prime})\\epsilon_{\\theta}(\\mathbf{x},t,\\mathcal{D})$ , where $w^{\\prime}\\in(0.5,1]$ . The epsilon prediction is the convex combination of the conditional prediction and the unconditional prediction. ", "page_idx": 23}, {"type": "text", "text": "V Societal Impact ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "V.1 Positive Societal Impact ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We firmly believe that our work has a profound positive impact on society. While diffusion techniques excel in producing high-quality images and videos, their iterative inference process incurs significant computational and power costs. Our approach accelerates the inference of general diffusion models by up to 20 times or more, thus substantially reducing computational and power consumption. Moreover, it fosters enthusiasm among creators engaged in AI-generated content creation and lowers entry barriers. ", "page_idx": 23}, {"type": "text", "text": "V.2 Negative Societal Impact ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Addressing potential negative consequences, given the generative nature of the model, there\u2019s a risk of generating false or harmful content. ", "page_idx": 24}, {"type": "text", "text": "V.3 Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To mitigate this, we conduct harmful information detection on user-provided text inputs. If harmful prompts are detected, the generation process is halted. Additionally, our method, fortunately, builds upon the open-source Stable Diffusion model, which includes an internal safety checker for detecting harmful content. This feature significantly enhances our ability to prevent the generation of harmful content. ", "page_idx": 24}, {"type": "text", "text": "VI Implementation Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "VI.1 Training Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For the comparison of baselines, the training code for InstaFlow [31], SDXL-Turbo [49], SDTurbo [49], TCD [74], and SDXL-Lightning [27] are not open-sourced yet, so we only compare their open-source weights. ", "page_idx": 24}, {"type": "text", "text": "For LCM [35], CTM [21], and our method, they are all implemented by us and trained with the same configuration. ", "page_idx": 24}, {"type": "text", "text": "Specifically, for multi-step models, we trained LoRA with a rank of 64. For models based on SD v1-5, we used a learning rate of 5e-6, a batch size of 160, and trained for 5k iterations. For models based on SDXL, we used a learning rate of 5e-6, a batch size of 80, and trained for 10k iterations. We did not use EMA for LoRA training. ", "page_idx": 24}, {"type": "text", "text": "For single-step models, we followed the approach of SD-Turbo and SDXL-Lightning, training all parameters. For models based on SD v1-5, we used a learning rate of 5e-6, a batch size of 160, and trained for 10k iterations. For models based on SDXL, we used a learning rate of 1e-6, a batch size of 16, and trained for $50\\mathrm{k}$ iterations. We used EMA $_{=0.99}$ for training. ", "page_idx": 24}, {"type": "text", "text": "For CTM, we additionally learned a timestep embedding to indicate the target timestep. ", "page_idx": 24}, {"type": "text", "text": "For all training settings, we uniformly sample 50 timesteps from the 1000 timesteps of StableDiffusion and apply DDIM as the ODE solver. ", "page_idx": 24}, {"type": "text", "text": "VI.2 Pseudo Training Code ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "mtBmKqyqGS/tmp/6f3cd904a8af82d1ffcf60ffcaf7093623e562e89a63e667a0d6bed9a459ca22.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/b629f44de67523623b6fa435bbb1175699fb1f8dfab8c5788bfbb5b492f3e3c9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "VI.3 Decoupled Consistency Distillation For Video Generation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our video generation model follows the design space of most current text-to-video generation models, viewing videos as temporal stacks of images and inserting temporal blocks to a pre-trained text-toimage generation U-Net to accommodate 3D features of noisy video inputs. We term this process temporal inflation. ", "page_idx": 25}, {"type": "text", "text": "Video generation models are typically much more resource-consuming than image generation models. Also, the overall caption and visual quality of video datasets are generally inferior to those of image datasets. Therefore, we apply the decoupled consistency learning to ease the training burden of text-to-video generation, which was first proposed by the previous work AnimateLCM. To be specific, we first conduct phased consistency distillation on stable diffusion v1-5 to obtain the one-step text-toimage generation models. Then we apply the temporal inflation to adapt the text-to-image generation model for video generation. Eventually, we conduct phased consistency distillation on the video dataset. We observe an obvious training speed-up with this decoupled strategy. This allows us to train the state-of-the-art fast video generation models with only 8 A 800 GPUs. ", "page_idx": 25}, {"type": "text", "text": "VI.4 Discriminator Design of Image Generation and Video Generation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "VI.4.1 Image Generation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The overall discriminator design was greatly inspired by previous work StyleGAN-T [48], which showed that a pre-trained visual backbone can work as a great discriminator. For training the discriminator, we freeze the original weight of pre-trained visual backbones and insert light-weight Convolution-based discriminator heads for training. ", "page_idx": 25}, {"type": "text", "text": "Discriminator backbones. We consider two types of visual backbones as the discriminator backbone: pixel-based and latent-based. ", "page_idx": 25}, {"type": "text", "text": "We apply DINO [4] as the pixel-based backbone. To be specific, once obtaining the denoised latent code, we decode it through the VAE decoder to obtain the generated image. Then, we resize the generated image into the resolution that DINO trained with (e.g., $224\\times224)$ and feed it as the inputs of the DINO backbone. We extract the hidden features of the DINO backbone of different layers and feed them to the corresponding discriminator heads. Since DINO is trained in a selfsupervised manner and no texts are used for training. Therefore, it is necessary to incorporate the text embeddings in the discriminator heads for text-to-image generation. To achieve that, we use the embedding of [CLS] token in the last layer of the pre-trained CLIP text encoder, linearly map it to the same dimension of discriminator output, and conduct affine transformation to the discriminator output. However, the pixel-based backbones have inevitable drawbacks. Firstly, it requires mapping the latent code to a very high-dimensional image through the VAE decoder, which is much more costly compared to traditional diffusion training. Secondly, it can only accept the inputs of clean images, which poses challenges to PCM training, since we require the discrimination of inputs of intermediate noisy inputs. Thirdly, it requires resizing the images into the relatively low resolution it trained with. When applying this backbone design with high-dimensional image generation with Stable-Diffusion XL, we observe unsatisfactory results. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "We apply the U-Net of the pre-trained diffusion model as the latent-based backbone. It has several advantages compared to the pixel-based backbones. Firstly, trained on the latent space, the UNet possesses rich knowledge about latent space and can directly work as the discriminator at the latent space, avoiding costly decoding processes. Besides, it can accept the inputs of latent code at different timesteps, which aligns well with the design of PCM and can be applied for regularizing the consistency of all the intermediate distributions instead (i.e., $\\mathbb{P}_{t}(\\mathbf{x}))$ of distribution at the edge points (i.e., $\\mathbb{P}_{s_{m}}(\\mathbf{x}))$ ). Additionally, since the text information is already encoded in the U-Net backbone, we do not need to incorporate the text information in the discriminator head, which simplifies the discriminator design. We insert several randomly initialized lightweight simple discriminator heads after each block of the pre-trained U-Net. Each discriminator head consists of two lightweight convolution blocks connected with residuals. Each convolution block is composed of a convolution 2D layer, Group Normalization, and GeLU non-linear activation function. Then we apply a 2D point-wise convolution layer and set the output channel to 1, thus mapping the input latent feature to a 2D scalar value output map with the same spatial size. ", "page_idx": 26}, {"type": "text", "text": "We train our one-step text-to-image model on Stable Diffusion XL using these two choices of discriminator respectively, while keeping the other settings unchanged. Our experiments reveal that the latent-based discriminator not only reduces the training cost but also provides more visually compelling generation results. Additionally, note that the teacher diffusion model applied for phased consistency distillation is actually a pre-trained U-Net. And we freeze the parameters of U-Net for the training discriminator. Therefore, we can apply the same U-Net, which simultaneously works as the teacher diffusion model for numerical ODE solver computation and discriminator visual backbone. ", "page_idx": 26}, {"type": "text", "text": "VI.4.2 Video Generation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For the video discriminator, we mainly follow our design on the image discriminator. We use the same U-Net with temporal inflation as the visual backbone for video latent code as well as the teacher video diffusion model for phased consistency distillation. We still apply the 2D convolution layers as discriminator heads for each frame of hidden features extracted from the temporal inflated U-Net since we do not observe performance gain when using 3D convolutions. Note that the temporal relationships are already processed by the pre-trained temporal inflated U-Net, therefore using simple 2D convolution layers as discriminator heads is enough for supervision of the distribution of video inputs. ", "page_idx": 26}, {"type": "text", "text": "VII More Generation Results. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Prompt: \u201ca happy white man in black suit, sky, red tie, river, mountain, colourful, clouds, best view, fall\u201d ", "page_idx": 27}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/1c515294eb27f94e28158428ed10160fb638727aa26cde0850afdff7a9aa266a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Prompt: \u201ca astronaut walking on the moon\u201d ", "page_idx": 27}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/44ce7dcee43365fc530ef4a285517cb5ea0f63b29833c53021e00b3a52ac5bd7.jpg", "img_caption": ["Figure 10: PCM video generation results with Stable-Diffusion v1-5 under $1\\sim4$ inference steps. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Prompt: \u201cPhoto of a dramatic cliffside lighthouse in a storm, waves crashing, symbol of guidance and resilience\u201d ", "page_idx": 28}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/b8ebc27e8cf530d26d758231e86b6680546ee79e56de4a35463328f443430f10.jpg", "img_caption": ["Prompt: \u201cRAW photo, face portrait photo of beautiful 26 y.o woman, cute face, wearing black dress, happy face, hard shadows, cinematic shot, dramatic lighting\u201d "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/7c15ee70738ece656e0dd91a24105831de5868615ef823864cf961124b01de22.jpg", "img_caption": ["Figure 11: PCM video generation results with Stable-Diffusion v1-5 under $1\\sim4$ inference steps. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Prompt: \u201ca car running on the snowy road\u201d ", "page_idx": 29}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/8a9678a271ea7351ea15089818110bb127a7b671b27e549881e6000f64e7ea31.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Prompt: \u201ca monkey eating apple\u201d ", "text_level": 1, "page_idx": 29}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/a33e19cb9b9cb3d848f12340d2547a1bacf51192464884e85ace6a4cd415410c.jpg", "img_caption": ["Figure 12: PCM video generation results with Stable-Diffusion v1-5 under $1\\sim4$ inference steps. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Prompt: \u201cVincent vangogh style, painting, a boy, clouds in the sky\u201d ", "page_idx": 30}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/408c8d0d9479e36aee560d339c5390520913b95586847ddeadf5aa9f2a873538.jpg", "img_caption": ["Prompt: \u201cbonfire, wind, snow land\u201d "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/454d12304c5f111802fe22da7d08fe2d33596110ec47b2a8d4b33901651b3e83.jpg", "img_caption": ["Figure 13: PCM video generation results with Stable-Diffusion v1-5 under $1\\sim4$ inference steps. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Prompt: ${}^{\\prime\\prime}\\partial$ lion on the grass land\u201d ", "page_idx": 31}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/92aa4972d4c161451318ff4dfe100c6e573386fb6cf4f0db886c73e4ce16dfec.jpg", "img_caption": ["Figure 14: PCM video generation results comparison with AnimateLCM under $1\\sim4$ inference steps. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Prompt: \u201criver reflecting mountain\u201d ", "page_idx": 32}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/ba2711b49672f419fc4763aa6c8e64dd322fa447174303ae720af26268152065.jpg", "img_caption": ["Figure 15: PCM video generation results comparison with AnimateLCM under $1\\sim4$ inference steps. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Prompt: \u201ca monkey living on the tree\u201d ", "page_idx": 33}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/a7ac406a5f2558bd80b233feb95d6567f304ac5e18100b9f7e7835995ce7e9e1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/6ad1c6bee9de3dfb3abda515a8dddfc3eb607f8567d56416c24ccfd11a0abbd5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Prompt: \u201cheart-like pink cloud in the sky\u201d ", "page_idx": 33}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/973609fe619e3b766e26950666ef31017a9fe5f73b65d82e72007674b509b07c.jpg", "img_caption": ["Figure 16: PCM generation results with Stable-Diffusion v1-5 under different inference steps. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/4cb0bff980a4440816e76c8a445274e48ea4a96d5aab47e47b323c6b21b2a783.jpg", "img_caption": ["Figure 17: PCM generation results with Stable-Diffusion v1-5 under different inference steps. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Prompt: \u201ca blue bed in the bottle, surrounded by clouds\u201d ", "page_idx": 35}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/eb6a7246f09ff782393b50887c47c8c2296eff2fa1fa98416fb7aa42f4a4eb22.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Prompt: \u201ca cat near the sea\u201d ", "page_idx": 35}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/77a868e11828f9e8430e4de50c6e8761647d7d0b2d1fbbd0eabec71ad9607d90.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Prompt: \u201ca girl with white dress\u201d ", "page_idx": 35}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/87b5e202f68c69906a7fcce67d08e2f1dc49ff4746e2e79d1fc12ddffd7f95be.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Prompt: \u201can ice made lion\u201d ", "page_idx": 35}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/4eb222de56ed9bfca2974b063f3a8097baffecca2cacf847ec683c1f4529f149.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Prompt: \u201cSon Goku made of marble\u201d ", "text_level": 1, "page_idx": 35}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/330e690450c8aa2e91d00d00b35cfb3a449b66f5ca52fc47e19fec5d01f74619.jpg", "img_caption": ["Figure 18: PCM generation results with Stable-Diffusion XL under different inference steps. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "2-step ", "page_idx": 36}, {"type": "text", "text": "Prompt: \u201ca pink dog wearing blue sunglasses\u201d ", "page_idx": 36}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/ae3eceeb998476ad18956cbc0810ec80cbeeace42cef5edb27b620947fda48dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Prompt: \u201ca dream island, surrounded by sea, with bird flying on the sky\u201d ", "page_idx": 36}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/73fa82389ecee2f0ba6a9861c3a8799fd9d1edbe062fa41391d30d3a8b49e06b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Prompt: \u201cphotography of a man kissing woman, vangogh style\u201d ", "page_idx": 36}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/679289ff4e393f529c71ac507bffa181322f129f80adb6870ba44ac3340d7333.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Prompt: \u201ca pikachu made of wood\u201d ", "page_idx": 36}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/bd088a4c6a39e49a7b94ab9b7e720523769cf654f0c8c4463c22f3afe0d5f296.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Prompt: \u201cfirework in the night, best quality, modern city, river reflecting\u201d ", "text_level": 1, "page_idx": 36}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/f0e76bde1ce62a34b7c83dbf51c1da907e422643e191ea0ab025adec5c60b2c1.jpg", "img_caption": ["Figure 19: PCM generation results with Stable-Diffusion XL under different inference steps. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/2193c0969eadab57753b5ddcaf7aa07d440181448b174b9ccf53e9f26bad53c6.jpg", "img_caption": ["Figure 20: PCM generation 1-step results with Stable-Diffusion v1-5. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/98dd52740efa3fd76c966b6f295a4d22820fbba0682e57fe9917dd2bcabbbab7.jpg", "img_caption": ["Figure 21: PCM generation 2-step results with Stable-Diffusion v1-5. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/ea8ca644bac659cab636d405a6e182d194b524d1bf237a18fa39984590541d89.jpg", "img_caption": ["Figure 22: PCM generation 4-step results with Stable-Diffusion v1-5. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/fd2b355d4aa8b071c1b861b68fcf619e34920d0c2533b7a61b2170a99b00da6f.jpg", "img_caption": ["Figure 23: PCM generation 8-step results with Stable-Diffusion v1-5. "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/c8a9061ef8a2c95c690de76fc2ef72705e086293fef746e64bd946d2b4376f11.jpg", "img_caption": ["Figure 24: PCM generation 16-step results with Stable-Diffusion v1-5. "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/8d2bcd1f432f80a4be496842634d6e419669942cefc5ca9a74185585cd898031.jpg", "img_caption": ["Figure 25: PCM generation 1-step results with Stable-Diffusion XL. "], "img_footnote": [], "page_idx": 42}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/cecadbda513fabd361da4eb67dbf8b26aed287e26c7ad1fc1219c0ac7843abf7.jpg", "img_caption": ["Figure 26: PCM generation 2-step results with Stable-Diffusion XL. "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/dabaf9097022749600f83f9c44b8e8f1a30bd759c290f0766ff9484ed32dcc20.jpg", "img_caption": ["Figure 27: PCM generation 4-step results with Stable-Diffusion XL. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/b4580e03b289a14cd8e7d016737e7b0a1f8782b3487c9a5b08fb04a6b48ce212.jpg", "img_caption": ["Figure 28: PCM generation 8-step results with Stable-Diffusion XL. "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/e8457b59a070a45db29f9726a722de6e0a95e8bb4335c93037e795b8b6837f7a.jpg", "img_caption": ["Figure 29: PCM generation 16-step results with Stable-Diffusion XL. "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/45ae2ad82688daa0ba6bdbfe05ec689621600f5dff3037f07ba64189b000c2ab.jpg", "img_caption": ["Figure 30: PCM generation 16-step results with Stable-Diffusion XL. "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/adec9d41b1658441d339a7f8406ebe9f986b10885723d87f1872b3804d096eea.jpg", "img_caption": ["Figure 31: Visual examples of ablation study on the proposed distribution consistency loss. Left: Results generated without the distribution consistency loss. Right: Results generated with the distribution consistency loss. "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/235b26df4aff10481dbf60a30f7841fe33bfdf00d98c2daaf18bc9f8b36c5a13.jpg", "img_caption": [], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/34843d88d71a20ee7afad5847adecf58daa51ec1ff96530cf75c9e0b3279544e.jpg", "img_caption": ["Figure 32: Visual examples of ablation study on the proposed way for stochastic sampling. Pure deterministic sampling algorithms sometimes bring artifacts in the generated results. Adding stochasticity to a certain degree can alleviate those artifacts. "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/ebaed915e03859de6e17571a6aea137ad491c6f72cb264128fff6781126f57b9.jpg", "img_caption": ["CFG value $=1.0$ . The entropy of color: 1.75 "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/84be153371b837511d9bb3a9a32c050111ff1ddc1f20b6b11fd8efc93e2c83f9.jpg", "img_caption": ["CFG value $=0.6$ . The entropy of color: 2.16 "], "img_footnote": [], "page_idx": 50}, {"type": "text", "text": "Figure 33: Visual examples of ablation study on the proposed strategy for promoting diversity. Upper: Batch samples generated with prompt \"a car\" with normal ${\\mathrm{CFG}}{=}1.0$ value in 4-step. Lower: Batch sample generated with prompt \"a car\" with our proposed strategy with ${\\mathrm{CFG}}{=}0.6$ . ", "page_idx": 50}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/b8cb22db5a0f3a29faa78308b614ac5a0391c6e4addde68dad8115547daf377d.jpg", "img_caption": ["CFG value $=$ 1.0. The entropy of color: 1.30 "], "img_footnote": [], "page_idx": 51}, {"type": "image", "img_path": "mtBmKqyqGS/tmp/6ce65ded2331b5ed4409d8815a60c6ca75043af6e6ee2f0a9219eea1da41786d.jpg", "img_caption": ["CFG value $=0.6$ . The entropy of color: 1.75 "], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "Figure 34: Visual examples of ablation study on the proposed strategy for promoting diversity. Upper: Batch samples generated with prompt \"a car\" with normal ${\\mathrm{CFG}}{=}1.0$ value in 4-step. Lower: Batch sample generated with prompt \"a car\" with our proposed strategy with ${\\mathrm{CFG}}{=}0.6$ . ", "page_idx": 51}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We have claimed in the abstract and introduction that, in this paper, we identify three key flaws in the current design of LCM. We investigate the reasons behind these limitations and propose the Phased Consistency Model (PCM), which generalizes the design space and addresses all identified limitations. Accordingly, in Lines 32 to 42 in the introduction of the main paper, we reveal the limitations of LCM, and in Sec. 3 we propose PCM. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 52}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We discuss the limitations of work in Sec. 5 ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 52}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: We present a complete proof in Sec. II. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 53}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We have carefully introduced the framework architecture in Sec. 3.1, the training strategy in Sec. 3.2 and Sec. 3.3. Also, we enclose the pseudo training code, the detailed framework design in the Appendix. Eventually, We will release the code and model of this paper. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 53}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 54}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: Code is available at https://github.com/G-U-N/ Phased-Consistency-Model. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 54}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We specify all the training and test details in Sec. VI, including the training and testing details in Sec. VI.1, the pseudo training scripts in Sec. VI.2, and additional details in Sec. VI.3 and Sec. VI.4. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 54}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 54}, {"type": "text", "text": "Answer: [No] ", "page_idx": 54}, {"type": "text", "text": "Justification: Error bars are not reported, because it would be too computationally expensive. However, we indeed present plenty of qualitative results to demonstrate the statistical significance of the experiments. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 55}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: We have clarified in the Abstract and Sec. III that our results are achieved by only 8 A800 GPUs. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 55}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 55}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The positive societal impacts are discussed in Sec. V.1, and the negative societal impacts are discussed in Sec. V.2. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 56}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Justification: Yes, we are the image generators, which naturally have a risk of generating harmful content. Therefore, in Sec. V.3, we introduce our solution to avoid harmful content. We will include the safeguards in our pipeline when releasing the code and model. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 56}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: All the assets used in this paper are public datasets, allowing for research use. Specifically, the used datasets include: ", "page_idx": 56}, {"type": "text", "text": "\u2022 CC3M \u2013 custom terms reading: The dataset may be freely used for any purpose. \u2022 WebVid-2M \u2013 custom terms reading: Non-Commercial research for 12 months, \u2022 COCO-2014 \u2013 Creative Commons Attribution 4.0 License. ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 57}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: We do not release new asserts in the submission. But we will release the code and model shortly. We will prepare documentation alongside the assets. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 57}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 57}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: We do not involve crowdsourcing, nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}]