[{"type": "text", "text": "GTBENCH: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jinhao Duan1\u2217Renming Zhang2\u2217 James Diffenderfer3 Bhavya Kailkhura3 Lichao Sun4 Elias Stengel-Eskin5 Mohit Bansal5 Tianlong Chen5,6,7\u2020 Kaidi $\\mathbf{X}\\mathbf{u}^{1\\dagger}$ 1Drexel University 2Boston University 3LLNL 4Lehigh University 5UNC Chapel Hill 6MIT 7Harvard University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs\u2019 reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBENCH, a languagedriven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we $\\pmb{\\mathrm{\\Omega}}$ Characterize the game-theoretic reasoning of LLMs; and $\\pmb{\\varphi}$ Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that $\\pmb{\\mathrm{\\Sigma}}$ LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; $\\pmb{\\varphi}$ Most open-source LLMs, e.g., CodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than commercial LLMs, e.g., GPT-4, in complex games, yet the recently released Llama-3-70b-Instruct makes up for this shortcoming. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. We further characterize the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games. Detailed error profiles are provided for a better understanding of LLMs\u2019 behavior. We hope our research provides standardized protocols and serves as a foundation to spur further explorations in the strategic reasoning of LLMs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) are increasingly being integrated into critical real-world applications, such as cybersecurity (Ameri et al., 2021; Aghaei et al., 2022), decision science (Jiang et al., 2023b), and finance (Wu et al., 2023). These areas involve advanced strategic thinking and logical reasoning skills, including the ability to foresee possible dangers and weaknesses (Yao et al., 2024b; Duan et al., 2024a), systematically examine difficulties, and make informed decisions based on provided evidence. However, evaluation environments that thoroughly assess these situations are not sufficiently explored. ", "page_idx": 0}, {"type": "text", "text": "There has been an emerging trend where LLMs are evaluated in various interactive role-playing environments, including collaborative environments such as CAMEL (Li et al., 2023), ReConcile (Chen et al., 2023), and competition environments such as Diplomacy (Bakhtin et al., 2022), Werewolf $\\mathrm{Xu}$ et al., 2023a), Avalon (Light et al., 2023; Stepputtis et al., 2023), multi-agent debate (Liang et al., 2023; Du et al., 2023; Chan et al., 2023; Xiong et al., 2023), board and card games (Duan et al., ", "page_idx": 0}, {"type": "image", "img_path": "ypggxVWIv2/tmp/8c179904f4543954e7f789b3ab30fba15d4064187138b3b491e538ac243088d8.jpg", "img_caption": ["Figure 1: The overall schematic of GTBENCH. There are three main components from right to left: Environments (c) for game hosting, observation providing, and action execution; Prompt Adapter (b) for converting observation to prompt and extracting actions from participants\u2019 generations; Participants (a) for reasoning and action generation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2024b). By engaging LLMs in simulated scenarios, role-playing-based environments offer useful potential for analyzing the cognitive reasoning abilities of LLMs. However, the extensive background and intricate details involved in role-play-based games dilute the pureness of logic and strategic reasoning that is typically found in game-theoretic tasks. Additionally, the evaluation is primarily verbal as it hinges on spoken or written exchanges between the LLMs. This could mask instances where LLMs might lack concrete reasoning abilities but navigate the scenario effectively through the proficient use of language. ", "page_idx": 1}, {"type": "text", "text": "Why are game-theoretic tasks unique and necessary for LLM reasoning evaluation? Gametheoretic tasks are typically conceptualized based on prevalent trade-offs and dilemmas manifesting in real-life scenarios and are designed to be easy to understand yet require difficult skills to be mastered. In contrast to the rich narrative contexts afforded in verbal- or role-playing-based games, e.g., Werewolf (Xu et al., 2023a) and Avalon (Light et al., 2023), the reality of game-theoretic games such as Chess and Go involve: $\\pmb{\\mathrm{\\Omega}}$ pure logic and strategic reasoning without the added complexity of backgrounds or character roles; $\\pmb{\\varphi}$ embracing rigorous rules with well-defined action/state space, which allow for an in-depth examination of the strategic reasoning of LLMs. ", "page_idx": 1}, {"type": "text", "text": "Hence, in order to spur more research in the LLM Game-Theoretic evaluation domain, we propose GTBENCH, an environment consisting of 10 widely recognized game-theoretic tasks, across a comprehensive taxonomy of games, e.g., complete- (Tic-Tac-Toe, Connect-4, Breakthrough) versus incomplete-information (Kuhn Poker, Liar\u2019s Dice) gaming, deterministic (Nim) versus probabilistic (Negotiation, Pig) gaming, static versus dynamic (Iterated Prisoner\u2019s Dilemma, Blind Auction) gaming. These environments require a variety of abilities including board strategy, collaboration, auction, and bidding. There are two key issues investigated in this paper: ", "page_idx": 1}, {"type": "text", "text": "Characterizing Strategic Reasoning of LLMs: How LLMs will perform when facing various gametheoretic scenarios? How do they perform compared to conventional solvers? How do essential factors, e.g., pertaining, parameter sizes, and reasoning methods, affect strategic reasoning? LLM-vs.-LLM Competitions as New Reasoning Evaluation: A new automated and adaptive benchmark that can be effective in evaluating reasoning errors even for future LLMs. ", "page_idx": 1}, {"type": "text", "text": "To address these crucial problems, we conduct experiments over two configurations: (a) LLMvs-Conventional where conventional solvers such as optimization- or search-based solvers, e.g., Monte-Carlo Tree Search (MCTS) (Chaslot et al., 2008), are taken as the opponent of LLMs; (b) LLM-vs.-LLM where two LLMs compete directly to reveal the reasoning limitations in an automated manner. We find that: \u278aLLMs almost always fail when playing against simple MCTS opponents in complete and deterministic gaming scenarios (Section 4.1), while $\\pmb{\\varphi}$ LLMs remain competitive in incomplete and probabilistic scenarios (Section 4.2); $\\pmb{\\wp}$ Code-pretraining beneftis game-theoretic reasoning, e.g., CodeLlama-34b-Instruct (Roziere et al., 2023) achieves comparable results as GPT3.5-turbo, and significantly outperforms Llama-2-70b-chat (Touvron et al., 2023) (Section 4.3); \u278d Advanced reasoning methods, such as Chain-of-Thought (CoT) (Wei et al., 2022), Self-Consistent CoT (SC-CoT) (Wang et al., 2022b), Tree-of-Thought (ToT) (Yao et al., 2024a) are not always helpful; $\\pmb{\\mathcal{\\Theta}}$ Most open-source LLMs are less competitive than commercial LLMs in games with complex rules and large action/state space, while the recently released Llama-3-70b-Instruct (Meta, 2024) makes up for this shortcoming. The interfaces of GTBENCH leaderboard can be found in Appendix A11. Our contributions can be summarized as the following: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 LLM Game-Theoretic Evaluation (GTBENCH): An LLM environment supporting 10 wellrecognized tasks across comprehensive game-theoretic taxonomy, is presented to spur future work for the community. The code and leaderboard will be public and continuously updated for future reasoning agents and LLMs. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Essential Factors for the Strategic Reasoning of LLMs: We investigate how essential factors, e.g., parameter size, code-pretraining, and reasoning methods, affect strategic reasoning. A detailed error profile is provided for a better understanding of LLMs\u2019 behaviors. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Characteriz the Game-Theoretic Properties of LLMs: We characterize distinct LLM behaviors when facing different game-theoretic scenarios, such as LLMs fail in complete-information and deterministic gaming yet remain competitive in probabilistic gaming. We further study the equilibrium and Pareto efficiency during the gameplay. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Background and Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "LLM-as-Agent Evaluation. Several studies have been conducted to measure the effectiveness of LLMs as agents in recent years. Hausknecht et al. (2020) carried out an extensive study to evaluate the performance of LLMs in interactive fiction games. Zhu et al. (2023) provides a valuable dataset for finetuning LLMs to improve usefulness in the strategic game Dungeons & Dragons. GRUE (Ramamurthy et al., 2023) uses reinforcement learning-based metrics to benchmark the performance of generation tasks in six different languages. Gandhi et al. (2023) test the use of LLMs as a broker with human contestants in the negotiation game \u201cDeal or No Deal\". A few studies have explored the use of text-based games as a means of facilitating learning in such environments. ALFWorld (Shridhar et al., 2020) introduced a novel virtual environment that allows agents to acquire learning in a text-based environment while executing in a visual environment. The environment was developed in conjunction with Building Understanding in Text world via Language for Embodied Reasoning (BUTLER) agent, which can acquire abstract text knowledge in the text world. Similarly, TextWorld (C\u00f4t\u00e9 et al., 2019) is introduced as an environment that enables RL agents to play text games. Wang et al. (2022a) proposed ScienceWorld, a benchmark used for evaluating agents\u2019 reasoning ability, and their findings showed that transformer-based models are not effective at reasoning in novel contexts. MTBench (Zheng et al., 2024) introduces LLM-as-a-Judge where GPT-4 (Achiam et al., 2023) is utilized as a judge to evaluate the quality of LLM generations. It indicates that GPT-4 shares close criteria as humans. There have been works evaluating LLMs in solving real-world tasks, such as graph reasoning (Besta et al., 2023), WebShop (Yao et al., 2022), AgentBench (Liu et al., 2023) for pragmatic missions, MINT (Wang et al., 2023b) for tool utilization. ", "page_idx": 2}, {"type": "text", "text": "Multiple LLMs-as-Agents in Gaming. A key research area is the competition and collaboration between LLMs. Many studies examine LLMs\u2019 strategic reasoning and performance, using evaluation frameworks to assess multiple LLM agents in individual games, such as: Social deduction or deception games (Xu et al., 2023a,b; O\u2019Gara, 2023; Light et al., 2023), diplomacy games (Mukobi et al., 2023; , FAIR), negotiation games (Abdelnabi et al., 2023; Davidson et al., 2023), coordination and cooperation games (Akata et al., 2023), and Minecraft (Gong et al., 2023; Wang et al., 2023a; Fan et al., 2022). These works not only provide evaluation frameworks for games and demonstrate the flexibility of LLMs to a variety of gaming tasks but some provide meaningful datasets for fine-tuning, policies for reinforcement learning to produce better strategies, or evaluate the strategic reasoning of LLMs. However, many of these standalone works quantify either individual or a subset of desirable strategic reasoning capabilities of LLMs, such as negotiation, deception, or coordination. Further, they often evaluate these capabilities for LLMs using one or two games which may produce less robust assurances of LLM abilities. ", "page_idx": 2}, {"type": "table", "img_path": "ypggxVWIv2/tmp/1aa3c84d366978db2cc9f24e50ebe4058ef5b4e26b432059049feff3f7fed051.jpg", "table_caption": ["Table 1: Game environments explored in GTBENCH. "], "table_footnote": ["\u2020 : Breakthrough has a slight first-player advantage which is not as significant as others. \u2021 : The iterated version of Prisoner\u2019s Dilemma allows participants access to the actions made by their opponents in the past rounds, achieving implicit collaboration. \u2020\u2020 : Inapplicable due to complex combination and dynamic environment. "], "page_idx": 3}, {"type": "text", "text": "We make an additional crucial contribution in this line of work by measuring strategic reasoning capabilities with games that are not found in the existing unified benchmark suites (Zhang et al., 2024), such as clembench (Chalamalasetti et al., 2023) focusing on conversational agents over non-zero-sum games and LMRL-Gym (Abdulhai et al., 2023) on verbal reinforcement learning tasks. (Chen et al., 2024) and Duan et al. (2024b) also proposes multi-agent strategic reasoning evaluation. However, they overlooked the analysis of LLM behaviors in response to different game-theoretic scenarios and their associated properties. Differently, GTBENCH seeks to provide a unified suite of games that are carefully curated to (1) evaluate a comprehensive collection of strategic reasoning abilities for a given agent and (2) enable competition-based scenarios (i.e., LLM agent-1 vs LLM agent-2) allowing for competition-based comparisons of strategic reasoning capabilities by LLM-based agents. ", "page_idx": 3}, {"type": "text", "text": "2.2 Problem Definition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notation: Gameplay. We formulate the gameplay as a Markov Decision Process $(S,{\\mathcal{A}},{\\mathcal{M}},{\\mathcal{O}})$ under a given game environment, among the alternating interaction of two participants. This process composes of an infinite state space $\\boldsymbol{S}$ , an infinite action space $\\boldsymbol{\\mathcal{A}}$ , the participants $\\bar{\\mathcal{M}}=\\{\\mathcal{M}_{1}^{\\bar{\\}},\\mathcal{M}_{2}\\}$ , and an observation space $\\scriptscriptstyle\\mathcal{O}$ . Considering the decision of $\\mathcal{M}_{i}\\left(i=1,2\\right)$ at the $t$ -th step of the process, we denote by $s_{t}\\in\\mathcal S$ the state that $\\mathcal{M}_{i}$ are placed and $o_{t}\\in\\mathcal{O}$ the observation that $\\mathcal{M}_{i}$ are observing. We assume $\\mathcal{M}_{i}$ follows policy $\\pi_{\\theta_{i}}\\big(a_{t}|s_{t},o_{t}\\big)$ for state transition $\\mathcal T:S\\times A\\rightarrow S.$ , where $a_{t}\\in\\mathcal A$ is the action sampled by $\\pi_{\\theta_{i}}$ under conditions $s_{t}$ and $o_{t}$ . $\\theta_{i}$ is determined by the implementation by $\\mathcal{M}_{i}$ , e.g., optimization-based solver, LLM-driven agents, which will be discussed in Section 3.2 in detail. In this way, the two-participate gameplay can be represented as $\\left(s_{0},a_{0},s_{1},a_{1},s_{2},\\cdot\\cdot\\cdot\\right,s_{n}\\right)$ , where $s_{0}$ is the initial state and $s_{n}$ is a terminal state, i.e., end of the game. The progress is driven by the alternating execution of actions sampled by participants. Please refer to Section 3.1 and Appendix A2 for all the supported games with the corresponding actions and observations. ", "page_idx": 3}, {"type": "text", "text": "Evaluation Metric: Normalized Relative Advantage. We introduce Normalized Relative Advantage (NRA), denoted $N R A(\\mathcal{M}_{i},\\mathcal{M}_{o},f_{s})$ , to measure to relative advantage of $\\mathcal{M}_{i}$ when competing against $\\mathcal{M}_{o}$ , under the score calculation $f_{s}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nN R A(\\mathcal{M}_{i},\\mathcal{M}_{o},f_{s})=\\frac{\\sum_{m}f_{s}(\\mathcal{M}_{i},m)-\\sum_{m}f_{s}(\\mathcal{M}_{o},m)}{\\sum_{m}f_{s}(\\mathcal{M}_{i},m)+\\sum_{m}f_{s}(\\mathcal{M}_{o},m)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f_{s}(\\mathcal{M}_{i},m)$ refers to the score earned by $\\mathcal{M}_{i}$ at the $m$ -th match $1\\leq m\\leq K,K$ is the number of performed matches): ", "page_idx": 3}, {"type": "text", "text": "\u2022 For zero-sum games, e.g., Tic-Tac-Toe, ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{s}(M_{i},m)=\\left\\{\\!\\!\\begin{array}{l l}{1,}&{\\mathrm{if}\\,\\mathcal{M}_{i}\\mathrm{\\,wins\\at\\,the\\}m\u2013\\mathrm{th\\,match}}\\\\ {0,}&{\\mathrm{if}\\,\\mathcal{M}_{i}\\mathrm{\\,loses\\at\\,the\\}m\u2013\\mathrm{th\\,match}}\\\\ {0.5,}&{\\mathrm{if}\\,\\mathcal{M}_{i}\\mathrm{\\,and\\,}\\mathcal{M}_{o}\\mathrm{\\,achieve\\a\\,draw}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "\u2022 For non-zero-sum games, e.g., Blind Auction, $f_{s}(M_{i},m)$ is the rewards earned by $\\mathcal{M}_{i}$ at the $m$ -th match. ", "page_idx": 3}, {"type": "text", "text": "$N R A(\\mathcal{M}_{i},\\mathcal{M}_{o},f_{s})$ is naturally normalized to $[-1,1]$ , providing an interpretable meaning regarding the performance of $\\mathcal{M}_{i}$ : $N R A({\\mathcal{M}}_{i},{\\mathcal{M}}_{o},f_{s})>\\dot{0}$ means $\\mathcal{M}_{i}$ is better than $\\mathcal{M}_{o};N R A(\\mathcal{M}_{i},\\mathcal{M}_{o},f_{s})<$ 0 means $\\mathcal{M}_{i}$ is worse than $\\mathcal{M}_{o};N R A(\\mathcal{M}_{i},\\mathcal{M}_{o},f_{s})=0$ means $\\mathcal{M}_{i}$ is as competitive as $\\mathcal{M}_{o}$ . ", "page_idx": 3}, {"type": "text", "text": "Evaluation Metric: Elo Rating. Following the conventional rating mechanism in the real world, e.g., Chess, we employ the popular Elo Rating (Elo, 1960) for calculating the relative skill levels of players in zero-sum games. Please refer to Appendix A7 for more details of Elo rating. ", "page_idx": 4}, {"type": "text", "text": "3 GTBENCH: Game-Theoretic Evaluation of LLMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "GTBENCH is a language-driven environment, making participating agents compete against each other in a game-theoretic manner. It is designed to be flexible and extensible, providing unified interfaces to participants and games, and supporting various multi-turn-based games which can be extended in the future. The overall framework is presented in Figure 1. There are three main components: Environment, Prompt Adapter, and Participant. Please refer to Appendix A1 for a detailed introduction of each component. ", "page_idx": 4}, {"type": "text", "text": "3.1 Taxonomy of Game-Theoretic Tasks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The chosen tasks and their detailed configurations are presented in Table 1. To comply with the common taxonomy (Lanctot et al., 2019) of game-theoretic tasks and provide diverse gaming scenarios, GTBENCH supports 10 different gaming environments, including Tic-Tac-Toe, Connect-4, Kuhn Poker, Breakthrough, Liar\u2019s Dice, Blind Auction, Negotiation, Nim, Pig, Iterated Prisoner\u2019s Dilemma, covering 6 mainstream game-theoretic configurations, including complete- and incomplete-information gaming, dynamic and static gaming, and probabilistic and deterministic gaming. The preferred abilities of each game could be characterized as the combination of board strategy, bids, collaboration, bluff, and math. Please refer to Appendix A2.1 for the rules of each game and Appendix A2.2 for an explanation of game-theoretic taxonomy. ", "page_idx": 4}, {"type": "text", "text": "3.2 Participants and Protocols ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Conventional Agents output actions through a conventional optimization or searching process. To provide fair comparisons, we employ the powerful Monte Carol Tree Search (MCTS) (Chaslot et al., 2008) as the conventional agent for most of the games, with the number of simulations as 1000. Since Iterated Prisoner $^,\\mathtt{s}$ Dilemma is dynamic gaming with very limited action space, i.e., ${<}\\mathrm{TESTIFY}{>}$ or ${}\\angle\\mathrm{SILENT}{}>$ , we utilize the more popular Tit-for-Tat (Axelrod, 1981) strategy, which simply repeating the opponent\u2019s last action, as the conventional agent. We also include Random Agent that randomly selects action at each turn, serving as a baseline and sanity check. Please refer to Appendix A3.1 for more details about MCTS Agent and Tit-for-Tat Agent. ", "page_idx": 4}, {"type": "text", "text": "LLM-Driven Reasoning Agent consists of backbone LLMs and reasoning paradigms. For reasoning schemes, we consider the following reasoning paradigms as they are widely known to be effective for general reasoning tasks: \u278aPrompt: Directly Prompt LLMs to generate responses, without additional reasoning steps; \u278bChain-of-Thought (CoT) (Wei et al., 2022): CoT Agent prompts LLMs by thinking step by step; \u278cSelf-Consistent CoT (Wang et al., 2022b): SC-CoT Agent prompts LLMs by generating multiple step-by-step thinking trajectories and performing majority voting to get the final response. The number of trajectories is set to 5 in this paper; \u278dTree-of-Thought (ToT) (Yao et al., 2024a): ToT Agent prompts LLMs to generate responses by incorporating exploration and deliberate decision-making, e.g., self-evaluation. The number of sequences for both answer generation and answer evaluations is set to 3. ", "page_idx": 4}, {"type": "text", "text": "Prompt Templates. Prompts are designed to be modular, consisting of four individual components: System Prompt, Head Prompt, Observation Prompt, and Reasoning Prompt. Reasoning prompts, e.g., CoT/ToT, are designed to only focus on instructing LLM how to think, regardless of the game environment. Thus, they could be automatically adapted when adding a new game. Please refer to Appendix A5 for the detailed prompts and observations for each game and agent. ", "page_idx": 4}, {"type": "text", "text": "Sanity Check. We provide the task completion rates of all the LLMs and reasoning agents in Appendix A5.6. We show that all the LLM agents achieve $\\ge90\\%$ completion rate, indicating that the prompts are properly configured and LLMs are capable of following instructions to finish the game. ", "page_idx": 4}, {"type": "image", "img_path": "ypggxVWIv2/tmp/dbf0e6095b775099ee03cc8524f5fe4c6ea10d622ead16c0c27ca89db8cbb245.jpg", "img_caption": ["Figure 2: The NRA of state-of-the-art LLM-driven reasoning agents when against MCTS Agents and Random Agents, over complete and deterministic scenarios. Red and gray lines mean the maximum NRA achieved by LLM agents. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "ypggxVWIv2/tmp/168ebd030b97c725afd5ccc8ae482d21afbc2aaaa91027251afe9881f6b31010.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: The game-wise NRA of LLMs when against MCTS/TfT Agents and Random Agents, over incomplete and probabilistic scenarios. Error bars are obtained over different reasoning methods. Green and gray lines mean the maximum NRA achieved by LLM agents. ", "page_idx": 5}, {"type": "text", "text": "4 Are LLMs Capable of Strategic Reasoning? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate the strategic reasoning capabilities of LLMs by conducting experiments among conventional solvers and LLM-driven agents. ", "page_idx": 5}, {"type": "text", "text": "Experimental Settings. We consider well-recognized LLMs such as commercial LLMs: GPT3.5-turbo-1106 and GPT-4-0613 (Achiam et al., 2023), and open-source LLMs: Llama-3-70bInstruct (Meta, 2024), Deepseek-LLM-67b-chat (Bi et al., 2024), Llama-2-70b-chat (Touvron et al., 2023), CodeLlama (Roziere et al., 2023), and Mistral-7b-Orca (Jiang et al., 2023a; Mukherjee et al., 2023). For all the LLMs, the temperature is set to 0.2 and the max number of generated tokens is 1024. For each competition, we run 50 valid matches. The final performance is measured by the averaged NRA over the 50 valid matches. To mitigate the first-player advantage, we have each participant take the first turn in 25 matches. ", "page_idx": 5}, {"type": "text", "text": "4.1 Complete and Deterministic Gaming ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "There are four complete and deterministic tasks supported in GTBENCH: Tic-Tac-Toe, Connect-4, Breakthrough, and Nim. We compare LLM-driven agents with Random Agent and MCTS Agent. Results are summarized in Figure 2. In general, we show that all LLMs achieve substantial relative advantages when competing against the Random Agent. Among all the agents, GPT-4 w/ CoT reasoning achieves the highest NRA. For open-source LLMs, Llama-3-70b-Instruct outperforms other open-source LLMs, achieving comparable capabilities as GPT-4. ", "page_idx": 5}, {"type": "text", "text": "However, when competing against the MCTS Agent, all the LLM agents equipped with various reasoning methods achieve NRA as $-1$ , meaning that LLM agents can barely win even a single match. This is because for board games with moderate action/state space such as the four involved complete and deterministic games in GTBENCH, MCTS agents with a sufficient number of simulations can achieve near-optimal strategies. Consequently, LLMs are not competitive in complete and deterministic games. ", "page_idx": 5}, {"type": "text", "text": "There are five probabilistic game-theoretic gaming tasks: Kuhn Poker, Liar\u2019s Dice, Blind Auction, Negotiation, Pig, and one dynamic task: Iterated Prisoner\u2019s Dilemma. We group these games together as they all involve stochasticity in the gameplay, which is essentially different from complete and deterministic games. The Random Agent as the opponent is omitted for both Negotiation and Iterated Prisoner\u2019s Dilemma because the Random Agent rarely chooses to collaborate, resulting ", "page_idx": 6}, {"type": "table", "img_path": "ypggxVWIv2/tmp/a50812345bd3ec1db7bcd01efc2661ae31d93a8ec22123e796b1d5b4f2f203d1.jpg", "table_caption": ["Table 2: Code-pretraining beneftis strategic reasoning. Gray rows are code-pretrained LLMs. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "in meaningless evaluation. Results are summarized in Figure 3. When competing against the MCTS Agent, it is shown that Liar\u2019s Dice shares a similar trend as the complete and deterministic scenarios (Figure 2), where LLM-driven agents achieve near $-1$ NRA. This is because the 2-player Liar\u2019s Dice has very limited stochasticity, making the gameplay tend to be complete information. For other tasks, we found that LLMs do not always fail. We observe that the NRA of LLM agents is close to 0 over all the tasks, indicating that they are equally competitive as conventional solvers or even better (e.g., Kuhn Poker where GPT-4 outperforms MCTS Agent). ", "page_idx": 6}, {"type": "text", "text": "4.3 LLM-vs.-LLM Competition ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We investigate whether popular LLMs remain competitive in game-theoretic scenarios. Specifically, we take GPT-3.5-turbo with Prompt Agent as the common opponent and make other LLM-driven agents compete against it. Please refer to Figure A6 for the full leaderboard evaluated by NRA. The Elo rating results are placed in Table 6. In general, GPT-4 is the most powerful LLM in strategic reasoning among all the examined LLMs. Moreover, Llama-3-70b-Instruct achieves comparable performances as GPT-4 and outperforms GPT-3.5-turbo. Here we break the results into 3 takeaways: ", "page_idx": 6}, {"type": "text", "text": "Code-Pretraining Benefits Game-Theoretic Tasks. In Table 2, we show code-pretrained LLMs, e.g., CodeLlama-34b-Instruct and Deepseek-Coder- $.6.7\\mathtt{b}$ -Instruct, significantly out", "page_idx": 6}, {"type": "image", "img_path": "ypggxVWIv2/tmp/dd6fd0690ad638ed98ba5d39470b687f91f25f556aa23ac363fe9059210c9545.jpg", "img_caption": ["Figure 4: The NRA of LLM agents when competing against Random Agent. Advanced reasoning does not always result in better results. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "perform larger chat LLMs, e.g., Llama-2-70b-chat and Deepseek-LLM-67b-chat. These codepretrained LLMs have less than half of the parameters, suggesting that code-pretraining benefits game-theoretic tasks. This verifies recent discoveries where code-pretraining beneftis logical reasoning (Madaan et al., 2022; Liang et al., 2022; Ma et al., 2023). ", "page_idx": 6}, {"type": "text", "text": "Advanced Reasoning Methods Do Not Always Help. We observe that advanced reasoning methods may lead to worse results in game-theoretic scenarios. To make it more clear, we present the averaged NRA obtained by reasoning methods across different LLMs when against Random Agent in Figure 4. In general, only Mistral-7b-Orca has a substantial improvement when equipped with CoT reasoning while advanced reasoning leads to worse results for other LLMs. ", "page_idx": 6}, {"type": "text", "text": "In Table 3, we present the results when against GPT3.5-turbo w/ Prompt Agent. We show that advanced reasoning benefits powerful LLMs, e.g., GPT-3.5- turbo, while it results in worse results for other LLMs. It suggests that advanced reasoning is a double-edged sword: $\\pmb{\\mathrm{\\Omega}}$ powerful LLMs are capable of leveraging advanced reasoning to achieve better results; $\\pmb{\\varphi}$ advanced reasoning may also impose reasoning errors and risks during the inference of ordinary LLMs. In Appendix A8, we further examine five different CoT strategies over the GPT-3.5-turbo model to mitigate the effect brought by prompt sensitivity, along with some failure cases presented. These CoT prompts resulting in different performances are all worse than the naive Prompt Agent. ", "page_idx": 6}, {"type": "table", "img_path": "ypggxVWIv2/tmp/e1c4ac61ed272b76ece836e0b6cc3acd6abb639740763eb251568b905cb3afcd.jpg", "table_caption": ["Table 3: The NRA of LLM agents w/ CoT reasoning. Cyan cells mean CoT results in better performance. Magenta cells mean CoT results in worse performance. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Most Open-source LLMs are Less Competitive than Commercial LLMs in Complex Games. We observe that most of open-source LLMs such as Llama-2-70b-chat and CodeLlama-34bInstruct are not good at games with complex rules and board states. In Table 4, we present the average NRA when in", "page_idx": 7}, {"type": "table", "img_path": "ypggxVWIv2/tmp/bd183545871b1e1e1e9e61eb0dc38228372d34603304ff12b6ca204142350339.jpg", "table_caption": ["Table 4: The average NRA of LLM-driven agents when Breakthrough is included and excluded. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "cluding and excluding Breakthrough3. It is shown that both Llama-2-70b-chat and CodeLlama34b-Instruct fail in Breakthrough, resulting in worse NRA scores than GPT-4. However, we found that the recently released Llama-3-70b-Instruct (Meta, 2024) has a significant performance in Breakthrough. This indicates that open-source LLMs achieve comparable capabilities when dealing with complex tasks and environments as commercial LLMs. ", "page_idx": 7}, {"type": "text", "text": "4.4 Error Profiles ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We introduce the most prevalent mistake patterns observed across different games, comprising Misinterpretation, Factual Inaccuracies, Overconfidence, Calculation Mistakes, and Endgame: ", "page_idx": 7}, {"type": "text", "text": "Misinterpretation denotes the misinterpretation of the game\u2019s current state by LLMs, including errors like misattributing piece ownership and failing to recognize vacant spots on the board. Factual Errors refer to situations where the ", "page_idx": 7}, {"type": "table", "img_path": "ypggxVWIv2/tmp/36ae29fc44cb62c9057ab03cf19ad722569d2c16c4e2094bf9e751e39b8c3ab7.jpg", "table_caption": ["Table 5: Quantitative results of error patterns. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "player has a reasonable plan but their actions do not align with their plan. For instance, in Breakthrough, GPT-4 w/ CoT agent plans to fend off frontal attacks by the opponent, which is reasonable. However, it takes rear pieces to achieve that, which is impossible. Over-confidence describes a scenario where a player overlooks potential risks in pursuit of greater rewards. Calculation Errors refer to errors that occur in arithmetic, such as calculating XOR in Nim. Endgame Misdetection means a failure to recognize immediate win/lose situations, e.g., a player fails to recognize a potential winning move. Demonstrations of each mistake pattern are presented in Appendix A9. ", "page_idx": 7}, {"type": "text", "text": "In Table 5, we present the quantitative results regarding these error patterns. It is obtained from GPT-4 w/ CoT agent when playing against conventional solvers, e.g., MCTS/TfT agent, as the opponent. We manually examined a total of 157 turns (50 matches, with 5 turns per match). We observe that LLM agents are capable of generating reasonable planning/strategies. However, they have difficulties in selecting the correct actions to align with their thoughts. Also, LLMs miss endgame situations, leading to a failure to recognize winning and losing moves. ", "page_idx": 7}, {"type": "table", "img_path": "ypggxVWIv2/tmp/1d15d74721b61be4e335e3f11d299f69b06aaf2afe3cebafb3f56a0788ec25a6.jpg", "table_caption": ["Table 6: The Elo rating results of LLM-vs.-LLM experiments. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ypggxVWIv2/tmp/116b8cd457f96a8a23751d27a150d5a42bbc918abd42d424f9b40d7bac1669a3.jpg", "img_caption": ["Figure 5: Game-theoretic properties. The results are obtained when competing against GPT-3.5-turbo w/ Prompt Agent as the opponent. In (b), each dot $(x,y)$ represents an agreement in a resource distribution with Player 1 obtaining reward $x$ and Player 2 obtaining reward $y$ . In (c), the system reward is calculated by the sum of the payoffs of all players. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 The Game-Theoretic Properties of LLMs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Nash Equilibrium with Regret. In game theory, being close to a Nash Equilibrium (Nash Jr, 1950) indicates that the strategies chosen by the players are near to optimal. It has been popular to approximate Nash Equilibrium with Regret4 (Johanson et al., 2012; Nisan and Noti, 2017; Zinkevich et al., 2007). In Figure 5a, we present the regret values of LLMs on Blind Auction and Iterated Prisoner\u2019s Dilemma. Please refer to Appendix A10 for how regret values are calculated for these two tasks. For Blind Auction, GPT-4 shows lower Regret, indicating achieving closer to optimal solutions than other LLMs. However, in Iterated Prisoner\u2019s Dilemma, CodeLlama34b-Instruct exhibits lower regret compared to GPT-4. Through human examination, we found that this is because GPT-4 tends to <Silent> more frequently, whereas Codellama has a significantly higher probability of $<$ Testify $>$ . This discrepancy may be due to the human preference alignment in GPT-4, such as a higher emphasis on morality (Pan et al., 2023) or maximizing system reward5, which makes GPT-4 less likely to <Testify $>$ . ", "page_idx": 8}, {"type": "text", "text": "Pareto Efficiency. We study Pareto Efficiency in two games: Negotiation and (Iterated) Prisoner\u2019s Dilemma. In Figure 5b, we count all agreements reached by participants and record the values attributed to each based on the agreed division. Most agreements result in substantial values for both participants, though some LLMs, like Llama-2-70b-chat and CodeLlama-34b-Instruct, may accept unfair resource divisions. In contrast, GPT-4 and Mistral struggle to reach agreements and tend to negotiate for Pareto improvements. A repeated game is a standard game that is played multiple times by the same players, with each player is able to observe the history of past plays (Aumann et al., 1995; Akata et al., 2023). In Figure 5c, we investigate the Pareto Improvement in Iterated Prisoner\u2019s Dilemma and ordinary Prisoner\u2019s Dilemma, i.e., each round is played individually. The Pareto Improvement is observed in the repeated-game scenario during the rounds, indicating that LLMs are capable of leveraging history to adjust their strategies. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work investigated LLMs\u2019 strategic and logical reasoning abilities under competitive scenarios. To achieve this, we created a broad evaluation scope by considering various classic and LLMbased gaming agents and 10 representative games. We conducted the benchmark study of gametheoretic evaluations for LLMs, shedding light on their reasoning performance. Our extensive evaluations revealed insightful LLMs\u2019 gaming behavior, such as their intrinsic failure in complete and deterministic games, impressive reasoning in incomplete and probabilistic games, and beneftiing from code-generation pertaining and appropriate prompt designs. ", "page_idx": 8}, {"type": "text", "text": "Limitations This research prompts LLMs to generate actions regarding various game scenarios, relying on pre-defined prompt templates. Thus, the results may suffer from certain variances introduced by prompt sensitivities. Although the introduced games are popular, their actions/state space is limited, which may not be well-distinguished for LLMs in the same skill levels. The generated actions may be illegal due to the incapabilities of the following instructions. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Impact Statements This paper examines the game-theoretic task proficiency of AI models. We acknowledge concerns about models becoming autonomous entities with their own objectives, especially in deception or negotiation scenarios. It\u2019s important to note that our research measures the current capabilities of models, rather than enhancing their abilities. We do not train AI models to be competent in game theory tasks or to bluff or defect. Instead, we assess existing competencies, contributing to a deeper understanding that can inform innovative measures against potential risks. We believe our work paves the way for responsible and effective AI safety. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract No. DE- AC52-07NA27344 and was supported by the LLNL LDRD Program under Project No. 23-ERD-030 and 24-ERD-058. This work was partially supported by the NSF award FMitF-2319242. It was also partially supported by NSF-AI Engage Institute DRL-2112635 and DARPA MCS Grant N66001-19-2-4031. The views, opinions, and/or findings contained in this article are those of the authors and not of the funding agency. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Sch\u00f6nherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games, 2023. ", "page_idx": 9}, {"type": "text", "text": "Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models, 2023.   \nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nEhsan Aghaei, Xi Niu, Waseem Shadid, and Ehab Al-Shaer. Securebert: A domain-specific language model for cybersecurity. In International Conference on Security and Privacy in Communication Systems, pages 39\u201356. Springer, 2022.   \nElif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. Playing repeated games with large language models, 2023.   \nKimia Ameri, Michael Hempel, Hamid Sharif, Juan Lopez Jr, and Kalyan Perumalla. Cybert: Cybersecurity claim classification by fine-tuning the bert language model. Journal of Cybersecurity and Privacy, 1(4):615\u2013637, 2021.   \nRobert J Aumann, Michael Maschler, and Richard E Stearns. Repeated games with incomplete information. MIT press, 1995.   \nRobert Axelrod. The emergence of cooperation among egoists. American political science review, 75 (2):306\u2013318, 1981.   \nAnton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sandra Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David J. Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378:1067 \u2013 1074, 2022.   \nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023. ", "page_idx": 9}, {"type": "text", "text": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. ", "page_idx": 10}, {"type": "text", "text": "Kranti Chalamalasetti, Jana G\u00f6tze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, and David Schlangen. clembench: Using game play to evaluate chat-optimized language models as conversational agents. arXiv preprint arXiv:2305.13455, 2023. ", "page_idx": 10}, {"type": "text", "text": "Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023. ", "page_idx": 10}, {"type": "text", "text": "Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck. Monte-carlo tree search: A new framework for game ai. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pages 216\u2013217, 2008. ", "page_idx": 10}, {"type": "text", "text": "Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, and Lijie Wen. Llmarena: Assessing capabilities of large language models in dynamic multi-agent environments. arXiv preprint arXiv:2402.16499, 2024. ", "page_idx": 10}, {"type": "text", "text": "Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023. ", "page_idx": 10}, {"type": "text", "text": "Marc-Alexandre C\u00f4t\u00e9, Akos K\u00e1d\u00e1r, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pages 41\u201375. Springer, 2019. ", "page_idx": 10}, {"type": "text", "text": "Tim Ruben Davidson, Veniamin Veselovsky, Michal Kosinski, and Robert West. Evaluating language models through negotiations. In The Twelfth International Conference on Learning Representations, 2023. ", "page_idx": 10}, {"type": "text", "text": "Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023. ", "page_idx": 10}, {"type": "text", "text": "Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5050\u20135063, 2024a. ", "page_idx": 10}, {"type": "text", "text": "Jinhao Duan, Shiqi Wang, James Diffenderfer, Lichao Sun, Tianlong Chen, Bhavya Kailkhura, and Kaidi Xu. Reta: Recursively thinking ahead to improve the strategic reasoning of large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 2232\u20132246, 2024b. ", "page_idx": 10}, {"type": "text", "text": "Arpad Elo. Elo rating system. https://en.wikipedia.org/wiki/Elo_rating_system, 1960. ", "page_idx": 10}, {"type": "text", "text": "Meta Fundamental AI Research Diplomacy Team (FAIR)\u2020, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624): 1067\u20131074, 2022. ", "page_idx": 10}, {"type": "text", "text": "Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35: 18343\u201318362, 2022. ", "page_idx": 10}, {"type": "text", "text": "Kanishk Gandhi, Dorsa Sadigh, and Noah D. Goodman. Strategic reasoning with language models, 2023.   \nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. Mindagent: Emergent gaming interaction, 2023.   \nMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C\u00f4t\u00e9, and Xingdi Yuan. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 7903\u20137910, 2020.   \nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a.   \nHaitao Jiang, Lin Ge, Yuhe Gao, Jianian Wang, and Rui Song. Large language model for causal decision making. arXiv preprint arXiv:2312.17122, 2023b.   \nMichael Johanson, Nolan Bard, Marc Lanctot, Richard G Gibson, and Michael Bowling. Efficient nash equilibrium approximation through monte carlo counterfactual regret minimization. In Aamas, pages 837\u2013846, 2012.   \nMarc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay, Julien P\u00e9rolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, et al. Openspiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453, 2019.   \nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.   \nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.   \nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.   \nJonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. Avalonbench: Evaluating llms playing the game of avalon. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.   \nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents, 2023.   \nYingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. At which training stage does code data help llms reasoning? arXiv preprint arXiv:2309.16298, 2023.   \nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384\u20131403, 2022.   \nMeta. Introducing meta llama 3: The most capable openly available llm to date. https://ai.meta. com/blog/meta-llama-3/, 2024. Accessed: 2024-05-18.   \nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.   \nGabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, and Jesse Clifton. Welfare diplomacy: Benchmarking language model cooperation. In Socially Responsible Language Modelling Research, 2023.   \nJohn F Nash Jr. Equilibrium points in n-person games. Proceedings of the national academy of sciences, 36(1):48\u201349, 1950. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Noam Nisan and Gali Noti. An experimental evaluation of regret-based econometrics. In Proceedings of the 26th International Conference on World Wide Web, pages 73\u201381, 2017. ", "page_idx": 12}, {"type": "text", "text": "Aidan O\u2019Gara. Hoodwinked: Deception and cooperation in a text-based game for language models, 2023.   \nAlexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark. In International Conference on Machine Learning, pages 26837\u201326867. PMLR, 2023.   \nRajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. In The Eleventh International Conference on Learning Representations, 2023.   \nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2020.   \nSimon Stepputtis, Joseph Campbell, Yaqi Xie, Zhengyang Qi, Wenxin Sharon Zhang, Ruiyi Wang, Sanketh Rangreji, Charles Michael Lewis, and Katia P Sycara. Long-horizon dialogue understanding for role identification in the game of avalon with large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.   \nRuoyao Wang, Peter Jansen, Marc-Alexandre C\u00f4t\u00e9, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11279\u201311298, 2022a.   \nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022b.   \nXingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023b.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023.   \nKai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining the inter-consistency of large language models: An in-depth analysis via debate. Association for Computational Linguistics, 2023.   \nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023a.   \nZelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic play in the werewolf game, 2023b.   \nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744\u201320757, 2022.   \nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Grifftihs, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024a.   \nYifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, page 100211, 2024b.   \nYadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. Llm as a mastermind: A survey of strategic reasoning with large language models. arXiv preprint arXiv:2404.01230, 2024.   \nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.   \nAndrew Zhu, Karmanya Aggarwal, Alexander Feng, Lara Martin, and Chris Callison-Burch. Fireball: A dataset of dungeons and dragons actual-play with structured game state information. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2023.   \nMartin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. Advances in neural information processing systems, 20, 2007. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A1 Overall Architecture ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "There are three main components in GTBENCH: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Environment. The environment ( Figure 1 (c)) is responsible for overseeing the crucial processes related to gameplay. Specifically, it is tasked with building up observations, managing gameplay, and applying the actions obtained from participants. In this paper, all of the gaming environments are built on top of OpenSpiel Lanctot et al. (2019). ", "page_idx": 14}, {"type": "text", "text": "\u2022 Prompt Adapter. The prompt adapter ( Figure 1 (b)) plays a vital role in facilitating effective communication between the environment and the virtual participants. It serves as an intermediary between the two entities by receiving observations from the environment, which it then translates into unified observation prompts. The prompts are then parsed and sent to the participating agents to formulate their responses. The adapter is also responsible for obtaining actions from the participants, which it transforms into legal actions before parsing them to the environment for game execution. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Participant. The participants ( Figure 1 (a)) involved in the gaming process generate responses according to the observation prompts received from the Prompt Adapter. These responses consist of actions that participants intend to take in this turn. ", "page_idx": 14}, {"type": "text", "text": "A2 Gameplay Configurations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A2.1 Games Introduction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tic-Tac- $\\mathbf{\\nabla}\\mathbf{Toe}^{6}$ is a paper-and-pencil game for two players who take turns marking the spaces in a three-by-three grid with X or O. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row is the winner. It is a solved game, with a forced draw assuming optimal play from both players. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains \u201copponent moves\u201d and \u201cself moves\u201d. \u201cOpponent moves\u201d contains all the current opponent agent\u2019s historical actions. \u201cSelf moves\u201d contains all the current agent\u2019s history actions. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Actions: We define our action in the following format: ${\\mathrm{<}}{\\mathrm{C}}x{\\mathrm{R}}y{\\mathrm{>}}$ , in which C and R mean columns and rows respectively, while $x$ and $y$ mean the index of column and row. Each player may make their own action in turn. ", "page_idx": 14}, {"type": "text", "text": "Prisoner\u2019s Dilemma7 is a game theory thought experiment that involves two rational agents, each of whom can cooperate for mutual benefit or betray their partner (\"defect\") for individual reward. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains \u201copponent moves\u201d and \u201cself moves\u201d. \u201cOpponent moves\u201d contains all the current opponent agent\u2019s historical actions. \u201cSelf moves\u201d contains all the current agent\u2019s history actions. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Actions: We define our action in the following format: $<$ <Silent $>$ or $<$ Testify $>$ . All players must take their action simultaneously. ", "page_idx": 14}, {"type": "text", "text": "Breakthrough8 Breakthrough is an abstract strategy board game invented by Dan Troyka in 2000 and made available as a Zillions of Games flie (ZRF). It won the $2001\\;8\\mathrm{x}8$ Game Design Competition. The first player to reach the opponent\u2019s home row \u2014 the one farthest from the player \u2014 is the winner. In our work, we scale the size of the board to $3{\\ast}8$ while maintaining its competitiveness. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains \u201copponent moves\u201d, \u201cself moves\u201d, and \u201cboard preview\u201d. \u201cOpponent moves\u201d contains all the current opponent agent\u2019s historical actions. \u201cSelf moves\u201d contains all the current agent\u2019s history actions. The \u201cboard preview\u201d feature maintains the status of each grid on the board through a list of strings, denoting whether it contains a black piece, a white piece, or is empty. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Actions: We define our action in the following format: ${\\bf A}x\\mathbf{-}\\!>\\!{\\bf B}y$ , in which A and B mean the current column index and destination column index respectively, while $x$ and $y$ mean the index of current row and destination row. Each player may make their own action in turn. ", "page_idx": 15}, {"type": "text", "text": "Connect $\\mathbf{Four}^{9}$ is a game in which the players choose a color and then take turns dropping colored tokens into a six-row, seven-column vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one\u2019s own tokens. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains \u201copponent moves\u201d and \u201cself moves\u201d. \u201cOpponent moves\u201d contains all the current opponent agent\u2019s historical actions. \u201cSelf moves\u201d contains all the current agent\u2019s history actions. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Actions: We define our action in the following format: $<\\!C x\\!>$ in which C means column, while $x$ means the index of column. Each player may make their action in turn. ", "page_idx": 15}, {"type": "text", "text": "Blind Auction10 is a common type of auction. In this type of auction, all bidders simultaneously submit sealed bids so that no bidder knows the bid of any other participant. The highest bidder pays the price that was submitted. All players must take their action simultaneously. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains \u201cvaluation\u201d.\u201cValuation\u201d contains each of the values of all the items for the current player.   \n\u2022 Actions: We define our action in the following format: $<x>$ , in which $x$ represents the amount that a certain player would like to bid for. ", "page_idx": 15}, {"type": "text", "text": "Kuhn Poker11 is a simplified form of poker. Kuhn is a simple model zero-sum two-player imperfectinformation game, amenable to a complete game-theoretic analysis. In Kuhn poker, the deck includes only three playing cards, for example, a King, Queen, and Jack. One card is dealt to each player, which may place bets similarly to a standard poker. If both players bet or both players pass, the player with the higher card wins, otherwise, the betting player wins. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains \u201ccard\u201d , and \u201cmoves\u201d. Among these, \u201ccard\u201d denotes the current player\u2019s hand card in this match, while \u201cmoves\u201d represents the history of all characters\u2019 moves together with the index of the rounds. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Actions: We define our action in the following format: $-\\mathrm{Pass}{>}$ or ${\\tt{-B e t-}}$ . Each player may make their own action in turn. ", "page_idx": 15}, {"type": "text", "text": "Liar\u2019s $\\mathbf{D}\\mathbf{ice}^{12}$ is a class of dice games for two or more players requiring the ability to deceive and detect an opponent\u2019s deception. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains: \u201cSelf dice face value\u201d and \u201clast move\u201d. \u201cSelf dice face value\u201d describes all the face values of dices the current player has, while \u201clast move\u201d represents the previous player\u2019s action. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Actions: We define our action in the following format: $<x$ dices, $y$ value $>$ or $_{<\\mathrm{Liar}>}$ . Among these, $x$ means the quantity of dice, and $y$ means the face values of the dice. The option \u201cLiar\u201d denotes the current player wants to stop and challenge the previous players. Each player may make their own action in turn. ", "page_idx": 15}, {"type": "text", "text": "$\\mathbf{P_{ig}}^{13}$ is a simple dice game. Players take turns to roll a single dice as many times as they wish, adding all roll results to a running total, but losing their gained score for the turn if they roll a 1. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains: \u201cself current score\u201d, \u201copponent current score\u201d, and \u201cturn total score\u201d. \u201cSelf current score\u201d and \u201copponent current score\u201d represent the game culminated score of the current player and opponent player respectively. While \u201cturn total score\u201d denotes the sum of the score of the current turn. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Actions: We define our action in the following format: <stop> or ${<}\\mathrm{roll}{>}$ . Each player may make their own action in turn. ", "page_idx": 16}, {"type": "text", "text": "$\\mathbf{Nim}^{14}$ is a mathematical game of strategy in which two players take turns removing objects from distinct heaps or piles. On each turn, a player must remove at least one object and may remove any number of objects provided they all come from the same heap or pile. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains: \u201cpiles\u201d. \u201cPiles\u201d denotes the number of matches different piles have. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Actions: We define our action in the following format: <pile: $x$ , take: $y\\!>$ . Among these, $x$ represents the index of the pile that the current player takes, and $y$ represents the number of matches the current player takes. Each player may make their own action in turn. ", "page_idx": 16}, {"type": "text", "text": "Negotiation15 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 Observation (input): Our observation contains: \u201cturn type\u201d, \u201citem pool\u201d, \u201cmost recent proposal\u201d, \u201cmost recent utterance\u201d, and \u201cself value vector\u201d. \u201cturn type\u201d is an enum variable, it has two options: proposal and utterance. The \u201cProposal\u201d is the turn that the current player could think about the desired quantities of the items, and the \u201cUtterance\u201d is the turn that the current player states the values to its opponent. \u201citem pool\u201d represents the quantities of all the items.\u201cmost recent proposal\u201d and \u201cmost recent utterance\u201d represent the opponent\u2019s latest proposal and utterance. \u201cself value vector\u201d represents how much the value of the items to the current player. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Actions: We define our action in the following format: <Agree> or $<x$ , $y$ , $z>$ . Among these, <Agree> represents the current player agreeing on the opponent\u2019s utterance. $x,y$ , and $z$ represent the quantities of different items that the current player wants to get. ", "page_idx": 16}, {"type": "text", "text": "A2.2 Gaming-Theoretic Taxonomy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Complete and Incomplete Information One fundamental dimension along which games are classified is the level of information available to players. In complete information games, players possess perfect knowledge regarding the game\u2019s structure, including the available strategies, payoffs, and the actions taken by other players. Examples of complete information games include canonical examples like chess and Tic-Tac-Toe, where all relevant information is transparent to all players throughout the game. Conversely, incomplete information games involve situations where players must make decisions without having full knowledge of the game\u2019s parameters or the actions of other players. Classic examples of incomplete information games include strategic interactions in economics, such as auctions or negotiations, where players have limited knowledge about the valuations or preferences of other participants. ", "page_idx": 16}, {"type": "text", "text": "Dynamic and Static Another crucial dimension for classifying games is the timing of players\u2019 decisions. In static games, players make decisions simultaneously, without the opportunity to observe or react to other players\u2019 moves. Examples of static games include simultaneous-move games like the Iterated Prisoner\u2019s Dilemma. In contrast, dynamic games involve sequential decision-making, where players observe previous moves before choosing their actions. Dynamic games encompass a wide range of strategic environments, from turn-based board games like chess to dynamic settings like Kuhn Poker, where players strategically make their actions based on the unfolding dynamics of the game. ", "page_idx": 16}, {"type": "text", "text": "Probabilistic and Deterministic Games can also be differentiated based on the role of uncertainty in decision-making. In deterministic games, the outcomes of players\u2019 actions are fully determined by the game\u2019s rules and the strategies chosen by players. Deterministic games include classic examples like chess or Tic-Tac-Toe, where each move leads to a predictable outcome based on the game\u2019s rules and the players\u2019 strategies. Conversely, probabilistic games involve randomness or uncertainty in determining outcomes. This uncertainty can stem from elements such as dice rolls, card draws. Examples of probabilistic games include games of chance like Kuhn Poker, Liar $^,\\mathtt{s}$ Dice, or Pig, where players must contend with the inherent uncertainty of probabilistic outcomes. ", "page_idx": 16}, {"type": "text", "text": "A3 Participants ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A3.1 Conventional Agent ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "MCTS Chaslot et al. (2008) is a heuristic search algorithm that has gained prominence in recent years, particularly in the domain of board games and decision-making under uncertainty. It is characterized by its ability to efficiently explore large search spaces by sampling potential future outcomes through Monte Carlo simulations. The algorithm iteratively builds a search tree by simulating random sequences of moves from the current game state and evaluating their outcomes through repeated simulations. By focusing computational resources on promising branches of the search tree, MCTS aims to guide the search towards regions of the game space that are more likely to lead to favorable outcomes. MCTS has demonstrated remarkable success in various domains, including games like Go, where traditional search algorithms struggle due to the game\u2019s immense complexity and branching factor. ", "page_idx": 17}, {"type": "text", "text": "Tit-for-Tat Axelrod (1981) is a simple but powerful strategy in the realm of repeated games and social dilemmas. The strategy is based on the principle of reciprocity, where an agent initially cooperates and then mimics the opponent\u2019s previous action in subsequent rounds. Specifically, Tit-for-Tat starts by cooperating in the first round and then replicates the opponent\u2019s last move in each subsequent round. Despite its simplicity, Tit-for-Tat has been shown to be remarkably effective in promoting cooperation and achieving favorable outcomes in various scenarios, including Iterated Prisoner\u2019s Dilemma and evolutionary simulations. Its success stems from its ability to balance cooperation and retaliation, fostering reciprocal behavior and encouraging cooperation among interacting agents. ", "page_idx": 17}, {"type": "text", "text": "A4 LLM-vs-LLM Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Figure A6, we present the confusion matrix of NRA when various LLM agents are against GPT-3.5-turbo and GPT-4. ", "page_idx": 17}, {"type": "image", "img_path": "ypggxVWIv2/tmp/5c74f1b30d10bbdb7fb884059ec03f17243494c6a17f6181c123124ab44e0ed2.jpg", "img_caption": ["Figure A6: NRA confusion matrix of LLM vs. LLM across ten games ranked by average NRA. GPT-3.5-turbo with Prompt Agent serve as the common opponent against multiple combinations of LLMs with agents. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "A5 Prompt and Protocol ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A5.1 Modular Prompt Structure ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "When prompting LLMs to generate the next action during the course of a game, the prompt is composed of four individual components, to make sure all the participants access the same observations and information from environments: ", "page_idx": 18}, {"type": "text", "text": "System Prompt provides general guidance on how the LLMs should perform. ", "page_idx": 18}, {"type": "text", "text": "Head Prompt provides the general background and rules of the game. ", "page_idx": 18}, {"type": "text", "text": "Observation Prompt is formatted by a fixed game-wise template, providing sufficient observations from the environment regarding the current gaming state, to make LLMs capable of making decisions. The following provides the template used in the Blind Auction environment: ", "page_idx": 18}, {"type": "text", "text": "Your budget is $\\mathrm{<VALUATION>}$ . Your bid must be strictly lower than or equal to $<$ VALUATION>. Your opponent also has an expected valuation and you do not know it.   \nThe legal actions are: <LEGAL_MOVES>. ", "page_idx": 18}, {"type": "text", "text": "Here $\\scriptstyle<\\mathrm{VALUATION}>$ and <LEGAL_MOVES> are variables and are obtained from a unified <observation $>$ object. In this way, all the participants are guaranteed to assess the same information. ", "page_idx": 18}, {"type": "text", "text": "Reasoning Prompt guides the LLM\u2019s generation process, e.g., \u201cLet\u2019s think step by step\u201d for the CoT Agent. ", "page_idx": 18}, {"type": "text", "text": "A5.2 System Prompt ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The system prompt is the initial text or context provided by the user to prompt the model to generate a response. This prompt serves as the starting point for the model to understand the user\u2019s query or input and generate a relevant and coherent response based on the provided context. We use the same system prompt for different tasks in our work. Our system prompt is presented as follows: ", "page_idx": 18}, {"type": "text", "text": "System Prompt: You are a powerful gaming agent who can make proper decisions to beat the user in gaming tasks. You are a helpful assistant that strictly follows the user\u2019s instructions. You must answer your questions by choosing one of the legal moves given by the user! ", "page_idx": 18}, {"type": "text", "text": "A5.3 Head Prompt ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The head prompt is the text that explains the rules of the games to the LLMs. In our work, we designed different prompts for each of the games. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Tic-Tac-Toe ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Head Prompt: Tic Tac Toe is a two-player game played on a grid. Players take turns marking a space with their respective symbols. The goal is to get 3 of one\u2019s own symbols in a row, either horizontally, vertically, or diagonally, before the opponent does. If all nine squares are fliled and no player has three in a row, the game is a draw. The Tic Tac Toe game is played on a 3 by 3 grid, with the winning length as 3. Each move is represented by a string consisting of two parts: the column (C) and the row (R), in that order. For instance, C1R2 means the movement at the position of the first column and the second row of the grid. You are playing this game with the user (opponent). ", "page_idx": 18}, {"type": "text", "text": "\u2022 Iterated Prisoner\u2019s Dilemma ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Head Prompt: You and your partner are in the Prisoner\u2019s Dilemma situation. Specifically, if you $<$ Testify $>$ against your partner and your partner remains $_{<\\mathrm{Silent}>}$ , you will go free while your partner will get 3 years in prison on the main charge. If you remain $_{<\\mathrm{Silent}>}$ but your partner <Testify $>$ against you, you will serve 3 years in prison and your partner will be set free. If you and your partner <Testify $>$ against each other, you and your partner will each serve 2 years. If both you and your partner remain $<$ <Silent>, you and your partner will each serve 1 year. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u2022 Breakthrough ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Head Prompt: Breakthrough is a two-player game played on a rectangular board. Players take turns moving their pieces, which can move one space straight or diagonally forward if the target square is empty. A piece can also move diagonally forward to capture an opponent\u2019s piece. Capturing is optional, and a player can only capture one piece per turn. The goal is to be the first to reach the opponent\u2019s home row, the farthest row from the player. If all of a player\u2019s pieces are captured, they lose. The game does not allow draws, as pieces can only move forward or be captured. The Breakthrough board is identified by columns labeled starting from A (from left to right) and rows numbered 1 to 8 (from bottom to top). The intersection of a column and a row specifies a unique square on the board. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Connect Four ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Head Prompt: Connect 4 is a two-player connection board game, where the players choose a color and then take turns dropping colored discs into a vertically suspended grid. The pieces fall straight down, occupying the next available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one\u2019s own discs. You are a gaming agent who aims to beat me in Connect 4 games. Each move is represented by a string consisting of two parts: the column (C) and the row (R), in that order. For instance, C1 means the first column. ", "page_idx": 19}, {"type": "text", "text": "\u2022 First-price sealed-bid auction ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Head Prompt: A first-price sealed-bid auction (FPSBA) is a common type of auction. It is also known as the blind auction. In this type of auction, all bidders simultaneously submit sealed bids so that no bidder knows the bid of any other participant. The highest bidder pays the price that was submitted. ", "page_idx": 19}, {"type": "text", "text": "Each action is represented by $<x>$ where $x$ refers to the bid. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Kuhn Poker ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Head Prompt: Kuhn poker is a simple model zero-sum two-player imperfect-information game, amenable to a complete game-theoretic analysis. In Kuhn poker, the deck includes only three playing cards: a King (K), a Queen (Q), and a Jack (J). One card is dealt to each player, and the third is put aside unseen. The players take turns either ${\\tt{<}}{\\tt B e t{>}}$ to match the bet raised by the opponent or $\\mathrm{<Pass>}$ to concede the game. ", "page_idx": 19}, {"type": "text", "text": "If a player bets, the other player must either call the bet by matching it or fold by conceding the game. If both players pass, the game is over, and the player with the higher-ranking card wins. The card rankings are as follows: King $(\\mathrm{K})>$ Queen $\\left(Q\\right)>$ Jack (J). ", "page_idx": 19}, {"type": "text", "text": "You are playing Kuhn poker with the opponent. The actions are denoted by ${\\tt{<}}{\\tt{B e t}}{\\tt{>}}$ and $-\\mathrm{Pass}{>}$ . ", "page_idx": 19}, {"type": "text", "text": "\u2022 Liar\u2019s Dice ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Head Prompt: Liar\u2019s Dice is a game of bluffing and probability, played with two players and each player has 1 dice. During each turn, a player can either bid a higher quantity of any particular face value or the same quantity of a higher face value than the previous bid. Each player tries to outbid their opponent without being caught in a lie. The move in this game is denoted in $<x$ dices, $y$ value $>$ , meaning there are at least $x$ dices with face values as $y$ . ", "page_idx": 19}, {"type": "text", "text": "Pig ", "page_idx": 19}, {"type": "text", "text": "Head Prompt: Pig is a fast-paced dice game where players risk accumulating points with each roll but risk losing them all if they roll a 1. Each player must decide when to stop rolling and bank their points, aiming to be the first to reach 100 points. You are playing Pig with the other. ", "page_idx": 19}, {"type": "text", "text": "Head Prompt: In Nim, a strategic game with a set of four piles containing 1, 3, 5, and 7 matches respectively, players aim to avoid taking the last match. During each turn, a player may take any number of matches from a single pile, but must take at least one and cannot exceed the number remaining in that pile. The objective is to force the opponent to pick up the final match, thereby winning the game. The action is presented in <pile: $x$ , take: $y\\!>$ , which means take $y$ match(es) from the $x$ -th pile. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Negotiation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Head Prompt: You are negotiating the division of Peppers, Strawberries, and Cherries with the opponent. Different values these items hold for both you and your opponent. The process is structured into two stages per round: the proposal stage and the utterance stage. ", "page_idx": 20}, {"type": "text", "text": "A5.4 Observations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our research team has developed a range of observation prompts tailored to different types of games.   \nThe list of these prompts is presented below. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Tic-Tac-Toe ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Observation Prompt: Your opponent has finished actions: <OPPONENT_MOVES>. You have finished actions: <SELF_MOVES>. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Iterated Prisoner\u2019s Dilemma ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Observation Prompt: You have been through this situation in the past and here are the decisions you and your partner made: (In the $i d x+1$ th round, you decided to ${<}\\mathrm{MOVE}{>}$ and your opponent decided to $<$ OPPONENT_MOVE>) \\* $n$ round ", "page_idx": 20}, {"type": "text", "text": "\u2022 Breakthrough ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Observation Prompt: The board now looks like : $<$ <BOARD_PREVIEW>. Among which, the letter \u2018b\u2019 represents a black piece, while the letter \u2018w\u2019 represents a white piece. And the character \u201c.\u201d represents vacant space. The numbers in the board are the indexes of the rows. Your opponent has finished actions: $<$ OPPONENT_MOVES $>$ .You have finished actions: <SELF_MOVES>. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Connect Four ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Observation Prompt: Your opponent has finished actions: <OPPONENT_MOVES>. You have finished actions: <SELF_MOVES>. ", "page_idx": 20}, {"type": "text", "text": "\u2022 First-price sealed-bid auction ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Observation Prompt: Now, you are in an auction with an opponent. You want to win the object and at the same time, your budget is $<$ VALUATION>. Your bid must be strictly lower than or equal to $\\scriptstyle<\\mathrm{VALUATION}>$ . You shall bid wisely against your opponent. Your opponent also has an expected valuation and you do not know it. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Kuhn Poker ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Observation Prompt: In this match, your card is ${<}C\\mathrm{ARD}{>}$ . Here are the past moves in this match: <SELF_MOVES>, <OPPONENT_MOVES>. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Liar\u2019s Dice ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Observation Prompt: Currently, the face value of your dice is <FACE_VALUE>. Last time, the opponent called <OPPONENT_LAST_ACTION>. You are playing the Liar\u2019s Dice with another opponent. Therefore, there are only two dice in total. ", "page_idx": 20}, {"type": "text", "text": "Observation Prompt: Right now, your current score is $<$ AGENT_CURRENT_SCORE> and your opponent\u2019s current score is $<$ OPPONENT_CURRENT_SCORE>. In this turn, you have earned <TURN_TOTAL_SCORE $>$ score. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Nim ", "page_idx": 21}, {"type": "text", "text": "Observation Prompt: Currently, the 1st pile has <PILES $[0]\\textgreater$ match(es), the 2nd pile has <PILES[1]> match(es), the 3rd pile has <PILES $[2]>$ match(es), 4th pile has <PILES[3]> match(es). ", "page_idx": 21}, {"type": "text", "text": "\u2022 Negotiation We proposed two different prompts for the \u201cproposal\u201d turn and \u201cutterance\u201d turn respectively. For the \u201cproposal\u201d turn, we have: ", "page_idx": 21}, {"type": "table", "img_path": "ypggxVWIv2/tmp/0d3b8c26e1a2b567f59f5855b8d7cd33f2c378b06cc748367708de2c8b80966e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "A5.5 Reasoning Prompt ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 Prompt agent: Prompt agent does not necessitate the use of LLMs to apply any predetermined strategy prior to decision-making. Rather, it simply requests LLMs for inference and subsequently provides the outcome. ", "page_idx": 21}, {"type": "text", "text": "You must choose a legal action to set up advantages. Your output must be in the following   \nformat:   \nAction: Your action wrapped with $<>$ , i.e., <format>   \nPlease return your answer without explanation! ", "page_idx": 21}, {"type": "text", "text": "\u2022 CoT agent: CoT agent makes LLMs consider the given observation first, then give out the action according to its thinking. ", "page_idx": 21}, {"type": "text", "text": "First think about your current situation, then you must choose one action from legal actions to set up advantages.   \nYour output must be in the following format strictly:   \nThought: Your thought.   \nAction: Your action wrapped by $<>$ , i.e., <format>   \nRemember, you can only choose one move from the legal actions. ", "page_idx": 21}, {"type": "text", "text": "\u2022 SC-CoT agent: SC-CoT agent is an advanced version of the CoT agent. It obtains actions from multiple CoT trajectories. It employs the same prompt templates as in the CoT agent. ", "page_idx": 21}, {"type": "text", "text": "First think about your current situation, then you must choose one action from legal actions to set up advantages.   \nYour output must be in the following format strictly:   \nThought: Your thought.   \nAction: Your action wrapped by $<>$ , i.e., <format>   \nRemember, you can only choose one move from the legal actions. ", "page_idx": 22}, {"type": "text", "text": "\u2022 ToT agent: we follow the text generation task implementation in the official codebase of ToT 16. Specifically, the ToT is factorized into 1). candidate thought generation, 2). thought voting, 3). candidate action generation, 4). action voting: Here we provide the basic prompt template used in ToT. ", "page_idx": 22}, {"type": "text", "text": "Step Prompt: First think about your current situation, then choose one move from legal positions to set up advantages.   \nYour output should be of the following format:   \nThought:   \nYour thought.   \nMove:   \nYour action wrapped with $<>$ , e.g., <format> ", "page_idx": 22}, {"type": "text", "text": "After executing step prompts in a breath-first search manner, we utilize the original ToT vote prompt: ", "page_idx": 22}, {"type": "text", "text": "Vote Prompt: Given an instruction and several choices, decide which choice is most promising. Analyze each choice in detail, then conclude in the last line \"The best choice is $\\mathrm{s}\"$ , where s the integer id of the choice. ", "page_idx": 22}, {"type": "text", "text": "A5.6 Sanity Check ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "completion rates of each game. The completion rates are calculated as To evaluate the effectiveness of our framework, we perform a sanity check by calculating the $\\frac{50}{N}$ where $N$ is the number of matches that will take to achieve 50 valid matches. Here, a valid match means all the participants will always generate legal moves at each turn of the match. Results are summarized in Table A7. We show that all the LLM agents achieve $\\geq90\\%$ completion rate, showing that the prompts are properly configured and LLMs are capable of following instructions to finish the game. ", "page_idx": 22}, {"type": "table", "img_path": "ypggxVWIv2/tmp/5bd6949b63dfe81a8416dc6fb5e8e38352923d5d10eb30fd8eacc25d837f9ec1.jpg", "table_caption": ["Table A7: Sanity check. The completion rates of LLM agents over all the games. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "A6 How Temperature Affects LLM Performance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To study how the temperature used in generating LLMs\u2019 responses affects performances, we conduct experiments by making LLMs with 0.2 temperature (the default setting as in our paper) play against LLMs with $0.4/0.6/0.8$ temperature, over CodeLlama-34b-Instruct and GPT-3.5-turbo-1106. For each experiment, we run 20 matches. The reasoning method is the PromptAgent. The results are summarized as in Table A8. We show that a larger temperature will result in worse performance for deterministic games, while it has a model-specific effect for probabilistic games. ", "page_idx": 22}, {"type": "table", "img_path": "ypggxVWIv2/tmp/5cd87be3bb256beeb3cd0a7888b82ca45f5a1e559bfcafd4029f0b053c84a387.jpg", "table_caption": ["Table A8: The affect of various temperatures for generation sampling. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "A7 Elo Rating System ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The Elo rating system Elo (1960) is a popular method for calculating the relative skill levels of players in two-player games such as Chess. It was used by various organizations to rank players. Assume there are two players: $A$ and $B$ , and each player has a rating, $R_{A}$ , $R_{B}$ , which is a numerical value representing their skill level. The expected score for a player is the probability that the player will win against another player: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{E_{A}=\\frac{1}{1+10^{(R_{B}-R_{A})/400}}}}\\\\ {{{}}}\\\\ {{E_{B}=\\frac{1}{1+10^{(R_{A}-R_{B})/400}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "After a match between $A$ and $B$ , the real values, $S_{A}$ and $S_{B}$ , are defined as ", "page_idx": 23}, {"type": "text", "text": "\u2022 If Player $A$ wins, $S_{A}=1$ and $S_{B}=0$ \u2022 If Player $B$ wins, $S_{A}=0$ and $S_{B}=1$ \u2022 If the game is a draw, $S_{A}=S_{B}=0.5$ ", "page_idx": 23}, {"type": "text", "text": "Then, the updated rating ${\\boldsymbol{R}}_{A}^{'}$ and $R_{B}^{'}$ are calculated as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{R_{A}^{\\prime}}=\\boldsymbol{R_{A}}+\\boldsymbol{K}*(\\boldsymbol{S_{A}}-\\boldsymbol{E_{A}}))}\\\\ {\\boldsymbol{R_{B}^{\\prime}}=\\boldsymbol{R_{B}}+\\boldsymbol{K}*(\\boldsymbol{S_{B}}-\\boldsymbol{E_{B}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $K$ is a constant that determines how much the rating changes after a game. A higher $K$ results in a larger change. In our paper, the initial rating is set to 1500, i.e., $R_{A}=R_{B}=1500$ , and $K=20$ . ", "page_idx": 23}, {"type": "image", "img_path": "ypggxVWIv2/tmp/896b965a282adf801b7a0ab4c58bd2e73c54605b4c919ab33245e73c55593dd0.jpg", "img_caption": ["Figure A7: Investigating the sensitivity of Chain-of-Thought prompt. Prompt (used) and CoT (used) refer to the prompts utilized by the Prompt Agent and the CoT Agent in this paper. Results are obtained from the model GPT-3.5-turbo over all the game-theoretic tasks. Please refer to Table A9 for Template 0 to Template 4. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A8 Chain-of-Thought Sensitivity ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We provide five different CoT strategies over the GPT-3.5-turbo model as shown in Table A9 the results presented in Figure A7. ", "page_idx": 24}, {"type": "text", "text": "We also include instances as shown in Table A10 where CoT agents were unable to produce legal outcomes, which serves as evidence of their limitations. ", "page_idx": 24}, {"type": "text", "text": "Table A9: Different Chain-of-Thought strategies. ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "ypggxVWIv2/tmp/0f0d232885edba147fb9b96c59c82be3c0dccb7e0c661a5eb18b7593d979b4e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ypggxVWIv2/tmp/941cf6a1100c75107ad1a3ae34f95ca6afb0acb73caf4b969b40e2ff6061b989.jpg", "table_caption": ["Table A10: Fail cases in different Chain-of-Thought prompts. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "A9 Error Pattern Demonstrations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We present demonstrations of error patterns in Table A11. ", "page_idx": 24}, {"type": "table", "img_path": "ypggxVWIv2/tmp/7b404f8dbf3433097052ec3bd05cd3c29a56242bdc334ede6a820a50b6a523ee.jpg", "table_caption": ["Table A11: We conclude 5 common error patterns in GTBENCH, including misinterpretation, factual error, math calculation, and over-confidence. For each demonstration, the explanation explains why LLM generation is incorrect. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "A10 Regret Value ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "A10.1 Regret Value for Blind Auction ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Assume that the last round of bidding was $b_{1}$ for the player and $b_{2}$ for the opponent. Assume the player\u2019s valuation is $v$ , then the regret value is calculated by ", "page_idx": 26}, {"type": "text", "text": "if $\\mathsf{b}_{-}1\\;>\\;\\mathsf{b}_{-}2\\;+\\;1$ : regret $=\\ b_{-}1\\ \\ -\\ \\ (\\ b_{-}2\\ +\\ 1)$   \nelse: $\\begin{array}{r l}&{\\mathrm{if~\\gamma(b\\mathrm{\\tt~-2~+~\\tt~1)~<~v:}~}}\\\\ &{\\qquad\\mathrm{regret~=~v~-~(b\\mathrm{\\tt~-2~+~\\tt~1)~}~}}\\\\ &{\\mathrm{e1se:~}}\\\\ &{\\qquad\\mathrm{regret~=~0~}}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "A10.2 Regret Value for Iterated Prisoner\u2019s Dilemma ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The regret value Iterated Prisoner $^,\\mathtt{s}$ Dilemma is simply the accumulation of the regret value of per-turn Prisoner $^,\\mathtt{s}$ Dilemma: ", "page_idx": 26}, {"type": "text", "text": "if player_move $==$ \u2019Testify\u2019 and opponent_move $==$ \u2019Silent\u2019: regret $=~0$   \nelif player_move $==$ \u2019Testify\u2019 and opponent_move $==$ \u2019Testify\u2019: regret $=~0$   \nelif player_move $==$ \u2019Silent\u2019 and opponent_move $==$ \u2019Testify\u2019: regret $\\mathit{\\Theta}=\\mathit{\\Theta}1$   \nelse: regret $=~2$ ", "page_idx": 26}, {"type": "text", "text": "A11 User Interfaces of GTBench Leaderboard ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The user interfaces of GTBENCH leaderboard are presented in Figures A8 and A9. ", "page_idx": 26}, {"type": "image", "img_path": "ypggxVWIv2/tmp/f75158bf888e83b40337dba6221fab8be9aed2a77051b41cb9fdcf72e66a298f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "GTBench:Uncovering the Strategic Reasoning Limitation of LLMs via Game-TheoreticEvaluations ", "page_idx": 27}, {"type": "text", "text": "GTBenchaimstoevaluate andrankLLMsreasoning abilitiesincompetitiveenvironmentsthroughgame-theoretictasks.g,boardandcardgames.tutilizes10widelyrecognizedgames supportedbypenSpieland evaluatewellrecgnizedLLMagentsinalanguage-drivenmannerTheevaluationcodeandprompttemplatescanbefoundinGTBench ", "page_idx": 27}, {"type": "text", "text": "PleaserefertoAboutformoredetailsofgamesandmetric. ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "ypggxVWIv2/tmp/e9db70e5e5ce0ac6226c63479bf352b3c4db5af8373dda9119412f0be28c895e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "ypggxVWIv2/tmp/605d79ff8a1d62139152d6c5d9823b4c19bb4408853c402c3fa97c41849c2c21.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "GTBench:Uncovering the Strategic Reasoning Limitation of LLMs via Game-TheoreticEvaluations ", "page_idx": 28}, {"type": "text", "text": "GTBenchamstevaluateandrankLMsreasoningabitisincompetitivenvironmentsthroughgame-theoretictask,g,boardandcardgamestutilieswidelyrecgnizedgamesuportedbyenSpiland evaluate well-recognized LLMagents inalanguage-drivenmanner.he evaluation code and prompt templates canbefound in GTBench. ", "page_idx": 28}, {"type": "text", "text": "PleaserefertoAbout formoredetailsofgamesandmetrics. ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The template is borrowed from Qpen LLM Leaderboard. ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "ypggxVWIv2/tmp/23c2c35156ffaf0cc3ddf641dcc9fc18d2e6c8abf46d2248d3a6533c2ff56b1d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "ypggxVWIv2/tmp/6c7db5a72b0bb76703a8448ba11f2b300620ba0304d534a55ecfde699b08418e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure A9: The user interface of GTBENCH leaderboard when various LLMs/agents and opponents are selected. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We summarize our main empirical observations and conclusions in the abstract and introduction. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Section Limitation ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We provide details about models and generative hyper-parameters in Section 4, and all the prompts utilized in Appendix A5. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide access to the data and code. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Section 3, Section 4, Appendix A5, A6, A7. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Section 4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The results are obtained from endpoint API providers, e.g., OpenAI (Section 4). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Section Impact Statements ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Section Impact Statements ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Section 4 ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]