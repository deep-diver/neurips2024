[{"figure_path": "lbLC5OV9GY/figures/figures_3_1.jpg", "caption": "Figure 1: Visualization of parameter traces and trust regions corresponding to different SAAs. If after an update \u03c6\u2209Sz,\u03b1(\u03c6), we set \u03c6\u2190\u03c6t to construct a new SAA and corresponding trust region.", "description": "This figure visualizes how the VISA algorithm iteratively updates the variational parameters (\u03c6) and constructs sequential sample average approximations (SAAs). Each circle represents an SAA, defined by a set of samples and a trust region (Sz,\u03b1(\u03c6)). The algorithm starts with an initial proposal (\u03c60). As the optimizer updates the parameters, it checks if the updated parameters lie within the current trust region. If not, a new SAA is constructed with a new set of samples drawn from the updated proposal, defining a new trust region. This process repeats until convergence.", "section": "3 SAA for Forward-KL Variational Inference"}, {"figure_path": "lbLC5OV9GY/figures/figures_6_1.jpg", "caption": "Figure 2: Symmetric KL-divergence as a function of the number of model evaluations for a Gaussian target with diagonal covariance matrix (top row) and dense covariance matrix (bottom row). For small learning rates (0.001, 0.005, 0.01) IWFVI and BBVI-SF, need a larger number of model evaluations to converge. VISA converges much faster as it compensates for the small step size by reusing samples. For a learning rate of 0.05 VISA becomes unstable and fails to reliably converge, while IWFVI still converges. For an even higher learning rate all methods but BBVI-RP fail to converge.", "description": "This figure compares the convergence speed of VISA and other variational inference methods (IWFVI, BBVI-SF, BBVI-RP) on Gaussian target distributions with diagonal and dense covariance matrices.  It shows how the symmetric KL-divergence decreases over the number of model evaluations for different learning rates and VISA's trust region threshold parameters (\u03b1). The results illustrate that VISA significantly outperforms IWFVI and BBVI-SF for smaller learning rates by reusing samples, but can become unstable with higher learning rates.", "section": "5.1 Gaussians"}, {"figure_path": "lbLC5OV9GY/figures/figures_7_1.jpg", "caption": "Figure 3: Results for Lotka-Volterra model with different learning rates. (Top row) Training objective over number of model evaluations. (Middle row) Approximate forward KL-divergence computed on reference samples obtained by MCMC. For smaller step sizes (0.001, 0.005) VISA achieves comparable forward KL-divergence to IWFVI while requiring significantly less model evaluations to converge (see vertical lines). For larger step sizes (0.01) VISA only converges with a high ess threshold (0.99) and requires more evaluations than IWFVI and VISA with a smaller step size (0.005).", "description": "This figure shows the results of applying VISA and IWFVI to the Lotka-Volterra model with different learning rates. The top row displays the training objective as a function of the number of model evaluations, while the bottom row shows the approximate forward KL-divergence.  The results demonstrate that VISA converges faster and achieves comparable accuracy to IWFVI for smaller learning rates, but requires more evaluations for larger learning rates unless a high effective sample size (ESS) threshold is used.", "section": "5.2 Lotka-Volterra"}, {"figure_path": "lbLC5OV9GY/figures/figures_8_1.jpg", "caption": "Figure 4: Results for Pickover attractor. (a) Approximate log-joint density over number of batch-evaluations of model. (b) Log-joint approximation plotted over domain of prior. The variational approximation capture the high density area containing the data. (c) Visualization of pickover attractor with ground truth parameters \u03b8 = [-2.3, 1.25]. (d) Visualization of attractor with average system parameters computed over 10.000 samples from the learned variational approximation. Each evaluation in the plot corresponds to evaluating a batch of N = 10 samples.", "description": "This figure presents the results of applying VISA and IWFVI to the Pickover attractor model.  Panel (a) shows the log-joint density approximation improving over the number of model evaluations for VISA and IWFVI. Panel (b) displays the learned variational approximation of the log-joint density in the parameter space. Panels (c) and (d) visualize the resulting Pickover attractor for the ground truth parameters and the average parameters from the variational approximation respectively.", "section": "5.3 Pickover Attractor"}, {"figure_path": "lbLC5OV9GY/figures/figures_13_1.jpg", "caption": "Figure 2: Symmetric KL-divergence as a function of the number of model evaluations for a Gaussian target with diagonal covariance matrix (top row) and dense covariance matrix (bottom row). For small learning rates (0.001, 0.005, 0.01) IWFVI and BBVI-SF, need a larger number of model evaluations to converge. VISA converges much faster as it compensates for the small step size by reusing samples. For a learning rate of 0.05 VISA becomes unstable and fails to reliably converge, while IWFVI still converges. For an even higher learning rate all methods but BBVI-RP fail to converge.", "description": "This figure compares the convergence speed of VISA against other methods (IWFVI, BBVI-SF, BBVI-RP) for approximating Gaussian distributions with diagonal and dense covariance matrices.  The x-axis represents the number of model evaluations, and the y-axis shows the symmetric KL-divergence, measuring the difference between the approximate and true distributions. The results demonstrate VISA's faster convergence, especially at lower learning rates, due to its sample reuse strategy. However, at higher learning rates, VISA's stability decreases compared to IWFVI.", "section": "5.1 Gaussians"}]