[{"type": "text", "text": "Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruifeng Ren Yong Liu\u2217 Gaoling School of Artificial Intelligence Gaoling School of Artificial Intelligence Renmin University of China Renmin University of China Beijing, China Beijing, China renruifeng920@ruc.edu.cn liuyonggsai@ruc.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pre-trained large language models based on Transformers have demonstrated remarkable in-context learning (ICL) abilities. With just a few demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we attempt to explore the ICL process in Transformers through a lens of representation learning. Initially, leveraging kernel methods, we figure out a dual model for one softmax attention layer. The ICL inference process of the attention layer aligns with the training procedure of its dual model, generating token representation predictions that are equivalent to the dual model\u2019s test outputs. We delve into the training process of this dual model from a representation learning standpoint and further derive a generalization error bound related to the quantity of demonstration tokens. Subsequently, we extend our theoretical conclusions to more complicated scenarios, including one Transformer layer and multiple attention layers. Furthermore, drawing inspiration from existing representation learning methods especially contrastive learning, we propose potential modifications for the attention layer. Finally, experiments are designed to support our findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, large language models (LLMs) based on the Transformer architectures [Vaswani et al., 2017] has shown surprising in-context learning (ICL) capabilities [Brown et al., 2020, Wei et al., 2022, Dong et al., 2022, Liu et al., 2023]. By prepending several training examples before query inputs without labels, the models can make predictions for the queries and achieve excellent performance without any parameter updates. This excellent capability enables pre-trained LLMs such as GPT models to be used in general downstream tasks conveniently. Despite the good performance of the ICL capabilities, the mechanism of ICL still remains an open question. ", "page_idx": 0}, {"type": "text", "text": "In order to better understand the ICL capabilities, many works began to give explanations from different aspects. Xie et al. [2021] propose a Bayesian inference framework to explain how ICL occurs between pretraining and test time, where the LLMs infers a shared latent concept among the demonstration examples. Garg et al. [2022] demonstrate through experiments that pre-trained Transformer-based models can learn new functions from in-context examples, including (sparse) linear functions, two-layer neural networks, and decision trees. Zhang et al. [2023b] adopt a Bayesian perspective and show that ICL implicitly performs the Bayesian model averaging algorithm, which is approximated by the attention mechanism. Li et al. [2023] define ICL as an algorithm learning problem where a transformer model implicitly builds a hypothesis function at inferencetime and derive generalization bounds for ICL. Han et al. [2023] suggest that LLMs can emulate kernel regression algorithms and exhibit similar behaviors during ICL. These works have provided significant insights into the interpretation of ICL capabilities from various perspectives. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In addition to the above explorations, there are also some attempts to relate ICL capabilities to gradient descent. Inspired by the dual form of linear attention proposed in Aiserman et al. [1964] and Irie et al. [2022], the ICL process is interpreted as implicit fine-tuning in the setting of linear attention by Dai et al. [2022]. However, there is still a certain noticeable gap between linear attention and the widely used softmax attention. Additionally, this comparison is more of a formal resemblance and the specific details of gradient descent, including the form of the loss function and training data, require a more fine-grained exploration. Aky\u00fcrek et al. [2022] show that by constructing specific weights, Transformer layers can perform fundamental operations (mov, mul, div, aff), which can be combined to execute gradient descent. Von Oswald et al. [2023a] adopt another construction, such that the inference process on a single or multiple linear attention layers can be equivalently seen as taking one or multiple steps of gradient descent on linear regression tasks. Building upon this weight construction method, subsequent work has conducted a more in-depth exploration of the capabilities of ICL under a causal setting, noticing that the inference of such attention layers is akin to performing online gradient descent [Ding et al., 2023, Von Oswald et al., 2023b]. However, these analyses are still conducted under the assumption of linear attention and primarily focus on linear regression tasks, adopting specific constructions for the input tokens (concatenated from features and labels) and model weights. This limits the explanation of the Transformer\u2019s ICL capabilities in more general settings. Thus, the question arises: Can we relate ICL to gradient descent under the softmax attention setting, rather than the linear attention setting, without assuming specific constructions for model weights and input tokens? ", "page_idx": 1}, {"type": "text", "text": "Motivated by the aforementioned challenges and following these works that connect ICL with gradient descent, we explore the ICL inference process from a representation learning lens. First, by incorporating kernel methods, we establish a connection between the ICL inference process of one softmax attention layer and the gradient descent process of its dual model. The test prediction of the trained dual model will be equivalent to the ICL inference result. We analyze the training process of this dual model from the perspective of representation learning and compare it with existing representation learning methods. Then, we derive a generalization error bound of this process, which is related to the number of demonstration tokens. Our conclusions can be easily extended to more complex scenarios, including a single Transformer layer and multiple attention layers. Furthermore, inspired by existing representation learning methods especially contrastive learning, we propose potential modifications to the attention layer and experiments are designed to support our findings. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 In-context Learning with Transformers ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The model we consider is composed of many stacked Transformer decoder layers, each of which is composed of an attention layer and a FFN layer. For simplicity, we have omitted structures such as residual connections and layer normalization, retaining only the most essential parts. We consider the standard ICL scenario, where the model\u2019s input consists of demonstrations followed by query inputs, that is, the input can be represented as $\\mathbf{\\dot{\\Delta}}\\mathbf{X}\\,=\\,[\\mathbf{X}_{D},\\mathbf{X}_{T}]\\,\\in\\,\\mathbb{R}^{d_{i}\\,\\times\\,(N+T)}$ , where ${\\bf X}_{D}\\,=\\,[{\\pmb x}_{1},{\\pmb x}_{2},...,{\\pmb x}_{N}]$ denotes $N$ demonstration tokens, and ${\\bf{\\cal X}}_{T}\\,=\\,[{\\pmb x}_{1}^{\\prime},{\\pmb x}_{2}^{\\prime},...,{\\pmb x}_{T}^{\\prime}]$ denotes $T$ query tokens. Here, we focus more on how tokens interact during model inference while ignoring the internal structure of demonstration tokens. For the query input at position $T+1$ , its output after one layer of Transformer can be represented as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{h}_{T+1}^{\\prime}=W_{V}\\boldsymbol{X}\\mathrm{softmax}\\left((\\boldsymbol{W_{K}}\\boldsymbol{X})^{T}\\boldsymbol{W_{Q}}\\boldsymbol{x}_{T+1}^{\\prime}/\\sqrt{d_{o}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{x}}_{T+1}^{\\prime}=W_{2}\\mathrm{ReLu}(W_{1}h_{T+1}^{\\prime}+b_{1})+b_{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\pmb{W_{K}},\\pmb{W_{Q}},\\pmb{W_{V}}\\ \\in\\ \\mathbb{R}^{d_{o}\\times d_{i}}$ are parameters for key, query, value projections and $W_{1}~\\in$ $\\mathbb{R}^{d_{h}\\times d_{o}},W_{2}\\in\\bar{\\mathbb{R}}^{d_{o}\\times d_{h}},\\pmb{b}_{1}\\in\\mathbb{R}^{d_{h}},\\pmb{b}_{2}\\times\\mathbb{R}^{d_{o}}$ are FFN parameters. Our concern is how the query token ${\\pmb x}_{T+1}^{\\prime}$ learns in-context information from demonstrations. Unlike previous work [Von Oswald et al., 2023a, Zhang et al., 2023a, Bai et al., 2023], here we do not make additional assumptions about the structure of input matrix $\\mathbf{\\deltaX}$ and parameters to study the Transformer\u2019s ability to implement some specific algorithms. Instead, we adopt the same setting as [Dai et al., 2022] to study more general cases. ", "page_idx": 1}, {"type": "text", "text": "2.2 Self-Supervised Representation Learning Using Contrastive Loss Functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Representation learning aims to learn embeddings of data to preserve useful information for downstream tasks. One class of methods most relevant to our work is probably contrastive learning methods without negative samples [Chen and He, 2021, Grill et al., 2020, Caron et al., 2020, Tian et al., 2021]. Contrastive learning is a significant approach of self-supervised learning (SSL) which aims at learning representations by minimizing the distance between the augmentations of the same data point (positive samples) while maximizing the distance from different data points (negative samples) [He et al., 2020, Chen et al., 2020b, Oord et al., 2018, Oh Song et al., 2016]. To alleviate the burden of constructing a sufficient number of negative samples while avoiding representational collapse, some works propose architectures for contrastive learning without negative samples, which mainly use weight-sharing network known as Siamese networks [Chen and He, 2021, Grill et al., 2020, Caron et al., 2020, Tian et al., 2021]. The architecture takes two augmentations $\\mathbf{\\boldsymbol{x}}_{1},\\mathbf{\\boldsymbol{x}}_{2}$ from the same data $\\textbf{\\em x}$ as inputs, which will be processed by online network and target network respectively to obtain the corresponding representations, that is, $\\hat{\\pmb{x}}_{1}=f_{\\mathrm{online}}(\\pmb{x}_{1}),\\hat{\\pmb{x}}_{2}=f_{\\mathrm{target}}(\\pmb{x}_{2})$ . The two encoder networks share weights directly or using Exponential Moving Average (EMA). Then, $\\hat{\\pmb{x}}_{1}$ will be input into a predictor head to obtain the predictive representation $z_{1}\\,=\\,g(\\hat{x}_{1})$ . Finally, we minimize the distance between the predictive representation and target representation, that is, $\\mathcal{L}\\left(z_{1},\\mathrm{StopGrad}(\\hat{\\pmb{x}}_{2})\\right)$ where $\\mathrm{StopGrad}(\\cdot)$ means $\\hat{\\pmb{x}}_{2}$ is treated as a constant during backpropagation. For $\\mathcal{L}(\\cdot)$ , we often choose the cosine similarity or the $l_{2}$ -norm as a measure of distance, although they are equivalent when the vector is normalized. Another class similar to our work is kernel contrastive learning [Esser et al., 2024]. Given an anchor $\\textbf{\\em x}$ and its positive and negative samples $x^{+},x^{-}$ , it aims to optimize the loss function $\\mathcal{L}=f(\\pmb{x})^{T}(f(\\pmb{x}^{-})-\\dot{\\pmb{f}}(\\pmb{x}^{+}))$ , where $\\bar{\\boldsymbol{f}}(\\boldsymbol{x})=\\bar{\\boldsymbol{W}}\\phi(\\boldsymbol{x})$ and $\\phi({\\pmb x})$ is the feature mapping for some kernel. We will consider the gradient descent process corresponding to the inference process of ICL from the perspective of representation learning and compare it with the two aforementioned representation learning patterns. ", "page_idx": 2}, {"type": "text", "text": "2.3 Gradient Descent on Linear Layer is the Dual Form of Linear Attention ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "It has been found that the linear attention can be connected to the linear layer optimized by gradient descent [Aiserman et al., 1964, Irie et al., 2022, Dai et al., 2022], that is, the gradient descent on linear layer can be seen as the dual form 2 of linear attention. A simple linear layer can be defined as $f_{L}(\\pmb{x})\\stackrel{!}{=}\\pmb{W}\\pmb{x}$ , where $W\\in\\mathbb{R}^{d_{o}\\times d_{i}}$ is the projection matrix. Given training inputs $[\\pmb{x}_{i}]_{i=1}^{N}\\in\\mathbb{R}^{d_{i}}$ with their labels $[{\\pmb y}_{i}]_{i=1}^{N}\\in\\mathbb{R}^{d_{o}}$ , a linear layer can output the predictions $[\\hat{\\pmb{y}}_{i}]_{i=1}^{N}$ where $\\hat{\\pmb y}_{i}=\\pmb W\\pmb x_{i}$ and then compute certain loss $\\mathcal{L}(\\hat{\\pmb{y}}_{i},\\pmb{y}_{i})$ for training. Backpropagation signals $[e_{i}]_{i=1}^{N}\\in\\mathbb{R}^{d_{o}}$ will be produced to update $W$ in gradient descent process where $\\bar{e_{i}}=-\\eta\\left(\\nabla_{\\hat{y}_{i}}\\bar{\\mathcal{L}}\\right)$ if we set $\\eta$ as the learning rate. During test time, the trained weight matrixW  can be represented by its initialization $W_{0}$ and the updated part $\\Delta{W}$ , that is, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{W}}=\\pmb{W}_{0}+\\Delta\\pmb{W}=\\pmb{W}_{0}+\\sum_{i=1}^{N}\\pmb{e}_{i}\\otimes\\pmb{x}_{i},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\otimes$ denotes the outer product according to the chain rule of differentiation. On the other hand, this process can be viewed from the perspective of linear attention. Let $[\\pmb{k}_{i}]_{i=1}^{N},[\\pmb{v}_{i}]_{i=1}^{N}\\in\\mathbb{R}^{d_{i}}$ denote the $N$ key and value vectors constituting matrices $K,V\\in\\mathbb{R}^{d_{i}\\times N}$ respectively. For a given query input $\\pmb q\\in\\mathbb{R}^{d_{i}}$ , linear attention is typically defined as the weighted sum of these value vectors ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{LA}(V,K,q)=V K^{T}q=\\sum_{i=1}^{N}{v_{i}k_{i}^{T}q}=\\left(\\sum_{i=1}^{N}{v_{i}\\otimes k_{i}}\\right)q.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, we can rewrite the output of a linear layer during test time as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{L}(\\pmb{x}_{t e s t})=\\widehat{\\pmb{W}}\\pmb{x}_{t e s t}=W_{0}\\pmb{x}_{t e s t}+\\left(\\sum_{i=1}^{N}e_{i}\\otimes\\pmb{x}_{i}\\right)\\pmb{x}_{t e s t}=W_{0}\\pmb{x}_{t e s t}+\\mathrm{LA}(\\pmb{E},\\pmb{X},\\pmb{x}_{t e s t}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/2a438332f4c525447ba6e034277f5273a62e3d52aa55af0ac1295cd01d546577.jpg", "img_caption": ["Figure 1: The ICL output $h_{N+1}^{\\prime}$ of one softmax attention layer is equivalent to the test prediction $\\hat{\\pmb{y}}_{t e s t}$ of its trained dual model $f({\\pmb x})=\\widehat{\\pmb W}\\phi({\\pmb x})$ . The training data and test input can be obtained by linear transformations of demonstration and query tokens, respectively. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "where $\\pmb{E}\\in\\mathbb{R}^{d_{o}\\times N}$ and $\\boldsymbol{X}\\in\\mathbb{R}^{d_{i}\\times N}$ are stacked by backpropagation signals $[e_{i}]_{i=1}^{N}$ and training inputs $[{\\pmb x}_{i}]_{i=1}^{N}$ respectively. We can find from Eq (4) that the trained weight $\\widehat{W}$ records all training datapoints and the test prediction of the linear layer indicates which training d atapoints are chosen to activate using $\\mathrm{LA}(\\cdot)$ where $[e_{i}]_{i=1}^{N}$ can be considered as values while $[{\\pmb x}_{i}]_{i=1}^{N}$ as keys and $\\pmb{x}_{t e s t}$ as the query. This interpretation uses gradient descent as a bridge to connect predictions of linear layers with linear attention, which can be seen as a simplified softmax attention used in Transformers. ", "page_idx": 3}, {"type": "text", "text": "Inspired by this relationship, Dai et al. [2022] understand ICL as implicit fine-tuning. However, this interpretation based on linear attention deviates from the softmax attention used in practical Transformers. Furthermore, this alignment is also ambiguous as the specific details of the gradient descent process, including the form of loss function and dataset, have not been explicitly addressed. In addition, Von Oswald et al. [2023a], Ding et al. [2023] also connect ICL with gradient descent for linear regression tasks using weight construction methods, where parameters $W_{K}$ , $W_{Q}$ and $W_{V}$ of the self-attention layer need to roughly adhere to a specific constructed form. However, these analyses rely on the setting of linear regression tasks and assumptions about the form of input tokens (concatenated with features and labels), which limits the interpretability of ICL capabilities from the perspective of gradient descent. Thus, we attempt to address these issues in the following sections. ", "page_idx": 3}, {"type": "text", "text": "3 Connecting ICL with Gradient Descent ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we will address two questions discussed above: (i) Without assuming specific constructions for model weights and input tokens, how to relate ICL to gradient descent in the setting of softmax attention instead of linear attention? (2) What are the specific forms of the training data and loss function in the gradient descent process corresponding to ICL? In addressing these two questions, we will explore the gradient descent process corresponding to ICL from the perspective of representation learning. ", "page_idx": 3}, {"type": "text", "text": "3.1 Connecting Softmax Attention with Kernels ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before we begin establishing the connection between ICL and gradient descent, we need to firstly rethink softmax attention with kernel methods. Dai et al. [2022] connect ICL with gradient descent under the linear attention setting. In fact, it is completely feasible to interpret ICL under softmax attention with the help of kernel methods. We define the attention block as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{A}=\\mathrm{softmax}\\left((\\pmb{W}_{K}\\pmb{X})^{T}\\pmb{W}_{Q}\\pmb{X}/\\sqrt{d_{o}}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which can be viewed as the product of an unnormalized part $A_{u}$ and a normalizing multiplier $_{D}$ , that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\nA=A_{u}D^{-1},\\;\\;A_{u}=\\exp\\left((W_{K}X)^{T}W_{Q}X/\\sqrt{d_{o}}\\right),\\;\\;D=\\mathrm{diag}(\\mathbf{1}_{N}^{T}A_{u}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\exp(\\cdot)$ is element-wise. Similar in [Choromanski et al., 2020], we define softmax kernel $K_{s m}:$ $\\mathbb{R}^{d_{o}}\\times\\mathbb{R}^{d_{o}}\\rightarrow\\mathbb{R}_{+}$ as $K_{s m}({\\pmb x},{\\pmb y})=e^{{\\pmb x}^{T}{\\pmb y}}=e^{\\frac{\\|{\\pmb x}\\|^{2}+\\|{\\pmb y}\\|^{2}}{2}}K_{g u a s s}({\\pmb x},{\\pmb y})$ where $K_{g u a s s}=e^{-\\|\\pmb{x}-\\pmb{y}\\|^{2}/2}$ is the guassian kernel when the variance $\\sigma^{2}=1$ . According to Mercer\u2019s theorem [Mercer, 1909], there exists some mapping function $\\phi:\\mathbb{R}^{d_{o}}\\rightarrow\\mathbb{R}^{d_{r}}$ satisfying that $K_{s m}({\\pmb x},{\\pmb y})=\\phi({\\pmb x})^{T}\\phi({\\pmb y})$ . Thus, noting that when omitting the $\\sqrt{d_{o}}$ -renormalization and equivalently normalize key and value vectors in Eq (6), every entry in the unnormalized part $A_{u}$ can be seen as the output of softmax kernel $K_{s m}$ defined for the mapping $\\phi$ , which can be formulated as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{u}(i,j)=\\exp\\left((W_{K}x_{i})^{T}W_{Q}x_{j}\\right)=K_{s m}(W_{K}x_{i},W_{Q}x_{j})=\\phi(W_{K}x_{i})^{T}\\phi(W_{Q}x_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "There have been many forms of mapping function $\\phi(\\cdot)$ used in linear Transformers research to approximate this non-negative kernel [Choromanski et al., 2020, Katharopoulos et al., 2020, Peng et al., 2021, Lu et al., 2021]. For example, we can choose $\\phi(\\cdot)$ as positive random features which has the form $\\phi(\\pmb{x})=e^{\\pmb{w}^{T}\\pmb{x}-\\lVert\\pmb{x}\\rVert^{2}/2}$ to achieve unbiased approximation [Choromanski et al., 2020]. Alternatively, we can also choose $\\phi(\\pmb{x})=\\mathrm{elu}(\\pmb{x})+1$ proposed by Katharopoulos et al. [2020]. ", "page_idx": 4}, {"type": "text", "text": "3.2 The Gradient Descent Process of ICL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now, we begin to establish the connection between the ICL inference process of a softmax attention layer and gradient descent. We focus on a softmax attention layer in a trained Transformer model, where the parameters $\\{W_{Q},W_{K},W_{V}\\}$ have been determined and the input $\\pmb{X}=[\\pmb{X}_{D},\\pmb{X}_{T}]$ has the form introduced in Section 2.1. Then, after the inference by one attention layer, the query token at position $T+1$ will have the form $h_{T+1}^{\\prime}$ formulated by Eq (1). ", "page_idx": 4}, {"type": "text", "text": "On the other hand, given a specific softmax kernel mapping function $\\phi(x)$ that satisfies Eq (7), we can define the dual model for the softmax attention layer as ", "page_idx": 4}, {"type": "equation", "text": "$$\nf({\\boldsymbol{x}})=W{\\boldsymbol{\\phi}}({\\boldsymbol{x}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $W\\in\\mathbb{R}^{d_{o}\\times d_{r}}$ is parameters. We assume that the dual model obtains its updated weights $\\widehat{W}$ after undergoing one step of gradient descent with some loss function $\\mathcal{L}$ . Subsequently, when we take $z_{t e s t}=W_{Q}{\\pmb x}_{T+1}^{\\prime}$ as the test input, we can obtain its test prediction as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{y}_{t e s t}=f(z_{t e s t})=f\\left(W_{Q}x_{T+1}^{\\prime}\\right)=\\widehat{W}\\phi\\left(W_{Q}x_{T+1}^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We will show that $h_{T+1}^{\\prime}$ in Eq (1), is strictly equivalent to the above test prediction $\\hat{\\pmb{y}}_{t e s t}$ , which implies that the inference process of ICL involves a gradient descent step on the dual model. This can be illustrated by the following theorem: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. The query token $h_{T+1}^{\\prime}$ obtained through ICL inference process with one softmax attention layer, is equivalent to the test prediction $\\hat{\\pmb{y}}_{t e s t}$ obtained by performing one step of gradient descent on the dual model $f({\\pmb x})=W\\phi({\\pmb x})$ . The form of the loss function $\\mathcal{L}$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{V}\\pmb{x}_{i}\\right)^{T}\\pmb{W}\\phi(W_{K}\\pmb{x}_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta$ is the learning rate and $D$ is a constant. ", "page_idx": 4}, {"type": "text", "text": "Proof can be found in Appendix A. Theorem 3.1 demonstrates the equivalence between the ICL inference process and gradient descent. Below, we delve into more detailed discussions: ", "page_idx": 4}, {"type": "text", "text": "Training Set and Test Input: In fact, once the attention layer has already been trained, that is, ${\\pmb W}_{K},{\\pmb W}_{Q},{\\pmb W}_{V}$ has been determined, the demonstration tokens $[\\pmb{x}_{i}]_{i=1}^{N}$ will be used to construct a tr(aii)ning set for the dual mode(li.) Specifically, the training data has the form $\\{\\boldsymbol{z}_{s t d}^{(i)},\\boldsymbol{y}_{s t d}^{(i)}\\}_{i=1}^{N}$ wh(eir)e zstd = WKxi as inputs and ystd = WV xi as their labels. During training stage, for each input zstd, the dual model outputs its prediction $\\begin{array}{r}{\\hat{\\pmb{y}}^{(i)}=f\\left(\\pmb{z}_{s t d}^{(i)}\\right)={\\mathbfcal{W}}\\phi\\left(\\pmb{z}_{s t d}^{(i)}\\right)={\\mathbfcal{W}}\\phi\\left({\\pmb{W}}_{K}\\pmb{x}_{i}\\right)}\\end{array}$ . Then, the loss function Eq (9) can be rewritten as $\\begin{array}{r}{\\mathcal{L}=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}(\\pmb{y}_{s t d}^{(i)})^{T}\\hat{\\pmb{y}}^{(i)}}\\end{array}$ , which can be regarded as the cosine similarity. Then, using this loss function and the training data, we can perform one step of Stochastic Gradient Descent (SGD) on the dual model and obtain the updated $\\widehat{W}$ . Finally, during the testing stage, we take $z_{t e s t}=W_{Q}{\\pmb x}_{T+1}^{\\prime}$ as the test input to get its prediction w hich will be consistent with the ICL result $h_{T+1}^{\\prime}$ , that is, $\\widehat{\\pmb{y}}_{t e s t}=f(\\pmb{z}_{t e s t})=\\widehat{\\pmb{W}}\\phi\\left(\\pmb{W}_{Q}\\pmb{x}_{T+1}^{\\prime}\\right)=\\pmb{h}_{T+1}^{\\prime}.$ This process can be illustrated in Figure 1. Demonstration tokens provi de information about the training data points and the weight matrixW is optimized to learn sufficient knowledge about demonstrations. This gradient descent process using the loss function $\\mathcal{L}$ applied to $f({\\pmb x})$ can be seen as the dual form of the ICL inference process of the attention layer. ", "page_idx": 4}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/e9749cd73225a47484bcfcfdabc441e1da44dbc0e9404a234ac03a6d488be0f1.jpg", "img_caption": ["Figure 2: Left Part: The representation learning process for the ICL inference by one attention layer. Remaining Part: Comparison of the ICL Representation Learning Process (Center Left), Contrastive Learning without Negative Samples (Center Right), and Contrastive Kernel Learning (Right). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Representation Learning Lens: Even though we have now clarified the details of the gradient descent process of ICL, what does this process more profoundly reveal to us? In fact, for a encoded demonstration token $\\pmb{x}_{i}$ , the key and value mapping will generate a pair of features $W_{K}{\\pmb x}_{i}$ and $W_{V}{\\pmb x}_{i}$ that exhibit a certain distance from each other, akin to positive samples in contrastive learning. And then, $\\phi({\\pmb x})$ projects $W_{K}{\\pmb x}_{i}$ into a higher-dimensional space to capture deeper features. Finally, the weight matrix $W$ , which maps $\\phi({\\pmb W}_{K}{\\pmb x}_{i})$ back to the original space, is trained to make the mapped vector as close as possible to $W_{V}\\pmb{x}_{i}$ . This process is illustrated in Figure 2. Below, we attempt to understand this process from the perspective of existing representation learning methods introduced in Section 2.2, although we emphasize that there are certain differences between them. ", "page_idx": 5}, {"type": "text", "text": "Comparison with Contrastive Learning without Negative Samples: If we consider the key and value mapping as two types of data augmentation, then from the perspective of contrastive learning without negative samples, this process can be similarly formalized as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W}\\;\\mathcal{L}\\left(\\hat{\\pmb{y}}^{(i)},\\pmb{y}_{s t d}^{(i)}\\right)=\\mathcal{L}\\left(\\hat{\\pmb{y}}^{(i)},\\mathrm{StopGrad}(\\pmb{y}_{s t d}^{(i)})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\mathrm{StopGrad}}(\\cdot)$ is naturally applicable because there are no learning parameters involved in the generation process of the representation ${\\boldsymbol y}_{s t d}^{(i)}$ . However, it\u2019s important to note that the representation learning process of ICL is much simpler: Firstly, the online and target networks are absent while the augmentations ${\\pmb{W}}_{K}{\\pmb{x}}_{i},{\\pmb{W}}_{V}{\\pmb{x}}_{i}$ are directly used as online and target representations respectively. Secondly, the predictor head is useful and not discarded, which is then used during test stage. ", "page_idx": 5}, {"type": "text", "text": "Comparison with Contrastive Kernel Learning: Given an anchor data $\\textbf{\\em x}$ and its positive and negative samples $x^{+}$ , $x^{-}$ , contrastive kernel learning aims to optimize the loss function ${\\mathcal L}\\,=\\,\\bar{f}({\\pmb x})(f({\\pmb x}^{-})\\,-\\,f({\\pmb x}^{+}))$ where $f({\\pmb x})\\;=\\;W\\phi({\\pmb x})$ . There are significant differences in the representation learning process of ICL: Firstly, it does not involve negative samples. Secondly, there is no corresponding processing for positive samples, leading to parameter updates being solely dependent on the processing of the anchor. ", "page_idx": 5}, {"type": "text", "text": "Extension to More Complicated Scenarios: Theorem 3.1 can be naturally extended to one single Transformer layer and multiple attention layers. As for one Transformer layer formed in Section 2.1, its dual model $f^{+}({\\pmb x})\\;=\\;\\bar{{\\pmb W}}\\phi({\\pmb x})\\,+\\,b$ introduces an additional bias $^{b}$ and only $W$ is trained while b remains fixed. In addition, the labels of training set will be ys(it)d $\\pmb{y}_{s t d}^{(i)}\\,=\\,\\pmb{W}_{F}\\pmb{W}_{K}\\pmb{x}_{i}$ where $W_{F}$ has potential low-rankness property induced by ${\\mathrm{Relu}}(\\cdot)$ . As for multiple attention layers, the ICL inference process will be equivalent to sequentially performing gradient descent and making predictions on the dual model sequence. We provide more details in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Compared to Dai et al. [2022] considering the connection under linear attention setting, Theorem 3.1 gives explanation for more generally used softmax attention and offers a more detailed exploration of the training process. Additionally, unlike Von Oswald et al. [2023a,b], Ding et al. [2023]\u2019s focus on particular linear regression task and specific configurations of token and parameters, we aim to explain the process of token interactions during ICL inference in a more general setting. ", "page_idx": 5}, {"type": "text", "text": "3.3 Generalization Bound of the dual gradient descent process for ICL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this part, we are interested in the generalization bound of the ICL gradient process. When ICL inference is performed for some task $\\tau$ , we cannot provide all demonstrations related to task $\\tau$ limited by the length of input tokens. We denote $\\mathcal{S}_{\\mathcal{T}}\\subseteq\\mathbb{R}^{\\dot{d}_{i}}$ as all possible tokens for the task $\\tau$ and assume that these tokens will be selected according to the distribution $\\mathcal{D}_{7}$ . During a particular instance of ICL inference, let $\\pmb{S}=\\{\\pmb{x}_{i}\\}_{i=1}^{N}\\subseteq S_{T}$ represent the example tokens we selected. We define the function class as ${\\mathcal{F}}:=\\{f({\\pmb x})=^{-}W\\phi(W_{K}{\\pmb x})\\mid\\|W\\|\\leq w\\}$ where $\\|\\cdot\\|$ denotes the Frobenius norm. Generally, ignoring constant term in Eq (9), we consider the representation learning loss as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}_{T}}\\left[-\\left(W_{V}\\mathbf{x}\\right)^{T}f(\\mathbf{x})\\right]=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}_{T}}\\left[-\\left(W_{V}\\mathbf{x}\\right)^{T}W\\phi(W_{K}\\mathbf{x})\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $f\\in\\mathcal F$ and $\\mathcal{D}_{7}$ is the distribution for some ICL task $\\tau$ . Correspondingly, the empirical loss will be formulated as $\\begin{array}{r}{\\hat{\\mathcal{L}}(f)=-\\frac{1}{N}\\sum_{i=1}^{N}\\left(W_{V}{\\pmb x}_{i}\\right)^{T}f({\\pmb x}_{i})}\\end{array}$ and we have $\\hat{f}=\\arg\\operatorname*{min}_{f\\in\\mathcal{F}}\\hat{L}(f)$ . In addition, we denote the kernel matrix of demonstration tokens $\\boldsymbol{S}$ as ${\\pmb K}_{\\cal S}\\in\\mathbb R^{N\\times N}$ where $(K_{S})_{i,j}=$ $\\langle\\phi(W_{K}{\\pmb x}_{i}),\\phi(W_{K}{\\pmb x}_{j})\\rangle$ , that is, the inner product of the feature maps after $W_{K}$ projection between the $i$ -th token and $j$ -th token. We state our theorem as follows: ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2. Define the function class as ${\\mathcal{F}}:=\\{f({\\pmb x})=W\\phi(W_{K}{\\pmb x})\\mid\\|W\\|\\leq w\\}$ and let the loss function defined as Eq $(I O)$ . Consider the given demonstration set as $\\pmb{S}^{\\top}=\\{\\pmb{x}_{i}\\}_{i=1}^{N}$ where $S\\subseteq S_{T}$ and $S_{T}$ is all possible demonstration tokens for some task $\\tau$ . With the assumption that $\\lVert\\boldsymbol{W}_{V}\\boldsymbol{x}_{i}\\rVert$ , $\\|W\\phi(W_{K}{\\pmb x}_{i})\\|\\leq\\rho_{:}$ , then for any $\\delta>0$ , the following statement holds with probability at least $1-\\delta$ for any $f\\in\\mathcal F$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{f})\\leq\\mathcal{L}(f)+O\\left(\\frac{w\\rho d_{o}\\sqrt{\\mathrm{Tr}(\\mathbf{K}_{S})}}{N}+\\sqrt{\\frac{l o g\\frac{1}{\\delta}}{N}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof of 3.2 can be found in Appendix C. Theorem 3.2 provides the generalization bound of the optimal dual model trained on a finite selected demonstration set under a mild assumption that $\\|\\pmb{W}\\|$ is bounded. Intuitively, as the number of demonstration (and therefore the number of demonstration tokens) increases, the generalization error decreases, which is consistent with existing experimental observations [Xie et al., 2021, Garg et al., 2022, Wang et al., 2024]. ", "page_idx": 6}, {"type": "text", "text": "4 Attention Modification Inspired by the Representation Learning Lens ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Analyzing the dual gradient descent process of ICL from the perspective of representation learning inspires us to consider that: Do existing representation learning methods, especially contrastive learning methods, also involve a dual attention inference process? Alternatively, can we modify the attention mechanism by drawing on existing methods? In fact, since there are lots of mature works in representation learning especially contrastive learning, it is possible for us to achieve this by drawing on these works [He et al., 2020, Chen et al., 2020c, Wu et al., 2018, Chen et al., 2020a, Chen and He, 2021]. We will provide some simple perspectives from the loss function, data augmentations and negative samples to try to adjust attention mechanism. It is worth noting that these modifications are also applicable to the self-attention mechanism, and we will explore these variants in experiments. More details can be seen in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Attention Modification inspired by the Contrastive Loss: It can be observed that the unnormalized similarity in Eq (9) allows $\\|\\pmb{W}\\|$ to be optimized to infinity if we ignore the Layer Normalization (LN) layer to prevent this. As for one single attention layer without LN layer, to address this issue, we can introduce regularization term to constrain the norm of $W$ , specifically by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{V}\\pmb{x}_{i}\\right)^{T}W\\phi(W_{K}\\pmb{x}_{i})+\\frac{\\alpha}{2\\eta}\\|\\pmb{W}\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\alpha$ is a hyperparameter. Equivalently, the attention output Eq (1) will be modified as ", "page_idx": 6}, {"type": "equation", "text": "$$\nh_{T+1}^{\\prime}=W_{V}\\left[X_{D},(1-\\alpha)X_{T}\\right]\\mathrm{softmax}\\left((W_{K}X)^{T}W_{Q}x_{T+1}^{\\prime}/\\sqrt{d_{o}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This modification is equivalent to retaining less prompt information for query token during aggregation and relatively more demonstration information will be attended to. ", "page_idx": 6}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/368acc21ea2dd0bb24c1a2bc1712a076d8ba4fb86e58be6f2c8d16469d7efcad.jpg", "img_caption": ["Figure 3: The equivalence between ICL of one softmax attention layer and gradient descent, along with analysis on different model modifications. Left Part: $||\\hat{y}_{t e s t}-\\mathbf{\\bar{h}}_{T+1}^{\\prime}||_{2}$ as the gradient descent proceeds under setting $N=15$ ; Remaining Part: the performance for regularized models (Center Left), augmented models (Center Right) and negative models (Right) with different settings. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Attention Modification inspired by the Data Augmentation: If we analogize the key and value mappings to data augmentations in contrastive learning, then for the representation learning process of ICL, these overly simple linear augmentations may limit the model\u2019s ability to learn deeper representations. Thus, more complicated augmentations can be considered. Denoting these two augmentations as $g_{1}$ and $g_{2}$ , the loss function will be modified as ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\mathcal{L}}=-{\\frac{1}{\\eta D}}\\sum_{i=1}^{N}\\left[g_{1}(W_{V}{\\pmb x}_{i})\\right]^{T}W{\\phi}(g_{2}(W_{K}{\\pmb x}_{i})).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Correspondingly, the attention layer can be adjusted as, ", "page_idx": 7}, {"type": "equation", "text": "$$\nh_{T+1}^{\\prime}=g_{1}(W_{V}X)\\mathrm{softmax}\\left([g_{2}(W_{K}X)]^{T}W_{Q}x_{T+1}^{\\prime}/\\sqrt{d_{o}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $g_{1}(\\cdot)$ and $g_{2}(\\cdot)$ will be column-wise here. Here we add augmentations for all tokens instead of only demonstration ones to maintain uniformity in the semantic space. In experiments, we simply select MLP for $g_{1}$ and $g_{2}$ . It\u2019s worth noting that here we only propose the framework, and for different tasks, the augmentation approach should be specifically designed to adapt them. ", "page_idx": 7}, {"type": "text", "text": "Attention Modification inspired by the Negative Samples: Negative samples play a crucial role in preventing feature collapse in contrastive learning methods while the representation learning process of ICL only brings a single pair of features closer, lacking the modeling of what should be pushed apart, which could potentially limit the model\u2019s ability to learn representations effectively. Therefore, we can introduce negative samples to address this: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\left.\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{V}\\tilde{x}_{i}\\right)^{T}W\\phi(W_{K}x_{i}),\\quad\\tilde{x}_{i}=x_{i}-\\frac{\\beta}{|\\mathcal{N}(i)|}\\sum_{j\\in\\mathcal{N}(i)}x_{j},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathcal{N}(i)$ is the set of the negative samples for $\\pmb{x}_{i}$ and $\\beta$ is a hyperparameter. Correspondingly, the attention layer is modified as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\boldsymbol{h}_{T+1}^{\\prime}=\\boldsymbol{W}_{V}\\left[\\tilde{\\boldsymbol{X}}_{D},\\boldsymbol{X}_{T}\\right]\\mathrm{softmax}\\left((\\boldsymbol{W}_{K}\\boldsymbol{X})^{T}\\boldsymbol{W}_{Q}\\boldsymbol{x}_{T+1}/\\sqrt{d_{o}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ${\\tilde{\\cal X}}_{D}\\;=\\;[{\\tilde{\\pmb x}}_{1},{\\tilde{\\pmb x}}_{2},...,{\\tilde{\\pmb x}}_{N}]$ . Here we simply use other tokens as negative samples and we emphasize that for specific tasks, an appropriate design of negative samples will be more effective. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we design experiments on synthetic tasks to support our findings and more experiments including on more realistic tasks can be seen in Appendix E. The questions of interest are: (i) Is the result of ICL inference equivalent to the test prediction of the trained dual model? (ii) Is it potential to improve the attention mechanism from the perspective of representation learning? ", "page_idx": 7}, {"type": "text", "text": "Linear Task Setting: Inspired by Von Oswald et al. [2023a], to validate the equivalence and demonstrate the effectiveness of the modifications, we firstly train one softmax self-attention layer using linear regression tasks. We generate the task by $\\scriptstyle s\\ =\\ W t$ where every element of $W\\,\\in$ $\\mathbb{R}^{d_{s}\\breve{\\times}d_{t}}$ is sampled from a normal distribution $W_{i j}\\sim\\mathcal{N}(0,1)$ and $\\pmb{t}$ from uniform distribution $\\pmb{t}\\sim$ ", "page_idx": 7}, {"type": "text", "text": "$U(-1,1)^{d_{t}}$ .i nWpeu ts emt $d_{t}=11$ wahnedr $d_{s}=1$ .s t Tthoekne,n  atw ielal cbhe  sutespe,d  wase  tuhsee  qgueenreyr tatoekde $\\{\\pmb{x}_{i}=[t_{i};s_{i}]\\}_{i=1}^{N+1}$ $\\mathbf{\\deltaX}$ will be masked, that is, ${\\bf}x_{N+1}=[t_{i};0]$ . Here we consider only one query token $T=0$ ) and we denote ${\\pmb x}_{T+1}^{\\prime}={\\pmb x}_{N+1}$ to maintain consistency of notation in Section 2.1. Finally, the attention layer is trained to predict $\\hat{s}_{N+1}$ to approximate the true label $s_{N+1}$ using mean square error (MSE) loss. ", "page_idx": 8}, {"type": "text", "text": "Model Setting: It is worth noting that to facilitate direct access to the dual model, we use positive random features as kernel mapping functions (Performer architecture [Choromanski et al., 2020]) to approximate the standard softmax attention, that is, $\\phi(\\pmb{x})=e^{\\pmb{w}^{T}\\pmb{x}-\\lVert\\pmb{x}\\rVert^{2}/2}$ where $w\\sim\\mathcal{N}(0,I)$ . We set the dimension of the random features as $d_{r}=100(d_{t}+d_{s})=1200$ to obtain relatively accurate estimation. After training, the weights of the attention layer have been determined. Thus, given specified input $\\mathbf{\\deltaX}$ , we can construct the dual model $f({\\pmb x})\\stackrel{!}{=}{\\pmb W}\\phi({\\pmb x})$ and its corresponding training data and test input according to Theorem 3.1. ", "page_idx": 8}, {"type": "text", "text": "We perform three experiments under different random seeds for linear regression tasks with the results of one presented in Figure 3. In addition, we also conduct more experiments including these on trigonometric, exponential synthetic regression tasks and more realistic tasks. More details of experiments setting and results can be found in Appendix E. We mainly discuss the results on the linear regression task as follows. ", "page_idx": 8}, {"type": "text", "text": "Equivalence Between ICL and Gradient Descent: To answer the first question, we generate the test input $X_{t e s t}$ using the same method as training and obtain the ICL result of the query token $h_{T+1}^{\\prime}$ . On the other hand, we use $X_{t e s t}$ to train the dual model according to Theorem 3.1 and get the test prediction $\\hat{\\pmb{y}}_{t e s t}$ . The result is shown in the left part part of Figure 3. It can be observed that after $N=15$ epochs training on the dual model, the test prediction $\\hat{\\pmb{y}}_{t e s t}$ is exactly equivalent to the ICL inference result $h_{T+1}^{\\prime}$ by one softmax attention layer, which aligns with our analysis in Theorem 3.1. More detailed experiments can be seen in Appendix E.1. ", "page_idx": 8}, {"type": "text", "text": "Analysis on the Modifications: In Section 4, we discussed different modifications to the attention mechanism from perspectives of contrastive loss, data augmentation and negative samples. Here we call these modifications regularized models, augmented models and negative models respectively. More details of modifications for self-attention mechanism can be seen in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "For regularized models, we vary different $\\alpha$ to investigate the impact on pretraining performance under the same setting, as shown in the center left part of Figure 3. It can be observed that when $\\alpha>0$ , the regularized models converges to a poorer result while when $\\alpha<0$ , the model converges faster and achieves final results comparable to the normal model without regularization $(\\alpha=0$ ). At least for this setting, this is a little contrary to our initial intention of applying regularization to the contrastive loss where $\\alpha$ should be positive. We explain it that the appropriate $\\alpha$ contributes to achieving a full-rank attention matrix as stated in Appendix D, preserving information and accelerating convergence. ", "page_idx": 8}, {"type": "text", "text": "For augmented models, we simply choose a single-layer MLP for $g_{1}(\\cdot)$ and $g_{2}(\\cdot)$ as data augmentations to enhance the value and key embeddings respectively in Eq (14) and we choose GELU [Hendrycks and Gimpel, 2016] as the activation function. It can be observed in the center right part of Figure 3 that when we only use $g_{2}$ , that is, only provide augmentation for keys, the model actually shows slightly faster convergence than other cases. Furthermore, when we use two-layer MLP as $g_{2}^{+}(x)$ as a more complicated augmentation function, the result indicates that although the model initially converges slightly slower due to the increased number of parameters, it eventually accelerates convergence and achieves a better solution. This indicates that appropriate data augmentation indeed have the potential to enhance the capabilities of the attention layer. ", "page_idx": 8}, {"type": "text", "text": "For negative models, we select the $k$ tokens with the lowest attention scores as negative samples for each token. From Eq (15), we can see that it is equivalent to subtracting a certain value from the attention scores corresponding to those negative samples. We vary the number of negative samples $k$ and $\\beta$ in Eq (15) and the results are shown in the right part of Figure 3. It can be found that the model has the potential to achieve slightly faster convergence with appropriate settings $k=3$ and $\\beta=0.1$ ). In fact, it can be noted that in the original attention mechanism, attention scores are always non-negative, indicating that some irrelevant information will always be preserved to some extent. However, in the modified structure, attention scores can potentially become negative, which makes the model more flexible to utilize information. Certainly, as we discussed in Section 4, for different tasks, more refined methods of selecting augmentations and constructing negative samples may be more effective and we also leave these aspects for future. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Since Transformers have shown remarkable ICL abilities [Brown et al., 2020], many works have aimed to analyze the underlying mechanisms [Garg et al., 2022, Wang et al., 2023]. To explain how Transformers can learn new tasks without parameter updates given few demonstrations, an intuitive idea is to link ICL with (implicit) gradient updates. The most relevant work to ours is that of Dai et al. [2022], which utilizes the dual form to understand ICL as an implicit fine-tuning (gradient descent) of the original model under a linear attention setting [Aiserman et al., 1964, Irie et al., 2022]. They design a specific fine-tuning setting where only the parameters for the key and value projection are updated and the causal language modeling objective is adopted. In this context, they find ICL will have common properties with fine-tuning. Based on this, Deutch et al. [2024] investigate potential shortcomings in the evaluation metrics used by Dai et al. [2022] in real model assessments and propose a layer-causal GD variant that performs better in simulating ICL. As a comparison, our research also uses the dual form to analyze the nonlinear attention layer and explores the specific form of the loss used in the training process. However, we link ICL to the gradient descent performed on the dual model rather than fine-tuning the original model. The former process utilizes a self-supervised representation learning loss formalized as Eq (9) determined by the attention structure itself while performing supervised fine-tuning on the original model is often determined by task-specific training objectives (or manually specified causal language modeling objective Dai et al. [2022]). A more formal and detailed comparison can be found in Appendix F. ", "page_idx": 9}, {"type": "text", "text": "Additionally, many other works also link ICL with gradient descent, aiming to explore the Transformer\u2019s ability to perform gradient descent algorithms to achieve ICL [Bai et al., 2023, Schlag et al., 2021]. Aky\u00fcrek et al. [2022] reveal that under certain constructions, Transformer can implement simple basic operations (mov, mul, div and aff), which can be combined to further perform gradient descent. Von Oswald et al. [2023a] provide a simple and appealing construction for solving least squares solutions in the linear attention setting. Subsequently, Zhang et al. [2023a], Ahn et al. [2023], Mahankali et al. [2023] provide theoretical evidence showing that the local or global minima will have a form similar to this specific construction proposed by Von Oswald et al. [2023a] under certain assumptions. These works, both experimentally and theoretically, often focus on specific linear regression tasks $(y\\,=\\,w^{T}x)$ and specific structured input format where each token takes the form $[{\\pmb x},y]$ consisting of the input part $\\textbf{\\em x}$ and the label part $y$ . In addition, the label part of the final query to be predicted is masked, represented as $[\\pmb{x},0]$ . Subsequent works have expanded this exploration under more complicated setups, including examining nonlinear attention instead of linear attention[Cheng et al., 2023, Collins et al., 2024], using unstructured inputs rather than structured ones[Xing et al., 2024], and considering casual or autoregressive setting[Ding et al., 2023, Von Oswald et al., 2023b]. As a comparison to these works, our work does not target specific tasks like linear regression; therefore, we do not make detailed assumptions about the model weights (simply treated as weights after pre-training) or specific input forms. Instead, we aim to view the ICL inference process from the perspective of representation learning in the dual model. However, we would like to point out that under these specific weight and input settings, an intuitive explanation can also be provided from a representation learning perspective (see Appendix F). We also notice that Shen et al. [2023] experimentally show that there may exist differences between ICL inference in LLMs and the fine-tuned models in real-world scenarios from various perspectives and assumptions used in previous works may be strong. As mentioned earlier, our analysis primarily focus on linking ICL with gradient descent on the dual model of a simplified Transformer rather than fine-tuning the original model. Analyzing more realistic models will also be our future directions. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Impact Statements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we establish a connection between the ICL process of Transformers and gradient descent of the dual model, offering novel insights from a representation learning lens. Based on this, we propose modifications for the attention layer and experiments under our setup demonstrate their potential. Although we have made efforts in understanding ICL, there are still some limitations in our analysis: (1) our work primarily focuses on the simplified Transformer and the impact of structures like layer normalization, residual connections, and others requires more nuanced analysis; (2) for more tasks and settings, the proposed model modifications may require more nuanced design and validation. We leave these aspects for future exploration. And we believe that this work mainly studies the theory of in-context learning, which does not present any foreseeable societal consequence. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We sincerely appreciate the anonymous reviewers for their helpful suggestions and constructive comments. This research was supported by National Natural Science Foundation of China (No.62476277, No.6207623), Beijing Natural Science Foundation (No.4222029), CCF-ALIMAMA TECH Kangaroo Fund (No.CCF-ALIMAMA OF 2024008), and Huawei-Renmin University joint program on Information Retrieval. We also acknowledge the support provided by the fund for building worldclass universities (disciplines) of Renmin University of China and by the funds from Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China, from Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education, from Intelligent Social Governance Interdisciplinary Platform, Major Innovation & Planning Interdisciplinary Platform for the \u201cDoubleFirst Class\u201d Initiative, Renmin University of China, from Public Policy and Decision-making Research Lab of Renmin University of China, and from Public Computing Cloud, Renmin University of China. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36:45614\u201345650, 2023.   \nMA Aiserman, Emmanuil M Braverman, and Lev I Rozonoer. Theoretical foundations of the potential function method in pattern recognition. Avtomat. i Telemeh, 25(6):917\u2013936, 1964.   \nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.   \nShun-ichi Amari. Backpropagation and stochastic gradient descent method. Neurocomputing, 5(4-5): 185\u2013196, 1993.   \nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912\u20139924, 2020.   \nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020a.   \nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020b.   \nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.   \nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020c.   \nXiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent to learn non-linear functions in context. arXiv preprint arXiv:2312.06528, 2023.   \nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.   \nLiam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, and Sanjay Shakkottai. In-context learning with transformers: Softmax attention adapts to function lipschitzness. arXiv preprint arXiv:2402.11639, 2024.   \nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn incontext? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022.   \nGilad Deutch, Nadav Magar, Tomer Natan, and Guy Dar. In-context learning and gradient descent revisited. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1017\u20131028, 2024.   \nNan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, and Radu Soricut. Causallm is not optimal for in-context learning. arXiv preprint arXiv:2308.06912, 2023.   \nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \nPascal Esser, Maximilian Fleissner, and Debarghya Ghoshdastidar. Non-parametric representation learning with kernels. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 11910\u201311918, 2024.   \nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.   \nJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.   \nJianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12175\u201312185, 2022.   \nChi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models explained as kernel regression. arXiv preprint arXiv:2305.12766, 2023.   \nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.   \nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \nKazuki Irie, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In International Conference on Machine Learning, pages 9639\u20139659. PMLR, 2022.   \nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. pages 5156\u20135165, 2020.   \nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, page 2. Minneapolis, Minnesota, 2019.   \nYingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. arXiv preprint arXiv:2301.07067, 2023.   \nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \nJiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, and Li Zhang. Soft: Softmax-free transformer with linear complexity. Advances in Neural Information Processing Systems, 34:21297\u201321309, 2021.   \nArvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023.   \nAndreas Maurer. A vector-contraction inequality for rademacher complexities. In Algorithmic Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings 27, pages 3\u201317. Springer, 2016.   \nJ. Mercer. Functions of positive and negative type, and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 209:415\u2013446, 1909. ISSN 02643952. URL http://www.jstor.org/stable/91043.   \nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.   \nHyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4004\u20134012, 2016.   \nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021.   \nIsaac Reid, Krzysztof Marcin Choromanski, Valerii Likhosherstov, and Adrian Weller. Simplex random features. In International Conference on Machine Learning, pages 28864\u201328888. PMLR, 2023.   \nAdam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer, Peter J. Liu, Sharan Narang, Wei Li, and Yanqi Zhou. Exploring the limits of transfer learning with a unified text-to-text transformer. Technical report, Google, 2019.   \nNikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning, pages 5628\u20135637. PMLR, 2019.   \nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pages 9355\u20139366. PMLR, 2021.   \nLingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent? arXiv preprint arXiv:2310.08540, 2023.   \nYuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In International Conference on Machine Learning, pages 10268\u201310278. PMLR, 2021.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nJohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. pages 35151\u201335174, 2023a.   \nJohannes Von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023b.   \nAlex Wang. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.   \nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. arXiv preprint arXiv:2305.14160, 2023.   \nXinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Advances in Neural Information Processing Systems, 36, 2024.   \nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.   \nT Wolf. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733\u20133742, 2018.   \nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \nYue Xing, Xiaofeng Lin, Namjoon Suh, Qifan Song, and Guang Cheng. Beneftis of transformer: Incontext learning in linear regression tasks with unstructured data. arXiv preprint arXiv:2402.00743, 2024.   \nFelix Xinnan X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice, and Sanjiv Kumar. Orthogonal random features. Advances in neural information processing systems, 29, 2016.   \nRuiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023a.   \nYufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023b. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Details of Theorem 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We repeat Theorem 3.1 as follows and provide proof and more discussion for it. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.1. The query token $h_{T+1}^{\\prime}$ obtained through ICL inference process with one softmax attention layer, is equivalent to the test prediction $\\hat{\\pmb{y}}_{t e s t}$ obtained by performing one step of gradient descent on the dual model $f(\\pmb{x})=W\\bar{\\phi}(\\pmb{x})$ . The form of the loss function $\\mathcal{L}$ is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{V}\\pmb{x}_{i}\\right)^{T}\\pmb{W}\\phi(W_{K}\\pmb{x}_{i}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where \u03b7 is the learning rate and $D$ is a constant. ", "page_idx": 14}, {"type": "text", "text": "Proof. The derivative of $\\mathcal{L}$ with respect to $W$ should be: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}}=-\\left[\\sum_{i=1}^{N}\\frac{1}{\\eta D}W_{V}\\pmb{x}_{i}\\otimes\\phi(\\pmb{W}_{K}\\pmb{x}_{i})\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, after one step of gradient descent , the learned $\\widehat{W}$ will be ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{W}}=\\pmb{W}_{0}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}}=\\pmb{W}_{0}+\\left[\\sum_{i=1}^{N}\\frac{1}{D}\\pmb{W}_{V}\\pmb{x}_{i}\\otimes\\phi(\\pmb{W}_{K}\\pmb{x}_{i})\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $W_{0}$ is the initialization of the reference model and $\\eta$ is the learning rate. So the test prediction will be ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{y}_{t e s t}=W_{0}\\phi\\left(W_{Q}\\pmb{x}_{T+1}^{\\prime}\\right)+\\left[\\sum_{i=1}^{N}\\frac{1}{D}W_{V}\\pmb{x}_{i}\\otimes\\phi(W_{K}\\pmb{x}_{i})\\right]\\phi\\left(W_{Q}\\pmb{x}_{T+1}^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "On the other hand, from the perspective of ICL process with one attention layer, with Eq (7) in our mind, we can rewrite Eq (1) as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{T+1}^{\\prime}=W_{V}X\\mathrm{softmax}\\left(\\frac{\\left(W_{K}X\\right)^{T}W_{Q}x_{T+1}^{\\prime}}{\\sqrt{d_{o}}}\\right)}\\\\ &{\\qquad=\\cfrac{1}{D^{\\prime}}W_{V}[X_{D},X_{T}]\\left[\\phi(W_{K}X_{D}),\\phi(W_{K}X_{T})\\right]^{T}\\phi(W_{Q}x_{T+1}^{\\prime})}\\\\ &{\\qquad=\\cfrac{1}{D^{\\prime}}[V_{D},V_{T}]\\left[\\phi(K_{D}),\\phi(K_{T})\\right]^{T}\\phi(q),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we use $[V_{D},V_{T}]=W_{V}[X_{D},X_{T}]$ , $[{\\cal K}_{D},{\\cal K}_{T}]={\\cal W}_{K}[{\\cal X}_{D},{\\cal X}_{T}]$ , ${\\pmb q}=W_{Q}{\\pmb x}_{T+1}^{\\prime}$ for simplify and $D^{\\prime}\\,=\\,{\\bf1}_{N}^{T}\\phi({\\bf K}_{D})^{T}\\phi({\\pmb q})\\,+{\\bf1}_{T}^{T}\\phi({\\pmb K}_{T})^{T}\\phi({\\pmb q})$ is a constant to normalize the equivalent attention block. Further, we expand the above equation to connect the inference process of ICL using softmax attention with the gradient descent as follows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{T+1}^{\\prime}=\\displaystyle\\frac{1}{D^{\\prime}}V_{T}\\phi(\\pmb{K}_{T})^{T}\\phi(\\pmb{q})+\\displaystyle\\frac{1}{D^{\\prime}}V_{D}\\phi(\\pmb{K}_{D})^{T}\\phi(\\pmb{q})}\\\\ {=\\pmb{W}_{0}^{\\prime}\\phi(\\pmb{q})+\\displaystyle\\frac{1}{D^{\\prime}}\\left[\\sum_{i=1}^{N}V_{D}^{(i)}\\otimes\\phi(\\pmb{K}_{D}^{(i)})\\right]\\phi(\\pmb{q})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{W_{0}^{\\prime}=\\frac{1}{D^{\\prime}}V_{T}\\phi(K_{T})^{T}}\\end{array}$ and $V_{D}^{(i)},K_{D}^{(i)}$ are the $i$ -th column vetors respectively. ", "page_idx": 14}, {"type": "text", "text": "Then, in Eq (18), when setting the initialization $W_{0}=W_{0}^{\\prime}$ and the constant $D=D^{\\prime}$ , we will find that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{y}_{t e s t}=W_{0}\\phi(\\pmb{q})+\\frac{1}{D}\\left[\\sum_{i=1}^{N}V_{D}^{(i)}\\otimes\\phi(\\pmb{K}_{D}^{(i)})\\right]\\phi(\\pmb{q})=\\pmb{h}_{T+1}^{\\prime},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which means $\\hat{\\pmb y}_{t e s t}$ is strictly equivalent to $h_{T+1}^{\\prime}$ . Thus, we have completed our proof. ", "page_idx": 14}, {"type": "text", "text": "Given a reference model $f({\\pmb x})=W\\phi({\\pmb x})$ , by comparing $\\operatorname{Eq}$ (19) and Eq (4), we can easily observe that the gradient descent on the loss function $\\mathcal{L}$ applied to $f(x)$ is the dual form of the inference process of ICL, where $V_{D}^{(i)}$ , $\\phi(\\pmb{K}_{D}^{(i)})$ and $\\phi(\\pmb q)$ play the roles of backpropagation signals, training inputs and test inputs respectively. Recalling the form of $\\operatorname{Eq}$ (4), we can interpret the $W_{0}$ as the initialization of the weight matrix which provide the information under the zero-shot case while the second part in Eq (19) shows that the demonstration examples in ICL acts as the training samples in gradient descent. The reference model $f({\\pmb x})=W\\phi({\\pmb x})$ , initialized with $W_{0}$ , will have test prediction $\\bar{y}_{t e s t}=h_{T+1}^{\\prime}$ after training. This is also why we refer to it as the dual model of the softmax attention layer. We also note that for different demonstrations, even though the model has the same query input, the different given demonstrations will result in different output results. This is equivalent to the dual model performing gradient descent in different directions from the same initialization. ", "page_idx": 15}, {"type": "text", "text": "B Extensions to more complex scenarios ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Theorem 3.1, we provided the dual form of gradient descent for the ICL of one softmax attention layer. Here, we extend the conclusion to more complex scenarios, including one Transformer layer (attention layer plus one FFN layer) and multiple attention layers. ", "page_idx": 15}, {"type": "text", "text": "B.1 Extension to one Transformer Layer ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As for one Transformer layer introduced in Section 2.1, we define the new dual model as ", "page_idx": 15}, {"type": "equation", "text": "$$\nf^{+}({\\pmb x})={\\pmb W}\\phi({\\pmb x})+{\\pmb b}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We will show that after performing gradient descent on $W$ , the test output $\\hat{\\pmb y}_{t e s t}=f^{+}({\\cal W}_{Q}{\\pmb x}_{T+1}^{\\prime})$ will be equivalent to $\\hat{\\pmb x}_{T+1}^{\\prime}$ . Our theorem is given as follows. ", "page_idx": 15}, {"type": "text", "text": "Theorem B.1. The output $\\hat{\\pmb x}_{N+1}^{\\prime}$ of ICL inference process with one Transformer layer, is strictly equivalent to the test prediction of its dual model $f^{+}({\\pmb x})={\\pmb W}\\phi({\\pmb x})+{\\pmb b}$ , where $f(x)$ is trained under the loss function $\\mathcal{L}$ formed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{F}W_{V}\\pmb{x}_{i}\\right)^{T}\\left(W\\phi(W_{K}\\pmb{x}_{i})+\\pmb{b}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\eta$ is the learning rate, $D$ is a constant, and $W_{F}$ will be determined once the specified pre-trained model, demonstrations and query tokens are given. ", "page_idx": 15}, {"type": "text", "text": "Proof. Recalling the proof of Theorem 3.1, we can rewrite Eq (1) as ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{T+1}^{\\prime}=W_{0}\\phi\\left(W_{Q}x_{T+1}^{\\prime}\\right)+\\left[\\sum_{i=1}^{N}\\frac{1}{D}W_{V}x_{i}\\otimes\\phi(W_{K}x_{i})\\right]\\phi\\left(W_{Q}x_{T+1}^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $D=\\mathbf{1}_{N}^{T}\\phi(W_{K}X_{D})^{T}\\phi(W_{Q}\\mathbf{x}_{T+1}^{\\prime})+\\mathbf{1}_{T}^{T}\\phi(W_{K}X_{T})^{T}\\phi(W_{Q}\\mathbf{x}_{T+1}^{\\prime})$ is a constant to normalize the attention scores and $\\begin{array}{r}{{W_{0}}=\\frac{1}{D}(W_{V}{X_{T}})\\phi(W_{K}{X_{T}})^{T}}\\end{array}$ . Furthermore, $h_{N+1}^{\\prime}$ will be taken as input for the FFN sublayer and the Eq (2) can be rewritten as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{T+1}^{\\prime}=W_{2}I_{M}(W_{1}h_{T+1}^{\\prime}+b_{1})+b_{2}=W_{2}I_{M}W_{1}h_{T+1}^{\\prime}+W_{2}I_{M}b_{1}+b_{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pmb{I}_{M}\\in\\mathbb{R}^{d\\times d}$ is a diagonal matrix whose $i$ -th diagonal element will be one if $(W_{1}h_{T+1}^{\\prime}{+}b_{1})_{i}\\geq$ 0 otherwise be zero. We need to note that this process is reasonable: for given demonstration and query tokens, once the parameters $\\{W_{Q},W_{K},\\bar{W}_{V},W_{1},b_{1}\\}$ of the Transformer layer are fixed after training, $\\scriptstyle{I_{M}}$ will be determined implicitly (otherwise, $\\scriptstyle{I_{M}}$ would be a function that varies with these settings). For simplify, we rewrite $\\hat{\\pmb x}_{T+1}^{\\prime}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\pmb{x}}_{T+1}^{\\prime}={\\pmb{W}}_{F}{\\pmb{h}}_{T+1}+{\\pmb{b}}_{F},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $W_{F}=W_{2}I_{M}W_{1}$ and $b_{F}=W_{2}I_{M}b_{1}+b_{2}$ . Furthermore, expanding $h_{T+1}^{\\prime}$ in the above Equation, we get: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\pmb{x}}_{T+1}^{\\prime}=W_{F}W_{0}\\phi\\left(W_{Q}\\pmb{x}_{T+1}^{\\prime}\\right)+\\displaystyle\\left[\\sum_{i=1}^{N}\\frac{1}{D}W_{F}W_{V}\\pmb{x}_{i}\\otimes\\phi(W_{K}\\pmb{x}_{i})\\right]\\phi\\left(W_{Q}\\pmb{x}_{T+1}^{\\prime}\\right)+b_{F}}\\\\ &{\\qquad=\\Bigg[W_{F}W_{0}+\\displaystyle\\sum_{i=1}^{N}\\frac{1}{D}W_{F}W_{V}\\pmb{x}_{i}\\otimes\\phi(W_{K}\\pmb{x}_{i})\\Bigg]\\phi\\left(W_{Q}\\pmb{x}_{T+1}^{\\prime}\\right)+b_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, we define a reference model: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf^{+}({\\pmb x})={\\pmb W}\\phi({\\pmb x})+{\\pmb b},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\phi(\\cdot)$ is exactly the mapping function satisfying Eq (7) to approximate the softmax kernel. Given the loss formed in Eq (21), we can note that the right part in $\\mathcal{L}$ is exactly the output of this reference model when taking $W_{K}{\\pmb x}_{i}$ as input, that is, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{F}W_{V}x_{i}\\right)^{T}\\left(W\\phi(W_{K}x_{i})+b\\right)=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{F}W_{V}x_{i}\\right)^{T}f^{+}\\left(W_{K}x_{i}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can calculate the derivative of $\\mathcal{L}$ with respect to $W$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}}=-\\frac{1}{\\eta D}\\left[\\sum_{i=1}^{N}W_{F}\\pmb{W}_{V}\\pmb{x}_{i}\\otimes\\phi(\\pmb{W}_{K}\\pmb{x}_{i})\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Suppose that the weight matrix $W$ in the reference model $f({\\boldsymbol{x}})$ is initialized as $W_{i n i t}$ , then using one step of stochastic gradient descent (SGD) [Amari, 1993] with learning rate $\\eta$ , the weight matrix $W$ will be updated as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{W}=W_{i n i t}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial W}=W_{i n i t}+\\left[\\sum_{i=1}^{N}\\frac{1}{D}W_{F}W_{V}x_{i}\\otimes\\phi(W_{K}x_{i})\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Compared to Eq (23), we can set ${\\pmb{W}}_{i n i t}={\\pmb{W}}_{F}{\\pmb{W}}_{0}$ , $\\pmb{b}=\\pmb{b}_{F}$ and take $W_{Q}{\\pmb x}_{T+1}^{\\prime}$ as test input. Then, after one step update to $W$ , the output of the reference model will be ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f^{+}(W_{Q}x_{T+1}^{\\prime})=\\widehat{W}\\phi(W_{Q}x_{T+1}^{\\prime})+b}&{}\\\\ &{\\qquad\\qquad=\\left[W_{i n i t}+\\displaystyle\\sum_{i=1}^{N}\\frac{1}{D}W_{F}W_{V}x_{i}\\otimes\\phi(W_{K}x_{i})\\right]\\phi(W_{Q}x_{T+1}^{\\prime})+b}\\\\ &{\\qquad\\qquad=\\left[W_{F}W_{0}+\\displaystyle\\sum_{i=1}^{N}\\frac{1}{D}W_{F}W_{V}x_{i}\\otimes\\phi(W_{K}x_{i})\\right]\\phi\\left(W_{Q}x_{T+1}^{\\prime}\\right)+b_{F}=\\hat{x}_{T+1}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies that if we initialize the reference model $f^{+}({\\pmb x})={\\pmb W}\\phi({\\pmb x})+{\\pmb b}$ with ${\\cal W}_{i n i t}={\\cal W}_{F}{\\cal W}_{0}$ , $\\pmb{b}=\\pmb{b}_{F}$ , then after one step of gradient descent for $W$ , the test output of $f^{+}(W_{Q}{\\pmb x}_{N+1})$ will be identical to the ICL result of one Transformer layer. Thus, we call the reference model with setting ${\\pmb{W}}_{i n i t}={\\pmb{W}}_{F}{\\pmb{W}}_{0}$ , $\\pmb{b}=\\pmb{b}_{F}$ as the dual model corresponding to the ICL inference process. Finally, we complete our proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Now, we discuss Theorem B.1 from the following perspectives: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Training set and test input: In fact, we can observe that the loss function $\\mathcal{L}$ can be seen as the sum of inner products of $N$ vector-pairs. In Eq (21), the right vector happens to be the predicted output $\\bar{\\pmb{y}}_{s t d}^{(i)}=f^{+}(\\pmb{z}_{s t d}^{(i)})=\\bar{\\pmb{W}}\\phi(\\pmb{z}_{s t d}^{(i)})\\bar{+}b$ of the dual model for training input $z_{s t d}^{(i)}=W_{K}{\\pmb x}_{i}$ . Correspondingly, the vector on the left can be regarded as the true label $\\pmb{y}_{s t d}^{(i)}=W_{F}W_{V}\\pmb{x}_{i}$ . In other words, it can be seen that the dual model performs one step SGD given training set {zs(it)d, ys(it)d}iN=1 on W using the loss L: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(\\pmb{y}_{s t d}^{(i)}\\right)^{T}\\hat{\\pmb{y}}_{s t d}^{(i)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "And then taking $z_{t e s t}=W_{Q}{\\pmb x}_{i}$ as test input, it finally output the prediction $\\pmb{y}_{t e s t}$ , which achieves the ICL result $\\hat{\\pmb{x}}_{N+1}$ . Compared to Theorem 3.1, after introducing the FFN layer, the main difference is that the labels of the training data become ys(it)d $\\pmb{y}_{s t d}^{(i)}=W_{F}W_{V}\\pmb{x}_{i}$ instead of $\\pmb{y}_{s t d}^{(i)}=\\pmb{W}_{V}\\pmb{x}_{i}$ . Additionally, compared to $f({\\boldsymbol{x}})$ , an extra bias $b$ is introduced in the new dual model $f^{+}(x)$ , which also have a different initialization ${\\cal W}_{i n i t}={\\cal W}_{F}{\\cal W}_{0}$ rather than $W_{0}$ . We also need to note that in the dual model $f^{+}(x)$ , only $W$ is trained, while $^{b}$ remains unchanged after initialization. ", "page_idx": 16}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/2dfbb78ce9512491703ac4a405ccc7bef931ec5455d1cedd19057e5a009be9ad.jpg", "img_caption": ["Figure 4: The representation learning process for the ICL inference by one Transformer layer. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "\u2022 Potential Low-rankness of $W_{F}$ : Noting that ${\\cal W}_{F}\\,=\\,{\\cal W}_{2}{\\cal I}_{M}{\\cal W}_{1}$ where $W_{1}\\,\\in\\,\\mathbb{R}^{d_{h}\\times d}$ , $W_{2}\\in\\mathbb{R}^{d\\times d_{h}}$ , $I_{M}\\in\\mathbb{R}^{d_{h}\\times d_{h}}$ (here we assume that $d_{i}=d_{o}=d$ for simplify), the rank of $W_{F}$ will satisfy ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{Rank}(W_{F})\\leq\\operatorname*{min}\\left\\{\\operatorname{Rank}(W_{1}),\\operatorname{Rank}(W_{2}),\\operatorname{Rank}(I_{M})\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We observe that $\\pmb{I}_{M}$ is a diagonal matrix with elements being zero or one, and its rank is determined by the number of non-zero elements. Here, we can make a mild assumption that we can set $\\operatorname{Rank}(W_{1})=\\operatorname{Rank}(W_{2})=\\operatorname*{min}\\{d,d_{h}\\}$ . This assumption is quite mild as even for any random square matrix as it will be non-singular with probability 1. In addition, we also assume $d_{h}\\,>\\,d$ which is consistent with settings in practice. Therefore, we get $\\operatorname{Rank}(W_{1})=\\operatorname{Rank}(W_{2})=d$ , and the upper bound of $\\mathrm{Rank}(W_{F})$ will be ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{Rank}(W_{F})\\leq\\operatorname*{min}\\left\\{d,\\operatorname{Rank}(I_{M})\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we can find that if we want to avoid losing information, $W_{F}$ should strive to maintain $\\mathrm{Rank}(I_{M})>d$ which will be more easily achieved as $d_{h}$ becomes larger than $d$ . Otherwise, $\\operatorname{Rank}(I_{M})$ is likely to gradually decrease with an increase in the number of Transformer layers. This explains the necessity of setting $d_{h}\\,>\\,d$ in practice. In some cases where $\\mathrm{Rank}(I_{M})<d$ , meaning that the number of non-zero elements in $\\scriptstyle I_{M}$ or positive elements in $W_{1}h_{N+1}+b_{1}$ is less than $d$ , the upper bound of $\\mathrm{Rank}(W_{F})$ will be $\\operatorname{Rank}(I_{M})$ and the lower bound of $\\mathrm{Rank}(W_{F})$ will be given as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{Rank}(W_{F})\\geq\\operatorname{Rank}(W_{2}I_{M})+\\operatorname{Rank}(I_{M}W_{1})-\\operatorname{Rank}(I_{M})=\\operatorname{Rank}(I_{M}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies the rank of $W_{F}$ will exactly equal to $\\operatorname{Rank}(I_{M})$ . We should note that this condition, i.e., $\\mathrm{Rank}(I_{M})<d$ , is easily satisfied when $d_{h}=d$ or when $d_{h}$ is slightly larger than $d$ (for example, $d<d_{h}<2d$ in an expected sense). Thus, we conclude that $W_{F}$ has the potential low-rank property. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Representation Learning Lens: For a encoded demonstration representation $\\pmb{x}_{i}$ , the key and value projections will generate a pair of feature $W_{K}{\\pmb x}_{i}$ and $W_{V}{\\pmb x}_{i}$ to create a certain distance between data representations in space. And then, on the one hand, a potential lowrank transformation $W_{F}$ is applied to the $W_{V}\\pmb{x}_{i}$ , attempting to compress some information which increases the difficulty of contrastive learning and forces the model to learn better features; on the other hand, $\\phi(\\cdot)$ projects $W_{K}{\\pmb x}_{i}$ into a higher-dimensional space to capture deeper-level features. Finally, we need to train the weight matrix $W$ , which maps $\\phi({\\pmb W}_{K}{\\pmb x}_{i})$ back to the original space, aiming to make the mapped vector as close as possible to $W_{F}W_{V}\\pmb{x}_{i}$ . This interpretation is illustrated in Figure 4. ", "page_idx": 17}, {"type": "text", "text": "B.2 Extension to Multiple Attention Layers ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this part , we extend Theorem 3.1 to multiple attention layers. Here we adopt the attention layer based on PrefixLM [Roberts et al., 2019], where the query tokens can compute attention with all preceding tokens (including itself), while for demonstration ones, attention can be computed between themselves, excluding the query tokens. Existing work [Ding et al., 2023] has theoretically and experimentally explained that PrefixLM achieves better results than CasualLM. In this paper, we assume we have only one query token, that is, there is no query input before the considered query token. With the assumption that $T=0$ , to maintain notational simplicity, we use $\\pmb{x}_{N+1}$ to represent the query token here instead of ${\\pmb x}_{T+1}^{\\prime}$ and the input will be $\\pmb{X}=[\\pmb{X}_{D},\\pmb{x}_{N+1}]$ . We assume that there are $L$ attention layers and the output of the $l$ -th layer $\\pmb{X}^{(l)}$ can be expressed as: ", "page_idx": 17}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/5de42ecdb90fc4214738a7cb48034a85129817d1d4af876da57f8e0b7794d7a6.jpg", "img_caption": ["Figure 5: Illustrating the ICL inference process of multiple softmax attention layers from the perspective of dual models. The layer-wise process of ICL can be viewed as a gradual gradient descent on the dual model sequence. The datasets used for each gradient descent, including training data and test input, are obtained from the outputs of the previous dual model before and after training. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H^{(l)}=[H_{D}^{(l)},h_{N+1}^{(l)}]=\\mathrm{Atten}(H^{(l-1)};\\,W_{Q}^{(l)},W_{K}^{(l)},W_{V}^{(l)}),}\\\\ &{H_{D}^{(l)}=W_{V}^{(l)}H_{D}^{(l-1)}\\mathrm{Softmax}\\left(\\frac{(W_{K}^{(l)}H_{D}^{(l-1)})^{T}W_{Q}^{(l)}H_{D}^{(l-1)}}{\\sqrt{d}}\\right),}\\\\ &{h_{N+1}^{(l)}=W_{V}^{(l)}H^{(l-1)}\\mathrm{Softmax}\\left(\\frac{(W_{K}^{(l)}H^{(l-1)})^{T}W_{Q}^{(l)}h_{N+1}^{(l-1)}}{\\sqrt{d}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we set $H^{(0)}={\\pmb X}=[{\\pmb X}_{D},{\\pmb x}_{N+1}]$ as the initial input. And the final output of the query token is \u02c6h(L) $\\hat{h}_{N+1}^{(L)}=h_{N+1}^{(L)}$ . Here, we assume that after training, the parameters $\\mathbf{W}_{Q}^{(l)},\\bar{\\mathbf{W}}_{K}^{(l)},\\mathbf{W}_{V}^{(l)}\\in\\bar{\\mathbb{R}}^{d_{o}\\times d_{i}}$ are fixed and we set $d_{o}=d_{i}=d$ . ", "page_idx": 18}, {"type": "text", "text": "Next, we extend Theorem 3.1 to the case of multiple softmax attention layers. Formally, we present our result in the following theorem. ", "page_idx": 18}, {"type": "text", "text": "Theorem B.2. Given $L$ softmax attention layers whose parameters $\\{W_{Q}^{(l)},W_{K}^{(l)},W_{V}^{(l)}\\}_{l=1}^{L}$ are fixed after training, the ICL output of these layers is equivalent to sequentially performing one step gradient descent on a sequence of dual models $\\left\\{f^{(l)}({\\pmb x})={\\pmb W}^{(l)}\\phi({\\pmb x})\\right\\}_{l=1}^{L}$ , where the loss function for the l-th dual model is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}^{(l)}=-\\frac{1}{\\eta D^{(l)}}\\sum_{i=1}^{N}\\left(\\pmb{W}_{V}^{(l)}\\pmb{h}_{i}^{(l-1)}\\right)^{T}\\pmb{W}^{(l)}\\phi(\\pmb{W}_{K}^{(l)}\\pmb{h}_{i}^{(l-1)}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\pmb{h}_{i}^{(l-1)}$ is the output of the $(l-1)$ -th attention layer for the $i$ -th token, $\\eta$ is the learning rate and $D^{(l)}$ is a constant. The input for the l-th dual model is generated by the trained $(l-1)$ -th dual model. ", "page_idx": 18}, {"type": "text", "text": "Proof. Given $H^{(l-1)}$ as the input for the $l$ -th attention layer, the inference process of $h_{N+1}^{(l)}$ hN+1 is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle h_{N+1}^{(l)}=W_{V}^{(l)}H^{(l-1)}\\mathrm{Softmax}\\left(\\frac{(W_{K}^{(l)}H^{(l-1)})^{T}W_{Q}^{(l)}h_{N+1}^{(l-1)}}{\\sqrt{d}}\\right)}}\\\\ {{\\displaystyle\\qquad=W_{0}^{(l)}\\phi\\left(W_{Q}^{(l)}h_{N+1}^{(l-1)}\\right)+\\left[\\sum_{i=1}^{N}\\frac{1}{D^{(l)}}W_{V}^{(l)}h_{i}^{(l-1)}\\otimes\\phi(W_{K}^{(l)}h_{i}^{(l-1)})\\right]\\phi\\left(W_{Q}^{(l)}h_{N+1}^{(l-1)}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "awnhde $D^{(l)}=\\mathbf{1}_{N+1}^{T}\\phi(W_{K}^{(l)}H^{(l-1)})^{T}\\phi(W_{Q}^{(l)}h_{N+1}^{(l-1)})$ cics oar dcionngs ttaon t Ttoh enoorremma l3i.z1e, t hwee a tctaenn tieoans islcy ogreest W 0(l) $\\begin{array}{r}{\\pmb{W}_{0}^{(l)}\\,=\\,\\frac{1}{D^{(l)}}(\\pmb{W}_{V}^{(l)}\\pmb{h}_{N+1}^{(l-1)})\\phi(\\pmb{W}_{K}^{(l)}\\pmb{h}_{N+1}^{(l-1)})^{T}}\\end{array}$   \nthe dual model $f_{i n i t}^{(l)}(\\pmb{h})=\\pmb{W}_{i n i t}^{(l)}\\phi(\\pmb{h})$ where the initialization is ${\\bf W}_{i n i t}^{(l)}={\\bf W}_{0}^{(l)}$ . Given the loss function $\\mathscr{L}^{(l)}$ formed as Equation 24 and training set $\\left\\{\\boldsymbol{z}_{s t d}^{(i)},\\boldsymbol{y}_{s t d}^{(i)}\\right\\}_{i=1}^{N}$ where zs(it)d = W (Kl )hi(l\u22121)and $\\pmb{y}_{s t d}^{(i)}=\\pmb{W}_{V}^{(l)}\\pmb{h}_{i}^{(l)}$ , we perform one step SGD with learning rate $\\eta$ on weight matrix $W^{(l)}$ and will get trained dual model: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}^{(l)}(\\pmb{x})=\\widehat{\\pmb{W}}^{(l)}\\phi(\\pmb{x})=(\\pmb{W}_{i n i t}^{(l)}+\\Delta\\pmb{W}^{(l)})\\phi(\\pmb{x})}\\\\ &{\\quad\\quad\\quad=\\left[\\pmb{W}_{0}^{(l)}+\\displaystyle\\sum_{i=1}^{N}\\frac{1}{D^{(l)}}\\pmb{W}_{V}^{(l)}\\pmb{h}_{i}^{(l-1)}\\otimes\\phi(\\pmb{W}_{K}^{(l)}\\pmb{h}_{i}^{(l-1)})\\right]\\phi(\\pmb{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking test input as $\\pmb{z}_{t e s t}^{(l)}=W_{Q}^{(l)}\\pmb{h}_{N+1}^{(l-1)}$ $\\hat{f}^{(l)}(z_{t e s t}^{(l)})$ $h_{N+1}^{(l)}$ ", "page_idx": 19}, {"type": "text", "text": "Next, we will show how to obtain $\\pmb{H}_{D}^{(l)}$ through the trained dual model ${\\hat{f}}^{(l)}({\\pmb x})$ . And after $\\pmb{W}_{K}^{(l+1)},\\pmb{W}_{Q}^{(l+1)},\\pmb{W}_{V}^{(l+1)}$ projections, $\\pmb{H}_{D}^{(l)}$ will constitute the training set as well as the test input for the next dual model $f_{i n i t}^{(l+1)}(\\pmb{x})$ . ", "page_idx": 19}, {"type": "text", "text": "Keeping the initialized dual model $f_{i n i t}^{(l)}(\\pmb{x})$ and the trained one ${\\hat{f}}^{(l)}({\\pmb x})$ in mind, we can compute the demonstration token output $h_{i}^{(l)}$ $\\mathit{\\Omega}_{\\mathit{i}}=1,2,...,N)$ of $l_{\\cdot}$ -th attention layer as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{h_{i}^{(l)}=W_{V}^{(l)}H_{D}^{(l-1)}\\mathrm{Softmax}\\left(\\frac{(W_{K}^{(l)}H_{D}^{(l-1)})^{T}W_{Q}^{(l)}h_{i}^{(l-1)}}{\\sqrt{d}}\\right)}}\\\\ &{=\\left[\\sum_{i=1}^{N}\\frac{1}{D_{i}^{(l)}}W_{V}^{(l)}h_{i}^{(l-1)}\\otimes\\phi(W_{K}^{(l)}h_{i}^{(l-1)})\\right]\\phi\\left(W_{Q}^{(l)}h_{i}^{(l-1)}\\right)}\\\\ &{=\\frac{D^{(l)}}{D_{i}^{(l)}}\\left[\\sum_{i=1}^{N}\\frac{1}{D^{(l)}}W_{V}^{(l)}h_{i}^{(l-1)}\\otimes\\phi(W_{K}^{(l)}h_{i}^{(l-1)})\\right]\\phi\\left(W_{Q}^{(l)}h_{i}^{(l-1)}\\right)}\\\\ &{=\\frac{D^{(l)}}{D_{i}^{(l)}}\\left[\\frac{W_{i}^{(l)}}{\\omega_{i}}+\\sum_{i=1}^{N}\\frac{1}{D^{(l)}}W_{V}^{(l)}h_{i}^{(l-1)}\\otimes\\phi(W_{K}^{(l)}h_{i}^{(l-1)})-W_{i\\ n t}^{(l)}\\right]\\phi\\left(W_{Q}^{(l)}h_{i}^{(l-1)}\\right)}\\\\ &{=\\frac{D^{(l)}}{D_{i}^{(l)}}\\left[\\widehat{\\Psi}^{(l)}-W_{i\\ n t}^{(l)}\\right]\\phi\\left(W_{Q}^{(l)}h_{i}^{(l-1)}\\right)=\\frac{D^{(l)}}{D_{i}^{(l)}}\\left[\\widehat{f}^{(l)}(W_{Q}^{(l)}h_{i}^{(l-1)})-f_{i n t}^{(l)}(W_{Q}^{(l)}h_{i}^{(l-1)})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $D_{i}^{(l)}=\\mathbf{1}_{N}^{T}\\phi(W_{K}^{(l)}H_{D}^{(l-1)})^{T}\\phi(W_{Q}^{(l)}h_{i}^{(l-1)})$ is a constant to normalize the attention scores for $h_{i}^{(l)}$ . Therefore, once we obtain the trained dual model ${\\hat{f}}^{(l)}(\\pmb{h})$ , we can use the Eq (25) to get the demonstration token output $\\pmb{H}_{D}^{(l)}=[\\pmb{h}_{1}^{(l)},\\pmb{h}_{2}^{(l)},...,\\pmb{h}_{N}^{(l)}]$ . These demonstration token outputs, along with the output $h_{N+1}^{(l)}$ for query tokens, will together constitute the training set and test input for the next dual model $f_{i n i t}^{(l+1)}(h)$ . This process continues layer by layer until we obtain the ultimate ICL output h(NL+)1. In summary, the ICL inference process across L attention layers is equivalent to performing gradient descent on dual models sequentially. Thus, we complete our proof. ", "page_idx": 19}, {"type": "text", "text": "This theorem is a natural extension of Theorem 3.1: when considering the stacking of multiple attention layers, a sequence of dual models is correspondingly generated. Although these dual models have the same form $f({\\pmb x})=W\\phi({\\pmb x})$ , they have different initializations and datasets. As the ICL inference process progresses layer by layer between attention layers, we equivalently perform gradient descent on the dual models one by one. The input $\\pmb{H}^{(l)}$ for each attention layer, including demonstration tokens and query tokens, can be obtained from the test output of the dual models. This can be illustrated in Figure 5. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "C Proof of the Generalization bound ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this part, we provide the proof regarding the generalization boundary in Theorem 3.2. We restate our theorem as follows: ", "page_idx": 20}, {"type": "text", "text": "Theorem C.1. Define the function class as ${\\mathcal{F}}:=\\{f({\\pmb x})=W\\phi(W_{K}{\\pmb x})\\mid\\|W\\|\\leq w\\}$ and let the loss function defined as Eq $(I O)$ . Consider the given demonstration set as $\\pmb{S}^{\\top}=\\{\\pmb{x}_{i}\\}_{i=1}^{N}$ where $S\\subseteq S_{T}$ and $S_{T}$ is all possible demonstration tokens for some task $\\tau$ . With the assumption that $\\lVert\\boldsymbol{W}_{V}\\boldsymbol{x}_{i}\\rVert$ , $\\|W\\phi(W_{K}{\\pmb x}_{i})\\|\\leq\\rho_{:}$ , then for any $\\delta>0$ , the following statement holds with probability at least $1-\\delta$ for any $f\\in\\mathcal F$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{f})\\leq\\mathcal{L}(f)+O\\left(\\frac{w\\rho d_{o}\\sqrt{\\mathrm{Tr}(\\mathbf{K}_{S})}}{N}+\\sqrt{\\frac{l o g\\frac{1}{\\delta}}{N}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Our proof is similar to the Lemma 4.2 in Saunshi et al. [2019], but here we focus on a different function class. Firstly, we consider the classical generalization bound based on the Rademacher complexity of the function class which can refer to Theorem 3.1 in Mohri et al. [2018]. For a real function class $G$ whose functions map from a set $Z$ to $[0,1]$ and for any $\\delta>0$ , if $\\boldsymbol{S}$ is a training set composed by $N$ iid samples $\\{{\\pmb x}_{i}\\}_{i=1}^{N}$ , then with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ , for all $g\\in G$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[g(\\pmb{x})\\right]\\leq\\frac{1}{N}\\sum_{i=1}^{N}g(\\pmb{x}_{i})+\\frac{2\\mathcal{R}_{S}(G)}{N}+3\\sqrt{\\frac{\\log\\frac{4}{\\delta}}{2N}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathcal{R}_{S}(G)$ is the traditional Rademacher complexity. By setting $\\boldsymbol{S}$ exactly the demonstration set and $G=\\left\\{g_{f}(\\pmb{x})=-\\left(W_{V}\\pmb{x}\\right)^{T}\\pmb{W}\\phi(W_{K}\\pmb{x})\\right|\\|\\pmb{W}\\|\\leq w\\right\\}$ , we can apply this bound to our case. Then, we construct a function class $\\tilde{\\mathcal{F}}=\\left\\{\\tilde{f}(\\pmb{x})=[f(\\pmb{x});\\pmb{W}_{V}\\pmb{x}]=[\\pmb{W}\\phi(\\pmb{W}_{K}\\pmb{x});\\pmb{W}_{V}\\pmb{x}]|\\|\\pmb{W}\\|\\le w\\right\\}$ whose functions map from $\\boldsymbol{S}$ to $\\mathbb{R}^{2d_{o}}$ . Next, we will first prove $\\mathcal{R}_{S}(G)\\leq2\\rho\\mathcal{R}_{S}(\\tilde{\\mathcal{F}})$ and to do this, we need to use the following Lemma: ", "page_idx": 20}, {"type": "text", "text": "Lemma C.2 (Corollary 4 in Maurer [2016]). Let $Z$ be any set, and $S=\\{z_{i}\\}_{i=1}^{M}\\in Z^{M}$ . Let $\\tilde{\\mathcal F}$ be $a$ class of functions $\\tilde{f}:Z\\to\\mathbb{R}^{n}$ and $h:\\mathbb{R}^{n}\\to\\mathbb{R}$ be $L$ -Lipschitz. For all $\\tilde{f}\\in\\tilde{\\mathcal{F}}$ , let $g_{\\tilde{f}}=h\\circ\\tilde{f}$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\underset{\\sigma\\sim\\{\\pm1\\}^{M}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{\\tilde{f}\\in\\tilde{F}}\\langle\\sigma,(g_{\\tilde{f}_{|S}})\\rangle\\right]\\leq\\sqrt{2}L\\underset{\\sigma\\sim\\{\\pm1\\}^{n M}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{\\tilde{f}\\in\\tilde{F}}\\langle\\sigma,(\\tilde{f}_{|S})\\rangle\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\tilde{f}_{|S}=\\left(\\tilde{f}_{t}(z_{j})\\right)_{t\\in[n],j\\in[M]}$ . ", "page_idx": 20}, {"type": "text", "text": "We apply Lemma C.2 to our case by setting $Z~=~\\mathbb{R}^{d_{i}}$ , $\\boldsymbol{S}$ to be exactly the demonstration set, $\\bar{\\tilde{\\mathcal{F}}}$ to be the function class we constructed and $\\textit{n}=\\textit{2d}$ . We also use $h:\\,\\mathbb{R}^{2d_{o}}\\ \\to\\ \\mathbb{R}$ where $h({\\pmb x})\\,=\\,-\\langle{\\pmb x}_{1:d_{o}},{\\pmb x}_{d_{o}+1:2d_{o}}\\rangle$ and thus we have $g_{\\tilde{f}}({\\pmb x})\\,=\\,h({\\tilde{f}}({\\pmb x}))\\,=\\,h([f({\\pmb x});{\\pmb W}_{V}{\\pmb x}])\\,=$ ", "page_idx": 20}, {"type": "text", "text": "$-\\left(W_{V}{\\pmb x}\\right)^{T}{\\pmb W}\\phi(W_{K}{\\pmb x})$ . We can find that $g_{f}(\\pmb{x})=g_{\\hat{f}}(\\pmb{x})$ and the left side of inequality (28) is exactly ${\\mathcal{R}}_{{\\mathcal{S}}}(G)$ . ", "page_idx": 20}, {"type": "text", "text": "Then we can see that $h$ is ${\\sqrt{2}}\\rho$ -Lipschitz with the assumption that $||W_{V}{\\pmb x}_{i}||,||W\\phi(W_{K}{\\pmb x}_{i})||\\leq\\rho$ and we have $\\mathcal{R}_{S}(G)\\leq2\\rho\\mathcal{R}_{S}(\\tilde{\\mathcal{F}})$ . Now using Lemma C.2 and the classical generalization bound (27), ", "page_idx": 20}, {"type": "text", "text": "we have that with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{f})\\leq\\hat{\\mathcal{L}}(\\hat{f})+O\\left(\\frac{\\rho\\mathcal{R}_{S}(\\tilde{F})}{N}+\\sqrt{\\frac{l o g\\frac{1}{\\delta}}{N}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $f^{*}\\in\\arg\\operatorname*{min}_{f\\in\\mathcal{F}}\\mathcal{L}(f)$ . According to Hoeffding\u2019s inequality, with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ , we have that L\u02c6(f \u2217) \u2264L(f \u2217) + 3 log \u03b42 . Combining this with (29), the fact that $\\hat{\\mathcal{L}}(\\hat{f})\\leq\\hat{\\mathcal{L}}(f^{*})$ and applying a union bound, we can get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{f})\\leq\\mathcal{L}(f)+O\\left(\\frac{\\rho\\mathcal{R}_{S}(\\tilde{F})}{N}+\\sqrt{\\frac{l o g\\frac{1}{\\delta}}{N}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we give the upper bound for $\\mathcal{R}_{S}(\\tilde{\\mathcal{F}})$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{R}}_{\\delta}(\\Vec{F})=\\underset{\\sigma\\sim(\\pm1)^{3N_{\\delta}}}{\\underbrace{\\mathbb{E}}}\\left[\\underset{|W|\\leq\\sigma_{\\gamma-1}}{\\underbrace{\\operatorname*{sup}}}\\frac{2N_{\\delta}}{r_{\\gamma}}\\sigma_{t}(\\hat{f}_{\\delta})_{\\gamma}\\right]}\\\\ &{\\qquad=\\underset{\\sigma\\sim(\\pm1)^{3N_{\\delta}}}{\\underbrace{\\mathbb{E}}}\\left[\\underset{|W|\\leq\\sigma_{\\gamma-1}}{\\underbrace{\\operatorname*{sup}}}\\frac{\\hat{f}_{\\delta}}{r_{\\gamma}}W_{\\frac{\\gamma}{2}}\\sigma_{t,\\beta}(W_{K}x_{\\lambda})\\right]}\\\\ &{\\qquad\\leq\\underset{\\sigma\\sim(\\pm1)^{3N_{\\delta}}}{\\underbrace{\\mathbb{E}}}\\left[\\underset{|W|\\leq\\sigma_{\\gamma-1}}{\\underbrace{\\operatorname*{sup}}}\\frac{\\sum_{1}^{\\infty}\\|W_{j}\\|_{\\infty}}{r_{\\gamma}\\!\\left\\|\\mathbf{k}\\right\\|_{\\infty}^{2}}\\underset{|\\lambda_{1}^{\\infty}}{\\underbrace{\\mathbb{E}}}\\sigma_{t,\\beta}(W_{K}x_{\\lambda})\\right]\\Bigg[}\\\\ &{\\qquad\\leq\\quad w d_{\\phi\\sim(\\pm1)^{3N_{\\delta}}}\\left[\\left\\|\\sum_{i=1}^{N}\\sigma_{i}(W_{K}x_{\\lambda})\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\qquad\\leq\\quad w d_{\\phi\\sim(\\pm1)^{3N_{\\delta}}}\\left[\\left\\|\\sum_{i=1}^{N}\\sigma_{i}(W_{K}x_{\\lambda})\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\qquad=w d_{\\phi\\sim(\\pm1)^{3N_{\\delta}}}(K)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(Definition of Rademacher complexity) $(W_{V}{\\pmb x}_{i}$ is independent of $W_{j}$ ) (By Cauchy-Schwartz inequality) (Using the fact that $\\|W_{j}\\|\\leq w)$ ) (By Jensen\u2019s inequality) ", "page_idx": 21}, {"type": "text", "text": "Substituting the upper bound of $\\mathcal{R}_{S}(\\tilde{\\mathcal{F}})$ into (30), we will get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{f})\\leq\\mathcal{L}(f)+O\\left(\\frac{w\\rho d_{o}\\sqrt{\\mathrm{Tr}(\\mathbf{K}_{S})}}{N}+\\sqrt{\\frac{l o g\\frac{1}{\\delta}}{N}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus we finish our proof. ", "page_idx": 21}, {"type": "text", "text": "C.2 Extension to negative models: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "One may also wonder whether the ratio of negative samples mentioned in Section 4 will affect the generalization bounds. In fact, after introducing negative samples and ignoring constant term in $\\operatorname{Eq}$ (9), we consider the following representation loss: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)=\\mathbb{E}_{x\\sim\\mathcal{D}_{T}}\\left[-\\frac{1}{K}\\sum_{j=1}^{K}\\left(W\\phi(W_{K}x)\\right)^{T}\\left(W_{V}x-W_{V}x_{j}^{-}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we consider sampling $K$ negative samples for each $x_{i}$ and $x_{j}^{-}$ denotes the $j$ -th negative sample for token $x$ . Correspondingly, the empirical loss will be considered as $\\hat{\\mathcal{L}}(f)\\;=\\;$ $\\begin{array}{r}{-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{K}\\sum_{j=1}^{K}\\left(W\\phi(W_{K}x_{i})\\right)^{T}\\left(W_{V}x_{i}-W_{V}x_{i j}^{-}\\right)}\\end{array}$ where $x_{i j}^{-}$ is the $j$ -th negative sample for $x_{i}$ . Then, by retaining the other definitions in Section 3.3, corresponding to Theorem 3.2, we can obtain the generalization bound as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{f})\\leq\\mathcal{L}(f)+O\\left(w\\rho d_{o}\\sqrt{\\mathrm{Tr}(K_{S})\\left(\\frac{5}{N^{2}}+\\frac{1}{r N^{3}}\\right)}+\\sqrt{\\frac{l o g\\frac{1}{\\delta}}{N}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{r=\\frac{K}{N}}\\end{array}$ is excatly the the ratio of the number of negative samples. It can be observed that as the ratio of negative samples increases, the generalization error decreases. However, we also notice that $\\begin{array}{r}{\\frac{5}{N^{2}}>\\frac{1}{r N^{3}}}\\end{array}$ thus the former term dominates, which means the reduction in generalization error due to an increased proportion of negative samples is limited. Nevertheless, we do not rule out the possibility of a tighter generalization bound, which is a promising direction for future research. ", "page_idx": 22}, {"type": "text", "text": "Proof Sketch. The proof process is similar to that of Theorem 3.2. The main difference lies in the fact that we should firstly define the function class $\\begin{array}{r l}{G}&{{}=}\\end{array}$ $\\begin{array}{r}{\\left\\{-\\frac{1}{K}\\sum_{j=1}^{K}\\left(W\\phi(W_{K}x_{i})\\right)^{T}\\left(W_{V}x_{i}-W_{V}x_{j}^{-}\\right)\\left|\\left|W\\right|\\right|\\leq w\\right\\}}\\end{array}$ to use the classical bound. In addition, we define $\\tilde{F}~=~\\left\\{\\tilde{f}(x)=[f(x);W_{V}x;W_{V}x_{1}^{-};...;W_{V}x_{K}^{-}]||W||\\leq w\\right\\}$ whose functions map from $\\boldsymbol{S}$ to $\\mathbb{R}^{(K+2)d_{o}}$ . Similarly, when using Lemma C.2, we set $Z~=~\\mathbb{R}^{d_{i}}$ , $\\tilde{F}$ be the above function class and $n\\ =\\ (K\\,+\\,2)d_{o}$ . We also use $h:\\mathbb{R}^{(K+2)d_{o}}\\;\\to\\;\\mathbb{R}$ defined as $\\begin{array}{r}{h(x)=-\\frac{1}{K}\\sum_{j=1}^{K}x_{1:d_{o}}^{T}\\big(x_{d_{o}+1:2d_{o}}-x_{(j+1)d_{o}+1:(j+2)d_{o}}\\big)}\\end{array}$ . Then we notice that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial h}{\\partial x_{1:d_{0}}}=-\\frac{1}{K}\\sum_{j=1}^{K}(x_{d_{0}+1:2d_{o}}-x_{(j+1)d_{o}+1:(j+2)d_{o}}),}\\\\ &{\\displaystyle\\frac{\\partial h}{\\partial x_{d_{o}+1:2d_{o}}}=-x_{1:d_{o}},\\;\\;\\frac{\\partial h}{\\partial x_{(j+1)d_{o}+1:(j+2)d_{o}}}=\\frac{1}{K}x_{1:d_{o}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With the assumption that $\\|W_{V}x\\|,\\|W\\phi(W_{K}x)\\|\\le\\rho.$ , we can get that the Frobenius norm of the Jocabian $J$ of $h$ has $\\begin{array}{r}{\\|J\\|_{F}^{2}\\le4\\rho^{2}+\\rho^{2}+\\frac{K}{K^{2}}\\rho^{2}=\\big(5+\\frac{1}{K}\\big)\\rho^{2}}\\end{array}$ . Thus we get that $h$ is $\\sqrt{5+\\frac{1}{K}}\\rho\\cdot$ Lipschitz. The rest of the proof process is similar to that of Theorem 3.2. Ultimately, we will obtain the aforementioned generalization error. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "D Details and More Discussions for Section 4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide a more detailed discussion on improving the model structure from the perspective of representation learning especially contrastive learning, which is presented in Section 4 of the main body. And we also point out the corresponding modifications in the self-attention mechanism, which are adopted in our experiments. ", "page_idx": 22}, {"type": "text", "text": "D.1 More Discussion on the Contrastive Loss ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Although we have figured out the representation learning loss of the implicit gradient updates, it can be observed that this loss function has a flaw: due to the lack of normalization for ${\\boldsymbol y}_{s t d}^{(i)}$ and $\\hat{\\pmb y}^{(i)}$ when calculating the cosine distance, the loss can theoretically be optimized to negative infinity. To address this issue, we introduce regularization to constrain the norm of $W$ , that is, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{V}\\pmb{x}_{i}\\right)^{T}W\\phi(W_{K}\\pmb{x}_{i})+\\frac{\\alpha}{2\\eta}\\|\\pmb{W}\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\alpha$ is a hyperparameter to balance the two parts. As a result, we can see that the gradient update for $W$ will be in an exponentially smoothed manner meaning that a portion of the initial part will be discarded at every step, that is, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{{W}}^{(t)}=\\mathbf{{W}}^{(t-1)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}}=(1-\\alpha)\\mathbf{{W}}^{(t-1)}+\\sum_{i=1}^{N}D^{-1}\\mathbf{W}_{V}\\mathbf{{h}}_{i}\\otimes\\phi(\\mathbf{W}_{K}\\mathbf{{h}}_{i}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Equivalently, the inference process of ICL can be seen as the first step of the aforementioned update, and the attention mechanism will be correspondingly adjusted as, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle h_{T+1}^{\\prime}=(1-\\alpha)W_{0}\\phi({\\pmb q})+D^{-1}\\left[\\sum_{i=1}^{N}V_{D}^{(i)}\\otimes\\phi({\\pmb K}_{D}^{(i)})\\right]\\phi({\\pmb q})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which means more demonstration information will be attended to. This will directly result in $\\mathrm{Eq}\\ 13$ . ", "page_idx": 23}, {"type": "text", "text": "This result can be easily extended to self-attention mechanism. As for a self-attention layer, if all other tokens adopt the same modification, the self-attention layer will become ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H=W_{V}X\\mathrm{softmax}\\left(\\frac{(W_{K}X)^{T}W_{Q}X}{\\sqrt{d_{o}}}\\right)-\\alpha W_{V}X}\\\\ {=W_{V}X\\left[\\mathrm{softmax}\\left(\\frac{(W_{K}X)^{T}W_{Q}X}{\\sqrt{d_{o}}}\\right)-\\alpha I\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which leads to the model structure incorporating an operation similar to skip connections. Furthermore, to ensure numerical stability, we normalize the attention scores yielding: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\pmb H}=W_{V}{\\pmb X}\\cdot\\mathrm{Norm}\\left(\\mathrm{softmax}\\left(\\frac{(W_{K}{\\pmb X})^{T}W_{Q}{\\pmb X}}{\\sqrt{d_{o}}}\\right)-\\alpha{\\pmb I}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\operatorname{Norm}(\\cdot)$ is performed column-wise to ensure that the attention scores sum to 1. The above modification reduce the attention score of each token to its own information during aggregation. It is worth noting that, although our initial intention is to impose regularization on the contrastive loss where $\\alpha>0$ to prevent it from diverging to negative infinity, we find in experiments that this modification remains effective even when $\\alpha$ is less than 0. We interpret this as possibly stemming from the fact that an appropriate $\\alpha$ helps the attention block become full-rank, thereby better preserving information, which can be illustrated by Lemma D.1: ", "page_idx": 23}, {"type": "text", "text": "Lemma D.1. Let the attention block $A\\,\\in\\,\\mathbb{R}^{n\\times n}$ . There exists some $\\delta\\;>\\;0$ such that, for any $0<|\\alpha|<\\delta_{;}$ , the attention block $A+\\alpha I_{n}$ will become full-rank. ", "page_idx": 23}, {"type": "text", "text": "Proof. Define $f(\\alpha)\\;=\\;\\operatorname*{det}(\\alpha I_{n}\\,+\\,{\\pmb A})$ , which is a polynomial of degree $n$ in $\\alpha$ . Then, $f(\\alpha)$ has only finitely roots. Let $\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{r}$ be the non-zero roots of $f(t)$ . Now, consider $\\delta\\,=$ $\\operatorname*{min}\\{|\\alpha_{1}|,|\\alpha_{2}|,\\ldots,|\\alpha_{r}|\\}$ . For $0<|\\alpha|<\\delta$ , we can claim that $f(\\alpha)=\\operatorname*{det}(\\alpha I_{n}+A)\\neq0$ . Thus, $A+\\alpha I_{n}$ becomes non-singular (full-rank) and we complete the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma D.1 provides one possible case for appropriate $\\alpha$ . In fact, the selection of $\\alpha$ can be quite flexible; for instance, similarly, when $\\delta=\\operatorname*{max}\\{|\\alpha_{1}|,|\\alpha_{2}|,\\dotsc,|\\alpha_{r}|\\}$ and $|\\alpha|>\\delta$ holds, $A+\\alpha I_{n}$ also remains full-rank. Our experimental results related to regularized models will further illustrate the effectiveness of an appropriate $\\alpha$ in enhancing model performance. ", "page_idx": 23}, {"type": "text", "text": "We also acknowledge that our modification is relatively straightforward and may not be optimal. However, we believe that it may be a good choice to make structural improvements to the model from the perspective of the loss function, or more generally, from an optimization standpoint. For example, to address the issue of non-normalized ${\\boldsymbol y}_{s t d}^{(i)}$ and $\\hat{\\pmb y}^{(i)}$ , we can also modify the loss function from the perspective of ridge regression as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{1}{2\\eta D}\\sum_{i=1}^{N}\\|W_{V}\\pmb{x}_{i}-\\pmb{W}\\phi(\\pmb{W}_{K}\\pmb{x}_{i})\\|_{F}^{2}+\\frac{\\alpha}{2\\eta}\\|\\pmb{W}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "And the optimal $W^{*}$ will be ", "page_idx": 23}, {"type": "equation", "text": "$$\nW^{\\ast}=\\left[\\phi(W_{K}X)\\phi(W_{K}X)^{T}+\\alpha D I\\right]^{-1}W_{V}X\\phi(W_{K}X).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Correspondingly, the attention mechanism will be modified to ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\cal H}=W^{*}\\phi(W_{Q}X)=\\left[\\phi(W_{K}X)\\phi(W_{K}X)^{T}+\\alpha D I\\right]^{-1}W_{V}X\\phi(W_{K}X)\\phi(W_{Q}X),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we neglect the normalization operation. This result is very similar to the mesa-layer proposed by Von Oswald et al. [2023b], which optimizes linear attention layers under the auto-regressive setting. Here, we presented its form on softmax self-attention setting using kernel methods and explained it from the perspectives of contrastive loss and ridge regression. Although the matrix inversion calculation in Eq (33) can be computationally expensive, effective methods for computing Eq (33), including both forward computation and backward propagation, have been thoroughly researched in Von Oswald et al. [2023b], which contributes to making the above modification practically applicable. ", "page_idx": 23}, {"type": "text", "text": "D.2 More Discussion on the Data Augmentation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In addition to discussing the loss function, the contrastive learning paradigm also offers our some insights. In the corresponding representation learning process of ICL, we can easily notice that \"data augmentation\" is performed using a simple linear mapping, which may be not sufficient for learning deeper-level features. To address this, we can employ more complicated nonlinear functions for more complex augmentations. Denoting these two augmentations as $g_{1}$ and $g_{2}$ , consequently, the process of contrastive learning will be modified as follows ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathcal{L}}=-{\\frac{1}{\\eta D}}\\sum_{i=1}^{N}\\left[g_{1}(W_{V}{\\pmb x}_{i})\\right]^{T}W{\\phi}(g_{2}(W_{K}{\\pmb x}_{i})).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Correspondingly, the gradient update for $W$ will become ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pmb{W}^{(t)}=\\pmb{W}^{(t-1)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}}=\\pmb{W}^{(t-1)}+\\sum_{i=1}^{N}D^{-1}g_{1}(\\pmb{W}_{V}\\pmb{x}_{i})\\otimes\\phi\\big(g_{2}(\\pmb{W}_{K}\\pmb{x}_{i})\\big).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "And from the perspective of ICL, correspondingly, the last token will be updated as ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{T+1}^{\\prime}=W_{0}\\phi(\\pmb{q})+D^{-1}\\left[\\sum_{i=1}^{N}g_{1}(\\pmb{V}_{D}^{(i)})\\otimes\\phi(g_{2}(\\pmb{K}_{D}^{(i)}))\\right]\\phi(\\pmb{q}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "And by reformulating the above equation we will get Eq (14) in the main body. ", "page_idx": 24}, {"type": "text", "text": "Correspondingly, the modification for self-attention layer can be adjusted as, ", "page_idx": 24}, {"type": "equation", "text": "$$\nH=g_{1}(W_{V}X)\\mathrm{softmax}\\left(\\frac{g_{2}(W_{K}X)^{T}W_{Q}X}{\\sqrt{d_{o}}}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $g_{1}(\\cdot)$ and $g_{2}(\\cdot)$ will be column-wise here. It is worth noting that here we have only presented the framework of using nonlinear functions as data augmentations to modify the self-attention layer and in the simplest case, we can set $g_{1}(x)$ and $g_{2}(x)$ as MLPs (Multi-Layer Perceptrons). However, in practice, it is encouraged to use data augmentation functions that are tailored to specific data structures. For example, in the case of CMT [Guo et al., 2022], the used Convolutional Neural Networks (CNNs) can be considered as a form of \"strong data augmentations\" suitable for image datas within our framework. We consider the exploration of various augmentation methods tailored to different types of data as an open question for future research. ", "page_idx": 24}, {"type": "text", "text": "D.3 More discussion on the Negative Samples ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Although the gradient descent process corresponding to ICL exhibits some similarities with traditional contrastive learning approaches without negative samples, there are also significant differences: In traditional Siamese networks, the augmented representations as positive pairs are further learned through target and online network that share weights (or at least influence each other using EMA). The output of the target network is then passed through a predictor to compute the contrastive loss. In contrast, the representation learning pattern corresponding to ICL indeed performs more simply, which may potentially limit the ability of the dual model to learn representations fully without negative samples. To address this, similar to most contrastive learning approaches, we can introduce negative samples forcing the model to separate the distances between positive and negative samples at the same time, that is, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{V}x_{i}\\right)^{T}W\\phi(W_{K}x_{i})+\\frac{\\beta}{\\eta D}\\sum_{i=1}^{N}\\frac{1}{|\\mathcal{N}(i)|}\\sum_{j\\in\\mathcal{N}(i)}\\left(W_{V}x_{j}\\right)^{T}W\\phi(W_{K}x_{i})}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{V}\\left(x_{i}-\\frac{\\beta}{|\\mathcal{N}(i)|}\\sum_{j\\in\\mathcal{N}(i)}x_{j}\\right)\\right)^{T}W\\phi(W_{K}x_{i})}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}\\left(W_{V}\\tilde{x}_{i}\\right)^{T}W\\phi(W_{K}x_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{\\pmb{x}}_{i}\\,=\\,\\pmb{x}_{i}\\,-\\,\\frac{\\beta}{|\\mathcal{N}(i)|}\\sum_{j\\in\\mathcal{N}(i)}\\pmb{x}_{j},\\mathcal{N}(i)}\\end{array}$ is the set of the negative samples for $\\pmb{x}_{i}$ and $\\beta$ is a hyperparameter. As a result, the gradient descent on will be modified as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{{W}}^{(t)}=\\mathbf{{W}}^{(t-1)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}}=\\mathbf{{W}}^{(t-1)}+\\sum_{i=1}^{N}D^{-1}\\mathbf{W}_{V}\\tilde{\\mathbf{x}}_{i}\\otimes\\phi(\\mathbf{W}_{K}\\mathbf{x}_{i}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Correspondingly, the ICL process for $\\hat{h}_{N+1}$ will be ", "page_idx": 25}, {"type": "equation", "text": "$$\nh_{T+1}^{\\prime}=W_{0}\\phi(W_{Q}{\\pmb x}_{T+1}^{\\prime})+D^{-1}\\left[\\sum_{i=1}^{N}W_{V}\\tilde{{\\pmb x}}_{i}\\otimes\\phi(W_{K}{\\pmb x}_{i})\\right]\\phi(W_{Q}{\\pmb x}_{T+1}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "And this will directly result in Eq (15) in the main body. ", "page_idx": 25}, {"type": "text", "text": "As for a self-attention layer, similarly, we can get the corresponding modification as ", "page_idx": 25}, {"type": "equation", "text": "$$\nH=W_{V}\\tilde{X}\\mathrm{softmax}\\left(\\frac{(W_{K}X)^{T}W_{Q}X}{\\sqrt{d_{o}}}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\tilde{\\mathbf{X}}^{(i)}=\\tilde{\\mathbf{x}}_{i}$ . In corresponding experiments, for each token, we simply choose other the $k$ least relevant tokens as its negative samples, i.e., the $k$ tokens with the lowest attention scores. Noting that here we simply use other token representations as negative samples for $\\pmb{x}_{i}$ . However, there are more ways to construct negative samples that are worth exploring (for instance, using noise vectors or tokens with low semantic similarity as negative samples). For specific data structures and application scenarios, customizing the selection or construction of negative samples may be more effective. ", "page_idx": 25}, {"type": "text", "text": "E More Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "E.1 More details of Experiments on Linear Task ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this part, we will discuss our experimental setup in more details and provide more results on linear regression task. ", "page_idx": 25}, {"type": "text", "text": "Inspired by Garg et al. [2022] and Von Oswald et al. [2023a], we choose to pretrain a softmax attention layer before exploring the equivalence proposed by Theorem 3.1. In fact, pretraining is not mandatory since our theoretical analysis does not depend on any specific weight construction. In other words, the inference results of ICL and the test prediction of the dual model will still remain consistent for an attention layer with arbitrary weights or even random initialization. However, for the convenience of further investigating the impact of subsequent modifications to the model structure and to better align with real-world scenarios, we still opted for pretraining to let the model acquire some task-specific knowledge. Additionally, our experiments are conducted in a self-attention setting. When we focus only on the last token, this is equivalent to considering the case with only one query token $T=0$ ) in Section 2.1. The experiments are completed on a single 24GB NVIDIA GeForce RTX 3090 and the experiments can be completed within one day. ", "page_idx": 25}, {"type": "text", "text": "For the linear regression task, we generate the task by $\\begin{array}{r}{\\pmb{s}=\\pmb{W}\\pmb{t}}\\end{array}$ where every element of $W\\in\\mathbb{R}^{d_{s}\\times d_{t}}$ is sampled from a normal distribution $W_{i j}\\sim\\mathcal{N}(0,1)$ and $\\pmb{t}$ is sampled from a Gaussian distribution $\\pmb{x}\\sim U(-1,1)^{d_{t}}$ . To facilitate more accurate estimation of attention matrices using random features and considering the limited learning capacity of a single attention layer, we only set a small value for $d_{t}=11$ and $d_{s}=1$ . Then, at each step, we use generated $\\{\\pmb{x}_{i}=\\}[\\pmb{t}_{i};s_{i}]\\}_{i=1}^{N+\\mathrm{i}}$ i]}iN=+11 to form the input matrix $\\mathbf{\\deltaX}$ while the label part of the query token is masked to be zero, that is, $\\pmb{x}_{N+1}^{\\pm}=[t_{i};0]$ where we consider only one query token and we denote ${\\pmb x}_{T+1}^{\\prime}={\\pmb x}_{N+1}$ to maintain consistency of notation in Section 2.1. The softmax attention layer is expected to predict $\\hat{s}_{N+1}$ to approximate the ground truth value $s_{N+1}$ . We use mean square error (MSE) as the loss function, that is, for each epoch, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{1}{N_{s t e p}}\\sum_{j=1}^{N_{s t e p}}||\\hat{\\pmb{s}}_{N+1}^{(j)}-\\pmb{s}_{N+1}^{(j)}||^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\hat{\\pmb{s}}_{N+1}^{(j)}$ and ${\\pmb s}_{N+1}^{(j)}$ are the prediction and ground truth value at $j$ -th step and $N_{s t e p}$ is the number of steps. We set $\\bar{N_{s t e p}}=1024$ for which means the total number of tokens remains ", "page_idx": 25}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/2b3dfa1023df9efd7f462abca04f83a8398f7d072ace4f64bad653e434ee12c4.jpg", "img_caption": ["Figure 6: The estimation of the attention matrix by positive random features when varying $d_{r}$ "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/71b4f1d102fac798d24572043558081551af54c031ecace6b00e8db15186825f.jpg", "img_caption": ["Figure 7: The estimation of the output matrix by positive random features when varying $d_{r}$ "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "16384. We choose stochastic gradient descent (SGD) [Amari, 1993] as the optimizer and we set the learning rate to 0.003 for normal and regularized models, while the remaining experiments to 0.005. We also attempt the multi-task scenario, where the input token at each step is generated from a different task. However, we find it challenging for a single attention layer to effectively learn in this setting, resulting in disordered predictions. Therefore, our experiments are currently limited to single-task settings, and the multi-task scenario is worth further investigation in the future. ", "page_idx": 26}, {"type": "text", "text": "It is worth noting that we approximate the attention matrix calculation using random features as kernel mapping function instead of using the traditional softmax function in the self-attention layer [Choromanski et al., 2020]. The mapping function $\\phi\\;:\\;\\mathbb{R}^{d_{o}}\\;\\rightarrow\\;\\mathbb{R}^{d_{r}}$ has the form of $\\phi(\\pmb{x})\\ =$ $e^{\\pmb{w}^{T}\\pmb{x}-\\lVert\\pmb{x}\\rVert^{2}/2}$ where $w\\sim\\mathcal{N}(0,I)$ . Orthogonal random features [Yu et al., 2016, Choromanski et al., 2020] or simplex random features [Reid et al., 2023] can be chosen to achieve better performance theoretically. We investigate the impact of changing the dimension of random features $d_{r}$ on the approximation of attention matrices and output, using Mean Squared Error (MSE) and Mean Absolute Error (MAE) as evaluation metrics, where we conduct 50 repeated experiments and calculated the average values for each value of $d_{r}$ , as shown in Figure 8. It can be observed that as the dimension of random features increases, the approximation performance gradually improves, with both errors reaching a low level in the end. We visualize the exact attention matrix and compare it with the estimated attention matrices obtained using different values of $d_{r}$ , as shown in Figure 6. Again, it can be seen that as $d_{r}$ increases, the approximation of the true attention matrix improves gradually and similar results can be observed for the analysis of output matrices in Figure 7. ", "page_idx": 26}, {"type": "text", "text": "To obtain a more accurate estimation of the attention matrix, we set the output dimension of the mapping function to be 100 times the input dimension, that is, $d_{r}=100(d_{s}+\\bar{d}_{t})=1200,$ . Furthermore, we visualize the exact attention matrix and the output with the approximation results, which are shown in the Figure 9. As we can see, although some larger values are not estimated accurately due to the limited dimension of the random features we select, the majority of the information is still estimated comprehensively well. These findings indicate that our choice of using positive random features as mapping functions to estimate the true softmax attention and conduct experiments is relatively feasible. ", "page_idx": 26}, {"type": "text", "text": "After the weights $W_{Q}$ , $W_{K}$ , $W_{V}$ of the attention layer have been determined, we generate test $N+1$ tokens in the same way where the $s$ part of the $(N+1)$ -th token is also set to be zero and finally input the test tokens into the attention layer to obtain the corresponding predicted $\\hat{\\pmb{h}}_{N+1}=[\\hat{\\pmb{t}}_{N+1}^{(1)},\\hat{s}_{N+1}^{(1)}]$ . Here, we also use $h_{T+1}^{\\prime}=\\hat{h}_{N+1}$ to maintain the notation consistency in Section 2.1. ", "page_idx": 26}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/19b1e0269fe8577dfa33591aeb31fafbf99ab5f6c50c0d3143c8156da32b4d8d.jpg", "img_caption": ["(a) Estimation error of attention matrix vs. $d_{r}$ "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/9d8d190e4cd0e6921845bbdc06236b6d2c2191c914b72ed6a9029d7117492e6e.jpg", "img_caption": ["Figure 8: The error of positive random features in estimating the attention and output matrices as $d_{r}$ varies. ", "(b) Estimation error of attention matrix vs. $d_{r}$ "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/d16bd5a11d63cf6cfaeeeec167a475674d3366a692fdaaa7e0cf7beb1fa63238.jpg", "img_caption": ["(a) the exact attention matrix and its approxi- (b) the exact output and its approximation mation "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 9: The comparison between the exact attention matrix, output and their estimated approximations using random features under setting $N=16$ and $d_{r}=1200$ . ", "page_idx": 27}, {"type": "text", "text": "On the other hand, we construct a dual model $f({\\pmb x})=W\\phi({\\pmb x})$ where $\\phi(\\cdot)$ is strictly equivalent to the kernel mapping function used in the attention layer. We transform the first $N$ tokens as the training set according to Theorem 3.1 and train the dual model using the loss formed by Eq (9). In fact, according to Theorem 3.1, after we perform one step of gradient descent on this training set, the test prediction $\\hat{\\pmb{y}}_{t e s t}=[\\hat{\\pmb{t}}_{N+1}^{(2)},\\hat{s}_{N+1}^{(2)}]$ of the dual model will strictly equal $h_{T+1}^{\\prime}$ . ", "page_idx": 27}, {"type": "text", "text": "We conduct experiments under the same setup using different random seeds to explore the effects of various model modifications. The data for all three experiments are generated under identical conditions. One set of experimental results is presented in the main text, while the results of the other two sets are shown in the Figure 10. Similar to the discussion in the main text, we can achieve better performance than the normal model with appropriate parameter settings. ", "page_idx": 27}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/289cb932f3952e2b1e814680a9ddc27c70eb06a3006dd8fcb9141788460da384.jpg", "img_caption": ["Figure 10: The performance for regularized models (Center Left), augmented models (Center Right) and negative models (Right) with different settings for different random seeds. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "E.2 More details of Experiments on Different Tasks ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In addition to conducting experiments on linear regression tasks, we also extended our experiments to involve trigonometric and exponential tasks. ", "page_idx": 28}, {"type": "text", "text": "E.2.1 More details of Experiments on Trigonometric Tasks ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/f373f117cbfa8a89f9a3d7ffcc415823ba807a11667df61301f41620828a5229.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 11: The equivalence between ICL of one softmax attention layer and gradient descent, along with analysis on different model modifications for trigonometric tasks. Left Part: $||\\hat{y}_{t e s t}-h_{T+1}^{\\prime}||_{2}$ as the gradient descent proceeds under setting $N=127$ ; Remaining Part: the performance for regularized models (Center Left), augmented models (Center Right) and negative models (Right) with different settings. ", "page_idx": 28}, {"type": "text", "text": "For trigonometric task, we generate the task by $\\pmb{s}=\\cos(\\pmb{W}t)$ where $\\cos(\\cdot)$ is element-wise, $W\\in$ $\\mathbb{R}^{d_{s}\\times d_{t}^{-}}$ is sampled from the normal distribution $W_{i j}\\sim\\mathcal{N}(0,1)$ while $\\pmb{t}$ is sampled from the uniform distribution $\\pmb{x}\\sim U(0,\\pi)^{d_{t}}$ . In experiments, we found that for one softmax attention layer, learning higher-dimensional tasks is challenging. Therefore, we only set $d_{t}=7$ and $d_{s}=1$ . At each step, we 1u6se3 $N+1=128$ ttoo ktheen ss $\\{\\pmb{x}_{i}=[\\pmb{t}_{i};\\pmb{s}_{i}]\\}_{i=1}^{N+1}$ fa lnidn etahre  ttaostkasl,  nwue mobbesre rovfe tdo tkheant sf orer mmaoirnes  cuonmcphlaenxg teads kast, $N+1=16$   \nthe attention layer needs to use more tokens to provide information at each training step. Similarly, we mask the label part of the last token, that is, $\\pmb{\\mathscr{s}}_{N+1}=\\mathbf{0}$ and use mean square error (MSE) loss to train the attention layer. We choose SGD as the optimizer and the learning rate is set as 0.005. The rest of the settings remain consistent with those used in the linear task. The result for trigonometric regression task is shown in Figure 11. ", "page_idx": 28}, {"type": "text", "text": "Firstly, as shown in the left part of Figure 11, the inference results is of ICL is strictly equivalent to the prediction of the dual model, that is, $\\hat{h}_{N+1}=\\hat{y}_{t e s t}$ as well as the label part $\\hat{s}_{N+1}^{(1)}=\\hat{s}_{N+1}^{(2)}$ , aligning with our analysis in Theorem 3.1. ", "page_idx": 28}, {"type": "text", "text": "The performance of modified model during training process can be seen in the remaining parts of Figure 11. For regularized models, as seen in the center left part of figure 11, the models when $\\alpha<0$ converge slightly faster and reach better final results compared to the normal model $(\\alpha=0)$ ). For augmented models, we use as the same augmentation functions $g_{1}$ and $g_{2}$ as the ones in the linear regression task, that is, $g_{1}(\\pmb{x})=g_{2}(\\pmb{x})=\\bar{\\sigma}(\\pmb{W x})$ where $\\sigma(\\cdot)$ is GELU activation function. However, for $g_{2}^{+}$ , we use ELU as the activation function. We can find from the center right part of Figure 11 that, compared to the normal model, using $g_{1}$ alone and using $g_{1}$ and $g_{2}$ simultaneously as data augmentations significantly degrade the model\u2019s performance, including convergence speed and final results. However, using $g_{2}$ alone yields comparable result with the normal model. Particularly, when using $g_{2}^{+}$ , the model accelerates its convergence speed. However, for negative models, the performance with the selected number of negative samples $k$ and the parameter $\\beta$ is worse than the normal model, which suggests that our simple approach of selecting those tokens low attention scores as negative samples is not a reasonable method. Just as we discussed in Section 4, for different tasks, a more refined strategy for selecting negative samples should be considered. ", "page_idx": 28}, {"type": "text", "text": "E.2.2 More details of Experiments on Exponential Tasks ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For exponential task, we generate the task by $\\pmb{\\mathscr{s}}\\mathrm{~=~}\\exp(W t)$ where $\\exp(\\cdot)$ is also element-wise, $W\\,\\in\\,\\dot{\\mathbb{R}}^{d_{s}\\times d_{t}}$ is sampled from the normal distribution $W_{i j}\\,\\sim\\,{\\mathcal{N}}(0,1)$ while $\\pmb{t}$ is sampled from the uniform distribution $\\pmb{x}\\sim U(-1,1)^{d_{t}}$ . We only set $d_{t}=6$ and $d_{s}=1$ considering the limited learning capacity of one softmax attention layer. At each training step, we use $N+1=512$ tokens $\\{\\pmb{x}_{i}=\\bar{[\\pmb{t}_{i};\\pmb{s}_{i}]}\\}_{i=1}^{\\bar{N}+1}$ and the total number of tokens remains unchanged at 16384. Compared to the setting $N+1=16$ of linear tasks and $N+1=128$ of trigonometric tasks, we also find that for exponential tasks, the attention layer needs more tokens to provide in-context information at each training step. The rest of the settings remain consistent with those used in the trigonometric task. The result for exponential regression task is shown in Figure 12. ", "page_idx": 28}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/0a7e128885d89043592148bc5eae475e74c738c37c4612f699dea3f9d464e0c5.jpg", "img_caption": ["Figure 12: The equivalence between ICL of one softmax attention layer and gradient descent, along with analysis on different model modifications for exponential tasks. Left Part: $||\\hat{y}_{t e s t}-h_{T+1}^{\\prime}||_{2}^{2}$ as the gradient descent proceeds under setting $N=511$ ; Remaining Part: the performance for regularized models (Center Left), augmented models (Center Right) and negative models (Right) with different settings. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Similarly, as shown in the left part of Figure 12, the result $\\hat{h}_{N+1}$ of ICL inference is equivalent to the test prediction $\\hat{\\pmb{y}}_{t e s t}$ of the dual model after training, just as stated in Theorem 3.1. For regularized models, it can be observed that when $\\alpha=16$ , the model converges faster and achieves better result. For augmented models, using $g_{1}$ or $g_{2}$ alone as data augmentations results in better performance. However, when both $g_{1}$ and $g_{2}$ are used simultaneously, the training process becomes unstable, so we did not show it in the center right part of Figure 12. For negative model, similar to the case in the trigonometric task, the different combinations of negative samples\u2019 number $k$ and parameter $\\beta$ do not show a significant improvement over the normal model, highlighting the importance of the strategy for selecting negative samples. We leave the exploration of a more refined negative sample selection strategy when facing various tasks for future consideration. ", "page_idx": 29}, {"type": "text", "text": "E.3 More Experiments on Combinations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In addition, we also conduct experiments with their combinations on linear tasks, trigonometric tasks , and exponential tasks. The results are shown in Figure 13. For linear tasks, a combination of regularized and augmented modifications is sufficient. However, for the other two tasks, the results are actually worse than using regularized or augmented modification individually (compared to Figures 11 and 12). We think this may be due to the ineffective selection of negative samples, which is amplified when combined. Therefore, when the design of augmentation or negative sample improvement methods is not effective, we recommend using a single modification method. ", "page_idx": 29}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/86dd50abb8518b8a8c691246cd77cbf8645b16f17cf9ee5c706a518634f3b525.jpg", "img_caption": ["Figure 13: Performance of different combinations on linear (Left), exponential (Center), and trigonometric (Right) tasks. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "E.4 More Experiments on One Transformer Layer ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Similar to the experiments with one softmax attention layer, we also conduct experiments on a Transformer layer (introducing one FFN layer after the attention layer) and trained its dual model based on Theorem B.1. As shown in Figure 14, the inference result $\\hat{h}_{N+1}$ of ICL remains equivalent to the test prediction $\\hat{\\pmb{y}}_{t e s t}$ of the trained dual model. Furthermore, to validate the potential low-rank ", "page_idx": 29}, {"type": "image", "img_path": "dB6gwSDXKL/tmp/e24200885d300aa5f70ece35aebcf352b2cb87f5912f5b2c00c83ed763d4133d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 14: The equivalence between ICL of one Transformer layer and gradient descent, along with analysis on upper bound of $\\mathrm{Rank}(W_{F})$ . Left: $||\\hat{y}_{t e s t}-h_{T+1}^{\\prime}||_{2}$ as the gradient descent proceeds under setting $N=15$ ; Right: the upper bound of $\\mathrm{Rank}(W_{F})$ when setting $d=12$ and varying $d_{h}$ . ", "page_idx": 30}, {"type": "text", "text": "property of matrix $W_{F}$ , we explore its upper bound of rank. Noting that $W_{F}=W_{2}I_{M}W_{1}$ where $\\mathbf{\\dot{W_{1}}}^{2}\\in\\dot{\\mathbb{R}}^{d_{h}\\times d}$ , $W_{2}\\in\\mathbb{R}^{d\\times d_{h}}$ , $\\dot{I}_{M}\\in\\mathbb{R}^{d_{h}\\times\\dot{d}_{h}}$ , the upper bound of $\\mathrm{Rank}(W_{F})$ is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname{Rank}(W_{F})\\leq\\operatorname*{min}\\left\\{d,d_{h},\\operatorname{Rank}(I_{M})\\right\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\operatorname{Rank}(I_{M})$ is equivalent to the number of non-zero elements in $\\scriptstyle I_{M}$ . We fix $d=12$ while varying the values of $d_{h}$ . We generate 1024 sets of $X_{t e s t}$ for different tasks and repeat the experiments 5 times. Finally, we calculate the average upper bound of the rank of $W_{F}$ . The results are shown in the right part of Figure 14, indicating that when $d_{h}\\,\\geq\\,2.75d\\,=\\,33$ , the upper bound remains stable and equals $d=12$ . Otherwise, when $d_{h}$ is set to a smaller value, $W_{F}$ exhibits clear low-rank property. ", "page_idx": 30}, {"type": "text", "text": "E.5 More Experiments on More Realistic NLP Tasks ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We supplement our experiments on more more realistic NLP tasks. We choose the BERT-baseuncased model (can be downloaded from Huggingface library[Wolf, 2019], hereafter referred to as BERT[Kenton and Toutanova, 2019]) to validate the effectiveness of modifications to the attention mechanism and select four relatively smaller GLUE datasets (CoLA, MRPC, STS-B, RTE) [Wang, 2018]. We load the checkpoint of the pre-trained BERT model, where \u2019classifier.bias\u2019 and \u2019classifier.weight\u2019 are newly initialized, and then we fine-tune the model to explore the performance of three attention modifications as well as their combinations. In terms of more detailed experiment settings, we set the batch size to 32, the learning rate to 2e-5, and the number of epochs to 5 for all datasets. All experiments are conducted on a single 24GB NVIDIA GeForce RTX 3090. All experimental results are presented in Table 1. Below, we discuss the various modifications and their performance. ", "page_idx": 30}, {"type": "text", "text": "For the regularized modification, we consider different values of $\\alpha$ , specifically selected from $\\left\\{-0.5,-0.1,0.1,0.5\\right\\}$ . As can be observed in Table 1, except for RTE, the best regularized models outperform the original model on the other three datasets. However, we also note that when the absolute value of $\\alpha$ is too large, the model\u2019s performance declines significantly, so we recommend using smaller absolute values for $\\alpha$ . ", "page_idx": 30}, {"type": "text", "text": "For the augmented modification, we also consider applying more complex \u201caugmentation\u201d functions to the linear key/value mappings. However, unlike the previous methods used in simulation tasks, we do not simply select $g_{1}$ and $g_{2}$ as MLPs, i.e., $g_{1}(W_{V}{\\pmb x})=W_{2}{\\sigma}(W_{1}W_{V}{\\pmb x})$ . This design is avoided because it could undermine the effort made during pre-training to learn the weights $W_{V}$ and $W_{K}$ , leading to difficulties in training and challenges in comparison. Instead, we adopt a parallel approach, i.e., $g_{1}\\big(W_{V}x\\big)=W_{V}x+c W_{2}\\sigma(W_{1}x)$ , where $c$ is a hyperparameter to control the influence of the new branch, $\\sigma$ is the GELU activation function and the hidden layer dimension is set to twice the original size of $W_{V}x$ . $g_{2}\\bigl(W_{K}x\\bigr)=W_{K}x+c W_{2}\\sigma\\bigl(W_{1}x\\bigr)$ follows the same format. ", "page_idx": 30}, {"type": "text", "text": "Experimental results show that the best augmented models achieve better performance than the original model across all four datasets. Notably, augmentation on the value mapping (i.e., using $g_{1}$ alone) proves to be more effective than other methods, both in terms of performance and the amount of additional parameters introduced. Using both $g_{1}$ and $g_{2}$ introduces more parameters, which is particularly undesirable for larger models. Thus, under the augmentation methods and experimental settings we selected, using $g_{1}$ alone is recommended. ", "page_idx": 30}, {"type": "text", "text": "In addition, we do not rule out the possibility of more powerful and efficient augmentation methods. Our choice of $g_{1}$ and $g_{2}$ as parallel MLPs is primarily motivated by the desire to make better use of the pre-trained weights $W_{K}$ and $W_{V}$ . We have also noticed that this specific augmentation function design is structurally similar to the Parallel Adapter [He et al., 2021]. However, we would like to emphasize that our parallel design is just a specific case within this broader augmented modification framework and this is a new perspective for understanding the Parallel Adapter. As for practical implementation, the Parallel Adapter method focuses more on efficient training, so it uses fewer parameters, and the original $W_{V}$ and $W_{K}$ are freezed\u2014only the newly introduced parameters are trained. In contrast, our approach aims to validate the benefits of introducing stronger nonlinear augmentation functions into the linear value/key mappings. Therefore, we set a higher hidden layer dimension (twice that of $W_{V}x$ or $W_{K}{\\boldsymbol{x}}$ ) and also train $W_{V}$ and $W_{K}$ simultaneously. This design is relatively general and does not take into account the specific characteristics of individual tasks. We still encourage the development of more task-specific augmentation strategies tailored to different tasks. ", "page_idx": 30}, {"type": "table", "img_path": "dB6gwSDXKL/tmp/f541884f803c57b8d20bd20f62e10e7b8eef06abb139df15fda75ca876475bb6.jpg", "table_caption": [], "table_footnote": ["Table 1: Partial GLUE test results of different modifications. \u201cLocal Best\" is used to display the best results for each modification type, where bolded results indicate the performance superior to the original model. \u201cGlobal Best\" is used to showcase the best results among all modifications. Matthews correlation, F1 scores/accuracy, Pearson/Spearman correlation, accuracy are reported for CoLA, MRPC, STS-B, RTE respectively. "], "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "For the negative modification, we continue to select tokens with lower attention scores as negative samples. The parameter $r$ represents the proportion of tokens used as negative samples, while $\\beta$ indicates the overall reduction in attention scores. We choose $r$ from $\\{0.1,0.2,0.3\\}$ and $\\beta$ from $\\{0.1,0.2\\}$ . Under these combinations, the best negative models only outperform the original model on CoLA and STS-B, whereas their performance on MRPC and RTE is worse than the original one. This suggests that our simple approach of considering tokens with low attention scores as negative samples might be too coarse. A more effective method for constructing negative samples should be designed, which is a direction worth exploring in the future. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "We also consider combining different modification methods. Specifically, we choose $\\alpha=0.1$ , $g_{1}/c=1$ and $r=0.2/\\beta=0.1$ respectively as the basis for combining the three types of modifications, considering their overall performance across all datasets. The results indicate that under our settings, the combination of augmented and negative modification achieves the best performance on CoLA, MRPC, and RTE, while the combination of regularized and augmented modification achieves the best performance on STS-B. However, their optimal performance is slightly inferior to the best performance achieved with augmented models alone. Therefore, we conclude that using all three modifications simultaneously is not necessary. With appropriate hyperparameter choices, using augmented modification alone or in combination with one other modification is sufficient. ", "page_idx": 32}, {"type": "text", "text": "Overall, the experimental results show that our modifications inspired by the representation learning process are helpful in enhancing performance. This further validates the potential of our approach of thinking about and improving the attention mechanism from a representation learning perspective. In addition, we would like to reiterate that more validation across additional tasks and models, and the development of task-specific augmentation and negative sampling methods are all interesting directions worth exploring in the future. ", "page_idx": 32}, {"type": "text", "text": "F More Details about Related Work ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we provide additional details about the related work in Section 6, especially those that involve formalization. Dai et al. [2022] interpret ICL as implicit fine-tuning: More specifically, let $\\pmb{X}=[\\pmb{X}_{D},\\pmb{X}_{T}]$ where ${\\cal X}_{D}=[{\\pmb x}_{1},{\\pmb x}_{2},...\\,,{\\pmb x}_{N}]$ denotes the demonstration tokens and $X_{T}=$ $[\\pmb{x}_{1}^{\\prime},\\pmb{x}_{2}^{\\prime},\\dots,\\pmb{x}_{T}^{\\prime}]$ be query tokens. On the one hand, for ICL, they consider the output of $\\textbf{\\em q}=$ $W_{Q}{\\pmb x}_{T+1}^{\\prime}$ under the linear attention setting as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{F}_{\\mathrm{ICL}}(\\pmb{q})=W_{V}[X_{D},\\pmb{X}_{T}](W_{K}[\\pmb{X}_{D};\\pmb{X}_{T}])^{T}\\pmb{q}}\\\\ &{\\quad\\quad\\quad=W_{V}\\pmb{X}_{T}(\\pmb{W}_{K}\\pmb{X}_{T})^{T}\\pmb{q}+W_{V}\\pmb{X}_{D}(W_{K}\\pmb{X}_{D})^{T}\\pmb{q}}\\\\ &{\\quad\\quad\\quad=W_{\\mathrm{ZSL}}\\pmb{q}+\\mathrm{LinearAtten}(W_{V}\\pmb{X}_{D},\\pmb{W}_{K}\\pmb{X}_{D},\\pmb{q})}\\\\ &{\\quad\\quad\\quad=W_{\\mathrm{ZSL}}\\pmb{q}+\\displaystyle\\sum_{i}\\left((W_{V}\\pmb{x}_{i})\\odot(\\pmb{W}_{K}\\pmb{x}_{i})\\right)^{T}\\pmb{q}}\\\\ &{\\quad\\quad\\quad=W_{\\mathrm{ZSL}}\\pmb{q}+\\Delta W_{\\mathrm{ICL}}\\pmb{q},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $W_{\\mathrm{ZSL}}\\pmb{q}$ is interpreted as the output in the zero-shot learning (ZSL) where no demonstrations are given. On the other hand, they consider a specific fine-tuning setting, which updates only the parameters for the key and value projection, that is, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{F}_{\\mathrm{FT}}(\\pmb{q})=({W}_{V}+\\Delta{W}_{V})\\pmb{X}\\pmb{X}^{T}({W}_{K}+\\Delta{W}_{K})^{T}\\pmb{q}}\\\\ &{\\quad\\quad\\quad=({W}_{\\mathrm{ZSL}}+\\Delta{W}_{\\mathrm{FT}})\\pmb{q}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\Delta W_{K}$ and $\\Delta W_{V}$ denote the parameter updates and they are acquired by back-propagation from task-specific training objectives [Dai et al., 2022], which is a supervised learning process of the original model. Considering the similarity in form between $\\tilde{F}_{\\mathrm{ICL}}$ and $\\tilde{F}_{F T}$ , their focus is on establishing a connection between ICL and implicit fine-tuning on the original model. ", "page_idx": 32}, {"type": "text", "text": "As a comparison, we turn our attention to establish a connection between ICL and the gradient descent process of the dual model, rather than the original model. More specifically, we consider the dual model $f({\\pmb x})=W\\phi({\\pmb x})$ of the nonlinear attention layer, where the weight $W$ are updated according to the following loss (presented as Eq (9) in Section 3.2): ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{1}{\\eta D}\\sum_{i=1}^{N}(W_{V}\\pmb{x}_{i})^{T}W\\phi(W_{K}\\pmb{x}_{i}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\pmb{x}_{i}$ is the $i$ -th demonstration token. The prediction output of the trained dual model will be consistent with the ICL output of the attention layer. The gradient descent process of the dual model using this loss can be viewed from a self-supervised learning lens: unlike in supervised fine-tuning, where the original model is instructed to perform gradient descent using a given objective (loss), this loss formed as Eq (9) is determined (derived) by the attention mechanism itself and it also does not require additional \"true label\" to supervise each token $\\pmb{x}_{i}$ (so called self-supervised). Therefore, modifications to this self-supervised learning loss will in turn cause modifications in the attention mechanism correspondingly, as we discussed in our work in Section 4. We believe this perspective offers several benefits: ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "\u2022 By analyzing from the dual perspective, we can transform the forward inference process into an optimization process. Since optimization processes are well-known and have established theoretical tools (for example, generalization error as mentioned in Section 3.3), this transformation can provide reverse insights into analyzing the model mechanisms.   \n\u2022 It can clearly observed that the dual model involves a self-supervised representation learning process from the dual perspective. Considering that there are lots of mature works in this area, we can draw on these works to reflect on the attention mechanism, which has also inspired attention modifications as illustrated in Section 4.   \n\u2022 Intuitively, this explanation might be also reasonable as the original model is not explicitly instructed to provide the answer under some given objective (e.g., minimizing cross-entropy) during ICL inference process. Instead, the underlying criterion should be determined by the model\u2019s own structure (self-supervised) as we mentioned above. ", "page_idx": 33}, {"type": "text", "text": "In addition, although we do not target specific tasks like linear regression as previous works mentioned in Section 6, we would like to point out that under those specific weight and input settings, an intuitive explanation can also be provided from a representation learning perspective. Here, we take the linear regression task as well as the weight constructions considered by Von Oswald et al. [2023a] as an example. Specifically, it assumes that the structured input is $H\\,=\\,[h_{i}]_{i=1}^{N}\\,\\in\\,\\mathbb{R}^{(d+1)\\times(N)}$ where $\\pmb{h}_{i}\\;=\\;[\\pmb{x}_{i},y_{i}]$ is sampled from some linear task $y\\;=\\;w^{T}x$ and the query token will be ${\\pmb h}_{N+1}=[{\\pmb x}_{N+1},-\\dot{\\pmb w}_{0}^{T}{\\pmb x}_{N+1}]$ . And the considered linear self-attention layer will take the constructed weights and query output as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\pmb{W}}_{K}={\\pmb{W}}_{Q}=\\left[\\begin{array}{c c}{{\\pmb{I}}_{d\\times d}}&{0}\\\\ {0}&{0}\\end{array}\\right],{\\pmb{W}}_{V}=\\left[\\begin{array}{c c}{\\!\\!0_{d\\times d}}&{0}\\\\ {\\!\\!w_{0}^{T}}&{-1\\!\\!}\\end{array}\\right],{\\pmb{P}}=\\frac{\\eta}{N}{\\pmb{I}},}\\\\ {\\tilde{\\pmb{h}}_{N+1}={\\pmb{h}}_{N+1}+{\\pmb{P}}({\\pmb{W}}_{V}{\\pmb{H}})({\\pmb{W}}_{K}{\\pmb{H}})^{T}{\\pmb{W}}_{Q}{\\pmb{h}}_{N+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\pmb{w}_{0}$ is the underlying initial matrix. Then the label part of $\\tilde{h}_{N+1}$ will has the form as $\\begin{array}{r}{\\tilde{y}_{N+1}=-w_{0}^{T}x_{N+1}+\\dot{\\Delta}w^{\\overline{{T}}}x_{N+1}=-(w_{0}^{T}-\\frac{\\eta}{N}\\sum_{i=1}^{N}(w_{0}^{T}\\overline{{\\mathbf{x}}}_{i}-y_{i})\\pmb{x}_{i}^{T})\\pmb{x}_{N+1}=-\\hat{y}_{N+1},}\\end{array}$ which is equivalent to the output $-\\hat{y}_{N+1}$ (multiplied by $-1_{,}$ ) of the linear layer $\\boldsymbol{y}\\;=\\;\\boldsymbol{w}^{T}\\boldsymbol{x}$ where $\\pmb{w}$ is initialized as $\\pmb{w}_{0}$ after performing one step of gradient descent under mean squared loss ${\\mathcal{L}}=$ $\\begin{array}{r}{\\frac{1}{2N}\\sum_{i=1}^{N}\\|\\pmb{w}_{0}^{T}\\pmb{x}_{i}-\\pmb{y}_{i}\\|^{2}}\\end{array}$ . ", "page_idx": 33}, {"type": "text", "text": "In practice, the underlying initial weight matrix $\\pmb{w}_{0}$ is set to be approximately 0 thus the test input can be formed as $\\pmb{h}_{N+1}=[\\pmb{x}_{i},\\mathbf{0}]$ [Von Oswald et al., 2023a]. In addition, when reading out the label $\\hat{y}_{N+1}$ , the test prediction $\\tilde{y}_{N+1}$ will be multiplied again by $-1$ , which can be done by a final projection matrix (or equivalently, $\\begin{array}{r}{P=-\\frac{\\eta}{N}I)}\\end{array}$ . In this case, we first note that the dual model of the linear attention layer can be written as $f(z)=W z$ where $W\\in\\mathbb{R}^{(d+1)\\times(d+1)}$ and similar to Eq (9), it will be trained under the loss below: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W}\\mathcal{L}=-\\frac{1}{\\eta}\\sum_{i=1}^{N}\\left(P W_{V}\\boldsymbol{h}_{i}\\right)^{T}W W_{K}\\boldsymbol{h}_{i}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By substituting the corresponding weights in Eq (35) where we replace $\\begin{array}{r}{P=-\\frac{\\eta}{N}I}\\end{array}$ for the readout, the loss can be reformulated as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W}\\mathcal{L}=-\\frac{1}{N}\\sum_{i=1}^{N}\\left[0,\\;y_{i}\\right]W\\left[\\!\\!\\begin{array}{c}{{x_{i}}}\\\\ {{0}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Recalling that $\\pmb{h}_{i}=[\\pmb{x}_{i},y_{i}]$ is sampled from some linear task $y=w^{T}x$ , we assume that $\\|\\pmb{W}\\|_{F}\\leq$ ${\\|\\pmb{w}\\|}_{2}$ , it can then be easily seen that the optimal solution for Eq (37) will be ", "page_idx": 33}, {"type": "equation", "text": "$$\nW^{*}=\\left[\\!\\!\\begin{array}{l l}{\\mathbf{0}}&{\\mathbf{0}}\\\\ {\\pmb{w}^{T}}&{\\mathbf{0}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Furthermore, similar to Section 3.2, we take $W_{Q}h_{N+1}$ as the input where $W_{Q}$ is constructed as Eq (35) and $\\pmb{h}_{N+1}\\,=\\,[\\pmb{x}_{N+1},0]$ , the optimal dual model will output the result $f(W_{Q}h_{N+1})\\,=$ $W^{*}W_{Q}{h_{N+1}}\\,=\\,[{\\bf0},{\\bf w}^{T}x_{N+1}]\\,=\\,[{\\bf0},y_{N+1}]$ where the label part will be just the answer for the test query. Additionally, it would also be interesting to explore how these weights converge to the constructed form in Eq (35) or other forms under this special setting as previous works illustrated from the perspective of the dual model. Investigating this issue goes beyond the scope of this paper, and we will leave it for future exploration. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We confirm that the main claims made in the abstract and introduction accurately reflect our contributions and scope. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We have discussed the limitations at the end of the main body. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We confirm that the paper provides the full set of assumptions and a complete (and correct) proof for each theoretical result. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We confirm that we have provided a detailed explanation of our experimental setup. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have provided our code and instructions in the supplemental material. Our experiments are primarily simulation-based, and we have provided the code to generate the data in the code part. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We have provided as detailed an explanation as possible for all details in the appendix. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We conduct experiments with different random seeds for three times in the linear task setting, and the results indicate that the model modifications are effective to some extent. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have provided sufficient information on the computer resources in Appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We confirm that our research meets the requirements. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have provided relevant explanations at the end of the main body. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: We think that our paper poses no such risks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not use existing assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}]