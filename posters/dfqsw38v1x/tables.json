[{"figure_path": "dfqsW38v1X/tables/tables_7_1.jpg", "caption": "Table 1: WikiText-2 perplexity results on 4-bit quantization of LLAMA-2 models with 2048 sequence length. We extract the results for SmoothQuant and OmniQuant results of [Shao et al., 2023]. 128G shows the group-wise quantization with group size 128.Here, we quantize all weights, activations, and caches in 4-bits in QuaRot.", "description": "This table presents the WikiText-2 perplexity scores achieved by various 4-bit quantization methods on the LLAMA-2 language models.  The models are tested with sequences of length 2048.  The table compares QuaRot's performance against SmoothQuant, OmniQuant, QUIK-4B, Atom-128G, and QuaRot-128G, highlighting the impact of different quantization techniques and outlier feature handling strategies on model accuracy.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_7_2.jpg", "caption": "Table 2: Zero-shot accuracy of LLAMA-2 models with 4-bit (A4W4KV4) QuaRot on PIQA (PQ), WinoGrande (WG), HellaSwag (HS), Arc-Easy (A-e), Arc-Challenge (A-c), and LAMBADA (LA).", "description": "This table presents the zero-shot accuracy results of LLAMA-2 models (7B, 13B, and 70B parameters) that use QuaRot for 4-bit quantization (weights, activations, and KV cache).  The accuracy is measured across six different zero-shot tasks: PIQA, WinoGrande, HellaSwag, Arc-Easy, Arc-Challenge, and LAMBADA.  The table compares the accuracy of the quantized models (QuaRot) to the original, full-precision models (FP16).", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_8_1.jpg", "caption": "Table 3: WikiText-2 Perplexity and zero-shot accuracy of QuaRot on the LLAMA-2 family using 4- and 8-bits with Round-to-Nearest (RTN) weights and activation quantization. For zero-shot tasks, we use PIQA (PQ), WinoGrande (WG), HellaSwag (HS), Arc-Easy (A-e), Arc-Challenge (A-c), and LAMBADA (LA). We quantize all weights, activations, and caches.", "description": "This table presents the results of QuaRot using 4 and 8 bits with round-to-nearest quantization for weights and activations on the LLAMA-2 family of models. It shows the WikiText-2 perplexity and zero-shot accuracy on six different tasks: PIQA, WinoGrande, HellaSwag, Arc-Easy, Arc-Challenge, and LAMBADA.  All weights, activations, and caches are quantized.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_8_2.jpg", "caption": "Table 1: WikiText-2 perplexity results on 4-bit quantization of LLAMA-2 models with 2048 sequence length. We extract the results for SmoothQuant and OmniQuant results of [Shao et al., 2023]. 128G shows the group-wise quantization with group size 128.Here, we quantize all weights, activations, and caches in 4-bits in QuaRot.", "description": "This table presents the WikiText-2 perplexity scores achieved by various 4-bit quantization methods on LLAMA-2 models with a sequence length of 2048.  It compares QuaRot's performance against SmoothQuant and OmniQuant, highlighting QuaRot's ability to quantize all weights, activations, and KV caches to 4 bits without significant performance loss.  The table also includes results using group-wise quantization with different group sizes (128G).", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_13_1.jpg", "caption": "Table 6: WikiText-2 perplexity with various KV cache precision using QuaRot.", "description": "This table presents the WikiText-2 perplexity scores achieved by QuaRot when using different bit precisions for the key (K bits) and value (V bits) components of the KV cache.  The results are shown for three different LLAMA-2 models (7B, 13B, and 70B parameters).  The table demonstrates how the model's performance varies depending on the bit precision allocated to the keys and values in the KV cache, highlighting the trade-off between model accuracy and memory efficiency.", "section": "5.3 Ablation Studies"}, {"figure_path": "dfqsW38v1X/tables/tables_14_1.jpg", "caption": "Table 7: Weight-only quantization results on WikiText-2 on LLAMA-2 models. We use asymmetric per-column quantization and keep the inputs and KV cache in FP16. We show the perplexity results >100 by Inf. We show the failed GPTQ experiments using NaN.", "description": "This table presents the results of applying weight-only quantization methods (RTN and GPTQ) to LLAMA-2 models of varying sizes (7B, 13B, and 70B parameters).  It shows the WikiText-2 perplexity achieved with different quantization bit-widths (A16W4, A16W3, A16W2 representing 16-bit activations and 4, 3, and 2-bit weights, respectively).  The results showcase the impact of QuaRot on the performance of weight-only quantization, demonstrating improved perplexity compared to standard RTN and GPTQ in several configurations.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_14_2.jpg", "caption": "Table 8: WikiText-2 perplexity of 4-bit QuaRot on LLAMA-2 models with different orthogonal matrices.", "description": "This table presents the results of applying random orthogonal matrices instead of Hadamard matrices in QuaRot for 4-bit quantization on LLAMA-2 models of various sizes. It compares the perplexity scores achieved using random orthogonal matrices against those obtained using Hadamard matrices, highlighting the impact of matrix choice on model performance.", "section": "5.3 Ablation Studies"}, {"figure_path": "dfqsW38v1X/tables/tables_15_1.jpg", "caption": "Table 1: WikiText-2 perplexity results on 4-bit quantization of LLAMA-2 models with 2048 sequence length. We extract the results for SmoothQuant and OmniQuant results of [Shao et al., 2023]. 128G shows the group-wise quantization with group size 128.Here, we quantize all weights, activations, and caches in 4-bits in QuaRot.", "description": "This table compares the performance of QuaRot against other 4-bit quantization methods on the WikiText-2 language modeling benchmark, using LLAMA-2 models of various sizes (7B, 13B, and 70B parameters).  It shows the perplexity scores achieved by each method, highlighting QuaRot's superior performance with minimal loss compared to the baseline, even without needing to identify and retain outlier features in higher precision. Group-wise quantization results are also included for comparison.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_15_2.jpg", "caption": "Table 1: WikiText-2 perplexity results on 4-bit quantization of LLAMA-2 models with 2048 sequence length. We extract the results for SmoothQuant and OmniQuant results of [Shao et al., 2023]. 128G shows the group-wise quantization with group size 128.Here, we quantize all weights, activations, and caches in 4-bits in QuaRot.", "description": "This table presents a comparison of different quantization methods on the WikiText-2 language modeling task using LLAMA-2 models.  The models are quantized to 4 bits, and the table shows the resulting perplexity scores.  The comparison includes baseline performance, SmoothQuant, OmniQuant, and QUIK, highlighting the performance of QuaRot with and without group-wise quantization.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_16_1.jpg", "caption": "Table 11: WikiText-2 perplexity results on 4-bit quantization of LLAMA-3 models with 2048 sequence length. 128G shows the group-wise quantization with group size 128.", "description": "This table presents the WikiText-2 perplexity results for different quantization methods applied to LLAMA-3 models.  The models used 2048 sequence lengths.  The baseline results are compared against QuaRot and QuaRot-128G which uses group-wise quantization with a group size of 128.  The table shows the impact of different quantization techniques on model performance for both 8B and 70B parameter models.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_16_2.jpg", "caption": "Table 12: Zero-shot accuracy of LLAMA-3 models with 4-bit QuaRot on PIQA (PQ), WinoGrande (WG), HellaSwag (HS), Arc-Easy (A-e), Arc-Challenge (A-c), and LAMBADA (LA).", "description": "This table presents the zero-shot accuracy results for LLAMA-3 models (8B and 70B parameters) after applying the QuaRot quantization method. The accuracy is measured across six different tasks: PIQA, WinoGrande, HellaSwag, Arc-Easy, Arc-Challenge, and LAMBADA.  The table compares the performance of the original FP16 models to the performance after applying QuaRot quantization.", "section": "5.2 Performance Analysis"}, {"figure_path": "dfqsW38v1X/tables/tables_16_3.jpg", "caption": "Table 1: WikiText-2 perplexity results on 4-bit quantization of LLAMA-2 models with 2048 sequence length. We extract the results for SmoothQuant and OmniQuant results of [Shao et al., 2023]. 128G shows the group-wise quantization with group size 128.Here, we quantize all weights, activations, and caches in 4-bits in QuaRot.", "description": "This table presents a comparison of different quantization methods on the WikiText-2 benchmark using LLAMA-2 models with a sequence length of 2048 tokens.  It shows the perplexity achieved by various methods, including the proposed QuaRot method, SmoothQuant, OmniQuant, and QUIK.  The table highlights the impact of different quantization techniques and the elimination of outlier features on model performance. The 'Outlier Features' column refers to the number of features retained in higher precision to accommodate outliers.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_18_1.jpg", "caption": "Table 1: WikiText-2 perplexity results on 4-bit quantization of LLAMA-2 models with 2048 sequence length. We extract the results for SmoothQuant and OmniQuant results of [Shao et al., 2023]. 128G shows the group-wise quantization with group size 128.Here, we quantize all weights, activations, and caches in 4-bits in QuaRot.", "description": "This table presents a comparison of different 4-bit quantization methods on the WikiText-2 dataset using LLAMA-2 models.  It shows the perplexity scores achieved by various methods, including SmoothQuant and OmniQuant,  comparing the number of outlier features retained at higher precision.  The table highlights QuaRot's performance, demonstrating its ability to achieve low perplexity with no outlier features retained.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_19_1.jpg", "caption": "Table 1: WikiText-2 perplexity results on 4-bit quantization of LLAMA-2 models with 2048 sequence length. We extract the results for SmoothQuant and OmniQuant results of [Shao et al., 2023]. 128G shows the group-wise quantization with group size 128.Here, we quantize all weights, activations, and caches in 4-bits in QuaRot.", "description": "This table compares the performance of QuaRot against other state-of-the-art 4-bit quantization methods on the WikiText-2 benchmark using LLAMA-2 models.  It shows the perplexity scores achieved by different methods, highlighting QuaRot's superior performance with minimal loss compared to the baseline (full precision).  The table also includes results for group-wise quantization, demonstrating QuaRot's effectiveness across various quantization strategies.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_19_2.jpg", "caption": "Table 1: WikiText-2 perplexity results on 4-bit quantization of LLAMA-2 models with 2048 sequence length. We extract the results for SmoothQuant and OmniQuant results of [Shao et al., 2023]. 128G shows the group-wise quantization with group size 128.Here, we quantize all weights, activations, and caches in 4-bits in QuaRot.", "description": "This table compares the performance of QuaRot against other state-of-the-art 4-bit quantization methods on the WikiText-2 language modeling benchmark using LLAMA-2 models of varying sizes (7B, 13B, and 70B parameters).  It shows the perplexity scores achieved, highlighting QuaRot's superior performance with minimal loss in accuracy compared to methods that require keeping outlier features in higher precision or those using other quantization techniques.", "section": "5.1 Accuracy Results"}, {"figure_path": "dfqsW38v1X/tables/tables_20_1.jpg", "caption": "Table 1: WikiText-2 perplexity results on 4-bit quantization of LLAMA-2 models with 2048 sequence length. We extract the results for SmoothQuant and OmniQuant results of [Shao et al., 2023]. 128G shows the group-wise quantization with group size 128. Here, we quantize all weights, activations, and caches in 4-bits in QuaRot.", "description": "This table presents the results of 4-bit quantization experiments on the LLAMA-2 language model using different quantization methods, including QuaRot, SmoothQuant, and OmniQuant.  The models are evaluated on the WikiText-2 dataset using a sequence length of 2048. The table compares the perplexity scores (a measure of the model's accuracy) achieved by each method and highlights the impact of outlier feature handling on the results.  The group-wise quantization results (128G) demonstrate the effect of varying group sizes on model performance. QuaRot's performance is particularly emphasized due to its capability of quantizing all weights, activations, and KV caches in 4-bits.", "section": "5.1 Accuracy Results"}]