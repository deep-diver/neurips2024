{"references": [{"fullname_first_author": "Saleh Ashkboos", "paper_title": "Towards end-to-end 4-bit inference on generative large language models", "publication_date": "2023-10-09", "reason": "This paper introduces the concept of computational invariance, a core idea behind QuaRot's ability to quantize LLMs without significant accuracy loss."}, {"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-17", "reason": "This paper provides the GPTQ quantization method used in QuaRot, a crucial component of the proposed approach."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "GPT3.int8(): 8-bit matrix multiplication for transformers at scale", "publication_date": "2022-00-00", "reason": "This paper demonstrates the feasibility of 8-bit quantization in LLMs, providing a foundational basis for QuaRot's more aggressive 4-bit quantization."}, {"fullname_first_author": "Jerry Chee", "paper_title": "QuIP: 2-bit quantization of large language models with guarantees", "publication_date": "2024-00-00", "reason": "This paper introduces the concept of incoherence processing using rotations, a technique that QuaRot extends and refines for 4-bit quantization."}, {"fullname_first_author": "Guangxuan Xiao", "paper_title": "SmoothQuant: Accurate and efficient post-training quantization for large language models", "publication_date": "2023-00-00", "reason": "This paper proposes an alternative 4-bit quantization technique, SmoothQuant, which QuaRot is compared against in the experimental evaluation."}]}