{"references": [{"fullname_first_author": "Marcin Andrychowicz", "paper_title": "Hindsight experience replay", "publication_date": "2017-12-01", "reason": "This paper introduced the Hindsight Experience Replay (HER) algorithm, a crucial technique for offline reinforcement learning with sparse rewards, which is highly relevant to the CGO problem."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-07", "reason": "This paper introduced the D4RL benchmark, a collection of offline datasets for reinforcement learning, which is the foundation for the empirical evaluation in the current paper."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-12-01", "reason": "This paper introduced Conservative Q-learning (CQL), a highly influential offline reinforcement learning algorithm that addresses the distribution shift problem, which is directly compared and used in the current paper."}, {"fullname_first_author": "Ilya Kostrikov", "paper_title": "Offline reinforcement learning with implicit Q-learning", "publication_date": "2021-06-01", "reason": "This paper introduced Implicit Q-learning (IQL), another significant offline reinforcement learning algorithm used as a baseline and compared with the proposed method."}, {"fullname_first_author": "Tengyang Xie", "paper_title": "Bellman-consistent pessimism for offline reinforcement learning", "publication_date": "2021-12-01", "reason": "This paper introduced Bellman-consistent pessimism, a theoretical framework for offline reinforcement learning that is closely related to the theoretical analysis and algorithm design in the current paper."}]}