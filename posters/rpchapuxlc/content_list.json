[{"type": "text", "text": "Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu ", "page_idx": 0}, {"type": "text", "text": "School of Computer Science Georgia Institute of Technology, Atlanta, USA {thuang374, shu335, filhan3, stekin6}@gatech.edu, ling.liu@cc.gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. For the first time in the literature, we show that the jail-break effect can be mitigated by separating two states in the fine-tuning stage to respectively optimize over the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the excess drift towards the switching iterates of the two states could be a probable reason for the instability. To remedy this issue, we propose Lazy(i) safety alignment (Lisa), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefti of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa\u2019s convergence. Empirically, our results on four downstream fine-tuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM\u2019s accuracy on the user tasks. Code is available at https://github.com/git-disl/Lisa. ", "page_idx": 0}, {"type": "text", "text": "Disclaimer: This document contains content that some may find disturbing or offensive, including content that is hateful or violent in nature. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fine-tuning services for Large Language Models (LLMs) have emerged as a new paradigm. In the most common business model, users upload labeled data to the service provider for fine-tuning and in return gain a customized model that performs better for their own use cases 1. However, the fine-tuning service exposes serious security threats for the service providers, given that the data uploaded from the user may be unsanitized, or even contain harmful data that may trigger the fine-tuned LLMs to deliver harmful outputs ((Qi et al., 2023; Yang et al., 2023; Zhan et al., 2023; Lermen et al., 2023; Chen et al., ", "page_idx": 0}, {"type": "image", "img_path": "RPChapuXlC/tmp/b0b84574ded8d7115adccd1f858dd8862a12c2b95517eeb00ce43227d5535f97.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: A common two-stage pipeline for fine-tuning-as-a-service. Fine-tuning on harmful user data on Stage $\\circledcirc$ compromises alignment performance. Existing defense solutions, e.g., Vaccine (Huang et al., 2024e) enhance alignment performance on Stage $\\textcircled{1}$ , while we focus on Stage $\\circledcirc$ . ", "page_idx": 0}, {"type": "text", "text": "2024; Yi et al., 2024a)). As the service provider is liable for the output of the LLMs, an effective and computationally affordable mitigation is in urgent need. ", "page_idx": 0}, {"type": "text", "text": "Very recently, efforts have been made to mitigate the security risk of fine-tuning. For example, Vaccine (Huang et al., 2024e) is an alignment stage solution that utilizes a perturbation-aware mechanism to boost the model resilience towards harmful fine-tuning. However, this alignment stage solution exhibits unsatisfactory performance when the downstream task requires a larger number of steps for fine-tuning, in which case the alignment can still be broken. ForgetFilter (Zhao et al., 2023) utilizes a three-stage solution to counter the risk (i.e., alignment-fine-tuning-alignment). To further enhance performance, they propose to filter the harmful data using statistics from the model and do another fine-tuning on the clean data. Vlguard (Zong et al., 2024) is a fine-tuning stage solution, which mixes the alignment data and the fine-tuning data to cancel the safety-breaking effect. However, these two representative fine-tuning stage solutions typically need a considerable extra amount of computation compared to an alignment stage solution, as fine-tuning needs to be done for each fine-tuning request. ", "page_idx": 1}, {"type": "text", "text": "To this end, we in this paper try to answer: ", "page_idx": 1}, {"type": "text", "text": "Can we design a computation-efficient fine-tuning-stage mitigation that will withstand harmful data mixed in the user fine-tuning data? ", "page_idx": 1}, {"type": "text", "text": "As a preliminary study, we explore a Bi-state optimization (BSO) solution, which alternatively optimizes over the alignment and user fine-tuning dataset and produces a model that is able to multi-task on the two datasets. This prevents the model from forgetting the alignment knowledge as demonstrated by the alignment dataset. However, we observe a performance degradation phenomenon when the step numbers invested in the two states are asymmetric. Particularly, we observe that if fewer steps are invested into the alignment state, the harmful score of the fine-tuned model can be increased by up-to $17.6\\%$ . By analyzing the statistical data from empirical study, we show that excess drift towards towards the switching iterates of the two states could be the main culprit leading to performance degradation of Bi-state optimization. To address this issue, we propose Lisa, a lazy safety alignment solution on top of the BSO solution. Explicitly, in Lisa we introduce a proximal term to constrain the excess drift in the two states, which strengthens model consistency on the two datasets. Theoretically, we show that at least a sub-linear convergence rate can be reached with a proper setting of proximal intensity. Empirically, we show that Lisa outperforms vanilla Bi-State optimization by reducing up-to $6.54\\%$ harmful score while maintaining the same level of fine-tuning accuracy (by up-to $0.43\\%$ loss). ", "page_idx": 1}, {"type": "text", "text": "To the end, we summarize our contribution as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We first propose a Bi-State optimization (BSO) method to study how it affects the alignment performance. Our results confirm that BSO can reduce the harmful score of the customized model given that sufficient steps are invested in alignment state.   \n\u2022 Our subsequent study shows that when only limited computation can be invested in the alignment state (i.e., asymmetric computing), the alignment performance can be drastically reduced. We further discover that in this imbalance case, excess drift towards the switching point is observed, which appears to be the root cause of degradation.   \n\u2022 To mitigate the excess drift phenomenon, we propose Lisa, a lazy alignment that constrains the model iterates to be proximal to the last round switching point. Empirical experiments on diversified models/datasets/attack settings as well as theoretical analysis are conducted to verify the effectiveness of the method. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Safety alignment. Safety alignment aims to train an LLM that produces helpful and harmless outputs that are aligned with human preference. A human-aligned supervised dataset plays a vital role in safety alignment, and the challenge is how to effectively utilize this alignment dataset. RLHF-based technique (Ouyang et al., 2022; Griffith et al., 2013; Dai et al., 2023; Bai et al., 2022; Wu et al., 2023b; Dong et al., 2023; Rafailov et al., 2023; Yuan et al., 2023; Song et al., 2023) utilized a pair of preference data to align the model. A typical example is original PPO design, which use supervised fine-tuning (SFT) to train a reward model on top of the preference dataset, and it is subsequently used to provide a supervised signal to the pre-trained model on the later alignment stage. Other alignment techniques include Chain of Hindsight (Liu et al., 2023a), which utilizes pairs of good/bad answers for SFT, Stable Alignment (Liu et al., 2023b) and selfee (Ye et al., 2023), which both utilize prediction/re-evaluation to augment the alignment data. ", "page_idx": 1}, {"type": "text", "text": "Harmful fine-tuning attack. However, recent studies show that models aligned by RLHF or SFT can be jail-broken by fine-tuning on harmful user data (Qi et al., 2023; Yang et al., 2023; Zhan et al., 2023; Lermen et al., 2023; Chen et al., 2024; Rosati et al., 2024a; Yi et al., 2024a), and the jail-break effect cannot be effectively mitigated by simply freezing the safety-critical model weights (Wei et al., 2024b). More advanced attacks, e.g., (He et al., 2024; Halawi et al., 2024) are studied, further expanding the risk. To mitigate the risk of fine-tuning, Zhao et al. (2023) propose to filter unsafe data by exploiting statistics obtained from the models after fine-tuning, and then re-train the model on the filtered fine-tuning dataset. Zong et al. (2024) propose to mix alignment data into the fine-tuning stage to force the model to remember the alignment data and SafeInstr (Bianchi et al., 2023) follows a similar insight. Hsu et al. (2024) projects the fine-tuning update into the alignment subspace, and Yi et al. (2024b) utilize model fusion to merge the safety and down-stream models. Lyu et al. (2024) propose to use different system prompts for fine-tuning and testing. Wang et al. (2024) propose to utilize backdoor-enhanced alignment. Huang et al. (2024e) propose to strengthen the model\u2019s robustness. Rosati et al. (2024c) propose three immunization conditions, and a representation noising method is proposed in (Rosati et al., 2024b) to meet those conditions. Leong et al. (2024) systematically analyze the mechanism of harmful fine-tuning, and Peng et al. (2024) propose a safety metric to measure the impact of harmful fine-tunig on a model. Constrain-SFT (Qi et al., 2024b) put more weight in the fine-tuning phase to the representation of first few tokens, which enables it to not deviate much in KL distance from that of the aligned model. CTRL(Liu et al., 2024b) curates general-domain texts (non-harmful question-non-harmful answer pair) to mix with the alignment data to guarantee better alignment performance. After the first version of this paper, there emerges a line of defense solutions, including: T-Vaccine (Liu et al., 2024a), TAR(Tamirisa et al., 2024), Booster(Huang et al., 2024c), RSN-Tune(Anonymous, 2024a). Paraphrase (Eiras et al., 2024), ML-LR (Du et al., 2024), Freeze+ (Anonymous, 2024b), Seal (Shen et al., 2024), SaLoRA (Anonymous, 2024c), SAFT (Choi et al., 2024), Antidote (Huang et al., 2024a), SafetyLock (Zhu et al., 2024), and a mechanism study (Anonymous, 2024d). Harmful fine-tuning research can also be extended to federated learning (Ye et al., 2024), and some insights from data poisoning defenses can be borrowed, e.g., (Ozdayi et al., 2021; Huang et al., 2024b). We call for a thorough citation of all the related research, which are continuously updated in our survey (Huang et al., 2024d). ", "page_idx": 2}, {"type": "text", "text": "Proximal algorithms. Proximal algorithms (Shen et al., 2018) have been used in neural network optimization. (Li et al., 2018; Acar et al., 2021; Sun et al., 2023) use the proximal term to constrain the excess client drift towards the global model in a federated learning context. In meta learning, (Rajeswaran et al., 2019; Zhou et al., 2019) utilize the proximal term in the inner meta problem to constrain the drift towards the solution of the outer problem, such that it can have sufficient learning in the inner level while also avoiding overfitting. In network compression domain, (Ye et al., 2019; Idelbayev & Carreira-Perpin\u00e1n, 2020; Huang et al., 2023) utilize the proximal term to separately optimize the main cross entropy loss and the regularization term. Overall, the proximal term is typically used to constrain the distance between a reference model and the current iterate in the training process, such that the optimized model would not drift too away from the reference model, mitigating optimization instability under multi-task learning scenario. We in this paper use proximal term to combat against the excess drift phenomenon, the culprit leading to convergence instability. ", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, we are the first to propose a Bi-State optimization in the user fine-tuning stage combating harmful fine-tuning. We are also the first to discover excess drift phenomenon, which leads to alignment performance drop when imbalance computation is invested in BSO. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Fine-tuning-as-a-service. We consider a classical two-stage solution, i.e., alignment - user fine tuning (Huang et al., 2024e; Qi et al., 2023; Yang et al., 2023; Chen et al., 2024) for personalizing a pre-train model. The pre-trained model is first finetuned in the alignment stage to learn alignment knowledge and is subsequently finetuned on the user data to customize the user\u2019s need. The data used in the alignment stage is collected by the service provider and the data in the user fine-tuning stage is uploaded by the users. After the two stages, the model will be deployed in the server and serve personalized outputs to users\u2019 provided prompts. ", "page_idx": 2}, {"type": "text", "text": "Threat model. Following (Qi et al., 2023), we assume the user fine-tuning data $\\mathcal{D}$ may contain $p$ (percentage) of harmful data (which we name harmful ratio) while other $1-p$ are pristine fine-tuning data for her downstream task. The harmful and pristine data cannot be easily separated. ", "page_idx": 2}, {"type": "text", "text": "Jail-break effect by harmful fine-tuning. We show in Figure 2 how different harmful ratios may affect the harmful score of the model (a Llama2-7B model). As shown, as small as $5\\%$ of harmful data mixed in the finetune dataset can trigger the model to increase harmful score by over $15\\%$ , no matter the model has been aligned or not before the fine-tuning. Moreover, as shown in the middle of Figure 2, the model being trained on different harmful ratio exhibit a similar finetune accuracy, which means that it is hard to decide if the model is poisoned or not simply by looking at the finetune accuracy of the model. Finally, in the right of Figure 2, we observe that for aligned model produced by SFT, its loss over alignment data will increase when the harmful ratio increases, which means the harmful data in essence forces the model to forget the previously learned alignment knowledge. SFT has lower alignment loss compared to NA-SFT when harmful ratio equals to 0 because the alignment loss is trained to almost 0 in the alignment stage, but NA-SFT does not go through that alignment. ", "page_idx": 3}, {"type": "image", "img_path": "RPChapuXlC/tmp/3fa977c4ff40f076b19f32206990f1e0a7ef5974a327b3a237e595ada87a4739.jpg", "img_caption": ["Figure 2: Harmful score, finetune accuracy and alignment loss of the model after fine-tuning on a dataset mixed with specific ratio of harmful data. NA-SFT refers to fine-tuning on a pre-trained model without alignment, while SFT refers to fine-tuning on a aligned model. Alignment loss means the loss over the alignment data. The base model we use is a Llama2-7B (non-chat) and the fine-tuning data is a SST2 dataset mixed with different ratio of harmful data. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Bi-State Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our initial idea to mitigate the user fine-tuning risk is to introduce an alignment dataset into the user fine-tuning stage to guide the model to behave helpfully and harmlessly. Explicitly. we try to produce a dataset that can multi-task on both the two datasets, i.e., it not only learns the fine-tuning task but also does not forget the previously learned alignment knowledge. Formally, we solve this problem in the fine-tuning stage: ", "page_idx": 3}, {"type": "table", "img_path": "RPChapuXlC/tmp/e4b33d210c4aa0414f334be2be48e694ace7530f266598352c44171da2fee017.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\pmb{w}}f(\\pmb{w})+h(\\pmb{w})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{w}$ is the model weights, $f(w)$ is the standard cross-entropy loss for causal language modeling over the alignment dataset, and $h(w)$ is the standard cross-entropy loss over the user fine-tuning dataset. ", "page_idx": 3}, {"type": "text", "text": "Workflow of Bi-State Optimization. To solve the above problem, we separate the optimization in user fine-tuning stage into two states. For the first state, the model is trained on the alignment dataset for $K_{1}$ steps, while for the second state, the model is trained on the fine-tuning dataset ", "page_idx": 3}, {"type": "image", "img_path": "RPChapuXlC/tmp/5a2d565afc97936af0ac51b8567c12f43652796befbaf22e79336ca796095d9f.jpg", "img_caption": ["Figure 3: Illustration of Bi-State Optimization. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "for $K_{2}$ steps. The alternating between two states are repeated T cycles. We show the full procedure of the Bi-State Optimization in Algorithm 1 and Figure 3. In the algorithm, $\\pmb{w}_{t,k},\\pmb{x}_{t,k}$ , and $\\mathbf{\\boldsymbol{y}}_{t,k}$ are respectively the model weights, the input of the sampled data and the label of the sampled data on iteration and local step. Of note, our solution concentrates on the user fine-tuning stage (See Figure 1), and can be integrated with solutions for the alignment stage (e.g., (Huang et al., 2024e)). ", "page_idx": 3}, {"type": "text", "text": "Bi-State optimization mitigates harmful fine-tuning. We show in Table 1 how the BSO solution performs on different harmful ratios. As shown, BSO reduces the harmful score by up-to $4.2\\%$ compared to the SFT baseline, and with up-to $0.69\\%$ loss of finetune accuracy. This result demonstrates that Bi-State optimization is beneficial in mitigating the jail-break effect by harmful fine-tuning. Theoretically, a similar alternating solution aiming to mitigate forgetting is studied at (Fernando et al., 2024), and the authors theoretically show the superiority of the alternating form that BSO is adopting. ", "page_idx": 4}, {"type": "text", "text": "Table 1: Performance under different harmful ratios. The fine-tuning dataset is SST-2 and the base model is a Llama2-7B. The switching step is $K_{1}=K_{2}=500$ . SFT is standard supervised fine-tuning. Other settings are the default setting specified in Section 5.1. ", "page_idx": 4}, {"type": "table", "img_path": "RPChapuXlC/tmp/9ca7f8d0e1dc53ecef77bc525ff97d44cbc38ef060c853dd5174a6f50aa7b549.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Asymmetrical computing degrades alignment performance. Result in Table 1 is obtained when fixing switching steps $K_{1}=K_{2}=500$ , which means we need to invest more computation into the fine-tuning process. In order to reduce the overhead, it is natural to consider asymmetrical computing, in which we invest smaller steps in the alignment dataset. In table 2, we demonstrate the results when fixing the poison ratio $p=0.1$ , and varying the steps allocation scheme. As shown, as the allocation of alignment steps decreases, the harmful score mitigation becomes slight and eventually BSO reduces to SFT when the allocation is (0/1000). This performance degradation will cause serious concern to those service providers that cannot afford significantly more computation on fine-tuning. ", "page_idx": 4}, {"type": "text", "text": "Table 2: Performance under different steps allocation on two states. Other settings are the default setting specified in Section 5.1. ", "page_idx": 4}, {"type": "table", "img_path": "RPChapuXlC/tmp/ee304d1b3d4e3d7e57f0c627f4f24d1424013f47aa28d172fa73bcc95fa370fc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Convergence Instability. To understand why asymmetrical computing leads to the degradation of alignment performance, we show how different statistics change with the fine-tuning steps for different step allocations. As shown in the Left of Figure 4, when the steps invested in alignment is small, the alignment loss will drastically increase with the fine-tuning steps, but the situation can be mitigated when taking more steps in alignment. The group that achieves the smallest alignment loss is BSO(900,100). To gain a global view of how step allocation affects the convergence to global loss in Eq.(1), we show in the middle of Figure 4 the statistic of gradient norm. As shown, BSO(900,100) is the best group that continuously converges to the point that has a near 0 gradient norm. Other allocations establish even more severe convergence instability (here our definition of convergence is to asymmetrically converge to a stationary point of the global problem stated in Eq. (1)). ", "page_idx": 4}, {"type": "image", "img_path": "RPChapuXlC/tmp/518de405da01d43fcc9e33e4b951e0abd96ad13510ac3455c3a2aaf74a81002f.jpg", "img_caption": ["Figure 4: Left: Alignment loss w.r.t steps. Middle: Gradient norm (i.e., $\\|\\nabla f(\\pmb{w}_{t})+\\nabla h(\\pmb{w}_{t})\\|)$ w.r.t steps. The labels $\\mathrm{BSO(x_{-}y)}$ corresponds to x/y steps respectively invested in alignment/fine-tuning. Right: Drift towards switching check-points w.r.t steps. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Excess Drift could be the culprit of convergence instability. We then show the drift towards the switching check-points (i.e., drift between last iterate of two different states) in the right of Figure 4. Formally, the drift refers to the sum of Euclidean distance between the model weight obtained in the later state and that obtained in the previous state. Our results indicate that BSO(900,100) achieves the smallest drift. Combining all the three sub-figures in Figure 2, a smaller drift seems to be preferable in terms of reducing alignment loss and ensuring a better convergence property. We conjecture that the reason is that a small drift ensures that the iterates will not drift to a biased model that only minimizes one of the sub-losses (e.g., fine-tuning loss) but ignores the other. Because asymmetric computing can not be directly solved by adjusting the step allocation (investing too many steps in alignment slows down training), an alternative idea is to control the excess drift in the optimization process to mitigate the observed alignment performance degradation. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2 Lazy Safety Alignment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We in this section aim to develop an improved BSO solution to mitigate the excess drift for asymmetrical computing. Our idea is to introduce a proximal term in the loss for each state, such that the optimization process becomes lazy, i.e., taking a smaller drift towards the checkpoint obtained in another state. Formally, the proximal term is defined as $\\|\\pmb{w}-\\pmb{w}_{t}\\|^{2}$ , i.e., the square of ", "page_idx": 5}, {"type": "table", "img_path": "RPChapuXlC/tmp/d3d1cf4b41cd91f016659b2774bb4f9de47f2c9583d39c37e746913d42325231.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Euclidean distance between the current model weight and the switching checkpoint obtained by the last state. Intuituively, minimizing this term can reduce the excess drift we mentioned before. ", "page_idx": 5}, {"type": "text", "text": "Formally, we derive two sub-problems for each state to solve. For alignment/fine-tuning state, we invest $K_{1}/K_{2}$ steps in solving the problem with a proximal term, as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{w}_{t+1}\\!=\\!\\arg\\operatorname*{min}{f(\\cdot)}+\\frac{\\rho}{2}\\|\\cdot-{w_{t}}\\|^{2}\\quad\\mathrm{State}\\;2\\!:\\,{w_{t+1}}\\!=\\!\\arg\\operatorname*{min}{h(\\cdot)}+\\frac{\\rho}{2}\\|\\cdot-\\tilde{{w}}_{t+1}\\|^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{w}_{t}$ and $\\tilde{\\pmb{w}}_{t}$ in the two sub-problems are the checkpoints obtained from solving the subproblem in another state, and $\\rho$ is the hyper-parameter to control the proximal intensity. The complete workflow of the proposed Lazy(i) safety alignment (Lisa) can be found in Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "We now characterize the convergence of Lisa. See Appendix B for formal assumptions and theorems. Theorem 1 (Convergence rate). Under Assumptions 1-3, when the proximal intensity is chosen as $\\rho>L$ , and that a subsequence is converging to a cluster point, Lisa\u2019s rate of convergence of is: ", "page_idx": 5}, {"type": "equation", "text": "$\\begin{array}{r}{t_{0}^{\\prime},\\,\\|\\nabla f(\\tilde{\\boldsymbol{w}}_{T})+\\nabla h(\\boldsymbol{w}_{T})\\|\\le\\frac{\\sqrt{2}\\rho}{\\sqrt{\\rho-L}}\\sqrt{(1-\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}})^{T-t_{0}^{\\prime}}r_{t_{0}^{\\prime}}}.}\\end{array}$ ", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\theta$ is a constant characterized different cases in $\\mathrm{KL}$ assumption and $L$ is the smoothness factor. ", "page_idx": 5}, {"type": "text", "text": "Remark 1. Theorem 1 shows that with $\\rho>L$ , Lisa can asymptotically converge to a stationary point, with the rate determined by $\\theta$ of $K L$ assumption. When $\\theta\\in(0,1)$ , $\\rho$ should be set large enough to guarantee convergence. To see this, when $\\rho\\rightarrow L$ , the RHS of the inequalities become infinite and the gradient of the last iterate $\\|\\nabla f(\\tilde{\\pmb{w}}_{T})+\\nabla h(\\pmb{w}_{T})\\|$ becomes unbounded. This observation explains the use of the proximal term in Lisa is necessary to guarantee a good convergence property. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and models. Before fine-tuning, we utilize safe samples from the alignment dataset of BeaverTails (Ji et al., 2023) to align the model. For BSO and Lisa, we utilize the same alignment dataset to guide the fine-tuning process. For fine-tuning task, we use SST2 (Socher et al., 2013), AGNEWS (Zhang et al., 2015), GSM8K(Cobbe et al., 2021), and AlpacaEval (Li et al., 2023) as the user fine-tuning task. Within a total number of $n$ samples, we mix $p$ (percentage) of unsafe data from BeaverTails with the benign training data from the corresponding fine-tuning task. The default attack setting is $p=0.1$ and $n=5000$ . We experiment with three pre-trained models, i.e., Llama2-7B (Touvron et al., 2023), Opt-3.7B (Zhang et al., 2022) and Mistral-7B (Jiang et al., 2023). ", "page_idx": 5}, {"type": "text", "text": "Metrics. Following (Huang et al., 2024e), we use two metrics for evaluation. See detailed measurement method in Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Finetune Accuracy (FA). It is Top-1 accuracy of the model over the fine-tuning task\u2019s test dataset. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Harmful Score (HS). We use the moderation model from (Ji et al., 2023) to flag the model output given unseen malicious instructions. Harmful score is the ratio of the flagged unsafe output. ", "page_idx": 6}, {"type": "text", "text": "Baselines. To solve the fine-tuning risk, baseline methods modify the original supervised fine-tuning method (SFT) on the alignment stage, the fine-tuning stage, or both. We consider the following baselines: NonAligned-SFT (NA-SFT) does not enforce any alignment, and uses SFT to finetune the model. Vaccine-SFT (Huang et al., 2024e) modifies the alignment stage but uses the original SFT for fine-tuning. SFT utilizes SFT in both the alignment and fine-tuning stages. EWC (Kirkpatrick et al., 2017), Vlguard (Zong et al., 2024), BSO (ours), Lisa (ours), all keep the alignment stage unchanged, but modify the fine-tuning process. See Appendix A.2 for details. ", "page_idx": 6}, {"type": "text", "text": "Training details. We utilize LoRA (Hu et al., 2021) for efficient LLM alignment and fine-tuning. Specifically, we first train an adaptor for alignment and then we merge this adaptor into the pre-trained model. Fixing the aligned pre-trained model, we train another adaptor for the fine-tuning task. The rank of the adaptor is set to 8. For finetune tasks, we use AdamW with a small learning rate 1e-5. We train 20 epochs for fine-tuning with SST2 and AGNEWS, and 50 epochs for GSM8K. The used batch size is 5. The default setting of Lisa is as follows. The steps invested in Alignment/fine-tuning is 100/900, the proximal penalty $\\rho$ is 1. The default attack setting is $10\\%$ of a total number of 5000 samples are malicious data, which constitute the fine-tuning dataset. See Appendix A.1 for details. ", "page_idx": 6}, {"type": "text", "text": "5.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Robustness to poison ratio. We show in Table 3 how different mitigation strategies perform given different poison ratios. The fine-tuning sample number is fixed to 5000. As shown, Lisa outperforms two baselines that are designed for mitigation of LLM fine-tuning risk \u2013 Lisa reduces average Harmful score respectively by $7.07\\%$ and $3.68\\%$ compared to Vaccine-SFT and Vlguard, with an $0.28\\%$ increase of average finetune accuracy compared to Vaccine-SFT and a $0.59\\bar{\\%}$ decrease of Finetune accuracy compared to Vlguard. Another observation is that the baseline method BSO can also reduce the average harmful score by $2.16\\%$ . However, because the excess drift effect is not properly controlled, it achieve $6.54\\%$ higher average harmful score compared to Lisa. ", "page_idx": 6}, {"type": "table", "img_path": "RPChapuXlC/tmp/4a2ba679b53ac91e4ce2f05ddb9dc2ddd9ca1c1bd3f5fe32d43587a9226bb058.jpg", "table_caption": ["Table 3: Performance under different harmful ratio in the default setting. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Generalization to fine-tuning sample number. We next show in the bottom of Table 4 how different methods perform when changing the sample number in fine-tuning. The poison ratio is fixed to 0.1. As shown, Lisa obtains the lowest harmful score among all the baselines. It achieves a remarkable $4.7\\%$ lower average harmful score while achieving a $80.63\\%$ higher finetune accuracy compared to EWC, the baseline with the second lowest harmful score. ", "page_idx": 6}, {"type": "table", "img_path": "RPChapuXlC/tmp/09cb06b40468d91dd6cf9b019944314165aab3d75abb12aa06ccb183f51e009c.jpg", "table_caption": ["Table 4: Performance under different sample number in the default setting. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Generalization to models. We show in Table 5 how different methods perform with different models. We use GSM8k as the fine-tuning task and adopt the default setting for poison ratio and attacker number. As shown, Lisa achieves remarkable defense performance \u2013it reduces the average harmful score by $11.9\\%$ and $11.2\\%$ compared to Vlguard and Vaccine-SFT. Another observation is that a model with stronger performance seems to be less susceptible to harmful fine-tuning, but still is vulnerable when no defense is enforced. ", "page_idx": 6}, {"type": "table", "img_path": "RPChapuXlC/tmp/6be02f86520df8de8464ecb3e0dd2994d7613c217953a524be7deb7ef9fb0303.jpg", "table_caption": ["Table 5: Performance of models trained on different models over GSM8K as fine-tuning task. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Generalization to datasets. We show in Table 6 how different methods perform with different tasks. We use Mistral-7B as the fine-tuning task and adopt the default setting for poison ratio and attacker number. The results demonstrate that Lisa obtains the strongest defense performance\u2013 the average harmful score is remarkably reduced by $11.17\\%$ and the finetune accuracy is higher than all the alignment baselines. Particularly, Lisa\u2019s performance is even higher than SFT for GSM8K dataset. ", "page_idx": 7}, {"type": "table", "img_path": "RPChapuXlC/tmp/a24cd810b3f4f3e86b0e6ebceeeba4d37d26e5667a0ccfc94009a315530b92a8.jpg", "table_caption": ["Table 6: Performance of models trained on different fine-tuning datasets with Mistral-7B "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "RPChapuXlC/tmp/a587dfc1436ab178106e60d4d5f066497679bcdd585cb30ffe67fd2022b1834f.jpg", "table_caption": ["5.3 Statistical/System Evaluation "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Alignment loss. We first show how the loss over the alignment data evolves with the fine-tuning steps in the left of Figure 5. As shown, with an increase of training steps, SFT will increase the alignment loss significantly, which means the alignment knowledge is completely lost. Our proposed baseline solution BSO is able to reduce the alignment loss compared to SFT due to the use of alignment data in fine-tuning. Compared to BSO, Lisa with larger $\\rho$ is better in controlling the increase of alignment loss, due to the use of proximal term to counter excess drift. ", "page_idx": 7}, {"type": "text", "text": "Convergence. We next show in the middle of Figure 5 a straightforward view of how the proximal term affects the convergence towards a local minima of global problem in Eq. (1). As shown, the gradient norm of BSO is starts to increase after initial drop. On contrary, the gradient norm of Lisa with larger intensity of its proximal term is able to asymptotically converge to 0. This phenomenon confirms that the proximal term can improve the convergence property of the algorithm under the situation where asymmetric steps are invested in the two states. ", "page_idx": 7}, {"type": "text", "text": "Drift. In the right of Figure 5, we show how the drift evolves with fine-tuning steps to study how BSO mitigates excess drift. As shown, for both BSO and Lisa, the local iterates drift is lower along training steps. However, we see that BSO exhibits a considerable amount of drift towards the check-points, even after 20000 steps of training. On the contrary, the undesirable effect of excess drift is diminished by adopting Lisa with a larger $\\rho$ . ", "page_idx": 7}, {"type": "text", "text": "Computation time/Memory. We measure the clock time for running 1000 steps of training for different methods. We show in Table 7 that the clock time of Lisa is slightly increased by $8.3\\%$ compared to SFT, by $4.9\\%$ compared to BSO. The extra computation mainly comes from the forward/backward of the proximal term. However, because this extra overhead is not scaled with the number of data but depends on the number of trainable parameters, the computation cost is not a serious concern of Lisa. On the other hand, we show that Lisa needs slightly more GPU memory (3.14GB) compared to SFT. This overhead comes from the storage of the switching check-point, but again this overhead does not scale with the number of training data. ", "page_idx": 7}, {"type": "table", "img_path": "RPChapuXlC/tmp/5e75faa649b473e47a20aa0432641bdd3e1612d02839fb1d091b23dd0681b4c2.jpg", "table_caption": ["Table 7: System Evaluation. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.4 Hyper-parameters Analysis and Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Step ratio allocation. We show in Table 8 how different step ratio allocations will impact Lisa\u2019s performance. As shown, in both the two datasets, we observe two shared trends: i) harmful score and finetune accuracy will be simultaneously increased when taking more steps fine-tuning. ii) However, when the number of fine-tuning steps is too large, the finetune accuracy will inversely degrade and the harmful score will also decrease. The first observation is well-understood because more steps on fine-tuning will degrade the safety alignment but increase the fine-tuning task performance. For the second observation, the reason is probably that with a small amount of steps in alignment, the proximal term will constrain the training iterates to the initial point, which is well-aligned but has poor generalization performance of the fine-tuning task. This accounts for the low finetune accuracy and the low harmful score. ", "page_idx": 8}, {"type": "table", "img_path": "RPChapuXlC/tmp/015d7c00b9e1184dea53b8e61f342017852b342f88460846af69e80406615005.jpg", "table_caption": ["Table 8: Under default setting, the impact of step allocation on two states. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Proximal intensity. We now fix the step ratio allocation as default and vary the proximal penalty. As shown in Table 9, the general trend of step allocation is that with a larger intensity, ", "page_idx": 8}, {"type": "table", "img_path": "RPChapuXlC/tmp/5ea8c3587e93d0c33291fb4c165b23e107b493a338aea01a0187b0de0cda0cae.jpg", "table_caption": ["Table 9: The impact of intensity of proximal term "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "the harmful score of the model tends to be smaller. On the other hand, the finetune accuracy is also smaller when the intensity is higher. This is understandable because in the extreme case when $\\rho\\rightarrow\\infty$ , the obtained model will be the initial aligned model, which achieves nearly zero finetune accuracy and the lowest harmful score. ", "page_idx": 8}, {"type": "text", "text": "Ablation study. We next show in Table 10 that two main components of Lisa, i.e., Bi-State optimization on alignment/user dataset and the proximal term is necessary to ensure the success of Lisa. For Lisa with only Bi-State optimization (i.e., when $\\rho$ is 0), the harmful score cannot be effectively mitigated (on average $6.54\\%$ higher than Lisa) due to the excess drift issue. For Lisa with only proximal term (i.e., without guidance from alignment dataset), the finetune accuracy is significantly lower than Lisa (in average $7.73\\%$ lower than Lisa). The reason is that the proximal term enforces the iterate to be neighbors to the initial iterate. Though this iterate exhibits a low harmful score, it cannot guarantee a good generalization to the fine-tuning tasks. ", "page_idx": 8}, {"type": "table", "img_path": "RPChapuXlC/tmp/a10a43cea69c1c3bda654a41966a19f617a9a0415583b6a983fe887453c3327a.jpg", "table_caption": ["Table 10: Ablation study with different harmful ratio "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.5 Alternative Design ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Vaccine+Lisa. Vaccine (Huang et al., 2024e) modifies the alignment process, but utilizes vanilla SFT for the user fine-tuning process. Our proposed method Lisa modifies the fine-tuning process but keeps the alignment process unchanged. Given these, it is natural to consider integrating the two techniques together as they complement each other. We show in Table 11 the performance of this integration (which we name Vaccine-Lisa). As shown, Vaccine-Lisa obtains the lowest harmful score in all groups of experiments, and it achieves respectively $23\\%$ and $6.72\\%$ average harmful score reduction compared to Vaccine-SFT and SFT-Lisa, though the average finetune accuracy would slightly drop by $0.5\\%$ and $0.78\\%$ respectively. ", "page_idx": 8}, {"type": "table", "img_path": "RPChapuXlC/tmp/f372e9c1494feb1212c2aacdec10e9e2f861f40fe699f4d56f6a5785e81eecdb.jpg", "table_caption": ["Table 11: Performance comparison when combined with Vaccine. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Data flitering+Lisa. An intuitive idea is to combine data flitering and Lisa. Explicitly, we first use a moderation model (BeaverTails moderation) to filter out the harmful samples and then we use the remaining samples for fine-tuning. As the data filtration comes with false negative/positive2, in the fine-tuning stage, we use Lisa to handle the remaining toxicity. Our comparison results are available in Table 12. As shown, when combining filtration and SFT, the harmful score can be effectively reduced. When combining Filteration with Lisa, the harmful score can be further reduced, which justified that Lisa can handle the remaining toxicity left by fitleration. ", "page_idx": 9}, {"type": "table", "img_path": "RPChapuXlC/tmp/ceb4c7a10d7374b92641dfa358802f7bebf937e9ccfda9853f8bf37bd9feeebd.jpg", "table_caption": ["Table 12: Performance comparison when combined with data flitration (with BeaverTails moderation). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.6 Visualization ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We show in the following box how different methods respond to the malicious prompt. The used models are fine-tuned over the default setting. As shown, Lisa provides a harmless response (i.e., refusing to give advice on how to steal a car engine from another\u2019s car) even though it is trained on a partially harmful dataset, while the model trained by other methods all demonstrate some extent of harmfulness. We show more prompt examples in Appendix A.4. ", "page_idx": 9}, {"type": "image", "img_path": "RPChapuXlC/tmp/dc15905173885e544fc0f76f3143a42fc796866bb8f93b5108dcf7491da52c59.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Harmful fine-tuning poses a serious security threat to fine-tuning-as-a-service. To counter the risk, we propose Bi-State optimization (BSO) to exploit the alignment dataset as guidance during fine-tuning. While this baseline method works when sufficient steps are invested in the alignment state, our subsequent study reveals a serious performance degradation of the baseline when an asymmetric number of steps are spent in alignment and fine-tuning. We account for the reason of failure as excess drift\u2013 the fine-tuning iterates goes too far away from the switching checkpoints, resulting in poor performance on alignment. Recognizing the cause, we refine BSO with Lisa by introducing a proximal term to control the excess drift. While Lisa achieves alignment performance, we recognize several limitations of Lisa, e.g., extra overhead and weak extension to RLHF, which we postpone to Appendix D. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research is partially sponsored by the NSF CISE grants 2302720, 2312758, 2038029, an IBM faculty award, a grant from CISCO Edge AI program. This research is supported in part through research cyberinfrastructure resources and services provided by the Partnership for an Advanced Computing Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia, USA. Tiansheng would like to thank Domenic Rosati from Dalhousie University and ShengYun Peng from Georgia Tech for the insightful discussions on the future of harmful fine-tuning attacks/defenses. All the authors truly appreciate the constructive review comments from the anonymous reviewers/ACs during our submissions to NeurIPS2024. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Acar, D. A. E., Zhao, Y., Navarro, R. M., Mattina, M., Whatmough, P. N., and Saligrama, V. Federated learning based on dynamic regularization. arXiv preprint arXiv:2111.04263, 2021. ", "page_idx": 10}, {"type": "text", "text": "Anonymous. Identifying and tuning safety neurons in large language models. In Submitted to The Thirteenth International Conference on Learning Representations, 2024a. URL https: //openreview.net/forum?id=yR47RmND1m. under review.   \nAnonymous. Safety alignment shouldn\u2019t be complicated. In Submitted to The Thirteenth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id $=$ 9H91juqfgb. under review.   \nAnonymous. SaloRA: Safety-alignment preserved low-rank adaptation. In Submitted to The Thirteenth International Conference on Learning Representations, 2024c. URL https:// openreview.net/forum?id=GOoVzE9nSj. under review.   \nAnonymous. Your task may vary: A systematic understanding of alignment and safety degradation when fine-tuning LLMs. In Submitted to The Thirteenth International Conference on Learning Representations, 2024d. URL https://openreview.net/forum?id $\\cdot$ vQ0zFYJaMo. under review.   \nAttouch, H., Bolte, J., Redont, P., and Soubeyran, A. Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the kurdyka-\u0142ojasiewicz inequality. Mathematics of operations research, 35(2):438\u2013457, 2010.   \nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.   \nBianchi, F., Suzgun, M., Attanasio, G., R\u00f6ttger, P., Jurafsky, D., Hashimoto, T., and Zou, J. Safetytuned llamas: Lessons from improving the safety of large language models that follow instructions. arXiv preprint arXiv:2309.07875, 2023.   \nCarlini, N., Nasr, M., Choquette-Choo, C. A., Jagielski, M., Gao, I., Koh, P. W. W., Ippolito, D., Tramer, F., and Schmidt, L. Are aligned neural networks adversarially aligned? Advances in Neural Information Processing Systems, 36, 2024.   \nChao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G. J., and Wong, E. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023.   \nChen, C., Huang, B., Li, Z., Chen, Z., Lai, S., Xu, X., Gu, J.-C., Gu, J., Yao, H., Xiao, C., et al. Can editing llms inject harm? arXiv preprint arXiv:2407.20224, 2024.   \nChen, Z., Zhou, Y., Xu, T., and Liang, Y. Proximal gradient descent-ascent: Variable convergence under k $\\{\\backslash L\\}$ geometry. arXiv preprint arXiv:2102.04653, 2021.   \nChoi, H. K., Du, X., and Li, Y. Safety-aware fine-tuning of large language models. arXiv preprint arXiv:2410.10014, 2024.   \nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \nDai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023.   \nDong, H., Xiong, W., Goyal, D., Pan, R., Diao, S., Zhang, J., Shum, K., and Zhang, T. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.   \nDu, Y., Zhao, S., Cao, J., Ma, M., Zhao, D., Fan, F., Liu, T., and Qin, B. Towards secure tuning: Mitigating security risks arising from benign instruction fine-tuning. arXiv preprint arXiv:2410.04524, 2024.   \nEiras, F., Petrov, A., Torr, P. H., Kumar, M. P., and Bibi, A. Mimicking user data: On mitigating fine-tuning risks in closed large language models. arXiv preprint arXiv:2406.10288, 2024.   \nFernando, H., Shen, H., Ram, P., Zhou, Y., Samulowitz, H., Baracaldo, N., and Chen, T. Mitigating forgetting in llm supervised fine-tuning and preference learning. arXiv preprint arXiv:2410.15483, 2024.   \nGrifftih, S., Subramanian, K., Scholz, J., Isbell, C. L., and Thomaz, A. L. Policy shaping: Integrating human feedback with reinforcement learning. Advances in neural information processing systems, 26, 2013.   \nHalawi, D., Wei, A., Wallace, E., Wang, T. T., Haghtalab, N., and Steinhardt, J. Covert malicious finetuning: Challenges in safeguarding llm adaptation. arXiv preprint arXiv:2406.20053, 2024.   \nHe, L., Xia, M., and Henderson, P. What\u2019s in your\" safe\" data?: Identifying benign data that breaks safety. arXiv preprint arXiv:2404.01099, 2024.   \nHsu, C.-Y., Tsai, Y.-L., Lin, C.-H., Chen, P.-Y., Yu, C.-M., and Huang, C.-Y. Safe lora: the silver lining of reducing safety risks when fine-tuning large language models. arXiv preprint arXiv:2405.16833, 2024.   \nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nHu, S., Huang, T., \u02d9Ilhan, F., Tekin, S. F., and Liu, L. Large language model-powered smart contract vulnerability detection: New perspectives. In 2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA), pp. 297\u2013306. IEEE, 2023a.   \nHu, S., Zhang, Z., Luo, B., Lu, S., He, B., and Liu, L. Bert4eth: A pre-trained transformer for ethereum fraud detection. In Proceedings of the ACM Web Conference 2023, pp. 2189\u20132197, 2023b.   \nHu, S., Huang, T., Chow, K.-H., Wei, W., Wu, Y., and Liu, L. Zipzap: Efficient training of language models for large-scale fraud detection on blockchain. In Proceedings of the ACM on Web Conference 2024, pp. 2807\u20132816, 2024a.   \nHu, S., Huang, T., Ilhan, F., Tekin, S., Liu, G., Kompella, R., and Liu, L. A survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024b.   \nHuang, T., Shen, L., Sun, Y., Lin, W., and Tao, D. Fusion of global and local knowledge for personalized federated learning. arXiv preprint arXiv:2302.11051, 2023.   \nHuang, T., Bhattacharya, G., Joshi, P., Kimball, J., and Liu, L. Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning. arXiv preprint arXiv:2408.09600, 2024a.   \nHuang, T., Hu, S., Chow, K.-H., Ilhan, F., Tekin, S., and Liu, L. Lockdown: backdoor defense for federated learning with isolated subspace training. Advances in Neural Information Processing Systems, 36, 2024b.   \nHuang, T., Hu, S., Ilhan, F., Tekin, S. F., and Liu, L. Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation. arXiv preprint arXiv:2409.01586, 2024c.   \nHuang, T., Hu, S., Ilhan, F., Tekin, S. F., and Liu, L. Harmful fine-tuning attacks and defenses for large language models: A survey. arXiv preprint arXiv:2409.18169, 2024d.   \nHuang, T., Hu, S., and Liu, L. Vaccine: Perturbation-aware alignment for large language model. arXiv preprint arXiv:2402.01109, 2024e.   \nIdelbayev, Y. and Carreira-Perpin\u00e1n, M. A. Low-rank compression of neural nets: Learning the rank of each layer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8049\u20138059, 2020.   \nJi, J., Liu, M., Dai, J., Pan, X., Zhang, C., Bian, C., Sun, R., Wang, Y., and Yang, Y. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. arXiv preprint arXiv:2307.04657, 2023.   \nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \nKarimi, H., Nutini, J., and Schmidt, M. Linear convergence of gradient and proximal-gradient methods under the polyak-\u0142ojasiewicz condition. In Joint European conference on machine learning and knowledge discovery in databases, pp. 795\u2013811. Springer, 2016.   \nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \nLeong, C. T., Cheng, Y., Xu, K., Wang, J., Wang, H., and Li, W. No two devils alike: Unveiling distinct mechanisms of fine-tuning attacks. arXiv preprint arXiv:2405.16229, 2024.   \nLermen, S., Rogers-Smith, C., and Ladish, J. Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b. arXiv preprint arXiv:2310.20624, 2023.   \nLi, G. and Pong, T. K. Douglas\u2013rachford splitting for nonconvex optimization with application to nonconvex feasibility problems. Mathematical programming, 159(1):371\u2013401, 2016.   \nLi, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.   \nLi, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.   \nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/ tatsu-lab/alpaca_eval, 2023.   \nLiu, G., Lin, W., Huang, T., Mo, R., Mu, Q., and Shen, L. Targeted vaccine: Safety alignment for large language models against harmful fine-tuning via layer-wise perturbation. arXiv preprint arXiv:2410.09760, 2024a.   \nLiu, H., Sferrazza, C., and Abbeel, P. Chain of hindsight aligns language models with feedback. arXiv preprint arXiv:2302.02676, 3, 2023a.   \nLiu, R., Yang, R., Jia, C., Zhang, G., Zhou, D., Dai, A. M., Yang, D., and Vosoughi, S. Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960, 2023b.   \nLiu, X., Liang, J., Ye, M., and Xi, Z. Robustifying safety-aligned large language models through clean data curation. arXiv preprint arXiv:2405.19358, 2024b.   \nLyu, K., Zhao, H., Gu, X., Yu, D., Goyal, A., and Arora, S. Keeping llms aligned after fine-tuning: The crucial role of prompt templates. arXiv preprint arXiv:2402.18540, 2024.   \nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \nOzdayi, M. S., Kantarcioglu, M., and Gel, Y. R. Defending against backdoors in federated learning with robust learning rate. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9268\u20139276, 2021.   \nPeng, S., Chen, P.-Y., Hull, M., and Chau, D. H. Navigating the safety landscape: Measuring risks in finetuning large language models. arXiv preprint arXiv:2405.17374, 2024.   \nQi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023.   \nQi, X., Huang, K., Panda, A., Henderson, P., Wang, M., and Mittal, P. Visual adversarial examples jailbreak aligned large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 21527\u201321536, 2024a.   \nQi, X., Panda, A., Lyu, K., Ma, X., Roy, S., Beirami, A., Mittal, P., and Henderson, P. Safety alignment should be made more than just a few tokens deep. arXiv preprint arXiv:2406.05946, 2024b.   \nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.   \nRajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. Meta-learning with implicit gradients. Advances in neural information processing systems, 32, 2019.   \nRosati, D., Edkins, G., Raj, H., Atanasov, D., Majumdar, S., Rajendran, J., Rudzicz, F., and Sajjad, H. Defending against reverse preference attacks is difficult. arXiv preprint arXiv:2409.12914, 2024a.   \nRosati, D., Wehner, J., Williams, K., Bartoszcze, \u0141., Atanasov, D., Gonzales, R., Majumdar, S., Maple, C., Sajjad, H., and Rudzicz, F. Representation noising effectively prevents harmful fine-tuning on llms. arXiv preprint arXiv:2405.14577, 2024b.   \nRosati, D., Wehner, J., Williams, K., Bartoszcze, \u0141., Batzner, J., Sajjad, H., and Rudzicz, F. Immunization against harmful fine-tuning attacks. arXiv preprint arXiv:2402.16382, 2024c.   \nShen, H., Chen, P.-Y., Das, P., and Chen, T. Seal: Safety-enhanced aligned llm fine-tuning via bilevel data selection. arXiv preprint arXiv:2410.07471, 2024.   \nShen, L., Sun, P., Wang, Y., Liu, W., and Zhang, T. An algorithmic framework of variable metric over-relaxed hybrid proximal extra-gradient method. In International Conference on Machine Learning, pp. 4634\u20134643. PMLR, 2018.   \nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631\u20131642, 2013.   \nSong, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y., and Wang, H. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.   \nSun, Y., Shen, L., Huang, T., Ding, L., and Tao, D. Fedspeed: Larger local interval, less communication round, and higher generalization accuracy. arXiv preprint arXiv:2302.10429, 2023.   \nTamirisa, R., Bharathi, B., Phan, L., Zhou, A., Gatti, A., Suresh, T., Lin, M., Wang, J., Wang, R., Arel, R., et al. Tamper-resistant safeguards for open-weight llms. arXiv preprint arXiv:2408.00761, 2024.   \nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023.   \nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \nWang, J., Li, J., Li, Y., Qi, X., Chen, M., Hu, J., Li, Y., Li, B., and Xiao, C. Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment. arXiv preprint arXiv:2402.14968, 2024.   \nWei, A., Haghtalab, N., and Steinhardt, J. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36, 2024a.   \nWei, B., Huang, K., Huang, Y., Xie, T., Qi, X., Xia, M., Mittal, P., Wang, M., and Henderson, P. Assessing the brittleness of safety alignment via pruning and low-rank modifications. arXiv preprint arXiv:2402.05162, 2024b.   \nWu, B., Shen, L., Zhang, T., and Ghanem, B. Map inference via $\\ell_{2}$ -sphere linear program reformulation. International Journal of Computer Vision, 128(7):1913\u20131936, 2020.   \nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023a.   \nWu, T., Zhu, B., Zhang, R., Wen, Z., Ramchandran, K., and Jiao, J. Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment. arXiv preprint arXiv:2310.00212, 2023b.   \nXu, J., Wang, S., Wang, L., and Yao, A. C.-C. Fedcm: Federated learning with client-level momentum. arXiv preprint arXiv:2106.10874, 2021.   \nYang, X., Wang, X., Zhang, Q., Petzold, L., Wang, W. Y., Zhao, X., and Lin, D. Shadow alignment: The ease of subverting safely-aligned language models. arXiv preprint arXiv:2310.02949, 2023.   \nYe, R., Chai, J., Liu, X., Yang, Y., Wang, Y., and Chen, S. Emerging safety attack and defense in federated instruction tuning of large language models. arXiv preprint arXiv:2406.10630, 2024.   \nYe, S., Feng, X., Zhang, T., Ma, X., Lin, S., Li, Z., Xu, K., Wen, W., Liu, S., Tang, J., et al. Progressive dnn compression: A key to achieve ultra-high weight pruning and quantization rates using admm. arXiv preprint arXiv:1903.09769, 2019.   \nYe, S., Jo, Y., Kim, D., Kim, S., Hwang, H., and Seo, M. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May, 3, 2023.   \nYi, J., Ye, R., Chen, Q., Zhu, B., Chen, S., Lian, D., Sun, G., Xie, X., and Wu, F. On the vulnerability of safety alignment in open-access llms. In Findings of the Association for Computational Linguistics ACL 2024, pp. 9236\u20139260, 2024a.   \nYi, X., Zheng, S., Wang, L., Wang, X., and He, L. A safety realignment framework via subspaceoriented model fusion for large language models. arXiv preprint arXiv:2405.09055, 2024b.   \nYuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., and Huang, F. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023.   \nZhan, Q., Fang, R., Bindu, R., Gupta, A., Hashimoto, T., and Kang, D. Removing rlhf protections in gpt-4 via fine-tuning. arXiv preprint arXiv:2311.05553, 2023.   \nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \nZhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015.   \nZhao, J., Deng, Z., Madras, D., Zou, J., and Ren, M. Learning and forgetting unsafe examples in large language models. arXiv preprint arXiv:2312.12736, 2023.   \nZhou, P., Yuan, X., Xu, H., Yan, S., and Feng, J. Efficient meta learning via minibatch proximal update. Advances in Neural Information Processing Systems, 32, 2019.   \nZhu, M., Yang, L., Wei, Y., Zhang, N., and Zhang, Y. Locking down the finetuned llms safety. arXiv preprint arXiv:2410.10343, 2024.   \nZong, Y., Bohdal, O., Yu, T., Yang, Y., and Hospedales, T. Safety fine-tuning at (almost) no cost: A baseline for vision large language models. arXiv preprint arXiv:2402.02207, 2024.   \nZou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R., Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K., et al. Representation engineering: A top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Missing Information for Experiments 18 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Detailed Setup . . 18   \nA.2 Baselines and its Description 18   \nA.3 More Results . 19   \nA.4 More Visualizations . 20 ", "page_idx": 16}, {"type": "text", "text": "B Missing contents in theoretical analysis 22 ", "page_idx": 16}, {"type": "text", "text": "B.1 Preliminaries 22   \nB.2 Assumptions . . 22   \nB.3 Facts . . 22   \nB.4 Theorems 23   \nB.5 Missing Proof of Theorem 2 23   \nB.5.1 Key Lemmas . . 24   \nB.5.2 Formal Proof . 25   \nB.6 Missing Proof of Theorem 3 26   \nB.6.1 Key Lemmas . . 26   \nB.6.2 Formal Proof 27 ", "page_idx": 16}, {"type": "text", "text": "C Broader Impact 29 ", "page_idx": 16}, {"type": "text", "text": "D Limitations 29 ", "page_idx": 16}, {"type": "text", "text": "A Missing Information for Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Detailed Setup ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Training Details. We follow the two-stage setting from (Huang et al., 2024e) (See Fig 1). The specific training settings for the two stages are as follows. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Alignment. Before employing Lisa into the fine-tuning stage, we first align the model with an alignment dataset. The learning rate for the alignment stage is 1e-3 with batch size 5, and the number of training epochs is 30. The alignment dataset is sampled from (Ji et al., 2023) and the sample number is 10000. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Fine-tuning. In the second stage, i.e., the user fine-tuning stage, we finetune on the harmful data mixed with the task-specific fine-tuning data. The harmful data is sampled from (Ji et al., 2023) and the task-specific fine-tuning data are sampled from the corresponding fine-tuning task. Specially, the task-specific data for AlpacaEval is a high-quality demonstration data produced by $G P T\\dot{4}^{3}$ . The default total number of fine-tuning sample is 5000 (Specially, 700 for AlpacaEval due to limited data number), and among which $10\\%$ of them are harmful data. For all the baselines in this stage, we employ the same learning rate of 1e-5, batch size of 5. Three baseline methods (Lisa/BSO/Vlguard) need guidance from the alignment dataset. For Lisa and BSO, we utilize the same alignment dataset (with 10000 samples) in the alignment stage. For Vlguard, we sample 1000 samples from the same alignment dataset, in order to maintain the same computation with Lisa and BSO for fair comparison. The default step ratio for Lisa and BSO is 100 and 900 respectively for alignment and fine-tuning. ", "page_idx": 17}, {"type": "text", "text": "For both two stages, we utilize LoRA (Hu et al., 2021) for efficient training and we follow the DoubleLoRA implementation from (Huang et al., 2024e), which utilizes two separated LoRA components respectively for alignment and fine-tuning. We use a workstation with an H100 for experiments. ", "page_idx": 17}, {"type": "text", "text": "System Prompt. We follow (Taori et al., 2023) to use a system prompt in the following box for constructing a supervised dataset for alignment/fine-tuning. ", "page_idx": 17}, {"type": "text", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Instruction:{instruction} Input:{input} Response: {response} ", "page_idx": 17}, {"type": "text", "text": "For different fine-tuning tasks, we accordingly construct the triplet of Instruction/Input/Response. For example, for SST2 tasks, the instruction is \"Analyze the sentiment of the input, and respond only positive or negative\", the input is the according sentence in SST2 dataset, and the response is the according label of the sentence, i.e., \"positive\" or \"negative\". ", "page_idx": 17}, {"type": "text", "text": "Metrics. Both harmful ratio and fine-tune accuracy are measured after a model completes fine-tuning on a specific task over. The metrics are measured with a testing dataset (which is unseen in the training phase). For measuring the harmful ratio, we sample 1000 samples from the testing dataset of (Ji et al., 2023), and use the moderation model (Ji et al., 2023), to flag the harmful answers. For SST2, AGNEWS, GSM8K and AlpacaEval, we respectively sample 872, 1000, 1000, and 122 samples from their testing dataset. A testing sample for the fine-tuning task is counted as Correct answer if the model gives the Correct answer classification answer. For GSM8K, a testing sample is classified to be a Correct answer if the final answer given by LLM is correct. The finetune accuracy of these three corresponding tasks is measured as the ratio of Correct answers over all the testing samples. For AlpacaEval, we utilize ChatGPT API to rate the output of the evaluated model. The finetune accuracy is defined as the win rate against text_Devinci_003\u2019s output. ", "page_idx": 17}, {"type": "text", "text": "A.2 Baselines and its Description ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We simulate 5 baselines for comparison. Because we consider a two-stage (alignment-fine-tuning) pipeline, the application of different baselines might concentrate on one stage. Below are detailed description of the baselines. ", "page_idx": 17}, {"type": "text", "text": "\u2022 NA-SFT. This baseline does not conduct an alignment stage but directly uses supervised fine-tuning (SFT) for fine-tuning. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Vaccine-SFT. Vaccine (Huang et al., 2024e) is an alignment stage solution by replacing SFT in the alignment stage. In the fine-tuning stage, vanilla SFT is used. ", "page_idx": 18}, {"type": "text", "text": "\u2022 SFT. This baseline aligns the model with SFT in the alignment stage, and it also uses SFT to finetune the model on the user fine-tuning task. ", "page_idx": 18}, {"type": "text", "text": "\u2022 EWC. This baseline aligns the model with SFT in the alignment stage, and it uses EWC (Kirkpatrick et al., 2017) to finetune the model on the user fine-tuning stage. Original EWC is used for countering catastrophic forgetting in continual learning. ", "page_idx": 18}, {"type": "text", "text": "\u2022 VLGuard.This baseline aligns the model with SFT in the alignment stage. In user fine-tuning stage, Vlguard (Zong et al., 2024) integrates alignment data in fine-tuning. ", "page_idx": 18}, {"type": "text", "text": "BSO and Lisa are the two methods we propose in this paper. It keeps the vanilla SFT in alignment stage, but changes the optimization in the fine-tuning stage. ", "page_idx": 18}, {"type": "text", "text": "We then briefly describe the high level idea of the used baselines. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Vaccine-SFT. Vaccine (Huang et al., 2024e) is the first alignment stage solution towards harmful fine-tuning attack. The idea is to straighten the model\u2019s robustness by modifying the alignment process, such that the model is immunized to the later fine-tuning attack. The idea is to add adversarial perturbation to the embedding of the model in alignment stage, such that the model can withstand the embedding drift incurred in the later fine-tuning process. ", "page_idx": 18}, {"type": "text", "text": "\u2022 VLGuard. VLGuard (Zong et al., 2024) is a fine-tuning stage defense solution towards harmful fine-tuning attack. The defense idea is to mix safety alignment data into the fine-tuning process, in order to constantly remind the model of the alignment knowledge. VLGuard is originally applied in the vision-LLM fine-tuning process but can be easily adpated to LLM. A similar method is also proposed in SafeInstr (Bianchi et al., 2023). ", "page_idx": 18}, {"type": "text", "text": "\u2022 EWC. EWC (Kirkpatrick et al., 2017) is originally proposed to counter the catastrophic forgetting issue for continual learning. In the harmful fine-tuning context, the idea can be easily extended to a fine-tuning stage solution. The idea is to add a regularized term $||\\pmb{w}-\\pmb{w}_{a l i g n}||^{2}$ in the fine-tuning process. By minimizing the regularized loss, the obtained model will not drift too far away from the initial aligned model $w_{a l i g n}$ . Different from Lisa, EWC does not follow an alternative optimization process. In other words, $w_{a l i g n}$ is a fixed model checkpoint throughout the fine-tuning process. ", "page_idx": 18}, {"type": "text", "text": "A.3 More Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Higher harmful ratio. It is interesting to study whether Lisa can resist the attack when harmful ratio is high. In table 13, we show the comparison when higher harmful ratio is adopted. As shown, Lisa obtains comparable defense performance even harmful ratio is high. In contrast, the alignment stage solution Vaccine cannot resist the attack under high harmful ratio (e.g., $\\mathsf{p}\\mathrm{{=}}1$ ), due to its design limitation. This suggests that a pure alignment stage solution is not sufficient to solve the fine-tuning problem. No matter how robust the aligned model is, the fine-tuning attack can still compromise the defense with more toxicity included in the user data. ", "page_idx": 18}, {"type": "table", "img_path": "RPChapuXlC/tmp/357ff561c1b22bf7b24468f6b2d27de6242eb150df133bf16da72e423378ef45.jpg", "table_caption": ["Table 13: Performance under different harmful ratio in the default setting. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Harmonic mean. In the evaluation part, we care about two metrics, i.e., harmful score and fine-tune accuracy, it is ideal to combine this two metric into one single metric to see how the solutions perform. In Table 14, we transform the statistic in Table 12 to Harmonic mean of the two metrics (100-harmful score and fine-tune accuracy). Our results again confirm that Filter+Lisa is more superior. ", "page_idx": 18}, {"type": "text", "text": "Resistance to more advanced attack. Recently new attacks have been proposed to make the attack more stealthy to be detected. Bi-directional Anchoring (He et al., 2024) is an attack aiming to select the most \"harmful\" data among a benign dataset, e.g., an Alpaca dataset. We perform an evaluation using the most \"harmful\" subset of Alpaca dataset for fine-tuning 4. As shown in Table 15, fine-tuning ", "page_idx": 18}, {"type": "text", "text": "Table 14: Harmonic mean when combined with data filtration (with BeaverTails moderation). ", "page_idx": 19}, {"type": "table", "img_path": "RPChapuXlC/tmp/72b65fed63486d49221c4aa89cb0346039a546bc6c3272fcc8610f2abca0a33a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "on this small subset can slightly increase harmful score ${\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\!\\,\\!\\!\\,\\!\\!\\,\\!\\!\\!\\,\\!\\!\\,\\!\\!\\!\\,\\!\\!\\!\\,\\!\\!\\!\\,\\!\\!\\!\\,\\!\\!\\!\\,\\!\\!\\!\\!\\,\\!\\!\\!\\!\\,\\!\\!\\!\\!\\,}\\!\\!\\!\\!\\!}\\!\\!\\!\\!\\!}\\!\\!\\!\\!\\!\\!}$ increase for SFT). By contrast, Lisa achieve superior defense performance (harmful score remains the same with that before fine-tuning). ", "page_idx": 19}, {"type": "table", "img_path": "RPChapuXlC/tmp/71d3898f21152b90c0c3c38c086613fef1ff2d08d284c675a3165f8102200386.jpg", "table_caption": ["Table 15: Performance evaluation with Bi-directional Anchoring (He et al., 2024) "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.4 More Visualizations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Visualization of Malicious prompts. We show in the following how different methods respond to some malicious prompt that triggers the LLM to answer malicious answers. For example, inquire how to steal a car, trigger profane words, or deliver racist statements. As shown, Lisa is able to answer these malicious prompts in a harmless way. ", "page_idx": 19}, {"type": "text", "text": "Visualization of GSM8K task. Next, we visualize how models trained by different methods produce answers to a GSM8K task. As shown, Lisa is able to provide the correct answer while other methods fail, which means that our approach can guarantee fine-tuning task performance without breaking the safety alignment. ", "page_idx": 20}, {"type": "table", "img_path": "RPChapuXlC/tmp/c517507ced127699bb2097296eab30391e353f2ccd818a8260dec177dc7babce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B Missing contents in theoretical analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we shall introduce the details of our theoretical results. ", "page_idx": 21}, {"type": "text", "text": "B.1 Preliminaries ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We first show the definition of KL property, which has been widely used to model the optimization landscape of many machine learning tasks, e.g., (Attouch et al., 2010). ", "page_idx": 21}, {"type": "text", "text": "Definition 1 (KL property). $A$ function $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is said to have the Kurdyka- Lojasiewicz $(K L)$ property at $\\tilde{x}$ if there exists $v\\in(0,+\\infty)$ , a neighbourhood $U$ of $\\tilde{x}$ , and a function $\\varphi:[0,v)\\rightarrow\\mathbb{R}_{+}$ , such that for all $x\\in U$ with $\\{x:g(\\tilde{x})<g(x)<g(\\tilde{x})+v\\}$ , the following condition holds, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varphi^{\\prime}(g(x)-g(\\tilde{x}))\\,\\mathrm{dist}(0,\\partial g(x))\\geqslant1,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\varphi(v)=c v^{1-\\theta}$ for $\\theta\\in[0,1)$ and $c>0$ . ", "page_idx": 21}, {"type": "text", "text": "The KL property is a useful analysis tool to characterize the local geometry around the critical points in the non-convex landscape, and could be viewed as a generalization of Polyak-\u0141ojasiewicz (PL) condition(Karimi et al., 2016) when the $\\mathrm{KL}$ parameter is $\\theta={\\textstyle{\\frac{1}{2}}}$ (Chen et al., 2021). We rely on the KL framework to derive the convergence bound. ", "page_idx": 21}, {"type": "text", "text": "Definition 2 (Potential function). To assist with our convergence analysis, we define a potential function $\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})$ as follows. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})=f(\\tilde{\\pmb{w}}_{t})+h(\\pmb{w}_{t})+\\frac{\\rho}{2}\\|\\tilde{\\pmb{w}}_{t}-\\pmb{w}_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.2 Assumptions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Assumption 1 (Proper closed loss function). $f(\\cdot)$ and $h(\\cdot)$ are proper 5 and closed 6. ", "page_idx": 21}, {"type": "text", "text": "Assumption 2 (L-smoothness). We assume $L$ -smoothness over the alignment/fine-tuning loss function. Formally, we assume there exists a positive constant $L$ such that $\\|\\nabla f(\\pmb{w})-\\dot{\\nabla}f(\\pmb{w}^{\\prime})\\|\\le L\\|\\pmb{w}-\\pmb{w}^{\\prime}\\|$ and $\\|\\nabla h(\\pmb{w})-\\nabla h(\\pmb{w}^{\\prime})\\|\\leq L\\|\\pmb{w}-\\pmb{w}^{\\prime}\\|$ holds for $\\pmb{w},\\pmb{w}^{\\prime}\\in\\mathbb{R}^{d}$ . ", "page_idx": 21}, {"type": "text", "text": "Remark 2. By Assumption $^{\\,I}$ , we intend to ensure that $i_{.}$ ) the loss is lower bounded, i.e., for $\\pmb{w}\\in\\mathbb{R}^{d}$ , $f(\\tilde{\\pmb{w}})>-\\infty,h(\\pmb{w})>-\\infty,$ , and $i i$ ) the loss is lower semi-continuous. The assumption is widely used in analysis of proximal algorithms. e.g., $L i$ & Pong, 2016; Wu et al., 2020). Assumption 2 are widely used to characterize the convergence property of optimization algorithms, e.g., $X u$ et al., 2021; Li et al., 2019) . ", "page_idx": 21}, {"type": "text", "text": "Assumption 3 (KL assumption). The potential function $\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})$ satisfies the KL property with function $\\varphi(v)=c v^{1-\\theta}$ given $\\theta\\in[0,1)$ . ", "page_idx": 21}, {"type": "text", "text": "Remark 3. Given that $f$ and $h$ is proper and closed as in Assumption 1, Assumption 3 holds true as long as the local objective $f(\\cdot)$ and $h(\\cdot)$ are sub-analytic function, logarithm functions, exponential functions, or semi-algebraic functions (Chen et al., 2021). This assumption is rather mild, since most of the nonconvex objective functions encountered in machine learning applications falls in this range, and the assumption is widely used in the existing literature e.g., (Attouch et al., 2010; Chen et al., 2021; Li & Pong, 2016). ", "page_idx": 21}, {"type": "text", "text": "B.3 Facts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Fact 1 (Optimality property of State I local problem). By the left of Eq. (2), the optimality gives, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla f(\\tilde{\\pmb{w}}_{t+1})+\\rho(\\tilde{\\pmb{w}}_{t+1}-\\pmb{w}_{t})=0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Fact 2 (Optimality property of State II local problem). By the right of Eq. (2), the optimality gives, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla h(\\boldsymbol{w}_{t+1})+\\rho(\\boldsymbol{w}_{t+1}-\\tilde{\\boldsymbol{w}}_{t+1})=0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "These two facts are derived from the intrinsic property of Lisa, which holds without needing any formal assumptions. ", "page_idx": 21}, {"type": "text", "text": "B.4 Theorems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We show two theorems as our main results of the convergence analysis of Lisa. The second theorem on the exact convergence rate relies on the first theorem on sub-sequence convergence. ", "page_idx": 22}, {"type": "text", "text": "Theorem 2 (Subsequence convergence). Suppose that Assumptions 1-2 hold true, the proximal penalty is chosen as $\\rho\\geq L$ , and that there exists a subsequence of $(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})$ converging to a cluster point $(\\tilde{w}^{*},w^{*})$ . Then, the subsequence generated by Lisa establishes the following property: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{j\\to\\infty}\\bigl(\\tilde{\\pmb{w}}_{t^{j}+1},\\pmb{w}_{t^{j}+1}\\bigr\\}\\bigr)=\\operatorname*{lim}_{j\\to\\infty}\\bigl(\\tilde{\\pmb{w}}_{t^{j}},\\pmb{w}_{t^{j}}\\bigr\\}\\bigr)=\\bigl(\\tilde{\\pmb{w}}^{*},\\pmb{w}^{*}\\bigr)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, the cluster point is indeed a stationary point of the global problem, or equivalently, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla f(\\tilde{\\pmb{w}}^{*})+\\nabla h(\\pmb{w}^{*})=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Remark 4. Theorem 2 states that if there exist a subsequence of the produced sequence that converges to a cluster point, then this cluster point is indeed a stationary point of the global problem (Eq. (1)). The additional assumption of converging subsequence holds if the sequence is bounded (per sequential compactness theorem). ", "page_idx": 22}, {"type": "text", "text": "Theorem 3 (Restate of Theorem 1). Suppose that Assumptions 1-3 hold, the proximal penalty is chosen as $\\rho>L,$ , and that there exists a subsequence of $(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})$ converging to a cluster point $(\\tilde{w}^{*},w^{*})$ . Under different settings of $\\theta$ of the $K L$ property, the generated sequence of Lisa establishes the following convergence rate: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Case $\\theta=0$ . For sufficiently large iteration $T>t_{0}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{w}_{T})+\\nabla h(\\pmb{w}_{T})\\|=0\\quad(f i n i t e\\;i t e r a t i o n s)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "\u2022 Case $\\theta=(0,{\\frac{1}{2}}]$ . For sufficiently large iteration $T>t_{0}^{\\prime}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\tilde{w}_{T})+\\nabla h(w_{T})\\|\\leq\\frac{\\sqrt{2}\\rho}{\\sqrt{\\rho-L}}\\sqrt{(1-\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}})^{T-t_{0}^{\\prime}}r_{t_{0}^{\\prime}}}\\quad(l i n e a r\\,c o n v e r g e n c e)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "\u2022 Case $\\textstyle\\theta={\\bigl(}{\\frac{1}{2}},1{\\bigr)}$ . For all $T>0$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\tilde{w}_{T})+\\nabla h(w_{T})\\|\\leq\\frac{\\sqrt{2}\\rho}{\\sqrt{\\rho-L}}\\,^{2-4\\theta}\\!\\!\\left/T(2\\theta-1)\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}\\right.\\;\\;(s u b\\!-\\!l i n e a r\\,c o n v e r g e n c e)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Remark 5. The convergence rate to a stationary point is heavily determined by parameter $\\theta$ in the $K L$ property. A smaller $\\theta$ implies that the potential function is descended faster in its geometry, and therefore guaranteeing a faster convergence rate. Specifically, for $\\theta=0$ , the stationary point could be reached within finite iterations. For $\\bar{\\theta}\\in(0,\\frac12]$ , linear convergence rate can be achieved. While for $\\textstyle\\theta\\in\\left({\\frac{1}{2}},1\\right)$ , only sub-linear convergence rate can be achieved. In summary, as long as the potential function satisfies the $K L$ property with $\\theta\\in[0,1)$ , sequence of Lisa always converges to a stationary point with respect to w in Eq. (1) if $T\\to\\infty$ . ", "page_idx": 22}, {"type": "text", "text": "B.5 Missing Proof of Theorem 2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Now we proceed to give the proof of Theorem 2. ", "page_idx": 22}, {"type": "text", "text": "Proof sketch. Our proof sketch can be summarized as follows: i) We showcase in Lemma 3 that the potential function is non-decreasing along the sequence, and its descent is positively related to $\\lVert\\tilde{\\boldsymbol{w}_{t+1}}-\\tilde{\\boldsymbol{w}_{t}}\\rVert$ and $\\|\\pmb{w}_{t+1}-\\pmb{w}_{t}\\|$ . Telescoping its descent along the whole sequence to infinite, we can prove that the final converged value of the potential function is the infinite sum of the above two norms. ii) By Lemma 2, we see that converged value of the potential function can not take negatively infinite, and therefore, we further conclude that $\\tilde{w}_{t+1}\\rightarrow\\tilde{w}_{t}$ and $\\pmb{w}_{t+1}\\rightarrow\\pmb{w}_{t}$ . iii) Then we start our proof of stationary property of the cluster point. Conditioned on the sequence convergence property obtained before, we sequentially show that the residual term in the RHS of the condition is eliminable, that the two local gradients at the cluster point and iterates point are interchangeable. iv) Plugging these claims into the global optimality condition Eq. (11), the stationary property follows as stated. ", "page_idx": 22}, {"type": "text", "text": "B.5.1 Key Lemmas ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 1 (Global optimality). The following equation holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla f(\\pmb{\\tilde{w}}_{t+1})+\\nabla h(\\pmb{w}_{t+1})+\\rho(\\pmb{w}_{t+1}-\\pmb{w}_{t})=0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. The results comes immediately by summing Eq. (4) and Eq. (5). ", "page_idx": 23}, {"type": "text", "text": "Lemma 2 (Lower bound of potential function). If the cluster point $(\\tilde{w}^{*},w^{*})$ exists, the potential function at the cluster point exhibits the following lower bound: ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\infty<\\mathcal{D}(\\tilde{\\pmb{w}}^{*},\\pmb{w}^{*})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. By definition of the potential function, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\tilde{\\pmb{w}}^{*},\\pmb{w}^{*})=f(\\tilde{\\pmb{w}}^{*})+h(\\pmb{w}^{*})+\\frac{\\rho}{2}\\|\\tilde{\\pmb{w}}^{*}-\\pmb{w}^{*}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With Assumption 1 and the fact that the proximal term $\\frac{\\rho}{2}\\|\\tilde{\\pmb{w}}^{*}-\\pmb{w}^{*}\\|^{2}$ cannot be a negative value, we complete the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma 3 (Sufficient and non-increasing descent). The descent of the potential function along the sequence generated by Lisa can be upper bounded as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\tilde{w}_{t+1},\\boldsymbol{w}_{t+1})-\\mathcal{D}(\\tilde{w}_{t},\\boldsymbol{w}_{t})\\leq-\\frac{\\rho-L}{2}(\\|\\boldsymbol{w}_{t+1}-\\boldsymbol{w}_{t}\\|^{2}+\\|\\tilde{\\boldsymbol{w}}_{t+1}-\\tilde{\\boldsymbol{w}}_{t}\\|^{2})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, if $\\rho$ is chosen as $\\rho\\geq L$ , the descent is non-increasing along $t$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. To evaluate the non-increasing property of potential function along the sequence $(\\tilde{{\\boldsymbol{w}}}_{t},{\\boldsymbol{w}}_{t})$ , we first show the property of the gap between two consecutive iterates, and notice that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\tilde{w}_{t+1},w_{t+1})-\\mathcal{D}(\\tilde{w}_{t},w_{t})=\\underbrace{\\mathcal{D}(\\tilde{w}_{t+1},w_{t+1})-\\mathcal{D}(\\tilde{w}_{t+1},w_{t})}_{T1}+\\underbrace{\\mathcal{D}(\\tilde{w}_{t+1},w_{t})-\\mathcal{D}(\\tilde{w}_{t},w_{t})}_{T2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Bounding T1. By definition of potential function, term T1 can be expanded and upper-bounded as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{D}(\\bar{w}_{t+1},w_{t+1})-\\mathcal{D}(\\bar{w}_{t+1},w_{t})}\\\\ &{=h(w_{t+1})-h(w_{t})+\\frac{\\rho}{2}\\|\\bar{w}_{t+1}-w_{t+1}\\|^{2}-\\frac{\\rho}{2}\\|\\bar{w}_{t+1}-w_{t}\\|^{2}}\\\\ &{=h(w_{t+1})-h(w_{t})+\\underbrace{\\frac{\\rho}{2}\\langle w_{t+1}+w_{t}-2\\bar{w}_{t+1},w_{t+1}-w_{t}\\rangle}_{\\mathrm{since}:\\rho^{-}(a+b)(a-b)}}\\\\ &{=h(w_{t+1})-h(w_{t})+\\rho\\langle w_{t+1}-\\bar{w}_{t+1},w_{t+1}-w_{t}\\rangle-\\frac{\\rho}{2}\\|w_{t+1}-w_{t}\\|^{2}}\\\\ &{=h(w_{t+1})-h(w_{t})+\\underbrace{\\langle\\nabla h(w_{t+1}),w_{t}-w_{t+1}\\rangle-\\frac{\\rho}{2}\\|w_{t+1}-w_{t}\\|^{2}}_{\\mathrm{by~Eq.(5)}}}\\\\ &{\\le-\\frac{\\rho-L}{2}\\|w_{t+1}-w_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality holds by L-smoothness of $h(\\cdot)$ , which means $-h(\\pmb{w}_{t})\\,\\leq\\,-h(\\pmb{w}_{t+1})\\,-$ $\\begin{array}{r}{\\langle\\nabla h({\\boldsymbol w}_{t+1}),{\\boldsymbol w}_{t}-{\\boldsymbol w}_{t+1}\\rangle+\\frac{L}{2}\\|{\\boldsymbol w}_{t}-{\\boldsymbol w}_{t+1}\\|^{2}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Bounding T2. Similarly Term T2 can be bounded as follows, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{D}(\\tilde{w}_{t+1},w_{t})-\\mathcal{D}(\\tilde{w}_{t},w_{t})}\\\\ &{=f(\\tilde{w}_{t+1})-f(\\tilde{w}_{t})+(\\frac{\\rho}{2}\\|w_{t}-\\tilde{w}_{t+1}\\|^{2}-\\frac{\\rho}{2}\\|w_{t}-\\tilde{w}_{t}\\|^{2})}\\\\ &{=f(\\tilde{w}_{t+1})-f(\\tilde{w}_{t})+\\frac{\\rho}{2}\\underbrace{\\langle2w_{t}-\\tilde{w}_{t}-\\tilde{w}_{t+1},\\tilde{w}_{t}-\\tilde{w}_{t+1}\\rangle}_{{a^{2}-b^{2}=(a+b)(a-b)}})}\\\\ &{=f(\\tilde{w}_{t+1})-f(\\tilde{w}_{t})+\\rho\\langle w_{t}-\\tilde{w}_{t+1},\\tilde{w}_{t}-\\tilde{w}_{t+1}\\rangle-\\frac{\\rho}{2}\\|\\tilde{w}_{t+1}-\\tilde{w}_{t}\\|^{2}}\\\\ &{=f(\\tilde{w}_{t+1})-f(\\tilde{w}_{t})+\\rho\\langle\\nabla f(\\tilde{w}_{t+1}),\\tilde{w}_{t}-\\tilde{w}_{t+1}\\rangle-\\frac{\\rho}{2}\\|\\tilde{w}_{t+1}-\\tilde{w}_{t}\\|^{2}}\\\\ &{\\leq-\\frac{\\rho-L}{2}\\|\\tilde{w}_{t+1}-\\tilde{w}_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality holds by L-smoothness of $h(\\cdot)$ ", "page_idx": 24}, {"type": "text", "text": "Summing the upper bound of Eq. (15), Eq. (16) and Eq. (17), we reach the following conclusion: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\tilde{w}_{t+1},w_{t+1})-\\mathcal{D}(\\tilde{w}_{t},w_{t})\\leq-\\frac{\\rho-L}{2}(\\|w_{t+1}-w_{t}\\|^{2}+\\|\\tilde{w}_{t+1}-\\tilde{w}_{t}\\|^{2})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If $\\rho$ is chosen as $\\rho\\geq L$ , the non-increasing property follows immediately. ", "page_idx": 24}, {"type": "text", "text": "B.5.2 Formal Proof ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Now we showcase the formal proof of Theorem 2. We derive the complete proof into two parts. The first part is to prove claim i) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{j\\to\\infty}\\bigl(\\tilde{\\pmb{w}}_{t^{j}+1},\\pmb{w}_{t^{j}+1}\\bigr\\}\\bigr)=\\operatorname*{lim}_{j\\to\\infty}\\bigl(\\tilde{\\pmb{w}}_{t^{j}},\\pmb{w}_{t^{j}}\\bigr\\}\\bigr)=\\bigl(\\tilde{\\pmb{w}}^{*},\\pmb{w}^{*}\\bigr)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Telescoping the descent. Lemma 3 shows that the descent of potential function satisfies some nice property (i.e., non-increasing) if properly choosing proximal intensity. To further extend the result in Lemma 3, we telescope the iterated descent from $t=0,\\dots,T-1$ , which gives, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\tilde{w}_{T},w_{T})-\\mathcal{D}(\\tilde{w}_{0},w_{0})\\leq-\\frac{\\rho-L}{2}\\sum_{t=0}^{T}(\\|w_{t+1}-w_{t}\\|^{2}+\\|\\tilde{w}_{t+1}-\\tilde{w}_{t}\\|^{2})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the other hand, by assumption, a cluster point $(\\tilde{w}^{*},w^{*})$ of sequence $(\\tilde{{\\boldsymbol{w}}}_{t},{\\boldsymbol{w}}_{t})$ exists. Then, there exists a subsequence $\\left(\\tilde{\\pmb{w}}_{t^{j}},\\pmb{w}_{t^{j}}\\right)$ satisfies: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{j\\rightarrow\\infty}(\\tilde{\\pmb{w}}_{t^{j}},\\pmb{w}_{t^{j}}\\})=(\\tilde{\\pmb{w}}^{*},\\pmb{w}^{*})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the lower semi-continuous property of $\\mathcal{D}(\\cdot)$ (given that the functions $f(\\cdot)$ and $h(\\cdot)$ are closed), we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\tilde{\\pmb{w}}^{*},\\pmb{w}^{*})\\leq\\operatorname*{lim}_{j\\rightarrow\\infty}\\operatorname*{inf}\\mathcal{D}(\\tilde{\\pmb{w}}_{t^{j}},\\pmb{w}_{t^{j}})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This together with inequality (20) yields: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{D}(\\tilde{\\boldsymbol{w}}^{*},\\boldsymbol{w}^{*})-\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{0},\\boldsymbol{w}_{0})}\\\\ &{\\leq\\underset{j\\rightarrow\\infty}{\\operatorname*{lim}}\\operatorname*{inf}\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{t^{j}},\\boldsymbol{w}_{t^{j}})-\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{0},\\boldsymbol{w}_{0})}\\\\ &{\\leq-\\,\\frac{\\rho-L}{2}\\displaystyle\\sum_{t=0}^{\\infty}(\\|\\boldsymbol{w}_{t+1}-\\boldsymbol{w}_{t}\\|^{2}+\\|\\tilde{\\boldsymbol{w}}_{t+1}-\\tilde{\\boldsymbol{w}}_{t}\\|^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lower bound the potential function at cluster point. Since $\\mathcal{D}(\\tilde{\\boldsymbol{w}}^{*},\\boldsymbol{w}^{*})$ is lower bounded as per Lemma 2, and $\\mathcal{D}(\\tilde{w}_{0},w_{0})$ is upper bounded (since $f(\\tilde{w}_{0})$ and $h(\\pmb{w}_{0})<\\infty)$ ). Therefore $\\mathcal{D}(\\tilde{\\boldsymbol{w}}^{*},\\boldsymbol{w}^{*})-$ $D(\\tilde{w}_{0},{\\pmb w}_{0})>-\\dot{\\infty}$ . It then follows that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n-\\infty<-\\frac{\\rho-L}{2}\\sum_{t=0}^{\\infty}(\\|\\pmb{w}_{t+1}-\\pmb{w}_{t}\\|^{2}+\\|\\tilde{\\pmb{w}}_{t+1}-\\tilde{\\pmb{w}}_{t}\\|^{2})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Derive the convergence property. Recall that $-\\,{\\frac{\\rho-L}{2}}\\leq0$ as per our choice of $\\rho$ . It follows, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\|w_{t+1}-w_{t}\\|=0\\Rightarrow w_{t+1}\\to w_{t},\\ \\operatorname*{lim}_{t\\to\\infty}\\|\\tilde{w}_{t+1}-\\tilde{w}_{t}\\|=0\\Rightarrow\\tilde{w}_{t+1}\\to\\tilde{w}_{t}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging the above results into Eq. (21), we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{j\\to\\infty}\\bigl(\\tilde{\\pmb{w}}_{t^{j}+1},\\pmb{w}_{t^{j}+1}\\bigr\\}\\bigr)=\\operatorname*{lim}_{j\\to\\infty}\\bigl(\\tilde{\\pmb{w}}_{t^{j}},\\pmb{w}_{t^{j}}\\bigr\\}\\bigr)=\\bigl(\\tilde{\\pmb{w}}^{*},\\pmb{w}^{*}\\bigr)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The second part of proof is to verify Claim $i i$ ): the cluster point is a stationary point of the global problem. ", "page_idx": 25}, {"type": "text", "text": "Starting from the global optimality condition. Choosing $t=t^{j}$ in Eq. (11) and taking the limit $j\\rightarrow\\infty$ , it follows that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{j\\rightarrow\\infty}\\nabla f(\\pmb{\\tilde{w}}_{t^{j}})+\\operatorname*{lim}_{j\\rightarrow\\infty}\\nabla h(\\pmb{w}_{t^{j}})+\\operatorname*{lim}_{j\\rightarrow\\infty}\\rho(\\pmb{w}_{t^{j}}-\\pmb{w}_{t^{j}-1})=0\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The residual term is eliminable. Since $\\begin{array}{r}{\\operatorname*{lim}_{j\\to\\infty}{\\pmb w}_{t^{j}}=\\operatorname*{lim}_{j\\to\\infty}{\\pmb w}_{t^{j}-1}}\\end{array}$ , the residual can be eliminated. It follows that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{j\\rightarrow\\infty}\\nabla f(\\tilde{\\pmb{w}}_{t^{j}})+\\operatorname*{lim}_{j\\rightarrow\\infty}\\nabla h(\\pmb{w}_{t^{j}})=0\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Terms $\\begin{array}{r}{\\operatorname*{lim}_{j\\to\\infty}\\nabla f(\\tilde{\\pmb{w}}_{t^{j}})}\\end{array}$ and $\\nabla f(\\tilde{w}^{*})$ are interchangeable. By $\\mathrm{L}$ -smoothness and definition of cluster point, for arbitrary $\\epsilon>0$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lim}_{j\\rightarrow\\infty}\\|\\nabla f(\\tilde{\\pmb{w}}_{t^{j}})-\\nabla f(\\tilde{\\pmb{w}}^{*})\\|\\leq\\operatorname*{lim}_{j\\rightarrow\\infty}L\\|\\tilde{\\pmb{w}}_{t^{j}}-\\tilde{\\pmb{w}}^{*}\\|}}\\\\ &{}&{<L\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last equality holds because $\\tilde{\\pmb{w}}_{t^{j}}\\rightarrow\\tilde{\\pmb{w}}^{*}$ . Subsequently, we indeed have $\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lim}_{j\\to\\infty}\\nabla f(\\tilde{\\pmb{w}}_{t^{j}})=}}\\end{array}$ $\\nabla f(\\tilde{w}^{*})$ , i.e., they are interchangeable. ", "page_idx": 25}, {"type": "text", "text": "Terms $\\begin{array}{r}{\\operatorname*{lim}_{j\\to\\infty}\\nabla h(\\pmb{w}_{t^{j}})}\\end{array}$ and $\\nabla h(w^{*})$ are interchangeable. Using the same deduction, one can also prove $\\begin{array}{r}{\\operatorname*{lim}_{j\\rightarrow\\infty}\\nabla h(\\pmb{w}_{t^{j}})=\\nabla h(\\pmb{w}^{\\ast})}\\end{array}$ , which means they are also interchangeable. ", "page_idx": 25}, {"type": "text", "text": "Plugging the interchangeable results into Eq. (28), we obtain the final result. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla f(\\tilde{\\pmb{w}}^{*})+\\nabla h(\\pmb{w}^{*})=0\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "B.6 Missing Proof of Theorem 3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Then we show the proof of Theorem 3.We first give a proof sketch for sake of readability. ", "page_idx": 25}, {"type": "text", "text": "Proof sketch. The milestone of the proof can be summarized as follows. i) We first define an auxiliary term called residual of the potential function, and subsequently we find that it has some very nice property (Lemma 4), i.e., $r_{t}\\to0$ and $r_{t}\\geq0$ . ii) We find that the squared gradient norm of the global loss can be bounded by a term with $\\lVert\\pmb{w}_{t}-\\pmb{w}_{t+1}\\rVert$ . On the other hand, we derive that $r_{t}$ can also be lower bounded by $\\lVert\\pmb{w}_{t}-\\pmb{w}_{t+1}\\rVert$ . Combining both derivations, we connect the gradient with $r_{t}$ . iii) Then we further derive the upper bound of $r_{t}$ . We find that it is connected with the gradient of the potential function, which is also related to the term $\\lVert\\pmb{w}_{t}-\\pmb{w}_{t+1}\\rVert$ . iv) By jointing all the derived factors, we derive the recursion $\\begin{array}{r}{r_{t}-r_{t+1}=\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}r_{t}^{2\\theta}}\\end{array}$ . Jointing the property of $r_{t}$ , we derive the analysis of final convergence rate under three cases of $\\theta$ , which completes the proof of our statement. ", "page_idx": 25}, {"type": "text", "text": "B.6.1 Key Lemmas ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma 4 (Limit of residual). Under the same assumption of Theorem 3, the residual $r_{t}\\,:=$ $D(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})-D(\\tilde{\\pmb{w}}^{*},\\pmb{w}^{*})$ establishes the following property: $i,$ ) $r_{t}\\geq0$ for $t>0$ , ii) $\\operatorname*{lim}_{t\\to\\infty}r_{t}=0$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. We first show that $r_{t}\\geq0$ for $t\\geq0$ . From the lower semi-continuity of $\\mathcal{D}(\\cdot)$ , we obtain that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{j\\to\\infty}\\operatorname*{inf}\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{t^{j}},\\boldsymbol{w}_{t^{j}})-\\mathcal{D}(\\tilde{\\boldsymbol{w}}^{*},\\boldsymbol{w}^{*})\\geq0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Further, by the non-increasing descent property shown by Lemma 3, for $t>0$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})\\geq\\operatorname*{lim}_{j\\rightarrow\\infty}\\operatorname*{inf}\\mathcal{D}(\\tilde{\\pmb{w}}_{t^{j}},\\pmb{w}_{t^{j}})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining Inequality (31) and (32) , we obtain that for any $t>0$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\nr_{t}=\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})-\\mathcal{D}(\\tilde{\\pmb{w}}^{\\ast},\\pmb{w}^{\\ast})\\geq0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This proves our first claim. ", "page_idx": 26}, {"type": "text", "text": "Now we show the limit of $r_{t}$ . On the other hand, by the convergence of sequence Eq. (6) and the continuity of $f(\\cdot)$ and $h(\\cdot)$ , we have limj\u2192\u221ef( w\u02dctj) = f( w\u02dc\u2217), $\\begin{array}{r}{\\operatorname*{lim}_{j\\rightarrow{\\infty}}\\bar{h}(\\pmb{w}_{t^{j}})\\;\\stackrel{}{=}\\;h(\\pmb{w}^{*})}\\end{array}$ , $\\begin{array}{r}{\\operatorname*{lim}_{j\\to\\infty}\\|\\tilde{\\pmb{w}}_{t^{j}}-\\tilde{\\pmb{w}}^{*}\\|^{2}=0}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{lim}_{j\\rightarrow\\infty}\\|\\pmb{w}_{t^{j}}-\\pmb{w}^{\\ast}\\|^{2}=0}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "With these factors in hand, we can proceed to prove that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{j\\rightarrow\\infty}{\\operatorname*{lim}}\\left\\lbrace\\mathcal{D}(\\tilde{w}_{t^{j}},w_{t^{j}})=f(\\tilde{w}_{t^{j}})+h(w_{t^{j}})+\\frac{\\rho}{2}\\|\\tilde{w}_{t^{j}}-w_{t^{j}}\\|^{2}\\right\\rbrace}\\\\ &{=\\underset{j\\rightarrow\\infty}{\\operatorname*{lim}}\\left\\lbrace f(\\tilde{w}_{t^{j}})+h(w_{t^{j}})+\\frac{\\rho}{2}\\|\\tilde{w}_{t^{j}}-\\tilde{w}^{\\ast}+\\tilde{w}^{\\ast}-w^{\\ast}+w^{\\ast}-w_{t^{j}}\\|^{2}\\right\\rbrace}\\\\ &{\\leq\\underset{j\\rightarrow\\infty}{\\operatorname*{lim}}\\left\\lbrace f(\\tilde{w}_{t^{j}})+h(w_{t^{j}})+\\frac{\\rho}{2}\\|\\tilde{w}_{t^{j}}-\\tilde{w}^{\\ast}\\|^{2}+\\|\\tilde{w}^{\\ast}-w^{\\ast}\\|^{2}+\\|w^{\\ast}-w_{t^{j}}\\|^{2}\\right\\rbrace}\\\\ &{\\leq f(\\tilde{w}^{\\ast})+h(w^{\\ast})+\\frac{\\rho}{2}\\|\\tilde{w}^{\\ast}-w^{\\ast}\\|^{2}}\\\\ &{=\\mathcal{D}(\\tilde{w}^{\\ast},w^{\\ast})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which indeed shows $\\begin{array}{r}{\\operatorname*{lim}_{j\\to\\infty}\\operatorname*{sup}r_{t^{j}}\\leq0.}\\end{array}$ . Combining this with Eq. (31), we arrive at $\\textstyle\\operatorname*{lim}_{j\\to\\infty}r_{t^{j}}=$ 0. Since $r_{t}$ is lower-bounded by 0, and is non-increasing, we see that the limitation $\\operatorname*{lim}_{t\\to\\infty}r_{t}$ exists. Given that $\\operatorname*{lim}_{t\\to\\infty}r_{t}$ exists, we reach the conclusion $\\textstyle\\operatorname*{lim}_{t\\to\\infty}r_{t}=0$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "B.6.2 Formal Proof ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. Let $r_{t}:=\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})-\\mathcal{D}(\\tilde{\\pmb{w}}^{\\ast},\\pmb{w}^{\\ast})$ captures the residual of potential function between an iterated point $(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})$ and the cluster point $(\\tilde{w}^{*},w^{*})$ . ", "page_idx": 26}, {"type": "text", "text": "Derive the upper bound of gradient. By Eq. (11), we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\nabla f(\\tilde{\\pmb{w}}_{t})+\\nabla h(\\pmb{w}_{t})+\\rho(\\pmb{w}_{t}-\\pmb{w}_{t-1})=0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which can be expanded as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lVert\\nabla f(\\tilde{\\pmb w}_{t})+\\nabla h(\\pmb w_{t})\\rVert\\leq\\rho\\lVert\\pmb w_{t}-\\pmb w_{t-1}\\rVert\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Connect the gradient with $r_{t}$ . On the other hand, by Lemma 3, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\nr_{t}-r_{t-1}\\geq\\frac{\\rho-L}{2}(\\|w_{t}-w_{t-1}\\|^{2}+\\|\\tilde{w}_{t}-\\tilde{w}_{t-1}\\|^{2})\\geq\\frac{\\rho-L}{2}\\|w_{t}-w_{t-1}\\|^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $r_{t-1}\\geq0$ for any $t>0$ (See Lemma 4), the following relation holds true: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\tilde{\\boldsymbol{w}}_{t})+\\nabla h(\\boldsymbol{w}_{t})\\|\\leq\\frac{\\sqrt{2}\\rho}{\\sqrt{\\rho-L}}\\cdot\\sqrt{r_{t}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the following, we shall introduce $\\mathrm{KL}$ property to achieve an upper bound of $r_{t}$ . ", "page_idx": 26}, {"type": "text", "text": "Upper bound $r_{t}$ with $\\mathbf{KL}$ property of the potential function. Since the potential function satisfies KL property with $\\phi(v)=\\bar{c}v^{1-\\bar{\\theta}}$ , we know for all $t$ that satisfies $r_{t}>0$ , the following relation holds true, ", "page_idx": 26}, {"type": "equation", "text": "$$\nc(1-\\theta)r_{t}^{-\\theta}\\|\\nabla\\mathcal{D}(\\tilde{{\\pmb w}}_{t},{\\pmb w}_{t})\\|\\geq1,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with its equivalence form as follows, ", "page_idx": 26}, {"type": "equation", "text": "$$\nr_{t}^{\\theta}\\leq c(1-\\theta)\\|\\nabla\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})\\|,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Upper bound the gradient of the potential function. We now show that the gradient of the potential function can indeed be upper bounded. Note that $\\nabla\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})\\triangleq(\\nabla_{\\tilde{\\pmb{w}}_{t}}\\mathcal{D}(\\cdot,\\cdot),\\nabla_{\\pmb{w}_{t}}\\mathcal{D}_{\\eta_{g}}(\\cdot,\\cdot)).$ . Now we separately give the gradient with respect to different groups of variables. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{\\tilde{w}_{t}}\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})=\\nabla f(\\tilde{\\boldsymbol{w}}_{t+1})+\\rho(\\tilde{\\boldsymbol{w}}_{t+1}-\\boldsymbol{w}_{t})=0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last equality holds by Eq. (4). ", "page_idx": 27}, {"type": "text", "text": "Similarly, the gradient with respect to $\\pmb{w}_{t}$ is as follows. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{w_{t}}\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})=\\nabla h(\\boldsymbol{w}_{t+1})+\\rho(\\boldsymbol{w}_{t}-\\tilde{\\boldsymbol{w}}_{t+1})=\\rho(\\boldsymbol{w}_{t}-\\boldsymbol{w}_{t+1})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequlaity holds by plugging Eq. (5). ", "page_idx": 27}, {"type": "text", "text": "Note that $\\nabla\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})=\\sqrt{\\|\\nabla_{\\tilde{w}_{t}}\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})\\|^{2}+\\|\\nabla_{\\boldsymbol{w}_{t}}\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})\\|^{2}}$ . Summing the gradient, we arrive at, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{D}(\\tilde{\\boldsymbol{w}}_{t},\\boldsymbol{w}_{t})=\\rho(\\boldsymbol{w}_{t}-\\boldsymbol{w}_{t+1})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Upper bound to $r_{t}$ with the gradient of the potential function. This together with Eq. (40) show that, $r_{t}$ can be bounded as follows, ", "page_idx": 27}, {"type": "equation", "text": "$$\nr_{t}^{\\theta}\\leq c(1-\\theta)\\rho\\|\\pmb{w}_{t}-\\pmb{w}_{t+1}\\|,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Recall that the norm term $\\lVert\\pmb{w}_{i,t}-\\pmb{w}_{i,t+1}\\rVert^{2}$ is bounded as Inequality (37). We first taking square of both sides of (44), yielding ", "page_idx": 27}, {"type": "equation", "text": "$$\nr_{t}^{2\\theta}\\leq c^{2}(1-\\theta)^{2}\\rho^{2}\\|w_{t}-w_{t+1}\\|^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Plugging Eq. (37) into the above results, under the case that $r_{t}>0$ for all $t>0$ , we can ensure: ", "page_idx": 27}, {"type": "equation", "text": "$$\nr_{t}-r_{t+1}\\geq\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}r_{t}^{2\\theta}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Separate into three cases. We then separate our analysis under three different settings of $\\theta$ . ", "page_idx": 27}, {"type": "text", "text": "\u2022 Firstly, assume $\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})$ satisfies the KL property with $\\theta=0$ . Per Eq. (46), if $r_{t}>0$ holds for all $t>0$ , we have $\\begin{array}{r}{r_{t}\\:\\geq\\:\\frac{\\rho-L}{\\rho^{2}c^{2}}}\\end{array}$ for all $t>0$ . Recall from Lemma 4 that $\\textstyle\\operatorname*{lim}_{t\\to\\infty}r_{t}=0$ , which means rT \u2265\u03c1\u03c12\u2212cL2 cannot be true when T is a sufficiently large number. Therefore, there must exist a $t_{0}$ such that $r_{t_{0}}=0$ . If this is the case, observed from Lemma 4 that $r_{t}\\geq0$ for all $t>0$ , and that $r_{t}$ is non-increasing. It is sufficient to conclude that for a sufficiently large number $T>t_{0}$ , $r_{T}=0$ must hold true. Inserting this result into RHS of Eq. (38), the desired rate follows immediately. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Then, consider the case $\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})$ satisfies the $\\mathrm{KL}$ property with $\\theta\\,\\in\\,(0,{\\frac{1}{2}}]$ . First we assume that $r_{t}\\,>\\,0$ for all $t\\,>\\,0$ . From Eq. (46), it follows that: $\\begin{array}{r}{r_{t+1}\\,\\leq\\,r_{t}\\,-\\,\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}r_{t}^{2\\theta}}\\end{array}$ . Since $\\textstyle\\operatorname*{lim}_{t\\to\\infty}r_{t}=0$ , there must exist a $t_{0}^{\\prime}$ such that, $r_{T}^{2\\theta}\\geq r_{T}$ hold for all $T>t_{0}^{\\prime}$ , and equivalently, $\\begin{array}{r}{r_{T+1}\\leq(1\\!-\\!\\frac{\\rho\\!-\\!L}{\\rho^{2}c^{2}(1-\\theta)^{2}})r_{T}}\\end{array}$ . This further implies that $\\begin{array}{r}{r_{T}\\leq\\big(1\\!-\\!\\frac{\\rho\\!-\\!L}{\\rho^{2}c^{2}(1-\\theta)^{2}}\\big)^{T-t_{0}^{\\prime}}r_{t_{0}^{\\prime}}}\\end{array}$ . Now consider another case that there exists a $t_{0}$ such that $r_{t}=0$ for all $T>t_{0}$ , following the same analysis given in the previous case we reach the same result $r_{T}=0$ holds for all sufficiently large $T\\geq t_{0}$ . These together with Eq. (38) implying that for a sufficiently large $T>t_{0}^{\\prime}$ , $\\lVert\\nabla f(\\tilde{\\pmb{w}}_{T})+\\nabla h(\\pmb{w}_{T})\\rVert\\leq$ $\\begin{array}{r}{\\operatorname*{max}(\\frac{\\sqrt{2}\\rho}{\\sqrt{\\rho-L}}\\cdot\\sqrt{\\big(1-\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}\\big)^{T-t_{0}^{\\prime}}r_{t_{0}^{\\prime}}},0\\big)\\leq\\frac{\\sqrt{2}\\rho}{\\sqrt{\\rho-L}}\\sqrt{\\big(1-\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}\\big)^{T-t_{0}^{\\prime}}r_{t_{0}^{\\prime}}}.}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "\u2022 Finally, suppose $\\mathcal{D}(\\tilde{\\pmb{w}}_{t},\\pmb{w}_{t})$ satisfies the KL property with $\\textstyle\\theta\\in({\\frac{1}{2}},1)$ . We first evaluate the case that $r_{t}\\,>\\,0$ for all $t\\,>\\,0$ . Define a continuous non-increasing function $g:\\,(0,+\\infty)\\,\\rightarrow\\,\\mathbb{R}$ by $g(x)=x^{-2\\theta}$ . Plugging this definition into Eq. (46), we have $\\begin{array}{r}{\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}\\leq(r_{t}-r_{t+1})g(r_{t})\\leq}\\end{array}$ $\\begin{array}{r}{\\int_{r_{t+1}}^{r_{t}}g(x)d x=\\frac{r_{t+1}^{1-2\\theta}-r_{t}^{1-2\\theta}}{2\\theta-1}}\\end{array}$ holds for all $t\\geq0$ . Since $2\\theta-1>0$ , we have $r_{t+1}^{1-2\\theta}-r_{t}^{1-2\\theta}\\le$ $\\begin{array}{r}{(2\\theta-1)\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}}\\end{array}$ . Summing from $t=0$ to $t=T-1$ , we have $\\begin{array}{r}{r_{T}\\leq\\sqrt[1-2\\theta]{T(2\\theta-1)\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}}}\\end{array}$ Moreover, same as the previous analysis, we have $r_{T}$ for all $t\\geq0$ . Thus, these together with Eq. (38) show that for T \u22650, \u2225\u2207f( w\u02dcT ) + \u2207h(wT )\u2225\u2264max(\u221a\u03c12\u2212\u03c1L $\\begin{array}{r}{||\\nabla f(\\tilde{w}_{T})+\\nabla h(w_{T})||\\leq\\operatorname*{max}(\\frac{\\sqrt{2}\\rho}{\\sqrt{\\rho-L}}\\mathbf{\\sigma}^{2-4\\theta}\\!\\!\\left/T(2\\theta-1)\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}},0\\right)\\leq\\!\\!\\!\\!\\!\\!\\!\\!}\\end{array}$ $\\begin{array}{r}{\\frac{\\sqrt{2}\\rho}{\\sqrt{\\rho-L}}\\overset{2-4\\theta}{\\sqrt{T(2\\theta-1)\\frac{\\rho-L}{\\rho^{2}c^{2}(1-\\theta)^{2}}}}.}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "C Broader Impact ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The studied harmful fine-tuning attack is parallel with jail-break attack (Qi et al., 2024a; Carlini et al., 2024; Zou et al., 2023; Chao et al., 2023; Wei et al., 2024a), in which the user can elicit harmful behaviors of the model by adding adversary input. Study on both of these attacks help us better understand the potential safety risk of LLMs, and therefore posing positive social impact. From the author\u2019s perspective, our research itself should not pose negative societal impacts as the proposed solutions are devoted to promoting the safety alignment of large language models. However, we are aware that the examples in Section 5.6 and Appendix A.4, which are generated by LLMs, may contain unethical items that contain negative societal risk. ", "page_idx": 28}, {"type": "text", "text": "D Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The paper has a few limitations that are not addressed. The main concern is that the solution requires additional computation for the fine-tuning stage. This is undesirable compared to an alignment stage solution, because fine-tuning needs to be done for every incoming user request, and thus the overhead will scale with a number of requests. Secondly, our solution only be applied on top of an SFT solution. However, RHLF-based technique (Ouyang et al., 2022) is currently the SOTA technique for safety alignment. Due to resource limitations, we are not able to verify whether the proposed method can also be generalized to a RLHF-based technique. Another weakness is that downstream dataset we use is not the \"coolest\" task that LLM can perform. Future optimization may include applying the technique to more downstream scenarios, e.g., conversational AI (Wu et al., 2023a) or agents for different tasks (Hu et al., 2023a, 2024b, 2023b, 2024a). ", "page_idx": 28}, {"type": "text", "text": "To completely solve the problem of harmful fine-tuning, efforts have to be made in both the two stages, i.e., alignment and user fine-tuning. We generally believe that an alignment stage solution is more desirable from a system and security perspective but the technical challenges will be larger. For our future works, we will continue working on both the two stages to form an improved solution. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The abstract reflects our contribution. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the limitation in Appendix D. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The theoretical assumptions and proof are available in Appendix B. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: It is available in Appendix A.1. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide the code in an anonymous repo. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: It is available in Appendix A.1. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Hyper-parameters analysis and repetitive experiments in different settings are available. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: It is available in Appendix A.1. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: It is available in Appendix C. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: No such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The license and terms of use are properly respected. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No new assets introduced except code. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No human involved research. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]