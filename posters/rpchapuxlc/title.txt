Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack