{"references": [{"fullname_first_author": "Dhariwal", "paper_title": "Diffusion models beat gans on image synthesis", "publication_date": "2021-12-12", "reason": "This paper is foundational for the field of diffusion models which are central to the work in this paper."}, {"fullname_first_author": "Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-20", "reason": "This paper is highly influential as it introduced a practical and effective method for generating high-resolution images via diffusion models."}, {"fullname_first_author": "Brooks", "paper_title": "Instructpix2pix: Learning to follow image editing instructions", "publication_date": "2023-06-22", "reason": "This paper directly inspired the current work, introducing instruction-following in image editing using diffusion models."}, {"fullname_first_author": "Karras", "paper_title": "Elucidating the design space of diffusion-based generative models", "publication_date": "2022-12-12", "reason": "This paper provides a comprehensive analysis of diffusion models, offering valuable insights into design choices and improving understanding of the field."}, {"fullname_first_author": "Chen", "paper_title": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2024-06-20", "reason": "This paper is highly relevant because it presents a large vision-language model that is leveraged for the auxiliary prompt module which contributes to the overall performance of the proposed model."}]}