[{"figure_path": "7V62sQ5Jra/figures/figures_5_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure compares different methods for constructing rank-sets of LLMs.  The x-axis shows the baseline intersection probability (how often the rank-sets constructed by a method overlap with those constructed using human judgments alone). The y-axis represents the average size of the constructed rank-sets.  Methods using only strong LLMs show lower intersection probability and larger rank-sets (more uncertainty) than methods combining strong LLMs and human judgments.  The results suggest that incorporating human input, even a small amount, significantly improves the accuracy and reliability of LLM ranking.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_7_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of  \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure displays the trade-off between the average size of the rank-sets and the baseline intersection probability for different methods of constructing rank-sets for 12 LLMs.  The methods compared are using only human pairwise comparisons, only strong LLMs' pairwise comparisons, and a combination of both. Smaller rank-sets and higher intersection probabilities indicate better results.  The figure shows that combining human and strong LLM data produces more accurate rank-sets than using only strong LLM data.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_7_2.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure compares different methods for constructing rank-sets of LLMs, using pairwise comparison data from both humans and strong LLMs.  The x-axis represents the baseline intersection probability (a measure of how well the rank-set aligns with a human-only baseline), and the y-axis is the average rank-set size. Smaller rank-sets and higher baseline intersection probabilities indicate better performance. The figure demonstrates that combining human and strong LLM data yields better results (PPR methods) than using only strong LLM data (LLM methods). Human-only data (HUMAN ONLY) is also shown for comparison.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_8_1.jpg", "caption": "Figure 3: Empirical probability that each ranking position is included in the rank-sets constructed by BASELINE, LLM GPT4 and PPR GPT4 for each of the LLMs under comparison. In all panels, n = 990 and a = 0.05. Larger (smaller) dots indicate higher (lower) empirical probability.", "description": "This figure compares the empirical probability of each ranking position being included in the rank-sets generated by three different methods: BASELINE (using human and strong LLM comparisons), LLM GPT4 (using only GPT-4 comparisons), and PPR GPT4 (combining human and GPT-4 comparisons).  The size of each dot represents the probability, with larger dots indicating higher probability. The results are shown for each of the 12 LLMs being evaluated.  The figure aims to demonstrate that the method combining human and strong LLM data produces rank-sets that more accurately represent the true human ranking than using only a strong language model.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_8_2.jpg", "caption": "Figure 3: Empirical probability that each ranking position is included in the rank-sets constructed by BASELINE, LLM GPT4 and PPR GPT4 for each of the LLMs under comparison. In all panels, n = 990 and a = 0.05. Larger (smaller) dots indicate higher (lower) empirical probability.", "description": "This figure compares the empirical probability of each ranking position being included in the rank-sets generated by three different methods: BASELINE (using only human pairwise comparisons), LLM GPT4 (using only GPT-4 pairwise comparisons), and PPR GPT4 (using both human and GPT-4 pairwise comparisons).  The results visualize the uncertainty in ranking positions for each LLM across the different methods, with larger dots representing higher probability. The parameters n=990 and \u03b1=0.05 were used for all methods.", "section": "Structure of the rank-sets"}, {"figure_path": "7V62sQ5Jra/figures/figures_16_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure displays the trade-off between the average size of rank-sets and their intersection probability with a baseline.  The rank-sets were generated using different methods: only strong LLMs, only human comparisons, and a combination of both. Smaller rank-sets and higher intersection probability indicate better ranking performance, with the combined human and LLM approach generally outperforming methods using only one data source.", "section": "5 Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_17_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "The figure displays the average rank-set size plotted against the baseline intersection probability for different methods of constructing rank-sets.  Each method uses a different combination of human and strong LLM pairwise comparisons.  Smaller rank-set sizes and higher intersection probabilities indicate better performance.  The results show that combining human and strong LLM comparisons yields better results than using only strong LLM comparisons.", "section": "5 Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_18_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure compares different methods for constructing rank sets of LLMs.  It shows the tradeoff between average rank-set size (uncertainty) and baseline intersection probability (how often the rank set covers the true ranking).  Smaller rank-sets and higher intersection probabilities are better. The methods compared include using only human pairwise comparisons, only strong LLM comparisons, and combining both. The results show that combining human and strong LLM comparisons generally performs better than using only one type of comparison.", "section": "5 Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_20_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure shows the trade-off between the average size of rank-sets and the baseline intersection probability for different methods of constructing rank-sets.  The methods compared include using only pairwise comparisons from a strong language model, only human comparisons, and a combination of both.  Smaller rank-sets and higher baseline intersection probabilities are preferred, indicating better ranking performance.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_21_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of a and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure compares different methods for constructing rank sets of LLMs.  The x-axis represents the baseline intersection probability (how often the rank sets overlap with those generated using only human comparisons), and the y-axis represents the average rank-set size.  Lower average sizes and higher intersection probabilities indicate better performance.  The results show that using a combination of human and strong LLM pairwise comparisons produces better rank sets than using only strong LLM or only human data.", "section": "5 Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_21_2.jpg", "caption": "Figure 6: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM CL3 and LLM GPT3.5), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4 top, PPR CL3 middle and PPR GPT3.5 bottom) for different a values and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure shows the relationship between the average size of rank-sets and their baseline intersection probability for different methods of constructing rank-sets. The methods include using pairwise comparisons from only a strong Language Model (LLM), only humans, and a combination of both. The results show that using both human and strong LLM comparisons leads to a better balance between rank-set size and intersection probability, suggesting that combining both sources of information is beneficial.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_22_1.jpg", "caption": "Figure 3: Empirical probability that each ranking position is included in the rank-sets constructed by BASELINE, LLM GPT4 and PPR GPT4 for each of the LLMs under comparison. In all panels, n = 990 and a = 0.05. Larger (smaller) dots indicate higher (lower) empirical probability.", "description": "This figure compares the empirical probability of each ranking position being included in the rank-sets generated by three different methods: BASELINE (using human pairwise comparisons), LLM GPT4 (using only GPT-4's pairwise comparisons), and PPR GPT4 (combining human and GPT-4 pairwise comparisons).  The results show the uncertainty associated with each model's ranking position across the three methods, revealing insights into how the reliability of the ranking changes depending on the data used (human judgments versus model-generated preferences). The size of the dots represents the probability, with larger dots signifying higher probability.", "section": "Structure of the rank-sets"}, {"figure_path": "7V62sQ5Jra/figures/figures_23_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure compares different methods for constructing rank-sets of LLMs, showing the trade-off between average rank-set size and baseline intersection probability.  It demonstrates that using a combination of human and strong LLM pairwise comparisons (PPR) leads to smaller rank-sets and higher intersection probabilities than using only strong LLMs or only humans. The results suggest that incorporating some human judgment improves ranking accuracy and reduces uncertainty.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_24_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure compares different methods for constructing rank sets of LLMs.  The x-axis represents the baseline intersection probability, indicating how often the rank sets constructed by a given method overlap with those generated using human comparisons. The y-axis shows the average size of the rank sets.  Methods using only strong LLMs show smaller intersection probability and larger rank sets, while methods incorporating both human and strong LLM data demonstrate better results with smaller rank sets and higher intersection probabilities.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_25_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure compares different methods for constructing rank-sets of LLMs.  The x-axis represents the baseline intersection probability (how often rank sets from the method overlap with rank sets from the human-only baseline). The y-axis is the average rank-set size.  Smaller rank-sets and higher baseline intersection probabilities indicate better performance.  The methods include using only a strong LLM's comparisons, only human comparisons, and a combination of both.  The results show that combining human and strong LLM data generally outperforms using only strong LLM data.", "section": "5 Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_26_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure compares different methods for constructing rank-sets of LLMs, based on pairwise comparisons from humans and strong LLMs.  It shows the trade-off between the average size of the rank-sets and their intersection probability with a baseline (human-only) ranking. Smaller rank-sets and higher intersection probabilities indicate better performance. The results highlight that combining human and strong LLM comparisons (PPR methods) generally yields better results than using only strong LLM comparisons (LLM methods), showing the benefit of incorporating human data.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_27_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure compares different methods for constructing rank-sets of LLMs based on pairwise comparisons from humans and strong LLMs.  It plots the average rank-set size against the baseline intersection probability. Smaller rank-sets and higher intersection probabilities indicate better performance. The results show that using a combination of human and strong LLM comparisons yields better results than using strong LLMs alone.", "section": "5 Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_28_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "This figure displays the average rank-set size plotted against the baseline intersection probability for different methods of constructing rank-sets.  The methods vary in how they utilize pairwise comparisons from humans and strong LLMs. Smaller rank-set sizes and higher intersection probabilities indicate better performance.  The results show that combining human and strong LLM comparisons generally outperforms using strong LLM comparisons alone, particularly in terms of having smaller rank-sets and higher baseline intersection probabilities.", "section": "Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_30_1.jpg", "caption": "Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (LLM GPT4, LLM GPT3.5 and LLM CL3), only pairwise comparisons by humans (HUMAN ONLY), and pairwise comparisons by both a strong LLM and humans (PPR GPT4, PPR GPT3.5 and PPR CL3) for different values of \u03b1 and n = 990. Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, 95% confidence bars for the rank-set size are not shown, as they are always below 0.02.", "description": "The figure displays a comparison of rank-set size and baseline intersection probability across various methods of constructing rank-sets for LLMs.  It evaluates the methods using only human pairwise comparisons, only strong LLMs' pairwise comparisons, and a combination of both. Smaller rank-sets and higher baseline intersection probabilities are preferred, indicating better accuracy in ranking the LLMs.", "section": "5 Experiments"}, {"figure_path": "7V62sQ5Jra/figures/figures_30_2.jpg", "caption": "Figure 12: Average rank-biased overlap (RBO) of rankings constructed by ordering the empirical win probabilities \ud835\udf03 estimated using only N + n synthetic pairwise comparisons by one out of three different simulated strong LLMs (LLM 0.05, LLM 0.1 and LLM 0.3), only n synthetic pairwise comparisons by humans (HUMAN ONLY), and both n synthetic pairwise comparisons by humans and N + n synthetic pairwise comparisons by one out of the same three strong LLMs (PPR 0.05, PPR 0.1 and PPR 0.3) for a = 0.1 and N + n = 50000. Each of the strong LLMs has a different level of alignment with human preferences controlled by a noise value u \u2208 {0.05,0.1, 0.3}. RBO was computed with respect to the true ranking constructed by ordering the true win probabilities \ud835\udf03, for p = 0.6. The shaded region shows a 95% confidence interval for the RBO among all 300 repetitions.", "description": "This figure compares the rank-biased overlap (RBO) values for different methods of ranking LLMs using synthetic data. The methods are using only human comparisons, only strong LLM comparisons, and combining human and strong LLM comparisons. The results show that incorporating human feedback significantly improves RBO, and better performance when strong LLMs are more closely aligned with human preferences.", "section": "E Synthetic Experiments"}]