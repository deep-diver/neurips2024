[{"type": "text", "text": "Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sattar Vakili Julia Olkhovskaya MediaTek Research TU Delft sattar.vakili@mtkresearch.com julia.olkhovskaya@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning utilizing kernel ridge regression to predict the expected value function represents a powerful method with great representational capacity. This setting is a highly versatile framework amenable to analytical results. We consider kernel-based function approximation for RL in the infinite horizon average reward setting, also referred to as the undiscounted setting. We propose an optimistic algorithm, similar to acquisition function based algorithms in the special case of bandits. We establish novel no-regret performance guarantees for our algorithm, under kernel-based modelling assumptions. Additionally, we derive a novel confidence interval for the kernel-based prediction of the expected value function, applicable across various RL problems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) has demonstrated substantial practical success across a variety of application domains, including gaming (Silver et al., 2016; Lee et al., 2018; Vinyals et al., 2019), autonomous driving (Kahn et al., 2017), microchip design (Mirhoseini et al., 2021), robot control (Kalashnikov et al., 2018), and algorithmic search (Fawzi et al., 2022). This empirical success has prompted deeper investigations into the analytical understanding of RL, especially in complex environments. Over the past decade, significant advances have been made in establishing theoretically grounded algorithms for various settings. In this work, we focus on the infinite horizon average reward setting, also known as the undiscounted setting (Wei et al., 2020, 2021). This setting is particularly well-suited for applications that involve continuing operations not divided into episodes such as load balancing and stock market operations. In contrast to the episodic setting (Jin et al., 2020) and the discounted setting (Zhou et al., 2021), theoretical understanding of RL algorithms is relatively limited for the undiscounted setting. We develop a computationally efficient algorithm and establish its theoretical performance guarantees in the undiscounted case. ", "page_idx": 0}, {"type": "text", "text": "There is a natural progression in the complexity of RL models corresponding to the structural complexity of the Markov Decision Process (MDP). This progression ranges from tabular models to linear, kernel-based, and deep learning-based models. The kernel-based structure is an extension of linear structure to an infinite-dimensional linear model in the feature space of a positive definite kernel, resulting in a highly versatile model with great representational capacity for nonlinear functions. In addition, the closed-form expressions for the prediction and the uncertainty estimate in kernel-based models allow the development of algorithms based on nonlinear function approximation that are amenable to theoretical analysis. Kernel-based models also serve as an intermediate step towards understanding the deep learning-based models (see, e.g., Yang et al., 2020) based on the Neural Tangent (NT) kernel approach (Jacot et al., 2018). ", "page_idx": 0}, {"type": "text", "text": "The infinite-horizon average-reward setting has been extensively explored under the tabular structure (Auer et al., 2008; Wei et al., 2020; Zhang and Xie, 2023). Under the performance measure of regret, defined as the difference in the total reward achieved by a learning algorith\u221am over $T$ steps and that of the optimal stationary policy, performance bounds of $\\mathcal{O}(\\mathrm{poly}(|S|,|A|)\\sqrt{T})$ have been established (see, e.g., Zhang et al., 2020), where $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ represent the state and action spaces, respectively, and the regret grows polynomial with their sizes. It is assumed for these results that the MDP is weakly communicating, a condition necessary for achieving sublinear regret (Bartlett and Tewari, 2009). Averaged over $T$ steps, the regret diminishes as $T$ increases, thereby offering what is known as a no-regret performance guarantee. The applicability of the tabular setting is limited, as many real-world problems feature very large or potentially infinite state-action spaces. Consequently, recent literature has explored the use of function approximation in RL, particularly through linear models (Abbasi-Yadkori et al., 2019a,b; Hao et al., 2021; Wei et al., 2021). This approach represents the value function or the transition model via a linear transformation applied to a predefined feature mapping. In the linear setting, regret bounds of ${\\mathcal{O}}((d T)^{\\frac{3}{4}})$ have been established (Wei et al., 2021), where $d$ represents the ambient dimension of the linear feature map. Kernel-based models can be considered as linear models in the feature space of the kernel. That, however, is often infinite dimensional $d=\\infty$ ). As such, the results with linear models do not translate to the kernel-based settings, necessitating novel analytical techniques. Also, for a discussion on further limitations of the linear models, see Lee and Oh (2023). ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose the first RL algorithm in the infinite horizon average reward setting with non-linear function approximation using kernel-ridge regression. This is one of the most general models that lends well to theoretical analysis. Our algorithm, referred to as Kernel-based Upper Confidence Bound (KUCB-RL), utilizes kernel ridge regression to build predictor and uncertainty estimates for the expected value function. Inspired by the principle of optimism in the face of uncertainty and equipped with these statistics, KUCB-RL builds an upper confidence bound on the state-action value function over a future window of $w$ steps. This bound serves as a proxy $q_{t}$ , at each step $t$ , for the state-action value function over this future window. At each step $t$ with the current state $s_{t}$ , the action is selected greedily with respect to this proxy: $a_{t}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}q_{t}(s_{t},a)$ This approach resembles the acquisition function based algorithms such as GP-UCB and GP-TS, using Upper Confidence Bound and Thompson sampling, respectively, in the context of kernel-based bandits, also known as Bayesian optimization (Srinivas et al., 2010; Chowdhury and Gopalan, 2017). Kernel-based bandit setting corresponds to the degenerate case of $|{\\cal S}|=1$ . In comparison, in the RL setting, the action is selected based on the current state, and the reward depends on both the state and the action. A kernel-based model is used to provide predictions for the expected value function, which varies due to the Markovian nature of the temporal dynamics. This makes the RL problem significantly more challenging than the bandit problem where the predictions are derived for a fixed reward function. To address this latter challenge, we derive a novel kernel-based confidence interval that is applicable across RL problems. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To summarize, our contributions are as follows. We develop a kernel based optimistic algorithm for the infinite horizon average reward setting, referred to as KUCB-RL. We establish no-regret guarantees for the proposed learning algorithm, which is the first for this setting to the best of our knowledge. Specifically, in Theorem 3, we prove a regret bound of $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{T}{w}+\\left(w+\\frac{w}{\\sqrt{\\rho}}\\sqrt{\\gamma(T;\\rho)+\\log(\\frac{T}{\\delta})}\\right)\\sqrt{\\rho T\\gamma(T;\\rho)+\\rho^{2}w^{2}\\gamma(T;\\rho)\\gamma(T/w;\\rho)}\\right)}\\end{array}$ , at a $1-\\delta$ confidence level, where $\\rho$ is the parameter of kernel ridge regression and $\\gamma(T;\\rho)$ is the maximum information gain, a kernel specific complexity term (see Section 2). This regret bound translates to $\\tilde{\\mathcal{O}}\\left(d^{{\\frac{1}{2}}}T^{{\\frac{3}{4}}}\\right)$ in the special case of a linear model, recovering the best existing results (Wei et al., 2021) in dependence on $T$ , and improving by a factor of $d^{\\frac{1}{4}}$ . When applied to very smooth kernels with exponential eigendecay such as the Squared Exponential (SE) kernel, we obtain a regret of $\\tilde{\\mathcal{O}}(T^{\\frac{3}{4}})$ , with the notation $\\tilde{\\mathcal{O}}$ hiding logarithmic factors. For one of the most general cases, the kernels with polynomial eigendecay with parameter $p>1$ (See Definition 1), that includes, for example, the Mat\u00e9rn family and NT kernels, we show that our regret bound translates to $\\tilde{\\mathcal{O}}(T^{\\frac{3p+5}{4p+4}})$ , which constitutes a no-regret guarantee. To highlight the significance of this result, we point out that no-regret guarantees for GP-UCB in the degenerate case of bandits were established only recently in Whitehouse et al. (2024), while the initial studies of GP-UCB (as well as GP-TS) (Srinivas et al., 2010; Chowdhury and Gopalan, 2017) did not provide no-regret guarantees for the case of polynomial eigendecay. As part of our analysis, in Theorem 1, we develop a novel confidence interval applicable across kernel-based RL problems that contributes to the eventual improved results. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The vast RL literature can be categorized across various dimensions. In addition to the average reward, episodic, and discounted settings, as well as tabular, linear, and kernel-based structures mentioned above, other notable distinctions among settings include model-based versus model-free approaches, and offline versus online versus settings where the existence of a generative model is assumed (allowing the learning algorithm to sample the state-action of its choice at each step, rather than following the Markovian trajectory). Covering the entire breadth of RL literature is challenging. Here, we will focus on highlighting and providing comparisons with the most closely related works, particularly in terms of their setting and structure. ", "page_idx": 2}, {"type": "text", "text": "The kernel-based MDP structure has been considered in several recent works under the episodic setting (Yang et al., 2020; Vakili and Olkhovskaya, 2023; Chowdhury and Oliveira, 2023; Domingues et al., 2021; Vakili, 2024). The regret bound proven in Yang et al. (2020) for the episodic setting applies only to very smooth kernels such as SE kernel. Vakili and Olkhovskaya (2023) addressed this limitation by extending the results to Mat\u00e9rn and NT families of the kernels, albeit with a sophisticated algorithm that actively partitions the state-action domain into possibly many subdomains, using only the observations within each subdomain to obtain kernel-based prediction and uncertainty estimates. Their work is also based on a particular assumption that relates the kernel eigenvalues to the size of the domain. The work of Chowdhury and Oliveira (2023) is most closely related to ours in terms of kernel-related assumptions. Specifically, our Assumption 4 is identical\u221a to Assumption 1 of Chowdhury and Oliveira (2023). They establish a regret bound of $\\mathcal{O}(H\\gamma(N;\\rho)\\sqrt{N})$ for the episodic MDP setting, where $N$ is the number of episodes, $\\gamma(N;\\rho)$ is the maximum information gain, a kernel-related complexity term, $H$ is the episode length and the value of $\\rho$ is a fixed constant close to 1. However, their regret bounds do not apply to general families of kernels, such as those with polynomially decaying eigenvalues (see Section 2.2 for the definition) including Mat\u00e9rn and NT kernels, as for this family of kernels $\\gamma(N;\\rho)$ possibly grows faster than $\\sqrt{N}$ . As a result, a no-regret guarantee cannot be established in many cases of interest. In comparison, the infinite horizon setting considered in this work is more challenging than the episodic setting as evident when comparing these settings with linear modeling. For this more challenging setting, we establish no-regret guarantees. A key element of our improved results is the novel confidence interval we utilize in our analysis (Theorem 1). This result is general and can be used across RL problems, for example, improving the results of Chowdhury and Oliveira (2023) as well. ", "page_idx": 2}, {"type": "text", "text": "In the tabular case, a lower bound of $\\Omega({\\sqrt{D|S||A|T}})$ on regret was established in Auer et al. (2008) in the infinite-horizon average-reward setting, where $D$ is the diameter of the MDP. For ergodic MDPs, Wei et al. (2020) shows a regret bound of $\\tilde{\\mathcal{O}}(\\sqrt{t_{\\mathrm{mix}}^{3}|S||A|T})$ , where $t_{\\mathrm{mix}}$ is the mixing time of an ergodic MDP. Furthermore, under the broader assumption of weakly communicating MDPs, which is necessary for low regret (Bartlett and Tewari, 2012), the best existing regret bound of model-free algorithms is $\\tilde{\\mathcal{O}}(|S|^{5}|A|^{2}\\sqrt{T})$ , achieved by the recent work of Zhang and Xie (2023). Several works have studied linear function approximation in the infinite horizon average reward setting under strong assumptions of uniformly mixing and uniformly excited feature conditions (Abbasi-Yadkori et al., 2019a,b; Hao et al., 2021). Notably, Hao et al. (2021) achieved a regret bound of $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(\\frac{1}{\\sigma}\\sqrt{t_{\\mathrm{mix}}^{3}T}\\right)}\\end{array}$ under the linear bias function assumption, where $\\sigma$ is the smallest eigenvalue of policy-weighted covariance matrix. Under the much less restrictive setting of Bellman optimality equation assumption (Assumption 1) for linear MDP, Wei et al. (2021) provides an algorithm with regret guarantee of $\\tilde{\\mathcal{O}}((d T)^{3\\bar{/}4})$ . We also consider our kernel-based approach under this general assumption on MDP. Furthermore, for examples of infeasible algorithms in the literature, see Wei et al. (2021), Algorithm 1. There also exists a separate model-based approach to the problem where the transition probability distribution (model) is learned and used for planning, usually requiring high memory and computational complexity and utilizing substantially different techniques and assumptions. While this approach is studied under tabular settings (Bartlett and Tewari, 2009; Auer et al., 2008) and linear settings (Wu et al., 2022), it is not clear whether model-based approaches can be feasibly constructed in the kernel-based setting, due to the space complexity of a kernel-based model. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Our work is also related to the simpler problem of kernelized bandits (Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Vakili et al., 2021b; Li and Scarlett, 2022; Salgia et al., 2021). Our construction of the confidence interval for the RL setting has been inspired by the previous work on bandits, utilizing novel analysis introduced in Whitehouse et al. (2024). Bandit settings can be considered a degenerate case of the RL framework with $|{\\cal S}|=1$ . In comparison, the temporal dependencies of MDP introduce substantial challenges, and the confidence intervals used in the bandit setting cannot be directly applied. ", "page_idx": 3}, {"type": "text", "text": "We summarize the most closely related work with a focus on model-free feasible algorithms in Table 1. We present the existing regret bounds under various assumptions on MDP and its structure (tabular, linear, kernel-based). The assumptions include weakly communicating MDP (See Puterman, 1990, Section 8.3.1), Bellman optimality equation (our Assumption 1), and uniform mixing assumption (see Wei et al., 2021, Assumption 3). For a formal definition of linear MDP, see Wei et al. (2021), Assumption 2, and for the linear bias function case, see Wei et al. (2021), Assumption 4. ", "page_idx": 3}, {"type": "table", "img_path": "VwUTz2pOnD/tmp/6f76c150bb3d2b2fc2409a5add87575d3ab785ed1d5c28cb5f0cd2d500b75012.jpg", "table_caption": ["Table 1: Summary of the existing regret bounds in the infinite horizon average reward setting under various cases with respect to MDP structure (tabular, linear, kernel based) and assumptions. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we overview the background on infinite horizon average reward (undiscounted) MDPs and kernel based modelling. ", "page_idx": 3}, {"type": "text", "text": "2.1 Infinite Horizon Average Reward MDP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "An undiscounted MDP is described by the tuple $(S,{\\mathcal{A}},r,P)$ where $\\boldsymbol{S}$ is a state space with a possibly infinite number of elements, $\\boldsymbol{\\mathcal{A}}$ is a finite action set, $r:S\\times A\\to[0,1]$ is the reward function, and $P(\\cdot|s,a)$ is the unknown transition probability distribution over $\\boldsymbol{S}$ of the next state when action $a$ is selected at state $s$ . Throughout the paper we use the notation $z=(s,a)$ for the state-action pairs, and $\\mathcal{Z}=\\mathcal{S}\\times\\mathcal{A}$ . ", "page_idx": 3}, {"type": "text", "text": "The learner interacts with the MDP through $T$ steps, starting from an arbitrary initial state $s_{1}\\in\\mathcal{S}$ . At each step $t$ , the learner observes state $s_{t}$ and takes an action $a_{t}$ resulting in a reward $\\boldsymbol{r}(\\boldsymbol{s}_{t},\\boldsymbol{a}_{t})$ . The next state $s_{t+1}$ is revealed as a sample drawn from the transition probability distribution: $s_{t+1}\\sim P(\\cdot|s_{t},a_{t})$ . ", "page_idx": 3}, {"type": "text", "text": "The goal of the learner is to compete against any fixed stationary policy. A stationary policy $\\pi:{\\mathcal{S}}\\rightarrow A$ is a possibly random mapping from the states to actions. The long-term average reward of a stationary policy $\\pi$ , starting from state $s\\in S$ , is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ^{\\pi}(s)=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\mathbb{E}\\left[\\sum_{t=1}^{T}r(s_{t},a_{t})\\Bigg|\\,s_{1}=s,\\forall t\\geq1,a_{t}=\\pi(s_{t}),s_{t+1}\\sim P(\\cdot|s_{t},a_{t})\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We assume that the MDP belongs to the broad class of MDPs where the following form of the Bellman optimality equation holds: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Bellman optimality equation). There exists $J^{\\star}\\ \\in\\ \\mathbb{R}$ and bounded measurable functions $v^{\\star}:S\\rightarrow\\mathbb{R}$ and $q^{\\star}:S\\times A\\rightarrow\\mathbb{R}$ such that the following conditions are satisfied for all ", "page_idx": 3}, {"type": "text", "text": "states $s\\in S$ and actions $a\\in A$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ^{\\star}+q^{\\star}(s,a)=r(x,a)+\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[v^{\\star}(s^{\\prime})\\right],\\quad v^{\\star}(s)=\\operatorname*{max}_{a\\in\\mathcal{A}}q^{\\star}(s,a).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This assumption was also used for the linear MDP case in Wei et al. (2021). By applying the Bellman optimality equation, it can be shown that a policy $\\pi^{\\star}(s)=\\ \\arg\\operatorname*{max}_{a\\in\\mathcal{A}}q^{\\star}(s,a)$ , which deterministically selects actions that maximize $q^{\\star}$ in the current state, is the optimal policy $\\pi^{\\star}=$ arg $\\operatorname*{max}_{\\pi}J^{\\pi}$ , with $J^{\\pi^{\\star}}(s)=J^{\\star}$ , for all $s$ (Wei et al., 2021). ", "page_idx": 4}, {"type": "text", "text": "For the finite state setting, Assumption 1 follows from the weakly communicating MDP assumption (see, e.g., Puterman, 1990, Chapter 9). Assumption 1 also holds under several other common conditions (Hern\u00e1ndez-Lerma (2012), Section 3.3). ", "page_idx": 4}, {"type": "text", "text": "The learner\u2019s performance is measured by regret, which is defined as the difference in total reward between the learner and the optimal stationary policy. Specifically, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(T)=\\sum_{t=1}^{T}(J^{\\star}-r(s_{t},a_{t})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We emphasize that under Assumption 1, for any initial state $s_{1}\\in S,J^{\\pi^{\\star}}(s_{1})=J^{\\star}$ , that is reflected in our regret definition. ", "page_idx": 4}, {"type": "text", "text": "For any value function $v:S\\rightarrow\\mathbb{R}$ , throughout the paper, we use the notation ", "page_idx": 4}, {"type": "equation", "text": "$$\n[P v](z)=\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|z)}[v(s^{\\prime})]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for the expected value function of the next state. ", "page_idx": 4}, {"type": "text", "text": "2.2 Kernel-Based Models and the RKHS ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Consider a positive definite kernel $k:\\mathcal{Z}\\times\\mathcal{Z}\\to\\mathbb{R}$ . Let $\\mathcal{H}_{k}$ be the reproducing kernel Hilbert space (RKHS) induced by $k$ , where $\\mathcal{H}_{k}$ contains a family of functions defined on $\\mathcal{Z}$ . Let $\\langle\\cdot,\\cdot\\rangle_{\\mathscr{H}_{k}}:$ $\\mathcal{H}_{k}\\times\\mathcal{H}_{k}\\rightarrow\\mathbb{R}$ and $\\parallel\\cdot\\parallel\\!\\!\\pi_{k}:\\mathcal{H}_{k}\\to\\mathbb{R}$ denote the inner product and the norm of $\\mathcal{H}_{k}$ , respectively. The reproducing property implies that for all $f\\in\\mathcal{H}_{k}$ , and $z\\in{\\mathcal{Z}}$ , $\\langle f,k(\\cdot,z)\\rangle_{\\mathcal{H}_{k}}=f(z)$ . Mercer theorem implies that $k$ can be represented using a possibly infinite dimensional feature map: ", "page_idx": 4}, {"type": "equation", "text": "$$\nk(z,z^{\\prime})=\\sum_{m=1}^{\\infty}\\lambda_{m}\\varphi_{m}(z)\\varphi_{m}(z^{\\prime}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{m}>0$ , and $\\sqrt{\\lambda_{m}}\\varphi_{m}\\in\\mathcal{H}_{k}$ form an orthonormal basis of $\\mathcal{H}_{k}$ . In particular, any $f\\in\\mathcal{H}_{k}$ can be represented using this basis and weights $w_{m}\\in\\mathbb{R}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nf=\\sum_{m=1}^{\\infty}w_{m}\\sqrt{\\lambda_{m}}\\varphi_{m},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\textstyle\\|f\\|_{\\mathcal{H}_{k}}^{2}=\\sum_{m=1}^{\\infty}w_{m}^{2}$ . A formal statement and the details are provided in Appendix 8. We refer to $\\lambda_{m}$ and $\\varphi_{m}$ as (Mercer) eigenvalues and eigenfunctions of kernel $k$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "2.3 Kernel-Based Prediction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Kernel-based models provide powerful predictors and uncertainty estimators which can be leveraged to guide the RL algorithm. In particular, consider a fixed unknown function $f\\in\\mathcal{H}_{k}$ . Assume a $t\\times1$ vector of noisy observations $\\pmb{\\dot{y}}_{t}=[y_{i}=f(z_{i})+\\varepsilon_{i}]_{i=1}^{t}$ at observation points $\\{z_{i}\\}_{i=1}^{t}$ is provided, where $\\varepsilon_{i}$ are independent zero mean noise terms. Kernel ridge regression provides the following predictor and uncertainty estimate, respectively (see, e.g., Sch\u00f6lkopf et al., 2002), ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}_{t}(z)=\\boldsymbol{k}_{t}^{\\top}(z)(\\boldsymbol{K}_{t}+\\rho I)^{-1}\\boldsymbol{y}_{t},}\\\\ &{\\sigma_{t}^{2}(z)=\\boldsymbol{k}(z,z)-\\boldsymbol{k}_{t}^{\\top}(z)(\\boldsymbol{K}_{t}+\\rho I)^{-1}\\boldsymbol{k}_{t}(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $k_{t}(z)=[k(z,z_{1}),\\ldots,k(z,z_{t})]^{\\top}$ is a $t\\times1$ vector of the kernel values between $z$ and observations, $K_{t}=[k(z_{i},z_{j})]_{i,j=1}^{t}$ is the $t\\times t$ kernel matrix, $I$ is the identity matrix appropriately sized to match $K_{t}$ , and $\\rho>0$ is a free regularization parameter. ", "page_idx": 4}, {"type": "text", "text": "Confidence bounds of the form $|f(z)-\\hat{f}_{t}(z)|\\le\\beta(\\delta)\\sigma_{t}(z)$ are established, for a confidence interval width multiplier $\\beta(\\delta)$ at a confidence level $1-\\delta$ , which depends on the assumptions on the setting and the noise. We will establish a such confidence interval specific to the RL setting, in Theorem 1, and utilize it in our regret analysis. ", "page_idx": 5}, {"type": "text", "text": "2.4 Kernel-Based Modelling in RL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In our RL setting, we use a kernel-based model to predict the expected value function. In particular, for a given transition probability distribution $P(s^{\\prime}|\\cdot,\\cdot)$ and a value function $v:S\\rightarrow\\mathbb{R}$ , we define $f=[\\bar{P}v]$ and use past observations to form predictions and uncertainty estimates for $f$ , as detailed in the following section. The value functions vary due to the Markovian nature of the temporal dynamics. To effectively use the confidence intervals established by the kernel-based models on $f$ , we require the following assumption. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. We assume $P(s^{\\prime}|\\cdot,\\cdot)\\in\\mathcal{H}_{k}$ , for some positive definite kernel $k$ , and $\\|P(s^{\\prime}|\\cdot,\\cdot)\\|_{\\mathcal{H}_{k}}\\leq$ 1 for all $s^{\\prime}\\in\\mathcal{S}$ . ", "page_idx": 5}, {"type": "text", "text": "2.5 Eigendecay and Information Gain ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our regret bounds are presented in terms of maximum information gain which is a kernel-specific complexity term. Specifically, for a kernel $k$ and sets of observation points $\\{z_{i}\\}_{i=1}^{t}$ , we define the maximum information gain $\\gamma(t;\\rho)$ as follows ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma(t;\\rho)=\\operatorname*{sup}_{\\{z_{i}\\}_{i=1}^{t}\\subset\\mathcal{Z}}\\frac{1}{2}\\log\\operatorname*{det}\\left(I+\\frac{K_{t}}{\\rho}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\rho>0$ , and $K_{t}$ is the kernel matrix defined in Section 2.3. Several works have established upper bounds on $\\gamma(t;\\rho)$ . In the special case of a $d$ -dimensional linear kernel, we have $\\gamma(t;\\rho)=\\mathcal{O}(d\\log^{*}(\\bar{t}))$ . For kernels with exponential eigendecay, including SE, $\\gamma(t;\\rho)\\,=\\,\\mathcal{O}(\\mathrm{polylog}(t))$ (Srinivas et al., 2010; Vakili et al., 2021b). For kernels with polynomial eigendecay, which represent a crucial case due to challenges in establishing no-regret guarantees in RL and bandits, and include kernels of both practical and theoretical interest such as the Mat\u00e9rn family and NT kernels, we first provide the definition below and then the bound on $\\gamma$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 1. A kernel $k$ is said to have a p-polynomial eigendecay $i f\\forall m\\ge1$ , $\\lambda_{m}\\leq C m^{-p}$ , for some $p>1$ , $C>0$ where $\\lambda_{m}$ are the Mercer eigenvalues of the kernel in decreasing order. ", "page_idx": 5}, {"type": "text", "text": "For kernels with $p$ -polynomial eigendecay, we have (Vakili et al., 2021b, Corollary 1): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma(t;\\rho)=\\mathcal{O}\\left(\\left(\\frac{t}{\\rho}\\right)^{\\frac{1}{p}}\\left(\\log\\left(1+\\frac{t}{\\rho}\\right)\\right)^{1-\\frac{1}{p}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3 KUCB-RL Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce our algorithm, Kernel-based Upper Confidence Bound for Reinforcement Learning (KUCB-RL). The algorithm\u2019s structure is similar to acquisition-based kernel bandit algorithms such as GP-UCB (Srinivas et al., 2010), where each action is chosen as the maximizer of an acquisition function. We construct an optimistic proxy $q_{t}$ for the state-action value function. At each step $t$ , given the current state $s_{t}$ , the action $a_{t}$ is selected as the maximizer of $q_{t}(s_{t},a)$ over $a$ . This proxy $q_{t}$ is derived using past observations of transitions, employing kernel ridge regression to provide a prediction and uncertainty estimate for the state-action value function over a future window of size $w\\in\\mathbb{N}$ . The proxy is established as an upper confidence bound, following the principle of optimism in the face of uncertainty. The value functions are computed in batches of $w$ steps, and the derived policies are unrolled over the subsequent $w$ steps. The details are presented next. ", "page_idx": 5}, {"type": "text", "text": "We define a fixed window size, $w\\in\\mathbb{N}$ , which represents the future interval that the algorithm will consider. For a given $t_{0}$ where $(t_{0}\\ \\ \\mathrm{mod}\\ w)=0$ , including $t_{0}\\,=\\,0$ , we initialize $v_{t_{0}+w+1}(s)=$ $0,\\forall s\\in S$ , reflecting the algorithm\u2019s consideration of the reward within this future window of size $w$ . Subsequently, we recursively obtain proxies $q_{t}$ and $v_{t}$ for all steps $t\\in\\{t:t_{0}+1\\leq t\\leq t_{0}+w\\}$ . Let $f_{t}$ denote $[P v_{t+1}]$ , $\\hat{f}_{t}$ represent the kernel ridge predictor of $[P v_{t+1}]$ , and $\\sigma_{t}$ be its uncertainty ", "page_idx": 5}, {"type": "text", "text": "Require: Regularization parameter $\\rho$ , window size $w$ , confidence interval width multiplier $\\beta$ , confi  \ndence level $1-\\delta,\\mathcal{S},\\mathcal{A},r$ .   \n1: for $t=0,1,2,\\cdot\\cdot\\cdot$ do   \n2: if $(t\\ \\ \\mathrm{mod}\\ w)=0$ then   \n3: Let $v_{t+w+1}=\\mathbf{0}$ ;   \n4: for $h=1,2,\\cdots\\,,w$ do   \n5: Compute $q_{t+w+1-h}$ and $v_{t+w+1-h}$ using equations (6) and (7).   \n6: end for   \n7: end if   \n8: Select at = arg $\\operatorname*{max}_{a\\in\\mathcal{A}}q_{t}(s_{t},a)$ ; Observe $s_{t+1}\\sim P(\\cdot|s_{t},a_{t})$ and receive $r(s_{t},a_{t})$ .   \n9: end for ", "page_idx": 6}, {"type": "text", "text": "estimator. The predictor and the uncertainty estimator are derived using the data set $\\mathcal{D}_{t_{0}}$ , which contains observations of past transitions up to $t_{0}$ . We use the notation $\\mathcal{D}_{t}\\,=\\,\\{(s_{j},a_{j},s_{j+1})\\}_{j\\le t}$ for the past transitions, and also define $\\boldsymbol{v}_{t+1,t_{0}}=[v_{t+1}(s_{2}),v_{t+1}(s_{3}),\\cdots,v_{t+1}(s_{t_{0}+1})]^{\\intercal}$ , for the values of the proxy value function at the history of state observations. We then have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}_{t}(z)=k_{t_{0}}^{\\top}(z)\\left(K_{t_{0}}+\\rho I\\right)^{-1}{\\pmb v}_{t+1,t_{0}},}\\\\ &{\\sigma_{t}^{2}(z)=k(z,z)-k_{t_{0}}^{\\top}(z)\\left(K_{t_{0}}+\\rho I\\right)^{-1}k_{t_{0}}(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $k_{t}(z)\\,=\\,[k(z,z_{1}),k(z,z_{2}),\\cdot\\cdot\\cdot\\,,k(z,z_{t}))]^{\\intercal}$ denotes the vector of kernel values between $z$ and $(z_{j}=(s_{j},a_{j}))_{j\\leq t}$ in the history of observations, and $K_{t}=[k(z_{i},z_{j})]_{i,j=1}^{t}$ denotes the kernel matrix. ", "page_idx": 6}, {"type": "text", "text": "Equipped with the kernel ridge predictor and uncertainty estimator, we define $q_{t}$ as an upper confidence bound for $f_{t}$ , as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nq_{t}(z)=\\Pi_{[0,w]}\\left(r(z)+\\hat{f}_{t}(z)+\\beta(\\delta)\\sigma_{t}(z)\\right),~~\\forall z\\in\\mathcal{Z},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $1-\\delta$ represents a confidence level, and $\\beta(\\delta)$ is a confidence interval width multiplier; the specific value of which is given in Theorem 3. The notation $\\Pi_{[a,b]}(\\cdot)$ is used for projection on $[a,b]$ interval. This step is natural since with the assumption $r:\\mathcal{Z}\\to[0,1]$ the value over a window of size $w$ can not be more than $w$ . We also define ", "page_idx": 6}, {"type": "equation", "text": "$$\nv_{t}(s)=\\operatorname*{max}_{a\\in\\mathcal{A}}q_{t}(s,a),\\;\\;\\forall s\\in\\mathcal{S}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By iteratively updating from $t=t_{0}+w$ to $t=t_{0}+1$ , we compute the values of $q_{t}$ and $v_{t}$ for all $t$ from $t_{0}+1$ to $t_{0}+w$ . Then, we unroll the learned policy over the subsequent $w$ steps, as the greedy policy with respect to $q_{t}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\na_{t}=\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\,q_{t}\\big(s_{t},a\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A pseudocode is provided in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Computational Complexity. KUCB-RL enjoys a polynomial computational complexity of $O(\\frac{T^{4}}{w})$ , where the bottleneck is the matrix inversion step in (5) in kernel ridge regression every $w$ steps. This is not unique to our work and is common across kernel-based supervised learning, bandit, and RL literature. Luckily, sparse approximation methods such as Sparse Variational Gaussian Processes (SVGP) or the Nystr\u00f6m method significantly reduce the computational complexity of matrix inversion step (to as low as linear in some cases), while maintaining the kernel-based confidence intervals and, consequently, the eventual rates (see, e.g., Vakili et al., 2022, and references therein). These results are, however, generally applicable and not specific to our problem. ", "page_idx": 6}, {"type": "text", "text": "4 Regret Bounds for KUCB-RL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide analytical results on the performance of KUCB-RL. We prove the first sublinear regret bounds in undiscounted RL setting under general assumptions based on kernel-based modelling. We first derive a novel confidence interval that is broadly applicable to the kernel-based RL problems. We then utilize this result to establish bounds on the regret of KUCB-RL. ", "page_idx": 6}, {"type": "text", "text": "4.1 Confidence Intervals for Kernel Based RL ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The analysis of our algorithm utilizes confidence intervals of the form $|f_{t}(z)-\\hat{f}_{t}(z)|\\le\\beta(\\delta)\\sigma_{t}(z)$ , where $f_{t}=[P v_{t}]$ denotes the expected value of a value function $v_{t}$ , and $\\hat{f}_{t}$ and $\\sigma_{t}$ represent the kernel ridge predictor and the uncertainty estimate of $f_{t}$ . Here, $\\beta(\\delta)$ represents the width multiplier for the confidence interval at a $1-\\delta$ confidence level. Similar confidence intervals are established in kernel ridge regression for a fixed function $f$ in the RKHS of a specified kernel $k$ (see, e.g., Abbasi-Yadkori, 2013; Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Vakili et al., 2021a; Whitehouse et al., 2024). In the RL context, specific considerations are required as both $f_{t}=[P v_{t}]$ and the observation noise depend on the value function $v_{t}$ that varies due to the Markovian nature of the temporal dynamics. We note that in this setting, for a given value function $v:{\\mathcal{S}}\\rightarrow\\mathbb{R}$ , the observation noise is captured by $v(s_{t+1})\\mathrm{~-~}[P v](s_{t},\\bar{a}_{t})$ . A possible approach involves deriving confidence intervals that apply to a class $\\mathcal{V}$ of value functions. Such results appear in some of the existing work (Chowdhury and Oliveira, 2023; Vakili and Olkhovskaya, 2023). The result most closely related to our is Chowdhury and Oliveira (2023), which derives its confidence interval under the exact same kernel related assumptions as our work, but for the episodic MDP setting. With the same assumptions, the confidence interval that we establish is different from the one in Chowdhury and Oliveira (2023). In particular, their confidence interval is applicable to a specific value of kernel ridge regression parameter $\\rho$ , constrained by their proof techniques. Inspired by Whitehouse et al. (2024), which established a confidence interval for kernel ridge regression (not within the RL context) but allowed for a judicious selection of $\\rho$ , we prove a new confidence interval suitable for the RL setting that allows tuning parameter $\\rho$ . As a result, we obtain the first improved no-regret algorithms in this setting. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 (Confidence Bound). Consider $v:S\\rightarrow\\mathbb{R},$ , a conditional probability distribution $P(s|z)$ , $s\\,\\in\\,{\\mathcal{S}},\\;z\\,\\in\\,{\\mathcal{Z}}$ , and two positive definite kernels $k:\\mathcal{Z}\\times\\mathcal{Z}\\to\\mathbb{R}$ and $k^{\\prime}:\\mathcal{S}\\times\\mathcal{S}\\to\\mathbb{R}$ , where $\\mathcal{Z}=\\mathcal{S}\\times\\mathcal{A}$ is compact subset of $\\mathbb{R}^{d}$ . Let $f=[P v]$ and assume $\\Vert\\boldsymbol{v}\\Vert_{\\mathcal{H}_{k^{\\prime}}}\\leq C_{v}$ , $v(s)\\leq w,\\forall s\\in S,$ , and $\\|f\\|_{\\mathcal{H}_{k}}\\le C_{f}$ , for some $C_{v},w,C_{f}>0$ . A dataset $\\{(z_{i},s_{i}^{\\prime})\\}_{i=1}^{n}\\subset(\\mathcal{Z}\\times\\mathcal{S})^{n}$ is provided such that $s_{i}^{\\prime}\\sim P(\\cdot|z^{i})$ . Let $\\lambda_{m}$ , $m=1,2,\\cdot\\cdot\\cdot$ denote the Mercer\u2019s eigenvalues of $k^{\\prime}$ in a decreasing order and $\\psi_{m}$ denote the corresponding eigenfunctions, with $\\psi_{m}\\leq\\psi_{\\mathrm{max}}$ for some $\\psi_{\\mathrm{max}}>0$ . ", "page_idx": 7}, {"type": "text", "text": "Let $\\hat{f}_{n}$ and $\\sigma_{n}$ be the kernel ridge predictor and the uncertainty estimate of $f$ using the observations: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{f}_{n}(z)=k_{n}^{\\top}(z)(\\rho I+K_{n})^{-1}v_{n},~~~~\\sigma_{n}^{2}(z)=k(z,z)-k_{n}^{\\top}(z)(\\rho I+K_{n})^{-1}k_{n}(z),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\pmb{v}_{n}=[v(s_{1}^{\\prime}),v(s_{2}^{\\prime}),\\cdots\\,,v(s_{n}^{\\prime}))]^{\\top}$ is the vector of observations. ", "page_idx": 7}, {"type": "text", "text": "For all $z\\in{\\mathcal{Z}}$ and $v:\\|v\\|_{\\mathcal{H}_{k^{\\prime}}}\\leq C_{v}$ , the following holds, with probability at least $1-\\delta$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n|f(z)-\\hat{f}_{n}(z)|\\leq\\beta(\\delta)\\sigma_{n}(z),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with $\\beta(\\delta)=$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\gamma_{f}+\\frac{C_{v}\\psi_{\\operatorname*{max}}}{\\sqrt{\\rho}}\\left(\\sum_{m=1}^{M}\\lambda_{m}\\right)^{\\frac12}\\left(2\\log\\left(\\sqrt{\\frac{M}{\\delta}d e t(I+\\rho^{-1}K_{n})}\\right)\\right)^{\\frac12}+\\frac{2C_{v}\\psi_{\\operatorname*{max}}}{\\sqrt{\\rho}}\\left(n\\sum_{m=M+1}^{\\infty}\\lambda_{m}\\right)^{\\frac12}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We can simplify the presentation of $\\beta$ under the following assumption. ", "page_idx": 7}, {"type": "text", "text": "Assumption 3. For the kernel $k^{\\prime}$ , we assume that for some $C_{1},C_{2}$ and $q>0$ , $\\textstyle\\sum_{m=1}^{M}\\lambda_{m}\\leq C_{1}$ and, $\\scriptstyle\\sum_{m=M+1}^{\\infty}\\lambda_{m}\\ \\le\\ C_{2}M^{-q}$ for any $M\\in\\mathbb{N}$ . ", "page_idx": 7}, {"type": "text", "text": "This is a mild assumption. For example, a $p$ -polynomial eigendecay proflie with $p>1$ , which applies to a large class of common kernels including SE, Mat\u00e9rn and NT kernels, satisfies this assumption with $\\begin{array}{r}{\\bar{C_{1}}=\\frac{p C}{p-1}}\\end{array}$ , $\\begin{array}{r}{C_{2}=\\frac{C}{p-1}}\\end{array}$ , and $q=p-1$ , where $C$ is the constant specified in Definition 1. ", "page_idx": 7}, {"type": "text", "text": "Remark 2. Under Assumption $^3$ , the expression of $\\beta$ in Theorem $^3$ can be simplified as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\beta(\\delta)=\\mathcal{O}\\left(C_{f}+\\frac{C_{v}}{\\sqrt{\\rho}}\\sqrt{\\log(\\frac{n}{\\delta})+\\gamma(\\rho;n)}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 2 can be observed by selecting $M=\\lceil n^{{\\frac{1}{q}}}\\rceil$ in the expression of $\\beta(\\delta)$ , which provides a straightforward presentation of the confidence interval width multiplier. ", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 1 involves the Mercer representation of $v$ in terms of $\\psi_{m}$ . The expression of the prediction error $|f(z)-{\\hat{f}}_{n}(z)|$ is then decomposed into error terms corresponding to each $\\psi_{m}$ . We then partition these terms into the first $M$ elements corresponding to eigenfunctions with the largest $M$ eigenvalues and the rest. For each of the first $M$ eigenfunctions, we obtain high probability bounds using existing confidence intervals from Whitehouse et al. (2024). Summing up over $m$ , and using a bound based on uncertainty estimates, we achieve a high probability bound\u2014corresponding to the second term in the expression of $\\beta(\\delta)$ . We then bound the remaining $m>M$ elements based on the decay of Mercer eigenvalues\u2014corresponding to the third term in the expression of $\\beta(\\delta)$ . A detailed proof is provided in Appendix 6. ", "page_idx": 8}, {"type": "text", "text": "Theorem 1 is presented in a self-contained way, making it broadly applicable across various RL settings. In the following section, we apply this theorem within the analysis of the infinite horizon average reward setting to obtain a no-regret algorithm. This is the first no-regret algorithm within this setting under general kernel-related assumptions. ", "page_idx": 8}, {"type": "text", "text": "4.2 Regret Analysis of KUCB-RL ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The weakest assumption regarding value functions is realizability, which suggests that the optimal value function $v^{\\star}$ either belong to the an RKHS or are at least well-approximated by its elements. In the degenerate case of bandits with $|{\\cal S}|=1$ , realizability alone is sufficient for provably efficient algorithms (Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Vakili et al., 2021a). However, for general MDPs, realizability is inadequate, necessitating stronger assumptions (Jin et al., 2020; Wang et al., 2019; Chowdhury and Oliveira, 2023). Building on these works, our main assumption involves a closure property for all value functions within the following class: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\left\\{s\\rightarrow\\operatorname*{min}\\left\\{w,\\operatorname*{max}_{a\\in\\mathcal{A}}\\left\\{r(s,a)+\\phi^{\\top}(s,a)\\pmb\\theta+\\beta\\sqrt{\\phi^{\\top}(s,a)\\Sigma^{-1}\\phi(s,a)}\\right\\}\\right\\}\\right\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\beta\\in\\mathbb{R}$ and $\\beta>0$ , $\\|\\pmb{\\theta}\\|<\\infty$ , and $\\Sigma$ is an $\\infty\\times\\infty$ matrix operator such that $\\Sigma\\succeq\\rho I$ for some $\\rho>0$ , and $\\phi=[\\phi_{1},\\phi_{2},\\cdot\\cdot\\cdot]$ , where $\\phi_{m}=\\sqrt{\\lambda_{m}}\\varphi_{m}$ , and $\\lambda_{m}$ and $\\varphi_{m}$ are the Mercer eigenvalues and eigenfunctions corresponding to a kernel $k$ defined on ${\\mathcal{Z}}\\times{\\mathcal{Z}}$ . We assume $\\nu$ is a subset of the RKHS of a kernel $k^{\\prime}$ defined on $s\\times s$ . ", "page_idx": 8}, {"type": "text", "text": "Assumption 4 (Optimistic Closure). For any $v\\in\\mathcal{V}$ , and for some positive constant $C_{v}$ , we have $\\|v\\|_{k^{\\prime}}\\leq C_{v}$ . Additionally, for $v:S\\rightarrow[0,w]$ , we assume $C_{v}=\\mathcal{O}(w)$ . ", "page_idx": 8}, {"type": "text", "text": "This technical assumption is the same as Assumption 1 in Chowdhury and Oliveira (2023). The optimistic closure assumption in the kernel-based setting is strictly weaker than the ones explored in the context of generalized linear function approximation (Wang et al., 2020). ", "page_idx": 8}, {"type": "text", "text": "Theorem 3. Consider the undiscounted MDP setting described in Section 2. Run KUCB-RL given in Algorithm 1 for $T$ steps. Under Assumptions 1, 2, 3, and 4, the regret of KUCB-RL, defined in Equation (2), satisfies, with probability at least $1-\\delta$ ", "page_idx": 8}, {"type": "equation", "text": "$$\nR(T)=\\mathcal{O}\\left(\\frac{T}{w}+\\left(w+\\frac{w}{\\sqrt{\\rho}}\\sqrt{\\gamma(T;\\rho)+\\log\\left(\\frac{T}{\\delta}\\right)}\\right)\\sqrt{\\rho T\\gamma(T;\\rho)+\\rho^{2}w^{2}\\gamma(T;\\rho)\\gamma(T/w;\\rho)}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proof of Theorem 3 utilizes standard methods from the analysis of optimistic algorithms in RL and bandits, such as the elliptical potential lemma, leverages the confidence interval proven in Theorem 1, and also incorporates novel techniques. Algorithm 1 updates the observation set every $w$ steps, requiring us to characterize and bound the effect of this delay in the proof. A straightforward application of the elliptical potential lemma results in loose bounds that do not guarantee no-regret. In Lemma 4, we establish a tight bound on the sum of standard deviations of a sequence of points with delayed updates of the observation sets, contributing to the improved regret bounds. This is independently a useful result in other settings with delayed updates, such as delayed feedback settings (Vakili et al., $2023\\mathrm{a}$ ; Kuang et al., 2023) or when observations are provided in a batch (Chowdhury and Gopalan, 2019). The details are provided in Appendix 7. ", "page_idx": 8}, {"type": "text", "text": "There is an apparent trade-off in choosing the window size. Intuitively, this trade-off balances the strength of the value function against the strength of the noise. A larger $w$ is preferred to capture the long-term performance of the policy, but a larger $w$ also increases the observation noise affecting the prediction error in kernel ridge regression. The optimal window size results from an interplay between these two factors, which is reflected in the regret bound. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We next instantiate our regret bounds for some special cases. In the linear case, with a choice of $w\\,=\\,T^{{\\frac{1}{4}}}d^{\\frac{-1}{4}}$ and replacing the bound on $\\gamma(T;\\rho)$ , we obtain $\\mathcal{R}(T)\\,=\\,\\tilde{\\mathcal{O}}(d^{\\frac{1}{2}}T^{\\frac{3}{4}})$ , recovering the existing results in their dependence on $T$ and improving by a factor of $d^{\\frac{1}{4}}$ . For kernels with exponential eigendecay, with a choice of $w=T^{{\\frac{1}{4}}}$ and replacing the bound on $\\gamma(T;\\rho)$ , we obtain $\\mathcal{R}(T)=\\tilde{\\mathcal{O}}(T^{\\frac{3}{4}})$ . We formalize the result with $p$ -polynomial kernels in the following remark as it may be of broader interest. ", "page_idx": 9}, {"type": "text", "text": "Remark 4. Under the setting of Theorem 3, with a $p$ -polynomial kernel, with the choice of parameters, $w=T^{\\frac{p-1}{4p+4}}$ and $\\rho=T^{\\frac{1}{p+1}}$ , we obtain the following no-regret guarantee $\\begin{array}{r}{\\mathcal{R}(T)=\\tilde{\\mathcal{O}}(T^{\\frac{3p+5}{4p+4}})}\\end{array}$ . ", "page_idx": 9}, {"type": "text", "text": "In the case of a Mat\u00e9rn kernel with smoothness parameter $\\nu$ , where $\\begin{array}{r}{p=1+\\frac{2\\nu}{d}}\\end{array}$ , the regret bound translates to $\\begin{array}{r}{\\mathcal{R}(T)=\\mathcal{O}\\left(T^{\\frac{3\\nu+4d}{4\\nu+4d}}\\right)}\\end{array}$ . This also directly extends to NT kernels using the equivalence between the RKHS of Mat\u00e9rn kernels and NT kernels with the appropriate smoothness (Vakili et al., 2023b). ", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed KUCB-RL in the infinite horizon average reward setting and proved no-regret guarantees with general kernels, including those with polynomial eigendecay such as Mat\u00e9rn and NT kernels. To highlight the significance of our results, we note that in the case of episodic MDPs, the existing work of (Yang et al., 2020; Chowdhury and Oliveira, 2023) do not provide no-regret guarantees with general kernels. The work of Vakili and Olkhovskaya (2023) utilizes sophisticated domain partitioning techniques and relies on a specific assumption about the scaling of kernel eigenvalues with the size of the domain. We achieve improved rates on regret leveraging a confidence interval proven in Theorem 1, which is applicable across various RL problems. We next point out two main limitations of our work. ", "page_idx": 9}, {"type": "text", "text": "Regarding optimality, we can juxtapose our results with the $\\Omega(T^{\\frac{\\nu+d}{2\\nu+d}})$ lower bounds proven in (Scarlett et al., 2017), for the degenerate case of bandits with Mat\u00e9rn kernel. Sophisticated algorithms, such as the sup variation of optimistic algorithms and those based on sample or domain partitioning (Valko et al., 2013; Salgia et al., 2021; Li and Scarlett, 2022), achieve this lower bound up to logarithmic factors in the case of bandits. However, a no-regret $\\tilde{\\mathcal{O}}(T^{\\frac{\\nu+2d}{2\\nu+2d}})$ guarantee, though suboptimal, for standard acquisition-based algorithms like GP-UCB has been provided only recently (Whitehouse et al., 2024). While we offer the first no-regret $\\tilde{\\mathcal{O}}(T^{\\frac{3\\nu+4d}{4\\nu+4d}})$ guarantee in the much more complex setting of RL, we cannot determine whether our results are improvable. This remains an area for future investigation. ", "page_idx": 9}, {"type": "text", "text": "Although RKHS elements of common kernels can approximate almost all continuous functions on compact subsets of $\\mathbb{R}^{d}$ (Srinivas et al., 2010), the optimistic closure assumption is somewhat limiting. A rigorous approach involves relaxing this assumption and finding an RKHS element that serves as an upper confidence bound on a function of interest $f$ within the same RKHS. While this method appears to reasonably address the assumption, it is a technically involved problem that invites further contributions from researchers in the field. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Y. Abbasi-Yadkori. Online learning for linearly parametrized control problems. 2013.   \nY. Abbasi-Yadkori, D. P\u00e1l, and C. Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011.   \nY. Abbasi-Yadkori, P. Bartlett, K. Bhatia, N. Lazic, C. Szepesvari, and G. Weisz. Politex: Regret bounds for policy iteration using expert prediction. In International Conference on Machine Learning, pages 3692\u20133702. PMLR, 2019a.   \nY. Abbasi-Yadkori, N. Lazic, C. Szepesvari, and G. Weisz. Exploration-enhanced politex. arXiv preprint arXiv:1908.10479, 2019b.   \nP. Auer, T. Jaksch, and R. Ortner. Near-optimal regret bounds for reinforcement learning. Advances in neural information processing systems, 21, 2008.   \nP. Bartlett and A. Tewari. Regal: a regularization based algorithm for reinforcement learning in weakly communicating mdps. In Uncertainty in Artificial Intelligence: Proceedings of the 25th Conference, pages 35\u201342. AUAI Press, 2009.   \nP. L. Bartlett and A. Tewari. Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps. arXiv preprint arXiv:1205.2661, 2012.   \nD. Calandriello, L. Carratino, A. Lazaric, M. Valko, and L. Rosasco. Scaling Gaussian process optimization by evaluating a few unique candidates multiple times. In International Conference on Machine Learning, pages 2523\u20132541. PMLR, 2022.   \nS. R. Chowdhury and A. Gopalan. On kernelized multi-armed bandits. In International Conference on Machine Learning, pages 844\u2013853. PMLR, 2017.   \nS. R. Chowdhury and A. Gopalan. On batch bayesian optimization. arXiv preprint arXiv:1911.01032, 2019.   \nS. R. Chowdhury and R. Oliveira. Value function approximations via kernel embeddings for no-regret reinforcement learning. In Asian Conference on Machine Learning, pages 249\u2013264. PMLR, 2023.   \nA. Christmann and I. Steinwart. Support Vector Machines. Springer New York, NY, 2008.   \nO. D. Domingues, P. M\u00e9nard, M. Pirotta, E. Kaufmann, and M. Valko. A kernel-based approach to non-stationary reinforcement learning in metric spaces. In International Conference on Artificial Intelligence and Statistics, pages 3538\u20133546. PMLR, 2021.   \nA. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov, F. J. R Ruiz, J. Schrittwieser, G. Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47\u201353, 2022.   \nB. Hao, N. Lazic, Y. Abbasi-Yadkori, P. Joulani, and C. Szepesv\u00e1ri. Adaptive approximate policy iteration. In International Conference on Artificial Intelligence and Statistics, pages 523\u2013531. PMLR, 2021.   \nO. Hern\u00e1ndez-Lerma. Adaptive Markov control processes, volume 79. Springer Science & Business Media, 2012.   \nA. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \nC. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 2137\u20132143. PMLR, 2020.   \nG. Kahn, A. Villaflor, V. Pong, P. Abbeel, and S. Levine. Uncertainty-aware reinforcement learning for collision avoidance. arXiv preprint arXiv:1702.01182, 2017.   \nD. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning, pages 651\u2013673. PMLR, 2018.   \nN. L. Kuang, M. Yin, M. Wang, Y.-X. Wang, and Y. Ma. Posterior sampling with delayed feedback for reinforcement learning with linear function approximation. Advances in Neural Information Processing Systems, 36:6782\u20136824, 2023.   \nS. P. Lalley. Concentration inequalities. Lecture notes, University of Chicago, 2013.   \nJ. Lee and M.-h. Oh. Demystifying linear mdps and novel dynamics aggregation framework. In The Twelfth International Conference on Learning Representations, 2023.   \nK. Lee, S.-A. Kim, J. Choi, and S.-W. Lee. Deep reinforcement learning in continuous action spaces: a case study in the game of simulated curling. In International Conference on Machine Learning,, pages 2937\u20132946. PMLR, 2018.   \nZ. Li and J. Scarlett. Gaussian process bandit optimization with few batches. In International Conference on Artificial Intelligence and Statistics, 2022.   \nJ. Mercer. Functions of positive and negative type, and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 209:415\u2013446, 1909.   \nA. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson, O. Pathak, A. Nazi, et al. A graph placement methodology for fast chip design. Nature, 594(7862): 207\u2013212, 2021.   \nM. L. Puterman. Markov decision processes. Handbooks in operations research and management science, 2:331\u2013434, 1990.   \nS. Salgia, S. Vakili, and Q. Zhao. A domain-shrinking based Bayesian optimization algorithm with order-optimal regret performance. Conference on Neural Information Processing Systems, 34, 2021.   \nJ. Scarlett, I. Bogunovic, and V. Cevher. Lower bounds on regret for noisy gaussian process bandit optimization. In Conference on Learning Theory, pages 1723\u20131742. PMLR, 2017.   \nB. Sch\u00f6lkopf, A. J. Smola, F. Bach, et al. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.   \nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.   \nN. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML 2010 - Proceedings, 27th International Conference on Machine Learning, pages 1015\u20131022, July 2010.   \nS. Vakili. Open problem: Order optimal regret bounds for kernel-based reinforcement learning. In The Thirty Seventh Annual Conference on Learning Theory, pages 5340\u20135344. PMLR, 2024.   \nS. Vakili and J. Olkhovskaya. Kernelized reinforcement learning with order optimal regret bounds. Advances in Neural Information Processing Systems, 36, 2023.   \nS. Vakili, N. Bouziani, S. Jalali, A. Bernacchia, and D.-s. Shiu. Optimal order simple regret for gaussian process bandits. Advances in Neural Information Processing Systems, 34:21202\u201321215, 2021a.   \nS. Vakili, K. Khezeli, and V. Picheny. On information gain and regret bounds in gaussian process bandits. In International Conference on Artificial Intelligence and Statistics, pages 82\u201390. PMLR, 2021b.   \nS. Vakili, J. Scarlett, D.-s. Shiu, and A. Bernacchia. Improved convergence rates for sparse approximation methods in kernel-based learning. In International Conference on Machine Learning, pages 21960\u201321983. PMLR, 2022.   \nS. Vakili, D. Ahmed, A. Bernacchia, and C. Pike-Burke. Delayed feedback in kernel bandits. In International Conference on Machine Learning, pages 34779\u201334792. PMLR, 2023a.   \nS. Vakili, M. Bromberg, J. Garcia, D.-s. Shiu, and A. Bernacchia. Information gain and uniform generalization bounds for neural kernel models. In 2023 IEEE International Symposium on Information Theory (ISIT), pages 555\u2013560. IEEE, 2023b.   \nM. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini. Finite-time analysis of kernelised contextual bandits. arXiv preprint arXiv:1309.6869, 2013.   \nO. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \nY. Wang, R. Wang, S. S. Du, and A. Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. arXiv preprint arXiv:1912.04136, 2019.   \nY. Wang, R. Wang, S. S. Du, and A. Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. In International Conference on Learning Representations, 2020.   \nC.-Y. Wei, M. J. Jahromi, H. Luo, H. Sharma, and R. Jain. Model-free reinforcement learning in infinite-horizon average-reward markov decision processes. In International conference on machine learning, pages 10170\u201310180. PMLR, 2020.   \nC.-Y. Wei, M. J. Jahromi, H. Luo, and R. Jain. Learning infinite-horizon average-reward mdps with linear function approximation. In International Conference on Artificial Intelligence and Statistics, pages 3007\u20133015. PMLR, 2021.   \nJ. Whitehouse, A. Ramdas, and S. Z. Wu. On the sublinear regret of gp-ucb. Advances in Neural Information Processing Systems, 36, 2024.   \nY. Wu, D. Zhou, and Q. Gu. Nearly minimax optimal regret for learning infinite-horizon averagereward mdps with linear function approximation. In International Conference on Artificial Intelligence and Statistics, pages 3883\u20133913. PMLR, 2022.   \nZ. Yang, C. Jin, Z. Wang, M. Wang, and M. Jordan. Provably efficient reinforcement learning with kernel and neural function approximations. Advances in Neural Information Processing Systems, 33:13903\u201313916, 2020.   \nS.-Y. Yeh, F.-C. Chang, C.-W. Yueh, P.-Y. Wu, A. Bernacchia, and S. Vakili. Sample complexity of kernel-based q-learning. In International Conference on Artificial Intelligence and Statistics, pages 453\u2013469. PMLR, 2023.   \nJ. Zhang, A. Koppel, A. S. Bedi, C. Szepesvari, and M. Wang. Variational policy gradient method for reinforcement learning with general utilities. Advances in Neural Information Processing Systems, 33:4572\u20134583, 2020.   \nZ. Zhang and Q. Xie. Sharper model-free reinforcement learning for average-reward markov decision processes. In The Thirty Sixth Annual Conference on Learning Theory, pages 5476\u20135477. PMLR, 2023.   \nD. Zhou, J. He, and Q. Gu. Provably efficient reinforcement learning for discounted mdps with feature mapping. In International Conference on Machine Learning, pages 12793\u201312802. PMLR, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "6 Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide a detailed proof the confidence bound given in Theorem 1. Let us use the notation ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\alpha_{n}(z)=k_{n}^{\\top}(z)(\\rho I+K_{n})^{-1},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\varepsilon_{i}=\\boldsymbol{v}(s_{i}^{\\prime})-f(z_{i}),\\varepsilon_{n}=[\\varepsilon_{1},\\varepsilon_{2},\\cdot\\cdot\\cdot\\cdot,\\varepsilon_{n}]^{\\intercal},f_{n}=[f(z_{1}),f(z_{2}),\\cdot\\cdot\\cdot\\cdot,f(z_{n})]^{\\intercal}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This allows us to rewrite the prediction error as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(z)-\\hat{f}_{n}(z)=f(z)-\\alpha_{n}^{\\top}(z)\\pmb{v}_{n}}\\\\ &{\\qquad\\qquad\\quad=f(z)-\\alpha_{n}^{\\top}(z)(\\pmb{f}_{n}+\\pmb{\\varepsilon}_{n})}\\\\ &{\\qquad\\qquad\\quad=\\left(f(z)-\\alpha_{n}^{\\top}(z)\\pmb{f}_{n}\\right)-\\alpha_{n}^{\\top}(z)\\pmb{\\varepsilon}_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The first term on the right-hand side represents the prediction error from noise-free observations, and the second term is the prediction error due to noise. The first term is deterministic (not random) and can be bounded following the standard approaches in kernel-based models, for example using the following result from Vakili et al. (2021a): ", "page_idx": 13}, {"type": "text", "text": "Lemma 1 (Proposition 1 in Vakili et al. (2021a)). We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}(z)=\\operatorname*{sup}_{f:\\|f\\|_{\\mathcal{H}}\\leq1}(f(z)-\\alpha_{n}^{\\top}(z)f_{n})^{2}+\\rho\\left\\|\\alpha_{n}(z)\\right\\|_{\\ell^{2}}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Based on this lemma, the first term on the right hand side of (11) can be deterministically bounded by $C_{f}\\sigma_{n}(z)$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n|f(z)-\\alpha_{n}^{\\top}(z)f_{n}|\\leq C_{f}\\sigma_{n}(z).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The challenging part in Equation (11) is the second term, which is the noise-dependent term $\\alpha_{n}^{\\top}(z)\\varepsilon_{n}$ .   \nNext, we provide a high probability bound on this term. ", "page_idx": 13}, {"type": "text", "text": "We leverage the Mercer representation of $v$ and write: ", "page_idx": 13}, {"type": "equation", "text": "$$\nv(s)=\\sum_{m=1}^{\\infty}w_{m}\\lambda_{m}^{\\frac{1}{2}}\\psi_{m}(s).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We rewrite the observation vector $\\pmb{v}_{n}$ as the sum of a noise term and the noise-free part $f$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nv(s_{i}^{\\prime})=\\underbrace{(v(s_{i}^{\\prime})-f(z_{i}))}_{\\qquad\\qquad\\qquad\\qquad}+\\qquad\\underbrace{f(z_{i})}_{\\qquad}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using the notation $\\overline{{\\psi}}_{m}(z)=\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|z)}\\psi(s^{\\prime})$ , we can rewrite $f(z_{i})$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f(z_{i})=\\mathbb{E}_{s\\sim P(\\cdot|z_{i}^{\\prime})}[v(s)]}\\\\ {\\displaystyle=\\mathbb{E}_{s\\sim P(\\cdot|z_{i})}\\left[\\sum_{m=1}^{\\infty}w_{m}\\lambda_{m}^{\\frac{1}{2}}\\psi_{m}(s)\\right]}\\\\ {\\displaystyle=\\sum_{m=1}^{\\infty}w_{m}\\lambda_{m}^{\\frac{1}{2}}\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|z_{i})}[\\psi_{m}(s^{\\prime})]}\\\\ {\\displaystyle=\\sum_{m=1}^{\\infty}w_{m}\\lambda_{m}^{\\frac{1}{2}}\\overline{{\\psi}}_{m}(z_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using this representation, we can rewrite the second term of (11) as follows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\sum_{i=1}^{n}{\\alpha_{i}(z)\\varepsilon_{i}=\\sum_{i=1}^{n}{\\alpha_{i}(z)\\left(\\displaystyle\\sum_{m=1}^{\\infty}{w_{m}\\lambda_{m}^{\\frac{1}{m}}\\psi_{m}(s_{i}^{\\prime})}-\\displaystyle\\sum_{m=1}^{\\infty}{w_{m}\\lambda_{m}^{\\frac{1}{m}}\\bar{\\psi}_{m}(z_{i})}\\right)}}}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{m=1}^{\\infty}{w_{m}\\lambda_{m}^{\\frac{1}{m}}\\sum_{i=1}^{n}{\\alpha_{i}(z)\\left(\\psi_{m}(s_{i}^{\\prime})-\\overline{{\\psi}}_{m}(z_{i})\\right)}}}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{m=1}^{M}{w_{m}\\lambda_{m}^{\\frac{1}{m}}\\sum_{i=1}^{n}{\\alpha_{i}(z)\\left(\\psi_{m}(s_{i}^{\\prime})-\\overline{{\\psi}}_{m}(z_{i})\\right)}}}\\\\ {\\displaystyle}&{\\displaystyle+\\sum_{m=M+1}^{\\infty}{w_{m}\\lambda_{m}^{\\frac{1}{m}}\\sum_{i=1}^{n}{\\alpha_{i}(z)\\left(\\psi_{m}(s_{i}^{\\prime})-\\overline{{\\psi}}_{m}(z_{i})\\right)}}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We decomposed the noise-related error term into an infinite series corresponding to each eigenfunction $\\psi_{m}$ , $m=1,2,\\cdot\\cdot\\cdot$ , and partitioned that into the first $M$ elements and the rest. For each of the first $M$ elements, we can apply the standard confidence intervals for kernel ridge regression. Specifically, Corollary 1 in Whitehouse et al. (2024) implies that, with probability at least $\\bar{1}-\\delta/M$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\alpha_{i}(z)(\\psi_{m}(s_{i}^{\\prime})-\\overline{{\\psi}}_{m}(z_{i}))\\leq\\frac{\\psi_{\\operatorname*{max}}\\sigma_{n}(z)}{\\sqrt{\\rho}}\\left(2\\log\\left(\\sqrt{\\frac{M}{\\delta}\\mathrm{det}(I+\\rho^{-1}K_{n})}\\right)\\right)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing up over the first $M$ elements, and using a probability union bound, with probability at least $1-\\delta$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{m=1}^{M}w_{m}\\lambda_{m}^{\\frac{1}{m}}\\sum_{i=1}^{n}\\alpha_{i}(z)(\\psi_{m}(s_{i}^{\\prime})-\\overline{{\\psi}}_{m}(z_{i}))}}\\\\ &{}&{\\leq\\sum_{m=1}^{M}w_{m}\\lambda_{m}^{\\frac{1}{m}}\\frac{\\psi_{m\\mathrm{ax}}\\sigma_{n}(z)}{\\sqrt{\\rho}}\\left(2\\log\\left(\\sqrt{\\frac{M}{\\delta}}\\mathrm{det}(I+\\rho^{-1}K_{n})\\right)\\right)^{\\frac{1}{2}}}\\\\ &{}&{\\leq\\frac{\\psi_{\\operatorname*{max}}\\sigma_{n}(z)}{\\sqrt{\\rho}}\\left(\\sum_{m=1}^{M}w_{m}^{2}\\right)^{\\frac{1}{2}}\\left(\\sum_{m=1}^{M}\\lambda_{m}\\right)^{\\frac{1}{2}}\\left(2\\log\\left(\\sqrt{\\frac{M}{\\delta}}\\mathrm{det}(I+\\rho^{-1}K_{n})\\right)\\right)^{\\frac{1}{2}}}\\\\ &{}&{\\leq\\frac{C\\,\\psi_{\\operatorname*{max}}\\sigma_{n}(z)}{\\sqrt{\\rho}}\\left(\\sum_{m=1}^{M}\\lambda_{m}\\right)^{\\frac{1}{2}}\\left(2\\log\\left(\\sqrt{\\frac{M}{\\delta}}\\mathrm{det}(I+\\rho^{-1}K_{n})\\right)\\right)^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality follows from the Cauchy-Schwarz inequality, and the third inequality follows from the bound $\\Vert\\boldsymbol{v}\\Vert_{\\mathcal{H}_{k}}\\leq C_{v}$ . ", "page_idx": 14}, {"type": "text", "text": "For the rest of the elements $m>M$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=M+1}^{\\infty}w_{m}\\lambda_{m}^{\\frac{1}{m}}\\sum_{i=1}^{n}\\alpha_{i}(z)\\left(\\psi_{m}(s_{i}^{\\prime})-\\overline{{\\psi}}_{m}(z_{i})\\right)\\leq2\\psi_{m a x}\\displaystyle\\sum_{m=1+1}^{\\infty}w_{m}\\lambda_{m}^{\\frac{1}{m}}\\displaystyle\\sum_{i=1}^{n}\\alpha_{i}(z)}\\\\ &{\\leq2\\psi_{m a x}\\displaystyle\\sum_{m=M+1}^{\\infty}w_{m}\\lambda_{m}^{\\frac{1}{m}}\\left(n\\displaystyle\\sum_{i=1}^{n}\\alpha_{i}^{2}(z)\\right)^{\\frac{1}{2}}}\\\\ &{\\leq\\displaystyle\\frac{2\\psi_{m a x}\\sigma_{n}(z)\\sqrt{n}}{\\sqrt{\\rho}}\\displaystyle\\sum_{m=M+1}^{\\infty}w_{m}\\lambda_{m}^{\\frac{1}{m}}}\\\\ &{\\leq\\displaystyle\\frac{2\\psi_{m a x}\\sigma_{n}(z)\\sqrt{n}}{\\sqrt{\\rho}}\\left(\\displaystyle\\sum_{m=M+1}^{\\infty}w_{m}^{2}\\right)^{\\frac{1}{2}}\\left(\\sum_{m=M+1}^{\\infty}\\lambda_{m}\\right)^{\\frac{1}{2}}}\\\\ &{\\leq\\displaystyle\\frac{2C\\psi_{m a x}\\sigma_{n}(z)}{\\sqrt{\\rho}}\\left(n\\displaystyle\\sum_{m=M+1}^{\\infty}\\lambda_{m}\\right)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The first inequality holds by the definition of $\\psi_{\\mathrm{max}}$ . The second inequality follows from the CauchySchwarz inequality. The third inequality is derived using Lemma 1. The fourth inequality again applies the Cauchy-Schwarz inequality, and the final inequality results from the upper bound on the RKHS norm of $v$ . ", "page_idx": 15}, {"type": "text", "text": "Putting all the terms together, with probability $1-\\delta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n|f(z)-\\hat{f}_{n}(z)|\\leq\\beta(\\delta)\\sigma_{n}(z),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\beta(\\delta)=$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma_{f}+\\frac{C_{v}\\psi_{\\operatorname*{max}}}{\\sqrt{\\rho}}\\left(\\sum_{m=1}^{M}\\lambda_{m}\\right)^{\\frac12}\\left(2\\log\\left(\\sqrt{\\frac{M}{\\delta}}\\mathrm{det}(I+\\rho^{-1}K_{n})\\right)\\right)^{\\frac12}+\\frac{2C_{v}\\psi_{\\operatorname*{max}}}{\\sqrt{\\rho}}\\left(n\\sum_{m=M+1}^{\\infty}\\lambda_{m}\\right)^{\\frac12},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "that completes the proof. ", "page_idx": 15}, {"type": "text", "text": "7 Proof of Theorem 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To analyze the performance of KUCB-RL, we first define an event $\\mathcal{E}$ that all the confidence intervals used in the algorithm hold true. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{|f_{t}(z)-\\hat{f}_{t}(z)|\\leq\\beta(\\delta)\\sigma_{t}(z),\\ \\ \\forall t\\in[T]\\right\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta(\\delta)=\\mathcal{O}\\left(w+\\frac{w}{\\sqrt{\\rho}}\\sqrt{\\log\\left(\\frac{T}{\\delta}\\right)+\\gamma(T,\\rho)}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Theorem 1, we have $\\mathrm{Pr}[\\mathcal{E}]\\ge1-\\delta/2$ . We note the under Assumption 4, $\\|v\\|_{\\mathcal{H}_{k^{\\prime}}}\\le C_{v}=\\mathcal{O}(w)$ . Also, for $v:S\\rightarrow[0,w]$ , we have $\\|P\\dot{v}\\|_{\\mathcal{H}_{k}}=\\mathcal{O}(w)$ . See Yeh et al. (2023), Lemma 3, for a proof. Since $v_{t}$ is upper bounded by $w$ by construction, we have $\\|P v_{t}\\|=\\mathcal{O}(w)$ that replaces $C_{f}$ in the expression of $\\beta$ in Theorem 1. ", "page_idx": 15}, {"type": "text", "text": "We condition the rest of the proof on event $\\mathcal{E}$ . ", "page_idx": 15}, {"type": "text", "text": "Consider $t_{0}$ such that $(t_{0}\\ \\ \\mathrm{mod}\\ w)\\,=\\,0$ we bound the regret over window $t\\,\\in\\,[t_{0}+1,t_{0}+w]$ , denoted by $\\mathcal{R}_{t_{0}}(w)$ . In addition let $V_{w}^{\\star}(s)$ denote the optimum achievable total reward over a window of size $w$ starting with initial state $s$ , and $V_{w}^{\\pi}(s)$ denote the total reward over a window of size $w$ achieved by KUCB-RL starting with initial state $s$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t_{0}}(w)=w J^{\\star}-\\sum_{t=t_{0}+1}^{t_{0}+w}r(s_{t},a_{t})=w J^{\\star}-V_{w}^{\\star}(s_{t_{0}+1})+V_{w}^{\\star}(s_{t_{0}+1})-\\sum_{t=t_{0}+1}^{t_{0}+w}r(s_{t},a_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For a bounded function $v:S\\rightarrow\\mathbb{R}$ , we define its span as $\\begin{array}{r}{\\operatorname{span}(v)=\\operatorname*{sup}_{s,s^{\\prime}\\in S}|v(s)-v(s^{\\prime})|.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "The first term is bounded by the span of $v^{*}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. For any s, $\\vert w J^{\\star}-V_{w}^{\\star}(s)\\vert\\le s p a n(v^{\\ast})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof follows the exact same lines as in the proof of Lemma 13 in Wei et al. (2021). ", "page_idx": 15}, {"type": "text", "text": "We next bound the second term in $\\mathcal{R}_{t_{0}}(w)$ . We first prove that $V_{w}^{\\star}(s)\\leq v_{t_{0}}(s)$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 3. Under event $\\mathcal{E}$ , we have $V_{w}^{\\star}(s)\\leq v_{t_{0}}(s),\\,\\forall s\\in S$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. We can prove this by induction. Note that $V_{0}^{\\star}(s)=v_{t_{0}+w+1}(s)=0$ . For any $j\\in[w]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{j}^{\\star}(s)-v_{t_{0}+w+1-j}=\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}Q_{j}^{\\star}(s,a)-\\underset{a^{\\prime}\\in\\mathcal{A}}{\\operatorname*{max}}q_{t_{0}+w+1-j}(s,a^{\\prime})}\\\\ &{\\hphantom{V_{j}^{\\star}(s)-v_{t_{0}+w+1-j}}\\leq\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\{Q_{j}^{\\star}(s,a)-q_{t_{0}+w+1-j}(s,a)\\}}\\\\ &{\\hphantom{V_{j}^{\\star}(s)-v_{t_{0}+w+1-j}}=\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\{[P V_{j+1}^{\\star}](s,a)-[P v_{t_{0}+w-j}](s,a)\\}}\\\\ &{\\hphantom{V_{j}^{\\star}(s)-v_{t_{0}+w-j}}=\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\{\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}[V_{j+1}^{\\star}(s^{\\prime})-v_{t_{0}+w-j}(s^{\\prime})]\\}}\\\\ &{\\hphantom{V_{j}^{\\star}(s)-v_{t_{0}+w-j}}\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first inequality is due to rearrangement of max, and the second inequality is by the induction assumption. We thus have $V_{w}^{\\star}(s)\\leq v_{t_{0}}(s)$ . ", "page_idx": 16}, {"type": "text", "text": "We now bound the difference between $v_{t_{0}}(s_{t_{0}+1})$ and sum of the reward over the window starting at step $t_{0}+1$ : $v_{t_{0}+1}\\big(s_{t_{0}+1}\\big)-V_{w}^{\\pi}\\big(s_{t_{0}+1}\\big)$ . We note that $v_{t_{0}+w}\\big(s_{t_{0}+w}\\big)=V_{0}^{\\pi}\\big(s_{t_{0}+w}\\big)=0$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nu_{t_{0}+j}(s_{t_{0}+j})-V_{w-j}^{\\pi}(s_{t_{0}+j})=q_{t_{0}+j}(s_{t_{0}+j},a_{t_{0}+j})-Q_{w-j}^{\\pi}(s_{t_{0}+j},a_{t_{0}+j})}&{}\\\\ {\\leq[P v_{t_{0}+j+1}](s_{t_{0}+j},a_{t_{0}+j})-[P V_{w-j}^{\\pi}](s_{t_{0}+j},a_{t_{0}+j})+2\\beta(\\delta)\\sigma_{t_{0}}(s_{t_{0}+j},a_{t_{0}+j})}&{}\\\\ {=v_{t_{0}+j+1}(s_{t_{0}+j+1})-V_{w-j-1}^{\\pi}(s_{t_{0}+j+1})+2\\beta(\\delta)\\sigma_{t_{0}}(s_{t_{0}+j},a_{t_{0}+j})}&{}\\\\ {+\\left([P v_{t_{0}+j+1}](s_{t_{0}+j},a_{t_{0}+j})-v_{t_{0}+j+1}(s_{t_{0}+j+1})\\right)}&{}\\\\ {+\\left(V_{w-j-1}^{\\pi}(s_{t_{0}+j+1})-[P V_{w-j}^{\\pi}](s_{t_{0}+j},a_{t_{0}+j})\\right).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The inequality holds under event $\\mathcal{E}$ . We obtained a recursive relationship for $v_{t_{0}+j}(s_{t_{0}+j})\\mathrm{~-~}$ $V_{w-j}^{\\pi}\\bar{(s_{t_{0}+j})}$ . Iterating over $j=w$ to $j=1$ , we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{t_{0}+1}(s_{t_{0}+1})-V_{w}^{\\pi}(s_{t_{0}+1})\\leq\\displaystyle\\sum_{t=t_{0}+1}^{t_{0}+w}2\\beta(\\delta)\\sigma_{t_{0}}(s_{t},a_{t})+\\displaystyle\\sum_{t=t_{0}+1}^{t_{0}+w}\\left([P v_{t+1}](s_{t},a_{t})-v_{t+1}(s_{t+1})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{t=t_{0}+1}^{t_{0}+w}\\left(V_{w+t_{0}-t-1}^{\\pi}(s_{t+1})-[P V_{w+t_{0}-t}^{\\pi}](s_{t},a_{t})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The second and third terms are zero mean martingales with a span of $2w$ , which are sub-Gaussian random variables with parameter $w$ . Therefore, by Azuma-Hoeffding inequality (Lalley, 2013), with probability at least $1-\\delta/2$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left([P v_{t+1}](s_{t},a_{t})-v_{t+1}(s_{t+1})\\right)+\\displaystyle\\sum_{t=1}^{T}\\left(V_{w+w\\lfloor(t-1)/w\\rfloor-t-1}^{\\pi}(s_{t+1})-[P V_{w+w\\lfloor(t-1)/w\\rfloor-t}^{\\pi}]\\big(s_{t},a_{t}\\big)\\right)}\\\\ &{\\quad=\\displaystyle\\mathcal{W}\\sqrt{2T\\log\\left(\\frac{2}{\\delta}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We note that for each $t\\in[T]$ , we can present the corresponding $t_{0}$ with $t_{0}=w\\lfloor(t\\!-\\!1)/w\\rfloor$ . Summing up the regret over all windows of size $w$ up to time $t$ , we have, with probability $1-\\delta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}(T)\\leq\\frac{T\\mathrm{span}(v^{*})}{w}+w\\sqrt{2T\\log\\left(\\frac{2}{\\delta}\\right)}+2\\beta(\\delta)\\sum_{t=1}^{T}\\sigma_{w\\lfloor(t-1)/w\\rfloor}\\bigl(z_{t}\\bigr).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It thus remains to bound $\\begin{array}{r}{\\sum_{t=1}^{T}\\sigma_{w\\lfloor(t-1)/w\\rfloor}(z_{t})}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "The sum of sequential standard deviations of a kernel based model is often bounded using the following result from Srinivas et al. (2010) that is similar to the elliptical potential lemma in linear bandits (see, Abbasi-Yadkori et al., 2011). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sigma_{t-1}^{2}(z_{t})\\leq\\frac{2\\gamma(T;\\rho)}{\\log(1+1/\\rho)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This result however is not directly applicable here due to the $w\\lfloor(t-1)/w\\rfloor$ subscript in $\\sigma_{w\\lfloor(t-1)/w\\rfloor}$ rather $\\sigma_{t-1}$ . A loose approach would be to partition the sequence into $w$ sequences, each for one $j\\in[w]$ of the form $\\sigma_{w(i-1)+j}$ , $i=1,2,\\cdot\\cdot\\cdot T/w$ . For each of those sequences, (15) is applicable and we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T/w}\\sigma_{w(i-1)+j}^{2}(z_{w i+j})\\leq\\frac{2\\gamma(T/w;\\rho)}{\\log(1+1/\\rho)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using this bound we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\sigma_{w\\lfloor(t-1)/w\\rfloor}^{2}(z_{t})=\\displaystyle\\sum_{j=1}^{w}\\sum_{i=1}^{T/w}\\sigma_{w(i-1)+j}^{2}(z_{w i+j})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{2w\\gamma(T/w;\\rho)}{\\log(1+1/\\rho)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we prove a stronger bound on $\\begin{array}{r}{\\sum_{t=1}^{T}\\sigma_{w\\lfloor(t-1)/w\\rfloor}(z_{t})}\\end{array}$ that contributes to the sublinear regret bounds in this paper. ", "page_idx": 17}, {"type": "text", "text": "Lemma 4. For a sequence of observation points $\\{z_{t}\\}_{t=1}^{T}$ and any $w\\in\\mathbb{N}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sigma_{w\\lfloor(t-1)/w\\rfloor}(s_{t},a_{t})\\leq\\sqrt{\\frac{2\\gamma(T;\\rho)}{\\log(1+1/\\rho)}\\left(T+\\frac{2w^{2}\\gamma(T/w;\\rho)}{\\log(1+1/\\rho)}\\right)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 4. We use the following lemma on the ratio of variances conditioned on two sets of observations. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5 (Proposition A.1 in Calandriello et al. (2022)). For any sequence of points $\\{z_{j}\\}_{j=1}^{T}$ , for any $z$ and $t^{\\prime}<t$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n1\\leq\\frac{\\sigma_{t^{\\prime}}^{2}(z)}{\\sigma_{t}^{2}(z)}\\leq1+\\sum_{j=t^{\\prime}+1}^{t}\\sigma_{t^{\\prime}}^{2}(z_{j}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We thus can write ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\sigma_{w\\lfloor(t-1)/w\\rfloor}(s_{t},a_{t})\\le\\sum_{t=1}^{T}\\sigma_{t}(s_{t},a_{t})\\sqrt{1+\\displaystyle\\sum_{j=w\\lfloor(t-1)/w\\rfloor+1}^{t}\\sigma_{w\\lfloor(t-1)/w\\rfloor}^{2}(s_{j},a_{j})}}}\\\\ &{\\le\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\sigma_{t}^{2}(s_{t},a_{t})}\\sqrt{T+w\\displaystyle\\sum_{t=1}^{T}\\sigma_{w\\lfloor(t-1)/w\\rfloor}^{2}(s_{t},a_{t})}}\\\\ &{\\le\\sqrt{\\displaystyle\\frac{2\\gamma(T;\\rho)}{\\log(1+1/\\rho)}\\left(T+\\frac{2w^{2}\\gamma(T/w;\\rho)}{\\log(1+1/\\rho)}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first inequality is by Lemma 5, the second inequality follows from Cauchy-Schwarz inequality, and the last inequality is the bound established in Equation (17). This completes the proof of Lemma 4. ", "page_idx": 17}, {"type": "text", "text": "Using Lemma 4, and substituting the value of $\\beta(\\delta)$ into (14), we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\nR(T)=\\mathcal{O}\\left(\\frac{T}{w}+\\left(w+\\frac{w}{\\sqrt{\\rho}}\\sqrt{\\gamma(T;\\rho)+\\log\\left(\\frac{T}{\\delta}\\right)}\\right)\\sqrt{\\rho T\\gamma(T;\\rho)+\\rho^{2}w^{2}\\gamma(T;\\rho)\\gamma(T/w;\\rho)}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The proof of the regret bound is complete. ", "page_idx": 18}, {"type": "text", "text": "8 Mercer Theorem and the RKHSs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Mercer theorem (Mercer, 1909) provides a representation of the kernel in terms of an infinite dimensional feature map (e.g., see, Christmann and Steinwart, 2008, Theorem 4.49). Let $\\mathcal{Z}$ be a compact metric space and $\\mu$ be a finite Borel measure on $\\mathcal{Z}$ (we consider Lebesgue measure in a Euclidean space). Let $L_{\\mu}^{2}(\\mathcal{Z})$ be the set of square-integrable functions on $\\mathcal{Z}$ with respect to $\\mu$ . We further say a kernel is square-integrable if ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}\\int_{\\mathcal{Z}}k^{2}(z,z^{\\prime})\\,d\\mu(z)d\\mu(z^{\\prime})<\\infty.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Theorem 5 (Mercer Theorem). Let $\\mathcal{Z}$ be a compact metric space and $\\mu$ be a finite Borel measure on $\\mathcal{Z}$ . Let $k$ be a continuous and square-integrable kernel, inducing an integral operator $T_{k}$ : $L_{\\mu}^{2}(\\mathcal{Z})\\to L_{\\mu}^{2}(\\mathcal{Z})$ defined by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(T_{k}f\\right)(\\cdot)=\\int_{\\mathcal{Z}}k(\\cdot,z^{\\prime})f(z^{\\prime})\\,d\\mu(z^{\\prime})\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $f\\in L_{\\mu}^{2}(\\mathcal{Z})$ . Then, there exists a sequence of eigenvalue-eigenfeature pairs $\\{(\\lambda_{m},\\varphi_{m})\\}_{m=1}^{\\infty}$ such that ${\\lambda_{m}}>0$ , and $T_{k}\\varphi_{m}=\\lambda_{m}\\varphi_{m},$ , for $m\\geq1$ . Moreover, the kernel function can be represented as ", "page_idx": 18}, {"type": "equation", "text": "$$\nk\\left(z,z^{\\prime}\\right)=\\sum_{m=1}^{\\infty}\\lambda_{m}\\varphi_{m}(z)\\varphi_{m}\\left(z^{\\prime}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the convergence of the series holds uniformly on ${\\mathcal{Z}}\\times{\\mathcal{Z}}$ . ", "page_idx": 18}, {"type": "text", "text": "According to the Mercer representation theorem (e.g., see, Christmann and Steinwart, 2008, Theorem 4.51), the RKHS induced by $k$ can consequently be represented in terms of $\\{(\\lambda_{m},\\varphi_{m})\\}_{m=1}^{\\infty}$ . ", "page_idx": 18}, {"type": "text", "text": "Theorem 6 (Mercer Representation Theorem). Let $\\{(\\lambda_{m},\\varphi_{m})\\}_{i=1}^{\\infty}$ be the Mercer eigenvalue eigenfeature pairs. Then, the RKHS of $k$ is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{H}_{k}=\\left\\{f(\\cdot)=\\sum_{m=1}^{\\infty}w_{m}\\lambda_{m}^{\\frac{1}{2}}\\varphi_{m}(\\cdot):w_{m}\\in\\mathbb{R},\\|f\\|_{\\mathcal{H}_{k}}^{2}:=\\sum_{m=1}^{\\infty}w_{m}^{2}<\\infty\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Mercer representation theorem indicates that the scaled eigenfeatures $\\{\\sqrt{\\lambda_{m}}\\varphi_{m}\\}_{m=1}^{\\infty}$ form an orthonormal basis for $\\mathcal{H}_{k}$ . ", "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The main claims include ", "page_idx": 19}, {"type": "text", "text": "\u2022 Proposing an optimistic algorithm for kenrel-based function approximation in the infinite horizon average reward RL setting: KUCB-RL (Algorithm 1) presented in Section 3.   \n\u2022 Establishing novel no-regret performance guarantees for our algorithm: The regret analysis provided in Section 4.   \n\u2022 Deriving a novel confidence interval for the kernel-based prediction of the expected value function, applicable across various RL problems: Provided in Theorem 1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Discussed in Section 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All of the Assumptions used in Theorem 1 are presented in the statement of the theorem, making it self-contained and easy to apply across various RL settings. The Assumptions used in Theorem 3 are Assumptions 1, 2, 3, and 4, clearly stated. The detailed proofs are provided in Appendix 6 and Appendix 7, with proof sketches briefly discussed in the main paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Paper does not include experiments ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 20}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer:[NA] . ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS code of Ethics and ensured that we are in full compliance. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 22}, {"type": "text", "text": "Justification: Our work provides guidance on designing efficient RL algorithms but does not address specific applications that constitute an explicit impact statement. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer:[NA] . ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] . Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer:[NA] . ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]