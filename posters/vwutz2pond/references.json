{"references": [{"fullname_first_author": "Y. Abbasi-Yadkori", "paper_title": "Improved algorithms for linear stochastic bandits", "publication_date": "2011-00-00", "reason": "This paper provides improved algorithms for linear stochastic bandits, which are foundational to the understanding of the methods used in this paper."}, {"fullname_first_author": "P. Auer", "paper_title": "Near-optimal regret bounds for reinforcement learning", "publication_date": "2008-00-00", "reason": "This paper establishes near-optimal regret bounds for reinforcement learning, providing a benchmark for evaluating the performance of new algorithms."}, {"fullname_first_author": "C.-Y. Wei", "paper_title": "Learning infinite-horizon average-reward mdps with linear function approximation", "publication_date": "2021-00-00", "reason": "This paper addresses the challenging problem of infinite-horizon average-reward Markov decision processes (MDPs) with linear function approximation, which is closely related to the problem tackled in this paper."}, {"fullname_first_author": "N. Srinivas", "paper_title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "publication_date": "2010-07-00", "reason": "This paper introduces Gaussian process optimization in the bandit setting, which is a key technique used in the development of kernel-based algorithms for reinforcement learning."}, {"fullname_first_author": "Y. Wang", "paper_title": "Optimism in reinforcement learning with generalized linear function approximation", "publication_date": "2019-12-00", "reason": "This paper introduces a novel optimistic algorithm for reinforcement learning with generalized linear function approximation, which is a key concept used in the approach proposed in this paper."}]}