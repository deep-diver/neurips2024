[{"type": "text", "text": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qinqian Lei1 Bo Wang2 Robby T. Tan1,3 ", "page_idx": 0}, {"type": "text", "text": "1National University of Singapore 2University of Mississippi 3ASUS Intelligent Cloud Services (AICS) qinqian.lei@u.nus.edu , hawk.rsrch@gmail.com , robby_tan@asus.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZHOI achieves state-of-the-art performance across various zero-shot settings with only $10.35\\%$ to $33.95\\%$ of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Human-Object Interaction (HOI) detection localizes human-object pairs and identifies the interactions. HOI has various practical applications, including robot manipulations, human-computer interaction, and human activity understanding [38, 41, 24, 32, 26, 1]. Additionally, it is a building block for related tasks such as action recognition, visual question answering, and image generation [18, 24, 4, 43, 11, 54]. However, the limited generalization ability of many existing HOI detectors makes zero-shot HOI detection particularly challenging, as it requires models to identify unseen HOI classes. ", "page_idx": 0}, {"type": "text", "text": "Vision-Language Models (VLMs) [47, 29, 2, 30] have gained popularity due to their extensive knowledge bases and effective ability to process and correlate complex patterns across visual and text data. Consequently, a group of methods has been developed to align HOI visual features with those of VLMs, leveraging the extensive knowledge from these models [51, 33, 44, 41, 4]. This alignment ensures that both the HOI model and the VLM can extract similar features to represent the same concept (e.g., an action). The degree of feature alignment can be quantified using cosine similarity, which measures the similarity between feature vectors. However, aligning with VLMs requires ", "page_idx": 0}, {"type": "image", "img_path": "R1Rrb2d5BH/tmp/c1ed30ce7fd2407b776385cdcf3b721a7976641dc47ad496b42d6b35386ffab8.jpg", "img_caption": ["(c) Our efficient zero-shot HOl detection (EZ-HOI) ", "(d) Zero-shot HOl detection comparison "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison of zero-shot HOI detection paradigms. (a) Methods that align HOI features with fixed VLMs [44, 33, 4, 41]. (b) Prompt learning methods to adapt VLMs for downstream tasks [22, 57]. (c) Our approach, which adapts VLMs to HOI tasks without compromising VLM generation capabilities. (d) Unseen, seen, and full mAP indicate the performance for unseen-verb, seen-verb, and full sets on the HICO-DET dataset [6]. Our EZ-HOI shows superior performance in these categories, with competitive trainable parameters and training epochs. ", "page_idx": 1}, {"type": "text", "text": "training transformer-based models, which is computationally expensive and leads to extended training time and significant difficulties. ", "page_idx": 1}, {"type": "text", "text": "An alternative approach involves adapting VLMs for HOI tasks, bypassing the demanding alignment process and leveraging VLM capabilities for action understanding in HOI detection. Prompt tuning, which adapts VLMs to various downstream tasks with a small number of learnable parameters, has shown considerable efficacy [62, 61, 19, 21]. One notable method, MaPLe [22], utilizes multi-modal prompt tuning specifically for image classification tasks. However, since adaptation typically involves only seen class labels, the adapted VLMs often overfit to seen classes but struggle to deal with unseen classes in zero-shot settings [62, 7, 9, 22]. Consequently, this limitation results in suboptimal performance for unseen classes in zero-shot HOI detection. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel method, Efficient Zero-Shot HOI detection (EZ-HOI), to enhance VLM adaptation via guided prompt learning with information from foundation models (e.g., LLM and VLM). We design both learnable text and visual prompts, leveraging fixed LLM and VLM to guide the adaptation process. Specifically, the text prompts are designed to capture detailed HOI class insights from an LLM, while the visual prompts are tailored to incorporate external visual semantics from a VLM. However, as the training datasets contain images with seen-class labels alone, the trainable prompts are naturally optimized for these seen classes rather than the unseen ones. ", "page_idx": 1}, {"type": "text", "text": "To address this limitation, we develop the Unseen Text Prompt Learning (UTPL) to leverage prompts from related seen classes effectively. We begin by measuring the relationship between HOIs using cosine similarity of text embeddings, which helps identify closely related seen classes for each unseen one. After establishing these connections, we enhance the learning of unseen prompts based on those from selected seen classes. To capture the distinctions between unseen and seen classes, we incorporate an LLM. This LLM provides what we call \u201cdisparity information\u201d, enhancing the learning of the distinctions and similarities between seen and unseen classes. Additionally, we enhance our approach by introducing intra- and inter-HOI feature fusion techniques following the visual encoder. Our method achieves competitive performance on established benchmarks in various zero-shot settings while requiring significantly fewer trainable parameters. ", "page_idx": 1}, {"type": "text", "text": "As illustrated in Fig. 1, we compare our method with two existing paradigms: (a) aligning HOI features to a fixed VLM and (b) adapting a VLM for HOI tasks. Unlike the existing approaches, our method facilitates VLM adaptation to HOI tasks and demonstrates effective generalization to unseen classes. Fig. 1(d) highlights that our method is competitive in terms of performance, model parameter efficiency, and training duration. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce EZ-HOI, a novel framework for zero-shot HOI detection that adapts a VLM to HOI tasks via guided prompt learning with foundation model information, enhancing the adapted VLM\u2019s generalizability in zero-shot HOI detection.   \n\u2022 We propose the UTPL module, which extracts information from related seen classes for the unseen learnable prompts. This module mitigates overftiting to seen classes, a common issue in task-specific VLM adaptations, improving performance on unseen classes.   \n\u2022 Our EZ-HOI method achieves state-of-the-art performance in various zero-shot settings while significantly reducing trainable parameters and mitigating training challenges. It lowers trainable model parameters by $66\\%$ to $78\\%$ compared to existing methods, demonstrating enhanced efficiency and effectiveness in zero-shot HOI detection. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Human-Object Interaction Detection Human-Object Interaction (HOI) detection involves identifying human-object pairs and recognizing their interactions, serving as a fundamental component for various downstream tasks in computer vision [53, 6]. HOI detection methods are typically categorized into two classes: one-stage and two-stage. One-stage methods simultaneously generate all outputs, including human bounding boxes, object bounding boxes and categories, and interaction classes. Recent advancements in one-stage detectors leverage transformer architectures, delivering promising performance [8, 46, 48, 63, 55, 52, 24]. An example is HOITrans [63], which utilizes the transformer\u2019s encoder and decoder to extract interaction features. The output features are then processed through multi-layer perceptrons to produce all output predictions at once. On the other hand, two-stage approaches divide HOI detection into two tasks: object detection and HOI classification [10, 14, 58]. Since separation allows each module to specialize, two-stage HOI detection is more memory-efficient [59]. Recent developments have seen the integration of transformer-based architectures into two-stage designs, which have shown promising results [59, 60, 45, 27]. Our method also falls into the two-stage design category. ", "page_idx": 2}, {"type": "text", "text": "Zero-Shot HOI Detection Prior zero-shot HOI detection efforts primarily address unseen composition settings, where models encounter action and object classes separately but not as combinations. To address this, several methods employ compositional learning strategies aimed at tackling the unseen-composition problem [14, 16, 15, 27]. However, compositional learning falls short in addressing unseen verb zero-shot HOI detection scenarios. Given the substantial image understanding capabilities of VLMs, they offer promising potential for enhancing HOI detection in zero-shot settings [2, 29, 30, 47, 37, 36], where labels for unseen HOI classes are absent from the training dataset. Recent studies have explored aligning HOI visual features with VLM [33, 44, 4, 41]. However, this approach requires training transformer-based models, which are computationally expensive. Consequently, aligning HOI models with VLMs is demanding, resulting in extended training times and significant training challenges. ", "page_idx": 2}, {"type": "text", "text": "Prompt Learning Prompt learning has gained popularity for adapting VLMs to downstream tasks [61, 62, 7, 9, 19, 23, 25, 40]. Context Optimization $({\\mathrm{CoOp}})$ [62] refines the prompt input of the text encoder by combining learnable domain-shared prompt tokens with class prompt tokens. MaPLe [22] integrates learnable domain-shared text tokens with visual learnable tokens. This combination leverages the highly connected text and visual encoders in VLMs, facilitating the sharing of crossdomain information and beneftiing both text and visual domains. However, fine-tuning VLM relies on training datasets with only seen class labels, which causes adapted VLMs to excel with seen classes but encounter difficulties with unseen ones in zero-shot scenarios. Consequently, this limitation results in suboptimal performance for unseen classes in zero-shot HOI detection. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We start with adapting a pre-trained CLIP model [47] to zero-shot HOI tasks using an innovative prompt learning approach. We design two sets of learnable prompts: text and visual. The visual prompts are derived from the text prompts by using projection layers. We denote learnable text prompts as $h_{T}$ and learnable visual prompts as $h_{V}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nh_{T}=\\operatorname{Proj}(h_{V}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "R1Rrb2d5BH/tmp/279fbf9f3728b68f878aa2a6704f58f8e22356f98ce8039e5e6485983fbe45d4.jpg", "img_caption": ["Figure 2: Overview of our EZ-HOI framework. Learnable text prompts capture detailed HOI class information from the LLM. To enhance their generalization ability, we introduce the Unseen Text Prompt Learning (UTPL) module. Meanwhile, visual learnable prompts are guided by a frozen VLM visual encoder. These learnable text and visual prompts are then separately input into the text and visual encoder. Finally, HOI predictions are made by calculating the cosine similarity between the text encoder output and the HOI image features. MHCA denotes multi-head cross-attention. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "where $h_{T}\\in\\mathcal{R}^{p*d_{t}}$ , and $h_{V}\\in\\mathcal{R}^{p\\ast d_{v}}$ . $d_{t}$ is the dimension of the text feature, and $d_{v}$ is the dimension of the visual feature. $p$ represents the number of tokens we design for each learnable prompt. ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathbb{V}=\\{v_{1},v_{2},\\cdot\\cdot\\cdot\\,,v_{N_{v}}\\}$ as the verb set and $\\mathbb{O}=\\left\\{o_{1},o_{2},\\cdot\\cdot\\cdot,o_{N_{o}}\\right\\}$ as the object set. Then the HOI set include all feasible verb-object pairs $\\mathbb{C}=\\{\\mathrm{hoi}_{i}=(v_{i},o_{i})|v_{i}\\in\\mathbb{V};o_{i}\\in\\mathbb{O}\\}$ . We set $C$ as the total number of HOI classes. Since it is impractical for any datasets to include comprehensive HOI classes, researchers propose zero-shot HOI detection settings to encourage the generalization of HOI detectors to unseen classes $\\mathbb{C}_{\\mathrm{unseen}}$ in inference [33]. Denote the seen HOI class set as $\\mathbb{S}$ and the unseen HOI class set as $\\mathbb{U}=\\{\\mathrm{hoi}_{i}\\ |\\ \\mathrm{hoi}_{i}\\not\\in\\mathbb{S},\\mathrm{hoi}_{i}\\in\\mathbb{C}\\}$ . Please refer to Appendix 7.9 for detailed definitions of the different zero-shot setting unseen set splits. ", "page_idx": 3}, {"type": "text", "text": "3.1 LLM and VLM Guidance for Learnable Prompts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Prompt learning techniques for VLMs are characterized by their ability to adapt large, pre-trained models to specific tasks using relatively small amounts of task-specific data. This often leads to a diminished generalization ability of the fine-tuned models for unseen classes [62, 7, 9, 22]. Given that foundation models, such as LLMs and VLMs, have demonstrated substantial knowledge capacity [29, 47], leveraging guidance from these models can improve performance on unseen HOI classes that are absent in the training data. ", "page_idx": 3}, {"type": "text", "text": "Text Prompt Design As shown in Fig. 2, we design input prompts, \u201cDescribe a person <acting> <object> and focus on the action\u201d related to each HOI class, for an LLM. The output from LLM contains richer semantic information with specific HOI class descriptions. Please refer to Appendix Section 7.2 for a detailed explanation with examples. ", "page_idx": 3}, {"type": "text", "text": "Then, we use a CLIP text encoder to obtain the text embedding for this HOI class description. We process text embeddings for all HOI classes including both seen and unseen in parallel, so the whole text embeddings are denoted as $\\mathcal{F}_{\\mathrm{txt}}\\in\\mathcal{R}^{C*d_{t}}$ and $\\mathcal{F}_{\\mathrm{txt}}=[f_{\\mathrm{txt_{1}}},f_{\\mathrm{txt_{2}}},\\cdot\\cdot\\cdot\\;,\\bar{f_{\\mathrm{txt}_{C}}}]$ . Then, the learnable text prompts $\\hat{h_{T}}$ can be obtained by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{h_{T}}=W_{\\mathrm{up}}\\cdot\\mathrm{MHCA}(Q=W_{\\mathrm{down}}\\cdot h_{T};\\ K,V=W_{\\mathrm{down}}\\cdot\\mathcal{F}_{\\mathrm{txt}})+h_{T}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "R1Rrb2d5BH/tmp/82e0d827c4f9b17f900cebc5ccfbe5c0f283a96252a4f312923975d6223e7b2a.jpg", "img_caption": ["Figure 3: Detailed architecture of Unseen Text Prompt Learning (UTPL). In the figure, we take the \u201chose a dog\u201d unseen HOI class in the unseen-verb zero-shot setting as an example. We first utilize the HOI class text embeddings to identify the most connected seen HOI class to \u201chose a dog\u201d. After selecting the seen class, we generate an input prompt to obtain disparity information from LLM. Finally, the unseen learnable prompt learns from the selected seen class prompt and the disparity information through MHCA. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $\\hat{h_{T}}=[\\hat{h_{T_{1}}},\\hat{h_{T_{2}}},\\cdot\\cdot\\cdot\\,,\\hat{h_{T_{C}}}]$ , and $\\hat{h_{T_{i}}}\\in\\mathcal{R}^{p*d_{t}}$ . Each class has its specific learnable text prompts after the process of Eq. 2. $W_{u p}$ and $W_{\\mathrm{down}}$ indicate the up-projection and down-projection layers. MHCA means the multi-head cross attention [50]. In Eq. 2, we only aggregate the useful information from ${\\mathcal F}_{t x t}$ using learnable attention, as the information provided by LLM may not all be relevant for our task. Moreover, to keep trainable parameters small, we apply a down-projection layer $W_{d o w n}$ before MHCA to reduce the feature dimension, and an up-projection layer $W_{u p}$ afterward. The same design strategy is used in Eq. 3 and Eq. 5. Later, the prompts corresponding to unseen classes are further refined, as detailed in Section 3.2. ", "page_idx": 4}, {"type": "text", "text": "Visual Prompt Design Visual prompt learning in our method is facilitated by a pre-trained Visual Language Model (VLM) visual encoder, specifically the CLIP visual encoder [47]. The pre-trained CLIP model inherently possesses knowledge of unseen HOI classes, offering richer visual semantics compared to models trained solely on task-specific datasets. The CLIP visual feature is denoted as $f_{v i s}^{Z}$ for an image $\\mathcal{T}$ , and we enhance the visual learnable prompts $h_{V}$ by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{h_{V}}=W_{\\mathrm{up}}\\cdot\\mathrm{MHCA}(Q=W_{\\mathrm{down}}\\cdot h_{V};\\ K,V=W_{\\mathrm{down}}\\cdot f_{v i s}^{T})+h_{V}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since the frozen VLM visual encoder can extract features for unseen HOIs, $\\hat{h_{V}}$ aggregates information from these visual features, improving performance on unseen HOIs. ", "page_idx": 4}, {"type": "text", "text": "3.2 Unseen-Class Text Prompt Learning (UTPL) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since the training data has no labeled image for unseen HOI classes, learnable text prompts for seen classes are inevitably optimized better than unseen classes. Therefore, we refine the unseen-class text prompts by learning from closely related seen-class prompts. Specifically, we denote one unseen HOI class as $u$ , and the learnable text prompt tokens as $\\Hat{h_{T_{u}}}$ , and denote one seen HOI class as $s$ and the related seen text prompt token as $\\hat{h_{T_{s}}}$ To identify the related seen class, we use text embeddings of the HOI class descriptions generated by the LLM, as explained in Text Prompt Design of Section 3.1. The related seen class is formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\ns=\\underset{s\\in\\mathbb{S}}{\\arg\\operatorname*{max}}\\ (f_{\\mathrm{txt_{u}}})^{T}\\cdot f_{\\mathrm{txt_{s}}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Directly refining prompts for unseen classes based solely on selected seen classes may be insufficient due to inherent differences between the seen and unseen classes. Thus, we propose utilizing disparity information, defined as the differences between unseen class $u$ and seen class $s$ , as provided by the LLM. The architecture of UTPL is shown in Fig. 3. Please refer to Appendix Section 7.3 for detailed explanation with examples. ", "page_idx": 4}, {"type": "text", "text": "Since the description can be too long to be encoded at once, we process each sentence separately by using the CLIP text encoder. Thus, set the text embedding of disparity descriptions for an unseen HOI class $u$ as $f_{\\mathrm{txt}_{u}}^{\\mathrm{disp}}\\in\\mathcal{R}^{m*d_{t}}$ , where $m$ is the number of sentences in the text description. Then we can compute the refined learnable text prompts $\\tilde{h_{T_{u}}}$ for unseen class $u$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K,V:W_{\\mathrm{down}}\\cdot\\mathrm{concat}(f_{\\mathrm{txt}_{u}}^{\\mathrm{disp}},\\hat{h_{T_{s}}},\\hat{h_{T_{u}}}))+{\\hat{h_{T_{u}}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where concat indicates concatenation. For seen classes, $\\tilde{h_{T_{s}}}={h_{T_{s}}}$ . Although ground-truth labels for the unseen classes are not available during training, we propose two strategies to update $\\tilde{h_{T_{u}}}$ in Eq. 5. First, we design a class-relation loss (refer to Eq. 16 in the Appendix) to keep the relationship between seen and unseen classes, measured by cosine similarity between text features. As a result, unseen prompts can be refined based on their relation to seen classes. Second, the annotated training data serves as negative samples for unseen HOIs. If the prediction score for an unseen class is too high, the model is penalized (Eq. 17 in the Appendix). ", "page_idx": 5}, {"type": "text", "text": "3.3 Deep Visual-Text Prompt Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We denote a set of learnable text prompts as $\\mathcal{H}_{\\mathcal{T}}=[\\tilde{h_{T}^{1}},\\tilde{h_{T}^{2}},\\cdots,\\tilde{h_{T}^{N}}]$ and learnable visual prompts as $\\mathcal{H}_{\\mathcal{V}}=[\\hat{h_{V}^{1}},\\hat{h_{V}^{2}},\\cdot\\cdot\\cdot\\hat{\\mathbf{\\Omega}},\\hat{h_{V}^{N}}]$ . $\\tilde{h_{T}^{i}}\\in\\mathcal{R}^{C*p*d_{t}},\\hat{h_{V}^{i}}\\in\\mathcal{R}^{p*d_{v}},i=1,2,\\cdots,N.\\ N$ means we intend to introduce new learnable prompts from the first layer to layer $N$ . ", "page_idx": 5}, {"type": "text", "text": "Deep Text Prompt Learning The text encoder is composed of $K$ transformer layers $\\{\\mathcal{T}_{i}\\}_{i=1}^{K}$ .The CLIP text encoder generates text features by tokenizing the words and converting them into word embeddings $W_{0}=\\overline{{[w_{0}^{1},w_{0}^{2},\\cdot\\cdot\\cdot\\cdot,w_{0}^{P}]}}\\in\\mathcal{R}^{\\check{P}*d_{t}}$ . Text features $W_{i}$ will be processed by $i^{\\mathrm{th}}$ layer of the text transformer: ", "page_idx": 5}, {"type": "equation", "text": "$$\n[W_{i+1},\\_]={\\mathcal{T}}_{i}(W_{i},\\tilde{h_{T}^{i}}),\\ i=1,2,\\cdots,N,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n[W_{i+1},h_{T}^{\\tilde{i}+1}]={\\cal T}_{i}(W_{i},\\tilde{h_{T}^{i}}),\\;i=N+1,N+2,\\cdot\\cdot\\cdot\\,,K.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The final text representation $W_{t}$ can be obtained by: ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{t}=\\mathrm{TextProj}(W_{K}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W_{t}\\in\\mathcal{R}^{1\\ast d_{a}}$ and $d_{a}$ is the feature dimension of the final aligned text and visual features in CLIP. Since our learnable text prompt $\\tilde{h_{T}^{i}}$ is specific for each HOI class, the whole text representation can be obtained $\\mathcal{W}_{t}=[W_{t_{1}},W_{t_{2}},\\cdot\\cdot\\cdot\\;,\\dot{W}_{t_{C}}]$ . ", "page_idx": 5}, {"type": "text", "text": "Deep Visual Prompt Learning The visual encoder is also composed of $K$ transformer layers $\\{\\nu_{i}\\}_{i=1}^{K}$ . After each layer $\\nu_{i}$ , there is an adapter $A_{i}$ to inject object position and category information [27]. CLIP visual encoder splits image $\\mathcal{T}$ into $D=d_{p}*d_{p}$ fixed-size patches, which are projected to obtain visual features $E_{1}\\in\\mathcal{R}^{D\\ast d_{v}}$ . Visual features $E_{i}$ are processed by $i^{\\mathrm{th}}$ layer of the visual transformer: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[c_{i+1},E_{i+1},\\underline{{\\big]}}=A_{i}(\\mathcal{V}_{i}(c_{i},E_{i},\\hat{h_{V}^{i}})),\\;i=1,2,\\cdots,N,}\\\\ &{[c_{i+1},E_{i+1},\\underline{{h_{V}^{i+1}}}]=A_{i}(\\mathcal{V}_{i}(c_{i},E_{i},\\hat{h_{V}^{i}})),\\;i=N+1,N+2,\\cdots,K,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c_{i}$ is the class token for the $i^{\\mathrm{th}}$ layer. ", "page_idx": 5}, {"type": "text", "text": "The final visual representation $E_{v}\\in\\mathcal{R}^{1*d_{a}}$ can be computed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nE_{v}=\\mathrm{VisualProj}(E_{K}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$E_{v}\\in\\mathcal{R}^{D*d_{v}}$ can be re-sized to $E_{v}\\in\\mathcal{R}^{d_{p}*d_{p}*d_{v}}$ . Since off-the-shelf detectors can provide object detection results, we select bounding boxes with confidence score $s c>\\theta$ and apply RoI-Align [13] to obtain features for each detected human and object $E_{\\mathrm{hum}},E_{\\mathrm{obj}}$ . Then, the intra-HOI feature fusion extracts HOI feature $E_{\\mathrm{hoi}}\\in\\mathcal{R}^{1*d_{a}}$ from $E_{\\mathrm{hum}},E_{\\mathrm{obj}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nE_{\\mathrm{hoi}}=M L P(E_{\\mathrm{hum}},E_{\\mathrm{obj}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M L P$ represents multi-layer perception. In any given image $\\mathcal{T}$ , multiple humans and objects may be present, leading to the detection of several potential human-object pairs. Assume there are $q$ human-object (H-O) pairs in image $\\mathcal{T}$ and denote all HOI features as $\\bar{\\mathcal{E}}_{\\mathrm{hoi}}^{\\mathcal{T}}\\dot{=}[E_{\\mathrm{hoi}}^{1},E_{\\mathrm{hoi}}^{2},\\cdot\\cdot\\cdot,E_{\\mathrm{hoi}}^{q}]$ Incorporating surrounding HOI features enriches context information for each HOI visual feature by capturing additional human-object relational details and interactions, thereby enhancing HOI features. We name this process inter-HOI feature fusion. Thus, final visual representation $\\tilde{\\mathcal{E}}_{\\mathrm{hoi}}^{\\mathcal{I}}$ is obtained by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\small\\tilde{\\mathcal{E}}_{\\mathrm{hoi}}^{\\mathbb{Z}}=W_{\\mathrm{up}}\\cdot\\mathrm{MHSA}(W_{\\mathrm{down}}\\cdot\\mathcal{E}_{\\mathrm{hoi}}^{\\mathbb{Z}})+\\mathcal{E}_{\\mathrm{hoi}}^{\\mathbb{Z}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where MHSA means multi-head self-attention [50] and $\\tilde{\\mathcal{E}}_{\\mathrm{hoi}}^{\\mathcal{T}}=[\\tilde{E}_{\\mathrm{hoi}}^{1},\\tilde{E}_{\\mathrm{hoi}}^{2},\\cdot\\cdot\\cdot,\\tilde{E}_{\\mathrm{hoi}}^{q}]$ . Then, we can calculate the prediction for each H-O pair $\\mathrm{hoi}_{i}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\mathrm{hoi}}(c|\\mathrm{hoi}_{i})=\\frac{\\exp(\\tilde{E}_{\\mathrm{hoi}}^{i}\\cdot(W_{t_{c}})^{T})}{\\sum_{k=1}^{C}\\exp(\\tilde{E}_{\\mathrm{hoi}}^{i}\\cdot(W_{t_{k}})^{T})},\\;c=1,2,\\cdots,C.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Please refer to Appendix Section 7.5 for more details for training and inference. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "R1Rrb2d5BH/tmp/919f1d346216f1e44b4dadc5e025bd780ce471f08e60e44c7601cf75b51d2093.jpg", "table_caption": ["Table 1: Unseen-verb (UV) comparison on HICO-DET with state-of-the-art methods. \\* indicates the model size is estimated according to papers [41, 4]. \u201cTP\u201d denotes the trainable parameters. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "R1Rrb2d5BH/tmp/83d9ff8cbb9ebc5f6757ae0cf39e5dfc5531e4d999b198dfbea397257644c04a.jpg", "table_caption": ["Table 2: Rare-first unseen-composition (RF-UC) and Nonrare-first unseen composition (NF-UC) comparison on HICO-DET with state-of-the-art methods. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Zero-Shot HOI Setting Definition ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our method follows the existing zero-shot HOI setting, which involves predicting unseen HOI classes and typically includes using unseen class names during training [14, 15, 16, 44, 51]. In particular, VCL, FCL and ATL [14, 16, 15] \u201ccompose novel HOI samples\u201d during training with the unseen (novel) HOI class names. EoID [51] distills CLIP \u201cwith predefined HOI prompts\u201d including both seen and unseen class names. HOICLIP [44] introduces \u201cverb class representation\u201d during training, including both seen and unseen classes. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our method on HICO-DET by following the established protocol of zero-shot two-stage HOI detection methods [14, 3, 27]. Our object detector utilizes a pre-trained DETR model [5] with a ResNet50 backbone [12]. As for our learnable prompts design, we set $p=2,N=9$ . The LLaVA-v1.5-7b model [37] is used to provide text description, as explained in Section 3.1 and 3.2. For all experiments, our batch size is set as 16 on 4 Nvidia A5000 GPUs. We use AdamW [39] as the optimizer and the initial learning rate is 1e-3. For more implementation details, please refer to Appendix Section 7.1. ", "page_idx": 6}, {"type": "text", "text": "4.3 Comparison with State-of-the-Art Zero-Shot HOI Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Unseen-Verb Setting In Table 1, we compare our method to existing zero-shot HOI detection approaches under the unseen-verb zero-shot setting. The results demonstrate that our method not only achieves competitive performance but also requires only $10.35\\%$ to $33.95\\%$ of the trainable parameters compared to existing zero-shot HOI detection methods thanks to our novel prompt learning design. While our method shows a minor drop in performance compared to CLIP4HOI under the unseen verb setting, it is important to consider the significant reduction in trainable parameters that our method achieves\u2014 $.87.9\\%$ fewer than CLIP4HOI. In contrast, the existing HOI methods [33, 44, 41, 4] that align with VLMs unanimously require significantly more trainable ", "page_idx": 6}, {"type": "text", "text": "Table 3: Unseen-object (UO) comparison on HICO-DET with state-of-the-art methods. \\* indicates the model size is estimated according to papers [41, 4]. $\\dagger$ denotes methods that use a DETR object detection model pretrained on HICO-DET. Results without $\\dagger$ indicate the use of a DETR object detection model pretrained on MS-COCO. \u201cTP\u201d denotes the trainable parameters. ", "page_idx": 7}, {"type": "table", "img_path": "R1Rrb2d5BH/tmp/a7c22f25f7093495b1552ea98241419eefa2cb7f33b113a9ff260e81b638b0d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "model parameters (e.g., $42.05\\mathrm{M}\\sim66.18\\mathrm{M}$ with ViT-B visual encoder), whereas our method operates efficiently with only 6.85M trainable parameters. ", "page_idx": 7}, {"type": "text", "text": "UniHOI [4] is the state-of-the-art method in the unseen verb (UV) setting. Compared to UniHOI which aligns HOI features to BLIP [29] text embeddings, our method achieves improved performance, especially in the unseen category with a 2.77 increase in mAP. At the same time, our model requires only $26.9\\%$ of the trainable parameters compared to UniHOI. This demonstrates the effectiveness and efficiency of our proposed prompt learning framework in adapting the VLM for zero-shot HOI tasks. ", "page_idx": 7}, {"type": "text", "text": "Unseen-Composition Setting In Table 2, we provide the zero-shot performance comparison in rare-first unseen-composition (RF-UC) and nonrare-first unseen-composition (NF-UC) settings. With the ViT-B visual encoder, our method establishes a new standard for unseen-class performance across both RF and NF settings, outperforming the previous state-of-the-art method, CLIP4HOI [41], while requiring only $12.08\\%$ of its trainable parameters. Compared to UniHOI with ViT-L visual encoder [4], our method surpasses it by $5.56\\;\\mathrm{mAP}$ in unseen performance of the RF-UC setting and by $7.88\\;\\mathrm{mAP}$ in the NF-UC setting with only $26.9\\%$ of the trainable parameters of UniHOI. ", "page_idx": 7}, {"type": "text", "text": "Unseen-Object Setting We provide the unseen-object setting comparison in Table 3. Following previous methods [44, 33], we employ a DETR object detector pre-trained on MS-COCO [34] for object detection. To keep consistent with CLIP4HOI [41] and FCL [16], we also provide the results by using the DETR model pre-trained on HICO-DET [27] (marked with $\\dagger$ in Table 3). CLIP4HOI [41] is the state-of-the-art method in unseen performance. With the same object detector pre-trained on HICO-DET, our method shows improved performance for unseen class prediction, outperforming CLIP4HOI by $1.49\\;\\mathrm{mAP},$ , with only $12.08\\%$ of its trainable model parameters. UniHOI [4] is the state-of-the-art method in seen performance. With the same object detector as UniHOI, our method outperforms UniHOI in unseen category by $13.36\\;\\mathrm{mAP}.$ ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct the ablation study for our text and visual prompt learning design in Table 4. The first row shows the performance of our baseline, MaPLe [22]. Our intra-HOI fusion module improves the seen HOI performance by $7.41\\;\\mathrm{mAP}$ , as shown in the second row. In the third row, the inclusion of the visual adapter [27] enhances performance for seen classes while negatively affecting unseen HOI performance. ", "page_idx": 7}, {"type": "text", "text": "Comparing the third row and fourth row, LLM guidance, shown in Fig. 2, notably enhances unseenclass learning, resulting in a $1.52\\mathrm{\\mAP}$ improvement. LLM guidance, by integrating learnable text prompts with detailed HOI class descriptions, improves the model\u2019s understanding of unseen classes, providing richer information compared to simple class names. Additionally, the inclusion of the UTPL module significantly boosts unseen class performance, with a $2.42\\;\\mathrm{mAP}$ improvement. Later, the inter-HOI fusion benefits both the seen and unseen learning by providing more context information for visual HOI features. Finally, the VLM guidance effectiveness is evaluated, which enhances the unseen performance by $1.33\\;\\mathrm{mAP}.$ ", "page_idx": 7}, {"type": "table", "img_path": "R1Rrb2d5BH/tmp/5794ac2eba20e83b387e57a7f246cd3009cf6f059611428a82fee0f468be51fa.jpg", "table_caption": ["Table 4: Ablation study of our method in zero-shot unseen verb setting on HICO-DET. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "R1Rrb2d5BH/tmp/de060655ef35b4f575bdbcb7754066cfebcc6450cc1ee42b6b5aeea8d4f33301.jpg", "table_caption": ["Table 5: Prompt learning evaluation in zero-shot unseen verb setting on HICO-DET. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We also provide the experimental comparison with CLIP [47] and MaPLe [22] in Table 5. Since they are designed for general image classification, directly applying it to HOI tasks for comparison is unfair as they may not focus on the human-object region features. Therefore, we crop the union region from the original visual features for each human-object pair to obtain the HOI feature. HOI prediction is obtained by using cosine similarity between HOI and text features for each class. ", "page_idx": 8}, {"type": "text", "text": "As shown in Table 5, although with learnable prompts, MaPLe [22] outperforms CLIP on seen classes, its performance on unseen classes is far behind that of the seen classes, showing reduced generalization capability. Compared to our method, MaPLe lags significantly behind on unseen performance, with an 11.63 decrease in mAP. Additionally, the third row introduces another baseline that incorporates MaPLe with visual adapter (i.e., $\\boldsymbol{A}_{i}$ ) [27], which is also used in our framework, ensuring a fair comparison. The performance improvement from the third row to the fourth row, with gains of $5.93\\;\\mathrm{mAP}$ for unseen classes and $3.86\\;\\mathrm{mAP}$ for seen classes, clearly demonstrates the effectiveness of our method in zero-shot HOI detection. ", "page_idx": 8}, {"type": "text", "text": "4.5 Qualitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Fig. 4 shows the qualitative results of MaPLe [22] and our method in the unseen-verb zero-shot setting. In particular, MaPLe struggles to detect unseen classes, either missing unseen HOI classes or predicting the wrong unseen HOI classes. For example, if an image only contains unseen classes, MaPLe tends to predict wrong seen classes and miss the correct ones. As shown in the bottom right of Fig. 4, this image contains unseen class only (\"wear tie\"), MaPLe predicts related wrong seen classes such as \"pulling tie\" and \"adjusting tie\", but fails to predict the ground-truth unseen HOI (\"wear tie\"). This shows the limited generalization ability of MaPLe to unseen classes. In contrast to MaPLe, our method can predict both seen and unseen classes more accurately. ", "page_idx": 8}, {"type": "text", "text": "4.6 Fully Supervised Setting for HOI Detection ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted the fully-supervised experiments on both the HICO-DET and V-COCO benchmarks [6, 34]. Our method achieves a competitive $38.61\\mathrm{~mAP}$ on the HICO-DET dataset, with a smaller performance drop between rare and non-rare classes $(1.19\\,\\mathrm{mAP})$ compared to AGER [49] $(4.18\\;\\mathrm{mAP})$ ), the current state-of-the-art one-stage method. Our method also outperforms the best-performing zero-shot HOI method, CLIP4HOI [41], by $3.28\\;\\mathrm{mAP}$ on HICO-DET. On the V-COCO benchmark, our method achieves state-of-the-art performance with 66.2 AP in Scenario 2. Detailed discussions and comparisons are provided in the Appendix 7.6. ", "page_idx": 8}, {"type": "image", "img_path": "R1Rrb2d5BH/tmp/92a49f67f71f7449dcd96b3b122f34ce13bf4cb9d135cf6e9095ffe8b605bd25.jpg", "img_caption": ["Figure 4: Qualitative comparison with MaPLe [22] for unseen-verb zero-shot HOI detection.The orange bar represents the unseen class prediction and the blue bar means the seen class prediction. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.7 Discussion of Descriptors from LLMs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Descriptors, developed to leverage the LLMs for downstream tasks [42, 20], involve attribute decomposition to benefit category recognition. We adopt the attribute decomposition concept from DVDet [20] and tailor it to our EZ-HOI framework. Following DVDet, we generate action descriptors for each class and integrate them into HOI class text features, thus enhancing the detail and distinctiveness of class representations. Descriptors with low cosine similarity to the class text features are discarded to avoid noise. With our straightforward adoption of action descriptors, the result shows $0.31\\;\\mathrm{mAP}$ improvement compared to our EZ-HOI, achieving $32.63\\mathrm{~mAP}$ under the unseen-verb setting. This indicates that LLM-generated descriptions, such as action descriptors, hold potential to enhance HOI detection and are worth further exploration. ", "page_idx": 9}, {"type": "text", "text": "4.8 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our method has some limitations. First, while our method significantly reduces the number of trainable parameters compared to existing approaches, it still requires fine-tuning on HOI-specific datasets such as HICO-DET. A training-free design, which could be used directly for HOI detection without the need for fine-tuning, would be more desirable. Second, zero-shot HOI detection requires the pre-definition of all HOI classes, both seen and unseen, following the current zero-shot HOI detection setting. This requirement restricts the generalizability to truly unseen classes. Consequently, our future work will aim to develop a robust open-category HOI detector that operates effectively without the need for predefined classes. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced the Efficient Zero-shot HOI Detection (EZ-HOI) approach, utilizing prompt learning to adapt visual-language models for zero-shot HOI detection. This method not only maintains competitive performance for unseen classes but also innovatively integrates learnable visual and text prompts. These prompts leverage foundation model information, thereby enriching prompt knowledge and enhancing the adaptability of VLMs. A significant challenge we addressed is the compromised performance on unseen classes, resulting from the training dataset containing only seen-class labels. To counter this, we developed a text prompt learning strategy that utilizes information from related seen classes to support the detection of unseen classes. We also employed an LLM to provide the nuanced differences between related seen and unseen classes, improving our method for unseen class prompt learning. Our method has demonstrated state-of-the-art performance across various zero-shot HOI detection settings while requiring only a third of the trainable parameters compared to existing methods. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research/project is supported by the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yihao Ai, Yifei Qi, Bo Wang, Yu Chen, Xinchao Wang, and Robby T. Tan. Domain-adaptive 2d human pose estimation via dual teachers in extremely low-light conditions. In ECCV, 2024.   \n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 23716\u201323736, 2022.   \n[3] Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, and Rama Chellappa. Detecting human-object interactions via functional generalization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10460\u201310469, 2020.   \n[4] Yichao Cao, Qingfei Tang, Xiu Su, Song Chen, Shan You, Xiaobo Lu, and Chang Xu. Detecting any human-object interaction relationship: Universal hoi detector with spatial prompt learning on foundation models. Advances in Neural Information Processing Systems, 36, 2024.   \n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.   \n[6] Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia Deng. Learning to detect humanobject interactions. In 2018 ieee winter conference on applications of computer vision (wacv), pages 381\u2013389. IEEE, 2018.   \n[7] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. Plot: Prompt learning with optimal transport for vision-language models. arXiv preprint arXiv:2210.01253, 2022.   \n[8] Junwen Chen and Keiji Yanai. Qahoi: query-based anchors for human-object interaction detection. arXiv preprint arXiv:2112.08647, 2021.   \n[9] Eulrang Cho, Jooyeon Kim, and Hyunwoo J Kim. Distribution-aware prompt tuning for visionlanguage models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22004\u201322013, 2023.   \n[10] Georgia Gkioxari, Ross Girshick, Piotr Doll\u00e1r, and Kaiming He. Detecting and recognizing human-object interactions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8359\u20138367, 2018.   \n[11] Albert Gordo and Diane Larlus. Beyond instance-level image retrieval: Leveraging captions to learn a global visual representation for semantic retrieval. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6589\u20136598, 2017.   \n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[13] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.   \n[14] Zhi Hou, Xiaojiang Peng, Yu Qiao, and Dacheng Tao. Visual compositional learning for humanobject interaction detection. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XV 16, pages 584\u2013600. Springer, 2020.   \n[15] Zhi Hou, Baosheng Yu, Yu Qiao, Xiaojiang Peng, and Dacheng Tao. Affordance transfer learning for human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 495\u2013504, 2021.   \n[16] Zhi Hou, Baosheng Yu, Yu Qiao, Xiaojiang Peng, and Dacheng Tao. Detecting human-object interaction via fabricated compositional learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14646\u201314655, 2021.   \n[17] Zhi Hou, Baosheng Yu, and Dacheng Tao. Discovering human-object interaction concepts via self-compositional learning. In European Conference on Computer Vision, pages 461\u2013478. Springer, 2022.   \n[18] ASM Iftekhar, Hao Chen, Kaustav Kundu, Xinyu Li, Joseph Tighe, and Davide Modolo. What to look at and where: Semantic and spatial refined transformer for detecting human-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5353\u20135363, 2022.   \n[19] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709\u2013727. Springer, 2022.   \n[20] Sheng Jin, Xueying Jiang, Jiaxing Huang, Lewei Lu, and Shijian Lu. Llms meet vlms: Boost open vocabulary object detection with fine-grained descriptors. arXiv preprint arXiv:2402.04630, 2024.   \n[21] Baoshuo Kan, Teng Wang, Wenpeng Lu, Xiantong Zhen, Weili Guan, and Feng Zheng. Knowledge-aware prompt tuning for generalizable vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15670\u201315680, 2023.   \n[22] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19113\u201319122, 2023.   \n[23] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15190\u201315200, 2023.   \n[24] Sanghyun Kim, Deunsol Jung, and Minsu Cho. Relational context learning for human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2925\u20132934, 2023.   \n[25] Dongjun Lee, Seokwon Song, Jihee Suh, Joonmyeong Choi, Sanghyeok Lee, and Hyunwoo J Kim. Read-only prompt optimization for vision-language few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1401\u20131411, 2023.   \n[26] Qinqian Lei, Bo Wang, and Robby T Tan. Few-shot learning from augmented label-uncertain queries in bongard-hoi. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2974\u20132982, 2024.   \n[27] Ting Lei, Fabian Caba, Qingchao Chen, Hailin Jin, Yuxin Peng, and Yang Liu. Efficient adaptive human-object interaction detection with concept-guided memory. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6480\u20136490, 2023.   \n[28] Ting Lei, Shaofeng Yin, and Yang Liu. Exploring the potential of large foundation models for open-vocabulary hoi detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16657\u201316667, 2024.   \n[29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.   \n[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.   \n[31] Liulei Li, Jianan Wei, Wenguan Wang, and Yi Yang. Neural-logic human-object interaction detection. Advances in Neural Information Processing Systems, 36, 2024.   \n[32] Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, and Cewu Lu. Hoi analysis: Integrating and decomposing human-object interaction. Advances in Neural Information Processing Systems, 33:5011\u20135022, 2020.   \n[33] Yue Liao, Aixi Zhang, Miao Lu, Yongliang Wang, Xiaobo Li, and Si Liu. Gen-vlkt: Simplify association and enhance interaction understanding for hoi detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20123\u2013 20132, June 2022.   \n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[35] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.   \n[36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.   \n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[38] Xinpeng Liu, Yong-Lu Li, Xiaoqian Wu, Yu-Wing Tai, Cewu Lu, and Chi-Keung Tang. Interactiveness field in human-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20113\u201320122, 2022.   \n[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[40] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. Swapprompt: Test-time prompt adaptation for vision-language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[41] Yunyao Mao, Jiajun Deng, Wengang Zhou, Li Li, Yao Fang, and Houqiang Li. Clip4hoi: Towards adapting clip for practical zero-shot hoi detection. Advances in Neural Information Processing Systems, 36, 2024.   \n[42] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. arXiv preprint arXiv:2210.07183, 2022.   \n[43] Gyeongsik Moon, Heeseung Kwon, Kyoung Mu Lee, and Minsu Cho. Integralaction: Posedriven feature integration for robust human action recognition in videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3339\u20133348, 2021.   \n[44] Shan Ning, Longtian Qiu, Yongfei Liu, and Xuming He. Hoiclip: Efficient knowledge transfer for hoi detection with vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23507\u201323517, 2023.   \n[45] Jeeseung Park, Jin-Woo Park, and Jong-Seok Lee. Viplo: Vision transformer based poseconditioned self-loop graph for human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17152\u201317162, 2023.   \n[46] Xian Qu, Changxing Ding, Xingao Li, Xubin Zhong, and Dacheng Tao. Distillation using oracle queries for transformer-based human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19558\u201319567, 2022.   \n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[48] Masato Tamura, Hiroki Ohashi, and Tomoaki Yoshinaga. Qpic: Query-based pairwise humanobject interaction detection with image-wide contextual information. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10410\u201310419, 2021.   \n[49] Danyang Tu, Wei Sun, Guangtao Zhai, and Wei Shen. Agglomerative transformer for humanobject interaction detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 21614\u201321624, October 2023.   \n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[51] Mingrui Wu, Jiaxin Gu, Yunhang Shen, Mingbao Lin, Chao Chen, and Xiaoshuai Sun. End-toend zero-shot hoi detection via vision and language knowledge distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2839\u20132846, 2023.   \n[52] Chi Xie, Fangao Zeng, Yue Hu, Shuang Liang, and Yichen Wei. Category query learning for human-object interaction classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15275\u201315284, 2023.   \n[53] Bangpeng Yao and Li Fei-Fei. Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses. IEEE transactions on pattern analysis and machine intelligence, 34(9):1691\u20131703, 2012.   \n[54] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring visual relationship for image captioning. In Proceedings of the European conference on computer vision (ECCV), pages 684\u2013699, 2018.   \n[55] Hangjie Yuan, Jianwen Jiang, Samuel Albanie, Tao Feng, Ziyuan Huang, Dong Ni, and Mingqian Tang. Rlip: Relational language-image pre-training for human-object interaction detection. Advances in Neural Information Processing Systems, 35:37416\u201337431, 2022.   \n[56] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Samuel Albanie, Yining Pan, Tao Feng, Jianwen Jiang, Dong Ni, Yingya Zhang, and Deli Zhao. Rlipv2: Fast scaling of relational languageimage pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21649\u201321661, 2023.   \n[57] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Unified vision and language prompt learning. arXiv preprint arXiv:2210.07225, 2022.   \n[58] Frederic Z Zhang, Dylan Campbell, and Stephen Gould. Spatially conditioned graphs for detecting human-object interactions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13319\u201313327, 2021.   \n[59] Frederic Z Zhang, Dylan Campbell, and Stephen Gould. Efficient two-stage detection of humanobject interactions with a novel unary-pairwise transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20104\u201320112, 2022.   \n[60] Frederic Z Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao Zhong, and Stephen Gould. Exploring predicate visual context in detecting of human-object interactions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10411\u201310421, 2023.   \n[61] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16816\u201316825, 2022.   \n[62] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.   \n[63] Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu Zhao, Boxun Li, Chenguang Zhang, Chi Zhang, Yichen Wei, et al. End-to-end human object interaction detection with hoi transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11825\u201311834, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "7 Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "7.1 Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we provide detailed implementation information. For the pre-trained CLIP model with the ViT-B visual encoder, the visual feature dimension $d_{v}=768$ , while the text feature dimension $d_{t}=512$ and the final feature dimension for aligned visual and text features $d_{a}=512$ . For the CLIP model with the ViT-L visual encoder, $d_{v}=102\\bar{4},d_{t}=768,d_{a}=768.$ . We use an off-the-shelf object detector and add a threshold $\\theta$ to fliter out some low-confident predictions and we set $\\theta=0.2$ . Since UniHOI [4] and CLIP4HOI [41] have not released their code, we estimate the trainable model parameters by modifying the HOICLIP [44] code according to the description in UniHOI and CLIP4HOI, which means that if some details are not mentioned in the UniHOI and CLIP4HOI papers, we use the HOICLIP design by default. In addition, we initialize the weights of all $W_{u p}$ to 0, which stabilizes training by gradually fine-tuning the attention outputs in Eq. 2, Eq. 3, Eq. 5 and Eq. 11. ", "page_idx": 15}, {"type": "text", "text": "Dataset and Evaluation Metrics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conducted extensive experiments using the widely-recognized HICO-DET [6] dataset for zeroshot HOI detection. HICO-DET comprises a total of 47,776 images, divided into 38,118 training images and 9,658 test images. This dataset features 600 Human-Object Interaction (HOI) classes, which are combinations derived from 117 action categories and 80 object categories. Our model\u2019s performance was evaluated in four distinct zero-shot HOI detection settings, categorized by the criterion for selecting the unseen HOI classes: rare-first unseen composition (RFUC), nonrare-first unseen composition (NFUC), unseen object (UO), and unseen verb (UV). These settings align with methodologies from previous research [33, 44, 41, 4]. ", "page_idx": 15}, {"type": "text", "text": "Additionally, we also evaluate our model in the fully supervised setting on both the HICO-DET dataset and the V-COCO [34] dataset. V-COCO is a subset of COCO, with 10,396 images\u2014split into 5,400 train-val images and 4,946 test images, and it encompasses 24 action classes and 80 object classes. Following standard evaluation protocols, we assessed our model using mean average precision (mAP) on the HICO-DET benchmark, while Average Precision (AP) in both Scenario 1 and Scenario 2 is used on the V-COCO benchmark [63, 44]. A prediction was deemed a true positive if the HOI classification was accurate and the Intersection over Union (IoU) between the predicted human and object bounding boxes and the ground-truth bounding boxes exceeded 0.5 [33, 27]. ", "page_idx": 15}, {"type": "text", "text": "7.2 HOI Class Description ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We utilize the LLM to provide detailed HOI class descriptions to guide the text prompt learning and we provide an example for detailed illustration. We take the HOI class \u201cSwing a baseball bat\u201d as an example, which is one unseen HOI class in the unseen-verb zero-shot setting on the HICO-DET benchmark. The generated HOI class description is ", "page_idx": 15}, {"type": "text", "text": "\u201cSwinging a baseball bat\u201d describes a person using a baseball bat to hit a ball. This action typically involves the person holding the bat with both hands, standing in a stance with their feet shoulder-width apart, and using their body rotation to contact the ball. ", "page_idx": 15}, {"type": "text", "text": "7.3 Disparity Information for UTPL Module ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The disparity information mentioned in Section 3.2 aims to explore the difference between the unseen class and the related seen class. Here we demonstrate it with a specific example. We take the unseen class \u201c hose a dog\u201d as an example and its selected seen class is \u201cwash a dog\u201d.The input text prompt for LLM to acquire the disparity description between the two classes is designed as \u201c Describe the definition of the phrase: hosing a dog and please focus on the attributes different from another phrase: washing a dog. \u201d Then we can obtain the detailed disparity description from LLM as ", "page_idx": 15}, {"type": "text", "text": "The phrase \"a person hosing a dog\" refers to the action of washing a dog using a hose. This action is different from \"a person washing a dog\" as the person is using a hose to clean the dog, rather than simply using their hands to wash the dog. The use of a hose adds an additional element of water pressure and flow, which can make the cleaning process more efficient and effective. ", "page_idx": 15}, {"type": "image", "img_path": "R1Rrb2d5BH/tmp/d9223683bb0ff53e7fac7fe68f31b3b32d789058ffd18b46ba84df6588b8192b.jpg", "img_caption": ["Figure 5: Detailed architecture for HOI feature fusion design. Intra-HOI feature fusion aims to extract HOI features from possible human region and object region features. Inter-HOI feature fusion aims to enhance the HOI features by incorporating the surrounding HOI feature context. \u201cMHSA\u201d refers to multi-head self-attention. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "7.4 HOI Feature Fusion ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As shown in Fig. 5, we also provide the detailed architecture of the intra- and inter-HOI feature fusion design. The intra-HOI feature fusion integrates human and object features for each human-object pair. To enrich context information, all HOI features are processed together using multi-head self-attention (MHSA), allowing each feature to become aware of its surroundings. This enhancement improves the context and overall performance of each HOI feature. ", "page_idx": 16}, {"type": "text", "text": "7.5 Training and Inference ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Since $\\mathcal{W}_{t}\\in\\mathcal{R}^{C*d_{t}}$ related to the number of total HOI classes, if the number is too large, the learnable prompts can be computationally expensive. Thus, we only select part of HOI classes and do action classification. Later, by combining action prediction with object detection results, we can obtain HOI prediction finally. Specifically, we select two HOI classes for each action with the following equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{hoi}_{i},\\mathrm{hoi}_{j}=\\underset{0<i,j\\leq C}{\\arg\\operatorname*{min}}\\,f_{t x t_{i}}\\cdot(f_{t x t_{j}})^{T},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which means we select two HOI classes containing the same action with the most different semantic meanings. Since some actions can have multiple interpretations in different contexts (e.g., \"hold apple\" vs. \"hold sheep\"), randomly choosing two HOI classes with the same action does not cover the comprehensive meanings of the action. By selecting the most different HOI classes, we aim to capture a richer range of information for the action. Then, We calculate the HOI prediction score for $i^{\\mathrm{th}}$ H-O pair by: ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\mathrm{hoi}}(c|\\mathrm{hoi}_{i})=\\frac{\\exp(\\tilde{E}_{\\mathrm{hoi}}^{i}\\cdot(W_{t_{c}})^{T})}{\\sum_{k=1}^{C}\\exp(\\tilde{E}_{\\mathrm{hoi}}^{i}\\cdot(W_{t_{k}})^{T})},\\;c=1,2,\\cdots,C.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The action scores can be obtained by: ", "page_idx": 16}, {"type": "equation", "text": "$$\ns_{a}=p_{\\mathrm{hoi}}*\\tilde{l}_{\\mathrm{align}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\tilde{l}_{\\mathrm{align}}\\in\\mathcal{R}^{C*d_{a}}$ means the action label for each selected HOI class. We adopt focal loss [35] $l_{f o c a l}$ to train the action prediction. We also design a class-relation loss shown in the following equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l_{\\mathrm{relation}}=D_{K L}[\\sin(\\mathcal{W}_{t},\\mathcal{W}_{t})||\\sin(\\mathcal{F}_{\\mathrm{txt}},\\mathcal{F}_{\\mathrm{txt}})],}\\\\ &{\\sin(X,Y)=X^{T}\\cdot Y}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "to keep the relation between adapted text class embeddings $W_{t}$ close to the original text embeddings of the HOI class description $f_{\\mathrm{txt}}.\\;D_{K L}[\\cdot||\\cdot]$ denotes the KL divergence. Therefore, the training loss $L_{\\mathrm{train}}$ is computed by: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{\\mathrm{train}}=l_{\\mathrm{focal}}+\\alpha l_{\\mathrm{relation}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $\\alpha$ is a hyperparameter. In the inference stage, we can obtain the HOI score for each human-object pair by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{h,o}^{a}=(s_{h}*s_{o})^{\\tau}*\\sigma(s_{a}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $s_{h},s_{o}$ are the object detection confidence scores for humans and objects. $\\tau$ is a hyperparameter.   \nIn Eq. 17, $\\alpha=150$ , and in Eq. 18, $\\tau=1$ during training and $\\tau=2.8$ during inference [58, 59]. ", "page_idx": 17}, {"type": "text", "text": "7.6 Quantitative Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fully Supervised HOI Detection Setting We conduct experiments under the fully supervised HOI detection setting on both the HICO-DET [6] and V-COCO [34] dataset, as shown in Table 6. In the UTPL module, we utilize common class prompts as inputs. This approach enables common HOIs to learn from rare ones, fostering a better differentiation between them. This strategy helps to alleviate overftiting to common HOI classes, thereby improving performance for both seen and unseen classes. As shown in Table 6, the performance drop between rare and non-rare classes $\\mathrm{1.19\\mAP)}$ is much smaller than AGER $\\left(4.18\\;\\mathrm{mAP}\\right)$ , the best performance among the one-stage method. Additionally, our method achieves the best performance among the two-stage methods on the HICO-DET [6] dataset, outperforming CLIP4HOI [41] by $3.28\\;\\mathrm{mAP}.$ ", "page_idx": 17}, {"type": "text", "text": "As for the V-COCO benchmark, we achieve competitive performance among the two-stage methods with 66.2 AP performance under Scenario 2 evaluation. However, the smaller number of verb classes in V-COCO (24 classes), which have weaker connections (i.e., jumping vs. skateboarding skateboard), compared to HICO-DET (117 verb categories), which exhibits stronger connections (i.e., jumping vs. filpping skateboard), limits the potential of our method. Our UTPL design requires the learnable prompts to extract information from prompts of other classes, which also aids in better differentiation between similar classes. Due to the weakly connected classes in V-COCO, our UTPL design cannot work effectively. Despite this, we achieve performance comparable to CLIP4HOI [41], further demonstrating the effectiveness of our method. ", "page_idx": 17}, {"type": "table", "img_path": "R1Rrb2d5BH/tmp/a66711b389e452070c5f291538b3759567947733d64fe8aa3bfaa031e0ee1b77.jpg", "table_caption": ["Table 6: State-of-the-art Comparison on HICO-DET and V-COCO in the fullysupervised setting. Bold highlights the best-performing method within each of the two groups: one-stage and two-stage methods. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "7.7 Ablation Studies ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 7 shows the ablation study for the hyperparameter $N$ , where $N$ means we introduce learnable prompts from the first layer until layer $N$ in both visual and text encoders. We use $\\scriptstyle\\mathrm{N}=9$ in our main paper because it shows the best unseen performance. ", "page_idx": 17}, {"type": "text", "text": "Table 8 shows the ablation study for the different positions of learnable prompts. Position $i-j$ means we insert learnable prompts from layer $i$ to layer $j$ in both the text encoder and visual encoder. Here we always insert learnable prompts into 9 layers and only change the position. We find that the position of learnable prompts does not affect the outcome too much. Thus, we follow [22] and fine-tune layers 1-9, which show slightly better unseen performance. ", "page_idx": 17}, {"type": "text", "text": "Table 7: Ablation study for hyperparameter N under the unseen-verb setting. ", "page_idx": 18}, {"type": "table", "img_path": "R1Rrb2d5BH/tmp/eba02dc7f46293012ec73fb7cb4695a154f2fd78e9e04ec2729f613795e9bd19.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "R1Rrb2d5BH/tmp/7481c3f3ccd7d2c71e0dcd88ffa64ca0f26a47b1b5bf0f4eb2e8c462251d5882.jpg", "table_caption": ["Table 8: Ablation study for hyperparameter $\\mathbf{N}$ under the unseen-verb setting. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "7.8 Qualitative Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As shown in Fig. 6, we provide more qualitative results in three zero-shot HOI settings: unseenverb, rare-first unseen-composition, and nonrare-first unseen-composition settings. Compared to MaPLe [22], our method obtains better generalization capability to unseen HOI classes owing to our efficient prompt learning design. ", "page_idx": 18}, {"type": "text", "text": "In Fig. 7, we show more qualitative comparisons to illustrate the effectiveness of the LLM guidance and UTPL design. The baseline here refers to the method in the main paper\u2019s third row of Table 4. LLM guidance design utilizes the general description from LLM for each HOI class and the UTPL module integrates the distinctive description from LLM. As shown in Fig. 7, the LLM guidance provides detailed class information, improving the performance over the baseline. Distinctive descriptions from UTPL design help to distinguish unseen classes from related seen HOIs, enhancing unseen performance and challenging case predictions. ", "page_idx": 18}, {"type": "text", "text": "7.9 Discussion for Zero-Shot HOI Detection Definition ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our method follows the standard zero-shot HOI setting, where unseen class names are used during training [14, 15, 16, 51, 44], as discussed in Section 4.1. Specifically, before training, the whole verb set $\\bar{\\mathbb{V}}\\stackrel{=}{=}\\{\\mathbb{V}_{\\mathrm{seen}},\\mathbb{V}_{\\mathrm{unseen}}\\}$ and the whole object set $\\mathbb{O}=\\{\\mathbb{O}_{\\mathrm{seen}},\\mathbb{O}_{\\mathrm{unseen}}\\}$ are all pre-defined. The four settings of zero-shot HOI detection include $:1$ ) Rare-First Unseen Composition (RF-UC), where for all $\\mathrm{hoi}_{i}^{-}=(v_{i},o_{i})\\in\\mathbb{U}$ , we have $v_{i}\\in\\mathbb{V}_{\\mathrm{seen}},o_{i}\\in\\mathbb{O}_{\\mathrm{seen}}$ and $\\mathrm{hoi}_{i}$ appears less than 10 times in the training set, belonging to rare HOI classes. 2) Nonrare-First Unseen Composition, where for all $\\mathrm{{hoi}}_{i}=(v_{i}^{\\bar{,}},o_{i})\\in\\mathbb{U}$ , we have $v_{i}\\,\\in\\,\\mathbb{V}_{\\mathrm{seen}},o_{i}\\,\\in\\,\\mathbb{O}_{\\mathrm{seen}}$ and $\\mathrm{hoi}_{i}$ appears more than 10 times in the training set, belonging to nonrare HOI classes. 3) Unseen Verb (UV), where $\\mathrm{hoi}_{i}=\\left(v_{i},o_{i}\\right)\\in\\mathbb{U},$ we have $v_{i}\\,\\in\\,\\mathbb{V}_{\\mathrm{unseen}},o_{i}\\,\\in\\,\\mathbb{O}_{\\mathrm{seen}}$ . 4) unseen Object (UO), where $\\mathrm{{hoi}}_{i}\\,=\\,(v_{i},o_{i})\\,\\in\\,\\mathbb{U}$ , we have $v_{i}\\in\\mathbb{V}_{\\mathrm{seen}}$ , $O_{i}\\in\\mathbb{O}_{\\mathrm{unseen}}$ . ", "page_idx": 18}, {"type": "text", "text": "Beyond the zero-shot HOI setting, there are HOI unknown concept discovery [17] and openvocabulary HOI detection [28], where unseen class names cannot be used in training. The openvocabulary setting differs from HOI unknown concept discovery, with a much wider range of unseen HOI classes during testing. ", "page_idx": 18}, {"type": "text", "text": "7.10 Discussion of Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our approach to zero-shot HOI detection reduces reliance on extensive annotated datasets, enhancing accessibility for organizations with limited resources and promoting inclusivity in technology adoption. This technology could notably improve assistive devices, offering more intuitive aids for individuals with disabilities by enabling better understanding of new environments. ", "page_idx": 18}, {"type": "text", "text": "However, these advancements also pose risks. The capability to interpret human-object interactions could be used for surveillance, potentially infringing on privacy. Additionally, reliance on existing visual-language models may maintain embedded biases, leading to discriminatory outcomes. To address these concerns, we recommend developing rigorous ethical guidelines and governance frameworks to regulate the deployment of HOI detection technologies, alongside efforts to identify and mitigate biases in foundational models. ", "page_idx": 18}, {"type": "image", "img_path": "R1Rrb2d5BH/tmp/c12ef33f1df36874171b181826a544dc09222bc02d43712fe76eb6b3b98e539c.jpg", "img_caption": ["Figure 6: Qualitative comparison of zero-shot HOI detection between our method and MaPLe [22]. We use orange color to represent unseen HOI classes and use blue color for seen ones. For images containing multiple HOI results, we only present one prediction for clearer demonstration and comparison. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "R1Rrb2d5BH/tmp/629aa0f5737381945dcfaf3ee78b016c16a207c91fe4e59d22512e6b2bdeeb07.jpg", "img_caption": ["(a):Baseline; (b): Baseline w/ LLM guidance; (c): Baseline w/ LLM guidance& UTPL "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 7: Qualitative results to show the effectiveness of the LLM guidance and UTPL design. We conduct the comparison under the unseen-verb zero-shot HOI setting. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We emphasize our contributions in the Introduction. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We included the Limitation section in the main paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We don\u2019t include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide comprehensive implementation details both in main paper and in appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We included the website link for the code repository in the abstract and our evaluation is based on public datasets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We included them in the implementation details in both main paper and the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer:[No] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We follow the default evaluations in the HOI detection field, which doesn\u2019t require error bars. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide them in implementation details. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We conducted in the paper conform, with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discussed the positive and negative societal impact in the supplementary material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We cited the original paper that produced the code package or dataset. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]