[{"type": "text", "text": "LoTLIP: Improving Language-Image Pre-training for Long Text Understanding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei $\\mathbf{W}\\mathbf{u}^{1}$ Kecheng Zheng2,3\u2020 Shuailei Ma4 Fan Lu1 Yuxin Guo5 Yifei Zhang6 Wei Chen3 Qingpei Guo2 Yujun Shen2 Zheng-Jun Zha1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1University of Science and Technology of China 2Ant Group 3Zhejiang University 4Northeastern University, China 5Institute of Automation, Chinese Academy of Sciences 6Shanghai Jiao Tong University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding long text is of great demands in practice but beyond the reach of most language-image pre-training (LIP) models. In this work, we empirically confirm that the key reason causing such an issue is that the training images are usually paired with short captions, leaving certain tokens easily overshadowed by salient tokens. Towards this problem, our initial attempt is to relabel the data with long captions, however, directly learning with which may lead to performance degradation in understanding short text (e.g., in the image classification task). Then, after incorporating corner tokens to aggregate diverse textual information, we manage to help the model catch up to its original level of short text understanding yet greatly enhance its capability of long text understanding. We further look into whether the model can continuously benefit from longer captions and notice a clear trade-off between the performance and the efficiency. Finally, we validate the effectiveness of our approach using a self-constructed large-scale dataset, which consists of $100M$ long caption oriented text-image pairs. It is noteworthy that, on the task of long-text image retrieval, we beat the competitor using long captions with $11.1\\%$ improvement (i.e., from $72.62\\%$ to $83.72\\%$ ). The project page is available here. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding long texts plays a key role in natural language processing (NLP) field, e.g., text generation [23, 28, 33]. Inspired by these works, in the multi-modality field, some text-to-image generation works (e.g., DALLE-3 [1] and Pixel-art [4, 5]) employ pre-trained captioners (e.g., LLaVA [21]) to generate more accurate and detailed captions-in other words, long captions\u2014for images. These long captions significantly reduce the generative model\u2019s tendency to hallucinate, thereby enhancing the quality of text-image alignment. However, in the realm of language-image pre-training, scant research has been conducted on modeling long texts in a way that effectively aligns with image representations. Despite the lack of exploration, this is a critical problem with practical demands because multi-modality models are increasingly common in applications that require nuanced understanding of textual descriptions corresponding to images. ", "page_idx": 0}, {"type": "text", "text": "There are two primary challenges hinder the effective integration of long-text understanding in language-image pre-training. The first one is the lack of large-scale long-caption image-text paired datasets. Most existing datasets focus on short captions (i.e., average text length in CC12M [3] is about 17), which limits the model\u2019s exposure to longer text forms. Consequently, the models trained on these datasets tend to neglect certain tokens that are easily overshadowed by salient tokens. As shown in Fig. 1, the model trained on short captions can be good at understanding the content of short captions (i.e., garden and castle). But when increasing the length of caption, we can see that \u2018garden token\u2019 is overshadowed by \u2018castle token\u2019, even if we move the \u2018garden token\u2019 front. This bias towards the salient tokens (\u2018castle token\u2019 in Fig. 1) of texts can severely restrict the model\u2019s ability to comprehend and generate responses based on the full context of longer inputs. The second challenge is the token length limitation of the text encoder. While directly increasing the token number limitation that a model can process appears to be a straightforward solution for accommodating longer texts, it does not automatically translate into better understanding for long captions. The fundamental issue remains the model\u2019s inability to effectively interpret long texts, primarily due to the lack of appropriate training data that includes long captions. ", "page_idx": 0}, {"type": "image", "img_path": "pc4GSBi1Hx/tmp/044a3fc71335bbbe4cb44cfd76e28ba6f4557ca4599649efeec77c42fdc607eb.jpg", "img_caption": ["Figure 1: Illustration of the impacts of long v.s. short captions on image-language pre-training, as observed in the cross-attention maps of CLIP. Training images are usually paired with short captions, leaving certain tokens (e.g., garden token) easily overshadowed by salient tokens (e.g., castle token). Fortunately, the usage of long captions can help bring the overshadowed tokens back into the light, and this phenomenon is not influenced by the order of tokens within the sentence. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we have undertaken an extensive project to re-caption 100 million data with long captions, aiming to enrich the training environment for our models. This initiative allows us to explore the effects of increased text length in image-text pre-trained models and its impacts on model performance. Based on these experiments, we empirically confirm that the key reason causing such an issue is that the training images are usually paired with short captions, leaving certain tokens easily overshadowed by salient tokens. However, directly learning with long captions may improve the long-text understanding of image-text pre-trained models, but lead to performance degradation in understanding short texts (e.g., in the image classification task) as shown in Fig. 2. After integrating corner tokens to aggregate diverse textual information, we successfully enable the model to regain its original proficiency in understanding short texts while significantly improving its ability to comprehend long texts. We also explore whether the model can continue to benefit from longer captions and observe a clear trade-off between performance and efficiency. It is noteworthy that, on the task of long-text image retrieval, we beat Long-CLIP [39], a competitor using long captions for fine-tuning, with $11.1\\%$ improvement (i.e., from $72.62\\%$ to $83.72\\%$ ). ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Language-Image Pre-training ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recently, using language-image pre-trained models to do zero-shot prediction has attracted a lot of attention. CLIP [24] and ALIGN [14] demonstrate contrastive pre-trained models can learn rich visual-language correspondence knowledge from large-scale image-text pairs on the Internet and achieve good performance on zero-shot predictions, including image-text retrieval [18] and classification [9]. Following their success, various studies [16, 34, 36, 17] have been devoted to improving image-text alignment. Among them, FILIP [34] focuses on fine-grained expressiveness between text tokens and image patches by modifying the training loss. While LiT [37] finds that apply contrastive-tuning to the pre-trained models with locked image encoder and unlocked text encoder can further improve the alignment. It is recognized that larger batch size brings better performance. For this reason, SigLIP [38] proposes to replace the softmax normalization among the standard contrastive loss with the sigmoid loss to scale up training batch size. In addition, LaCLIP [11], RLEG [40] and some other works [19, 13, 22, 30] utilize multi-modality generative models to improve data quality for enhancing pre-training. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Long-text Understanding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Detailed long texts are necessary for many artificial intelligence tasks (e.g., human-computer interaction). Therefore, modeling and parsing long texts has become one of the most important research directions in natural language processing. Many studies in fields such as text generation [23, 32, 33] have shown that advanced transformer structures have the ability to interpret long texts. However, in the field of language-image pre-training, research on using long-text descriptions of images to enhance multimodal representations is still very scarce. DreamLIP [41] utilizes multimodality large language model to re-caption image data with detailed descriptions and then use them in language-image pre-training. In fact, during training, DreamLIP randomly extracts sub-captions from the detailed description for training and does not completely use all the information of the long text. One of the reasons why previous language-image pre-training rarely directly applied long texts for training is the text encoder of traditional CLIP [24] is restricted by the token number limit $(\\le77)$ . Then, Long-CLIP [39] firstly introduces long captions into CLIP model to finetune, where the model is pre-trained on short-text-image datasets. The fine-tuning process equips the model with the ability to comprehend long texts, albeit at the expense of its proficiency in understanding shorter texts. In contrast, we incorporate long captions during the pre-training stage, which can not only enable the model to regain its original competency in interpreting short texts but also significantly improves its understanding of long texts. We further explored the trade-off between the benefti of long texts to the model and the training efficiency. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminary of Language-Image Pre-training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Language-image pre-training models, i.e., CLIP [24] and LiT [37], typically consist of two parts: an image encoder and a text encoder. In the pre-training stage, the language-image model takes image-text pairs as input, and the image encoder and text encoder are used to extract embeddings from images and texts, respectively. Then, two encoders are trained with contrastive objectives, ensuring that paired image and text embeddings are close in the embedding space, while unpaired pairs are far apart. Specifically, a batch of images $\\{I_{1},I_{2},\\cdot\\cdot\\cdot,I_{N}\\}$ and the corresponding short texts $\\{T_{1},T_{2},\\bar{\\cdot}\\cdot\\cdot,T_{N}\\}$ are randomly sampled from the pre-training dataset. Each image and its corresponding text are treated as a positive pair, while others in the same batch are negative pairs. Then, the image encoder extracts image global feature $\\mathbf{v}^{i}$ from the $i^{\\th}$ -th image $I_{i}$ within the batch, while the text encoder obtains text feature $\\mathbf{t}^{j}$ from the $j$ -th text $T_{j}$ . These two encoders are then optimized using contrastive loss $\\mathcal{L}$ , which consists of image-to-text loss $\\mathcal{L}^{i2t}$ and text-to-image loss $\\dot{\\mathcal{L}^{t2i}}$ . It can be formulated as follows: ", "page_idx": 2}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}^{i2t}+\\mathcal{L}^{t2i},\n$$$$\n\\begin{array}{r l r}&{}&{\\mathcal{L}=\\mathcal{L}^{\\iota\\alpha}+\\mathcal{L}^{\\iota\\alpha},}\\\\ &{}&{\\mathcal{L}^{i2t}=-\\displaystyle\\sum_{i=1}^{N}\\log\\frac{\\exp\\left(\\cos\\langle\\mathbf{v}^{i},\\mathbf{t}^{i}\\rangle/\\tau\\right)}{\\sum_{j=1}^{N}\\exp\\left(\\cos\\langle\\mathbf{v}^{i},\\mathbf{t}^{j}\\rangle/\\tau\\right)},}\\\\ &{}&{\\mathcal{L}^{t2i}=-\\displaystyle\\sum_{i=1}^{N}\\log\\frac{\\exp\\left(\\cos\\langle\\mathbf{t}^{i},\\mathbf{v}^{i}\\rangle/\\tau\\right)}{\\sum_{j=1}^{N}\\exp\\left(\\cos\\langle\\mathbf{t}^{i},\\mathbf{v}^{j}\\rangle/\\tau\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tau$ is a learnable temperature parameter, and $\\cos\\langle\\cdot,\\cdot\\rangle$ means the cosine similarity between two normalized feature vectors. ", "page_idx": 2}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/685fcdc979a582b4113039680d633b0ff246ee5195b1500a74721a021847d665.jpg", "table_caption": ["Table 1: Dataset details of long-text-image retrieval and short-text-image retrieval tasks. We use BERT tokenizer for tokenization. ShareGPT4V-1k and 10k are selected from the ShareGPT4V dataset. For DCI and IIW, all images with human-authored long descriptions are used while evaluating. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "4 Long Texts in Language-Image Pre-training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Currently available image-text pair datasets, e.g.CC12M [27], typically include short texts that have an average length of approximately 17 tokens. Language-image models pre-trained on these datasets perform well on short-text comprehension tasks. However, we find that they struggle to comprehend long texts for text-image alignment. Concretely, they tend to overlook or neglect some tokens or sub-captions in long texts, as shown in Fig. 1. A potential reason for such a situation is the lack of long-text-image pairs in pre-training. Thus, we collected and re-captioned 100M images with long texts. In this section, we provide details of re-captioning and explore the usage of long texts in language-image pre-training. ", "page_idx": 3}, {"type": "text", "text": "4.1 Long Text-Image Pair Dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Training Dataset. To construct long text-image pairs for language-image pre-training, we recaptioned 100 million images with long texts. Specifically, we collected the images from CC3M [27], CC12M [27], YFCC15M [29], LAION [26], and COYO [2] dataset. Then, we instructed three multimodality large language models (MLLMs), i.e., InstructBLIP [7], LLaVA [20], and ShareGPT4V [6] to generate diverse and descriptive long texts based on the collected images. In this step, we used \u201cDescribe the image in detail.\u201d as the text prompt, following DreamLIP [41]. Finally, each image in the collected datasets is paired with four texts: a raw text from the original dataset and three re-captioned long texts. The raw texts exhibit an average length of approximately 18 tokens, whereas the re-captioned long texts consist of around 136 tokens. ", "page_idx": 3}, {"type": "text", "text": "Evaluation Dataset. Most zero-shot evaluation tasks for language-image pre-training primarily rely on short textual input. For example, in short-text-image-retrieval tasks, the textual inputs contain fewer than 15 tokens on average, as shown in Table 1. Given the short texts, these tasks are not suitable to access the long text comprehension ability of pre-trained models. Therefore, we collected long text-image pairs from DCI [31], IIW [12], and ShareGPT4V [6] datasets for constructing long-text-image retrieval evaluation task. Specifically, for DCI and IIW datasets, we use images with human-annotated long descriptions for retrieval. For ShareGPT4V dataset, we sample 1,000 and 10,000 data from ShareGPT4V dataset to construct ShareGPT4V-1k and ShareGPT4V-10k retrieval dataset, respectively. The ShareGPT4V-1k dataset is constructed following Long-CLIP [39]. All images in ShareGPT4V-1k and ShareGPT4V-10k are from SA-1B [15] dataset, and all long texts are generated by ShareGPT4V-Captioner [6]. As shown in Table 1, each image within these datasets has one paired long text, which consists of more than 8 sub-captions and 170 tokens on average. Here, a sub-caption is a complete sentence ending with a period. The long-text-image retrieval task necessitates the alignment of text features from long texts with the image features from the corresponding images, which thereby evaluates the model\u2019s ability to comprehend long texts. ", "page_idx": 3}, {"type": "text", "text": "4.2 Exploring the Influence of Text length ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Long-CLIP [39] enables long text processing ability by fine-tuning CLIP model with long text-image pairs. However, the benefits and drawbacks of using long texts in pre-training are still unknown. It ", "page_idx": 3}, {"type": "image", "img_path": "pc4GSBi1Hx/tmp/275a9e0a0755edf634c2a8313af8b966b4a757be21f8c7c3a986bbd80b3464ed.jpg", "img_caption": ["", "", ""], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: The influence of text length. A significant improvement is observed across all tasks when we added one randomly sampled sub-caption from generated texts to the pre-training stage. As the number of sub-captions increases, the performance of the pre-trained model on long-textimage retrieval tasks consistently improves and becomes stable (a). However, there is a performance degradation in MSCOCO retrieval task (b) and ImageNet classification task (c). ", "page_idx": 4}, {"type": "image", "img_path": "pc4GSBi1Hx/tmp/67249c185caca00c8fdbe52350991dec4b77bdf91d71cc57894e1a44b763b3d5.jpg", "img_caption": ["Figure 3: Overview of LoTLIP. We add multiple learnable corner tokens ([Cor 1], [Cor 2], . \u00b7 \u00b7 \u00b7 ) after [CLS] token. These corner tokens are initialized differently for aggregating diverse token features. Besides, an attention mask mechanism is used to limit the interaction between [CLS] and corner tokens to ensure the diversity of gathered features. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "is also unknown how the length of the text affects the performance of the pre-trained model. To explore the influence of using texts in different lengths for pre-training, we change the length of long texts by selecting different numbers of consecutive sub-captions. Each sub-caption in the long texts has an average of approximately 22 tokens. The experiment is conducted on 3M scale dataset and the results are illustrated in Fig. 2. Sub-caption number equals 0 indicates that the model is trained without long texts. When introducing one sub-caption, a noticeable gain is achieved across all tasks. The results also indicate that longer texts, composed of more sub-captions, enhance the performance of the pre-trained model on long-text-image retrieval tasks. But it tends to stabilize when the number of sub-captions reaches three. It confirms that using long text-image pairs for pre-training improves pre-trained models\u2019 understanding of long texts. However, longer texts in language-image pre-training degrades the performance on short-text-image retrieval and image classification tasks. ", "page_idx": 4}, {"type": "text", "text": "4.3 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As presented in Sec. 4.2, directly incorporating long texts into pre-training negatively impacts the performance of model in short-text comprehension tasks. In order to find a solution that well balances both long and short texts, we design to add extra text tokens for text encoders, termed corner tokens, which can aggregate diverse text features. This strategy benefits the class (i.e., [CLS]) token by extracting more representative features for long and short text. Next, we will describe the details. ", "page_idx": 4}, {"type": "text", "text": "Corner Tokens. Different text encoder architectures (e.g., BERT [10], T5 [25]) can be utilized in language-image pre-training. In this section, we take BERT [10] as an example of our approach. Given a long text, it is expressed as $[\\mathrm{CLS}],\\cdot\\cdot\\cdot[\\mathrm{SEP}],\\cdot\\cdot\\cdot[\\mathrm{SEP}]$ , where the first token of every text input is its class token [CLS], and all sub-captions are separated by a special token [SEP]. The omitted part in $\\dots$ denotes the tokens obtained after tokenizing words within the sub-caption. Based on the tokenized long text, we insert multiple learnable corner tokens $\\mathcal{C}=\\{[\\mathrm{Cor~}~1],\\dot{\\cdot}\\cdot\\cdot\\cdot,[\\mathrm{Cor~}~\\mathfrak{m}]\\}$ after the class token, where $m$ is the number of corner tokens. In this way, the form of the tokenized text input is [CLS], $[\\mathsf{C o r~\\mathsf~{1}}]\\cdot\\cdot\\cdot[\\mathsf{C o r~\\mathsf~{m}}],\\cdot\\cdot\\cdot[\\mathsf{S E P}],\\cdot\\cdot\\cdot[\\mathsf{S E P}]$ . Moreover, we design an attention mask mechanism $\\boldsymbol{\\mathcal{A}}$ for the text encoder to promote the diversity of the aggregated features. Specifically, when calculating the attention scores, the corner tokens and [CLS] token are guided to neglect each other but attend to all other sub-caption tokens. Meanwhile, in the attention mask mechanism, each text tokens are designed to only interact with other text tokens and the [CLS] token, to keep the interactions between local and global information. The attention mask $\\boldsymbol{\\mathcal{A}}$ is formulated as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nA(q,k)={\\left\\{\\begin{array}{l l}{0,}&{{\\mathrm{if~}}(k\\in{\\mathcal{C}}{\\mathrm{~or~}}q,k\\in{\\mathcal{C}}\\cup\\{[{\\mathrm{CLS}}]\\}){\\mathrm{~and~}}q\\neq k}\\\\ {1,}&{{\\mathrm{otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $q$ and $k$ represent the query and key tokens in attention block, respectively. The features of the [CLS] and corner tokens are regarded as text global feature $\\mathbf{t}_{g}$ and corner features $\\mathbf{t}_{c_{1}},\\mathbf{t}_{c_{2}},...,\\mathbf{t}_{c_{m}}$ , respectively. ", "page_idx": 5}, {"type": "text", "text": "Optimization. The short-text-image contrastive loss $\\mathcal{L}_{\\mathrm{short}}$ is calculated in the same way as Eq. (1). Meanwhile, the long-text-image contrastive loss $\\mathcal{L}_{\\mathrm{long}}$ between image global feature $v$ and $\\mathbf{t}_{g},\\mathbf{t}_{c_{1}},\\mathbf{t}_{c_{2}},...,\\mathbf{t}_{c_{m}}$ of long text is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{long}}=\\mathcal{L}_{\\mathrm{long}}^{i2t}+\\mathcal{L}_{\\mathrm{long}}^{t2i},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{long}}^{i2t}=-\\sum_{i=1}^{N}(\\log\\frac{\\exp\\left(\\cos\\langle\\mathbf{v}^{i},\\mathbf{t}_{g}^{i}\\rangle/\\tau\\right)}{\\sum_{j=1}^{N}\\exp\\left(\\cos\\langle\\mathbf{v}^{i},\\mathbf{t}_{g}^{j}\\rangle/\\tau\\right)}+\\sum_{k=1}^{m}\\log\\frac{\\exp\\left(\\cos\\langle\\mathbf{v}^{i},\\mathbf{t}_{c_{k}}^{i}\\rangle/\\tau\\right)}{\\sum_{j=1}^{N}\\exp\\left(\\cos\\langle\\mathbf{v}^{i},\\mathbf{t}_{c_{k}}^{j}\\rangle/\\tau\\right)}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{long}}^{t2i}=-\\sum_{i=1}^{N}(\\log\\frac{\\exp{(\\cos\\langle\\mathbf{t}_{g}^{i},\\mathbf{v}^{i}\\rangle/\\tau)}}{\\sum_{j=1}^{N}\\exp{(\\cos\\langle\\mathbf{t}_{g}^{i},\\mathbf{v}^{j}\\rangle/\\tau)}}+\\sum_{k=1}^{m}\\log\\frac{\\exp{(\\cos\\langle\\mathbf{t}_{c_{k}}^{i},\\mathbf{v}^{i}\\rangle/\\tau)}}{\\sum_{j=1}^{N}\\exp{(\\cos\\langle\\mathbf{t}_{c_{k}}^{i},\\mathbf{v}^{j}\\rangle/\\tau)}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The total training loss is $\\mathcal{L}_{\\mathrm{LoTLIP}}=\\mathcal{L}_{\\mathrm{long}}+\\mathcal{L}_{\\mathrm{short}}$ . ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Implementation Details and Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Pre-training Datasets. As presented in Sec. 4.1, we collected 100M data from five publicly available image-text pair datasets and re-captioned the collected images with long texts. Based on this dataset, we construct 4 scales of pre-training data: (1) 3M, including CC3M. (2) 12M, including CC12M. (3) 30M, including CC3M, CC12M, and YFCC15M. (4) 100M, including all re-captioned data. We conduct ablation studies to validate our model on the 3M scale pre-training data. The performance of LoTLIP pre-trained with 12M and 30M scale datasets is shown in the Supplementary Material. ", "page_idx": 5}, {"type": "text", "text": "Downstream Datasets. To assess the ability of the pre-trained models on short-text and long-text understanding, we select 3 downstream tasks for evaluation under the zero-shot setting, including long-text-image retrieval, short-image-text retrieval, and image classification. For long-text-image retrieval, we present the Recall at 1 $(\\mathbf{R}\\@1)$ metric of the pre-trained models on DCI, IIW, and ShareGPT4V-1k, and ShareGPT4V-10k long-text-image retrieval tasks. For short-image-text retrieval, we evaluate on MSCOCO [18] and Flickr30k Caption [35] and report Recall at 1/5 $(\\mathrm{R}\\odot1/5)$ metric for comparison. For image classification, we use ImageNet1k [9] for evaluation and present top-1 accuracy $(\\operatorname{Acc}\\@leftrightarrow1)$ on image classification. Following [24], we use class names incorporated with pre-defined text prompts as text inputs for zero-shot classification. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details. Following LiT [37], we use a vision transformer pre-trained on ImageNet 21K as the image encoder and Bert [10] as the text encoder. The architecture of the image encoder is ViT-B/16. We report other variants of vision transformers in the Supplementary Material. The images are resized to $224\\times224$ . The maximum text token length is set to 128 unless specifically stated. Three consecutive sub-captions are randomly selected to from long texts as text input. We train 10 epochs on the 3M and 100M scale datasets. For the 3M dataset, the batch size is set to 2560, while that of 100M is set to 16384. The other pre-training hyperparameters are under the same setting, e.g.learning rate, warmup steps, and weight decay. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Analyze the effectiveness of LoTLIP in language-image pre-training with long texts. The architecture of the image encoder is ViT-B/16. I2T and T2I indicate $\\mathbf{R}\\@1$ on text and image retrieval, respectively. We use 3M scale dataset for pre-training. $\"\\checkmark\"$ indicates we add long texts in the training stage. ", "page_idx": 6}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/6d979067e6df822efc9067ad72dc01eb8dd38de3936995d257b28e1ce8de13a6.jpg", "table_caption": [], "table_footnote": ["\\* We apply the primary Component matching strategy proposed by Long-CLIP [39] to LiT and train the model with the same training setting using 3M data for fair comparison. "], "page_idx": 6}, {"type": "text", "text": "5.2 Ablation Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Exploring the Influence of Token Number Limitation. Token number limitation is the maximum length of text input that the text encoder can process at a time. For languageimage pre-training models, the token number limitation of text encoder can be arbitrarily set to any positive integer, which is typically set to 77 [24, 38, 41]. The benefits derived from using long texts in pre-training might be constrained by the 77 token limitation. To investigate this hypothesis, we incrementally raise the token number limitation from 32 to 512. The results are illustrated in Fig. 4, which indicate a limitation of 77 tokens is insufficient for a model training with long texts. On DCI and ShareGPT4V-10k retrieval tasks, the best results are observed when the token number limitation is set to 192. Meanwhile, the highest performance on the IIW retrieval task is reached when the text token length limitation is 256. ", "page_idx": 6}, {"type": "image", "img_path": "pc4GSBi1Hx/tmp/3cde5af47a07f662cf2274bed5e1cef2c82349fbe896c5026c9d2034c7ac4626.jpg", "img_caption": ["Figure 4: Influence of token number limitation on LoTLIP. The performance of the pre-trained model on different tasks improves when the token number limitation increases up to 192, which exceeds the commonly used 77. Meanwhile, the FLOPs of the text encoder (red stars) rapidly increase with the text token number limitation. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Since the average text length of the evaluation datasets is less than 240 tokens, as shown in Table 1. When the text token length limitation gets larger than 256, these datasets can\u2019t well prove the long-text understanding ability of the model. In Fig. 4, we also illustrate the FLOPs of the text encoder, which increases with the text token number limitation. To balance the training efficiency and performance, we set the token number limitation to 128 for the text encoder of LoTLIP. ", "page_idx": 6}, {"type": "text", "text": "Compare LoTLIP with Other Methods in Pre-training with Long Texts. LoTLIP aims to enhance the long-text and short-text comprehension ability of language-image pre-training. To demonstrate the effectiveness of LoTLIP, we compare it with other approaches that train with long text-image pairs. The first approach is directly using long texts in the training stage of LiT. The second approach is based on another contrastive learning method related to long texts, namely LongCLIP [39]. Concretely, we incorporate the primary component matching strategy and losses proposed by Long-CLIP to LiT (LiT+Long-CLIP). For a fair comparison, all models are trained with 3M scale dataset. As shown in Table 2, directly using long texts in pre-training procedural significantly improves LiT over all tasks. Moreover, LiT+Long-CLIP exceeds LiT on the long-text-image retrieval tasks when both models incorporate long texts during pre-training. But on short-text comprehension tasks, i.e., short-text-image retrieval and image classification, the performance of LiT+Long-CLIP is inferior to that of LiT. Instead, LoTLIP improves LiT and LiT+Long-CLIP on both long and short text comprehension tasks. Concretely, LoTLIP surpasses LiT and LiT $^{\\ast}$ Long-CLIP by $1.97\\%$ and $1.51\\%$ on average over three long-text-image retrieval tasks. Besides, LoTLIP improves LiT by $2.29\\%$ and $1.47\\%$ on MSCOCO retrieval and ImageNet classification, respectively. The results demonstrate ", "page_idx": 6}, {"type": "text", "text": "Table 3: Analyze the influence of the number of corner tokens and the attention mask mechanism.   \nWe use 3M scale dataset for training. The architecture of the image encoder is ViT-B/16. ", "page_idx": 7}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/6b6f5e4580388b1d81d7f5b8ea835ccbad3a24a53a97a4bff28f8bb329a7bbfc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/ade6abbedf4ac6b213172a2a4d48fe3b05ced0af9b6650f2b1c65cd2abfefb98.jpg", "table_caption": ["Table 4: Zero-shot evaluation of different models on long-text-image retrieval tasks. I2T and T2I indicate $\\mathbf{R}\\mathcal{@}1$ on text and image retrieval, respectively. "], "table_footnote": ["\\* Long-CLIP fine-tunes pre-trained CLIP model with ShareGPT4v-10k data from ShareGPT4v datasets except ShareGPT4v-1k. "], "page_idx": 7}, {"type": "text", "text": "that LoTLIP beneftis from long texts and corner tokens, thereby exhibiting a better understanding of the long and short texts. ", "page_idx": 7}, {"type": "text", "text": "Implementation of Corner Tokens. In this part, we study the influence of the attention mask mechanism and the number of corner tokens on different downstream tasks as shown in Table 3. LoTLIP pre-trained without the pre-defined attention mask encounters performance degradation on most tasks compared to when using the pre-defined attention mask. It indicates that direct interaction between the CLS token and corner tokens limits their ability to aggregate diverse textual features. On short-text-image retrieval and image classification tasks, the performance of LoTLIP improves when introducing more corner tokens. It proves that the corner tokens help with enhancing the short text understanding ability of LoTLIP. When the number of corner tokens is set to more than two, the performance improvement across all tasks is relatively small. Thus, we use two corner tokens in LoTLIP. ", "page_idx": 7}, {"type": "text", "text": "5.3 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare LoTLIP with the state-of-the-art methods on downstream tasks involving long texts and short texts. The experimental results are shown in Table 4 and Table 5, respectively. On 3M data scale, LoTLIP significantly improves the state-of-the-art methods on all tasks. Specifically, LoTLIP improves the second best method LiT by $29.99\\%$ on average over four long-text-image retrieval tasks. Moreover, LoTLIP improves the second competitor LiT by $6.58\\%$ and $10.98\\%$ on image classification task and short-text-image retrieval tasks, respectively. It proves that LoTLIP significantly enhances language-image model for understanding short and long captions by involving long captions in pre-training and incorporating corner tokens in text inputs. It is worth noting that LoTLIP trained with 100M data exceeds all state-of-the-art methods on long-text-image retrieval tasks, even though these methods are pre-trained on a larger scale of data. Concretely, LoTLIP (trained on 100M data) exceeds SigLIP (trained with 12B) by an average of $7.13\\%$ over four long-text-image retrieval tasks. Moreover, compared to Long-CLIP, which first uses long texts in CLIP for fine-tuning, LoTLIP improves averaged performance by $11.1\\%$ on long-text-image retrieval tasks. ", "page_idx": 7}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/44ff059fddf8676565c31fb5fbfe982993896988f2b68e2cfdac59859feadfa1.jpg", "table_caption": ["Table 5: Zero-shot evaluation of different models on short-text-image retrieval and classification tasks. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we empirically confirm that a key issue arises because training images are typically paired with short captions, which can cause certain tokens to be overshadowed by more salient ones. To address this issue, our initial strategy involved relabeling the data with long captions. However, directly learning from these long captions might lead to degraded performance in tasks requiring an understanding of short text, such as image classification. Subsequently, by incorporating corner tokens to aggregate diverse textual information, we are able to help the model regain its original proficiency in understanding short texts while significantly enhancing its capability to comprehend long texts. We also investigated whether the model could continue to benefti from longer captions and observed a clear trade-off between performance and efficiency. Finally, we validated the effectiveness of our approach using a self-constructed large-scale dataset, which consists of 100 million longcaption-oriented text-image pairs. It is noteworthy that in the task of long-text image retrieval, our method outperforms the long caption-related competitor, Long-CLIP, with a significant improvement of $11.1\\%$ . ", "page_idx": 8}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work was supported by National Natural Science Foundation of China (NSFC) under Grants 62225207, and Zhejiang Provincial Natural Science Foundation of China under Grants LD24F020011. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn.openai.com/papers/dall-e-3.pdf, 2:8, 2023.   \n[2] M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim. Coyo-700m: Image-text pair dataset. https: //github.com/kakaobrain/coyo-dataset, 2022. [3] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. [4] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, et al. Pixartalpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [5] J. Chen, Y. Wu, S. Luo, E. Xie, S. Paul, P. Luo, H. Zhao, and Z. Li. Pixart-{\\delta}: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024.   \n[6] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. ShareGPT4V: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.   \n[7] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung, and S. Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. Adv. Neural Inform. Process. Syst., 36, 2023.   \n[8] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski. Vision transformers need registers. In Int. Conf. Learn. Represent., 2024.   \n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., 2009.   \n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[11] L. Fan, D. Krishnan, P. Isola, D. Katabi, and Y. Tian. Improving CLIP training with language rewrites. Adv. Neural Inform. Process. Syst., 36:35544\u201335575, 2023.   \n[12] R. Garg, A. Burns, B. K. Ayan, Y. Bitton, C. Montgomery, Y. Onoe, A. Bunner, R. Krishna, J. Baldridge, and R. Soricut. Imageinwords: Unlocking hyper-detailed image descriptions. arXiv preprint arXiv:2405.02793, 2024.   \n[13] H. A. A. K. Hammoud, H. Itani, F. Pizzati, P. Torr, A. Bibi, and B. Ghanem. SynthCLIP: Are we ready for a fully synthetic clip training? arXiv preprint arXiv:2402.01832, 2024.   \n[14] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Int. Conf. Mach. Learn., pages 4904\u20134916, 2021.   \n[15] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. In Int. Conf. Comput. Vis., pages 4015\u20134026, 2023.   \n[16] J. Lee, J. Kim, H. Shon, B. Kim, S. H. Kim, H. Lee, and J. Kim. UniCLIP: Unified framework for contrastive language-image pre-training. Adv. Neural Inform. Process. Syst., 35:1008\u20131019, 2022.   \n[17] Y. Li, H. Fan, R. Hu, C. Feichtenhofer, and K. He. Scaling language-image pre-training via masking. In IEEE Conf. Comput. Vis. Pattern Recog., pages 23390\u201323400, 2023.   \n[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Common objects in context. In Eur. Conf. Comput. Vis., pages 740\u2013755. Springer, 2014.   \n[19] D. Liu, H. Zhang, F. Wu, and Z.-J. Zha. Learning to assemble neural module tree networks for visual grounding. In Int. Conf. Comput. Vis., pages 4673\u20134682, 2019.   \n[20] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.   \n[21] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Adv. Neural Inform. Process. Syst., 36, 2023.   \n[22] Y. Liu, K. Wang, W. Shao, P. Luo, Y. Qiao, M. Z. Shou, K. Zhang, and Y. You. MLLMs-augmented visual-language representation learning. arXiv preprint arXiv:2311.18765, 2023.   \n[23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Adv. Neural Inform. Process. Syst., 35:27730\u201327744, 2022.   \n[24] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn., 2021.   \n[25] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[26] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.   \n[27] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Association for Computational Linguistics, 2018.   \n[28] G. Tan, D. Liu, M. Wang, and Z.-J. Zha. Learning to discretely compose reasoning module networks for video captioning. arXiv preprint arXiv:2007.09049, 2020.   \n[29] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64\u201373, 2016.   \n[30] Y. Tian, L. Fan, P. Isola, H. Chang, and D. Krishnan. StableRep: Synthetic images from text-to-image models make strong visual representation learners. Adv. Neural Inform. Process. Syst., 36:48382\u201348402, 2023.   \n[31] J. Urbanek, F. Bordes, P. Astolf,i M. Williamson, V. Sharma, and A. Romero-Soriano. A picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. arXiv preprint arXiv:2312.08578, 2023.   \n[32] H. Wang, Z.-J. Zha, L. Li, D. Liu, and J. Luo. Structured multi-level interaction network for video moment localization via language query. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7026\u20137035, 2021.   \n[33] W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.   \n[34] L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li, X. Jiang, and C. Xu. FILIP: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021.   \n[35] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, pages 67\u201378, 2014.   \n[36] Z.-J. Zha, D. Liu, H. Zhang, Y. Zhang, and F. Wu. Context-aware visual policy network for fine-grained image captioning. IEEE Trans. Pattern Anal. Mach. Intell., 44:710\u2013722, 2019.   \n[37] X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov, and L. Beyer. LiT: Zero-shot transfer with locked-image text tuning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 18123\u201318133, 2022.   \n[38] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In Int. Conf. Comput. Vis., pages 11975\u201311986, 2023.   \n[39] B. Zhang, P. Zhang, X. Dong, Y. Zang, and J. Wang. Long-clip: Unlocking the long-text capability of clip. arXiv preprint arXiv:2403.15378, 2024.   \n[40] L. Zhao, K. Zheng, Y. Zheng, D. Zhao, and J. Zhou. RLEG: vision-language representation learning with diffusion-based embedding generation. In Int. Conf. Mach. Learn., pages 42247\u201342258, 2023.   \n[41] K. Zheng, Y. Zhang, W. Wu, F. Lu, S. Ma, X. Jin, W. Chen, and Y. Shen. DreamLIP: Language-image pre-training with long captions. arXiv preprint arXiv:2403.17007, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "8 Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "8.1 Data Statistics ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "As shown in Table 6, we report the data statistics of our dataset and other text-image paired datasets. Compared to the texts in the source datasets, which were used to collect images, the re-captioned texts are significantly longer, averaging 136 tokens v.s. 18 tokens. To the best of our knowledge, LoTLIP dataset is the largest dataset consisting of long texts for multi-modal learning. We are continuing to expand the size of LoTLIP by integrating additional MLLMs, as well as gathering more publicly available datasets for long text generation. ", "page_idx": 11}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/ad4cdcad1a01f5f71686102c1ddf8385d833ffd3413f7b26776dae084782c569.jpg", "table_caption": ["Table 6: Data statistic of LoTLIP dataset and other text-image paired dataset. Our dataset is the largest dataset consisting of long texts for multi-modal learning. "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "8.2 More Experimental Analysis ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "8.2.1 Influence of Sub-caption Number on LoTLIP and LiT ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "During the pre-training stage, we randomly select multiple consecutive sub-captions from long texts in LoTLIP as long text inputs. In Fig. 5, we show the influence of the number of sub-captions when the length of text input is limited to 128. With the number of sub-captions increasing, LiT and LoTLIP reaches better performance in both text retrieval and image retrieval on the ShareGPT4v-10k and DCI datasets. There are small improvements when the number of sub-captions gets larger than 3. Moreover, both methods exhibit performance degradation on text-image retrieval and image classification tasks as the number of sub-captions increases. But the decline in performance of LoTLIP is more gradual. It further proves that LoTLIP can make better use of long texts to enhance the understanding of long texts while retaining the short-text understanding ability. ", "page_idx": 11}, {"type": "text", "text": "8.2.2 Compare Corner Tokens with Register Tokens ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In LoTLIP, we employ learnable tokens, namely corner tokens, to help the model regain its original proficiency in understanding short texts while significantly enhancing its capability to comprehend long texts. Existing method [8] also adds learnable tokens in the encoder, which they refer to as registered tokens. The register tokens are used to get rid of artifacts in image feature maps. In order to fairly compare corner tokens with register tokens, we implement the same number of register tokens on the text encoder and train the model with the same scale of dataset as LoTLIP. As shown in Table 7, register tokens can\u2019t enhance the ability of the language-image pre-trained model to understand short and long texts. Instead, corner tokens improve the performance of the baseline model over all tasks. ", "page_idx": 11}, {"type": "text", "text": "8.2.3 Contrastive Learning without Pre-trained Weights ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In Table 8, we present the performance of LoTLIP without loading pre-trained weights for text and image encoder on long and short-text related tasks. The results show that directly involving long texts in the pre-training stage can also enhance the performance of the language-image model with randomly initialized weights over all tasks. Besides, LoTLIP further improves CLIP on long and short text understanding ability, when they are both pre-trained with long texts. In our methodology, we opt to load pre-trained weights and fix the image encoder during training, following LiT. This is because LiT is a strong baseline and such a training setting significantly conserves computational resources. ", "page_idx": 11}, {"type": "image", "img_path": "pc4GSBi1Hx/tmp/c1f1cb9c6878dd9e4ab8e5a5f8c33a824de237a297f1cb5522585c5165ed62bb.jpg", "img_caption": ["Figure 5: Influence of the number of sub-captions used in the pre-training stages. Both LiT and LoTLIP are trained with long texts. The performance on ShareGPT4v and DCI retrieval are shown in (a)(b). (c)(d) represent the performance on MSCOCO retrieval. (e) shows the performance of image classification on ImageNet. "], "img_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/12e21ac5366d10370fc812ee54f4f91eaf2facd2bf3bc41012c4528129b30ab3.jpg", "table_caption": ["Table 7: Compare corner tokens with register tokens. The models are trained with 3M scale dataset. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Table 8: Analyze the effectiveness of LoTLIP without loading pre-trained weights for image and text encoder. The architecture of image encoder is ViT-B/16. We use 3M scale dataset for pre-training. $\\surd\\nabla^{\\prime\\prime}$ indicates we use long texts in the training stage. ", "page_idx": 12}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/c61fa8062b3e9b5ce7e67f0cab160cd35b7caa84e71e8c7bb74b98158506382f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "8.2.4 Utilizing Long Captions from Different MLLMs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use three MLLMs, i.e., InstructBLIP [7], LLaVA [20], and ShareGPT4V [6], to generate diverse long captions for 100M images. In this part, we conduct experiments to analyze the influence of using long texts generated by different MLLMs in pre-training. As shown in Table 9, LoTLIP trained with all generated long texts reach the best performance on average over all tasks. It means that diverse long texts help language-image pre-trained models to better comprehend texts to align with images. Moreover, LoTLIP trained with long captions generated by ShareGPT4V reaches higher scores on long-text-image retrieval tasks than other MLLMs. This indicates that the long captions generated by ShareGPT4V are of higher quality, which can enhance the long-text comprehension ability of LoTLIP. ", "page_idx": 13}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/e3273459ed368051e235f6b8debcfcd71768e4d77a01ddce28f35dae619c1ffb.jpg", "table_caption": ["Table 9: Utilizing long captions generated by different MLLMs in the training statge. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "8.2.5 Pre-training with different scale of dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We compare LoTLIP and LiT on different scales of the pre-training dataset, i.e., 3M, 12M, 30M, and 100M on long-text-image retrieval, text-image retrieval, and image classification. The results on long-text and short-text centered tasks are shown in Table 10 and Table 11, respectively. LoTLIP significantly improves LiT on all tasks in both ViT-B/32 and ViT-B/16 under different pre-training settings. It proves that involving long captions in the pre-training stage can help the language-image model to better deal with both long and short-comprehension tasks. ", "page_idx": 13}, {"type": "text", "text": "8.3 Visualization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We visualize the attention map of LiT, LiT trained with long texts, Long-CLIP (our implementation), and LoTLIP in Fig. 6. Given the long caption in the first column, the attention visualization map of LiT can only activate regions corresponding to partial objects mentioned in the long caption, e.g., flat rock. This situation is alleviated after incorporating long captions in LiT pre-training. Benefiting from corner tokens, the highlighted image regions of LoTLIP are well aligned with the given long caption. ", "page_idx": 13}, {"type": "text", "text": "8.4 Limitation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We re-wright the captions of 100M images using three popular open-sourced multi-modal large language models (i.e.InstructBLIP [7], LLaVA [20] and ShareGPT4V [6]), but we observed hallucination elements in the synthesized long captions. The hallucinations in captions, which do not correspond with the image information, may restrict the full potential of long captions in enhancing the understanding of lengthy texts by language-image pre-trained models. ", "page_idx": 13}, {"type": "text", "text": "8.5 Experiment Hardware ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To obtain our LoTLIP trained with 100M scale dataset, we apply A100 GPU with 80G memory for training, which costs about 133 GPU days. For models trained on datasets of other scales (i.e.3M, 12M, 30M), the training duration decreases linearly with the amount of training data. ", "page_idx": 13}, {"type": "text", "text": "8.6 Social Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The 100M images used for pre-training are publicly accessible, and the re-annotated long captions are synthesized using public multi-modal large language models on these public datasets, posing no ", "page_idx": 13}, {"type": "image", "img_path": "pc4GSBi1Hx/tmp/e90ea651c88fbb6b4fd2534e4bc23d6476ebff34187a939e6a91402bb48cdfab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 6: Visualize the attention map of LiT, LiT trained with long texts (LiT+Long Texts), LongCLIP, and LoTLIP. Here, both Long-CLIP (our implementation) and LoTLIP are trained with long texts. Benefiting from long texts and corner tokens, the highlighted image regions of LoTLIP are better aligned with the given long caption compared to other methods. ", "page_idx": 14}, {"type": "text", "text": "ethical risk in the data source. Our LoTLIP is incapable of generating images and text, thus there is no need for concern regarding the negative social impact resulting from fake, violent, pornographic, or discriminatory content. Moreover, LoTLIP has the potential for positive social impact, considering its excellent image-text retrieval performance, it may serve as a valuable asset in image retrieval libraries in the future. ", "page_idx": 14}, {"type": "text", "text": "8.7 Ethic Statement ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We have conducted a thorough review to ensure that there has been no violation of the NeurIPS Code of Ethics in this paper. ", "page_idx": 14}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/eb69a9037b62cb8eddc09e90a195a41a3bb17cce3ab10ee4075dd46e133d8bd4.jpg", "table_caption": ["Table 10: Zero-shot evaluation of different models on long-text-image retrieval. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "pc4GSBi1Hx/tmp/464cf4bbe8cd8ddc07d820d2a017d09ec5620fb72d63b20e2bba39f020447f83.jpg", "table_caption": ["Table 11: Zero-shot evaluation of different models on image-text retrieval and classification tasks. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 17}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 17}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 17}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 17}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 17}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: In the abstract and introduction, we explicitly state that our initial attempt is to relabel the data with long captions to facilitate the understanding of long text by the language-image pre-training (LIP) model. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: In the appendix, we discuss the limitations regarding the the potential harmfulness of the hallucination elements in long captions. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: We primarily demonstrate through experimentation that short captions fail to unlock the potential of LIP in understanding long texts, and that our approach effectively models long captions in a way that aligns with image representation, as described in Sec. 4 and Sec. 5. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In both the description of our method in Sec. 4 and the details of experimental implementation in Sec. 5, we disclose the information needed to reproduce the main experimental results. We will also release our code, model, and the new dataset to facilitate the reproducibility. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: The code for this paper requires approval before it can be made open source, hence it is not provided in this submission. However, the code, models, and datasets of this paper will be made publicly accessible after this submission to ensure the reproducibility of the experiments and to foster research progress within the community. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In Sec. 5, we explicitly present the implementation details, including the training data, pretraining hyperparameters, the settings of comparison methods, etc. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper proposes a foundation model that incurs significant training expenses, so we fixed the random seed and trained it only once. During the evaluation, we also fixed the random seed to ensure the constant results. Consequently, the fluctuation and error in our experimental results are slight. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In the appendix, we have specified the GPU type, the memory, the numbers of GPUs required for one training session, and the training duration for our model. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have conducted a thorough review to ensure that there has been no violation of the NeurIPS Code of Ethics in this paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Our model has the potential for positive social impact, given its strong imagetext retrieval performance, it may serve as a valuable asset in image retrieval libraries in the future. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The image data used for training is publicly accessible, and the long captions were synthesized using public models on public datasets, posing no security risk. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have cited the original paper that produced the code and dataset in the paper. The image data used for training is publicly accessible, and the long captions were synthesized using public models. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have rewritten captions for 100M images, but the new data requires approval before release. The new data will be made available to the public with our code after submission. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]