[{"figure_path": "pc4GSBi1Hx/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the impacts of long v.s. short captions on image-language pre-training, as observed in the cross-attention maps of CLIP. Training images are usually paired with short captions, leaving certain tokens (e.g., garden token) easily overshadowed by salient tokens (e.g., castle token).", "description": "This figure illustrates how the length of training captions affects the attention mechanism in CLIP (Contrastive Language-Image Pre-training) models.  When trained with short captions, the model focuses primarily on salient tokens (like \"castle\"), overshadowing less prominent but still relevant tokens (like \"garden\").  The cross-attention maps visualize this effect. However, when trained with long captions, the model's attention distributes more evenly, incorporating the previously overshadowed tokens into the overall understanding of the image. This highlights the importance of long captions in enabling more comprehensive image-text representation learning.", "section": "1 Introduction"}, {"figure_path": "pc4GSBi1Hx/figures/figures_4_1.jpg", "caption": "Figure 2: The influence of text length. A significant improvement is observed across all tasks when we added one randomly sampled sub-caption from generated texts to the pre-training stage. As the number of sub-captions increases, the performance of the pre-trained model on long-text-image retrieval tasks consistently improves and becomes stable (a). However, there is a performance degradation in MSCOCO retrieval task (b) and ImageNet classification task (c).", "description": "This figure shows the impact of increasing the length of text captions used during the pre-training phase of a language-image model.  The experiment varied the number of sub-captions added to the training data.  The results show that adding even one sub-caption leads to significant improvements in long-text image retrieval tasks, and performance continues to improve with additional sub-captions, until it plateaus. However,  this improvement comes at the cost of decreased performance on short-text image retrieval (using the MSCOCO dataset) and image classification (using the ImageNet dataset).  This indicates a trade-off between the model's ability to handle long and short texts.", "section": "4.2 Exploring the Influence of Text length"}, {"figure_path": "pc4GSBi1Hx/figures/figures_4_2.jpg", "caption": "Figure 3: Overview of LoTLIP. We add multiple learnable corner tokens ([Cor 1], [Cor 2],.\u2026\u2026\u2026) after [CLS] token. These corner tokens are initialized differently for aggregating diverse token features. Besides, an attention mask mechanism is used to limit the interaction between [CLS] and corner tokens to ensure the diversity of gathered features.", "description": "This figure illustrates the architecture of the proposed LoTLIP model.  It highlights the addition of multiple learnable corner tokens after the [CLS] token in the text encoder. These corner tokens are designed to aggregate diverse textual information from different parts of the long caption.  A mask mechanism is also employed to control the attention mechanism, preventing the corner tokens from dominating the attention and ensuring a balanced representation of all textual information. This approach helps to improve the model's ability to understand both short and long texts.", "section": "4.3 Method"}, {"figure_path": "pc4GSBi1Hx/figures/figures_6_1.jpg", "caption": "Figure 4: Influence of token number limitation on LoTLIP. The performance of the pre-trained model on different tasks improves when the token number limitation increases up to 192, which exceeds the commonly used 77. Meanwhile, the FLOPs of the text encoder (red stars) rapidly increase with the text token number limitation.", "description": "This figure shows the impact of adjusting the maximum token length in the text encoder on the LoTLIP model's performance across different tasks (long-text-image retrieval on three datasets: DCI, IIW, ShareGPT4V-10k).  It demonstrates that increasing the token limit up to 192 significantly enhances the model's performance on these tasks; exceeding the standard limit of 77 tokens. However, further increasing the token limit beyond 192 yields diminishing returns.  Concurrently, the figure also displays the computational cost (FLOPs of the text encoder) which increases proportionally with the maximum token length.", "section": "4.2 Exploring the Influence of Text length"}, {"figure_path": "pc4GSBi1Hx/figures/figures_12_1.jpg", "caption": "Figure 5: Influence of the number of sub-captions used in the pre-training stages. Both LiT and LoTLIP are trained with long texts. The performance on ShareGPT4v and DCI retrieval are shown in (a)(b). (c)(d) represent the performance on MSCOCO retrieval. (e) shows the performance of image classification on ImageNet.", "description": "This figure analyzes the impact of varying the number of sub-captions used during the pre-training phase on the performance of both LiT and LoTLIP models, which were trained using long texts. The results are shown across five different evaluation tasks: ShareGPT4V retrieval (I2T and T2I), DCI retrieval (I2T and T2I), MSCOCO retrieval (I2T and T2I), and ImageNet classification. The x-axis represents the number of sub-captions, and the y-axis shows the performance metric for each task (R@1 for retrieval and accuracy for classification). This visualization helps to understand how increasing the length of training text influences the models' performance on various downstream tasks.  It reveals the optimal length for balancing performance across long and short text understanding tasks.", "section": "4.2 Exploring the Influence of Text length"}, {"figure_path": "pc4GSBi1Hx/figures/figures_14_1.jpg", "caption": "Figure 6: Visualize the attention map of LiT, LiT trained with long texts (LiT+Long Texts), Long-CLIP, and LoTLIP. Here, both Long-CLIP (our implementation) and LoTLIP are trained with long texts. Benefiting from long texts and corner tokens, the highlighted image regions of LoTLIP are better aligned with the given long caption compared to other methods.", "description": "This figure visualizes the attention maps generated by four different models: LiT (baseline), LiT trained with long texts, Long-CLIP, and LoTLIP.  Two example images with their corresponding long captions are shown. Each model's attention map is displayed alongside the image, highlighting the regions of the image that the model focuses on when processing the caption.  The figure demonstrates that LoTLIP, which uses both long texts and corner tokens during training, produces attention maps that are more accurately aligned with the relevant parts of the image described in the long captions, compared to the other models. This highlights the effectiveness of LoTLIP's approach in improving long-text understanding in language-image models.", "section": "8.3 Visualization"}]