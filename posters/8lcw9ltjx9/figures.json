[{"figure_path": "8lcW9ltJx9/figures/figures_1_1.jpg", "caption": "Figure 1: The overall framework of Any2Policy. The Any2Policy framework is structured to handle multi-modal inputs, accommodating them either separately or in tandem at the levels of instruction and observation. We design embodied alignment modules, which are engineered to synchronize features between different modalities, as well as between instructions and observations, ensuring a seamless and effective integration of diverse input types.", "description": "This figure illustrates the Any2Policy architecture, which processes multi-modal inputs (text, audio, image, video, point cloud) for both instruction and observation.  Multi-modal encoders extract features, which are then projected into standardized token representations. An embodied alignment module synchronizes these features, and a policy network generates actions for robot control. The design ensures seamless integration of diverse input types for robust and adaptable robot behavior.", "section": "3.1 Overall Architecture"}, {"figure_path": "8lcW9ltJx9/figures/figures_3_1.jpg", "caption": "Figure 2: The architecture of embodied alignment and policy network.", "description": "The figure illustrates the architecture of the embodied alignment and policy network in Any2Policy. It shows how multi-modal instructions (text, audio, image, video) and observations (image, video, point cloud) are processed.  Each modality is first encoded using modality-specific encoders and then projected into a unified representation.  A token pruning technique reduces the dimensionality of visual data. Then, embodied alignment, a transformer-based architecture employing cross-attention and self-attention, synchronizes instruction and observation tokens. Finally, a policy network generates actions for robot control. The diagram highlights the key components of the system's multi-modal integration, feature alignment, and action generation.", "section": "3.1 Overall Architecture"}, {"figure_path": "8lcW9ltJx9/figures/figures_5_1.jpg", "caption": "Figure 3: This is the setup of our Franka real robot. We have compiled examples of several tasks. To facilitate better understanding for our readers, we provide only the language-based versions of these task descriptions.", "description": "This figure shows the workspace setup of the Franka real robot used in the experiments.  It showcases examples of several tasks performed by the robot, each with a short textual description of the task's goal, such as 'Pick up cube place to left box'. The images illustrate the initial state and a sequence of steps for each task, providing a visual representation of the robot's actions and the environment.", "section": "3.2 Dataset Construction"}, {"figure_path": "8lcW9ltJx9/figures/figures_8_1.jpg", "caption": "Figure 4: Performance of Any2Policy in Franka Kitchen with 10 or 25 demonstration demos. Comparison with R3M, BLIP-2, and EmbodiedGPT. On all tasks except for Knobs-left with 25 demonstrations, we obtained superior performance over SOTA methods.", "description": "This figure presents a comparison of the Any2Policy model's performance against three state-of-the-art (SOTA) models: R3M, BLIP-2, and EmbodiedGPT, on the Franka Kitchen benchmark.  Two sets of experiments are shown, one using 10 demonstrations and another using 25, to assess the impact of the number of demonstrations on the models' performance.  The results demonstrate that Any2Policy generally outperforms the SOTA methods across a range of tasks within the Franka Kitchen environment.", "section": "4.2 Simulation Evaluation"}]