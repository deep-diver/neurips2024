{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that is heavily used in the field and is directly relevant to the multi-modal learning focus of the target paper."}, {"fullname_first_author": "Anthony Brohan", "paper_title": "RT-1: Robotics Transformer for real-world control at scale", "publication_date": "2022-12-01", "reason": "This paper introduces RT-1, a significant advancement in robotic control that leverages transformers and is directly comparable to the Any2Policy model discussed in the target paper."}, {"fullname_first_author": "Yunfan Jiang", "paper_title": "VIMA: General robot manipulation with multimodal prompts", "publication_date": "2022-10-01", "reason": "This paper introduces VIMA, a state-of-the-art approach to robotic manipulation using multi-modal prompts, providing a strong benchmark and comparison for the Any2Policy model."}, {"fullname_first_author": "Rohit Girdhar", "paper_title": "ImageBind: One embedding space to bind them all", "publication_date": "2023-06-01", "reason": "This paper introduces ImageBind, a highly influential unified multi-modal encoder used within the Any2Policy framework, significantly impacting its design and capabilities."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-01", "reason": "This paper details BLIP-2, another significant vision-language model used as a component within Any2Policy, contributing substantially to its multi-modal capabilities and performance."}]}