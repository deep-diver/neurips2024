{"references": [{"fullname_first_author": "Balle", "paper_title": "Reconstructing training data with informed adversaries", "publication_date": "2022", "reason": "This paper is foundational to the study of reconstruction attacks, providing a theoretical framework that is directly relevant to the current work."}, {"fullname_first_author": "Bertran", "paper_title": "Scalable membership inference attacks via quantile regression", "publication_date": "2023", "reason": "This paper provides a state-of-the-art membership inference attack, which serves as a relevant comparison to the reconstruction attack in this paper."}, {"fullname_first_author": "Carlini", "paper_title": "Extracting training data from large language models", "publication_date": "2021", "reason": "This paper demonstrates the vulnerability of large language models to data extraction attacks, motivating the current work's focus on the vulnerability of simpler models."}, {"fullname_first_author": "Chen", "paper_title": "When machine unlearning jeopardizes privacy", "publication_date": "2021", "reason": "This paper highlights the privacy risks associated with machine unlearning, setting the stage for this paper's analysis of the specific vulnerabilities to reconstruction attacks."}, {"fullname_first_author": "Dwork", "paper_title": "Calibrating noise to sensitivity in private data analysis", "publication_date": "2006", "reason": "This paper introduces the concept of differential privacy, which provides a formal definition for privacy guarantees and serves as a benchmark for privacy-preserving algorithms."}]}