[{"type": "text", "text": "Reconstruction Attacks on Machine Unlearning: Simple Models are Vulnerable ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Martin Bertran \u22c6 Shuai Tang \u22c6 Michael Kearns Amazon AWS AI/ML Jump Trading University of Pennsylvania Amazon AWS AI/ML ", "page_idx": 0}, {"type": "text", "text": "Jamie Morgenstern Aaron Roth Zhiwei Steven Wu University of Washington University of Pennsylvania Carnegie Mellon University Amazon AWS AI/ML Amazon AWS AI/ML Amazon AWS AI/ML ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine unlearning is motivated by desire for data autonomy: a person can request to have their data\u2019s influence removed from deployed models, and those models should be updated as if they were retrained without the person\u2019s data. We show that, counter-intuitively, these updates expose individuals to high-accuracy reconstruction attacks which allow the attacker to recover their data in its entirety, even when the original models are so simple that privacy risk might not otherwise have been a concern. We show how to mount a near-perfect attack on the deleted data point from linear regression models. We then generalize our attack to other loss functions and architectures, and empirically demonstrate the effectiveness of our attacks across a wide range of datasets (capturing both tabular and image data). Our work highlights that privacy risk is significant even for extremely simple model classes when individuals can request deletion of their data from the model. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As model training on personal data becomes commonplace, there has been a growing literature on data protection in machine learning (ML), which includes at least two aspects: ", "page_idx": 0}, {"type": "text", "text": "Data Privacy The primary concern regarding data privacy in machine learning (ML) applications is that models might inadvertently reveal details about the individual data points used in their training. This type of privacy risk can manifest in various ways, ranging from membership inference attacks (Shokri et al., 2017)\u2014which only seek to confirm whether a specific individual\u2019s data was used in the training\u2014to more severe reconstruction attacks (Dick et al., 2023) that attempt to recover entire data records of numerous individuals. To address these risks, algorithms that adhere to differential privacy standards (Dwork et al., 2006) provide proven safeguards, specifically limiting the ability to infer information about individual training data. ", "page_idx": 0}, {"type": "text", "text": "Machine Unlearning Proponents of data autonomy have advocated for individuals to have the right to decide how their data is used, including the right to retroactively ask that their data and its influences be removed from any model trained on it. Data deletion, or machine unlearning, refer to technical approaches which allow such removal of influence (Ginart et al., 2019; Cao & Yang, 2015). The idea is that, after an individual\u2019s data is deleted, the resulting model should be in the state it would have been had the model originally been trained without the individual in question\u2019s data. The primary ", "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Figure 2: CIFAR10 samples reconstructed from a logistic regression model over a random Fourier feature embedding (4096) of the raw input. We randomly chose one deleted sample per label (Row 1) and compared them against the reconstructed sample using our method (HRec, Row 2) and a perturbation baseline (MaxDiff, Row 3) which searches for the public sample with the largest prediction difference before and after sample deletion. HRec produces reconstructions similar to the deleted images both visually and quantitatively measured by cosine similarity. ", "page_idx": 1}, {"type": "text", "text": "focus of this literature has been on achieving or approximating this condition for complex models in ways that are more computationally efficient than full retraining (see e.g. Golatkar et al. (2020); Izzo et al. (2021); Gao et al. (2022); Neel et al. (2021); Bourtoule et al. (2021); Gupta et al. (2021).) ", "page_idx": 1}, {"type": "text", "text": "Practical work on both privacy attacks (like membership inference and reconstruction attacks) and machine unlearning has generally focused on large, complex models like deep neural networks. This is because (1) these models are the ones that are (perceived as) most susceptible to privacy attacks, since they have the greatest capacity to memorize data, and (2) they provide the most technically challenging case for machine unlearning (since for simple models, the baseline of just retraining the model is feasible). For simple (e.g., linear) models, the common wisdom has been that the risk of privacy attacks is low, and indeed, we verify in Appendix A that state-of-the-art membership inference attacks fail to achieve non-trivial performance when attacking linear models trained on tabular data, and an example is shown in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "The main message of our paper is that the situation changes starkly when we consider privacy risks in the presence of machine unlearning. As we show, absent additional protections like differential privacy, requesting that your data be removed\u2014even from a linear regression model\u2014can expose you to a complete reconstruction attack. Informally, this is because it gives the adversary two models that differ in whether your data was used in training, which allows them to attempt a differencing attack. We show that the parameter difference between the two models can be approximately (or exactly, for linear models) expressed as a function of the gradient of the deleted sample and the expected Hessian of the model w.r.t. public data. This allows us to equate model unlearning to releasing the gradient of the unlearned samples, and leverage existing literature on reconstruction from sample gradients to achieve our results. ", "page_idx": 1}, {"type": "text", "text": "Our Contributions We consider the following threat model: An attacker has access to the parameters of some model both before and after a deletion request is made. The attacker also has sampling access to the underlying data distribution, but no access to the training set of the models. In this setting, we first study exact reconstruction attacks on linear regression. We give an attack that accurately recovers the deleted sample given the pair of linear models before and after sample deletion. This is made possible by leveraging the closed-form single-sample training algorithm for linear regression as well as the ability to accurately estimate the covariance matrix of the training data from a modestly sized disjoint sample of public data drawn from the same distribution. ", "page_idx": 1}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/4f046b66b9bf15c6d9bcdb707598ba3b04aa1c27c81377baa47c590776fbdde7.jpg", "img_caption": ["", "Figure 1: We conduct membership inference attacks on a ridge regression on ACS Income task; the attack performance is poor (close to random guessing). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We then extend our attack to the setting where the model consists of a (fixed and known) embedding function, followed by a trained linear layer. Our goal remains to recover the original data point (not only its embedding). This is a natural class of simple models, and also captures last-layer fine-tuning on top of a pre-trained model - a common setting in which machine unlearning guarantees are offered. ", "page_idx": 2}, {"type": "text", "text": "Finally, we give a second-order unlearning approximation via Newton\u2019s method which extends our attack to generic loss functions and model architectures. This provides a way to approximate the gradient of a deleted sample with respect to the model parameters, and later the sample itself. We remark that Newton\u2019s update approximation has itself been proposed as a way to approximate unlearning (Izzo et al., 2021; Gao et al., 2022), and is naturally related to the literature on influence functions (Hampel, 1974; Koh & Liang, 2017; Zhang & Zhang, 2022), which examines the effect any (set of) samples has on the final trained model. ", "page_idx": 2}, {"type": "text", "text": "We experimentally demonstrate the effectiveness of our attack on a variety of simple tabular and image classification and regression tasks. The success of our attack highlights the privacy risks of retraining models to remove the influence of individual\u2019s data, without additional protections like differential privacy (as advocated by e.g. Chourasia et al. (2022) in another context). Figure 2 shows several deleted samples from a model trained on CIFAR10 (Krizhevsky et al., 2009) alongside the recovered reconstructions for our method and a baseline based on public data. ", "page_idx": 2}, {"type": "text", "text": "1.1 Additional Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We elaborate on our threat model and related work to position our approach within the existing literature. Our model involves a scenario where a model maintainer trains a model on a private dataset, $X_{\\mathrm{priv}}\\,\\in\\,\\mathbb{R}^{n\\times d}$ and $y_{\\mathrm{priv}}~\\in~\\mathbb{R}^{n}$ , to minimize a loss function $\\ell$ , yielding parameters $\\beta^{+}$ . Upon a user\u2019s request for data deletion, the maintainer re-trains the model excluding the user\u2019s data, resulting in new parameters $\\beta^{-}$ . Our adversary, equipped with both model parameters before and after the deletion $\\bar{\\beta}^{+},\\beta^{-}$ and access to public samples $(X_{\\mathrm{pub}},y_{\\mathrm{pub}})$ from the same distribution, aims to reconstruct the deleted sample $(x,y)$ using the algorithm $A(\\dot{\\beta}^{+},\\beta^{-},X_{\\mathrm{pub}},y_{\\mathrm{pub}})\\rightarrow(\\tilde{x},\\tilde{y})$ . ", "page_idx": 2}, {"type": "text", "text": "Extensive literature exists on membership inference and reconstruction attacks against static models, with notable references including Shokri et al. (2017); Carlini et al. (2021, 2022); Bertran et al. (2023); Carlini et al. (2023). These studies primarily address attacks on large-scale models, whereas our work focuses on simpler models and explores the changes induced by deletion operations. ", "page_idx": 2}, {"type": "text", "text": "Pioneering work in model inversion attacks on linear models by Fredrikson et al. (2014); Wu et al.   \n(2015) demonstrates that partial information about a data sample can be inferred from model outputs.   \nHowever, these attacks do not directly utilize model parameters. ", "page_idx": 2}, {"type": "text", "text": "The concept of privacy risks in machine unlearning was first outlined by Chen et al. (2021), who introduced a membership inference attack based on shadow models applicable to updates in machine learning models. This attack\u2019s efficacy increases with model complexity. In contrast, our approach targets the reconstruction of the exact data point, extending beyond mere membership inference. ", "page_idx": 2}, {"type": "text", "text": "Further research by Salem et al. (2020) explored reconstruction attacks using single-gradient updates on complex models. Unlike their approach, which relies solely on API access to the model, our method utilizes direct access to model parameters, allowing for efficient reconstruction of data points even in fully retrained simple models. ", "page_idx": 2}, {"type": "text", "text": "Recent works have also examined gradient-based reconstruction attacks, typically focusing on untrained models (Zhu et al., 2019; Zhao et al., 2020; Wang et al., 2023). Our contribution extends these techniques to the context of machine unlearning, highlighting the utility of analyzing updates for data reconstruction. ", "page_idx": 2}, {"type": "text", "text": "Balle et al. (2022) present a scenario where the adversary possesses almost complete knowledge of the training dataset, aiming to reconstruct the missing sample. This represents a distinct model from ours, where the adversary\u2019s knowledge is limited to model parameters and public samples. ", "page_idx": 2}, {"type": "text", "text": "Overall, our work contributes to the understanding of privacy vulnerabilities in machine learning, particularly in scenarios involving model updates and unlearning, where adversaries exploit the slight yet informative differences between model parameters. Most work on machine unlearning asks that they \u201cunlearned\u201d models be indistinguishable from what would have been obtained had the model been retrained without the deleted points \u2014 the baseline that we attack in this paper. However there are several exceptions Cohen et al. (2023); Garg et al. (2020) that ask for stronger conditions (satisfied, e.g. by requiring that the entire sequence of models released satisfy differential privacy like conditions) that can preclude such attacks. The concurrent work in Hu et al. (2024) shares thematic similarities by attacking specific (linearized) approximations to model unlearning. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We develop an attack aimed at reconstructing the features of a deleted user from a (regularized) linear regression model. Specifically, our focus is on reconstructing a sample $(x,y)$ , previously part of the private training dataset $X_{\\mathrm{priv}},y_{\\mathrm{priv}}$ , using models trained before and after the deletion of this sample. ", "page_idx": 3}, {"type": "text", "text": "The parameters of these models, $\\beta^{+}$ and $\\beta^{-}$ , are solutions to a regularized linear regression problem. The model including the deleted sample yields: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta^{+}=\\arg\\operatorname*{min}_{\\beta}\\|X_{\\mathrm{priv}}\\beta-y_{\\mathrm{priv}}\\|_{2}^{2}+\\lambda\\|\\beta\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with a closed-form solution $\\beta^{+}\\;=\\;C^{-1}X_{\\mathrm{priv}}^{\\top}y_{\\mathrm{priv}}$ . Here, $C\\;=\\;X_{\\mathrm{priv}}^{\\top}X_{\\mathrm{priv}}\\;+\\;\\lambda I\\;$ represents the regularized covariance matrix. ", "page_idx": 3}, {"type": "text", "text": "For the model post-deletion, $\\beta^{-}$ can be described similarly, but adjusted for the absence of $(x,y)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta^{-}=(C-x x^{\\top})^{-1}(X_{\\mathrm{priv}}^{\\top}y_{\\mathrm{priv}}-x^{\\top}y).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Using the Sherman-Morrison formula, we can relate $\\beta^{+}$ and $\\beta^{-}$ via: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta^{+}=\\beta^{-}+\\frac{y-x^{\\top}\\beta^{-}}{1+x^{\\top}C^{-1}x}C^{-1}x.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This equation leads to an expression for the change in parameters due to the deletion of $(x,y)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nC(\\beta^{+}-\\beta^{-})=\\alpha(x,y)x,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha(x,y)$ is a scalar function dependent on the sample. This representation shows that the difference between $\\beta^{+}$ and $\\beta^{-}$ , scaled by $C$ , is proportional to $x$ , which suggests a potential avenue for reconstructing $x$ . ", "page_idx": 3}, {"type": "text", "text": "However, when we do not have access to $X_{\\mathrm{priv}}$ or $\\lambda$ , we must rely on publicly available data $X_{\\mathrm{pub}}$ , $y_{\\mathrm{pub}}$ . We approximate $C$ using public data as $\\hat{C}=X_{\\mathrm{pub}}^{\\top}X_{\\mathrm{pub}}$ , considering that $C$ and $\\hat{C}$ are both empirical estimates of the underlying statistical covariance $\\mathbb{E}[x^{t}x]$ . For a rigorous analysis on bounding the errors in this approximation, see Tropp et al. (2015). ", "page_idx": 3}, {"type": "text", "text": "Linear models are often learned with bias terms, which can be interpreted as a feature with value 1. To simply the notation, we assume that the $d_{\\cdot}$ -th dimension of a sample $x$ is 1, noted as $x_{d}=1$ . Given our assumption, the scaling factor $\\alpha(x,y)$ is adjusted by normalizing the reconstructed feature vector to ensure the scale of $x$ is maintained. We then estimate the reconstructed sample $(\\tilde{x},y)$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{x}=\\hat{z}/\\hat{z}_{d},\\qquad\\mathrm{where}\\qquad\\hat{z}=\\hat{C}(\\beta^{+}-\\beta^{-})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{z}_{d}$ is the $d$ -th element of $\\hat{z}$ , ensuring $\\tilde{x}_{d}=1$ . This method offers a systematic approach to estimate deleted user features from differential changes in model parameters, employing public data to approximate necessary statistics and regularization impacts. ", "page_idx": 3}, {"type": "text", "text": "3 Beyond Linear Regression ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our attack is derived for the simple models used in linear regression. However, the same ideas can be generalised beyond linear regression. The attack generalises immediately to any model which performs linear regression on top of a fixed embedding: our attack recovers the embedding of the deleted point, and reduces the problem to inverting the embedding. We can also generalise to other loss functions\u2014The primary challenge is that we no longer have closed-form expressions for an \u201cupdate\u201d, but we can approximate this, as we describe in Section 3.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Fixed Embedding Functions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our attack is built upon the analytical update of adding or deleting a sample to a linear regression model, therefore, it also generalises to linear models trained on top of embeddings of the original features. Suppose that both parameter vectors $\\beta^{+}$ and $\\beta^{-}$ along with the embedding function $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d^{\\prime}}$ are publicly known (and the embedding function has a bias term). Our attack first reconstructs the embeddings as in Eq. (5), and then reconstructs features by finding a data point whose embedding best matches the reconstructed transformed features as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{x}=\\arg\\operatorname*{min}_{x}\\left\\|\\tilde{z}/\\tilde{z}_{d^{\\prime}}-\\phi(x)\\right\\|,\\qquad\\mathrm{where}\\qquad\\tilde{z}=\\phi(X_{\\mathrm{pub}})^{\\top}\\phi(X_{\\mathrm{pub}})\\left(\\beta^{+}-\\beta^{-}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, we assume that the embedding $\\phi$ is fixed\u2014that is, it doesn\u2019t change after deleting a sample. This is the case when e.g. performing last-layer fine-tuning on top of a pre-trained model, and for data-independent embeddings like random Fourier features. ", "page_idx": 4}, {"type": "text", "text": "3.2 Arbitrary Loss Functions and Architectures ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our foundational equation, Eq. (4), specifically addresses linear models minimized under the mean squared error. When broadening this scope to include alternative loss functions and model architectures, the luxury of closed-form solutions vanishes. However, we can utilize Newton\u2019s method for approximating the \"update function\" necessary after data deletion. Consider a model maintainer optimizing an empirical risk function represented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell(\\beta;X_{\\mathrm{priv}},y_{\\mathrm{priv}})=\\frac{1}{n}\\Sigma_{(x,y)\\in\\{X_{\\mathrm{priv}},y_{\\mathrm{priv}}\\}}\\ell(\\beta;x,y),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta^{+}$ and $\\beta^{-}$ are the optimal parameters before and after excluding a specific data point, $(x,y)$ , respectively. By adopting a second-order Taylor approximation via Newton\u2019s method, we estimate: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\beta^{-}\\approx\\beta^{+}-H^{-1}\\nabla\\ell,}\\\\ &{\\mathrm{where~}H=\\nabla_{\\beta=\\beta^{+}}^{2}\\ell(\\beta;X_{\\mathrm{priv}}\\backslash x,y_{\\mathrm{priv}}\\backslash y),}\\\\ &{\\qquad\\nabla\\ell=\\nabla_{\\beta=\\beta^{+}}\\ell(\\beta;X_{\\mathrm{priv}}\\backslash x,y_{\\mathrm{priv}}\\backslash y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using the first-order optimality conditions, we deduce that the aggregate gradient over the remaining samples inversely equals that of the removed sample: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\nabla_{\\beta=\\beta^{+}}\\ell(\\beta;X_{\\mathrm{priv}},y_{\\mathrm{priv}})=0,}\\\\ {n\\nabla\\ell=-\\nabla_{\\beta=\\beta^{+}}\\ell(\\beta;x,y).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Integrating Eq. (10) into Eq. (8), we derive: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\beta^{-}\\approx\\beta^{+}+\\displaystyle\\frac{H^{-1}}{n}\\nabla_{\\beta=\\beta^{+}}\\ell(\\beta;x,y),}}\\\\ {{n H(\\beta^{+}-\\beta^{-})\\approx-\\nabla_{\\beta=\\beta^{+}}\\ell(\\beta;x,y).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In linear regression, this method precisely recovers the known Eq (4). In the general case, the Hessian matrix, analogous to the covariance matrix in linear regression, is estimated using public data sharing the same distribution as the private data: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{H}=\\frac{1}{m}\\Sigma_{x^{\\prime},y^{\\prime}\\in\\{X_{\\mathrm{pub}},y_{\\mathrm{pub}}\\}}\\nabla_{\\beta=\\beta^{+}}^{2}\\ell(\\beta;x^{\\prime},y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For linear models under attack, the gradients at the removed loss sample, particularly for certain model layers, correlate directly with the data of the removed sample. Efficiently approximated through influence matrices such as the Fisher information matrix, these gradients serve as crucial elements in reconstructive attacks on model privacy. ", "page_idx": 4}, {"type": "text", "text": "For non-linear models and more intricate architectures, the materialization of the Hessian may become impractical due to its size, prompting the use of efficient Hessian-vector product computations. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Generalized Attack ", "page_idx": 5}, {"type": "table", "img_path": "i4gqCM1r3z/tmp/d16f0d1ff39c52fdb92579212cf9ed96e1c85fd6bf1f9739bc56d96b9961e1fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3 Multiclass Classification and Label Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In a multiclass classification setting, our approach described in Eq. (12) facilitates an estimation of parameter gradients with respect to the deleted sample. Employing Eq. (6), initially defined in Section 3.1, allows for reconstructing the deleted data point. In models using linear layers directly post-embedding, recovery becomes straightforward. However, for models with multiple class-specific parameters, label inference requires additional steps. ", "page_idx": 5}, {"type": "text", "text": "Employing a softmax nonlinearity for outputting probability vectors, we observe that the derivative of the loss with respect to the bias for the correct label is distinctively negative, setting it apart from the other biases which demonstrate positive derivatives under typical loss functions: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{b_{j}}[-\\ln f_{y}(x;\\beta^{+})]=f_{j}(x;\\beta^{+})-\\mathbf{1}[j=y].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The deleted label $y$ can then be inferred as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{y}=\\arg\\operatorname*{min}_{j}\\nabla_{b_{j}}\\ell(\\beta;x,y)\\big|_{\\beta=\\beta^{+}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This approach significantly extends the capabilities of privacy attacks to encompass a wider array of multiclass classification models. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We assess our attack across diverse datasets, including tabular and image data, and for both classification and regression tasks. Initially, we train a model on the complete dataset $X_{\\mathrm{priv}},y_{\\mathrm{priv}}$ to derive parameters $\\beta^{+}$ , and then retrain it\u2014excluding a single sample $(x,y),$ \u2014to obtain $\\beta^{-}$ . We note that $\\beta^{-}$ is achieved through full retraining, not approximate unlearning methods\u2014our attack does not depend on imperfect unlearning to be effective. ", "page_idx": 5}, {"type": "text", "text": "Our attack leverages public data samples from the same distribution as the training data but does not require knowledge of the deleted sample\u2019s features or label. We evaluate our approach against two baselines that utilize public data: ", "page_idx": 5}, {"type": "text", "text": "\u201cAvg\u201d: Predicts the deleted sample as $\\begin{array}{r}{\\hat{x}=\\frac{1}{m}\\sum_{x\\in X_{\\mathrm{pub}}}x}\\end{array}$ , an average of the public samples. ", "page_idx": 5}, {"type": "text", "text": "\u201cMaxDiff\u201d: Identifies the public sample that maximizes the prediction discrepancy between $\\beta^{+}$ and $\\beta^{-}$ as $\\begin{array}{r}{\\hat{x}=\\arg\\operatorname*{max}_{x\\in X_{\\mathrm{pub}}}\\grave{|}|x^{T}(\\beta^{+}\\stackrel{\\cdot}{-}\\beta^{-})||}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "These baselines exploit similarities between public and private data, with \u201cMaxDiff\u201d additionally considering the change in model parameters. Our threat model differs significantly from that of Salem et al. (2020), who assume black-box access and a simpler model update scenario; details and comparisons are found in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "For practical simulations, each dataset is split into two: one for private training and the other for public samples. We simulate the deletion of each sample in the private set, retrain the model, and attempt its reconstruction using our method and the baselines. ", "page_idx": 5}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/2c8327b54a4eec5e0f7ea74a89c9e4a1ed7424621cc776d7f73ac9e9852b2555.jpg", "img_caption": ["(c) CIFAR10 results "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Cumulative distribution function of cosine similarity between deleted and reconstructed sample via the average, MaxDiff, and HRec (our) attack on MNIST, Fashion MNIST and CIFAR10 for three model architectures (linear cross-entropy, ridge regression over 4096 random Fourier features, and cross-entropy over 4096 random Fourier features). Lower curves correspond to more effective attacks than higher curves. Our attack achieves better cosine similarity with the deleted sample across all settings; the effect is especially apparent in the denser CIFAR10 dataset. ", "page_idx": 6}, {"type": "text", "text": "Hyperparameters, specifically the $\\ell_{2}$ regularization strength $\\lambda$ , are optimized on the private set and remain constant when recalculating $\\beta^{-}$ . We explore attacks on unregularized models in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "We quatify the efficacy of our attacks and baselines with the cosine similarity between deleted and reconstructed samples, aggregated into a Cumulative Distribution Function (CDF) of these similarity scores; a sharp peak near 1 indicates precise reconstruction. CDFs which are \u201cbelow\" others correspond to more effective attacks, which have a higher fraction of reconstructed points with very high similarity to the original data. The performance of target models is evaluated in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "4.1 Image Data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our generalized attack methodology, outlined in Algorithm 1, across three distinct model configurations on Fashion MNIST (FMNIST), MNIST, and CIFAR10 datasets. Fashion MNIST and MNIST consist of $28\\times28$ grayscale images, whereas CIFAR10 includes $32\\times32\\times3$ RGB images, with all datasets aimed at 10-way classification (Xiao et al., 2017; LeCun et al., 1998; Krizhevsky et al., 2009). Each dataset undergoes a normalization process where input features are scaled to the range $[-1,1]$ . ", "page_idx": 6}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/5ef3e998856078e0c1b9fd0be1296c4b928cba273bf9143bc7c11aef30f3de21.jpg", "img_caption": ["(a) Fashion MNIST ", "(b) MNIST "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Sample reconstructions on Fashion MNIST/ MNIST for a $40K$ parameter model (crossentropy over random Fourier features of the raw input). We randomly chose one deleted sample per label (Rows 1, 4) and compared them against the reconstructed sample using our method (HRec, Rows 2, 5) and a perturbation baseline (MaxDiff, Rows 3, 6) which searches for the public sample with the largest prediction difference before and after sample deletion. HRec produces reconstructions that are highly similar to the deleted images. ", "page_idx": 7}, {"type": "text", "text": "Cross-entropy Loss for Multiclass Classification: We first consider a linear model with a softmax output, trained to minimize the cross-entropy loss $\\ell_{C E}(\\beta,x,y)=-\\log\\sigma_{y}(x^{\\top}\\beta)+\\lambda\\|\\beta\\|_{2}^{2}$ . This model facilitates direct gradient estimation with respect to the parameters $\\beta^{+}$ proportional to the raw input $x$ , negating the need for Eq. (6) ", "page_idx": 7}, {"type": "text", "text": "Ridge Regression with Random Fourier Features: The second model incorporates an embedding function $\\phi$ , generating random Fourier features (Rahimi & Recht, 2007). The associated loss function, $\\ell_{R i d g e}(\\beta,\\bar{\\phi(x)},y)\\overset{\\sim}{=}\\|\\phi(x)^{\\top}\\beta-y\\|_{2}^{2}+\\lambda\\|\\beta\\|_{2}^{2}$ , admits an analytical solution for the Hessian matrix $H=\\phi(X)^{\\top}\\phi(X)$ concerning $\\phi(x)$ , but requires embedding inversion via Eq. (6). ", "page_idx": 7}, {"type": "text", "text": "Cross-entropy Loss with Random Fourier Features: This model merges the complexities of the first two: it lacks a closed-form update solution and necessitates embedding inversion, addressing scenarios with both softmax nonlinearity and random Fourier embeddings. ", "page_idx": 7}, {"type": "text", "text": "Figure 3 illustrates the efficacy of our attack across all three model types on MNIST, FMNIST and CIFAR10, demonstrating the capability to consistently recover samples highly similar to the original, deleted samples. Figures 2, 4a, and 4b depict randomly sampled deletions and their nearest recovered samples using HRec and MaxDiff techniques, particularly under the challenging conditions of crossentropy loss over random Fourier features. Further results for additional model configurations can be found in Section E, expanding upon the robustness and versatility of our attack methodology across varied settings and data modalities. ", "page_idx": 7}, {"type": "text", "text": "4.2 Tabular Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ridge Regression for Income Prediction We perform an attack on ridge regression models using American Community Survey (ACS) income data from 2018 (Ding et al., 2021). Our approach involves direct application of Eq. (4) along with an intercept normalization technique. Figure 5 (first row) illustrates the attack performance. If the covariance matrix of the private data and the regularization parameter $\\lambda$ were known, our approach could perfectly recover the deleted sample. However, our approximation assumes $\\lambda=0$ and estimates the covariance matrix from public samples, which introduces some estimation error. Despite these approximations, we achieve near-perfect reconstruction of the deleted sample. ", "page_idx": 7}, {"type": "text", "text": "Ridge Regression with Random Features In a variation of the previous attack, we target a ridge regression model trained with random Fourier features (Rahimi & Recht, 2007). Assuming access to both the random Fourier features and the model weights, we reconstruct the embedding $\\tilde{z}$ of the deleted sample and then solve an inverse problem to recover the original features. The results, depicted in the second row of Figure 5, demonstrate significant improvement over the baselines, achieving almost perfect reconstruction accuracy. ", "page_idx": 7}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/0f14a1ad3f40dbe59d87653ec5c96127368a4afefa084224176f828ac5b62885.jpg", "img_caption": ["Figure 5: ACS Income Regression. Target models are ridge regression with tuned hyperparameters on original features (first row), and over random Fourier features (second row). ACS Income data from three states are used to demonstrate the effectiveness of our attack. Given the analytical single-sample update rules of linear regression, our attack (HRec) reconstructs the deleted sample almost perfectly on all datasets and different embedding functions. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Binary Classification for Income Level Prediction We extend our analysis to binary classification tasks using logistic regression and support vector machines (SVMs) with squared hinge loss. Both models allow analytical computation of their Hessian matrices: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{H}=X_{\\mathrm{pub}}^{\\top}D X_{\\mathrm{pub}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $D\\,\\in\\,\\mathbb{R}^{M\\times M}$ is a diagonal matrix. For logistic regression, the diagonal terms are defined as $D_{i i}=\\sigma(x_{i}^{\\top}\\beta^{+})(1-\\sigma(x_{i}^{\\top}\\beta^{+}))$ , with $\\sigma$ being the sigmoid function. For SVMs, the terms are $D_{i i}\\,=\\,\\mathbb{1}(1-y_{i}x_{i}^{\\top}\\beta^{+}\\,\\geq\\,0)$ . The reconstruction of the deleted sample\u2019s features is obtained via $\\hat{H}(\\beta^{+}-\\beta^{-})$ , enhanced by the intercept normalization trick. The performance of these attacks is showcased in Figure 6a, where our method, HRec, consistently outperforms the baselines across all datasets and model classes. ", "page_idx": 8}, {"type": "text", "text": "Binary Classification with Random Features Lastly, we attack binary classification models trained on an enriched set of random Fourier features. Figure 6b presents the performance curves for our attack compared to various baselines. Our method outperforms all baselines in attacks on logistic regression models and performs competitively with the MaxDiff baseline in attacks on SVM models, highlighting the efficacy of our approach. ", "page_idx": 8}, {"type": "text", "text": "Our comprehensive attack strategies on regression and binary classification models demonstrate the potential vulnerabilities in these machine learning setups, especially when certain model parameters or features are accessible to an adversary. These findings underscore the need for robust privacypreserving mechanisms in machine learning applications. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present a strong reconstruction attack that targets data points that are deleted from simple models. Our reconstructions are nearly perfect for linear regression models, and still achieve high-quality reconstructions for linear models constructed on top of embeddings, and for models which optimize various objective functions. This shines a light on the privacy risk inherent in even very simple ", "page_idx": 8}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/19ad7fb522f3f6b7a1c1daa0c03d2e351cde92fcb7a8bbdc8563e3f98d7699d5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "", "img_caption": ["(b) Random Fourier Features. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 6: ACS Income Level Prediction. Target models are logistic regression (first row) and SVM (second row) for binary classification tasks. These methods do not have analytical forms of the single-sample update; however, our approximation using Newton\u2019s update facilitates the outstanding performance of HRec among all attacks. While this reconstruction is imperfect due to approximation errors, a large number of deleted samples can still be reconstructed with high similarity scores. ", "page_idx": 9}, {"type": "text", "text": "models in the context of data deletion or \u201cmachine unlearning\u201d, and motivates using technologies like differential privacy to mitigate reconstruction risk. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Balle, B., Cherubin, G., and Hayes, J. Reconstructing training data with informed adversaries. In 2022 IEEE Symposium on Security and Privacy (SP), pp. 1138\u20131156. IEEE, 2022. ", "page_idx": 10}, {"type": "text", "text": "Bertran, M., Tang, S., Kearns, M., Morgenstern, J., Roth, A., and Wu, Z. S. Scalable membership inference attacks via quantile regression. arXiv preprint arXiv:2307.03694, 2023.   \nBourtoule, L., Chandrasekaran, V., Choquette-Choo, C. A., Jia, H., Travers, A., Zhang, B., Lie, D., and Papernot, N. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy $(S P_{\\mathrm{.}}$ ), pp. 141\u2013159. IEEE, 2021.   \nCao, Y. and Yang, J. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy, pp. 463\u2013480. IEEE, 2015.   \nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633\u20132650, 2021.   \nCarlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., and Tramer, F. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP), pp. 1897\u20131914. IEEE, 2022.   \nCarlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag, V., Tramer, F., Balle, B., Ippolito, D., and Wallace, E. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pp. 5253\u20135270, 2023.   \nChen, M., Zhang, Z., Wang, T., Backes, M., Humbert, M., and Zhang, Y. When machine unlearning jeopardizes privacy. In Proceedings of the 2021 ACM SIGSAC conference on computer and communications security, pp. 896\u2013911, 2021.   \nChourasia, R., Shah, N., and Shokri, R. Forget unlearning: Towards true data-deletion in machine learning. arXiv preprint arXiv:2210.08911, 2022.   \nCohen, A., Smith, A., Swanberg, M., and Vasudevan, P. N. Control, confidentiality, and the right to be forgotten. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pp. 3358\u20133372, 2023.   \nDick, T., Dwork, C., Kearns, M., Liu, T., Roth, A., Vietri, G., and Wu, Z. S. Confidence-ranked reconstruction of census microdata from published statistics. Proceedings of the National Academy of Sciences, 120(8):e2218605120, 2023. doi: 10.1073/pnas.2218605120. URL https://www. pnas.org/doi/abs/10.1073/pnas.2218605120.   \nDing, F., Hardt, M., Miller, J., and Schmidt, L. Retiring adult: New datasets for fair machine learning. Advances in Neural Information Processing Systems, 34, 2021.   \nDwork, C., McSherry, F., Nissim, K., and Smith, A. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pp. 265\u2013284. Springer, 2006.   \nFredrikson, M., Lantz, E., Jha, S., Lin, S., Page, D., and Ristenpart, T. Privacy in pharmacogenetics: An {End-to-End} case study of personalized warfarin dosing. In 23rd USENIX security symposium (USENIX Security 14), pp. 17\u201332, 2014.   \nGao, J., Garg, S., Mahmoody, M., and Vasudevan, P. N. Deletion inference, reconstruction, and compliance in machine (un) learning. arXiv preprint arXiv:2202.03460, 2022.   \nGarg, S., Goldwasser, S., and Vasudevan, P. N. Formalizing data deletion in the context of the right to be forgotten. In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pp. 373\u2013402. Springer, 2020.   \nGinart, A., Guan, M., Valiant, G., and Zou, J. Y. Making ai forget you: Data deletion in machine learning. Advances in neural information processing systems, 32, 2019.   \nGolatkar, A., Achille, A., and Soatto, S. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9304\u20139312, 2020.   \nGupta, V., Jung, C., Neel, S., Roth, A., Sharifi-Malvajerdi, S., and Waites, C. Adaptive machine unlearning. Advances in Neural Information Processing Systems, 34:16319\u201316330, 2021.   \nHampel, F. R. The influence curve and its role in robust estimation. Journal of the american statistical association, 69(346):383\u2013393, 1974.   \nHu, H., Wang, S., Dong, T., and Xue, M. Learn what you want to unlearn: Unlearning inversion attacks against machine unlearning. arXiv preprint arXiv:2404.03233, 2024.   \nIzzo, Z., Smart, M. A., Chaudhuri, K., and Zou, J. Approximate data deletion from machine learning models. In International Conference on Artificial Intelligence and Statistics, pp. 2008\u20132016. PMLR, 2021.   \nKoh, P. W. and Liang, P. Understanding black-box predictions via influence functions. In International conference on machine learning, pp. 1885\u20131894. PMLR, 2017.   \nKrizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.   \nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \nNeel, S., Roth, A., and Sharifi-Malvajerdi, S. Descent-to-delete: Gradient-based methods for machine unlearning. In Algorithmic Learning Theory, pp. 931\u2013962. PMLR, 2021.   \nRahimi, A. and Recht, B. Random features for large-scale kernel machines. Advances in neural information processing systems, 20, 2007.   \nSalem, A., Bhattacharya, A., Backes, M., Fritz, M., and Zhang, Y. {Updates-Leak}: Data set inference and reconstruction attacks in online learning. In 29th USENIX security symposium (USENIX Security 20), pp. 1291\u20131308, 2020.   \nShokri, R., Stronati, M., Song, C., and Shmatikov, V. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 3\u201318. IEEE, 2017.   \nTropp, J. A. et al. An introduction to matrix concentration inequalities. Foundations and Trends\u00ae in Machine Learning, 8(1-2):1\u2013230, 2015.   \nWang, Z., Lee, J., and Lei, Q. Reconstructing training data from model gradient, provably. In International Conference on Artificial Intelligence and Statistics, pp. 6595\u20136612. PMLR, 2023.   \nWu, X., Fredrikson, M., Wu, W., Jha, S., and Naughton, J. F. Revisiting differentially private regression: Lessons from learning theory and their consequences. arXiv preprint arXiv:1512.06388, 2015.   \nXiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \nZhang, R. and Zhang, S. Rethinking influence functions of neural networks in the over-parameterized regime. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 9082\u2013 9090, 2022.   \nZhao, B., Mopuri, K. R., and Bilen, H. idlg: Improved deep leakage from gradients. arXiv preprint arXiv:2001.02610, 2020.   \nZhu, L., Liu, Z., and Han, S. Deep leakage from gradients. Advances in neural information processing systems, 32, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Membership Inference Attacks on Linear Models ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Linear models usually have lower privacy risks compared to neural networks because the parameters of a linear model are significantly fewer. To demonstrate the low privacy risks, we conduct a state-ofthe-art membership inference attack (MIA) proposed by Carlini et al. (2022) \u2014 Likelihood Ratio Attack (LiRA) \u2014 on the same tabular tasks. The goal of MIA is to determine whether a sample is in the training set of the target model. ", "page_idx": 12}, {"type": "text", "text": "On each task, we split the dataset into three splits, including $40\\%$ for training the target model, another $40\\%$ as the public samples for learning shadow models, and the rest $20\\%$ as the holdout set for evaluation. After training the target model on the private training set, we train 64 shadow models using the same optimization algorithm on the public samples with Bootstrap. Then, we use the joint set of the private samples and the holdout samples as samples under attack, and evaluate the attack performance. ", "page_idx": 12}, {"type": "text", "text": "As shown in Figure 7, the attack performance is close to random guessing, which implies that it is already challenging to determine which sample has been used in training when the target model is linear. ", "page_idx": 12}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/fb30d3609de391ae5fcdf918f6c7fcbac7efa9ac337799072c3aff0d512b750c.jpg", "img_caption": ["Figure 7: Membership inference attacks on ACS tasks. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Additional Comparisons ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Here we provide a limited comparison of Updates-leak Salem et al. (2020) against our method and baselines for the same simple model architecture (cross-entropy loss over a linear model on top of 4096 random Fourier features). We stress that the threat model for Updates-Leak differs from our own in two important ways. First, they assume query access to the model, while we assume access to the parameters. Second, we carry out our attack on two models, fully trained to convergence on two different datasets, $((X_{\\mathrm{priv}},y_{\\mathrm{priv}})$ and $(X_{\\mathrm{priv}}\\setminus x,y_{\\mathrm{priv}}\\setminus y))$ . In contrast, Updates-leak instead attacks the difference between a model trained on $(X_{\\mathrm{priv}}\\setminus x,y_{\\mathrm{priv}}\\setminus y)$ and an updated model where in the update, only a single gradient descent step is taken on the \u2018update\u2019 sample $(x,y)$ (single sample attack version). The Updates-Leak approach also incurs a significantly higher computational cost due to its shadow model and encoder learning approach. For these reasons, we limit the comparison to a single model architecture on CIFAR10 while stressing this comparison is not \u2018apples to apples\u2019. In particular, even though we plot the reconstrution cosine similarity curves on the same axis (and see that ours improves), our technique and UpdatesLeak are attacking different pairs of models (we attack the model that results from full retraining, whereas they attack the model that results from a single gradient update). ", "page_idx": 12}, {"type": "text", "text": "Figure 8 shows the cosine similarity comparison, and Figure 9 show some example reconstructions for Updates-Leak in this scenario. We leveraged their publicly available code to produce these comparisons, using their default configuration (10, 000 shadow models are used for training, and their DC-GAN generator is trained for 10, 000 epochs on the shadow model dataset). ", "page_idx": 12}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/40664a11f30d81c9afc967bd4e81b27ba5e4d616d2c48544cb7d625df501fab2.jpg", "img_caption": ["Figure 8: Cumulative distribution function of cosine similarity between the target (deleted) sample and the reconstructed sample via the average, MaxDiff, Updates-Leak, and HRec (our) attack on CIFAR10 on a simple model (cross-entropy loss over a linear model on top of 4096 random Fourier features). All attacks save for Updates-Leak operate against the full retraining baseline, that is the comparison between two models trained from scratch until convergence in a dataset with and without the \u2018deleted\u2019 sample $((X_{\\mathbf{priv}},y_{\\mathbf{priv}})$ and $(X_{\\mathbf{priv}}\\setminus x,y_{\\mathbf{priv}}\\setminus\\bar{y}))$ , Updates-Leak instead attacks two models, one trained until convergence on the dataset without the sample $((X_{\\mathbf{priv}}\\setminus x,y_{\\mathbf{priv}}\\setminus y))$ , and one that took a single gradient descent step on the loss of the updated sample $x$ , $y$ . Here lower curves dominate higher curves. Our attack achieves better cosine similarity with the deleted sample. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/f2877436add37501dcc3c7207acb63205f2d1d88c0af15c066f41d2563b7e154.jpg", "img_caption": ["Figure 9: Sample reconstructions for Updates-Leak on CIFAR10. Original images and their corresponding reconstruction are shown side by side in an alternating fashion. The model architecture is cross-entropy loss over a linear model on top of 4096 random Fourier features. The models before and after the update differ in a single gradient descent step being taken on the update sample $(x,y)$ . "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Attacking Unregularized Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our main experiments, we emulate the more realistic situation where the model maintainer tunes the hyperparameter of the target model on the entire dataset, and keeps it fixed during unlearning. Since the impact of regularization on the attack performance is rather challenging to analyze and not immediately obvious, we here present results on attacking models without regularization. ", "page_idx": 15}, {"type": "text", "text": "C.1 ACS Income Regression ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "On this task, the model maintainer directly optimizes the following objective without the regularization term: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta^{*}=\\arg\\operatorname*{min}_{\\beta}\\|X\\beta-y\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This problem admits an analytical expression for $\\beta^{+}$ and $\\beta^{-}$ , which can be written as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta^{+}=C^{-1}X_{\\mathrm{priv}}^{\\top}y_{\\mathrm{priv}},}\\\\ &{\\beta^{-}=(C-x x^{\\top})^{-1}(X_{\\mathrm{priv}}^{\\top}y_{\\mathrm{priv}}-x^{\\top}y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $C=X_{\\mathrm{priv}}^{\\top}X_{\\mathrm{priv}}$ is the covariance matrix. In the scenario where the inverse of the covariance matrix doesn\u2019t exist, we use the Moore\u2013Penrose inverse instead. Our attack still stays the same. ", "page_idx": 15}, {"type": "text", "text": "The results are presented in Figure 10, and we can see that without regularization ", "page_idx": 15}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/09e7150a4ad517c0bc694e52f421690c99e6aed1e1d952e9da0a79a6429919fc.jpg", "img_caption": ["Figure 10: ACS Income Regression. Target models are ordinary linear regression on original features (first row), and over random Fourier features (second row). Our attack HRec reconstructs the deleted sample almost perfectly. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Image Classification Tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we additionally show results across CIFAR10, MNIST, and Fashion MNIST on the more challenging target model scenario (RBF Cross Entropy). For these results, the model maintainer does not use any form of regularization. Results are shown in Figure 11. ", "page_idx": 15}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/c7c74ef9d3428a78bd489333c68bf1e9a5f2ce893acb17c6b2238d18e3d755cb.jpg", "img_caption": ["(a) Fashion MNIST results "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/96aa7e4b3ab9f046468840007607eff24078846ae5e0617e1f04b8d273059341.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "", "img_caption": ["(c) CIFAR10 results "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 11: Cumulative distribution function of cosine similarity between the target (deleted) sample and the reconstructed sample via the average, MaxDiff, and HRec (our) attack on Fashion MNIST, MNIST, and CIFAR10 for a target model using cross-entropy over 4096 random Fourier features. In this scenario, the model maintainer does not use any form of regularization when training the original or updated model. Here lower curves dominate higher curves. Our attack achieves better cosine similarity with the deleted sample across all settings; the effect is especially apparent in the denser CIFAR10 dataset. ", "page_idx": 16}, {"type": "text", "text": "D Performance of Target Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 ACS Income Tasks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 1: Performance of target models on ACS Income tasks. Regression tasks are evaluated using $r^{2}$ , which indicates the portion of explained variance, and classification tasks are evaluated using $F1$ score since class labels are not balanced. ", "page_idx": 17}, {"type": "table", "img_path": "i4gqCM1r3z/tmp/01c507eac6749d620f4ece579692e418077ecc6680eab41c3d3936833d0ddc2a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.2 Image tasks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 2: Out of sample accuracy of target models on Image tasks ", "page_idx": 17}, {"type": "table", "img_path": "i4gqCM1r3z/tmp/d8445146bfd2da2f04b5fe757aa64000ec2c5cd6c09096629b50d06d56aeedf9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/dccdca6601228a2d025c84352dd65cb868ec2a7acd80895b166b0b14785df341.jpg", "img_caption": ["Figure 12: Sample reconstructions on CIFAR10. Rows 1-3 rows show results of attacking a linear cross-entropy model, and rows 4-6 show similar results for ridge regression over 4096 random Fourier features. We randomly chose one deleted sample per label (shown in rows 1 and 4) and compared them against the reconstructed sample using our method (HRec, rows 2 and 5) and a perturbation baseline (MaxDiff, rows 3 and 6) which searches for the public sample with the largest prediction difference before and after sample deletion. HRec produces reconstructions that are highly similar to the deleted images. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/53b4c6bda6d03ce57da17644ba48757f4172ee56a71ad7083959bdebe19fef74.jpg", "img_caption": ["Figure 13: Sample reconstructions on Fashion MNIST. Rows 1-3 rows show results of attacking a linear cross-entropy model, and rows 4-6 show similar results for ridge regression over 4096 random Fourier features. We randomly chose one deleted sample per label (shown in rows 1 and 4) and compared them against the reconstructed sample using our method (HRec, rows 2 and 5) and a perturbation baseline (MaxDiff, rows 3 and 6) which searches for the public sample with the largest prediction difference before and after sample deletion. HRec produces reconstructions that are highly similar to the deleted images. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "i4gqCM1r3z/tmp/d27a14a10de5b5a23f5786eee8ef187e0eef6c93c5c1ab76e07830f2f479f7f7.jpg", "img_caption": ["Figure 14: Sample reconstructions on MNIST. Rows 1-3 rows show results of attacking a linear cross-entropy model, and rows 4-6 show similar results for ridge regression over 4096 random Fourier features. We randomly chose one deleted sample per label (shown in rows 1 and 4) and compared them against the reconstructed sample using our method (HRec, rows 2 and 5) and a perturbation baseline (MaxDiff, rows 3 and 6) which searches for the public sample with the largest prediction difference before and after sample deletion. HRec produces reconstructions that are highly similar to the deleted images. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The abstract describes the context of the contribution and describes the main theoretical and practical results presented ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide clear context on the settings where our contribution applies and substantiate each claim with theory and/or empirical results. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: assumptions and results are clearly stated. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide a detailed algorithm and description of the exact setup of our experiments ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Open source code will be provided at a later date ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: setup and hyperparameters are well described ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: While we don\u2019t report error bars, we report the full distribution of the reconstruction error for our experiments in the form of cosine similarity CDF curves. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: The computational costs of the experiments were small enough that they could be run serially on a single GPU machine without great effort. ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] Justification: We comply fully with the code of ethics ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the potential impacts of our work in the introduction and conclusion. Chief among them that more care needs to be taken to ensure model unlearning preserves user privacy (e.g., using DP at the model learning/disgorging phase). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper highlights an existing vulnerability in the field of machine unlearning. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: (public) datasets and architectures are properly cited. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA]   \nJustification: No new assets are provided   \nGuidelines: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: NA ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: NA ", "page_idx": 21}]