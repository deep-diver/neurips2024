{"importance": "This paper is crucial for researchers working on decision-making and confidence modeling due to its **novel normative framework** that unifies existing models.  It **addresses the limitations** of previous approaches by incorporating reward, prior knowledge, and uncertainty, opening new avenues for research in this critical area. The **superior performance of the model** compared to existing methods across multiple experiments makes it a highly valuable contribution.", "summary": "New normative framework for decision confidence models diverse tasks by incorporating rewards, priors, and uncertainty, outperforming existing methods.", "takeaways": ["A novel normative framework for modeling decision confidence is introduced that generalizes across various tasks.", "The model maps to the planning-as-inference framework, maximizing reward and information entropy.", "Superior performance is demonstrated over other approaches in explaining subjects' confidence reports across diverse experiments."], "tldr": "Many existing computational models of decision confidence are limited to specific scenarios, such as comparing choices with identical values. This restricts their applicability to various real-world tasks. Moreover, most experiments in value-based decision-making focus solely on value and neglect the role of perceptual uncertainty. This paper introduces a new normative framework for modeling decision confidence that addresses these issues. \nThe proposed framework models decision confidence as the probability of making the best decision and is generalizable to various tasks and experimental setups. It maps to the planning-as-inference framework, where the objective function is maximizing gained reward and information entropy. This model's efficacy was validated on two different psychophysics experiments and shows superiority to existing methods in explaining subjects' confidence reports.", "affiliation": "University of Washington", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "BRvGfN3Xfm/podcast.wav"}