[{"figure_path": "XVfevb9XFx/figures/figures_4_1.jpg", "caption": "Figure 1: The results of actors updated with different critics.", "description": "This figure compares the performance of actors updated with three different critics: a fixed re-evaluated critic, an iteratively re-evaluated critic (which updates with the policy), and the proposed aligned critic.  The results show that the performance of the re-evaluated critics sharply declines at the initial stage and does not recover, while the aligned critic achieves stable and favorable performance.  This highlights the importance of aligning the critic with the offline policy for effective online fine-tuning.", "section": "4.2 Value Alignment"}, {"figure_path": "XVfevb9XFx/figures/figures_4_2.jpg", "caption": "Figure 1: The results of actors updated with different critics.", "description": "This figure compares the performance of three different critics in online reinforcement learning: a fixed re-evaluated critic, an iteratively re-evaluated critic, and an aligned critic.  The results show that the fixed and iterated critics experience a sharp decline in performance at the beginning of online training, while the aligned critic maintains stable and favorable performance. This highlights the importance of aligning the critic with the offline policy for effective online fine-tuning.", "section": "4.2 Value Alignment"}, {"figure_path": "XVfevb9XFx/figures/figures_7_1.jpg", "caption": "Figure 2: Performance curves on D4RL [9] MuJoCo locomotion tasks during online fine-tuning.", "description": "This figure shows the performance curves of various offline-to-online reinforcement learning (RL) methods on several MuJoCo locomotion tasks from the D4RL benchmark.  The x-axis represents the number of evaluation epochs during online fine-tuning, and the y-axis represents the normalized return achieved by each method.  The figure allows comparison of the proposed methods (O2SAC, O2TD3, O2PPO) against existing state-of-the-art methods (AWAC, IQL, PEX, Off2On, Cal-QL, ACA).  The different colored lines and shaded areas represent the mean and standard deviation across multiple runs for each method, highlighting the performance consistency and stability of each algorithm on the tasks.", "section": "5 Experiments"}, {"figure_path": "XVfevb9XFx/figures/figures_8_1.jpg", "caption": "Figure 3: The fine-tuning performance achieved by transferring to three online algorithms from their heterogeneous offline algorithms.", "description": "This figure demonstrates the transferability of the proposed O2O method.  Three online RL algorithms (SAC, TD3, and PPO) are fine-tuned using offline policies trained with different offline RL methods (TD3+BC, ODT). The results illustrate that the proposed method consistently improves online performance, showing that the method is not dependent on the specific offline algorithm used.", "section": "5 Experiments"}, {"figure_path": "XVfevb9XFx/figures/figures_14_1.jpg", "caption": "Figure 4: Performance of our O2PPO and direct PPO from IQL on D4RL [9] MuJoCo locomotion tasks during online fine-tuning. The solid lines and shaded regions represent mean and standard deviation.", "description": "This figure compares the online fine-tuning performance of the proposed O2PPO method with a direct application of PPO initialized from an IQL offline policy.  The results are shown across multiple MuJoCo locomotion tasks from the D4RL benchmark.  The plots show the normalized return over evaluation epochs. The solid lines represent the average performance across multiple runs, and the shaded regions indicate the standard deviation, illustrating the stability and variability of the methods.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "XVfevb9XFx/figures/figures_15_1.jpg", "caption": "Figure 5: Ablation results of our methods, PR=Policy re-evaluation, VA=Value Alignment, CF=Constrained Fine-tuning. For O2PPO, VA means the use of the auxiliary advantage, and CF means the update of the reference policy.", "description": "This figure presents ablation studies evaluating the impact of each component of the proposed O2O (Offline-to-Online) RL method on several MuJoCo locomotion tasks.  The components are Policy Re-evaluation (PR), Value Alignment (VA), and Constrained Fine-tuning (CF).  Each subfigure shows the performance curves for a specific task and algorithm (O2SAC, O2TD3, and O2PPO), comparing the full method with versions missing one or more components. The results demonstrate the contribution of each component to the overall stable and efficient performance improvement.", "section": "Ablation Study"}, {"figure_path": "XVfevb9XFx/figures/figures_17_1.jpg", "caption": "Figure 2: Performance curves on D4RL [9] MuJoCo locomotion tasks during online fine-tuning.", "description": "This figure shows the performance curves of different offline-to-online reinforcement learning methods on several MuJoCo locomotion tasks from the D4RL benchmark.  The x-axis represents the number of evaluation epochs during online fine-tuning, and the y-axis represents the normalized return achieved by each method.  The figure visually compares the performance of the proposed O2O RL methods (O2SAC, O2TD3, O2PPO) against several state-of-the-art baselines (AWAC, IQL, PEX, Off2On, Cal-QL, ACA). The shaded area around each line represents the standard deviation across multiple runs.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "XVfevb9XFx/figures/figures_17_2.jpg", "caption": "Figure 2: Performance curves on D4RL [9] MuJoCo locomotion tasks during online fine-tuning.", "description": "This figure displays the performance curves of various offline-to-online reinforcement learning (RL) methods on several MuJoCo locomotion tasks from the D4RL benchmark.  The x-axis represents the number of evaluation epochs during online fine-tuning, and the y-axis shows the normalized return achieved by each method.  Multiple methods are compared, including the proposed approach (O2SAC, O2TD3, and O2PPO) and several state-of-the-art baselines (AWAC, IQL, PEX, Off2On, Cal-QL, and ACA). The figure visualizes the stability and efficiency of each method\u2019s performance improvement during online fine-tuning on different tasks (with medium, replay, and expert datasets).", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "XVfevb9XFx/figures/figures_18_1.jpg", "caption": "Figure 2: Performance curves on D4RL [9] MuJoCo locomotion tasks during online fine-tuning.", "description": "This figure displays the performance of various offline-to-online reinforcement learning (RL) methods across multiple MuJoCo locomotion tasks from the D4RL benchmark.  Each sub-plot represents a specific task (e.g., HalfCheetah, Hopper, Walker2d) and dataset variation (medium, replay, expert). The x-axis shows the number of evaluation epochs during online fine-tuning, and the y-axis represents the normalized return. The curves compare the performance of the proposed method (O2SAC, O2TD3, O2PPO) with existing state-of-the-art methods.  The figure illustrates the stability and efficiency of the proposed method in achieving performance improvement over offline-only baselines.", "section": "5 Experiments"}, {"figure_path": "XVfevb9XFx/figures/figures_19_1.jpg", "caption": "Figure 8: Comparisons on different ways of updating the reference policy for O2SAC and O2TD3.", "description": "This figure compares the performance of O2SAC and O2TD3 algorithms when using two different methods for updating the reference policy during online fine-tuning.  The first method uses the optimal historical policy, while the second uses a fixed update interval. The results are shown for various MuJoCo locomotion tasks. The shaded areas represent the standard deviation of multiple runs. This experiment highlights the impact of different reference policy update strategies on the stability and efficiency of online fine-tuning in offline-to-online reinforcement learning.", "section": "C.3 Comparisons with the results of updating the reference policy update at a fixed interval"}, {"figure_path": "XVfevb9XFx/figures/figures_20_1.jpg", "caption": "Figure 2: Performance curves on D4RL [9] MuJoCo locomotion tasks during online fine-tuning.", "description": "The figure presents the performance curves of various offline-to-online reinforcement learning (RL) methods on several MuJoCo locomotion tasks from the D4RL benchmark.  It shows how the return (reward accumulated over time) of different algorithms changes during the online fine-tuning phase.  The x-axis represents the number of evaluation epochs, and the y-axis represents the normalized return.  Multiple lines represent different algorithms.  The shaded area around each line shows the standard deviation. The purpose is to illustrate the comparative performance of different offline-to-online RL methods, highlighting their stability and efficiency in improving the online policy from a pre-trained offline policy.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "XVfevb9XFx/figures/figures_20_2.jpg", "caption": "Figure 11: Comparison with PROTO and PROTO+TD3 [25] on D4RL [9] MuJoCo locomotion tasks during online fine-tuning. The solid lines and shaded regions represent mean and standard deviation.", "description": "This figure compares the online fine-tuning performance of the proposed O2SAC and O2TD3 methods with the PROTO and PROTO+TD3 methods on three MuJoCo locomotion tasks from the D4RL benchmark. The solid lines represent the average normalized return over five random seeds, and the shaded areas represent the standard deviation.  The results show that O2SAC and O2TD3 achieve better or comparable performance than PROTO and PROTO+TD3, demonstrating their effectiveness in online fine-tuning of offline-trained policies.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "XVfevb9XFx/figures/figures_21_1.jpg", "caption": "Figure 2: Performance curves on D4RL [9] MuJoCo locomotion tasks during online fine-tuning.", "description": "This figure displays the performance curves of various offline-to-online reinforcement learning (RL) methods on several MuJoCo locomotion tasks from the D4RL benchmark.  The x-axis represents the number of evaluation epochs during online fine-tuning, and the y-axis represents the normalized return achieved by each method.  Different colors and shaded regions represent different methods and their standard deviations. The figure visually demonstrates the comparative performance of various methods, highlighting differences in stability and convergence during the online fine-tuning process.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "XVfevb9XFx/figures/figures_25_1.jpg", "caption": "Figure 13: Policy performance during value alignment with different \u03b1", "description": "This figure shows the performance of the policy during the value alignment process with different values of alpha (\u03b1). Alpha is a hyperparameter in the O2SAC algorithm that controls the strength of the value alignment. The figure shows that the performance of the policy improves as alpha increases, but the improvement is marginal after a certain point. This suggests that there is an optimal value of alpha for the value alignment process, and that using a value of alpha that is too high or too low can negatively impact the performance of the policy.", "section": "G Algorithm Details"}, {"figure_path": "XVfevb9XFx/figures/figures_30_1.jpg", "caption": "Figure 14: Normalized return of evaluation and exploration during IQL offline training, where evaluation means policy output the mean of action distribution and exploration means actions are sampled from the distribution.", "description": "This figure shows the performance of evaluation and exploration scores during offline training using IQL.  The evaluation score represents the average return when the policy selects actions based on the mean of the action distribution.  The exploration score, conversely, uses actions sampled from the action distribution, reflecting the policy's ability to explore various actions beyond the mean.  The plot demonstrates the relationship between exploration and the overall normalized return during offline training, highlighting how much exploration is needed to reach a high return.", "section": "H.5 O2PPO implementation"}]