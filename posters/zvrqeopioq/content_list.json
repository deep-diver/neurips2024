[{"type": "text", "text": "Rethinking No-reference Image Exposure Assessment from Holism to Pixel: Models, Datasets and Benchmarks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shuai $\\mathbf{H}\\mathbf{e}^{\\dagger,1}$ Shuntian Zheng\u2020,2 Anlong Ming\u2020,1,\u2217 Banyu Wu1 Huadong Ma1 1 Beijing University of Posts and Telecommunications 2 University of Warwick {hs19951021}@bupt.edu.cn Shuntian.Zheng@warwick.ac.uk {mal, wubanyu, mhd}@bupt.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The past decade has witnessed an increasing demand for enhancing image quality through exposure, and as a crucial prerequisite in this endeavor, Image Exposure Assessment (IEA) is now being accorded serious attention. However, IEA encounters two persistent challenges that remain unresolved over the long term: the accuracy and generalizability of No-reference IEA are inadequate for practical applications; the scope of IEA is confined to qualitative and quantitative analysis of the entire image or subimage, such as providing only a score to evaluate the exposure level, thereby lacking intuitive and precise fine-grained evaluation for complex exposure conditions. The objective of this paper is to address the persistent bottleneck challenges from three perspectives: model, dataset, and benchmark. 1) Model-level: we propose a Pixel-level IEA Network (P-IEANet) that utilizes Haar discrete wavelet transform (DWT) to analyze, decompose, and assess exposure from both lightness and structural perspectives, capable of generating pixel-level assessment results under no-reference scenarios. 2) Dataset-level: we elaborately build an exposure-oriented dataset, IEA40K, containing 40K images, covering 17 typical lighting scenarios, 27 devices, and ${50+}$ scenes, with each image densely annotated by more than 10 experts with pixel-level labels. 3) Benchmark-level: we develop a comprehensive benchmark of 19 methods based on IEA40K. Our P-IEANet not only achieves state-of-the-art (SOTA) performance on all metrics but also seamlessly integrates with existing exposure correction and lighting enhancement methods. To our knowledge, this is the first work that explicitly emphasizes assessing complex image exposure problems at a pixel level, providing a significant boost to the IEA and exposure-related community. The code and dataset are available in here. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Exposure, one of the 3A factors (Auto Exposure, Focus and White Balance) in camera technology, plays a crucial role in controlling image quality. Image exposure assessment (IEA) is a prerequisite for improving exposure [1\u20133]; however, even leading phone and camera manufacturers heavily rely on manual evaluations due to the unavailability and high cost of human raters. Nevertheless, largescale adoption of manual assessments is impractical. Similar to mainstream AI applications, deep learning and data-driven approaches hold promise as potential solutions to overcome this limitation. Nevertheless, the traditional data-driven IEA paradigm encounters two major challenges: ", "page_idx": 0}, {"type": "text", "text": "1) A Dilemma between Applicability and Practicability: While full-reference IEA methods deliver satisfactory results [4\u20139], their applicability in non-preset scenarios is limited due to the general unavailability of reference images. Conversely, no-reference IEA methods, which do not rely on reference information, struggle with natural images distorted by unknown factors. This difficulty arises from the inability to identify specific features for assessing exposure, as the quality prediction problem becomes agnostic to the type of exposure distortion, thereby restricting both performance and practicality [10, 11]. ", "page_idx": 0}, {"type": "image", "img_path": "zVrQeoPIoQ/tmp/a49260b9fe253d423fc11279835ee6da201c2be985e137b745d5d06cecc93c4e.jpg", "img_caption": ["Figure 1: Comparison between the brightness histogram (a) and our method (b) for assessing image exposure. (b) offers a more intuitive and accurate reflection of the exposure conditions in each area. (c) shows how to determine exposure levels for each area using Adams\u2019 zone system theory [14, 15]. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "2) Restricted Generalization Capacity: Traditional IEA annotation relies on scenario-specific criteria, leading to significant subjectivity across datasets. These datasets typically provide only a holistic quantitative score reflecting the overall exposure condition [12], lacking detailed and fine-grained assessments. Consequently, the subjective and coarse-grained labels introduce restricted generalization capacity into learning-based IEA methods, reducing their adaptability to diverse scenarios and assessment criteria. ", "page_idx": 1}, {"type": "text", "text": "How can an ideal method be designed to tackle the aforementioned challenges? The method should primarily address three key issues: firstly, as a no-reference method, it should effectively simulate reference images in non-preset scenarios, functioning like full-reference methods; secondly, it should achieve fine-grained assessments by adapting to diverse high-level evaluation criteria or application scenarios directly or through fine-tuning; finally, the learned features should be decoupled from subjective criteria and aligned with naive exposure features to mitigate narrow inductive biases. ", "page_idx": 1}, {"type": "text", "text": "This paper presents P-IEANet, an innovative method that leverages large-scale, pixel-level annotated datasets to delve into the fundamental unit of IEA: pixels. This approach allows us to identify exposure issues with unprecedented precision and to handle IEA tasks of varying granularity beyond the pixel level, without being influenced by subjective criteria. Grounded in the wellestablished theory that the power spectrum of natural images is a function of frequency, represented as $1/f^{\\gamma}$ where $\\gamma$ varies slightly at specific frequencies [11, 13], we leverage this insight to analyze exposure characteristics in specific frequency domains for improved adaptability across varying criteria and scenarios. Through employing the dedicated Haar Discrete Wavelet Transform (DWT), P-IEANet decomposes the original image to criteria-agnostic frequency features, thus avoiding narrow inductive biases. Additionally, with pixel-level supervision, P-IEANet enables ideal exposure reconstruction from frequency space, effectively creating reference images for further analysis. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To our knowledge, this is the first work to implement a pixel-level evaluation paradigm in IEA. It enhances the generalizability and accuracy of no-reference IEA tasks, while effectively addressing challenges associated with reusing underlying data and architectures. \u2022 We present the P-IEANet, showcasing that pixel-level IEA can be decomposed into criteriaagnostic lightness and structure information via the dedicated Haar DWT. This design enables efficient execution of pixel-level IEA while minimizing parameter usage. \u2022 To convincingly validate our method, we have developed a dataset exclusively tailored for IEA, called IEA40K. This dataset specifically focuses on exposure and comprises 40,000 of images with the most comprehensive annotations to date, including pixel-level annotations. \u2022 Building upon IEA40K, we have evaluated 19 baselines, establishing our benchmark as the most comprehensive to date for IEA. Our work not only achieves SOTA performance but also serves as a pivotal catalyst, offering the community a new roadmap to explore further solutions for IEA. ", "page_idx": 1}, {"type": "image", "img_path": "zVrQeoPIoQ/tmp/fdf0127befd842096dd078f1782f691996e5ce34e0fdcdac6cbd71767ed2139b.jpg", "img_caption": ["Figure 2: Images visualized along with their corresponding pixel-level (heat map of exposure residual) and holistic IEA results by P-IEANet (where a higher score shows more visually pleasing exposure). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "No-reference IEA Methods and Datasets. Previous studies on no-reference IEA can be broadly classified into two primary categories: 1) Statistical-feature based methods. Datta et al. [4] utilized average pixel intensity to evaluate light usage. Liu et al.[5] explored image brightness histograms, and Hanmandlu et al.[7] developed indicators based on the image brightness histogram for crucial auto-exposure control. Rahman et al. [8] and Lu et al. [9] adopted information entropy as a criterion for exposure evaluation. In some auto-exposure work, [16\u201319] incorporated gradient information to determine the ideal exposure settings for cameras, suggesting that maximum information entropy indicates ideal image exposure. Efimov et al. [20] and Dong et al. [21] proposed subdividing images into blocks for individual assessment, classifying each based on its brightness histogram into categories. 2) Data-driven methods. The increasingly popular methods [12, 22\u201324] utilize human-labeled datasets to develop scenario-specific features. ", "page_idx": 2}, {"type": "text", "text": "However, when it comes to statistical-feature based methods, manual features often assume only one type of distortion, which is problematic in complex situations where overexposure and underexposure coexist [11, 25]. On the other hand, data-driven methods become less effective when assessment criteria or application scenarios change; moreover, the holistic scores from these datasets lack the detailed supervisory information required for the precision and granularity demanded in applications. ", "page_idx": 2}, {"type": "text", "text": "Pixel-level Tasks. In conventional terms, the concept of \u201cexposure\u201d refers to not only exposure time but also two other parameters (aperture and ISO, referred to [26]). Rather than being characterized as a global attribute of the image, the parameters would be more appropriate to be described as a global attribute associated with the camera for capturing the image. However, according to the claim made by the classical photographic theory (Adams\u2019 theory) [27] that \u201cThe exposure time is the same for all elements, but the image exposure varies with the luminance of each subject element,\u201d the coarse global camera exposure attribute fails to match each subject element in an image, potentially resulting in some subject elements being under-exposed and others being over-exposed. Given this, in the context of evaluating images, the term \u201cexposure\u201d is no longer a global attribute, as referred to [27] that \u201cAny scene of photographic interest contains elements of different luminance; consequently, the \u2018exposure\u2019 actually is many different exposures.\u201d Therefore, the pixel-level IEA is highly desired. ", "page_idx": 2}, {"type": "text", "text": "However, to our knowledge, there are currently no pixel-level IEA methods, despite advancements in related visual tasks such as semantic understanding and fine-grained analysis. For instance, DiffuMask [28] exploits powerful zero-shot text-to-image generative models to provide pixel-level ", "page_idx": 2}, {"type": "image", "img_path": "zVrQeoPIoQ/tmp/bd7a939c06030545f455d4c6703534ed93e37aa1275f12ba83682327f9cecb2c.jpg", "img_caption": ["Figure 3: After Haar DWT decomposes an image, swapping its low-frequency component $\\langle{\\pmb{3}}\\rangle$ with the high-frequency components $\\mathbf{\\Gamma}(\\mathfrak{D}\\emptyset\\emptyset)$ of the same image under different exposures produces visually similar results (a-d) as well as similar t-SNE features (e). "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "zVrQeoPIoQ/tmp/7da79a22ca48e26b8e64a81fdff0ef48cab4165a3f12012f4160c2f3ee04c7dc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 4: Pipeline of P-IEANet. The original image is decomposed into low/high-frequency components via Haar DWT. Subsequently, the Structure Feature Module analyzes the high-frequency components with gradient maps to extract structural features (cf. Sec. 3.1). Simultaneously, the Lightness Feature Module handles the low-frequency component and extracts lightness features by attention mechanisms (cf. Sec. 3.2). These features are then composed through Haar inverse DWT for pixel-level IEA. Dedicated convolution kernels (d) are employed to facilitate the DWT process. ", "page_idx": 3}, {"type": "text", "text": "segmentation annotations across diverse classes. Similarly, PixelLM [29] leverages GPT4V to produce 246,000 pixel-level question-answer pairs, enhancing its capabilities in pixel-level reasoning and comprehension. ", "page_idx": 3}, {"type": "text", "text": "However, directly applying these methods to IEA tasks is very challenging due to specific requirements in data collection and annotation processes, which entail avoiding selection bias, accurately aligning images, obtaining ideal references, and providing detailed annotations (cf. Sec. 4). ", "page_idx": 3}, {"type": "text", "text": "3 Architecture of P-IEANet ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Preliminaries. Images captured with incorrect exposure settings often suffer from visual problems, including lightness and structure distortions [30\u201332]. For instance, overexposed images exhibit unnatural artifacts, inconsistencies in exposure blending, and blurred structural details. This paper demonstrates the potential of the Haar DWT for analyzing IEA issues. The mathematical representation of the Haar DWT can be formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nD W T(L,H)=\\frac{1}{\\sqrt{2^{m}}}\\sum_{k}f(k)(\\phi\\left(\\frac{n-k2^{m}}{2^{m}}\\right),\\psi\\left(\\frac{n-k2^{m}}{2^{m}}\\right)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The mother wavelet $\\phi$ and $\\psi$ of the Haar DWT can decompose an image into low frequency components $L$ (approximation coefficients), and high frequency components $H$ (coefficients in horizontal, vertical, and diagonal directions), as shown in Fig. 3. We label the underexposed, overexposed, and ideal images as $X_{o v e r}$ , $X_{u n d e r}$ , and $X_{i d e a l}$ , respectively. Their corresponding Haar DWT representations in frequency are denoted as $D W T(L(X_{o v e r}),H(X_{o v e r}))$ , $D W T(L(X_{u n d e r}),H(X_{u n d e r}))$ and $D W T(\\bar{L(}X_{i d e a l}),H(X_{i d e a l}))$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "For IEA tasks, we examine whether the low-frequency and high-frequency components correspond to the frequency-domain representations of lightness and structure, respectively. The images obtained by reversing these components, such as $D W T^{-1}(L(X_{i d e a l}),\\dot{H}(X_{u n d e r}))$ and ", "page_idx": 3}, {"type": "text", "text": "$D W T^{-1}(L(X_{i d e a l}),H(X_{o v e r}))$ , show an exposure close to $X_{i d e a l}$ (Fig. 3(c)(d)). Conversely, $D W T^{-1}(L(X_{o v e r}),H(X_{u n d e r}))$ exhibits a exposure similar to that of $X_{o v e r}$ , as shown in Fig. 3(b). To further validate these findings, we conducted a similar analysis using t-SNE dimensionality reduction [33] on 200 sample images in Fig. 3(e). The t-SNE results reveal that the high frequency components remain relatively consistent across different exposures, while the low frequency components exhibit significant variation. ", "page_idx": 4}, {"type": "text", "text": "Based on the above observations, we deduce that the low-frequency components primarily represent an image\u2019s lightness, while the high-frequency components indicate structural details and are less affected by lightness variations. By exploiting this characteristic, we can decompose the exposure of a distorted input image into two frequency representations and then construct an ideal reference image in the frequency space. Subsequently, differences in the frequency domain are mapped to pixel-level IEA results. This approach effectively mitigates any influence from irrelevant image semantics or noise associated with IEA. ", "page_idx": 4}, {"type": "text", "text": "Pipeline. P-IEANet comprises three essential modules (Fig. 4). The Structure Feature Module (cf. Sec. 3.1) and Lightness Feature Module (cf. Sec. 3.2) are responsible for extracting structural features from high-frequency components and lightness features from low-frequency components, respectively. Ultimately, the Prediction Module (cf. Sec. 3.3) integrates these features to predict pixel-level IEA results and generate the final prediction. ", "page_idx": 4}, {"type": "text", "text": "3.1 Structure Feature Module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "There are two categories of structural features relevant to the IEA tasks: 1) Long-range features encompassing the overall layout and distant objects of an image, which provide a comprehensive understanding of its structure and global exposure [34\u201338]. 2) Short-range features focusing on fine details and textures, such as edges and localized patterns, are crucial for capturing local exposure variations [38\u201341]. To obtain these features effectively, we first derive gradient maps from the high-frequency components of the Haar DWT. These maps highlight edge regions, thus enhancing the representation of basic structural details [30, 38, 42]. Subsequently, we refine the extraction process by subjecting these gradient maps to a Long-Range Encoder (LRE) and a Short-Range Encoder (SRE). Further details are provided below. ", "page_idx": 4}, {"type": "text", "text": "Gradient Maps. The input image $X$ is decomposed by the Haar DWT to obtain high-frequency components, and then processed by a multi-layer encoder to extract naive features. For each layer $z_{i}$ , where $i$ ranges from 1 to $N$ (the total number of layers), we compute the gradient map as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla z_{i}=\\{g_{d}(z_{i})|d\\in D\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $g_{d}(z_{i})$ applies the first-order gradient function $g$ to $z_{i}$ in direction $d$ . The set $D$ includes all directions under consideration: $+x,-x,+y,-y,+x+y,+x-y,-x+y,-x-y$ , which correspond to the $\\mathbf{X}$ -axis, y-axis, and their diagonals. These directions ensure comprehensive emphasis on edges, thereby enhancing the formulation of structural features. ", "page_idx": 4}, {"type": "text", "text": "Long-range and Short-range Encoders. To enhance the extraction of structural features, we feed both the original input feature, $z_{i}$ , and its gradient maps $\\nabla z_{i}$ into two distinct modules: a Transformer-based LRE ${\\bar{Z}}^{l}$ and a CNN-based SRE $Z^{s}$ . The feature extraction process is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nl_{i}=Z_{i}^{l}(z_{i}),\\quad s_{i}=Z_{i}^{s}(z_{i}),\\quad\\nabla l_{i}=\\nabla Z_{i}^{l}(\\nabla z_{i}),\\quad\\nabla s_{i}=\\nabla Z_{i}^{s}(\\nabla z_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $l_{i}$ and $s_{i}$ represent the long-range and short-range features, respectively, these features are then integrated using a Structure Fusion Module $Z^{f}$ , which employs multiple MLPs, as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nZ_{o}=\\left\\|Z_{i}^{f}(l_{i},s_{i}),Z_{i}^{f}(\\nabla l_{i},\\nabla s_{i})\\right\\|_{i}^{N}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this formulation, $\\lVert...\\rVert$ signifies the stacking of operations along the feature channel dimensions, facilitating a comprehensive synthesis of the extracted features. ", "page_idx": 4}, {"type": "text", "text": "3.2 Lightness Feature Module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The human visual system, possessing a high dynamic range, is skilled at globally detecting varying light levels of objects. However, due to limited attention capacity, it also tends to focus on specific regions with distinct lightness levels. Our method selectively processes global channels and local pixel regions to enhance the network\u2019s management of a broad spectrum of information. ", "page_idx": 4}, {"type": "image", "img_path": "zVrQeoPIoQ/tmp/73c98907e68768a214a40c7adbbcaacecc5079bd59d0f6ac24c3375e2a2b4328.jpg", "img_caption": ["Figure 5: Visualization of different components in P-IEANet. Long-range (a) and Short-range (b) features highlight the important structural information for IEA tasks, while pixel-level attention (c) and channel-based attention (d-e) characterize its distribution in terms of light information. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Lightness Channel Attention. Firstly, our lightness channel attention (LCA) processes the channelwise global spatial information, $L_{c}$ $\\mathrm{~\\boldmath~\\Omega~}_{\\!\\mathrm{{\\Omega}}}\\left(C\\times H\\times W\\right)$ , through global average pooling to create a channel descriptor, $A_{c}$ $(C\\times1\\times1)$ . To determine the weights for different channels, the descriptor undergoes further refinement in two convolution layers, followed by sigmoid $\\sigma$ and ReLU $\\gamma$ activation functions. This procedure is formalized as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{c}=\\sigma(\\mathbf{MLP}(\\gamma(\\mathbf{MLP}(\\frac{1}{H\\times W}\\sum_{i=1}^{H}\\sum_{i=1}^{W}L_{c}(i,j))))),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L_{c}(i,j)$ represents the lightness value at position $(i,j)$ in the $c$ -th channel $L_{c}$ , this channel attention strategy highlights that lightness variations across different channels convey distinct and weighted information. Finally, the channel weights are element-wise multiplied with the input to generate the output $F_{c}=A_{c}\\otimes L_{c}$ $(C\\times W\\times H)$ . ", "page_idx": 5}, {"type": "text", "text": "Lightness Pixel Attention. The variable distribution of lightness among image pixels necessitates our lightness pixel attention (LPA) mechanism. This mechanism processes the output $F_{c}$ from the LCA using self-attention (SA) and convolution layers, coupled with ReLu and sigmoid activation functions. The attention mechanism is formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{p}=\\sigma\\left(\\sum_{k\\in K}w_{k}\\cdot\\mathrm{Conv}_{k}(\\gamma(\\mathrm{SA}(F_{c})))\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\mathrm{Conv}_{k}$ denotes a convolution operation with multi-scale kernel sizes $k$ , which aims to enhance the network\u2019s focus on fine-grained and multi-scale exposure features under complex scenario. Larger kernel sizes help perceive overall brightness and contrast, while smaller kernel sizes detect localized overexposure or underexposure issues. Additionally, self-attention allows the system to analyze lightness distribution and recognize patterns at multiple scales. ", "page_idx": 5}, {"type": "text", "text": "We then perform element-wise multiplication to merge the input $L_{c}$ with $A_{p}$ $(1\\times H\\times W)$ , generating the output $F_{p}=L_{c}\\otimes A_{p}$ $\\langle C\\times H\\times W\\rangle$ . The final stage integrates the outputs from both channel and pixel attention mechanisms to yield the comprehensive output $A_{o}={\\bf M}{\\bf L}{\\bf P}({F_{c},F_{p}})$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Prediction Module ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We employed Haar $\\mathrm{DWT}^{-1}$ to integrate lightness and structure features for ideal exposure representation reconstruction in the frequency space, then using the formula $P_{p}=\\mathrm{MLP}(D\\dot{W}T^{-1}(A_{o}^{\\star},Z_{o}))$ to predict the exposure residual, which measures the deviation of each pixel from the ideal exposure. Both Haar DWT and $\\mathrm{DWT}^{-1}$ involve four dedicated convolutional kernels to simulate the wavelet transform\u2019s decomposition and reconstruction processes (Fig. 4(d)). To evaluate prediction accuracy, we define a loss function as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p i x e l}=\\frac{1}{H\\times W}\\sum_{i=1}^{H}\\sum_{j=1}^{W}\\left|P_{p}(i,j)-\\widetilde{P_{p}}(i,j)\\right|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\widetilde{P_{p}}$ is the pixel-level ground truth, these residual maps can be converted into a coarser-grained prediction above pixel, e.g., holistic IEA score (cf. Sec. 5.2). ", "page_idx": 5}, {"type": "image", "img_path": "zVrQeoPIoQ/tmp/5a296bd78f13248e84051ccaf9c17fd1b2e756431f66f9ec28f4ed6c61f93ced.jpg", "img_caption": ["Figure 6: Proposed IEA40K dataset. (a) Visualization of images with different exposure conditions; (b) Scenes containing 8 super-classes with ${50+}$ sub-classes; (c) 17 typical lighting scenarios. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Proposed IEA40K Dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Image Collection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "How to Avoid Selection Bias when Building a Comprehensive IEA Dataset. Selection bias can arise when certain types of intended exposure conditions, scenes and devices are underrepresented in the dataset, which may compromise the validity and generalizability of training models. To mitigate this issue, we considered 5 key aspects: varied scenes, diverse light conditions, sufficient devices, uniform resolution and comprehensive simulations (Fig. 6). These factors can significantly impact the quality of the dataset. Further details are provided in Appendix A.1. ", "page_idx": 6}, {"type": "text", "text": "How to Align a set of Images through Pre-processing Strategies. Effectively aligning a set of images captured under various exposure conditions is a significant challenge. Factors like camera shake and slight subject movement during shooting parameter adjustments can lead to misalignment [43]. Such misalignment adversely affects the generation of supervised information and the training process. To tackle this issue, we first apply the Structural Similarity Index Measure (SSIM) algorithm to fliter out misalignment images from the series. Those falling below a specified SSIM threshold are eliminated. Subsequently, we employ image alignment algorithms [43] to further enhance the alignment of the remaining images. This entire process is automated and can be executed in an unsupervised manner. ", "page_idx": 6}, {"type": "text", "text": "4.2 Data Annotation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "How to Obtain an Ideal Reference. Obtaining a reference image with ideal exposure in each region is crucial for our subsequent image annotation. However, ensuring the representatives of the reference image while minimizing the randomness and subjectivity introduced by humans presents significant challenges. To address this, we start by creating a preliminary reference image using a multi-exposure fusion algorithm. Subsequently, we segment this image into blocks with the super-pixel segmentation algorithm [44] based on lightness and structure. Finally, experts optimize each block\u2019s exposure conditions utilizing Adams\u2019 zone system theory of classical photography [14, 15] (Fig. 1(c)). This theory provides precise guidelines for achieving ideal exposure across different elements. ", "page_idx": 6}, {"type": "text", "text": "How to Obtain Pixel-Level Labels by Human-in-the-Loop Methods. Given the exorbitant cost and intricate nature of pixel-level annotation, we have streamlined the process using a combination of expert judgment and weak supervision techniques (Fig. 7). The initial pixel-level annotations were generated by comparing a reference image with the 8 distorted images, documenting exposure residual across pixels. Subsequently, experts further refined the final pixel-level annotations to rectify potential errors, such as accurately identifying areas with logos as severely overexposed and addressing discrete anomalous pixel labeling, to ensure that the final exposure residual closely aligns with the perceived deviation of each pixel from ideal exposure. For experts, distinguishing between the reference and distorted images is relatively straightforward and far more accurate, thus facilitating practical data annotation. ", "page_idx": 6}, {"type": "image", "img_path": "zVrQeoPIoQ/tmp/24f875b3102348aedb37601b7e212f7c6750447988f6bdf3f6d4ceb6bcc2648b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 7: Overview of the proposed IEA dataset annotation process. First, collect images of different exposure conditions and then adjust the raw image by experts to obtain reference image; Second, calculate exposure residual between the reference and 8 distorted images, and then have experts adjust the local exposure residual. The closer it is to -1, the more overexposed the corresponding pixel is; and the closer it is to 1, the more underexposed the corresponding pixel is. ", "page_idx": 7}, {"type": "image", "img_path": "zVrQeoPIoQ/tmp/3095c19df0ca43139e3144729a7ea787a401b4c1183adadcefdc8027ce8d3a69.jpg", "img_caption": ["Figure 8: Example exposure residuals, a robust quantitative indicator for precise assessment, uniquely generated by P-IEANet (a lower absolute exposure residual suggests a visually more pleasing result). (a) Inputs: an original image and its enhanced counterparts by classical light enhancement methods. (b) Outputs: the exposure residuals hilighting the disparity between the input and the ideal exposure. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Benchmark Models and Training Protocols. To the best of our knowledge, there is no publicly available pixel-level IEA model. Therefore, we have selected several deep learning baselines based on the following criteria: classical architectures with publicly available code and SOTA performance in a specific domain. For pixel-level IEA, we selected light enhancement, light-aware, and image quality assessment (IQA) models as backbones, complemented by appropriate output heads. For holistic IEA, we chose IQA and image aesthetics assessment (IAA) models to regress scores. Further details regarding training protocols can be found in Appendix A.3. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics. For pixel-level IEA, we adopt SSIM and MAE to measure the structure and lightness similarity between the ground truth and predicted exposure residual. For holistic IEA, we adopt the Spearman\u2019s rank correlation coefficient (SRCC) and the linear correlation coefficient (LCC), to measure the correlation between the predicted IEA score and human opinion [45]. ", "page_idx": 7}, {"type": "text", "text": "5.2 Performance Evaluations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Pixel-level Assessment. Table 1 presents the results of P-IEANet and 12 other models on the IEA40k dataset. Our P-IEANet achieves SOTA performance, surpassing the second-best model with a remarkable $-40\\%$ reduction in MAE loss and a significant $+25\\%$ improvement in SSIM, while using an impressive $-97\\%$ fewer training parameters. The efficiency of P-IEANet can be largely attributed to Haar DWT, which effectively minimizes the number of required feature extraction layers. Additionally, both the Lightness Feature Module and Structure Feature Module effectively utilize information, contributing to its exceptional performance with fewer parameters. ", "page_idx": 7}, {"type": "text", "text": "Holistic Level Assessment. We validated the effectiveness of P-IEANet on the holistic IEA task using the representative SPAQ dataset [12]. This dataset provides annotations for holistic exposure, allowing us to test the capabilities of various baseline models. P-IEANet supports three prediction methods: 1) With Fine-tuning: after obtaining the residual map, it is processed through additional MLPs to predict the holistic exposure score, supervised by the MAE loss. A comparison of P-IEANet with 18 other models is presented in Table 2, where P-IEANet achieves SOTA performance. 2) Without Fine-tuning: after obtaining the residual map, we compute the average of absolute values and subtract this average from 1, mapping it to a scale of 0-10 (Fig. 2). Remarkably, without requiring fine-tuning for holistic exposure scoring on SPAQ, we achieved a LRCC of 0.69 and SRCC of 0.65, even surpassing some methods that do require fine-tuning. The above results show that P-IEANet exhibits strong criteria and scenario robustness beyond pixel-level tasks. 3) Criteria-oriented without Fine-tuning: Moreover, we additionally discuss an industry-applicable criteria-oriented scoring methodology in Appendix A.2. ", "page_idx": 7}, {"type": "table", "img_path": "zVrQeoPIoQ/tmp/408c50f7a756d1943c721246acef88117553cbd270c0590d4457455573589577.jpg", "table_caption": ["Table 1: Comparing the pixel-level IEA performance of 13 models on IEA40K. We adjusted the output headers of all light enhancement/awareness and IQA/IAA models to ensure their support for fine-tuning on our IEA40k. All models utilized only the exposure residual as pixel-level ground truth and were retrained for the best performance. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "zVrQeoPIoQ/tmp/adec9400a7c016f096ad8f12080df68213c5fb80ca953601f2565d2c28b8704b.jpg", "table_caption": ["Table 2: Comparing the holistic level IEA performance of 19 models on SPAQ. All models utilized only the exposure score as holistic-level ground truth, and were retrained for the best performance. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Ablation Studies. Table 3 evaluates the effectiveness of P-IEANet\u2019s modules. The absence of the Haar DWT and two other modules, Structure and Lightness, significantly impact the performance of P-IEANet. Specifically, the SSIM decreases by $42.6\\%$ , $28.0\\%$ , and $44.0\\%$ respectively, while the SRCC falls by $14.3\\%$ , $7.1\\%$ , and $15.8\\%$ . These results confirm that each module, particularly the Lightness Feature Module which processes low-frequency information, plays a crucial role in enhancing the model\u2019s overall performance. Qualitative visual effects analysis is provided in Fig. 5. ", "page_idx": 8}, {"type": "text", "text": "Predictions for Images. The prediction examples are shown in Fig. 2. Similar to human perception, P-IEANet\u2019s pixel-level evaluation results effectively identify areas of overexposure and underexposure that are visually displeasing, even in unconventional scenes where both underexposure and overexposure coexist. Moreover, when combined with semantic segmentation algorithms, P-IEANet enables more precise object- and pixel-level IEA results (Appendix A.2). ", "page_idx": 8}, {"type": "text", "text": "5.3 Advancing Light Enhancement Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "P-IEANet, owing to its exceptional sensitivity towards exposure, offers advantages for the exposure enhancement community in the following two aspects: ", "page_idx": 8}, {"type": "text", "text": "1) Analyzing Performance Better: Traditionally, assessing the efficacy of image enhancement algorithms has been a time-consuming and imprecise task, relying solely on human observations. The exposure residual, uniquely generated by P-IEANet, serves as a robust quantitative indicator for precise assessment (refer to Fig. 8, where input can be either an original image or an enhanced one). ", "page_idx": 8}, {"type": "text", "text": "2) Enhancing Performance Better: P-IEANet is compatible with many existing light enhancement methods, enabling it to boost their performance. To demonstrate this, we chose two open-source and SOTA methods, Retinex [47] and GASD [50], as baseline models. We incorporated P-IEANet as a sample evaluator in these models after enhancement, freezing P-IEANet\u2019s parameters and obtaining the absolute exposure residual as loss to include in the baseline models. Table 4 shows that on both representative datasets, LOL-v1 [61] and LOLv2-real [62], the performance is improved to some extent, suggesting that P-IEANet has the potential to become an important enhancer in this field. ", "page_idx": 8}, {"type": "table", "img_path": "zVrQeoPIoQ/tmp/9f5945af3c91703cb59da3a709ff9757ff1ec23657d3132696d725550c20d5d5.jpg", "table_caption": ["Table 3: Ablation studies conducted on IEA40K. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "zVrQeoPIoQ/tmp/68a479a33cda714de79b12a642446d3ce75db76e132d19963e87fbb1e56ae9c9.jpg", "table_caption": ["Table 4: Our P-IEANet can enhance some low-light enhancement methods. Our retrained results are marked by \u2018\\*\u2019. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper investigates IEA with a novel paradigm: from holism to pixel. To our knowledge, our work introduces a new roadmap by proposing a model, dataset, and benchmark for the community. However, several challenges still remain to be addressed. For instance, evaluating images with severe misalignment issues caused by high-speed moving objects poses significant challenges. In future work, we aim to optimize our framework to support multimodal outputs and enhance the exposure perception in artificial intelligence generated content (AIGC). ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the Funds for Creative Research Groups of China under Grant 61921003. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.   \n[2] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690\u20134699, 2019.   \n[3] Wenhan Yang, Ye Yuan, Wenqi Ren, Jiaying Liu, Walter J Scheirer, Zhangyang Wang, Taiheng Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, et al. Advancing image understanding in poor visibility environments: A collective benchmark study. IEEE Transactions on Image Processing, 29:5737\u20135752, 2020.   \n[4] Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. Studying aesthetics in photographic images using a computational approach. In European conference on computer vision, pages 288\u2013301. Springer, 2006.   \n[5] Min Liu, Po Yuan, and Richard S Turner Jr. Automatic analysis and adjustment of digital images with exposure problems, Apr. 15 2008. US Patent 7,359,572.   \n[6] Juan Torres and Jos\u00e9 Manuel Men\u00e9ndez. Optimal camera exposure for video surveillance systems by predictive control of shutter speed, aperture, and gain. In Real-Time Image and Video Processing 2015, volume 9400, pages 238\u2013251. SPIE, 2015.   \n[7] Madasu Hanmandlu, Om Prakash Verma, Nukala Krishna Kumar, and Muralidhar Kulkarni. A novel optimal fuzzy system for color image enhancement using bacterial foraging. IEEE Transactions on Instrumentation and Measurement, 58(8):2867\u20132879, 2009.   \n[8] Mohammad T Rahman, Nasser Kehtarnavaz, and Qolamreza R Razlighi. Using image entropy maximum for auto exposure. Journal of electronic imaging, 20(1):013007, 2011.   \n[9] Huimin Lu, Hui Zhang, Shaowu Yang, and Zhiqiang Zheng. Camera parameters auto-adjusting technique for robust robot vision. In 2010 IEEE International Conference on Robotics and Automation, pages 1518\u20131523. IEEE, 2010.   \n[10] Wufeng Xue, Xuanqin Mou, Lei Zhang, Alan C Bovik, and Xiangchu Feng. Blind image quality assessment using joint statistics of gradient magnitude and laplacian features. IEEE Transactions on Image Processing, 23(11):4850\u20134862, 2014.   \n[11] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on image processing, 21(12):4695\u20134708, 2012.   \n[12] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3677\u20133686, 2020.   \n[13] Anuj Srivastava, Ann B Lee, Eero P Simoncelli, and S-C Zhu. On advances in statistical modeling of natural images. Journal of mathematical imaging and vision, 18:17\u201333, 2003.   \n[14] Ansel Adams. Basic Photo: The Negative v. 2. New York Graphic Society, New York, NY, Mar. 1978.   \n[15] Ansel Adams. New photo series 2: Negative:. Ansel Adams Photography. Bulfinch Press, New York, NY, June 1995.   \n[16] Inwook Shim, Joon-Young Lee, and In So Kweon. Auto-adjusting camera exposure for outdoor robotics using gradient information. In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 1011\u20131017. IEEE, 2014.   \n[17] Inwook Shim, Tae-Hyun Oh, Joon-Young Lee, Jinwook Choi, Dong-Geol Choi, and In So Kweon. Gradientbased camera exposure control for outdoor mobile platforms. IEEE Transactions on Circuits and Systems for Video Technology, 29(6):1569\u20131583, 2018.   \n[18] Zichao Zhang, Christian Forster, and Davide Scaramuzza. Active exposure control for robust visual odometry in hdr environments. In 2017 IEEE international conference on robotics and automation (ICRA), pages 3894\u20133901. IEEE, 2017.   \n[19] Joowan Kim, Younggun Cho, and Ayoung Kim. Exposure control using bayesian optimization based on entropy weighted image gradient. In 2018 IEEE International conference on robotics and automation (ICRA), pages 857\u2013864. IEEE, 2018.   \n[20] S Efimov, A Nefyodov, and M Rychagov. Block-based image exposure assessment and indoor/outdoor classification. In Proc. of 17th Conf. on Computer Graphics GraphiCon, 2007.   \n[21] Xuan Dong, Lu Yuan, Weixin Li, and Alan L Yuille. Temporally consistent region-based video exposure correction. In 2015 IEEE International Conference on Multimedia and Expo (ICME), pages 1\u20136. IEEE, 2015.   \n[22] Yuzhe Yang, Liwu Xu, Leida Li, Nan Qie, Yaqian Li, Peng Zhang, and Yandong Guo. Personalized image aesthetics assessment with rich attributes. arXiv preprint arXiv:2203.16754, 2022.   \n[23] Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, and Charless Fowlkes. Photo aesthetics ranking network with attributes and content adaptation. In European conference on computer vision, pages 662\u2013679. Springer, 2016.   \n[24] Wenhan Zhu, Guangtao Zhai, Zongxi Han, Xiongkuo Min, Tao Wang, Zicheng Zhang, and Xiaokang Yangand. A multiple attributes image quality database for smartphone camera photo quality assessment. In 2020 IEEE International Conference on Image Processing (ICIP), pages 2990\u20132994. IEEE, 2020.   \n[25] Michele A Saad, Alan C Bovik, and Christophe Charrier. Blind image quality assessment: A natural scene statistics approach in the dct domain. IEEE transactions on Image Processing, 21(8):3339\u20133352, 2012.   \n[26] Lijun Zhang, Lin Zhang, Xiao Liu, Ying Shen, and Dongqing Wang. Image exposure assessment: a benchmark and a deep convolutional neural networks based model. In 2018 IEEE International Conference on Multimedia and Expo (ICME), pages 1\u20136. IEEE, 2018.   \n[27] Ansel Adams. The negative: Exposure and development. ansel adams basic photography series/book 2. New York Graphic Society, Boston, MA, USA, 1948.   \n[28] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1206\u20131217, 2023.   \n[29] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. arXiv preprint arXiv:2312.02228, 2023.   \n[30] Anmin Liu, Weisi Lin, and Manish Narwaria. Image quality assessment based on gradient similarity. IEEE Transactions on Image Processing, 21(4):1500\u20131512, 2011.   \n[31] Dario Fuoli, Luc Van Gool, and Radu Timofte. Fourier space losses for efficient perceptual image super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2360\u20132369, 2021.   \n[32] Jie Huang, Yajing Liu, Feng Zhao, Keyu Yan, Jinghao Zhang, Yukun Huang, Man Zhou, and Zhiwei Xiong. Deep fourier-based exposure correction network with spatial-frequency interaction. In European Conference on Computer Vision, pages 163\u2013180. Springer, 2022.   \n[33] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \n[34] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12299\u201312310, 2021.   \n[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[36] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 22\u201331, 2021.   \n[37] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 579\u2013588, 2021.   \n[38] Xiaogang Xu, Ruixing Wang, and Jiangbo Lu. Low-light image enhancement via structure modeling and guidance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9893\u20139903, 2023.   \n[39] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel CohenOr. Encoding in style: a stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2287\u20132296, 2021.   \n[40] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder for stylegan image manipulation. ACM Transactions on Graphics (TOG), 40(4):1\u201314, 2021.   \n[41] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. High-fidelity gan inversion for image attribute editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11379\u201311388, 2022.   \n[42] Wenqi Ren, Sifei Liu, Lin Ma, Qianqian Xu, Xiangyu Xu, Xiaochun Cao, Junping Du, and Ming-Hsuan Yang. Low-light image enhancement via a deep hybrid network. IEEE Transactions on Image Processing, 28(9):4364\u20134375, 2019.   \n[43] Jirong Zhang, Chuan Wang, Shuaicheng Liu, Lanpeng Jia, Nianjin Ye, Jue Wang, Ji Zhou, and Jian Sun. Content-aware unsupervised deep homography estimation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages 653\u2013669. Springer, 2020.   \n[44] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S\u00fcsstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE transactions on pattern analysis and machine intelligence, 34(11):2274\u20132282, 2012.   \n[45] Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE transactions on image processing, 27(8):3998\u20134011, 2018.   \n[46] Yijie Zhou, Chao Li, Jin Liang, Tianyi Xu, Xin Liu, and Jun Xu. 4k-resolution photo exposure correction at 125 fps with\u02dc 8k parameters. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1587\u20131597, 2024.   \n[47] Yuanhao Cai, Hao Bian, Jing Lin, Haoqian Wang, Radu Timofte, and Yulun Zhang. Retinexformer: One-stage retinex-based transformer for low-light image enhancement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12504\u201312513, 2023.   \n[48] Dewei Zhou, Zongxin Yang, and Yi Yang. Pyramid diffusion models for low-light image enhancement. arXiv preprint arXiv:2305.10028, 2023.   \n[49] Yuhui Wu, Chen Pan, Guoqing Wang, Yang Yang, Jiwei Wei, Chongyi Li, and Heng Tao Shen. Learning semantic-aware knowledge guidance for low-light image enhancement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1662\u20131671, 2023.   \n[50] Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, and Hui Yuan. Global structure-aware diffusion process for low-light image enhancement. Advances in Neural Information Processing Systems, 36, 2024.   \n[51] Zhexin Liang, Chongyi Li, Shangchen Zhou, Ruicheng Feng, and Chen Change Loy. Iterative prompt learning for unsupervised backlit image enhancement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8094\u20138103, 2023.   \n[52] Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, and Alberto Del Bimbo. Arniqa: Learning distortion manifold for image quality assessment. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 189\u2013198, 2024.   \n[53] Avinab Saha, Sandeep Mishra, and Alan C Bovik. Re-iqa: Unsupervised learning for image quality assessment in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5846\u20135855, 2023.   \n[54] Guanyi Qin, Runze Hu, Yutao Liu, Xiawu Zheng, Haotian Liu, Xiu Li, and Yan Zhang. Data-efficient image quality assessment with attention-panel decoder. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2091\u20132100, 2023.   \n[55] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5148\u20135157, 2021.   \n[56] Shuang Ma, Jing Liu, and Chang Wen Chen. A-lamp: Adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment. In CVPR, pages 722\u2013731, 2017.   \n[57] Vlad Hosu, Bastian Goldlucke, and Dietmar Saupe. Effective aesthetics prediction with multi-level spatially pooled features. In CVPR, 2019.   \n[58] Shuai He, Yongchang Zhang, Rui Xie, Dongxiang Jiang, and Anlong Ming. Rethinking image aesthetics assessment: Models, datasets and benchmarks. IJCAI, 2022.   \n[59] Shuai He, Anlong Ming, Shuntian Zheng, Haobin Zhong, and Huadong Ma. Eat: An enhancer for aesthetics-oriented transformers. In Proceedings of the 31st ACM International Conference on Multimedia, pages 1023\u20131032, 2023.   \n[60] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023.   \n[61] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018.   \n[62] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient regularized deep retinex network for robust low-light image enhancement. IEEE Transactions on Image Processing, 30:2072\u20132086, 2021.   \n[63] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr\u00e9do Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In CVPR 2011, pages 97\u2013104. IEEE, 2011.   \n[64] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The abstract and/or introduction clearly state the claims made, as shown at the end of the abstract and introduction. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: We briefly discuss the limitations of this paper\u2019s work in Appendix A.6. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: This paper is not concerned with proving specific theorems, all assumptions have been clearly stated or referenced. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The code and dataset are available in the supplementary material to support the reproducibility of experimental results. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The code and dataset are available in the supplementary material. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: All the training details and protocols are provided in Appendix A.3. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [No] ", "page_idx": 15}, {"type": "text", "text": "Justification: All results of this paper are not accompanied by error bars, confidence intervals, or statistical significance tests, and, at the same time, do not affect the conclusions of this paper. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Experiments compute resources are provided in Appendix A.3. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our code strictly follows the NeurIPS Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: At the end of the abstract and introduction, we briefly discuss the potential positive societal impacts, and we found no negative societal impacts of the work performed. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We describe safeguards in Appendix A.7. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We describe Licenses in Appendix A.7. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All new assets introduced in the paper well documented. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper is not crowdsourcing experiments and research with human subjects. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "image", "img_path": "zVrQeoPIoQ/tmp/ae112f0b6532cd2362cdd7bd1544f619a1a668a84289c5b318d38b6dd8e93bb9.jpg", "img_caption": ["Figure 9: Based on the IEA criteria provided by different manufacturers, our pixel-level IEA results can be mapped to the corresponding IEA scores according to the provided criteria without fine-tuning. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A.1 Image Collection Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1) Varied Scenes. The scene defines the image\u2019s content and overall lighting conditions. As shown in Fig. 6(a)(b), IEA40K spans over 50 scene categories, enabling a comprehensive understanding of image exposure in dynamic scenes. 2) Diverse Light. The type and direction of the light source greatly influence light distribution within an image. Our dataset includes a variety of light sources and directions, as shown in Fig. 6(c). We also consider the shading tone\u2019s impact on an image\u2019s visual appeal, selecting images with dark, clear, and neutral tones. 3) Sufficient Devices. The performance of the capturing device can affect image quality and exposure. To address this, our dataset features images from over 27 different devices, ranging from smartphones like the iPhone 13 and Huawei Mate40 to digital cameras from Canon, Nikon, and others. 4) Uniform Resolution. Initially captured at high resolutions, our images were downsized for manageability. Keeping the aspect ratio intact, we reduced the image size so that the shorter side measures 512 pixels, storing them in PNG format to balance quality and model compatibility. 5) Comprehensive Simulations. Capturing varied exposure levels for the same scene poses a challenge. To overcome this, we simulated $80\\%$ of the images at different exposure EV levels from the original raw flies, or collected images from MIT-Adobe FiveK dataset [63]. After expert selection and adjustment, we compiled a set of 8 specific exposure levels for each raw image, resulting in 1 ideal reference and 8 exposure distorted images per scene. ", "page_idx": 19}, {"type": "text", "text": "A.2 Criteria-oriented IEA without fine-tuning ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Different manufacturers have their own criteria for IEA scoring. Our pixel-level IEA results can adapt well to these different criteria. In Fig. 9, we provide an example where manufacturers can map the pixel-level IEA results to their overall scoring criteria to ultimately obtain their IEA scores. ", "page_idx": 19}, {"type": "text", "text": "A.3 Experimental Settings ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our model is implemented in PyTorch and trained with the Adam optimizer [64]. We set the initial learning rate to $\\bar{3}\\times10^{-5}$ with a decay rate of 0.1 after every 10 epochs, and the mini-batch size is set to 36. On the Intel 10900X CPU and RTX 3090 platform, the entire training time is about 14 hours for 30 epochs (early stopping), and the inference time is 0.083 seconds for a $256\\times256$ image (supports larger input sizes). To reduce the bias caused by a random splitting, we run the random train-test splitting operation five times, and the comparison results are reported as the average of the five evaluation experiments. ", "page_idx": 19}, {"type": "text", "text": "A.4 Why not Employ Light Enhancement Methods to Obtain an Exposure Residual? ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "There is a common misconception that subtracting the inputs and outputs from existing light enhancement methods, or comparing distorted images to reference images from existing light enhancement datasets, will yield the pixel-level IEA prediction results or ground truth (exposure residual) described in this paper. However, there are two fundamental differences: 1) Different Data Production Processes: light enhancement methods only optimize reference images\u2019 exposure in global areas through linear adjustments, e.g., adjusting the exposure value (EV) across the entire image. Our method involves non-linear adjustments by experts to optimize local exposure in specific areas of reference images. Additionally, human experts correct any errors in the exposure residual. As a result, the exposure residual obtained from these two approaches contains different amounts of information. 2) Different Task Objectives: light enhancement methods generally aim to rectify image issues to a level acceptable to human vision, ensuring consistency in image semantics and using complete reference images for supervision. In contrast, our task is focused solely on resolving exposure issues. We do not consider the impact of image semantics on predictions nor attempt to restore image semantics. Instead, we use only the exposure residual for supervision, eliminating other factors that could impact exposure quality. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "A.5 More Performance Evaluations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Comparison of Different Wavelets. As outlined in the paper, the Haar wavelet distinctly aligns its component decomposition with exposure characteristics, a unique attribute not shared by other wavelets. Experimental results also show that the Haar wavelet surpasses other wavelets in performance. Table 5 presents a comparative analysis of the Haar wavelet against other notable wavelets (Daubechies and Symlet) on the IEA40K dataset. ", "page_idx": 20}, {"type": "table", "img_path": "zVrQeoPIoQ/tmp/e9ca5413eb61b7ea24469f8cbcc0703a60a88abb92bed3a24eaf6d665c61a7e9.jpg", "table_caption": ["Table 5: Ablation of various wavelets. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "zVrQeoPIoQ/tmp/2ac096bd5b8bfbf6f15e4ed5e4521bd5b871d9726b515bfcebb6e2d868e3ce9c.jpg", "table_caption": ["Table 6: Ablation of different loss functions. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Comparison of Different Loss Functions. During training, we evaluated three primary types of loss functions: L1-norm, L2-norm, and Smooth L1. Comparative results are detailed in Table 6. The L1-norm demonstrates superior robustness and faster training speeds, as evidenced by earlier convergence epochs in the IEA task. ", "page_idx": 20}, {"type": "table", "img_path": "zVrQeoPIoQ/tmp/73c51bdbd433f915a9d3c254a4083b46639247893bb8f273a5675778f75cc07a.jpg", "table_caption": ["Table 7: The PSNR results on IEA40k. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.6 Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The dataset includes few images of high-speed moving objects because capturing well-aligned images with different exposures for such objects is difficult. The misalignment cannot be easily corrected using existing image alignment methods. Therefore, different exposure images for these objects can only be simulated by adjusting the EV value of ideal images. ", "page_idx": 20}, {"type": "text", "text": "Additionally, resizing high-resolution images for evaluation can significantly compress the image, affecting pixel-level exposure assessments. Increasing the input size during network training can address this issue but will also increase the inference time. ", "page_idx": 21}, {"type": "text", "text": "A.7 Safeguards and Licenses for Existing Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The original owners of assets (e.g., code, data, models) used in the paper are properly credited, and the licenses and terms of use are explicitly mentioned and properly respected, ensuring that there are no copyright issues and no risk of misuse. ", "page_idx": 21}]