[{"figure_path": "gjEzL0bamb/figures/figures_3_1.jpg", "caption": "Figure 1: The inference process of MimicTalk. We use an in-context stylized audio-to-motion model to produce expressive facial motion mimicking the talking style of a reference video. Then, a personalized renderer could render high-quality talking face videos that mimic the static and dynamic visual attributes of the target identity.", "description": "This figure illustrates the overall pipeline of MimicTalk.  It begins with the user providing both driving audio and a reference video showcasing the desired talking style. This information is fed into an \"In-Context Stylized Audio-to-Motion\" module which generates facial motion mimicking the reference video's style. This motion data, along with a person-specific 3D face representation, is input to the \"SD-hybrid Adapted Talking Face Renderer\", which generates the final, high-quality expressive talking face video of the target person.", "section": "3 MimicTalk"}, {"figure_path": "gjEzL0bamb/figures/figures_4_1.jpg", "caption": "Figure 2: The training process of the personalized TFG renderer via the static-dynamic (SD)-hybrid adaptation pipeline. We adopt a pretrained one-shot person-agnostic 3D TFG model as the backbone, then fine-tune a person-dependent 3D face representation to memorize the static geometry and texture details. We also inject LoRA units into the backbone to learn the personalized dynamic features.", "description": "This figure illustrates the training process for the personalized talking face generation (TFG) renderer using a static-dynamic hybrid adaptation pipeline.  It starts with a pre-trained one-shot person-agnostic 3D TFG model. The pipeline then fine-tunes a person-dependent 3D face representation to capture static features (geometry and texture).  Low-rank adaptation (LoRA) units are injected into the model to learn the dynamic, personalized characteristics of a specific individual's facial expressions.", "section": "3.2 Static-Dynamic-Hybrid Identity Adaptation"}, {"figure_path": "gjEzL0bamb/figures/figures_5_1.jpg", "caption": "Figure 3: The process of in-context stylized motion prediction. For the training process please refer to Fig. 7.", "description": "This figure illustrates the inference process of the In-context Stylized Audio-to-Motion (ICS-A2M) model.  The model takes as input driving audio, a talking style prompt (from a reference video), and a noisy motion representation.  It uses a transformer network to predict the velocity of the noisy motion, which is then iteratively denoised via an ODE solver to generate the final stylized, context-aware motion. The talking style prompt implicitly guides the model to produce motion that matches the style of the reference video.", "section": "3.3 In-Context Stylized Audio-to-Motion"}, {"figure_path": "gjEzL0bamb/figures/figures_8_1.jpg", "caption": "Figure 4: Training/data efficiency of SD-Hybrid adaptation: CSIM results at different iterations and data scales. The baseline RAD-NeRF uses 180-second-long training samples and is updated for 250,000 iterations.", "description": "This figure demonstrates the training and data efficiency of the SD-Hybrid adaptation method used in MimicTalk.  The left subplot shows how CSIM (a metric for identity similarity) improves as the number of adaptation steps (iterations during fine-tuning) increases, converging to a high similarity score. The right subplot shows the impact of varying the length of the training video on CSIM, illustrating that even short training videos yield good identity preservation.  The results are compared to the baseline RAD-NeRF, which requires significantly more training data and time.", "section": "4.2 Quantitative Evaluation"}, {"figure_path": "gjEzL0bamb/figures/figures_14_1.jpg", "caption": "Figure 5: The detailed network structure of the person-agnostic renderer.", "description": "This figure shows the detailed architecture of the person-agnostic 3D talking face renderer used as the backbone in MimicTalk. It consists of four main modules: (a) a SegFormer-based motion adapter that takes source and target PNCCs (projected normalized coordinate codes) as input and generates an expression tri-plane; (b) SegFormer blocks that process the input feature map; (c) a volume renderer that combines the tri-plane with the motion adapter output to render a low-resolution volume-rendered image; and (d) a super-resolution module that upsamples the low-resolution image to a high-resolution one. The figure also illustrates the process of transforming the canonical 3D face into a target expression.", "section": "3.1 NeRF-based Person-Agnostic Renderer"}, {"figure_path": "gjEzL0bamb/figures/figures_15_1.jpg", "caption": "Figure 6: The process that plugs LoRAs into convolutional/linear layers of the person-agnostic renderer.", "description": "This figure illustrates how Low-Rank Adaptation (LoRA) is implemented within the person-agnostic renderer.  LoRA injects low-rank matrices (A and B) into the pre-trained convolutional and linear layers.  The pre-trained weights are kept frozen (indicated by locks), while only the smaller LoRA matrices are updated during training, making the adaptation process more efficient.", "section": "3.2 Static-Dynamic-Hybrid Identity Adaptation"}, {"figure_path": "gjEzL0bamb/figures/figures_15_2.jpg", "caption": "Figure 7: The training process and inference usage of the Audio-Guided Motion Infilling Task.", "description": "This figure illustrates the training and inference processes for the Audio-Guided Motion Infilling task.  During training, the model learns to reconstruct randomly masked segments of motion tracks given the complete audio track and the surrounding unmasked motion. This allows the model to learn the talking style from context.  During inference, there are two usage scenarios: 1) providing an audio-motion pair of the target speaker as a talking style prompt to mimic that style; and 2) audio-only sampling, where the model generates motions with a randomly sampled style.", "section": "3.3 In-Context Stylized Audio-to-Motion"}]