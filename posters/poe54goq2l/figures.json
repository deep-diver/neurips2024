[{"figure_path": "poE54GOq2l/figures/figures_1_1.jpg", "caption": "Figure 1: The graph shows the simplified workflow of SnapKV, where the orange area represents the cluster of features per head selected by SnapKV. These features are then used to form new Key-Value pairs concatenated with the features in the observation window. Together, the selected prefix and observation windows constitute the new KV cache utilized for the generation.", "description": "The figure illustrates the SnapKV workflow.  It shows how SnapKV selects important features from the prompt (the initial input to the LLM) using a voting and selection mechanism based on an observation window at the end of the prompt. These features are then clustered and concatenated with the observation window features to create a compressed KV cache for generation, significantly reducing memory usage and improving efficiency compared to traditional methods.", "section": "1 Introduction"}, {"figure_path": "poE54GOq2l/figures/figures_2_1.jpg", "caption": "Figure 2: The overlap rates between attention features of the input sequence, selected by various windows along the input and during generation, with each line representing a model layer.", "description": "This figure shows the consistency of attention patterns in LLMs.  It plots the overlap rate between attention features selected by different windows within the input prompt sequence and the actual attention features used during generation.  Each line represents a different layer in the LLM, demonstrating that the attention patterns identified by the last window of the input sequence are highly consistent with the patterns observed during generation. This suggests that the LLM focuses on specific features early in the process, which is the basis for the SnapKV algorithm.", "section": "3 Observations"}, {"figure_path": "poE54GOq2l/figures/figures_2_2.jpg", "caption": "Figure 3: The layer-wise overlap rates between input sequence attention features selected by the last window of input sequence and those selected by 4 windows along generation.", "description": "This figure shows the layer-wise overlap rates between the attention features selected by the last window of the input sequence and those selected by four windows during the generation process. The x-axis represents the layer number, and the y-axis represents the overlap rate (percentage).  Each colored line corresponds to a different window during generation (window 0, window 1, window 2, window 3). The high overlap rates across layers suggest that the attention focus on important input features remains relatively consistent throughout the generation process.", "section": "Observations"}, {"figure_path": "poE54GOq2l/figures/figures_5_1.jpg", "caption": "Figure 4: The layer-wise overlap of important positions utilized by different question-answer pairs in the same dataset.", "description": "This figure shows the layer-wise overlap of important positions used by different question-answer pairs within the same dataset.  The x-axis represents the layer number, and the y-axis represents the overlap percentage.  Three datasets (QMSum, Openreview, and SPACE) are shown, each represented by a different colored line. The graph demonstrates the consistency of important attention features across different question-answer pairs within a dataset, even across different layers of the model. This consistency supports the claim that LLMs know what information is important before generation begins.", "section": "4.2 Robustness Analysis of Hit Rate"}, {"figure_path": "poE54GOq2l/figures/figures_5_2.jpg", "caption": "Figure 5: The layer-wise average hit rate of important positions used by prompts with questions at the beginning and the end.", "description": "This figure shows the layer-wise average hit rate of important positions used by prompts containing questions placed at the beginning versus the end of the prompts. The hit rate represents the percentage of important attention features successfully selected by the observation window.  The x-axis shows the layer number, and the y-axis represents the hit rate.  Different colored lines correspond to different datasets (QMSum, Openreview, SPACE).  The average prompt length, average context length, and the total number of samples used in the experiment are indicated in the legend.", "section": "4.2 Robustness Analysis of Hit Rate"}, {"figure_path": "poE54GOq2l/figures/figures_6_1.jpg", "caption": "Figure 6: Needle-in-a-Haystack test performance comparison on single A100-80GB GPU, native HuggingFace implementation with only a few lines of code changed. The x-axis denotes the length of the document (\"haystack\") from 1K to 380K tokens; the y-axis indicates the position that the \"needle\" (a short sentence) is located within the document. For example, 50% indicates that the needle is placed in the middle of the document. Here LWMChat with SnapKV is able to retrieve the needle correctly before 140k and with only a little accuracy drop after. Meanwhile, the original implementation encounters OOM error with 33k input tokens (white dashed line).", "description": "This figure shows the results of the Needle-in-a-Haystack test, which evaluates the ability of a model to find a short sentence (\"needle\") within a long document (\"haystack\").  The x-axis represents the length of the document, and the y-axis represents the location of the needle within the document.  The results show that SnapKV allows the model to process much longer documents (up to 380k tokens) than the baseline, with only a small decrease in accuracy.", "section": "5.1.1 Needle-in-a-Haystack"}, {"figure_path": "poE54GOq2l/figures/figures_7_1.jpg", "caption": "Figure 7: Decoding latency comparison of baseline implementation and SnapKV optimized solutions on various batch sizes. The x-axis denotes the input sequence length; the y-axis indicates decoding latency (ms/token). All experiments are conducted on an A100 80GB GPU. The red dotted line denotes the common context length of state-of-the-art long sequence models.", "description": "This figure compares the decoding latency (in milliseconds per token) of a baseline LLM model and the same model optimized using SnapKV, across various input sequence lengths and batch sizes.  It shows how SnapKV maintains a nearly constant latency even as the input sequence length increases, whereas the baseline latency grows linearly, ultimately exceeding memory limits (OOM). The red line indicates the typical maximum input length for state-of-the-art models. The results demonstrate the improved decoding speed and memory efficiency of SnapKV.", "section": "5.1.2 Decoding Speed and Memory Bound"}, {"figure_path": "poE54GOq2l/figures/figures_8_1.jpg", "caption": "Figure 8: Ablation study of pooling on LongEval-Lines. The evaluation includes inputs, each comprised of lines formatted as \"line makeshift-penguin: REGISTER_CONTENT is <10536>\", where the key is an adjective-noun pair and the value is a random 5-digit number. The model needs to retrieve the value based on a given key. The x-axis denotes the length of the input; the y-axis indicates the position of the groundtruth, from 5K to 30K tokens. With the pooling, the model can retrieve correct values before 16k and performs significantly better than the one without pooling.", "description": "This figure shows the ablation study of the pooling mechanism within the SnapKV algorithm, using the LongEval-Lines benchmark dataset.  The heatmaps compare the performance of the Mistral-7B-Instruct-v0.2 model with and without pooling. The x-axis represents the input length, and the y-axis shows the position of the ground truth value. The color intensity reflects the accuracy. The results demonstrate that pooling significantly improves the model's ability to accurately retrieve information, especially when dealing with longer input sequences.", "section": "5.2 Ablation Study of Effectiveness of Pooling"}, {"figure_path": "poE54GOq2l/figures/figures_12_1.jpg", "caption": "Figure 9: Comparison of generation latency (ms/token). The baseline is the Huggingface implementation of naive decoding.", "description": "This figure compares the generation latency in milliseconds per token for three different methods: Medusa with SnapKV, Medusa alone, and a baseline method (HuggingFace's naive decoding). The x-axis represents the prompt length in thousands of tokens, while the y-axis shows the latency. The graph demonstrates that Medusa with SnapKV significantly reduces latency compared to Medusa alone and the baseline, especially as the prompt length increases.", "section": "A Case Study: Compatibility with Parallel Decoding"}, {"figure_path": "poE54GOq2l/figures/figures_13_1.jpg", "caption": "Figure 10: The prefilling time and maximum memory allocated comparison between Mistral-7B-Instruct-v0.2 with and without SnapKV on an H100.", "description": "This figure compares the prefilling time and maximum memory usage between the original Mistral-7B-Instruct-v0.2 model and the same model using SnapKV on an H100 GPU.  It shows the performance for input sequences ranging from 5,000 to 45,000 tokens. The results demonstrate that SnapKV introduces minimal overhead in terms of both prefilling time and memory consumption, even for very long sequences.", "section": "B Overhead Analysis of Prefilling Stage"}, {"figure_path": "poE54GOq2l/figures/figures_13_2.jpg", "caption": "Figure 11: The prompting time and generation time comparison between Mistral model with and without SnapKV.", "description": "This figure compares the prompting time and generation time of the Mistral model with and without SnapKV for varying input sequence lengths.  It shows that while prompting time increases relatively slowly with input length, the generation time for the standard model increases dramatically. SnapKV maintains a nearly constant generation time regardless of input length, significantly reducing the overall processing time.  The speed advantage of SnapKV is more pronounced for longer input sequences.", "section": "C Discussion of Generation Time Speedup"}, {"figure_path": "poE54GOq2l/figures/figures_15_1.jpg", "caption": "Figure 6: Needle-in-a-Haystack test performance comparison on single A100-80GB GPU, native HuggingFace implementation with only a few lines of code changed. The x-axis denotes the length of the document (\"haystack\") from 1K to 380K tokens; the y-axis indicates the position that the \"needle\" (a short sentence) is located within the document. For example, 50% indicates that the needle is placed in the middle of the document. Here LWMChat with SnapKV is able to retrieve the needle correctly before 140k and with only a little accuracy drop after. Meanwhile, the original implementation encounters OOM error with 33k input tokens (white dashed line).", "description": "This figure showcases the results of a Needle-in-a-Haystack test, comparing the performance of the original HuggingFace implementation and the SnapKV-enhanced version on an A100-80GB GPU.  The test evaluates the ability of the models to locate a short sentence (\"needle\") within a long document (\"haystack\") of varying lengths (1K to 380K tokens). The x-axis represents the document length, and the y-axis shows the location of the \"needle\" within the document. The results demonstrate that SnapKV significantly improves the model's ability to retrieve the needle, even at extremely long document lengths, while the original implementation runs out of memory (OOM) at just 33K tokens.", "section": "5.1.1 Needle-in-a-Haystack"}]