{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report on GPT-4, a large language model that is referenced as having impressive capabilities for handling long context."}, {"fullname_first_author": "Guangxuan Xiao", "paper_title": "Efficient streaming language models with attention sinks", "publication_date": "2023-09-17", "reason": "This paper proposes an efficient method for handling long sequences in LLMs, which is directly relevant to the problem addressed in the target paper."}, {"fullname_first_author": "Zhenyu Zhang", "paper_title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models", "publication_date": "2024-00-00", "reason": "This paper introduces a method for compressing KV caches in LLMs, which is directly compared to the method proposed in the target paper."}, {"fullname_first_author": "Zichang Liu", "paper_title": "ScissorHands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time", "publication_date": "2024-00-00", "reason": "This paper proposes a method for compressing KV caches in LLMs during generation, which is another relevant approach compared to the method proposed in the target paper."}, {"fullname_first_author": "Suyu Ge", "paper_title": "Model tells you what to discard: Adaptive kv cache compression for llms", "publication_date": "2023-10-01", "reason": "This paper introduces another method for compressing KV caches in LLMs, which is yet another relevant approach compared to the method proposed in the target paper."}]}