[{"figure_path": "tWkL7k1u5v/tables/tables_8_1.jpg", "caption": "Table 1: MAE of Equiformer trained with and without our optimization framework on a set of molecules from the MD17 dataset. The energy is reported in meV and the force in meV/\u00c5 units", "description": "This table presents the mean absolute error (MAE) for energy and force prediction using the Equiformer model, both with and without the proposed optimization framework.  The results are shown for four different molecules from the MD17 dataset.  The MAE values are reported in milli-electron volts (meV) for energy and milli-electron volts per \u00c5ngstr\u00f6m (meV/\u00c5) for forces.", "section": "4.3 Molecular Dynamics Simulation"}, {"figure_path": "tWkL7k1u5v/tables/tables_9_1.jpg", "caption": "Table 2: RMSE error on the synthetic smoke plume dataset with approximate rotational and scale symmetries. In the \"Future\" evaluation we train and evaluate the models in the same simulation location but we test for later time steps in the simulation from the ones used in training. In the \"Domain\" evaluation we train and evaluate the models on the same timesteps but on different spatial locations in the simulation.", "description": "This table presents the RMSE (Root Mean Squared Error) results for different models on a synthetic smoke plume dataset, evaluating their performance under approximate rotational and scale symmetries. Two evaluation scenarios are considered: \"Future\", where models are tested on later time steps within the same simulation location as training, and \"Domain\", where models are evaluated on different spatial locations but the same time steps as training. The models compared include a simple MLP, a convolutional network (Conv), an equivariant convolutional network (Equiv), three approximately equivariant networks (RPP, Lift, RSteer), and the proposed RSteer+Ours method.", "section": "4.4 Optimizing Approximately Equivariant Networks"}, {"figure_path": "tWkL7k1u5v/tables/tables_15_1.jpg", "caption": "Table 3: Additional Number of parameters and Computational Overhead introduced by the proposed method", "description": "This table shows a comparison of the number of parameters and training time per epoch for different models with and without the proposed method.  It demonstrates the additional computational cost introduced by the method, showing both the increase in the number of parameters and the small increase in the training time.", "section": "A.3 Computational and Memory Overhead of proposed method"}, {"figure_path": "tWkL7k1u5v/tables/tables_16_1.jpg", "caption": "Table 4: Comparison of our proposed method with previous works performing equivariant adaption or finetuning, on ModelNet40 classification (Base model: VN-PointNet). Here it is important to note that in the case of Equi-Tuning, equivariance is achieved by group averaging. As a result, during inference the model is required to perform multiple forward passes, which slows down the method's inference.", "description": "The table compares the performance of the proposed optimization framework with two other methods (Equiv-Adapt and Equi-Tuning) and a standard training approach on the ModelNet40 point cloud classification task.  The base model used is the VN-PointNet.  It highlights that the proposed method achieves comparable results to the best-performing method (Equi-Tuning) which requires significantly more computation during inference.", "section": "A.5 Additional comparison with Methods using Equivariant Adaptation/Fine Tuning"}]