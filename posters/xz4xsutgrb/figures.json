[{"figure_path": "XZ4XSUTGRb/figures/figures_1_1.jpg", "caption": "Figure 1: The mesh is analytically extracted from piecewise trilinear networks, comprising both HashGrid [12] and ReLU neural networks, that have been trained to learn a signed distance function with the eikonal loss. We start with initial vertices and edges defined using the grid marks (1), and its linear subdivision (2a); however, intersecting polygons (ref. Section 3.2) need bilinear subdivision (2b) and consequentially trilinear subdivision (2c). Note that the linear and bilinear subdivisions are specific instances of trilinear subdivisions with straightforward solutions.", "description": "This figure illustrates the mesh extraction process from piecewise trilinear networks. It starts with a grid and progressively refines it through linear, bilinear, and trilinear subdivisions to accurately capture the curved surfaces. The process involves identifying intersecting polygons and using trilinear interpolation to approximate the intersection points.", "section": "4 Method"}, {"figure_path": "XZ4XSUTGRb/figures/figures_6_1.jpg", "caption": "Figure 2: Trilinear regions in the xy-plane at z = 0.04, identified by the sign-vectors (Definition 3.4), are represented with random colors. (a) Grids described in Section 5 and Algorithm 4. (b) The neurons of the first layer representing folded hypersurfaces (blue arrow). (c) All neurons representing every nonlinear boundary. (d) Select all zero-set vertices and edges. (e) Skeletonized as in Section 4.3.", "description": "This figure visualizes the process of mesh extraction from a piecewise trilinear network. It starts with a grid (a), identifies trilinear regions using sign-vectors (b, c), extracts zero-set vertices and edges (d), and finally skeletonizes the mesh (e). Each stage highlights different aspects of the mesh extraction process.", "section": "5 Implementation"}, {"figure_path": "XZ4XSUTGRb/figures/figures_7_1.jpg", "caption": "Figure 3: Chamfer distance for the bunny with the Large model comparing with MC, MT, and NDC.", "description": "This figure shows the chamfer distance, a metric for evaluating the similarity between two sets of points from two meshes, for the bunny model using the Large model setting. It compares the proposed method with three other methods: Marching Cubes (MC), Marching Tetrahedra (MT), and Neural Dual Contour (NDC). The x-axis represents the number of vertices, and the y-axis represents the chamfer distance. The figure shows that the proposed method achieves lower chamfer distance with fewer vertices compared to the other three methods.", "section": "6 Experiment"}, {"figure_path": "XZ4XSUTGRb/figures/figures_8_1.jpg", "caption": "Figure 1: The mesh is analytically extracted from piecewise trilinear networks, comprising both HashGrid [12] and ReLU neural networks, that have been trained to learn a signed distance function with the eikonal loss. We start with initial vertices and edges defined using the grid marks (1), and its linear subdivision (2a); however, intersecting polygons (ref. Section 3.2) need bilinear subdivision (2b) and consequentially trilinear subdivision (2c). Note that the linear and bilinear subdivisions are specific instances of trilinear subdivisions with straightforward solutions.", "description": "This figure illustrates the process of analytically extracting a mesh from piecewise trilinear networks.  It starts with a grid and progressively subdivides it (linearly, bilinearly, then trilinearly) to handle intersecting regions, ultimately producing a mesh representation of the network's decision boundaries.", "section": "Method"}, {"figure_path": "XZ4XSUTGRb/figures/figures_8_2.jpg", "caption": "Figure 5: Effect of the weight of eikonal loss on the flatness error in Equation (11). For the plot, we conduct experiments using the unit sphere.", "description": "The figure shows the impact of different weights of the eikonal loss on the flatness error during training.  The flatness error measures how well the hypersurfaces in the trilinear network approximate planes, as predicted by the theory.  The plot shows that using a small weight for the eikonal loss (0.000-0.01) leads to higher flatness errors, meaning the hypersurfaces are not well approximated by planes. However, as the weight of the eikonal loss is increased, the flatness error decreases, indicating a better approximation of planes. The experiment is conducted using the unit sphere, demonstrating the correlation between the eikonal loss and the planarity of the hypersurfaces.", "section": "Supplementary experiments"}, {"figure_path": "XZ4XSUTGRb/figures/figures_9_1.jpg", "caption": "Figure 6: This figure provides a detailed visualization of Figure 10 in the Appendix and its competitors. MC 64, MT 32 (Marching Tetrahedra), and NDC (Neural Dual Contour [32]) suffer over-smoothing or inaccuracy of the actual surface in the Small networks within the nose, whereas our method reflects these details (see the boundaries of a nose) with consistent normals (in colors). MT efficiently demands SDF values by utilizing six tetrahedra within grids to get intermediate vertices. However, it is inefficient in terms of the number of vertices extracted. NDC faces a generalization issue for a zero-shot setting for the given networks, producing unsmooth surfaces.", "description": "This figure compares the mesh generated by the proposed method with other methods (Marching Cubes, Marching Tetrahedra, and Neural Dual Contour) for a small neural network model. It highlights the proposed method's ability to accurately capture fine details with consistent normals, while the other methods either suffer from over-smoothing or inaccuracies.", "section": "Visualizations"}, {"figure_path": "XZ4XSUTGRb/figures/figures_14_1.jpg", "caption": "Figure 7: A classical polynomial x\u00b2 + y\u00b2 + 2xy + 2x + 2y + 2 turns into a tropical polynomial as max(1+2x,1+ 2y, 2 + x + y, 2 + x, 2 +y, 2), taking into account the implicit coefficient of 1. We redraw Figure 1 of the tropical curve from Zhang et al. [27] as a region map. Each color indicates a region where each tropical monomial has maximum.", "description": "This figure illustrates how a classical polynomial is transformed into a tropical polynomial.  Each color-coded region represents where a specific monomial term within the tropical polynomial takes on its maximum value. This visualization helps explain the concept of tropical hypersurfaces which are central to the paper's theoretical analysis.", "section": "A Tropical geometry and tropical algebra of neural networks"}, {"figure_path": "XZ4XSUTGRb/figures/figures_25_1.jpg", "caption": "Figure 8: Chamfer distance for the Bunny with the Large model applying QEM (a mesh simplification). Both benefit from QEM while our method efficiently retains better chamfer distances.", "description": "This figure compares the chamfer distance achieved by the proposed method with and without mesh simplification using Quadric Error Metrics (QEM).  The results show that both methods benefit from QEM, indicating improved mesh efficiency.  However, the proposed method consistently maintains lower chamfer distances, suggesting its effectiveness in retaining mesh quality even after simplification.", "section": "F Supplementary experiments"}, {"figure_path": "XZ4XSUTGRb/figures/figures_28_1.jpg", "caption": "Figure 1: The mesh is analytically extracted from piecewise trilinear networks, comprising both HashGrid [12] and ReLU neural networks, that have been trained to learn a signed distance function with the eikonal loss. We start with initial vertices and edges defined using the grid marks (1), and its linear subdivision (2a); however, intersecting polygons (ref. Section 3.2) need bilinear subdivision (2b) and consequentially trilinear subdivision (2c). Note that the linear and bilinear subdivisions are specific instances of trilinear subdivisions with straightforward solutions.", "description": "This figure illustrates the process of analytically extracting a mesh from piecewise trilinear networks.  It begins with a grid, then progressively subdivides edges and faces to accurately represent the curved surfaces created by the neural network's implicit function. The process handles intersections of surfaces, ultimately resulting in a complete mesh.", "section": "4 Method"}, {"figure_path": "XZ4XSUTGRb/figures/figures_29_1.jpg", "caption": "Figure 1: The mesh is analytically extracted from piecewise trilinear networks, comprising both HashGrid [12] and ReLU neural networks, that have been trained to learn a signed distance function with the eikonal loss. We start with initial vertices and edges defined using the grid marks (1), and its linear subdivision (2a); however, intersecting polygons (ref. Section 3.2) need bilinear subdivision (2b) and consequentially trilinear subdivision (2c). Note that the linear and bilinear subdivisions are specific instances of trilinear subdivisions with straightforward solutions.", "description": "This figure illustrates the analytical mesh extraction process from piecewise trilinear networks.  It starts by defining initial vertices and edges from a grid.  Then, it shows how the mesh is refined through linear, bilinear, and trilinear subdivisions to handle intersections of polygons in the trilinear region.  The process culminates in an accurate mesh representing the surface learned by the network.", "section": "4 Method"}, {"figure_path": "XZ4XSUTGRb/figures/figures_29_2.jpg", "caption": "Figure 1: The mesh is analytically extracted from piecewise trilinear networks, comprising both HashGrid [12] and ReLU neural networks, that have been trained to learn a signed distance function with the eikonal loss. We start with initial vertices and edges defined using the grid marks (1), and its linear subdivision (2a); however, intersecting polygons (ref. Section 3.2) need bilinear subdivision (2b) and consequentially trilinear subdivision (2c). Note that the linear and bilinear subdivisions are specific instances of trilinear subdivisions with straightforward solutions.", "description": "This figure illustrates the analytical mesh extraction process from piecewise trilinear networks. It starts with a grid and progressively refines the mesh by subdividing edges and faces based on linear, bilinear, and trilinear interpolations. The process is driven by the eikonal constraint, which transforms hypersurfaces into flat planes within the trilinear regions, simplifying mesh extraction.  The resulting mesh provides a precise and efficient representation of the network's geometry.", "section": "4 Method"}, {"figure_path": "XZ4XSUTGRb/figures/figures_30_1.jpg", "caption": "Figure 6: This figure provides a detailed visualization of Figure 10 in the Appendix and its competitors. MC 64, MT 32 (Marching Tetrahedra), and NDC (Neural Dual Contour [32]) suffer over-smoothing or inaccuracy of the actual surface in the Small networks within the nose, whereas our method reflects these details (see the boundaries of a nose) with consistent normals (in colors). MT efficiently demands SDF values by utilizing six tetrahedra within grids to get intermediate vertices. However, it is inefficient in terms of the number of vertices extracted. NDC faces a generalization issue for a zero-shot setting for the given networks, producing unsmooth surfaces.", "description": "This figure compares the mesh generated by the proposed method with those generated by three other methods (Marching Cubes with 64 samples, Marching Tetrahedra with 32 samples, and Neural Dual Contouring with 64 samples) for a small-sized network. The figure highlights that the proposed method produces a more accurate mesh, especially around the nose area, which is a difficult region to accurately represent with conventional methods.", "section": "Visualizations"}, {"figure_path": "XZ4XSUTGRb/figures/figures_30_2.jpg", "caption": "Figure 1: The mesh is analytically extracted from piecewise trilinear networks, comprising both HashGrid [12] and ReLU neural networks, that have been trained to learn a signed distance function with the eikonal loss. We start with initial vertices and edges defined using the grid marks (1), and its linear subdivision (2a); however, intersecting polygons (ref. Section 3.2) need bilinear subdivision (2b) and consequentially trilinear subdivision (2c). Note that the linear and bilinear subdivisions are specific instances of trilinear subdivisions with straightforward solutions.", "description": "This figure illustrates the process of analytically extracting a mesh from piecewise trilinear networks. It starts with a grid and progressively subdivides it using linear, bilinear, and trilinear methods to capture intersections of hypersurfaces. The final result is a mesh representation of the learned signed distance function.", "section": "Method"}]