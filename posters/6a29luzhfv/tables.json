[{"figure_path": "6A29LUZhfv/tables/tables_5_1.jpg", "caption": "Table 1: The key statistics of MixEval and MixEval-Hard. With dynamic benchmarking, the numbers may vary slightly while the number of queries will not change.", "description": "This table presents key statistics for the MixEval and MixEval-Hard benchmarks, including the number of queries, average number of tokens per query and input, average, minimum and maximum numbers of tokens per input, the percentage of English queries, and the evaluation type (ground truth).  Note that because these benchmarks are dynamically updated, the numbers presented might vary slightly over time, but the total number of queries remains consistent.", "section": "3.3 MixEval-Hard"}, {"figure_path": "6A29LUZhfv/tables/tables_6_1.jpg", "caption": "Table 2: Stability test for dynamic benchmarking. Five models tested across five updated versions of MixEval show an average mean of 77.64 and a Std. of 0.36, validating the stability of model scores over versions. The unique web query ratio, averaged across all version pairs, is 99.71%, and the unique benchmark query ratio is 85.05%, indicating significant differences between versions.", "description": "This table presents the results of a stability test for the dynamic benchmarking approach used in MixEval.  Five different LLMs were evaluated across five different versions of the MixEval benchmark. The table shows the average score and standard deviation for each model across the versions, demonstrating high stability. It also shows the percentage of unique web queries and benchmark queries across the different versions, highlighting the significant changes in data between versions despite model score stability. This demonstrates the effectiveness of the dynamic updating mechanism in mitigating benchmark contamination.", "section": "3.4 Dynamic Benchmarking"}, {"figure_path": "6A29LUZhfv/tables/tables_19_1.jpg", "caption": "Table 3: The Evaluation results of chat models on MixEval, MixEval-Hard, and their sub-splits.", "description": "This table presents the detailed evaluation results of various chat models on MixEval, MixEval-Hard, and their main subsets.  It shows the performance scores of each model across different subsets of the benchmarks, allowing for a comparison of model capabilities across various task complexities and distributions. The proportion of each benchmark included in each subset is also shown.", "section": "Results"}]