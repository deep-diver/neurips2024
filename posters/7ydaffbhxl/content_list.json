[{"type": "text", "text": "Provably and Practically Efficient Adversarial Imitation Learning with General Function Approximation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tian $\\mathbf{Xu}^{*1,2,3}$ , Zhilong Zhang\\*12.3, Ruishuo Chenf12, Yihao $\\mathbf{Sun}^{1,2}$ , and Yang $\\mathbf{Y}\\mathbf{u}\\ddag^{1,2,3}$ ", "page_idx": 0}, {"type": "text", "text": "1National Key Laboratory for Novel Software Technology, Nanjing University, China 2School of Artificial Intelligence, Nanjing University, China 3Polixir.ai ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As a prominent category of imitation learning methods, adversarial imitation learning (AIL) has garnered significant practical success powered by neural network approximation. However, existing theoretical studies on AIL are primarily limited to simplified scenarios such as tabular and linear function approximation and involve complex algorithmic designs that hinder practical implementation, highlighting a gap between theory and practice. In this paper, we explore the theoretical underpinnings of online AIL with general function approximation. We introduce a new method called optimization-based AIL (OPT-AIL), which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. Theoretically, we prove that OPT-AIL achieves polynomial expert sample complexity and interaction complexity for learning near-expert policies. To our best knowledge, OPT-AIL is the first provably efficient AIL method with general function approximation. Practically, OPT-AIL only requires the approximate optimization of two objectives, thereby facilitating practical implementation. Empirical studies demonstrate that OPT-AIL outperforms previous state-of-the-art deep AIL methods in several challenging tasks.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sequential decision-making tasks are prevalent in real-world applications, where agents seek policies that maximize long-term returns. Reinforcement learning (RL) [49] provides a well-known framework for developing effective policies through trial and error. However, RL often necessitates carefully designed reward functions and typically requires millions of interactions with the environment to achieve satisfactory performance [36, 20]. In contrast, imitation learning (IL) offers a more sampleefficient approach to learning effective policies by mimicking expert demonstrations, bypassing the need for explicit reward functions. As a result, IL has gained popularity and demonstrated success in a wide range of real-world applications such as recommendation systems [11, 46] and generalist robot learning [10, 35]. ", "page_idx": 0}, {"type": "text", "text": "IL encompasses two main categories of methods: behavioral cloning (BC) and adversarial imitation learning (AIL). BC employs supervised learning to directly infer expert policies from demonstration data [39, 44, 9]. In contrast, AIL utilizes an adversarial learning process to replicate the expert's state-action distribution. This process involves the learner recovering an adversarial reward to maximize the policy value gap and subsequently learning a policy that minimizes this gap under the recovered reward. Building on these foundational principles, numerous practical algorithms have been developed [55, 27, 9, 22, 26, 16, 15, 34, 30], achieving significant empirical advancements. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "From these empirical advances, a notable observation is that AIL often significantly outperforms BC [16, 26, 27, 15]. To better understand this phenomenon, recent research has focused on the theoretical underpinnings of AIL [61, 45, 63, 33, 64, 57], particularly in the online setting. This research examines both expert sample complexity (the number of expert trajectories required) and interaction complexity (the number of trajectories needed when interacting with the environment), both of which are crucial for practical applications. In the tabular setting, the best-known complexity result is achieved in [64]. They developed the MB-TAIL algorithm, which leverages advanced distribution estimation, achieving the expert sample complexity $\\widetilde{\\mathcal{O}}(H^{3/2}|S|/\\varepsilon)$ and interaction complexity $\\widetilde{\\mathcal{O}}(H^{3}|S|^{2}|A|/\\varepsilon^{2})$ , where $|{\\mathcal{S}}|$ and $|{\\mathcal{A}}|$ are the state space size and action space size, respectively, $H$ is the horizon length and $\\varepsilon$ is the desired value gap. Furthermore, [33, 57] investigated the AIL theory in the linear function approximation setting. Notably, the BRIG approach proposed in [57] uses linear regression for policy evaluation and achieves the expert sample complexity $\\widetilde{\\mathcal{O}}(H^{2}d/\\varepsilon^{2})$ and interaction complexity $\\tilde{\\mathcal{O}}(H^{4}d^{3}/\\varepsilon^{2})$ , where $d$ is the feature dimension. For a complete summary of related results, please refer to Table 1. ", "page_idx": 1}, {"type": "text", "text": "Despite substantial theoretical advances, there still exists a gap between theory and practice in AIL. First, prior theoretical analysis primarily focuses on restricted settings such as tabular [41, 45, 64] or linear function approximation [33, 57], which deviate from practice where AIL approaches often operate with general function approximation (e.g., neural network approximation). Besides, most previous theoretical works involve algorithmic designs such as count-based [45, 64] or covariancematrix-based [33, 57] bonuses, which are tailored to their respective settings. Implementing such algorithmic designs in practical settings, where neural network approximation is employed, presents significant challenges [65, 54]. ", "page_idx": 1}, {"type": "table", "img_path": "7YdafFbhxL/tmp/d1b67643dfc41de20c600ab84c00146dda8b1179446b212cb18f38f0b85c9d9d.jpg", "table_caption": ["Table 1: A summary of the expert sample complexity and interaction complexity. Here $H$ is the horizon length, $\\varepsilon$ is the desired imitation gap, $|{\\mathcal{S}}|$ is the state space size, $|{\\mathcal{A}}|$ is the action space size, $|\\Pi|$ is the cardinality of the finite policy class $\\Pi$ $d$ is the dimension of the feature space, $d_{\\mathrm{GEC}}$ is the generalized eluder coefficient, $\\bar{\\mathcal{N}}(\\mathcal{R}_{h})\\,\\$ and ${\\mathcal{N}}(\\mathcal{Q}_{h})$ are the covering numbers of the reward class $\\mathcal{R}_{h}$ and Q-value class $\\mathcal{Q}_{h}$ , respectively. We use $\\widetilde O$ to hide logarithmic factors.2 "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Contribution. This paper aims to bridge the gap between theory and practice in AIL by developing a provably efficient algorithm with general function approximation and providing a practical implementation equipped with neural networks. ", "page_idx": 1}, {"type": "text", "text": "First, we introduce a new AIL approach called optimization-based adversarial imitation learning (OPT-AIL) and provide a comprehensive theoretical analysis for general function approximation. The core of OPT-AIL involves minimizing two key objectives. To recover the reward, OPT-AIL solves an online optimization problem using a no-regret approach. For policy learning, inspired by [32], OPT-AIL infers the Q-value functions by minimizing the optimism-regularized Bellman error and then derives the corresponding greedy policies. Under mild assumptions, we prove that OPT-AIL achieves the expert sample complexity $\\tilde{\\tilde{\\mathcal{O}}}(H^{2}\\log(\\operatorname*{max}_{h\\in[H]}\\mathcal{N}(\\mathcal{R}_{h}))/\\varepsilon^{2})$ and interaction complexity $\\widetilde{\\mathcal{O}}((H^{4}d_{\\mathrm{GEC}}\\log(\\operatorname*{max}_{h\\in[H]}\\mathcal{N}(\\mathcal{Q}_{h})\\mathcal{N}(\\mathcal{R}_{h}))+H^{2})/\\varepsilon^{2})$ . Here $d_{\\mathrm{GEC}}$ is the generalized eluder coeffcient, originally proposed in [68] to measure the complexity of RL with function approximation, which we adapt to the AIL setting. $\\mathcal{N}(\\mathcal{R}_{h})$ and ${\\mathcal{N}}(\\mathcal{Q}_{h})$ are the covering numbers of the reward class $\\mathcal{R}_{h}$ and $Q\\cdot$ value class ${\\mathcal{Q}}_{h}$ , respectively. To our best knowledge, OPT-AIL is the first provably efficient AIL approach with general function approximation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Furthermore, we offer a practical implementation of OPT-AIL, demonstrating its competitive performance on standard benchmarks. Notably, OPT-AIL only requires the approximate optimization of two objectives, simplifying its practical implementation with deep neural networks. Leveraging this advantage, we implement OPT-AIL using neural network approximations and compare its performance against prior state-of-the-art (SOTA) deep AIL methods, which often lack theoretical guarantees. Experimental results indicate that OPT-AIL outperforms SOTA deep AIL approaches on several challenging tasks within the DMControl benchmark. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Adversarial Imitation Learning.  The theoretical foundations of AIL have been extensively explored in numerous studies [1, 52, 48, 60, 67,41, 45, 33, 40, 63, 50, 56, 64, 57]. Early research [1, 52, 48, 41, 60, 67, 62, 50, 56] focused on ideal settings where the transition function is known or an exploratory data distribution is available, primarily addressing expert sample effciency. Notably, under mild conditions, [63] proved that AIL can achieve a horizon-free imitation gap bound $\\mathcal{O}(\\operatorname*{min}\\{1,\\sqrt{|S|/N}\\})$ ,where $N$ denotes the number of expert trajectories. Recently, a new research direction has emerged that addresses more practical scenarios, specifically online AIL with unknown transitions [45, 33, 64, 57]. This line of work investigates both expert sample complexity and interaction complexity. These recent advancements were discussed in the previous section and thus will not be reiterated here. Most existing theoretical works focus on either tabular [41, 45, 64] or linear function approximation settings [33, 57], and often lack practical implementations due to algorithmic designs tailored to specific settings. In contrast, this work simultaneously provides theoretical guarantees for general function approximation and offers a practical implementation that demonstrates competitive performance. ", "page_idx": 2}, {"type": "text", "text": "On the empirical side, there has been extensive research [19, 27, 28, 16, 26, 15] developing practical AIL approaches that leverage general function (or neural network) approximation. A seminal method in this field is generative adversarial imitation learning (GAIL) [19]. In GAIL, a discriminator is trained to distinguish between samples from expert demonstrations and those generated by a policy, while the policy (or generator) learns to maximize the reward signal provided by the discriminator. More recently, [15] proposed inverse Q-Learning (IQLearn), which achieves SOTA performance across a diverse set of tasks. However, these practical methods often lack rigorous theoretical guarantees for general function approximation. ", "page_idx": 2}, {"type": "text", "text": "General Function Approximation in Reinforcement Learning. Our work is closely related to a body of research focused on general function approximation in RL [38, 21, 59, 23, 32]. Notably, [32] proposed an algorithmic framework that incorporates a unified objective to balance exploration and exploitation in RL, demonstrating a sublinear regret bound. In this paper, we adapt this algorithmic design to address several RL sub-problems within the context of AIL. Unlike the RL setting, where a fixed reward is provided in advance, AIL involves inferring the reward function from expert demonstrations and environment interactions collected by the learning policies. Therefore, our work requires developing a theoretical analysis for the joint learning process of both rewards and policies, highlighting a unique challenge in AIL compared to traditional RL. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Markov Decision Process. In this paper, we consider episodic Markov Decision Processes (MDPs), represented by the tuple $\\mathcal{M}=(\\bar{S_{*}}\\bar{\\mathcal{A}},P,r^{\\mathrm{{true}}},H,s_{1})$ . Here, $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ denote the state and action spaces, respectively. $H$ signifies the planning horizon, while $s_{1}$ stands for the fixed initial state. The set $P=\\{\\bar{P}_{1},\\dots,\\bar{P}_{H}\\}$ characterizes the non-stationary transition function of this MDP. Specifically, $P_{h}(s_{h+1}|s_{h},a_{h})$ determines the probability of transiting to state $s_{h+1}$ given state $s_{h}$ and action $a_{h}$ at $h$ $h\\in[H]$ $\\boldsymbol{r}^{\\mathrm{true}}=\\{r_{1}^{\\mathrm{true}},\\ldots,r_{H}^{\\mathrm{true}}\\}$ MDP. Without loss of generality, we assume rtr $r_{h}^{\\mathrm{true}}:S\\times A\\rightarrow[0,1]$ $h\\in[H]$ ", "page_idx": 2}, {"type": "text", "text": "policy is denoted by $\\pi=\\{\\pi_{1},\\ldots,\\pi_{H}\\}$ with $\\pi_{h}:S\\to\\Delta(A)$ , where $\\Delta(A)$ denotes the probability simplex. Here, $\\pi_{h}(a|s)$ represents the probability of selecting action $a$ in state $s$ at time step $h$ , for $h\\in[H]$ ", "page_idx": 3}, {"type": "text", "text": "The quality of policy $\\pi$ is evaluated by policy value: ", "page_idx": 3}, {"type": "equation", "text": "$$\nV^{\\pi}=\\mathbb{E}\\left[\\sum_{h=1}^{H}r_{h}^{\\mathrm{true}}(s_{h},a_{h})\\bigg|a_{h}\\sim\\pi_{h}(\\cdot|s_{h}),s_{h+1}\\sim P_{h}(\\cdot|s_{h},a_{h}),\\forall h\\in[H]\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We denote the Q-value function of policy $\\pi$ at time step $h$ as $Q_{h}^{\\pi}:S\\times A\\rightarrow\\mathbb{R}$ where $Q_{h}^{\\pi}(s,a)=$ $\\begin{array}{r}{\\mathbb{E}_{\\boldsymbol\\pi}[\\sum_{\\ell=h}^{H}r_{\\ell}^{\\mathrm{true}}(s_{\\ell},a_{\\ell})|s_{h}=s,a_{h}=a]}\\end{array}$ The optimalQ-valuefntion $Q_{h}^{\\star}:S\\times A\\rightarrow\\mathbb{R}$ isdeined as $\\begin{array}{r}{Q_{h}^{\\star}(s,a):=\\operatorname*{sup}_{\\pi\\in\\Pi}Q_{h}^{\\pi}(s,a)}\\end{array}$ . It is known that $Q_{h}^{\\star}$ is the fixed point of Bellman operator $\\mathcal{T}_{h}$ $\\begin{array}{r}{\\mathfrak{r}(\\bar{s},a)=(\\mathcal{T}_{h}Q_{h+1}^{\\star}\\bar{)}(s,\\bar{a}):=r_{h}^{\\mathrm{true}}(s,a)+\\mathbb{E}_{s^{\\prime}\\sim P_{h}(\\cdot\\vert s,a)}[\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}Q_{h+1}^{\\star}(s^{\\prime},a^{\\prime})]}\\end{array}$ . In other words, $Q^{\\star}$ has zero Bellman error, i.e., $Q_{h}^{\\star}(s,a)-(T_{h}Q_{h+1}^{\\star})(s,a)=0$ ", "page_idx": 3}, {"type": "text", "text": "Imitation Learning. The essence of $\\mathrm{IL}$ lies in acquiring a high-quality policy without the reward function $r^{\\mathrm{true}}$ . In pursuit of this objective, we typically posit the existence of a near-optimal expert policy $\\pi^{\\mathrm{E}}$ capable of interacting with the environment to generate a dataset, comprising $N$ trajectories each of length $H\\colon\\mathcal{D}^{\\mathrm{E}}=\\left\\{\\tau=(s_{1},a_{1},s_{2},a_{2},\\ldots,\\bar{s}_{H},a_{H})\\,;a_{h}\\,\\sim\\,\\pi_{h}^{\\mathrm{E}}(\\cdot|s_{h}\\bar{)},s_{h+1}\\,\\sim\\,\\pi_{h}^{\\mathrm{E}}(\\cdot|s_{h}\\bar{)},s_{h+1}\\,\\sim\\,\\pi_{h}^{\\mathrm{E}}(\\cdot|s_{h}\\bar{)},s_{h+1}\\,\\sim\\,\\pi_{h}^{\\mathrm{E}}(\\cdot|s_{h}\\bar{)}\\right\\}.$ $P_{h}(\\cdot|s_{h},a_{h}),\\forall h\\in[H]\\}$ . Subsequently, the learner leverages this dataset $\\mathcal{D}^{\\mathrm{E}}$ to mimic the behavior of the expert and thereby derives an effective policy. The quality of this imitation is measured by the imitation gap [1, 43, 41]: $V^{\\pi^{\\mathrm{E}}}-V^{\\pi}$ ,where $\\pi$ represents the learned policy. Essentially, we hope that the learned policy can perfectly mimic the expert such that the imitation gap is small. ", "page_idx": 3}, {"type": "text", "text": "AIL is a prominent class of $\\mathrm{IL}$ methods that imitates expert behavior through an adversarial learning process dened by minmax, V , where $V_{r}^{\\pi}$ denotes thevalue of policy $\\pi$ under reward $r$ In this framework, AIL infers a reward function that maximizes the value gap between the expert policy and the learning policy. Subsequently, it learns a policy that minimizes this value gap using the inferred reward. Essentially, AIL involves solving several RL sub-problems, as the outer optimization problem concerning the policy is equivalent to an RL problem under the inferred reward $r$ ", "page_idx": 3}, {"type": "text", "text": "General Function Approximation. This work considers AIL with general function approximation. In this setup, the learner first has access to a reward class $\\mathcal{R}\\,=\\,\\mathcal{R}_{1}\\,\\times\\,\\mathcal{R}_{2}\\,\\times\\,.\\,.\\,\\times\\,\\mathcal{R}_{H}$ With $\\forall h\\in[H],\\bar{\\mathcal{R}}_{h}\\subseteq(S\\times A\\rightarrow[0,1])$ to infer the reward. We assume that $\\mathcal{R}$ captures the unknown true reward. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Realizability of $\\mathcal{R}$ ). The unknown true reward lies in the reward class i. $r^{\\mathrm{true}}\\in{\\mathcal{R}}$ ", "page_idx": 3}, {"type": "text", "text": "Besides, the learner also has access to a $\\mathrm{^Q}$ -value function class $\\mathcal{Q}=\\mathcal{Q}_{1}\\times\\mathcal{Q}_{2}\\times...\\times\\mathcal{Q}_{H}$ with $\\forall h\\,\\in\\,[H],\\mathcal{Q}_{h}\\,\\subseteq\\,(\\mathcal{S}\\times\\mathcal{A}\\,\\to\\,[0,H])$ , which is used for solving several RL sub-problems under different inferred rewards in AIL. Since there is no reward in the $H\\!+\\!1$ step, we always set $Q_{H+1}\\equiv0$ Below, we present two standard assumptions about the function class $\\mathcal{Q}$ that are commonly adopted in the literature of RL with function approximation [59, 23, 68, 32]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Realizability of $\\mathcal{Q}$ 0. For reward $r\\in\\mathcal{R}$ $Q^{\\star,r}\\in\\mathcal{Q},$ where $Q^{\\star,r}$ denotes the optimal $Q$ -value function underreward $r$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 3 (Bellman Completeness of $\\mathcal{Q}$ ). For reward $r\\in\\mathcal{R}$ $T_{h}^{r}\\,{\\mathcal{Q}}_{h+1}\\subseteq{\\mathcal{Q}}_{h}$ \uff0c $\\forall h\\in[H]$ where $\\mathcal{T}_{h}^{r}$ denotes the Bellman operator under reward $r$ and $T_{h}^{r}\\mathscr{Q}_{h+1}=\\{T_{h}^{r}Q_{h+1}:Q_{h+1}\\in\\mathscr{Q}_{h+1}\\}$ ", "page_idx": 3}, {"type": "text", "text": "In short, Assumption 2 states that the Q-value class $\\mathcal{Q}$ should capture the optimal Q-value function, while Assumption 3 indicates the closeness of $\\mathcal{Q}$ under Bellman update. It is easy to verify that Assumptions 1, 2 and 3 are more general than the tabular MDP [45, 64], linear mixture MDP [33] and linear MDP [57] assumptions used in previous works. ", "page_idx": 3}, {"type": "text", "text": "When the function class contains a finite number of elements, its cardinality can be used to quantify its \"size\". However, for general function approximation, where the function class may contain an infinite number of elements, we utilize the standard $\\varepsilon$ -covering number [58] to measure its complexity. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 $\\overline{{\\varepsilon}}$ -covering number).For function class $\\mathcal{F}\\subseteq(\\mathcal{X}\\rightarrow\\mathbb{R})$ ,the $\\varepsilon$ -coveringnumberof $\\mathcal{F}$ denoted as $\\mathcal{N}_{\\varepsilon}(\\mathcal{F})$ ,is defined as the minimum integer n such that there exists a finite subset ${\\mathcal{F}}^{\\prime}\\subseteq{\\mathcal{F}}$ with $|{\\mathcal{F}}^{\\prime}|\\,=\\,n$ such that for anyfunction $f\\,\\in\\,{\\mathcal{F}}$ there exists $f^{\\prime}\\,\\in\\,{\\mathcal{F}}^{\\prime}$ satisfying that $\\begin{array}{r}{\\operatorname*{max}_{x\\in{\\mathcal{X}}}|f(x)-{\\dot{f}}^{\\prime}(x)|\\leq\\varepsilon}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "4  Optimization-based Adversarial Imitation Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce a provably efficient method called Optimization-Based Adversarial Imitation Learning (OPT-AIL). In Section 4.1, we delve into the core components of OPT-AIL, which involves online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. We discuss the underlying principles and provide theoretical guarantes with general function approximation. Thanks to its easy-to-implement merit, we provide a practical implementation of OPT-AIL using stochastic-gradient-based methods in Section 4.2. ", "page_idx": 4}, {"type": "text", "text": "4.1  Theoretical Analysis of OPT-AIL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our provably efficient method OPT-AIL with general function approximation; see Algorithm 1 for an overview. To start with, recall that our theoretical goal is to ensure the algorithm can output a policy with $\\varepsilon$ -imitation gap by using finite expert samples and environment interactions. To obtain the final policy, we leverage the standard online-to-batch conversion technique [37]. Specifically, during the learning process, the learner iteratively generates a sequence of rewards $\\bar{\\{r^{k}\\}}_{k=1}^{K}$ and policies andoutputsthepolicythatisuniformlysampled from To analyze the imitation gap of $\\overline{{\\pi}}$ , we leverage the following standard error decomposition lemma. ", "page_idx": 4}, {"type": "text", "text": "Lemma1.Cnieseqfr $\\{r^{k}\\}_{k=1}^{K}$ and polies $\\{\\pi^{k}\\}_{k=1}^{K}$ and the poliey $\\overline{{\\pi}}$ uniformly sampled from $\\{\\pi^{k}\\}_{k=1}^{K}$ . Then it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\nV^{\\pi^{\\mathrm{E}}}-V^{\\pi}=\\frac{1}{K}\\sum_{k=1}^{K}\\left(V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-V_{r^{\\mathrm{true}}}^{\\pi^{k}}-\\left(V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{k}}\\right)\\right)+\\frac{1}{K}\\sum_{k=1}^{K}\\left(V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{k}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Lemma 1 suggests that to achieve a small imitation gap, it is crucial to control both reward error and policy error. Specifically, reward error quantifies the distance between the true reward $r^{\\mathrm{true}}$ and the learned reward $r^{k}$ through the imitation gap. Besides, policy error measures the value difference between the expert policy $\\pi^{\\mathrm{E}}$ and the learned policy $\\pi^{k}$ under the inferred reward $r^{k}$ . Notably, this policy error differs from the concept of regret in RL [23, 32], where the reward is fixed. ", "page_idx": 4}, {"type": "text", "text": "To theoretically minimize the reward error and policy error, we consider an iterative approach, in which each iteration first updates the reward and subsequently derives the policy. The subsequent parts detail the reward and policy updates, which involve solving two optimization problems. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Optimization-based Adversarial Imitation Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: Reward class $\\mathcal{R}$ , Q-value class $\\mathcal{Q}$ , initialized reward $r^{0}$ , policy $\\pi^{0}$ and dataset $\\mathcal{D}^{0}=\\varnothing$   \n1: for $k=1,2,\\ldots,K$ do   \n2:Apply $\\pi^{k-1}$ to rll out a trajectory $\\tau^{k-1}$ and append it to the dataset ${\\mathcal{D}}^{k}={\\mathcal{D}}^{k-1}\\cup\\{\\tau^{k-1}\\}$   \n3:Obtain $r^{k}$ by running a no-regret algorithm to solve the online optimization problem with observed lossfunctions $\\{\\mathcal{L}^{i}(\\boldsymbol{r})\\}_{i=0}^{k-1}$ up to an eror $\\varepsilon_{\\mathrm{opt}}^{r}$ where $\\begin{array}{r}{\\mathcal{L}^{i}(r)=\\widehat{V}_{r}^{\\pi^{i}}-\\widehat{V}_{r}^{\\pi^{\\mathrm{E}}}}\\end{array}$   \n4:Obtain $Q^{k}$ by solving the following optimization problem up to an error $\\varepsilon_{\\mathrm{opt}}^{Q}$ $\\operatorname*{min}_{Q\\in\\mathcal{Q}}\\mathcal{L}^{k}(Q):=\\mathrm{BE}^{k}(Q)-\\lambda\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{1}\\big(s_{1},a\\big),$ where $\\begin{array}{r}{\\mathrm{BE}^{k}(Q)=\\sum_{h=1}^{H}\\mathcal{E}_{h}(Q_{h},Q_{h+1};\\mathcal{D}^{k},r^{k})-\\operatorname*{inf}_{Q_{h}^{\\prime}\\in\\mathcal{Q}_{h}}\\mathcal{E}_{h}(Q_{h}^{\\prime},Q_{h+1};\\mathcal{D}^{k},r^{k}).}\\end{array}$   \n5:Obtain $\\pi^{k}$ by $\\pi_{h}^{k}(s)=\\operatorname{argmax}_{a\\in\\mathcal{A}}Q_{h}^{k}(s,a)$ ", "page_idx": 4}, {"type": "text", "text": "6: end for ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Output: $\\overline{{\\pi}}$ sampled uniformly from $\\{\\pi^{k}\\}_{k=1}^{K}$ ", "page_idx": 4}, {"type": "text", "text": "Reward Update via Online Optimization (Line 3 in Algorithm 1). The goal of this step is to control the reward error. More concretely, in iteration $k$ , we aim to learn a reward $r^{k}$ such that the error $V_{r^{k}}^{\\pi^{k}}-V_{r^{k}}^{\\pi^{\\mathrm{E}}}-(V_{r^{\\mathrm{true}}}^{\\pi^{k}}-V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}})$ ismallfla tsprsannmzatr. In teration $k$ using the previously oserved lossfunctions $\\{V_{r}^{\\pi^{i}}-V_{r}^{\\pi^{\\mathrm{E}}}\\}_{i=0}^{k-1}$ , thereward arner selects $r^{k}$ and then observes the current loss function $V_{r}^{\\pi^{k}}-V_{r}^{\\pi^{\\mathrm{E}}}$ . Moreover, since the previous expected loss functions $\\{V_{r}^{\\pi^{i}}-V_{r}^{\\pi^{\\mathrm{E}}}\\}_{i=0}^{k-1}$ /T j=0 are not available, we instead minimize the estimated loss functions. In particular, we leverage expert demonstrations $\\mathcal{D}^{\\mathrm{E}}$ and the trajectory $\\tau^{i}$ collected by policy $\\pi^{i}$ to establish an ubiased estimation $\\begin{array}{r}{\\mathcal L^{i}(r)=\\widehat V_{r}^{\\pi^{i}}-\\widehat V_{r}^{\\pi^{\\mathrm E}}}\\end{array}$ for $V_{r}^{\\pi^{i}}-V_{r}^{\\pi^{\\mathrm{E}}}$ , where ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{V}_{r}^{\\pi^{i}}=\\sum_{h=1}^{H}r_{h}(s_{h}^{i},a_{h}^{i}),\\:\\widehat{V}_{r}^{\\pi^{\\mathtt{E}}}=\\frac{1}{N}\\sum_{\\tau\\in\\mathcal{D}^{\\mathtt{E}}}\\sum_{h=1}^{H}r_{h}(\\tau(s_{h}),\\tau(a_{h})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $(\\tau(s_{h}),\\tau(a_{h}))$ is the state-action pair of trajectory $\\tau$ visited in time step $h$ and $\\tau^{i}{\\mathrm{~\\,~}}=$ $\\{s_{1}^{i},a_{1}^{i},\\ldots,s_{H}^{i},a_{H}^{i}\\}$ is the trajectory collected by policy $\\pi^{i}$ : The ultimate goal of the reward leaneristo minimizethecumultiv losses $\\begin{array}{r}{\\sum_{k=1}^{K}\\widehat{V}_{\\underline{{r}}^{k}}^{\\pi^{k}}-\\widehat{V}_{\\underline{{r}}^{k}}^{\\pi^{\\mathrm{E}}}}\\end{array}$ To achieve thisgoalwey a no-regret algorithm [18]. In the following part, we formally define the reward optimization error resulting from running the no-regret algorithm. ", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Reward Optimization Error). For any sequence of policies $\\{\\pi^{k}\\}_{k=1}^{K}$ ,the no-regret reward optimization algorithm sequentially outputs rewards $r^{1},\\ldots,r^{K}$ The reward optimization error $\\varepsilon_{\\mathrm{opt}}^{r}$ is defined as $\\begin{array}{r}{\\varepsilon_{\\mathrm{opt}}^{r}:=(1/K)\\cdot\\operatorname*{max}_{r\\in\\mathcal{R}}\\sum_{k=1}^{K}\\widehat{V}_{r^{k}}^{\\pi^{k}}-\\widehat{V}_{r^{k}}^{\\pi^{\\mathrm{E}}}-(\\widehat{V}_{r}^{\\pi^{k}}-\\widehat{V}_{r}^{\\pi^{\\mathrm{E}}})}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "The reward optimization error, as defined above, aligns with the standard average regret in online optimization [18], a concept not extensively explored in the context of AIL. When the loss functions $\\{\\dot{C}^{k}(r)\\}_{k=0}^{K}$ are convex functions and thereward clas $\\mathcal{R}$ is a convex set, we can apply online projected gradient descent [18] as the no-regret algorithm, which ensures the reward optimization error $\\varepsilon_{\\mathrm{opt}}^{r}={\\mathcal O}(1/{\\sqrt{K}})$ . As for non-convex functions and sets, employing Follow-the-PerturbedLeader can similarly achieve $\\varepsilon_{\\mathrm{opt}}^{r}={\\mathcal O}(1/{\\sqrt{K}})$ [47]. ", "page_idx": 5}, {"type": "text", "text": "Policy Update via Optimism-Regularized Bellman-error Minimization (Lines 4-5 in Algorithm 1). The target of policy updates is to control the policy error. In iteration $k$ , the policy learner aims to recover a policy $\\pi^{k}$ such that the policy error $V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{k}}$ is small, where $r^{k}$ is the recently recovered reward function. This is essentially an RL problem under reward function $r^{k}$ . Building upon [32], we leverage a model-free approach, based on Bellman error minimization, to solve this RL sub-problem. In particular, we first learn Q-value functions by solving the optimization problem of ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}}\\mathcal{L}^{k}(Q):=\\mathrm{BE}^{k}(Q)-\\lambda\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{1}\\big(s_{1},a\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{with~BE}^{k}(Q)=\\sum_{h=1}^{H}\\mathcal{E}_{h}(Q_{h},Q_{h+1};\\mathcal{D}^{k},r^{k})-\\operatorname*{inf}_{Q_{h}^{\\prime}\\in Q_{h}}\\mathcal{E}_{h}(Q_{h}^{\\prime},Q_{h+1};\\mathcal{D}^{k},r^{k}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\xi_{h}(Q_{h},Q_{h+1};\\mathcal{D}^{k},r^{k})\\;=\\;\\sum_{i=0}^{k-1}(Q_{h}(s_{h}^{i},a_{h}^{i})\\,-\\,r_{h}^{k}\\,-\\,\\operatorname*{max}_{a^{\\prime}\\in A}Q_{h+1}(s_{h+1}^{i},a^{\\prime}))^{2},}\\end{array}$ $\\mathcal{D}^{k}\\ =$ $\\{\\tau^{i}\\}_{i=0}^{k-1}$ with $\\tau^{i}\\,=\\,\\{s_{1}^{i},a_{1}^{i},\\dots,s_{H}^{i},a_{H}^{i}\\}$ and $\\lambda\\,>\\,0$ is the regularization coefcient. As shown in [5, 23], $\\mathrm{BE}^{k}(Q)$ is the estimated squared Bellman error of $Q$ with respect to reward $r^{k}$ and dataset $\\mathcal{D}^{k}$ . In this optimization problem, the main objective $\\mathrm{BE}^{k}(Q)$ ensures a small Bellman error while the regularization term $\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{1}(s_{1},a)$ tends to search an optimistic $\\mathrm{^Q}$ -value function for encouraging exploration. It is worth noting that Algorithm 1 only requires approximately solving the optimization problem up to an error $\\varepsilon_{\\mathrm{opt}}^{Q}$ with $\\begin{array}{r}{\\varepsilon_{\\mathrm{opt}}^{Q}=\\mathcal{L}^{k}(Q^{k})-\\operatorname*{min}_{Q\\in\\mathcal{Q}}\\mathcal{L}^{k}(Q)}\\end{array}$ . After obtaining the Q-value function $Q^{k}$ , we derive $\\pi^{k}$ as the greedy policy of $Q^{k}$ ", "page_idx": 5}, {"type": "text", "text": "Theoretical Guarantee of OPT-AIL. In the above part, we have explained the algorithmic mechanisms of OPT-AIL. Now we present the theoretical guarantee. To ensure the sample efficiency of solving RL sub-problems within AIL, we make a structural assumption on the underlying MDP. In particular, we assume that the MDP has a small generalized eluder coefficient. This coefficient, introduced in [68], quantifies the inherent difficulty of learning the MDP with function approximation in RL. We adapt this concept to AIL where the reward function is changing. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4 (Low generalized eluder coefficient [68]). We assume that given an $\\varepsilon\\:>\\:0$ the generalized eludercoefficient $d_{\\mathrm{GEC}}(\\varepsilon)$ is the smallest $d$ $'d\\ge0,$ such that for any sequence of $\\left\\{r^{k}\\right\\}_{k=1}^{K}\\subseteq\\mathcal{R}_{k}$ $\\{Q^{k}\\}_{k=1}^{K}\\subseteq\\mathcal{Q}$ andthecorresondinredypolce $\\{\\pi^{k}\\}_{k=1}^{K}$ \uff0c ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=1}^{K}Q_{1}^{k}(s_{1},\\pi^{k})-V_{r^{k}}^{\\pi^{k}}\\le\\operatorname*{inf}_{\\mu\\ge0}\\displaystyle\\frac{\\mu}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{k-1}\\mathbb{E}\\left[\\displaystyle\\sum_{h=1}^{H}\\left(Q_{h}^{k}(s_{h},a_{h})-T_{h}^{r^{k}}Q_{h+1}^{k}(s_{h},a_{h})\\right)^{2}\\Big|\\pi^{i}\\right]+\\displaystyle\\frac{d}{2\\mu}\\sum_{k=1}^{K}Q_{h+1}^{k}(s_{h},\\pi^{k})}&{{}}\\\\ {+\\displaystyle\\sqrt{d H K}+\\varepsilon H K,}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Q_{1}^{k}(s_{1},\\pi^{k}):=\\mathbb{E}_{a_{1}\\sim\\pi_{1}^{k}(\\cdot|s_{1})}[Q_{1}^{k}(s_{1},a_{1})].$ ", "page_idx": 5}, {"type": "text", "text": "Intuitively, a low generalized eluder coefficient ensures that the prediction error $Q_{1}^{k}(s_{1},\\pi^{k})-V_{r^{k}}^{\\pi^{k}}$ for $\\pi^{k}$ can be effectively controlled by the Bellman eror on the dataset generated by historical policies $\\{\\pi^{i}\\}_{i=1}^{k-1}$   \n[24] and MDPs with low Bellman eluder dimension [23]. Now we are ready to present the theoretical guarantee of OPT-AIL. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Under Assumptions 1, 2, 3 and 4. For any fixed $\\varepsilon\\in(0,1]$ and $\\delta\\,\\in\\,(0,1]$ consider Algorithm $^{\\,l}$ with $\\lambda=c_{1}\\sqrt{(K H^{3}\\log(4K H\\mathcal{N}_{\\rho}(\\mathcal{Q})\\mathcal{N}_{\\rho}(\\mathcal{R})/\\delta)+K^{2}H^{3}\\rho)/d_{\\mathrm{GEC}}}$ where $d_{\\mathrm{GEC}}:=$ $d_{\\mathrm{GEC}}(\\varepsilon/H)$ $\\rho:=c_{2}\\varepsilon^{2}/(H^{2}d_{\\mathrm{GEC}}+H)$ \uff0c $c_{1}$ and $c_{2}$ are absolute constants. Then with probability atleast $1-\\delta$ we have that $V^{\\pi^{\\mathrm{E}}}-V^{\\overline{{\\pi}}}\\leq\\varepsilon+\\varepsilon_{\\mathrm{opt}}^{r}+(\\varepsilon_{\\mathrm{opt}}^{Q}/\\lambda)$ if thexpet sampecomplexiry and interaction complexity satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N\\gtrsim\\frac{H^{2}\\log(\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)}{\\varepsilon^{2}},}\\\\ &{K\\gtrsim\\frac{H^{4}d_{\\mathrm{GEC}}\\log(H d_{\\mathrm{GEC}}\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/(\\delta\\varepsilon))+H^{2}\\log(1/\\delta)}{\\varepsilon^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 1 can be found in Appendix B.2. Theorem 1 indicates that when $d_{\\mathrm{GEC}}=\\Omega(1)$ OPT-AIL achieves the expert sample complexity $\\begin{array}{r}{\\widetilde{\\mathcal{O}}(H^{2}\\log(\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{R}_{h}))/\\varepsilon^{2})}\\end{array}$ and interaction complexity $\\widetilde{\\mathcal{O}}(H^{4}d_{\\mathrm{GEC}}\\log(\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h}))/\\varepsilon^{2})$ in the general function approximation setting. To our best knowledge, OPT-AIL is the first provably efficient online AIL algorithm for general function approximation. ", "page_idx": 6}, {"type": "text", "text": "Notably, OPT-AIL improves over BC [13] by an order of $\\mathcal{O}(H)$ , suggesting that OPT-AIL can still provably mitigate the compounding errors issue in BC for general function approximation. When restricting Theorem 1 to linear MDPs with dimension $d$ [68], OPT-AIL can achieve the expert sample complexity $\\widetilde{\\mathcal{O}}(H^{2}d/\\varepsilon^{2})$ and interaction complexity $\\widetilde{\\cal O}(H^{5}d^{2}/\\varepsilon^{2})$ . Furthermore, when $\\mathcal{Q}$ and $\\mathcal{R}$ are neural network classes commonly employed in practice, we can obtain the corresponding complexity result by plugging the covering number bound of neural networks [7] into Theorem 1. Finally, OPT-AIL only requires the approximate optimization of two objectives, thereby facilitating a practical implementation with neural networks, which will be presented in the next section. ", "page_idx": 6}, {"type": "text", "text": "Although Theorem 1 produces desirable outcomes, it does have some limitations. One of the limitations is that Theorem 1 requires the Bellman completeness condition for the Q-value class (i.e., Assumption 3). Nevertheless, recent advances [4] in RL have developed new techniques to remove this assumption. We leave the extension of these techniques to AIL for future work. ", "page_idx": 6}, {"type": "text", "text": "4.2Practical Implementation of OPT-AIL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide a practical implementation for OPT-AIL, which is based on the stochasticgradient-based methods; see Algorithm 2 for an overview. We elaborate on the practical reward update and policy update in detail as follows. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Practical Implementation of OPT-AIL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: Initialized reward $r^{0}$ , Q-value $Q^{0}$ , target Q-value $\\overline{{Q}}^{0}=Q^{0}$ , policy $\\pi^{0}$ and dataset $\\mathcal{D}^{0}=\\varnothing$   \n1: for $k=1,2,\\ldots,K$ do   \n2:Apply $\\pi^{k-1}$ to roll out a trajectory $\\tau^{k-1}$ and append it to the dataset ${\\mathcal{D}}^{k}={\\mathcal{D}}^{k-1}\\cup\\{\\tau^{k-1}\\}$ \uff1a   \n3: Update the reward function by $r^{k}\\leftarrow r^{k-1}-\\alpha_{r}\\nabla\\ell^{k}(r)$ from Eq. (2).   \n4: Update the Q-value function by $Q^{k}\\leftarrow Q^{k-1}-\\alpha_{Q}\\nabla\\ell^{k}(Q)$ from Eq. (3).   \n5: Update the policy by $\\pi^{k}\\leftarrow\\pi^{k-1}+\\alpha_{\\pi}\\nabla\\ell^{k}(\\pi)$ , where $\\begin{array}{r}{\\ell^{k}(\\pi):=\\mathbb{E}_{\\tau\\sim\\mathcal{D}^{k}}[\\sum_{h=1}^{H}Q_{h}^{k}(s_{h},\\pi)].}\\end{array}$   \n6: UdatetetargetQ-valuebyQQ+(1)   \n7: end for ", "page_idx": 6}, {"type": "text", "text": "Practical Reward Update. Here we detail a practical implementation of the reward update by applying an online optimization approach. Recall that in line 3 of Algorithm 1, a no-regret algorithm is employed to solve the online optimization problem. To implement this mechanism, we choose the classical online optimization approach Follow-the-Regularized-Leader (FTRL) [2] as the no-regret approach. In iteration $k$ , FTRL minimizes the sum of all historical loss functions with a regularization. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{r\\in\\mathcal{R}}\\ell^{k}(r):=\\displaystyle\\sum_{i=0}^{k-1}\\mathcal{L}^{i}(r)+\\beta\\psi(r)}\\\\ &{\\quad\\quad\\quad\\quad=k\\bigg(\\mathbb{E}_{\\tau\\sim\\mathcal{D}^{k}}\\bigg[\\displaystyle\\sum_{h=1}^{H}r_{h}(s_{h}^{i},a_{h}^{i})\\bigg]-\\mathbb{E}_{\\tau\\sim\\mathcal{D}^{\\mathrm{E}}}\\left[\\displaystyle\\sum_{h=1}^{H}r_{h}(s_{h}^{i},a_{h}^{i})\\right]\\bigg)+\\beta\\psi(r),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathbb{E}_{\\mathcal{D}}[\\cdot]$ denotes the empirical distribution of dataset $\\mathcal{D}$ .Here $\\psi(r)$ is the regularization term. In practice, we choose $\\psi(r)$ as the gradient penalty [6] of the reward model, which can help stabilize the learning process [27]. According to Eq. (2), the reward learner aims to maximize the value gap between the expert policy and all previous policies. ", "page_idx": 7}, {"type": "text", "text": "Besides, as indicated in Eq. (2), all historical samples $\\mathcal{D}^{k}$ are utilized for the reward update. This learning style is exactly off-policy reward learning [27, 28]. In particular, applying FTRL for the reward update and off-policy reward learning share the same main objective. Previous practical works [27, 28] found that this off-policy reward learning works well in practice, but could not give an explanation. In this work, we justify this off-policy learning style from an online optimization perspective: performing off-policy learning, which aligns with the FTRL approach, can effectively control the reward optimization error. ", "page_idx": 7}, {"type": "text", "text": "Practical Policy Update. To implement the policy update in practice, we adopt the actor-critic framework [14, 17, 27]. In particular, we maintain a policy model $\\pi$ and a $\\mathrm{^Q}$ valuemodel $Q$ .Recall in line 4 of Algorithm 1, the $\\mathrm{{Q}}.$ -value function is learned by minimizing the optimism-regularized Bellman error. To implement this principle, following [12, 8], we leverage the temporal difference loss [31] of the Q-value model and its delayed target to approximate the theoretical Bellman error. Then we arrive at the following objective. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}}\\ell^{k}(Q):=\\mathbb{E}_{\\tau\\sim\\mathcal{D}^{k}}\\left[\\sum_{h=1}^{H}\\Big(Q_{h}(s_{h},a_{h})-r_{h}^{k}-\\overline{{Q}}_{h+1}^{k-1}(s_{h+1},\\pi^{k-1})\\Big)^{2}\\right]-\\lambda Q_{1}(s_{1},\\pi^{k-1}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here $\\overline{{{Q}}}\\;\\;=\\;\\;\\{\\overline{{{Q}}}_{1},\\ldots,\\overline{{{Q}}}_{H}\\}$ is the delayed target $\\mathrm{^Q}$ -value model. Besides, we define that $\\begin{array}{r}{\\bar{Q}_{h+1}^{k-1}(s_{h+1},\\pi^{k-1}):=\\mathbb{E}_{a^{\\prime}\\sim\\pi_{h+1}^{k-1}(\\cdot|s_{h+1})}[\\overline{{Q}}_{h+1}(s_{h+1},a^{\\prime})]}\\end{array}$ where the previous geedypoliey $\\pi^{k-1}$ is used to approximate the maximum operator [17]. Consequently, we derive the greedy policy by optimizing the objective of $\\begin{array}{r}{\\operatorname*{max}_{\\pi}\\ell^{k}(\\pi):=\\mathbb{E}_{\\tau\\sim\\mathcal{D}^{k}}[\\sum_{h=1}^{H}Q_{h}^{k}(s_{h},\\pi)]}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the expert sample efficiency and environment interaction efficiency of OPT-AIL through experiments. Below, we provide a brief overview of the experimental set-up, with detailed information available in Appendix C due to space constraints. ", "page_idx": 7}, {"type": "text", "text": "5.1  Experiment Set-up ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Environment. We conduct experiments on 8 tasks sourced from the feature-based DMControl benchmark [53], a leading benchmark in IL that offers a diverse set of continuous control tasks. For each task, we adopt online DrQ-v2 [66] to train an agent with sufficient environment interactions and regard the resultant policy as the expert policy. Then we roll out this expert policy to collect expert demonstrations. Each algorithm is tested over five trials with different random seeds, and in each run, we evaluate the policy return using Monte Carlo approximation with 10 trajectories. ", "page_idx": 7}, {"type": "text", "text": "Baselines. Existing theoretical AIL approaches like MB-TAIL [64] and OGAIL [33] include countbased or covariance-based bonuses, making it challenging to implement these designs when using neural network approximations. Thus, we do not include these methods in our experiments. Instead, we compare OPT-AIL with prior deep IL methods, including BC [39], IQLearn [15], PPIL [56], FILTER [51] and HyPE [42], despite that most of them lack theoretical guarantees. Notably, IQLearn, FILTER and HyPE represent prior SOTA deep AIL approaches. To ensure a fair comparison, we implement all these methods within the same codebase. For detailed implementations, please refer to AppendixC. ", "page_idx": 7}, {"type": "image", "img_path": "7YdafFbhxL/tmp/5cfbe2ebf58ab9c6ce9c983ba834674830af9ffb6dcfbb24350b923a7296fd74.jpg", "img_caption": ["Figure 1: Overall performance on 8 DMControl tasks over 5 random seeds following $500\\mathrm{k}$ interactions with the environment. Here the $x$ -axis is the number of expert trajectories and the $y$ -axis is the return. The solid lines are the mean of results while the shaded region corresponds to the standard deviation over 5 random seeds. Same as the following figures. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "7YdafFbhxL/tmp/e45b2e3d5f51c22540ffa0579889ae60c71a89f371c6aeaef496bc3eea7cc723.jpg", "img_caption": ["Figure 2: Learning curves on 8 DMControl tasks over 5 random seeds using 1 expert trajectory. Here the $x$ -axis is the number of environment interactions and the $y$ -axis is the return. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2  Experiment Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Expert Sample Efficiency. Figure 1 shows the performance of the learned policies after $500\\mathrm{k}$ environment interactions with varying numbers of expert trajectories. First, OPT-AIL significantly outperforms BC, verifying the theoretical claim that OPT-AIL can mitigate the compounding errors issue inherent in BC for general function approximation. Moreover, OPT-AIL consistently matches or exceeds the performance of prior SOTA AIL methods on all tasks. Notably, OPT-AIL demonstrates outstanding performance in scenarios with limited expert demonstrations, a common occurrence in real-world applications. In particular, when there is only one expert trajectory, our method uniquely achieves expert or near-expert performance on tasks like Finger Spin, Walker Run and Hopper Hop. ", "page_idx": 8}, {"type": "text", "text": "Environment Interaction Efficiency. Figure 2 displays the learning curves of different algorithms with 1 expert trajectory. Compared with prior SOTA AIL approaches, OPT-AIL achieves comparable or better performance regarding interaction efficiency on all 8 tasks. Notably, on Hopper Hop, Walker Run and Walker Run, OPT-AIL can achieve near-expert performance with substantially fewer environment interactions compared with prior approaches. We also demonstrate the superior interaction efficiency of OPT-AIL with other numbers of expert trajectories; please refer to Appendix D for additional results. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To narrow the gap between theory and practice in adversarial imitation learning, this paper investigates AIL with general function approximation. We develop a new AIL approach termed OPT-AIL, which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. In theory, OPT-AIL achieves polynomial expert sample complexity and interaction complexity for general function approximation. In practice, OPT-AIL only requires approximately solving two optimization problems, which enables an efficient implementation. Our experiments demonstrate that OPT-AIL outperforms prior SOTA methods in several challenging tasks, highlighting its potential to bridge theoretical soundness with practical efficiency. ", "page_idx": 9}, {"type": "text", "text": "In tabular MDPs, the currently optimal expert sample complexity is ${\\mathcal O}(H^{3/2}/\\varepsilon)$ [41, 64], which is better than ${\\mathcal{O}}(H^{2}/\\varepsilon^{2})$ attained in this paper. Therefore, a promising and valuable future direction would be to develop more advanced AIL approaches that achieve this expert sample complexity in the setting of general function approximation. Besides, [63] established a horizon-free imitation gap bound for AIL in tabular MDPs. Thus it is interesting to explore horizon-free bounds for AIL with general function approximation. ", "page_idx": 9}, {"type": "text", "text": "7Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Ziniu Li and Yichen Li for their helpful discussions and feedback. This work was supported by the Fundamental Research Program for Young Scholars (PhD Candidates) of the National Science Foundation of China (623B2049) and Jiangsu Science Foundation (BK20243039). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning, pages 1-8, 2004.   \n[2]  Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In Proceedings of the 2lst Annual Conference on Learning Theory, pages 263-274, 2008.   \n[3]  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning, pages 1638-1646, 2014.   \n[4] Philip Amortila, Dylan J Foster, Nan Jiang, Ayush Sekhari, and Tengyang Xie. Harnessing density ratios for online reinforcement learning. arXiv, 2401.09681, 2024.   \n[5]  Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71:89-129, 2008.   \n[6]  Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning, pages 214-223,2017.   \n[7]  Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. Advances in Neural Information Processing Systems 30, 2017.   \n[8]  Mohak Bhardwaj, Tengyang Xie, Byron Boots, Nan Jiang, and Ching-An Cheng. Adversarial model for offline reinforcement learning. Advances in Neural Information Processing Systems 37,36,2024.   \n[9] Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In Proceedings of the 8th International Conference on Learning Representations, 2020.   \n[10]  Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanskiiali Ding,DanyriAvinavaDubey,ChesaFintal. R2:Visinlangua action models transfer web knowledge to robotic control. arXiv, 2307.15818, 2023.   \n[11] Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. Generative adversarial user model for reinforcement learning based recommendation system. In Proceedings of the 36th International Conference on Machine Learning, pages 1052-1061, 2019.   \n[12] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. In Proceedings of the 39th International Conference on Machine Learning, pages 3852-3878, 2022.   \n[13] Dylan J Foster, Adam Block, and Dipendra Misra. Is behavior cloning all you need? understanding horizon in imitation learning. arXiv, 2407.15007, 2024.   \n[14] Scot Fujmtrk vanHoof, andDavidMgerddressing uctonapprxiatrn actor-criticmethods InProceedings of the 35th International Conference onMachine Learning, pages 1582-1591, 2018.   \n[15] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn: Inverse soft-q learning for imitation. In Advances in Neural Information Processing Systems 34, pages 4028-4039, 2021.   \n[16]  Seyed Kamyar Seyed Ghasemipour, Richard S. Zemel, and Shixiang Gu. A divergence minimization perspective on imitation learning methods. In Proceedings of the 3rd Annual Conference on Robot Learning, pages 1259-1277, 2019.   \n[17]  Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning, pages 1856-1865, 2018.   \n[18] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157-325, 2016.   \n[19]  Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems 29, pages 4565-4573, 2016.   \n[20] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems 32, pages 12498-12509, 2019.   \n[21] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In Proceedings of the 34th International Conference on Machine Learning, pages 1704-1713, 2017.   \n[22] Shengyi Jiang, Jingcheng Pang, and Yang Yu. Offline imitation learning with a misspecified simulator. Advances in Neural Information Processing Systems 33, 2020.   \n[23]  Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-effcient algorithms. In Advances in Neural Information Processing Systems 34, pages 13406-13418, 2021.   \n[24]  Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement learning with linear function approximation. In Proceedings of the 33rd Annual Conference on Learning Theory, pages 2137-2143, 2020.   \n[25]  Yue Kang, Cho-Jui Hsieh, and Thomas Chun Man Lee. Robust lipschitz bandits to adversarial corruptions. Advances in Neural Information Processing Systems 37, 2023.   \n[26] Liyiming Ke, Matt Barnes, Wen Sun, Gilwoo Lee, Sanjiban Choudhury, and Siddhartha S. Srinivasa. Imitation learning as f-divergence minimization. arXiv, 1905.12888, 2019.   \n[27] Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson. Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning. In Proceedings of the 7th International Conference on Learning Representations,2019.   \n[28] lya Kostrikov, Ofr Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution matching. In Proceedings of the 8th International Conference on Learning Representations, 2020.   \n[29]  Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offine reinforcement learning. Advances in Neural Information Processing Systems, 33:1179- 1191, 2020.   \n[30] Ziniu Li, Tian Xu, Zeyu Qin, Yang Yu, and Zhi-Quan Luo. Imitation learning from imperfection: Theoretical justifications and algorithms. Advances in Neural Information Processing Systems 37, 2023.   \n[31]  Ziniu Li, Tian Xu, and Yang Yu. A note on target q-learning for solving finite mdps with a generative 0racle. arXiv, 2203.11489, 2022.   \n[32] Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, and Zhaoran Wang. Maximize to explore: One objective function fusing estimation, planning, and exploration. Advances in Neural Information Processing Systems 36, 2024.   \n[33] Zhihan Liu, Yufeng Zhang, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Provably efficient generative adversarial imitation learning for online and offline seting with linear function approximation. arXiv, 2108.08765, 2021.   \n[34] Fan-Ming Luo, Xingchen Cao, Rong-Jun Qin, and Yang Yu. Transferable reward learning by dynamics-agnostic discriminator ensemble. arXiv, 2206.00238, 2022.   \n[35] Oier Mees, Dibya Ghosh, Karl Pertsch, Kevin Black, Homer Rich Walke, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024.   \n[36] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.   \n[37] Francesco Orabona. A modern introduction to online learning. arXiv, 1912.13213, 2019.   \n[38] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. In Advances in Neural Information Processing Systems 27, pages 1466-1474, 2014.   \n[39] Dean Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1):88-97, 1991.   \n[40] Nived Rajaraman, Yanjun Han, Lin Yang, Jingbo Liu, Jiantao Jiao, and Kannan Ramchandran. On the value of interaction and function approximation in imitation learning. In Advances in Neural Information Processing Systems 34, pages 1325-1336, 2021.   \n[41]  Nived Rajaraman, Lin F. Yang, Jiantao Jiao, and Kannan Ramchandran. Toward the fundamental limits ofimitation learning. In Advances in Neural Information Processing Systems 3, pages 2914-2924, 2020.   \n[42] Juntao Ren, Gokul Swamy, Zhiwei Steven Wu, J Andrew Bagnell, and Sanjiban Choudhury. Hybrid inverse reinforcement learning. Proceedings of the 41st International Conference on Machine Learning, 2024.   \n[43]  Stephane Ross and Drew Bagnell. Effcient reductions for imitation learning. In Proceedings ofthe 13rd International Conference on Artificial Intelligence and Statistics, pages 661-668, 2010.   \n[44]  Stephane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 627-635, 2011.   \n[45] Lior Shani, Tom Zahavy, and Shie Mannor. Online apprenticeship learning. arXiv, 2102.06924, 2021.   \n[46] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and Anxiang Zeng. Virtual-taobao: virtualizing real-world online retail environment for reinforcement learning. In Proceedings of the 33rd AAA1 Conference on Artificial Intelligence, pages 4902-4909, 2019.   \n[47]  Arun Sai Suggala and Praneeth Netrapalli. Online non-convex learning: Following the perturbed leader is optimal. InProceedings of the 3lst International Conference onAlgorithmic Learning Theory, pages 845-861, 2020.   \n[48] Wen Sun, Anirudh Vemula, Byron Boots, and Drew Bagnell. Provably efficient imitation learning from observation alone. In Proceedings of the 36th International Conference on Machine Learning, pages 6036-6045, 2019.   \n[49] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.   \n[50] Gokul Swamy, Nived Rajaraman, Matt Peng, Sanjiban Choudhury, J Bagnell, Steven Z Wu, Jiantao Jiao, and Kannan Ramchandran. Minimax optimal online imitation learning via replay estimation. Advances in Neural Information Processing Systems 35, pages 7077-7088, 2022.   \n[51] Gokul Swamy, David Wu, Sanjiban Choudhury, Drew Bagnell, and Steven Wu. Inverse reinforcement learning without reinforcement learning. In Proceedings of the 4Oth International Conference on Machine Learning, 2023.   \n[52]  Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. In Advances in Neural Information Processing Systems 20, pages 1449-1456, 2007.   \n[53] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv: 1801.00690, 2018.   \n[54] Danil Tiapkin, Denis Belomestny, Eric Moulines, Alexey Naumov, Sergey Samsonov, Yunhao Tang, Michal Valko, and Pierre Menard. From dirichlet to rubin: Optimistic exploration in rl without bonuses. In Proceedings of the 39th International Conference on Machine Learning, pages 21380-21431, 2022.   \n[55] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4950- 4957, 2018.   \n[56] Luca Viano, Angeliki Kamoutsi, Gergely Neu, Igor Krawczuk, and Volkan Cevher. Proximal point imitation learning. Advances in Neural Information Processing Systems, 35:24309-24326, 2022.   \n[57]  Luca Viano, Stratis Skoulakis, and Volkan Cevher. Better imitation learning in discounted linear MDP, 2024.   \n[58]  Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University Press, 2019.   \n[59] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. Advances in Neural Information Processing Systems 33, pages 6123-6135, 2020.   \n[60] Yizhou Wang, Tianyi Liu, Zhuoran Yang, Xingguo Li, Zhaoran Wang, and Tuo Zhao. On computation and generalization of generative adversarial imitation learning. In Proceedings of the 8th International Conference on Learning Representations, 2020.   \n[61] Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments. In Advances in Neural Information Processing Systems 33, pages 15737-15749, 2020.   \n[62] Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments for reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.   \n[63] Tian Xu, Ziniu Li, Yang Yu, and Zhi-Quan Luo. Understanding adversarial imitation learning in small sample regime: A stage-coupled analysis. arXiv, 2208.01899, 2022.   \n[64] Tian Xu, Ziniu Li, Yang Yu, and Zhi-Quan Luo. Provably efficient adversarial imitation learning with unknown transitions. In Proceedings of the 39th Conference on Uncertainty in Artificial Intelligence, pages 2367-2378, 2023.   \n[65] Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng, Peng Liu, and Zhen Wang. Exploration in deep reinforcement learning: a comprehensive survey. arXiv, 2109.06668, 2021.   \n[66] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. In International Conference on Learning Representations, 2021.   \n[67] Yufeng Zhang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Generative adversarial imitation learning with neural network parameterization: Global optimality and convergence rate. In Proceedings of the 37th International Conference on Machine Learning, pages 11044-11054, 2020.   \n[68] Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. A posterior sampling framework for interactive decision making. arXiv, 2211.01962, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This study explores the theoretical foundations of adversarial imitation learning with general function approximation and demonstrates the efficiency of the proposed algorithm through standard benchmarks. Although the paper does not reveal any immediate social impacts, the potential practical applications of our research could drive positive change. By broadening the scope of adversarial imitation learning, our work may enable the creation of more efficient and effective solutions in fields such as robotics and autonomous vehicles. Nonetheless, we must recognize the potential for negative consequences if this technology is misused. For example, imitation learning learns from human expert demonstrations and could raise privacy concerns. Therefore, it is essential to ensure that the advancements in imitation learning are applied responsibly and ethically. ", "page_idx": 14}, {"type": "text", "text": "B Omitted Proof ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 1 is a standard error decomposition lemma in adversarial imitation learning and variants of Lemma 1 have appeared in [45, 57]. According to the definition of $\\overline{{\\pi}}$ ,wehavethat ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{V}^{\\pi^{\\mathrm{E}}}-\\boldsymbol{V}^{\\overline{{\\pi}}}=\\boldsymbol{V}_{\\mathrm{rtrue}}^{\\pi^{\\mathrm{E}}}-\\boldsymbol{V}_{\\mathrm{rtrue}}^{\\overline{{\\pi}}}}\\\\ &{\\phantom{\\displaystyle{=}}=\\cfrac{1}{K}\\sum_{k=1}^{K}\\boldsymbol{V}_{\\mathrm{rtrue}}^{\\pi^{\\mathrm{E}}}-\\boldsymbol{V}_{\\mathrm{rtrue}}^{\\pi^{k}}}\\\\ &{\\phantom{\\displaystyle{=}}=\\cfrac{1}{K}\\sum_{k=1}^{K}\\bigg(\\boldsymbol{V}_{\\mathrm{rtrue}}^{\\pi^{\\mathrm{E}}}-\\boldsymbol{V}_{\\mathrm{rtrue}}^{\\pi^{k}}-\\left(\\boldsymbol{V}_{\\mathrm{r}^{k}}^{\\pi^{\\mathrm{E}}}-\\boldsymbol{V}_{\\mathrm{r}^{k}}^{\\pi^{k}}\\right)\\bigg)+\\cfrac{1}{K}\\sum_{k=1}^{K}\\boldsymbol{V}_{\\mathrm{r}^{k}}^{\\pi^{\\mathrm{E}}}-\\boldsymbol{V}_{\\mathrm{r}^{k}}^{\\pi^{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We complete the proof. ", "page_idx": 14}, {"type": "text", "text": "B.2Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present the proof of Theorem 1. ", "page_idx": 14}, {"type": "text", "text": "To prove Theorem 1, we need the following two useful lemmas which upper bound the reward error and policy error, respectively. Please refer to Appendix B.3 and B.4 for the detailed proof. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2 (Upper Bound on Reward Error). Under Assumption 1. For any fixed $\\delta\\in(0,1]$ consider Algorithm $^{\\,l}$ with probability at least $1-\\delta$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}V_{\\mathrm{rtrue}}^{\\pi^{\\mathtt{E}}}-V_{\\mathrm{rtrue}}^{\\pi^{k}}-\\Big(V_{r^{k}}^{\\pi^{\\mathtt{E}}}-V_{r^{k}}^{\\pi^{k}}\\Big)\\leq2H\\sqrt{\\frac{\\log(6\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)}{N}}+4H\\rho}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\,2H\\sqrt{\\frac{\\log(3/\\delta)}{K}}+\\varepsilon_{\\mathrm{opt}}^{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 3 (Upper Bound on Policy Error). Under Assumptions 2, 3 and 4. For any fixed $\\delta\\in(0,1]$ withprobabilityatleast $1-\\delta$ itholdsthat ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{k}}\\le\\frac{57H^{4}\\log(4K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+57K H^{3}\\rho+\\varepsilon_{\\mathrm{opt}}^{Q}}{\\lambda}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\frac{\\lambda d_{\\mathrm{GEC}}(\\varepsilon^{\\prime})}{2K}+\\sqrt{\\frac{d_{\\mathrm{GEC}}(\\varepsilon^{\\prime})H}{K}}+\\varepsilon^{\\prime}H.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we start to prove Theorem 1. With Lemma 1, we can derive that ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-V_{r^{\\mathrm{true}}}^{\\pi}=\\frac{1}{K}\\sum_{k=1}^{K}V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-V_{r^{\\mathrm{true}}}^{\\pi^{k}}-\\Big(V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{k}}\\Big)+\\frac{1}{K}\\sum_{k=1}^{K}V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{k}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, Lemma 2 and Lemma 3 offer upper bounds on reward error and policy error, respectively. By union bound, with probability at least $1-\\delta$ weobtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{r}}}-V_{r^{\\mathrm{true}}}^{\\pi}}\\\\ &{\\leq2H\\sqrt{\\frac{\\log(12\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)}{N}}+4H\\rho+2H\\sqrt{\\frac{\\log(6/\\delta)}{K}}+\\varepsilon_{\\mathrm{opt}}^{r}}\\\\ &{\\quad+\\,\\frac{57H^{4}\\log(8K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+57K H^{3}\\rho+\\varepsilon_{\\mathrm{opt}}^{Q}}{\\lambda}}\\\\ &{\\quad+\\,\\frac{\\lambda d_{\\mathrm{GEC}}(\\varepsilon^{\\prime})}{2K}+\\sqrt{\\frac{d_{\\mathrm{GEC}}(\\varepsilon^{\\prime})H}{K}}+\\varepsilon^{\\prime}H.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We choose $\\varepsilon^{\\prime}=\\varepsilon/H$ and obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-V_{r^{\\mathrm{true}}}^{\\pi}}\\\\ &{\\leq2H\\sqrt{\\frac{\\log(12\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)}{N}}+4H\\rho+2H\\sqrt{\\frac{\\log(6/\\delta)}{K}}}\\\\ &{\\quad+\\,\\frac{57H^{4}\\log(8K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+57K H^{3}\\rho}{\\lambda}}\\\\ &{\\quad+\\,\\frac{\\lambda d_{\\mathrm{GEC}}}{2K}+\\sqrt{\\frac{d_{\\mathrm{GEC}}H}{K}}+\\varepsilon_{\\mathrm{opt}}^{r}+\\frac{\\varepsilon_{\\mathrm{opt}}^{Q}}{\\lambda}+\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ${d_{\\mathrm{GEC}}:=d_{\\mathrm{GEC}}(\\varepsilon/H)}$ . By choosing the regularization coefficient ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda=\\sqrt{\\frac{114K H^{4}\\log(8K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+114K^{2}H^{3}\\rho}{d_{\\mathrm{GEC}}}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we further obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\mathrm{sc}}^{\\prime\\prime}-V_{\\mathrm{sc}}^{\\prime\\prime}}\\\\ &{\\le2\\eta V\\displaystyle{\\frac{\\sqrt{\\pi}\\mathrm{L}[\\mathrm{or~}](2\\mathrm{since}\\ y,X_{0}^{\\prime}(\\mathrm{Re}))^{2}}{\\eta V}}+4\\eta\\rho_{x}+2\\eta V\\displaystyle{\\frac{\\sqrt{\\pi}\\mathrm{Re}[\\tilde{\\alpha}_{0}\\tilde{\\beta}_{0}^{\\prime}]}{\\hbar}}}\\\\ &{\\quad+\\sqrt{\\frac{\\mathrm{lim}\\tilde{L}[\\tilde{\\alpha}_{0}\\mathrm{cxo}_{\\mathrm{b}}](\\hbar{\\Omega}\\mathrm{mac}_{\\mathrm{b}}(\\mathrm{Re}))V_{0}(\\mathrm{Re})\\mathbb{N}_{\\mathrm{e}}[\\mathrm{Re}_{\\mathrm{b}}](\\cdot)}{\\eta V}}+10\\eta V\\displaystyle{\\frac{\\sqrt{\\pi}\\mathrm{Re}[\\tilde{\\alpha}_{0}\\tilde{\\beta}_{0}^{\\prime}]}{\\hbar}}}\\\\ &{\\quad+\\sigma_{\\mathrm{sc}}^{\\prime\\prime}+\\frac{c_{\\mathrm{sc}}^{\\prime\\prime}}{\\hbar}+\\epsilon}\\\\ &{\\le2\\eta V\\displaystyle{\\frac{\\sqrt{\\pi}\\mathrm{L}[(2\\mathrm{since}\\ y,X(\\mathrm{Re}))]^{2}}{\\hbar}}+4\\eta+2\\eta V\\displaystyle{\\frac{\\sqrt{\\pi}\\mathrm{Re}[\\tilde{\\alpha}_{0}\\tilde{\\beta}_{0}^{\\prime}]}{\\hbar}}}\\\\ &{\\quad+\\sqrt{\\frac{\\mathrm{lim}\\tilde{L}[\\mathrm{or~}](2\\mathrm{since}\\ y,X(\\mathrm{Re}))\\mathbb{N}_{\\mathrm{e}}[\\mathrm{Re}_{\\mathrm{b}}](\\cdot)}{\\eta V}}+\\sqrt{\\mathrm{lim}\\tilde{L}[\\mathrm{or~}](2\\mathrm{stance}\\ y-\\sqrt{\\frac{c_{\\mathrm{sc}}\\tilde{\\alpha}_{0}\\tilde{\\beta}_{0}^{\\prime}}{\\hbar}})}}\\\\ &{\\quad+\\sqrt{\\frac{\\mathrm{lim}\\tilde{L}[\\mathrm{or~}](2\\mathrm{since}\\ y,X(\\mathrm{Re}))\\mathbb{N}_{\\mathrm{e}}[\\mathrm{Re}_{\\mathrm{b}}](\\cdot)}{\\eta V}}+\\sqrt{\\mathrm{lim}\\tilde{L}[\\mathrm{or~}](2\\mathrm{since}\\ y-\\sqrt{\\frac{c_{\\mathrm{sc}}\\tilde{\\alpha}_{0}\\tilde{\\beta}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Inequality (a) follows ${\\sqrt{a+b}}\\,\\leq\\,{\\sqrt{a}}\\,+\\,{\\sqrt{b}},\\,\\ {\\forall a,b}\\,\\geq\\,0$ and inequality (b) holds because of the choice $\\dot{\\rho_{}}=\\varepsilon^{2}/(54H^{3}d_{\\mathrm{GEC}}+4H)$ . Now we determine the number of expert trajectories and the number of interaction trajectories. With Lemma 8, when the expert sample complexity and interaction complexitysatisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N\\ge4\\frac{H^{2}\\log(12\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)}{\\varepsilon^{2}},}\\\\ &{K\\ge2304\\frac{\\left(H^{4}d_{\\mathrm{GEC}}\\log(768H^{5/2}d_{\\mathrm{GEC}}^{1/2}\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/(\\delta\\varepsilon))+H^{2}\\log(6/\\delta)\\right)}{\\varepsilon^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-V_{r^{\\mathrm{true}}}^{\\overline{{\\pi}}}\\leq6\\varepsilon+\\varepsilon_{\\mathrm{opt}}^{r}+\\frac{\\varepsilon_{\\mathrm{opt}}^{Q}}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Scaling $\\varepsilon$ as $\\varepsilon/6$ completes the proof. ", "page_idx": 16}, {"type": "text", "text": "B.3Proof of Lemma 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To prove Lemma 2, we first perform the following error decomposition. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}V_{\\mathrm{rtrue}}^{\\pi^{\\mathrm{E}}}-V_{\\mathrm{rtrue}}^{\\pi^{k}}-\\left(V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{k}}\\right)}\\\\ &{=\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\left(\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{k}}-\\left(\\widehat{V}_{r^{k}}^{\\pi^{\\mathrm{E}}}-\\widehat{V}_{r^{k}}^{\\pi^{k}}\\right)\\right)+V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}+\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\widehat{V}_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{\\mathrm{E}}}}\\\\ &{\\displaystyle+\\,\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{k}}-V_{r^{\\mathrm{true}}}^{\\pi^{k}}+\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}V_{r^{k}}^{\\pi^{k}}-\\widehat{V}_{r^{k}}^{\\pi^{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall that for any reward function $r$ $\\widehat V_{r}^{\\pi^{i}}$ and $\\widehat{V}_{r}^{\\pi^{\\mathrm{E}}}$ are unbiased estimations of V\\* and VE, respectively. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{V}_{r}^{\\pi^{i}}=\\sum_{h=1}^{H}r_{h}(s_{h}^{i},a_{h}^{i}),\\:\\widehat{V}_{r}^{\\pi^{\\mathtt{E}}}=\\frac{1}{N}\\sum_{\\tau\\in\\mathcal{D}^{\\mathtt{E}}}\\sum_{h=1}^{H}r_{h}(\\tau(s_{h}),\\tau(a_{h})).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first term in the RHS of Eq. (4) is the estimated reward error while the remaining terms are estimation errors. To upper bound the first term, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\hat{V}_{r^{\\mathrm{true}}}^{\\pi^{k}}-\\hat{V}_{r^{\\mathrm{true}}}^{\\pi^{k}}-\\left(\\hat{V}_{r^{k}}^{\\pi^{k}}-\\hat{V}_{r^{k}}^{\\pi^{k}}\\right)}\\\\ &{\\displaystyle=\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\hat{V}_{r^{\\mathrm{tr}}}^{\\pi^{k}}-\\hat{V}_{r^{\\mathrm{t}}}^{\\pi^{\\mathrm{E}}}-\\left(\\hat{V}_{r^{\\mathrm{true}}}^{\\pi^{k}}-\\hat{V}_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}\\right)}\\\\ &{\\displaystyle\\le\\frac{1}{K}\\operatorname*{max}_{r\\in\\mathcal{R}_{k=1}}^{\\operatorname*{max}}\\sum_{k=1}^{K}\\hat{V}_{r^{\\mathrm{t}}}^{\\pi^{\\mathrm{E}}}-\\hat{V}_{r^{\\mathrm{t}}}^{\\pi^{k}}-\\left(\\hat{V}_{r}^{\\pi^{k}}-\\hat{V}_{r}^{\\pi^{\\mathrm{E}}}\\right)}\\\\ &{\\displaystyle\\overset{(c)}{=}\\varepsilon_{\\mathrm{opt}}^{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Equation (c) follows the definition of reward optimization error in Definition 2. Then we can obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-V_{r^{\\mathrm{true}}}^{\\pi^{k}}-\\left(V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{k}}\\right)}\\\\ &{\\leq V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}+\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\widehat{V}_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{\\mathrm{E}}}+\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{k}}-V_{r^{\\mathrm{true}}}^{\\pi^{k}}+\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}V_{r^{k}}^{\\pi^{k}}-\\widehat{V}_{r^{k}}^{\\pi^{k}}+\\varepsilon_{\\mathrm{opt}}^{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we proceed to upper bound the estimation errors. First, we first upper bound the estimation error caused by using $\\overleftarrow{V}_{r}^{\\pi^{\\mathrm{E}}}$ to approximate $V_{r}^{\\pi^{\\mathrm{E}}}$ . In particular, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\widehat{V}_{r}^{\\pi^{\\mathbf{E}}}-V_{r}^{\\pi^{\\mathbf{E}}}\\right|=\\left|\\displaystyle\\frac{1}{N}\\sum_{\\tau\\in\\mathcal{D}^{\\mathbb{E}}}\\sum_{h=1}^{H}r_{h}(s_{h}(\\tau),a_{h}(\\tau))-\\mathbb{E}\\left[\\displaystyle\\sum_{h=1}^{H}r_{h}(s_{h},a_{h})\\bigg|\\pi^{\\mathrm{E}}\\right]\\right|}\\\\ &{\\qquad\\qquad\\qquad=\\left|\\displaystyle\\sum_{h=1}^{H}\\displaystyle\\frac{1}{N}\\sum_{\\tau\\in\\mathcal{D}^{\\mathbb{E}}}r_{h}(s_{h}(\\tau),a_{h}(\\tau))-\\sum_{h=1}^{H}\\mathbb{E}\\left[r_{h}(s_{h},a_{h})\\bigg|\\pi^{\\mathrm{E}}\\right]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{h=1}^{H}\\left|\\displaystyle\\frac{1}{N}\\sum_{\\tau\\in\\mathcal{D}^{\\mathbb{E}}}r_{h}(s_{h}(\\tau),a_{h}(\\tau))-\\mathbb{E}\\left[r_{h}(s_{h},a_{h})\\bigg|\\pi^{\\mathrm{E}}\\right]\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Hoeffding's inequality [58], for any fixed timestep $\\textit{h}\\in[H]$ and any fixed reward function $r_{h}\\in\\mathcal{R}_{h}$ , with probability at least $1-\\delta$ ,we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N}\\sum_{\\tau\\in\\mathcal{D}^{\\mathbb{E}}}r_{h}(s_{h}(\\tau),a_{h}(\\tau))-\\mathbb{E}\\left[r_{h}(s_{h},a_{h})\\bigg|\\pi^{\\mathbb{E}}\\right]\\right|\\leq\\sqrt{\\frac{\\log(2/\\delta)}{N}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $(\\mathcal{R}_{h})_{\\rho}$ be a $\\rho$ -cover of $\\mathcal{R}$ . By union bound, with probability at least $1-\\delta$ , for all $h\\in[H]$ and all $\\widehat{r}_{h}\\in(\\mathcal{R}_{h})_{\\rho}$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N}\\sum_{\\tau\\in\\mathcal{D}^{\\mathbb{E}}}\\widehat{r}_{h}(s_{h}(\\tau),a_{h}(\\tau))-\\mathbb{E}\\left[\\widehat{r}_{h}(s_{h},a_{h})\\bigg|\\pi^{\\mathbf{E}}\\right]\\right|\\leq\\sqrt{\\frac{\\log(2H|(\\mathcal{R}_{h})_{\\rho}|/\\delta)}{N}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then with probability at least $1-\\delta$ , for all $\\widehat{r}=(\\widehat{r}_{1},\\ldots,\\widehat{r}_{H})\\in(\\mathcal{R}_{1})_{\\rho}\\times\\ldots\\times(\\mathcal{R}_{1})_{\\rho},$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widehat{V}_{\\widehat{r}}^{\\pi^{\\mathtt{E}}}-V_{\\widehat{r}}^{\\pi^{\\mathtt{E}}}\\right|\\leq\\displaystyle\\sum_{h=1}^{H}\\left|\\frac{1}{N}\\sum_{\\tau\\in\\mathcal{D}^{\\mathtt{E}}}\\widehat{r}_{h}(s_{h}(\\tau),a_{h}(\\tau))-\\mathbb{E}\\left[\\widehat{r}_{h}(s_{h},a_{h})\\bigg|\\pi^{\\mathrm{E}}\\right]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{h=1}^{H}\\sqrt{\\frac{\\log(2|(\\mathcal{R}_{h})_{\\rho}|/\\delta)}{N}}}\\\\ &{\\qquad\\qquad\\qquad\\leq H\\sqrt{\\frac{\\log(2\\operatorname*{max}_{h\\in[H]}|(\\mathcal{R}_{h})_{\\rho}|/\\delta)}{N}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "According to the definition of $\\rho$ -cover, for any reward function $r=(r_{1},\\ldots,r_{H})\\in\\mathcal{R}$ , there exists $\\widehat{r}=(\\widehat{r}_{1},\\ldots,\\widehat{r}_{H})\\in(\\mathcal{R}_{1})_{\\rho}\\times\\ldots\\times(\\mathcal{R}_{1})_{\\rho}$ such that $\\forall h\\in[H]$ \uff0c $\\begin{array}{r}{\\operatorname*{max}_{(s,a)\\in S\\times A}|r_{h}(s,a)-\\widehat{r}_{h}(s,a)|\\leq}\\end{array}$ $\\rho$ . Then we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widehat{V}_{r}^{\\pi^{\\mathtt{E}}}-\\widehat{V}_{\\widehat{r}}^{\\pi^{\\mathtt{E}}}\\right|\\leq\\displaystyle\\frac{1}{N}\\sum_{\\tau\\in\\mathcal{D}^{\\mathrm{E}}}\\sum_{h=1}^{H}|r_{h}(s_{h}(\\tau),a_{h}(\\tau))-\\widehat{r}_{h}(s_{h}(\\tau),a_{h}(\\tau))|\\leq H\\rho,}\\\\ &{\\left|V_{r}^{\\pi^{\\mathtt{E}}}-V_{\\widehat{r}}^{\\pi^{\\mathtt{E}}}\\right|\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{h=1}^{H}|r_{h}(s_{h},a_{h})-\\widehat{r}_{h}(s_{h},a_{h})|\\left|\\pi^{\\mathrm{E}}\\right.\\right]\\leq H\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, with probability at least $1-\\delta$ , for all reward function $r\\in\\mathcal{R}$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\widehat{V}_{r}^{\\pi^{\\mathrm{E}}}-V_{r}^{\\pi^{\\mathrm{E}}}\\right|\\leq\\left|\\displaystyle\\widehat{V}_{\\widehat{r}}^{\\pi^{\\mathrm{E}}}-V_{\\widehat{r}}^{\\pi^{\\mathrm{E}}}\\right|+2H\\rho}\\\\ &{\\qquad\\qquad\\qquad\\leq H\\sqrt{\\frac{\\log(2\\operatorname*{max}_{h\\in[H]}|(\\mathcal{R}_{h})_{\\rho}|/\\delta)}{N}}+2H\\rho}\\\\ &{\\qquad\\qquad\\leq H\\sqrt{\\frac{\\log(2\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)}{N}}+2H\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we have obtained the upper bound on the estimatio eror $|\\widehat{V}_{r}^{\\pi^{\\mathrm{E}}}-V_{r}^{\\pi^{\\mathrm{E}}}|$ Then we proceed to upperbound theestimation ror $\\begin{array}{r}{(1/K)\\cdot\\textstyle\\sum_{k=1}^{K}\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{k}}-V_{r^{\\mathrm{true}}}^{\\pi^{k}}}\\end{array}$ and $\\begin{array}{r}{\\stackrel{\\cdot}{(1/K)}\\cdot\\sum_{k=1}^{K}V_{r^{k}}^{\\pi^{k}}-\\widehat{V}_{r^{k}}^{\\pi^{k}}}\\end{array}$ . With the Hoeffding's inequality [58, 25], with probability at least $1-\\delta$ , we obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{k}}-V_{r^{\\mathrm{true}}}^{\\pi^{k}}\\leq H\\sqrt{\\frac{\\log(1/\\delta)}{K}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We proceed to analyze the term $\\textstyle\\sum_{k=1}^{K}V_{r^{k}}^{\\pi^{k}}-{\\widehat{V}}_{r^{k}}^{\\pi^{k}}$ . Notice that $r^{k}$ are learned from historical trajectories $\\{\\tau^{1},\\dots,\\tau^{k-1}\\}$ and thus statisically depends on $\\{\\tau^{1},\\dots,\\tau^{k-1}\\}$ Therefore, $\\widehat V_{r^{1}}^{\\pi^{1}},\\cdots,\\widehat V_{r^{k}}^{\\pi^{k}}$ are not independent and the standard Hoeffding's inequality is not applicable. To address this issue, we apply Azuma-Hoeffding's inequality [58] for martingale. In particular, we define $\\mathcal{F}^{k}$ as the filtration induced by $\\{\\tau^{1},\\cdots,\\tau^{\\check{k}}\\}$ and can obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[V_{r^{k}}^{\\pi^{k}}-\\widehat{V}_{r^{k}}^{\\pi^{k}}|\\mathcal{F}^{k-1}\\right]=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, $\\{(V_{r^{k}}^{\\pi^{k}}-\\widehat{V}_{r^{k}}^{\\pi^{k}},\\mathcal{F}^{k})\\}_{k=1}^{\\infty}$ is a martingale difference sequence. With Azuma-Hoeffding's inequality, we can derive that with probability at least $1-\\delta$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}V_{r^{k}}^{\\pi^{k}}-\\widehat{V}_{r^{k}}^{\\pi^{k}}\\leq H\\sqrt{\\frac{\\log(1/\\delta)}{K}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In summary, we have derived the following three high-probability inequalities: Eq. (5), Eq. (6) and Eq. (7). With union bound, with probability at least $1-\\delta$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall r\\in\\mathcal{R},\\left|\\widehat{V}_{r}^{\\pi^{\\mathtt{E}}}-V_{r}^{\\pi^{\\mathtt{E}}}\\right|\\le H\\sqrt{\\frac{\\log(6\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)}{N}}+2H\\rho,}\\\\ &{\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{k}}-V_{r^{\\mathrm{true}}}^{\\pi^{k}}\\le H\\sqrt{\\frac{\\log(3/\\delta)}{K}},\\ \\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}{V}_{r^{k}}^{\\pi^{k}}-\\widehat{V}_{r^{k}}^{\\pi^{k}}\\le H\\sqrt{\\frac{\\log(3/\\delta)}{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "With the above three inequalities, we can derive that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}V_{\\mathrm{rtrue}}^{\\pi^{\\mathrm{E}}}-V_{\\mathrm{r^{true}}}^{\\pi^{\\mathrm{k}}}-\\Big(V_{r^{\\mathrm{k}}}^{\\pi^{\\mathrm{E}}}-V_{r^{\\mathrm{k}}}^{\\pi^{\\mathrm{k}}}\\Big)}\\\\ &{\\leq V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}-\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{E}}}+\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\widehat{V}_{r^{\\mathrm{k}}}^{\\pi^{\\mathrm{E}}}-V_{r^{\\mathrm{k}}}^{\\pi^{\\mathrm{E}}}+\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\widehat{V}_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{k}}}-V_{r^{\\mathrm{true}}}^{\\pi^{\\mathrm{k}}}+\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}V_{r^{\\mathrm{k}}}^{\\pi^{\\mathrm{k}}}-\\widehat{V}_{r^{\\mathrm{k}}}^{\\pi^{\\mathrm{k}}}+\\varepsilon_{\\mathrm{opt}}^{r}}\\\\ &{\\leq2H\\sqrt{\\frac{\\log\\left(6\\operatorname*{max}_{h\\in[H]}\\Lambda_{\\rho}\\left(\\mathcal{R}_{h}\\right)/\\delta\\right)}{N}}+4H\\rho+2H\\sqrt{\\frac{\\log\\left(3/\\delta\\right)}{K}}+\\varepsilon_{\\mathrm{opt}}^{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We complete the proof. ", "page_idx": 18}, {"type": "text", "text": "B.4Proof of Lemma 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To prove Lemma 3, we need the following two auxiliary lemmas. The detailed proof is presented in Appendix B.5 and Appendix B.6. ", "page_idx": 18}, {"type": "text", "text": "Lemma 4. For any fixed $\\delta\\in(0,1]$ with probability at least $1-\\delta$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall k\\in[K],\\mathrm{BE}^{k}(Q^{\\star,r^{k}})\\leq16H^{4}\\log\\left(K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta\\right)+30k H^{3}\\rho.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma 5. For any fixed $\\delta\\in(0,1]$ with probability at least $1-\\delta$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall k\\in[K],\\mathrm{BE}^{k}(Q^{k})\\geq\\displaystyle\\frac{1}{2}\\sum_{i=0}^{k-1}\\mathbb{E}\\left[\\sum_{h=1}^{H}\\left(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{r^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\\right)^{2}\\bigg|\\pi^{i}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-41H^{4}\\log\\left(2K H\\operatorname*{max}_{h\\in[H]}\\mathrm{N}_{\\rho}(\\mathcal{Q}_{h})\\mathrm{N}_{\\rho}(\\mathcal{R}_{h})/\\delta\\right)-27k H^{2}\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we proceed to analyze the policy error. First of all, we perform the following error decomposition. ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{1}{K}}\\sum_{k=1}^{K}{V_{r^{k}}^{\\pi^{\\mathtt{E}}}}-{V_{r^{k}}^{\\pi^{k}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\frac{1}{K}\\sum_{k=1}^{K}V_{r^{k}}^{\\star}-V_{r^{k}}^{\\pi^{k}}}\\\\ {\\displaystyle=\\frac{1}{K}\\sum_{k=1}^{K}\\left(V_{r^{k}}^{\\star}-Q_{1}^{k}(s_{1},\\pi^{k})\\right)+\\frac{1}{K}\\sum_{k=1}^{K}\\left(Q_{1}^{k}(s_{1},\\pi^{k})-V_{r^{k}}^{\\pi^{k}}\\right)}\\\\ {\\displaystyle=\\frac{1}{K}\\sum_{k=1}^{K}\\left(\\operatorname*{max}_{a\\in A}Q_{1}^{\\star,r^{k}}(s_{1},a)-\\operatorname*{max}_{a\\in A}Q_{1}^{k}(s_{1},a)\\right)+\\frac{1}{K}\\sum_{k=1}^{K}\\left(Q_{1}^{k}(s_{1},\\pi^{k})-V_{r^{k}}^{\\pi^{k}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here $V_{r^{k}}^{\\star}$ denotes the optimal policy value under reward $r^{k}$ ", "page_idx": 19}, {"type": "text", "text": "From line 4 in Algorithm 1, we know that $Q^{k}$ is an approximate solution of $\\operatorname*{min}_{Q\\in\\mathcal{Q}}\\mathcal{L}^{k}(Q)$ with an error Eopt : With $Q^{\\star,r^{k}}\\in\\mathcal{Q}$ from Assumption 2, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{BE}^{k}(Q^{k})-\\lambda\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{1}^{k}(s_{1},a)\\leq\\mathrm{BE}^{k}(Q^{\\star,r^{k}})-\\lambda\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{1}^{\\star,r^{k}}(s_{1},a)+\\varepsilon_{\\mathrm{opt}}^{Q}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Rearrange the above inequality yields that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{1}^{\\star,r^{k}}(s_{1},a)-\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{1}^{k}(s_{1},a)\\leq\\frac{1}{\\lambda}\\left(\\mathrm{BE}^{k}(Q^{\\star,r^{k}})-\\mathrm{BE}^{k}(Q^{k})\\right)+\\frac{\\varepsilon_{\\mathrm{opt}}^{Q}}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From Lemma 4, with probability at least $1-\\delta$ ,we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{BE}^{k}(Q^{\\star,r^{k}})\\leq16H^{4}\\log\\left(K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\!\\!\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta\\right)+30k H^{3}\\rho.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "On the other hand, with probability at least $1-\\delta$ ,wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{BE}^{k}(Q^{k})\\geq\\displaystyle\\frac{1}{2}\\sum_{i=0}^{k-1}\\mathbb{E}\\left[\\sum_{h=1}^{H}\\left(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{r^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\\right)^{2}\\bigg|\\pi^{i}\\right]}\\\\ &{\\quad\\quad\\quad\\quad-41H^{4}\\log\\left(2K H\\operatorname*{max}_{h\\in[H]}\\!\\!\\!N_{\\rho}(\\mathcal{Q}_{h})\\!\\!N_{\\rho}(\\mathcal{R}_{h})/\\delta\\right)-27k H^{2}\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By union bound, with probability at least $1-\\delta$ \uff0c ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}Q_{1}^{\\star,r^{k}}(s_{1},a)-\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}Q_{1}^{k}(s_{1},a)}\\\\ &{\\leq-\\displaystyle\\frac{1}{2\\lambda}\\sum_{i=0}^{k-1}\\mathbb{E}\\left[\\sum_{h=1}^{H}\\left(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{r^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\\right)^{2}\\Big|\\pi^{i}\\right]}\\\\ &{\\ \\ +\\displaystyle\\frac{57H^{4}\\log(4K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+57k H^{3}\\rho+\\varepsilon_{\\mathrm{opt}}^{Q}}{\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{\\mathrm{E}}}}\\\\ &{\\le-\\displaystyle\\frac{1}{2\\lambda}\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{i=0}^{k-1}\\mathbb{E}\\left[\\displaystyle\\sum_{h=1}^{H}\\Big(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(T_{h}^{r^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\\Big)^{2}\\,\\Bigg|\\pi^{i}\\right]}\\\\ &{\\displaystyle\\ \\ +\\frac{57H^{4}\\log(4K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(Q_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+57K H^{3}\\rho+\\varepsilon_{\\mathrm{opt}}^{Q}}{\\lambda}}\\\\ &{\\displaystyle\\ \\ +\\frac{1}{K}\\sum_{k=1}^{K}\\Big(Q_{1}^{k}(s_{1},\\pi^{k})-V_{r^{k}}^{\\pi^{k}}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now we upper bound the last term in RHS of the above inequality. From Assumption 4, for any $\\mu\\geq0$ ,it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac1K\\sum_{k=1}^{K}Q_{1}^{k}(s_{1},\\pi^{k})-V_{r^{k}}^{\\pi^{k}}\\le\\frac\\mu{2K}\\sum_{k=1}^{K}\\sum_{i=1}^{k-1}\\mathbb{E}\\left[\\sum_{h=1}^{H}\\left(Q_{h}^{k}(s_{h},a_{h})-T_{h}^{r^{k}}Q_{h+1}^{k}(s_{h},a_{h})\\right)^{2}\\bigg|\\pi^{i}\\right]+\\frac{d}{2\\mu K}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle+\\,\\sqrt{\\frac{d H}{K}}+\\varepsilon H}\\\\ &{\\displaystyle=\\frac{1}{2\\lambda K}\\sum_{k=1}^{K}\\sum_{i=1}^{k-1}\\mathbb{E}\\left[\\sum_{h=1}^{H}\\left(Q_{h}^{k}(s_{h},a_{h})-T_{h}^{r^{k}}Q_{h+1}^{k}(s_{h},a_{h})\\right)^{2}\\bigg|\\pi^{i}\\right]+\\frac{\\lambda d}{2K}}\\\\ &{\\displaystyle+\\,\\sqrt{\\frac{d H}{K}}+\\varepsilon H.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The last equation is obtained by setting $\\mu=1/\\lambda$ . Combining the above two inequalities yields that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{K}\\sum_{k=1}^{K}V_{r^{k}}^{\\pi^{\\mathrm{E}}}-V_{r^{k}}^{\\pi^{\\mathrm{A}}}}\\quad}&{}\\\\ &{\\leq\\frac{57H^{4}\\log(4K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+57K H^{3}\\rho+\\varepsilon_{\\mathrm{opt}}^{Q}}{\\lambda}+\\frac{\\lambda d}{2K}+\\sqrt{\\frac{d H}{K}}+\\varepsilon H.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.5Proof of Lemma 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall the definition of the estimated Bellman error. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{BE}^{k}(Q^{\\star,r^{k}})=\\displaystyle\\sum_{h=1}^{H}\\mathcal{E}_{h}(Q_{h}^{\\star,r^{k}},Q_{h+1}^{\\star,r^{k}};\\mathcal{D}^{k},r^{k})-\\displaystyle\\operatorname*{inf}_{Q_{h}^{\\prime}\\in\\mathcal{Q}_{h}}\\mathrm{BE}_{h}(Q_{h}^{\\prime},Q_{h+1}^{\\star,r^{k}};\\mathcal{D}^{k},r^{k})}&{}\\\\ {=\\displaystyle\\sum_{h=1}^{H}\\sum_{i=1}^{k-1}\\Big(Q_{h}^{\\star,r^{k}}(s_{h}^{i},a_{h}^{i})-r_{h}^{k}(s_{h}^{i},a_{h}^{i})-\\displaystyle\\operatorname*{max}_{a^{\\prime}}Q_{h+1}^{\\star,r^{k}}(s_{h+1}^{i},a^{\\prime})\\Big)^{2}}&{}\\\\ {-\\displaystyle\\operatorname*{inf}_{Q_{h}^{\\prime}\\in\\mathcal{Q}_{h}}\\sum_{i=0}^{k-1}\\Big(Q_{h}^{\\prime}(s_{h}^{i},a_{h}^{i})-r_{h}^{k}(s_{h}^{i},a_{h}^{i})-\\displaystyle\\operatorname*{max}_{a^{\\prime}}Q_{h+1}^{\\star,r^{k}}(s_{h+1}^{i},a^{\\prime})\\Big)^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For any fixed tuple $(k,h,Q^{\\prime},r)\\in[K]\\times[H]\\times\\mathcal{Q}\\times\\mathcal{R}$ , we define the random variable ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{h}^{i}(Q^{\\prime},r):=\\left(Q_{h}^{\\prime}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{\\star,r}(s_{h+1}^{i},a^{\\prime})\\right)^{2}}\\\\ &{\\qquad\\qquad-\\left(Q_{h}^{\\star,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{\\star,r}(s_{h+1}^{i},a^{\\prime})\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, we define the filtration $\\mathcal{F}_{h}^{i}\\,=\\,\\sigma\\big(\\{(s_{1}^{j},a_{1}^{j},\\boldsymbol{\\cdot}\\,.\\,.\\,,s_{H}^{j},a_{H}^{j})\\}_{j=0}^{i-1}\\cup\\big\\{s_{1}^{i},a_{1}^{i},\\boldsymbol{\\cdot}\\,.\\,.\\,,s_{h}^{i},a_{h}^{i}\\big\\}\\big)$ Then we calculate the expectation and variance of $Z_{h}^{i}(Q^{\\prime},r)$ conditioned on $\\mathcal{F}_{h}^{i}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[Z_{h}^{*}(Q_{h}^{*},r)|\\mathcal{F}_{h}^{\\lambda}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(Q_{h}^{*}(s_{h}^{\\lambda},a_{h}^{i})-Q_{h}^{*,r}(s_{h}^{\\lambda},a_{h}^{i})+Q_{h}^{*,r}(s_{h}^{\\lambda},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h}^{*,r}(s_{h+1}^{\\lambda},a_{h}^{r})\\right)^{2}\\bigg|\\mathcal{F}_{h}\\right]}\\\\ &{\\quad-\\mathbb{E}\\left[\\left(Q_{h}^{*}(s_{h}^{\\lambda},a_{h}^{i})-r_{h}(s_{h}^{\\lambda},a_{h}^{i})\\right)-\\operatorname*{max}Q_{h}^{*,r}(s_{h+1}^{\\lambda},a_{h+1}^{r},a^{r})\\right)^{2}\\bigg|\\mathcal{F}_{h}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(Q_{h}^{*}(s_{h}^{\\lambda},a_{h}^{i})-Q_{h}^{*,r}(s_{h}^{\\lambda},a_{h}^{i})\\right)^{2}\\bigg|\\mathcal{F}_{h}\\right]}\\\\ &{\\quad+2\\mathbb{E}\\left[\\left(Q_{h}^{*}(s_{h}^{\\lambda},a_{h}^{i})-Q_{h}^{*,r}(s_{h}^{\\lambda},a_{h}^{i})\\right)^{2}\\bigg|\\mathcal{F}_{h}\\right]}\\\\ &{\\quad+2\\mathbb{E}\\left[\\left(Q_{h}^{*}(s_{h}^{\\lambda},a_{h}^{i})-Q_{h}^{*,r}(s_{h}^{\\lambda},a_{h}^{i})\\right)\\left(Q_{h}^{*}(r_{h}^{\\lambda},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h}^{*,r}(s_{h+1}^{\\lambda},a^{r})\\right)\\bigg|\\mathcal{F}_{h}\\right]}\\\\ &{\\quad=\\left(Q_{h}^{*}(s_{h}^{\\lambda},a_{h}^{i})-Q_{h}^{*,r}(s_{h}^{\\lambda},a_{h}^\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,2\\left(Q_{h}^{\\prime}(s_{h}^{i},a_{h}^{i})-Q_{h}^{\\star,r}(s_{h}^{i},a_{h}^{i})\\right)\\left(Q_{h}^{\\star,r}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{r}Q_{h+1}^{\\star,r})(s_{h}^{i},a_{h}^{i})\\right)}\\\\ &{=\\left(Q_{h}^{\\prime}(s_{h}^{i},a_{h}^{i})-Q_{h}^{\\star,r}(s_{h}^{i},a_{h}^{i})\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the conditional variance, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\mathop{Var}}\\left[Z_{h}^{i}(Q^{\\prime},\\boldsymbol{r})\\bigg|\\mathcal{F}_{h}^{i}\\right]}\\\\ &{\\le\\mathbb{E}\\left[\\left(Z_{h}^{i}(Q^{\\prime},\\boldsymbol{r})\\right)^{2}\\bigg|\\mathcal{F}_{h}^{i}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(Q_{h}^{\\prime}(s_{h}^{i},a_{h}^{i})-Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})\\right)^{2}\\cdot}\\\\ &{\\ \\ \\ \\ \\left(Q_{h}^{\\prime}(s_{h}^{i},a_{h}^{i})+Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-2\\left({r}_{h}(s_{h}^{i},a_{h}^{i})+\\operatorname*{max}_{\\alpha\\in\\mathcal{A}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})\\right)\\right)^{2}\\bigg|\\mathcal{F}_{h}^{i}\\right]}\\\\ &{\\overset{(a)}{\\le}\\mathbb{E}\\mathbb{E}H^{2}\\left(Q_{h}^{i}(s_{h}^{i},a_{h}^{i})-Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})\\right)^{2}}\\\\ &{=\\mathbb{E}\\mathbb{E}H^{2}\\mathbb{E}\\left[Z_{h}^{i}(Q^{\\prime},\\boldsymbol{r})\\bigg|\\mathcal{F}_{h}^{i}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here inequality. $(a)$ holds_ since $\\begin{array}{r c c c c c l}{{|Q_{h}^{\\prime}(s_{h}^{i},a_{h}^{i})}}&{{+}}&{{Q_{h}^{\\star,r}(s_{h}^{i},a_{h}^{i})}}&{{-}}&{{2(r_{h}(s_{h}^{i},a_{h}^{i})}}&{{+}}&{{}}\\end{array}$ $\\begin{array}{r}{\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}Q_{h+1}^{\\star,r}(s_{h+1}^{i},a^{\\prime}))|\\le4H}\\end{array}$ almost surely. ", "page_idx": 21}, {"type": "text", "text": "Notice that $\\{Z_{h}^{i}(Q^{\\prime},r)-\\mathbb{E}\\left[Z_{h}^{i}(Q^{\\prime},r)|\\mathcal{F}_{h}^{i}\\right]\\}_{i=0}^{k-1}$ is the martingale difference sequence adapted to $\\{\\mathcal{F}_{h}^{i}\\}_{i=0}^{k-1}$ . Besides, almost surely, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|Z_{h}^{i}(Q^{\\prime},r)\\right|\\leq\\operatorname*{max}\\bigg\\{\\bigg(Q_{h}^{\\prime}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{\\star,r}(s_{h+1}^{i},a^{\\prime})\\bigg)^{2}\\,,}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\bigg(Q_{h}^{\\star,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{\\star,r}(s_{h+1}^{i},a^{\\prime})\\bigg)^{2}\\bigg\\}}\\\\ &{\\quad\\quad\\quad\\quad\\leq4H^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we immediately get that $|Z_{h}^{i}(Q^{\\prime},r)-\\mathbb{E}\\left[Z_{h}^{i}(Q^{\\prime},r)|\\mathcal{F}_{h}^{i}\\right]|\\le8H^{2}$ almost surely. Thus we can apply Lemma 6 and obtain that for any $\\eta\\in(0,1/(4H^{2})]$ , with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\sum_{i=0}^{k-1}Z_{h}^{i}(Q^{\\prime},r)-\\sum_{i=0}^{k-1}\\mathbb{E}\\left[Z_{h}^{i}(Q^{\\prime},r)\\bigg|\\mathcal{F}_{h}^{i}\\right]\\right|}\\\\ &{\\leq\\eta\\displaystyle\\sum_{i=0}^{k-1}\\mathrm{Var}\\left[Z_{h}^{i}(Q^{\\prime},r)\\bigg|\\mathcal{F}_{h}^{i}\\right]+\\frac{\\log(1/\\delta)}{\\eta}}\\\\ &{\\leq36H^{2}\\eta\\displaystyle\\sum_{i=0}^{k-1}\\mathbb{E}\\left[Z_{h}^{i}(Q^{\\prime},r)\\bigg|\\mathcal{F}_{h}^{i}\\right]+\\frac{\\log(1/\\delta)}{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{-\\displaystyle\\sum_{i=0}^{k-1}Z_{h}^{i}(Q^{\\prime},r)\\leq\\left(36H^{2}\\eta-1\\right)\\displaystyle\\sum_{i=0}^{k-1}\\mathbb{E}\\left[Z_{h}^{i}(Q^{\\prime},r)\\Big|\\mathcal{F}_{h}^{i}\\right]+\\frac{\\log(1/\\delta)}{\\eta}}}\\\\ {{\\leq16H^{2}\\log(1/\\delta).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The last equation is obtained by choosing $\\eta=1/(16H^{2})$ ", "page_idx": 21}, {"type": "text", "text": "We define $(\\mathcal{Q}_{h})_{\\rho}$ and $(\\mathcal{R}_{h})_{\\rho}$ as the $\\rho$ -cover of $\\mathcal{Q}_{h}$ and $\\mathcal{R}_{h}$ , respectively. It is direct to have that $\\mathcal{Q}_{\\rho}=(\\mathcal{Q}_{1})_{\\rho}\\times\\dot{\\dots}(\\mathcal{Q}_{H})_{\\rho}$ and $\\mathcal{R}_{\\rho}=(\\mathcal{R}_{1})_{\\rho}\\times...\\,(\\mathcal{R}_{H})_{\\rho}$ are $\\rho$ -covers of $\\mathcal{Q}$ and $\\mathcal{R}$ , respectively. By union bound, with probability at least $1-\\delta$ , for all $(k,h,\\widehat{Q},\\widehat{r})\\in[K]\\times[H]\\times\\mathcal{Q}_{\\rho}\\times\\mathcal{R}_{\\rho}.$ we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\sum_{i=0}^{k-1}Z_{h}^{i}(\\widehat{Q},\\widehat{r})\\leq16H^{2}\\log\\left(K H\\prod_{h=1}^{H}(|(\\mathcal{Q}_{h})_{\\rho}||(\\mathcal{R}_{h})_{\\rho}|)/\\delta\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\leq16H^{3}\\log\\left(K H\\operatorname*{max}_{h\\in[H]}|(\\mathcal{Q}_{h})_{\\rho}||(\\mathcal{R}_{h})_{\\rho}|/\\delta\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, for any $(Q,r)\\in\\mathcal{Q}\\times\\mathcal{R}$ , there exists $(\\widehat{Q},\\widehat{r})\\in\\mathcal{Q}_{\\rho}\\times\\mathcal{R}_{\\rho}$ such that $\\|Q-\\widehat{Q}\\|_{\\infty}\\leq\\rho$ and $\\|r-\\widehat{r}\\|_{\\infty}\\le\\rho$ . Then we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=0}^{k-1}Z_{h}^{i}(Q,r)-\\sum_{i=0}^{k-1}Z_{h}^{i}(\\widehat{Q},\\widehat{r})\\right|\\leq\\sum_{i=0}^{k-1}\\left|Z_{h}^{i}(Q,r)-Z_{h}^{i}(\\widehat{Q},\\widehat{r})\\right|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For each term, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|Z_{h}^{i}(Q,r)-Z_{h}^{i}(\\hat{Q},\\hat{r})\\right|}\\\\ &{\\le\\bigg|\\left(Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\\prime})\\right)^{2}}\\\\ &{\\qquad-\\left(\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-\\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\\prime})\\right)^{2}\\bigg|}\\\\ &{\\quad+\\bigg|\\left(Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\\prime})\\right)^{2}}\\\\ &{\\qquad-\\left(Q_{h}^{*,\\widehat{r}}(s_{h}^{i},a_{h}^{i})-\\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{*,\\widehat{r}}(s_{h+1}^{i},a^{\\prime})\\right)^{2}\\bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the first term in RHS, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\left(Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})\\right)^{2}}\\\\ &{\\qquad-\\left(\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-\\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})\\right)^{2}\\bigg|}\\\\ &{\\le\\left|Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})+\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-\\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a_{h}^{r})\\right|}\\\\ &{\\quad\\bigg|Q_{h}(s_{h}^{i},a_{h}^{i})-\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})+\\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})+\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})}\\\\ &{\\le4H\\bigg(\\left|Q_{h}(s_{h}^{i},a_{h}^{i})-\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})\\right|+\\left|r_{h}(s_{h}^{i},a_{h}^{i})-\\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})\\right|}\\\\ &{\\quad+\\operatorname*{max}\\left|Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})-Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})\\right|\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last inequality follows Lemma 7. Similarly, for the second term in RHS, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\left(Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})\\right)^{2}}\\\\ &{\\begin{array}{r l}&{-\\left(Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-\\widehat r_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})\\right)^{2}\\bigg|}\\\\ &{\\leq\\left|Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})\\right.}\\end{array}}\\\\ &{\\begin{array}{r l}&{+\\left.Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-\\widehat r_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})\\right|}\\\\ &{\\ddots\\left|Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})+\\widehat r_{h}(s_{h}^{i},a_{h}^{i})\\right|}\\end{array}}\\\\ &{\\begin{array}{r l}&{-\\left.\\left|Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})+\\widehat r_{h}(s_{h}^{i},a_{h}^{i})\\right.}\\end{array}}\\\\ &{\\begin{array}{r l}&{-\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})+\\operatorname*{max}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{r})\\bigg|}\\\\ &{\\left.\\qquad\\mathrm{or}\\;\\mathrm{~(}s_{h}^{\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le6H\\bigg(\\left|Q_{h}^{\\star,r}(s_{h}^{i},a_{h}^{i})-Q_{h}^{\\star,\\widehat{r}}(s_{h}^{i},a_{h}^{i})\\right|+\\left|r_{h}(s_{h}^{i},a_{h}^{i})-\\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})\\right|}\\\\ &{\\quad+\\operatorname*{max}_{a^{\\prime}\\in A}\\left|Q_{h+1}^{\\star,r}(s_{h+1}^{i},a^{\\prime})-Q_{h+1}^{\\star,\\widehat{r}}(s_{h+1}^{i},a^{\\prime})\\right|\\bigg)}\\\\ &{\\le18H^{2}\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining the above four inequalities yields that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=0}^{k-1}Z_{h}^{i}(Q,r)-\\sum_{i=0}^{k-1}Z_{h}^{i}(\\widehat{Q},\\widehat{r})\\right|\\leq\\sum_{i=0}^{k-1}\\left|Z_{h}^{i}(Q,r)-Z_{h}^{i}(\\widehat{Q},\\widehat{r})\\right|\\leq30k H^{2}\\rho.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, for all $(Q,r)\\in\\mathcal{Q}\\times\\mathcal{R}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{-\\sum_{i=0}^{k-1}Z_{h}^{i}(Q,r)\\le-\\sum_{i=0}^{k-1}Z_{h}^{i}(\\widehat{Q},\\widehat{r})+\\left|\\displaystyle\\sum_{i=0}^{k-1}Z_{h}^{i}(Q,r)-\\sum_{i=0}^{k-1}Z_{h}^{i}(\\widehat{Q},\\widehat{r})\\right|}}\\\\ &{\\le16H^{3}\\log(K H\\operatorname*{max}_{h\\in[H]}|(\\mathcal{Q}_{h})_{\\rho}||(\\mathcal{R}_{h})_{\\rho}|/\\delta)+30k H^{2}\\rho}\\\\ &{\\le16H^{3}\\log(K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+30k H^{2}\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(Q_{h}^{\\star,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\displaystyle\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}Q_{h+1}^{\\star,r}(s_{h+1}^{i},a^{\\prime})\\right)^{2}}\\\\ &{\\le\\displaystyle\\operatorname*{inf}_{Q_{h}\\in\\mathcal{Q}_{h}}\\left(Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\displaystyle\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}Q_{h+1}^{\\star,r}(s_{h+1}^{i},a^{\\prime})\\right)^{2}}\\\\ &{\\quad+16H^{3}\\log(K H\\displaystyle\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+30k H^{2}\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, we can derive the upper bound on $\\mathrm{BE}^{k}(Q^{\\star,r^{k}})$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{BE}^{k}(Q^{\\star,r^{k}})}\\\\ &{=\\displaystyle\\sum_{h=1}^{H}\\Bigg(\\sum_{i=0}^{k-1}\\Big(Q_{h}^{\\star,r^{k}}\\big(s_{h}^{i},a_{h}^{i}\\big)-r_{h}^{k}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}}{\\operatorname*{max}}Q_{h+1}^{\\star,r^{k}}(s_{h+1}^{i},a^{\\prime})\\Big)^{2}}\\\\ &{\\quad-\\displaystyle\\operatorname*{inf}_{Q_{h}^{\\star}\\in Q_{h}}\\sum_{i=0}^{k-1}\\Big(Q_{h}^{\\prime}\\big(s_{h}^{i},a_{h}^{i}\\big)-r_{h}^{k}\\big(s_{h}^{i},a_{h}^{i}\\big)-\\underset{a^{\\prime}}{\\operatorname*{max}}Q_{h+1}^{\\star,r^{k}}\\big(s_{h+1}^{i},a^{\\prime}\\big)\\Big)^{2}\\Bigg)}\\\\ &{\\le16H^{4}\\log(K H\\underset{h\\in[H]}{\\operatorname*{max}}{\\operatorname*{max}}\\mathcal{N}_{\\rho}(Q_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta)+30k H^{3}\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We complete the proof. ", "page_idx": 23}, {"type": "text", "text": "B.6Proof of Lemma 5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For any fixed tuple $(k,h,Q,r)\\in[K]\\times[H]\\times\\mathcal{Q}\\times\\mathcal{R}$ , we define the random variable. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{h}^{i}(Q,r):=\\left(Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}}{\\operatorname*{max}}Q_{h+1}(s_{h+1}^{i},a^{\\prime})\\right)^{2}}\\\\ &{\\qquad\\qquad-\\left((\\mathcal{T}_{h}^{r}Q_{h+1})(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\underset{a^{\\prime}}{\\operatorname*{max}}Q_{h+1}(s_{h+1}^{i},a^{\\prime})\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$\\mathcal{F}^{i}=\\sigma(\\{(s_{1}^{j},a_{1}^{j},\\dots,s_{H}^{j},a_{H}^{j})\\}_{j=0}^{i-1})$ In theflowing at we calcalatethe expectation and variance of $X_{h}^{i}(Q,r)$ conditioned on $\\mathcal{F}^{i}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}Q_{h+1}(s_{h+1}^{i},a^{\\prime})\\right)^{2}\\bigg|\\mathcal{F}^{i}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{E}\\left[\\left((Q_{k}(x_{i})_{i})_{i}\\omega_{i}^{\\top}\\right)-r_{k}(x_{i},\\omega_{i}^{\\top})-m_{k}\\alpha_{k}^{\\top}Q_{k+1}(x_{i}^{\\top},\\omega)\\right)^{2}\\Big|^{p}\\Big]}\\\\ {=}&{\\mathbb{E}\\left[\\left(Q_{k}(x_{i},\\omega_{i}^{\\top})-(T_{k}^{2}Q_{k+1})(x_{i}^{\\top},\\omega_{i}^{\\top})+(T_{k}^{2}Q_{k+1})(x_{i}^{\\top},\\omega_{i}^{\\top})-r_{k}(x_{i}^{\\top},\\omega)-m_{k}\\alpha_{k}Q_{k+1}(x_{i}^{\\top},\\omega_{i}^{\\top})\\right.}\\\\ &{\\left.-\\mathbb{E}\\left[\\left((T_{k}^{2}Q_{k+1})(x_{i},\\omega_{i}^{\\top})-r_{k}(x_{i}^{\\top},\\omega_{i}^{\\top})\\right)-m_{k}\\alpha_{k}Q_{k+1}(x_{i}^{\\top},\\omega_{i}^{\\top})\\right)^{2}\\Big|^{p}\\right]}\\\\ {=}&{\\mathbb{E}\\left[\\left(Q_{k}(x_{i}^{\\top},\\omega_{i}^{\\top})-(T_{k}^{2}Q_{k+1})(x_{i}^{\\top},\\omega_{i}^{\\top})\\right)^{2}\\Big|^{p}\\right]}\\\\ &{+\\mathbb{E}\\left[Q_{k}(x_{i}^{\\top},\\omega_{i}^{\\top})-(T_{k}^{2}Q_{k+1})(x_{i}^{\\top},\\omega_{i}^{\\top})\\right]\\left(T_{k}^{2}Q_{k+1}(x_{i}^{\\top},\\omega_{i}^{\\top})-r_{k}(x_{i}^{\\top},\\omega_{k}^{\\top})-m_{k}\\alpha_{k}Q_{k+1}(x_{i}^{\\top},\\omega_{i}^{\\top})\\right.}\\\\ &{\\left.-\\mathbb{E}\\left[Q_{k}(x_{i}^{\\top},\\omega_{i}^{\\top})-(T_{k}^{2}Q_{k+1})(x_{i}^{\\top},\\omega_{i}^{\\top})\\right]^{2}\\Big|^\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\boldmath~\\var}\\left[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}\\right]}\\\\ &{\\le\\mathbb{E}\\left[\\left(X_{h}^{i}(Q,r)\\right)^{2}|\\mathcal{F}^{i}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(Q_{h}(s_{h}^{i},a_{h}^{i})+(T_{h}^{r}Q_{h+1})(s_{h}^{i},a_{h}^{i})-2r_{h}(s_{h}^{i},a_{h}^{i})-2\\operatorname*{max}Q_{h+1}(s_{h+1}^{i},a^{r})\\right)^{2}\\right]}\\\\ &{\\quad\\cdot\\left(Q_{h}(s_{h}^{i},a_{h}^{i})-(T_{h}^{r}Q_{h+1})(s_{h}^{i},a_{h}^{i})\\right)^{2}\\Bigg|\\mathcal{F}^{i}\\Bigg]}\\\\ &{\\le16H^{2}\\mathbb{E}\\left[\\left(Q_{h}(s_{h}^{i},a_{h}^{i})-(T_{h}^{r}Q_{h+1})(s_{h}^{i},a_{h}^{i})\\right)^{2}\\Bigg|\\pi^{i}\\right]}\\\\ &{=16H^{2}\\mathbb{E}\\left[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Furthermore, $\\{X_{h}^{i}(Q,r)\\,-\\,\\mathbb{E}[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}]\\}_{i=0}^{k-1}$ is a martingale difference sequence adapted to $\\{\\mathcal{F}^{i}\\}_{i=0}^{k-1}$ Besides itis asy to obtain that $|X_{h}^{i}(Q,r)|\\leq9H^{2}$ almost surely, Thus, we can apply Lemma 6 and obtain that with probability at least $1-\\delta$ , for any $\\eta\\in(0,1/(9H^{2})]$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\lvert\\sum_{i=0}^{k-1}X_{h}^{i}(Q,r)-\\sum_{i=0}^{k-1}\\mathbb{E}[X_{h}^{i}(Q,r)\\vert\\mathcal{F}^{i}]\\right\\rvert\\leq\\eta\\displaystyle\\sum_{i=0}^{k-1}\\mathrm{Var}\\left[X_{h}^{i}(Q,r)\\vert\\mathcal{F}^{i}\\right]+\\frac{\\log(2/\\delta)}{\\eta}}&{}\\\\ {\\displaystyle\\leq16H^{2}\\eta\\displaystyle\\sum_{i=0}^{k-1}\\mathbb{E}\\left[X_{h}^{i}(Q,r)\\vert\\mathcal{F}^{i}\\right]+\\frac{\\log(2/\\delta)}{\\eta}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By choosing $\\eta=\\operatorname*{min}\\{1/(9H^{2}),\\sqrt{\\log(2/\\delta)/(16H^{2}\\sum_{i=0}^{k-1}\\mathbb{E}\\left[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}\\right])}\\},$ we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=0}^{k-1}X_{h}^{i}(Q,r)-\\sum_{i=0}^{k-1}\\mathbb{E}[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}]\\right|\\leq8H\\sqrt{\\sum_{i=0}^{k-1}\\mathbb{E}\\left[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}\\right]\\log(2/\\delta)}+9H^{2}\\log(2/\\delta).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k-1}\\mathbb{E}[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}]-8H\\sqrt{\\sum_{i=0}^{k-1}\\mathbb{E}\\left[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}\\right]\\log(2/\\delta)}\\leq\\sum_{i=0}^{k-1}X_{h}^{i}(Q,r)+9H^{2}\\log(2/\\delta).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This establishes a quadratic formula of $x^{2}\\mathrm{~-~}b x\\mathrm{~-~}c\\mathrm{~\\leq~}0$ with $\\begin{array}{r}{x\\;=\\;\\sqrt{\\sum_{i=0}^{k-1}\\mathbb{E}[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}]},}\\end{array}$ $b=8H\\sqrt{\\log(2/\\delta)}$ and $\\begin{array}{r}{c=\\sum_{i=0}^{k-1}X_{h}^{i}(Q,r)+9H^{2}\\log(2/\\delta)}\\end{array}$ Solving this quadratic formula yields that $(b-\\sqrt{b^{2}+4c})/2\\leq x\\leq(b+\\sqrt{b^{2}+4c})/2$ , which implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\nx^{2}\\leq{\\frac{(b+{\\sqrt{b^{2}+4c}})^{2}}{4}}\\leq{\\frac{2\\left(b^{2}+b^{2}+4c\\right)}{4}}=b^{2}+2c.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus we obtain that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k-1}\\mathbb{E}[X_{h}^{i}(Q,r)|\\mathcal{F}^{i}]\\leq2\\sum_{i=0}^{k-1}X_{h}^{i}(Q,r)+82H^{2}\\log(2/\\delta).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We define $(\\mathcal{Q}_{h})_{\\rho}$ and $(\\mathcal{R}_{h})_{\\rho}$ as the $\\rho$ -covers of ${\\mathcal{Q}}_{h}$ and $\\mathcal{R}_{h}$ , respectively. It is direct to have that $\\mathcal{Q}_{\\rho}=(\\mathcal{Q}_{1})_{\\rho}\\times\\dot{\\dots}(\\mathcal{Q}_{H})_{\\rho}$ and $\\mathcal{R}_{\\rho}=(\\mathcal{R}_{1})_{\\rho}\\times...\\,(\\mathcal{R}_{H})_{\\rho}$ are $\\rho$ -covers of $\\mathcal{Q}$ and $\\mathcal{R}$ , respectively. By union bound, with probability at least $1-\\delta$ , for all $(k,h,\\widehat{Q},\\widehat{r})\\in[K]\\times[H]\\times\\,\\mathcal{Q}_{\\rho}\\times\\mathcal{R}_{\\rho}.$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{k-1}\\mathbb{E}[X_{h}^{i}(\\widehat{Q},\\widehat{r})|\\mathcal{F}^{i}]\\leq2\\displaystyle\\sum_{i=0}^{k-1}X_{h}^{i}(\\widehat{Q},\\widehat{r})+82H^{2}\\log(2K H|Q_{\\rho}||\\mathcal{R}_{\\rho}|/\\delta)}\\\\ &{\\displaystyle=2\\displaystyle\\sum_{i=0}^{k-1}X_{h}^{i}(\\widehat{Q},\\widehat{r})+82H^{2}\\log\\left(2K H\\prod_{h=1}^{H}\\left(|(Q_{h})_{\\rho}||(\\mathcal{R}_{h})_{\\rho}|\\right)/\\delta\\right)}\\\\ &{\\displaystyle\\leq2\\displaystyle\\sum_{i=0}^{k-1}X_{h}^{i}(\\widehat{Q},\\widehat{r})+82H^{3}\\log\\left(2K H\\operatorname*{max}_{h\\in[H]}|(\\mathcal{Q}_{h})_{\\rho}||(\\mathcal{R}_{h})_{\\rho}|/\\delta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We have calculated the conditional expectation in the LHS and obtain that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{k-1}\\mathbb{E}\\left[\\left(\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{\\widehat{r}}\\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})\\right)^{2}\\bigg|\\pi^{i}\\right]}\\\\ &{\\displaystyle\\leq2\\displaystyle\\sum_{i=0}^{k-1}X_{h}^{i}(\\widehat{Q},\\widehat{r})+82H^{3}\\log\\left(2K H\\operatorname*{max}_{h\\in[H]}|(\\mathcal{Q}_{h})_{\\rho}||(\\mathcal{R}_{h})_{\\rho}|/\\delta\\right)}\\\\ &{\\displaystyle\\leq2\\displaystyle\\sum_{i=0}^{k-1}X_{h}^{i}(\\widehat{Q},\\widehat{r})+82H^{3}\\log\\left(2K H\\operatorname*{max}_{h\\in[H]}N_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "According to the definition of $\\rho$ -cover, for $(Q^{k},r^{k})$ , there exists $(\\widehat{Q},\\widehat{r})\\in\\mathcal{Q}_{\\rho}\\times\\mathcal{R}_{\\rho}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{(s,a,h)\\in S\\times A\\times[H]}\\left|\\widehat{Q}_{h}(s,a)-Q_{h}^{k}(s,a)\\right|\\leq\\rho,\\ \\operatorname*{max}_{(s,a,h)\\in S\\times A\\times[H]}\\left|\\widehat{r}_{h}(s,a)-r_{h}^{k}(s,a)\\right|\\leq\\rho.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then we can upper bound the errors caused by approximating $(Q^{k},r^{k})$ with $(\\widehat{Q},\\widehat{r})$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\bigg|\\left(\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{\\widehat{r}}\\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})\\right)^{2}-\\left(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{\\boldsymbol{r}^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\\right)^{2}\\bigg|}\\\\ &{\\le\\left|\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{\\widehat{r}}\\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})+Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{\\boldsymbol{r}^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\\right|}\\\\ &{\\quad\\left|\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{\\widehat{r}}\\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})-Q_{h}^{k}(s_{h}^{i},a_{h}^{i})+(\\mathcal{T}_{h}^{\\boldsymbol{r}^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\\right|}\\\\ &{\\le2H\\left|\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{\\widehat{r}}\\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})-Q_{h}^{k}(s_{h}^{i},a_{h}^{i})+(\\mathcal{T}_{h}^{\\boldsymbol{r}^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\\right|}\\\\ &{\\le6H\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|X_{h}^{i}(\\widehat{Q},\\widehat{r})-X_{h}^{i}(Q^{k},r^{k})\\right|}\\\\ &{\\leq\\bigg|\\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})+Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-\\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})-r_{h}^{k}(s_{h}^{i},a_{h}^{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{-\\operatorname*{inp}\\phi_{1+1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})-m\\phi_{2+1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})\\pi\\Big|}\\\\ &{-\\frac{1}{n}\\Big\\{\\bar{\\phi}_{1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}})-m\\phi_{1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})-\\bar{\\phi}_{2}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}})+\\bar{\\phi}_{2}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}})}\\\\ &{-\\frac{1}{n}\\bar{\\phi}_{1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}})+m\\phi_{2}\\phi_{1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}})+m\\phi_{2}\\Big\\}}\\\\ &{+\\frac{1}{n}\\left(\\pi^{2}\\bar{\\phi}_{2+1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}})+m\\phi_{2}\\phi_{1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})^{2}\\right)}\\\\ &{-\\frac{m}{n}\\bar{\\phi}_{1+1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})-m\\phi_{2}\\phi_{2-1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})\\pi\\Big|}\\\\ &{-\\frac{m}{n}\\bar{\\phi}_{1+1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})\\pi\\cdot\\frac{m}{n}\\frac{m}{\\sqrt{n}}\\frac{\\bar{\\phi}_{1+1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}})-m^{2}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})}{n}\\pi\\Big|}\\\\ &{\\cdot\\Big|\\pi^{2}\\bar{\\phi}_{1+1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})-\\Omega^{\\prime}\\bar{\\phi}_{2-1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})\\pi\\Big|,}\\\\ &{-m\\phi_{2}\\phi_{1+1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})+m\\Psi\\Big|\\phi_{1+1}(\\phi_{\\pm\\frac{1}{\\sqrt{n}}-1})\\Big|}\\\\ &{\\le M\\Big\\{\\bar{\\phi} \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With the above bounds, we can obtain that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{k-1}\\mathbb{E}\\left[\\left(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\\mathcal{T}_{h}^{r^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\\right)^{2}\\bigg|\\pi^{i}\\right]}\\\\ &{\\displaystyle\\leq2\\sum_{i=0}^{k-1}X_{h}^{i}(Q^{k},r^{k})+82H^{3}\\log\\left(2K H\\operatorname*{max}_{h\\in[H]}\\mathcal{N}_{\\rho}(\\mathcal{Q}_{h})\\mathcal{N}_{\\rho}(\\mathcal{R}_{h})/\\delta\\right)+54k H\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "According to the definition of $\\mathrm{BE}^{k}$ , we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{h=1}^{\\mathrm{BE}^{k}\\big(Q_{h}^{k}\\big)}}\\\\ &{=\\displaystyle\\sum_{h=1}^{H^{k}\\big(E_{h}^{k-1}\\big(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-r_{h}^{k}(s_{h}^{i},a_{h}^{i})-\\operatorname*{max}_{a^{-}}Q_{h+1}^{k}(s_{h+1}^{i},a^{t})\\big)^{2}}}\\\\ &{\\quad-\\operatorname*{inf}_{\\phi\\in\\mathcal{C}_{h-1}^{k}}\\sum_{i=1}^{H^{k}}\\Big(Q_{h}^{k}\\big(s_{h}^{i},a_{h}^{i}\\big)-r_{h}^{k}\\big(s_{h}^{i},a_{h}^{i}\\big)-\\operatorname*{max}_{a^{-}}Q_{h+1}^{k}\\big(s_{h+1}^{i},a^{t}\\big)\\Big)^{2}}\\\\ &{\\overset{(a)}{\\displaystyle\\int}\\displaystyle\\sum_{h=1}^{H^{k}\\big(E_{h}^{k-1}\\big)}X_{h}^{i}(Q^{k},r^{k})}\\\\ &{\\geq\\displaystyle\\frac{1}{2}\\sum_{h=1}^{H^{k}}\\sum_{i=0}^{k-1}\\mathbb{E}\\bigg[\\Big(Q_{h}^{k}\\big(s_{h}^{i},a_{h}^{i}\\big)-(T_{h}^{k}Q_{h+1}^{k})\\big(s_{h}^{i},a_{h}^{i}\\big)\\Big)^{2}\\bigg|\\neq\\bigg]}\\\\ &{\\quad-41H^{k}\\log\\left(2K H\\operatorname*{max}_{h\\in\\mathbb{H}}N_{\\rho}(Q_{h})\\Lambda_{\\rho}^{\\rho}(R_{h})/\\delta\\right)-27k H^{2}\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Inequality(afollows Assumption 3 that $T_{h}^{r^{k}}Q_{h+1}^{r^{k}}\\in\\mathcal{Q}_{h}$ We complete the prof. ", "page_idx": 26}, {"type": "text", "text": "B.7  Technical Lemmas ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma 6 (Freedman's inequality [3]). Let $(X_{t})_{t\\leq T}$ be a real-valued martingale difference sequence adapted to filtration $\\mathcal{F}_{t}$ and let $\\mathbb{E}_{t}[\\cdot]=\\mathbb{E}[\\cdot\\mid\\mathcal{F}_{t}]$ If $|X_{t}|\\le R$ almost surely, then for any $\\eta\\in[0,\\frac{1}{R}]$ it holds that with probability at least $1-\\delta$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}X_{t}\\leq\\eta\\sum_{t=1}^{T}\\mathbb{E}_{t-1}[X_{t}^{2}]+\\frac{\\log(1/\\delta)}{\\eta}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma 7. For any reward functions $r,\\widehat{r}_{;}$ wehave that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H],\\;\\left|Q_{h}^{\\star,r}(s,a)-Q_{h}^{\\star,\\widehat{r}}(s,a)\\right|\\leq\\sum_{h^{\\prime}=h}^{H}\\operatorname*{max}_{s\\in\\mathcal{S},a\\in\\mathcal{A}}\\left|r_{h}(s,a)-\\widehat{r}_{h}(s,a)\\right|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here $Q^{\\star,r}$ is the optimal $Q$ -valuefunction of $r$ ", "page_idx": 27}, {"type": "text", "text": "Proof. According to the Bellman optimality equation, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\allowdisplaybreaks}&{s,a)-Q_{h}^{s,\\varepsilon}(s,a)\\Big|}\\\\ &{=\\int_{\\mathbb{R}}\\Big(s,a)+\\mathbb{E}_{x^{\\prime}\\sim\\mathcal{N}_{h}(\\cdot),a}\\left[\\operatorname*{max}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})-\\operatorname*{max}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})\\right]\\right]}\\\\ &{=\\int_{\\mathbb{R}}\\Big(s,a)+\\mathbb{E}_{x^{\\prime}\\sim\\mathcal{N}_{h}(\\cdot),a}\\left[\\operatorname*{max}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})-\\operatorname*{max}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})\\right]\\right]}\\\\ &{=\\int_{\\mathbb{R}}\\Big(s,a)+Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})-\\operatorname*{max}_{s\\in\\mathcal{N}_{h}(\\cdot)}\\left[\\operatorname*{max}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})\\right]-\\operatorname*{max}Q_{h+1}^{s,\\varepsilon}\\mathcal{Q}_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})\\Big|\\right],}\\\\ &{\\operatorname*{sup}\\operatorname*{max}_{s\\in\\mathcal{N}_{h}(\\cdot)}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})-\\operatorname*{max}_{s\\in\\mathcal{N}_{h}(\\cdot)}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})\\Big|,}\\\\ &{\\operatorname*{max}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})-\\operatorname*{max}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})+\\mathscr{O}_{h}^{(s,\\varepsilon)}(s^{\\prime},a^{\\prime})}\\\\ &{=Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})-Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})}\\\\ &{\\leq Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})-Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})}\\\\ &{\\operatorname*{max}Q_{h+1}^{s,\\varepsilon}(s^{\\prime},a^{\\prime})-\\\n$$We analyze the t ", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here $a^{1}\\in\\mathrm{argmax}_{a^{\\prime}\\in A}\\,Q_{h+1}^{\\star,r}(s^{\\prime},a^{\\prime}),a^{2}\\in\\mathrm{argmax}_{a^{\\prime}\\in A}\\,Q_{h+1}^{\\star,\\widehat{r}}(s^{\\prime},a^{\\prime})$ . Thus, we can get that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{\\boldsymbol{a}^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{\\star,r}(s^{\\prime},\\boldsymbol{a}^{\\prime})-\\underset{\\boldsymbol{a}^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{\\star,\\widehat{r}}(s^{\\prime},\\boldsymbol{a}^{\\prime})\\right|\\leq\\underset{\\boldsymbol{a}^{\\prime}\\in A}{\\operatorname*{max}}Q_{h+1}^{\\star,r}(s^{\\prime},\\boldsymbol{a}^{\\prime})-Q_{h+1}^{\\star,\\widehat{r}}(s^{\\prime},\\boldsymbol{a}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\underset{\\boldsymbol{a}^{\\prime}\\in A}{\\operatorname*{max}}\\left|Q_{h+1}^{\\star,r}(s^{\\prime},\\boldsymbol{a}^{\\prime})-Q_{h+1}^{\\star,\\widehat{r}}(s^{\\prime},\\boldsymbol{a}^{\\prime})\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we have that $\\forall(s,a)\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|Q_{h}^{\\star,r}(s,a)-Q_{h}^{\\star,\\widehat{r}}(s,a)\\right|}\\\\ &{\\leq\\left|r_{h}(s,a)-\\widehat{r}_{h}(s,a)\\right|+\\mathbb{E}_{s^{\\prime}\\sim P_{h}(\\cdot\\vert s,a)}\\left[\\left|\\operatorname*{max}_{a^{\\prime}\\in A}Q_{h+1}^{\\star,r}(s^{\\prime},a^{\\prime})-\\operatorname*{max}_{a^{\\prime}\\in A}Q_{h+1}^{\\star,\\widehat{r}}(s^{\\prime},a^{\\prime})\\right|\\right]}\\\\ &{\\leq\\left|r_{h}(s,a)-\\widehat{r}_{h}(s,a)\\right|+\\operatorname*{max}_{s^{\\prime}\\in S,a^{\\prime}\\in A}\\left|Q_{h+1}^{\\star,r}(s^{\\prime},a^{\\prime})-Q_{h+1}^{\\star,\\widehat{r}}(s^{\\prime},a^{\\prime})\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Applying the above recursion inequality repeatedly from $h^{\\prime}=h$ to $h^{\\prime}\\,=\\,H$ With $Q_{H+1}^{\\star,r}(s,a)=$ $Q_{H+1}^{\\star,\\widehat{r}}(s,a)=0$ completes the prof. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Lemma 8. For $a\\ge1$ and $\\varepsilon\\leq1$ ,when $K\\geq4\\log(4a/\\varepsilon)/\\varepsilon^{2}$ ,we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\sqrt{\\frac{\\log(a K)}{K}}}\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We consider the function $f(K)=\\sqrt{\\log(a K)/K}$ and calculate the gradient. ", "page_idx": 28}, {"type": "equation", "text": "$$\nf^{\\prime}(K)=\\frac{1}{2}\\left(\\frac{\\log(a K)}{K}\\right)^{-1/2}\\left(\\frac{1-\\log(a K)}{K^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "When $K\\geq4\\log(4a/\\varepsilon)/\\varepsilon^{2}\\geq4$ we have that $f^{\\prime}(K)\\leq0$ , implying that $f(K)$ is a monotonically decreasing function in this range. Then we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\frac{\\log(a K)}{K}}\\le\\sqrt{\\frac{\\log\\left(4a\\log(4a/\\varepsilon)/\\varepsilon^{2}\\right)}{4\\log(4a/\\varepsilon)}}\\varepsilon}\\\\ &{\\qquad\\qquad=\\sqrt{\\frac{\\log(4a/\\varepsilon)+\\log(\\log(4a/\\varepsilon))+\\log(1/\\varepsilon)}{4\\log(4a/\\varepsilon)}}\\varepsilon}\\\\ &{\\qquad\\qquad\\overset{(a)}{\\le}\\sqrt{\\frac{\\log(4a/\\varepsilon)+\\log(4a/\\varepsilon)+\\log(1/\\varepsilon)}{4\\log(4a/\\varepsilon)}}\\varepsilon}\\\\ &{\\qquad\\qquad\\overset{(b)}{\\le}\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Inequality $(a)$ follows that $\\log(x)\\leq x+1$ and inequality $(b)$ follows that $a\\geq1$ ", "page_idx": 28}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "C.1 Implementation Details of OPT-AIL ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Reward Update. As mentioned in Section 4.2, we choose $\\psi(r)$ in Eq. (2) as the gradient penalty (GP) regularization of the reward model [6], which can help stabilize the online optimization process by enforcing 1-Lipschitz continuity of the reward model $r$ .Here $\\mathcal{D}^{I}$ is linear interpolations between the replay buffer $\\dot{\\mathcal{D}}^{k}$ and expert demonstrations $\\mathcal{D}^{\\mathrm{E}}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\psi(r)=\\mathbb{E}_{\\tau\\sim\\mathcal{D}^{I}}\\left[\\sum_{h=1}^{H}(\\Vert\\nabla r_{h}(s_{h},a_{h})\\Vert-1)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Policy Update. Here we present the implementation details of policy updates. Firstly, to stabilize the training process, we refine the optimism regularization term by subtracting a baseline Q-value function from random policy $\\mu\\equiv\\operatorname{Unif}(A)$ , which has been utilized in [29, 32]. Furthermore, recognizing that initial state samples can be limited and lack diversity, we employ both the replay buffer $\\mathcal{D}^{k}$ and expert demonstrations $\\mathcal{D}^{\\mathrm{E}}$ to compute the $\\mathrm{^Q}$ -value loss, which is a common data augmentation approach and has been validated in many deep AIL methods [28, 15, 56]. Incorporating these two enhancements, we reformulate the Q-value model training objective as follows. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{Q\\in\\mathcal{Q}}\\mathbb{E}_{\\tau\\sim\\mathcal{D}^{k}\\cup\\mathcal{D}^{\\mathtt{E}}}\\left[\\sum_{h=1}^{H}\\left(Q_{h}(s_{h},a_{h})-r_{h}^{k}-\\overline{{Q}}_{h+1}(s_{h+1},\\pi^{k})\\right)^{2}\\right]}\\\\ &{\\quad-\\lambda\\mathbb{E}_{\\tau\\sim\\mathcal{D}^{k}\\cup\\mathcal{D}^{\\mathtt{E}}}\\left[\\sum_{h=1}^{H}\\left(Q_{h}(s_{h},\\pi^{k})-Q_{h}(s_{h},\\mu)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "C.2  Architecture and Training Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The experiments are conducted on a machine with 64 CPU cores and 4 RTX4090 GPU cores. Each experiment is replicated five times using different random seeds. For each task, we adopt online DrQ-v2 [66] to train an agent with sufficient environment interactions 1 and regard the resultant policy as the expert policy. Then we roll out this expert policy to collect expert demonstrations. The architecture and training details of OPT-AIL and all baselines are listed below. ", "page_idx": 28}, {"type": "text", "text": "OPT-AIL: Our codebase of OPT-AIL extends the open-sourced framework of IQLearn. We retain the structure and parameter design of the actor and critic from the original framework while employing SAC [17] with a fixed temperature for policy update. We also implement a discriminator with a similar architecture to the critic network, and additionally incorporate layer normalization and tanh activation before the output to improve training stability. A comprehensive enumeration of the hyperparameters of OPT-AIL is provided in Table 2. ", "page_idx": 29}, {"type": "text", "text": "BC: We implement BC based on our codebase. The actor model is trained using Mean Squared Error (MSE) loss over 10k training steps. ", "page_idx": 29}, {"type": "text", "text": "PPIL: We use the author's codebase, which is available at https://github.com/lviano/p2il. ", "page_idx": 29}, {"type": "text", "text": "IQLearn: We use the author's codebase, which is available at https://github.com/Div99/IQ-Learn. ", "page_idx": 29}, {"type": "text", "text": "DAC: We reproduce the DAC based on our codebase. Due to the difference in updating the discriminator compared to OPT-AIL, we refer to the official DAC implementation when reproducing the discriminator. We remove the layer normalization and the tanh activation function before the output, and find that this resulted in better performance. ", "page_idx": 29}, {"type": "text", "text": "FILTER: We use the author's codebase, which is available at https://github.com/gkswamy98/fast_irl ", "page_idx": 29}, {"type": "text", "text": "HyPE: We use the author's codebase, which is available at https://github.com/gkswamy98/hyper. ", "page_idx": 29}, {"type": "text", "text": "We emphasize that for a fair comparison, all algorithms are implemented using the same codebase 2, with all hyperparameters kept consistent except for the gradient penalty coefficient. Specifically, in OPT-AIL, the gradient penalty coefficient is set to 1 for Cartpole Swingup, Walker Walk, and Walker Stand, and 10 for other tasks. For baselines, the gradient penalty coefficient is always set to 10 as provided by the authors. We also attempt to adjust this parameter for the baselines but find that the default parameters provided by the authors work well. ", "page_idx": 29}, {"type": "table", "img_path": "7YdafFbhxL/tmp/61bb2023a426a244dfa8c9e0ef2cceb0b841cea484aa31e715dc8ddc749056bb.jpg", "table_caption": ["Table 2: OPT-AIL Hyper-parameters. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "D  Additional Experimental Results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we list the learning curves for 8 DMControl tasks with 4, 7, and 10 expert trajectories respectively. The corresponding results are depicted in Figure 3, Figure 4, and Figure 5. Here the ${\\bf X}$ -axis is the number of environment interactions and the y-axis is the return. The solid lines are the mean of results while the shaded region corresponds to the standard deviation over 5 random seeds. Our results demonstrate that OPT-AIL consistently achieves better interaction sample efficiency than state-of-the-art (SOTA) deep AIL methods, across varying numbers of expert trajectories. ", "page_idx": 30}, {"type": "image", "img_path": "7YdafFbhxL/tmp/ab5d6f60b05473327ad825118b2c4083b3ef3b2ba91284531c91bfe0a31be84c.jpg", "img_caption": ["Figure 3: Learning curves on 8 DMControl tasks over 5 random seeds using 4 expert trajectories. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "7YdafFbhxL/tmp/dc226bbb4387df2d4c2118c57baeb6fe19574a48d1dc67a1b9218766d783a304.jpg", "img_caption": ["Figure 4: Learning curves on 8 DMControl tasks over 5 random seeds using 7 expert trajectories. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "7YdafFbhxL/tmp/16c5d49556fc4e23b9a0eef2fc7a3679fcf665cc5ee4e04ebdc2b81c8db5d213.jpg", "img_caption": ["Figure 5: Learning curves on 8 DMControl tasks over 5 random seeds using 10 expert trajectories. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We clearly state the contribution and scope of this paper in the abstract and introduction. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The limitations of this work are discussed in Section 4.1 and Section 6. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The full set of assumptions is stated in each theoretical result and the complete and correct proofs are provided in Appendix B. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We present all implementation details for reproducing the main experimental results of this paper in Appendix C. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (t) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We submit the code for reproducing the main experimental results in the supplemental material. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All experimental details are described in Section 5 and Appendix C. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We report the standard deviation over 5 random seeds for all experiments in this paper; see the detailed results in Section 5 and Appendix D. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We describe the information on the computer resources for running the experiments in Appendix C. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper investigates the theoretical underpinnings of imitation learning and conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We discuss both potential positive societal impacts and negative societal impacts of this work in Appendix A. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 35}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper conducts experiments on a simulated environment for continuous control and poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We cite the original paper that produced the codebase and expert dataset, and provide the corresponding URLs in Appendix C. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}]