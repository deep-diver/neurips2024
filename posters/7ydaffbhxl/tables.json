[{"figure_path": "7YdafFbhxL/tables/tables_1_1.jpg", "caption": "Table 1: A summary of the expert sample complexity and interaction complexity. Here H is the horizon length, \u025b is the desired imitation gap, |S| is the state space size, |A| is the action space size, |\u03a0| is the cardinality of the finite policy class \u03a0, d is the dimension of the feature space, dGEC is the generalized eluder coefficient, N(R<sub>h</sub>) and N(Q<sub>h</sub>) are the covering numbers of the reward class R<sub>h</sub> and Q-value class Q<sub>h</sub>, respectively. We use \u00d5 to hide logarithmic factors.", "description": "This table summarizes the expert sample complexity and interaction complexity of several imitation learning algorithms under different settings.  The settings include tabular Markov Decision Processes (MDPs), linear mixture MDPs, and MDPs with general function approximation. The algorithms are behavioral cloning (BC), optimization-based adversarial imitation learning (OAL), modified Bayesian adversarial imitation learning (MB-TAIL), optimism-guided adversarial imitation learning (OGAIL), Bellman residual guided imitation learning (BRIG), and the proposed optimization-based adversarial imitation learning (OPT-AIL). The table shows the complexity results in big O notation, highlighting the dependence on the horizon length (H), the desired imitation gap (\u025b), state and action space sizes (|S|, |A|), policy class size (|\u03a0|), feature dimension (d), and generalized eluder coefficient (dGEC).", "section": "1 Introduction"}, {"figure_path": "7YdafFbhxL/tables/tables_29_1.jpg", "caption": "Table 1: A summary of the expert sample complexity and interaction complexity. Here H is the horizon length, \u03b5 is the desired imitation gap, |S| is the state space size, |A| is the action space size, |\u03a0| is the cardinality of the finite policy class \u03a0, d is the dimension of the feature space, dGEC is the generalized eluder coefficient, N(Rh) and N(Qh) are the covering numbers of the reward class Rh and Q-value class Qh, respectively. We use O\u0303 to hide logarithmic factors.", "description": "This table summarizes the expert sample complexity and interaction complexity of various imitation learning algorithms under different settings. The settings include tabular Markov Decision Processes (MDPs), linear MDPs, and MDPs with general function approximation. The complexities are expressed in terms of the horizon length (H), desired imitation gap (\u03b5), state space size (|S|), action space size (|A|), cardinality of the policy class (|\u03a0|), feature dimension (d), generalized eluder coefficient (dGEC), and covering numbers of reward and Q-value function classes (N(Rh), N(Qh)).", "section": "1 Introduction"}]