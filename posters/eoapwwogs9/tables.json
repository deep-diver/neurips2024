[{"figure_path": "eOAPWWOGs9/tables/tables_3_1.jpg", "caption": "Table 1: Performance of Mixtral-Instruct on GSM8K. All results are reported in accuracy (%).", "description": "This table presents the performance of the Mixtral-Instruct language model on the GSM8K benchmark dataset.  It shows the accuracy of the model in correctly identifying the top answer (Pass@1), the top 5 answers (Pass@5), and the self-consistency of its responses.  The results are expressed as percentages.", "section": "3.2 Motivation"}, {"figure_path": "eOAPWWOGs9/tables/tables_3_2.jpg", "caption": "Table 2: Comparison of different selection methods across various model sizes for selecting a response from candidate responses generated by Mixtral-Instruct. All results are reported in accuracy (%).", "description": "This table compares the performance of different LLMs (Mistral-Instruct, Llama2-chat, and Qwen) as response selectors, each using various prompt strategies (Pairwise Classification, Classification + CoT, Scoring, Scoring + CoT) to choose the correct answer from multiple candidate responses generated by Mixtral-Instruct. The accuracy of each method is presented for different model sizes, showing the effectiveness of each approach.", "section": "3.3 Training Methodology"}, {"figure_path": "eOAPWWOGs9/tables/tables_4_1.jpg", "caption": "Table 3: Performance of OSV models across different configurations.", "description": "This table presents the performance of two different Outcome-Supervised Verifier (OSV) models (OSV (Mistral) and OSV (Phi)) across various Large Language Models (LLMs) used as response generators.  The performance is measured by three metrics: Pass@1, Pass@5, and Self-Consistency (SC). Pass@k represents the percentage of times the correct response is among the top k responses. Self-Consistency evaluates the consistency of the model's predictions across multiple runs. The results showcase the effectiveness and scalability of the OSV models in selecting the correct answer from multiple LLM outputs, particularly when using larger and more powerful response generators.  The OSV models consistently outperform the Self-Consistency baseline, highlighting their potential to improve response selection in various reasoning tasks.", "section": "4.1 Experiment on Outcome-Supervised Verifier Performance"}, {"figure_path": "eOAPWWOGs9/tables/tables_5_1.jpg", "caption": "Table 4: Model sizes and training data accuracy for training OSVs.", "description": "This table presents a consolidated view of the model sizes (OSV (Mistral) and OSV (Phi)) along with the precision metrics of their outcome supervision training data.  It highlights the training data quality (accuracy) and quantity (number of samples per question) used for training each outcome-supervised verifier model. The difference in training data quality is attributed to the capabilities of their respective base models.", "section": "4.1 Experiment on Outcome-Supervised Verifier Performance"}, {"figure_path": "eOAPWWOGs9/tables/tables_6_1.jpg", "caption": "Table 5: Process Calculation Error Detection Performance with Varying Threshold (\u03b8) Values.", "description": "This table presents the performance of the math calculation error detection method across different threshold values.  The metrics used to evaluate the performance are Precision (Prec.), Recall, and F1-Score.  Precision measures the proportion of correctly identified calculation errors, Recall measures the proportion of actual calculation errors identified, and F1-Score is the harmonic mean of Precision and Recall providing a balanced measure of performance. The table shows how these metrics change as the threshold value (\u03b8) varies, which is a parameter of the method.", "section": "4 Preliminary Findings"}, {"figure_path": "eOAPWWOGs9/tables/tables_7_1.jpg", "caption": "Table 6: Results on mathematics benchmarks.", "description": "This table presents the performance of three different response selection methods (Self-Consistency, Outcome-Supervised Verifier (OSV), and Process-Supervised enhanced verifier (OSV+PSV)) on two mathematical reasoning benchmarks: GSM8K and MATH.  The results are reported as Pass@5, which represents the probability of selecting the correct answer among the top 5 candidates generated by an LLM.  The table shows the performance of each method for three different LLMs: Mistral-Instruct, Mixtral-Instruct, and Qwen, highlighting the effectiveness of the process-supervised approach (OSV+PSV).", "section": "5.2 Enhanced LLMs Reasoning via Process Supervision"}, {"figure_path": "eOAPWWOGs9/tables/tables_7_2.jpg", "caption": "Table 7: Results on commonsense reasoning benchmarks.", "description": "This table presents the results of the three different methods (Self-Consistency, Outcome-supervised verifier, and Process-supervised enhanced verifier) on three commonsense reasoning benchmarks: HellaSwag, Winogrande, and ANLI.  The results are shown for three different response generators: Mistral-Instruct, Mixtral-Instruct, and Qwen-72b. Pass@5, representing the top 5 accuracy, is reported for each method and generator combination on each benchmark.", "section": "5.2 Enhanced LLMs Reasoning via Process Supervision"}, {"figure_path": "eOAPWWOGs9/tables/tables_8_1.jpg", "caption": "Table 8: Comparison of process labeling methods' performance across different response generators on GSM8K and MATH datasets. The table evaluates the Pass@5, Self-Consistency (Self-Cons.), and response selection performance of models fine-tuned using process annotations labeled by MCTS and AUTOPSV.", "description": "This table compares the performance of two process labeling methods (MCTS and AUTOPSV) on two datasets (GSM8K and MATH) across three different large language models (Mistral-Instruct, Mixtral-Instruct, and Qwen-72B).  The Pass@5 metric represents the accuracy of selecting the correct response from five candidates. Self-Consistency serves as a baseline, while Process (MCTS) represents a commonly used method for generating process-level annotations. The table highlights the effectiveness of AUTOPSV in improving the response selection performance of LLMs by providing more efficient process annotations compared to MCTS.", "section": "6.1 Advantages of AutoPSV in Labeled and Unlabeled Settings"}, {"figure_path": "eOAPWWOGs9/tables/tables_8_2.jpg", "caption": "Table 9: Comparison of annotation costs between MCTS and AUTOPSV for process labeling on the GSM8K and MATH datasets. Annotation cost represents the processed tokens into a model when generating process annotations, encompassing both input and output tokens.", "description": "This table compares the computational costs of two process annotation methods: MCTS and AUTOPSV. It shows the average and total number of steps and tokens processed for each method on the GSM8K and MATH datasets. The results highlight the significantly lower computational cost of AUTOPSV compared to MCTS.", "section": "6.1 Advantages of AutoPSV in Labeled and Unlabeled Settings"}, {"figure_path": "eOAPWWOGs9/tables/tables_8_3.jpg", "caption": "Table 10: Performance enhancement of our proposed AutoPSV method in unlabeled settings, where both MCTS and OSV-only training are unable to utilize unlabeled data.", "description": "This table compares the performance of different methods for process labeling in unlabeled settings.  It shows the Pass@5 scores achieved by different LLMs (Mistral-Instruct, Mixtral-Instruct, and Qwen) using three methods: Self-Consistency (baseline), OSV trained only on GSM8K data, MCTS on GSM8K data, OSV+PSV on GSM8K, and OSV+PSV trained on both GSM8K and WizardLM unlabeled data. The results demonstrate the effectiveness of AutoPSV in leveraging unlabeled data to improve performance.", "section": "6.1 Advantages of AutoPSV in Labeled and Unlabeled Settings"}, {"figure_path": "eOAPWWOGs9/tables/tables_9_1.jpg", "caption": "Table 11: Experimental results showing the performance of OSV models across different configurations tested on GSM8K test sets.", "description": "This table shows the performance comparison of different models on the GSM8K test set. The models compared are Self-Consistency (SC), Outcome-Supervised Verifier (OSV), and Process-Supervised Verifier (PSV).  The results are presented for three different response generators: Mistral-Instruct, Mixtral-Instruct, and Qwen-72b.  The metrics used to evaluate performance are Pass@1 and Pass@5, representing the percentage of times the correct answer is ranked first and within the top five, respectively. The table highlights the impact of incorporating process-level supervision (PSV) on model performance compared to outcome-level supervision (OSV) and the baseline Self-Consistency method.", "section": "6.2 Outcome-Supervised Verification vs. Process-Supervised Verification"}, {"figure_path": "eOAPWWOGs9/tables/tables_16_1.jpg", "caption": "Table 6: Results on mathematics benchmarks.", "description": "This table presents the results of the experiments conducted on mathematical reasoning benchmarks (GSM8K and MATH). It compares the performance of three different approaches: Self-Consistency (Self-Cons.), Outcome-supervised verifier (OSV), and Process-supervised enhanced verifier (OSV + PSV). The performance metric used is Pass@5, representing the top 5 accuracy in selecting the correct answer. The table is broken down by response generator (Mistral-Instruct, Mixtral-Instruct, and Qwen) and benchmark dataset. This allows for a detailed comparison of the different methods across different scales of LLMs and problem types.", "section": "5.2 Enhanced LLMs Reasoning via Process Supervision"}, {"figure_path": "eOAPWWOGs9/tables/tables_18_1.jpg", "caption": "Table 3: Performance of OSV models across different configurations.", "description": "This table presents the performance of two Outcome-Supervised Verifier (OSV) models, OSV (Mistral) and OSV (Phi), across various Large Language Models (LLMs) used as response generators.  The performance is measured by Pass@1, Pass@5, and Self-Consistency (SC) on the GSM8K dataset.  It demonstrates the effectiveness and scalability of OSV in response selection.", "section": "4.1 Experiment on Outcome-Supervised Verifier Performance"}, {"figure_path": "eOAPWWOGs9/tables/tables_18_2.jpg", "caption": "Table 12: Performance of the LlaMA model on GSM8K training data, evaluated through few-shot cot prompting strategy.", "description": "This table presents the performance of the LlaMA model on the GSM8K training dataset.  The evaluation is done using a few-shot chain-of-thought (cot) prompting strategy. Three key performance metrics are reported: Pass@5 (the percentage of times the correct answer is among the top 5 generated answers), Self-Consistency (a measure of the model's internal consistency in generating answers), and Math Calculation Error Detection (the percentage of times the model correctly identifies mathematical calculation errors). The table shows that the LlaMA model exhibits moderate performance across all three metrics, with room for improvement in its ability to generate correct answers and detect calculation errors. This data is used to establish a baseline for comparing the performance of the AUTOPSV method.", "section": "4.1 Experiment on Outcome-Supervised Verifier Performance"}, {"figure_path": "eOAPWWOGs9/tables/tables_18_3.jpg", "caption": "Table 3: Performance of OSV models across different configurations.", "description": "This table presents the performance of two outcome-supervised verifier (OSV) models, OSV (Mistral) and OSV (Phi), across different large language models (LLMs) used as response generators.  It evaluates the performance of each OSV model's ability to select the correct answer from multiple LLM-generated responses using three metrics: Pass@1 (accuracy of selecting the correct response as the top choice), Pass@5 (accuracy of selecting the correct response within the top five choices), and Self-Consistency (a baseline metric representing the consistency of the LLM itself). The results show the effectiveness and scalability of OSV models in response selection across LLMs of varying sizes.", "section": "4.1 Experiment on Outcome-Supervised Verifier Performance"}, {"figure_path": "eOAPWWOGs9/tables/tables_19_1.jpg", "caption": "Table 14: SFT Training Hyperparameters.", "description": "This table lists the hyperparameters used during the supervised fine-tuning (SFT) process to train the outcome-supervised verifier model.  The hyperparameters shown include global batch size, learning rate (LR), number of epochs (Epo), maximum sequence length (Max Length), weight decay, and warmup ratio.", "section": "F.1 Training Hyperparameters"}, {"figure_path": "eOAPWWOGs9/tables/tables_20_1.jpg", "caption": "Table 15: Comparison of aggregation functions on GSM8K dataset.", "description": "This table presents the comparison of two different aggregation functions (Product and Minimum) for step-level scores, evaluated on the GSM8K dataset using three different response generators: Mistral-Instruct, Mixtral-Instruct, and Qwen.  The results show that the choice of aggregation function has a minor impact on the overall performance.", "section": "5.2 Enhanced LLMs Reasoning via Process Supervision"}]