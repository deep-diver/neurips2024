[{"figure_path": "v3jHuoxMw8/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of behavioural cloning (BC) and ENP for VLN. Previous methods use BC to optimize the conditional action distribution directly. ENP models the joint state-action distribution through an energy-based model. The low energy values correspond to the state-action pairs that the expert is most likely to perform.", "description": "This figure compares the behavioral cloning (BC) method and the proposed Energy-based Navigation Policy (ENP) for Vision-Language Navigation (VLN).  BC directly optimizes the conditional action distribution, meaning it learns to predict the next action given the current state.  ENP, on the other hand, models the joint state-action distribution using an energy-based model. In ENP, low energy values represent state-action pairs the expert is most likely to take, while high energy values indicate less likely pairs. This allows ENP to capture the global relationship between states and actions, leading to a more robust and generalizable policy.", "section": "1 Introduction"}, {"figure_path": "v3jHuoxMw8/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of ENP. At each step t, the agent acquires a series of observations, and predicts the next step based on the instruction and navigation history. ENP optimizes the marginal state matching loss Ls through SGLD sampling from Marginal State Memory (Eq. 9), and minimizes the cross-entropy loss L\u2081 jointly (Eq. 1).", "description": "The figure illustrates the Energy-based Navigation Policy (ENP) framework.  It shows how, at each time step, the agent takes in an instruction, current observation, and past navigation history. This information is processed by a cross-modal state encoder to produce a state representation.  This representation is then used by an action classifier to predict the next action.  Critically, ENP incorporates a marginal state memory and uses Stochastic Gradient Langevin Dynamics (SGLD) to optimize both the cross-entropy loss of the action prediction and a marginal state matching loss, ensuring alignment with the expert's navigation policy.", "section": "3 Method"}, {"figure_path": "v3jHuoxMw8/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative results on R2R [1] (\u00a74). (a) DUET [6] arrives in the wrong room instead of 'recreation room' since the scene contains multiple rooms. Our agent reaches the goal successfully, demonstrating better decision-making ability. (b) Failure case: Due to partial observability and occlusion of the environment, it is hard to find 'kitchen' at some positions. Thus our agent goes the wrong way and ends in failure (\u00a74.1).", "description": "This figure compares the qualitative results of the proposed Energy-based Navigation Policy (ENP) and the existing DUET method on the R2R benchmark.  Two examples are shown. In (a), DUET fails to reach the correct room, while ENP successfully navigates to the goal, highlighting ENP's improved decision-making abilities. In (b), both methods fail because of partial observability and occlusion in the environment; this emphasizes the challenges of VLN.", "section": "4.1 VLN in Discrete Environments"}, {"figure_path": "v3jHuoxMw8/figures/figures_14_1.jpg", "caption": "Figure 2: Overview of ENP. At each step t, the agent acquires a series of observations, and predicts the next step based on the instruction and navigation history. ENP optimizes the marginal state matching loss Ls through SGLD sampling from Marginal State Memory (Eq. 9), and minimizes the cross-entropy loss L\u2081 jointly (Eq. 1).", "description": "This figure illustrates the Energy-based Navigation Policy (ENP) framework.  The agent receives an instruction, observes the environment, and uses its history to predict its next action. The ENP model learns by optimizing both a cross-entropy loss (L1) and a marginal state matching loss (Ls) using Stochastic Gradient Langevin Dynamics (SGLD) sampling and a marginal state memory to improve sampling efficiency. This contrasts with previous methods that only focus on the conditional action distribution. ", "section": "3 Method"}]