[{"figure_path": "iQUxHrCna0/figures/figures_1_1.jpg", "caption": "Figure 1: Training of a universal embedding on multiple fine-grained visual domains. The baseline approach of [45] (left) uses classification loss across training classes from all domains. It is prone to cancelling out contradicting cues from different domains. To overcome this issue, a naive multi-teacher distillation approach (middle) first trains one specialized teacher per domain (with a classification loss) to capture domain specifics, then distils them to the universal embedding (student). Our proposed Universal Dynamic Online distillatioN \u2013 UDON (right) jointly trains the specialized teacher embeddings and the universal embedding (student) with classification and, at the same time, distills the teacher embeddings to the universal embedding. Due to joint training of a shared backbone, UDON scales to a large number of domains.", "description": "This figure compares three different approaches for training a universal image embedding model. The leftmost shows a baseline approach that trains a single model on all domains simultaneously, which can lead to performance issues due to conflicting cues between domains. The middle shows a naive multi-teacher distillation approach, where separate models are trained for each domain and then distilled into a single student model. The rightmost shows the proposed UDON method, which jointly trains the specialized teacher models and the student model with a shared backbone, allowing for efficient knowledge transfer and scaling to a large number of domains.", "section": "1 Introduction"}, {"figure_path": "iQUxHrCna0/figures/figures_4_1.jpg", "caption": "Figure 2: Block diagram of UDON's training process. Each batch of size B contains images from a single domain (e.g., cars, natural world, etc). When a batch with domain i is processed, the i-th teacher head is used. Both the teacher and the student employ a classification loss (Lti_cls, Lu_cls) on top of their batched logits (Lt_i, Lu), predicting among Ci classes. The student is additionally trained via distillation, by learning intra-batch relationships (Lrel_ti) and logits (Llog_ti) with the domain teacher guidance. Note that the distillation losses are backpropagated only through the student's head.", "description": "This figure illustrates the training process of the UDON model.  It shows how batches of images from a single domain are processed. Each domain has a specialized teacher model that shares a backbone with a universal student model. Both teacher and student use classification loss.  Additionally, the student model is trained via distillation from the teacher model, using both relational and logit distillation losses to improve its accuracy. The distillation losses only backpropagate through the student's head.", "section": "3.2 Universal Dynamic Online distillatioN (UDON)"}, {"figure_path": "iQUxHrCna0/figures/figures_7_1.jpg", "caption": "Figure 2: Block diagram of UDON's training process. Each batch of size B contains images from a single domain (e.g., cars, natural world, etc). When a batch with domain i is processed, the i-th teacher head is used. Both the teacher and the student employ a classification loss (Lti,cls, Lus,cls) on top of their batched logits (Lti, Ltu), predicting among Ci classes. The student is additionally trained via distillation, by learning intra-batch relationships (Ltrel) and logits (Llog) with the domain teacher guidance. Note that the distillation losses are backpropagated only through the student's head.", "description": "The figure illustrates the training process of the UDON model. It shows how batches of images from a single domain are processed, with each batch activating a corresponding teacher head. Both the teacher and student models use a classification loss, and the student model also uses distillation losses to learn from the teacher.  The shared backbone and online nature of the training are highlighted.", "section": "3.2 Universal Dynamic Online distillatioN (UDON)"}, {"figure_path": "iQUxHrCna0/figures/figures_13_1.jpg", "caption": "Figure 2: Block diagram of UDON's training process. Each batch of size B contains images from a single domain (e.g., cars, natural world, etc). When a batch with domain i is processed, the i-th teacher head is used. Both the teacher and the student employ a classification loss (Luis, Lus) on top of their batched logits (Lt\u2081, Lu), predicting among Ci classes. The student is additionally trained via distillation, by learning intra-batch relationships (Lel) and logits (Log) with the domain teacher guidance. Note that the distillation losses are backpropagated only through the student's head.", "description": "This figure illustrates the training process of the UDON model.  It shows how batches of images from a single domain are fed into a shared backbone network.  For each domain, a teacher head and a student head process the output of the backbone. Both teacher and student employ classification losses to learn from their respective logits. Additionally, the student undergoes distillation training, leveraging both intra-batch relationship similarities and logits from its corresponding domain teacher. This distillation process is designed to transfer domain-specific knowledge from the teachers to the universal student model, while keeping the distillation losses' backpropagation confined to the student's head only.", "section": "3.2 Universal Dynamic Online distillatioN (UDON)"}, {"figure_path": "iQUxHrCna0/figures/figures_13_2.jpg", "caption": "Figure 1: Training of a universal embedding on multiple fine-grained visual domains. The baseline approach of [45] (left) uses classification loss across training classes from all domains. It is prone to cancelling out contradicting cues from different domains. To overcome this issue, a naive multi-teacher distillation approach (middle) first trains one specialized teacher per domain (with a classification loss) to capture domain specifics, then distils them to the universal embedding (student). Our proposed Universal Dynamic Online distillatioN \u2013 UDON (right) jointly trains the specialized teacher embeddings and the universal embedding (student) with classification and, at the same time, distills the teacher embeddings to the universal embedding. Due to joint training of a shared backbone, UDON scales to a large number of domains.", "description": "This figure illustrates three different approaches for training a universal image embedding model on multiple fine-grained visual domains. The left shows a baseline method using classification loss, the middle shows a naive multi-teacher distillation approach, and the right shows the proposed UDON method, which jointly trains specialized teacher embeddings and the universal embedding using a shared backbone.", "section": "1 Introduction"}, {"figure_path": "iQUxHrCna0/figures/figures_13_3.jpg", "caption": "Figure 1: Training of a universal embedding on multiple fine-grained visual domains. The baseline approach of [45] (left) uses classification loss across training classes from all domains. It is prone to cancelling out contradicting cues from different domains. To overcome this issue, a naive multi-teacher distillation approach (middle) first trains one specialized teacher per domain (with a classification loss) to capture domain specifics, then distils them to the universal embedding (student). Our proposed Universal Dynamic Online distillatioN \u2013 UDON (right) jointly trains the specialized teacher embeddings and the universal embedding (student) with classification and, at the same time, distils the teacher embeddings to the universal embedding. Due to joint training of a shared backbone, UDON scales to a large number of domains.", "description": "This figure illustrates three different approaches to training a universal image embedding model. The left shows a baseline method that uses classification loss across all domains, which can lead to conflicting cues and suboptimal learning. The middle depicts a naive multi-teacher distillation approach that pre-trains specialized models per domain before distilling knowledge to the universal model, increasing computational costs. The right panel introduces the proposed UDON method, which jointly trains both specialized teacher models and the universal embedding model efficiently by sharing the model backbone. This approach overcomes the limitations of previous approaches, resulting in better performance and scalability.", "section": "1 Introduction"}, {"figure_path": "iQUxHrCna0/figures/figures_16_1.jpg", "caption": "Figure 2: Block diagram of UDON's training process. Each batch of size B contains images from a single domain (e.g., cars, natural world, etc). When a batch with domain i is processed, the i-th teacher head is used. Both the teacher and the student employ a classification loss (Lti,cls, Lus,cls) on top of their batched logits (Lti, Luls), predicting among Ci classes. The student is additionally trained via distillation, by learning intra-batch relationships (Ltrel) and logits (Llog) with the domain teacher guidance. Note that the distillation losses are backpropagated only through the student's head.", "description": "This figure illustrates the training process of the UDON model.  It shows how batches of images from a single domain are processed. Each domain has a specialized teacher head and a shared backbone.  The student model learns from both classification and distillation losses, improving its ability to generalize across different domains.", "section": "3.2 Universal Dynamic Online distillatioN (UDON)"}]