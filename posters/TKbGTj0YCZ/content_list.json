[{"type": "text", "text": "Robust Conformal Prediction under Joint Distribution Shift ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Uncertainty prevails due to the lack of knowledge about data or model, and con  \n2 formal prediction (CP) predicts multiple potential targets, hoping to cover the true   \n3 target with a high probability. Regarding CP robustness, importance weighting   \n4 can address covariate shifts, but CP under joint distribution shifts remains more   \n5 challenging. Prior attempts addressing joint shift via $f$ -divergence ignores the   \n6 nuance of calibration and test distributions that are critical for coverage guaran  \n7 tees. More generally, with multiple test distributions shifted from the calibration   \n8 distribution, simultaneous coverage guarantees for all test domains requires a new   \n9 paradigm. We design Multi-domain Robust Conformal Prediction $(m R C P)$ that first   \n10 formulates the coverage difference that importance weighting fails to capture under   \n11 any joint shift. To squeeze such coverage difference and guarantee the $(1-\\alpha)$   \n12 coverage in all test domains, we propose Normalized Truncated Wasserstein dis  \n13 tance (NTW) to comprehensively capture the nuance of any test and calibration   \n14 conformal score distributions, and design an end-to-end training algorithm incorpo  \n15 rating NTW to provide elasticity for simultaneous coverage guarantee over distinct   \n16 test domains. With diverse tasks (seven datasets) and architectures (black-box and   \n17 physics-informed models), NTW strongly correlates (Pearson coefficient ${=}0.9051$ )   \n18 with coverage differences beyond covariate shifts, while mRCP reduces coverage   \n19 gap by $50\\%$ on average robustly over multiple distinct test domains. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 The growing data volume, enhanced computation capability, and advanced models significantly   \n22 improve machine learning predictive accuracy. Nevertheless, noises, unobservable factors, and the   \n23 lack of knowledge lead to uncertainty that stakeholders should ponder along model predictions   \n24 when making decisions particularly in areas such as fintech [25], autonomous driving [2], traffic   \n25 forecasting [4], and epidemiology [32, 27]. Conformal Prediction (CP) addresses uncertainty by   \n26 predicting a set of possible target(s) rather than a single guess [31]. Specifically, CP computes   \n27 conformal scores (residuals between predicted and true targets for regression tasks) of a trained   \n28 model $f$ on a calibration set, and calculates the $1-\\alpha$ quantile $q$ of these scores. For any input $x$ , CP   \n29 produces the smallest prediction set $C(x)$ consisting of target values whose conformal scores are less   \n30 than $q$ . Assuming that the test and calibration data are exchangeable (including i.i.d.), the true target   \n31 $y$ is guaranteed to be covered by $C(x)$ with at least $1-\\alpha$ probability.   \n32 In practice, calibration distribution $P_{X Y}$ and test distribution $Q_{X Y}$ may differ thus $P_{X Y}\\neq Q_{X Y}$ ,   \n33 termed as joint distribution shift and violate the exchangeability assumption. Joint shift can occur   \n34 with either covariate shift $(P_{X}\\neq Q_{X})$ or concept shift $(P_{Y|X}\\neq Q_{Y|X})$ , though what causes a   \n35 joint shift is difficult to infer from the observed data only. With importance weighting, covariate   \n36 shift is shown not to affect the coverage confidence guarantee [29]. To address CP under joint   \n37 shift, $f$ -divergence is adopted in [33, 6] to measure the difference between $P_{X Y}$ and $Q_{X Y}$ or   \n38 the corresponding conformal scores distributions. However, $f$ -divergence ignores where the two   \n39 distributions differ, which quantiles and coverage guarantees depend on (Figure 1, (c)). When test   \n40 data are sampled from multiple distinct test distributions $Q_{X Y}^{(e)},e\\in\\mathcal{E}=\\{e_{1},...,e_{M}\\}$ , it is desired   \n41 to ensure simultaneous coverage for all test distributions. Previous work selects the highest   \n42 1 \u2212\u03b1 quantile from all test distributions and constructs C(x) for x \u2208Q(Xe) , \u2200e \u2208E = {e1, ..., eM},   \n43   \n44 test domain than was expected during calibration, leading to prediction overconfidence. Without a   \n45 new paradigm to guarantee coverage under multiple shifted test distributions, the dilemma between   \n46 CP coverage efficiency and confidence guarantee seems unavoidable.   \n47 We first decompose the coverage difference under any joint distribution shift to a component due to   \n48 covariate shift $\\bar{(P_{X}\\neq Q_{X}^{(e)}}$ , addressed by importance weighting [29]) and that due to concept shift   \n49 $(P_{Y|X}\\neq Q_{Y|X}^{(e)})$ . We propose Normalized Truncated Wasserstein distance (NTW) to robustly capture   \n50 where the test and importance-weighted calibration conformal score cumulative density function   \n51 (CDF) deviate (Figure 1, (b)). We design Multi-domain Robust Conformal Prediction $(m R C P)$ by   \n52 minimizing all NTW terms over $\\mathcal{E}=\\{e_{1},...,e_{M}\\}$ during model training (Figure 1, (d)) to elastically   \n53 guarantee coverage confidence for all test domains. Experiments on regression tasks on seven datasets   \n54 demonstrate that: $^{\\,l}$ ) NTW well-correlates with the coverage difference after importance weighting   \n55 (Pearson coefficient 0.905); 2) mRCP provides conformal predictions that reduce average coverage   \n56 difference by $50\\%$ compared to baselines under multiple joint shifts; 3) mRCP is sufficiently general   \n57 to address joint distribution shifts even after incorporating domain knowledge when available. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "TKbGTj0YCZ/tmp/99c516debbf20c8a8e72e568ab8cc0839f2fa52ed5a3d9a40b60edfcdb5ad724.jpg", "img_caption": ["Figure 1: (a) Multiple test domains $\\mathcal{E}=\\{e_{1},...,e_{M}\\}$ with joint shifts $(Q_{X Y}^{(e)}\\neq P_{X Y})$ ; (b) coverage difference $D_{\\mathrm{joint}}=\\hat{F}_{Q}(q)-\\hat{F}_{P}(q)$ (Eq. (5)) due to $Q_{X Y}^{(e)}\\neq P_{X Y}$ is decomposed into the $D_{\\mathrm{coviriate}}$ (Eq. (7)) caused by covariate shift $(Q_{X}^{(e)}\\neq P_{X})$ and the remaining $D_{\\mathrm{concept}}$ (Eq. (8)) due to concept shift $(Q_{Y\\mid X}^{(e)}\\,\\ne\\,P_{Y\\mid X})$ ; (c) Wasserstein-1 (W-) distances (Eq. (11)) between test and weighted calibration conformal score CDFs capture the expected $D_{\\mathrm{concept}}$ (Eq. (10)). However, $f$ -divergence (e.g., total variation, KL divergence) does not compare two CDFs pointwisely and fails to capture such an expectation. Test domains 1 and 2 both have identical total variations to the calibration domain but different W-distances. With multiple test domains, using a single $1-\\alpha$ quantile [33, 6] lead to the dilemma of CP coverage efficiency and confidence guarantee; (d) Solution: Normalized Truncated W-distance (Eq. (16)) is robust to outlier scores and different score scales across test domains, and the mRCP algorithm reduces NTW on all test domains can elastically train a model to guarantee conformal coverage for $Q_{Y|X}^{(\\bar{e})}\\neq P_{Y|X}\\forall e\\in\\mathcal{E}=\\{e_{1},...,e_{M}\\}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "58 2 Background and related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "59 2.1 Conformal prediction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "60 Let $x\\,\\in\\,{\\mathcal{X}}$ and $y\\in\\mathcal{V}$ denote the input and output random variable, respectively, where $\\mathcal{X}$ and   \n61 $\\mathcal{V}\\subseteq\\mathbb{R}$ is the input and output space, respectively. On $\\mathcal X\\times\\mathcal X$ , the calibration domain is defined by a   \n62 joint distribution $P_{X Y}$ , and we consider a calibration set $S_{c}=\\{(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\}$ are drawn   \n63 i.i.d. from $P_{X Y}$ . Similarly, a test set $S_{t}=\\{(x_{1},y_{1}),\\dots,(x_{m},y_{m})\\}$ is drawn i.i.d. from test domain,   \n64 which is defined by a joint distribution $Q_{X Y}$ .   \n65 With a trained regression model $f$ , the conformal score $v_{i}=v(x_{i},y_{i})=|f(x_{i})-y_{i}|$ is the residual   \n66 between the predicted target $f(x_{i})$ and the true target $y_{i}$ . The set of calibration conformal scores is   \n67 denoted as $\\bar{V_{c}}=\\{v(x_{i},y_{i})|(\\bar{x}_{i},y_{i})\\in S_{c}\\}$ . Let $q$ be the $\\lceil(1-\\alpha)(n+1)\\rceil/n$ quantile of $V_{c}$ : ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nq=\\mathrm{Quantile}\\left(\\frac{\\lceil(1-\\alpha)(n+1)\\rceil}{n},\\frac{1}{n}\\sum_{v_{i}\\in V_{c}}\\delta_{v_{i}}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "68 where $\\delta_{v_{i}}$ represents the point mass at $v_{i}$ (i.e., the distribution placing all mass at the value $v_{i}$ ).   \n69 Quantile $(1-\\alpha,F):=\\operatorname*{inf}\\{z|\\mathbf{Pr}(Z<z)\\geq1-\\alpha\\}$ and $F$ is the CDF of $Z$ . With the quantile $q$ , the   \n70 CP prediction set of an input $x$ from $S_{t}$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\nC(x)=\\left\\{\\hat{y}\\in\\mathbb{R}||f(x)-\\hat{y}|\\le\\ q,(x,y)\\in S_{t}\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "71 Most CP methods, such as[22, 23], rely on the assumption of exchangeability, which is relaxed from   \n72 the i.i.d. assumption [31]. In our scenario, if the calibration and test samples are drawn from the   \n73 identical joint probability distribution $\\prime P_{X Y}=Q_{X Y})$ ), these calibration and test samples are i.i.d.   \n74 Under this assumption, the probability that the true target $y$ is included in $C(x)$ is at least $1-\\alpha$ ,   \n75 which is called coverage guanrantee, or more formally, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(y\\in C(x)\\right)\\geq1-\\alpha.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "76 2.2 Conformal prediction under domain shift ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "77 Covariate shift $\\textstyle P_{X}\\neq Q_{X})$ means marginal distributions between the calibration and test domains   \n78 are different. CP under covariate shift is addressed using importance weighting [29]. Under a   \n79 probabilistic view, [14] defined the covariate shift as a bounded perturbation on any test input and   \n80 developed adaptive probabilistically robust CP. The condition of multiple test domains is discussed   \n81 in [15], and similar topics include coverages under feature-stratification [7, 11].   \n82 Joint distribution shift $(P_{X Y}\\ne Q_{X Y})$ indicates at least one of covariate shift $\\textstyle P_{X}\\neq Q_{X})$ and   \n83 concept shift (different conditional distributions, $P_{Y|X}\\neq Q_{Y|X})$ will occur [17]. This shift is more   \n84 general and the importance weighting method cannot address changes in conditional distribution.   \n85 With $M$ test domains $\\mathcal{E}=\\{e_{1},...,e_{M}\\}$ , each $e\\in\\mathcal{E}$ is defined by a joint distribution $Q_{X Y}^{(e)}$ and holds   \n86 a joint shift with calibration domain $P_{X Y}$ (i.e., $P_{X Y}\\neq Q_{X Y}^{(e)})$ . Considering this condition, previous   \n87 works, such as [6, 33], presume all test domains fall in a predefined $f$ -divergence range, calculate   \n88 confidence-specified quantile of each test domain, and apply the highest quantile to all domains.   \n89 This method causes excessively high coverages and thus overlarge prediction sets, which reduces   \n90 prediction efficiency because smaller prediction sets can help locate true targets better. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "91 3 Conformal prediction under joint distribution shift ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "92 3.1 Decomposition of coverage difference ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "93 We decompose the coverage difference between a calibration domain $P_{X Y}$ and a test domain $Q_{X Y}$   \n94 under joint distribution shift at a user-specified confidence $(1-\\alpha)$ .   \n95 Similar to $V_{c}$ , we define the test conformal score set $V_{t}\\;=\\;\\{v(x_{i},y_{i})|(x_{i},y_{i})\\;\\in\\;S_{t}\\}$ . With the   \n96 indicator function $\\mathbb{1}$ , empirical CDFs of calibration and test conformal scores are ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{F}_{P}(v)=\\frac{1}{n}\\sum_{v_{i}\\in V_{c}}\\delta_{v}\\mathbb{1}_{v_{i}<v},\\ \\ \\hat{F}_{Q}(v)=\\frac{1}{m}\\sum_{v_{i}\\in V_{t}}\\delta_{v}\\mathbb{1}_{v_{i}<v}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "97 With given $1-\\alpha$ confidence, quantile $q$ is calculated in Eq. (1), and the coverage difference under a   \n98 joint distribution shift can be quantified as ", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{\\mathrm{joint}}(q)=\\hat{F}_{Q}(q)-\\hat{F}_{P}(q).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "99 [29] employs importance weighting for CP under covariate shift. Specifically, if the ratio of test   \n100 to calibration covariate likelihoods, $Q_{X}/P_{X}$ , is known, a calibration conformal score $v_{i}\\in V_{c}$ is   \n101 weighted by $\\begin{array}{r}{p_{i}=w(x_{i})/\\!\\sum_{j=1}^{n}w(x_{j})}\\end{array}$ , where $w(x_{i})=Q_{X}(x_{i})/P_{X}(x_{i})$ . Therefore, the empirical   \n102 CDF of weighted empirical calibration scores is given by ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{F}_{Q/P}(v)=\\sum_{i=1}^{n}p_{i}\\delta_{v_{i}}\\mathbb{1}_{v_{i}<v},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "103 where the subscript $Q/P$ indicates conformal scores of calibration domain $P$ is weighted by confor  \n104 mal scores of test domain $Q$ . The confidence-specified quantile of the weighted calibration conformal   \n105 scores is ", "page_idx": 3}, {"type": "equation", "text": "$$\nq^{*}=\\mathrm{Quantile}\\left(\\lceil(1-\\alpha)(n+1)\\rceil/n,\\sum_{i=1}^{n}p_{i}\\delta_{v_{i}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "106 As importance weighting ensures the $1-\\alpha$ coverage as though covariate shift were absent, coverage   \n107 difference $D_{\\mathrm{covariate}}$ caused by covariate shift is the gap between the coverages under test conformal   \n108 score CDF using quantiles on unweighted and weighted calibration conformal score distributions. ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{\\mathrm{covariate}}(q,q^{*})=\\hat{F}_{Q}(q)-\\hat{F}_{Q}(q^{*}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "109 Importance weighting can not address CP under joint shift as it fails to capture changes in conditional   \n110 probability distribution caused by concept shift, thus we present the coverage difference caused by   \n111 concept shift is ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{\\mathrm{concept}}(q,q^{*})=D_{\\mathrm{joint}}(q)-D_{\\mathrm{covariate}}(q,q^{*})=\\hat{\\cal F}_{Q}(q^{*})-\\hat{\\cal F}_{P}(q),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "112 which is remaining coverage difference after applying importance weighting. Here we assume   \n113 $\\hat{F}_{P}(q)=\\hat{F}_{Q/P}(q^{*})$ , so we can rewrite $D_{\\mathrm{concept}}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{\\mathrm{concept}}(q^{*})=\\hat{F}_{Q}(q^{*})-\\hat{F}_{Q/P}(q^{*}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "114 The error bound for the assumption is quite small especially when the calibration set size $n$ is large.   \n115 The detailed proof is provided in Appendix B. We denote $D_{\\mathrm{concept}}$ as $D$ for simplification. ", "page_idx": 3}, {"type": "text", "text": "116 3.2 Normalized Truncated Wasserstein distance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "117 To develop a metric that is independent of confidence level and can quantify the overall closeness   \n118 between weight calibration and test conformal scores, we estimate the expected coverage difference   \n119 under concept shift as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[D]=\\frac{1}{n}\\sum_{v_{i}\\in V_{c}}\\left|\\hat{F}_{Q}(v_{i})-\\hat{F}_{Q/P}(v_{i})\\right|,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "120 based on the approximation in Eq. (9), where $\\mathbb{E}$ indicates the expectation function. ", "page_idx": 3}, {"type": "text", "text": "121 Definition 1 (Wasserstein-1 Distance). If $F_{1}$ and $F_{2}$ are two cumulative distribution functions $(C D F s)$ ,   \n122 the Wasserstein-1 distance, $d_{\\mathrm{W}}$ , is quantified by the area between $F_{1}$ and $F_{2}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{\\mathbb{W}}(F_{1},F_{2})=\\int_{\\mathbb{R}}|F_{1}(v)-F_{2}(v)|d x.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "123 Applying Wasserstein-1 distance (W-distance) in Eq. (11) to $\\hat{F}_{Q}$ and $\\hat{F}_{Q/P}$ , we get ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{\\mathbb{W}}(\\hat{F}_{Q},\\hat{F}_{Q/P})=\\int_{0}^{\\infty}|\\hat{F}_{Q}(v)-\\hat{F}_{Q/P}(v)|d v.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "124 As we define conformal scores as the residuals between predicted and true targets, they are always   \n125 positive, so we only need to integral from 0 to $\\infty$ in Eq. (12).   \n126 We assume $V_{c}$ is sorted. As both $\\hat{F}_{Q}$ and $\\hat{F}_{Q/P}$ are empirical CDFs, we can approximately represent   \n127 $d_{\\mathrm{W}}(\\hat{F}_{Q},\\hat{F}_{Q/P})$ in a discrete form as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\mathrm{w}(\\hat{F}_{Q},\\hat{F}_{Q/P})\\approx\\sum_{i=1}^{n-1}\\left|\\hat{F}_{Q}(v_{i})-\\hat{F}_{Q/P}(v_{i})\\right|(v_{i+1}-v_{i}),\\;\\;v_{i}\\in V_{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "128 Eq. (13) shows $d_{\\mathrm{W}}(\\hat{F}_{Q},\\hat{F}_{Q/P})$ can be estimated as a weighted summation of $|\\hat{F}_{Q}(v_{i})-\\hat{F}_{Q/P}(v_{i})|$   \n129 for $v_{i}\\in V_{c}\\backslash\\{v_{n}\\}$ with the corresponding weight $v_{i+1}-v_{i}$ . Also, Eq. (10) indicates that $\\mathbb{E}[D]$ can   \n130 be regarded as the weighted summation of $|\\hat{F}_{Q}(v_{i})-\\hat{F}_{Q/P}(v_{i})|$ for $v_{i}\\in V_{c}$ with weight $1/n$ . The   \n131 similarity between Eq. (13) and Eq. (10) allows us to apply the W-distance between the test and   \n132 weighted calibration conformal score to capture expected coverage difference under concept shift.   \n133 Care needs to be taken for Eq. (13) to make this metric more robust. At first, we expect the weights   \n134 $v_{i+1}-v_{i}$ to be approximately equal, as weights in Eq. (10) are constants $1/n$ . However, some outlier   \n135 calibration conformal scores have large distances from their neighbors, causing involved weights   \n136 much higher than $1/n$ . These outlier scores are represented as a long tail of $\\hat{F}_{Q/P}$ when it converges   \n137 to 1. Therefore, it is necessary to establish a partition threshold to truncate the long tail. We calculate   \n138 the partition threshold ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nv_{\\sigma}=\\operatorname*{inf}\\left\\{v_{i}|\\hat{F}_{Q/P}(v_{i})\\geq1-\\sigma,v_{i}\\in V_{c}\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "139 which is the smallest calibration conformal score whose coverage is greater or equal to a user-defined   \n140 value $1-\\sigma$ . In contrast to the original $d_{\\mathrm{w}}(\\hat{F}_{Q},\\hat{F}_{Q/P})$ integrated on the set of real numbers, the   \n141 truncated form is integrated from 0 to $v_{\\sigma}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{\\mathrm{TW}}(\\hat{F}_{Q},\\hat{F}_{Q/P})=\\int_{0}^{v_{\\sigma}}|\\hat{F}_{Q}(v)-\\hat{F}_{Q/P}(v)|d v.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "142 Secondly, as the summation of weights in Eq. (10) is 1, we also need to divide each $v_{i+1}-v_{i}$   \n143 by $v_{\\sigma}-v_{1}$ . When the calibration set is large enough, it is plausible to assume the existence of a   \n144 calibration sample fitting the trained model $f$ very well, causing the smallest calibration conformal   \n145 score $v_{1}\\approx0$ . Therefore, this normalized can be formulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{\\mathrm{NTW}}(\\hat{F}_{Q},\\hat{F}_{Q/P})=\\frac{1}{v_{\\sigma}}\\int_{0}^{v_{\\sigma}}|\\hat{F}_{Q}(v)-\\hat{F}_{Q/P}(v)|d v.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "146 A lower $d_{\\mathrm{NTW}}$ indicates more similarity between F\u02c6Q/P and F\u02c6Q, thus leading to more robust conformal   \n147 prediction in the test domain. As a result, NTW enables us to assess the expected coverage difference   \n148 due to concept shift in Eq. (10). Experiment results in Section 5 and Appendix E show the necessity   \n149 of truncation and normalization. We also prove that the W-distance between the test and weighted   \n150 calibration conformal score population CDF can establish an upper bound for coverage difference   \n151 under concept shift in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "152 4 Multi-domain robust conformal prediction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "153 If a calibration set $S_{c}$ , and a test set $S_{t}$ are drawn from a domain $P_{X Y}$ , the i.i.d. assumption is   \n154 satisfied, and the coverage guarantee in Eq. (3) holds for $(x,y)\\in S_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "155 The domain $P_{X Y}$ can be decomposed into $M$ multiple domains, denoted as $\\mathcal{E}=\\{e_{1},...,e_{M}\\}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{X Y}(x,y)=\\frac{1}{M}\\sum_{e\\in\\mathcal{E}}Q_{X Y}^{(e)}(x,y)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "1 56 However, for $e\\in\\mathcal{E}$ , denote $S_{t}^{(e)}$ a test set drawn from $Q_{X Y}^{(e)}$ , then the coverage guarantee may no   \n157 longer hold for $(x,y)\\in S_{t}^{(e)}$ , because joint distribution shift may occur between $P_{X Y}$ and $Q_{X Y}^{(e)}$ . It   \n158 indicates CP can be overconfident and underconfident for samples from different QXY , resulting in   \n159 prediction biases.   \n160 Inspired by the works of multi-domain generalization [26, 18, 19, 1], we propose Multi-domain   \n161 Robust Conformal Prediction (mRCP) to make the coverage approach confidence in all domains,   \n162 using a training set $S^{(e)}$ from the data distribution $Q_{X Y}^{(e)}$ for $e\\in\\mathcal{E}$ and a calibration set $S_{c}$ from $P_{X Y}$ .   \n163 The objective function of mRCP includes two components. First, for the minimization of prediction   \n164 residuals, denoting $l$ a loss function, Empirical Risk Minimization (ERM) [30] is incorporated as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathtt{E R M}}(\\theta)=\\sum_{e\\in\\mathcal{E}}\\mathcal{L}^{(e)}(\\theta)=\\sum_{e\\in\\mathcal{E}}\\mathbb{E}_{(x_{i},y_{i})\\sim S^{(e)}}\\left[l\\big(f_{\\theta}(x_{i}),y_{i}\\big)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "165 Secondly, we aim for robust conformal prediction on each domain during testing, seeking a low value   \n166 of $\\mathbb{E}[D]$ in Eq. (10) across test domains, so mRCP needs to address coverage differences due to   \n167 covariate and concept shifts simultaneously. To remove coverage differences due to covariate shifts,   \n168 it applies importance weighting to each domain $e\\in\\mathcal{E}$ during training and obtains $\\hat{F}_{Q^{\\left(e\\right)}/P}$ , which is   \n169 the calibration conformal score CDF weighted by $Q_{X Y}^{(e)}$ .   \n170 Besides, as we have a training set $S^{(e)}$ from domain $Q_{X Y}^{(e)}$ , an empirical CDF of conformal scores in   \n171 $Q_{X Y}^{(e)}$ )Y can be computed, denoted as F\u02c6 tQr(e). NTW quantifies the expected coverage difference caused   \n172 by concept shift between F\u02c6Q(e)/P and training conformal score CDF F\u02c6 tQr(e) . Combining these two   \n173 components, the objective function of mRCP is ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{mRCP}}(\\theta)=\\sum_{e\\in\\mathcal{E}}\\mathcal{L}^{(e)}(\\theta)+\\beta\\sum_{e\\in\\mathcal{E}}d_{\\mathrm{NTW}}(\\hat{F}_{Q^{(e)}}^{t r},\\hat{F}_{Q^{(e)}/P}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta$ is a hyperparameter balancing these two parts. mRCP algorithm is shown in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Multi-domain Robust Conformal Prediction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Require: $M$ training sets $S^{(e)}$ , $e\\in\\mathcal{E}$ ; one calibration set $S_{c}$ ; $N$ training epochs; model $f_{\\theta}$ ; partition value $\\sigma$ ;   \nloss function $l$ ; penalty hyperparameter $\\beta$ .   \n1: for $e\\in\\mathcal{E}$ do   \n2: for $(x_{i},y_{i})\\in S_{c}$ do   \n3: $\\begin{array}{r}{w(x_{i})=\\frac{Q_{X}^{(e)}(x_{i})}{P_{X}(x_{i})},p_{(i,e)}=\\frac{w(x_{i})}{\\sum_{j=1}^{n}w(x_{j})}}\\end{array}$ \u25b7Covariate shift between $Q_{X Y}^{(e)}$ and $P_{X Y}$   \n4: end for   \n5: end for   \n6:   \n7: for 8: V $i=1$ $\\begin{array}{r l r}&{=1\\tan{\\cal K}\\,\\mathbf{\\Phi}(\\mathbf{do})}&{\\in{\\cal S}_{c}\\mathrm{loil\\,}}\\\\ &{\\stackrel{r=}{\\mathbf{c}}=\\left\\{v(x_{i},y_{i})\\right\\vert(x_{i},y_{i})\\in{\\cal S}_{c}\\right\\}}&{\\forall{\\mathrm{~Calibration\\,}}\\,\\mathrm{score}\\,\\mathrm{~set}}\\\\ &{\\mathbf{\\Phi}^{\\mathrm{nr}}\\,\\mathrm{e}^{\\,\\varepsilon}\\,\\mathrm{~d}\\mathbf{o}}&{\\triangleright\\mathsf{{E R M}}\\,\\mathrm{loss\\,of\\,domain}\\,\\mathrm{e}}\\\\ &{\\mathcal{L}^{(e)}(\\theta)=\\mathbb{E}_{(x_{i},y_{i})\\sim{\\cal S}^{(e)}}\\left[l(f_{\\theta}(x_{i}),y_{i})\\right]}&{\\forall{\\mathrm{~Training\\,}}\\,\\mathrm{score\\,~set}\\,\\mathrm{of~domain}\\,\\mathrm{e}}\\\\ &{\\stackrel{V}{F}_{\\theta^{(e)}}=\\left\\{v(x_{i},y_{i})\\vert(x_{i},y_{i})\\in{\\cal S}^{(e)}\\right\\}}&{\\triangleright\\mathrm{~Traing\\,}\\mathrm{~score\\,~CDF~of~domain}\\,\\mathrm{e}}\\\\ &{\\hat{\\cal F}_{Q^{(e)}}=\\sum_{v_{i}\\in{\\cal V}^{(e)}}\\delta_{v_{i}}\\mathbb{1}_{v_{i}\\le v}}&{\\triangleright\\mathrm{~Training\\,}\\mathrm{score\\,}\\mathrm{CDF~of~domain}\\,\\mathrm{e}}\\\\ &{\\hat{\\cal F}_{Q^{(e)},/P}(v)=\\sum_{v_{i}\\in{\\cal V}_{c}}p_{(i,e)}\\delta_{v_{i}}\\mathbb{1}_{v_{i}\\le v}}&{\\succ\\mathrm{Calibration\\,}\\mathrm{~score\\,~CDF~weighted~by~}Q_{X Y}^{(e)}}\\\\ &{v_{\\sigma}=\\operatorname*{inf}\\left\\{\\hat{\\cal F}_{Q^{(e)},/P}(v_{i})\\ge1-\\sigma,v_{i}\\in{\\cal V}_{c}\\right\\}}&{\\triangleright\\mathrm{~Trumcation~thresholi}}\\\\ &{\\quad}&{\\Vert\\mathbf{\\Phi}_{H\\mathrm{T}}\\,\\mathrm{ere}\\,\\left(\\hat{\\cal F}_{Q^{(e)}}^{t r},\\hat{$   \n9: f   \n10:   \n11:   \n1123::   \n14:   \n15:   \n16: end for   \n17: Optimize $f_{\\theta}$ based on $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{mRCP}}(\\theta)=\\sum_{e\\in\\mathcal{E}}\\mathcal{L}^{(e)}(\\theta)+\\beta\\sum_{e\\in\\mathcal{E}}d_{\\mathrm{NTw}}\\left(\\hat{F}_{Q^{(e)}}^{t r},\\hat{F}_{Q^{(e)}/P}\\right)}\\end{array}$   \n18: end for ", "page_idx": 5}, {"type": "text", "text": "174 ", "page_idx": 5}, {"type": "text", "text": "175 5 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "176 In this section, we validate NTW in Eq. (16) as a good indicator of expected coverage difference due   \n177 to concept shift and demonstrate the effectiveness of mRCP in obtaining coverage robustness across   \n178 different test domains. ", "page_idx": 5}, {"type": "text", "text": "179 5.1 Datasets and models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "180 We conducted experiments across various datasets: (a) the airfoil self-noise dataset [5]; (b) Seattle  \n181 loop [9], PeMSD4, PeMSD8 [16] for traffic speed prediction; (c) US-Regions, US-States, and   \n182 Japan-Prefectures [10] for epidemic spread forecasting. The airfoil dataset was manually altered to   \n183 create three subsets demonstrating covariate and concept shifts. 24 domains for the traffic datasets   \n184 were designated based on data generation hours, while epidemic dataset instances were categorized   \n185 into four domains reflecting different pandemic stages. A multilayer perceptron (MLP) with a (input   \n186 dimension, 64, 64, 1) architecture was utilized for all datasets. Traffic and epidemic prediction tasks   \n187 were also trained on corresponding physics-informed partial differential equations (PDEs), which are   \n188 the Susceptible-Infected-Recovered (SIR) model and the Reaction-Diffusion (RD) model respectively.   \n189 We refer to Appendix D for detailed experiment setups.   \n191 For each of the experiment setups, a training set, a validation set, and a test set were sampled from   \n192 each $Q_{X Y}^{(e)}$ for $e\\in{\\mathcal{E}}$ . One calibration set was sampled from $P_{X Y}$ which is a mixture probability   \n119934 dwies torinbluyt inoene do ft $Q_{X Y}^{(e)}$ EfRor $e\\in\\mathcal{E}$ q, .a (s1 s8h) otow tnr aiinn  Ethq.e  (m17o)d. elT , alwidhiacteh  cNaTn Wbe i sa na  gMoLoPd  oirn dai cPaDtoEr.  oTf $\\mathbb{E}[D]$   \n$f_{\\theta}$   \n195 function $l$ is the $\\ell_{1}$ norm, as same as how we compute conformal scores.   \n196 After training, for $e\\in\\mathcal{E}$ , we first calculated the NTW between the calibration conformal score CDF   \n197 weighted by $Q_{X}^{(e)}/P_{X}$ , and validation conformal score CDF of $Q_{X}^{(e)}$ . Denote the NTW of domain   \n198 $e$ as $d_{\\mathrm{NTW}}^{(e)}$ . Then, we estimated the expected coverage difference caused by concept shift on a test   \n199 domain $e$ , denoted as $\\mathbb{E}_{\\alpha}[D^{(e)}]$ , using the coverage difference expectation between the test and   \n200 weighted calibration conformal score CDFs on a $1-\\alpha$ confidence set $\\{0.1,...,0.9\\}$ .   \n201 E\u03b1[D(e)] and d(NeT)W should have a positive correlation for $e\\,\\in\\,{\\mathcal{E}}$ , proving NTW can capture the   \n202 expected coverage difference caused by concept shift.   \n203 Baselines: We select six baseline metrics to validate the effectiveness of NTW. Total variation   \n204 $d_{\\mathrm{TV}}$ [13], and Kullback-Leibler (KL) divergence $d_{\\mathrm{KL}}$ [21] are chosen as two typical $f$ -divergence   \n205 metrics. Expectation difference $\\Delta\\mathbb{E}$ [19] is selected since it is a widely applied generalization metric.   \n206 We also measure standard, normalized, and truncated W-distance, denoted as $d_{\\mathrm{W}}$ , $d_{\\mathrm{NW}}$ , and $d_{\\mathrm{TW}}$   \n207 respectively, to demonstrate applying normalization and truncation together is necessary.   \n208 Metric: We apply the Pearson coefficient to quantify the correlations between metrics and the   \n209 coverage difference expectation. It measures the linear correlation between two values by giving a   \n210 value between -1 and 1 inclusive. 1,0, and -1 indicate perfect positive linear, no linear, and negative   \n211 linear correlations, respectively. Therefore, if the Pearson coefficient of a metric is higher, this metric   \n212 can indicate the expected coverage difference better. We provide a detailed definition of the Pearson   \ncoefficient in Appendix E.   \n214 Results: Table 1 illustrates the Pearson coefficients between NTW and the coverage difference   \n215 expectation among seven datasets and different models, compared with the other six baseline metrics.   \n216 We highlight that NTW keeps holding the largest Pearson coefficient among all experiment setups,   \n217 which means the proposed metric can keep indicating the coverage difference expectation. Specifically,   \n218 the coefficients of total variation $d_{\\mathrm{TV}}$ and $\\mathrm{KL}$ divergence $d_{\\mathrm{KL}}$ fluctuate along experiments, meaning   \n219 that they can not truly indicate the coverage difference expectation. $\\Delta\\mathbb{E}$ can not capture the coverage   \n220 difference expectation either. Lastly, due to the lack of robustness to score scales and outliers,   \n221 standard, normalized, and truncated W-distance, denoted as $d_{\\mathrm{W}}$ , $d_{\\mathrm{NW}}$ , and $d_{\\mathrm{TW}}$ respectively, can   \n222 not indicate the coverage difference expectation as well as $d_{\\mathrm{NTW}}$ . It also displays the average and   \n223 standard deviation of the Pearson coefficient of the proposed NTW and six baselines. NTW not only   \n224 has the highest average Pearson coefficient but also has the lowest standard deviation, which means   \n225 the correlation between NTW and the coverage difference expectation caused by concept shift is   \n226 very stable. In Figure 3 and Figure 4, we also visually show the correlation between the expected   \n227 coverage difference under concept shift and each metric. We refer to Appendix E for detailed analysis.   \n228 This observation suggests the potential of incorporating NTW in the training process, leading to the   \n229 development of the mRCP approach. By applying the NTW metric, mRCP aims to enhance coverage   \n230 robustness in test domains. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "TKbGTj0YCZ/tmp/7e00acfdd6e2a9e920788c72da976179ef89ac5a1af87bad5ea0e414f0f8e2e7.jpg", "table_caption": ["Table 1: Pearson coefficients between metrics and coverage difference expectation under concept shift "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "231 5.3 Experiments of mRCP ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "232 Since we prove NTW can assess expected coverage difference under concept shift effectively, mRCP   \n233 is designed to minimize it during training. In this case, validation sets are unnecessary, and we only   \n234 draw training, and test sets from $Q_{X Y}^{(e)}$ . Again, we draw one calibration set from $P_{X Y}$ . The model $f_{\\theta}$   \n235 can also be an MLP or PDE based on different experiment setups. The loss function $l$ is the $\\ell_{1}$ norm.   \n236 We implement mRCP according to Algorithm 1.   \n237 Baselines: Two methods of optimization with out-of-distribution data are selected as baselines. DRO   \n238 in Eq. (20) by [26] follows the minimax principle to reduce the highest $\\textstyle{\\mathcal{L}}^{(e)}$ to obtain fair prediction   \n239 among test distributions. On the other hand, V-REx in Eq. (21), introduced by [18], focuses on   \n240 reducing the variance of $\\textstyle{\\mathcal{L}}^{(e)}$ to obtain fairness. As we include importance weighting in mRCP, we   \n241 do not take it as a baseline, and the effectiveness of importance weighting is discussed in Section 6. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DRO}}(\\theta)=\\operatorname*{max}_{e\\in\\mathcal{E}}\\mathcal{L}^{(e)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "242 ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{V-REx}}(\\theta)=\\sum_{e\\in{\\mathcal{E}}}{\\mathcal{L}}^{(e)}+\\beta\\,\\mathrm{Var}({\\mathcal{L}}^{(e)}\\,|\\,e\\in{\\mathcal{E}}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "243 Metric: Denote $\\mathbb{E}_{e}^{\\prime}[\\mathbb{E}_{\\alpha}[D^{(e)}]]$ the expectation of coverage difference over confidence levels   \n244 and test domains and $\\mathbb{E}_{e}^{\\prime}[\\mathcal{L}^{(e)}]$ the expectation of prediction residual over test domains. The   \n245 two expectations become smaller means the algorithm\u2019s performance is better. Both values are   \n246 normalized by the corresponding results from the same experiment setup trained by ERM. Changing   \n247 the weight $\\beta$ in Eq. (19) will draw a Pareto front, thus we want the Pareto front closer to the origin.   \n248 Since V-REX is also controlled by a hyperparameter, we draw Pareto fronts for it as well.   \n249 Result: Figure 2 displays the Pareto fronts for mRCP, DRO, and V-REx, highlighting the trade-offs   \n250 between prediction residual and coverage difference expectation across different models and datasets.   \n251 Figure 2, (a) shows the results for the airfoil self-noise dataset when trained with a Multilayer   \n52 Perceptron (MLP) model. The mRCP method achieves a more favorable Pareto front compared to   \n53 V-REx, indicating a better balance between prediction residual and coverage difference expectation.   \n254 Additionally, mRCP attains a lower normalized coverage difference expectation than DRO at a   \n255 comparable level of the prediction residual. In Figure 2, (b), we observe the experiment results on   \n256 the epidemic spread prediction task using three epidemic datasets. With the same MLP architecture,   \n257 mRCP delivers superior Pareto fronts relative to the baselines. When employing the epidemic PDE,   \n258 the SIR model only has two trainable parameters, so their data points can not compose Pareto curves   \n259 due to the model\u2019s limited flexibility. Thus, we show the average of these points. Despite this   \n260 limitation, mRCP maintains its advantage over the baseline methods. Figure 2, (c) and (d) present   \n261 results from the traffic prediction task on three different traffic datasets. Here, the Pareto curves   \n262 for both the MLP and the reaction-diffusion (RD) PDE model are well-defined, because RD model   \n263 with six parameters, offers greater adaptability, allowing for clearer Pareto fronts. Overall, Figure 2   \n264 collectively indicates that mRCP consistently achieves lower coverage difference expectations without   \n265 compromising prediction residual as significantly as DRO and V-REx in different tasks and datasets. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "266 6 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "267 mRCP can distinguish coverage differences under concept shift and covariate shift. A notable   \n268 feature of the mRCP Pareto curves depicted in Figure 2 is their results when $\\beta$ is small, which are not   \n269 at $\\mathbb{E}_{e}^{\\prime}[\\mathbb{E}_{\\alpha}[D^{(e)}]]=1$ , unlike the Pareto curves of V-REx. This is because, during training, mRCP   \n270 has considered the coverage difference under covariate shift by applying importance weighting to   \n271 calibration conformal score CDF. Consequently, as $\\beta$ in Eq. (19) increases, the NTW term is only   \n272 trained to mitigate the coverage difference under the concept shift, as shown in Figure 2,(a).   \n273 DRO and V-REx are defeated because of improper selection of optimization metrics. Examining   \n274 Eq. (20) and Eq. (21), we can see both baselines aim to promote fairness by equalizing the expected   \n275 losses across different domains. As the loss function is $\\ell_{1}$ norm, which is identical to how conformal   \n276 scores are calculated, the experiment results of $\\Delta\\mathbb{E}$ in the last row of Figure 3 show this metric is   \n277 ineffective in capturing the coverage difference due to concept shift.   \n278 Nonetheless, mRCP\u2019s limitations arise from the inherent challenges associated with penalty  \n279 based optimization algorithms. Whether it is mRCP or V-REx, penalty-based optimization algo  \n280 rithms necessitate a model with a high capacity for ftiting complex patterns. For instance, in Figure 2,   \n281 (b), the Pareto curves are not discernible when predictions are derived from an epidemic PDE (SIR   \n282 model) with only two adjustable parameters. In contrast, as shown in Figure 2, (d), the traffic PDE   \n283 (RD model) demonstrates greater flexibility and adaptability with six tunable parameters, exhibiting   \n284 distinct Pareto curves. ", "page_idx": 7}, {"type": "image", "img_path": "TKbGTj0YCZ/tmp/f60442cfa68812e2dd0ae369031c3fa6b622ea2e32b5bb8be63e13cd0230a3f6.jpg", "img_caption": ["Figure 2: Pareto fronts of Multi-domain Robust Conformal Prediction $(\\mathbf{mRCP})$ ), compared with DRO and V-REx: Experimental results of (a) airfoil self-noise example, (b) epidemic spread prediction, and (c) (d) traffic speed prediction. mRCP always reaches a smaller coverage difference expectation than DRO and V-REx with less increase in prediction residual. Red boxes in (b) are zoomed-in areas. Shadow areas and error bars indicate the standard error. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "285 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "286 This study begins by decomposing the coverage difference caused by covariate and concept shifts.   \n287 We then introduce the Normalized Truncated Wasserstein distance (NTW) as a metric for capturing   \n288 coverage difference expectation under concept shift by comparing the test and weighted calibration   \n289 conformal score CDFs. This metric can indicate the discrepancy position in calibration and test score   \n290 distributions. Normalization and truncation make the metric score scales and outliers. Finally, we   \n291 develop an end-to-end algorithm called Multi-domain Robust Conformal Prediction (mRCP) that   \n292 incorporates NTW during training, allowing coverage to approach confidence in all test domains. ", "page_idx": 8}, {"type": "text", "text": "293 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "294 [1] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk mini  \n295 mization. arXiv preprint arXiv:1907.02893, 2019.   \n296 [2] Mrinal R Bachute and Javed M Subhedar. Autonomous driving architectures: insights of   \n297 machine learning and deep learning algorithms. Machine Learning with Applications, 6:100164,   \n298 2021.   \n299 [3] Leonardo Bellocchi and Nikolas Geroliminis. Unraveling reaction-diffusion-like dynamics in   \n300 urban congestion propagation: Insights from a large-scale road network. Scientific reports,   \n301 10(1):4876, 2020.   \n302 [4] Azzedine Boukerche and Jiahao Wang. Machine learning-based traffic prediction models for   \n303 intelligent transportation systems. Computer Networks, 181:107530, 2020.   \n304 [5] Pope D. Brooks, Thomas and Michael Marcolini. Airfoil Self-Noise. UCI Machine Learning   \n305 Repository, 2014. DOI: https://doi.org/10.24432/C5VW2C.   \n306 [6] Maxime Cauchois, Suyash Gupta, Alnur Ali, and John C Duchi. Robust validation: Confident   \n307 predictions even when distributions shift. Journal of the American Statistical Association, pages   \n308 1\u201366, 2024.   \n309 [7] Maxime Cauchois, Suyash Gupta, and John C Duchi. Knowing what you know: valid and   \n310 validated confidence sets in multiclass and multilabel prediction. Journal of machine learning   \n311 research, 22(81):1\u201342, 2021.   \n312 [8] Ian Cooper, Argha Mondal, and Chris G Antonopoulos. A sir model assumption for the spread   \n313 of covid-19 in different communities. Chaos, Solitons & Fractals, 139:110057, 2020.   \n314 [9] Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. Traffic graph convolutional   \n315 recurrent neural network: A deep learning framework for network-scale traffic learning and   \n316 forecasting. IEEE Transactions on Intelligent Transportation Systems, 2019.   \n317 [10] Songgaojun Deng, Shusen Wang, Huzefa Rangwala, Lijing Wang, and Yue Ning. Cola-gnn:   \n318 Cross-location attention based graph neural networks for long-term ili prediction. In Proceedings   \n319 of the 29th ACM international conference on information & knowledge management, pages   \n320 245\u2013254, 2020.   \n321 [11] Shai Feldman, Stephen Bates, and Yaniv Romano. Improving conditional coverage via orthog  \n322 onal quantile regression. Advances in neural information processing systems, 34:2060\u20132071,   \n323 2021.   \n324 [12] Robert E. Gaunt and Siqi Li. Bounding kolmogorov distances through wasserstein and related   \n325 integral probability metrics, 2022.   \n326 [13] Amanda Gentzel, Dan Garant, and David Jensen. The case for evaluating causal models   \n327 using interventional measures and empirical data. Advances in Neural Information Processing   \n328 Systems, 32, 2019.   \n329 [14] Subhankar Ghosh, Yuanjie Shi, Taha Belkhouja, Yan Yan, Jana Doppa, and Brian Jones.   \n330 Probabilistically robust conformal prediction. In Uncertainty in Artificial Intelligence, pages   \n331 681\u2013690. PMLR, 2023.   \n332 [15] Isaac Gibbs, John J Cherian, and Emmanuel J Cand\u00e8s. Conformal prediction with conditional   \n333 guarantees. arXiv preprint arXiv:2305.12616, 2023.   \n334 [16] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. Attention based spatial  \n335 temporal graph convolutional networks for traffic flow forecasting. In Proceedings of the AAAI   \n336 Conference on Artificial Intelligence, volume 33, pages 922\u2013929, 2019.   \n337 [17] Wouter M Kouw and Marco Loog. An introduction to domain adaptation and transfer learning.   \n338 arXiv preprint arXiv:1812.11806, 2018.   \n339 [18] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai   \n340 Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap  \n341 olation (rex). In International Conference on Machine Learning, pages 5815\u20135826. PMLR,   \n342 2021.   \n343 [19] Sara Magliacane, Thijs Van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and   \n344 Joris M Mooij. Domain adaptation by using causal inference to predict invariant conditional   \n345 distributions. Advances in neural information processing systems, 31, 2018.   \n346 [20] Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of   \n347 Probability, pages 1269\u20131283, 1990.   \n348 [21] Harsh Parikh, Carlos Varjao, Louise Xu, and Eric Tchetgen Tchetgen. Validating causal   \n349 inference methods. In International conference on machine learning, pages 17346\u201317358.   \n350 PMLR, 2022.   \n351 [22] Yaniv Romano, Evan Patterson, and Emmanuel J. Cand\u00e8s. Conformalized quantile regression.   \n352 In Neural Information Processing Systems, 2019.   \n353 [23] Yaniv Romano, Matteo Sesia, and Emmanuel J. Cand\u00e8s. Classification with valid and adaptive   \n354 coverage. arXiv: Methodology, 2020.   \n355 [24] Nathan Ross. Fundamentals of stein\u2019s method. 2011.   \n356 [25] Hyun-Sun Ryu and Kwang Sun Ko. Sustainable development of fintech: Focused on uncertainty   \n357 and perceived quality issues. Sustainability, 12(18):7669, 2020.   \n358 [26] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally   \n359 robust neural networks for group shifts: On the importance of regularization for worst-case   \n360 generalization. arXiv preprint arXiv:1911.08731, 2019.   \n361 [27] Silvia Seoni, Vicnesh Jahmunah, Massimo Salvi, Prabal Datta Barua, Filippo Molinari, and   \n362 U Rajendra Acharya. Application of uncertainty quantification to artificial intelligence in   \n363 healthcare: A review of last decade (2013\u20132023). Computers in Biology and Medicine, page   \n364 107441, 2023.   \n365 [28] Yue Sun, Chao Chen, Yuesheng Xu, Sihong Xie, Rick S Blum, and Parv Venkitasubramaniam.   \n366 Reaction-diffusion graph ordinary differential equation networks: Traffic-law-informed speed   \n367 prediction under mismatched data. The 12th International Workshop on Urban Computing, held   \n368 in conjunction with ..., 2023.   \n369 [29] Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal   \n370 prediction under covariate shift. Advances in neural information processing systems, 32, 2019.   \n371 [30] Vladimir Vapnik. Principles of risk minimization for learning theory. Advances in neural   \n372 information processing systems, 4, 1991.   \n373 [31] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random   \n374 world, volume 29. Springer, 2005.   \n375 [32] Timothy L Wiemken and Robert R Kelley. Machine learning in epidemiology and health   \n376 outcomes research. Annu Rev Public Health, 41(1):21\u201336, 2020.   \n377 [33] Xin Zou and Weiwei Liu. Coverage-guaranteed prediction sets for out-of-distribution data. In   \n378 Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17263\u201317270,   \n379 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "table", "img_path": "TKbGTj0YCZ/tmp/f286afd5b6c994f575be170c16f2e30de5e9687d6739bfa64883821768ea594f.jpg", "table_caption": ["Table 2: Related works and mRCP "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "381 B Error bound for the assumption of identical coverages ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "382 According to the computation of $q$ and $q^{*}$ in Eq. (1) and Eq. (6), respectively, we can define the   \n383 coverages in unweighted and weighted calibration score distributions as ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{F}_{P}(q)=\\operatorname*{inf}\\left\\{\\hat{F}_{P}(v_{i})|\\hat{F}_{P}(v_{i})\\geq\\lceil(1-\\alpha)(n+1)\\rceil/n,v_{i}\\in V_{c}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "384 ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\hat{F}_{Q/P}(q^{*})=\\operatorname*{inf}\\left\\{\\hat{F}_{Q/P}(v_{i})|\\hat{F}_{Q/P}(v_{i})\\geq\\lceil(1-\\alpha)(n+1)\\rceil/n,v_{i}\\in V_{c}\\right\\}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "385 Denoting $q_{+}=\\operatorname*{inf}\\left\\{v_{i}|v_{i}\\in V_{c},v_{i}>q\\right\\}$ and $q_{+}^{*}=\\operatorname*{inf}\\{v_{i}|v_{i}\\in V_{c},v_{i}>q^{*}\\}$ , we can bound $\\hat{F}_{P}(q)$   \n386 and $\\hat{F}_{Q/P}(q^{*})$ as ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\hat{F}_{P}(q)\\in\\left[\\lceil(1-\\alpha)(n+1)\\rceil/n,\\hat{F}_{P}(q_{+})\\right),\\;\\;\\hat{F}_{Q/P}(q^{*})\\in\\left[\\lceil(1-\\alpha)(n+1)\\rceil/n,\\hat{F}_{Q/P}(q_{+}^{*})\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "387 Therefore, the absolute difference between $\\hat{F}^{*}(q^{*})$ and $\\hat{F}(q)$ is bounded by ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\hat{F}_{Q/P}(q^{*})-\\hat{F}_{P}(q)|<\\operatorname*{max}\\Big(\\hat{F}_{Q/P}(q_{+}^{*})-\\lceil(1-\\alpha)(n+1)\\rceil/n,\\hat{F}_{P}(q_{+})-\\lceil(1-\\alpha)(n+1)\\rceil/n\\Big)<0.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "388 Especially, when the calibration set size $n$ is large enough (like having thousands of samples),   \n389 $\\hat{F}_{Q/P}$ and $\\hat{F}_{P}$ will be quite smooth, the upper above will be even negligible, allowing us to assume   \n390 $\\hat{F}_{Q/P}^{\\ ^{*}}(q^{*})=\\hat{F}_{P}(q).$ . ", "page_idx": 11}, {"type": "text", "text": "391 C Upper bound of coverage difference under concept shift ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "392 In this section, we prove that the W-distance between a test and weighted calibration conformal score   \n393 population CDF can establish an upper bound for coverage difference under concept shift.   \n394 As $D$ quantifies the absolute difference between $\\hat{F}_{Q/P}$ and $\\hat{F}_{Q}$ at a calibration conformal score, it   \n395 can be constrained by an upper bound given by the Kolmogorov distance [12] defined as follows.   \n396 Definition 2 (Kolmogorov Distance). If $F_{1}$ and $F_{2}$ are two cumulative distribution functions $(C D F s)$ ,   \n397 the Kolmogorov distance, $d_{\\mathrm{K}}$ , is defined as the maximum absolute difference between the CDFs. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\nd_{\\mathrm{K}}(F_{1},F_{2})=\\operatorname*{sup}_{v\\in\\mathbb{R}}|F_{1}(v)-F_{2}(v)|.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "398 As $\\hat{F}_{Q/P}$ and $\\hat{F}_{Q}$ are empirical (not population) CDFs of weighted calibration and test conformal   \n399 scores, the bounding relationship can be reformulated as ", "page_idx": 11}, {"type": "equation", "text": "$$\nd_{\\mathsf{K}}(\\hat{F}_{Q},\\hat{F}_{Q/P})=\\operatorname*{sup}_{v\\in V_{c}\\cup V_{t}}|\\hat{F}_{Q}(v)-\\hat{F}_{Q/P}(v)|\\ge\\operatorname*{sup}_{v\\in V_{c}}|\\hat{F}_{Q}(v)-\\hat{F}_{Q/P}(v)|=\\operatorname*{sup}_{v\\in V_{c}}|D(v)|.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "400 The upper bound $d_{\\mathrm{K}}(\\hat{F}_{Q},\\hat{F}_{Q/P})$ depends on the two conformal score sets $V_{c}$ and $V_{t}$ , indicating that   \n401 the inclusion of samples in $S_{c}$ and $S_{t}$ is likely to introduce variability in $d_{\\mathrm{K}}(\\hat{F}_{Q},\\hat{F}_{Q/P})$ . Nevertheless,   \n402 we aim for an upper bound that is not reliant on specific samples and relies on the calibration and test   \n403 conformal score population CDFs, $F_{P}$ and $F_{Q}$ .   \n404 Firstly, we convert the upper limit in Eq. (22) into terms of $F_{P}$ and $F_{Q}$ . Denoting the joint probability   \n405 density function (PDF) of features and score in the calibration and test domain as $\\mathbf{p}_{X V}$ and $\\mathbf{q}_{X V}$   \n406 respectively, the corresponding continuous CDFs of conformal scores are illustrated as ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\nF_{P}(v)=\\int_{0}^{v}\\int_{X}\\mathbf{p}_{X V}(u,t)d u d t,\\;\\;F_{Q}(v)=\\int_{0}^{v}\\int_{X}\\mathbf{q}_{X V}(u,t)d u d t,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "407 where $\\mathcal{X}$ is the space of the feature variable $X$ . ", "page_idx": 12}, {"type": "text", "text": "408 PDFs of features in calibration and test domains, denoted as $\\mathbf{p}_{X}$ and $\\mathbf{q}_{X}$ respectively, are defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{p}_{X}=\\int_{\\mathbb{R}}\\mathbf{p}_{X V}(u,t)d t,\\,\\,\\,\\mathbf{q}_{X}=\\int_{\\mathbb{R}}\\mathbf{q}_{X V}(u,t)d t.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "409 To address the coverage difference due to covariate shift, importance weighting from [29] is rewritten   \n410 as $\\begin{array}{r}{w=\\frac{\\mathbf q x}{\\mathbf p_{X}}}\\end{array}$ . Also, normalization is unnecessary, because $w$ here is a correction function to transform   \n411 the marginal distribution of $\\mathbf{p}_{X}$ into $\\mathbf{q}_{X}$ . The weighted version of $\\mathbf{p}_{X V}$ is denoted as $\\mathbf{p}_{X V}^{\\prime}\\,=$   \n412 $w\\mathbf{p}_{X V}=\\mathbf{q}_{X}\\mathbf{p}_{V|X}$ , which can be applied to derive the weighted continuous CDF of calibration   \n413 conformal score by ", "page_idx": 12}, {"type": "equation", "text": "$$\nF_{Q/P}(v)=\\int_{0}^{v}\\int_{\\mathcal{X}}\\mathbf{p}_{X V}^{\\prime}(u,t)d u d t=\\int_{0}^{v}\\int_{\\mathcal{X}}\\mathbf{q}_{X}(u)\\mathbf{p}_{V|X}(u,t)d u d t.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "414 The Kolmogorov distance between $F_{Q/P}$ and $F_{Q}$ is $d_{\\mathrm{K}}(F_{Q},F_{Q/P})=\\operatorname*{sup}_{v\\in\\mathbb{R}}|F_{Q}(v)-F_{Q/P}(v)|$ .   \n415 Theorem 1 (Triangular Inequality for Kolmogorov Distance). If $F_{1},\\,F_{2}$ , and $F_{3}$ are three cumulative   \n416 distribution functions $(C D F s)$ , their Kolmogorov distances follow this inequality: ", "page_idx": 12}, {"type": "equation", "text": "$$\nd_{\\mathrm{K}}(F_{1},F_{3})\\leq d_{\\mathrm{K}}(F_{1},F_{2})+d_{\\mathrm{K}}(F_{2},F_{3}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "417 Proof. Consider any point $x\\in\\mathbb R$ , then we have $|F_{1}(x)-F_{3}(x)|\\,\\le\\,|F_{1}(x)-F_{2}(x)|+|F_{2}(x)-$   \n418 $F_{3}(x)|$ .This inequality holds due to the triangle inequality for absolute values. Now, taking the supre  \n419 mum over all $x$ , we have $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathbb{R}}|F_{1}(x)-\\bar{F_{3}}(x)|\\leq\\operatorname*{sup}_{x\\in\\mathbb{R}}\\left(|F_{1}(x)-F_{2}\\right.\\right.}\\end{array}$ $|\\dot{\\leq}\\operatorname*{sup}_{x\\in\\mathbb{R}}\\left(|F_{1}(x)-F_{2}(x)|+|F_{2}(x)^{-}-F_{3}(\\bar{x})|\\right)$ .   \n420 Note that the right-hand side is not necessarily equal to the sum of the suprema of the individ  \n421 ual terms, because the points at which the suprema of $|F_{1}(x)\\,-\\,F_{2}(x)|$ and $|F_{2}(x)\\,-\\,F_{3}(x)|$   \n422 are attained may be different. However, we know that for any $x$ , $|F_{1}(x)\\,-\\,F_{2}(x)|$ is at most   \n423 $d_{K}(F_{1},F_{2})$ and $|F_{2}(x)-F_{3}(x)|$ is at most $d_{K}(F_{2},F_{3})$ . Therefore, $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathbb{R}}|F_{1}(x)-F_{3}(x)|\\ \\leq}\\end{array}$   \n424 $d_{K}(F_{1},F_{2})+d_{K}(F_{2},F_{3})$ . Since the left-hand side is the definition of $d_{K}(\\tilde{F}_{1},F_{3})$ , we can demon  \n425 strate that $d_{\\mathrm{K}}(F_{1},F_{3})\\le d_{\\mathrm{K}}(F_{1},F_{2})+d_{\\mathrm{K}}(F_{2},F_{3})$ . \u53e3   \n426 As Kolmogorov distance satisfies the triangular inequality theorem, as shown and proved in Theo  \n427 rem 1, the triangular inequality relationship can be expanded to ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\nd_{\\mathrm{K}}(\\hat{F}_{Q},\\hat{F}_{Q/P})\\leq d_{\\mathrm{K}}(F_{Q/P},\\hat{F}_{Q/P})+d_{\\mathrm{K}}(F_{Q},F_{Q/P})+d_{\\mathrm{K}}(\\hat{F}_{Q},F_{Q}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "428 Secondly, the Kolmogorov distance between an empirical CDF and its corresponding population CDF   \n429 can be constrained by Dvoretzky\u2013Kiefer\u2013Wolfowitz (DKW) inequality [20], defined in Definition 3.   \n430 Definition 3 (Dvoretzky\u2013Kiefer\u2013Wolfowitz (DKW) Inequality). If $F$ is a population cumulative   \n431 distribution function $(C D F)$ , and $\\hat{F}$ is an empirical CDF with $n$ samples of a random variable $X$ ,   \n432 then for any $\\begin{array}{r}{\\epsilon\\geq\\sqrt{\\frac{1}{2n}\\ln{2}},}\\end{array}$ , the following inequality holds. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(d_{\\mathrm{K}}(\\hat{F},F)>\\epsilon)\\leq e^{-2n\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "433 Based on Definition 3, saying $|V_{c}|\\;\\;=\\;n$ and $|V_{t}|\\;\\;=\\;\\;m$ , we can apply DKW inequality to   \n434 $d_{\\mathrm{K}}\\big(\\hat{F}_{Q/P},F_{Q/P}\\big)$ and $d_{\\mathrm{K}}(\\hat{F}_{Q},F_{Q})$ as follows, for $\\epsilon\\geq\\sqrt{\\frac{1}{2n}\\ln{2}}$ and $\\rho\\ge\\sqrt{\\frac{1}{2m}\\ln{2}}$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}(d_{\\mathrm{K}}(\\hat{F}_{Q/P},F_{Q/P})\\leq\\epsilon)>e^{-2n\\epsilon^{2}},\\ \\operatorname*{Pr}(d_{\\mathrm{K}}(\\hat{F}_{Q},F_{Q})\\leq\\rho)>e^{-2m\\rho^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "435 If the two events $d_{\\mathrm{K}}\\big(\\hat{F}_{Q/P},F_{Q/P}\\big)\\,<\\,\\epsilon$ and $d_{\\mathrm{K}}(\\hat{F}_{Q},F_{Q})\\,<\\,\\rho$ are independent, the inequality in   \n436 Eq. (26) can be expanded in Eq. (27), which holds with at least probability $e^{-2(n\\epsilon^{2}+m\\rho^{2})}$ . By applying   \n437 DKW inequality, we successfully quantify the variability of $d_{\\mathrm{K}}(\\hat{F}_{Q},\\hat{F}_{Q/P})$ in Eq. (22) as a form of a   \n438 probable event, and use the population conformal score CDFs to limit the worst-case of coverage   \n439 difference under concept shift. ", "page_idx": 13}, {"type": "equation", "text": "$$\nd_{\\mathrm{K}}(\\hat{F}_{Q},\\hat{F}_{Q/P})\\le d_{\\mathrm{K}}(F_{Q},F_{Q/P})+\\rho+\\epsilon.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "440 Finally, having established in Eq. (13) that the W-distance can serve as an estimator for coverage   \n441 difference expectation, we explore whether Eq. (27) may similarly be bounded by this metric. The   \n442 W-distance of the two population conformal score CDFs are explicitly shown as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{I_{\\mathbf{w}}(F_{Q},F_{Q/P})=\\int_{\\mathbb{R}}\\big|F_{Q}(v)-F_{Q/P}(v)\\big|d v=\\int_{\\mathbb{R}}\\bigg|\\int_{0}^{v}\\int_{\\mathbb{R}}\\mathbf{q}_{X V}(u,t)d u d t-\\int_{0}^{v}\\int_{\\mathbb{R}}\\mathbf{p}_{X V}^{\\prime}(u,t)d u d t\\bigg|}}\\\\ &{}&{=\\int_{\\mathbb{R}}\\bigg|\\int_{0}^{v}\\int_{\\mathbb{R}}\\mathbf{q}_{X V}(u,t)d u d t-\\int_{0}^{v}\\int_{\\mathbb{R}}\\mathbf{q}_{X}(u)\\mathbf{p}_{V|X}(u,t)d u d t\\bigg|d v}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "443 According to [24], if the weighted calibration conformal score probability density function (PDF) has   \n444 Lebesgue density bounded by $\\mathcal{C}$ , which means ${\\bf p}_{V}^{\\prime}$ does not exceed $\\mathcal{C}$ , then for any test conformal   \n445 score PDF $\\mathbf{q}_{V}$ , $\\bar{d}_{\\mathrm{K}}(F_{Q},F_{Q/P})$ can be bounded as ", "page_idx": 13}, {"type": "equation", "text": "$$\nd_{\\mathrm{K}}(F_{Q},F_{Q/P})\\leq\\sqrt{2\\mathcal{C}d_{\\mathrm{W}}(F_{Q},F_{Q/P})}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "446 Finally, we can derive the upper limit of coverage difference under concept shift, $\\operatorname*{sup}_{v\\in V_{c}}|D(v)|$ , in   \n447 Eq. (30) at least probability e\u22122(n\u03f5 +m\u03c1 ). ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{v\\in V_{c}}|D(v)|\\leq d_{\\mathsf{K}}\\big(\\hat{F}_{Q},\\hat{F}_{Q/P}\\big)\\leq\\sqrt{2\\mathcal{C}d_{\\mathsf{W}}\\big(F_{Q},F_{Q/P}\\big)}+\\epsilon+\\rho\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "448 This property is attractive in that the maximum difference in coverage due to concept shift can also   \n449 be constrained in relation to the W-distance of population score CDFs, denoted as $\\dot{d}_{\\mathrm{W}}(F_{Q},F_{Q/P})$ .   \n450 Despite the unobservability of $d_{\\mathrm{W}}(F_{Q},F_{Q/P})$ , we can still estimate it using its empirical form,   \n451 $d_{\\mathrm{W}}\\big(\\hat{F}_{Q},\\hat{F}_{Q/P}\\big)$ .   \n452 Even though coverage guarantee on an arbitrary joint shift is almost impossible, Eq. (28) demonstrates   \n453 robust conformal prediction is attainable if we can train a function reducing the discrepancy between   \n454 calibration and test conformal score distributions. To be specific, $d_{\\mathrm{W}}(F_{Q},\\stackrel{-}{F}_{Q/P})$ can be reduced to   \n455 zero as far as $\\mathbf{p}_{V|X}=\\mathbf{q}_{V|X}$ . In other words, if we regard $\\mathbf{p}_{X V}$ and $\\mathbf{q}_{X V}$ as push-forward probability   \n456 distribution of $P_{X V}$ and $Q_{X V}$ by the trained model $f$ , making the concept shift between $\\mathbf{p}_{V|X}$ and   \n457 $\\mathbf{q}_{V}|X$ smaller will reduce coverage difference expectation on test domain. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "458 D Datasets, models, and experiment setups ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "459 Extensive experiments are conducted under 3 tasks with 7 datasets. Some tasks involve both black-box   \n460 and physics-informed models to demonstrate the generalizability of NTW and mRCP. ", "page_idx": 13}, {"type": "text", "text": "461 D.1 Airfoil self-noise example ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "462 The airfoil dataset from the UCI Machine Learning Repository [5] consists of 1503 instances of   \n463 1-dimensional target $Y$ and 5-dimensional feature $X\\,=\\,(X_{1},X_{2},X_{3},X_{4},X_{5})$ . This dataset is   \n464 manually separated and modified to create three different domains. ", "page_idx": 13}, {"type": "text", "text": "465 Domain separation: ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "466 Step 1. Covariate Shift by Data Separation. The original dataset is initially segmented into three   \n467 primary subsets $A,B,C$ based on the $33\\%$ and $66\\%$ quantiles of the first dimension $X_{1}$ . Subsequently,   \n468 each of these subsets is further divided into three smaller portions at a 7:2:1 ratio, denoted like   \n469 $A_{0.7},A_{0.2},A_{0.1}$ from $A$ . Finally, we assemble three new datasets with covariate shift as $S^{(e_{1})}=$   \n470 $)\\,B_{0.2}\\cup C_{0.1},\\,S^{(e_{2})}=A_{0.2}\\cup B_{0.1}\\cup C_{0.7},\\,S^{(e_{3})}=A_{0.2}\\cup B_{0.1}\\cup C_{0.2}$ .   \n471 Step 2. Concept Shift by Target Modification. Differently distributed random noises are added to   \n472 target values to cause concept shifts. For $y_{i}$ from $S^{(e_{1})}$ , $y_{i}+\\mathrm{~=~}y_{i}/1000\\mathrm{~*~}\\tau$ ; for $y_{i}$ from $S^{(e_{2})}$ ,   \n473 $y_{i}+=y_{i}/\\tau$ ; for $y_{i}$ from $S^{(e_{3})}$ , $y_{i}+=\\tau$ . $\\tau$ follows a normal distribution $N(0,10^{2})$ . Since we obtain   \n474 three subsets in the end, $|\\mathcal{E}|=3$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "475 Model selection: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "476 We utilize a straightforward multilayer perceptron (MLP) as a trainable model, with an architecture   \n477 of (input dimension, 64, 64, 1) tailored for the regression task. ", "page_idx": 14}, {"type": "text", "text": "478 D.2 Traffic speed prediction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "479 The Seattle-loop [9], and PeMSD4, PeMSED8 datasets [16] contain sensor-observed traffic volume   \n480 and speed data collected in Seattle, San Francisco, and San Bernardino. The snapshots from sensors   \n481 are taken at 5-minute intervals. This task aims to predict the traffic speed of the local road segment in   \n482 the next time step, using the traffic data from local and neighboring segments collected currently. ", "page_idx": 14}, {"type": "text", "text": "483 Domain separation: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "484 Naturally, instances can be categorized into 24 subsets, $|\\mathcal{E}=24|$ , based on the hour they are obtained.   \n485 It is anticipated that there are joint shifts between the data distribution of every single hour (test   \n486 domains) and the data distribution of the whole day (calibration domain), as traffic patterns vary over   \n487 time, making it unnecessary to modify any data. We select the workday data from the three datasets. ", "page_idx": 14}, {"type": "text", "text": "488 Model selection: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "489 (a) MLP with the same structure (input dimension, 64, 64, 1) is applied to the traffic prediction task. ", "page_idx": 14}, {"type": "text", "text": "490 (b) The Reaction-Diffusion (RD) model is selected as the physics-informed Partial differential   \n491 equation (PDE) for traffic speed prediction. Reaction-diffusion mechanism, originally formulated for   \n492 chemical systems to describe particle dynamics, has been adapted for traffic analysis by [3] to uncover   \n493 traffic patterns on different road segments, offering an alternative to purely data-driven models like   \n494 long-short-term memory. [28] further advanced this approach by integrating the RD model into   \n495 graphical neural networks to capture traffic state interactions among adjacent road segments, with   \n496 the reaction term accounting for influences against traffic flow and the diffusion term for influences   \n497 along it. To be specific, for a given sensor $i$ , with $N^{d}$ upstream and $N^{r}$ downstream neighboring   \n498 sensors, the traffic states from these sensors impact sensor $i$ after $\\delta t$ time through diffusion and   \n499 reaction effects, respectively. We expand the original RD model in [28] to Eq. (31), where the traffic   \n500 speed and volume at sensor $i$ at time $t$ is $u_{i}(t)$ and $q_{i}(t)$ , respectively. The parameters $\\rho_{(i,j)}$ and $\\sigma_{(i,j)}$   \n501 represent the diffusion and reaction strengths between sensor $i$ and sensor $j$ , while their superscripts   \n502 indicate if they serve for speed or volume. Also, $d_{i}$ and $r_{i}$ are bias terms for the two components. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{i}(t+\\delta t)-u_{i}(t)=\\displaystyle\\sum_{j\\in N^{d}}(\\rho_{(i,j)}^{u}(u_{i}(t)-u_{j}(t))+\\rho_{(i,j)}^{q}(q_{i}(t)-q_{j}(t))+d_{i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\operatorname{tanh}(\\displaystyle\\sum_{j\\in N^{r}}\\sigma_{(i,j)}^{u}(u_{i}(t)-u_{j}(t))+\\sigma_{(i,j)}^{q}(q_{i}(t)-q_{j}(t))+r_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "503 D.3 Epidemic spread prediction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "504 Three epidemic datasets, US-Regions, US-States, and Japan-Prefectures [10] include the number of   \n505 patients infected by influenza-like illness (ILI) recorded by U.S. Department of Health and Human   \n506 Services, Center for Disease Control and Prevention (CDC), and Japan Infectious Diseases Weekly   \n507 Report. We aim to use the local population, the rise in the number of infected patients observed this   \n508 week, and the cumulative total of infections as predictive features of the increase in infections for the   \n509 upcoming week.   \n510 Domain separation: According to the Pandemic Intervals Framework (PIF) by CDC, samples are   \n511 divided by four pandemic intervals, Initiation, Acceleration, Declaration, and Subsidence, so $\\bar{|\\mathcal{E}|}=4$ .   \n512 We establish the interval endpoints based on specific percentages of the total infected patient count,   \n513 specifically at the $15\\%$ , $50\\%$ , and $85\\%$ thresholds. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "514 Model selection: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "515 (a) MLP with the same architecture is utilized for the epidemic spread forecasting task as well. ", "page_idx": 14}, {"type": "text", "text": "516 (b) PDE for this task is the SIR model that categorizes the population into three groups: those   \n517 susceptible to the disease $S$ , those infectious $I$ , and those who have recovered and gained immunity   \n518 $R$ . It outlines the temporal changes in their populations, as described by [8]. The governing   \n519 differential equations can be expressed as Eq. 32, where $N,\\lambda$ , and $\\gamma$ represent the total population,   \n520 infection rate, and recovery rate, respectively. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\frac{d S(t)}{d t}=\\frac{-\\lambda S(t)I(t)}{N},}\\\\ {\\frac{d I(t)}{d t}=\\frac{\\lambda S(t)I(t)}{N}-\\gamma I(t)=(\\frac{\\lambda S(t)}{N}-\\gamma)I(t),}\\\\ {\\frac{d R(t)}{d t}=\\gamma I(t).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "521 We make the assumption that the location is isolated, hence $N=S(t)+I(t)+R(t)$ . Additionally,   \n522 the population of recovered individuals is represented by $\\begin{array}{r}{R(t)\\,=\\,\\gamma\\int_{0}^{t}I(t)d t}\\end{array}$ . Given this, if $t_{o}$   \n523 signifies the initial time of the current epidemic and $\\delta t$ denotes the time step, which is a week in the   \n524 three datasets, we can express the dynamic change of infectious individuals discretely as Eq. (33). ", "page_idx": 15}, {"type": "equation", "text": "$$\nI(t+\\delta t)-I(t)=\\left(\\frac{\\lambda(N-I(t)-\\gamma\\sum_{t_{o}}^{t}I(t))}{N}-\\gamma\\right)I(t).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "525 D.4 Experiment setups for NTW and baseline metrics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "526 As we only need to validate the positive correlation between NTW and coverage difference ex  \n527 pectation, all models are trained by ERM. In the airfoil self-noise example, 100 trials are carried   \n528 out. For the traffic task, 61 locations from the Seattle-loop, 59 locations from PeMSD4, and 33   \n529 locations from PeMSD8 are chosen, with 10 trials conducted at each location. For simplicity in the   \n530 calculation, all selected locations have just one segment upstream and one segment downstream. For   \n531 epidemic datasets, all locations from US-Regions, US-States, and Japan-Prefectures (49 locations   \n532 in US-States, 10 locations in US-Regions, and 46 locations in Japan-Prefectures) are encompassed   \n533 in the experiments, with 10 trials implemented on each location. The same experiment setups are   \n534 operated on all baseline metrics and NTW. $\\sigma$ values for MLP and PDE are 0.8 and 0.95, respectively.   \n535 The ratio of training, calibration, validation, and testing data on airfoil self-noise datasets, three   \n536 traffic datasets, and three epidemic datasets are 1:1:1:1, 3:2:2:3, and 1:2:1:1, respectively. Data   \n537 separation was conducted randomly. Adam optimizer with a learning rate of 0.001 was applied for all   \n538 experiments. On average, one trial requires one hours on a workstation with double NVIDIA RTX   \n539 3090 GPU. ", "page_idx": 15}, {"type": "text", "text": "540 D.5 Experiment setups for mRCP, V-REx, and DRO ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "541 We define 1 trial as running a series of experiments of all predefined $\\beta$ values once, except for DRO.   \n542 For the airfoil self-noise example, 100 trials with random data preprocessing are conducted. For the   \n543 traffic speed prediction task, we randomly select 10 locations from each of the three traffic datasets   \n544 and operate one trial on all selected locations. In the epidemic spread prediction task, all locations of   \n545 the three datasets are included and we operate one trial on each of them. All combinations of models   \n546 (MLP and PDE) and algorithms (mRCP, DRO, V-REx) share the same experiment setups mentioned   \n547 above. $\\sigma$ values for MLP and PDE are 0.8 and 0.95, respectively. $\\beta$ values for mRCP and V-REx in   \n548 different experiment setups are shown in Table 3. Each Pareto curve consists of at least $10\\ \\beta$ values.   \n549 For airfoil self-noise datasets and three traffic datasets, the original data is evenly and randomly split   \n550 for training, calibration, and testing. For three epidemic datasets, we randomly split the original data   \n551 for training, calibration, and testing with a ratio of 2:1:2. Adam optimizer with a learning rate of   \n552 0.001 was applied for all experiments. On average, one trial requires 12 hours on a workstation with   \n553 double NVIDIA RTX 3090 GPU. ", "page_idx": 15}, {"type": "table", "img_path": "TKbGTj0YCZ/tmp/5bce50be16d8abb8132b4e4006d3324567822a515e58587bfe42be39bcf58605.jpg", "table_caption": ["Table 3: $\\beta$ values for mRCP and V-REx in experiment setups "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "554 E Additional experiment results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "555 E.1 Pearson coefficient definition ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "556 Here we provide a detailed definition of the Pearson coefficient as follows. ", "page_idx": 16}, {"type": "text", "text": "557 Definition 4 (Pearson coefficient). The Pearson correlation coefficient, denoted as $r$ , is calculated as   \n558 the covariance of the two variables divided by the product of their standard deviations, as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\nr={\\frac{\\sum(x_{i}-{\\overline{{x}}})(y_{i}-{\\overline{{y}}})}{\\sqrt{\\sum(x_{i}-{\\overline{{x}}})^{2}\\sum(y_{i}-{\\overline{{y}}})^{2}}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "559 where $x_{i}$ and $y_{i}$ are the individual sample points of random variables $X$ and $Y$ indexed with i and $\\overline{{x}}$   \n560 and $\\overline{y}$ are the means of their samples, respectively.   \n561 The Pearson correlation coefficient measures the linear correlation between two variables. It gives a   \n562 value between -1 and 1 inclusive, where 1 indicates a perfect positive linear relationship, -1 indicates   \n563 a perfect negative linear relationship, and 0 indicates no linear correlation. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "564 E.2 Correlation visualization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "565 Figure 3 shows the experimental results of the correlation between NTW and coverage difference   \n566 expectation, compared with three baselines: total variation, KL divergence, and expectation difference.   \n567 It is organized into a matrix of subplots, with each column corresponding to a specific dataset and   \n568 each row depicting the performance of a metric. Within these subplots, individual points represent   \n569 the conjunction of a metric\u2019s value with the associated coverage difference expectation for a given   \n570 test domain. A positive trend between NTW and the coverage difference expectation is shown in the   \n571 top row, showcasing NTW\u2019s strong correlation. In contrast, the other metrics exhibit inconsistent   \n572 correlations across the varied datasets and models, as seen in the lower three rows of subplots.   \n573 Figure 4 also illustrates the expected coverage difference\u2019s correlation to NTW, standard W-distance,   \n574 normalized W-distance, and truncated W-distance, proving that normalization and truncation are   \nequally important for robust correlations. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "TKbGTj0YCZ/tmp/ec79fc5f538090cb3940392fafbdcfb38af36675629747ba38c263785d6a0ae6.jpg", "img_caption": ["Figure 3: Experimental results of the correlation between Normalized Truncated Wasserstein distance (NTW) and coverage difference expectation, compared with total variation, KL divergence, and expectation difference. Each point represents a pair of metric value and coverage difference expectation for a test domain. The first row of the subplots demonstrates NTW indicates the expectation across different datasets and models, whereas other baseline metrics, represented in the other three rows, can not consistently capture it. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "TKbGTj0YCZ/tmp/3c4b44f9b2da2148cd95d3cc5f58bacd1589159a8803a26df834406a89b86b32.jpg", "img_caption": ["Figure 4: Experimental results of the correlation between Normalized Truncated Wasserstein distance (NTW) and coverage difference expectation of concept shift, compared with standard, normalized, and truncated Wasserstein distance. Each point represents a pair of metric value and coverage difference expectation of a test domain. By comparing the first row with the rest three rows, we validate the necessity of applying normalization and truncation together. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "576 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "8 Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "81 Justification: We focus on robust conformal prediction under joint distribution shift. The   \n82 abstract and introduction accurately state our contributions. We propose Normalized Wasser  \n83 stein distance to quantify the coverage difference caused by concept shift and develop   \n84 multi-domain robust conformal prediction to make coverage approach confidence when   \n85 multiple test domains hold joint shifts with the calibration domain. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitation of our proposed method in Section 6. The proposed method requires the training model\u2019s enough capacity to fit complex patterns. Also, as it needs to approximate the ratio of covariate likelihood between calibration and test domains, it requires enough training and calibration samples to conduct accurate kernel density estimation. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 18}, {"type": "text", "text": "28 judgment and recognize that individual actions in favor of transparency play an impor  \n629 tant role in developing norms that preserve the integrity of the community. Reviewers   \n30 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "32 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n33 a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Justification: We refer to Section 3, Appendix B, Appendix C for detailed theoretical work. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "647 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We show detailed experiment setups, including models, datasets, and algorithms, in Section 5 and Appendix D. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 19}, {"type": "text", "text": "681 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n682 authors are welcome to describe the particular way they provide for reproducibility.   \n683 In the case of closed-source models, it may be that access to the model is limited in   \n684 some way (e.g., to registered users), but it should be possible for other researchers   \n685 to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "86 5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "7 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n88 tions to faithfully reproduce the main experimental results, as described in supplemental   \n9 material? ", "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "713 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "714 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n715 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n716 results?   \n8 Justification: We provide detailed information about data splits and preprocessing, hyper  \n19 parameters, optimizer, black-box model architecture, and physics-informed equations in   \n20 Appendix D. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "727 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "733 \u2022 The answer NA means that the paper does not include experiments.   \n734 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n735 dence intervals, or statistical significance tests, at least for the experiments that support   \n736 the main claims of the paper.   \n737 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n738 example, train/test split, initialization, random drawing of some parameter, or overall   \n739 run with given experimental conditions).   \n740 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n741 call to a library function, bootstrap, etc.)   \n742 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n743 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n744 of the mean.   \n745 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n746 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n747 of Normality of errors is not verified.   \n748 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n749 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n750 error rates).   \n751 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n752 they were calculated and reference the corresponding figures or tables in the text.   \n753 8. Experiments Compute Resources   \n754 Question: For each experiment, does the paper provide sufficient information on the com  \n755 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n756 the experiments?   \n757 Answer: [Yes]   \n758 Justification: We provide the information about our workstation and computation time for   \n759 one trial in Appendix D.   \n760 Guidelines:   \n761 \u2022 The answer NA means that the paper does not include experiments.   \n762 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n763 or cloud provider, including relevant memory and storage.   \n764 \u2022 The paper should provide the amount of compute required for each of the individual   \n765 experimental runs as well as estimate the total compute.   \n766 \u2022 The paper should disclose whether the full research project required more compute   \n767 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n768 didn\u2019t make it into the paper).   \n769 9. Code Of Ethics   \n770 Question: Does the research conducted in the paper conform, in every respect, with the   \n771 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n772 Answer: [Yes]   \n773 Justification: Our research follows the NeurPIS Code of Ethics.   \n774 Guidelines:   \n775 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n776 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n777 deviation from the Code of Ethics.   \n778 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n779 eration due to laws or regulations in their jurisdiction).   \n780 10. Broader Impacts ", "page_idx": 21}, {"type": "text", "text": "", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "781 Question: Does the paper discuss both potential positive societal impacts and negative   \n782 societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "783 Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "784 Justification: We mentioned the positive impact of robust conformal prediction in the   \n785 introduction. We do not see negative societal impacts.   \n786 Guidelines:   \n787 \u2022 The answer NA means that there is no societal impact of the work performed.   \n788 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n789 impact or why the paper does not address societal impact.   \n790 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n791 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n792 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n793 groups), privacy considerations, and security considerations.   \n794 \u2022 The conference expects that many papers will be foundational research and not tied   \n795 to particular applications, let alone deployments. However, if there is a direct path to   \n796 any negative applications, the authors should point it out. For example, it is legitimate   \n797 to point out that an improvement in the quality of generative models could be used to   \n798 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n799 that a generic algorithm for optimizing neural networks could enable people to train   \n800 models that generate Deepfakes faster.   \n801 \u2022 The authors should consider possible harms that could arise when the technology is   \n802 being used as intended and functioning correctly, harms that could arise when the   \n803 technology is being used as intended but gives incorrect results, and harms following   \n804 from (intentional or unintentional) misuse of the technology.   \n805 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n806 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n807 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n808 feedback over time, improving the efficiency and accessibility of ML).   \n809 11. Safeguards   \n810 Question: Does the paper describe safeguards that have been put in place for responsible   \n811 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n812 image generators, or scraped datasets)?   \n813 Answer: [NA]   \n814 Justification: Our submission does not pose such risks.   \n815 Guidelines:   \n816 \u2022 The answer NA means that the paper poses no such risks.   \n817 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n818 necessary safeguards to allow for controlled use of the model, for example by requiring   \n819 that users adhere to usage guidelines or restrictions to access the model or implementing   \n820 safety filters.   \n821 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n822 should describe how they avoided releasing unsafe images.   \n823 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n824 not require this, but we encourage authors to take this into account and make a best   \n825 faith effort.   \n826 12. Licenses for existing assets   \n827 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n828 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n829 properly respected?   \n830 Answer: [Yes]   \n831 Justification: Datasets and models are properly cited in the paper.   \n832 Guidelines:   \n833 \u2022 The answer NA means that the paper does not use existing assets.   \n834 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n835 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n836 URL.   \n837 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n838 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n839 service of that source should be provided.   \n840 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n841 package should be provided. For popular datasets, paperswithcode.com/datasets   \n842 has curated licenses for some datasets. Their licensing guide can help determine the   \n843 license of a dataset.   \n844 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n845 the derived asset (if it has changed) should be provided.   \n846 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n847 the asset\u2019s creators.   \n848 13. New Assets   \n849 Question: Are new assets introduced in the paper well documented and is the documentation   \n850 provided alongside the assets?   \n851 Answer: [NA]   \n852 Justification: We do not introduce new assets.   \n853 Guidelines:   \n854 \u2022 The answer NA means that the paper does not release new assets.   \n855 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n856 submissions via structured templates. This includes details about training, license,   \n857 limitations, etc.   \n858 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n859 asset is used.   \n860 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n861 create an anonymized URL or include an anonymized zip file.   \n862 14. Crowdsourcing and Research with Human Subjects   \n863 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n864 include the full text of instructions given to participants and screenshots, if applicable, as   \n865 well as details about compensation (if any)?   \n866 Answer: [NA]   \n867 Justification: This research does not involve crowdsourcing or research with human subjects.   \n868 Guidelines:   \n869 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n870 human subjects.   \n871 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n872 tion of the paper involves human subjects, then as much detail as possible should be   \n873 included in the main paper.   \n874 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n875 or other labor should be paid at least the minimum wage in the country of the data   \n876 collector.   \n877 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n878 Subjects   \n879 Question: Does the paper describe potential risks incurred by study participants, whether   \n880 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n881 approvals (or an equivalent approval/review based on the requirements of your country or   \n882 institution) were obtained?   \n883 Answer: [NA]   \n884 Justification: This research does not involve crowdsourcing or research with human subjects.   \n885 Guidelines:   \n886 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n887 human subjects. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]