{"references": [{"fullname_first_author": "Diederik P Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-12-12", "reason": "Adam is a widely used optimization algorithm, and understanding its properties, particularly its relation to the Gauss-Newton matrix, is crucial for this paper."}, {"fullname_first_author": "James Martens", "paper_title": "Optimizing neural networks with kronecker-factored approximate curvature", "publication_date": "2020-01-01", "reason": "K-FAC is a popular second-order optimization method, and this paper leverages insights from its development for its analysis of the Gauss-Newton matrix."}, {"fullname_first_author": "Vineet Gupta", "paper_title": "Shampoo: Preconditioned stochastic tensor optimization", "publication_date": "2018-01-01", "reason": "Shampoo is another important second-order optimization method relevant to the paper's focus on the Gauss-Newton matrix."}, {"fullname_first_author": "Hong Liu", "paper_title": "Sophia: A scalable stochastic second-order optimizer for language model pre-training", "publication_date": "2023-01-01", "reason": "Sophia is a recently proposed second-order optimizer highlighting the importance of Gauss-Newton approximations, which directly relates to this paper's analysis."}, {"fullname_first_author": "Utku Evci", "paper_title": "The difficulty of training sparse neural networks", "publication_date": "2019-06-10", "reason": "This paper discusses the challenges of training sparse networks, directly motivating the study of the Gauss-Newton matrix's conditioning in relation to sparsity."}]}