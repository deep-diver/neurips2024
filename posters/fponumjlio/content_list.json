[{"type": "text", "text": "Theoretical characterisation of the Gauss-Newton conditioning in Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jim Zhao\u2217 University of Basel, Switzerland jim.zhao@unibas.ch ", "page_idx": 0}, {"type": "text", "text": "Sidak Pal Singh\u2217 ETH Z\u00fcrich, Switzerland sidak.singh@inf.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Aurelien Lucchi University of Basel, Switzerland aurelien.lucchi@unibas.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Gauss-Newton (GN) matrix plays an important role in machine learning, most evident in its use as a preconditioning matrix for a wide family of popular adaptive methods to speed up optimization. Besides, it can also provide key insights into the optimization landscape of neural networks. In the context of deep neural networks, understanding the GN matrix involves studying the interaction between different weight matrices as well as the dependencies introduced by the data, thus rendering its analysis challenging. In this work, we take a first step towards theoretically characterizing the conditioning of the GN matrix in neural networks. We establish tight bounds on the condition number of the GN in deep linear networks of arbitrary depth and width, which we also extend to two-layer ReLU networks. We expand the analysis to further architectural components, such as residual connections and convolutional layers. Finally, we empirically validate the bounds and uncover valuable insights into the influence of the analyzed architectural components. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The curvature is a key geometric property of the loss landscape, which is characterized by the Hessian matrix or approximations such as the Gauss-Newton (GN) matrix, and strongly influences the convergence of gradient-based optimization methods. In the realm of deep learning, where models often have millions of parameters, understanding the geometry of the optimization landscape is essential to understanding the effectiveness of training algorithms. The Hessian matrix helps identify the directions in which the loss function changes most rapidly, aiding in the selection of appropriate learning rates and guiding optimization algorithms to navigate the complex, high-dimensional space of parameters. However, in practice, the Hessian is not easily accessible due to the high computational cost and memory requirements. Instead, the GN matrix (or its diagonal form) is commonly employed in adaptive optimization methods such as Adam [Kingma and Ba, 2014] with the goal to improve the conditioning of the landscape. Although the Gauss-Newton matrix $\\mathbf{G}_{O}$ is only an approximation to the full Hessian matrix, it does seem to capture the curvature of the loss very well given the success of many second-order optimization methods based on approximations of the Gauss-Newton matrix, such as K-FAC [Martens and Grosse, 2020], Shampoo [Gupta et al., 2018] or Sophia [Liu et al., 2023]. Particularly interesting is the last method, in which the authors observe that their optimizer based on the Gauss-Newton matrix performs even better than their optimizer based on the full Hessian matrix, implying that the Gauss-Newton matrix is a good preconditioner and captures the curvature of the loss landscape well. ", "page_idx": 0}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/f607e0f6e58fd649e8bfb2ff201a1b4ede79cabe7345bec5cecc58b3f99e0ce2.jpg", "img_caption": ["Figure 1: Training loss (left) and condition number $\\kappa$ of GN (right) for a ResNet20 trained on a subset of Cifar10 $(n\\,=\\,1000)$ with different proportions of pruned weights. Weights were pruned layerwise by magnitude at initialization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The prevalence of adaptive optimizers and their success in training neural networks illustrates the ability of the GN matrix to capture curvature information similar to that of the Hessian matrix. ", "page_idx": 1}, {"type": "text", "text": "Although the non-convexity of neural loss landscapes can be considered as a given, some landscapes can be tougher to traverse than others. This can, for instance, arise due to, or be amplified by, aspects such as: (i) disparate scales of the input data and intermediate features, (ii) initializing the optimization process at a degenerate location in the landscape (imagine an entire set of neurons being dead), (iii) architectural aspects such as width, depth, normalization layers, etc. ", "page_idx": 1}, {"type": "text", "text": "Given the close relation of the GN matrix to the network Jacobian (with respect to the parameters) and hence signal propagation [Lou et al., 2022], its significance also reaches more broadly to problems such as model pruning and compression. For instance, it has been extensively observed empirically that training extremely sparse neural networks from scratch poses a significantly greater challenge compared to pruning neural networks post-training [Evci et al., 2019]. In an illustrative example shown in Fig. 1, we observe that training sparse networks from scratch using stochastic gradient descent results in a slowdown of training, which is also reflected in an increase in the condition number of the GN matrix (similar experiments on Vision Transformers and other architectures can be found in Appendix I.1). This underscores the relevance of the conditioning of the GN matrix for understanding the behaviour of gradient-based optimizers and emphasizes the significance of maintaining a well-behaved loss landscape of the neural network throughout training. ", "page_idx": 1}, {"type": "text", "text": "In fact, many fundamental components of deep learning frameworks, such as skip-connections as well as various normalization techniques [Ioffe and Szegedy, 2015, Salimans and Kingma, 2016, Ba et al., 2016, Wu and He, 2018, Miyato et al., 2018], have been to some degree designed to mitigate the challenges posed by ill-conditioning in neural network optimization. This line of work continues to expand to this date, seeking out better normalization schemes, optimization maneuvers, and regularization strategies [Kang et al., 2016, Wan et al., 2013] that allow for easier and faster optimization while avoiding directions of pathological curvature. But despite the numerous approaches to redress the poor conditioning of the landscape, understanding the precise factors of ill-conditioning within the network structure, and their relative contributions, has remained largely underdeveloped. ", "page_idx": 1}, {"type": "text", "text": "Hence, our aim in this work is to carry out a detailed theoretical analysis of how the conditioning of the Gauss-Newton matrix is shaped by constituting structural elements of the network \u2014 i.e., the hidden-layer width, the depth, and the presence of skip connections, to name a few. We will shortly formally introduce the Gauss-Newton matrix and how it is connected to the Hessian of the loss function. Concretely, we would like to provide tight bounds for the condition number of these two terms as a function of the spectra of the various weight matrices and reveal the interplay of the underlying architectural parameters on conditioning. Furthermore, we aim to investigate the impact of both the dataset\u2019s structure and the initialization procedure on the conditioning of the loss landscape. ", "page_idx": 1}, {"type": "text", "text": "Contributions. Taking inspiration from prior theoretical analyses of deep linear networks, we make a first foray into this problem by rigorously investigating and characterizing the condition number of the GN matrix for linear neural networks (the extension to the second term of the Gauss-Newton decomposition is touched upon in the Appendix). Our analysis holds for arbitrary-sized networks and unveils the intriguing interaction of the GN matrix with the conditioning of various sets of individual layers and the data-covariance matrix. We also complement our analysis in the linear case by studying the effect of non-linearities, via the Leaky-ReLU activation, albeit for two-layer networks. Importantly, as a consequence of our analysis, we show the precise manner in which residual networks with their skip connections or batch normalization can help enable better conditioning of the loss landscape. While our work builds on Singh et al. [2021], our main contribution is the introduction of tight upper bounds for the condition number of the Gauss-Newton (GN) matrix for linear and residual networks of arbitrary depth and width. To the best of our knowledge, this has not been addressed in the literature before. Lastly, given that our bounds are agnostic to the specific values of the parameters in the landscape, we show the phenomenology of conditioning during the training procedure and the corresponding validity of our bounds. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Setup and background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Setting. Suppose we are given an i.i.d. dataset $S=\\{(\\mathbf{x}_{1},\\mathbf{y}_{1}),\\dots,(\\mathbf{x}_{n},\\mathbf{y}_{n})\\}$ , of size $\\vert S\\vert=n$ , drawn from an unknown distribution $p_{\\mathbb{X},\\mathbb{Y}}$ , consisting of inputs $\\mathbf{x}\\in\\mathbb{X}\\subseteq\\mathbb{R}^{d}$ and targets $\\mathbf{y}\\in\\mathbb{Y}\\subseteq\\mathbb{R}^{k}$ . Based on this dataset $S$ , consider we use a neural network to learn the mapping from the inputs to the targets, $\\operatorname{F}_{\\pmb{\\theta}}:\\mathbb{X}\\mapsto\\mathbb{Y}$ , parameterized by $\\pmb{\\theta}\\in\\Theta\\subseteq\\mathbb{R}^{p}$ . To this end, we follow the framework of Empirical Risk Minimization [Vapnik, 1991], and optimize a suitable loss function $\\mathrm{L}:\\Theta\\mapsto\\mathbb{R}$ . In other words, we solve the following optimization problem, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{\\star}=\\underset{\\pmb{\\theta}\\in\\Theta}{\\mathrm{argmin}}\\ \\mathrm{L}(\\pmb{\\theta})=\\frac1n\\sum_{i=1}^{n}\\ell\\left(\\pmb{\\theta};(\\mathbf{x}_{i},\\mathbf{y}_{i})\\right)\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "say with a first-order method such as (stochastic) gradient descent and the choices for $\\ell$ could be the mean-squared error (MSE), cross-entropy (CE), etc. For simplicity, we will stick to the MSE loss. ", "page_idx": 2}, {"type": "text", "text": "Gauss-Newton Matrix. We analyze the properties of the outer gradient product of the loss function, $\\mathbf{G}_{\\mathrm{O}}$ , which we call the Gauss-Newton matrix, defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{G}_{\\mathrm{O}}=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{\\pmb{\\theta}}\\mathbf{F}_{\\pmb{\\theta}}(\\mathbf{x}_{i})\\,\\nabla_{\\pmb{\\theta}}\\mathbf{F}_{\\pmb{\\theta}}(\\mathbf{x}_{i})^{\\top},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where, $\\nabla_{\\pmb{\\theta}}\\mathrm{F}_{\\pmb{\\theta}}(\\mathbf{x}_{i})\\in\\mathbb{R}^{p\\times k}$ is the Jacobian of the function with respect to the parameters $\\pmb{\\theta}$ , $p$ is the number of parameters, $k$ is the number of outputs or targets. This outer product of the gradient is closely related to the Hessian of the loss function via the Gauss-Newton decomposition [Schraudolph, 2002, Sagun et al., 2017, Martens, 2020, Botev, 2020], hence the chosen name, which decomposes the Hessian via the chain rule as a sum of the following two matrices: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathrm{L}}=\\mathbf{H}_{\\mathrm{O}}+\\mathbf{H}_{\\mathrm{F}}={\\frac{1}{n}}\\sum_{i=1}^{n}\\nabla_{\\theta}\\mathbf{F}_{\\theta}(\\mathbf{x}_{i})\\left[\\nabla_{\\mathrm{F}_{\\theta}}^{2}\\ \\ell_{i}\\right]\\nabla_{\\theta}\\mathbf{F}_{\\theta}(\\mathbf{x}_{i})^{\\top}+{\\frac{1}{n}}\\sum_{i=1}^{n}\\sum_{c=1}^{K}[\\nabla_{\\mathrm{F}_{\\theta}}\\ell_{i}]_{c}\\,\\nabla_{\\theta}^{2}\\,\\mathbf{F}_{\\theta}^{c}(\\mathbf{x}_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\nabla_{\\mathbf{F}_{\\pmb{\\theta}}}^{2}\\,\\ell_{i}\\,\\in\\,\\mathbb{R}^{k\\times k}$ is the Hessian of the loss with respect to the network function, at the $i$ -th sample. Note that if $\\ell_{i}$ is the MSE loss, then $\\mathbf{H}_{\\mathrm{O}}=\\mathbf{G}_{\\mathrm{O}}$ . ", "page_idx": 2}, {"type": "text", "text": "Remark R1 (Difference between ${\\bf{H}}_{L}$ and $\\mathbf{G}_{O}$ ). When considering MSE loss, the difference between the Gauss Newton matrix $\\mathbf{G}_{O}$ and the Hessian of the loss function ${\\bf{H}}_{L}$ depends on both the residual and the curvature of the network $F_{\\pmb{\\theta}}(\\mathbf{x})$ . Thus, close to convergence when the residual becomes small, the contribution of $\\mathbf{H}_{F}$ will also be negligible and $\\mathbf{G}_{O}$ is essentially equal to ${\\bf{H}}_{L}$ . Furthermore, Lee et al. [2019] show that sufficiently wide neural networks of arbitrary depth behave like linear models during training with gradient descent. This implies that the Gauss-Newton matrix is a close approximation of the full Hessian in this regime throughout training. ", "page_idx": 2}, {"type": "text", "text": "Condition number and its role in classical optimization. Consider we are given a quadratic problem, $\\scriptstyle\\operatorname{argmin}_{\\mathbf{w}\\in\\mathbb{R}^{p}}\\frac{1}{2}\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}$ , where $\\mathbf A\\succ0$ is a symmetric and positive definite matrix. The optimal solution occurs for $\\mathbf{w}^{*}\\;=\\;0$ . When running gradient descent with constant step size $\\eta\\:>\\:0$ , the obtained iterates would be $\\mathbf{w}_{k}\\;=\\;(\\mathbf{I}\\,-\\,\\bar{\\eta}\\bar{\\mathbf{A}})\\mathbf{w}_{k-1}$ . This yields a convergence rate of \u2225\u2225wwkk\u22121\u2212\u2212ww\u2217\u2225\u2217\u2225 \u2264max |1 \u2212\u03b7\u03bbmax(A)|, |1 \u2212\u03b7\u03bbmin(A)| . The best convergence is obtained for $\\eta\\,=\\,2\\,(\\lambda_{\\operatorname*{max}}(\\mathbf{A})+\\lambda_{\\operatorname*{min}}(\\mathbf{A}))^{-1}$ , resulting in $\\begin{array}{r}{\\|\\mathbf{w}_{k}\\|\\;\\leq\\;\\frac{\\kappa-1}{\\kappa+1}\\|\\mathbf{w}_{k-1}\\|}\\end{array}$ , where $\\begin{array}{r}{\\kappa(\\mathbf{A})\\,=\\,\\frac{\\lambda_{\\mathrm{max}}(\\mathbf{A})}{\\lambda_{\\mathrm{min}}(\\mathbf{A})}}\\end{array}$ is called the condition number of the matrix. This ratio which, intuitively, measures how disparate are the largest and smallest curvature directions, is an indicator of the speed with which gradient descent would converge to the solution. When $\\kappa\\rightarrow\\infty$ , the progress can be painfully slow, and $\\kappa\\rightarrow1$ indicates all the curvature directions are balanced, and thus the error along some direction does not trail behind the others, hence ensuring fast progress. ", "page_idx": 2}, {"type": "text", "text": "Effect of condition number at initialization on the convergence rate As the condition number is a very local property, it is in general hard to connect the conditioning at network initialization to a global convergence rate. However, we would like to argue below that an ill-conditioned network initialization will still affect the rate of convergence for gradient descent (GD) in the initial phase of training. Let us denote the Lipschitz constant by $L$ and the smoothness constant by $\\mu$ . Furthermore, let the step size be such that $\\begin{array}{r}{\\eta_{k}\\leq\\frac{1}{L}}\\end{array}$ . We present a modified analysis of GD for strongly convex functions, where we use local constants $\\mu(k)$ and $L(k)$ instead of the global smoothness and Lipschitz constant, respectively. Then by the definition of a single step of gradient descent and using the strong convexity and smoothness assumption2 we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n||\\pmb{\\theta}_{k+1}-\\pmb{\\theta}^{*}||^{2}\\leq(1-\\eta_{k}\\mu)||\\pmb{\\theta}_{k}-\\pmb{\\theta}^{*}||^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "So by recursively applying (2) and replacing $\\mu$ by the local smoothness constants $\\mu(k)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n||\\theta_{k}-\\theta^{*}||^{2}\\leq\\prod_{i=0}^{k-1}(1-\\eta_{i}\\mu(i))||\\theta_{0}-\\theta^{*}||^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "One can clearly see the effect of $\\mu(0)$ in the bound, which is even more dominant when $\\mu(k)$ changes slowly. Of course, the effect of $\\mu(0)$ attenuates over time, and that\u2019s why we are talking about a local effect. However, one should keep in mind that overparametrization leads the parameter to stay closer to initialization (at least in the NTK regime [Lee et al., 2019]). ", "page_idx": 3}, {"type": "text", "text": "3 Related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Since the Gauss-Newton matrix is intimately related to the Hessian matrix, and the fact that towards the end of training, the Hessian approaches the Gauss-Newton matrix [Singh et al., 2021], we carry out a broader discussion of the related work, by including the significance of the Hessian at large. ", "page_idx": 3}, {"type": "text", "text": "The relevance of the Hessian matrix for neural networks. (i) Generalization-focused work: There is a rich and growing body of work that points towards the significance of various Hessianbased measures in governing different aspects of optimization and generalization. One popular hypothesis [Hochreiter and Schmidhuber, 1997, Keskar et al., 2016, Dziugaite and Roy, 2017, Chaudhari et al., 2019] is that flatter minima generalize better, where the Hessian trace or the spectral norm is used to measure flatness. This hypothesis is not undisputed [Dinh et al., 2017], and the extent to which this is explanatory of generalization has been put to question recently [Granziol, 2021, Andriushchenko et al., 2023]. Nevertheless, yet another line of work has tried to develop regularization techniques that further encourage reaching a flatter minimum, as shown most prominently in proposing sharpness-aware minimization [Foret et al., 2021]. ", "page_idx": 3}, {"type": "text", "text": "(ii) Understanding Architectural and Training aspects of Neural Networks: Some other work has studied the challenges in large-batch training via the Hessian spectrum in Yao et al. [2018]. Also, the use of large learning rates has been suggested to result in flatter minima via the initial catapult phase [Lewkowycz et al., 2020]. The effect of residual connections and Batch normalization on Hessian eigenspectrum were empirically studied in Ghorbani et al. [2019], Yao et al. [2020], which introduced PyHessian, a framework to compute Hessian information in a scalable manner. More recently, the so-called edge of stability [Cohen et al., 2021] phenomenon connects the optimization dynamics of gradient descent with the maximum eigenvalue of the loss Hessian. Very recently, the phenomenon of Deep neural collapse was studied in Beaglehole et al. [2024] via the average gradient outer-product. ", "page_idx": 3}, {"type": "text", "text": "(iii) Applications: There has also been a dominant line of work utilizing the Hessian for second-order optimization, albeit via varying efficient approximations, most notably via K-FAC [Martens and Grosse, 2020] but also others such as Yao et al. [2021], Liu et al. [2023], Lin et al. [2023]. Given its versatile nature, the Hessian has also been used for model compression through pruning [LeCun et al., 1989, Hassibi et al., 1993, Singh and Alistarh, 2020] as well as quantization [Dong et al., 2019, Frantar et al., 2023], but also in understanding the sensitivity of predictions and the function learned by neural networks via influence functions [Koh and Liang, 2020, Grosse et al., 2023], and countless more. ", "page_idx": 3}, {"type": "text", "text": "Theoretical and empirical studies of the Hessian. There have been prior theoretical studies that aim to deliver an understanding of the Hessian spectrum in the asymptotic setting [Pennington and Bahri, 2017, Jacot et al., 2019], but it remains unclear how to extract results for finite-width networks as used in practice. Besides, past work has analyzed the rank, empirically [Sagun et al., 2016, 2017] as well as theoretically [Singh et al., 2021, 2023]. Further, the layer-wise Hessian of a network can be roughly approximated by the Kronecker product of two smaller matrices, whose top eigenspace has shown to contain certain similarities with that of the Hessian [Wu et al., 2020]. In a different line of work by Liao and Mahoney [2021], the limiting eigenvalue distribution of the Hessian of generalized generalized linear models (G-GLMs) and the behaviour of potentially isolated eigenvalue-eigenvector pairs is analyzed. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Hessian and landscape conditioning. The stock of empirical repertoire in deep learning has been enriched by successful adaptive optimization methods plus their variants [Kingma and Ba, 2014] as well as various tricks of the trade, such as Batch Normalization [Ioffe and Szegedy, 2015], Layer Normalization [Ba et al., 2016], orthogonal initialization [Saxe et al., 2013, Hu et al., 2020], and the kind, all of which can arguably be said to aid the otherwise ill-conditioning of the landscape. There have also been theoretical works establishing a link between the conditioning of the Hessian, at the optimum and the double-descent like generalization behavior of deep networks [Belkin et al., 2019, Singh et al., 2022]. ", "page_idx": 4}, {"type": "text", "text": "Gauss-Newton matrix and NTK. In the context of over-parametrized networks, $\\mathbf{G}_{\\mathrm{O}}$ is for instance connected to the (empirical) Neural Tangent Kernel, which has been the focus of a major line of research in the past few years [Jacot et al., 2018, Wang et al., 2022, Yang, 2020] as the NTK presents an interesting limit of infinite-width networks. As a result the asymptotic spectrum and the minimum eigenvalue of the NTK has been studied in [Nguyen et al., 2022, Liu and Hui, 2023], but the implications for finite-width networks remain unclear. ", "page_idx": 4}, {"type": "text", "text": "Despite this and the other extensive work discussed above, a detailed theoretical study on the Gauss Newton conditioning of neural networks has been absent. In particular, there has been little work trying to understand the precise sources of ill-conditioning present within deep networks. Therefore, here we try to dissect the nature of conditioning itself, via a first principle approach. We hope this will spark further work that aims to precisely get to the source of ill-conditioning in neural networks and, in a longer horizon, helps towards designing theoretically-guided initialization strategies or normalization techniques that seek to also ensure better conditioning of the GN matrix. ", "page_idx": 4}, {"type": "text", "text": "4 Theoretical characterisation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The main part of this paper will focus on analyzing the conditioning of $\\mathbf{G}_{\\mathrm{O}}$ in Eq. (1) as prior work [Ren and Goldfarb, 2019, Schraudolph, 2002] has demonstrated its heightened significance in influencing the optimization process. We will further discuss an extension to $\\mathbf{H}_{\\mathrm{F}}$ in Appendix F. Tying both bounds together yields a bound on the condition number of the overall loss Hessian in some simple setups. ", "page_idx": 4}, {"type": "text", "text": "Pseudo-condition number. Since the GN matrix of deep networks is not necessarily full rank [Sagun et al., 2017, Singh et al., 2021], we will analyze the pseudo-condition number defined as the ratio of the maximum eigenvalue over the minimum non-zero eigenvalue. This choice is rationalized by the fact that gradient-based methods will effectively not steer into the GN null space, and we are interested in the conditioning of the space in which optimization actually proceeds. For brevity, we will skip making this distinction between condition number and pseudo-condition number hereafter. ", "page_idx": 4}, {"type": "text", "text": "4.1 Spectrum of Gauss-Newton matrix ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We will start with the case of linear activations. In this case a network with $L$ layers can be expressed by $\\mathrm{F}_{\\pmb{\\theta}}(\\mathbf{x})=\\mathbf{W}^{L}\\mathbf{W}^{L-1}\\,\\cdot\\,\\cdot\\,\\mathbf{W}^{1}\\mathbf{x}$ , with $\\mathbf{W}^{\\ell}\\in\\mathbb{R}^{a_{\\ell}\\times a_{\\ell-1}}$ for $\\ell=1,\\ldots,L$ and $a_{L}=k,a_{0}=d.$ . To facilitate the presentation of the empirical work, we will assume that the widths of all hidden layers are the same, i.e. $\\alpha_{\\ell}=m$ for all $\\ell=1,\\ldots,L-1$ . Also, let us denote by $\\begin{array}{r}{\\Sigma={\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\dagger}\\in}\\end{array}$ $\\mathbb{R}^{d\\times d}$ the empirical input covariance matrix. Furthermore, we introduce the shorthand-notation $\\mathbf{W}^{k:\\ell}=\\mathbf{W}^{k}\\cdot\\cdot\\cdot\\mathbf{W}^{\\ell}$ for $k>\\ell$ and $k<\\ell$ , $\\mathbf{W}^{k:\\ell}=\\mathbf{W}^{k^{\\top}}\\cdot\\cdot\\cdot\\mathbf{W}^{\\ell^{\\top}}$ . Then, Singh et al. [2021] show that the GN matrix can be decomposed as $\\mathbf{G}_{\\mathrm{O}}=\\mathbf{U}(\\mathbf{I}_{k}\\otimes\\Sigma)\\mathbf{U}^{\\top}$ , where ${\\bf{I}}_{k}$ is the identity matrix of dimension $k$ and $\\mathbf{U}\\in\\mathbb{R}^{p\\times k d}$ is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{U}=\\left(\\mathbf{W}^{2:L}\\otimes\\mathbf{I}_{d}\\,\\ldots\\cdot\\mathbf{W}^{\\ell+1:L}\\otimes\\mathbf{W}^{\\ell-1:1}\\,\\ldots\\,\\mathbf{I}_{k}\\otimes\\mathbf{W}^{L-1:1}\\right)^{\\top}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By rewriting $\\mathbf{U}(\\mathbf{I}_{K}\\otimes\\Sigma)\\mathbf{U}^{\\top}=\\mathbf{U}(\\mathbf{I}_{K}\\otimes\\Sigma^{1/2})(\\mathbf{I}_{K}\\otimes\\Sigma^{1/2})\\mathbf{U}^{\\top}$ , where $\\Sigma^{1/2}$ is the unique positive semi-definite square root of $\\Sigma$ and noting that AB and BA have the same non-zero eigenvalues, we ", "page_idx": 4}, {"type": "text", "text": "have that the non-zero eigenvalues of $\\mathbf{G}_{\\mathrm{O}}$ are the same as those of ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{G}}_{O}=\\left(\\mathbf{I}_{K}\\otimes\\boldsymbol{\\Sigma}^{1/2}\\right)\\mathbf{U}^{\\intercal}\\mathbf{U}(\\mathbf{I}_{K}\\otimes\\boldsymbol{\\Sigma}^{1/2})\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{U}^{\\top}\\mathbf{U}\\in\\mathbb{R}^{K d\\times K d}$ is eq ual to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{U}^{\\top}\\mathbf{U}=\\sum_{\\ell=1}^{L}\\mathbf{W}^{L:\\ell+1}\\mathbf{W}^{\\ell+1:L}\\otimes\\mathbf{W}^{1:\\ell-1}\\mathbf{W}^{\\ell-1:1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Warm-up: the one-hidden layer case. In the case of one-hidden layer network, $\\mathrm{F}_{\\pmb{\\theta}}(\\mathbf{x})=\\mathbf{W}\\mathbf{V}\\mathbf{x},$ we have that $\\widehat{\\mathbf{G}}_{O}\\;=\\;\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\boldsymbol{\\Sigma}+\\mathbf{I}_{k}\\otimes\\boldsymbol{\\Sigma}^{1/2}\\mathbf{V}^{\\top}\\mathbf{V}\\boldsymbol{\\Sigma}^{1/2}$ . To derive an upper bound of the condition num  ber, we will lower bound the smallest eigenvalue $\\lambda_{\\operatorname*{min}}(\\widehat{\\mathbf{G}}_{O})$ and upper bound the largest eigenvalue $\\lambda_{\\mathrm{max}}(\\widehat{\\mathbf{G}}_{O})$ separately. Using standard perturbation bo u nds for matrix eigenvalues discussed in Appendix A ,  we obtain the following upper bound on the condition number: ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let $\\beta_{w}=\\sigma_{\\mathrm{min}}^{2}(\\mathbf{W})/(\\sigma_{\\mathrm{min}}^{2}(\\mathbf{W})+\\sigma_{\\mathrm{min}}^{2}(\\mathbf{V}))$ . Then the condition number of GN for the one-hidden layer network with linear activations with $\\dot{m}>\\operatorname*{max}\\{d,k\\}$ is upper bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq\\,\\kappa(\\mathbf{Z})\\,\\cdot\\,\\frac{\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{W})+\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{V})}{\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W})+\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{V})}=\\,\\kappa(\\mathbf{Z})\\,\\cdot\\,\\left(\\beta_{w}\\,\\kappa(\\mathbf{W})^{2}+\\left(1-\\beta_{w}\\right)\\kappa(\\mathbf{V})^{2}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof sketch. Choosing $\\mathbf{A}=\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\pmb{\\Sigma}$ and $\\mathbf{B}=\\mathbf{I}_{k}\\otimes\\mathbf{\\Sigma}^{1/2}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{\\Sigma}^{1/2}$ and $i=j=1$ for the Weyl\u2019s inequality, $i=j=n$ for the dual Weyl\u2019s inequality and using the fact that WW $^{\\top}\\otimes\\Sigma$ and $\\mathbf{I}_{k}\\otimes\\pmb{\\Sigma}^{1/2}\\bar{\\mathbf{V}}^{\\top}\\bar{\\mathbf{V}}\\pmb{\\Sigma}^{1/2}$ are positive semidefinite yield the result. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "It is important to point out that the convex combination in Eq. (6) is crucial and a more naive bound where we take the maximum of both terms is instead too loose, see details in Appendix E. Later on, we will observe that the general case with $L$ layers also exhibits a comparable structure, wherein the significance of the convex combination becomes even more pronounced in deriving practical bounds. ", "page_idx": 5}, {"type": "text", "text": "Remark R2 (Role of the data covariance). Besides the above dependence in terms of the convex combination of the bounds, we also see how the conditioning of the input data affects the conditioning of the GN spectra. This is observed in Figure 19, where the condition number of the GN matrix is calculated on whitened and not whitened data. This observation might also shed light on why data normalization remains a standard choice in deep learning, often complemented by the normalization of intermediate layer activations. ", "page_idx": 5}, {"type": "text", "text": "4.2 The general $L$ -layer case ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following our examination of the single-hidden layer case, we now broaden our analysis to include $L$ - layer linear networks. As before, we will first derive an expression of the GN matrix and subsequently bound the largest and smallest eigenvalue separately to derive a bound on the condition number. Obtaining the GN matrix involves combining (4) and (5), which yields ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{G}}_{O}=(\\mathbf{I}_{K}\\otimes\\mathbf{\\Sigma}^{1/2})\\mathbf{U}^{\\top}\\mathbf{U}(\\mathbf{I}_{K}\\otimes\\mathbf{\\Sigma}^{1/2})=\\sum_{l=1}^{L}(\\mathbf{W}^{L:\\ell+1}\\mathbf{W}^{\\ell+1:L})\\otimes(\\mathbf{\\Sigma}^{1/2}\\mathbf{W}^{1:\\ell-1}\\mathbf{W}^{\\ell-1:1}\\mathbf{Z}^{1/2}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By repeatedly applying Weyl\u2019s inequalities, we obtain an upper bound on the condition number. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. Assume that $m>\\operatorname*{max}\\{d,k\\}$ and that $\\alpha_{\\ell}:=\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W}^{L:\\ell+1})\\cdot\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W}^{1:\\ell-1})>0\\;\\forall\\ell=$ $\\begin{array}{r}{1,\\dotsc,L.\\ L e t\\gamma_{\\ell}:=\\frac{\\alpha_{\\ell}}{\\sum_{i=1}^{L}\\alpha_{i}}}\\end{array}$ . Then the condition number of the GN matrix of a $L$ -layer linear network can be upper-bounded in the following way: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq\\kappa(\\mathbf{Z})\\sum_{\\ell=1}^{L}\\gamma_{\\ell}\\kappa({\\mathbf{W}}^{L;\\ell+1})^{2}\\kappa({\\mathbf{W}}^{1;\\ell-1})^{2}\\leq\\kappa(\\mathbf{Z})\\operatorname*{max}_{1\\leq\\ell\\leq L}\\left\\{\\kappa({\\mathbf{W}}^{L;\\ell+1})^{2}\\kappa({\\mathbf{W}}^{1;\\ell-1})^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As mentioned earlier, we observe the same convex combination structure which, as we will soon see experimentally, is crucial to obtain a bound that works in practice. ", "page_idx": 5}, {"type": "text", "text": "Empirical validation. The empirical results in Figure 2a show that the derived bound seems to be tight and predictive of the trend of the condition number of GN at initialization. If the width of the hidden layer is held constant, the condition number grows with a quadratic trend. However, the condition number can be controlled if the width is scaled proportionally with the depth. This gives another explanation of why in practice the width of the network layers is scaled proportionally with the depth to enable faster network training. ", "page_idx": 5}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/3fc2ae4280a2503d6b9d3683ba2d0394d474963f22b595360c638af7eefbce67.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: a) Condition number at initialization under Kaiming normal initialization of GN $\\kappa(\\hat{\\mathbf{G}}_{O})$ and first upper bound derived in Lemma 2 and Eq.(7) for whitened MNIST as a function of depth $L$ for different hidden layer widths $m$ for a Linear Network over 3 initializations. b) Scaling the width of the hidden layer proportionally to the depth leads to slower growth of the condition number (left) or improves the condition number with depth if the scaling factor is chosen sufficiently large (right). ", "page_idx": 6}, {"type": "text", "text": "W(1) \u2463 w\u2192 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Figure 4: Adding skip connections between each layer for a general L-layer linear Neural Network. ", "page_idx": 6}, {"type": "text", "text": "Furthermore, Figure 3 empirically demonstrates the importance of bounding the condition number as a convex combination of the condition number of the weight matrices, as simply taking the maximum makes the bound vacuous. This is due to a \u2018self-balancing\u2019 behavior of the individual terms in Lemma 2, which is further elaborated in Appendix E. We show that the difference in magnitude between each condition number is largely driven by their smallest eigenvalue. Thus at the same time, they also have a smaller weight in the convex combination, making the overall condition number grow slower than the maximum bound would predict. ", "page_idx": 6}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/d2114b1492d0052ae498d2fdd22a6216d1ab2e2a43196e6fa34455ebe4c64119.jpg", "img_caption": ["Figure 3: Comparison of derived upper bounds in Lemma 2 for the condition number at initialization for whitened MNIST over 20 runs. Note the logarithmic scaling of the $y$ -axis. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 $L$ -layer linear residual networks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Following the examination of linear networks, our focus will now shift to the analysis of linear networks where we introduce a skip connection with weight $\\beta$ between each layer, as illustrated in Figure 4. This results in a residual network of the form $\\bar{F}_{\\theta}(\\mathbf{x})=(\\mathbf{W}^{(L)}+\\beta\\bar{\\mathbf{I}})\\cdot\\cdot\\cdot(\\mathbf{W}^{(1)}+\\beta\\mathbf{I})\\mathbf{x}$ . Given the architecture of the residual network, a substantial portion of the analysis conducted in Section 4.2 for the $L$ -layer linear neural network can be repurposed. ", "page_idx": 6}, {"type": "text", "text": "The key insight lies in the realization that when the skip connection precisely bypasses one layer, it implies a modification of Eq. (5) to $\\begin{array}{r}{\\mathbf{U}^{\\top}\\mathbf{U}=\\sum_{\\ell=1}^{L}\\mathbf{W}_{\\beta}^{L:\\ell+1}\\mathbf{W}_{\\beta}^{\\ell+1:L}\\otimes\\mathbf{W}_{\\beta}^{1:\\ell-1}\\mathbf{W}_{\\beta}^{\\ell-1:1}}\\end{array}$ , where we define $\\mathbf{W}_{\\beta}^{k:l}:=\\left(\\mathbf{W}^{k}+\\beta\\mathbf{I}\\right)\\cdot\\mathbf{\\cdot}\\cdot\\left(\\mathbf{W}^{l}+\\beta\\mathbf{I}\\right)$ i f $k>l$ and $\\mathbf{W}_{\\beta}^{k:l}:=(\\mathbf{W}^{k}+\\beta\\mathbf{I})^{\\top}\\cdot\\cdot\\cdot(\\mathbf{W}^{l}+\\beta\\mathbf{I})^{\\top}$ if $k<l$ . I denotes the rectangular identity matrix of appropriate dimensions. Note, that we can apply the upper bounds derived in Lemma 2 analogously to arrive at ", "page_idx": 6}, {"type": "text", "text": "$\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq\\kappa(\\mathbf{\\Sigma})\\sum_{\\ell=1}^{L}\\gamma_{\\ell}^{\\beta}\\,\\kappa^{2}(\\mathbf{W}_{\\beta}^{L;\\ell+1})\\,\\kappa^{2}(\\mathbf{W}_{\\beta}^{1;\\ell-1})\\leq\\kappa(\\mathbf{\\Sigma})\\,\\operatorname*{max}_{1\\leq\\ell\\leq L}\\kappa^{2}(\\mathbf{W}_{\\beta}^{L;\\ell+1})\\,\\kappa^{2}(\\mathbf{W}_{\\beta}^{1;\\ell-1})$ (7) where \u03b3\u2113\u03b2:= $\\begin{array}{r}{\\gamma_{\\ell}^{\\beta}:=\\frac{\\alpha_{\\ell}^{\\beta}}{\\sum_{i=1}^{L}\\alpha_{i}^{\\beta}}}\\end{array}$ \u03b2 and analogously to the L-layer case \u03b1\u2113\u03b2:= \u03c32min( $\\alpha_{\\ell}^{\\beta}:=\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W}_{\\beta}^{L:\\ell+1})\\cdot\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W}_{\\beta}^{1:\\ell-1})$ for $\\ell=1,\\ldots,\\overline{{L}}$ . ", "page_idx": 6}, {"type": "text", "text": "Let us now analyze how skip connections affect the condition number. Denoting the SVD of $\\mathbf{W}^{\\ell}=$ $\\mathbf{U}^{\\ell}\\mathbf{S}^{\\ell}\\mathbf{V}^{\\ell^{\\top}}$ note that $\\mathbf{W}^{\\ell}+\\beta\\mathbf{I}=\\mathbf{U}^{\\ell}(\\mathbf{S}^{\\ell}\\!+\\!\\beta\\mathbf{I})\\mathbf{V}^{\\ell^{T}}$ , and therefore $\\sigma_{i}(\\mathbf{W}^{\\ell}+\\beta\\mathbf{I})=\\sigma_{i}(\\mathbf{W}^{\\ell})+\\beta$ for all singular values of $\\mathbf{W}^{\\ell}$ . Incorporating skip connections results in a spectral shift of each weight matrix to the positive direction by a magnitude of $\\beta$ . Furthermore, we will assume that the left and right singular vectors of the layers in-between coincide, that is $\\mathbf{V}^{\\ell}=\\mathbf{U}^{\\ell-1}$ . This assumption is fulfilled if the initialization values are sufficiently close to zero [Saxe et al., 2013]. Then for each $\\ell$ we have ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\kappa^{2}(\\mathbf{W}_{\\beta}^{L:\\ell+1})\\kappa^{2}(\\mathbf{W}_{\\beta}^{1:\\ell-1})=\\displaystyle\\frac{\\prod_{i=\\ell+1}^{L}(\\sigma_{\\operatorname*{max}}(\\mathbf{W}^{i})+\\beta)^{2}\\prod_{i=1}^{\\ell-1}(\\sigma_{\\operatorname*{max}}(\\mathbf{W}^{i})+\\beta)^{2}}{\\prod_{i=\\ell+1}^{L}(\\sigma_{\\operatorname*{min}}(\\mathbf{W}^{i})+\\beta)^{2}\\prod_{i=1}^{\\ell-1}(\\sigma_{\\operatorname*{min}}(\\mathbf{W}^{i})+\\beta)^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\prod_{i\\neq\\ell}\\left(\\frac{\\sigma_{\\operatorname*{max}}(\\mathbf{W}^{i})+\\beta}{\\sigma_{\\operatorname*{min}}(\\mathbf{W}^{i})+\\beta}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{\\sigma_{\\operatorname*{max}}(\\mathbf{W}^{i})+\\beta}{\\sigma_{\\operatorname*{min}}(\\mathbf{W}^{i})+\\beta}<\\frac{\\sigma_{\\operatorname*{max}}(\\mathbf{W}^{i})}{\\sigma_{\\operatorname*{min}}(\\mathbf{W}^{i})}}\\end{array}$ for all $\\beta>0$ , we can conclude that the second upper bound in Eq. (7) for residual networks is smaller than the second upper bound for linear networks in Lemma 2. We verify that this observation also holds for the actual condition number and the tight upper bound in Eq. (7) as can be seen in Figure 5. The latter shows that adding skip connections improves the conditioning of the network. Furthermore, Figure 20 in the appendix shows that the condition number is improved more for larger $\\beta$ because this pushes the condition number towards one as can be seen from Eq. (8). To conclude, the analysis of the condition number provides a rigorous explanation for the popularity of residual networks compared to fully connected networks. This is in line with existing empirical results in the literature [He et al., 2016, Li et al., 2018, Liu et al., 2019]. ", "page_idx": 7}, {"type": "text", "text": "Remark: extension to convolutional layers. Note that the analysis on fully connected layers can also be extended to convolutional layers in a straightforward way by making use of the fact that the convolution operation can be reformulated in the form of a matrix-vector product using Toeplitz matrices, as discussed for instance in Singh et al. [2023]. In this case, we can apply the same analysis as for fully connected networks. See Appendix B for more details. ", "page_idx": 7}, {"type": "text", "text": "5 Extension to non-linear activations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "After the analysis of networks with a linear activation in the previous section, we will extend our results to non-linear activation functions $\\sigma$ that satisfy $\\sigma(z)=\\bar{\\sigma}^{\\prime}(z)z$ for one-hidden layer networks $F_{\\theta}(\\mathbf{x})=\\mathbf{W}\\sigma(\\mathbf{V}\\mathbf{x})$ . Specifically, this extension includes all piece-wise linear activation functions such as ReLU or Leaky ReLU. For this purpose, we first rewrite the network output as a super-position of unit-networks $F_{\\theta_{i}}(\\mathbf{x})$ over the number of hidden neurons $m$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\nF_{\\theta}(\\mathbf{x})=\\sum_{i=1}^{m}F_{\\theta_{i}}(\\mathbf{x})=\\sum_{i=1}^{m}\\mathbf{W_{\\bullet,i}}\\sigma(\\mathbf{V}_{i,\\bullet}\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathbf{W}_{\\bullet,i}$ denotes the $i$ -th column of $\\mathbf{W}$ and $\\mathbf{V}_{i.}$ ,\u2022 denotes the $i$ -th row of $\\mathbf{V}$ . Using the following lemma, we can then derive an expression for the Gauss-Newton matrix. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3. (Lemma 25 in Singh et al. [2021]) Let $F_{\\theta_{i}}(\\mathbf{x})\\,=\\,\\mathbf{W_{\\bullet,}}\\sigma(\\mathbf{V}_{i,\\bullet}\\mathbf{x})$ be a unit-network corresponding to $i$ -th neuron, with the non-linearity $\\sigma$ such that $\\sigma(z)=\\sigma^{\\prime}(z)z$ . Let $\\mathbf{X}\\in\\mathbb{R}^{d\\times n}$ denote the data matrix. Further, let $\\mathbf{A}^{i}\\in\\mathbb{R}^{n\\times n}$ be defined as $(\\mathbf{A}^{i})_{j j}=\\sigma^{\\prime}(\\mathbf{V}_{i,\\bullet}\\mathbf{\\dot{x}})_{j}$ , for $j=1,\\dots,n$ and zero elsewhere. Then the Jacobian matrix $\\nabla_{\\theta}F_{\\theta_{i}}(\\mathbf{X})$ is given (in transposed form) by: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}F_{\\theta_{i}}(\\mathbf{X})=\\left(\\mathbf{0}\\quad\\mathbf{X}\\mathbf{A}^{i}\\otimes\\mathbf{W}_{\\bullet\\,i}^{\\top}\\quad\\mathbf{V}_{i\\,\\bullet}\\mathbf{X}\\mathbf{A}^{i}\\otimes\\mathbf{I}_{k}\\quad\\mathbf{0}\\right)^{\\top}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The Jacobian matrix of the full network is simply the sum over all units $\\begin{array}{r l}{\\nabla_{\\theta}F_{\\theta}(\\mathbf{X})}&{{}=}\\end{array}$ $\\textstyle\\sum_{i=1}^{m}\\nabla_{\\theta}F_{\\theta_{i}}(\\mathbf{X})$ . Now that we have an expression for the Jacobian matrix, we can compute the GN matrix as $\\mathbf{G}_{O}\\ =\\ \\nabla F_{\\theta}(\\mathbf{X})\\nabla F_{\\theta}(\\mathbf{X})^{\\top}$ . Note that the non-zero eigenvalue of $\\mathbf{G}_{O}$ and $\\widehat{\\mathbf{G}}_{O}:=\\nabla F_{\\theta}(\\mathbf{X})^{\\top}\\nabla F_{\\theta}(\\mathbf{X})\\in\\mathbb{R}^{k n\\times k n}$ are the same, for which we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{G}}_{O}=\\sum_{i=1}^{m}\\nabla F_{\\theta_{i}}(\\mathbf{X})^{\\top}\\nabla F_{\\theta_{i}}(\\mathbf{X})=\\sum_{i=1}^{m}\\widehat{\\mathbf{G}}_{O}^{i}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with $\\widehat{\\mathbf{G}}_{O}^{i}\\;:=\\;\\Lambda^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\Lambda^{i}\\otimes\\mathbf{W}_{\\bullet i}\\mathbf{W}_{\\bullet i}^{\\top}+\\Lambda^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i\\bullet}^{\\top}\\mathbf{V}_{i\\bullet}\\mathbf{X}\\Lambda^{i}\\otimes\\mathbf{I}_{k}\\;\\in\\;\\mathbb{R}^{k n\\times k n}$ , since the mixed term s $\\bar{\\nabla}F_{\\theta_{i}}(\\mathbf{X})^{\\top}\\nabla F_{\\theta_{j}}(\\mathbf{X})$ with $i\\neq j$ are zero due to the block structure of the Jacobian of the unit-networks. Using the mixed product property of the Kronecker product we obtain ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbf{G}}_{O}=\\sum_{i=1}^{m}\\mathbf{\\Lambda}\\mathbf{\\Lambda}^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{\\Lambda}^{i}\\otimes\\mathbf{W}_{\\bullet i}\\mathbf{W}_{\\bullet i}^{\\top}+\\sum_{i=1}^{m}\\mathbf{\\Lambda}\\mathbf{\\Lambda}^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i\\bullet}^{\\top}\\mathbf{V}_{i\\bullet}\\mathbf{X}\\mathbf{\\Lambda}^{i}\\otimes\\mathbf{I}_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/d379ebe566eaf5efe9db7ac2d9f73ee0317332580eb45f8e240968eeb7336c87.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Comparison of condition number of GN $\\kappa(\\hat{\\mathbf{G}}_{O})$ between Linear Network and Residual Network with $\\beta=1$ for whitened MNIST (left) and whitened Cifar-10 (right) at initialization using Kaiming normal initialization over three seeds. The upper bounds refer to the first upper bound in Lemma 2 and Eq. (7), respectively. ", "page_idx": 8}, {"type": "text", "text": "To improve readability, we will write $\\begin{array}{r}{\\mathbf{T}:=\\sum_{i=1}^{m}\\mathbf{A}^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i\\bullet}^{\\top}\\mathbf{V}_{i\\bullet}\\mathbf{X}\\mathbf{A}^{i}}\\end{array}$ hereafter. ", "page_idx": 8}, {"type": "text", "text": "Leaky ReLU activation. Let\u2019s now consider the case where the non-linear activation $\\sigma$ is LeakyReLU $\\mathbf{\\Phi}^{\\cdot}(x)=\\operatorname*{max}\\{\\alpha x,x\\}$ for some constant $\\alpha\\in[0,1)$ . Typically $\\alpha$ is chosen to be close to zero, e.g. $\\alpha=0.01$ . Again, the condition number $\\kappa(\\widehat{\\mathbf{G}}_{O})$ can be upper bounded by bounding the extreme eigenvalues independently. By observing tha t  all terms in Eq. (10) are symmetric, we can again apply the Weyl\u2019s and dual Weyl\u2019s inequality. However to get a lower bound larger than zero for $\\lambda_{\\operatorname*{min}}(\\widehat{\\mathbf{G}}_{O})$ a different approach is needed as $\\mathbf{W}_{\\bullet,i}\\mathbf{W}_{\\bullet,i}^{\\top}$ is rank-deficient and the same steps to bound the sm a llest eigenvalue would lead to a vacuous value of zero. Instead, we will use the following observation that we can bound the extreme eigenvalues of the sum of a Kronecker product of PSD matrices $\\mathbf{A}_{i},\\mathbf{B}_{i}$ for $i=1,\\hdots,m$ by $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}\\left(\\sum_{i=1}^{m^{\\circ}}(\\mathbf{A}_{i}\\otimes\\mathbf{B}_{i})\\right)\\geq\\operatorname*{min}_{1\\leq j\\leq m}\\lambda_{\\operatorname*{min}}(\\mathbf{A}_{j}^{\\star})\\cdot\\,\\lambda_{\\operatorname*{min}}\\left(\\sum_{i=1}^{m}\\mathbf{B}_{i}\\right)\\!.}\\end{array}$ . ", "page_idx": 8}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/d8aeccb4a3ae7f04eec60fef07d4ec045b36814b36e58bdc119193ba49a7b71e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "By first using Weyl\u2019s inequalities and subsequently applying the above bound with $\\mathbf{A}_{i}\\ =\\ \\mathbf{\\Lambda}\\mathbf{\\Lambda}^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{\\Lambda}^{i}\\mathbf{\\Lambda}$ and $\\mathbf{\\dot{B}}_{i}^{\\cdot}\\,=\\,\\mathbf{W}_{\\bullet,i}\\mathbf{W}_{\\bullet,i}^{\\top}$ we achieve the following bound for the condition number for $\\kappa(\\widehat{\\mathbf{G}}_{O})$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 4. Consider an on e -hidden layer network with a Leaky-ReLU activation with negative slope $\\alpha\\in[0,1]$ . Then the condition number of the GN matrix defined in Equation (10) can be bounded as: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq\\frac{\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{X})\\,\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{W})+\\lambda_{\\operatorname*{max}}\\,(\\mathbf{Gamma})}{\\alpha^{2}\\,\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{X})\\,\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W})+\\lambda_{\\operatorname*{min}}\\,(\\mathbf{\\Gamma})}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Figure 6: Condition number of a one-hidden layer Leaky ReLU network with $\\alpha~=~0.01$ and upper bounds for whitened MNIST over $n~=~500$ data points. The upper bound refers to Eq. (11). Note that the y-axis is log scaled. ", "page_idx": 8}, {"type": "text", "text": "The proof can be found in Appendix H. We further empirically validate the tightness of the upper bounds on a subset of the MNIST dataset $n=500)$ ), where we chose $\\alpha=0.01$ , which is the default value in Pytorch [Paszke et al., 2019]. As can be seen in Figure 6, contrary to the linear setting the condition number increases with width. The upper bound also becomes tighter and more predictive with increasing width. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Comparison to linear network. By running the same setup for varying values of $\\alpha\\in[0,1]$ we can interpolate between ReLU $(\\alpha=0)$ ) and linear activation $\\left\\langle\\alpha\\right.=1$ ) to see how the introduction of the non-linearity affects the conditioning. In Figure 23, which had to be deferred to the appendix due to space constraints, we can see how reducing the value of $\\alpha$ seems to consistently improve the conditioning of the GN matrix for the one-hidden layer case. ", "page_idx": 8}, {"type": "text", "text": "6 Conditioning under batch normalization ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Based on the observation that conditioning of the input data affects the conditioning of the GN spectrum, we also tested whether Batch normalization (BN) [Ioffe and Szegedy, 2015], which is a very commonly used normalization scheme, has a similar effect on the condition number. For this, the condition number of the GN matrix was calculated for a one-hidden layer linear network with and without a Batch normalization layer. The experiment was run on a downsampled and subsampled version of Cifar10, which was converted to grayscale with $d=64$ and $n=1000$ . The data was not whitened to see the effect of BN more strongly. As can be seen in Figure 7, we indeed observe a clear improvement of the condition number when Batch normalization layers are added. Also, the trend of improved conditioning with increasing width remains after adding BN. ", "page_idx": 9}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/a52cb78eb7a31375cce8b2e7f110f498fe64b0a2085ea924702bf65fc8a0dbc6.jpg", "img_caption": ["Figure 7: Comparison of conditioning of the GN matrix for a one-hidden layer linear network with and without Batch normalization on downsampled Cifar-10 data $(d=64,n=$ 1000) at initialization over 5 runs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Discussion and conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary. In this work, we derived new analytical bounds for the condition number of neural networks, showcasing the following key findings: a) The conditioning of the input data has a nearly proportional impact on the conditioning of the GN spectra, underscoring the significance of data normalization. Empirical evidence further demonstrates that Batch Normalization similarly enhances the condition number. b) For linear networks, we showed that the condition number grows quadratically with depth for fixed hidden width. Also, widening hidden layers improves conditioning, and scaling the hidden width proportionally with depth can compensate for the growth. c) We showed how adding residual connections improves the condition number, which also explains how they enable the training of very deep networks. d) Preliminary experiments suggest that the ReLU activation seems to improve the conditioning compared to linear networks in the one-hidden layer case. ", "page_idx": 9}, {"type": "text", "text": "Interesting use cases of our results. Through our analysis, we highlighted that the condition number as a tool from classical optimization is also an attractive option to better understand challenges in neural network training with gradient-based methods. Especially, knowing how different architectural choices will affect the optimization landscape provides a more principled way to design the network architecture for practitioners, for instance how to scale the width in relation to the depth of the network. The paper also gives a justification of why pruned networks are more difficult to train as they have worse conditioning. Although this is not our focus, it is possible that our analysis could inspire better techniques for pruning neural networks. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work. We made a first step toward understanding the impact of different architectural choices on the conditioning of the optimization landscape. However, there are still many design choices, that have not been covered yet, such as an extension to non-linear networks for arbitrary layers, other architectures, such as transformers, and analytic bounds for normalization schemes, such as batch or layer normalization, which we will leave to future work. Another limitation of our current work is that the derived upper bounds are agnostic to the training dynamics. Therefore, they cannot distinguish the difference between random initializations and solutions of deep learning models after convergence. Incorporating training dynamics into the upper bounds would allow us to characterize solutions found by different architectures, which is left for future work. Another future direction is to extend the analysis to the Generalized Gauss-Newton matrix, which is particularly relevant for training with cross-entropy loss. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Aurelien Lucchi acknowledges the financial support of the Swiss National Foundation, SNF grant No 223031. Sidak Pal Singh would like to acknowledge the financial support of Max Planck ETH Center for Learning Systems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature, 2020.   \nVineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning, pages 1842\u20131850. PMLR, 2018.   \nHong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training, 2023.   \nYizhang Lou, Chris E Mingard, and Soufiane Hayou. Feature learning and signal propagation in deep neural networks. In International Conference on Machine Learning, pages 14248\u201314282. PMLR, 2022.   \nUtku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difficulty of training sparse neural networks. arXiv preprint arXiv:1906.10732, 2019.   \nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448\u2013456. pmlr, 2015.   \nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. Advances in neural information processing systems, 29, 2016.   \nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.   \nYuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3\u201319, 2018.   \nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.   \nGuoliang Kang, Jun Li, and Dacheng Tao. Shakeout: A new regularized deep neural network training scheme. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.   \nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International conference on machine learning, pages 1058\u20131066. PMLR, 2013.   \nSidak Pal Singh, Gregor Bachmann, and Thomas Hofmann. Analytic insights into structure and rank of neural network hessian maps. Advances in Neural Information Processing Systems, 34, 2021.   \nVladimir Vapnik. Principles of risk minimization for learning theory. Advances in neural information processing systems, 4, 1991.   \nNicol N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14:1723\u20131738, 2002.   \nLevent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.   \nJames Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1\u201376, 2020.   \nAleksandar Botev. The Gauss-Newton matrix for Deep Learning models and its applications. PhD thesis, UCL (University College London), 2020.   \nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. Advances in neural information processing systems, 32, 2019. ", "page_idx": 10}, {"type": "text", "text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural computation, 9(1):1\u201342, 1997. ", "page_idx": 11}, {"type": "text", "text": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.   \nGintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data, 2017.   \nPratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124018, 2019.   \nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pages 1019\u20131028. PMLR, 2017.   \nDiego Granziol. Flatness is a false friend, 2021. URL https://openreview.net/forum?id $=$ I6-3mg29P6y.   \nMaksym Andriushchenko, Francesco Croce, Maximilian M\u00fcller, Matthias Hein, and Nicolas Flammarion. A modern look at the relationship between sharpness and generalization. arXiv preprint arXiv:2302.07011, 2023.   \nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization, 2021.   \nZhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based analysis of large batch training and robustness to adversaries. Advances in Neural Information Processing Systems, 31, 2018.   \nAitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218, 2020.   \nBehrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via hessian eigenvalue density, 2019.   \nZhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE international conference on big data (Big data), pages 581\u2013590. IEEE, 2020.   \nJeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. arXiv preprint arXiv:2103.00065, 2021.   \nDaniel Beaglehole, Peter S\u00faken\u00edk, Marco Mondelli, and Mikhail Belkin. Average gradient outer product as a mechanism for deep neural collapse. arXiv preprint arXiv:2402.13728, 2024.   \nZhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael W. Mahoney. Adahessian: An adaptive second order optimizer for machine learning, 2021.   \nWu Lin, Felix Dangel, Runa Eschenhagen, Kirill Neklyudov, Agustinus Kristiadi, Richard E Turner, and Alireza Makhzani. Structured inverse-free natural gradient: Memory-efficient & numericallystable kfac for large neural nets. arXiv preprint arXiv:2312.05705, 2023.   \nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989.   \nBabak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network pruning. In IEEE international conference on neural networks, pages 293\u2013299. IEEE, 1993.   \nSidak Pal Singh and Dan Alistarh. Woodfisher: Efficient second-order approximation for neural network compression. Advances in Neural Information Processing Systems, 33:18098\u201318109, 2020.   \nZhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision, 2019.   \nElias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023.   \nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions, 2020.   \nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile\u02d9 Luko\u0161iu\u00afte\u02d9, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying large language model generalization with influence functions, 2023.   \nJeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random matrix theory. In International conference on machine learning, pages 2798\u20132806. PMLR, 2017.   \nArthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. The asymptotic spectrum of the hessian of dnn throughout training. arXiv preprint arXiv:1910.02875, 2019.   \nLevent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singularity and beyond. arXiv preprint arXiv:1611.07476, 2016.   \nSidak Pal Singh, Thomas Hofmann, and Bernhard Sch\u00f6lkopf. The hessian perspective into the nature of convolutional neural networks. arXiv preprint arXiv:2305.09088, 2023.   \nYikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, and Rong Ge. Dissecting hessian: Understanding common structure of hessian in neural networks. arXiv preprint arXiv:2010.04261, 2020.   \nZhenyu Liao and Michael W Mahoney. Hessian eigenspectra of more realistic nonlinear models. Advances in Neural Information Processing Systems, 34:20104\u201320117, 2021.   \nAndrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.   \nWei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Learning Representations, 2020.   \nMikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849\u201315854, July 2019. ISSN 1091-6490. doi: 10.1073/pnas.1903070116.   \nSidak Pal Singh, Aurelien Lucchi, Thomas Hofmann, and Bernhard Sch\u00f6lkopf. Phenomenology of double descent in finite-width neural networks. In International Conference on Learning Representations, 2022.   \nArthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \nSifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. Journal of Computational Physics, 449:110768, 2022.   \nGreg Yang. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint arXiv:2006.14548, 2020.   \nQuynh Nguyen, Marco Mondelli, and Guido Montufar. Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks, 2022.   \nChaoyue Liu and Like Hui. Relu soothes the ntk condition number and accelerates optimization for wide neural networks. arXiv preprint arXiv:2305.08813, 2023.   \nYi Ren and Donald Goldfarb. Efficient subsampled gauss-newton and natural gradient methods for training neural networks. arXiv preprint arXiv:1906.02353, 2019.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018.   \nTianyi Liu, Minshuo Chen, Mo Zhou, Simon S Du, Enlu Zhou, and Tuo Zhao. Towards understanding the importance of shortcut connections in residual networks. Advances in neural information processing systems, 32, 2019.   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \nHermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen, 71(4):441\u2013479, 1912.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \nRoman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.   \nJack W Silverstein. The smallest eigenvalue of a large dimensional wishart matrix. The Annals of Probability, pages 1364\u20131368, 1985.   \nYann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Standard pertubation bounds for matrix eigenvalues ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Weyl\u2019s inequality and dual Weyl\u2019s inequality ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we first briefly review the Weyl\u2019s inequality and its dual form. We then demonstrate how these inequalities can be used to bound the extreme eigenvalues and thus the condition number of the sum of two Hermitian matrices $\\mathbf{A},\\mathbf{B}$ by the extreme eigenvalues of $\\mathbf{A}$ and $\\mathbf{B}$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Weyl [1912] Let $\\mathbf{1},\\mathbf{B}\\in\\mathbb{R}^{n\\times n}$ be two Hermitian matrices, and let $\\lambda_{1}\\,\\geq\\,.\\,.\\,\\geq\\,\\lambda_{n}$ denote the ordered eigenvalues, then the Weyl\u2019s inequality and dual Weyl\u2019s inequality state that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{i+j-1}(\\mathbf{A}+\\mathbf{B})\\leq\\lambda_{i}(\\mathbf{A})+\\lambda_{j}(\\mathbf{B}),\\ i,j\\geq1,i+j-1\\leq n}\\\\ &{\\lambda_{i+j-n}(\\mathbf{A}+\\mathbf{B})\\geq\\lambda_{i}(\\mathbf{A})+\\lambda_{j}(\\mathbf{B}),\\ i,j\\geq1,i+j-n\\leq n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the following, we derive an upper and lower bound of the condition number of a symmetric matrix $\\mathbf{A}$ , for which we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\kappa(\\mathbf{A})=\\frac{|\\lambda_{\\operatorname*{max}}(\\mathbf{A})|}{|\\lambda_{\\operatorname*{min}}(\\mathbf{A})|}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall that given a Hermitian $n\\times n$ matrix $\\mathbf{A}$ , we can diagonalize it by the spectral theorem. We obtain the following sequence of real eigenvalues ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{1}(\\mathbf{A})\\geq\\lambda_{2}(\\mathbf{A})\\geq...\\geq\\lambda_{n}(\\mathbf{A}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We will make use of the Weyl\u2019s inequality and the dual Weyl\u2019s inequality to bound the eigenvalues of the sum of two Hermitian matrices $\\mathbf{A}+\\mathbf{B}$ . ", "page_idx": 14}, {"type": "text", "text": "From Lemma 5, we obtain the following inequalities: From Weyl\u2019s inequality we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{1}(\\mathbf{A}+\\mathbf{B})\\leq\\lambda_{1}(\\mathbf{A})+\\lambda_{1}(\\mathbf{B})}\\\\ &{\\lambda_{n}(\\mathbf{A}+\\mathbf{B})\\leq\\lambda_{n-j}(\\mathbf{A})+\\lambda_{1+j}(\\mathbf{B}),\\quad\\mathrm{for}\\;0\\leq j\\leq n-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From the dual Weyl\u2019s inequality we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{1}(\\mathbf{A}+\\mathbf{B})\\geq\\lambda_{n-j}(\\mathbf{A})+\\lambda_{1+j}(\\mathbf{B}),\\quad\\mathrm{for}\\;0\\leq j\\leq n-1}\\\\ &{\\lambda_{n}(\\mathbf{A}+\\mathbf{B})\\geq\\lambda_{n}(\\mathbf{A})+\\lambda_{n}(\\mathbf{B}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, we can find a lower and upper bound for the condition number of $\\mathbf{A}+\\mathbf{B}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{|\\operatorname*{max}_{0\\le j\\le n-1}(\\lambda_{n-j}(\\mathbf{A})+\\lambda_{1+j}(\\mathbf{B}))|}{|\\operatorname*{min}_{0\\le j\\le n-1}(\\lambda_{n-j}(\\mathbf{A})+\\lambda_{1+j}(\\mathbf{B}))|}\\le\\kappa(\\mathbf{A}+\\mathbf{B})\\le\\frac{|\\lambda_{\\operatorname*{max}}(\\mathbf{A})+\\lambda_{\\operatorname*{max}}(\\mathbf{B})|}{|\\lambda_{\\operatorname*{min}}(\\mathbf{A})+\\lambda_{\\operatorname*{min}}(\\mathbf{B})|}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Further bounds for extreme eigenvalues for positive semidefinite matrices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For square matrices $\\mathbf{A}\\,\\in\\,\\mathbb{R}^{p\\times p},\\mathbf{B}\\,\\in\\,\\mathbb{R}^{q\\times q}$ , denote by $\\lambda_{1},\\ldots,\\lambda_{p}$ the eigenvalues of $\\mathbf{A}$ and by $\\mu_{1},\\ldots,\\mu_{q}$ the eigenvalues of $\\mathbf{B}$ listed by multiplicity. Then the eigenspectrum of $\\mathbf{A}\\otimes\\mathbf{B}\\in\\mathbb{R}^{p q\\times p\\dot{q}}$ consists of the eigenvalues ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{i}\\mu_{j}\\quad i=1,\\ldots,p,\\,j=1,\\ldots,q.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that if $\\mathbf{A},\\mathbf{B}$ are additionally positive semidefinite (PSD), we have the following equality for the largest and smallest eigenvalues of $\\mathbf{A}\\otimes\\mathbf{B}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(\\mathbf{A}\\otimes\\mathbf{B})=\\lambda_{\\operatorname*{max}}(\\mathbf{A})\\lambda_{\\operatorname*{max}}(\\mathbf{B})}\\\\ {\\lambda_{\\operatorname*{min}}(\\mathbf{A}\\otimes\\mathbf{B})=\\lambda_{\\operatorname*{min}}(\\mathbf{A})\\lambda_{\\operatorname*{min}}(\\mathbf{B}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, it follows from the sub-multiplicativity of the matrix norm that for square, PSD matrices $\\mathbf{A},\\mathbf{B}$ of same dimensions it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(\\mathbf{A}\\mathbf{B})\\leq\\lambda_{\\operatorname*{max}}(\\mathbf{A})\\cdot\\lambda_{\\operatorname*{max}}(\\mathbf{B})}\\\\ {\\lambda_{\\operatorname*{min}}(\\mathbf{A}\\mathbf{B})\\geq\\lambda_{\\operatorname*{min}}(\\mathbf{A})\\cdot\\lambda_{\\operatorname*{min}}(\\mathbf{B}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Extension of analysis to convolutional layers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The analysis on fully connected layers can be extended to convolutional layers in a straight forward way, by using the fact that the convolution operation can be reformulated in the form of a matrixvector product using Toeplitz matrices, as discussed for instance in Singh et al. [2023]. This would mean that we can apply the same analysis as for fully connected networks. For completeness we will discuss how below how the convolution operation can be rewritten as a matrix-vector product. For further details, the readers is referred to Singh et al. [2023]. ", "page_idx": 15}, {"type": "text", "text": "Consider a convolution operation $\\mathbf{W}*\\mathbf{x}$ of an input $\\mathbf{x}\\in\\mathbb{R}^{d}$ with $m$ fliters of size $k\\leq d$ , which are organized in the matrix $\\bar{\\mathbf{W}}\\in\\mathbb{R}^{m\\times k}$ . For simplicity a stride of 1 and zero padding is assumed. Let $\\mathbf{z}_{j:j+k-1}\\in R^{k}$ denote the vector formed by considering the indices $j$ to $j+k-1$ of the original vector $\\mathbf{z}\\in\\mathbb{R}^{d}$ . In this case, the convolution operation can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{W}*\\mathbf{x}=\\left(\\begin{array}{c c c}{\\left\\langle\\mathbf{W}_{1\\bullet},\\mathbf{x}_{1:k}\\right\\rangle}&{\\ldots}&{\\left\\langle\\mathbf{W}_{1\\bullet},\\mathbf{x}_{d-k+1:d}\\right\\rangle}\\\\ {\\vdots}&&{\\vdots}\\\\ {\\left\\langle\\mathbf{W}_{m\\bullet},\\mathbf{x}_{1:k}\\right\\rangle}&{\\ldots}&{\\left\\langle\\mathbf{W}_{m\\bullet},\\mathbf{x}_{d-k+1:d}\\right\\rangle}\\end{array}\\right)\\in\\mathbb{R}^{m\\times(d-k+1)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{W}_{i}$ \u2022 denotes the $i^{\\th}$ -th row of $\\mathbf{W}$ . ", "page_idx": 15}, {"type": "text", "text": "Let us further introduce Toeplitz matrices, $\\left\\{\\mathbf{T}^{\\mathbf{W}_{i\\bullet}}\\right\\}_{i=1}^{m}$ , for each fliter with $\\mathbf{T}^{\\mathbf{W}_{i\\bullet}}:=\\mathrm{toep}(\\mathbf{W}_{i\\bullet},d)\\in$ $\\mathbb{R}^{(d-k+1)\\times d}$ such that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{T}^{\\mathbf{W}_{i\\bullet}}=\\left(\\begin{array}{c c c c c c}{\\mathbf{W}i1}&{\\mathbf{\\Lambda}\\cdot\\cdot\\cdot}&{\\mathbf{W}_{i k}}&{0}&{\\cdots}&{0}\\\\ {0}&{\\mathbf{W}_{i1}}&{\\mathbf{\\Lambda}\\cdot\\cdot}&{\\mathbf{W}_{i k}}&{0}&{\\vdots}\\\\ {\\vdots}&{0}&{\\ddots}&{\\mathbf{\\ddots}}&{\\ddots}&{0}\\\\ {0}&{\\dots}&{0}&{\\mathbf{W}_{i1}}&{\\mathbf{\\Lambda}\\cdot\\cdot}&{\\mathbf{W}_{i k}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, by stacking the Toeplitz matrices in a row-wise fashion, that is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{T}^{\\mathbf{W}}:=\\left(\\!\\!\\begin{array}{l}{\\mathbf{T}^{\\mathbf{W}_{1\\bullet}}}\\\\ {\\quad\\cdots\\!\\!}\\\\ {\\mathbf{T}^{\\mathbf{W}_{m\\bullet}}\\!\\!}\\end{array}\\!\\!\\right)\\in\\mathbb{R}^{m(d-k+1)\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we see that the matrix multiplication of $\\mathbf{T}^{\\mathbf{W}}$ with an input $\\mathbf{x}$ gives the same result as vectorizing the convolution operation row-wise, i.e. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{vec}_{r}(\\mathbf{W}*\\mathbf{x})=\\mathbf{T}^{\\mathbf{W}}\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For a linear network with $L$ hidden layers, each of which is a convolutional kernel, the network function can be formally represented as ", "page_idx": 15}, {"type": "equation", "text": "$$\nF_{\\theta}({\\bf x})={\\mathcal W}^{(L+1)}*{\\mathcal W}^{(L)}*\\ldots*{\\mathcal W}^{(1)}*{\\bf x},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the parameters of each hidden layer $\\ell$ are denoted by $\\mathcal{W}^{(\\ell)}\\in\\mathbb{R}^{m_{\\ell}\\times m_{\\ell-1}\\times k_{l}}$ . $m_{\\ell}$ is the number of output channels, $m_{\\ell-1}$ the number of input channels, and $k_{\\ell}$ the kernel size. As we assumed $\\mathbf{x}$ to be one-dimensional, we have $m_{0}=1$ . ", "page_idx": 15}, {"type": "text", "text": "Similar to a single convolutional operation, the output of a single convolutional layer can be expressed via Toeplitz matrices. The Toeplitz matrix associated with the $\\ell$ -th convolutional layer $\\mathscr{W}^{(\\ell)}$ can be expressed by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{T}^{(\\ell)}:=\\left(\\begin{array}{l l l l}{\\mathbf{T}^{\\mathcal{W}_{(1,1)\\bullet}^{(\\ell)}}}&{\\dots}&{\\mathbf{T}^{\\mathcal{W}_{(1,m_{\\ell-1})\\bullet}^{(\\ell)}}}\\\\ {\\vdots}&&{}&{}\\\\ {\\mathbf{T}^{\\mathcal{W}_{(m_{\\ell},1)\\bullet}^{(\\ell)}}}&{\\dots}&{\\mathbf{T}^{\\mathcal{W}_{(m_{\\ell},m_{\\ell-1})\\bullet}^{(\\ell)}}}\\end{array}\\right)\\in\\mathbb{R}^{m_{\\ell}d_{\\ell}\\times m_{\\ell-1}d_{\\ell-1}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where W(\u2113) $\\boldsymbol{\\mathcal{W}}_{(i,j)\\bullet}^{(\\ell)}\\in\\mathbb{R}^{k_{\\ell}}$ refers to the $(i,j)$ -th fibre of $\\mathscr{W}^{(\\ell)}$ and $\\mathbf{T}^{\\mathcal{W}_{(i,j)\\bullet}^{(\\ell)}}\\,:=\\,\\mathrm{toep}(\\mathcal{W}_{(i,j)\\bullet}^{(\\ell)},d_{\\ell-1})\\,\\in$ $\\mathbb{R}^{d_{\\ell}\\times d_{\\ell-1}}$ to itsassociated Toeplitz matrix. Now, the network function can be equivalently written as ", "page_idx": 15}, {"type": "equation", "text": "$$\nF_{\\theta}(\\mathbf{x})=\\mathbf{T}^{(L+1)}\\mathbf{T}^{(L)}\\dots\\mathbf{T}^{(1)}\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "MNIST whitened, $\\mathtt{n=}1000$ , linear CNN ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/a8a9b91ae36ae7f7c82e7e7f80d84c688b40793a4e452d9cc0c6702d82733b9f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 8: Experiments on the condition number of the Gauss Newton matrix at initialization for a linear two-layer CNN at initialization on a random subsample of $\\scriptstyle{\\mathtt{n}=1000}$ of MNIST, whitened, with varying kernel size and number of filters. We can see a trend where the number of filters increases the condition number (in analogy to depth in MLPs) and the kernel size improves conditioning (in analogy to width in MLPs). ", "page_idx": 16}, {"type": "text", "text": "We further provide experiments on linear CNNs in Figure 8 at initialization showcasing that we can observe similar trends where the number of filters increases the condition number of the Gauss Newton matrix (in analogy to depth in MLPs) and the kernel size improves conditioning (in analogy to width in MLPs). ", "page_idx": 16}, {"type": "text", "text": "C Sensitivity of condition number on choice of smallest eigenvalue ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This work considers the pseudo condition number of some matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ , which is the ratio of the largest eigenvalue over the smallest non-zero eigenvalue nz,min \u03bb\u03bbmax((AA)). If A is of full rank, the pseudo condition number is equivalent to the condition number. However, if $\\mathbf{A}$ is rank deficient, it has to be typically estimated numerically if the rank is not known analytically. Based on the numerical estimation of the matrix rank, the smallest eigenvalue is chosen accordingly and the pseudo condition number is calculated. ", "page_idx": 16}, {"type": "text", "text": "In this section we evaluate the sensitivity of choice of the smallest eigenvalue on the resulting pseudo condition number. This is illustrated in the example of a 1-hidden layer network with ReLU activation function and a hidden width of $m=20$ at initialization and after 100 epochs of training. The details of this experimental setup are further specified in Appendix I.2. ", "page_idx": 16}, {"type": "text", "text": "In Figure 9 we can see that depending on the eigenvalue distribution, the condition number can be quite robust (Epoch $=\\!0$ ), but also very sensitive (Epoch $=100$ ) to the numerical matrix rank estimation. ", "page_idx": 16}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/fd68730194949d4c0d61115b9ef159aaa7be38688b69ba2010f4769dbe975c54.jpg", "img_caption": ["Figure 9: The spectrum of the GN matrix ordered by magnitude at initialization and after 100 epochs. The star marks the smallest non-zero eigenvalue, which is determined by the computed matrix rank (left). Sensitivity of condition number as a function of the matrix rank (right). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Evolution of the condition number during training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conducted an experiment to track the condition number and evaluate how tight the upper bound is throughout training. For this we trained a 3-layer linear network with a hidden width of $m=500$ with three different seeds for 5000 epochs with SGD with a mini-batch size of 256 and a constant learning rate of 0.2 on a subset of Cifar-10 $n=1000)$ ) [Krizhevsky et al., 2009], which has been downsampled to $3\\times8\\times8$ images and whitened. The network was trained on a single NVIDIA GeForce RTX 3090 GPU and took around 5 minutes per run. The condition number was computed via the explicit formula in Lemma 2 on CPU and took around 4 minutes. The condition number and the upper bound are shown in Figure 10 together with the training loss. We make the following two observations: 1. The condition number takes values in the range of 6 to 12 throughout training, which corresponds to a maximal change of around $50\\%$ of the condition number at initialization. This indicates that the condition number at initialization can be indicative of how the conditioning is along the optimization trajectory. 2. The upper bound remains tight throughout training, which highlights that it can provide reliable information about the condition number of the Gauss-Newton matrix throughout training. ", "page_idx": 17}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/bfec23c7619af4bebc9f71c63ba7295d970f7f57204849056f97684515329c41.jpg", "img_caption": ["Figure 10: Training loss (left) and corresponding evolution of the condition number throughout training for three seeds. The shaded area in the figures corresponds to one standard deviation from the mean. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Understanding the difference between the convex combination bound and the maximum Bound ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Lemma 2 we have seen that the upper bound of the condition number can be expressed in terms of a convex combination of the condition number of the weight matrices. In this section, we will elucidate the \u2018self-balancing\u2019 behavior of the individual terms in Lemma 2. For completeness, let us restate Lemma 2 here again: ", "page_idx": 18}, {"type": "text", "text": "Lemma 2. Assume that $m>\\operatorname*{max}\\{d,k\\}$ and that $\\alpha_{\\ell}:=\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W}^{L:\\ell+1})\\cdot\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W}^{1:\\ell-1})>0\\;\\forall\\ell=$ $\\begin{array}{r}{1,\\dotsc,L.\\ L e t\\gamma_{\\ell}:=\\frac{\\alpha_{\\ell}}{\\sum_{i=1}^{L}\\alpha_{i}}}\\end{array}$ . Then the condition number of the GN matrix of a $L$ -layer linear network can be upper-bounded in the following way: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq\\kappa(\\mathbf{Z})\\sum_{\\ell=1}^{L}\\gamma_{\\ell}\\kappa({\\mathbf{W}}^{L;\\ell+1})^{2}\\kappa({\\mathbf{W}}^{1;\\ell-1})^{2}\\leq\\kappa(\\mathbf{Z})\\operatorname*{max}_{1\\leq\\ell\\leq L}\\left\\{\\kappa({\\mathbf{W}}^{L;\\ell+1})^{2}\\kappa({\\mathbf{W}}^{1;\\ell-1})^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By looking at the individual values of $\\kappa^{2}(\\mathbf{W}^{L:\\ell+1})$ and $\\sigma_{\\mathrm{min}}^{2}(\\mathbf{W}^{L:\\ell+1})$ and similarly $\\kappa^{2}(\\mathbf{W}^{1:\\ell-1})$ and $\\sigma_{\\mathrm{min}}^{2}(\\mathbf{W}^{1:\\ell-1})$ over $\\ell\\,=\\,1,\\ldots,L$ for networks of increasing depth $L$ , we can see the \u2018selfbalancing\u2019 behavior in Figure 11 for downsampled MNIST $d=196)$ ) over 10 runs and $m=300$ neurons per hidden layer. Note that this behavior is also consistent for different widths $m$ , input dimension $d$ , and output dimension $k$ . By first looking at the first row, we observe that $\\kappa^{2}(\\mathbf{W}^{1:\\ell-1})$ (top left) dominates the product of each term in Lemma 2 and follows an exponential growth rule in $\\ell$ (note the log scale of the $y$ -axis). By looking at the second row, we see that at the same time, the smallest singular value of $\\bar{\\kappa}^{2}(\\mathbf{W}^{1:\\ell-\\bar{1}})$ (bottom left) is decreasing exponentially, leading to the \u2018self-balancing\u2019 behavior of the condition number. The same observation also holds for $\\mathbf{W}^{L;\\ell+1}$ , but since the number of weight matrices decreases with $\\ell$ this leads to a mirrored plot. The \u2018self-balancing\u2019 behavior can be seen below in Figure 12 for ease of reading, where the individual terms of Lemma 2 are plotted with and without the weighting factor $\\gamma_{\\ell}$ . ", "page_idx": 18}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/a3ab1cb88e5a35c9a34fe1cd0b014cb6771d264797c2f753f8ece7ce20306e0b.jpg", "img_caption": ["Figure 11: Condition number and smallest eigenvalue of $\\mathbf{W}^{1:\\ell-1}$ (first column) and $\\mathbf{W}^{L;\\ell+1}$ (second column) for downsampled MNIST $\\(d\\,=\\,196)$ ) for three seeds. Shaded area corresponds to one standard deviation. Note that the $\\mathrm{y}$ -axis is log-scaled at the different limits for each subplot. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/9f0af94072b98f481a8660e96a2bf8a917e8bb674e59d1be8515a082183379ea.jpg", "img_caption": ["Figure 12: Condition number of weight matrices with and without weighting factor $\\gamma_{\\ell}$ for downsampled MNIST $d=196)$ ) for three seeds, which highlights the \u2018self-balancing\u2019 effect of the weight condition number. Note that the y-axis is log-scaled and both plots have different limits. Shaded area corresponds to one standard deviation. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Conditioning of functional Hessian for one-hidden layer network ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Consider the case of a one-hidden layer linear network $\\mathrm{F}_{\\pmb{\\theta}}(\\mathbf{x})=\\mathbf{W}\\mathbf{V}\\mathbf{x}$ , with $\\mathbf{x}\\in\\mathbb{R}^{d}$ , $\\mathbf{W}\\in\\mathbb{R}^{k\\times m}$ , and $\\mathbf{V}\\in\\mathbb{R}^{m\\times d}$ and some corresponding label $\\mathbf{y}\\in\\mathbb{R}^{k}$ . Under the assumption of the MSE loss, the functional Hessian is then given as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathrm{F}}=\\left(\\!\\!\\begin{array}{l l}{\\mathbf{0}_{k m\\times k m}}&{\\boldsymbol{\\Omega}\\otimes\\mathbf{I}_{m}}\\\\ {\\boldsymbol{\\Omega}^{\\top}\\otimes\\mathbf{I}_{m}}&{\\mathbf{0}_{d m\\times d m},}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\Omega=\\mathbb{E}[\\delta_{\\mathbf{x},\\mathbf{y}}\\mathbf{x}^{\\top}]\\,\\in\\,\\mathbb{R}^{k\\times d}$ is the residual-input covariance matrix, with $\\delta_{\\mathbf{x},\\mathbf{y}}:=\\mathrm{F}_{\\pmb{\\theta}}(\\mathbf{x})-\\mathbf{y}$ being the residual. Since we would like to estimate the spectrum of $\\mathbf{H}_{\\mathrm{F}}$ , let us suppose that it has an eigenvector $\\mathbf{v}={\\binom{\\mathbf{a}\\otimes\\mathbf{b}}{\\mathbf{c}\\otimes\\mathbf{d}}}$ . Assuming that $\\lambda$ is the corresponding eigenvalue, we have the following eigenproblem: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathrm{F}}\\cdot\\mathbf{v}={\\binom{\\Omega\\,\\mathbf{c}\\,\\otimes\\,\\mathbf{d}}{\\Omega^{\\top}\\mathbf{a}\\otimes\\mathbf{b}}}=\\lambda\\,\\left(\\mathbf{a}\\otimes\\mathbf{b}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let\u2019s start with the guess that $\\mathbf{b}=\\mathbf{d}$ , then we obtain the following equations: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda\\,{\\mathbf a}=\\Omega\\,{\\mathbf c}\\quad\\mathrm{and}\\quad\\lambda\\,{\\mathbf c}=\\Omega^{\\top}{\\mathbf a}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Solving this gives: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Omega^{\\top}\\Omega\\,{\\mathbf c}=\\lambda^{2}\\,{\\mathbf c}\\quad\\mathrm{and}\\quad\\Omega\\Omega^{\\top}{\\mathbf a}=\\lambda^{2}\\,{\\mathbf a}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, a and c are simply the eigenvectors of $\\Omega\\Omega^{\\top}$ and $\\Omega^{\\top}\\Omega$ respectively. Both these matrices have the same non-zero eigenvalues, and the resulting eigenvalues for the functional Hessian are nothing but the square root of corresponding eigenvalues of the matrix $\\Omega^{\\top}\\Omega$ \u2014 with both positive and negative signs. About $\\mathbf{b}=\\mathbf{d}$ : We still have a degree of freedom left in choosing them. Any $m$ -orthogonal vectors with unit norm would satisfy the above equation and, for simplicity, we can just pick the canonical basis vectors in $\\mathbb{R}^{m}$ . ", "page_idx": 19}, {"type": "text", "text": "Teacher-Student setting Let us now further assume a Teacher-student setting, i.e. $\\mathbf{y}=\\mathbf{Z}\\mathbf{x}$ for some $\\mathbf{Z}\\in\\mathbb{R}^{k\\times d}$ . In this case note that $\\Omega=\\mathbb{E}[(\\mathbf{W}\\mathbf{V}\\mathbf{x}-\\mathbf{y})\\mathbf{x}^{\\top}]=(\\mathbf{W}\\mathbf{V}-\\mathbf{Z})\\mathbb{E}[\\bar{\\mathbf{x}}\\mathbf{x}^{\\top}]\\stackrel{*}{=}(\\mathbf{W}\\mathbf{V}-\\mathbf{Z})\\mathbf{\\Sigma}$ . Now recall the definition of the condition number $\\kappa(\\mathbf{H}_{F})=\\|\\mathbf{H}_{F}\\|\\cdot\\|\\mathbf{H}_{F}^{-1}\\|$ and using the observation ", "page_idx": 19}, {"type": "text", "text": "from above we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\bf H}_{F}\\|=\\sigma_{\\operatorname*{max}}({\\bf H}_{F})=\\sigma_{\\operatorname*{max}}(({\\bf W V}-{\\bf Z}){\\boldsymbol\\Sigma})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sigma_{\\operatorname*{max}}({\\bf W V}-{\\bf Z})\\cdot\\lambda_{\\operatorname*{max}}({\\boldsymbol\\Sigma})}\\\\ &{\\|{\\bf H}_{F}^{-1}\\|=\\sigma_{\\operatorname*{min}}({\\bf H}_{F})=\\sigma_{\\operatorname*{min}}(({\\bf W V}-{\\bf Z}){\\boldsymbol\\Sigma})}\\\\ &{\\qquad\\qquad\\qquad\\geq\\sigma_{\\operatorname*{min}}({\\bf W V}-{\\bf Z})\\cdot\\lambda_{\\operatorname*{min}}({\\boldsymbol\\Sigma}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\kappa({\\bf H}_{F})\\leq\\kappa({\\bf W V-Z})\\cdot\\kappa({\\boldsymbol\\Sigma}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similar to bounding the condition number of the GN matrix, we can also clearly see a dependency on the conditioning of the input covariance. ", "page_idx": 20}, {"type": "text", "text": "G Concrete bounds at initialization for one-hidden layer linear network ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "G.1 Non-asymptotic bound ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Based on the upper bound in Eq. (6) derived for the one-hidden layer linear network, we can look at how this bound behaves concretely at initialization. If we assume the entries of W and $\\mathbf{V}$ to be i.i.d. Gaussian entries, then $\\mathbf{W}\\mathbf{W}^{\\top}$ and $\\mathbf{V}^{\\top}\\mathbf{V}$ will be Wishart matrices, for which asymptotically the distribution of the smallest and largest eigenvalues are known. Also there are known non-asymptotic bounds to bound the maximum and minimum singular value of a Gaussian matrix: ", "page_idx": 20}, {"type": "text", "text": "Lemma 6 (Corollary 5.35 in Vershynin [2010]). Let A be an $N\\times n$ matrix whose entries are independent standard normal random variables ${\\mathcal{N}}(0,1)$ . Then for every $t\\geq0$ , with probability at least $\\begin{array}{r}{1-2\\exp(-\\frac{t^{2}}{2})}\\end{array}$ one has ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sqrt{N}-\\sqrt{n}-t\\leq\\sigma_{\\operatorname*{min}}(\\mathbf{A})\\leq\\sigma_{\\operatorname*{max}}(\\mathbf{A})\\leq\\sqrt{N}+\\sqrt{n}+t.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the Lemma above and a union bound, we can also derive a non-asymptotic bound for the condition number of the GN matrix under the assumption that we have a wide layer $m>\\operatorname*{max}(d,k)$ : ", "page_idx": 20}, {"type": "text", "text": "Lemma 7. Assume that $m\\,\\geq\\,\\operatorname*{max}(d,k)$ and that the entries of $\\textbf{V}\\in\\,\\mathbb{R}^{m\\times d}$ , $\\textbf{W}\\in\\,\\mathbb{R}^{k\\times m}$ are initialized Gaussian i.i.d. with $\\mathbf{V}_{i j}\\sim\\mathcal{N}(0,\\sigma_{v}^{2})$ and $\\mathbf{W}_{i j}\\sim\\mathcal{N}(0,\\sigma_{w}^{2})$ , then with probability at least $1-8\\exp(-\\frac{t^{2}}{2})$ the condition number of the GN matrix $\\kappa(\\widehat{\\mathbf{G}}_{O})$ is upper bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\kappa(\\widehat{\\mathbf{G}}_{O})=\\frac{\\lambda_{\\operatorname*{max}}(\\widehat{\\mathbf{G}}_{O})}{\\lambda_{\\operatorname*{min}}(\\widehat{\\mathbf{G}}_{O})}\\leq\\kappa(\\Sigma)\\frac{(\\sigma_{w}^{2}(\\sqrt{m}+\\sqrt{k}+t)^{2}+\\sigma_{v}^{2}(\\sqrt{m}+\\sqrt{d}+t)^{2})}{(\\sigma_{w}^{2}(\\sqrt{m}-\\sqrt{k}-t)^{2}+\\sigma_{v}^{2}(\\sqrt{m}-\\sqrt{d}-t)^{2})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Assume that $\\mathbf{V}\\in\\mathbb{R}^{m\\times d},\\mathbf{W}\\in\\mathbb{R}^{k\\times m}$ are initialized Gaussian i.i.d. with $\\mathbf{V}_{i j}\\sim\\mathcal{N}(0,\\sigma_{v}^{2})$ and $\\mathbf{W}_{i j}\\sim\\mathcal{N}(0,\\sigma_{w}^{2})$ , then the bounds in Lemma 6 are scaled with $\\sigma_{v}$ and $\\sigma_{w}$ respectively: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{v}(\\sqrt{m}-\\sqrt{d}-t)\\leq\\sigma_{\\operatorname*{min}}(\\mathbf{V})\\leq\\sigma_{\\operatorname*{max}}(\\mathbf{V})\\leq\\sigma_{v}(\\sqrt{m}+\\sqrt{d}+t)}\\\\ {\\sigma_{w}(\\sqrt{m}-\\sqrt{k}-t)\\leq\\sigma_{\\operatorname*{min}}(\\mathbf{W})\\leq\\sigma_{\\operatorname*{max}}(\\mathbf{W})\\leq\\sigma_{w}(\\sqrt{m}+\\sqrt{k}+t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Further noting that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda_{\\operatorname*{min}}(\\mathbf{V}^{\\top}\\mathbf{V})=\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{V}),\\quad\\lambda_{\\operatorname*{max}}(\\mathbf{V}^{\\top}\\mathbf{V})=\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{V})}\\\\ &{\\lambda_{\\operatorname*{min}}(\\mathbf{W}\\mathbf{W}^{\\top})=\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W})\\quad\\lambda_{\\operatorname*{max}}(\\mathbf{W}\\mathbf{W}^{\\top})=\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{W}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we get the following non-asymptotic upper respectively lower bounds for the extreme eigenvalues with probability at least $\\begin{array}{r}{1-\\dot{2}\\exp(-\\frac{t^{2}}{2})}\\end{array}$ each: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda_{\\operatorname*{min}}(\\mathbf{V}^{\\top}\\mathbf{V})\\geq\\sigma_{v}^{2}(\\sqrt{m}-\\sqrt{d}-t)^{2},\\quad\\lambda_{\\operatorname*{max}}(\\mathbf{V}^{\\top}\\mathbf{V})\\leq\\sigma_{v}^{2}(\\sqrt{m}+\\sqrt{d}+t)^{2}}\\\\ &{\\lambda_{\\operatorname*{min}}(\\mathbf{W}\\mathbf{W}^{\\top})\\geq\\sigma_{w}^{2}(\\sqrt{m}-\\sqrt{k}-t)^{2},\\quad\\lambda_{\\operatorname*{max}}(\\mathbf{W}\\mathbf{W}^{\\top})\\leq\\sigma_{w}^{2}(\\sqrt{m}+\\sqrt{k}+t)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using a union bound, we get the upper bound for the condition number of the GN matrix. ", "page_idx": 20}, {"type": "text", "text": "Based on the bound derived above, we can understand how the condition number behaves under initialization schemes used in practice. Consider the setting of a wide hidden layer $m\\gg\\operatorname*{max}\\{d,k\\}$ , in which the input dimension is much larger than the output dimension, i.e. $d\\gg k$ . This is for instance the case for image classification, where the number of image pixels is typically much larger than the number of image classes. A popular initialization scheme is the one proposed in He et al. [2015], commonly known as the Kaiming/He initialization, which corresponds to $\\begin{array}{r}{\\bar{\\sigma}_{w}^{2}=\\frac{1}{m},\\sigma_{v}^{2}=\\frac{1}{d}}\\end{array}$ in our case. Hence, under the assumption of , we will have $\\sigma_{v}^{2}\\gg\\sigma_{w}^{2}$ and therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq\\kappa(\\Sigma)\\frac{\\left(\\sigma_{w}^{2}\\left(\\sqrt{m}+\\sqrt{k}+t\\right)^{2}+\\sigma_{v}^{2}\\left(\\sqrt{m}+\\sqrt{d}+t\\right)^{2}\\right)}{\\left(\\sigma_{w}^{2}\\left(\\sqrt{m}-\\sqrt{k}-t\\right)^{2}+\\sigma_{v}^{2}(\\sqrt{m}-\\sqrt{d}-t)^{2}\\right)}}\\\\ {\\approx\\kappa(\\Sigma)\\frac{\\left(\\sqrt{m}+\\sqrt{d}+t\\right)^{2}}{(\\sqrt{m}-\\sqrt{d}-t)^{2}}\\approx\\kappa(\\Sigma).\\hfill}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "G.2 Asymptotic bound ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To complement the analysis, we can also derive an asymptotic bound for the condition number when $d,k,m$ go to infinity together. ", "page_idx": 21}, {"type": "text", "text": "For this, we will use a result from Silverstein [1985], in which it is shown that the smallest eigenvalue of the matrix $\\begin{array}{r}{{\\mathbf M}_{s}\\,=\\,\\frac{1}{s}{\\mathbf U}_{s}{\\mathbf U}_{s}^{\\top}}\\end{array}$ , where $\\mathbf{U}_{s}\\,\\in\\,\\mathbb{R}^{n\\times s}$ composed of i.i.d. $\\mathcal{N}(0,1)$ random variables, converges to $(1-\\sqrt{y})^{2}$ as $s\\to\\infty$ , while ${\\frac{n}{s}}\\to y\\in(0,1)$ as $s\\to\\infty$ . This corresponds to the case in which the hidden layer is larger than the input and output dimensions, that is $m\\geq\\operatorname*{max}(d,k)$ . Similarly it can be shown that $\\bar{\\lambda_{\\operatorname*{max}}}\\to(1+\\sqrt{y})^{2}$ for all $y>0$ for $s\\to\\infty$ . ", "page_idx": 21}, {"type": "text", "text": "Using this result we can bound the extreme eigenvalues of $\\mathbf{W}\\mathbf{W}^{\\top}$ and $\\mathbf{V}^{\\top}\\mathbf{V}$ at initialization. Assuming that the entries of $\\mathbf{W}$ and $\\mathbf{V}$ are entry-wise independent normally distributed $\\mathbf{W}_{i j}\\,\\sim$ $\\mathcal{N}(0,\\sigma_{w}^{2}),\\mathbf{V}_{i j}\\sim\\mathcal{N}(0,\\sigma_{v}^{2})$ (for instance $\\begin{array}{r}{\\sigma_{w}^{2}\\,=\\,\\frac{1}{m},\\sigma_{v}^{2}\\,=\\,\\frac{1}{d}}\\end{array}$ for Kaiming-initialization) we get for $d,k,m\\rightarrow\\infty$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{min}}(\\mathbf{W}\\mathbf{W}^{\\top})=\\sigma_{w}^{2}\\cdot m\\cdot\\lambda_{\\operatorname*{min}}\\left(\\frac{1}{m}\\mathbf{U}_{m}\\mathbf{U}_{m}^{\\top}\\right)\\overset{k,m\\rightarrow\\infty}{\\rightarrow}\\sigma_{w}^{2}\\cdot m\\cdot\\left(1-\\sqrt{\\frac{k}{m}}\\right)^{2}}\\\\ &{\\lambda_{\\operatorname*{max}}(\\mathbf{W}\\mathbf{W}^{\\top})=\\sigma_{w}^{2}\\cdot m\\cdot\\lambda_{\\operatorname*{min}}\\left(\\frac{1}{m}\\mathbf{U}_{m}\\mathbf{U}_{m}^{\\top}\\right)\\overset{k,m\\rightarrow\\infty}{\\rightarrow}\\sigma_{w}^{2}\\cdot m\\cdot\\left(1+\\sqrt{\\frac{k}{m}}\\right)^{2}}\\\\ &{\\ \\ \\ \\lambda_{\\operatorname*{min}}(\\mathbf{V}\\mathbf{V}^{\\top})=\\sigma_{v}^{2}\\cdot m\\cdot\\lambda_{\\operatorname*{min}}\\left(\\frac{1}{m}\\mathbf{U}_{m}\\mathbf{U}_{m}^{\\top}\\right)\\overset{d,m\\rightarrow\\infty}{\\rightarrow}\\sigma_{v}^{2}\\cdot m\\cdot\\left(1-\\sqrt{\\frac{d}{m}}\\right)^{2}}\\\\ &{\\lambda_{\\operatorname*{max}}(\\mathbf{V}\\mathbf{V}^{\\top})=\\sigma_{v}^{2}\\cdot m\\cdot\\lambda_{\\operatorname*{max}}\\left(\\frac{1}{m}\\mathbf{U}_{m}\\mathbf{U}_{m}^{\\top}\\right)\\overset{d,m\\rightarrow\\infty}{\\rightarrow}\\sigma_{v}^{2}\\cdot m\\cdot\\left(1+\\sqrt{\\frac{d}{m}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, in the asymptotic case in which the hidden layer is wide $m\\geq\\operatorname*{max}\\{d,k\\}$ , we can derive the following upper bound for the condition number: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq C\\stackrel{d,k,\\underset{m\\rightarrow\\infty}{\\longrightarrow}\\infty}\\kappa(\\Sigma)\\cdot\\frac{\\sigma_{w}^{2}\\cdot\\left(1+\\sqrt{\\frac{k}{m}}\\right)^{2}+\\sigma_{v}^{2}\\cdot\\left(1+\\sqrt{\\frac{d}{m}}\\right)^{2}}{\\sigma_{w}^{2}\\cdot\\left(1-\\sqrt{\\frac{k}{m}}\\right)^{2}+\\sigma_{v}^{2}\\cdot\\left(1-\\sqrt{\\frac{d}{m}}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Again, if $m\\gg\\operatorname*{max}\\{d,k\\}$ we also have in the asymptotic limit that $\\kappa(\\widehat{\\mathbf{G}}_{O})\\approx\\kappa(\\Sigma)$ . ", "page_idx": 21}, {"type": "text", "text": "H Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 1. Let $\\beta_{w}=\\sigma_{\\mathrm{min}}^{2}(\\mathbf{W})/(\\sigma_{\\mathrm{min}}^{2}(\\mathbf{W})+\\sigma_{\\mathrm{min}}^{2}(\\mathbf{V}))$ . Then the condition number of GN for the one-hidden layer network with linear activations with $\\dot{m}>\\operatorname*{max}\\{d,k\\}$ is upper bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq\\,\\kappa(\\mathbf{Z})\\,\\cdot\\,\\frac{\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{W})+\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{V})}{\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W})+\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{V})}=\\,\\kappa(\\mathbf{Z})\\,\\cdot\\,\\left(\\beta_{w}\\,\\kappa(\\mathbf{W})^{2}+\\left(1-\\beta_{w}\\right)\\kappa(\\mathbf{V})^{2}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Note that we have by the Weyl\u2019s in dual Weyl\u2019s inequality that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{l}_{\\operatorname*{max}}(\\widehat{\\mathbf{G}}_{O})=\\lambda_{\\operatorname*{max}}(\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\pmb{\\Sigma}+\\mathbf{I}_{k}\\otimes\\pmb{\\Sigma}^{1/2}\\mathbf{V}^{\\top}\\mathbf{V}\\pmb{\\Sigma}^{1/2})\\leq\\lambda_{\\operatorname*{max}}(\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\pmb{\\Sigma})+\\lambda_{\\operatorname*{max}}(\\mathbf{I}_{k}\\otimes\\pmb{\\Sigma}^{1/2}\\mathbf{V}\\pmb{\\Sigma}^{1/2})\\leq\\lambda_{\\operatorname*{max}}(\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\pmb{\\Sigma}),}\\\\ &{\\lambda_{\\operatorname*{min}}(\\widehat{\\mathbf{G}}_{O})=\\lambda_{\\operatorname*{min}}(\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\pmb{\\Sigma}+\\mathbf{I}_{k}\\otimes\\pmb{\\Sigma}^{1/2}\\mathbf{V}^{\\top}\\mathbf{V}\\pmb{\\Sigma}^{1/2})\\geq\\lambda_{\\operatorname*{min}}(\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\pmb{\\Sigma})+\\lambda_{\\operatorname*{min}}(\\mathbf{I}_{k}\\otimes\\pmb{\\Sigma}^{1/2}\\mathbf{V}\\mathbf{W}^{\\top}\\otimes\\pmb{\\Sigma}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then note that $\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\pmb{\\Sigma}$ and $\\mathbf{I}_{k}\\otimes\\pmb{\\Sigma}^{1/2}\\mathbf{V}^{\\top}\\mathbf{V}\\pmb{\\Sigma}^{1/2}$ are positive semidefinite and using the equality for the extreme eigenvalues of Kronecker products in Eq. (13), we have the bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{imax}\\bigl(\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\mathbf{Z}\\bigr)+\\lambda_{\\operatorname*{max}}\\bigl(\\mathbf{I}_{k}\\otimes\\mathbf{Z}^{1/2}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{Z}^{1/2}\\bigr)=\\lambda_{\\operatorname*{max}}(\\mathbf{W}\\mathbf{W}^{\\top})\\lambda_{\\operatorname*{max}}(\\mathbf{Z})+\\lambda_{\\operatorname*{max}}(\\mathbf{Z}^{1/2}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{Z})=\\lambda_{\\operatorname*{max}}(\\mathbf{W}\\mathbf{W}^{\\top})\\lambda_{\\operatorname*{max}}(\\mathbf{Z})}\\\\ &{\\lambda_{\\operatorname*{min}}\\bigl(\\mathbf{W}\\mathbf{W}^{\\top}\\otimes\\mathbf{Z}\\bigr)+\\lambda_{\\operatorname*{min}}\\bigl(\\mathbf{I}_{k}\\otimes\\mathbf{Z}^{1/2}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{Z}^{1/2}\\bigr)=\\lambda_{\\operatorname*{min}}(\\mathbf{W}\\mathbf{W}^{\\top})\\lambda_{\\operatorname*{min}}(\\mathbf{Z})+\\lambda_{\\operatorname*{min}}(\\mathbf{Z}^{1/2}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{Z})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the submultiplicativity of the matrix norm we get the upper bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq\\kappa(\\Sigma)\\cdot\\frac{\\lambda_{\\operatorname*{max}}(\\mathbf{W}\\mathbf{W}^{\\top})+\\lambda_{\\operatorname*{max}}(\\mathbf{V}^{\\top}\\mathbf{V})}{\\lambda_{\\operatorname*{min}}(\\mathbf{W}\\mathbf{W}^{\\top})+\\lambda_{\\operatorname*{min}}(\\mathbf{V}^{\\top}\\mathbf{V})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, by noting that we have $\\lambda_{i}(\\mathbf{W}\\mathbf{W}^{\\top})=\\sigma_{i}^{2}(\\mathbf{W})$ and $\\lambda_{i}(\\mathbf{V}^{\\top}\\mathbf{V})=\\sigma_{i}^{2}(\\mathbf{V})$ for $i\\in\\{\\operatorname*{min},\\operatorname*{max}\\}$ because we have $m>\\operatorname*{max}\\{d,k\\}$ yields the result. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma 4. Consider an one-hidden layer network with a Leaky-ReLU activation with negative slope $\\alpha\\in[0,1]$ . Then the condition number of the GN matrix defined in Equation (10) can be bounded as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\kappa(\\widehat{\\mathbf{G}}_{O})\\leq\\frac{\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{X})\\,\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{W})+\\lambda_{\\operatorname*{max}}\\,(\\mathbf{Gamma})}{\\alpha^{2}\\,\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{X})\\,\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W})+\\lambda_{\\operatorname*{min}}\\,(\\mathbf{\\Gamma})}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. As before we will bound the condition number by separately bounding the extreme eigenvalues first. ", "page_idx": 22}, {"type": "text", "text": "First, recall the expression of the GN matrix as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbf{G}}_{O}=\\sum_{i=1}^{m}\\mathbf{\\Lambda}\\mathbf{\\Lambda}^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{\\Lambda}^{i}\\otimes\\mathbf{W}_{\\bullet i}\\mathbf{W}_{\\bullet i}^{\\top}+\\sum_{i=1}^{m}\\mathbf{\\Lambda}\\mathbf{\\Lambda}^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i\\bullet}^{\\top}\\mathbf{V}_{i\\bullet}\\mathbf{X}\\mathbf{\\Lambda}^{i}\\otimes\\mathbf{I}_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and note that we can apply the Weyl\u2019s and dual Weyl\u2019s inequality because $(\\mathbf{A}^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{A}^{i}){\\otimes}(\\mathbf{W}_{\\bullet,i}\\mathbf{W}_{\\bullet,i}^{\\top})$ and $\\left(\\mathbf{A}^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i,\\bullet}\\mathbf{V}_{i,\\bullet}^{\\top}\\mathbf{X}\\mathbf{A}^{i}\\right)\\otimes\\mathbf{I}_{k}$ are symmetric, yielding the following bounds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\lambda_{\\operatorname*{min}}(\\widehat{\\mathbf{G}}_{\\mathcal{O}})\\geq\\lambda_{\\operatorname*{min}}\\left(\\sum_{i=1}^{m}(\\mathbf{A}^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{A}^{i})\\otimes(\\mathbf{W}_{\\bullet,i}\\mathbf{W}_{\\bullet,i}^{\\top})\\right)+\\lambda_{\\operatorname*{min}}\\left(\\sum_{i=1}^{m}(\\mathbf{A}^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i,\\bullet}^{\\top}\\mathbf{V}_{i,\\bullet}\\mathbf{X}\\mathbf{A}^{i})\\otimes\\mathbf{I}_{k}\\right)}\\\\ &{\\displaystyle\\operatorname*{suax}_{i=1}(\\widehat{\\mathbf{G}}_{\\mathcal{O}})\\leq\\lambda_{\\operatorname*{max}}\\left(\\sum_{i=1}^{m}(\\mathbf{A}^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{A}^{i})\\otimes(\\mathbf{W}_{\\bullet,i}\\mathbf{W}_{\\bullet,i}^{\\top})\\right)+\\lambda_{\\operatorname*{max}}\\left(\\sum_{i=1}^{m}(\\mathbf{A}^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i,\\bullet}^{\\top}\\mathbf{V}_{i,\\bullet}^{\\top}\\mathbf{X}\\mathbf{A}^{i})\\otimes\\mathbf{I}_{k}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then we can bound the first term by using the fact that for the sum of a Kronecker product of PSD matrices $\\mathbf{A}_{i},\\mathbf{B}_{i}$ for $i=1,\\hdots,m$ we have $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}\\left(\\sum_{i=1}^{m}(\\mathbf{A}_{i}\\otimes\\mathbf{B}_{i})\\right)\\geq\\operatorname*{min}_{1\\leq j\\leq m}\\bar{\\lambda}_{\\operatorname*{min}}(\\mathbf{A}_{j})}\\end{array}$ \u00b7 $\\textstyle\\lambda_{\\operatorname*{min}}\\left(\\sum_{i=1}^{m}\\mathbf{B}_{i}\\right)$ to get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\sum_{i=1}^{m}(\\Lambda^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{A}^{i})\\otimes(\\mathbf{W}_{\\bullet,i}\\mathbf{W}_{\\bullet,i}^{\\top})\\right)\\geq\\lambda_{\\operatorname*{min}}(\\Lambda^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{A}^{i})\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\sum_{i=1}^{m}\\mathbf{W}_{\\bullet,i}\\mathbf{W}_{\\bullet,i}^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\alpha^{2}\\lambda_{\\operatorname*{min}}(\\mathbf{X}^{\\top}\\mathbf{X})\\lambda_{\\operatorname*{min}}(\\mathbf{W}\\mathbf{W}^{\\top})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\alpha^{2}\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{X})\\sigma_{\\operatorname*{min}}^{2}(\\mathbf{W})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "by noting that $\\Lambda^{i}$ only contains 1 or $-\\alpha$ on its diagonals in case of Leaky-ReLU activation and $\\begin{array}{r}{\\dot{\\sum_{i=1}^{m}\\mathbf{W_{\\bullet,i}}}\\mathbf{W}_{\\bullet,i}^{\\top}=\\dot{\\mathbf{W}}\\mathbf{W}^{\\top}}\\end{array}$ . Analogously, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}\\left(\\sum_{i=1}^{m}(\\Lambda^{i}\\mathbf{X}^{\\top}\\mathbf{X}\\Lambda^{i})\\otimes(\\mathbf{W}_{\\bullet,i}\\mathbf{W}_{\\bullet,i}^{\\top})\\right)\\leq\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{X})\\sigma_{\\operatorname*{max}}^{2}(\\mathbf{W}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This yields the first upper bound. ", "page_idx": 23}, {"type": "text", "text": "To get the second upper bound, we need to further bound the second term. For this, note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\lambda_{\\operatorname*{min}}\\left(\\sum_{i=1}^{m}(\\mathbf{A}^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i,\\bullet}^{\\top}\\mathbf{V}_{i,\\bullet}\\mathbf{X}\\mathbf{A}^{i})\\otimes\\mathbf{I}_{k}\\right)=\\lambda_{\\operatorname*{min}}\\left(\\sum_{i=1}^{m}(\\mathbf{A}^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i,\\bullet}^{\\top}\\mathbf{V}_{i,\\bullet}\\mathbf{X}\\mathbf{A}^{i})\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\alpha^{2}\\lambda_{\\operatorname*{min}}\\left(\\sum_{i=1}^{m}(\\mathbf{X}^{\\top}\\mathbf{V}_{i,\\bullet}^{\\top}\\mathbf{V}_{i,\\bullet}\\mathbf{X})\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\alpha^{2}\\lambda_{\\operatorname*{min}}\\left(\\mathbf{X}^{\\top}\\sum_{i=1}^{m}(\\mathbf{V}_{i,\\bullet}^{\\top}\\mathbf{V}_{i,\\bullet})\\mathbf{X}\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\alpha^{2}\\lambda_{\\operatorname*{min}}\\left(\\mathbf{X}^{\\top}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{X}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Analogously we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}\\left(\\sum_{i=1}^{m}(\\Lambda^{i}\\mathbf{X}^{\\top}\\mathbf{V}_{i,\\bullet}^{\\top}\\mathbf{V}_{i,\\bullet}\\mathbf{X}\\Lambda^{i})\\otimes\\mathbf{I}_{k}\\right)\\leq\\lambda_{\\operatorname*{max}}\\left(\\mathbf{X}^{\\top}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{X}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which gives the second upper bound. ", "page_idx": 23}, {"type": "text", "text": "H.1 Further details on the effect of condition number at initialization on convergence rate ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We present here further details of the modified analysis of GD for strongly convex functions to study the effect of the condition number at initialization on the convergence rate, where we use local constants $\\mu(k)$ and $L(k)$ instead of the global smoothness and Lipschitz constant, respectively. Let $L$ denote the Lipschitz constant and let the smoothness constant be denoted by $\\mu$ . Furthermore, let the step size be such that $\\begin{array}{r}{\\eta_{k}\\leq\\frac{1}{L}}\\end{array}$ . Then by the definition of gradient descent we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{k+1}-\\theta^{*}\\|^{2}=\\|\\theta_{k}-\\theta^{*}-\\eta_{k}\\nabla f(\\theta_{k})\\vert\\vert^{2}}\\\\ &{\\quad\\quad\\quad\\quad=\\|\\theta_{k}-\\theta^{*}\\|^{2}-2\\eta_{k}\\nabla f\\left(\\theta_{k}\\right)^{\\top}\\left(\\theta_{k}-\\theta^{*}\\right)+\\eta_{k}^{2}\\|\\nabla f\\left(\\theta_{k}\\right)\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\quad\\quad(1-\\eta_{k}\\mu)\\|\\theta_{k}-\\theta^{*}\\|^{2}-2\\eta_{k}(f(\\theta_{k})-f(\\theta^{*}))+\\eta_{k}^{2}||\\nabla f\\left(\\theta_{k}\\right)||^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\quad(1-\\eta_{k}\\mu)\\|\\theta_{k}-\\theta^{*}\\|^{2}-2\\eta_{k}(f(\\theta_{k})-f(\\theta^{*}))+2\\eta_{k}^{2}L(f(\\theta_{k})-f(\\theta^{*}))}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\quad(1-\\eta_{k}\\mu)\\|\\theta_{k}-\\theta^{*}\\|^{2}-2\\eta_{k}(1-\\eta_{k}L)(f(\\theta_{k})-f(\\theta^{*}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since we assumed that $\\begin{array}{r}{\\eta_{k}\\leq\\frac{1}{L}}\\end{array}$ , the last term is negative. Therefore: ", "page_idx": 23}, {"type": "equation", "text": "$$\n||\\pmb{\\theta}_{k+1}-\\pmb{\\theta}^{*}||^{2}\\leq(1-\\eta_{k}\\mu)||\\pmb{\\theta}_{k}-\\pmb{\\theta}^{*}||^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So by recursively applying (27) and replacing $\\mu$ by the local smoothness constants $\\mu(k)$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n||\\theta_{k}-\\theta^{*}||^{2}\\leq\\prod_{i=0}^{k-1}(1-\\eta_{i}\\mu(i))||\\theta_{0}-\\theta^{*}||^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "One can see the effect of $\\mu(0)$ in the bound, which is even more dominant when $\\mu(k)$ changes slowly.   \nOf course, the effect of $\\mu(0)$ attenuates over time, and that\u2019s why we are talking about a local effect. ", "page_idx": 23}, {"type": "text", "text": "I Further experimental results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "I.1 Further experiments on pruning networks at initialization ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We repeated the experiments of pruning the weights at initialization for different architectures, including a small Vision Transformer $\\mathrm{(ViT)}^{3}$ (Figure 13), a ResNet20 (Figure 14), a ResNet32 (Figure 15), a VGG5Net4(Figure 16) and a Feed-forward network (Figure 17). The default initialization from PyTorch [Paszke et al., 2019] was used. In all setups, the weights were pruned layer-wise by magnitude and trained on a subset of Cifar-10 of $n=1000$ images. The ViT was trained with AdamW with a learning rate of $1e^{-2}$ and weight decay $1e^{-2}$ . The ResNet20, ResNet32 and VGGnet were trained with SGD with momentum $=0.9$ and weight decay of $10^{-4}$ and a learning rate of 0.1 with a step decay to 0.01 after 91 epochs for ResNet20 and ResNet32 and after 100 epochs for the VGGnet. The Feed-forward network was trained with SGD with a constant learning rate of 0.01. All networks were trained with a mini-batch size of 64, the ResNet20, ResNet32 and the Feed-forward network were trained on a single NVIDIA GeForce RTX 3090 GPU and the ViT on a single GPU NVIDIA GeForce RTX 4090. The training time was around 1 hour for each setup. The computation of the condition number was run in parallel on GPU and took around 8 hours for each pruning rate of each network. ", "page_idx": 24}, {"type": "text", "text": "I.2 Effect of width on conditioning and convergence speed ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For a one-hidden layer feed-forward network with ReLU activation, we empirically examined the effect of the hidden width on both the conditioning and the convergence speed when trained on a subset of MNIST $(n=1000)$ ). The networks were trained with SGD with a fixed learning rate, which was chosen via grid-search, on a single NVIDIA GeForce RTX 3090 GPU. The learning rate was 0.3 for the networks with width 15 and 20 and 0.5 for all the remaining networks. The networks were initialized via the Xavier normal initialization in PyTorch [Paszke et al., 2019]. The training time took less than one hour for all networks. The computation of the condition number was performed in parallel for each width and took around 4 hours in the largest setting. Figure 18 shows the trend that a larger width improves both the conditioning and the convergence speed. Additionally, the condition number is also more stable throughout training for larger widths. ", "page_idx": 24}, {"type": "text", "text": "I.3 Additional experiments on the effect of residual connections ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "As we have seen previously in Figure 3, the upper bound as a linear combination in Lemma 2 is crucial to get a tight bound on the condition number. Additionally, as illustrated in Figure 19, ", "page_idx": 24}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/c4732db1bf2a56964ad9392e37b50518e3e06ca65f9ca64335dc735ff0d136b5.jpg", "img_caption": ["Figure 14: Training loss (left) and condition number (right) for different amounts of pruning at initialization for a ResNet20. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/9e95ad42426605a8ae1c894ea58274fbf6195d5fd0b6834f15afcc8e5f7f395d.jpg", "img_caption": ["Figure 15: Training loss (left) and condition number (right) for different amounts of pruning at initialization for a ResNet32. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/044ebe807d8b7e7768d90224257c6560327b58cddeff835973ef9ef5c16f3b34.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 16: Training loss (left) and condition number (right) for different amounts of pruning at initialization for a VGG5. ", "page_idx": 25}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/843a9957847c32be458b2af36be5c12299981a12ee27be3b7c6b27ce935f01c7.jpg", "img_caption": ["Figure 17: Training loss (left) and condition number (right) for different amounts of pruning at initialization for a Feed-forward network. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/5e467415f7b63710d7a9bc416ececee33cb00c4bc4c47449a3f41b9d7ac4912b.jpg", "img_caption": ["Figure 18: Training loss (left) and condition number (right) for different widths of a one-hidden layer Feed-forward network trained on subset of MNIST $n=1000)$ ). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "whitening the data is essential to improve the conditioning. Therefore, if not otherwise specified, MNIST LeCun et al. [1998] and Cifar-10 Krizhevsky et al. [2009] will refer to whitened data and we refer to Lemma 2 or its equivalent in Eq. (7) for residual networks as the \u201cupper bound\u201d. The shaded area in the figures corresponds to one standard deviation from the mean. If not other specified via the Kaiming normal initialization in PyTorch was used. ", "page_idx": 26}, {"type": "text", "text": "As one can see from (8), the condition number for each term in the sum of the first upper bound in (8) improves when $\\beta$ increases. This can be seen by the fact that the ratio will be dominated by $\\beta$ and will go to 1 for $\\beta\\to\\infty$ . This is also what we observe empirically in Figure 20 and Figure 22, where the condition number is smaller for $\\beta=1$ compared to the other two settings, where $\\begin{array}{r}{\\beta=\\frac{1}{L}<\\frac{1}{\\sqrt{L}}<1}\\end{array}$ for deeper networks with $L>1$ . ", "page_idx": 26}, {"type": "text", "text": "In Figure 23 the condition number of the GN matrix for a one-hidden layer network with LeakyReLU activation is plotted for different values of $\\alpha$ , where $\\alpha=0$ corresponds to ReLU and $\\alpha=1$ corresponds to a linear activation. We can see how ReLU activation improves the conditioning in the one-hidden layer setting. ", "page_idx": 26}, {"type": "text", "text": "The condition number was computed on CPU via the explicit formulas given in Lemma 2 and Eq.   \n(7) and took approximately a few hours per experiment. ", "page_idx": 26}, {"type": "text", "text": "I.4 Further details on experiments compute resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The total compute for all experiments conducted amounts to approximately less than 100 hours. The main bottleneck is the computation of the condition number, which can be potentially further sped up by making full use of parallel computations on all available GPUs. The full research project required more compute than the experiments reported in the paper due to preliminary experiments. ", "page_idx": 26}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/bd65dea76cbe5267918a7deb5df05612e00e9e1305c8ef590094c9e4128d7940.jpg", "img_caption": ["Figure 19: Comparison of the condition number for a Linear Network with hidden width $m=3100$ with and without whitening Cifar-10 at initialization over three runs. Note, that the $y$ -axis is displayed in log scale. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/8e15bcb7bc067ab72f1385c2ce123de61d59a44d8e66e0d837b271194e03353e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 20: Condition number of outer-product Hessian $\\kappa(\\widehat{\\mathbf{G}}_{O})$ and upper bound of a Residual Network for different values of $\\beta$ as a function of depth for wh i tened MNIST at initialization over 20 runs. ", "page_idx": 27}, {"type": "text", "text": "Nevertheless, the total compute of the full research project amounts to a run time of similar order of magnitude. ", "page_idx": 27}, {"type": "text", "text": "I.4.1 Computational complexity of computing condition number of Gauss-Newton matrix ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In order to compute the condition number we need to compute the eigenspectrum of the GN matrix $\\mathbf{G}_{O}$ , which has dimension $p\\times p$ , where $p$ is the number of parameters or of the matrix $\\hat{\\mathbf{G}}_{O}$ , which has dimensions $k d\\times k d$ , where $d$ and $k$ are the input, respectively output dimension of the network. The time complexity of calculating the eigenvalue decomposition has a cubic complexity. That is, in order to compute the condition number, we have a computational complexity of $\\bar{\\mathcal{O}}(\\operatorname*{min}(p,k d)^{3})$ . ", "page_idx": 27}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/cae7a4c88ec665e5e75e6df5ecf5cc64ae3064327b505cbae1a97f8a85d618d8.jpg", "img_caption": ["Figure 21: Comparison of condition number of outer-product Hessian $\\kappa(\\widehat{\\mathbf{G}}_{O})$ between Linear Network and Residual Network for whitened Cifar-10 at initialization over th r ee runs. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/dd71039bc4afff2204d7c62646f4c9029482f5d6bf28d40732fb67f0e512a929.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 22: Condition number of outer-product Hessian $\\kappa(\\widehat{\\mathbf{G}}_{O})$ and upper bounds for Residual Network with different values of $\\beta$ at initialization over three  r uns for Cifar-10. ", "page_idx": 28}, {"type": "image", "img_path": "fpOnUMjLiO/tmp/56bb7e508f5b06c90b251d0f4253d7a4a5fdbcb4b7fbd9e8797d0ce4d50d5b9b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 23: Condition number of a one-hidden layer Leaky-ReLU network for different values of $\\alpha$ for whitened MNIST over $n=500$ data points, where $\\alpha=0$ corresponds to ReLU and $\\alpha=1$ corresponds to a linear activation. Note that the y-axis is log-scaled. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We state our main contributions in the abstract and devote a separate paragraph in the introduction on the paper\u2019s contributions and scope. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: In the final paragraph of the paper on \"Limitations and future work\" we discuss settings, which the paper does not cover, such as the extension to non-linear activations for arbitrary number of hidden layers, the extension to transformer architectures and analytic bounds for normalization layers. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: All assumptions are stated at the beginning of the paper and formal proofs of all Lemmas and Theorems are provided in the Appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: For the main experimental results, all information needed, such as the network architecture, the initialization scheme, and the optimizer with its hyperparameters are specified. Additionally, the dataset as well as the preprocessing steps were specified. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 30}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Code to run the experiments is provided in the supplementary material together with instructions how to reproduce the results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: For each experimental setting, the architectural setup, the initialization scheme and the optimizer together with its hyperparameters were specified. Since our work only evaluated the training setting, a data split was not necessary. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: For the experiments that support the main claims of the paper 1-sigma error bars are shown as shaded area. Figure 2a, Figure 2b, and Figure 5 were calculated over 3 seeds and Figure 3 over 20 seeds. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: For each experiment the (approximate) compute and whether CPU or GPU were used were specified. Additionally a brief subsection is provided in Appendix I.4 to discuss the total estimated compute. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper conforms in every aspect with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper is a theoretic contribution and as such we do not see any direct societal impact of the work performed. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: This paper poses no such risks for misuse or dual-use as it is primarily theoretical in nature. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The creators and original owners of the datasets used (MNIST and Cifar-10) the creators and original owners of models which were modified and used (ViT, ResNet20, ResNet32) were properly credited. In the case of the ResNet20, and ResNet32, the original owner Jiang Wenjie provided explicit permission to use the code. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]