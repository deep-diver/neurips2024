{"importance": "This paper is crucial for researchers working with large language models (LLMs) and their applications in mathematical reasoning.  It **uncovers the previously unknown mechanism** by which pre-trained LLMs perform addition using Fourier features, a finding that **challenges existing assumptions** about LLM mathematical capabilities and **opens new avenues** for improving LLM performance on algorithmic tasks. This mechanistic understanding will **inform future LLM design** and improve their ability to perform complex mathematical tasks, pushing the boundaries of AI capabilities.", "summary": "Pre-trained LLMs surprisingly use Fourier features to perform addition, with MLP layers approximating magnitude and attention layers handling modular arithmetic; this mechanism requires pre-training.", "takeaways": ["Pre-trained LLMs utilize Fourier features\u2014sparse frequency representations of numbers\u2014to perform addition.", "MLP layers primarily focus on magnitude approximation while attention layers handle modular arithmetic in the addition process.", "Pre-training is critical; models trained from scratch lack this mechanism, highlighting the importance of pre-trained representations for complex tasks."], "tldr": "Large Language Models (LLMs) show impressive mathematical abilities, but how they compute basic arithmetic like addition remains unclear.  This paper investigates how pre-trained LLMs perform this seemingly simple task. Previous research often focuses on smaller, simpler models or models trained from scratch, providing limited insight into the mechanisms of complex pre-trained LLMs. This paper addresses the limitations by directly analyzing a pre-trained, state-of-the-art LLM and its internal workings.\nThis study reveals that pre-trained LLMs surprisingly employ Fourier features to perform addition.  Specifically, it discovers that Multi-Layer Perceptrons (MLPs) primarily approximate the magnitude of the answer, leveraging low-frequency features, while attention mechanisms focus on modular addition (e.g., checking if the result is even or odd), using high-frequency features.  Importantly, the study demonstrates that pre-training is essential for this mechanism; randomly initialized models struggle to learn this method.  The research provides a detailed, mechanistic explanation, offering a significant contribution to our understanding of LLM capabilities.", "affiliation": "UC Los Angeles", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "i4MutM2TZb/podcast.wav"}