{"references": [{"fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017", "reason": "This paper introduced the foundational RLHF framework, which is the core method improved upon in the current paper."}, {"fullname_first_author": "R\u00e9mi Munos", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2023-10-26", "reason": "This paper provides the theoretical foundation for the direct use of preferences in reinforcement learning, a key idea leveraged in the current paper\u2019s algorithms."}, {"fullname_first_author": "Nisan Stiennon", "paper_title": "Learning to summarize from human feedback", "publication_date": "2020", "reason": "This paper demonstrated a successful application of RLHF for fine-tuning LLMs, inspiring the current paper\u2019s extension of the approach to the multi-turn setting."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022", "reason": "This work showcases a practical and successful application of RLHF for aligning LLMs with human preferences, providing a benchmark that the current work improves on."}, {"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2023", "reason": "This paper offers a theoretical framework for preference-based reinforcement learning, which is used in the current work's algorithms and analysis."}]}