[{"type": "text", "text": "Multi-turn Reinforcement Learning from Preference Human Feedback ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lior Shani \u22171 Aviv Rosenberg \u22171 Asaf Cassel \u22171 3 liorshani@google.com avivros@google.com acassel@mail.tau.ac.il ", "page_idx": 0}, {"type": "text", "text": "Oran Lang 1 Daniele Calandriello 2 Avital Zipori 1 Hila Noga 1 Orgad Keller 1 ", "page_idx": 0}, {"type": "text", "text": "Bilal Piot 2 Idan Szpektor 1 Avinatan Hassidim 1 Yossi Matias 1 R\u00e9mi Munos ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations. In the tabular setting, we present a novel mirrordescent-based policy optimization algorithm for the general multi-turn preferencebased RL problem, and prove its convergence to Nash equilibrium. To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent guides a student in learning a random topic, and show that a deep RL variant of our algorithm outperforms RLHF baselines. Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A pinnacle of human intelligence is the ability to communicate with an environment, forming complex interactions to accomplish challenging goals. Dialogues are one example of such dynamic communication, where one party reacts to signals from the other parties and dynamically plans ahead to steer communication towards their purpose. Recent years have seen scientific breakthroughs in developing Large Language Models (LLMs) that can communicate with humans in natural language [Ouyang et al., 2022, Anil et al., 2023, Touvron et al., 2023, OpenAI, 2024, Google, 2024]. In order to align these models to human needs, many efforts have been made to train them with a given human feedback. In some concordance with human learning, this is usually achieved by reinforcing behaviors that align with the feedback, using a technique now called Reinforcement Learning from Human Feedback (RLHF; Christiano et al. [2017], Ziegler et al. [2019], Stiennon et al. [2020]). ", "page_idx": 0}, {"type": "text", "text": "RLHF methods build on the long studied field of Reinforcement Learning (RL), which focuses on learning optimal actions through reward feedback (a numerical signal) from the environment. However, defining a suitable reward function is challenging, leading to the common practice of collecting human preferences between choices. In the absence of rewards, a mapping from preference to reward is typically assumed in the form of the Bradely-Terry (BT; Bradley and Terry [1952]) model [Stiennon et al., 2020, Rafailov et al., 2023], enabling the use of a wide-variety of well-researched ", "page_idx": 0}, {"type": "text", "text": "RL techniques. Alternatively, recent research [Munos et al., 2023, Azar et al., 2023, Tang et al., 2024] suggests a more direct use of preferences for learning, eliminating the need for this potentially limiting assumption. ", "page_idx": 1}, {"type": "text", "text": "Still, so far the main focus of both the RLHF and the direct preference learning literature was on single-turn scenarios, where given relevant context, the LLM generates one response and receives an immediate feedback that reflects its alignment quality. Importantly, while single-turn RLHF already provides significant gains for valuable AI systems, it lacks the adaptive and long-term capabilities that make human communication such a powerful tool, and usually characterize RL methods. This is especially apparent in temporally extended tasks, such as multi-turn dialogue [Irvine et al., 2023], complex tool use [Wang et al., 2022] and multi-step games [Hendrycks et al., 2022]. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we focus on improving the communication of AI agents with dynamic environments. To this end, we first extend the RLHF paradigm to the multi-turn setting, where the agent has a series of exchanges with an external (stochastic) environment (Section 3). Importantly, we consider (human) feedback that compares entire multi-turn conversations as opposed to single-turn scenarios, which compare individual actions on a per-turn basis. Conversation-level feedback allows to capture the long-term effect of individual actions, which may not be immediately apparent, and thus hard to define through turn-level feedback. For example, a seller agent asking too high a price may seem immediately bad, but becomes potentially good as part of a complete strategy to increase sale price. This difference is apparent in our preference model, making it better suited for multi-turn interactions. ", "page_idx": 1}, {"type": "text", "text": "Formalizing the multi-turn setting as a Contextual Markov Decision Process with end of interaction preference feedback, we devise several theoretically grounded algorithms (Section 4). Our main algorithm, Multi-turn Preference Optimization (MTPO), is a new policy optimization algorithm for the general multi-turn preference-based setting. MTPO is based on the Mirror Descent (MD) method [Nemirovskij and Yudin, 1983, Beck and Teboulle, 2003] together with self-play [Silver et al., 2017], and is proven to converge to a Nash equilibrium [Nash et al., 1950], i.e., a policy which is preferred over any other policy. We prove similar results for $\\mathrm{MTPO}{-}\\tau$ , a slight variant of MTPO that, similarly to Munos et al. [2023], uses a geometric mixture policy which interpolates the agent\u2019s policy with a fixed reference policy (with mixing rate $\\tau$ ). These algorithms utilize a new form of preference-based Q-function that accounts for the long-term consequences of individual actions. Finally, leveraging our theoretical framework, we modify this Q-function to create a multi-turn RLHF algorithm and prove its convergence to an optimal policy (w.r.t the learned reward function). ", "page_idx": 1}, {"type": "text", "text": "We complement our theoretical findings with a policy-gradient version of our multi-turn algorithms for deep learning architectures (Section 4.1). To validate our approach, we apply our algorithms to train a T5 encoder-decoder LLM [Raffel et al., 2020], aiming to enhance its multi-turn dialogue abilities (Sections 5 and 6). We test our approach in a scenario without explicit rewards, where conversation quality is evaluated solely through preferences. To that end, we create a new environment called Education Dialogue, where a teacher guides a student in learning a random topic, by prompting Gemini [Team et al., 2023]. The conversation is judged based on preference feedback, using a constitution that defines effective learning [Bai et al., 2022, Lee et al., 2023] (see Section 5). In this environment, our multi-turn algorithms significantly outperform single-turn baselines, and our direct multi-turn preference approach outperforms multi-turn RLHF (Section 6). As an additional contribution, we publicly release the data of Education Dialogue.1 Finally, we demonstrate that even in a reward-based environment, our preference-based algorithm achieves comparable performance to learning directly from rewards, as in standard RL, despite using a weaker signal. For this experiment, we utilize the LMRL-Gym [Abdulhai et al., 2023] Car Dealer environment, simulating a conversation where the agent (car dealer) aims to maximize the sale price. ", "page_idx": 1}, {"type": "text", "text": "Related work. Most related to our work is the RLHF literature, which aims to improve an LLM policy using preference data collected from humans. Earlier methods model a proxy reward [Ouyang et al., 2022] or preference function [Zhao et al., 2022], and apply traditional RL techniques. More recent methods directly optimize the policy [Rafailov et al., 2023, Azar et al., 2023, Tang et al., 2024, Song et al., 2024, Ethayarajh et al., 2024]. Another line of work, which forms the basis for MTPO, extends RLHF to games, aiming to compute a Nash equilibrium instead of an optimal policy w.r.t a fixed reward/preference. This includes Nash-MD [Munos et al., 2023], self-play and mixtures of the two like IPO-MD [Calandriello et al., 2024]. Nonetheless, these methods only consider single-turn problems, whereas MTPO provides the first guarantees for multi-turn settings. Note the difference from concurrent attempts to extend direct preference optimization to the token level [Rafailov et al., 2024], while a true multi-turn approach must deal with the additional uncontrollable tokens generated by the human in-between agent turns. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "More broadly, preference-based RL (see survey by Wirth et al. [2017]) studies feedback in terms of preferences over two alternatives rather than absolute rewards. Feedback can be provided in various ways, e.g., at the level of states (turn-level), or entire trajectories [Chen et al., 2022, Saha et al., 2023, Wu and Sun, 2023, Wang et al., 2023, Zhan et al., 2023a,b]. The focus of this work is last state feedback which is an instance of trajectory feedback. Another closely related model is RL with aggregate feedback where only the sum of rewards in a trajectory is revealed to the agent [Efroni et al., 2021, Cohen et al., 2021, Chatterji et al., 2021, Cassel et al., 2024]. ", "page_idx": 2}, {"type": "text", "text": "Lastly, there is vast literature on using RL to improve natural language generation for dialogue systems. The pioneering work of Li et al. [2016] focuses on designing rewards to capture important dialogue attributes such as semantic coherence and ease of answering. Other works tackle taskoriented dialogue, using RL to enhance the agent\u2019s ability to solve the underlying dialogue task [Wei et al., 2018]. Instead, this works takes a more general and fundamental approach, focusing on the algorithmic process of aligning an agent which repeatedly interacts with an environment. While dialogue systems are a promising application of our approach, as suggested by the experimental results of this paper (Sections 5 and 6), our algorithmic approach is much broader, including processes such as tool-use, reasoning, and many other applications that require aligning a complex multi-turn agent with human preferences. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The interaction between an AI agent and its environment is captured in the fundamental contextual RL model, where the context is the initial prompt, the states are conversation summaries, and the actions are responses. ", "page_idx": 2}, {"type": "text", "text": "Contextual Markov decision process. A finite-horizon contextual Markov decision process (CMDP) $\\mathcal{M}$ is defined by a tuple $(\\mathcal{C},\\mathcal{X},\\mathcal{Y},H,x_{1},\\rho_{c},p)$ where $\\mathcal{C}$ is the context space, $\\mathcal{X}$ is the state space, $\\boldsymbol{\\wp}$ is the action space, $H$ is the horizon, $x_{1}\\in\\mathcal{X}$ is the initial state, $\\rho_{c}\\in\\Delta_{\\mathcal{C}}$ is the context distribution and $p:\\mathcal{C}\\times\\mathcal{X}\\times\\mathcal{Y}\\to\\Delta_{\\mathcal{X}}$ is the transition function such that $p(x^{\\prime}\\mid c,x,y)$ is the probability to transition to state $x^{\\prime}$ after taking action $y$ in state $x$ , given context $c$ . ", "page_idx": 2}, {"type": "text", "text": "An interaction between the agent and the CMDP environment proceeds in $H$ steps. First, a context $c\\in\\mathcal{C}$ is sampled from $\\rho_{c}$ , and then the agent begins in the initial state $x_{1}$ . In step $h\\in[H]$ , the agent observes the current state $x_{h}\\,\\in\\,{\\mathcal{X}}$ , picks an action $y_{h}\\,\\in\\,\\mathcal{V}$ and transitions to the next state $x_{h+1}$ sampled from the transition function $p\\big(\\cdot\\,\\big|\\;c,x_{h},y_{h}\\big)$ . At the end of the interaction, the agent arrives in a final state $x_{H+1}$ . For simplicity, we assume that the state space can be decomposed into $H+1$ disjoint subsets $\\mathcal{X}=\\left\\{\\cdot\\right\\}_{h=1}^{H+1}X_{h}$ such that, in step $h$ of the interaction, the agent is in some state $x_{h}\\in\\mathcal{X}_{h}$ . A policy $\\pi:{\\mathcal{C}}\\times{\\mathcal{X}}\\to\\Delta_{\\mathcal{Y}}$ is a mapping from a context and state to a distribution over actions. Together with transition $p,\\,\\pi$ induces a distribution over trajectories denoted by $\\operatorname{Pr}_{\\pi,p}[\\cdot]$ (and $\\mathbb{E}_{\\pi,p}[\\cdot]$ for the expectation), in which the trajectory is generated by sampling the actions according to the policy and next states according to the environment. ", "page_idx": 2}, {"type": "text", "text": "2.1 Single-turn Reinforcement Learning from Human Feedback ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unlike standard RL where the agent observes reward feedback for its actions, the influential work of Christiano et al. [2017] suggests to leverage preference data. In the single-turn setting, the agent generates a single sequence $y\\in\\mathcal{V}$ given a context $c\\in\\mathcal{C}$ . This is modeled as a Contextual MultiArmed Bandit (CMAB), which is a CMDP instance with horizon $H=1$ . The feedback is given in the form of preference between two generated sequences. Formally, there exists a preference model $\\mathcal{P}:\\mathcal{C}\\times\\bar{\\mathcal{V}}\\times\\mathcal{Y}\\rightarrow[0,1]$ such that $\\mathcal{P}(y\\,\\succ\\,y^{\\prime}\\,\\bar{|}\\,c)$ gives the probability that $y$ is preferred over $y^{\\prime}$ given context $c$ . Preferences naturally extend to policies via expectation ${\\mathcal{P}}(\\pi>\\pi^{\\prime}\\mid c)=$ $\\mathbb{E}_{y\\sim\\pi(\\cdot|c),y^{\\prime}\\sim\\pi^{\\prime}(\\cdot|c)}[\\mathcal{P}(y\\succ y^{\\prime}\\mid c)]$ . ", "page_idx": 2}, {"type": "text", "text": "Reinforcement Learning from Human Feedback. In RLHF, it is assumed that there is a hidden reward function $r:\\mathcal{C}\\times\\mathcal{Y}\\to\\mathbb{R}$ that defines the preferences through the Bradely-Terry (BT) model, i.e., $\\mathcal{P}(y\\succ y^{\\prime}\\mid c)=\\sigma(r(c,y)-r(c,y^{\\prime}))$ , where $\\sigma$ is the sigmoid. To reconstruct the reward, the RL algorithm is used to optimize the ELO score of the chosen action $y$ using a cross-entropy loss. This technique was adapted to RL fine-tuning LLMs [Ziegler et al., 2019], and has become the standard approach for aligning LLMs to human feedback [Stiennon et al., 2020, Rafailov et al., 2023]. ", "page_idx": 3}, {"type": "text", "text": "Learning from direct preferences. Recently Munos et al. [2023], Azar et al. [2023] suggested to drop the BT assumption, and learn a direct preference model instead of reward. Munos et al. [2023] propose the Nash-MD algorithm which converges to the Nash equilibrium of a (regularized) preference model, i.e., a policy which is preferred over any other policy. In iteration $t+1$ , Nash-MD updates its policy $\\pi_{t+1}$ using a mirror descent (MD) step projected to a geometric mixture policy. The mixture policy $\\pi_{t}^{\\alpha}(\\cdot\\mid c\\overline{{{\\big)}}}\\propto\\pi_{t}(\\cdot\\mid c)^{1-\\alpha\\eta_{t}}\\mu(\\cdot\\mid c)^{\\alpha\\eta_{t}^{}}$ interpolates between the policy $\\pi_{t}$ and a reference policy $\\mu$ , given a regularization coefficient $\\alpha>0$ . Formally, for learning rate $\\eta_{t}>0$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{t+1}\\big(\\cdot\\;\\big|\\;c\\big)=\\arg\\operatorname*{max}_{\\pi(\\cdot|c)\\in\\Delta_{y}}\\eta_{t}\\mathcal{P}\\big(\\pi\\times\\pi_{t}^{\\alpha}\\;\\big|\\;c\\big)-\\mathrm{KL}\\big(\\pi\\|\\pi_{t}^{\\alpha}\\big)\\big[c\\big]\\qquad\\forall c\\in\\mathcal{C},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Multi-turn Preference-Based RL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the multi-turn setting, the agent repeatedly interacts with an external environment, an interaction we formulate using the CMDP model. Similarly to the single-turn case, we consider preferencebased RL, where the feedback is given as preference instead of reward. However, in our case, we assume preferences are between final CMDP states with a shared initial context. Formally, there exists a preference model $\\mathcal{P}:\\mathcal{C}\\times\\mathcal{X}_{H+1}\\times\\mathcal{X}_{H+1}\\rightarrow[0,1]$ such that $\\mathcal{P}(x_{H+1}\\succ x_{H+1}^{\\prime}\\mid c)$ gives the probability that $x_{H+1}$ is preferred over $x_{H+1}^{\\prime}$ given context $c$ . That is, in order to receive feedback, a learning algorithm performs two interactions with the environment and observes a Bernoulli sample for which one is preferred. We follow the natural assumption of Munos et al. [2023] that the preference model is symmetric, i.e., $\\mathcal{P}(x_{H+1}^{\\prime}\\succ x_{H+1}\\mid c)=\\bar{1}-\\mathcal{P}\\big(x_{H+1}\\succ x_{H+1}^{\\prime}\\mid c\\big)$ . We define the preference between a final state and a policy by $\\mathcal{P}(x\\succ\\pi\\mid c)=\\mathbb{E}_{\\pi,p}[\\mathcal{P}(x\\succ x_{H+1}\\mid c)]$ . Similarly, $\\mathcal{P}(\\pi\\succ\\pi^{\\prime}\\mid c)=\\mathbb{E}_{\\pi,p}[\\mathcal{P}(x_{H+1}\\succ\\pi^{\\prime}\\mid c)]$ . For brevity, since contexts are independent, we omit the context throughout the rest of the paper. Similarly to Munos et al. [2023], our objective is to find a policy $\\pi^{\\star}$ which is preferred over any other alternative policy, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi^{\\star}\\in\\arg\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\pi^{\\prime}}\\mathcal{P}(\\pi\\succ\\pi^{\\prime}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is a Nash equilibrium in the above two-player game defined by the preference model, following the minimax theorem [Von Neumann, 1928] (see Lemma 3.2). Notably, due to the anti-symmetric nature of the preference objective, the Nash equilibrium will have both agents following the same policy, and thus can be expressed as a single policy. ", "page_idx": 3}, {"type": "text", "text": "Regularized preference model. In the rest of the paper, we will consider a regularized version of the preference model. This is motivated by practical RLHF algorithms [Stiennon et al., 2020], and generalizes the single-turn model in Munos et al. [2023]. Let $\\mu$ be a reference policy, and define the $\\alpha$ -regularized preference model as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\alpha}(\\pi\\succ\\pi^{\\prime})=\\mathcal{P}(\\pi\\succ\\pi^{\\prime})-\\alpha\\mathrm{KL}_{p}(\\pi\\|\\mu)+\\alpha\\mathrm{KL}_{p}(\\pi^{\\prime}\\|\\mu),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{KL}_{p}(\\cdot\\|\\cdot)$ is the KL-divergence between the distributions that the policies induce over trajectories in the CMDP. We prove the following two results for the regularized preference model. First, its KL term has a value difference-like decomposition into the KL-divergences at individual states. Second, it has a unique Nash equilibrium (proofs in Appendix A). ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1. Let $\\pi,\\pi^{\\prime}$ be two policies, then: $\\begin{array}{r}{\\mathrm{KL}_{p}(\\pi||\\pi^{\\prime})=\\mathbb{E}_{\\pi,p}\\bigl[\\sum_{h=1}^{H}\\mathrm{KL}(\\pi||\\pi^{\\prime})[x_{h}]\\bigr].}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.2. There exists a unique Nash equilibrium of the regularized preference model $\\mathcal{P}_{\\alpha}$ . ", "page_idx": 3}, {"type": "text", "text": "Trajectory-wise vs. turn-wise preference feedback. A naive adaptation of single-turn RLHF to the multi-turn scenario would treat each turn as a separate single-turn problem. This would require feedback for the preference between two actions in each turn. Instead, in the setting we consider, the preference feedback is only between two full trajectories. Note that a single feedback for the entire trajectory is much more natural when considering conversations, since only the full conversations tell whether the objective was reached. Moreover, collecting preference data for intermediate actions could lead to destructive biases because the quality of an action can change dramatically depending on the actions taken later in the trajectory. For example, a chatbot directly answering a user query is usually a required behavior. Yet, when the chatbot does not have sufficient information to respond well, asking the user for more details might be a better action. Consequently, it is very hard for a rater to know which of these actions is better without observing how the conversation unrolls, i.e., without observing the user\u2019s reaction to the chatbot\u2019s question, and how the chatbot\u2019s response changes given this reaction. This difference demonstrates the challenge of multi-turn RL as it requires planning ahead instead of myopic reward maximization, which is the approach for single-turn RL. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The multi-turn setting in LLMs. While we consider a general preference-based RL setup (through the CMDP model), our focus is on applying this framework to multi-turn language-based interactions. The action space $\\boldsymbol{\\wp}$ is a sequence of tokens of a vocabulary $\\nu$ , and the state space at step $h$ , $\\chi_{h}$ , is a sequence based on the past sequences. For example, in conversational dialogues, the state $x_{h}$ holds the whole dialogue up to the $h$ -th turn, the action $y_{h}$ is the current sequence generated by the agent, and the next-state is simply the concatenation of the conversation $x_{h}$ with the new $y_{h}$ and a next sequence sampled by the environment (the user\u2019s response). Alternatively, in the complex tool-use case, where an agent repeatedly interacts with different APIs, the current state includes the original user query and a summary of results from APIs so far, the action is a new API call or user-facing response, and next state is a new sequence summarizing previous state with the new API response. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.3 (Token-level application to the single-turn auto-regressive case). Notably, this formulation also captures the single-turn auto-regressive case. Clearly, this holds when considering only one turn, $H=1$ , but it ignores the token-level optimization done at each turn. Instead, we frame the auto-regressive problem by limiting the actions at each step to single vocabulary tokens, $\\mathcal{V}=\\mathcal{V}$ , and assuming a null deterministic environment $x_{h+1}$ is the concatenation of $x_{h}$ and $y_{h}$ ). Importantly, our results apply to the token-level, which is usually neglected when devising single-turn algorithms. ", "page_idx": 4}, {"type": "text", "text": "4 Algorithms for the multi-turn setting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Preference-based Q-function. Our algorithms rely on a fundamental concept in RL \u2013 the value and Q-functions. In reward-based RL, it is essential to define the value $V^{\\pi}:\\mathcal{X}\\rightarrow\\mathbb{R}$ as the expected reward when playing policy $\\pi$ starting in some state $x_{h}$ , i.e., $\\begin{array}{r}{V^{\\pi}(x_{h})=\\mathbb{E}_{\\pi,p}\\Big[\\sum_{h^{\\prime}=h}^{H}r\\big(x_{h^{\\prime}},y_{h^{\\prime}}\\big)\\mid x_{h}\\Big]}\\end{array}$ In the preference-based scenario, we argue that value functions remain a powerful tool, even though there is no reward to maximize. We define the following regularized preference-based value functions, which are key to our algorithm. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{Q_{\\alpha}^{\\pi,\\pi^{\\prime}}}(x_{h},y_{h})=\\mathbb{E}_{\\pi,p}\\Big[\\mathcal{P}(x_{H+1}\\succ\\pi^{\\prime})-\\alpha\\sum_{h^{\\prime}=h}^{H}\\mathrm{KL}(\\pi\\|\\mu)[x_{h^{\\prime}}]\\;\\mid x_{h},y_{h}\\Big],}\\\\ &{\\quad V_{\\alpha}^{\\pi,\\pi^{\\prime}}(x_{h})=\\mathbb{E}_{\\pi,p}\\Big[\\mathcal{P}(x_{H+1}\\succ\\pi^{\\prime})-\\alpha\\sum_{h^{\\prime}=h}^{H}\\mathrm{KL}(\\pi\\|\\mu)[x_{h^{\\prime}}]\\;\\mid x_{h}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "There are a few interesting points in the definition above. First, note that these values are functions of two policies $\\pi,\\pi^{\\prime}$ . This is because the quality of a policy $\\pi$ cannot be measured on its own, and must be compared to another policy $\\pi^{\\prime}$ . Second, while $\\pi$ starts at the state $x_{h}$ , the comparison policy $\\pi^{\\prime}$ starts its trajectory from the initial state. This is a significant difference from the usual paradigm of Q-functions in RL and might seem peculiar at first glance. However, this formulation captures the fact that the optimal policy in a state should be preferred not only over any other policy along the sub-tree starting from this state, but also over any other policy, even ones that do not pass through this state at all. Although different in concept, the following lemma shows that our preference-based Q-function satisfies a value difference lemma, allowing us to optimize the policy locally in order to maximize our global objective (proof in Appendix B). ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.1. Let $\\pi,\\pi^{\\prime},\\bar{\\pi}$ be policies, then the following value difference lemma holds: ", "page_idx": 4}, {"type": "text", "text": "$\\begin{array}{r l}&{\\mathsf{D}_{\\alpha}\\big(\\pi\\succ\\bar{\\pi}\\big)-\\mathcal{P}_{\\alpha}\\big(\\pi^{\\prime}\\succ\\bar{\\pi}\\big)=\\mathbb{E}_{\\pi^{\\prime},p}\\Big[\\!\\sum_{h=1}^{H}\\langle\\pi-\\pi^{\\prime},Q_{\\alpha}^{\\pi,\\bar{\\pi}}\\rangle[x_{h}]+\\alpha\\mathrm{KL}(\\pi^{\\prime}||\\mu)[x_{h}]-\\alpha\\mathrm{KL}(\\pi||\\mu)[x_{h}]\\Big],}\\end{array}$ where $\\langle\\pi-\\pi^{\\prime},Q\\rangle[x]\\triangleq\\langle\\pi(\\cdot\\mid x)-\\pi^{\\prime}(\\cdot\\mid x),Q(x,\\cdot)\\rangle$ and $\\begin{array}{r}{\\langle x,y\\rangle=\\sum_{i}x(i)y(i)}\\end{array}$ is the inner product. ", "page_idx": 4}, {"type": "text", "text": "MTPO. We present the MTPO (Multi-turn Preference Optimization) algorithm, which provably solves the multi-turn preference-based RL objective. Formally, we prove MTPO converges to the unique Nash equilibrium of the regularized preference model. MTPO is based on two key principles: First, the regularized preference model defines a two-player anti-symmetric constant-sum game which can be solved using a self-play mirror descent method [Munos et al., 2023, Calandriello et al., 2024]. Second, our introduced Q-function allows to reduce the (global) optimization of the game into local mirror descent optimization problems in each state. Together they yield the MTPO update rule for iteration $(t+1)$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{t+1}\\big(\\cdot\\,\\big|\\;x_{h}\\big)=\\arg\\operatorname*{max}_{\\pi}\\eta_{t}\\langle\\pi,Q_{\\alpha}^{\\pi_{t},\\pi_{t}}\\rangle\\big[x_{h}\\big]-\\alpha\\eta_{t}\\mathrm{KL}(\\pi\\|\\mu)\\big[x_{h}\\big]-(1-\\alpha\\eta_{t})\\mathrm{KL}(\\pi\\|\\pi_{t})\\big[x_{h}\\big],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta_{t}$ is a learning rate. The solution can be made explicit in the following form (Appendix E.2): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{t+1}(y_{h}\\mid x_{h})\\propto\\mu(y_{h}\\mid x_{h})^{\\alpha\\eta_{t}}\\pi_{t}(y_{h}\\mid x_{h})^{1-\\alpha\\eta_{t}}e^{\\eta_{t}Q_{\\alpha}^{\\pi_{t},\\pi_{t}}(x_{h},y_{h})}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The intuition behind the algorithm is observed nicely in this update rule \u2013 we improve the current policy in the direction of the regularized preference against itself (represented by the self-play $\\mathrm{Q}-$ function), while not deviating too much and keeping close to the reference policy. The following is our main theoretical result: last-iterate convergence to Nash equilibrium (proof in Appendix B). ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. Let $\\pi_{\\alpha}^{\\star}$ be the Nash equilibrium of the regularized preference model, and $\\mathbb{Q}$ be a bound on the magnitude of the $Q$ -functions. Then, for $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\alpha(t+2)}}\\end{array}$ , MTPO guarantees at every iteration $t_{\\perp}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t})\\leq\\frac{32H\\mathbb{Q}^{2}}{\\alpha^{2}(t+1)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $\\mu_{\\mathrm{min}}$ be the minimal non-zero probability assigned by $\\mu_{\\scriptscriptstyle-}$ , then $\\begin{array}{r}{\\mathbb{Q}\\leq\\operatorname*{max}\\{4\\alpha H\\log\\frac{1}{\\mu_{\\operatorname*{min}}},1\\}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof sketch. By Lemma 3.1, the global $\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star}\\big|\\big|\\pi_{t+1}\\big)$ can be decomposed to the local $\\mathrm{KL}$ in each state $x_{h}$ , $\\mathrm{KL}(\\pi_{\\alpha}^{\\star}||\\pi_{t+1})[x_{h}]$ . Then, we use MD analysis in each state to bound the local KL as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\pi_{\\alpha}^{\\star}\\|\\pi_{t+1})[x_{h}]\\le(1-\\eta_{t}\\alpha)\\mathrm{KL}(\\pi_{\\alpha}^{\\star}\\|\\pi_{t})[x_{h}]+2\\eta_{t}^{2}\\mathbb{Q}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\eta_{t}\\big(\\langle\\pi_{t}-\\pi_{\\alpha}^{\\star},Q_{\\alpha}^{\\pi_{t},\\pi_{t}}\\rangle[x_{h}]+\\alpha\\mathrm{KL}(\\pi_{\\alpha}^{\\star}\\|\\mu)[x_{h}]-\\alpha\\mathrm{KL}(\\pi_{t}\\|\\mu)[x_{h}]\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "giving a recursive guarantee dependent on the local one-step regularized advantage of the current policy against the Nash policy and an additional term bounded by $\\mathbb{Q}$ . We plug this local bound into the KL decomposition (Lemma 3.1), which gathers the local KL terms back to $\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t})$ . Importantly, the value difference lemma (Lemma 4.1) aggregates the advantage terms to the global regularized preference $\\mathcal{P}_{\\alpha}(\\pi_{t}>\\pi_{t})-\\mathcal{P}_{\\alpha}(\\pi_{\\alpha}^{\\star}>\\pi_{t})$ , which is non-positive by the optimality of $\\pi_{\\alpha}^{\\star}$ . This leaves us with the global recursive bound: $\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t+1})\\leq\\hat{(1-\\eta_{t}\\alpha)}\\bar{\\mathrm{KL}}_{p}(\\bar{\\pi_{\\alpha}^{\\star}}||\\pi_{t})+\\bar{2}\\eta_{t}^{2}\\mathbb{Q}^{2}$ . We conclude by unrolling the recursion with the chosen $\\eta_{t}$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "MTPO with mixture policy. Inspired by Nash-MD [Munos et al., 2023], we present a variant of MTPO which makes use of the mixture policy $\\pi_{t}^{\\alpha}(\\cdot\\mid x)\\,\\propto\\,\\pi_{t}(\\cdot\\mid x)^{1-\\alpha\\eta_{t}}\\bar{\\mu(\\cdot\\mid x)^{\\alpha\\eta_{t}}}$ . This variant, which we call MTPO- $\\tau$ (where $\\tau$ will be the mixing coefficient in our experiments), gives similar theoretical guarantees (see Theorem B.2 in Appendix B) and performs better in practice (see Section 6). In fact, the following MTPO- $\\tau$ update rule is almost equivalent to MTPO (Equation (1)) with the only difference being the policies that define the Q-function, $Q_{\\alpha}^{\\pi_{t},\\pi_{t}}$ vs. $Q_{\\alpha}^{\\pi_{t}^{\\alpha},\\pi_{t}^{\\alpha}}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{t+1}\\big(\\cdot\\;\\big|\\;x_{h}\\big)=\\arg\\operatorname*{max}_{\\pi}\\eta_{t}\\Big\\langle\\pi,Q_{\\alpha}^{\\pi_{t}^{\\alpha},\\pi_{t}^{\\alpha}}\\Big\\rangle\\big[x_{h}\\big]-\\mathrm{KL}\\big(\\pi\\|\\pi_{t}^{\\alpha}\\big)[x_{h}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "MTPO- $\\tau$ naturally extends Nash-MD to the multi-turn setting, and reveals that Nash-MD is a self-play algorithm itself, but plays $\\pi_{t}^{\\alpha}$ instead of $\\pi_{t}$ . Practically, MTPO has the computational advantage over MTPO- $\\tau$ (and Nash-MD) of not keeping the additional policy $\\pi_{t}^{\\alpha}$ . Moreover, MTPO avoids the difficulty of computing the geometric mixture, which Munos et al. [2023] approximate heuristically via linear interpolation between the logits of the two policies. ", "page_idx": 5}, {"type": "text", "text": "Multi-turn RLHF. While we focused so far on our preference-based algorithms, our derivation holds for any online reward function since it is built on the mirror-descent method. Specifically, in the case of multi-turn RLHF, we consider the reward function $r^{\\mathrm{RLHF}}$ learned from preference data using the Bradley-Terry model, and define the corresponding regularized Q-function $Q_{\\alpha}^{\\pi,\\mathrm{RLHF}}(x_{h},y_{h})\\stackrel{\\smile}{=}$ $\\begin{array}{r}{\\mathbb{E}_{\\pi,p}\\!\\left[r^{\\mathrm{RLHF}}(x_{H+1})-\\alpha\\sum_{h^{\\prime}=h}^{H}\\mathrm{KL}(\\pi||\\mu)[x_{h^{\\prime}}]\\enspace|\\;x_{h},y_{h}\\right]\\!,}\\end{array}$ . By replacing $Q_{\\alpha}^{\\pi_{t},\\pi_{t}}$ in Equation (1) with $Q_{\\alpha}^{\\pi_{t},\\mathrm{RLHF}}$ , we obtain the multi-turn RLHF algorithm that converges to the regular RLHF objective \u2013 the optimal regularized policy w.r.t. the reward $r^{\\mathrm{RLHF}}$ (see Theorem B.6 in Appendix B). This complementary contribution emphasizes the similarity and difference between the RLHF and MTPO algorithms: The optimization process is identical for both methods, with the exception that the RLHF reward is fixed and computed w.r.t. the data policy, whereas preference-based MTPO uses an adaptive self-play mechanism to compute preferences w.r.t. the current policy. ", "page_idx": 6}, {"type": "text", "text": "4.1 Deep RL implementation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our deep RL implementation is a natural adaptation of the tabular algorithms presented in the previous section. At each iteration, training data is acquired by sampling a batch of contexts from the data, and using each context to sample two trajectories with the current policy $\\pi_{\\theta_{t}}$ . Then, the final states of both trajectories serve as inputs to a direct preference model that outputs the probability of one being preferred over the other. Similarly to the way the preference model is trained in Nash-MD [Munos et al., 2023], this preference model is trained in advance on the available offline preference data. ", "page_idx": 6}, {"type": "text", "text": "The update rule in Equation (1) relies on the $Q$ -function of the current policy. We therefore use an actor-critic policy optimization based approach and train two models, a policy $\\pi_{\\theta}$ , and its value $V_{\\alpha,\\phi}^{\\pi_{\\theta_{t}},\\pi_{\\theta_{t}}}$ , which is typically used to estimate the advantage, $A_{\\alpha}^{\\pi,\\pi^{\\prime}}(x,y)\\triangleq Q_{\\alpha}^{\\pi,\\pi^{\\prime}}(x,y)-V_{\\alpha}^{\\pi,\\pi^{\\prime}}(x)$ [Schulman et al., 2017]. For simplicity and computational efficiency, we implement a policy-gradient (PG) based approach and ignore the MD stability term $\\mathrm{KL}(\\pi_{\\theta_{t}}||\\pi_{\\theta_{t-1}}^{\\dot{\\ }})$ , similarly to the implementation of the Nash-MD algorithm. We justify this simplification with the fact that the KL regularization w.r.t. the fixed reference policy $\\mu$ already provides stability to our online algorithm, somewhat similarly to the way the Follow-The-Regularized-Leader (FTRL; Orabona [2019]) algorithm operates. Nevertheless, we believe that this additional MD penalty should contribute to the performance and stability of the algorithm, as shown in [Tomar et al., 2020], and we leave this for further research. This yields the following losses, when action $y$ is played at state $x$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{policy}}(\\theta;x,y)=-\\hat{A}_{\\alpha}^{\\pi_{\\theta_{t}},\\pi_{\\theta_{t}}}(x,y)\\log\\pi_{\\theta}(y\\mid x)+\\alpha\\mathrm{KL}(\\pi_{\\theta}\\|\\mu)[x],}\\\\ &{\\mathcal{L}_{\\mathrm{value}}(\\phi;x)\\quad=\\Big(\\hat{V}_{\\alpha}^{\\pi_{\\theta_{t}},\\pi_{\\theta_{t}}}(x)-V_{\\alpha,\\phi}^{\\pi_{\\theta_{t}},\\pi_{\\theta_{t}}}(x)\\Big)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{V}_{\\alpha}^{\\pi_{\\theta_{t}},\\pi_{\\theta_{t}}},\\hat{A}_{\\alpha}^{\\pi_{\\theta_{t}},\\pi_{\\theta_{t}}}$ , are estimations of the current value and advantage using Generalized Advantage Estimation (GAE, Schulman et al. [2017]). We also batch-normalize the value-loss and advantage as recommended in [Andrychowicz et al., 2020]. Finally, when the policy is an autoregressive language model, which generates actions token-by-token until an end-of-sequence signal is generated, we use a turn-level value (and not a token-level value as done in [Stiennon et al., 2020]). That is, the value model gets as input a state represented by a sequence of tokens, and outputs a single scalar value instead of a scalar value for each token in the action sequence. This is justified by our analysis which treats whole turns as single actions. We leave the many ways to combine turn-level and token-level values for future research. ", "page_idx": 6}, {"type": "text", "text": "5 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section describes the domains and models used in our experiments. To create online environments suited for multi-turn RL, we mimic the RLHF process [Stiennon et al., 2020], replacing the human parts with prompted state-of-the-art LLMs, similarly to Abdulhai et al. [2023] (see Figure 1): ", "page_idx": 6}, {"type": "text", "text": "1. Dataset creation: First, we devise a story-line for the user and the environment, describing their characters and goals. Then, we generate a dataset by prompting a state-of-the-art LLM such as Gemini [Team et al., 2023] or GPT [Brown et al., 2020] with the story-line. When generating data, a full conversation is sampled at once, meaning that both the agent and environment are generated together to make them more consistent. Furthermore, to create a diverse set of conversations, we devise a diverse list of attributes for both the agent and environment, sample attributes out of the list, and pass it to the generation prompt. ", "page_idx": 6}, {"type": "image", "img_path": "rVSc3HIZS4/tmp/66ed21b9c45c3656921184d0f61bff93ae29999482d7a6f53934576828f3c429.jpg", "img_caption": ["Figure 1: Education Dialogue data generation process. Top: prompt used to generate conversation with Gemini. Middle: conversations sampled from the the interaction between the teacher and student models. Bottom: prompt used for the preference oracle. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "2. Environment preparation: Once the data is curated, we use it to fine-tune two smaller LLMs, one for the agent and one for the environment, using teacher forcing. ", "page_idx": 7}, {"type": "text", "text": "3. Preference/reward learning: We prepare preference data by sampling pairs of conversations from the agent and environment models. To label the data, we prompt a high-capacity LLM with either instructions on how to score a conversation, or criteria for preferring a conversation over another. The data is used to fine-tune two smaller LLMs: an RLHF reward model (with BT loss), and a preference model (with probability regression loss). ", "page_idx": 7}, {"type": "text", "text": "We experiment with two domains, preference-based Education Dialogue and reward-based Car Dealer: ", "page_idx": 7}, {"type": "text", "text": "Education Dialogue. The core of our approach is learning when there is no clear reward, instead only (human) preferences can be acquired. To validate our approach in this scenario, we created a novel multi-turn task for evaluating algorithms based on preference data. In this scenario, which we term Education Dialogue, a teacher (agent) is faced with the task of teaching a student (environment) a given topic in the best means possible. We follow the dataset creation procedure and prompt Gemini Ultra [Google, 2024] to create such interactions between the teacher and student. The teacher is prompted with a learning topic in science, history, etc. The student is prompted with the characteristics of its learning habits, e.g., prefers interactive learning, lecture-based learning or hands-on activities. The preference model is prompted with instructions that define a good learning interaction. For reproducibility, and to further advance the research of the multi-turn setting, we openly release the data and prompts used to create this new benchmark.1For more details, see Appendix C and the example in Figure 1. ", "page_idx": 7}, {"type": "table", "img_path": "rVSc3HIZS4/tmp/2e778e8a613d28ea89e1bb583bf29d9e10cc7c1fa1861da1c483b3c0cb0742ab.jpg", "table_caption": ["Table 1: Side-by-side evaluation for Education Dialogue using Flan-T5 XL as the prompted preference model. Each entry is the average preference of 1,600 conversations generated with row method $y$ , over ones generated with column method $y^{\\prime}$ . We evaluate each method using 3 different seeds, compute 3 $\\times\\,3$ comparisons matrix and report the mean (the standard deviation is reported in Appendix D). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Car Dealer. In this LMRL-Gym [Abdulhai et al., 2023] domain, a car dealer is assigned with the task of selling a car at the highest price to a customer. We skip the data creation step, and directly use the Car Dealer published data to fine-tune the dealer (agent) and customer (environment) T5-large models. The reward is calculated by prompting a Flan-T5 model to extract the sale price from the conversation, whenever a sale has occurred. When using a preference-based algorithm, the preference of one trajectory over the other is computed using the BT model with the rewards of the two trajectories. ", "page_idx": 8}, {"type": "text", "text": "Single-turn baselines. The key hypothesis of this work is that conversation-level signals are preferred over single-turn signals for optimizing multi-turn trajectories. To verify this in the Education Dialogue domain, we devise two single-turn baselines by sampling data where each conversation turn has two different policy responses. The first baseline, called single-turn-reward, rates the two responses using a modified preference prompt (see Appendix C), in which the model is asked to evaluate the responses by their effect on the overall conversation. This technique is prevalent when human raters are asked to evaluate multi-turn data. The second baseline, called single-turn-value, assumes access to a Monte-Carlo estimate of the value: it uses our original preference prompt (see Figure 1) by continuing the trajectories of both possibilities and then calculating the preference in the end. For both baselines, we train an RLHF algorithm and a preference-based Nash-MD algorithm. ", "page_idx": 8}, {"type": "text", "text": "Models. The agent and environment are modeled with T5 encoder-decoder models. Specifically, we use the T5-large (770M) and T5-XL (3B) models. The same models are used for the RLHF BT-based reward and preference-based models. For prompted reward/preference models, we make use of the Flan-T5 XL (3B) [Chung et al., 2024]. For training, we use a configuration of $4\\times4$ Tensor Processing Units (TPUs; Jouppi et al. [2023]) which typically yields 0.1 training steps per second, where a step consists of learning a 10-turn episode. A detailed list of hyperparameters is found in Appendix D. We run each evaluation on 1600 random samples from an independent evaluation set. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section we evaluate the algorithms proposed in Section 4. We start with the preference-based Education Dialogue environment (see Section 5), and compare our multi-turn algorithms to SFT (supervised fine-tuning) as well as single-turn baselines. We note that, unlike single-turn benchmarks which are based on data with real human preferences, our golden preference data itself is generated by an LLM (Gemini Ultra). Therefore, the true goal in our curated environment is to align the model with the preference of this highly capable LLM rather than a human rater. While human evaluation is always interesting, here it is actually only a proxy to alignment with the data distribution. To efficiently validate our models, we start with a thorough comparison between our baselines and candidates using a prompted Flan-T5 XL model as a judge, which was verified to correlate with the high-capacity Gemini Ultra (Table 1). We then compare our best candidates using the same Gemini Ultra which generated the preference alignment feedback (Table 2). ", "page_idx": 8}, {"type": "table", "img_path": "rVSc3HIZS4/tmp/9025e9459dae8fd46ef8b02279919937c84ab762b135f3e43540ba957755a921.jpg", "table_caption": ["Table 2: Side-by-side evaluation for Education Dialogue using Gemini Ultra as the prompted preference model. Each entry is the average preference of 1,000 conversations generated with row method $y$ , over ones generated with column method $y^{\\prime}$ . "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "rVSc3HIZS4/tmp/8e1b45dcdcf71eb4f466b6fceb4582e5bb991e48a4d6ec70ac9acf9d0326225b.jpg", "table_caption": ["Table 3: Car Dealer experiments averaged across 5 seeds and reported with $95\\%$ confidence interval. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Multi vs. single turn. Tables 1 and 2 show that all multi-turn algorithms (MTPO and multiturn RLHF) with conversation-level feedback significantly outperform the single-turn baselines, validating our hypothesis. We conjecture that it is attributed to several factors: First, the effect of a single-decision on the whole conversation is hard to capture, causing highly inaccurate single-turn reward/preference models. Notably, this leads to inferior performance of Nash-MD compared to single-turn RLHF, since it optimizes to find Nash equilibrium of this inaccurate model while RLHF does not stray so far from the reference. Second, even if one could estimate the current policy\u2019s value, this estimate becomes biased when the policy changes during training. Finally, single-turn preferences consider only \u201clocal\u201d decisions which share the same conversational path, and not how these decisions \u201cglobally\u201d compare to other possible paths, as captured by the preference-based Q-function $Q_{\\alpha}^{\\pi_{t},\\pi_{t}}$ (see Section 4). ", "page_idx": 9}, {"type": "text", "text": "MTPO vs. multi-turn RLHF. Comparing our three multi-turn algorithms, we see two main results. First, the two variants of MTPO outperform multi-turn RLHF. This is expected since the environment is not reward-based, and hence it extends the results of [Munos et al., 2023, Calandriello et al., 2024] from the single-turn case, and supports the theoretical claim that MTPO converges to the Nash policy while multi-turn RLHF converges to the optimal policy w.r.t the learned reward (which is based only on the reference policy). Second, MTPO- $\\tau$ outperforms MTPO. While both algorithms converge to the same Nash equilibrium, we conjecture that the superior performance of MTPO- $\\tau$ stems from the stochasticity that the mixture policy $\\pi_{t}^{\\alpha}$ introduces. Namely, $\\pi_{t}$ might tend towards deterministic behavior, causing less informative feedback from self-play, as the two sampled trajectories would be very similar. On the other hand, $\\pi_{t}^{\\alpha}$ is more stochastic, providing diversity in the sampled trajectories. ", "page_idx": 9}, {"type": "text", "text": "Reward-based environment. In an additional experiment, we test MTPO and multi-turn RLHF in the reward-based Car Dealer environment, where the goal is maximizing sale price (see Section 5). We compare a standard policy-gradient RL algorithm against our algorithms in two scenarios: an online scenario where the reward or preference feedback is given using an online oracle, and an RLHF-like setting, where we first create preference data using the oracle, and then use it to fine-tune a (BT) reward and preference models. Table 3 shows that even though MTPO receives preferences instead of the explicit optimization target (rewards), it still learns as good as RL. Interestingly, MTPO recovers a slightly higher reward than multi-turn RLHF despite the fact the true preferences are sampled from a BT model. This may imply that a preference model generalizes better than a BT-reward model, perhaps because it is independent of the sampling policy. ", "page_idx": 9}, {"type": "text", "text": "Limitations. This work presents a proof of concept for the potential of MTPO to improve existing single-turn techniques. Our experimental setup might be limited by the relatively small T5-based models and the use of prompt-based environments. We leave applications to state-of-the-art models and algorithms, and more realistic environments to future work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint arXiv:2311.18232, 2023. ", "page_idx": 10}, {"type": "text", "text": "Charalambos D. Aliprantis and Kim C. Border. Infinite Dimensional Analysis: a Hitchhiker\u2019s Guide. Springer, Berlin; London, 2006. ", "page_idx": 10}, {"type": "text", "text": "Marcin Andrychowicz, Anton Raichuk, Piotr Stan\u00b4czyk, Manu Orsini, Sertan Girgin, Raphael Marinier, L\u00e9onard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What matters in on-policy reinforcement learning? a large-scale empirical study. arXiv preprint arXiv:2006.05990, 2020. ", "page_idx": 10}, {"type": "text", "text": "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023. ", "page_idx": 10}, {"type": "text", "text": "Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R\u00e9mi Munos. A general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023.   \nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.   \nAmir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167\u2013175, 2003.   \nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nDaniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, et al. Human alignment of large language models through online preference optimisation. arXiv preprint arXiv:2403.08635, 2024.   \nAsaf Cassel, Haipeng Luo, Aviv Rosenberg, and Dmitry Sotnikov. Near-optimal regret in linear mdps with aggregate bandit feedback. arXiv preprint arXiv:2405.07637, 2024.   \nNiladri Chatterji, Aldo Pacchiano, Peter Bartlett, and Michael Jordan. On the theory of reinforcement learning with once-per-episode feedback. Advances in Neural Information Processing Systems, 34: 3401\u20133412, 2021.   \nXiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In International Conference on Machine Learning, pages 3773\u20133793. PMLR, 2022.   \nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1\u201353, 2024.   \nAlon Cohen, Haim Kaplan, Tomer Koren, and Yishay Mansour. Online markov decision processes with aggregate bandit feedback. In Mikhail Belkin and Samory Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 1301\u20131329. PMLR, 15\u201319 Aug 2021.   \nYonathan Efroni, Nadav Merlis, and Shie Mannor. Reinforcement learning with trajectory feedback. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.   \nKawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.   \nEyal Even-Dar, Sham M Kakade, and Yishay Mansour. Online markov decision processes. Mathematics of Operations Research, 34(3):726\u2013736, 2009.   \nMatthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In International Conference on Machine Learning, pages 2160\u20132169. PMLR, 2019.   \nGoogle. Gemini: A family of highly capable multimodal models, 2024.   \nDan Hendrycks, Christine Zhu, Mantas Mazeika, Jesus Navarro, Dawn Song, Andy Zou, Bo Li, Sahil Patel, and Jacob Steinhardt. What would jiminy cricket do? towards agents that behave morally. Advances in neural information processing systems, 2022.   \nRobert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, and William Beauchamp. Rewarding chatbots for real-world engagement with millions of users, 2023.   \nNorm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1\u201314, 2023.   \nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.   \nJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.   \nR\u00e9mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.   \nJohn F Nash et al. Non-cooperative games. 1950.   \nArkadij Semenovi\u02c7c Nemirovskij and David Borisovich Yudin. Problem Complexity and Method Efficiency in Optimization. A Wiley-Interscience publication. Wiley, 1983. ISBN 9780471103455. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "URL https://books.google.co.il/books?id=6ULvAAAAMAAJ. ", "page_idx": 11}, {"type": "text", "text": "Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213, 2019.   \nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u2013 27744, 2022.   \nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.   \nRafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From $r$ to $Q^{\\star}$ : Your language model is secretly a q-function. arXiv preprint arXiv:2404.12358, 2024.   \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \nAviv Rosenberg and Yishay Mansour. Online stochastic shortest path with bandit feedback and unknown transition function. In Advances in Neural Information Processing Systems, pages 2209\u20132218, 2019a.   \nAviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision processes. In International Conference on Machine Learning, pages 5478\u20135486. PMLR, 2019b.   \nAadirupa Saha, Aldo Pacchiano, and Jonathan Lee. Dueling rl: Reinforcement learning with trajectory preferences. In International Conference on Artificial Intelligence and Statistics, pages 6263\u20136289. PMLR, 2023.   \nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \nLior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5668\u20135675, 2020a.   \nLior Shani, Yonathan Efroni, Aviv Rosenberg, and Shie Mannor. Optimistic policy optimization with bandit feedback. In International Conference on Machine Learning, pages 8604\u20138613. PMLR, 2020b.   \nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.   \nMaurice Sion. On general minimax theorems. Pacific Journal of Mathematics, 8(1):171 \u2013 176, 1958.   \nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18990\u201318998, 2024.   \nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \nYunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, R\u00e9mi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo \u00c1vila Pires, and Bilal Piot. Generalized preference optimization: A unified approach to offline alignment, 2024.   \nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. ", "page_idx": 12}, {"type": "text", "text": "Manan Tomar, Lior Shani, Yonathan Efroni, and Mohammad Ghavamzadeh. Mirror descent policy optimization. arXiv preprint arXiv:2005.09814, 2020. ", "page_idx": 13}, {"type": "text", "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. ", "page_idx": 13}, {"type": "text", "text": "J Von Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295\u2013320, 1928. ", "page_idx": 13}, {"type": "text", "text": "Ruoyao Wang, Peter Jansen, Marc-Alexandre C\u00f4t\u00e9, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11279\u201311298, 2022.   \nYuanhao Wang, Qinghua Liu, and Chi Jin. Is RLHF more difficult than standard RL? a theoretical perspective. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nWei Wei, Quoc Le, Andrew Dai, and Jia Li. Airdialogue: An environment for goal-oriented dialogue research. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3844\u20133854, 2018.   \nChristian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F\u00fcrnkranz. A survey of preferencebased reinforcement learning methods. Journal of Machine Learning Research, 18(136):1\u201346, 2017.   \nRunzhe Wu and Wen Sun. Making rl with preference-based feedback efficient via randomization. arXiv preprint arXiv:2310.14554, 2023.   \nWenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline reinforcement learning with human feedback. arXiv preprint arXiv:2305.14816, 2023a.   \nWenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. How to query human feedback efficiently in rl? arXiv preprint arXiv:2305.18505, 2023b.   \nYao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations, 2022.   \nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Proofs for Section 3 16 ", "page_idx": 14}, {"type": "text", "text": "A.1 KL decomposition (proof of Lemma 3.1) . . . 16   \nA.2 Existence and uniqueness of the Nash equilibrium (proof of Lemma 3.2) . . 16 ", "page_idx": 14}, {"type": "text", "text": "B Proofs for section 4 19 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Regularized preference-based Q-function (proof of Lemma 4.1) . . . 19   \nB.2 Convergence of MTPO (Proof of Theorem 4.2) . . . . 19   \nB.3 MTPO with mixture policy (MTPO- $\\tau$ ) . . . . 20   \nB.4 Convergence of multi-turn RLHF . . . 21 ", "page_idx": 14}, {"type": "text", "text": "C The Education Dialogue environment 22 ", "page_idx": 14}, {"type": "text", "text": "C.1 Prompts for creating the environment . . 22   \nC.2 Examples of interactions . . 24 ", "page_idx": 14}, {"type": "text", "text": "D Hyperparameters and additional experimental results 27 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Hyperparameters . 27   \nD.2 Additional experimental results 27 ", "page_idx": 14}, {"type": "text", "text": "E Mirror descent policy optimization for regularized adversarial MDPs 28 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Model . . 28   \nE.2 Algorithm 1: mirror descent policy optimization . . . 30   \nE.3 Analysis of algorithm 1 . . . 30   \nE.4 Algorithm 2: mixture mirror descent policy optimization . . . 32   \nE.5 Analysis of algorithm 2 . . . . 32   \nE.6 Bounding the Q-function . . . 33 ", "page_idx": 14}, {"type": "text", "text": "A Proofs for Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 KL decomposition (proof of Lemma 3.1) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma (restatement of Lemma 3.1). Let $\\pi,\\pi^{\\prime}$ be two policies, then: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi||\\pi^{\\prime})=\\mathbb{E}_{\\pi,p}\\Bigg[\\sum_{h=1}^{H}\\mathrm{KL}(\\pi||\\pi^{\\prime})[x_{h}]\\Bigg].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. $\\mathrm{KL}_{p}(\\cdot\\|\\cdot)$ is defined as the KL-divergence between the distributions that the policies induce over trajectories in the MDP (denoted by $\\tau=\\left(x_{1},y_{1},\\dots,x_{H},y_{H},x_{H+1}\\right)$ , formally: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{KL}_{p}(\\pi||\\pi^{\\prime})=}&{\\sum_{\\tau,\\tau^{\\prime}}\\mathrm{Pr}[\\tau]\\log\\frac{\\mathrm{Pr}_{\\pi}}{\\mathrm{Pr}_{\\pi^{\\prime}}\\tau}[\\tau]}\\\\ &{=\\sum_{\\tau,\\tau^{\\prime},\\tau^{\\prime}}[\\tau]\\log\\frac{\\prod_{i=1}^{\\prime}\\pi(y_{i}|\\,x_{h})p(x_{h+1}\\,|\\,x_{h},y_{h})}{\\prod_{i=1}^{\\prime}\\pi^{\\prime}(y_{i}|\\,x_{h})p(x_{h+1}\\,|\\,x_{h},y_{h})}}\\\\ &{=\\sum_{\\tau}\\mathrm{Pr}[\\tau]\\log\\frac{H}{\\prod_{i=1}^{\\prime}\\pi^{\\prime}(y_{h}\\,|\\,x_{h})}}\\\\ &{=\\mathop{\\mathbb{E}}_{\\pi,\\tau}\\biggl[\\log\\frac{H}{\\prod_{i=1}^{\\prime}\\pi^{\\prime}(y_{h}\\,|\\,x_{h})}}\\\\ &{=\\mathop{\\mathbb{E}}_{\\pi,\\tau}\\biggl[\\log\\frac{H}{\\prod_{i=1}^{\\prime}\\pi^{\\prime}(y_{h}\\,|\\,x_{h})}\\biggr]}\\\\ &{=\\mathop{\\mathbb{E}}_{\\pi,\\tau}\\biggl[\\frac{H}{\\prod_{i=1}^{\\prime}\\log\\frac{\\pi}{\\tau^{\\prime}(y_{h}\\,|\\,x_{h})}}\\biggr]}\\\\ &{=\\mathop{\\mathbb{E}}_{\\pi,\\tau}\\biggl[\\underbrace{\\frac{H}{\\prod_{i=1}^{\\prime}\\log(\\tau/x_{h})}}_{\\mathrm{Cl}}[|\\pi_{\\tau}^{\\prime}(\\cdot\\,|\\,x_{h})|]\\biggr]}\\\\ &{=\\mathop{\\mathbb{E}}_{\\pi,\\tau}\\biggl[\\underbrace{\\sum_{k=1}^{\\infty}\\mathrm{K}(\\pi(\\cdot\\,|\\,x_{h})|\\,\\tau^{\\prime}(\\cdot\\,|\\,x_{h}))}_{\\mathrm{Cl}}\\biggr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.2 Existence and uniqueness of the Nash equilibrium (proof of Lemma 3.2) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma (restatement of Lemma 3.2). There exists a unique Nash equilibrium of the regularized preference model $\\mathcal{P}_{\\alpha}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. The existence of the Nash equilibrium is proved in Theorem A.1. In order to prove the uniqueness, we use the fact that, from Theorem 4.2, the algorithm MTPO produces a sequence of policies $\\pi_{t}$ that converges to any Nash equilibrium $\\pi^{*}$ , in the sense that $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\mathrm{KL}_{p}(\\pi^{*}||\\pi_{t})=0}\\end{array}$ From the definition of the KL-divergence between policies, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi^{*}\\|\\pi_{t})=\\mathbb{E}_{\\pi^{*},p}\\left[\\sum_{h=1}^{H}\\mathrm{KL}(\\pi^{*}\\|\\pi_{t})[x_{h}]\\right]=\\sum_{x\\in\\mathcal{X}}\\rho^{\\pi^{*}}(x)\\mathrm{KL}(\\pi^{*}\\|\\pi_{t})[x],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\rho^{\\pi^{*}}(x)$ is the probability to reach $x$ when following $\\pi^{*}$ . ", "page_idx": 15}, {"type": "text", "text": "Now, the fixed point of the MTPO dynamics (Equation (2)) shows that any Nash equilibrium satisfies, for any $x\\in X,y\\in\\mathcal{Y}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi^{*}(y\\mid x)\\propto\\mu(y\\mid x)e^{{\\frac{1}{\\alpha}}Q_{\\alpha}^{\\pi^{*},\\pi^{*}}(x,y)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, we notice that any Nash equilibrium has the same support as $\\mu$ , thus the set of reachable states under $\\mu$ is exactly the same set as the set of reachable states under any Nash equilibrium $\\pi^{*}$ . ", "page_idx": 15}, {"type": "text", "text": "So, from any reachable state $x\\in\\mathscr{X}$ (i.e., such that $\\rho^{\\mu}(x)>0,$ ), we have that the sequence $\\pi_{t}(\\cdot\\mid x)$ converges (in KL-divergence) to the Nash equilibrium $\\pi^{*}(\\cdot\\mid x)$ . Since a single sequence cannot converge to two different values, we have that the Nash equilibrium $\\pi^{*}(\\cdot\\mid x)$ is uniquely defined in that state. Since the behavior generated by a policy depends on the policy at the set of states that are reacheable only, and since we have seen that all Nash equilibria have the same set of reachable states, we deduce the uniqueness of the policy defined by a Nash equilibrium. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Theorem A.1. The game defined by the payoff function $(\\pi,\\pi^{\\prime})\\mapsto\\mathcal{P}_{\\alpha}(\\pi\\succ\\pi^{\\prime})$ has a Nash equilibrium. ", "page_idx": 15}, {"type": "text", "text": "Proof. First we prove that there exists (at least) one max-min policy $\\pi^{*}\\in\\arg\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\pi^{\\prime}}\\mathcal{P}_{\\alpha}(\\pi>$ $\\pi^{\\prime}$ ). ", "page_idx": 16}, {"type": "text", "text": "For any $\\pi$ , the map $\\pi^{\\prime}\\in\\mathcal{X}\\mapsto\\mathcal{P}_{\\alpha}(\\pi\\succ\\pi^{\\prime})$ is continuous, thus upper semi-continuous (u.s.c.). We know that the pointwise minimum of u.s.c. functions is also u.s.c. (see, e.g. Aliprantis and Border [2006], Lemma 2.41). Thus the function $\\pi\\in\\Pi\\mapsto\\operatorname*{min}_{\\pi^{\\prime}}\\mathcal{P}_{\\alpha}(\\pi\\succ\\pi^{\\prime})$ is also u.s.c. and since $\\Pi$ is compact, we can apply Theorem 2.43 of Aliprantis and Border [2006] to deduce that this function attains a maximum value in $\\Pi$ and that the set of maximizers is compact. Thus there exists (at least) one policy denoted by $\\begin{array}{r}{\\pi^{*}\\in\\arg\\operatorname*{max}_{\\pi\\in\\Pi}\\operatorname*{min}_{\\pi^{\\prime}\\in\\Pi}\\mathcal{P}_{\\alpha}(\\pi\\succ\\pi^{\\prime})}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Now from Lemma A.2 we know that the regularized preference model $\\mathcal{P}_{\\alpha}(\\pi>\\pi^{\\prime})$ defines a concaveconvexlike game. Also for any $\\pi^{\\prime}$ , the map $\\bar{\\pi}\\mapsto\\bar{\\mathcal P_{\\alpha}}\\bar{(}\\bar{\\pi}\\succ\\pi^{\\prime})$ is u.s.c. Thus we can apply the minimax Theorem 4.2 of Sion [1958] to deduce that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\pi^{\\prime}}\\mathcal{P}_{\\alpha}(\\pi>\\pi^{\\prime})=\\operatorname*{min}_{\\pi^{\\prime}}\\operatorname*{max}_{\\pi}\\mathcal{P}_{\\alpha}(\\pi>\\pi^{\\prime}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We deduce from ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{2}=\\operatorname*{max}_{\\pi}\\mathcal{P}_{\\alpha}(\\pi\\times\\pi)\\ge\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\pi^{\\prime}}\\mathcal{P}_{\\alpha}(\\pi\\times\\pi^{\\prime})=\\operatorname*{min}_{\\pi^{\\prime}}\\operatorname*{max}_{\\pi}\\mathcal{P}_{\\alpha}(\\pi\\times\\pi^{\\prime})\\ge\\operatorname*{min}_{\\pi^{\\prime}}\\mathcal{P}_{\\alpha}(\\pi^{\\prime}\\succ\\pi^{\\prime})=\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "that the value of the game is $1/2$ , and that $\\operatorname*{min}_{\\pi^{\\prime}}\\mathcal{P}_{\\alpha}(\\pi^{*}\\,>\\,\\pi^{\\prime})\\;=\\,1/2$ . Thus $(\\pi^{*},\\pi^{*})$ is a Nash equilibrium of the game defined by the regularized preference model $(\\pi,\\pi^{\\prime})\\mapsto\\mathscr{P}_{\\alpha}(\\pi>\\pi^{\\prime})$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma A.2. The mapping $(\\pi,\\pi^{\\prime})\\mapsto\\mathcal{P}_{\\alpha}(\\pi\\succ\\pi^{\\prime})$ is concave-convexlike, which, in the context of a symmetric preference model, means that for any couple of policies $(\\pi_{1},\\pi_{2})$ and any coefficient $c\\in[0,1],$ , there exists a policy $\\pi_{c}$ such that for any policy $\\pi^{\\prime}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nc\\mathcal{P}_{\\alpha}\\big(\\pi_{1}>\\pi^{\\prime}\\big)+\\big(1-c\\big)\\mathcal{P}_{\\alpha}\\big(\\pi_{2}>\\pi^{\\prime}\\big)\\leq\\mathcal{P}_{\\alpha}\\big(\\pi_{c}>\\pi^{\\prime}\\big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Let us define the notion of reach probability: for any state $x_{h}\\in\\mathcal{X}_{h}$ , let us write $\\rho^{\\pi}(x_{h})$ the probability to reach the specific state $x_{h}\\in\\mathcal{X}_{h}$ when following $\\pi\\colon\\rho^{\\pi}(x_{h})=\\operatorname*{Pr}_{\\pi,p}[x_{h}]$ . First notice we can represent the regularized preference $\\mathcal{P}_{\\alpha}(\\pi>\\pi^{\\prime})$ using reach probabilities: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{\\alpha}(\\pi\\succ\\pi^{\\prime})=\\displaystyle\\sum_{x_{H+1},x_{H+1}^{\\prime}\\in\\mathcal{X}_{H+1}}\\rho^{\\pi}(x_{H+1})\\rho^{\\pi^{\\prime}}(x_{H+1}^{\\prime})\\mathcal{P}(x_{H+1}\\succ x_{H+1}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\displaystyle\\sum_{h=1}^{H}\\sum_{x_{h}\\in\\mathcal{X}_{h}}\\rho^{\\pi}(x_{h})\\mathrm{KL}(\\pi||\\mu)[x_{h}]-\\rho^{\\pi^{\\prime}}(x_{h})\\mathrm{KL}(\\pi^{\\prime}||\\mu)[x_{h}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, consider two policies $\\pi_{1}$ and $\\pi_{2}$ and a coefficient $c\\in[0,1]$ . From Lemma A.3 we have that there exists a policy $\\pi_{c}$ such that for any $x_{h}$ , we have $\\rho^{\\pi_{c}}(x_{h})^{\\prime}\\!=c\\bar{\\rho}^{\\pi_{1}}(x_{h})+(1-c)\\rho^{\\pi_{2}}(x_{h})$ . We can write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{P}_{\\alpha}(\\overline{{\\pi}}_{c}>\\pi^{\\prime})=}&{\\displaystyle\\sum_{x_{H+1}}\\sum_{x_{H+1}<x_{H+1}}\\left[c\\rho^{\\pi_{1}}(x_{H+1})+(1-c)\\rho^{\\pi_{2}}(x_{H+1})\\right]\\rho^{\\pi^{\\prime}}(x_{H+1}^{\\prime})\\mathcal{P}(x=\\delta)}\\\\ &{\\quad\\quad\\quad-\\alpha\\displaystyle\\sum_{h=1}^{H}\\sum_{x_{H}<x_{H}}\\left[c\\rho^{\\pi_{1}}(x_{h})+(1-c)\\rho^{\\pi_{2}}(x_{h})\\right]\\mathrm{KL}(\\overline{{\\pi}}_{c}||\\mu)[x_{h}]}\\\\ &{\\quad\\quad\\quad+\\alpha\\displaystyle\\sum_{h=1}^{H}\\sum_{x_{H}<x_{h}}\\rho^{\\pi^{\\prime}}(x_{h})\\mathrm{KL}(\\pi^{\\prime}||\\mu)[x_{h}]}\\\\ &{\\quad\\quad\\quad=c\\mathcal{P}(\\pi_{1})\\cdot\\pi^{\\pi_{1}}\\delta(x_{h}^{\\prime})+(1-c)\\mathcal{P}(\\pi_{2}\\cdot\\pi^{\\prime})}\\\\ &{\\quad\\quad\\quad-\\alpha\\displaystyle\\sum_{h=1}^{H}\\sum_{x_{H}<x_{H}}\\left[c\\rho^{\\pi_{1}}(x_{h})+(1-c)\\rho^{\\pi^{\\prime}}(x_{h})\\right]\\mathrm{KL}(\\overline{{\\pi}}_{c}||\\mu)[x_{h}]}\\\\ &{\\quad\\quad\\quad+\\alpha\\displaystyle\\sum_{h=1}^{H}\\sum_{x_{H}<x_{h}}\\rho^{\\pi^{\\prime}}(x_{h})\\mathrm{KL}(\\pi^{\\prime}||\\mu)[x_{h}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now from the convexity of $\\pi\\mapsto\\mathrm{KL}(\\pi||\\mu)[x_{h}]$ , and the definition of $\\pi_{c}$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho^{\\pi_{c}}(x_{h})\\mathrm{KL}(\\pi_{c}\\|\\mu)[x_{h}]=\\left[c\\rho^{\\pi_{1}}(x_{h})+(1-c)\\rho^{\\pi_{2}}(x_{h})\\right]\\mathrm{KL}(\\pi_{c}\\|\\mu)[x_{h}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq c\\rho^{\\pi_{1}}(x_{h})\\mathrm{KL}(\\pi_{1}\\|\\mu)[x_{h}]+(1-c)\\rho^{\\pi_{2}}(x_{h})\\mathrm{KL}(\\pi_{2}\\|\\mu)[x_{h}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{\\alpha}(\\pi_{c}>\\pi^{\\prime})\\geq c\\mathcal{P}(\\pi_{1}>\\pi^{\\prime})+(1-c)\\mathcal{P}(\\pi_{2}>\\pi^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad-\\alpha\\displaystyle\\sum_{h=1}^{H}\\sum_{x_{h}\\in\\mathcal{X}_{h}}c\\rho^{\\pi_{1}}(x_{h})\\mathrm{KL}(\\pi_{1}||\\mu)[x_{h}]+(1-c)\\rho^{\\pi_{2}}(x_{h})\\mathrm{KL}(\\pi_{2}||\\mu)[x_{h}]}\\\\ &{\\qquad\\qquad\\qquad+\\alpha\\displaystyle\\sum_{h=1}^{H}\\sum_{x_{h}\\in\\mathcal{X}_{h}}c\\rho^{\\pi^{\\prime}}(x_{h})\\mathrm{KL}(\\pi^{\\prime}||\\mu)[x_{h}]+(1-c)\\rho^{\\pi^{\\prime}}(x_{h})\\mathrm{KL}(\\pi^{\\prime}||\\mu)[x_{h}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=c\\mathcal{P}_{\\alpha}(\\pi_{1}>\\pi^{\\prime})+(1-c)\\mathcal{P}_{\\alpha}(\\pi_{2}>\\pi^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma A.3. For any state $x_{h}\\in\\mathcal{X}_{h}$ , let us write $\\rho^{\\pi}(x_{h})$ the probability to reach the specific state $x_{h}\\in\\mathcal{X}_{h}$ when following $\\pi$ , i.e., $\\rho^{\\pi}(x_{h})=\\operatorname*{Pr}_{\\pi,p}[x_{h}]$ . Then, for any two policies $(\\pi_{1},\\pi_{2})$ and any coefficient $c\\in[0,1]$ , there exists a policy $\\pi_{c}$ such that for any $h=1,\\ldots,H+1$ , and any $x_{h}\\in\\mathcal{X}_{h}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\rho^{\\pi_{c}}(x_{h})=c\\rho^{\\pi_{1}}(x_{h})+(1-c)\\rho^{\\pi_{2}}(x_{h}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let us define $\\pi_{c}$ as a function of $\\pi_{1},\\pi_{2}$ , and $c$ : for any $x\\in\\mathcal{X},\\,y\\in\\mathcal{Y}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pi_{c}(y\\mid x)=\\frac{c\\rho^{\\pi_{1}}(x)\\pi_{1}(y\\mid x)+(1-c)\\rho^{\\pi_{2}}(x)\\pi_{2}(y\\mid x)}{c\\rho^{\\pi_{1}}(x)+(1-c)\\rho^{\\pi_{2}}(x)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now prove the lemma by induction on $h$ . It holds for $h=1$ since we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\rho^{\\pi_{c}}(x_{1})=1=c+(1-c)=c\\rho^{\\pi_{1}}(x_{1})+(1-c)\\rho^{\\pi_{2}}(x_{1}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now assume the claim holds for any $x_{h}\\in\\mathcal{X}_{h}$ , then for any $x_{h+1}\\in\\mathcal{X}_{h+1}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mu^{\\pi_{c}}(x_{h+1})=\\sum_{x_{h}\\in\\mathcal{X}_{h}}\\int^{\\pi_{c}}(x_{h})\\sum_{y_{h}\\in\\mathcal{Y}_{h}}p(x_{h+1}\\mid x_{h},y_{h})\\pi_{c}(y_{h}\\mid x_{h})}}\\\\ &{=\\sum_{x_{h}\\in\\mathcal{X}_{h}}\\int^{\\pi_{c}}(x_{h})\\sum_{y_{h}\\in\\mathcal{Y}_{h}}p(x_{h+1}\\mid x_{h},y_{h})\\frac{C\\rho^{\\pi_{1}}(x_{h})\\pi_{1}(y_{h}\\mid x_{h})+\\left(1-c\\right)\\rho^{\\pi_{2}}(x_{h})\\pi_{2}(y_{h}\\mid x_{h})}{c\\rho^{\\pi_{1}}(x_{h})+\\left(1-c\\right)\\rho^{\\pi_{2}}(x_{h})}}\\\\ &{=\\sum_{x_{h}\\in\\mathcal{X}_{h}}\\rho^{\\pi_{c}}(x_{h})\\sum_{y_{h}\\in\\mathcal{Y}_{h}}p(x_{h+1}\\mid x_{h},y_{h})\\frac{C\\rho^{\\pi_{1}}(x_{h})\\pi_{1}(y_{h}\\mid x_{h})+\\left(1-c\\right)\\rho^{\\pi_{2}}(x_{h})\\pi_{2}(y_{h}\\mid x_{h})}{\\rho^{\\pi_{c}}(x_{h})}}\\\\ &{=\\sum_{x_{h}\\in\\mathcal{X}_{h}}\\sum_{y_{h}\\in\\mathcal{Y}_{h}}p(x_{h+1}\\mid x_{h},y_{h})\\left[c\\rho^{\\pi_{1}}(x_{h})\\pi_{1}(y_{h}\\mid x_{h})+\\left(1-c\\right)\\rho^{\\pi_{2}}(x_{h})\\pi_{2}(y_{h}\\mid x_{h})\\right]}\\\\ &{=\\sum_{x_{h}\\in\\mathcal{X}_{h}}\\sum_{y_{h}\\in\\mathcal{Y}_{h}}p(x_{h+1}\\mid x_{h},y_{h})\\left[c\\rho^{\\pi_{1}}(x_{h})\\pi_{1}(y_{h}\\mid x_{h})+\\left(1-c\\right)\\rho^{\\pi_{2}}(x_{h})\\pi_{2}(y_{h}\\mid x_{h})\\right]}\\\\ &{=c\\rho^{\\pi_{1\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third inequality is by the induction hypothesis. ", "page_idx": 17}, {"type": "text", "text": "B Proofs for section 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Regularized preference-based Q-function (proof of Lemma 4.1) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma (restatement of Lemma 4.1). Let $\\pi,\\pi^{\\prime}$ be two policies. For every $x_{H+1}\\in\\mathcal{X}_{H+1}$ , it holds that $V_{\\alpha}^{\\pi,\\pi^{\\prime}}(x_{H+1})=\\mathcal{P}(x_{H+1}\\times\\pi^{\\prime})$ . Furthermore, for every $h\\in[H]$ the following recursive relations hold: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\;\\;V_{\\alpha}^{\\pi,\\pi^{\\prime}}(x_{h})=\\mathbb{E}_{y_{h}\\sim\\pi(\\cdot|x_{h})}Q_{\\alpha}^{\\pi,\\pi^{\\prime}}(x_{h},y_{h}),}\\\\ &{\\quad\\;Q_{\\alpha}^{\\pi,\\pi^{\\prime}}(x_{h},y_{h})=\\mathbb{E}_{x_{h+1}\\sim p(\\cdot|x_{h},y_{h})}V_{\\alpha}^{\\pi,\\pi^{\\prime}}(x_{h+1})-\\alpha\\mathrm{KL}(\\pi\\|\\mu)[x_{h}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, let $\\bar{\\pi}$ be a third policy, then the following value difference lemma holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{D}_{\\alpha}\\big(\\pi\\succ\\bar{\\pi}\\big)-\\mathcal{P}_{\\alpha}\\big(\\pi^{\\prime}\\succ\\bar{\\pi}\\big)=\\mathbb{E}_{\\pi^{\\prime},p}\\biggl[\\sum_{h=1}^{H}\\langle\\pi-\\pi^{\\prime},Q_{\\alpha}^{\\pi,\\bar{\\pi}}\\rangle[x_{h}]+\\alpha\\mathrm{KL}(\\pi^{\\prime}||\\mu)[x_{h}]-\\alpha\\mathrm{KL}(\\pi||\\mu)[x_{h}]\\biggr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We prove the lemma by casting the preference-based $\\mathrm{RL}$ problem as an adversarial MDP, see Appendix E for the details. Set $r^{t}(x_{H+1})=\\mathcal{P}(x_{H+1}\\succ\\pi^{\\prime})$ , then $Q_{\\alpha}^{\\pi,\\pi^{\\prime}}=Q_{\\alpha}^{\\pi,t}$ (see Definition E.3 for the definition of the regularized Q-function). Now the lemma follows directly from Lemmas E.4 and E.5. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.2 Convergence of MTPO (Proof of Theorem 4.2) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem (restatement of Theorem 4.2). Let $\\pi_{\\alpha}^{\\star}$ be the Nash equilibrium of the regularized preference model $\\mathcal{P}_{\\alpha}$ . Then, for the choice $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\alpha(t+2)}}\\end{array}$ , MTPO guarantees at every iteration $t$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t})\\leq\\frac{32H\\mathbb{Q}^{2}}{\\alpha^{2}(t+1)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbb{Q}$ is a bound on the magnitude of the $Q$ -functions. ", "page_idx": 18}, {"type": "text", "text": "Proof. The theorem follows directly from Theorem B.1. ", "page_idx": 18}, {"type": "text", "text": "Theorem B.1. Running the MTPO algorithm, we have that at every iteration $t$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}\\|\\pi_{t+1})\\leq(1-\\eta_{t}\\alpha)\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}\\|\\pi_{t})+2\\eta_{t}^{2}\\mathbb{E}_{\\pi_{\\alpha}^{\\star},p}\\left[\\sum_{h=1}^{H}\\left\\|Q_{\\alpha}^{\\pi_{t},\\pi_{t}}(x_{h},\\cdot)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\mid x_{h})}{\\mu(\\cdot\\mid x_{h})}\\right\\|_{\\infty}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, for the choice $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\alpha(t+2)}}\\end{array}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t})\\leq\\frac{8}{\\alpha^{2}(t+1)}\\cdot\\operatorname*{max}_{t}\\mathbb{E}_{\\pi_{\\alpha}^{\\star},p}\\left[\\sum_{h=1}^{H}\\left\\|Q_{\\alpha}^{\\pi_{t},\\pi_{t}}(x_{h},\\cdot)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\mid x_{h})}{\\mu(\\cdot\\mid x_{h})}\\right\\|_{\\infty}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t})\\leq\\frac{32H}{t+1}\\cdot\\operatorname*{max}\\biggl\\{\\frac{1}{\\alpha^{2}},H^{2}\\log^{2}\\mu_{\\operatorname*{min}}\\biggr\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\mu_{\\mathrm{min}}=\\operatorname*{min}_{(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}:\\mu(y|x)>0}\\mu(y\\mid x)}\\end{array}$ is the minimal non-zero probability assigned by $\\mu$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. We prove the theorem by casting the preference-based RL problem as an adversarial MDP, see Appendix $\\boldsymbol{\\mathrm E}$ for the details. Set $r^{t}(x_{H+1})=\\Bar{\\mathcal{P}}\\big(x_{H+1}\\succ\\pi_{t}\\big)$ , then $Q_{\\alpha}^{\\pi_{t},\\pi_{t}}=Q_{\\alpha}^{\\pi_{t},t}$ (see Definition E.3 for the definition of the regularized Q-function). Now, MTPO is equivalent to running mirror descent policy optimization. Thus, by Lemma E.6 with $\\pi=\\pi_{\\alpha}^{\\star}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}\\|\\pi_{t+1})\\leq\\big(1-\\eta_{t}\\alpha\\big)\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}\\|\\pi_{t})+2\\eta_{t}^{2}\\mathbb{E}_{\\pi_{\\alpha}^{\\star},p}\\Bigg[\\displaystyle\\sum_{h=1}^{H}\\left\\|Q_{\\alpha}^{\\pi_{t},\\pi_{t}}(x_{h},\\cdot)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\,|\\,x_{h})}{\\mu(\\cdot\\,|\\,x_{h})}\\right\\|_{\\infty}^{2}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\eta_{t}(\\mathcal{P}_{\\alpha}(\\pi_{t}\\times\\pi_{t})-\\mathcal{P}_{\\alpha}(\\pi_{\\alpha}^{\\star}\\times\\pi_{t}))}\\\\ &{\\qquad\\qquad\\leq\\big(1-\\eta_{t}\\alpha\\big)\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}\\|\\pi_{t})+2\\eta_{t}^{2}\\mathbb{E}_{\\pi_{\\alpha}^{\\star},p}\\Bigg[\\displaystyle\\sum_{h=1}^{H}\\left\\|Q_{\\alpha}^{\\pi_{t},\\pi_{t}}(x_{h},\\cdot)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\,|\\,x_{h})}{\\mu(\\cdot\\,|\\,x_{h})}\\right\\|_{\\infty}^{2}\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality optimality of $\\pi_{\\alpha}^{\\star}$ . The last claim follows directly from Lemma E.8. Similarly to Nash-MD Munos et al. [2023], this is the Nash-equilibrium of regularized preference model, $\\mathcal{P}_{\\alpha}$ , following the minimax theorem [Von Neumann, 1928] \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.3 MTPO with mixture policy (MTPO- $\\tau$ ) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Inspired by Nash-MD, we present a variant of MTPO which makes use of the mixture policy $\\pi_{t}^{\\alpha}$ , which we call MTPO- $\\tau$ . Define the regularized policy $\\pi_{t}^{\\alpha}$ as a geometric mixture between the current policy $\\pi_{t}$ and the reference policy $\\mu$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi_{t}^{\\alpha}(y\\mid x)={\\frac{\\pi_{t}(y\\mid x)^{1-\\eta_{t}\\alpha}\\mu(y\\mid x)^{\\eta_{t}\\alpha}}{\\sum_{y^{\\prime}\\in{\\mathcal{Y}}}\\pi_{t}(y^{\\prime}\\mid x)^{1-\\eta_{t}\\alpha}\\mu(y^{\\prime}\\mid x)^{\\eta_{t}\\alpha}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The MTPO- $\\tau$ update rule is then: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi_{t+1}\\big(\\cdot\\,\\big|\\;x_{h}\\big)=\\arg\\operatorname*{max}_{\\pi}\\eta_{t}\\Big\\langle\\pi,Q_{\\alpha}^{\\pi_{t}^{\\alpha},\\pi_{t}^{\\alpha}}\\Big\\rangle[x_{h}]-\\mathrm{KL}(\\pi\\|\\pi_{t}^{\\alpha})[x_{h}]\\qquad\\forall h\\in[H],x_{h}\\in\\mathcal{X}_{h}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathrm{KL}(\\cdot||\\cdot)$ is the standard KL-divergence. It is well-known that his optimization problem has the following explicit closed-form: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi_{t+1}\\big(y_{h}\\mid x_{h}\\big)=\\frac{\\pi_{t}^{\\alpha}\\big(y_{h}\\mid x_{h}\\big)e^{\\eta_{t}Q_{\\alpha}^{\\pi^{\\alpha},\\pi_{t}^{\\alpha}}}(x_{h},y_{h})}{\\sum_{y_{h}^{\\prime}\\in\\mathcal{Y}}\\pi_{t}^{\\alpha}\\big(y_{h}^{\\prime}\\mid x_{h}\\big)e^{\\eta_{t}Q_{\\alpha}^{\\pi^{\\alpha},\\pi_{t}^{\\alpha}}}(x_{h},y_{h}^{\\prime})}\\qquad\\forall h\\in[H],x_{h}\\in\\mathcal{X}_{h},y_{h}\\in\\mathcal{Y}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we show that MTPO- $\\tau$ converges to the Nash equilibrium of the $\\alpha$ -regularized preference model. ", "page_idx": 19}, {"type": "text", "text": "Theorem B.2. Let $\\pi_{\\alpha}^{\\star}$ be the Nash equilibrium of the regularized preference model $\\mathcal{P}_{\\alpha}$ . Then, for the choice $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\alpha(t+2)}}\\end{array}$ , MTPO- $\\tau$ guarantees at every iteration t that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t}^{\\alpha})\\leq\\frac{9H\\mathbb{Q}^{2}}{\\alpha^{2}(t+1)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbb{Q}$ is a bound on the magnitude of the $Q$ -functions. ", "page_idx": 19}, {"type": "text", "text": "Proof. The theorem follows directly from Theorem B.3 and corollary B.4. ", "page_idx": 19}, {"type": "text", "text": "Theorem B.3. Running the MTPO- $\\tau$ algorithm, we have that at every iteration $t$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star}||\\pi_{t+1}\\big)\\leq\\big(1-\\eta_{t}\\alpha\\big)\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star}||\\pi_{t}\\big)+2\\eta_{t}^{2}\\mathbb{E}_{\\pi_{\\alpha}^{\\star},p}\\Bigg[\\sum_{h=1}^{H}\\Big\\|Q_{\\alpha}^{\\pi_{t}^{\\alpha},\\pi_{t}^{\\alpha}}\\big(x_{h},\\cdot\\big)\\Big\\|_{\\infty}^{2}\\Bigg].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, for the choice $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\alpha(t+2)}}\\end{array}$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t})\\leq\\frac{8}{\\alpha^{2}(t+1)}\\cdot\\operatorname*{max}_{t}\\mathbb{E}_{\\pi_{\\alpha}^{\\star},p}\\biggl[\\sum_{h=1}^{H}\\left\\|Q_{\\alpha}^{\\pi_{t}^{\\alpha},\\pi_{t}^{\\alpha}}(x_{h},\\cdot)\\right\\|_{\\infty}^{2}\\biggr].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t})\\leq\\frac{8H}{t+1}\\cdot\\operatorname*{max}\\biggl\\{\\frac{1}{\\alpha^{2}},H^{2}\\log^{2}\\mu_{\\operatorname*{min}}\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We prove the theorem by casting the preference-based RL problem as an adversarial MDP, see Appendix E for the details. Set $r^{t}(x_{H+1})=\\mathcal{P}\\big(x_{H+1}\\succ\\pi_{t}^{\\alpha}\\big)$ , then $\\bar{Q}_{\\alpha}^{\\pi_{t}^{\\alpha},\\pi_{t}^{\\alpha}}=Q_{\\alpha}^{\\pi_{t}^{\\alpha},t}$ (see Definition E.3 for the definition of the regularized Q-function). Now, Nash-MD is equivalent to running mixture mirror descent policy optimization. Thus, by Lemma E.7 with $\\pi=\\pi_{\\alpha}^{\\star}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star}\\|\\pi_{t+1}\\big)\\leq\\big(1-\\eta_{t}\\alpha\\big)\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star}\\|\\pi_{t}\\big)+2\\eta_{t}^{2}\\mathbb{E}_{\\pi_{\\alpha}^{\\star},p}\\Bigg[\\underset{h=1}{\\overset{H}{\\sum}}\\left\\|Q_{\\alpha}^{\\pi_{t}^{\\alpha},\\pi_{t}^{\\alpha}}\\big(x_{h},\\cdot\\big)\\right\\|_{\\infty}^{2}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\eta_{t}\\big(\\mathcal{P}_{\\alpha}\\big(\\pi_{t}^{\\alpha}\\times\\pi_{t}^{\\alpha}\\big)-\\mathcal{P}_{\\alpha}\\big(\\pi_{\\alpha}^{\\star}\\times\\pi_{t}^{\\alpha}\\big)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\big(1-\\eta_{t}\\alpha\\big)\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star}\\|\\pi_{t}\\big)+2\\eta_{t}^{2}\\mathbb{E}_{\\pi_{\\alpha}^{\\star},p}\\Bigg[\\underset{h=1}{\\overset{H}{\\sum}}\\left\\|Q_{\\alpha}^{\\pi_{t}^{\\alpha},\\pi_{t}^{\\alpha}}\\big(x_{h},\\cdot\\big)\\right\\|_{\\infty}^{2}\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality optimality of $\\pi_{\\alpha}^{\\star}$ . The last claim follows directly from Lemma E.9. ", "page_idx": 19}, {"type": "text", "text": "Corollary B.4. Running the MTPO- $\\tau$ algorithm, we have that at every iteration $t$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star}\\big||\\pi_{t}^{\\alpha}\\big)\\leq\\big(1-\\eta_{t}\\alpha\\big)\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star}||\\pi_{t}\\big)+\\frac{\\eta_{t}}{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, for the choice $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\alpha(t+2)}}\\end{array}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\pi_{t}^{\\alpha})\\leq\\frac{9H}{t+1}\\cdot\\operatorname*{max}\\biggl\\{\\frac{1}{\\alpha^{2}},H^{2}\\log^{2}\\mu_{\\operatorname*{min}}\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By Munos et al. [2023, Lemma 1], for every $x_{h}\\in\\mathcal{X}_{h}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\big(\\pi_{\\alpha}^{\\star}||\\pi_{t}^{\\alpha}\\big)\\big[x_{h}\\big]\\leq\\big(1-\\eta_{t}\\alpha\\big)\\mathrm{KL}\\big(\\pi_{\\alpha}^{\\star}||\\pi_{t}\\big)\\big[x_{h}\\big]+\\eta_{t}\\alpha\\mathrm{KL}\\big(\\pi_{\\alpha}^{\\star}||\\mu\\big)\\big[x_{h}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We finish the proof by taking the expectation $\\mathbb{E}_{\\pi_{\\alpha}^{\\star},p}[\\cdot]$ and using Lemmas 3.1 and B.5. The second part is by Theorem B.3. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma B.5. It holds that $\\begin{array}{r}{\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\mu)\\leq\\frac{1}{2\\alpha}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. By the optimality of $\\pi_{\\alpha}^{\\star}$ we have that $\\mathcal{P}_{\\alpha}(\\pi_{\\alpha}^{\\star}\\succ\\mu)\\,\\geq\\mathcal{P}_{\\alpha}(\\pi_{\\alpha}^{\\star}\\succ\\pi_{\\alpha}^{\\star})$ . Now, since $\\mathcal{P}_{\\alpha}(\\pi_{\\alpha}^{\\star}>$ $\\pi_{\\alpha}^{\\star})=1/2$ , we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\alpha\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}||\\mu)\\leq\\mathcal{P}(\\pi_{\\alpha}^{\\star}\\succ\\mu)-\\frac{1}{2}\\leq1-\\frac{1}{2}=\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.4 Convergence of multi-turn RLHF ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem B.6. Let $\\pi_{\\alpha}^{\\star}\\mathrm{{,RLHF}}(\\cdot\\mid x)=\\arg\\operatorname*{max}_{\\pi}V_{\\alpha}^{\\pi,\\mathrm{RLHF}}(x)$ for every $x\\in\\mathscr{X}$ . Then, for the choice $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\alpha(t+2)}}\\end{array}$ , multi-turn RLHF guarantees at every iteration $t_{;}$ , $\\begin{array}{r}{\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star,\\mathrm{RLHF}}||\\pi_{t}\\big)\\leq\\frac{32H\\mathbb{Q}^{2}}{\\alpha^{2}(t+1)}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. The theorem follows directly from Theorem B.7. ", "page_idx": 20}, {"type": "text", "text": "Theorem B.7. Running the multi-turn RLHF algorithm, we have that at every iteration $t$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{\\langleL_{p}(}\\pi_{\\alpha}^{\\star,\\mathrm{RLHF}}\\lVert\\pi_{t+1})\\leq(1-\\eta_{t}\\alpha)\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star}\\lVert\\pi_{t})+2\\eta_{t}^{2}\\mathbb{E}_{\\pi_{\\alpha}^{\\star}\\mathrm{,RLHF},p}\\Bigg[\\sum_{h=1}^{H}\\left\\lVert Q_{\\alpha}^{\\pi_{t},\\pi_{t}}(x_{h},\\cdot)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\left|\\pi_{h}\\right>)}{\\mu(\\cdot\\left|\\pi_{h}\\right>)}\\right\\rVert_{\\infty}\\Bigg],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, for the choice $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\alpha(t+2)}}\\end{array}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}(\\pi_{\\alpha}^{\\star,\\mathrm{RLHF}}\\|\\pi_{t})\\leq\\frac{8}{\\alpha^{2}(t+1)}\\cdot\\operatorname*{max}_{t}\\mathbb{E}_{\\pi_{\\alpha}^{\\star,\\mathrm{RLHF}},p}\\left[\\sum_{h=1}^{H}\\left\\|Q_{\\alpha}^{\\pi_{t},\\pi_{t}}(x_{h},\\cdot)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\,|\\,x_{h})}{\\mu(\\cdot\\,|\\,x_{h})}\\right\\|_{\\infty}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{p}\\big(\\pi_{\\alpha}^{\\star,\\mathrm{RLHF}}\\|\\pi_{t}\\big)\\leq\\frac{32H}{t+1}\\cdot\\operatorname*{max}\\biggr\\{\\frac{1}{\\alpha^{2}},H^{2}\\log^{2}\\mu_{\\operatorname*{min}}\\biggr\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\mu_{\\mathrm{min}}=\\operatorname*{min}_{(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}:\\mu(y|x)>0}\\mu(y\\mid x)}\\end{array}$ is the minimal non-zero probability assigned by $\\mu$ . ", "page_idx": 20}, {"type": "text", "text": "sPereo oAf.p pWene dpixr $\\boldsymbol{\\mathrm E}$ ef otrh teh teh deeotraeilms . bSye tc $r^{t}(\\check{x_{H+1}})=\\check{r}^{\\mathrm{RLHF}}(x_{H+1})$ ,H tFh epnr $Q_{\\alpha}^{\\pi_{t},\\mathrm{RLHF}}=Q_{\\alpha}^{\\pi_{t},t}$ s(asreiea l DMefDiniP-, tion E.3 for the definition of the regularized Q-function). Now, multi-turn RLHF is equivalent to running mirror descent policy optimization. Thus, by Lemma E.6 with $\\pi=\\pi_{\\alpha}^{\\star,\\mathrm{RLHF}}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\langleL_{p}(\\pi_{\\alpha}^{*,\\mathrm{RLHF}}\\lVert\\pi_{t+1})\\leq(1-\\eta_{t}\\alpha)\\mathrm{KL}_{p}(\\pi_{\\alpha}^{*,\\mathrm{RLHF}}\\lVert\\pi_{t})+2\\eta_{t}^{2}\\mathbb{E}_{\\pi_{\\alpha}^{*,\\mathrm{RLHF}},p}\\Bigg[\\underset{h=1}{\\overset{H}{\\sum}}\\Bigg\\lVert Q_{\\alpha}^{\\pi_{t},\\pi_{t}}\\big(x_{h},\\cdot\\big)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\vert-x_{t})}{\\mu(\\cdot\\vert\\pi_{h})}\\Bigg\\rVert_{\\mu=1}^{\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\eta_{t}\\bigg(V_{\\alpha}^{\\pi_{t},\\mathrm{RLHF}}(x_{1})-V_{\\alpha}^{\\pi_{\\alpha}^{*,\\mathrm{RLHF}},\\mathrm{RLHF}}(x_{1})\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(1-\\eta_{t}\\alpha)\\mathrm{KL}_{p}(\\pi_{\\alpha}^{*}\\big\\lVert\\pi_{t})+2\\eta_{t}^{2}\\mathbb{E}_{\\pi_{\\alpha}^{*},p}\\Bigg[\\underset{h=1}{\\overset{H}{\\sum}}\\Bigg\\lVert Q_{\\alpha}^{\\pi_{t},\\pi_{t}}\\big(x_{h},\\cdot\\big)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\vert~x_{h})}{\\mu(\\cdot\\vert~x_{h})}\\Bigg\\rVert_{\\infty}^{2}\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality optimality of $\\pi_{\\alpha}^{\\star,\\mathrm{RLHF}}$ . The last claim follows directly from Lemma E.8. ", "page_idx": 20}, {"type": "text", "text": "C The Education Dialogue environment ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Prompts for creating the environment ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We use the following prompt to generate conversations using Gemini [Google, 2024]: ", "page_idx": 21}, {"type": "text", "text": "Simulate a conversation between a teacher in school and a student. There is a small chance that the teacher is successful in teaching the student so he understands the topic. The conversation lasts roughly 10-15 turns but ends when either side says [end of conversation]. The teacher wants to teach the student about {topic}. The student likes {student_pref}. The teacher does not know that beforehand. The student prefers to learn this way, {student_reaction}. The teacher likes {teacher_pref}. He prefers to teach this way, {teacher_reaction}. Output the conversation and the probability that the student understood the material, in the following format.   \n#   \nConversation:   \n[   \nTeacher: I   \nStudent:   \nTeacher:   \nStudent:   \n]   \nProbability:   \n# ", "page_idx": 21}, {"type": "text", "text": "The topic is sampled from the following topics list: ", "page_idx": 21}, {"type": "text", "text": "Photosynthesis, Evolution, DNA, Newton\u2019s First Law of Motion, Newton\u2019s Second Law of Motion, Newton\u2019s Third Law of Motion, Archimedes\u2019 Principle, Conservation of Energy, Pythagorean Theorem, Allegory, Metaphor, Personification, Foreshadowing, Irony, Atoms, Elements, Molecules, The Periodic Table, The French Revolution, The Industrial Revolution, The Russian Revolution, World War 1, World War 2, The American Civil War, The September 11th Attacks, The Declaration of Independence, The Pyramids, The Parthenon, The Colosseum, The Hagia Sophia, The Taj Mahal, The Great Wall of China, The Machu Picchu, Angkor Wat, The Palace of Versailles, The White House, The Tower of London, Notre Dame Cathedral, The Eiffel TowerConfucius, Julius Caesar, Leonardo da Vinci, William Shakespeare, Napoleon Bonaparte, Abraham Lincoln, Albert Einstein, Martin Luther King, Nelson Mandela, Marie Curie, Genghis Khan, Christopher Columbus, Joan of Arc, Winston Churchill, Vincent van Gogh, Pablo Picasso, Salvador Dali, The Roman Empire, The Cold War, Zeus, Poseidon, Ares, Hercules, Achilles, Minotaur, Medusa, The Solar System, The Big Bang, Supply and Demand, Communism, Capitalism, Democracy, Dictatorship, Sigmund Freud, Cells, The Circulatory System, The Respiratory System, The Respiratory System, The Nervous System, Neurons ", "page_idx": 21}, {"type": "text", "text": "The student\u2019s learning preferences (student_pref) are sampled from the following list: ", "page_idx": 21}, {"type": "text", "text": "interactive learning/class discussions/asking questions, direct instruction/lecture-based learning, hands-on activities/real-world applications, creative expression/story telling/gamification ", "page_idx": 21}, {"type": "text", "text": "The student\u2019s reactions to not learning in their preferred ways (student_reaction) are sampled from the following list: ", "page_idx": 22}, {"type": "text", "text": "and gets rude otherwise, and gets disengaged otherwise, and gets frustrated otherwise, and gets anxious otherwise, but might adapt to other methods, and might tell it to the teacher ", "page_idx": 22}, {"type": "text", "text": "The teacher\u2019s teaching preferences (teacher_pref) are sampled from the following list: ", "page_idx": 22}, {"type": "text", "text": "direct instruction/lecture-based learning, interactive learning/class discussions/inquiry-based learning, experiential learning/hands-on activities, formative assessment ", "page_idx": 22}, {"type": "text", "text": "The teacher\u2019s reactions to different learning methods (teacher_reaction) are sampled from the following list: ", "page_idx": 22}, {"type": "text", "text": "and gets frustrated otherwise, and blames the student otherwise, and gives up otherwise, but might adapt to the student, and might insist on teaching this way ", "page_idx": 22}, {"type": "text", "text": "We use the following prompt to query Gemini for the preference between two conversations (conv1 and conv2): ", "page_idx": 22}, {"type": "text", "text": "You are an expert at assesing teachers. Here are two interactions between a teacher and a student.   \n#   \nInteraction 1:   \n{conv1}   \n#   \nInteraction 2:   \n{conv2}   \n#   \nA good interaction between a teacher and student is   \ncharacterized by several key elements other than whether the student was able to understand the topic. The teacher should present information clearly and enthusiastically, encouraging questions and active participation. Students should feel comfortable asking for clarification, offering their own insights, and respectfully challenging ideas. Which interaction is better (do not let the order   \ninteractions affect your answer)? Output 1 or 2. ", "page_idx": 22}, {"type": "text", "text": "For the single-turn baseline, we use the following modified prompt to query Gemini for preferences: ", "page_idx": 22}, {"type": "text", "text": "You are an expert at assesing teachers. Here is an   \ninteraction between a teacher and a student.   \n#   \nInteraction:   \n{conv}   \n#   \nHere are two possible responses by the teacher:   \n#   \nResponse 1:   \n{resp1}   \n#   \nResponse 2:   \n{resp2}   \n#   \nA good interaction between a teacher and student is   \ncharacterized by several key elements other than whether the student was able to understand the topic. The teacher should present information clearly and enthusiastically, encouraging questions and active participation. Students should feel comfortable asking for clarification, offering their own insights, and respectfully challenging ideas. Assuming that the teacher and student continue the   \ninteraction with one of these responses, which response will lead to a better interaction (do not let the order interactions affect your answer)? Output 1 or 2. ", "page_idx": 23}, {"type": "text", "text": "C.2 Examples of interactions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Teacher: Good morning. Today, we\u2019re going to talk about personification, a literary device that gives human qualities to nonhuman things.   \nStudent: That sounds a bit abstract. Can you give me an example?   \nTeacher: Sure. \u2019The wind whispered through the trees.\u2019   \nStudent: But the wind can\u2019t actually whisper.   \nTeacher: That\u2019s correct. But we\u2019re using personification to create a more vivid image in our minds. Let\u2019s try an activity. I\u2019ll give you a sentence, and you try to personify it.   \nStudent: Okay, that sounds fun.   \nTeacher: The car purred down the street.   \nStudent: The car roared down the street!   \nTeacher: Excellent! You\u2019re getting the hang of it. Now, let\u2019s talk about why writers use personification.   \nStudent: I\u2019m not really understanding why they do.   \nTeacher: That\u2019s okay. Let\u2019s brainstorm. Can you think of any reasons why authors might use personification?   \nStudent: Maybe to make the story more interesting?   \nTeacher: That\u2019s a great reason. Anything else?   \nStudent: Maybe to make it more relatable?   \nTeacher: Yes, personification can help us relate to objects and ideas on a deeper level. Do you think you understand personification now?   \nStudent: I think so. It\u2019s a creative way to make things come alive.   \nTeacher: Exactly. I\u2019m glad you enjoyed learning about it. Let\u2019s do a quick review before we end the class.   \nStudent: Sounds good.   \nTeacher: Great. I\u2019ll ask you a few questions, and you can answer as if you were a personifying writer.   \nStudent: Okay. ", "page_idx": 23}, {"type": "text", "text": "2. Teacher: Today, we\u2019re going to learn about The Solar System. ", "page_idx": 23}, {"type": "text", "text": "Student: Sounds boring. Can we do something more hands-on? Teacher: I have a great activity planned. We\u2019re going to build a scale model of the Solar System using different-sized balls. Student: That sounds cool! Teacher: First, let\u2019s talk about the planets. There are eight planets in our Solar System. Student: I know some of them, like Earth and Mars. Teacher: Excellent! Now, let\u2019s start building our model. We\u2019ll use these balls to represent the planets and arrange them in order from the Sun. Student: This is fun! I can see how big the planets are compared to each other. Teacher: I\u2019m glad you\u2019re enjoying it. Now, let\u2019s talk about the moons and other objects that orbit the planets. Student: Can we build a model of the moons? Teacher: Absolutely! This will help you understand their different characteristics. Student: I think I get it now. This is a great way to learn! Teacher: I\u2019m so glad to hear that. Let\u2019s finish building our model and then we\u2019ll review what we\u2019ve learned. Student: Okay. Teacher: Well done! I believe you have a good understanding of The Solar System now. Student: Thanks! I learned a lot. You\u2019re welcome. [end of conversation]   \n3. Teacher: Today, we\u2019re going to learn about World War 1. Student: Okay. Teacher: The war began in 1914 when Archduke Franz Ferdinand of Austria-Hungary was assassinated. Student: I\u2019ve heard of him. Teacher: The war was fought between two main alliances: the Allies and the Central Powers. Student: Can you tell me more about the Allies? Teacher: Sure. The Allies included France, Russia, and Great Britain. Student: What were the Central Powers? Teacher: They were Germany, Austria-Hungary, and Ottoman Russia. Student: This is a lot of information. Can we slow down a bit? Teacher: Of course. Let\u2019s review what we\u2019ve learned so far. Student: I\u2019m not sure I understand everything yet. Teacher: That\u2019s okay. We can go over it again. But first, I want you to try to summarize what we\u2019ve learned so far. Student: World War 1 started when Archduke Ferdinand was assassinated, and the Allies and Central Powers fought each other. Teacher: Good. Now, let\u2019s go over the key events of the war. Student: Okay. Teacher: The war ended in 1918 with the defeat of the Central Powers. Student: I think I understand it now. Teacher: Great! I\u2019m glad you do. Student: Thanks, I appreciate it. You\u2019re welcome. [end of conversation]   \n4. Teacher: Today, we\u2019re going to learn about Machu Picchu, the ancient Incan city. Student: Can you just tell me the facts? I don\u2019t need all this extra stuff. Teacher: Sure. Machu Picchu was built in the 15th century in the Andes Mountains of Peru. Student: What made it so special? Teacher: Its location on a mountain ridge provided stunning views of the surrounding landscape. Student: That\u2019s it? Teacher: Well, there\u2019s more to it. Machu Picchu was a royal estate or religious sanctuary for the Incan emperor Pachacuti. Student: Why didn\u2019t they just build it on the ground?   \nTeacher: They thought it would be more likely to be seen from the outside.   \nStudent: Can we just move on?   \nTeacher: No, it\u2019s important to understand the historical significance of Machu Picchu. Student: I don\u2019t care. Just tell me what I need to know for the test.   \nTeacher: I\u2019m trying to help you understand the material, not just memorize it. Student: I don\u2019t need your help. Just give me the notes.   \nTeacher: [end of conversation] ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "5. Teacher: Today, we\u2019re going to talk about foreshadowing in literature. Student: I\u2019m not really into reading. Is there a way we could learn about it in a more creative way? Teacher: No, I\u2019m afraid not. Foreshadowing is an important concept that you need to understand. Student: But I learn better through storytelling or games. Teacher: That\u2019s too bad. You need to learn to focus on the material, even if it\u2019s not presented in your preferred style. Student: Maybe we could act out a scene where there\u2019s foreshadowing? Teacher: That would be a waste of time. We need to cover the key points of foreshadowing. Student: I\u2019m not sure I\u2019m going to understand it this way. Teacher: You will if you pay attention and ask questions. Student: Can you at least give me some examples of foreshadowing? Teacher: Sure. In \u2019Romeo and Juliet,\u2019 the prologue foreshadows the tragic end of the two lovers. Student: That makes sense. How does foreshadowing help the reader? Teacher: It builds suspense and keeps the reader engaged. Student: Okay, I think I\u2019m starting to get it. Teacher: That\u2019s great. I\u2019m glad you\u2019re understanding. Student: Thanks for working with me. You\u2019re welcome. [end of conversation] ", "page_idx": 25}, {"type": "text", "text": "D Hyperparameters and additional experimental results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Hyperparameters ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For both RLHF-based and preference-based algorithms we conducted a sweep over the KL regularization coefficient $\\alpha\\in\\left\\{0.0025,0.005,0.01,0.02,0.05,0.1\\right\\}$ . For preference-based algorithm we also conducted a sweep over the mixing coefficient $\\tau\\in\\{0,0.0375,0.0625,0.125\\}$ . All models are trained for 50000 steps. ", "page_idx": 26}, {"type": "table", "img_path": "rVSc3HIZS4/tmp/0a90de58a4461db265aa2a770156b04ec274940e971b7639c023da1bc87f5920.jpg", "table_caption": ["Table 4: Hyperparameters of all multi-turn algorithms. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.2 Additional experimental results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Table (full version of Table 1): Side-by-side evaluation for Education Dialogue using Flan-T5 XL as the prompted preference model. Each entry is the average preference of 1,600 conversations generated with row method $y$ , over ones generated with column method $y^{\\prime}$ . We evaluate each method using 3 different seeds, compute $3\\times3$ comparisons matrix and report the mean (together with the standard deviation). ", "page_idx": 26}, {"type": "table", "img_path": "rVSc3HIZS4/tmp/9356020f95e0d5c5da125994b72a155c73e3f04211feca9ce62ceb6ec0312317.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "E Mirror descent policy optimization for regularized adversarial MDPs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "E.1 Model ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We start by defining the regularized adversarial MDP model. ", "page_idx": 27}, {"type": "text", "text": "Consider a setting where the agent interacts with an MDP model for $T$ episodes, such that, in each episode $t\\in[T]$ , the agent performs $H$ steps in the MDP (from horizon $h=1$ up to horizon $h=H+1)$ ) In short, an adversarial MDP is a generalization of this standard episodic MDP setting to the scenario where the reward function is different in every episode. This model was extensively studied in recent years (see, e.g., Even-Dar et al. [2009], Rosenberg and Mansour [2019b,a], Shani et al. [2020b]). We consider a slightly different definition which is more focused on our setting. ", "page_idx": 27}, {"type": "text", "text": "Definition E.1 (Adversarial MDP). A finite-horizon adversarial MDP $\\mathcal{M}$ is defined by a tuple $(\\mathcal{X},\\mathcal{Y},H,x_{1},p,\\{r^{t}\\}_{t=1}^{T})$ where $\\mathcal{X}$ is the state space, $\\boldsymbol{\\wp}$ is the action space, $H$ is the horizon, $x_{1}\\in\\mathcal{X}_{1}$ is the initial state, $p:\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\Delta_{\\mathcal{X}}$ is the transition function, and $r^{\\bar{t}}\\colon\\mathcal{X}_{H+1}\\rightarrow[0,1]$ is the reward function in episode $t$ . ", "page_idx": 27}, {"type": "text", "text": "An interaction between the agent and the adversarial MDP environment proceeds in $T$ episodes, and each episode $t\\in[T]$ proceeds in $H$ steps. The agent begins in an initial state $x_{1}^{t}=x_{1}$ . In step $h\\in[H]$ , the agent observes the current state $x_{h}^{t}\\in\\mathcal{X}$ , picks an action $y_{h}^{t}\\in\\mathcal{Y}$ and transitions to the next state ${\\boldsymbol x}_{h+1}^{t}$ sampled from the transition function $p(\\cdot\\,\\mid x_{h}^{t},y_{h}^{t})$ . At the end of the interaction, the agent arrives in a final state $x_{H+1}^{t}$ and observes the reward $r^{t}(x_{H+1}^{t})$ . For simplicity, we assume that the state space can be decomposed into $H+1$ disjoint subsets $\\bar{\\mathcal{X}}=\\bigcup_{h=1}^{H+1}X_{h}$ such that, in step $h$ of the interaction, the agent is in some state $x_{h}\\in\\mathcal{X}_{h}$ . ", "page_idx": 27}, {"type": "text", "text": "Now, we define the value function in an adversarial MDP, i.e., the expected reward of a policy when interacting with the MDP. ", "page_idx": 27}, {"type": "text", "text": "Definition E.2 (Value function). Let $\\mathcal{M}$ be an adversarial MDP and $\\pi:\\mathcal{X}\\to\\Delta_{\\mathcal{Y}}$ be a policy. The value function $V^{\\pi,t}:\\mathcal{X}\\rightarrow\\mathbb{R}$ of policy $\\pi$ in episode $t$ is defined as $V^{\\pi,t}(x_{h})=\\mathbb{E\\dot{z_{\\pi,p}}}[r^{t}(\\bar{x_{H+1}})\\mid x_{h}]$ for every $h\\in[H]$ and $x_{h}\\in\\mathcal{X}_{h}$ . Similarly, the Q-function $Q^{\\pi,t}:\\mathcal{X}\\!\\times\\!\\mathcal{y}\\!\\to\\!\\mathbb{R}$ is defined $Q^{\\pi,t}(x_{h},y_{h})=$ $\\mathbb{E}_{\\pi,p}[r^{t}(x_{H+1}\\big)\\ \\big]\\ x_{h},y_{h}]$ . ", "page_idx": 27}, {"type": "text", "text": "Next, we consider a regularized version of the adversarial MDP model. Regularized MDPs were also studied recently (see, e.g., Geist et al. [2019], Shani et al. [2020a]). The following definition presents the regularized value with respect to some reference policy $\\mu$ . ", "page_idx": 27}, {"type": "text", "text": "Definition E.3 (Regularized value function). Let $\\mu$ be a reference policy and $\\alpha>0$ be a regularization coefficient. The regularized value function and Q-function of policy $\\pi$ in episode $t$ are defined as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\;V_{\\alpha}^{\\pi,t}(x_{h})=\\mathbb{E}_{\\pi,p}\\Bigg[r^{t}(x_{H+1})-\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\mathrm{KL}(\\pi||\\mu)[x_{h}]\\^{\\textit{\\footnotesize\\texttt{\\vert}}}x_{h}\\Bigg]}\\\\ &{\\quad\\;Q_{\\alpha}^{\\pi,t}(x_{h},y_{h})=\\mathbb{E}_{\\pi,p}\\Bigg[r^{t}(x_{H+1})-\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\mathrm{KL}(\\pi||\\mu)[x_{h}]\\^{\\textit{\\footnotesize\\texttt{\\vert}}}x_{h},y_{h}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now present a 1-step recursive formula and a value difference lemma for the regularized value function. ", "page_idx": 27}, {"type": "text", "text": "Lemma E.4 (Regularized value function recursive relation). Let $\\pi$ be a policy. For every $x_{H+1}\\in$ $\\chi_{H+1}$ , it holds that $V_{\\alpha}^{\\pi,t}(x_{H+1})=r^{t}(x_{H+1})$ . Furthermore, for every $h=1,\\dots,H$ and $(x_{h},y_{h})\\in$ $\\mathcal{X}_{h}\\times\\mathcal{Y}_{v}$ , the following recursive relations hold: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad V_{\\alpha}^{\\pi,t}(x_{h})=\\displaystyle\\sum_{y_{h}\\in\\mathcal{Y}}\\pi(y_{h}\\mid x_{h})Q_{\\alpha}^{\\pi,t}(x_{h},y_{h})}\\\\ &{Q_{\\alpha}^{\\pi,t}(x_{h},y_{h})=\\displaystyle\\sum_{x_{h+1}\\in\\mathcal{X}_{h+1}}p(x_{h+1}\\mid x_{h},y_{h})V_{\\alpha}^{\\pi,t}(x_{h+1})-\\alpha\\mathrm{KL}(\\pi\\|\\mu)[x_{h}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We prove the claim by backwards induction on $h$ . The base case $h\\,=\\,H\\,+\\,1$ follows by definition of the value function and the adversarial MDP. Assuming that the claim holds for $h+1$ , we ", "page_idx": 27}, {"type": "text", "text": "have that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{V_{\\alpha}^{\\pi,t}(x_{h})=\\underset{y=0}{\\overset{\\sum}{\\operatorname{r}}}\\Bigg\\lbrack x^{t}(x_{H+1})-\\alpha\\underset{h\\rightarrow h}{\\overset{H}{\\sum}}\\mathrm{KL}(\\pi\\vert\\vert\\mu\\vert)\\big\\vert x_{h}\\big\\rangle\\Bigg\\rbrack\\;\\;\\;\\boldsymbol{x}_{h}\\Bigg\\rbrack}\\\\ {=}&{\\underset{x=K_{H+1}}{\\overset{\\sum}{\\sum}}\\frac{\\Gamma\\Gamma\\big[x_{H+1}=x\\big]}{\\pi}\\,\\boldsymbol{\\chi}_{h}\\Bigg\\vert^{t}(x)-\\alpha\\underset{h\\rightarrow h}{\\overset{H}{\\sum}}\\Bigg\\lbrack\\sum_{h^{t}=K_{H}+1}^{H}\\frac{\\Gamma\\big[x_{H}/\\pi}{\\Gamma\\big]}\\,\\boldsymbol{\\chi}_{h}^{\\intercal}\\intercal\\boldsymbol{x}}\\\\ &{=\\underset{y\\in\\mathcal{Y}}{\\overset{t}{\\sum}}\\frac{\\Gamma\\big(y_{h}\\big)}{\\pi}\\,\\underset{x\\in\\mathcal{X}_{h+1}}{\\overset{t}{\\sum}}\\frac{\\Gamma\\mathrm{Pr}\\big[x_{H+1}=x\\big]}{\\pi}\\,\\boldsymbol{\\chi}_{h}^{\\intercal}\\boldsymbol{\\mu}^{\\intercal}(x)}\\\\ &{\\qquad-\\alpha\\underset{y\\in\\mathcal{Y}}{\\overset{t}{\\sum}}\\frac{\\pi}{\\pi}\\,\\boldsymbol{\\chi}_{h}^{\\intercal}\\underset{h\\rightarrow k}{\\overset{H}{\\sum}}\\frac{\\Gamma\\mathrm{Pr}\\big[x_{h},\\prime\\big]}{\\Gamma^{\\alpha_{h}}\\times\\pi\\mathcal{X}_{h}^{\\intercal}\\boldsymbol{\\mu}^{\\intercal}}\\,\\boldsymbol{\\chi}_{h}^{\\intercal}(x)}\\\\ &{=\\underset{y\\in\\mathcal{Y}}{\\sum}\\frac{\\pi}{\\pi}\\,\\boldsymbol{\\chi}_{h}^{\\intercal}\\Bigg\\rbrack\\;\\boldsymbol{\\Xi}_{\\pi}\\Bigg\\rbrack\\;\\boldsymbol{r}^{t}(x_{H+1})-\\alpha\\underset{h\\rightarrow h}{\\overset{H}{\\sum}}\\mathrm{KL}(\\pi\\|\\mu\\rangle\\big\\vert\\,\\boldsymbol{x}_{h},y_{h}\\Bigg\\rbrack}\\\\ &{\\underset{y\\in\\mathcal{Y}}{\\sum}\\alpha(y_{h}\\big)\\;\\big\\vert x_{h}\\big\\rangle\\mathbb{Q}_{\\pi}^{\\pi,t}(x_{h},y_{h})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Q_{n}^{\\prime\\prime\\prime}(x_{{\\mathrm{A}}},y_{{\\mathrm{B}}})=\\mathbb{E}_{x_{n}}\\sqrt{n}^{\\prime}(x_{{\\mathrm{I}}}+\\gamma)-\\alpha\\Bigg[\\underbrace{\\gamma}_{{\\mathrm{A}}\\times{\\mathrm{A}}}_{I}\\mathbb{E}_{x_{n}}[\\sigma(\\mathbf{I}\\rho)[x_{{\\mathrm{F}}}]\\mid x_{{\\mathrm{I}}},y_{{\\mathrm{B}}}]\\Bigg]}\\\\ &{\\qquad=\\underset{x_{n}\\to{\\mathrm{A}}_{\\mathrm{Fro}}}{\\sum}\\underset{y_{n}\\to{\\mathrm{Fro}}}{\\sum}[x_{{\\mathrm{I}}},\\sigma(x_{n})=x_{n},y_{n}]{\\mathrm{Fro}}(x)}\\\\ &{\\qquad\\quad-\\alpha\\frac{\\beta}{{\\mathrm{Fro}}}\\underset{x_{n}\\to{\\mathrm{A}}_{\\mathrm{Fro}}}{\\sum}\\underset{y_{n}\\to{\\mathrm{Fro}}}{\\sum}[x_{{\\mathrm{Fro}}},x_{{\\mathrm{B}}}]\\mathbb{E}_{\\mathbf{I}}(\\mathbf{I}\\mathbf{I}\\mathbf{I}\\mathbf{I}\\rho)[x]}\\\\ &{\\qquad=\\underset{x_{n}\\to{\\mathrm{A}}_{\\mathrm{Fro}}}{\\sum}\\underset{y_{n}\\to{\\mathrm{Fro}}}{\\sum}[x_{{\\mathrm{A}}},y_{n}]{\\mathrm{Fro}}(x_{n})\\sum_{\\alpha={\\mathrm{B}}}\\mathbb{E}_{\\mathbf{I}\\mathbf{I}\\mathbf{I}}_{+\\alpha}[x_{n+1}]r(x)}\\\\ &{\\qquad=\\underset{x_{n}\\to{\\mathrm{A}}_{\\mathrm{Fro}}}{\\sum}\\underset{y_{n}\\to{\\mathrm{Fro}}}{\\sum}[x_{{\\mathrm{A}}+1}]x_{\\alpha,{\\mathrm{H}}}[x_{n},y_{n}]\\underset{x_{n}\\to{\\mathrm{Fro}}}{\\sum}\\underset{y_{n}\\to{\\mathrm{Fro}}}{\\sum}[x_{{\\mathrm{A}}},y_{\\alpha}][x_{n}][x_{{\\mathrm{I}}}]}\\\\ &{\\qquad=\\alpha{\\mathrm{B}}[(\\mathbf{I}\\rho)[x_{{\\mathrm{I}}}]]}\\\\ &{\\qquad=\\underset{x_{n}\\to{\\mathrm{A}}_{\\mathrm{Fro}}}{\\sum}[x_{{\\mathrm{A}}}]\\mathbb{E}_{x_{n}}[y_{\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma E.5 (Regularized Value Difference Lemma). Let $\\pi,\\pi^{\\prime}$ be two policies. Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\nV_{\\alpha}^{\\pi,t}(x_{1})-V_{\\alpha}^{\\pi^{\\prime},t}(x_{1})=\\mathbb{E}_{\\pi^{\\prime},p}\\Bigg[\\sum_{h=1}^{H}\\langle\\pi-\\pi^{\\prime},Q_{\\alpha}^{\\pi,t}\\rangle[x_{h}]+\\alpha\\mathrm{KL}(\\pi^{\\prime}\\|\\mu)[x_{h}]-\\alpha\\mathrm{KL}(\\pi\\|\\mu)[x_{h}]\\Bigg].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Let $x_{h}\\in\\mathcal{X}_{h}$ . First, by Lemma E.5, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\alpha}^{\\pi,t}(x_{h})\\!-\\!V_{\\alpha}^{\\pi^{\\prime},t}(x_{h})=}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\left\\langle\\pi,Q_{\\alpha}^{\\pi,t}\\right\\rangle\\!\\left[x_{h}\\right]-\\left\\langle\\pi^{\\prime},Q_{\\alpha}^{\\pi^{\\prime},t}\\right\\rangle\\!\\left[x_{h}\\right]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\left\\langle\\pi-\\pi^{\\prime},Q_{\\alpha}^{\\pi,t}\\right\\rangle\\!\\left[x_{h}\\right]+\\left\\langle\\pi^{\\prime},Q_{\\alpha}^{\\pi,t}-Q_{\\alpha}^{\\pi^{\\prime},t}\\right\\rangle\\!\\left[x_{h}\\right]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\left\\langle\\pi-\\pi^{\\prime},Q_{\\alpha}^{\\pi,t}\\right\\rangle\\!\\left[x_{h}\\right]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\left\\langle\\pi-\\pi^{\\prime},Q_{\\alpha}^{\\pi,t}\\right\\rangle\\!\\left[x_{h}\\right]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that for $h=H+1$ for any $\\pi,x_{H+1}$ we have $V_{\\alpha}^{\\pi,t}(x_{H+1})=r^{t}(x_{H+1})$ . By recursively unrolling the above relation, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\alpha}^{\\pi,t}(x_{h})-V_{\\alpha}^{\\pi^{\\prime},t}(x_{h})=\\mathbb{E}_{\\pi^{\\prime},p}\\Bigg[\\underset{h^{\\prime}=h}{\\sum}\\left\\langle\\pi-\\pi^{\\prime},Q_{\\alpha}^{\\pi,t}\\right\\rangle[x_{h^{\\prime}}]\\mid x_{h}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\;\\alpha\\mathbb{E}_{\\pi^{\\prime},p}\\Bigg[\\underset{h^{\\prime}=h}{\\sum}\\mathrm{KL}(\\pi^{\\prime}||\\mu)[x_{h^{\\prime}}]-\\mathrm{KL}(\\pi||\\mu)[x_{h^{\\prime}}]\\mid x_{h}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Taking the expectation over the initial state finishes the proof. ", "page_idx": 29}, {"type": "text", "text": "E.2 Algorithm 1: mirror descent policy optimization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We define the following mirror descent policy optimization algorithm. In the first episode the algorithm plays the reference policy, i.e., $\\pi_{1}\\,=\\,\\mu$ . Then, its update rule for iteration $(t+1)$ is as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi_{t+1}(\\cdot\\,|\\;x_{h})=\\arg\\operatorname*{max}_{\\pi}\\eta_{t}\\bigl\\langle\\pi,Q_{\\alpha}^{\\pi_{t},t}\\bigr\\rangle[x_{h}]-\\alpha\\eta_{t}\\mathrm{KL}(\\pi\\|\\mu)[x_{h}]-(1-\\alpha\\eta_{t})\\mathrm{KL}(\\pi\\|\\pi_{t})[x_{h}],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\eta_{t}$ is a learning rate. The solution can also be made explicit in the following form: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi_{t+1}(y_{h}\\mid x_{h})\\propto\\mu(y_{h}\\mid x_{h})^{\\alpha\\eta_{t}}\\pi_{t}(y_{h}\\mid x_{h})^{1-\\alpha\\eta_{t}}e^{\\eta_{t}Q_{\\alpha}^{\\pi_{t},t}(x_{h},y_{h})}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To show this, note that by the definition of the $\\mathrm{KL}$ , we can write this update rule differently: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi_{t+1}(\\cdot\\mid x_{h})=\\arg\\operatorname*{max}_{\\pi}\\eta_{t}\\sum_{y_{h}\\in\\mathcal{Y}}\\pi(y_{h}\\mid x_{h})\\bigg(Q_{\\alpha}^{\\pi_{t},t}(x_{h},y_{h})-\\alpha\\log\\frac{\\pi_{t}(y_{h}\\mid x_{h})}{\\mu(y_{h}\\mid x_{h})}\\bigg)-\\mathrm{KL}(\\pi\\|\\pi_{t})[x_{h}].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This is exactly the MD step for policy optimization [Orabona, 2019, Shani et al., 2020a]. Thus, the solution in its explicit form is: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{t+1}\\!\\left(y_{h}\\mid x_{h}\\right)\\propto\\pi_{t}\\!\\left(y_{h}\\mid x_{h}\\right)\\!e^{\\eta_{t}\\left(Q_{\\alpha}^{\\pi_{t},t}\\left(x_{h},y_{h}\\right)-\\alpha\\log\\frac{\\pi_{t}\\left(y_{h}\\mid x_{h}\\right)}{\\mu\\left(y_{h}\\mid x_{h}\\right)}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We recover Equation (4) by noticing that $\\begin{array}{r}{\\exp\\Bigl(-\\alpha\\eta_{t}\\log\\frac{\\pi_{t}(y_{h}|x_{h})}{\\mu(y_{h}|x_{h})}\\Bigr)=\\pi_{t}(y_{h}\\mid x_{h})^{-\\alpha\\eta_{t}}\\mu(y_{h}\\mid x_{h})^{\\alpha\\eta_{t}}.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "E.3 Analysis of algorithm 1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma E.6 (Fundamental inequality of mirror descent policy optimization for regularized adversarial MDPs). The following holds when running mirror descent policy optimization (Equation (3)) in $a$ regularized adversarial MDP, for every policy $\\pi$ and every episode $t$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}_{p}(\\pi||\\pi_{t+1})\\leq(1-\\eta_{t}\\alpha)\\mathrm{KL}_{p}(\\pi||\\pi_{t})+2\\eta_{t}^{2}\\mathbb{E}_{\\pi,p}\\Bigg[\\sum_{h=1}^{H}\\left\\|Q_{\\alpha}^{\\pi_{t},t}(x_{h},\\cdot)-\\alpha\\log\\frac{\\pi_{t}\\left(\\cdot\\;\\middle|\\;x_{h}\\right)}{\\mu(\\cdot\\;|\\;x_{h})}\\right\\|_{\\infty}^{2}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;\\eta_{t}\\big(V_{\\alpha}^{\\pi_{t},t}(x_{1})-V_{\\alpha}^{\\pi,t}(x_{1})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Fix a state $x_{h}\\in\\mathcal{X}_{h}$ . We start by applying Munos et al. [2023, Lemma 2] with $\\pi^{+}=\\pi_{t+1}$ , $\\pi^{-}=\\pi_{t}$ and the vector $\\begin{array}{r}{\\delta(y)=\\eta_{t}\\Big(Q_{\\alpha}^{\\pi_{t},t}(x_{h},y)-\\alpha\\log\\frac{\\pi_{t}(y|x_{h})}{\\mu(y|x_{h})}\\Big)}\\end{array}$ . This implies that for any policy $\\pi$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\pi||\\pi_{t+1})[x_{h}]\\leq\\mathrm{KL}(\\pi||\\pi_{t})[x_{h}]+2\\eta_{t}^{2}\\,\\Big\\|Q_{\\alpha}^{\\pi_{t},t}(x_{h},\\cdot)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\,|\\,x_{h})}{\\mu(\\cdot\\,|\\,x_{h})}\\Big\\|_{\\infty}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\eta_{t}\\Big\\langle\\pi_{t}(\\cdot\\,|\\,x_{h})-\\pi(\\cdot\\,|\\,x_{h}),Q_{\\alpha}^{\\pi_{t},t}(x_{h},\\cdot)-\\alpha\\log\\frac{\\pi_{t}(\\cdot\\,|\\,x_{h})}{\\mu(\\cdot\\,|\\,x_{h})}\\Big\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}_{p}\\big(\\pi\\|\\pi_{t+1}\\big)=\\mathbb{E}_{\\pi,p}\\Bigg[\\displaystyle\\sum_{h=1}^{H}\\mathrm{KL}\\big(\\pi\\|\\pi_{t+1}\\big)\\big[x_{h}\\big]\\Bigg]}\\\\ &{\\qquad\\qquad\\le\\mathbb{E}_{\\pi,p}\\Bigg[\\displaystyle\\sum_{h=1}^{H}\\mathrm{KL}\\big(\\pi\\|\\pi_{t}\\big)\\big[x_{h}\\big]\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad+\\,2\\eta_{t}^{2}\\mathbb{E}_{\\pi,p}\\Bigg[\\displaystyle\\sum_{h=1}^{H}\\Bigg\\|Q_{\\alpha}^{\\pi_{t},t}\\big(x_{h},\\cdot\\big)-\\alpha\\log\\displaystyle\\frac{\\pi_{t}\\big(\\cdot\\,\\big|\\,x_{h}\\big)}{\\mu(\\cdot\\,|\\,x_{h})}\\Bigg\\|_{\\infty}^{2}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad+\\,\\eta_{t}\\mathbb{E}_{\\pi,p}\\Bigg[\\displaystyle\\sum_{h=1}^{H}\\Big\\langle\\pi_{t}\\big(\\cdot\\,|\\,x_{h}\\big)-\\pi\\big(\\cdot\\,|\\,x_{h}\\big),Q_{\\alpha}^{\\pi_{t},t}\\big(x_{h},\\cdot\\big)-\\alpha\\log\\displaystyle\\frac{\\pi_{t}(\\cdot\\,|\\,x_{h})}{\\mu(\\cdot\\,|\\,x_{h})}\\Big\\rangle\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that ", "text_level": 1, "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg\\langle\\pi_{t}(\\cdot\\mid x_{h})-\\pi(\\cdot\\mid x_{h}),\\log\\frac{\\pi_{t}(\\cdot\\mid x_{h})}{\\mu(\\cdot\\mid x_{h})}\\bigg\\rangle=}\\\\ &{\\qquad=\\mathrm{KL}(\\pi_{t}\\|\\mu)\\big[x_{h}\\big]-\\bigg\\langle\\pi(\\cdot\\mid x_{h}),\\log\\frac{\\pi_{t}(\\cdot\\mid x_{h})}{\\mu(\\cdot\\mid x_{h})}\\bigg\\rangle}\\\\ &{\\qquad=\\mathrm{KL}(\\pi_{t}\\|\\mu)\\big[x_{h}\\big]-\\bigg\\langle\\pi(\\cdot\\mid x_{h}),\\log\\frac{\\pi_{t}(\\cdot\\mid x_{h})}{\\mu(\\cdot\\mid x_{h})}\\bigg\\rangle+\\mathrm{KL}(\\pi\\|\\mu)\\big[x_{h}\\big]-\\mathrm{KL}(\\pi\\|\\mu)\\big[x_{h}\\big]}\\\\ &{\\qquad=\\mathrm{KL}(\\pi_{t}\\|\\mu)\\big[x_{h}\\big]+\\mathrm{KL}(\\pi\\|\\pi_{t})\\big[x_{h}\\big]-\\mathrm{KL}(\\pi\\|\\mu)\\big[x_{h}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\pi\\|\\mu)[x_{h}]\\!-\\!\\Bigg\\langle\\pi(\\cdot\\!\\mid x_{h}),\\log\\frac{\\pi_{t}(\\cdot\\!\\mid x_{h})}{\\mu(\\cdot\\!\\mid x_{h})}\\Bigg\\rangle=}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\displaystyle\\sum_{y_{h}\\in\\mathcal{Y}}\\pi(y_{h}\\mid x_{h})\\Bigg(\\log\\frac{\\pi(y_{h}\\mid x_{h})}{\\mu(y_{h}\\mid x_{h})}-\\log\\frac{\\pi_{t}(y_{h}\\mid x_{h})}{\\mu(y_{h}\\mid x_{h})}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\displaystyle\\sum_{y_{h}\\in\\mathcal{Y}}\\pi(y_{h}\\mid x_{h})\\log\\frac{\\pi(y_{h}\\mid x_{h})}{\\pi_{t}(y_{h}\\mid x_{h})}}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\mathrm{KL}(\\pi\\|\\pi_{t})[x_{h}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname{Ku}_{s}(\\tau|\\tau|\\tau|\\tau_{+1})\\leq\\mathbb{E}_{\\sigma_{\\tau}}\\Bigg[\\frac{\\sqrt{\\lambda}}{\\Delta}\\operatorname{LG}_{s}\\Bigg(\\tau|\\tau|\\tau_{+1}\\Big)\\Bigg]}\\\\ &{\\quad+\\mathcal{D}_{\\sigma_{\\tau}}\\Bigg[\\frac{\\sqrt{\\lambda}}{2}\\Bigg(\\tau^{2}\\Delta\\tau_{+}-\\sigma_{\\tau}\\Big)-\\operatorname{odd}_{\\sigma_{\\tau}}\\frac{\\pi_{+}(\\tau+\\tau_{+1})}{\\mu_{0}(\\tau+\\tau_{+1})}\\Bigg]_{\\tau_{+}}^{2}\\Bigg]}\\\\ &{\\quad+\\eta\\sum_{s}\\sqrt{\\sum_{i=1}^{N}\\bigg(\\tau^{2}\\Delta\\tau_{+}-Q_{\\sigma_{\\tau}}^{2}\\Big)^{2}[\\lambda_{\\sigma_{\\tau}}]}}\\\\ &{\\quad-\\eta\\exp_{\\sigma_{\\tau}}\\Bigg[\\frac{\\sqrt{\\lambda}}{\\Delta}\\mathrm{LG}_{s}\\Bigg(\\tau|\\eta|\\tau_{+1}\\Big)[\\lambda_{\\sigma_{\\tau}}]+\\mathrm{KH}_{\\sigma}\\big(\\tau|\\tau_{+1}\\big)[\\sigma_{\\tau}]-\\mathrm{KH}_{\\sigma}\\big(\\tau|\\eta|\\tau_{+1}\\Big)}\\\\ &{\\quad+(1-\\eta)\\log_{\\sigma_{\\tau}}\\Bigg[\\frac{\\sqrt{\\lambda}}{\\Delta}\\mathrm{LG}_{s}\\Bigg(\\tau|\\eta|\\tau_{+1}\\Big)[\\sigma_{\\tau}]\\Bigg]}\\\\ &{\\quad+\\frac{2}{3}\\eta^{2}\\mathbb{E}_{\\sigma_{\\tau}}\\Bigg[\\frac{\\sqrt{\\lambda}}{\\Delta}\\Bigg]\\Bigg[Q_{\\sigma}^{2}\\sigma_{\\tau}^{2}(\\tau_{+1},\\sigma_{+})-\\operatorname{odd}_{\\sigma_{\\tau}}\\frac{\\pi_{+}(\\tau+\\tau_{+1})}{\\mu_{0}(\\tau+\\tau_{+1})}\\Bigg]_{\\tau_{+}}^{2}\\Bigg]}\\\\ &{\\quad+\\eta\\sum_{s}\\sqrt{\\sum_{i=1}^{N}\\bigg(\\tau\\frac{\\lambda_{\\sigma}}{\\Delta}\\tau_{+}-Q_{\\sigma_{\\tau}}^{2}\\sigma_{\\tau}^{2}\\big)[\\lambda_{\\sigma_{\\tau}}]+\\mathrm{KH}_{\\sigma}\\big(\\tau|\\eta|\\tau_{+1}\\big)[\\sigma_ \n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the third relation is by Lemmas 3.1 and E.5. ", "page_idx": 31}, {"type": "text", "text": "E.4 Algorithm 2: mixture mirror descent policy optimization ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Define the mixture policy in iteration $t$ as: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\pi_{t}^{\\alpha}(y\\mid x)={\\frac{\\pi_{t}(y\\mid x)^{1-\\eta_{t}\\alpha}\\mu(y\\mid x)^{\\eta_{t}\\alpha}}{\\sum_{y^{\\prime}\\in{\\mathcal{Y}}}\\pi_{t}(y^{\\prime}\\mid x)^{1-\\eta_{t}\\alpha}\\mu(y^{\\prime}\\mid x)^{\\eta_{t}\\alpha}}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now define the following mixture mirror descent policy optimization algorithm. In the first episode the algorithm plays the reference policy, i.e., $\\pi_{1}=\\mu$ . Then, its update rule for iteration $(t+1)$ is as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\pi_{t+1}(\\cdot\\mid x_{h})=\\arg\\operatorname*{max}_{\\pi}\\eta_{t}\\sum_{y_{h}\\in\\mathcal{Y}}\\pi(y_{h}\\mid x_{h})Q_{\\alpha}^{\\pi_{t}^{\\alpha},t}(x_{h},y_{h})-\\mathrm{KL}(\\pi\\|\\pi_{t}^{\\alpha})[x_{h}].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The solution can be made explicit: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\pi_{t+1}(y_{h}\\mid x_{h})\\propto\\pi_{t}^{\\alpha}(y_{h}\\mid x_{h})e^{\\eta_{t}Q_{\\alpha}^{\\pi_{t}^{\\alpha},t}(x_{h},y_{h})}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "E.5 Analysis of algorithm 2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Lemma E.7 (Fundamental inequality of mixture mirror descent policy optimization for regularized adversarial MDPs). The following holds when running mixture mirror descent policy optimization (Equation (5)) in a regularized adversarial MDP, for every policy $\\pi$ and every episode $t$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}_{p}(\\pi||\\pi_{t+1})\\leq(1-\\eta_{t}\\alpha)\\mathrm{KL}_{p}(\\pi||\\pi_{t})+2\\eta_{t}^{2}\\mathbb{E}_{\\pi,p}\\Bigg[\\sum_{h=1}^{H}\\left\\|Q_{\\alpha}^{\\pi_{t}^{\\alpha},t}(x_{h},\\cdot)\\right\\|_{\\infty}^{2}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\eta_{t}\\Big(V_{\\alpha}^{\\pi_{t}^{\\alpha},t}(x_{1})-V_{\\alpha}^{\\pi,t}(x_{1})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Fix a state $x_{h}\\in\\mathcal{X}_{h}$ . We start by applying Munos et al. [2023, Lemma 2] with $\\pi^{+}=\\pi_{t+1}$ , $\\pi^{-}=\\pi_{t}^{\\alpha}$ and the vector $\\delta(y)=\\eta_{t}Q_{\\alpha}^{\\pi_{t}^{\\alpha},t}(x_{h},y)$ . This implies that for any policy $\\pi$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}(\\pi||\\pi_{t+1})[x_{h}]\\leq\\mathrm{KL}(\\pi||\\pi_{t}^{\\alpha})[x_{h}]+\\eta_{t}\\Big\\langle\\pi_{t}^{\\alpha}-\\pi,Q_{\\alpha}^{\\pi_{t}^{\\alpha},t}\\Big\\rangle[x_{h}]+2\\eta_{t}^{2}\\left\\|Q_{\\alpha}^{\\pi_{t}^{\\alpha},t}(x_{h},\\cdot)\\right\\|_{\\infty}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Next, we plug this into Lemma 3.1 to obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{k}(\\mathbf{j}_{\\alpha}\\left\\vert\\nabla_{\\beta}\\right\\vert)_{\\mathbf{t}+\\mathbf{j}_{\\alpha}}+\\mathbf{j}_{\\alpha}\\sum_{i=1}^{\\infty}[\\frac{\\mathbf{k}(\\mathbf{j}_{\\alpha}\\left\\vert\\nabla_{\\beta}\\right\\vert)}{\\mathbf{k}(\\mathbf{j}_{\\alpha}\\left\\vert\\nabla_{\\beta}\\right\\vert)}][\\mathbf{z}_{\\alpha}]\\Bigg\\vert}&{}\\\\ {\\leq\\mathbb{E}_{\\mathbf{k}}\\Bigg[\\frac{1}{2}\\mathbf{k}(\\mathbf{j}_{\\alpha}\\left\\vert\\nabla_{\\beta}\\right\\vert)^{\\alpha}\\left\\vert\\mathbf{k}_{\\alpha}\\right\\vert}&{}\\\\ {\\phantom{\\frac{1}{2}}+\\eta\\beta_{\\alpha}\\sum_{i=1}^{\\infty}[\\frac{\\mathbf{k}(\\mathbf{j}_{\\alpha}\\left\\vert\\nabla_{\\beta}\\right\\vert)}{\\mathbf{k}(\\mathbf{j}_{\\alpha}\\left\\vert\\nabla_{\\beta}\\right\\vert)}][\\mathbf{z}_{\\beta}]+\\frac{2\\eta^{2}\\delta_{\\alpha}}{3}\\rho_{\\alpha}\\frac{\\left\\vert\\mathbf{k}_{\\alpha}\\right\\vert^{2}}{\\left\\vert\\mathbf{k}\\right\\vert}\\left\\vert\\mathbf{g}_{i}^{\\alpha\\beta;\\prime\\,(\\mathbf{r}_{\\alpha},\\cdot)}\\right\\vert\\frac{2}{\\rho_{\\alpha}}\\Bigg]}\\\\ {\\leq\\mathbb{E}_{\\mathbf{k}}\\Bigg[\\frac{1}{2}\\left(1-\\eta_{0}\\right)\\mathrm{k}[\\alpha_{\\alpha}^{\\mathrm{par}}\\left\\vert\\nabla_{\\beta}\\right\\vert)\\left\\vert\\mathbf{z}_{\\alpha}\\right\\vert+\\eta_{0}\\mathrm{k}[\\Gamma_{\\beta}][\\mu_{\\alpha}]\\left\\vert\\mathbf{z}_{\\alpha}\\right\\vert-\\eta_{0}\\mathrm{k}[\\delta_{\\alpha}^{\\mathrm{par}}][\\mu_{\\alpha}]\\Bigg],}\\\\ {\\phantom{\\frac{1}{2}}+\\eta\\delta_{\\alpha}\\sum_{i=1}^{\\infty}\\Bigg[\\frac{\\left(\\mathbf{k}_{\\alpha}^{\\mathrm{par}}\\left\\vert\\nabla_{\\beta}\\right\\vert-\\mathbf{r}_{\\alpha}^{\\mathrm{par}}\\left\\vert\\nabla_{\\beta}\\right\\vert\\right)}{\\left\\vert\\mathbf{r}_{\\alpha}\\right\\vert}\\Big]+\\eta_{0}^{2}\\mathbb{E}_{\\mathbf{k}}\\Bigg[\\frac{\\left(\\mathbf{k}_{\\alpha}^{\\mathrm{par}}\\left\\vert\\nabla_{\\beta}\\right\\vert\\right)}{\\left\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality is by Munos et al. [2023, Lemma 1], and the last relation are by Lemmas 3.1 and E.5. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "E.6 Bounding the Q-function ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Define $\\mu_{\\mathrm{min}}$ as the minimal positive probability assigned by the reference policy $\\mu$ , i.e., $\\mu_{\\mathrm{min}}\\,=$ $\\operatorname*{min}_{(x,y)\\in{\\mathcal{X}}\\times{\\mathcal{Y}}:\\mu(y|x)>0}\\mu(y\\mid x)$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma E.8. For the choice $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\alpha(t+2)}}\\end{array}$ , in every iteration $t$ of mirror descent policy optimization it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\alpha(H+1)\\log\\mu_{\\operatorname*{min}}\\le Q_{\\alpha}^{\\pi_{t},t}(x,y)-\\alpha\\log\\frac{\\pi_{t}(y\\mid x)}{\\mu(y\\mid x_{h})}\\le2-\\alpha H\\log\\mu_{\\operatorname*{min}}\\qquad\\forall(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Follows directly from Lemmas E.9 and E.10. ", "page_idx": 32}, {"type": "text", "text": "Lemma E.9. For every iteration $t$ of mirror descent policy optimization it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\alpha H\\log\\mu_{\\operatorname*{min}}\\leq Q_{\\alpha}^{\\pi_{t},t}(x,y)\\leq1\\qquad\\forall(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The same holds for $Q_{\\alpha}^{\\pi_{t}^{\\alpha},t}$ when running mixture mirror descent policy optimization. ", "page_idx": 32}, {"type": "text", "text": "Proof. By the OMD optimization problem, $\\pi_{t}(y\\mid x)$ will not be positive unless $\\mu(y\\mid x)>0$ (the same holds for $\\pi_{t}^{\\alpha}$ ). Thus, for every $x\\in\\mathscr{X}$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{0\\leq\\mathrm{KL}(\\pi_{t}\\|\\mu)[x]=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\pi_{t}(y\\mid x)\\log\\frac{\\pi_{t}(y\\mid x)}{\\mu(y\\mid x)}}&{}\\\\ {=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\pi_{t}(y\\mid x)\\log\\pi_{t}(y\\mid x)-\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\pi_{t}(y\\mid x)\\log\\mu(y\\mid x)}&{}\\\\ {\\leq\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\pi_{t}(y\\mid x)\\log\\frac{1}{\\mu(y\\mid x)}\\leq\\log\\frac{1}{\\mu_{\\mathrm{min}}}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, by definition, the Q-function is bounded from above by 1 and from below by $\\alpha H\\log\\mu_{\\mathrm{min}}$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma E.10. For the choice \u03b7t =\u03b1(t2+2), in every iteration $t$ of mirror descent policy optimization it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\nH\\log\\mu_{\\mathrm{min}}-\\frac{1}{\\alpha}\\leq\\log\\frac{\\pi_{t}(y\\mid x)}{\\mu(y\\mid x)}\\leq\\log\\frac{1}{\\mu_{\\mathrm{min}}}\\qquad\\forall(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. For the upper bound, notice that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\log\\frac{\\pi_{t}(y\\mid x)}{\\mu(y\\mid x)}=\\log\\pi_{t}(y\\mid x)-\\log\\mu(y\\mid x)\\leq\\log\\frac{1}{\\mu(y\\mid x)}\\leq\\log\\frac{1}{\\mu_{\\operatorname*{min}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the lower bound, we repeat a similar analysis to that in Shani et al. [2020a, Lemma 25]. We start by bounding the partition function as follows ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\displaystyle\\sum_{y^{\\prime}\\in y}\\pi_{t}(y^{\\prime}\\,|\\,x)e^{\\eta_{t}\\left(Q_{\\alpha}^{\\pi_{t},t}(x,y^{\\prime})-\\alpha\\log\\frac{\\pi_{t}(y^{\\prime}\\mid x)}{\\mu(y^{\\prime}\\mid x)}\\right)}\\le\\log\\displaystyle\\sum_{y^{\\prime}\\in y}\\pi_{t}(y^{\\prime}\\,|\\,x)e^{\\eta_{t}\\left(1-\\alpha\\log\\frac{\\pi_{t}(y^{\\prime}\\mid x)}{\\mu(y^{\\prime}\\mid x)}\\right)}}&{}\\\\ {=\\eta_{t}+\\log\\displaystyle\\sum_{y^{\\prime}\\in y}\\pi_{t}(y^{\\prime}\\,|\\,x)\\Bigg(\\frac{\\mu(y^{\\prime}\\,|\\,x)}{\\pi_{t}(y^{\\prime}\\,|\\,x)}\\Bigg)^{\\eta_{t}\\alpha}}&{}\\\\ {\\le\\eta_{t}+\\log\\left(\\displaystyle\\sum_{y^{\\prime}\\in y}\\pi_{t}(y^{\\prime}\\,|\\,x)\\frac{\\mu(y^{\\prime}\\,|\\,x)}{\\pi_{t}(y^{\\prime}\\,|\\,x)}\\right)^{\\eta_{t}\\alpha}=\\eta_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the first inequality follows since $Q_{\\alpha}^{\\pi_{t},t}(x,y)\\leq1$ , and the second uses Jensen inequality (with the fact that $\\eta_{t}\\alpha\\leq1$ ). Now, by the update rule of $\\pi_{t}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\frac{\\pi_{t+1}(y\\mid x)}{\\mu(y\\mid x)}=\\log\\frac{\\pi_{t}(y\\mid x)}{\\mu(y\\mid x)}+\\eta_{t}Q_{\\alpha}^{\\pi_{t},t}(x,y)-\\eta_{t}\\alpha\\log\\frac{\\pi_{t}(y\\mid x)}{\\mu(y\\mid x)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\log\\sum_{y^{\\prime}\\in\\mathcal{Y}}\\pi_{t}(y^{\\prime}\\mid x)e^{\\eta_{t}\\left(Q_{\\alpha}^{\\pi_{t},t}(x,y^{\\prime})-\\alpha\\log\\frac{\\pi_{t}(y^{\\prime}\\mid x)}{\\mu(y^{\\prime}\\mid x)}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq(1-\\eta_{t}\\alpha)\\log\\frac{\\pi_{t}(y\\mid x)}{\\mu(y\\mid x)}+\\eta_{t}Q_{\\alpha}^{\\pi_{t},t}(x,y)-\\eta_{t}}\\\\ &{\\qquad\\qquad\\qquad\\geq(1-\\eta_{t}\\alpha)\\log\\frac{\\pi_{t}(y\\mid x)}{\\mu(y\\mid x)}+\\eta_{t}(\\alpha H\\log\\mu_{\\operatorname*{min}}-1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To finish the proof plug in $\\eta_{t}$ and unroll $t$ to $0$ . ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The abstract and intro make the following claims: (i) formalizing the multiturn setting, which is then presented in Section 3; (ii) presenting multi-turn algorithms and proving their convergence, the algorithms are presented and analyzed in Section 4; (ii) demonstrating the multi-turn algorithms are superior empirically, the experiments are detailed in Sections 5 and 6. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: See paragraph on limitations in the end of the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The theoretical results of the paper are provided in Sections 3 and 4. The full proofs of all the claims are found in Appendices A and B. In addition, a proof sketch of the main theoretical result (Theorem 4.2) is found in the main paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Section 5 describes how the experimental setup is created, and Appendix C presents the exact prompts that were used. Section 4.1 gives the implementation details for our algorithms, and a list of hyperparameters is provided in Appendix D. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 35}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We publicly release the data of the Education Dialogue environment. Due to technical difficulties, we currently do not release code. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All the details about the data, the experimental environments, and the hyperparameters for the algorithms and how they were chosen are described in Sections 5 and 6 and appendix D. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All the algorithms are evaluated using 3 different seed and the results are computed on $3\\times3$ comparisons matrices, where we report the mean and the standard deviation. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The computer resources for rour experiments are detailed in Sections 5 and 6. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS code of ethics and made sure that our paper conforms with it. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This is a foundational research paper that presents generic algorithms to improve the alignment of LLMs with reinforcement learning algorithms. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: All the released data is generated by prompting Gemini, we release the exact prompts as well. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All data/environments that we used were cited accordingly. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Section 5 describes the exact way in which we created the Education Dialogue environments, and Appendix C provides the exact prompts that we used. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]