[{"figure_path": "Dokew2u49m/figures/figures_0_1.jpg", "caption": "Figure 1: Illustration of C-Flat overcoming catastrophe forgetting by fine-tuning the old model parameter to flat minima of new task. a) loss minima for current task only can cause catastrophe forgetting on previous ones. b) balanced optima aligned by regularization leads to unsatisfying results for both old and new tasks. c) C-Flat seeks global optima for all tasks with flattened loss landscape.", "description": "This figure illustrates the core idea of C-Flat, comparing it to direct tuning and regularization-based approaches. Direct tuning (a) leads to catastrophic forgetting. Regularization (b) provides a balance but suboptimal results. C-Flat (c) achieves better generalization by finding a global optimum with a flatter loss landscape, preventing catastrophic forgetting.", "section": "3 Method"}, {"figure_path": "Dokew2u49m/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of C-Flat overcoming catastrophe forgetting by fine-tuning the old model parameter to flat minima of new task. a) loss minima for current task only can cause catastrophe forgetting on previous ones. b) balanced optima aligned by regularization leads to unsatisfying results for both old and new tasks. c) C-Flat seeks global optima for all tasks with flattened loss landscape.", "description": "This figure illustrates three different approaches to continual learning (CL) and how they address the issue of catastrophic forgetting.  (a) Direct Tuning shows that focusing solely on minimizing the loss of the new task leads to catastrophic forgetting of previous tasks. (b) Regularization attempts to balance the optimization between new and old tasks, but may still lead to suboptimal performance. (c) C-Flat, the proposed method, aims to find a global optima with a flattened loss landscape, thus improving the generalization ability and preventing catastrophic forgetting. The figure highlights the difference in the loss landscape and the optimal points achieved by each method.", "section": "3 Method"}, {"figure_path": "Dokew2u49m/figures/figures_6_1.jpg", "caption": "Figure 2: The Hessian eigenvalues and the traces at epochs 50, and 150 on B0_Inc10 setting (MEMO, CIFAR-100) w/ and w/o C-Flat plugged in.", "description": "This figure displays the distribution of Hessian eigenvalues and their traces for MEMO (Memory-based method) on the CIFAR-100 dataset with B0_Inc10 setting (10 classes per increment).  It compares the results with and without the application of C-Flat optimization.  The plots visualize how C-Flat alters the eigenvalue distribution, leading to flatter minima and a smaller trace, indicating improved model generalization and a less sharp loss landscape. The trace is a measure of the overall flatness of the loss landscape, while the maximum eigenvalue represents the sharpness of the loss function.  The results showcase that C-Flat leads to flatter minima with smaller eigenvalues and trace values, resulting in improved generalization ability in continual learning.", "section": "4.3 Hessian Eigenvalues and Hessian Traces"}, {"figure_path": "Dokew2u49m/figures/figures_7_1.jpg", "caption": "Figure 3: The parametric loss landscapes of Replay (Mem.), WA (Reg.) and MEMO (Exp.) are plotted by perturbing the model parameters at the end of training (CIFAR-100, B0_Inc10) across the first two Hessian eigenvectors.", "description": "This figure visualizes the loss landscapes of three different continual learning methods: Replay, WA, and MEMO, both with and without the application of C-Flat. By perturbing model parameters around the training completion point (CIFAR-100 dataset, B0_Inc10 setting), the plots illustrate the impact of C-Flat on the shape of the loss landscape, highlighting how it leads to flatter minima compared to the baselines.  The visualization uses the first two Hessian eigenvectors.", "section": "4.4 Visualization of Landscapes"}, {"figure_path": "Dokew2u49m/figures/figures_7_2.jpg", "caption": "Figure 4: C-Flat vs. Zero-order flatness", "description": "This figure compares the performance of C-Flat against a zeroth-order flatness method across different continual learning settings (B0_Inc10 and B0_Inc20).  It shows average accuracy across multiple continual learning methods (Replay, iCaRL, WA, PODNet, DER, FOSTER, MEMO). The results indicate that C-Flat consistently outperforms the zeroth-order sharpness method, demonstrating its effectiveness in improving model generalization.", "section": "4.4 Visualization of Landscapes"}, {"figure_path": "Dokew2u49m/figures/figures_7_3.jpg", "caption": "Figure 5: Analysis of computation overhead", "description": "The figure analyzes the computation overhead of the proposed C-Flat optimizer compared to other methods (SGD, SAM, GAM).  Subfigure (a) shows the convergence speed and accuracy, demonstrating that C-Flat converges faster and achieves higher accuracy with fewer iterations.  Subfigure (b) shows the training time, indicating that C-Flat outperforms other optimizers with less time, especially when using a smaller number of iterations.", "section": "4 Analysis"}, {"figure_path": "Dokew2u49m/figures/figures_8_1.jpg", "caption": "Figure 6: Ablation study about \u03bb and \u03c1. (a) and (b) represents the effect of \u03bb and \u03c1 on different CL methods (WA, Replay, MEMO). (c) and (d) represents the effect of \u03c1 and \u03c1 scheduler on MEMO with different optimizers (SGD (red line), SAM, GAM, C-Flat).", "description": "This figure presents an ablation study on the hyperparameters \u03bb and \u03c1 of the C-Flat algorithm. Subfigures (a) and (b) show the impact of \u03bb and \u03c1, respectively, on the performance of three different continual learning methods: WA, Replay, and MEMO. Subfigures (c) and (d) focus on the MEMO method, comparing the effects of \u03c1 and its scheduler on the performance when using different optimizers: SGD, SAM, GAM, and C-Flat. The results visualize how the performance varies with different settings of \u03bb and \u03c1, offering insight into the algorithm's sensitivity and robustness.", "section": "4.6 Computation Overhead"}, {"figure_path": "Dokew2u49m/figures/figures_9_1.jpg", "caption": "Figure 8: Loss and forgetting of old tasks.", "description": "This figure visualizes the change in loss and forgetting of old tasks in continual learning.  The top panel shows the loss on old tasks over epochs for both the MEMO method and MEMO with C-Flat. The bottom panel shows the accuracy on old tasks over epochs for both methods.  The results indicate that C-Flat leads to lower loss and less forgetting of old tasks during continual learning.", "section": "4.5 Revisiting Zeroth-order Flatness"}, {"figure_path": "Dokew2u49m/figures/figures_15_1.jpg", "caption": "Figure 3: The parametric loss landscapes of Replay (Mem.), WA (Reg.) and MEMO (Exp.) are plotted by perturbing the model parameters at the end of training (CIFAR-100, B0_Inc10) across the first two Hessian eigenvectors.", "description": "This figure visualizes the loss landscapes of three different continual learning methods: Replay, WA, and MEMO, both with and without the application of C-Flat.  By perturbing model parameters around their final trained values along the two largest Hessian eigenvectors, the plots illustrate the shape of the loss surface near the minimum. The goal is to show how C-Flat results in a flatter loss landscape, which is associated with better generalization and less catastrophic forgetting.", "section": "4.4 Visualization of Landscapes"}, {"figure_path": "Dokew2u49m/figures/figures_15_2.jpg", "caption": "Figure 10: The visualizations of loss landscapes during CL.", "description": "This figure visualizes the loss landscapes at different tasks during continual learning using PyHessian.  It shows that the loss landscape becomes flatter when using C-Flat, compared to a baseline method, which supports the claim that flatter loss landscapes improve continual learning performance.", "section": "4.4 Visualization of Landscapes"}]