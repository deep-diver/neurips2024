[{"figure_path": "lOdBHkqzRH/tables/tables_7_1.jpg", "caption": "Table 1: Average relative regret ratio of different methods over 50 replications when both the policy-inducing model and the nuisance model are correctly specified. The logging policy is a random policy.", "description": "This table presents the average relative regret ratio for various methods across 50 replications of an experiment.  The results are shown for different training dataset sizes (400, 1000, and 1600 data points) when the model used to induce the policy and the nuisance model are correctly specified and the logging policy is random.  The relative regret is a measure of how suboptimal a given policy is in comparison to the best possible policy.", "section": "5 Numerical Experiments"}, {"figure_path": "lOdBHkqzRH/tables/tables_8_1.jpg", "caption": "Table 1: Average relative regret ratio of different methods over 50 replications when both the policy-inducing model and the nuisance model are correctly specified. The logging policy is a random policy.", "description": "This table shows the average relative regret ratio for different methods across 50 replications of the experiment for a random logging policy. The results are shown for three different training data sizes (400, 1000, 1600).  The methods compared include ETO, different variants of SPO+ (SPO+ DM, SPO+ DR PI, SPO+ DR Lambda, SPO+ DR Clip), and naive versions of ETO and SPO+.  The correctly specified models provide a baseline for comparison.", "section": "5 Numerical Experiments"}, {"figure_path": "lOdBHkqzRH/tables/tables_23_1.jpg", "caption": "Table 1: Average relative regret ratio of different methods over 50 replications when both the policy-inducing model and the nuisance model are correctly specified. The logging policy is a random policy.", "description": "This table presents the average relative regret ratio for various methods across 50 replications.  The results are categorized by the size of the training data (400, 1000, 1600) and the method used (ETO, SPO+ variations, and Naive approaches). A correctly specified policy-inducing model and nuisance model are assumed, and a random logging policy is employed.  The relative regret is calculated as a ratio to the expected cost of the globally optimal policy. Lower values indicate better performance.", "section": "5 Numerical Experiments"}, {"figure_path": "lOdBHkqzRH/tables/tables_24_1.jpg", "caption": "Table 1: Average relative regret ratio of different methods over 50 replications when both the policy-inducing model and the nuisance model are correctly specified. The logging policy is a random policy.", "description": "This table presents the average relative regret ratio for various methods in a contextual linear optimization problem. The results are based on 50 replications of the experiment and show the performance when both the policy-inducing model and the nuisance model are correctly specified.  The experiment uses a random logging policy to select decisions.", "section": "5 Numerical Experiments"}, {"figure_path": "lOdBHkqzRH/tables/tables_24_2.jpg", "caption": "Table 1: Average relative regret ratio of different methods over 50 replications when both the policy-inducing model and the nuisance model are correctly specified. The logging policy is a random policy.", "description": "This table presents the average relative regret ratio for various methods across 50 replications, under the condition that both the policy-inducing model and the nuisance model are correctly specified.  The results are categorized by the size of the training data (400, 1000, and 1600). A random policy is used as the logging policy. The relative regret is calculated as the ratio of the method's regret to the expected cost of the globally optimal policy.", "section": "5 Numerical Experiments"}, {"figure_path": "lOdBHkqzRH/tables/tables_25_1.jpg", "caption": "Table 1: Average relative regret ratio of different methods over 50 replications when both the policy-inducing model and the nuisance model are correctly specified. The logging policy is a random policy.", "description": "This table shows the average relative regret ratio for various methods in a contextual linear optimization problem with bandit feedback.  The results are based on 50 replications of the experiment, and show the performance when both the model used to induce the policy and the nuisance model are correctly specified.  The logging policy used to generate the data is a random policy. The lower the regret ratio, the better the method's performance.", "section": "5 Numerical Experiments"}, {"figure_path": "lOdBHkqzRH/tables/tables_25_2.jpg", "caption": "Table 1: Average relative regret ratio of different methods over 50 replications when both the policy-inducing model and the nuisance model are correctly specified. The logging policy is a random policy.", "description": "This table presents the average relative regret ratios for various methods in a contextual linear optimization problem with bandit feedback.  The results are based on 50 replications of the experiment and show the performance when both the model used to induce the policy and the nuisance model are correctly specified. The logging policy, which determines how decisions are made in the data collection process, is a random policy.", "section": "5 Numerical Experiments"}, {"figure_path": "lOdBHkqzRH/tables/tables_26_1.jpg", "caption": "Table 1: Average relative regret ratio of different methods over 50 replications when both the policy-inducing model and the nuisance model are correctly specified. The logging policy is a random policy.", "description": "This table presents the average relative regret ratio for various methods across 50 replications.  The policy-inducing model and nuisance model are correctly specified.  The results are separated by training data size (400, 1000, 1600) and show the performance when using a random logging policy.", "section": "5 Numerical Experiments"}]