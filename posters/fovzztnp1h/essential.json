{"importance": "This paper is crucial for researchers in time series forecasting because it **demonstrates the effectiveness of adapting large language models (LLMs) for this task.**  It introduces a novel autoregressive approach, surpassing previous methods in accuracy and efficiency.  This opens **new avenues for research exploring LLM applications in other sequential data domains** and developing foundation models for time series analysis.", "summary": "AutoTimes repurposes LLMs as autoregressive time series forecasters, achieving state-of-the-art results with minimal trainable parameters and faster training/inference.", "takeaways": ["AutoTimes leverages LLMs' autoregressive capabilities for accurate and efficient time series forecasting.", "The proposed 'in-context forecasting' method enhances prediction accuracy by incorporating relevant contextual information.", "AutoTimes demonstrates superior performance with minimal parameters and significantly faster training/inference than existing LLM-based forecasters."], "tldr": "Traditional time series forecasting methods often struggle with variable-length data and require separate model training for different prediction horizons.  Recent attempts to use Large Language Models (LLMs) have been hindered by the mismatch between LLMs' inherent autoregressive nature and the common non-autoregressive approaches used in time series forecasting.  This leads to inefficient use of LLM capabilities and often results in lower generalization performance.\nAutoTimes directly addresses these issues. It introduces a novel method that fully leverages LLMs' autoregressive properties to forecast time series of arbitrary lengths. By representing time series as prompts and incorporating contextual information, AutoTimes achieves state-of-the-art accuracy with only 0.1% trainable parameters and a significant speedup in training and inference compared to other LLM-based approaches. This method exhibits flexibility in handling variable-length lookback windows and scales well with larger LLMs, opening up exciting new possibilities for time series forecasting and foundation model development.", "affiliation": "Tsinghua University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "FOvZztnp1H/podcast.wav"}