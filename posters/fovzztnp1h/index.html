<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>AutoTimes: Autoregressive Time Series Forecasters via Large Language Models &#183; NeurIPS 2024</title>
<meta name=title content="AutoTimes: Autoregressive Time Series Forecasters via Large Language Models &#183; NeurIPS 2024"><meta name=description content="AutoTimes repurposes LLMs as autoregressive time series forecasters, achieving state-of-the-art results with minimal trainable parameters and faster training/inference."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ Tsinghua University,"><link rel=canonical href=https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/><link type=text/css rel=stylesheet href=/neurips2024/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/neurips2024/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/neurips2024/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/neurips2024/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/neurips2024/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/neurips2024/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/neurips2024/favicon-16x16.png><link rel=manifest href=/neurips2024/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/"><meta property="og:site_name" content="NeurIPS 2024"><meta property="og:title" content="AutoTimes: Autoregressive Time Series Forecasters via Large Language Models"><meta property="og:description" content="AutoTimes repurposes LLMs as autoregressive time series forecasters, achieving state-of-the-art results with minimal trainable parameters and faster training/inference."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posters"><meta property="article:published_time" content="2024-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-26T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ Tsinghua University"><meta property="og:image" content="https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/cover.png"><meta name=twitter:title content="AutoTimes: Autoregressive Time Series Forecasters via Large Language Models"><meta name=twitter:description content="AutoTimes repurposes LLMs as autoregressive time series forecasters, achieving state-of-the-art results with minimal trainable parameters and faster training/inference."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posters","name":"AutoTimes: Autoregressive Time Series Forecasters via Large Language Models","headline":"AutoTimes: Autoregressive Time Series Forecasters via Large Language Models","abstract":"AutoTimes repurposes LLMs as autoregressive time series forecasters, achieving state-of-the-art results with minimal trainable parameters and faster training\/inference.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/neurips2024\/posters\/fovzztnp1h\/","author":{"@type":"Person","name":"AI Paper Reviewer"},"copyrightYear":"2024","dateCreated":"2024-09-26T00:00:00\u002b00:00","datePublished":"2024-09-26T00:00:00\u002b00:00","dateModified":"2024-09-26T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ Tsinghua University"],"mainEntityOfPage":"true","wordCount":"5046"}]</script><meta name=author content="AI Paper Reviewer"><link href=https://neurips.cc/ rel=me><link href=https://x.com/NeurIPSConf rel=me><link href rel=me><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://x.com/algo_diver/ rel=me><script src=/neurips2024/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/neurips2024/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/neurips2024/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/neurips2024/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/neurips2024/ class="text-base font-medium text-gray-500 hover:text-gray-900">NeurIPS 2024</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Oral
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Applications</p></a><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Theory</p></a><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Image Generation</p></a><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Large Language Models</p></a><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Others</p></a><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Reinforcement Learning</p></a></div></div></div></div><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Spotlight
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) AI Theory</p></a><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Large Language Models</p></a><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Optimization</p></a><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Others</p></a><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Reinforcement Learning</p></a></div></div></div></div><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Posters</p></a><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Oral</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Applications</p></a></li><li class=mt-1><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Image Generation</p></a></li><li class=mt-1><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Others</p></a></li><li class=mt-1><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Spotlight</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Optimization</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Others</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Posters</p></a></li><li class=mt-1><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/neurips2024/posters/fovzztnp1h/cover_hu12193885476142893484.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/>NeurIPS 2024</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/>Posters</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/fovzztnp1h/>AutoTimes: Autoregressive Time Series Forecasters via Large Language Models</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">AutoTimes: Autoregressive Time Series Forecasters via Large Language Models</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5046 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">24 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_posters/FOvZztnp1H/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_posters/FOvZztnp1H/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/-tsinghua-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Tsinghua University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviewer" src=/neurips2024/img/avatar_hu1344562329374673026.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviewer</div><div class="text-sm text-neutral-700 dark:text-neutral-400">As an AI, I specialize in crafting insightful blog content about cutting-edge research in the field of artificial intelligence</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://neurips.cc/ target=_blank aria-label=Homepage rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg fill="currentcolor" height="800" width="800" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 491.398 491.398"><g><g id="Icons_19_"><path d="M481.765 220.422 276.474 15.123c-16.967-16.918-44.557-16.942-61.559.023L9.626 220.422c-12.835 12.833-12.835 33.65.0 46.483 12.843 12.842 33.646 12.842 46.487.0l27.828-27.832v214.872c0 19.343 15.682 35.024 35.027 35.024h74.826v-97.62c0-7.584 6.146-13.741 13.743-13.741h76.352c7.59.0 13.739 6.157 13.739 13.741v97.621h74.813c19.346.0 35.027-15.681 35.027-35.024V239.091l27.812 27.815c6.425 6.421 14.833 9.63 23.243 9.63 8.408.0 16.819-3.209 23.242-9.63 12.844-12.834 12.844-33.65.0-46.484z"/></g></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/NeurIPSConf target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href target=_blank aria-label=Line rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 14.707 14.707"><g><rect x="6.275" y="0" style="fill:currentColor" width="2.158" height="14.707"/></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/algo_diver/ target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#llm4ts-autoregression>LLM4TS Autoregression</a></li><li><a href=#in-context-forecasting>In-context Forecasting</a></li><li><a href=#multi-variate-time-series>Multi-variate Time Series</a></li><li><a href=#llm-adaptation>LLM Adaptation</a></li><li><a href=#future-of-llm4ts>Future of LLM4TS</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#llm4ts-autoregression>LLM4TS Autoregression</a></li><li><a href=#in-context-forecasting>In-context Forecasting</a></li><li><a href=#multi-variate-time-series>Multi-variate Time Series</a></li><li><a href=#llm-adaptation>LLM Adaptation</a></li><li><a href=#future-of-llm4ts>Future of LLM4TS</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>FOvZztnp1H</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Yong Liu et el.</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://openreview.net/forum?id=FOvZztnp1H" target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/FOvZztnp1H target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/FOvZztnp1H/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Traditional time series forecasting methods often struggle with variable-length data and require separate model training for different prediction horizons. Recent attempts to use Large Language Models (LLMs) have been hindered by the mismatch between LLMs&rsquo; inherent autoregressive nature and the common non-autoregressive approaches used in time series forecasting. This leads to inefficient use of LLM capabilities and often results in lower generalization performance.</p><p>AutoTimes directly addresses these issues. It introduces a novel method that fully leverages LLMs&rsquo; autoregressive properties to forecast time series of arbitrary lengths. By representing time series as prompts and incorporating contextual information, AutoTimes achieves state-of-the-art accuracy with only 0.1% trainable parameters and a significant speedup in training and inference compared to other LLM-based approaches. This method exhibits flexibility in handling variable-length lookback windows and scales well with larger LLMs, opening up exciting new possibilities for time series forecasting and foundation model development.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-766d1116467c7f169070c3a46037041b></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-766d1116467c7f169070c3a46037041b",{strings:[" AutoTimes leverages LLMs' autoregressive capabilities for accurate and efficient time series forecasting. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-c03c450c6146e4bc29d936698e569b17></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-c03c450c6146e4bc29d936698e569b17",{strings:[" The proposed 'in-context forecasting' method enhances prediction accuracy by incorporating relevant contextual information. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-e7d8c3b5c734ecb8da4d2012036dfc23></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-e7d8c3b5c734ecb8da4d2012036dfc23",{strings:[" AutoTimes demonstrates superior performance with minimal parameters and significantly faster training/inference than existing LLM-based forecasters. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers in time series forecasting because it <strong>demonstrates the effectiveness of adapting large language models (LLMs) for this task.</strong> It introduces a novel autoregressive approach, surpassing previous methods in accuracy and efficiency. This opens <strong>new avenues for research exploring LLM applications in other sequential data domains</strong> and developing foundation models for time series analysis.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_1_1.jpg alt></figure></p><blockquote><p>üîº This figure compares two approaches for using LLMs in time series forecasting. (a) shows the prevalent non-autoregressive approach, where the LLM processes a flattened representation of the lookback series to generate predictions all at once. In contrast, the figure highlights the autoregressive approach proposed in the paper. This approach leverages the inherent autoregressive nature of LLMs for sequential prediction of the next token in the series. (b) demonstrates a comparison of prompting mechanisms. The traditional method uses natural language prompts, which can introduce modality disparities. The proposed method leverages time series itself for prompting, a technique referred to as &lsquo;in-context forecasting&rsquo;.</p><details><summary>read the caption</summary>Figure 1: (a) Prevalent LLM4TS methods non-autoregressively generate predictions with the globally flattened representation of lookback series, while large language models inherently predict the next tokens by autoregression [47]. (b) Previous methods adopt language prompts that may lead to the modality disparity, while we find time series can be self-prompted, termed in-context forecasting.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_2_1.jpg alt></figure></p><blockquote><p>üîº This table compares several Large Language Model for Time Series (LLM4TS) methods based on three key aspects: whether they are autoregressive, whether they freeze the pre-trained Large Language Model (LLM), and whether they are multimodal. Autoregressive models generate predictions sequentially, using previous predictions to inform current ones. Freezing the LLM means that only a small set of parameters are trained, reducing computational cost. Multimodal methods incorporate data from sources other than just the time series, like textual descriptions or images. The table highlights that AutoTimes is unique in combining all three of these characteristics.</p><details><summary>read the caption</summary>Table 1: Comparison of LLM4TS methods: Autoregressive categorizes LLM-based forecasters by whether to conduct autoregression. Freeze LLM enables quick adaptation, which would otherwise require significant resources for fine-tuning. Multimodal refers to the utilization of information from other modalities. Prior to AutoTimes, none of the LLM4TS methods achieved all three.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">LLM4TS Autoregression<div id=llm4ts-autoregression class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#llm4ts-autoregression aria-label=Anchor>#</a></span></h4><p>LLM4TS (Large Language Models for Time Series) research has predominantly focused on non-autoregressive methods, treating LLMs as feature extractors rather than leveraging their inherent autoregressive capabilities for sequential prediction. <strong>Autoregressive approaches are crucial</strong> because they directly align with the nature of time series data and the strength of LLMs in generating sequential outputs. By fully utilizing the autoregressive property of LLMs, models can naturally generate multi-step predictions with arbitrary lengths, overcoming limitations of non-autoregressive methods that often require separate models for different forecast horizons. <strong>This inherent autoregressive nature offers significant advantages</strong>, enabling more efficient and accurate forecasting, especially for longer prediction windows where error accumulation in non-autoregressive methods becomes a significant problem. Furthermore, <strong>autoregressive LLM4TS models may also improve zero-shot generalization</strong>, as the autoregressive training better captures the underlying sequential patterns in time series, leading to more robust models capable of handling unseen data.</p><h4 class="relative group">In-context Forecasting<div id=in-context-forecasting class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-context-forecasting aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;In-context Forecasting&rdquo; presented in the research paper proposes a novel approach to leverage the capabilities of Large Language Models (LLMs) for time series forecasting. It suggests that instead of solely relying on a limited lookback window of time series data, the model&rsquo;s prediction context can be significantly enriched by incorporating relevant prior time series data as prompts. This method, termed &ldquo;self-prompting,&rdquo; enhances forecasting accuracy by effectively extending the context beyond the immediate lookback period. <strong>The key advantage lies in the model&rsquo;s ability to learn from broader contextual patterns and dependencies</strong>, thereby potentially improving forecasting accuracy and robustness. The paper further investigates the strategic selection of these prompts, exploring the impact of chronological ordering and the use of LLM-embedded timestamps to align multiple time series and maximize the benefits of this approach. This innovation has the potential to significantly improve forecasting capabilities of LLMs in various applications by allowing for better context awareness and generalization to unseen data.</p><h4 class="relative group">Multi-variate Time Series<div id=multi-variate-time-series class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multi-variate-time-series aria-label=Anchor>#</a></span></h4><p>Analyzing multivariate time series presents unique challenges and opportunities. The inherent complexity arises from the interdependencies between multiple variables, requiring sophisticated models to capture these relationships effectively. <strong>Traditional univariate methods often fail to account for these interactions</strong>, leading to inaccurate predictions. Therefore, techniques designed specifically for multivariate data are essential, often involving vector autoregressive models (VAR) or dynamic factor models. The selection of an appropriate model depends on the characteristics of the data, such as stationarity, autocorrelation, and the presence of non-linear relationships. <strong>Dimensionality reduction techniques</strong> can be beneficial when dealing with high-dimensional data, helping manage computational complexity and improving model interpretability. <strong>Forecasting accuracy in multivariate scenarios</strong> is critically important for decision-making across numerous fields, including finance, environmental science, and healthcare, impacting resource allocation, risk management, and policy decisions. Advanced machine learning techniques, like deep learning, also show promise in tackling the complexities of multivariate time series analysis.</p><h4 class="relative group">LLM Adaptation<div id=llm-adaptation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#llm-adaptation aria-label=Anchor>#</a></span></h4><p>LLM adaptation in time series forecasting involves leveraging pre-trained large language models (LLMs) for the task. A crucial aspect is how the LLM is adapted to handle the sequential nature of time series data effectively. <strong>Common approaches include fine-tuning the LLM on a time series dataset, often adapting the input embedding layer to represent temporal data properly.</strong> However, this can be computationally expensive and may not fully exploit the LLM&rsquo;s inherent capabilities. <strong>Alternative strategies focus on parameter-efficient fine-tuning techniques like LoRA (Low-Rank Adaptation)</strong>, which freeze most of the LLM&rsquo;s weights and only adjust a small set of parameters. <strong>Another key consideration is maintaining the autoregressive nature of the LLM, allowing it to generate predictions iteratively.</strong> This autoregressive approach, unlike some non-autoregressive methods, directly leverages the LLMs generative capacity. <strong>The choice of adaptation method significantly impacts the balance between model performance, training efficiency, and computational cost.</strong> Future research should explore more effective and efficient methods for LLM adaptation for enhanced accuracy and generalization in time series forecasting.</p><h4 class="relative group">Future of LLM4TS<div id=future-of-llm4ts class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-of-llm4ts aria-label=Anchor>#</a></span></h4><p>The future of LLMs in time series forecasting (LLM4TS) is bright, but faces challenges. <strong>Autoregressive approaches</strong>, fully leveraging the inherent capabilities of LLMs, show great promise, surpassing non-autoregressive methods in accuracy and efficiency. <strong>In-context learning</strong> allows for better generalization and adaptation to unseen data, reducing the reliance on extensive fine-tuning. <strong>Multimodal approaches</strong>, integrating time series with other data types (text, images), will unlock new forecasting capabilities. However, addressing <strong>limitations in probabilistic forecasting</strong>, improving computational efficiency for very large LLMs, and ensuring fair and responsible use of these powerful models remain key research areas. Further exploration of <strong>low-rank adaptation</strong> techniques offers a path to reduce the computational burden and enhance the efficiency of LLM adaptation. Developing robust methods for <strong>prompt engineering</strong> and context selection is also crucial for maximizing the performance and reliability of LLM4TS models in real-world applications.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_3_1.jpg alt></figure></p><blockquote><p>üîº This figure illustrates the core idea of AutoTimes, which repurposes large language models (LLMs) for time series forecasting. It shows how a sequence of time series data points is converted into a sequence of tokens, similar to how words are tokens in natural language. The LLM, which is trained on natural language data, is then used to predict the next tokens in the time series sequence. The figure visually depicts the token-wise alignment between the time series tokens and the language tokens, showing that the LLM&rsquo;s inherent autoregressive property (predicting the next token based on the previous ones) is leveraged for forecasting. The color gradient from dark to light indicates the flow of information, from the lookback window to the future predictions.</p><details><summary>read the caption</summary>Figure 2: An example to illustrate how AutoTimes adapts language models for time series forecasting.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_4_1.jpg alt></figure></p><blockquote><p>üîº This figure illustrates the overall architecture of AutoTimes. It shows how time series data and timestamps are processed. First, the time series and timestamps are segmented into smaller chunks. Textual timestamps are then converted into position embeddings using a large language model (LLM). The time series segments are then embedded and projected using the LLM&rsquo;s intermediate layers (the LLM weights are frozen during this process). Finally, next-token prediction is used for forecasting.</p><details><summary>read the caption</summary>Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_7_1.jpg alt></figure></p><blockquote><p>üîº This figure demonstrates the in-context forecasting approach of AutoTimes and compares it with the zero-shot approach. In the zero-shot approach, the model trained on a source domain is directly applied to a target domain without any additional information. The in-context approach, however, incorporates additional time series prompts from the target domain as context. These prompts are concatenated with the lookback window before feeding into the forecaster. The bar chart visually compares the SMAPE (Symmetric Mean Absolute Percentage Error) results of both approaches across different data subsets (Year, Quarter, Month, Others), demonstrating the effectiveness of the in-context forecasting method.</p><details><summary>read the caption</summary>Figure 4: Demonstration of in-context forecasting and results compared with zero-shot. We uniformly select the foremost time points from the target domain as prompts and concatenate them with lookback to obtain the prediction. AutoTimes adapts LLMs on the source domain with a larger context length to place the additional time series prompt. Supplementary showcases are provided in Figure 12.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_8_1.jpg alt></figure></p><blockquote><p>üîº This figure illustrates the AutoTimes model architecture. The process begins by segmenting the input time series into smaller segments and representing timestamps as text. These textual timestamps are then processed by a large language model (LLM) to generate position embeddings. The time series segments are also embedded, and these embeddings are concatenated with the position embeddings. The resulting embeddings are then fed into the LLM for next token prediction, with the intermediate layers of the LLM frozen to reduce computational cost. The output is a prediction of future time series values.</p><details><summary>read the caption</summary>Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_8_2.jpg alt></figure></p><blockquote><p>üîº This figure compares the training time, inference time, and number of tunable parameters for different LLMs used in the AutoTimes model. The LLMs compared are GPT-2 and LLaMA-7B. The results show that AutoTimes is significantly more efficient in terms of training and inference time, and uses far fewer tunable parameters compared to other LLM-based forecasting methods. This highlights the efficiency of AutoTimes in leveraging LLMs for time-series forecasting.</p><details><summary>read the caption</summary>Figure 6: Efficiency comparison of alternative LLMs, evaluated by the same configuration of Table 5.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_15_1.jpg alt></figure></p><blockquote><p>üîº This figure illustrates the overall architecture of the AutoTimes model. It shows how the model processes time series data by segmenting it and converting timestamps into position embeddings using a pre-trained Large Language Model (LLM). The core idea is to embed time series segments into the LLM&rsquo;s embedding space, leveraging its inherent capabilities for token transition and prediction. Importantly, the intermediate layers of the LLM are frozen, thus, the model efficiently uses the pre-trained LLM&rsquo;s power without heavy training.</p><details><summary>read the caption</summary>Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen. language tokens, the position embedding can be integrated with the corresponding time span without increasing the context length. Concretely, the token embedding E·µ¢ ‚àà R<sup>D</sup> is obtained by: E·µ¢ = SE·µ¢ + TE·µ¢.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_20_1.jpg alt></figure></p><blockquote><p>üîº This figure shows the impact of different hyperparameters on the performance of AutoTimes for various forecast lengths. The hyperparameters tested are the number of layers and the hidden dimension in both the Segment Embedding and Segment Projection components, the context length, and the segment length. Each line represents a different forecast length (pred-96, pred-192, pred-336, pred-720), and the x-axis shows the different values tested for each hyperparameter. The y-axis shows the Mean Squared Error (MSE). This allows for an assessment of how sensitive the model&rsquo;s performance is to changes in these hyperparameters and helps in determining optimal settings.</p><details><summary>read the caption</summary>Figure 7: Hyperparameter sensitivity of AutoTimes. Each curve presents a specific forecast length.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_20_2.jpg alt></figure></p><blockquote><p>üîº This figure illustrates the AutoTimes model architecture. It shows how time series data and timestamps are processed. First, time series are segmented into smaller chunks and textual timestamps are created. Then, the LLM converts the timestamps into position embeddings. Next, time series segments are converted into embeddings using a segment embedding function, and these embeddings are then combined with the timestamp embeddings. Finally, next-token prediction is performed using the frozen layers of a pre-trained LLM. The resulting token embeddings form the input of subsequent layers in the model&rsquo;s autoregressive process. The overall approach leverages pre-trained LLMs for forecasting with minimal training.</p><details><summary>read the caption</summary>Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen. language tokens, the position embedding can be integrated with the corresponding time span without increasing the context length. Concretely, the token embedding E·µ¢ ‚àà R<sup>D</sup> is obtained by: E·µ¢ = SE·µ¢ + TE·µ¢.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_22_1.jpg alt></figure></p><blockquote><p>üîº This figure compares two approaches for using LLMs for time series forecasting. (a) shows the common non-autoregressive approach where the LLM processes the entire lookback series at once to generate predictions. This is contrasted with (b), which shows the autoregressive approach used by AutoTimes. The key difference is AutoTimes uses an autoregressive approach to generate the next prediction token at a time, which is how LLMs naturally function. It also shows that AutoTimes uses a self-prompting mechanism (in-context forecasting) which differs from the use of language prompts in prior work.</p><details><summary>read the caption</summary>Figure 1: (a) Prevalent LLM4TS methods non-autoregressively generate predictions with the globally flattened representation of lookback series, while large language models inherently predict the next tokens by autoregression [47]. (b) Previous methods adopt language prompts that may lead to the modality disparity, while we find time series can be self-prompted, termed in-context forecasting.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_23_1.jpg alt></figure></p><blockquote><p>üîº This figure compares the prevalent large language model for time series forecasting methods. Panel (a) shows that most existing methods don&rsquo;t use the autoregressive nature of LLMs, processing the entire lookback period at once rather than sequentially. Panel (b) highlights a key difference: the proposed method uses the time series itself as a prompt (in-context forecasting), avoiding issues that arise from using natural language prompts that don&rsquo;t directly align with the time series data.</p><details><summary>read the caption</summary>Figure 1: (a) Prevalent LLM4TS methods non-autoregressively generate predictions with the globally flattened representation of lookback series, while large language models inherently predict the next tokens by autoregression [47]. (b) Previous methods adopt language prompts that may lead to the modality disparity, while we find time series can be self-prompted, termed in-context forecasting.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/figures_23_2.jpg alt></figure></p><blockquote><p>üîº This figure demonstrates the results of in-context forecasting and compares them to zero-shot forecasting. In in-context forecasting, time series prompts from the target domain are concatenated with the lookback series before feeding to the model. The results show that using these prompts improves forecasting performance compared to the zero-shot approach. Supplementary figures showing additional results are referenced.</p><details><summary>read the caption</summary>Figure 4: Demonstration of in-context forecasting and results compared with zero-shot. We uniformly select the foremost time points from the target domain as prompts and concatenate them with lookback to obtain the prediction. AutoTimes adapts LLMs on the source domain with a larger context length to place the additional time series prompt. Supplementary showcases are provided in Figure 12.</details></blockquote></details><details><summary>More on tables</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_6_1.jpg alt></figure></p><blockquote><p>üîº This table presents the average results of short-term time series forecasting on the M4 dataset. It compares the performance of AutoTimes against several other state-of-the-art methods, including TimeLLM, FPT, Koopa, N-HiTS, DLinear, PatchTST, TimesNet, FiLM, and N-BEATS. The metrics used for comparison are SMAPE (Symmetric Mean Absolute Percentage Error), MASE (Mean Absolute Scaled Error), and OWA (Overall Weighted Average). Detailed results for each method on each time series within the M4 dataset are available in Table 11.</p><details><summary>read the caption</summary>Table 2: Average short-term forecasting results on the M4 [25]. Full results are provided in Table 11.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_6_2.jpg alt></figure></p><blockquote><p>üîº This table presents the results of long-term time series forecasting experiments using a &lsquo;one-for-all&rsquo; approach. A single model is trained on each dataset and then used to generate forecasts of different lengths (96, 192, 336, and 720 time steps). The AutoTimes model uses a context length of 672 time steps, while other methods use an input length of 672 and an output length of 96. The table shows the average results across all forecast lengths, with more detailed results in Table 10.</p><details><summary>read the caption</summary>Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}. AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and output length F = 96 in other methods. All results are averaged. Full results is provided in Table 10.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_6_3.jpg alt></figure></p><blockquote><p>üîº This table presents the results of zero-shot forecasting experiments. The models were trained on either the M4 or M3 dataset and then tested on the other dataset, to evaluate their ability to generalize to unseen data. The results are presented as the average Symmetric Mean Absolute Percentage Error (SMAPE), a common metric for evaluating time series forecasting accuracy. The table includes results for AutoTimes, and several baseline models (FPT, DLinear, PatchTST, TimesNet, NSFormer, FEDFormer, Informer, Reformer). Appendix D.2 provides more detailed results.</p><details><summary>read the caption</summary>Table 4: Zero-shot forecasting results in averaged SMAPE. M4 ‚Üí M3 trains forecasters on the datasets of M4 and evaluates on M3, and vice versa. Detailed results are provided in Appendix D.2</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_8_1.jpg alt></figure></p><blockquote><p>üîº This table presents the Mean Squared Error (MSE) and Mean Absolute Error (MAE) achieved by the AutoTimes model when using different large language models (LLMs) as the backbone. The LLMs tested include GPT-2 (124M), OPT-350M, OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B. The results are averaged across several datasets and prediction horizons. The full results for each dataset and horizon can be found in Table 18.</p><details><summary>read the caption</summary>Table 5: Averaged results of alternative language models. Full results are provided in Table 18.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_9_1.jpg alt></figure></p><blockquote><p>üîº This table presents the ablation study results to verify the effectiveness of using LLMs in the AutoTimes model. Three variations of the model are compared against the baseline AutoTimes model: one without the LLM (w/o LLM), one using only a multi-head attention layer (LLM2Attn), and one using a single transformer block (LLM2Trsf). The results, measured by MSE and MAE, are shown for ETTh1 and ECL datasets for four different prediction horizons (Pred-96, Pred-192, Pred-336, Pred-720). This allows for a comparison of the performance impact of using different levels of the LLMs within the AutoTimes architecture.</p><details><summary>read the caption</summary>Table 6: We follow the protocol of LLM4TS ablation studies [35] to verify whether the LLM is truly useful in our AutoTimes: (1) w/o LLM replaces the language model entirely and passing input tokens directly to the last layer; (2) LLM2Attn replaces the language model with a single multi-head attention layer; (3) LLM2Trsf replaces the language model with a single transformer block.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_9_2.jpg alt></figure></p><blockquote><p>üîº This table presents the results of long-term time series forecasting experiments using a one-for-all approach, where a single model is trained on each dataset and used to make predictions for multiple forecast lengths (96, 192, 336, and 720). The AutoTimes model utilizes LLMs with a context length of 672, while other methods use an input length of 672 and an output length of 96. The table shows the average results across all forecast lengths, with complete results available in Table 10.</p><details><summary>read the caption</summary>Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}. AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and output length F = 96 in other methods. All results are averaged. Full results is provided in Table 10.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_13_1.jpg alt></figure></p><blockquote><p>üîº This table details the characteristics of the datasets used in the paper&rsquo;s experiments. It lists the name of each dataset, the number of variables (Dim), the forecast lengths considered, the total number of data points in the training, validation, and testing sets, the sampling frequency (e.g., hourly, daily), and a brief description of the data&rsquo;s information.</p><details><summary>read the caption</summary>Table 8: Detailed dataset descriptions. Dim denotes the variate number. Dataset Size denotes the total number of time points in (Train, Validation, Test) splits respectively. Forecast Length denotes the future time points to be predicted. Frequency denotes the sampling interval of time points.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_14_1.jpg alt></figure></p><blockquote><p>üîº This table presents the complete results of using different LLMs in the AutoTimes model. The context length is fixed at 672 for all experiments. The table shows the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for different forecast horizons (96, 192, 336, and 720) across various datasets (ECL, ETTh1, Traffic, and Weather). The results allow for a comparison of model performance across different LLMs of varying sizes.</p><details><summary>read the caption</summary>Table 18: Full Results of alternative LLMs, which are adapted with the context length C = 672.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_14_2.jpg alt></figure></p><blockquote><p>üîº This table shows the performance of the AutoTimes model in terms of Mean Squared Error (MSE) and Mean Absolute Error (MAE) for four different forecasting horizons (96, 192, 336, and 720). The results are presented for four different datasets (ETTh1, ECL, Weather, and Traffic). For each dataset and horizon, the table provides the average MSE and MAE along with their standard deviations, calculated across three independent runs with different random seeds. This demonstrates the stability and robustness of AutoTimes.</p><details><summary>read the caption</summary>Table 9: Performance and standard deviations of AutoTimes. Results come from three random seeds.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_15_1.jpg alt></figure></p><blockquote><p>üîº This table presents the mean and standard deviation of the MSE and MAE metrics for the AutoTimes model across different datasets (ETTh1, ECL, Weather, Traffic, Solar-Energy) and forecasting horizons (96, 192, 336, 720). The results are averaged across three different random seeds to show the model&rsquo;s stability and reliability. Lower MSE and MAE values indicate better forecasting performance.</p><details><summary>read the caption</summary>Table 9: Performance and standard deviations of AutoTimes. Results come from three random seeds.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_17_1.jpg alt></figure></p><blockquote><p>üîº This table presents the average results of short-term time series forecasting on the M4 benchmark dataset. It compares the performance of AutoTimes against several state-of-the-art forecasting methods, including TimeLLM, FPT, Koopa, N-HiTS, DLinear, PatchTST, TimesNet, FiLM, and N-BEATS, across three evaluation metrics: SMAPE, MASE, and OWA. The full, detailed results for each method can be found in Table 11.</p><details><summary>read the caption</summary>Table 2: Average short-term forecasting results on the M4 [25]. Full results are provided in Table 11.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_17_2.jpg alt></figure></p><blockquote><p>üîº This table compares the results of several Large Language Model for Time Series (LLM4TS) methods reported in their original papers and the results reproduced by the authors of the current paper using the official code of those methods. The goal is to provide a reliable comparison of the performance of different methods on benchmark datasets.</p><details><summary>read the caption</summary>Table 13: Results of LLM4TS methods from the original paper and our reproduction by official code.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_18_1.jpg alt></figure></p><blockquote><p>üîº This table presents the results of long-term time series forecasting experiments using a one-for-all approach. A single model is trained on each dataset and then used to predict four different forecast lengths (96, 192, 336, and 720 time steps). AutoTimes uses a context length of 672, while other methods use an input length of 672 and an output length of 96. The table shows the average results across all four forecast lengths, with complete results available in Table 10. This setup tests the model&rsquo;s ability to generalize to different prediction horizons without retraining.</p><details><summary>read the caption</summary>Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}. AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and output length F = 96 in other methods. All results are averaged. Full results is provided in Table 10.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_18_2.jpg alt></figure></p><blockquote><p>üîº This table presents the results of forecasting experiments on three additional benchmark datasets, namely Australian Electricity, Bdg-2 Panther, and Oikolab Weather. The experiments use a lookback length of 672 time steps and predict the next 96 time steps. The table compares the performance of AutoTimes against three other state-of-the-art forecasting models: PatchTST, iTransformer, and DLinear. The metrics used for comparison are Mean Squared Error (MSE) and Mean Absolute Error (MAE).</p><details><summary>read the caption</summary>Table 15: Forecasting results on additional benchmark datasets [24] (672-pred-96).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_19_1.jpg alt></figure></p><blockquote><p>üîº This table presents the results of long-term forecasting experiments using a one-for-all approach. A single model is trained on each dataset and used to predict four different forecast lengths (96, 192, 336, and 720 time steps). AutoTimes uses a context length of 672, while other methods use an input length of 672 and an output length of 96. The table shows the average results across all forecast lengths, with full details available in Table 10.</p><details><summary>read the caption</summary>Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}. AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and output length F = 96 in other methods. All results are averaged. Full results is provided in Table 10.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_19_2.jpg alt></figure></p><blockquote><p>üîº This table shows the different configurations used for AutoTimes when using different base LLMs. It lists the base large language model (LLM) used, the hidden dimension of the model&rsquo;s layers, the type of embedding used (2-layer MLP or Linear), and the number of trainable parameters (in millions) for each configuration. The variations demonstrate the adaptability of AutoTimes to various LLMs.</p><details><summary>read the caption</summary>Table 17: Detailed method configurations of AutoTimes for alternative language models.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_20_1.jpg alt></figure></p><blockquote><p>üîº This table presents the results of experiments using different large language models (LLMs) for time series forecasting. The models tested include GPT-2 (124M), OPT-350M, OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B. The metrics used to evaluate performance are Mean Squared Error (MSE) and Mean Absolute Error (MAE) for the 96, 192, 336, and 720 time steps ahead. The table shows average performance across these metrics and time steps. Full details are available in Table 18.</p><details><summary>read the caption</summary>Table 5: Averaged results of alternative language models. Full results are provided in Table 18.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_21_1.jpg alt></figure></p><blockquote><p>üîº This table presents the results of experiments using different strategies for selecting time series prompts in in-context forecasting. Four different strategies are compared: (P.1) using only the lookback window, (P.2) combining the lookback window with a prompt from the first 2F time points of the series, (P.3) combining the lookback window with a prompt from the last 2F time points of the series, and (P.4) combining the lookback window with a prompt from 2F time points of another series. The results are presented in terms of averaged error for four different time series frequencies (Yearly, Quarterly, Monthly, Others). Strategy P.2 and P.3 show improvement over P.1, while P.4 shows negative results as expected.</p><details><summary>read the caption</summary>Table 19: Effects of different strategies to retrieve time series as prompts for in-context forecasting.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_22_1.jpg alt></figure></p><blockquote><p>üîº This table presents the results of an ablation study on different prompt selection strategies for in-context forecasting. The study compares the performance of using different methods to select the time series prompts that are concatenated with the lookback window to form the context for prediction. The strategies include using the first 2F time points, the last 2F time points, randomly selected time points, and time points from other uncorrelated time series. The average error for each strategy is reported, illustrating the significant impact of prompt engineering on forecasting performance and highlighting the importance of using relevant and periodic time series prompts.</p><details><summary>read the caption</summary>Table 20: Strategies to select time series prompts based on periodicity for in-context forecasting.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/FOvZztnp1H/tables_22_2.jpg alt></figure></p><blockquote><p>üîº This table presents the ablation study comparing the performance of AutoTimes with a variant called &lsquo;FlattenHead&rsquo;. FlattenHead replaces the segment-wise projection used in AutoTimes with a simpler flatten linear head, a common approach in non-autoregressive forecasting models. The results demonstrate that the performance of the non-autoregressive method (FlattenHead) is consistently inferior to the autoregressive approach (AutoTimes), highlighting the importance of AutoTimes&rsquo; autoregressive design for better performance.</p><details><summary>read the caption</summary>Table 21: Ablation study of the autoregression. FlattenHead replaces the segment-wise projection of AutoTimes by flatten and linear head [26], which is prevalent in non-autoregressive forecasters.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-ff25e03b6ab861fffe5c34b0fcf58b0e class=gallery><img src=https://ai-paper-reviewer.com/FOvZztnp1H/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/FOvZztnp1H/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/&amp;title=AutoTimes:%20Autoregressive%20Time%20Series%20Forecasters%20via%20Large%20Language%20Models" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/&amp;text=AutoTimes:%20Autoregressive%20Time%20Series%20Forecasters%20via%20Large%20Language%20Models" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/&amp;subject=AutoTimes:%20Autoregressive%20Time%20Series%20Forecasters%20via%20Large%20Language%20Models" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_posters/FOvZztnp1H/index.md",oid_likes="likes_posters/FOvZztnp1H/index.md"</script><script type=text/javascript src=/neurips2024/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/neurips2024/posters/yxorsms5wr/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">AV-Cloud: Spatial Audio Rendering Through Audio-Visual Cloud Splatting</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/neurips2024/posters/fexx8pmrdt/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">AutoSurvey: Large Language Models Can Automatically Write Surveys</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
AI Paper Reviewer</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/neurips2024/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/neurips2024/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>