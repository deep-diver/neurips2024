[{"figure_path": "FOvZztnp1H/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Prevalent LLM4TS methods non-autoregressively generate predictions with the globally flattened representation of lookback series, while large language models inherently predict the next tokens by autoregression [47]. (b) Previous methods adopt language prompts that may lead to the modality disparity, while we find time series can be self-prompted, termed in-context forecasting.", "description": "This figure compares two approaches for using LLMs in time series forecasting. (a) shows the prevalent non-autoregressive approach, where the LLM processes a flattened representation of the lookback series to generate predictions all at once. In contrast, the figure highlights the autoregressive approach proposed in the paper. This approach leverages the inherent autoregressive nature of LLMs for sequential prediction of the next token in the series. (b) demonstrates a comparison of prompting mechanisms.  The traditional method uses natural language prompts, which can introduce modality disparities. The proposed method leverages time series itself for prompting, a technique referred to as \"in-context forecasting\".", "section": "1 Introduction"}, {"figure_path": "FOvZztnp1H/figures/figures_3_1.jpg", "caption": "Figure 2: An example to illustrate how AutoTimes adapts language models for time series forecasting.", "description": "This figure illustrates the core idea of AutoTimes, which repurposes large language models (LLMs) for time series forecasting.  It shows how a sequence of time series data points is converted into a sequence of tokens, similar to how words are tokens in natural language. The LLM, which is trained on natural language data, is then used to predict the next tokens in the time series sequence. The figure visually depicts the token-wise alignment between the time series tokens and the language tokens, showing that the LLM's inherent autoregressive property (predicting the next token based on the previous ones) is leveraged for forecasting. The color gradient from dark to light indicates the flow of information, from the lookback window to the future predictions.", "section": "3 Modality Alignment"}, {"figure_path": "FOvZztnp1H/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen.", "description": "This figure illustrates the overall architecture of AutoTimes.  It shows how time series data and timestamps are processed.  First, the time series and timestamps are segmented into smaller chunks.  Textual timestamps are then converted into position embeddings using a large language model (LLM).  The time series segments are then embedded and projected using the LLM's intermediate layers (the LLM weights are frozen during this process). Finally, next-token prediction is used for forecasting.", "section": "3 Method"}, {"figure_path": "FOvZztnp1H/figures/figures_7_1.jpg", "caption": "Figure 4: Demonstration of in-context forecasting and results compared with zero-shot. We uniformly select the foremost time points from the target domain as prompts and concatenate them with lookback to obtain the prediction. AutoTimes adapts LLMs on the source domain with a larger context length to place the additional time series prompt. Supplementary showcases are provided in Figure 12.", "description": "This figure demonstrates the in-context forecasting approach of AutoTimes and compares it with the zero-shot approach.  In the zero-shot approach, the model trained on a source domain is directly applied to a target domain without any additional information.  The in-context approach, however, incorporates additional time series prompts from the target domain as context.  These prompts are concatenated with the lookback window before feeding into the forecaster.  The bar chart visually compares the SMAPE (Symmetric Mean Absolute Percentage Error) results of both approaches across different data subsets (Year, Quarter, Month, Others), demonstrating the effectiveness of the in-context forecasting method.", "section": "4.3 In-Context Forecasting"}, {"figure_path": "FOvZztnp1H/figures/figures_8_1.jpg", "caption": "Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen.", "description": "This figure illustrates the AutoTimes model architecture.  The process begins by segmenting the input time series into smaller segments and representing timestamps as text.  These textual timestamps are then processed by a large language model (LLM) to generate position embeddings. The time series segments are also embedded, and these embeddings are concatenated with the position embeddings. The resulting embeddings are then fed into the LLM for next token prediction, with the intermediate layers of the LLM frozen to reduce computational cost.  The output is a prediction of future time series values.", "section": "3 Method"}, {"figure_path": "FOvZztnp1H/figures/figures_8_2.jpg", "caption": "Figure 6: Efficiency comparison of alternative LLMs, evaluated by the same configuration of Table 5.", "description": "This figure compares the training time, inference time, and number of tunable parameters for different LLMs used in the AutoTimes model.  The LLMs compared are GPT-2 and LLaMA-7B. The results show that AutoTimes is significantly more efficient in terms of training and inference time, and uses far fewer tunable parameters compared to other LLM-based forecasting methods. This highlights the efficiency of AutoTimes in leveraging LLMs for time-series forecasting.", "section": "Method Analysis"}, {"figure_path": "FOvZztnp1H/figures/figures_15_1.jpg", "caption": "Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen. language tokens, the position embedding can be integrated with the corresponding time span without increasing the context length. Concretely, the token embedding E\u1d62 \u2208 R<sup>D</sup> is obtained by: E\u1d62 = SE\u1d62 + TE\u1d62.", "description": "This figure illustrates the overall architecture of the AutoTimes model.  It shows how the model processes time series data by segmenting it and converting timestamps into position embeddings using a pre-trained Large Language Model (LLM).  The core idea is to embed time series segments into the LLM's embedding space, leveraging its inherent capabilities for token transition and prediction.  Importantly, the intermediate layers of the LLM are frozen, thus, the model efficiently uses the pre-trained LLM's power without heavy training.", "section": "3 Method"}, {"figure_path": "FOvZztnp1H/figures/figures_20_1.jpg", "caption": "Figure 7: Hyperparameter sensitivity of AutoTimes. Each curve presents a specific forecast length.", "description": "This figure shows the impact of different hyperparameters on the performance of AutoTimes for various forecast lengths.  The hyperparameters tested are the number of layers and the hidden dimension in both the Segment Embedding and Segment Projection components, the context length, and the segment length.  Each line represents a different forecast length (pred-96, pred-192, pred-336, pred-720), and the x-axis shows the different values tested for each hyperparameter. The y-axis shows the Mean Squared Error (MSE). This allows for an assessment of how sensitive the model's performance is to changes in these hyperparameters and helps in determining optimal settings.", "section": "C Hyperparameter Sensitivity"}, {"figure_path": "FOvZztnp1H/figures/figures_20_2.jpg", "caption": "Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen. language tokens, the position embedding can be integrated with the corresponding time span without increasing the context length. Concretely, the token embedding E\u1d62 \u2208 R<sup>D</sup> is obtained by: E\u1d62 = SE\u1d62 + TE\u1d62.", "description": "This figure illustrates the AutoTimes model architecture.  It shows how time series data and timestamps are processed. First, time series are segmented into smaller chunks and textual timestamps are created.  Then, the LLM converts the timestamps into position embeddings.  Next, time series segments are converted into embeddings using a segment embedding function, and these embeddings are then combined with the timestamp embeddings. Finally, next-token prediction is performed using the frozen layers of a pre-trained LLM. The resulting token embeddings form the input of subsequent layers in the model's autoregressive process.  The overall approach leverages pre-trained LLMs for forecasting with minimal training.", "section": "3 Method"}, {"figure_path": "FOvZztnp1H/figures/figures_22_1.jpg", "caption": "Figure 1: (a) Prevalent LLM4TS methods non-autoregressively generate predictions with the globally flattened representation of lookback series, while large language models inherently predict the next tokens by autoregression [47]. (b) Previous methods adopt language prompts that may lead to the modality disparity, while we find time series can be self-prompted, termed in-context forecasting.", "description": "This figure compares two approaches for using LLMs for time series forecasting.  (a) shows the common non-autoregressive approach where the LLM processes the entire lookback series at once to generate predictions. This is contrasted with (b), which shows the autoregressive approach used by AutoTimes. The key difference is AutoTimes uses an autoregressive approach to generate the next prediction token at a time, which is how LLMs naturally function.  It also shows that AutoTimes uses a self-prompting mechanism (in-context forecasting) which differs from the use of language prompts in prior work.", "section": "1 Introduction"}, {"figure_path": "FOvZztnp1H/figures/figures_23_1.jpg", "caption": "Figure 1: (a) Prevalent LLM4TS methods non-autoregressively generate predictions with the globally flattened representation of lookback series, while large language models inherently predict the next tokens by autoregression [47]. (b) Previous methods adopt language prompts that may lead to the modality disparity, while we find time series can be self-prompted, termed in-context forecasting.", "description": "This figure compares the prevalent large language model for time series forecasting methods.  Panel (a) shows that most existing methods don't use the autoregressive nature of LLMs, processing the entire lookback period at once rather than sequentially.  Panel (b) highlights a key difference: the proposed method uses the time series itself as a prompt (in-context forecasting), avoiding issues that arise from using natural language prompts that don't directly align with the time series data.", "section": "1 Introduction"}, {"figure_path": "FOvZztnp1H/figures/figures_23_2.jpg", "caption": "Figure 4: Demonstration of in-context forecasting and results compared with zero-shot. We uniformly select the foremost time points from the target domain as prompts and concatenate them with lookback to obtain the prediction. AutoTimes adapts LLMs on the source domain with a larger context length to place the additional time series prompt. Supplementary showcases are provided in Figure 12.", "description": "This figure demonstrates the results of in-context forecasting and compares them to zero-shot forecasting. In in-context forecasting, time series prompts from the target domain are concatenated with the lookback series before feeding to the model. The results show that using these prompts improves forecasting performance compared to the zero-shot approach. Supplementary figures showing additional results are referenced.", "section": "4.3 In-Context Forecasting"}]