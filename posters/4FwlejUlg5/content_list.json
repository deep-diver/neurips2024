[{"type": "text", "text": "Causal Effect Estimation with Mixed Latent Confounders and Post-treatment Variables ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Causal inference from observational data has attracted considerable attention among   \n2 researchers. One main obstacle is the handling of confounders. As direct mea  \n3 surement of confounders may not be feasible, recent methods seek to address the   \n4 confounding bias via proxy variables, i.e., covariates postulated to be conducive to   \n5 the inference of latent confounders. However, the selected proxies may scramble   \n6 both confounders and post-treatment variables in practice, which risks biasing the   \n7 estimation by controlling for variables affected by the treatment. In this paper, we   \n8 systematically investigate the bias due to latent post-treatment variables, i.e., latent   \n9 post-treatment bias, in causal effect estimation. Specifically, we first derive the   \n10 bias when selected proxies scramble both confounders and post-treatment variables,   \n11 which we demonstrate can be arbitrarily bad. We then propose a novel Confounder  \n12 identifiable VAE (CiVAE) to address the bias. Based on a mild assumption that the   \n13 prior of latent variables that generate the proxy belongs to a general exponential   \n14 family with at least one invertible sufficient statistic in the factorized part, CiVAE   \n15 individually identifies latent confounders and latent post-treatment variables up   \n16 to bijective transformations. We then prove that with individual identification,   \n17 the intractable disentanglement problem of latent confounders and post-treatment   \n18 variables can be transformed into a tractable independence test problem. Finally,   \n19 we prove that the true causal effects can be unbiasedly estimated with transformed   \n20 confounders inferred by CiVAE. Experiments on both simulated and real-world   \n21 datasets demonstrate significantly improved robustness of CiVAE. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 Causal inference, which aims to infer cause-and-effect relations from data, has gained increasing   \n24 prominence in various fields, such as social science, economics, and public health [10, 17, 34].   \n25 Traditional methods rely on the golden standard of randomized control trials (RCT) to draw valid   \n26 causal conclusions via experimentation [6]. Recently, more attention has been dedicated to causal   \n27 inference from observational data, where treatments, outcomes, and unit features are passively   \n28 observed, and researchers have no control over the treatment assignment mechanism [36, 37, 40].   \n29 One main obstacle to inferring valid causal relations from observational data is the confounding   \n30 bias, which occurs when we fail to account for the systematic difference between the treatment and   \n31 non-treatment group due to variables that causally influence the past treatments and the outcome, i.e.,   \n32 unobserved confounders [16]. If the confounders can be measured, a simple strategy to address the   \n33 bias is to control them via covariate adjustment [33] or propensity score re-weighting [24]. However,   \n34 confounders are not always measurable [23]. Therefore, recent methods seek to adjust for the   \n35 influence of unobserved confounders based on their proxies, which are easily acquirable covariates   \n36 postulated to be causally related with the unobserved confounders [29, 42, 28]. One exemplar work   \n37 is the causal effect variational auto-encoder (CEVAE) [25], which has demonstrated that confounding   \n38 bias can be mitigated by controlling latent variables inferred from the proxies of confounders.   \n39 Although proxy-based methods have achieved substantial progress in recent years, they may risk   \n40 controlling latent post-treatment variables scrambled in the proxies, where latent post-treatment   \n41 bias can be introduced. Here, we note that the negative effects of controlling observed post-treatment   \n42 variables have been investigated in prior research [1, 9, 21]. For example, Montgomery et al. [30]   \n43 found that more than $50\\%$ of the papers published in top journals of politics inadvertently control   \n44 post-treatment variables in the experimental setting, even though researchers have complete control   \n45 over which covariates to control for. On this basis, we postulate that the post-treatment bias could   \n46 be even worse for proxy-based methods in the setting of observational study where variables are   \n47 passively recorded. In addition, the post-treatment variables can be latent and scrambled into the   \n48 observed covariates together with the latent confounders, which makes them difficult to disentangle.   \n49 Consider a real-world example from the Company1. We found that changing a job from onsite to   \n50 online mode causes applicants to make different decisions, and we want to estimate the causal effects   \n51 of switching a job from onsite to online mode to the decisions of the applicants (reflected by statistics   \n52 of applicants that apply for the job). In this case, the Company collected two groups of online (treated)   \n53 and onsite (control) jobs, where the statistics of the applicants (e.g., the average age) are calculated as   \n5 the surrogate outcome. Clearly, job seniority is a confounder, since less senior jobs are more likely to   \n55 permit online work, and applicants for these jobs tend to be younger. However, the seniority level of   \n56 a job can be difficult to measure. Therefore, the required skills of the job can be used as the proxy of   \n57 the confounder \"seniority\", as senior jobs tend to require more advanced skills. However, a caveat is   \n58 that switching to an online work mode may also alter the required skills of a job, thereby affecting the   \n59 qualification and, therefore, the decision of the applicants. Consequently, directly using the skills as   \n60 the proxy of the confounder \"seniority\" for adjustment could unintentionally control latent mediators   \n61 (changed skills), which introduces latent post-treatment bias in the causal effect estimation.   \n62 Addressing the latent post-treatment bias faces multi-faceted challenges. First, there lacks a   \n63 theoretical formulation of the bias when selected proxies scramble latent post-treatment variables   \n64 for existing proxy-based methods. In addition, it is difficult to distinguish confounders and post  \n65 treatment variables in the latent space due to their similar observed behaviors. Existing covariate   \n66 disentanglement-based methods, e.g., TEDVAE [44], focus on an easier task of disentangling latent   \n67 confounders with latent adjusters and instrumental variables, which can be achieved by leveraging   \n68 their different predictive abilities w.r.t. the treatment and outcome. However, since both latent   \n69 confounders and post-treatment variables correlate with the treatment and the outcome, they cannot   \n70 be disentangled by these methods. Finally, even if latent confounders can be distinguished from post  \n71 treatment variables, since most existing latent variable models have no identifiability guarantee [19],   \n72 it is unclear whether controlling the inferred latent variables, which may be arbitrary transformations   \n73 of the true confounders, can provide unbiased estimations of true causal effects.   \n74 To address the aforementioned challenges, we first analyze existing proxy-based methods when se  \n75 lected proxies scramble both latent confounders and post-treatment variables and show the estimation   \n76 can be arbitrarily biased. We then propose a novel Confounder-identifiable VAE (CiVAE) to address   \n77 the latent post-treatment bias. Specifically, we prove that based on a mild assumption that the prior   \n78 of latent variables that generate the observed proxy (i.e., the latent confounders and post-treatment   \n79 variables) belong to a general exponential family with at least one invertible sufficient statistic in the   \n80 factorized part, latent confounders and latent post-treatment variables can be individually identified up   \n81 to simple bijective transformations. With such identifiability guarantee, based on the causal relations   \n82 among confounders, mediators, and treatment, we further demonstrate that the inferred confounders   \n83 (which are actually transformed proxies of the true confounders) could be properly distinguished   \n84 from the latent post-treatment variables with pair-wise conditional independence tests. Finally, we   \n85 prove that the true causal effects can be unbiasedly estimated based on transformed confounders   \n86 inferred by CiVAE. Experiments on both simulated and real-world datasets demonstrate that CiVAE   \n87 shows more robustness to latent post-treatment bias than existing methods. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "4FwlejUlg5/tmp/65708aae3564e0dead7e167c659a351527e9469b5af783b9ba70944ce56698ea.jpg", "img_caption": ["Figure 1: Comparison between the causal models assumed by CEVAE, TEDVAE, and CiVAE. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "88 2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "89 In this paper, we assume the causal model in Fig. 1-(c). We use a binary random variable $T$ to   \n90 denote the treatment, a random vector $\\pmb{X}\\in\\mathbb{R}^{K_{X}}$ to denote the observed covariates (i.e., the proxy),   \n91 and a random scalar $Y\\in\\mathbb{R}$ to denote the outcome. Furthermore, the observed covariates $X$ are   \n92 assumed to be generated from $K_{C}$ independent latent confounders $C\\triangleq[C_{1},C_{2}...,C_{K_{C}}]$ causally   \n93 influencing both $T$ and $Y$ , and $K_{M}$ latent post-treatment variables $M\\triangleq[M_{1},M_{2}...,M_{K_{M}}]$ under   \n94 the causal influence of the treatment (where the relation between $_M$ and $Y$ can be arbitrary). We use   \n95 the random vector $Z\\triangleq[C||M|\\in\\mathbb{R}^{K_{Z}=K_{C}+K_{M}}$ to denote all latent factors. Our aim is to estimate   \n96 the average causal effects of treatment $T$ on outcome $Y$ with auxiliary confounder information in $\\mathbf{\\deltaX}$ ,   \n97 where the estimation should be devoid of both confounding bias and post-treatment bias. ", "page_idx": 2}, {"type": "text", "text": "98 3 Theoretical Analysis of Latent Post-Treatment Bias ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "99 3.1 Preliminaries and Assumptions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "100 To achieve such a purpose, we first define the (conditional) average treatment effects (C/ATE) when   \n101 covariates $\\mathbf{\\deltaX}$ scramble both latent confounders $_{C}$ and post-treatment variables $_M$ . We then define   \n102 the post-treatment bias when covariates $\\mathbf{\\deltaX}$ are directly used as the proxy of confounders. To facilitate   \n103 the analysis, we make the following assumption regarding the causal generative process.   \n104 Assumption 1. (Noisy-Injectivity). We assume ${\\pmb X}\\,=\\,f({\\pmb C},M)+{\\pmb\\epsilon},$ , where $f$ is a deterministic   \n105 function that combines latent confounders $_{C}$ and latent post-treatment variables $_M$ into observations   \n106 $\\mathbf{\\deltaX}$ , and $\\epsilon$ is random noise. In addition, we assume that the function $f$ is injective; beyond injectivity,   \n107 $f$ can be arbitrarily nonlinear. We use $f^{\\dagger}\\,:\\,X\\,\\rightarrow\\,[C]|M]$ to denote its left inverse. We use   \n108 $f_{C}^{\\dagger}:X\\to C$ and $f_{M}^{\\dagger}:X\\to M$ to denote the mapping from $\\mathbf{\\deltaX}$ to $C,\\,M$ , respectively.   \n109 Noisy-Injectivity is a common assumption made either explicitly or implicitly in most existing proxy  \n110 of-confounder-based causal inference algorithms. For example, if both $\\mathbf{\\deltaX}$ and $_{C}$ are categorical,   \n111 [31] assumes that $\\mathbf{\\deltaX}$ has at least the same number of categories as $_{C}$ , whereas the effect restoration   \n112 algorithm [35] assumes that the matrix of $p(C,X)$ to be full-rank. Although CEVAE [25] makes no   \n113 explicit injectivity assumption between $_{C}$ and $\\mathbf{\\deltaX}$ , it requires that the joint distribution $p(C,X,T,Y)$   \n114 can be fully recovered from the observations $(X,T,Y)$ . [2] show that some of the possible identifica  \n115 tion criteria for the recovery include 1) having multiple independent views of $_{C}$ in $\\mathbf{\\deltaX}$ [8], and 2) $_{C}$   \n116 is categorical and $\\mathbf{\\deltaX}$ is a mixture of Gaussian components determined by $_{C}$ (that is, $\\mathbf{\\deltaX}$ is generated   \n117 by bijective mapping of $_{C}$ to the mean of the corresponding component with added Gaussian noise).   \n118 In the following part of this section, we omit the noise $\\epsilon$ to gain better intuition of latent post-treatment   \n119 bias (but all the exact conclusions will still hold in the posterior sense [19]). In Section 4, we assume   \n120 noise exists and demonstrate that our method can still properly identify the latent confounders. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "121 3.2 Causal Estimand and the True ATE ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "122 Based on Assumption 1, we are ready to define the estimand of average treatment effect (ATE)   \n123 through controlling the covariates $X^{\\prime}$ , as well the as the true (conditional) average treatment effects. ", "page_idx": 2}, {"type": "text", "text": "124 Definition 1. (DCEV & DEV). We define the Difference in Conditional Expected Values (DCEV) as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nD C E V(\\pmb{x}^{\\prime})=\\mathbb{E}[Y|T=1,\\pmb{X}^{\\prime}=\\pmb{x}^{\\prime}]-\\mathbb{E}[Y|T=0,\\pmb{X}^{\\prime}=\\pmb{x}^{\\prime}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "125 which is the difference of the expected value of $Y$ for units with variable $X^{\\prime}=x^{\\prime}$ in the treatment   \n126 group and the non-treatment group. Based on $D C E V({\\pmb x}^{\\prime})$ , we define the Difference in Expected   \n127 Value (DEV) as $D E V(\\pmb{X^{\\prime}})=\\bar{\\mathbb{E}}_{p(\\pmb{X^{\\prime}})}[D C E V(\\pmb{X^{\\prime}})]$ as the expectation of $D C E V$ w.r.t. $p(X^{\\prime})$ .   \n128 $D E V(X^{\\prime})$ denotes the estimand of ATE when $X^{\\prime}$ is the covariates that we choose to control (i.e.,   \n129 calculate the expected difference in each stratum of $X^{\\prime}=x^{\\prime}.$ ). If $X^{\\prime}=\\emptyset$ , $D E V(\\varnothing)$ represents   \n130 the naive estimator that directly calculates the expected difference of the outcome $Y$ between the   \n131 treatment group and the non-treatment group. With the causal estimand $D E V(X^{\\prime})$ defined, we then   \n132 derive the true causal effects with the covariates $X^{\\prime}$ when it scrambles both latent confounders and   \n133 post-treatment variables according to the generative process described in Assumption 1:   \n134 Definition 2. Under Assumption $^{\\,I}$ , we define the Conditional Average Treatment Effect (CATE) for   \n135 individuals with observed covariates $X=x$ by controlling only the confounder part in $\\mathbf{\\deltaX}$ as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nC A T E(\\pmb{x})=\\mathbb{E}[Y|T=1,C=f_{C}^{\\dagger}(\\pmb{x})]-\\mathbb{E}[Y|T=0,C=f_{C}^{\\dagger}(\\pmb{x})],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "136 with the Average Treatment Effect (ATE) of treatment $T$ defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nA T E=\\mathbb{E}[Y|d o(T=1)]-\\mathbb{E}[Y|d o(T=0)]=\\mathbb{E}_{p(C)}[\\mathbb{E}[Y|T=1,C]-\\mathbb{E}[Y|T=0,C]].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 Please note that we only consider the latent confounder component of the observed features $\\mathbf{\\deltaX}$ in the   \n138 definition of CATE in Eq. (2). This is because the causal relationship between the post-treatment   \n139 variables $_M$ and the outcome $Y$ is indeterminate. However, if the specific relationship between $_M$   \n140 and $Y$ can be further established by the researcher (e.g., all elements of $_M$ are latent mediators),   \n141 more precise forms of CATE can be derived with path-specific counterfactual analysis [5, 14]. ", "page_idx": 3}, {"type": "text", "text": "142 3.3 Latent Post-Treatment Bias ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "143 With $D E V(X^{\\prime})$ (the ATE estimator that control for the covariates $X^{\\prime}$ ), CATE, and ATE defined in   \n144 Section 3.2, in this section, we analyze the latent post-treatment bias of existing proxy-of-confounder  \n145 based causal inference methods, such as CEVAE, that control for latent variables inferred from   \n146 the covariates $\\mathbf{\\deltaX}$ to estimate the ATE of $T$ on $Y$ , when $\\mathbf{\\deltaX}$ scrambles both latent confounders and   \n147 post-treatment variables as Assumption 1. In our analysis, Lemma 3.1 will be frequently used. ", "page_idx": 3}, {"type": "text", "text": "148 Lemma 3.1. For an injective function g $,\\,\\mathbb{E}[Y|X^{\\prime}=\\pmb{x}^{\\prime}]=\\mathbb{E}[Y|g(\\pmb{X}^{\\prime})=g(\\pmb{x}^{\\prime})]$ holds. ", "page_idx": 3}, {"type": "text", "text": "149 The proof when $g$ is differentiable a.e. can be referred to in Appendix C.1. Since the latent variable   \n150 models used in existing methods (such as VAE with factorized Gaussian prior in CEVAE) lack   \n151 identifiability guarantee (i.e., the recovery of the exact latent variables), we assume that these models   \n152 can recover the true latent space $Z=[C,M]$ up to invertible transformations $\\bar{f}$ , where the inference   \n153 process can be represented as $\\hat{\\pmb Z}=\\tilde{f}(\\pmb X)=\\bar{f}\\circ f^{\\dagger}(\\pmb X)$ . With such an assumption, we have the   \n154 following theorem regarding the latent post-treatment bias when $\\mathbf{\\deltaX}$ mixes post-treatment variables.   \n155 Theorem 3.2. If the observed covariates $\\mathbf{\\deltaX}$ are generated from latent confounders $_{C}$ and latent   \n156 post-treatment variables $_M$ according to Assumption $^{\\,l}$ , the latent post-treatment bias of a proxy  \n157 based causal inference algorithm that controls latent variables $\\hat{Z}$ inferred from $\\mathbf{\\deltaX}$ via $\\tilde{f}=\\bar{f}\\circ f^{\\dagger}$ :   \n158 $\\mathbb{R}^{K_{X}}\\rightarrow\\mathbb{R}^{K_{C}+K_{M}}$ to estimate the ATE can be formulated as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B i a s(\\pmb{X})=A T E-D E V(\\tilde{f}(\\pmb{X}))=A T E-\\mathbb{E}[\\mathbb{E}[Y|T=1,\\tilde{f}(\\pmb{X})]-\\mathbb{E}[Y|T=0,\\tilde{f}(\\pmb{X})]]}\\\\ &{=A T E-\\mathbb{E}[\\mathbb{E}[Y|1,\\bar{f}\\circ f^{\\dagger}(f(C,M))]-\\mathbb{E}[Y|0,\\bar{f}\\circ f^{\\dagger}(f(C,M))]]}\\\\ &{=\\mathbb{E}[\\mathbb{E}[Y|1,C]-\\mathbb{E}[Y|0,C]]-\\mathbb{E}[\\mathbb{E}[Y|1,C,M]-\\mathbb{E}[Y|0,C,M]],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "159 which can be arbitrarily bad. Therefore, the estimator of existing proxy-of-confounder-based meth  \n160 ods, i.e., $D E V({\\tilde{f}}(X))$ , is an arbitrarily biased estimator of the ATE, when the selected proxy of   \n161 confounders $\\mathbf{\\deltaX}$ accidentally mixes in latent post-treatment variables $_M$ .   \n162 The final step of Eq. (4) can be proved since $f$ is injective and $\\bar{f}$ bijective, the composite $\\bar{f}\\circ f^{\\dagger}\\circ f:$   \n163 $[C,M]\\to{\\hat{Z}}$ is bijective, so we can use Lemma 3.1 to remove $\\bar{f}\\circ f^{\\dagger}\\circ f$ in the condition. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "164 3.4 Examples in the Linear Case ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "165 Generally, the latent post-treatment bias defined in Eq. (4) cannot be simplified, because $(i)$ the   \n166 causal relationship between $_M$ and $Y$ are indeterminate, and $(i i)$ the causal influence of $C,M$ ,   \n167 and $T$ on $Y$ can be arbitrary. However, for linear structural causal models with determined causal   \n168 relationships between $_M$ and $Y$ (e.g., $_M$ are mediators, which are post-treatment variables that have   \n169 causal influences on the outcomes), stronger conclusions can be drawn as follows: ", "page_idx": 3}, {"type": "text", "text": "170 Corollary 3.3. (Mixed Latent Mediator). For the linear Structural Causal Model (SCM) defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(i)\\:T\\leftarrow\\mathbb{1}(\\alpha_{T}+\\sum\\beta_{i}\\cdot C_{i}>a),\\:(i i)\\:M_{j}\\leftarrow\\alpha_{M}+\\gamma_{j}\\cdot T}}\\\\ {{(i i i)\\:X\\leftarrow\\alpha_{X}+\\mathbf{A}[M||C],\\:(i v)\\:Y\\leftarrow\\alpha_{Y}+\\tau\\cdot T+\\sum\\theta_{j}\\cdot M_{j}+\\sum\\kappa_{i}\\cdot C_{i},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 where the mixture function $f=\\mathbf{A}\\in\\mathbb{R}^{K_{X}\\times(K_{C}+K_{M})}$ is a full column-rank matrix, the CATE, ATE,   \n172 and the bias of proxy-of-confounder-based causal inference model that controls the latent variables   \n173 $\\hat{Z}$ inferred via ${\\hat{Z}}={\\tilde{f}}(X)=\\mathbf{B}^{T}X$ can be formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A T E=C A T E=\\tau+\\displaystyle\\sum\\gamma_{j}\\cdot\\theta_{j},\\mathrm{~and~}D E V(\\hat{\\pmb Z})=\\mathbb{E}[D C E V(\\hat{\\pmb Z})]=D C E V(\\hat{\\pmb Z})=\\tau}\\\\ &{B i a s(\\hat{\\pmb Z})=A T E-D E V(\\hat{\\pmb Z})=\\displaystyle\\sum\\gamma_{j}\\cdot\\theta_{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 where $\\mathbf{B}\\in\\mathbb{R}^{K_{X}\\times(K_{C}+K_{M})}$ is another full column-rank matrix. Since $\\sum\\gamma_{j}\\cdot\\theta_{j}$ is arbitrary, the   \n175 estimator $D E V(\\hat{\\pmb{Z}})=\\mathbb{E}[D C E V(\\mathbf{B}^{T}\\pmb{X})]$ is arbitrarily biased for ATE estimation.   \n176 The proof of Eq. (6) is provided in Appendix C.2. In addition, we show that post-treatment variables   \n177 $_M$ DO NOT necessarily need to have direct causal effects on the outcome $Y$ to incur arbitrary bias   \n178 in ATE estimation. In Appendix C.3, we provide another example (i.e., Mixed Latent Correlator) in   \n179 the linear case where $_M$ is correlated with $Y$ through unobserved confounders $U$ in Corollary C.1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "180 4 Methodology ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "181 In this section, we introduce the proposed Confounder-identifiable Variational Auto-Encoder (CiVAE)   \n182 in detail. Specifically, we first prove that if the prior distribution of the true latent variables ${\\boldsymbol{Z}}=$   \n183 $[C,M]$ satisfies certain weak assumptions, CiVAE individually identify $[C,M]$ up to bijective   \n184 transformations. Then, utilizing the causal relations between $C,M$ , and $T$ , we novelly transform the   \n185 challenging confounder-identifiability problem into a tractable pair-wise conditional independence   \n186 test problem, which can be effectively solved with kernel-based methods. The generalization of   \n187 CiVAE to address the interactions among $[C,M]$ are discussed in Section D of the Appendix. ", "page_idx": 4}, {"type": "text", "text": "188 4.1 Generative Process ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "189 The fundamental work on the identifiability of deep variational inference, i.e., the identifiable VAE   \n190 (iVAE) [19], makes a strict assumption that the prior of true latent variables $\\mathbf{Z}$ (i.e., $[C,M]$ in   \n191 our case) is conditionally factorized given the available covariates. However, since both $_{C}$ and   \n192 $_M$ form fork structures with the outcome $Y$ (see Fig. 1-(c)) [22], $C_{i}$ , $C_{j}$ , $M_{i}$ , and $M_{j}$ are not   \n193 independent given $Y$ . Recently, Non-Factorized iVAE (NF-iVAE) [26] was proposed that allows   \n194 arbitrary dependence among the true latent variables $Z$ in the conditional priors, where $Z$ can be   \n195 identified up to arbitrary non-linear transformations. However, the transformation is not necessarily   \n196 invertible, which is risky as multiple values of the confounders may collapse, leading to bias when   \n197 estimating the ATE by averaging the $D C E V$ calculated in each stratum of the inferred confounders.   \n198 In contrast to NF-iVAE, CiVAE guarantees the individual and bijective identifiability of $Z$ by putting   \n199 a general exponential family with at least one invertible sufficient statistic in the factorized part as its   \n200 prior when conditioning on treatment $T$ and outcome $Y$ , which can be formulated as follows.   \n201 Assumption 2. Let $Z\\,=\\,[C||M]$ be the random vector for latent variables that causally gen  \n202 erate the observed covariates $\\mathbf{\\deltaX}$ according to Assumption $^{\\,l}$ . We assume that the conditional   \n203 prior of $Z$ given the outcome $Y$ and the treatment $T$ belongs to a general exponential family   \n204 with parameter vector $\\lambda(Y,T)$ and sufficient statistics $\\bar{\\pmb{S}}(\\pmb{Z})\\bar{\\pmb{\\mathscr{\\tau}}}=[\\pmb{S}_{f}\\bar{(\\pmb{Z})}^{T},\\pmb{S}_{n f}\\bar{(\\pmb{Z})}^{T}]^{T}$ . Specif  \n205 ically, $S(Z)$ is composed of $(i)$ the sufficient statistics of a factorized exponential family, i.e.,   \n206 $\\begin{array}{r}{{\\pmb S}_{f}({\\pmb Z})\\,=\\,[{\\pmb S}_{1}(Z_{1})^{T},\\cdot\\cdot\\cdot\\,,{\\pmb S}_{K_{Z}}(Z_{K_{Z}})^{T}]^{T}}\\end{array}$ , where all components $S_{i}(Z_{i})$ have dimension larger   \n207 than or equal to 2 and each $S_{i}$ has at least one invertible dimension, and (ii) $S_{n f}(Z)$ , where $S_{n f}$ is   \n208 a neural network with ReLU activation. The density of the conditional prior can be formulated as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{S,\\lambda}(Z|Y,T)=\\mathcal{Q}(Z)/\\mathcal{C}(Y,T)\\exp[S(Z)^{T}\\lambda(Y,T)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "209 where $\\mathcal{Q}(Z)$ is the base measure, and $\\mathcal{C}(Y,T)$ is the normalizing constant independent of $Z$ . ", "page_idx": 4}, {"type": "text", "text": "210 We justify that assumption 2 is weak and practical as follows. (i) Neural networks with ReLU   \n211 activation have universal approximation ability of distributions [27]. Therefore, Eq. (7) can model   \n212 arbitrary dependence between true latent confounders $_{C}$ and post-treatment variables $_M$ conditional   \n213 on $T$ and $Y$ . (ii) Although CiVAE makes an extra assumption that $\\forall i$ , at least one dimension of $S_{i}$ is   \n214 invertible, this can be easily satisfied as most commonly used exponential family distributions, such   \n215 as Gaussian, Bernoulli, etc., has at least one invertible sufficient statistics2.   \n216 The reason why we use ReLU as the activation is that, the identifiability of iVAE relies on the   \n217 condition that the sufficient statistics $\\boldsymbol{S}$ have zero second-order cross-derivative. The factorized part,   \n218 i.e., $S_{f}$ , satisfies it trivially as all cross-derivatives of $S_{f}$ are zero. In addition, since the ReLU neural   \n219 networks are linear a.e., all second-order derivatives of $S_{n f}$ are zero. Therefore, identifiability holds   \n220 after adding $S_{n f}$ in the prior that allows the capturing of arbitrary dependence among $Z$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "221 4.2 Optimization Objective ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "222 Combining Assumptions 1 and 2, the generative process assumed by CiVAE can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n(i)\\;p_{\\theta}(X,Z\\mid Y,T)=p_{f}(X\\mid Z),(i i)\\;\\;p s,\\lambda(Z\\mid Y,T),\\;(i i i)\\;p_{f}(X\\mid Z)=p_{\\epsilon}(X-f(Z)).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "223 where $\\pmb{\\theta}=(f,\\pmb{\\lambda},\\pmb{S})\\in\\Theta$ are the parameters of the generative distribution. Since the generative   \n224 process of CiVAE is parameterized by deep neural networks, the posterior distribution of $Z$ , i.e.,   \n225 $p_{\\theta}(Z\\mid X,Y,T)$ , is intractable. Therefore, we resort to variational inference [4], where we introduce   \n226 an approximate posterior $q_{\\phi}(Z\\mid X,Y,T)$ parameterized by a deep neural network with a trainable   \n227 parameter $\\phi$ , and in $q_{\\phi}(Z|\\cdot)$ finds the one closest to $p_{\\pmb{\\theta}}(\\pmb{Z}|\\cdot)$ measured by KL divergence. The   \n228 minimization of KL is equivalent to maximization of the evidence lower bound (ELBO): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta,\\phi):=\\mathbb{E}_{q_{\\phi}}\\left[\\log p_{f}(\\boldsymbol{X}\\mid\\boldsymbol{Z})+\\underbrace{\\log p_{{S},\\lambda}(\\boldsymbol{Z}\\mid\\boldsymbol{Y},\\boldsymbol{T})-\\log q_{\\phi}(\\boldsymbol{Z}\\mid\\cdot)}_{\\mathrm{KL~of~posterior~with~prior~}}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "229 Since the normalization constant $\\mathcal{C}$ in Eq. (7) is generally intractable, it is infeasible to directly learn   \n230 $S,\\lambda$ by optimizing Eq. (9). Therefore, we substitute the KL term in Eq. (9) with the widely-used   \n231 score matching [13] to learn unnormalized densities instead as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(S,\\lambda,\\phi):=\\mathbb{E}_{q_{\\phi}(\\pmb{Z}|\\cdot)}\\left[\\|\\nabla_{\\pmb{Z}}\\log q_{\\phi}(\\pmb{Z}\\mid\\cdot)-\\nabla_{\\pmb{Z}}\\log p_{S,\\lambda}(\\pmb{Z}\\mid Y,T)\\|^{2}\\right]}\\\\ &{=\\mathbb{E}_{q_{\\phi}(\\pmb{Z}|\\cdot)}\\left[\\displaystyle\\sum_{j=1}^{K_{z}}\\left[\\frac{\\partial^{2}p_{S,\\lambda}(\\pmb{Z}\\mid Y,T)}{\\partial Z_{j}^{2}}+\\frac{1}{2}\\left(\\frac{\\partial p_{S,\\lambda}(\\pmb{Z}\\mid Y,T)}{\\partial Z_{j}}\\right)^{2}\\right]\\right]+\\mathrm{~const,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "232 4.3 Identifiability of CiVAE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "233 With the generative process and optimization objective of CiVAE discussed in previous sub-sections,   \n234 we are ready to introduce the final assumption of CiVAE, which, combined with Assumptions 1 and   \n235 2, leads to the main Theorem of this paper, which states the identifiability of CiVAE.   \n236 Assumption 3. Assume the following: (i) The set $\\{X\\in{\\mathcal{X}}|\\phi(X)=0\\}$ has measure zero, where $\\phi$   \n237 is the characteristic function of the density $p_{f}$ in Eq. (8). (ii) The sufficient statistics, $S_{i}$ in $S_{f}$ are all   \n238 twice differentiable. (iii) The mixture function $f$ in $E q$ . (8) has all second-order cross derivatives.   \n239 $(i\\nu)$ There exist $k+1$ distinct points $(Y,T)_{0},\\cdot\\cdot\\cdot\\ ,(Y,T)_{k}$ s.t. the matrix $\\mathbf{L}\\;=\\;[\\lambda((Y,T)_{1})\\;-$   \n240 $\\lambda((Y,T)_{0}),\\cdot\\cdot\\cdot\\cdot,\\lambda((Y,T)_{k})-\\lambda((Y,T)_{0})]$ of size $k\\times k$ is invertible, where $k=D i m(S)$ .   \n241 Here, we note that Assumptions $(i)-(i i i)$ are trivial for differentiable neural networks. The Assumption   \n242 $(i\\nu)$ can be intuitively understood as independent samples of $(Y,T)$ are required to identify $_{C}$ and   \n243 $_M$ . The identifiability theorem of CiVAE can be formulated as follows.   \n244 Theorem 4.1. If Assumptions 1, 2, and 3 hold, and if $\\theta,\\tilde{\\theta}\\in\\Theta\\rightarrow p_{\\theta}(X|Y,T)=p_{\\tilde{\\theta}}(X|Y,T),$ , the   \n245 true latent variables $_{z}$ are identifiable up to permutation and element-wise bijective transformation.   \n246 Furthermore, in the case of variational inference, $i f$ we denote the true parameter that generates the   \n247 data as $\\theta^{*}$ , $i f(i)$ the distribution family $q_{\\phi}(Z|X,Y,T)$ contains the posterior $p_{\\theta}(Z|X,Y,T)$ , and   \n248 $q_{\\phi}(Z|X,Y,T)>0,$ , (ii) we optimize Eq. (4) w.r.t. both $\\theta,\\phi$ , then in the limit of infinite data, true   \n249 parameters $\\theta^{*}$ can be learned up to a permutation and bijective transformation of $Z$ .   \n250 The proof of Theorem 4.1 non trivially extends the NF-iVAE paper [26] by incorporating the new   \n251 assumption introduced in CiVAE (i.e., each $S_{i}$ has at least one invertible dimension) to ensure that the   \n252 transformation of each $Z_{i}$ is bijective. The detailed proof is provided in Appendix C.4 for reference. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "253 4.4 Identification of Latent Confounders ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "254 Theorem 4.1 ensures that the latent variables $\\hat{Z}$ inferred by CiVAE cannot $(i)$ mix confounders   \n255 and post-treatment variables in each dimension, or $(i i)$ collapsing of different values of the latent   \n256 confounders into the same value. To further determine the dimensions of confounder and post  \n257 treatment variable in $\\hat{Z}$ , we rely on the causal relations between latent variables $\\hat{Z}$ and the treatment   \n258 $T$ and the associated marginal/conditional dependence properties, which are discussed as follows.   \n259 \u2022 Case 1. Intra-Confounders. Latent confounders $C_{i}$ , $C_{j}$ and the treatment $T$ form the $V$ structure   \n260 $C_{i}\\ \\to\\ T\\,\\leftarrow\\,C_{j}$ . Therefore, $C_{i}$ and $C_{j}$ are marginally independent, whereas they become   \n261 dependent when conditioning on the assigned treatment $T$ .   \n262 \u2022 Case 2. Intra-Post Treatment Variables. Latent post-treatment variables $M_{i}$ , $M_{j}$ and the treatment   \n263 $T$ form a Fork-structure $M_{i}\\gets T\\rightarrow M_{j}$ , where $M_{i}$ , $M_{j}$ are marginally dependent, but they   \n264 become independent after conditioning on the assigned treatment $T$ .   \n265 \u2022 Case 3. Cross-Confounder and Post-Treatment Variables. Latent confounder $C_{i}$ , latent post  \n266 treatment variable $M_{j}$ , and the treatment $T$ forms a Chain structure $C_{i}\\to T\\to M_{j}$ , where $C_{i}$ ,   \n267 $M_{j}$ are marginally dependent, and they become independent after conditioning on $T$ .   \n268 From the above analysis we can find that, the dependence between two latent variables ${\\hat{Z}}_{i}$ and $\\hat{Z}_{j}$   \n269 increases after conditioning on the treatment $T$ ONLY in the case of intra-confounders. Therefore,   \n270 if more than one latent confounder exists, which is highly probable when covariates $\\mathbf{\\deltaX}$ are high  \n271 dimensional, we can conduct independence test $\\mathtt{I n d}(\\hat{Z}_{i},\\hat{Z}_{j})$ and $\\mathsf{C I n d}(\\hat{Z}_{i},\\hat{Z}_{j}|T)$ for all pairs of   \n272 inferred latent variables, which can be implemented via kernel-based methods as [43], and select   \n273 the pairs where the $\\mathbf{p}$ -value of CInd is larger than that of Ind as latent confounders. Here, we note   \n274 that the kernel-based (conditional) independence test incurs $N^{2}\\times K_{Z}^{2}$ complexity in the training   \n275 phase. However, once the dimensions of the confounders in $\\hat{Z}$ are determined, CiVAE has the same   \n276 complexity as CEVAE for the estimation of CATE and ATE in the test phase. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "277 4.5 ATE Estimator with Transformed Confounders ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "278 Finally, we demonstrate that controlling the transformed confounders $\\hat{C}$ inferred by CiVAE provides   \n279 an unbiased estimation of ATE. Specifically, we have the final Theorem show the unbiasedness.   \n280 Theorem 4.2. Controlling bijective of confounders is equivalent to original confounders in ATE   \n281 estimation, i.e., $D E V(\\tilde{C})\\stackrel{}{=}\\stackrel{}{D E V}(g(\\stackrel{.}{C}))\\stackrel{.}{=}A T E,$ if the transformation function $g$ is bijective.   \n282 The proof of Theorem 4.2 for discrete $_{C}$ is trivial (where ${\\hat{C}}=g(C)$ represents a simple relabeling   \n283 of the stratum that we calculate the $D C E V$ and take the expectation). The proof in the continuous   \n284 case where $g$ is differentiable is provided in Appendix C.5. With Theorem 4.2, we can control the   \n285 identified latent confounders as true confounders, providing an unbiased estimate of ATE. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "286 5 Empirical Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "287 In this section, we provide and analyze the experiments we conduct on both simulated and real-world   \n288 datasets, where a code demo written in PyTorch and Pyro is provided in this anonymous URL. ", "page_idx": 6}, {"type": "text", "text": "289 5.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "290 Simulated Datasets. We first establish two simulated datasets, i.e., LatentMediator and   \n291 LatentCorrelator, that consider two types of post-treatment variables, i.e., $(i)$ mediators and   \n292 (ii) correlators, i.e., variables that are correlated with the outcome $Y$ via latent confounders $U$ , where   \n293 the causal generative process is under the full control of the experimenter. The generative process of   \n294 the two datasets can be referred to in Corollary 3.3 and Corollary C.1 in the Appendix, respectively.   \n295 In our experiments, $_{C}$ are generated from Gaussian distribution as $C\\sim G a u s s i a n(0,{\\bf I}_{K_{C}})$ . For   \n296 LatentMediator, $\\gamma$ is set as $[-1,-1,-1]$ , $\\pmb{\\theta}$ is set as $[1,1,1]$ , and $\\tau$ is set as 2, which results in   \n297 $A T E=-1$ . For the LatentCorrelator dataset, we set the same $\\gamma$ and $\\pmb{\\theta}$ as the LatentMediator   \n298 dataset, where parameters $\\phi$ and $\\tau$ are set to 1, which results in an overall $A T E$ of 1.   \n299 Real-world Datasets. In addition, we build real-world datasets from the Company to estimate the   \n300 ATE of switching a job from onsite to online work mode to the statistics of the applicants. The   \n301 average age and the variance of gender of the applicants are two outcomes of interest. Covariates   \n302 $\\overline{{\\boldsymbol{X}\\in\\{0,1\\}}}^{K_{X}}$ include the required skills of the job. Specifically, we establish a cohort of 3,228   \n303 jobs from the Bay Area in the US, where a preliminary study shows that $D E V(\\emptyset)\\approx2$ years3(i.e.,   \n304 online job applicants are two years younger than onsite job applicants in the collected data), and   \n305 $D E V(\\varnothing)\\approx-0.015$ (i.e., online jobs exhibit 0.015 more gender variance than onsite jobs in the   \n306 collected data). To simulate $_{C}$ and $_M$ , we first learn a generative model as follows: ", "page_idx": 6}, {"type": "image", "img_path": "4FwlejUlg5/tmp/a818c30532dca4f024c272486c3fc7595590347db767f1f885897a4be83cd767.jpg", "img_caption": ["Figure 2: Visualization of $p$ -value of independence test before and after conditioning on treatment $T$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nZ\\sim G a u s s i a n(\\mathbf{0},\\mathbf{I}_{K_{Z}}),X\\sim M u l t i(N N_{f}(Z)),Y\\sim G a u s s i a n(w\\odot Z,1),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "307 where Multi represents multinomial distribution, $N N_{f}$ is a neural network with softmax activation,   \n308 $\\boldsymbol{Z},\\boldsymbol{w}\\,\\in\\,\\mathbb{R}^{K_{Z}}$ , $K_{Z}\\,=\\,8$ , and $\\odot$ represents the element-wise product operator, respectively. We   \n309 then treat the first $K_{C}~=~5$ dimensions of $Z$ as the latent confounders $_{C}$ and the remaining   \n310 $K_{M}=K_{Z}-K_{C}$ dimensions as the latent mediators $_M$ . After learning $N N_{f}$ and $\\mathbf{\\nabla}w$ according to   \n311 Eq. (11), we draw latent confounders $C\\in G a u s s i a n(0,{\\bf I})$ , latent mediators ${\\dot{M}}=T\\cdot\\gamma$ , and set the   \n312 outcome $Y=\\pmb{w}\\odot[\\pmb{C}||\\pmb{M}]+\\pmb{\\tau}\\cdot\\pmb{T}$ , where the true ATE can be calculated as $s u m(\\gamma\\odot{\\pmb w}_{-K_{M}:})+\\tau$ ", "page_idx": 7}, {"type": "text", "text": "313 5.1.1 Disentangle Confounders and Post-treatment Variables ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "314 We first show the $p$ -value of the kernel-based pairwise independence test of the true latent variables   \n315 before and after conditioning on the assigned treatment $T$ . From Fig. 2, we can find that the distinction   \n316 of the intra-confounder case from the other two cases discussed in Subsection 4.4 is significant. Here,   \n317 we should note this relies on the assumption that latent confounders are independent. If the latent   \n318 confounders are correlated, we can first use causal discovery techniques such as the PC algorithm [39]   \n319 to find direct parents of $T$ , and use our algorithm as the refinement to determine the true confounders   \n320 $C$ from the misidentified post-treatment variables (Experiments see Section D) in Appendix. ", "page_idx": 7}, {"type": "text", "text": "321 5.2 Baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "322 The baselines we include for comparisons can be categorized into three classes. (i) Unawareness,   \n323 where no information in $\\mathbf{\\deltaX}$ is used for ATE estimation. We implement the naive LR0 estimator, which   \n324 regresses $Y$ on $T$ and uses the coefficient to estimate the ATE [15] (LR0 equals to $D E V(\\varnothing)$ , i.e., the   \n325 difference of the average outcome between the treatment and non-treatment group). $(\\romannumeral3)$ Control- $\\mathbf{\\deltaX}$ ,   \n326 which directly controls the covariates $\\mathbf{\\deltaX}$ . In this class, LR1 regresses $Y$ on $T$ and $\\mathbf{\\deltaX}$ , whereas TarNet   \n327 uses a two-branch neural network to estimate the $D E V(X)$ (iii) Control- $_{z}$ , which controls latent   \n328 variables $_{z}$ learned from the covariates $\\mathbf{\\deltaX}$ . Methods from this class include the CEVAE [25] and   \n329 covariate disentanglement methods, such as DR-CFR [12], TEDVAE [44], NICE [38], and AFS [41]. ", "page_idx": 7}, {"type": "text", "text": "330 5.2.1 Results and Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "331 From Table 1, we can find that for all four datasets, CEVAE is worse than the naive LR0 estimator.   \n332 In addition, for the LatentMediator and Company (Age) dataset, all methods except CiVAE fail   \n333 to predict the negativity of the ATE. Covariates disentanglement-based methods, i.e., DR-CFR   \n334 and TEDVAE, inherit the latent post-treatment bias of CEVAE. The reason is that, these methods   \n335 disentangle latent confounders $_{C}$ from latent instrumental variables $\\boldsymbol{\\mathit{I}}$ and latent adjusters $\\pmb{A}$ by   \n336 utilizing their causal relations with $T$ and $Y$ , i.e., $\\boldsymbol{\\mathit{I}}$ is predictive only for $T$ , $\\pmb{A}$ is predictive only   \n337 for $Y$ , whereas $_{C}$ is predictive for both $T$ and $Y$ . For example, TEDVAE includes three encoders   \n338 to infer three sets of latent variables ${\\hat{I}}.$ , $\\hat{A}$ , $\\hat{C}$ from $\\mathbf{\\deltaX}$ and adds classification losses $p(T|\\hat{\\cal I},\\hat{C})$   \n339 and $p(Y|T,\\hat{C},\\hat{A})$ on the CEVAE loss. However, since both latent confounders $_{C}$ and latent post  \n340 treatment variables $_M$ are correlated with both $T$ and $Y$ , these methods cannot disentangle $_{C}$ from   \n341 $_M$ . An exception is NICE [38], which uses invariant risk minimization (IRM) [3] to find all causal   \n342 parents of the outcome $Y$ as the confounders, which makes it more robust in the LatentCorrelator   \n343 case. However, since mediators $\\mathbf{M}$ are also the causal parent of $Y$ , the performance degrades   \n344 substantially on the LatentMediator dataset. Although AFS [41] considers the existence of post  \n345 treatment variables $_M$ in the proxy $\\mathbf{\\deltaX}$ , it assumes that they can be separated from other variables in   \n346 $\\mathbf{\\deltaX}$ in the observational space, and no relationship exists between the post-treatment variables and the   \n347 outcome, so it still has poor performance in our setting since both assumptions are violated. ", "page_idx": 7}, {"type": "table", "img_path": "4FwlejUlg5/tmp/7d5ab86676e6116f4bbe21367ddafba34634207659ae872968c10eb628f16e94.jpg", "table_caption": ["Table 1: Comparison of CiVAE with baselines under latent post-treatment bias on various datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "348 5.3 Sensitivity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "349 In this part, we vary the number   \n350 of confounders and post-treatment   \n351 variables that generate proxy $\\mathbf{\\deltaX}$ in   \n352 the Company (Age) and Company   \n353 (Gender) datasets and compare   \n354 CiVAE with the baseline TEDVAE   \n355 in Fig. 3. Fig. 3 shows that the er  \n356 ror is consistently lower for CiVAE.   \n357 In addition, the error is compara  \n358 tively higher when the number of con  \n359 founders is low since the misidenti  \n360 fication of latent post-treatment vari  \n361 ables as confounders can have a com  \n362 paratively larger influence on the ATE estimation. In addition, when the number of confounders   \n363 becomes larger, the performance gap between CiVAE and TEDVAE gracefully shrinks. ", "page_idx": 8}, {"type": "image", "img_path": "4FwlejUlg5/tmp/1007c318cbf46ea5cbf5579eea48be1fb67c462e3a3f4e7ae79f35416433ad69.jpg", "img_caption": ["Figure 3: Error with different ratio of latent confounders and latent post-treatment variable in the latent space. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "364 6 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "365 In this paper, we systematically investigate the latent post-treatment bias in causal inference from   \n366 observational data. We first prove that unresolved latent post-treatment variables scrambled in the   \n367 proxy of confounders can arbitrarily bias the ATE estimation. To address the bias, we proposed   \n368 the Confounder-identifiable VAE (CiVAE), which, utilizing a mild assumption regarding the prior   \n369 of latent factors, guarantees the identifiability of latent confounders up to bijective transformations.   \n370 Finally, we show that controlling the latent confounders inferred by CiVAE can provide an unbiased   \n371 estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that   \n372 CiVAE has superior robustness to latent post-treatment bias compared to state-of-the-art methods. ", "page_idx": 8}, {"type": "text", "text": "373 References", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "374 [1] A. Acharya, M. Blackwell, and M. Sen. Explaining causal findings without bias: Detecting and   \n375 assessing direct effects. American Political Science Review, 110(3):512\u2013529, 2016.   \n376 [2] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for   \n377 learning latent variable models. Journal of Machine Learning Research, 15:2773\u20132832, 2014.   \n378 [3] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv   \n379 preprint arXiv:1907.02893, 2019.   \n380 [4] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians.   \n381 Journal of the American Statistical Association, 112(518):859\u2013877, 2017.   \n382 [5] L. Cheng, R. Guo, and H. Liu. Causal mediation analysis with hidden confounders. In   \n383 Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,   \n384 pages 113\u2013122, 2022.   \n385 [6] T. D. Cook, D. T. Campbell, and W. Shadish. Experimental and quasi-experimental designs for   \n386 generalized causal inference. Houghton Mifflin Boston, MA, 2002.   \n387 [7] P. Ding and L. W. Miratrix. To adjust or not to adjust? sensitivity analysis of m-bias and   \n388 butterfly-bias. Journal of Causal Inference, 3(1):41\u201357, 2015.   \n389 [8] J. K. Edwards, S. R. Cole, and D. Westreich. All your data are always missing: incorporating   \n390 bias due to measurement error into the potential outcomes framework. International Journal of   \n391 Epidemiology, 44(4):1452\u20131459, 2015.   \n392 [9] F. Elwert and C. Winship. Endogenous selection bias: The problem of conditioning on a collider   \n393 variable. Annual review of sociology, 40:31\u201353, 2014.   \n394 [10] T. A. Glass, S. N. Goodman, M. A. Hern\u00e1n, and J. M. Samet. Causal inference in public health.   \n395 Annual Review of Public Health, 34:61\u201375, 2013.   \n396 [11] N. Hassanpour and R. Greiner. Counterfactual regression with importance sampling weights.   \n397 In IJCAI, pages 5880\u20135887, 2019.   \n398 [12] N. Hassanpour and R. Greiner. Learning disentangled representations for counterfactual   \n399 regression. In International Conference on Learning Representations, 2020.   \n400 [13] A. Hyv\u00e4rinen and P. Dayan. Estimation of non-normalized statistical models by score matching.   \n401 Journal of Machine Learning Research, 6(4), 2005.   \n402 [14] K. Imai, L. Keele, and D. Tingley. A general approach to causal mediation analysis. Psycholog  \n403 ical Methods, 15(4):309, 2010.   \n404 [15] G. W. Imbens and D. B. Rubin. Causal inference in statistics, social, and biomedical sciences.   \n405 Cambridge University Press, 2015.   \n406 [16] K. Jager, C. Zoccali, A. Macleod, and F. Dekker. Confounding: what it is and how to deal with   \n407 it. Kidney international, 73(3):256\u2013260, 2008.   \n408 [17] F. Johansson, U. Shalit, and D. Sontag. Learning representations for counterfactual inference.   \n409 In International Conference on Machine Learning, pages 3020\u20133029, 2016.   \n410 [18] M. Kalisch and P. B\u00fchlman. Estimating high-dimensional directed acyclic graphs with the   \n411 pc-algorithm. Journal of Machine Learning Research, 8(3), 2007.   \n412 [19] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational autoencoders and nonlinear   \n413 ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics,   \n414 pages 2207\u20132217. PMLR, 2020.   \n415 [20] G. King. A hard unsolved problem? post-treatment bias in big social science questions. In   \n416 Hard Problems in Social Science\u201d Symposium, April, volume 10, 2010.   \n417 [21] G. King and L. Zeng. The dangers of extreme counterfactuals. Political Analysis, 14(2):131\u2013159,   \n418 2006.   \n419 [22] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT   \n420 press, 2009.   \n421 [23] M. Kuroki and J. Pearl. Measurement bias and effect restoration in causal inference. Biometrika,   \n422 101(2):423\u2013437, 2014.   \n423 [24] F. Li, K. L. Morgan, and A. M. Zaslavsky. Balancing covariates via propensity score weighting.   \n424 Journal of the American Statistical Association, 113(521):390\u2013400, 2018.   \n425 [25] C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, and M. Welling. Causal effect inference   \n426 with deep latent-variable models. Advances in Neural Information Processing Systems, 30,   \n427 2017.   \n428 [26] C. Lu, Y. Wu, J. M. Hern\u00e1ndez-Lobato, and B. Sch\u00f6lkopf. Invariant causal representation   \n429 learning for out-of-distribution generalization. In International Conference on Learning Repre  \n430 sentations, 2021.   \n431 [27] Y. Lu and J. Lu. A universal approximation theorem of deep neural networks for expressing   \n432 probability distributions. In Advances in Neural Information Processing Systems, pages 3094\u2013   \n433 3105, 2020.   \n434 [28] D. Madras, E. Creager, T. Pitassi, and R. Zemel. Fairness through causal awareness: Learning   \n435 causal latent-variable models for biased data. In Proceedings of the Conference on Fairness,   \n436 Accountability, and Transparency, pages 349\u2013358, 2019.   \n437 [29] W. Miao, Z. Geng, and E. J. Tchetgen Tchetgen. Identifying causal effects with proxy variables   \n438 of an unmeasured confounder. Biometrika, 105(4):987\u2013993, 2018.   \n439 [30] J. M. Montgomery, B. Nyhan, and M. Torres. How conditioning on posttreatment variables   \n440 can ruin your experiment and what to do about it. American Journal of Political Science,   \n441 62(3):760\u2013775, 2018.   \n442 [31] J. Pearl. On measurement bias in causal inference. arXiv preprint arXiv:1203.3504, 2012.   \n443 [32] J. Pearl. Conditioning on post-treatment variables. Journal of Causal Inference, 3(1):131\u2013137,   \n444 2015.   \n445 [33] S. J. Pocock, S. E. Assmann, L. E. Enos, and L. E. Kasten. Subgroup analysis, covariate   \n446 adjustment and baseline comparisons in clinical trial reporting: current practiceand problems.   \n447 Statistics in Medicine, 21(19):2917\u20132930, 2002.   \n448 [34] M. Prosperi, Y. Guo, M. Sperrin, J. S. Koopman, J. S. Min, X. He, S. Rich, M. Wang, I. E.   \n449 Buchan, and J. Bian. Causal inference and counterfactual prediction in machine learning for   \n450 actionable healthcare. Nature Machine Intelligence, 2(7):369\u2013375, 2020.   \n451 [35] K. J. Rothman, S. Greenland, T. L. Lash, et al. Modern epidemiology, volume 3. Wolters   \n452 Kluwer Health/Lippincott Williams & Wilkins Philadelphia, 2008.   \n453 [36] U. Shalit, F. D. Johansson, and D. Sontag. Estimating individual treatment effect: generalization   \n454 bounds and algorithms. In International Conference on Machine Learning, pages 3076\u20133085,   \n455 2017.   \n456 [37] C. Shi, D. Blei, and V. Veitch. Adapting neural networks for the estimation of treatment effects.   \n457 In Advances in Neural Information Processing Systems, 2019.   \n458 [38] C. Shi, V. Veitch, and D. M. Blei. Invariant representation learning for treatment effect estimation.   \n459 In Uncertainty in Artificial Intelligence, pages 1546\u20131555. PMLR, 2021.   \n460 [39] P. Spirtes, C. N. Glymour, R. Scheines, and D. Heckerman. Causation, prediction, and search.   \n461 MIT press, 2000.   \n462 [40] S. Wager and S. Athey. Estimation and inference of heterogeneous treatment effects using   \n463 random forests. Journal of the American Statistical Association, 113(523):1228\u20131242, 2018.   \n464 [41] H. Wang, K. Kuang, H. Chi, L. Yang, M. Geng, W. Huang, and W. Yang. Treatment effect   \n465 estimation with adjustment feature selection. In Proceedings of the 29th ACM SIGKDD   \n466 Conference on Knowledge Discovery and Data Mining, pages 2290\u20132301, 2023.   \n467 [42] L. Yao, S. Li, Y. Li, M. Huai, J. Gao, and A. Zhang. Representation learning for treatment effect   \n468 estimation from observational data. In Advances in Neural Information Processing Systems,   \n469 volume 31, 2018.   \n470 [43] K. Zhang, J. Peters, D. Janzing, and B. Sch\u00f6lkopf. Kernel-based conditional independence test   \n471 and application in causal discovery. arXiv preprint arXiv:1202.3775, 2012.   \n472 [44] W. Zhang, L. Liu, and J. Li. Treatment effect estimation with disentangled latent factors. In   \n473 Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10923\u201310930,   \n474 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "475 Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "476 A Broader Impact ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "477 The proposed CiVAE is a universal model for causal effect estimation with observational data.   \n478 Although we use the Company job data that estimate the causal effects of online working mode to   \n479 applicant statistics as a real-world example, proxy-of-confounder-based methods have been heavily   \n480 used in other observational studies, which may be susceptible to latent post-treatment bias. Therefore,   \n481 we speculate that the proposed CiVAE will have a broader impact on causal inference community. ", "page_idx": 12}, {"type": "text", "text": "482 B Related Work ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "483 B.1 Post-Treatment Bias in Causal Inference ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "484 Bias due to accidentally controlling post-treatment variables, i.e., post-treatment bias, has long been   \n485 recognized as dangerous in causal effect estimation [20]. Back at 2005, Pearl [32] cautioned that   \n486 controlling more is not better, and uses the collider bias [9] and M-Bias [7] as two examples to   \n487 show that bias can be increased when controlling the post-treatment variables. Furthermore, [30]   \n488 show that indirect correlations between post-treatment variable $_M$ and outcome $Y$ can still cause   \n489 bias. Recent works prove that even if $_M$ has no causal relationship with $Y$ , controlling it can still   \n490 increase the variance of estimand [12]. However, most of these works study the post-treatment bias   \n491 in the observational space, where latent post-treatment variables that are mixed with confounders to   \n492 generate the observed covariates can be easily ignored by the researcher. Therefore, it motivates us to   \n493 develop CiVAE, which is robust to the latent post-treatment bias under mild assumptions. ", "page_idx": 12}, {"type": "text", "text": "494 B.2 Covariate Disentanglement ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "495 Recently, researchers have realized that directly controlling proxy of confounders $\\mathbf{X}$ may not be   \n496 safe, as variables other than confounders could lurk in the proxy and ruin the ATE estimation [12].   \n497 Traditional methods assume that the variables that generate $\\mathbf{X}$ are a mixture of confounders, adjusters,   \n498 and influencers [36], where adjusters should not be controlled as it can increase the estimation   \n499 variance [11]. Most methods rely on the fact that adjusters are correlated only with the treatment   \n500 to separate them from other variables [12, 44] (see Fig. (1)). This can also be used to remove post  \n501 treatment variables that are not correlated with the outcome, which have similar statistics properties   \n502 with adjustors [41]. Here, a different work is NICE [38], which uses the fact that confounders and   \n503 influencers are direct causal parents of the outcome to find these variables with invariant learning as   \n504 the control set [3]. However, since mediators are also direct parents of the outcome, NICE is still not   \n505 robust to general post-treatment bias. Given that all above methods cannot satisfactorily address the   \n506 latent post-treatment in general cases, it is imperative to design the CiVAE, where confounders can   \n507 be identified and distinguished with latent post-treatment variables for unbiased adjustment. ", "page_idx": 12}, {"type": "text", "text": "508 C Theoretical Analysis ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "509 C.1 Proof of Lemma 3.1. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "510 Proof. Let $Z\\ =\\ f(X)$ and $z\\;=\\;f(x)$ . If $f$ is injective and differentiable a.e., and $f^{\\dagger}$ is the   \n511 left-inverse, we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathrm{~}_{Y\\mid f(X)}(y\\vert f(x))=f_{Y\\mid Z}(y\\vert z)={\\frac{f_{Y,Z}(y,z)}{f_{Z}(z)}}={\\frac{f_{Y,X}(y,f^{\\dagger}(z))\\vert{\\bf J}_{f^{\\dagger}}(z)\\vert}{f_{X}(f^{\\dagger}(z))\\vert{\\bf J}_{f^{\\dagger}}(z)\\vert}}={\\frac{f_{Y,X}(y,x)}{f_{X}(x)}}=f_{Y\\mid X}(y\\vert z).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "512 where $f.$ \u00b7 and $f_{\\cdot\\vert}$ \u00b7 represent the marginal and conditional density function, respectively, and $\\mathbf{J}_{f^{\\dagger}}(z)$ is   \n513 the Jacobian matrix of function $f^{\\dagger}$ evaluated at $_{\\textit{z}}$ . Based on Eq. (12), we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y|X]=\\int y\\cdot f_{Y|X}(y|x)d y=\\int y\\cdot f_{Y|Z}(y|z)d y=\\mathbb{E}[Y|Z=z]=\\mathbb{E}[Y|f(X)=f(x)].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "514 ", "page_idx": 12}, {"type": "text", "text": "516 Proof. For $X=x$ , let $[\\pmb{c}||\\pmb{m}]\\doteq[f_{C}^{\\dagger}(\\pmb{x})||f_{M}^{\\dagger}(\\pmb{x})]\\doteq f^{\\dagger}(\\pmb{x})=\\mathbf{A}^{\\dagger}(\\pmb{x}-\\alpha_{X})$ , where $\\mathbf{A}^{\\dagger}$ is the left   \n517 inverse of the full column-rank matrix A in Eq. (2), we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma A T E(x)=\\mathbb{E}[Y|T=1,C=f_{C}^{\\dagger}(x)]-\\mathbb{E}[Y|T=0,C=f_{C}^{\\dagger}(x)]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}[Y|T=1,C=c]-\\mathbb{E}[Y|T=0,C=c]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}[\\alpha\\gamma+\\tau\\cdot T+\\sum\\theta_{j}\\cdot M_{j}+\\sum\\kappa_{i}\\cdot C_{i}|T=1,C=c]}\\\\ &{\\quad\\quad\\quad-\\mathbb{E}[\\alpha\\gamma+\\tau\\cdot T+\\sum\\theta_{j}\\cdot M_{j}+\\sum\\kappa_{i}\\cdot C_{i}|T=0,C=c]}\\\\ &{\\quad\\quad\\quad=\\alpha\\gamma+\\tau\\cdot\\mathbb{E}[T|T=1,C=c]+\\sum\\theta_{j}\\cdot\\mathbb{E}[M_{j}|T=1,C=c]+\\sum\\kappa_{i}\\cdot\\mathbb{E}[C_{i}|T=1,C]}\\\\ &{\\quad\\quad\\quad-\\alpha\\gamma+\\tau\\cdot\\mathbb{E}[T|T=0,C=c]+\\sum\\theta_{j}\\cdot\\mathbb{E}[M_{j}|T=0,C=c]+\\sum\\kappa_{i}\\cdot\\mathbb{E}[C_{i}|T=0,C=c]}\\\\ &{\\quad\\quad\\quad=\\tau\\cdot(1-0)+\\sum\\theta_{j}\\cdot(\\gamma_{j}\\cdot(1-0))+\\sum\\kappa_{i}\\cdot(c_{i}-c_{i})}\\\\ &{\\quad\\quad\\quad=\\tau+\\sum\\theta_{j}\\cdot\\gamma_{j}=\\mathbb{E}[\\tau+\\sum\\theta_{j}\\cdot\\gamma_{j}]=A T E,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "518 where the first equality is due to the definition of CATE in Eq. (2). In addition, the causal estimand   \n519 and bias of a proxy-of-confounder-based causal inference model that controls the latent variable $_{z}$   \n520 inferred via ${\\dot{Z}}={\\dot{f}}(X)=\\mathbf{B}^{T}X$ (where $\\mathbf{B}$ is also a full column-rank matrix) can be formulated as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{2C E V(\\mathbf{B}^{T}x)=\\mathbb{E}[Y|T=1,Z=\\mathbf{B}^{T}x]-\\mathbb{E}[Y|T=0,Z=\\mathbf{B}^{T}x]}\\\\ {=\\mathbb{E}[Y|T=1,Z=\\mathbf{B}^{T}\\alpha_{X}+\\mathbf{B}^{T}\\mathbf{A}[c]|m]]-\\mathbb{E}[Y|T=0,Z=\\mathbf{B}^{T}\\alpha_{X}+\\mathbf{B}^{T}\\mathbf{A}[c]|\\eta}\\\\ {\\overset{(a)}{=}\\mathbb{E}[Y|T=1,C=c,M=m]-\\mathbb{E}[Y|T=0,C=c,M=m]}\\\\ {=\\alpha_{Y}+\\tau\\cdot1+\\sum\\theta_{j}\\cdot\\mathbb{E}[M_{j}|T=1,C=c,M=m]+\\sum\\kappa_{i}\\cdot\\mathbb{E}[C_{i}|T=1,C=}\\\\ {-\\alpha_{Y}+\\tau\\cdot0+\\sum\\theta_{j}\\cdot\\mathbb{E}[M_{j}|T=0,C=c,M=m]+\\sum\\kappa_{i}\\cdot\\mathbb{E}[C_{i}|T=0,C=}\\\\ {=\\tau\\cdot(1-0)+\\sum\\theta_{j}\\cdot(m_{j}-m_{j})+\\sum\\kappa_{i}\\cdot(c_{i}-c_{i})}\\\\ {=\\tau=\\mathbb{E}[\\tau]=\\mathbb{E}[D C E V(\\mathbf{B}^{T}X)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "521 where step (a) is due to the fact that, since both A and $\\mathbf{B}$ are full column-rank matrices, $\\mathbf{B}^{T}\\mathbf{A}$ is   \n522 an invertible matrix, and the mapping $f=\\mathbf{B}^{T}\\alpha_{X}+\\mathbf{B}^{T}\\mathbf{A}$ is bijective. Therefore, we can invoke   \n523 Lemma 3.1 and apply the left-inverse of $f$ , i.e., $f^{\\dagger}=(\\mathbf{B}^{T}\\mathbf{A})^{-1}-\\mathbf{B}^{T}\\alpha_{X}$ , to the condition of the   \n524 expectation. The rest steps are based on the structural causal equations defined in Eq. (2). \u53e3 ", "page_idx": 13}, {"type": "text", "text": "525 C.3 Another Case of Linear SCM with Latent Correlators ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "526 Corollary C.1. For another Linear Structural Causal Model defined as follows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{T\\leftarrow\\mathbb{1}(\\alpha_{T}+\\sum\\beta_{i}\\cdot C_{i}>a)}\\\\ {M_{j}\\leftarrow\\alpha_{M}+\\gamma_{j}\\cdot T+\\phi_{j}\\cdot U_{j}}\\\\ {X\\leftarrow\\alpha_{X}+\\mathbf{A}[M||C]}\\\\ {Y\\leftarrow\\alpha_{Y}+\\tau\\cdot T+\\sum\\theta_{j}\\cdot U_{j}+\\sum\\kappa_{i}\\cdot C_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "527 where $f\\,=\\,{\\bf A}\\,\\in\\,\\mathbb{R}^{K_{X}\\times(K_{C}+K_{M})}$ is a full column-rank matrix, the CATE, ATE, and the bias of   \n528 proxy-of-confounder-based causal inference model that controls the latent variable $Z$ inferred via   \n529 ${\\dot{Z}}={\\bar{f}}(X)=\\mathbf{B}^{T}X$ can be formulated as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A T E=C A T E=\\tau}\\\\ &{{\\mathbb{E}}[D C E V({\\pmb Z}={\\bf B}^{T}{\\pmb X})]=D C E V({\\pmb Z}={\\bf B}^{T}{\\pmb X})=\\tau-\\sum\\frac{\\theta_{j}\\cdot\\gamma_{j}}{\\phi_{j}}}\\\\ &{B i a s=A T E-{\\mathbb{E}}[D C E V({\\bf B}^{T}{\\pmb X})]=\\sum\\frac{\\theta_{j}\\cdot\\gamma_{j}}{\\phi_{j}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "530 where $\\mathbf{B}\\in\\mathbb{R}^{K_{X}\\times(K_{C}+K_{M})}$ is another full column-rank matrix. Since \u03b8j\u03d5\u00b7\u03b3j is arbitrary, the   \n531 estimator $\\mathbb{E}[D C E V(\\mathbf{B}^{T}X)]$ is arbitrarily biased for the estimation of ATE.   \n532 Proof. The proof of the CATE and ATE is trivial. The causal estimand and the bias of a proxy  \n533 of-confounder-based causal inference model that controls the latent variables $Z$ inferred via $Z=$   \n534 $\\bar{f}({\\cal X})={\\bf B}^{T}{\\cal X}$ (where $\\mathbf{B}$ is also a full column-rank matrix) can be formulated as follows: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Im C E V(\\mathbf B^{T}\\mathbf x)=\\mathbb{E}[Y|T=1,Z=\\mathbf B^{T}\\mathbf x]-\\mathbb{E}[Y|T=0,Z=\\mathbf B^{T}\\mathbf x]}\\\\ &{=\\mathbb{E}[Y|T=1,Z=\\alpha_{X}+\\mathbf B^{T}\\mathbf A[c]|\\mathbf m]]-\\mathbb{E}[Y|T=0,Z=\\alpha_{X}+\\mathbf B^{T}\\mathbf A[c]|m]]}\\\\ &{\\overset{(a)}{=}\\mathbb{E}[Y|T=1,C=c,M=m]-\\mathbb{E}[Y|T=0,C=c,M=m]}\\\\ &{=\\alpha_{Y}+\\tau\\cdot1+\\sum_{j}\\theta_{j}\\cdot\\mathbb{E}[U_{j}|T=1,C=c,M=m]+\\sum_{\\kappa_{i}}\\cdot\\mathbb{E}[C_{i}|T=1,C=\\cdot\\infty]}\\\\ &{-\\alpha_{Y}+\\tau\\cdot0+\\sum_{\\theta_{j}}\\cdot\\mathbb{E}[U_{j}|T=0,C=c,M=m]+\\sum_{\\kappa_{i}}\\cdot\\mathbb{E}[C_{i}|T=0,C=c,M=m]}\\\\ &{=\\tau\\cdot(1-0)+\\sum_{\\theta_{j}}\\cdot(\\phi_{j}^{-1}\\cdot(m_{j}-\\alpha_{M}-\\gamma_{j})-\\phi_{j}^{-1}\\cdot(m_{j}-\\alpha_{M}))+\\sum_{\\kappa_{i}}\\cdot(c_{i}-c_{i})}\\\\ &{=\\tau-\\sum_{\\phi=\\tau}\\frac{\\theta_{j}\\cdot\\gamma_{j}}{\\phi_{j}}=\\mathbb{E}\\left[\\tau-\\sum_{\\phi=\\tau}\\frac{\\theta_{j}\\cdot\\gamma_{j}}{\\phi_{j}}\\right]=\\mathbb{E}[D C E V(\\mathbf B^{T}X)],}\\\\ &{\\quad{-\\alpha_{Y}+\\tau\\cdot0+\\sum_{\\phi=\\tau}\\mathbb{E}[U_{j}|T=0,C=c,M=m]+\\sum_{\\kappa_{i}}\\cdot\\mathbb{E}[C_{i}|T=0,C=\\tau]}}\\\\ &{=\\tau\\cdot(1-0)+\\sum_{\\phi=\\tau}\\cdot(\\phi_{j}^{-1}\\cdot(m_{j}-\\alpha_{M}-\\gamma_{j})-\\phi_{j}^{-1}\\cdot(m_{j}-\\alpha_{M}))+\\sum_{\\kappa_{i}}\\cdot(c_{i}-c_{i})}\\\\ &{=\\tau-\\sum_{\\phi=\\tau}\\frac{\\theta_{j}\\cdot\\gamma_{j}}{\\phi_{j}}=\\mathbb{E}\\left[\\tau-\\sum_{\\phi\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "535 ", "page_idx": 14}, {"type": "text", "text": "536 where step (a) and the rest of the proof follow the same logic as the proof in Section 3.3. ", "page_idx": 14}, {"type": "text", "text": "537 C.4 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "538 The strict definitions of the exponential family, strong exponential (which is assumed for the factorized   \n539 part of the conditional prior), and identifiability follow [19, 26], and can be referred to in Appendix   \n540 E, F of [26], which we omit to avoid redundancy. The proof of Theorem 4.1 is largely based on the   \n541 NF-iVAE paper [26], where most of the details can be found, with the new assumption introduced in   \n542 CiVAE that each $S_{f,i}$ has at least one invertible dimension incorporated to ensure that each dimension   \n543 of the inferred latent variables is a bijective transformation of the corresponding true latent variable. ", "page_idx": 14}, {"type": "text", "text": "544 C.4.1 PART I ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "545 Step I. In this step, we transform the equality of noisy conditional marginal distribution of $\\mathbf{\\deltaX}$ given   \n546 $Y,T$ of two models with parameter $\\theta,\\dot{\\tilde{\\theta}}\\in\\dot{\\Theta}$ into the equality of noise-free distributions. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathrm{P}}(\\boldsymbol{\\mathrm{A}}\\mid\\boldsymbol{\\mathrm{T}},\\boldsymbol{\\mathrm{I}})=p_{0}\\delta\\left(\\boldsymbol{\\mathrm{A}}\\mid\\boldsymbol{\\mathrm{T}},\\boldsymbol{\\mathrm{T}},\\boldsymbol{\\mathrm{I}}\\right)}\\\\ &{\\implies\\int_{\\mathcal{X}}p_{i}(\\boldsymbol{\\mathrm{X}}\\mid\\boldsymbol{\\mathrm{Z}})\\,\\boldsymbol{\\mathrm{P}}_{i}\\boldsymbol{\\mathrm{X}},\\boldsymbol{\\mathrm{A}}(\\boldsymbol{\\mathrm{Z}})\\,\\int_{\\mathcal{X}}p_{i}(\\boldsymbol{\\mathrm{X}}\\mid\\boldsymbol{\\mathrm{Z}})\\,|\\mathcal{X}_{\\delta,\\lambda}(\\boldsymbol{\\mathrm{Z}})|\\,|\\mathcal{X}\\rangle}\\\\ &{\\implies\\int_{\\mathcal{X}}p_{i}(\\boldsymbol{\\mathrm{X}}-\\boldsymbol{\\mathrm{f}}(\\boldsymbol{\\mathrm{Z}}))p_{\\delta,\\lambda}(\\boldsymbol{\\mathrm{Z}}\\mid\\boldsymbol{\\mathrm{Y}},\\boldsymbol{\\mathrm{Z}})\\,\\mathrm{d}\\boldsymbol{\\mathrm{Z}}=\\int_{\\mathcal{X}}p_{i}(\\boldsymbol{\\mathrm{X}}-\\boldsymbol{\\mathrm{f}}(\\boldsymbol{\\mathrm{Z}}))p_{\\delta,\\lambda}(\\boldsymbol{\\mathrm{Z}}\\mid\\boldsymbol{\\mathrm{Y}},\\boldsymbol{\\mathrm{T}})d\\boldsymbol{\\mathrm{Z}}}\\\\ &{\\implies\\int_{\\mathcal{X}}p_{i}(\\boldsymbol{\\mathrm{X}}-\\boldsymbol{\\mathrm{X}})p_{\\delta,\\lambda}\\left(\\boldsymbol{\\mathrm{f}}^{\\mathrm{T}}(\\boldsymbol{\\mathrm{X}})\\mid\\boldsymbol{\\mathrm{Y}},\\boldsymbol{\\mathrm{T}}\\right)\\,\\mathrm{en}\\left(\\boldsymbol{\\mathrm{A}}_{\\gamma}\\right)\\delta\\boldsymbol{\\mathrm{X}}-}\\\\ &{\\qquad\\int_{\\mathcal{X}}p_{i}(\\boldsymbol{\\mathrm{X}}-\\boldsymbol{\\mathrm{X}})p_{\\delta,\\lambda}\\left(\\boldsymbol{\\mathrm{f}}^{\\mathrm{T}}(\\boldsymbol{\\mathrm{X}})\\mid\\boldsymbol{\\mathrm{Y}},\\boldsymbol{\\mathrm{T}}\\right)\\,\\mathrm{en}\\left(\\lambda_{\\gamma}(\\boldsymbol{\\mathrm{X}})\\right)d\\boldsymbol{\\mathrm{X}}}\\\\ &{\\stackrel{(i)}{\\longrightarrow}\\int_{\\mathcal{Y}}p_{i}(\\boldsymbol{\\mathrm{X}}-\\boldsymbol{\\mathrm{X}})\\underbrace{\\tilde{p}_{i}}_{\\delta,\\lambda,\\gamma}(\\boldsymbol{\\mathrm{X}})\\delta\\boldsymbol{\\mathrm{X}}-\\int_{\\mathcal{X}}p_{i}(\\boldsymbol{\\mathrm{X}}-\\boldsymbol{\\mathrm{X}})\\tilde{p}_{\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "547 Step (a) is based on the rule of change-of-variable, where $\\mathrm{vol}(\\mathbf{A})\\,=\\,\\sqrt{\\operatorname*{det}\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)}$ . In step (b),   \n548 we define $\\tilde{p}_{f,S,\\mathbf{\\lambda},Y,T}(\\mathbf{\\lambda}X)\\triangleq p_{S,\\mathbf{\\lambda}}\\big(f^{\\dagger}(X)\\mid Y,T\\big)$ vol $\\left(\\mathbf{J}_{f^{\\dagger}}(X)\\right)\\mathbb{I}_{\\mathcal{X}}(X)$ . In step (c), we use $F[\\cdot]$ to   \n549 denote the Fourier transform. In step (d), we drop $\\varphi_{\\varepsilon}(\\omega)$ as it is non-zero a.e. (see Assumption 3).   \n550 Step $\\mathbf{II}$ . In this step, we transform the equality of the noise-free distributions into the relationship of   \n551 the sufficient statistics $\\boldsymbol{S}$ and $\\Tilde{S}$ . By taking logarithm of both sides of Eq. (19), we have: ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\operatorname{vol}\\left(J_{f^{\\dagger}}(X)\\right)+\\log\\mathcal{Q}\\left(f^{\\dagger}(X)\\right)-\\log\\mathcal{C}(Y,T)+\\left\\langle S\\left(f^{\\dagger}(X)\\right),\\lambda(Y,T)\\right\\rangle}\\\\ &{=\\log\\operatorname{vol}\\left(J_{\\tilde{f}^{\\dagger}}(X)\\right)+\\log\\tilde{\\mathcal{Q}}\\left(\\tilde{f}^{\\dagger}(X)\\right)-\\log\\tilde{\\mathcal{C}}(Y,T)+\\left\\langle\\tilde{S}\\left(\\tilde{f}^{\\dagger}(X)\\right),\\tilde{\\lambda}(Y,T)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "552 Let $(Y,T)_{0},\\cdot\\cdot\\cdot\\ ,(Y,T)_{k}$ be the $k+1$ distinct points defined in Assumption 3 - (iv). We obtain $k+1$   \n553 equations by evaluating the Eq. (20) at these points, where the first equation is subtracted from the   \n554 remaining ones, which leads to the following equation system: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\left\\langle{\\cal S}\\left(f^{\\dagger}(X)\\right),\\lambda\\left((Y,T)_{l}\\right)\\right.\\left.-\\lambda\\left((Y,T)_{0}\\right)\\right\\rangle+\\log\\displaystyle\\frac{\\mathcal{C}\\left((Y,T)_{0}\\right)}{\\mathcal{C}\\left((Y,T)_{l}\\right)}}}\\\\ {{\\left.=\\left\\langle{\\tilde{\\cal S}}\\left(\\tilde{f}^{\\dagger}(X)\\right),\\tilde{\\lambda}\\left((Y,T)_{l}\\right)-\\tilde{\\lambda}\\left((Y,T)_{0}\\right)\\right\\rangle+\\log\\displaystyle\\frac{\\tilde{\\mathcal{C}}\\left((Y,T)_{0}\\right)}{\\tilde{\\mathcal{C}}\\left((Y,T)_{l}\\right)},\\quad l=1,\\cdots,k.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "555 Let $\\mathbf{L}$ be the invertible matrix defined in Assumption 3 - (iv) and $\\tilde{\\bf L}$ be the counterpart for $\\tilde{\\lambda}$ , if we   \n556 summarize all terms irrelevant to $\\mathbf{\\deltaX}$ into a constant $^{b}$ ,we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{L}^{T}S\\left(f^{\\dagger}(\\mathbf{\\boldsymbol{X}})\\right)=\\tilde{\\mathbf{L}}^{T}\\tilde{S}\\left(\\tilde{f}^{\\dagger}(\\mathbf{\\boldsymbol{X}})\\right)+\\pmb{b}}\\\\ {\\implies S\\left(f^{\\dagger}(\\mathbf{\\boldsymbol{X}})\\right)=\\mathbf{A}\\tilde{S}\\left(\\tilde{f}^{\\dagger}(\\mathbf{\\boldsymbol{X}})\\right)+\\pmb{c},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "557 where $\\mathbf{A}=\\mathbf{L}^{-T}\\tilde{\\mathbf{L}}\\in\\mathbb{R}^{k\\times k}$ , and $\\pmb{c}=\\mathbf{L}^{-T}\\pmb{b}\\in\\mathbb{R}^{k}$ . ", "page_idx": 15}, {"type": "text", "text": "558 Step III. Ideally, to prove the element-wise bijective identifiability of the latent variables $_{z}$ , the   \n559 transformation of the sufficient statistics $\\boldsymbol{S}$ derived in Eq. (22) should be bijective. We claim that if   \n560 the conditional prior $p s,\\lambda^{(Z\\mid Y,T)}$ is strongly exponential and $\\mathbf{L}$ is invertible, $\\tilde{\\bf L}$ and $\\mathbf{A}$ must also   \n561 be invertible. The proof is omitted, and can be referred to in Appendix H.1.1 of [26]. ", "page_idx": 15}, {"type": "text", "text": "562 C.4.2 PART II ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "563 In this part, we prove that, if Assumptions 1, 2 and 3 hold, we can identify the factorized part   \n564 of the sufficient statistics $S(Z)$ , i.e., $\\bar{S}_{f}(Z)$ , up to permutation and element-wise transformation.   \n565 Specifically, if we use $\\pmb{v}$ to denote the composite map $\\tilde{f}^{\\dagger}\\circ f:\\mathcal{Z}\\rightarrow\\mathcal{Z}$ , Eq. (22) can be rewritten into: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\cal S}(Z)={\\bf A}\\tilde{\\cal S}(v(Z))+c.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "566 We aim to prove that $\\mathbf{A}$ in Eq. (23) is a block permutation matrix. ", "page_idx": 15}, {"type": "text", "text": "567 Step I. We start by showing that $\\pmb{v}$ is a component-wise function. If we differentiate both sides of Eq.   \n568 (23) with respect to $Z_{s}$ and $Z_{t}$ , where $s\\neq t$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial S({\\pmb Z})}{\\partial Z_{s}}=\\mathbf{A}\\sum_{i=1}^{K_{z}}\\frac{\\partial\\tilde{S}(v({\\pmb Z}))}{\\partial v_{i}({\\pmb Z})}\\cdot\\frac{\\partial v_{i}({\\pmb Z})}{\\partial Z_{s}}}\\\\ {\\displaystyle\\frac{\\partial^{2}S({\\pmb Z})}{\\partial Z_{s}\\partial Z_{t}}=\\mathbf{A}\\sum_{i=1}^{K_{z}}\\sum_{i=1}^{K_{z}}\\frac{\\partial^{2}\\tilde{S}(v({\\pmb Z}))}{\\partial v_{i}({\\pmb Z})\\partial v_{j}({\\pmb Z})}\\cdot\\frac{\\partial v_{j}({\\pmb Z})}{\\partial Z_{t}}\\cdot\\frac{\\partial v_{i}({\\pmb Z})}{\\partial Z_{s}}+\\mathbf{A}\\sum_{i=1}^{K_{z}}\\frac{\\partial\\tilde{S}(v({\\pmb Z}))}{\\partial v_{i}({\\pmb Z})}\\cdot\\frac{\\partial^{2}v_{i}({\\pmb Z})}{\\partial Z_{s}\\partial Z_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "569 Note that for the factorized part of the sufficient statistics $\\boldsymbol{S}$ , i.e., $S_{f}$ , all cross-derivatives are zero,   \n570 and for the non-factorized part of $\\boldsymbol{S}$ , i.e., $S_{n f}$ , which is a neural network with ReLU activation (i.e.,   \n571 linear a.e.), all second-order derivatives are zero. Therefore, the second order cross-derivatives on   \n572 the LHS. of Eq. (24) are zero, which leads to the following equality: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\bf0}={\\bf A}\\sum_{i=1}^{K_{Z}}\\frac{\\partial^{2}\\tilde{S}(v({\\bf Z}))}{\\partial v_{i}({\\bf Z})^{2}}\\cdot\\frac{\\partial v_{i}({\\bf Z})}{\\partial Z_{t}}\\cdot\\frac{\\partial v_{i}({\\bf Z})}{\\partial Z_{s}}+{\\bf A}\\sum_{i=1}^{K_{Z}}\\frac{\\partial\\tilde{S}(v({\\bf Z}))}{\\partial v_{i}({\\bf Z})}\\cdot\\frac{\\partial^{2}v_{i}({\\bf Z})}{\\partial Z_{s}\\partial Z_{t}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "573 Eq. (25) can be written into the matrix-vector product form as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{0}=\\mathbf{A}\\Tilde{S}^{\\prime\\prime}(Z)\\mathbf{{v}}_{s,t}^{\\prime}(Z)+\\mathbf{A}\\Tilde{S}^{\\prime}(Z)\\mathbf{{v}}_{s,t}^{\\prime\\prime}(Z),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\tilde{\\pmb{S}}^{\\prime\\prime}(\\pmb{Z})=\\left[\\frac{\\partial^{2}\\tilde{S}(\\pmb{v}(\\pmb{Z}))}{\\partial v_{1}(\\pmb{Z})^{2}},\\cdots,\\frac{\\partial^{2}\\tilde{S}(\\pmb{v}(\\pmb{Z}))}{\\partial v_{K_{z}}(\\pmb{Z})^{2}}\\right]\\in\\mathbb{R}^{k\\times K_{Z}},\\quad}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\quad v_{s,t}^{\\prime}(\\pmb{Z})=\\left[\\frac{\\partial v_{1}(\\pmb{Z})}{\\partial Z_{t}}\\cdot\\frac{\\partial v_{1}(\\pmb{Z})}{\\partial Z_{s}},\\cdots,\\frac{\\partial v_{K_{z}}(\\pmb{Z})}{\\partial Z_{t}}\\cdot\\frac{\\partial v_{K_{z}}(\\pmb{Z})}{\\partial Z_{s}}\\right]^{T}\\in\\mathbb{R}^{K_{Z}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\pmb{S}}^{\\prime}(\\pmb{Z})=\\left[\\frac{\\partial\\tilde{\\pmb{S}}(\\pmb{v}(\\pmb{Z}))}{\\partial v_{1}(\\pmb{Z})},\\dots,\\frac{\\partial\\tilde{\\pmb{S}}(\\pmb{v}(\\pmb{Z}))}{\\partial v_{K_{z}}(\\pmb{Z})}\\right]\\in\\mathbb{R}^{k\\times K_{z}},}\\\\ &{\\pmb{v}_{s,t}^{\\prime\\prime}(\\pmb{Z})=\\left[\\frac{\\partial^{2}v_{1}(\\pmb{Z})}{\\partial Z_{s}\\partial Z_{t}},\\dots,\\frac{\\partial^{2}v_{K_{z}}(\\pmb{Z})}{\\partial Z_{s}\\partial Z_{t}}\\right]^{T}\\in\\mathbb{R}^{K_{z}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "574 If we denote the concatenation as $\\Tilde{S}^{\\prime\\prime\\prime}(Z)~=~\\Big[\\Tilde{S}^{\\prime\\prime}(Z),\\Tilde{S}^{\\prime}(Z)\\Big]~\\in~\\mathbb{R}^{k\\times2K_{Z}}$ and ${\\pmb v}_{s,t}^{\\prime\\prime}({\\pmb Z})\\;=\\;$   \n575 $\\left[{\\boldsymbol{v}}_{s,t}^{\\prime}(Z)^{T},{\\boldsymbol{v}}_{s,t}^{\\prime\\prime}(Z)^{T}\\right]^{T}\\in\\mathbb{R}^{2K_{z}}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{0}=\\mathbf{A}\\tilde{S}^{\\prime\\prime\\prime}(Z)v_{s,t}^{\\prime\\prime\\prime}(Z).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "576 Finally, if we denote the rows of $\\tilde{S}^{\\prime\\prime\\prime}(Z)$ that correspond to the factorized part of $\\boldsymbol{S}$ by $\\tilde{S}_{f}^{\\prime\\prime\\prime}(Z)$ ,   \n577 according to Lemma 5 of the iVAE paper [19] and the assumption that $k\\geq2K_{Z}$ , we have that the   \n578 rank of $\\tilde{S}_{f}^{\\prime\\prime\\prime}(Z)$ is $2K z$ . Since $k\\geq2K_{Z}$ , the rank of $\\tilde{S}_{f}^{\\prime\\prime\\prime}(Z)$ is also $2K z$ . Since the rank of $\\mathbf{A}$ is $k$ ,   \n579 the rank of $\\mathbf{A}\\tilde{S}^{\\prime\\prime\\prime}(Z)$ is $2K z$ , which implies that ${\\pmb v}_{s,t}^{\\prime\\prime\\prime}({\\pmb Z})\\in\\mathbb R^{2K_{Z}}$ is a zero vector. Therefore, we   \n580 have $\\mathbf{\\boldsymbol{v}}_{s,t}^{\\prime}(Z)=\\mathbf{0},\\forall s\\neq t$ , and we have demonstrated that $\\pmb{v}$ is a component-wise function.   \n581 Step II. Based on Step $\\mathbf{I}$ , we demonstrate that $\\mathbf{A}$ is a block permutation matrix. Without loss of gen  \n582 erality, we assume that the permutation in $\\pmb{v}$ is Identity, where $\\boldsymbol{v}(Z)=\\left[v_{1}\\left(Z_{1}\\right),\\cdots,v_{K_{Z}}\\left(Z_{K_{Z}}\\right)\\right]^{T}$   \n583 and each $v_{i}$ is a nonlinear univariate scalar function. Since $f$ and $\\tilde{f}$ are injective, $\\pmb{v}$ is bijective and   \n584 $\\boldsymbol{v}^{-1}(Z)=\\left[v_{1}^{-1}\\left(Z_{1}\\right),\\cdots,v_{K_{Z}}^{-1}\\left(Z_{K_{Z}}\\right)\\right]^{T}$ . If we denote $\\overline{{S}}(v(Z))=\\tilde{S}(v(Z))+\\mathbf{A}^{-1}c$ , Eq. (23)   \n585 can be reformulated as $S(Z)=\\mathbf{A}\\overline{{S}}(\\pmb{v}(\\pmb{Z}))$ . We then apply $v^{-1}$ to $Z$ on both sides, which gives ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S\\left(v^{-1}(Z)\\right)=\\mathbf{A}\\overline{{S}}(Z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "586 Let $t$ be the index of an entry in $\\boldsymbol{S}$ that corresponds to the factorized part $S_{f}$ . For all $s\\neq t$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n0=\\frac{\\partial\\pmb{S}\\left(\\pmb{v}^{-1}(\\pmb{Z})\\right)_{t}}{\\partial Z_{s}}=\\sum_{j=1}^{k}a_{t j}\\frac{\\partial\\overline{{\\pmb{S}}}(\\pmb{Z})_{j}}{\\partial Z_{s}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "587 Since the entries of $\\tilde{S}$ are linearly independent, $a_{t j}$ is zero for any $j$ such that \u2202S(Z)j \u0338= 0. This   \n588 includes the entries $S_{j}$ that correspond to (1) the factorized part that does not depend on $Z_{t}$ ; and (2)   \n589 the non-factorized part $S_{n f}$ . Therefore, when $t$ is the index of an entry in the sufficient statistics $\\boldsymbol{S}$   \n590 that corresponds to factor $i$ in the factorized part $S_{f}$ , i.e., $S_{f,i}$ , the only non-zero $a_{t j}$ are the ones that   \n591 map between $S_{f,i}\\left(Z_{i}\\right)$ and $\\overline{{S}}_{f,i}\\left(v_{i}\\left(Z_{i}\\right)\\right)$ . Therefore, we can construct an invertible submatrix ${\\bf A}_{i}^{\\prime}$   \n592 with all non-zero elements $a_{t j}$ for all $t$ that corresponds to factor $i$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{f,i}\\left(Z_{i}\\right)=\\mathbf{A}_{i}^{\\prime}\\overline{{S}}_{f,i}\\left(v_{i}\\left(Z_{i}\\right)\\right)=\\mathbf{A}_{i}^{\\prime}\\tilde{S}_{f,i}\\left(v_{i}\\left(Z_{i}\\right)\\right)+c_{i},\\quad i=1,\\cdots,K_{Z},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "593 where $c_{i}$ denotes the corresponding elements of $^c$ . Eq. (30) means that for each $i=1,\\cdots,K_{Z}$ ,   \n594 the matrix block $\\mathbf{A}_{i}^{\\prime}$ of $\\mathbf{A}$ affinely transforms the $i$ -specific sufficient statistics vector $S_{f,i}\\left(Z_{i}\\right)$ into   \n595 $\\tilde{S}_{f,i}\\left(v_{i}\\left(Z_{i}\\right)\\right)$ . In addition, there is also an additional block $\\mathbf{A}^{\\prime}$ that affinely transforms $S_{n f}(Z)$ in   \n596 into $S_{n f}(v(Z))$ . This completes the proof that $\\mathbf{A}$ is a block permutation matrix. ", "page_idx": 16}, {"type": "text", "text": "597 C.4.3 PART III ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "598 Let $\\tilde{Z}_{i}=v_{i}\\left(Z_{i}\\right)=\\tilde{f}^{\\dagger}({\\pmb X})_{i}$ be the $i$ th inferred latent variable. Assume again that the permutation in   \n599 $\\pmb{v}$ is Identity. In this part, we prove that if Assumption 2 holds, each inferred latent variable ${\\tilde{Z}}_{i}$ is the   \n600 bijective transformation of the true latent variable. The proof is as follows. ", "page_idx": 17}, {"type": "text", "text": "601 Proof. Plugging ${\\tilde{Z}}_{i}$ into Eq. (30), we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{f,i}(Z_{i})=\\mathbf{A}_{i}^{\\prime}\\bar{S}_{f,i}(\\tilde{Z}_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "602 According to Assumption 2, there exists one dimension of $S_{f,i}$ , i.e., $j$ , such that $S_{f,i j}$ is bijective.   \n603 This implies that $S_{f,i}$ is injective, and therefore it has a left-inverse $S_{f,i}^{\\dagger}$ . we apply $S_{f,i}^{\\dagger}$ to both sides   \n604 of Eq. (31), which gives: ", "page_idx": 17}, {"type": "equation", "text": "$$\nZ_{i}=S_{f,i}^{\\dagger}{\\bf A}_{i}^{\\prime}\\bar{S}_{f,i}(\\tilde{Z}_{i}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "605 Since $\\mathbf{A}_{i}^{\\prime}$ is a block of an invertible block permutation matrix, $\\mathbf{A}_{i}$ is also an invertible matrix, and   \n606 therefore $\\mathbf{A}_{i}^{\\prime}$ is a bijective mapping. In addition, since $\\tilde{S}_{f,i}$ is injective, $\\bar{S}_{f,i}$ is also injective, and   \n607 therefore the composite map $\\begin{array}{r}{S_{f,i}^{\\dagger}\\mathbf{A}_{i}^{\\prime}\\bar{S}_{f,i}:\\mathbb{R}\\rightarrow\\mathbb{R}}\\end{array}$ that applies on ${\\tilde{Z}}_{i}$ is a bijective. This completes   \n608 the proof that each inferred latent variable ${\\tilde{Z}}_{i}$ is the bijective transformation of the true latent variable   \n609 in the case of no noise, where $Z=f^{\\dagger}(X)$ are the true latent variables. If noise $\\varepsilon$ exists, the posterior   \n610 distribution of the latent variables can be identified up to an analogous bijective indeterminacy. ", "page_idx": 17}, {"type": "text", "text": "611 C.4.4 Consistency ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "612 Proof. If the family of the variational posterior $q_{\\phi}(Z|X,Y,T)$ contains the true posterior   \n613 $p_{\\theta}(Z|X,Y,T)$ , then by optimizing the loss of Eq. (9) (with the KL term replaced by the score match  \n614 ing loss defined in Eq. (10)) over its parameter $\\phi$ , the score matching term will eventually vanish.   \n615 Therefore, the ELBO term in Eq. (9) will be equal to the log-likelihood. Under this circumstance,   \n616 CiVAE inherits all the properties of maximum likelihood estimation (MLE). Since the identifiability   \n617 of CiVAE is guaranteed up to permutation and component-wise bijective transformation of the latent   \n618 variables, the consistency property of MLE means that the model will converge to the true parameter   \n619 $\\theta^{*}$ up to such mild indeterminacy of the latent variables in the limit of infinite data. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "620 C.5 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "621 Proof. Let $_{C}$ be the true latent confounders and $\\tilde{C}$ be the transformed confounders, where the   \n622 transformation function $f$ is bijective and differentiable a.e. Let $f^{-1}$ denote its inverse. The ATE   \n623 estimator that controls transformed confounders $\\tilde{C}$ can be formulated as: ", "page_idx": 17}, {"type": "equation", "text": "$$\nD E V(\\tilde{C})=\\mathbb{E}_{p(\\tilde{C})}[\\mathbb{E}[Y|T=1,\\tilde{C}=\\tilde{\\pmb{c}}]-\\mathbb{E}[Y|T=0,\\tilde{C}=\\tilde{\\pmb{c}}]].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "624 Specifically, for the continuous case where density functions exist, for each term, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p(\\tilde{C})}[\\mathbb{E}[Y|T=t,\\tilde{C}=\\tilde{c}]]=\\int f_{\\tilde{C}}(\\tilde{c})\\int y\\cdot f_{Y|T,\\tilde{C}}(y|t,\\tilde{c})d y d\\tilde{c}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "625 For the marginal density $f_{\\tilde{C}}(\\tilde{\\boldsymbol{c}})$ , the following equality holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{\\tilde{C}}(\\tilde{c})=f_{C}(f^{-1}(\\tilde{c}))|J_{f^{-1}}(\\tilde{c})|=f_{C}(c)|J_{f^{-1}}(\\tilde{c})|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "626 As for the conditional density $f_{Y|T,\\tilde{C}}(y|t,\\tilde{{\\boldsymbol{c}}})$ , since $f$ is bijective, according to Eq. (12), we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{Y|T,\\tilde{C}}(y|t,\\tilde{c})=f_{Y|T,C}(y|t,c).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "627 Combining Eqs. (35) and (36), and given that $d\\tilde{\\pmb{c}}=|J_{f}(\\pmb{c})|d\\pmb{c}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(34)=\\displaystyle\\int f_{C}(c)|\\mathbf{J}_{f^{-1}}(\\tilde{c})|\\int y\\cdot f_{Y|T,C}(y|t,c)d y|\\mathbf{J}_{f}(c)|d c}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=|\\mathbf{J}_{f^{-1}}(\\tilde{c})|\\cdot|\\mathbf{J}_{f}(c)|\\displaystyle\\int f_{C}(c)\\displaystyle\\int y\\cdot f_{Y|T,C}(y|t,c)d y d c}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 17}, {"type": "table", "img_path": "4FwlejUlg5/tmp/f2b303019a8300e4010dd314ac9b7ddcb90779a267d02a3c675f7184b999cf10.jpg", "table_caption": ["Table 2: Comparison of CiVAE with baselines when intra-interactions among $_M$ exist. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "4FwlejUlg5/tmp/fa6b8e229320750cf35d0fc09da931fb2e278d7290c636a36591ae472dd165df.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "628 where the term $|J_{f-1}(\\tilde{c})|\\cdot|J_{f}(c)|$ vanishes in step (a) as the two factors have the product of one.   \n629 Therefore, if we plug Eq. (37) into Eq. (33), it leads to the following equality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D E V(\\tilde{C})=\\mathbb{E}_{p(\\tilde{C})}[\\mathbb{E}[Y|T=1,\\tilde{C}=\\tilde{c}]-\\mathbb{E}[Y|T=0,\\tilde{C}=\\tilde{c}]]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{p(C)}[\\mathbb{E}[Y|T=1,C=c]-\\mathbb{E}[Y|T=0,C=c]]=D E V(C)=A T E,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "630 where the last step is due to Eq. (2) in Definition 2, which completes our proof that controlling   \n631 bijectively transformed confounders provides an unbiased estimation of ATE. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "632 D Extending CiVAE to address Latent Interactions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "633 In this section, we extend CiVAE to more general cases where interactions exist among the latent   \n634 confounders $_{C}$ and the latent post-treatment variables $_M$ . Here, we note that the identification   \n635 of latent confounders $_{C}$ in CiVAE is achieved in two steps. (i) CiVAE individually identifies   \n636 latent variables $[C,M]$ that generate $\\mathbf{\\deltaX}$ in inferred $Z$ (but which dims of $_{z}$ correspond to $_{C}$   \n637 or $_M$ is unknown). $(\\romannumeral2)$ pairwise independence test to identify $_{C}$ . Since Assumption 2 allows   \n638 arbitrary dependence among $_{C}$ and $_M$ , step $(i)$ still holds when interactions among $[C,M]$ exist.   \n639 To distinguish $_{C}$ in these cases, we can use more general causal discovery algorithms, e.g., the   \n640 PC algorithm [18] in the second step. In this section, we consider two cases of interaction: $(i)$   \n641 Intra-Interaction among mediators, and $(i i)$ Inter-Interaction among mediators and confounders. ", "page_idx": 18}, {"type": "text", "text": "642 D.1 Intra-Interactions among Latent Mediators ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "643 In this subsection, we discuss the case where latent post-treatment variables $_M$ interact with each   \n644 other. Since in this case, $_M$ cannot causally influence the latent confounders $_{C}$ (otherwise $_{C}$ will be   \n645 post-treatment), and the PC algorithm orients edges in causal graphs via colliders, latent confounders   \n646 can still be identified from the inferred $_{z}$ as they form colliders with the treatment $T$ .   \n647 To empirically verify the claim, we extend the simulated datasets described in Section 5.1, where we   \n648 make (i) $T$ directly affects $M_{1}$ , (ii) $M_{1}$ affects $M_{2}$ , and (iii) $M_{1}$ , $M_{2}$ affect $M_{3}$ . The coefficients are   \n649 randomly sampled from $\\mathcal{N}(0,1/3)$ . In step $(i i)$ , we use the PC algorithm [18] to identify $_{C}$ from   \n650 the inferred $_{z}$ . The results in Table 2 demonstrate that the adapted CiVAE is still significantly more   \n651 robust to latent post-treatment bias compared to CEVAE and TEDVAE, which empirically verify our   \n652 claim that PC-adapted CiVAE can address the interaction among post-treatment variables. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "653 D.2 Inter-Interactions between Latent Mediators and Latent Confounders ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "654 In this subsection, we discuss another case where inter-interactions exist between latent confounders   \n655 $_{C}$ and latent post-treatment variables $_M$ . Since in this case, $_M$ still cannot causally influence $_{C}$   \n656 (otherwise $_{C}$ will be post-treatment), and the PC algorithm orients edges in causal graph via colliders,   \n657 latent confounders $_{C}$ can still be identified from $_{z}$ as they form colliders with the treatment $T$ .   \n658 To verify the claim, we extend the simulated datasets described in Section 5.1 to allow each latent   \n659 confounder $C_{i}\\in\\mathbb{R}^{3}$ to determine $M\\in\\mathbb{R}^{3}$ . The coefficients are randomly sampled from $\\mathcal{N}(0,1/3)$ .   \n660 In step $(i i)$ , we use the PC algorithm to identify $_{C}$ from the inferred $Z$ . The results in Table 3   \n661 demonstrate that the PC-adapted CiVAE is still significantly more robust to latent post-treatment bias   \n662 compared to CEVAE and TEDVAE, which empirically verify our claim that PC-adapted CiVAE can   \n663 address the case where inter-interactions exist among latent confounders and post-treatment variables. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "664 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "665 1. Claims   \n666 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n667 paper\u2019s contributions and scope?   \n668 Answer: [Yes]   \n669 Justification: The contribution of this paper can be summarized as: We study a critical but   \n670 easily overlooked problem in causal effect estimation: latent post-treatment bias, and we   \n671 propose a novel framework, i.e., CiVAE, to address the bias. The details are in Section 4.   \n672 2. Limitations   \n673 Question: Does the paper discuss the limitations of the work performed by the authors?   \n674 Answer: [Yes]   \n675 Justification: We have discussed the potential issue of the vanilla when interactions among   \n676 the latent variables exists. However, in Section D we have addressed the issue by extendeding   \n677 our framework.   \n678 3. Theory Assumptions and Proofs   \n679 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n680 a complete (and correct) proof?   \n681 Answer: [Yes]   \n682 Justification: We have introduced the three mild assumptions required for the identification   \n683 of causal effects under latent post-treatment bias. In addition, we have provided the proof   \n684 for all the theorems in the Appendix.   \n685 4. Experimental Result Reproducibility   \n686 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n687 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n688 of the paper (regardless of whether the code and data are provided or not)?   \n689 Answer: [Yes]   \n690 Justification: We have provided implementation details in Section 5.1. In addition, we have   \n691 provided a code demo in an anonymous URL.   \n692 5. Open access to data and code   \n693 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n694 tions to faithfully reproduce the main experimental results, as described in supplemental   \n695 material?   \n696 Answer: [Yes]   \n697 Justification: See Checklist 4.   \n698 6. Experimental Setting/Details   \n699 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n700 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n701 results?   \n702 Answer: [Yes]   \n703 Justification: See Checklist 4.   \n704 7. Experiment Statistical Significance   \n705 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n706 information about the statistical significance of the experiments?   \n707 Answer: [Yes]   \n708 Justification: We have reported the error of five independent run for both the proposed   \n709 CiVAE and all the baselines in the main paper.   \n710 8. Experiments Compute Resources   \n711 Question: For each experiment, does the paper provide sufficient information on the com  \n712 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n713 the experiments?   \n714 Answer: [Yes]   \n715 Justification: See Checklist 4.   \n716 9. Code Of Ethics   \n717 Question: Does the research conducted in the paper conform, in every respect, with the   \n718 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n719 Answer: [Yes]   \n720 Justification: We have carefully read the code of ethics and behaved strictly according to it.   \n721 10. Broader Impacts   \n722 Question: Does the paper discuss both potential positive societal impacts and negative   \n723 societal impacts of the work performed?   \n724 Answer: [Yes]   \n725 Justification: See Section A of the Appendix.   \n726 11. Safeguards   \n727 Question: Does the paper describe safeguards that have been put in place for responsible   \n728 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n729 image generators, or scraped datasets)?   \n730 Answer: [NA]   \n731 Justification: Our model does not have a high risk for misuse.   \n732 12. Licenses for existing assets   \n733 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n734 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n735 properly respected?   \n736 Answer: [Yes]   \n737 Justification: We have cited the papers of our baselines and honor their license of code.   \n738 13. New Assets   \n739 Question: Are new assets introduced in the paper well documented and is the documentation   \n740 provided alongside the assets?   \n741 Answer: [Yes]   \n742 Justification: We provide the Readme file along side the codes.   \n743 14. Crowdsourcing and Research with Human Subjects   \n744 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n745 include the full text of instructions given to participants and screenshots, if applicable, as   \n746 well as details about compensation (if any)?   \n747 Answer: [NA]   \n748 Justification: No human subjects are involved in our experiments.   \n749 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n750 Subjects   \n751 Question: Does the paper describe potential risks incurred by study participants, whether   \n752 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n753 approvals (or an equivalent approval/review based on the requirements of your country or   \n754 institution) were obtained?   \n755 Answer: [NA]   \n756 Justification: No human subjects are involved in our experiments. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}]