[{"type": "text", "text": "Adaptive Experimentation When You Can't Experiment ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yao Zhao Kwang-Sung Jun Tanner Fiez University of Arizona University of Arizona Amazon yaoz@arizona.edu kjun@cs.arizona.edu fieztann@amazon.com ", "page_idx": 0}, {"type": "text", "text": "Lalit Jain University of Washington lalitj@uw.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper introduces the confounded pure exploration transductive linear bandit (CPET-LB) problem. As a motivating example, often online services cannot directly assign users to specific control or treatment experiences either for business or practical reasons. In these settings, naively comparing treatment and control groups that may result from self-selection can lead to biased estimates of underlying treatment effects. Instead, online services can employ a properly randomized encouragement that incentivizes users toward a specific treatment. Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such encouragement designs. We consider a more general underlying model captured by a linear structural equation and formulate pure exploration linear bandits in this setting. Though pure exploration has been extensively studied in standard adaptive experimental design settings, we believe this is the first work considering a setting where noise is confounded. Elimination-style algorithms using experimental design methods in combination with a novel finitetime confidence interval on an instrumental variable style estimator are presented with sample complexity upper bounds nearly matching a minimax lower bound. Finally, experiments are conducted that demonstrate the efficacy of our approach. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this study, we present a methodology for adaptive experimentation in scenarios characterized by potential confounding. Online services routinely conduct thousands of A/B tests annually [23]. In most online A/B/N experimentation, meticulous user-level randomization is essential to ensure unbiased estimates of treatment effects at the population level, commonly known as average treatment effects (ATE). In this setting, firms are often interested in understanding the treatment with the highest average outcome if presented to all members of the population. However, in many settings firms may not be able to randomize, for example if a feature must be rolled out to all users for various business reasons [30]. In such instances, users may choose to engage with a feature or not based on potentially unobservable preferences. Thus the resulting measured outcome may be correlated with the decision to engage in a specific feature. I.e. the underlying characteristics of the user confound the relationship between the decision to use the feature being evaluated and the effect of the feature. Thus, naively comparing the average outcome for users who engage with a feature with those who do not suffers from a (selection) bias. This setting is captured in Figure 1. ", "page_idx": 0}, {"type": "text", "text": "A potential solution is for services to employ encouragement designs where users are presented with incentives that encourage users to engage with a specific feature [4, 7, 12]. As a concrete example, many online services have introduced membership levels with different offerings and $38\\mathrm{th}$ Conference on Neural Information Processing Systems (NeurIPS 2024). ", "page_idx": 0}, {"type": "text", "text": "prices available to all users. Given a set of membership level options, the service is interested in knowing the counterfactual of which level has the optimal outcome (e.g., total revenue) if every user chooses to join that membership level. In this setting, encouragements could be coupons or trials for corresponding membership levels. In these settings, the firm can use intent-to-treat (ITT) estimates for the treatment effect which naively compare the average outcomes between the groups given different encouragements. In practice, given an encouragement a user may not engage with the corresponding feature choosing either the control or a different feature. Hence, the resulting ITT estimate may be a diluted estimate of the ATE [3]. However, all is not lost: if the encouragement presented to a user is properly randomized, and the service guarantees that the encouragement only affects the outcome through the choice of user treatment, then the encouragement acts as an instrumental variable. Standard analysis from the econometrics and compliance literature show that two-stage least squares (2SLs) estimators can then be used to provide consistent estimates of treatment effects. ", "page_idx": 1}, {"type": "text", "text": "At the same time, firms are also increasingly utilizing adaptive experimentation techniques, often known as pure exploration multi-armed bandits (MAB) algorithms [26, 15], to accelerate traditional A/B/N testing. Pure exploration MAB techniques promise to deliver accurate inferences in a fraction of the time and cost as traditional methods. Similar to A/B testing, bandit methods assume users are properly randomized and can fail to learn the optimal treatment if naively used and may be sample inefficient if they fail to take the confounded structure into account. ", "page_idx": 1}, {"type": "image", "img_path": "2mqiTiJKrx/tmp/de2f946193b0412ee561ac702bc0b420f27630cd52e1aac859750f3ac050debc.jpg", "img_caption": ["Figure 1: Causal graph of the model. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we provide a methodology   \nfor experimenters seeking to use adaptive experimentation in settings with confounding where encouragements are available. We formulate this work in the more general and novel setting of confounded pure exploration transductive linear bandits (CPET-LB) (Section 1.1). We present algorithms using experimental design for the CPET-LB problem and analyze the resulting sample complexity. As we demonstrate, even in the simple multi-armed bandit setting described above, computing an effective sampling pattern requires using the machinery of linear bandits. Without knowledge of the underlying structural model, existing linear bandit approaches could lead to suboptimal sampling. The main technical challenge we face is simultaneously improving our estimate of the structural model while designing with inaccurate estimates (Section 3). This approach crucially relies on our development of novel finite-time confidence bounds for two-stage least squares (2SLS) style estimators that may be of independent interest (Section 2.2). Moreover, we provide worst-case sample complexity lower bounds that are nearly matched by our sample complexity upper bounds (Appendix D). Though the goal of this work is primarily theoretical, we empirically show the efficacy of our method over existing solutions (Appendix E). ", "page_idx": 1}, {"type": "text", "text": "1.1  General Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A confounded pure exploration transductive linear bandits (CPET-LB) instance consists of finite collections of measurement vectors $\\mathcal{Z}\\subset\\mathbb{R}^{d}$ and evaluation vectors $\\mathcal{W}\\subset\\mathbb{R}^{d}$ . At each time $t\\in\\mathbb{N}$ the learner selects $z_{t}\\in\\mathcal{Z}$ and observes a pair of noisy responses $x_{t}\\in\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $y_{t}\\in\\mathbb{R}$ generated via the structural equation model ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{t}=\\Gamma^{\\top}z_{t}+\\eta_{t},\\quad y_{t}=x_{t}^{\\top}\\theta+\\varepsilon_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\Gamma\\in\\mathbb{R}^{d\\times d}$ and $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ are model parameters.1 We define the history $\\mathcal{H}_{t-1}=\\{(z_{s},x_{s},y_{s})\\}_{s<t}$ and $\\mathbb{E}_{t-1}[\\cdot]=\\mathbb{E}[\\cdot|\\mathscr{H}_{t-1}]$ denoting the conditional expectation under the filtration generated by $\\mathcal{H}_{t-1}$ The noise $\\{\\eta_{t}\\}_{t=1}^{\\infty}$ and $\\{\\varepsilon_{t}\\}_{t=1}^{\\infty}$ satisfy the following set of assumptions unless otherwise noted. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1. We assume $\\varepsilon_{t}\\mid\\mathcal{H}_{t-1}$ is 1-sub-Gaussian (and thus $\\mathbb{E}[\\varepsilon_{t}\\mid\\mathcal{H}_{t-1}]=0)$ . Furthermore, $\\eta_{t}\\mid\\mathcal{H}_{t-1}\\;\\;$ .s $\\sigma_{\\eta}^{2}$ -sub-Gaussian vectors (and thus $\\mathbb{E}[\\eta_{t}\\ |\\ \\mathcal{H}_{t-1}]=0)$ , i.e., ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\forall\\beta\\in\\mathbb{R},\\operatorname*{max}_{a:\\|a\\|_{2}\\leq1}\\mathbb{E}[\\exp\\bigl(\\beta\\langle a,\\eta_{t}\\rangle\\bigr)]\\leq\\exp\\left(\\frac{\\beta^{2}\\sigma_{\\eta}^{2}}{2}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Goal. The objective is to identify $w^{*}:=\\arg\\operatorname*{max}_{w\\in\\mathcal{W}}w^{\\top}\\theta$ with probability at least $1-\\delta$ for $\\delta\\in(0,1)$ while taking a minimum number of measurements. ", "page_idx": 2}, {"type": "text", "text": "In the setting where $\\Gamma=I,\\eta=0$ and $\\mathbb{E}_{t-1}[\\varepsilon_{t}|x_{t}]=0$ , our setting reduces to the standard pure exploration transductive linear bandit problem [33, 15]. In general, the joint noise process $[\\eta_{t},\\varepsilon_{t}]$ may be dependent across the entries. In particular, we are allowing for the data generating process to be endogenous, meaning that $\\mathbb{E}_{t-1}[\\varepsilon_{t}|x_{t}]\\neq0$ . That is, $\\varepsilon_{t}$ can affect not just $y_{t}$ , but also $x_{t}$ given a choice of $z_{t}$ . The presence of endogeneity is a key challenge in the CPET-LB problem. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2 (Exclusion Restriction). We assume that $\\mathbb{E}_{t-1}[z_{t}\\varepsilon_{t}]=0$ , or alternatively that $z_{t}$ is uncorrelated with $\\varepsilon_{t}$ ", "page_idx": 2}, {"type": "text", "text": "The variable $z_{t}$ is commonly referred to as an instrumental variable [3]. We consider algorithms for the CPET-LB problem that stop at a $\\mathcal{H}_{t}$ -measurable time $\\tau\\in\\mathbb{N}$ , and produce a recommendation $\\widehat{w}\\in\\mathcal{W}$ . The goal is $\\delta$ -PAC algorithms with effcient sample complexity guarantees. ", "page_idx": 2}, {"type": "text", "text": "Definition 1.1 $\\boldsymbol{\\delta}$ -PAC). We say an algorithm is $\\delta$ -PAC for a CPET-LB problem with $\\mathcal{W},\\mathcal{Z}\\subset\\mathbb{R}^{d}$ if for all $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ and $\\Gamma\\in\\mathbb{R}^{d\\times d}$ , it holds that $\\mathbb{P}_{\\theta,\\Gamma}(\\widehat{w}\\neq w^{*})\\leq\\delta$ for $\\delta\\in(0,1)$ ", "page_idx": 2}, {"type": "text", "text": "1.2  Encouragement Designs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The CPET-LB feedback model generalizes the classical compliance setting. ", "page_idx": 2}, {"type": "text", "text": "Compliance as a Special Case. In compliance problems, a decision-maker has access to a set of treatment that can be offered to users, while the users themselves have the option to accept the treatment they are presented or instead opt-in to a different treatment. The goal is to identify the treatment with the optimal average outcome if all users were to accept it. Specifically, given a finite set $\\mathcal{A}=\\{1,2,\\ldots,\\bar{d}\\}$ , a decision-maker presents user $t\\in\\mathbb{N}$ with an encouragement for a treatment $i\\in\\mathcal{A}$ , the user then selects treatment $j\\in\\mathcal A$ where potentially $j\\neq i$ , and an outcome $y_{t}$ results. To capture compliance with the CPET-LB framework, we set $\\overrightharpoon{\\mathcal{Z}}=\\mathcal{X}=\\mathcal{W}=\\{e_{1},\\cdots,e_{d}\\}$ and the parameter $\\Gamma$ captures the probability of accepting a treatment given an encouragement for a potentially different treatment. Specifically, $\\Gamma(i,\\bar{j})=\\stackrel{\\bullet}{\\mathbb{P}}\\!\\left(x_{t}=e_{i}\\mid\\check{z_{t}}=e_{j}\\right)$ , and a straightforward computation shows that $x_{t}=\\Gamma^{\\top}z_{t}+\\eta_{t}$ where $\\mathbb{E}[\\eta_{t}|\\boldsymbol{z}_{t}]=0$ with ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\eta_{t}=\\boldsymbol{x}_{t}-\\left[\\mathbb{P}\\big(\\boldsymbol{e}_{1}\\mid\\boldsymbol{z}_{t}\\big),\\cdot\\cdot\\cdot\\mathbf{\\mu},\\mathbb{P}\\big(\\boldsymbol{e}_{d}\\mid\\boldsymbol{z}_{t}\\big)\\right]^{\\intercal}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Moreover, $y_{t}=x_{t}^{\\top}\\theta+\\varepsilon_{t}$ gives the resulting reward, which is clearly correlated with the user choice so that $\\mathbf{cov}(\\eta_{t},\\varepsilon_{t})\\neq0$ Finally, $e_{i}^{\\top}\\theta=\\theta_{i}$ gives the expected value of treatment $i$ over the population and our goal is to identify $\\boldsymbol{w}^{*}=\\operatorname*{irg\\,max}_{e_{i}\\in\\mathcal{W}}\\boldsymbol{e}_{i}^{\\top}\\boldsymbol{\\theta}$ . 2 Note that when $\\Gamma=I$ we automatically have that $\\eta_{t}=0$ and there is no confounding. This reduces to the standard MAB seting. ", "page_idx": 2}, {"type": "text", "text": "Motivating Compliance Example. As a motivating compliance example representing the membership level discussion from the introduction, consider a location model that assumes each user $t\\in\\mathbb{N}$ arriving online has an underlying unobserved one-dimensional preference $u_{t}\\sim\\mathcal N(0,\\sigma_{u}^{2})$ . If an algorithm presents the user with encouragement $z_{I_{t}}=e_{I_{t}}$ for $I_{t}\\in\\mathcal A$ , then the user selects into the membership level given by $\\begin{array}{r}{J_{t}=\\operatorname*{min}_{j\\in\\mathcal{A}}|I_{t}+u_{t}-j|}\\end{array}$ so that $x_{t}=e_{J_{t}}$ . This process captures a user being more likely to opt-in to membership levels that are closer to the encouragement that they were presented. The outcome is then given by $y_{t}=x_{t}^{\\top}\\theta+u_{t}$ ", "page_idx": 2}, {"type": "text", "text": "We conduct an experiment with this problem instance (see Fig. 2 and Appendix B for more details). Specifically, $d=6$ $\\theta=\\left[1\\begin{array}{l l l l l l}{\\phantom{-}}&{\\!-0.95}&{0}&{0.45}&{0.95}&{0.99\\right]\\!,}\\end{array}$ and $\\sigma_{u}^{2}=0.35$ . Observe that $w^{*}=$ $e_{1}=\\arg\\operatorname*{max}_{w\\in\\mathcal{W}}w^{\\top}\\theta$ . An upper confidence bound (UCB) selection strategy is simulated that maintains estimates of the mean reward of each encouragement $i\\in\\mathcal{A}$ namely $\\begin{array}{r}{\\widehat{\\mu}_{i,t}=\\sum_{s=1}^{t}\\mathbf{1}\\{z_{t}=}\\end{array}$ $e_{i}\\}y_{t}$ , and then pulls the one with the highest UCB. The UCB selection strategy is combined with a pair of recommendation strategies. The UCB-OLS algorithm estimates the mean reward of each treatment using an OLS estimator, namely $\\begin{array}{r}{\\widehat{\\theta}_{\\mathrm{LS}}^{i,t}\\,=\\,\\sum_{s=1}^{t^{'}}\\mathbf{1}\\big\\{x_{t}\\,=\\,e_{i}\\big\\}y_{t}\\big/\\sum_{s=1}^{t}\\mathbf{1}\\big\\{x_{t}\\,=\\,e_{i}\\big\\}}\\end{array}$ , and recommends arg $\\operatorname*{max}_{a\\in A}\\widehat{\\theta}_{\\mathrm{LS}}^{i,t}$ . Moreover, the UCB-IV algorithm uses an instrumental variableestimator (see the next section) that incorporates knowledge of $\\Gamma$ similar to 2SLS to deconfound estimates of the mean rewards of each treatment and recommends the treatment with the maximum estimate. The results over 100 simulations are shown in Fig. 2. UCB-OLS completely fails to identify $\\theta_{1}\\,=\\,\\arg\\operatorname*{max}_{i\\in d}\\theta_{i}$ due to a biased estimate, whereas UCB-IV does better. However, UCB-IV methods seem to have a constant probability of error. To see why, note that the expected reward from pulling $z=e_{i}$ is $e_{i}^{\\top}\\Gamma\\theta$ . These values are plotted in orange in Figure 2a. In particular, with some constant probability, UCB zeroes in on arm 6 becauses of the mean estimates on the $z$ 's, and as a result fails to give enough samples to learn that arm 1 is indeed the best. In contrast, our proposed method CPEG, Algorithm 1 manages to find the best arm with significantly higher probability. ", "page_idx": 2}, {"type": "image", "img_path": "2mqiTiJKrx/tmp/fa79ae408d30cedc632dde07479c0065bb7cbf13829b9755ac28a81883a38e29.jpg", "img_caption": ["Figure 2: (a) A bar chart showing $\\mathbb{E}[y|x~=~w]~=~w^{\\top}\\theta$ and $\\mathbb{E}[y|z\\;=\\;w]\\;=\\;z^{\\top}\\Gamma\\theta$ for all $w\\ \\in\\ \\mathcal{W}$ This chart shows that the optimal evaluation vector is $\\begin{array}{r}{w^{*}\\ =\\ e_{1}\\ =\\ \\arg\\operatorname*{max}_{w\\in\\mathcal{W}}\\mathbb{E}[y|x\\ =\\ w]}\\end{array}$ : while $e_{6}\\;=\\;\\arg\\operatorname*{max}_{w\\in\\mathcal{W}}\\mathbb{E}[y|z\\;=\\;w]$ and consequently estimation based on this quantity is problematic. (b) The probability of identifying $\\boldsymbol{w}^{*}=\\boldsymbol{e}_{1}$ for a collection of algorithms on the CPET-LB instance from Section 1.2. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Notation. Let $\\Delta(\\mathcal{Z})\\,=\\,\\{\\lambda\\,\\in\\,\\mathbb{R}^{|\\mathcal{Z}|}\\,:\\,\\lambda\\,\\geq\\,0,\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\,=\\,1\\}$ denote the set of probability distributions over the set $\\mathcal{Z}$ .Given a distribution $\\lambda\\ \\in\\ \\Delta(\\mathcal{Z})$ and matrix $\\Gamma\\ \\in\\ \\mathbb{R}^{d\\times d}$ , define the operator $\\begin{array}{r}{A(\\lambda,\\Gamma):=\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma}\\end{array}$ Given $Z\\,\\in\\,\\mathbb{R}^{T\\times d}$ and $\\Gamma\\,\\in\\,\\mathbb{R}^{d\\times d}$ , define the operator $\\begin{array}{r}{\\bar{A}(Z,\\Gamma)\\;:=\\;\\sum_{t=1}^{T}\\Gamma^{\\top}z_{t}z_{t}^{\\top}\\Gamma\\;=\\;\\Gamma^{\\top}Z^{\\top}Z\\Gamma}\\end{array}$ where $z_{t}\\ \\in\\ \\mathbb{R}^{d}$ denotes row $t$ of $Z$ .Given a vector $x\\in\\mathbb{R}^{d}$ and a symmetric positive-definite matrix $A\\in\\mathbb{R}^{d\\times d}$ we let $\\|x\\|_{A}^{2}=x^{\\top}A x$ . We adopt the standard notation that $(a\\vee b)\\equiv\\operatorname*{max}\\{a,b\\}$ and $(a\\wedge b)\\equiv\\operatorname*{min}\\{a,b\\}$ for $a,b\\in\\mathbb{R}$ $\\sigma_{\\operatorname*{min}}(A),\\sigma_{\\operatorname*{max}}(A)$ denote the minimum and maximum singular value of a matrix $A$ We denote by polylo $\\mathbf{g}(x_{1},\\ldots,x_{n})$ any polylogarithmic factors of $x_{1},\\ldots,x_{n}$ ", "page_idx": 3}, {"type": "text", "text": "1.3 Related Works ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our work is at the intersection of several parallel tracks of literature, pure exploration linear bandits, causal bandits, and econometrics. The most relevant work on pure exploration in linear bandits is the RAGE algorithm of [15, 33]. RAGE is nearly instance optimal for linear bandits in the non-confounded setting. Extensions of RAGE to various noise models including logistic and heteroskedastic noise have been considered [35, 20]. Other algorithms for pure exploration linear bandits have been proposed - and we leave it for future work to extend the ideas of this paper to those settings [27, 10]. ", "page_idx": 3}, {"type": "text", "text": "Confounding in bandits was first considered in the regret minimization setting by [5]. They introduces the Multi-armed bandit with unobserved confounders (MABUC) problem. They empirically demonstrate traditional bandit algorithms can have linear regret in this setting and provide an algorithm that effectively employs observed intuition. The early work of [21] also assumes there is an additional unobserved latent class at each time that determines confounding in a compliance setting. They provide novel notions of regret, relative to the instrument with the highest reward (arg max $\\breve{Z}^{\\top}\\Gamma^{\\top}\\breve{\\theta}$ in our notation), the highest treatment (arg $\\operatorname*{max}_{w}w^{\\top}\\theta)$ , regret relative to the best latent class at each time, and regret on the set of \u201ccompliers\". They discuss the suitability of these various notions of regret, and discuss when sublinear regret is possible. We remark that their approach is similar to ours in the sense that they assume a form of homogeneous effects across the population, and use an estimate of $\\Gamma$ . Recently [24] also consider the problem of compliance, however they don't take explicit non-confounding into account and assume an explicit parametric model that determines the non-compliance. This is analogous to the Heckman selection model considered in econometrics [18]. ", "page_idx": 3}, {"type": "text", "text": "The recent works of [11, 36, 17] considered an online setting where at each time they observe a set $\\{(x_{t},z_{t})\\}$ where $x_{t}$ is the action of interest and $z_{t}$ is an associated instrument. If action $I_{t}$ is selected, the reward observed is $y_{t}=x_{I_{t}}^{\\top}\\theta_{t}+\\varepsilon_{t}$ \uff0cwhere $x_{t}$ may be endogeneous. Similar to the standard linear bandit setting [1, 26], the goal is to minimize regret relative to the best action at each time. We remark that this setting is very different from ours. Effectively, we are choosing which instrument to select at each time to learn the best-performing treatment - in particular we can't choose a particular intervention. In their setting, they are choosing an intervention at each time and using the instrument purely for de-confounding the result. Experimental design for instruments to have more effective estimation has been considered by [8]. ", "page_idx": 4}, {"type": "text", "text": "In the causal bandit problem, an underlying causal graph between a set of interventions and a reward value is assumed. Actions correspond to intervening (i.e. a \u201cdo\u201d operation [31]) at one or more specific nodes in the causal graph and then observing the corresponding value at the reward node. Causal bandits have been studied extensively in the regret setting [25, 28, 6] and the pure exploration setting [32]. Though past works have allowed for unobserved confounders in the graph e.g. [29], their goal is to learn the best performing intervention, which in our setting would be arg $\\operatorname*{max}_{z\\in{\\mathcal{Z}}}z^{\\top}\\Gamma\\theta$ insteadof $w^{*}$ ", "page_idx": 4}, {"type": "text", "text": "Encouragement designs have been considered in many applications in online and offline settings. One of the earliest works on encouragement designs is [7], which considers the problem of using encouragements to determine the impact of coupons at a grocery store. More recent applications include [4, 30, 13] all in the context of online services and treatments that are required to be served to all users. Most of these works consider a small number of treatments and a heterogeneous treatment effect - hence are interested in LATE estimator. As far as we are aware, we are the only work that considers adaptive encouragement design in the context of the model given in Equation 1 and for multiple treatments. ", "page_idx": 4}, {"type": "text", "text": "2 Estimators and Inference ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now present estimators for the unknown parameter $\\theta$ and prove the associated statistical properties.   \nThe estimators discussed in this section are critical to our algorithmic solution outlined in Section 3. ", "page_idx": 4}, {"type": "text", "text": "2.1 Estimators ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Before describing our solution concept, we quickly review potential options for estimating $\\theta$ basedon adataset $Z_{T}=\\check{[z_{1},\\cdots,z_{T}]}^{\\top}\\in\\mathbb{R}^{T\\times d},\\boldsymbol{X}_{T}^{\\ast}=[\\check{x_{1}},\\cdots,\\check{x_{T}}]^{\\top}\\in\\mathbb{R}^{T\\times d},Y_{T}=[y_{1},\\cdots,y_{T}]^{\\top}$ assumed to be generated according to the model in Eq. (1). Recall that the ordinary-least-squares (OLS) estimator for $\\theta$ isgivenby ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\theta}_{\\mathrm{LS}}:=\\arg\\operatorname*{min}_{\\widehat{\\theta}\\in\\mathbb{R}^{d}}\\sum_{t=1}^{T}(y_{t}-x_{t}^{\\top}\\widehat{\\theta})^{2}=(X_{T}^{\\top}X_{T})^{-1}X_{T}^{\\top}Y_{T}=\\theta+(X_{T}^{\\top}X_{T})^{-1}X_{T}^{\\top}\\varepsilon_{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Observe that $\\widehat{\\theta}_{\\mathrm{LS}}$ is potentially a biased and inconsistent estimator for $\\theta$ in the presence of endogenous noise since $\\mathbb{E}[\\varepsilon_{t}|x_{t}\\]\\neq0$ . To remediate this problem, we define a general class of estimators that includes several standard estimators. Given an invertible matrix $\\bar{\\Psi}\\in\\mathbb{R}^{d\\times d}$ , let $\\bar{X}_{T}:=Z_{T}\\Psi$ , and consider corresponding estimators termed $\\Psi$ -IV estimators of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\theta}_{\\Psi}:=(\\bar{X}_{T}^{\\top}\\bar{X}_{T})^{-1}\\bar{X}_{T}^{\\top}Y_{T}=(\\Psi^{\\top}Z_{T}^{\\top}Z_{T}\\Psi)^{-1}\\Psi^{\\top}Z_{T}^{\\top}Y_{T}=(Z_{T}^{\\top}Z_{T}\\Psi)^{-1}Z_{T}^{\\top}Y_{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When $\\Psi=I$ we recover the OLS estimator. In the rest of the paper, we will focus on two different potential options for $\\Psi$ ", "page_idx": 4}, {"type": "text", "text": "Case 1: Oracle. $\\Psi=\\Gamma.$ . To begin, observe that the structural equation model from Eq. (1) can be combined by substituting the second equation into the first to obtain the reduced form ", "page_idx": 4}, {"type": "equation", "text": "$$\ny_{t}=z_{t}^{\\top}\\Gamma\\theta+\\eta_{t}^{\\top}\\theta+\\varepsilon_{t}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since $z_{t}$ is independent of the i.i.d. process $\\eta_{t}^{\\top}\\theta+\\varepsilon_{t}$ , the least squares estimator which regresses $y_{t}$ onto $z_{t}^{\\top}\\Gamma$ is unbiased for estimation of $\\theta$ and given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\theta}_{\\mathrm{oracle}}=(\\bar{X}_{T}^{\\top}\\bar{X}_{T})^{-1}\\bar{X}_{T}^{\\top}Y_{T}=(Z_{T}^{\\top}Z_{T}\\Gamma)^{-1}Z_{T}^{\\top}Y_{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This estimator will be used to design our general solution concept presented in Section 3. Of course in practice we cannot expect to know $\\Gamma$ , but we may be able to estimate it. ", "page_idx": 4}, {"type": "text", "text": "Case 2: P-2SLS. $\\Psi=\\widehat{\\Gamma}$ . We consider a setting where $\\widehat{\\Gamma}$ is an (unbiased) estimator of $\\Gamma$ , learned using least-squares from an independent dataset $\\bar{Z}_{T_{1}}=[z_{1}^{\\prime},\\cdot\\cdot\\cdot\\cdot,z_{T_{1}}^{\\prime}],X_{T_{1}}=[z_{1}^{\\prime},\\cdot\\cdot\\cdot\\cdot,z_{T_{1}}^{\\prime}]$ collected non-adaptively.3 That is, $\\widehat{\\Gamma}=(Z_{T_{1}}^{\\top}Z_{T_{1}})^{-1}Z_{T_{1}}^{\\top}X_{T_{1}}$ and: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{\\mathrm{P-2SLS}}=(\\widehat{\\Gamma}^{\\top}Z_{T}^{\\top}Z_{T}\\widehat{\\Gamma})^{-1}\\widehat{\\Gamma}^{\\top}Z_{T}^{\\top}Y_{T}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We refer to the resulting estimator as a pseudo two stage least squares (P-2SLS) estimator. The main advantage of the P-2SLS estimator over standard 2SLS (given in Appendix C) is easier inference sincenow $\\{\\varepsilon_{t}\\}_{t\\le T}$ of our dataset is independent of the measurements of the first dataset $Z_{T_{1}},X_{T_{1}}$ In the econometrics literature, such an estimator is referred to as a two-sample 2sLS estimator [19]. ", "page_idx": 5}, {"type": "text", "text": "2.2  Confidence Intervals ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the section that follows, we develop a general algorithmic approach that relies on experimental design aimed at reducing the uncertainty in our estimates of the optimal treatment. To this end, we first develop finite-time confidence intervals for estimators presented in the previous section given data generated according to the model in Eq. (1) and collected from non-adaptive designs. ", "page_idx": 5}, {"type": "text", "text": "We begin by characterizing the properties of the noise structure in the combined model of Eq. (5) with the following set of results. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.1. Under Assumption $^{\\,l}$ , the noise process $\\boldsymbol{\\nu}:=\\boldsymbol{\\eta}^{\\top}\\boldsymbol{\\theta}+\\varepsilon$ is $\\sigma_{\\nu}^{2}$ -sub-Gaussian where $\\sigma_{\\nu}^{2}=2(\\sigma_{\\eta}^{2}\\|\\theta\\|_{2}^{2}+1)$ specifically when the instance is compliance, $\\sigma_{\\nu}^{2}=2(\\dot{4}\\|\\theta\\|_{2}^{2}+1)$ ", "page_idx": 5}, {"type": "text", "text": "Oracle Confidence Interval. As in the last section, we assume that we have access to a dataset $(Z_{T},X_{T},Y_{T})$ generated according to Eq. 1 and collected non-adaptively. Given Lemma 2.1, it can be shown that $w^{\\top}\\widehat{\\theta}_{\\mathsf{o r a c l e}}$ is a sub-Gaussian random variable satisfying the following. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.2. With probability at least $1-\\delta$ for $\\delta\\in(0,1)$ and $w\\in\\mathbb{R}^{d}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n|w^{\\top}(\\widehat{\\theta}_{\\mathrm{oracle}}-\\theta)|\\leq\\sqrt{2\\sigma_{\\nu}^{2}\\|w\\|_{\\bar{A}(Z_{T},\\Gamma)^{-1}}^{2}\\log\\bigl(2/\\delta\\bigr)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\sigma_{\\nu}^{2}$ is the sub-Gaussian parameter of the noise $\\nu:=\\eta^{\\top}\\theta+\\varepsilon$ characterized in Lemma 2.1. ", "page_idx": 5}, {"type": "text", "text": "The proof of this result is in Appendix G.2. ", "page_idx": 5}, {"type": "text", "text": "P-2SLS Confidence Interval. We now present a novel finite-time confidence interval for the P-2SLS estimator. As discussed in the previous section with respect to this estimator, we assume access a set ofdata $(Z_{T_{1}},X_{T_{1}})$ generated according to Eq. (1) and collected non-adaptively for the purpose of estimating $\\Gamma$ . Moreover, assume access to a separate set of data $(Z_{T_{2}},X_{T_{2}},Y_{T_{2}})$ generated according to Eq. (1) and collected non-adaptively for the purpose of estimating $\\theta$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.3. Suppose that $\\widehat\\Gamma=(Z_{T_{1}}^{\\top}Z_{T_{1}})^{-1}Z_{T_{1}}^{\\top}X_{T_{1}}$ and $\\widehat{\\theta}_{P-2S L S}\\,=\\,(\\widehat{\\Gamma}^{\\top}Z_{T_{2}}Z_{T_{2}}\\widehat{\\Gamma})^{-1}\\widehat{\\Gamma}Z_{T_{2}}^{\\top}Y_{T_{2}}.$ Then, for any $w\\in\\mathcal{W}$ with probability at least $1-\\delta$ for $\\delta\\in(0,1)$ \uff0c ", "page_idx": 5}, {"type": "equation", "text": "$$\n|w^{\\top}(\\widehat{\\theta_{\\mathtt{P}-2\\mathrm{SIS}}}-\\theta)|\\leq\\|w\\|_{\\bar{A}(Z_{T_{2}},\\widehat{\\Gamma})^{-1}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\biggl(\\frac{4}{\\delta}\\biggr)}+\\|w\\|_{\\bar{A}(Z_{T_{1}},\\widehat{\\Gamma})^{-1}}\\|\\theta\\|_{2}\\sqrt{\\sigma_{\\eta}^{2}\\overline{{\\log\\bigl(Z_{T_{1}},\\delta/4\\bigr)}}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{og}}}(Z_{T},\\delta):=8d\\ln\\left(1+\\frac{2T L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(Z_{T_{1}}^{\\top}Z_{T_{1}}))}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta}\\cdot\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(Z_{T_{1}}^{\\top}Z_{T_{1}})}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is presented in Appendix G.3. Observe that the first term in the P-2SLS estimator confidence interval given by $\\sqrt{2\\sigma_{\\nu}^{2}\\|w\\|_{\\bar{A}(Z_{T_{2}},\\widehat{\\Gamma})^{-1}}^{2}\\log\\left(4/\\delta\\right)}$ matches the Oracle estimator conidence interal in Lemma 2.2 when $\\widehat{\\Gamma}=\\Gamma$ The second term salingike $\\mathcal{O}(\\|w\\|_{\\bar{A}(Z_{T_{2}},\\hat{\\Gamma})^{-1}}\\|\\theta\\|_{2}\\sigma_{\\eta}\\sqrt{d+\\log(1/\\delta)})$ \uff0c is an upper bound on the approximation error $w^{\\top}(\\widehat{\\Gamma}^{-1}\\Gamma-I)\\theta$ for any $w\\in\\mathbb{R}^{d}$ , assuming that $\\widehat{\\Gamma}$ is learned from an OLS estimator (see Theorem G.3 for details). ", "page_idx": 5}, {"type": "text", "text": "We will see that the form of this confidence interval is particularly convenient for our algorithmic approachgiven in Section3 In particulr th form of thevariance $\\|w\\|_{\\bar{A}(Z_{T_{2}},\\Gamma)-1}^{2}$ Oon thefirstermn only depends on a design over instruments. Thus, we can choose an experimental design over $Z$ which reduces this variance optimally. ", "page_idx": 6}, {"type": "text", "text": "Remark 2.4. In practice we expect the first stage of samples, $(Z_{T_{1}},X_{T_{1}})$ tobe collected from either a burn-in period or from existing historical data. We remark that assuming two stages of samples is common in the orthogonal and double machine learning for estimating nuisance parameters in the data generating process (e.g. F) [9]. Our result matches the existing literature on the asymptotic variance of two sample 2SLS estimators (e.g., Theorem 1 of [19]). ", "page_idx": 6}, {"type": "text", "text": "Remark 2.5. The asymptotic variance of standard 2SLS is known to involve a factor $\\sigma_{\\varepsilon}^{2}$ ,instead of $\\sigma_{\\nu}^{2}$ as we have [18]. Recent work by [11] shows a variance involving $d\\sigma_{\\varepsilon}^{2}$ .However, it's unclear how to use the form of their confidence interval directly for experimental design. In addition, their work is not sufficiently general to handle the general forms of noise that we consider in Lemma 2.1. ", "page_idx": 6}, {"type": "text", "text": "3  Adaptive Experimental Design Algorithms ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now present adaptive experimental design algorithms for the CPET-LB problem. Our main insight utilizes Eq. 1 by plugging the model for $x$ into the top equation resulting in the relationship ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y=z^{\\top}\\Gamma\\theta+{\\theta}^{\\top}{\\eta}+\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "When $\\Gamma$ is known, by Eq. 5, we see that CPET-LB reduces to a standard pure exploration transductive linear bandit problem where the measurement set is given by $\\{\\Gamma^{\\top}z\\}_{z\\in\\mathcal{Z}}^{}\\subset\\mathring{\\mathbb{R}}^{d}$ , the evaluation set is $\\mathcal{W}\\subset\\mathbb{R}^{d}$ , and the feedback model is given by $y=v^{\\top}\\theta+\\bar{\\nu}$ where the noise $\\nu=\\theta^{\\top}\\eta+\\varepsilon$ is sub-Gaussian and as before the goal is to identify arg $\\operatorname*{max}_{w\\in\\mathcal{W}}w^{\\top}\\theta$ . An existing approach to this problem is given by the RAGE algorithm [15], which we use as the basis of our approach. Addressing the case of unknown $\\Gamma$ is our major algorithmic contribution, where we develop solutions to improve our estimate of $\\Gamma$ and learn $w^{*}$ simultaneously. As a warm-up to this approach, we first consider the setting when $\\Gamma$ is known. ", "page_idx": 6}, {"type": "text", "text": "3.1 Warm-Up: Known Structural Model ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Algorithm 1 assumes a parameter $L_{\\nu}$ , which acts as an upper bound on the sub-Gaussian constant of the noise $\\nu=\\theta^{\\top}\\eta+\\varepsilon$ . In each round $k$ , an active set of potentially optimal vectors $\\widehat{\\mathcal{W}}_{k}\\subset\\mathcal{W}$ is maintained. CPEG aims to sample in such a way that reduces the uncertainty of the estimates on the gaps $(w-w^{\\prime})^{\\top}\\theta$ for each pair $\\overline{{w}},w^{\\prime}\\in\\widehat{\\mathcal{W}_{k}}$ maximally each round. In any given round the algorithm takes $N_{k}$ samples $Z_{N_{k}}$ , the confidence interval of Lemma 2.2 shows that the error in estimating $\\mathbf{\\Pi}(w-$ $w^{\\prime})^{\\top}\\theta$ scale with $\\|w-w^{\\prime}\\|_{(\\Gamma^{\\top}Z_{N_{k}}^{\\top}Z_{N_{k}}\\Gamma)^{-1}}^{2}.$ Thismotvatesuzin aneprmldesgrach where we choose a distribution $\\lambda_{k}\\in\\Delta(\\mathcal{Z})$ to minimize $\\begin{array}{r l}{\\operatorname*{max}_{w,w^{\\prime}\\in\\widehat{\\mathcal{W}}_{k}}\\|w-w^{\\prime}\\|_{(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma)^{-1}}}\\end{array}$ The number of resulting samples taken from this design $N_{k}$ is chosen to guarantee that the confidence interval of Lemma 2.2 is less than $2^{-k}$ . Then, the elimination step in Line 8 guarantees that all $w\\in\\mathscr{W}$ such that $(w^{*}-w)^{\\top}\\theta>2\\cdot2^{-k}$ are then eliminated from the active set by round $k+1$ of the procedure. To actually choose our samples, as is common in this literature [15], we use an efficient rounding procedure, ROUND that requires a minimum number of samples $r(\\omega)$ ", "page_idx": 6}, {"type": "text", "text": "Sample Complexity Guarantee. The sample complexity of Algorithm 1 depends on the following problem-dependent quantity $\\rho^{*}(\\gamma)$ that captures the underlying hardness of a problem instance in terms of $(\\mathcal{W},\\mathcal{Z},\\Gamma,\\theta)$ ,when $\\gamma=0$ ,we abbreviate $\\boldsymbol{\\rho}^{*}(\\boldsymbol{0})=\\dot{\\boldsymbol{\\rho}}^{*}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho^{*}(\\gamma)=\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w\\in\\mathcal{W}\\backslash\\{w^{*}\\}}\\frac{\\|w^{*}-w\\|_{(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma)^{-1}}^{2}}{\\langle w^{*}-w,\\theta\\rangle^{2}\\vee\\gamma^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1. Algorithm $^{\\,l}$ is $\\delta{-}P A C$ and terminates in at most $c(1+\\omega)L_{\\nu}\\rho^{*}\\log(1/\\delta)+c r(\\omega)$ samples, where $c$ hides logarithmic factors of $\\Delta:=\\operatorname*{min}_{w}\\langle w^{*}-w,\\theta\\rangle$ and $|\\mathcal{W}|$ , as well as constants. ", "page_idx": 6}, {"type": "text", "text": "The proof of this result is in Appendix H.1. In the unconfounded case when $\\Gamma\\,=\\,I$ and $\\eta\\,=$ $0,\\mathbb{E}_{t-1}[\\varepsilon_{t}|x_{t}]=0$ this matches the sample complexity of [15]. In particular, for the case where $\\mathcal{Z}\\,=\\,\\mathcal{X}\\,=\\,\\mathcal{W}$ , the problem further reduces to a standard multi-armed bandit, and if $\\varepsilon$ is 1-subGaussian noise,[33] shows that $\\begin{array}{r}{\\rho^{*}=O(\\sum_{i=2}^{d}(\\theta_{1}-\\theta_{i})^{-2}))}\\end{array}$ , which is the optimal sample complexity of best-arm identification for multi-armed bandits. The following lemma shows that the conditioning Oof $\\Gamma$ can have a strong impact on the resulting sample complexity. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Lemma 3.2. For tecompliane seting we have $\\begin{array}{r l}{\\operatorname*{min}_{\\lambda\\in\\Delta^{d}}\\operatorname*{max}_{j,j^{\\prime}}\\,\\|e_{j}-e_{j^{\\prime}}\\|_{(\\sum_{i=1}^{d}\\lambda_{i}\\Gamma^{\\top}e_{i}e_{i}^{\\top}\\Gamma)^{-1}}^{2}\\le}&{}\\end{array}$ $d\\operatorname*{max}_{j,j^{\\prime}}\\,\\lVert\\Gamma^{-1}(e_{j}-e_{j^{\\prime}})\\rVert_{2}^{2}$ Furthermore, $\\begin{array}{r}{\\rho^{*}\\leq\\frac{d\\sigma_{\\operatorname*{min}}^{2}(\\Gamma)^{-1}}{\\Delta_{\\operatorname*{min}}^{2}}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "To further illustrate the impact of $\\Gamma$ , imagine an extreme setting where $\\Gamma=(\\bar{1}-\\varepsilon)/{d\\bf11}^{\\top}+\\varepsilon I$ and $\\varepsilon\\approx0$ i.e. $\\Gamma$ is a perturbation of $1/d\\mathbf{11}^{\\top}$ It's straightforward to show that the upper bound in the first display of Lemma 3.2 is of the order $\\dot{O}(\\dot{d}\\varepsilon^{-2})$ (this is also a lower bound - see Appendix K.1). In particular, the upper bound on the sample complexity is of the form $d\\varepsilon^{-2}/\\Delta_{\\operatorname*{min}}^{2}$ . This is in sharp contrast to the linear bandit case, when $\\Gamma=I$ and we are guaranteed a sample complexity of no more than $d/\\Delta_{\\mathrm{min}}^{2}$ samples. To gain some intuition, regardless of the choice of $\\lambda$ $\\begin{array}{r}{\\sum_{i\\leq d}\\lambda_{i}\\boldsymbol{\\Gamma}^{\\top}e_{i}e_{i}^{\\top}\\boldsymbol{\\Gamma}\\approx\\boldsymbol{\\Gamma}}\\end{array}$ . As a result, ", "page_idx": 7}, {"type": "table", "img_path": "2mqiTiJKrx/tmp/4174003f44175d08d2b16d41096fcecca89f88c555f9dd0f998b23111c8f4a14.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "$\\rho^{*}\\to\\infty$ as $\\varepsilon\\rightarrow0$ . Intuitively in the limit, regardless of which instrument $i\\leq d$ is being pulled, the resulting distribution on the treatments is uniform (the instruments are weak). Thus, it is impossible to deconfound the measurement noise, and recover an estimate of $\\theta$ . This is a phenomenon which does not arise in the standard multi-armed bandit case with unconfounding. ", "page_idx": 7}, {"type": "text", "text": "Remark 3.3. We also consider a setting where instead of given $\\Gamma$ directly, we are given an estimate T of $\\Gamma$ based on offline data. We discuss such an adaptation of Algorithm 1 to this setting in Appendix I and provide a sample complexity which reflects the error in $\\Gamma$ (scaling with $\\rho^{*}(\\gamma)$ for $\\gamma>0$ ).We remark that this result is subsumed by the approach of Section 3.2 and so we omit it in the main text. ", "page_idx": 7}, {"type": "text", "text": "Lower bound. Due to the noise model from confounding and the dependence of the noise $\\theta^{\\top}\\eta+\\varepsilon$ the instance-dependent lower bounds of [15] do not immediately apply. We develop a lower bound tailored for the confounding setting that nearly match the upper bounds of our algorithms. What's more, our lower bound illustrates the additional difficulty that arises from confounding by an additional factorof $d^{2}$ compared to the standard transductive linear bandit problem in the most general setting where entries of $\\eta$ are sub-Gaussian, but not necessarily independent nor bounded. Due to space limit, we defer it to Appendix D. ", "page_idx": 7}, {"type": "text", "text": "3.2 Fully Unknown Structural Model ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now consider the setting where $\\Gamma$ is fully unknown. The difficulty of this setting is that the data collection process needs to support both estimation of $\\Gamma$ and $\\theta$ simultaneously. Our algorithm, built upon Algorithm 1, is summarized in Algorithm 3. At its core, each phase of the algorithm is divided into two sub-phases, for estimating $\\Gamma$ and $\\theta$ respectively. Specifically, the second sub-phase is essentially same as Algorithm 1 with $\\widehat{\\Gamma}_{k}$ in place of $\\Gamma$ where $\\boldsymbol{\\widetilde{\\Gamma}}_{k}$ is estimated from the first sub-phase. The main novelty of our algorithmic design lies in the first sub-phase, which resolves the challenge of performing the optimal design for estimating $\\Gamma$ . To explain this challenge, the confidence interval for P-2SLS estimators of Theorem 2.3 indicates that one should pull arms so that we control both $D_{2}:=\\operatorname*{max}_{w,w^{\\prime}}\\|w-w^{\\prime}\\|_{\\bar{A}(Z_{T_{2}},\\widehat{\\Gamma}_{k})}^{2}$ (er from $\\hat{\\theta}_{\\mathsf{P}-2\\mathsf{S}\\mathsf{L}\\mathsf{S}})$ and $D_{1}:=\\operatorname*{max}_{w,w^{\\prime}}\\|w-w^{\\prime}\\|_{\\bar{A}(Z_{T_{1}},\\widehat{\\Gamma}_{k})}^{2}$ (error from $\\widehat{\\Gamma}_{k}^{~~}$ ) to be below the target error $O(\\zeta_{k}^{2})$ at each phase (ignoring unimportant factors for discussion). Controlling $D_{2}$ is trivial, which is done in the second sub-phase as we described above. ", "page_idx": 7}, {"type": "text", "text": "However, for $D_{1}$ , a similar strategy cannot be done because the estimate $\\widehat{\\Gamma}_{k}$ is computed directly by sampling arms in $Z_{T_{1}}$ . That is, the ideal design, based on which we willcollect data points $z_{1},\\ldots,z_{n}$ \uff0c requires accesto the random matrix $\\widehat{\\Gamma}_{k}$ that can only be computed after sampling $z_{1},\\dots,z_{n},$ . This ", "page_idx": 7}, {"type": "text", "text": "Input $\\begin{array}{r l}&{\\mathcal{Z},\\mathcal{W},\\delta,L_{\\nu}\\geq\\sigma_{\\nu}^{2},L_{\\eta}\\geq\\sigma_{\\eta}^{2},\\omega,\\gamma_{\\operatorname*{min}}\\leq\\lambda_{\\operatorname*{min}}(\\Gamma),\\lambda_{E},\\kappa_{0}}\\\\ &{\\mathrm{ize}:k=1,\\mathcal{W}_{1}=\\mathcal{W},\\widehat{\\Gamma}_{0}=\\!\\bot,\\zeta_{1}=1}\\\\ &{\\ f(w,w^{\\prime},\\Gamma,\\lambda):=\\Vert w-w^{\\prime}\\Vert_{(\\Sigma_{z\\in\\mathcal{Z}}\\Gamma^{\\top}\\lambda_{z}z z^{\\top}\\Gamma)^{-1}}^{2},M:=\\frac{32L_{\\eta}}{\\gamma_{\\operatorname*{min}}^{2}\\sigma_{\\operatorname*{min}}\\left(A(\\lambda_{E},I)\\right)}\\vee1,\\delta_{\\ell}:=\\frac{\\delta}{4\\ell^{2}}}\\end{array}$   \nInitial   \nDefne   \nwhile $|\\mathcal{W}_{k}|>1$ do   \n$\\begin{array}{r l}&{\\widehat{\\Gamma}_{k}\\stackrel{!}{=}\\Gamma-\\mathtt{e s t i m a t o r}\\Big(\\mathcal{W}_{k},\\widehat{\\Gamma}_{k-1},\\zeta_{k},\\delta/k^{2},\\omega,\\lambda_{E},M,L_{\\eta}\\Big)}\\\\ &{\\widehat{\\theta}_{\\mathtt{p}-2\\mathrm{sts}}^{\\mathtt{k}}=\\theta-\\mathtt{e s t i m a t o r}\\Big(\\mathcal{W}_{k},\\delta/k^{2},\\zeta_{k},\\widehat{\\Gamma}_{k},\\omega,L_{\\nu}\\Big)}\\\\ &{\\mathcal{W}_{k+1}=\\mathcal{W}_{k}\\backslash\\left\\{w\\in\\mathcal{W}_{k}\\ \\vert\\ \\exists w^{\\prime}\\in\\mathcal{W}_{k},\\mathrm{s.t.},\\Big\\langle w^{\\prime}-w,\\widehat{\\theta}_{\\mathtt{p}-2\\mathrm{sts}}^{k}\\Big\\rangle>\\zeta_{k}\\right\\}}\\\\ &{k\\stackrel{!}{=}-k+1,\\zeta_{k}=2^{-k}}\\end{array}$ Step 1: update $\\widehat{\\Gamma}$ Step 2: update $\\widehat{\\theta}$ \u2265 Step 3: elimination   \nend while   \nOutput: $\\mathcal{W}_{k}$ ", "page_idx": 8}, {"type": "text", "text": "creates a cycle that seems impossible to resolve. Such an issue, to our knowledge, has not been seen in existing work on pure exploration, and thus resolving it is our key technical contribution. ", "page_idx": 8}, {"type": "text", "text": "Our solution is to compute the design based on $\\widehat{\\Gamma}_{k}$ from the previous phase. We then perform a doubling trick where we double the sample size (while following the computed design) until $D_{1}$ becomes smaller than the target error $O(\\zeta_{k}^{2})$ . The intuition is that in later phases the estimate $\\widehat{\\Gamma}_{k}$ from the previous phase will be accurate enough to ensure that the design is efficient. Note that this novel algorithm induces extra randomness in how many samples we end up collecting in the first sub-phase, which remains random even after conditioning on the history, unlike the second sub-phase. This makes the analysis challenging, which we describe after the main result. ", "page_idx": 8}, {"type": "text", "text": "Our algorithm additionally employs the so-called E-optimal design to ensure that the covariance matrix of the collected data used to estimate $\\Gamma$ is well-conditioned.  This conditioning is required to ensure that fk concentrates fast enough to $\\Gamma$ as shown in the analysis. The E-optimal design is a well-known design  objective  in  experimental design that aims to maximize the smallest singular value:. $\\lambda_{E}^{\\ast}\\quad:=$ $\\begin{array}{r}{\\operatorname{arg\\,min}_{\\lambda\\in\\Delta(\\bar{\\mathcal{Z}})}\\sigma_{\\operatorname*{max}}(V^{-1}(\\lambda)).}\\end{array}$ where $\\begin{array}{r c l}{V}&{=}&{\\sum_{z\\in\\mathcal{Z}}{\\lambda_{z}z z^{\\top}}}\\end{array}$ We denote $\\kappa_{0}^{-1}\\;:=\\;\\sigma_{\\operatorname*{max}}(V^{-1}(\\lambda_{E}^{*}))\\;=$ ", "page_idx": 8}, {"type": "table", "img_path": "2mqiTiJKrx/tmp/6082a4e858c8eddac64087c4dcc596b344ea4286e5f5586f46309c69460d91dd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "$\\sigma_{\\mathrm{min}}^{-1}(V(\\bar{\\lambda}_{E}^{*}))$ as the smallest singular value achieved by the E-optimal design. ", "page_idx": 8}, {"type": "text", "text": "We present our analysis result Theorem 3.4 where we show that, even without knowledge of $\\Gamma$ the sample complexity scales with the key problem difficulty $\\rho^{*}$ almost matching the sample complexity of Algorithm 1 which relies on knowledge of $\\Gamma$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 3.4. Algorithm $^3$ is $\\delta$ -PAC and terminates inat most ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1+\\omega)((L_{\\nu}\\log(1/\\delta)+L_{\\eta}\\|\\theta\\|_{2}^{2}(d+\\log(1/\\delta)))\\rho^{*}+(d+\\log(1/\\delta))(L_{\\eta}\\|\\theta\\|_{2}^{2}\\rho_{0}+M))}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "pulls, ignoring both of the additive and multiplicative logarithms of $\\Delta,|\\mathcal{W}|,\\rho^{*},\\rho_{0},M,$ where ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\rho_{0}=\\operatorname*{max}_{w\\in\\mathcal{W}\\backslash\\{w^{*}\\}}\\|w^{*}-w\\|_{(\\sum_{z\\in\\mathcal{Z}}\\lambda_{E,z}\\Gamma^{\\top}z z^{\\top}\\Gamma)^{-1}}^{2},\\,a n d\\,M=\\frac{32L_{\\eta}}{\\gamma_{\\operatorname*{min}}^{2}\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},I)\\big)}\\vee1.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Notethat $\\rho_{0}$ does not get hurt by $\\left\\langle w^{*}-w,\\theta\\right\\rangle$ $\\boldsymbol{\\rho}^{*}$ does). It comes from the fact that in the first phase, we initialize that algorithm with $E$ -optimaldesign. ", "page_idx": 8}, {"type": "text", "text": "The challenge of the analysis can be summarized in two-fold. First, since the concentration result in Theorem 2.3 is w.r.t. $\\tilde{\\Gamma}_{k}$ , we need to analyze how the random matrix $\\widehat{\\Gamma}_{k}$ concentrates around $\\Gamma$ and how this impacts the sample complexity. For this, we develop a novel concentration inequality ", "page_idx": 8}, {"type": "text", "text": "Input $\\mathcal{W},\\widehat{\\Gamma},\\zeta,\\delta,\\omega,\\lambda_{E},M,L_{\\eta}$   \nDefine $\\begin{array}{r}{\\mathtt{S t o p}(\\mathcal{W},Z,\\Gamma,\\delta):=\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}}\\big\\|w-w^{\\prime}\\big\\|_{\\bar{A}(Z,\\Gamma)^{-1}}\\|\\theta\\|_{2}\\sqrt{L_{\\eta}\\overline{{\\log}}(Z,\\delta)}}\\end{array}$   \nInitialize $\\ell=1$ $N_{0,0}=0$ doubling trick initialization $\\widehat{\\Gamma}=\\!\\!\\perp$ then while $\\ell=1$ or $\\mathtt{s t o p}\\Big(\\mathcal{W},Z_{0,\\ell},\\widehat{\\Gamma}^{\\prime},\\delta\\ell\\Big)>1$ do get $\\begin{array}{r}{2^{\\ell-1}\\Big(r(\\omega)\\vee\\frac{2}{\\kappa_{0}}\\Big)}\\end{array}$ samples denoted as $\\{Z_{0,\\ell},X_{0,\\ell},Y_{0,\\ell}\\}$ per design $\\lambda_{E}$ via ROUND Update $\\widehat{\\Gamma}^{\\prime}$ by OLS on $\\{Z_{0,\\ell},X_{0,\\ell}\\},\\ell\\gets\\ell+1$ end while   \nelse $\\begin{array}{r l}&{\\tilde{\\lambda}=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(Z)}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}}f(w,w^{\\prime},\\widehat{\\Gamma},\\lambda)}\\\\ &{N^{\\prime}=\\Bigg|4g d M\\ln\\bigg(1+2M\\Big(d+L_{z}^{2}\\Big)+2M2g d M\\bigg)+8M\\ln\\bigg(\\frac{2\\cdot6^{d}}{\\delta}\\bigg)\\vee r(\\omega)\\Bigg]}\\\\ &{\\mathbf{while}\\;\\ell=1\\;\\mathrm{or}\\;\\mathrm{}\\mathrm{Stop}\\bigg(\\mathcal{W},Z_{0,\\ell}\\cup Z_{1,\\ell},\\widehat{\\Gamma},\\delta_{\\ell}\\bigg)>\\zeta\\;\\mathbf{do}}\\end{array}$ $N_{1,\\ell}=2^{\\ell}N^{\\prime}$ \u2265 doubling trick update get $N_{1,\\ell}$ samples per $\\tilde{\\lambda}$ denoted as $\\{Z_{1,\\ell},X_{1,\\ell},Y_{1,\\ell}\\}$ $\\triangleright$ via ROUND $\\begin{array}{r}{N_{0,\\ell}=\\left\\lceil2g d M\\ln\\Big(M\\Big(d+N_{1,\\ell}+L_{z}^{2}\\Big)\\Big)+4M\\ln\\Big(\\frac{2\\cdot6^{d}}{\\delta_{\\ell}}\\Big)\\vee r(\\omega)\\vee\\frac{2}{\\kappa_{0}}\\right\\rceil}\\end{array}$ get $\\left(N_{0,\\ell}-N_{0,\\ell-1}\\right)$ samples per $\\lambda_{E}$ augmented to $\\{Z_{0,\\ell-1},X_{0,\\ell-1}\\}$ and get $\\{Z_{0,\\ell},X_{0,\\ell}\\}$ Update $\\widehat{\\Gamma}^{\\prime}$ by OLS on $\\{Z_{0,\\ell}\\cup Z_{1,\\ell},X_{0,\\ell}\\cup X_{1,\\ell}\\}$ \uff0c $\\ell\\gets\\ell+1$ end while   \nend if ", "page_idx": 9}, {"type": "text", "text": "that relates the confidence width involving $\\hat{\\Gamma}$ from Theorem 2.3 with the same quantity involving $\\Gamma$ in place of $\\hat{\\Gamma}$ . Second, our algorithm creates a long-range error propagation, which is highly nontrivial to analyze. To see this, the quality of $\\hat{\\Gamma}_{k}$ is affected by the design objective function $\\operatorname*{max}_{w,w^{\\prime}}f(w,w^{\\prime},\\hat{\\Gamma}_{k-1},\\lambda)$ , which depends on the error of the estimate $\\bar{\\Gamma}_{k-1}$ from the previous phase. This error is, in turn, affected by the error of $\\hat{\\Gamma}_{k-2}$ by the same mechanism. This is repeated all the way back to the first phase. Thus, any abnormal behavior from the first iteration will have a cumulative impact to even the end. In our analysis, we successfully analyze how the error is propagated from the previous iterations, which forms a complicated recursion. Resolving this recursion is our key novelty in the analysis. ", "page_idx": 9}, {"type": "text", "text": "Remark 3.5. Our algorithm requires knowledge of a lower bound $\\gamma_{\\mathrm{min}}$ of $\\lambda_{\\mathrm{min}}(\\Gamma)$ . The knowledge of $\\gamma_{\\mathrm{min}}$ is for simplicity only as one can obtain such a lower bound that is at least half of the true value $\\lambda_{\\mathrm{min}}(\\Gamma)$ via an efficient sampling procedure that we describe in Appendix K. ", "page_idx": 9}, {"type": "text", "text": "Experiments. We provide experiments for the instance of Section 1.2 in the Appendix E. The experiments show that our approach is more sample efficient than natural passive baselines (e.g. A/B testing), or naively applying existing Pure-Exploration linear bandit methods and performs similarly to the oracle complexity. ", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work introduces the CPET-LB problem in which the learning protocol is characterized by a linear structural equation model governed by parameters $\\Gamma$ and $\\theta$ . We provide a general solution that simultaneously estimates the structural model while optimally designing to learn the best-arm. The key ideas behind our approach are based on linear experimental design techniques, an instrumental variable estimator whose variance can be controlled by the design, and novel finite-time confidence intervals on this estimator. This paper presents a number of directions for future work including considering situations where the $d_{z}\\neq d_{x}$ , analysis to improve the dependence on the underlying noise variance, and the pursuit of a tight information-theoretic instance-dependent lower-bound. We hope that this line of work motivates increased discussion of the real impact of confounding on applicability of adaptive experimentation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Kwang-Sung Jun and Yao Zhao were supported in part by the National Science Foundation under grantCCF-2327013. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Abbasi-Yadkori, Y., Pal, D., and Szepesvari, C. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011.   \n[2]  Allen-Zhu, Z., Li, Y., Singh, A., and Wang, Y. Near-optimal discrete optimization for experimental design: A regret minimization approach. Mathematical Programming, 186:439-478, 2021.   \n[3]  Angrist, J. D., Imbens, G. W., and Rubin, D. B. Identification of causal effects using instrumental variables. Journal of the American Statistical Association, 91(434):444-455, 1996.   \n[4]  Bakshy, E., Eckles, D., and Bernstein, M. S. Designing and deploying online field experiments. In International Conference on World Wide Web, pages 283-292, 2014.   \n[5] Bareinboim, E., Forney, A., and Pearl, J. Bandits with unobserved confounders: A causal approach. Advances in Neural Information Processing Systems, 28, 2015.   \n[6] Bilodeau, B., Wang, L., and Roy, D. Adaptively exploiting d-separators with causal bandits. Advances in Neural Information Processing Systems, 35:20381-20392, 2022.   \n[7]  Bradlow, E. Encouragement designs: an approach to self-selected samples in an experimental design. Marketing Letters, 9:383-391, 1998.   \n[8] Chandak, Y., Shankar, S., Syrgkanis, V., and Brunskill, E. Adaptive instrument design for indirect experiments. arXiv preprint arXiv:2312.02438, 2023. [9] Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. Double/debiased machine learning for treatment and structural parameters, 2018.   \n[10] Degenne, R., Menard, P., Shang, X., and Valko, M. Gamification of pure exploration for linear bandits. In International Conference on Machine Learning, pages 2432-2442. PMLR, 2020.   \n[11]  Della Vecchia, R. and Basu, D. Online instrumental variable regression: Regret analysis and bandit feedback. arXiv preprint arXiv:2302.09357, 2023.   \n[12] Elbers, B.  Encouragement designs  and  instrumental  variables for a/b testing,  August 2023. m:  URL   https://engineering.atspotify.com/2023/08/ encouragement-designs-and-instrumental-variables-for-a-b-testing/. Accessed: 2024-05-19.   \n[13] Engineering, S. Encouragement designs and instrumental variables for a/b testing, 2023. Accessed: December 26, 2023.   \n[14] Even-Dar, E., Mannor, S., Mansour, Y, and Mahadevan, S. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006.   \n[15]  Fiez, T., Jain, L., Jamieson, K. G., and Ratliff, L. Sequential experimental design for transductive linear bandits. Advances in Neural Information Processing Systems, 32, 2019.   \n[16] Gales, S. B., Sethuraman, S., and Jun, K.-S. Norm-agnostic linear bandits. In International Conference on Artificial Intelligence and Statistics, pages 73-91. PMLR, 2022.   \n[17]  Gong, X. and Zhang, J. Dual instrumental method for confounded kernelized bandits. arXiv preprint arXiv:2209.03224, 2022.   \n[18] Greene, W. H. Econometric analysis. Pearson Education India, 2003.   \n[19] Inoue, A. and Solon, G. Two-sample instrumental variables estimators. The Review of Economics and Statistics, 92(3):557-561, 2010.   \n[20] Jun, K.-S., Jain, L., Mason, B., and Nassif, H. Improved confidence bounds for the linear logistic model and applications to bandits. In International Conference on Machine Learning, pages 5148-5157. PMLR, 2021.   \n[21] Kallus, N. Instrument-armed bandits. In Algorithmic Learning Theory, pages 529-546. PMLR, 2018.   \n[22] Katz-Samuels, J., Jain, L., Karnin, Z., and Jamieson, K. G. An empirical process approach to the union bound: Practical algorithms for combinatorial and linear bandits. In Advances in Neural Information Processing Systems, volume 33, pages 10371-10382, 2020.   \n[23] Kohavi, R., Tang, D., and Xu, Y. Trustworthy online controlled experiments: A practical guide to a/b testing. Cambridge University Press, 2020.   \n[24] Kveton, B., Liu, Y., Kruijssen, J. M., and Nie, Y. Non-compliant bandits. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 1138-1147, 2023.   \n[25] Lattimore, F., Lattimore, T., and Reid, M. D. Causal bandits: Learning good interventions via causal inference. Advances in Neural Information Processing Systems, 29, 2016.   \n[26] Lattimore, T. and Szepesvari, C. Bandit algorithms. Cambridge University Press, 2020.   \n[27] Li, Z., Jamieson, K., and Jain, L. Optimal exploration is no harder than thompson sampling. arXiv preprint arXiv:2310.06069, 2023.   \n[28] Lu, Y., Meisami, A., Tewari, A., and Yan, W. Regret analysis of bandit problems with causal background knowledge. In Conference on Uncertainty in Artificial Intelligence, pages 141-150. PMLR, 2020.   \n[29] Malek, A., Aglietti, V., and Chiappa, S. Additive causal bandits with unknown graph. In International Conference on Machine Learning, 2023.   \n[30] Mummalaneni, S., Yoganarasimhan, H., and Pathak, V. V. Producer and consumer engagement on social media platforms. SSRN 4173537, 2022.   \n[31]  Pearl, J. Causality. Cambridge university press, 2009.   \n[32] Sen, R., Shanmugam, K., Dimakis, A. G., and Shakkottai, S. Identifying best interventions through online importance sampling. In International Conference on Machine Learning, pages 3057-3066. PMLR, 2017.   \n[33] Soare, M., Lazaric, A., and Munos, R. Best-arm identification in linear bandits. Advances in Neural Information Processing Systems, 27, 2014.   \n[34]  Vershynin, R. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[35] Weltz, J., Fiez, T., Volfovsky, A., Laber, E., Mason, B., Nassif, H., and Jain, L. Experimental designs for heteroskedastic variance. arXiv preprint arXiv:2310.04390, 2023.   \n[36] Zhang, J., Chen, Y, and Singh, A. Causal bandits: Online decision-making in endogenous settings. arXiv preprint arXiv:2211.08649, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Broader Impacts 13 ", "page_idx": 12}, {"type": "text", "text": "B lustrative Example 13 ", "page_idx": 12}, {"type": "text", "text": "C Standard 2SLS estimator 14 ", "page_idx": 12}, {"type": "text", "text": "D   A non-interactive lower bound 14 ", "page_idx": 12}, {"type": "text", "text": "E Experiments 16   \nE.1 Comparison Algorithms . . 16   \nE.2  Experiment 1: Jump-Around Instance . - 16   \nE.3  Experiment 2: Interpolation Instance 17   \nF Proofs of the lower bound 18   \nF.1 Proof of Theorem D.1 . 18   \nF.2Proof of Corollary D.2 . 20   \nG Proofs of the confidence interval 21   \nG.1 Proof of Lemma 2.1 21   \nG.2Proof of Lemma 2.2 22   \nG.3Proof of Theorem 2.3 24   \nH Proofs of sample complexity when given I 28   \nH.1 Proof of Theorem 3.1. 28 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "1   Proofs of sample complexity when given T 31 ", "page_idx": 12}, {"type": "text", "text": "JProofs of sample complexity with unknown I 35 ", "page_idx": 12}, {"type": "text", "text": "K Estimating $\\lambda_{\\mathrm{min}}(\\Gamma)$ 54   \nK.1 Proof of Lemma 3.2 . . 57   \nK.2  lemma for solving $\\mathbf{X}$ less than $\\ln(\\mathbf{x})$ 57 ", "page_idx": 12}, {"type": "text", "text": "A Broader Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This work is algorithmic and not tied to a particular application that would have immediate negative impact. ", "page_idx": 12}, {"type": "text", "text": "B llustrative Example ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We now present an illustrative experiment that highlights the challenges of endogenous noise and the insufficiency of standard experimentation approaches used in the absence of confounding. ", "page_idx": 12}, {"type": "text", "text": "Instance Definition. Toward connecting back to membership example in Section 1, consider that a service has $d$ membership options given by the set $\\mathcal{A}=\\{1,\\ldots,d\\}$ . Let the set $\\mathcal{Z}=\\{e_{1},\\cdot\\cdot\\cdot,e_{d}\\}$ represent encouragements (incentives or advertisements) for the corresponding membership options given by $\\mathcal{W}=\\bar{\\mathcal{X}}=\\{e_{1},\\cdot\\cdot\\cdot,e_{d}\\}$ . We consider a location model that assumes each user $t\\,\\in\\,\\mathbb{N}$ arriving online has an underlying unobserved one-dimensional preference $u_{t}\\sim\\mathcal{N}(0,\\sigma_{u}^{2})$ . If an algorithm presents the user with encouragement $z_{I_{t}}=e_{I_{t}}$ for $I_{t}\\in\\mathcal A$ , then the user selects into the membershiplevel given by $\\begin{array}{r}{J_{t}=\\operatorname*{min}_{j\\in\\mathcal{A}}|I_{t}+u_{t}-j|}\\end{array}$ so that $x_{t}=e_{J_{t}}$ . For a visual depiction, see Figure 3a. This process captures a user being more likely to opt-in to membership levels that are closer to the encouragement that they were presented. The outcome is then given by $y_{t}=x_{t}^{\\top}\\theta+u_{t}$ This problem instance is a specific compliance instance. For this experiment, we take $d=6$ , let $\\theta={\\left[1\\begin{array}{l l l l l l}{-0.95}&{0}&{0.45}&{{\\dot{0}}.95}&{0.99{\\dot{7}}}\\end{array}\\right]}$ and $\\sigma_{u}^{2}=0.35$ Observe that the optimal evaluation vector is $\\boldsymbol{w}^{*}=\\boldsymbol{e}_{1}=\\arg\\operatorname*{max}_{\\boldsymbol{w}\\in\\mathcal{W}}\\boldsymbol{w}^{\\intercal}\\boldsymbol{\\theta}$ ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "We simulate a UCB strategy which maintains estimates of the average reward of each of the possible $d$ incentives, namely $\\begin{array}{r}{\\widehat{\\mu}_{i,t}\\,=\\,\\sum_{s=1}^{t}{\\bf1}\\{z_{t}\\,=\\,e_{i}\\}y_{t}}\\end{array}$ and then pulls the one with the highest upper confidence bound. This models current practice of using a bandit algorithm to select which incentive to show a user. Our results averaged over 100 simulations are in Figure 3d. At each round we estiate theaverage rewardof each level using an OLS stimator,ie. $\\begin{array}{r}{\\widehat{\\theta}_{i,t}^{O L S}\\,=\\,\\sum_{s=1}^{t}{\\mathbf{1}\\{x_{t}\\,=\\,}}}\\end{array}$ $\\begin{array}{r}{e_{i}\\}y_{t}/\\sum_{s=1}^{t}\\mathbf{1}\\{x_{t}\\,=\\,e_{i}\\}}\\end{array}$ and check whetherit matches the true value (denoted as UCB-OLS). We also consider an instrumental variable-estimator (see the next Section) which incorporates knowledge of $\\Gamma$ similar to 2SLS to deconfound our estimate (UCB-IV). As the plot demonstrates, UCB-OLS completely fails to identify $\\theta_{1}=\\arg\\operatorname*{max}_{i\\in d}\\theta_{i}$ (this line is hard to see it is at O) due to a biased estimate, whereas UCB-IV does better. However, UCB-IV methods seem to have a constant probability of error. To see why, note that the expected reward from pulling $z=e_{i}$ is $e_{i}^{\\top}\\Gamma\\theta$ . These values are plotted in orange in Figure 3c. In particular, with some constant probability, UCB runs on the empirical rewards from pulling $z$ 's zeroes in on arm 6, and as a result fails to give enough samples to learn that arm 1 is indeed the best. In contrast, our proposed method CPEG, Algorithm 1 manages to find the best arm with significantly higher probability (the algorithm was run with $\\delta=.1$ in the given time horizon. ", "page_idx": 13}, {"type": "text", "text": "C  Standard 2SLS estimator ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Consider $\\Psi=\\widehat{\\Gamma}_{2S L S}=(Z_{T}^{\\top}Z_{T})^{-1}Z_{T}^{\\top}X_{T}$ In this setting, we recover the standard two-stage-leastsquares (2SLS) estimator, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\theta}_{\\mathrm{2SLS}}=(X_{T}^{\\top}Z_{T}(Z_{T}^{\\top}Z_{T})^{-1}Z_{T}^{\\top}X_{T})^{-1}X_{T}^{\\top}(Z_{T}^{\\top}Z_{T})^{-1}Z_{T}^{\\top}Y_{T}=(Z_{T}^{\\top}X_{T})^{-1}Z_{T}^{\\top}Y_{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that the 2SLS estimator is a biased, but consistent estimator of the parameter $\\theta$ [3, 18]. ", "page_idx": 13}, {"type": "text", "text": "Note that in prtiula, the asymptoic variance of SLS is kown to be $\\sigma_{\\varepsilon}^{2}\\|w\\|_{(\\widehat{\\Gamma}_{2\\mathrm{SLS}}^{\\top}Z_{T}^{\\top}Z_{T}\\widehat{\\Gamma}_{2\\mathrm{SLS}})^{-1}}[18]$ Recent  work by  [1l]  provides  a confidence  interval  of  the  form $\\begin{array}{r l}{|w^{\\top}(\\widehat{\\theta}_{2\\mathrm{sLS}}~-~\\theta)|}&{{}\\leq}\\end{array}$ $O(d\\sigma_{\\varepsilon}^{2}\\Vert w\\Vert_{(\\widehat{\\Gamma}_{\\mathrm{2SLS}}^{\\top}Z_{T}^{\\top}Z_{T}\\widehat{\\Gamma}_{2\\mathrm{SLS}})^{-1}}\\sqrt{\\log(T/\\delta)})$ Hwever, is ulwtuete fmof t fidence interval directly for experimental design due to the dependence of $\\hat{\\Gamma}_{2S L S}$ on the random quantity $X$ . In addition, their work is not sufficiently general to handle the general forms of noise that we consider in Lemma 2.1. ", "page_idx": 13}, {"type": "text", "text": "D   A non-interactive lower bound ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Due to the noise model from confounding and the dependence of the noise $\\theta^{\\top}\\eta+\\varepsilon$ ,the instancedependent lower bounds of [15] do not immediately apply. In this section, we develop a lower bound tailored for the confounding setting. ", "page_idx": 13}, {"type": "text", "text": "Toward characterizing the optimal sample complexity, we develop a lower bound for a specific non-adaptive algorithm $\\boldsymbol{\\mathcal{A}}$ that has access to the matrix $\\Gamma$ governing the structural equation model. In particular, suppose that the non-adaptive algorithm $\\boldsymbol{\\mathcal{A}}$ is allowed to select a sequence of $T$ measurements $\\{z_{I_{1}},\\ldots z_{I_{t}}\\ldots,z_{I_{T}}\\}$ to query prior to collecting any observations, where $I_{t}$ represents the index of the vector $z\\ \\in\\ {\\mathcal{Z}}$ chosen at time $t\\ \\in\\ \\{1,\\ldots,T\\}$ .Then, given the observations $\\{y_{1},\\dots,\\dots y_{t},\\dots y_{T}\\}$ generated by the environment, a candidate optimal vector $\\widehat{w}\\in\\mathcal{W}$ is returned by the algorithm. We are interested in the necessary number of observations $T$ that must be collected in order to ensure $\\mathbb{P}(\\widehat{w}\\,\\neq\\,w^{*})\\;\\leq\\;\\delta$ for some $\\delta~\\in~(0,1)$ . Thus, it is natural that the optimal non-adaptive algorithm $\\boldsymbol{\\mathcal{A}}$ using the estimator $\\widehat{\\theta}_{\\mathtt{o r a c l e}}$ forms a recommendation rule such that $\\widehat{w}=$ arg $\\operatorname*{max}_{w\\in\\mathcal{W}}\\boldsymbol{w}^{\\top}\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{oracle}}$ . We now state our lower bound result with respect to the non-adaptive oracle algorithm. ", "page_idx": 13}, {"type": "image", "img_path": "2mqiTiJKrx/tmp/5163bd19d5ed8d2cd2b6dddb5596db45da23b5179ddc0c01c7f882db9ca3f604.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 3: (a) A visual depiction of the problem instance from Section B. The user is presented with encouragement $I_{t}\\in\\mathcal{A}$ and the user choice is given by $J_{t}$ where $\\begin{array}{r}{J_{t}=\\operatorname*{min}_{j\\in\\mathcal{A}}|I_{t}+u_{t}-j|}\\end{array}$ and $u_{t}\\sim\\mathcal{N}(0,\\sigma_{u}^{2})$ . b) A heat-map showing the structural parameter $\\Gamma$ for the problem instance from Section B. (c) A bar chart showing $\\mathbb{E}[y|x=w]=w^{\\top}\\theta$ and $\\mathbb{E}[y|z=w]\\,=\\,z^{\\top}\\Gamma\\theta$ for all $w\\in\\mathcal{W}$ This chart shows that the optimal evaluation vector is $w^{*}=e_{1}=\\arg\\operatorname*{max}_{w\\in\\mathcal{W}}\\mathbb{E}[y|x=w]$ ,while $e_{6}=\\arg\\operatorname*{max}_{w\\in\\mathcal{W}}\\mathbb{E}[y|z=w]$ and consequently estimation based on this quantity is problematic. (d) The probability of identifying $\\boldsymbol{w}^{*}=\\boldsymbol{e}_{1}$ for a collection of algorithms on the CPET-LB instance described in Section B. Standard optimistic sampling approaches in combination with an ordinary least squares estimator leads to faulty inferences. Given an instrumental variable estimator, these experimental designs eventually give high probability identification but do so inefficiently compared to our proposed approach (see Section 3). ", "page_idx": 14}, {"type": "text", "text": "Theorem D.1 (Non-Adaptive Oracle Lower Bound). Consider a problem instance characterized by $\\mathcal{W}\\subset\\mathbb{R}^{d},\\mathcal{Z}\\subset\\mathbb{R}^{d}$ \uff0c $\\Gamma^{'}\\in\\mathbb{R}^{d\\times d}$ and $\\theta\\in\\mathbb{R}^{d}$ Assume $\\Gamma$ is known, $\\theta$ is unknown, and the noise processis jointly Gaussian and defined by $\\gamma:=[\\eta\\quad\\varepsilon]\\sim\\mathcal{N}(0,\\Sigma)$ where $\\Sigma\\in\\mathbb{R}^{(d+1)\\times(d+1)}$ is an arbitrarycorrelationmatrix.For $\\delta\\in(0,0.05].$ if thenon-adaptive oraclealgorithm acquires $T\\leq$ $\\sigma^{2}\\rho^{*}\\log(1/\\delta)/2$ samples on the problem instance where $\\sigma^{2}:=v^{\\top}\\Sigma v$ and $\\boldsymbol{v}:=\\left[\\boldsymbol{\\theta}\\quad\\boldsymbol{1}\\right]\\in\\mathbb{R}^{d+1}$ \uff0c then $\\mathbb{P}(\\widehat{w}\\neq\\dot{w}^{*})\\ge\\delta$ ", "page_idx": 14}, {"type": "text", "text": "Corollary D.2. There exists a problem instance characterized by $\\mathcal{W}\\subset\\mathbb{R}^{d},\\mathcal{Z}\\subset\\mathbb{R}^{d}$ $\\Gamma\\in\\mathbb{R}^{d\\times d}$ and $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ witha noise processsatisfyingAssumption1 suchthat if thenon-adaptive oracle algorithm acquires $T\\leq\\operatorname*{max}\\{d\\|\\theta\\|_{2}^{2},\\sqrt{d}\\|\\theta\\|_{2}\\}\\rho^{*}\\log(1/\\delta)/2$ samples, then $\\mathbb{P}(\\widehat{\\boldsymbol{w}}\\neq\\boldsymbol{w}^{*})\\ge\\delta$ for $\\delta\\in(0,0.05]$ ", "page_idx": 14}, {"type": "text", "text": "The proof of Theorem D.1 is in Appendix F.1. Notably, the result is reminiscent of lower bounds for the standard pure exploration transductive linear bandit problem without confounding [15, 22] when given the measurement set $\\{\\Gamma^{\\top}z\\}_{z\\in W}$ ,evaluation set $\\mathcal{Z}$ , and parameter $\\theta$ ", "page_idx": 14}, {"type": "text", "text": "Notably, the upper bounds for our algorithms nearly match the lower bound of Theorem D.1. However, it is interesting to observe that the sample complexity incurs an additional factor of $d^{2}$ relativeto the standard transductive linear bandit problem in the most general setting where entries of $\\eta$ are sub-Gaussian, but not necessarily independent nor bounded. This illustrates the additional difficulty that arises from confounding. We point out that this is not likely to be a tight lower bound. In particular, it is a lower bound with respect to a non-adaptive algorithm that uses the particular choice of estimator. We leave improved lower bounds to future work. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "E Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We now present experiments a collection of experiments on CPET-LB problem instances. The experiments demonstrate that our approach produces efficient designs for inference and estimation. ", "page_idx": 15}, {"type": "text", "text": "E.1 Comparison Algorithms ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The baselines that our approaches are compared with are discussed below. We run experiments both when $\\Gamma$ is known and when $\\Gamma$ is fully unknown. ", "page_idx": 15}, {"type": "text", "text": "E.1.1 Known I. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To standardize the experiments, the baselines considered run in rounds mirroring the structure of Algorithm 1. Specifically, in round $k\\in\\mathbb{N}$ a sampling algorithm selects a design $\\bar{\\lambda}_{k}\\in\\Delta(\\mathcal{Z})$ , collects $N_{k}$ samples from the design, and forms a $\\Psi\\,-\\,\\mathtt{I V}$ estimate of $\\theta$ with $\\Psi\\,=\\,\\Gamma$ that is combined with a confidence interval (Lemma 2.2 to either eliminate evaluation vectors or validate a stopping condition. The number of samples $N_{k}$ taken in round $k\\in\\mathbb{N}$ by any of the algorithms is given by $N_{k}=\\lceil2(1\\!+\\!\\omega)\\zeta_{k}^{-2}\\rho(\\mathcal{W}_{k})L_{\\nu}\\log\\bigl(4k^{2}|\\mathcal{W}|/\\delta\\bigr)\\rceil\\vee r(\\omega)$ where $\\rho(\\mathcal{W}_{k})=\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}f(w,w^{\\prime},\\Gamma,\\lambda)$ for design $\\lambda_{k}\\in\\Delta(\\mathcal{Z})$ and an active set of evaluation vectors $\\mathcal{W}_{k}$ . The round sample count guarantees that given any experimental design, all vectors $w\\in\\mathcal{W}$ such that $(w^{*}-w)^{\\top}\\theta>2\\cdot2^{-k}$ can be determined to be suboptimal by the end of round $k$ . The sampling methods we consider are now described. ", "page_idx": 15}, {"type": "text", "text": "\u00b7 Static Oracle. This design selects $\\lambda_{k}=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w\\in\\mathcal{W}\\setminus\\{w^{*}\\}}f\\bigl(w^{*},w^{\\prime},\\Gamma,\\lambda\\bigr).$   \n\u00b7 Static XY-Optimal. This design selects $\\begin{array}{r}{\\lambda_{k}=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}}f\\big(w,w^{\\prime},\\Gamma,\\lambda\\big).}\\end{array}$   \n\u00b7 Static Uniform. This design selects $\\lambda_{k,z}=1/|\\mathcal{Z}|\\,\\forall\\,z\\in\\mathcal{Z}$   \n\u00b7 Adaptive Uniform $(S E)$ . This design selects $\\lambda_{k,w}\\,=\\,1/|\\mathcal{W}_{k}|\\,\\,\\forall\\,\\,w\\,\\in\\,\\mathcal{W}_{k}$ . Note that this algorithm is effectively an adaption of action-elimination [14] . ", "page_idx": 15}, {"type": "text", "text": "The static designs are independent of the round and simply terminate when all evaluation vectors can be eliminated except for a recommended optimal vector $\\widehat{w}$ ", "page_idx": 15}, {"type": "text", "text": "E.1.2 Unknown I. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For this set of experiments, we compare Algorithm 3 against a collection of variations of the sampling procedures. Specifically, we compare against methods that either replace only the experimental design for estimating $\\Gamma$ , or only the experimental design for estimating $\\theta$ , or both with uniform sampling. We label the approaches as $N-N$ ,where $N$ represents the sampling approaches (XY or uniform) for $\\Gamma$ and $\\theta$ respectively. Moreover, to make our approach more practical, we modify the algorithm so that $\\overline{{\\log}}(Z_{T},\\delta)=4d+\\log\\left(1/\\delta\\right)$ The step of incrementally adding more E-optimal design samples is also removed, so we collect E-optimal design samples only once in the beginning of Algorithm 4. We find that even with these modifications to the algorithm, correctness is maintained empirically. ", "page_idx": 15}, {"type": "text", "text": "E.2  Experiment 1: Jump-Around Instance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first return back to the location model of Section B. Recall that $\\mathcal{Z}=\\mathcal{W}=\\mathcal{X}=\\{e_{1},\\cdot\\cdot\\cdot,e_{d}\\}$ For this experiment, we take $d=6$ ,let $\\theta=\\left[1\\begin{array}{l l l l l l}{\\phantom{-}}&{\\!\\!\\!-0.95}&{0.45}&{0.45}&{0.95}&{0.45}\\end{array}\\right]$ and $\\sigma_{u}^{2}=0.275$ The results of the experiment are shown in Figure 4a for the case of known $\\Gamma$ . We see that Algorithm 1 performs much better than the baselines and nearly matches the oracle design. Delving into the approach, it is able to quickly eliminate all but $w_{1}$ and $w_{d-1}$ and then puts more mass on $z_{1}$ and $z_{d-1}$ to reduce the uncertainty on $w_{1}$ and $w_{d-1}$ . For the case of unknown $\\Gamma$ , the results are shown in Figure 4d, where $\\theta_{5}$ is reduced to 0.9 so that all approaches could finish. ", "page_idx": 15}, {"type": "image", "img_path": "2mqiTiJKrx/tmp/1353fd190d1c7c2a52eb38e56b95dc306c2484d92b7cd0e683f205a969bb4e4f.jpg", "img_caption": ["Figure 4: Sample complexity for algorithms on CPET-LB problems. Our approach is consistently competitive across the experiments.) "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E.3  Experiment 2: Interpolation Instance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Let $\\mathcal{Z}=\\mathcal{W}=\\boldsymbol{\\mathcal{X}}=\\{e_{1},\\dots,e_{d}\\}$ define the measurement, evaluation, and observation sets. We frst considerthat $\\begin{array}{r}{\\Gamma:=\\frac{(1-\\varepsilon)}{d}\\mathbf{1}_{d}\\mathbf{1}_{d}^{\\top}+\\varepsilon I_{d}}\\end{array}$ for a parameter $\\varepsilon\\in(0,1)$ where $1_{d}$ isa $d$ dimensional vector of 1's and $I_{d}$ is the $d$ -dimensional identity matrix. For this experiment, we take $d=4$ and let $\\theta\\,=\\,\\left[0.5\\quad0.583\\quad0.67\\quad0.75\\right]$ . As in all compliance instances, $\\dot{\\eta}_{t}=\\boldsymbol{x}_{t}-\\boldsymbol{\\Gamma}^{\\top}\\boldsymbol{z}_{t}$ , and in this simulation $\\eta_{t}=0.4\\eta_{t}^{\\top}v_{t}$ , where $v_{t}=\\bar{v}_{t}/\\|\\bar{v}_{t}\\|_{2}$ and $\\bar{v}_{t}\\sim\\mathcal{N}(0,I_{d})$ . The results of the experiment are shown in Figure 4b for $\\varepsilon\\in\\{1,0.9,0.8,0.7\\}$ with $\\Gamma$ known. Note that Static-XY and Uniform overlap, and SE and CPEG overlap. We see that Algorithm 1 and the adaptive uniform strategy perform similarly and near optimally. This is to be expected since the most efficient way to gather observations for treatments is to encourage that treatment, given that if the encouragement is not followed each of the alternatives is equally likely and provides no additional information of interest. Moreover, as discussed earlier, the problem gets more challenging as $\\Gamma\\rightarrow1_{d}1_{d}^{\\top}/d$ . Note that the identity matrix could be replaced with a permutation matrix, in which case uniform sampling with elimination becomes highly suboptimal. The results for the case of $\\Gamma$ unknown are shown in Figure 4e with $\\varepsilon=0.99$ . This shows the value that comes from the experimental design for estimating both $\\Gamma$ and $\\theta$ ", "page_idx": 16}, {"type": "text", "text": "Todfw $\\begin{array}{r}{\\Gamma:=\\frac{(1-\\varepsilon)}{d}{1}_{d}{1}_{d}^{\\top}+\\varepsilon I_{d}^{p}}\\end{array}$ where $I_{d}^{p}$ is a permutation matrix as follows, ", "page_idx": 16}, {"type": "equation", "text": "$$\nI_{d}^{p}=\\left[\\!\\!\\begin{array}{c c c c}{{0}}&{{1}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{1}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{1}}\\\\ {{1}}&{{0}}&{{0}}&{{0}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "All other settings remain the same as in the previous interpolation instance. The results for the known $\\Gamma$ case, shown in Figure $4c$ , indicate that SE exhibits significant underperformance due to its sampling rule not accounting for the permutation effect in $\\Gamma$ . In contrast, CPEG consistently achieves near-optimal performance. Note that Static-XY and Uniform still overlap. Figure 4f presents the results for the unknown $\\Gamma$ case, where we can notice that, comparing with Figure 4e, estimating different permutation matrices (with the identity matrix as a special case) does not affect problem difficulty. ", "page_idx": 16}, {"type": "text", "text": "F  Proofs of the lower bound ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Proof of Theorem D.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem $D.I$ . Consider a problem instance characterized by $\\mathcal{W}\\subset\\mathbb{R}^{d},\\mathcal{Z}\\subset\\mathbb{R}^{d}$ \uff0c $\\Gamma\\in\\mathbb{R}^{d\\times d}$ , and $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ .Assume $\\Gamma$ is known, $\\theta$ is unknown, and the noise process is jointly Gaussian and defined by $\\gamma\\;:=\\;[\\eta\\;\\;\\;\\varepsilon]\\;\\sim\\;{\\mathcal N}(0,\\Sigma)$ where $\\Sigma~\\in~\\mathbb{R}^{(d+1)\\times(d+1)}$ is an arbitrary correlation matrix. For $\\delta\\in(0,0.05]$ , if the non-adaptive oracle algorithm acquires $T\\leq\\sigma^{2}\\rho^{*}\\log\\left(1/\\delta\\right)/2$ samples on the problem instance where $\\sigma^{2}:=v^{\\top}\\Sigma v$ and $v:=[\\theta\\quad1]\\in\\mathbb{R}^{d+1}$ , then $\\mathbb{P}(\\widehat{\\boldsymbol{w}}\\neq\\boldsymbol{w}^{*})\\ge\\delta$ ", "page_idx": 17}, {"type": "text", "text": "Proof. We begin by recalling the framework of the non-adaptive oracle algorithm and discussing the properties of its estimator for the noise structure described in the statement of the result. ", "page_idx": 17}, {"type": "text", "text": "Non-Adaptive Oracle and Instance Definition. The non-adaptive oracle algorithm $\\boldsymbol{\\mathcal{A}}$ selects $T$ measurements to query prior to collecting any data. Let $I_{t}$ represent the index of the vector $z\\in{\\mathcal{Z}}$ chosen at time $t\\in\\{1,\\ldots,T\\}$ . The noise process for the instance under consideration is assumed to be jointly Gaussian and defined by $\\gamma_{t}:=\\left[\\eta_{t}\\quad\\varepsilon_{t}\\right]\\sim\\mathcal{N}(0,\\Sigma)$ where $\\Sigma\\,\\in\\,\\mathbb{R}^{(d+1)\\times(d+1)}$ is an arbitrary positive semidefinite matrix. Defining $\\bar{x}_{I_{t}}:=\\Gamma^{\\top}z_{I_{t}}$ \uff0c $\\boldsymbol{v}:=\\left[\\boldsymbol{\\theta}\\quad\\boldsymbol{1}\\right]\\in\\mathbb{R}^{d+1}$ and $\\nu_{t}:=v^{\\top}\\gamma_{t}$ \uff0c the fedback model can be described as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{t}=x_{t}\\theta+\\varepsilon_{t}}\\\\ &{\\quad=(\\Gamma^{\\top}z_{I_{t}})^{\\top}\\theta+\\eta_{t}^{\\top}\\theta+\\varepsilon_{t}}\\\\ &{\\quad=:(\\Gamma^{\\top}z_{I_{t}})^{\\top}\\theta+v^{\\top}\\gamma_{t}}\\\\ &{\\quad=:\\bar{x}_{I_{t}}^{\\top}\\theta+\\nu_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Observe that the noise is independent and identically distributed as $\\nu_{t}\\sim\\mathcal{N}(0,\\sigma^{2})$ where $\\sigma^{2}:=v^{\\top}\\Sigma v$ since $\\boldsymbol{\\gamma}_{t}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma})$ . Moreover, the noise process is exogeneous with $\\mathbb{E}[\\nu_{t}|\\dot{\\bar{x}}_{I_{t}}]\\,=\\,0$ since $\\Bar{x}_{I_{t}}$ is deterministic given the index choice $I_{t}$ ", "page_idx": 17}, {"type": "text", "text": "Let $\\{z_{I_{t}}\\}_{t=1}^{T},\\{\\bar{x}_{I_{t}}\\}_{t=1}^{T}$ , and $\\{y_{t}\\}_{t=1}^{T}$ denote the observations collected by the non-adaptive oracle algorithm $\\boldsymbol{\\mathcal{A}}$ and define $Z_{T}\\in\\mathbb{R}^{T\\times d}$ \uff0c $\\bar{X}_{T}\\in\\mathbb{R}^{T\\times d}$ , and $Y_{T}\\in\\mathbb{R}^{T}$ to contain the respective stacked observations. Algorithm $\\boldsymbol{\\mathcal{A}}$ obtains an estimate $\\widehat{\\theta}_{\\mathtt{o r a c l e}}$ by minimizing the sum of squares as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\theta}_{\\mathrm{oracle:}}:=\\arg\\operatorname*{min}_{\\widehat{\\theta}\\in\\mathbb{R}^{d}}\\sum_{t=1}^{T}(y_{t}-\\bar{x}_{I_{t}}\\widehat{\\theta})^{2}=(\\bar{X}_{T}^{\\top}\\bar{X}_{T})^{-1}\\bar{X}_{T}^{\\top}Y_{T}=(Z_{T}^{\\top}Z_{T}\\Gamma)^{-1}Z_{T}^{\\top}Y_{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Given $\\widehat{\\theta}_{\\mathtt{o r a c l e}}$ , the non-adaptive oracle algorithms $\\boldsymbol{\\mathcal{A}}$ returns a recommendation defined by $\\widehat{w}\\,=$ arg $\\operatorname*{max}_{w\\in\\mathcal{W}}\\boldsymbol{w}^{\\top}\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{oracle}}$ . Note that since $\\widehat{\\theta}_{\\mathtt{o r a c l e}}$ is obtained by least squares with exogeneous, independent and identically distributed mean-zero Gaussian noise, it is straightforward to verify the estimator is distributed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{0\\mathrm{rac1e}}-\\theta\\sim\\mathcal{N}\\bigg(0,\\sigma^{2}\\cdot\\bar{A}(Z_{T},\\Gamma)^{-1}\\bigg),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{A}(Z_{T},\\Gamma):=\\Big(\\sum_{t=1}^{T}\\Gamma^{\\top}z_{I_{t}}z_{I_{t}}\\Gamma\\Big)=\\bar{X}_{T}^{\\top}\\bar{X}_{T}=\\Gamma^{\\top}Z_{T}^{\\top}Z_{T}\\Gamma.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof by Contradiction. To begin, recall that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\rho^{*}:=\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w\\in\\mathcal{W}\\setminus\\{w^{*}\\}}\\frac{\\|w^{*}-w\\|_{A(\\lambda,\\Gamma)^{-1}}^{2}}{\\langle w^{*}-w,\\theta\\rangle^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Suppose for the sake of contradiction that the number of samples collected by the non-adaptive oracle algorithm $\\boldsymbol{\\mathcal{A}}$ is $T\\,\\leq\\,\\sigma^{2}\\rho^{*}\\log\\bigl(1/\\delta\\bigr)/2$ and $\\mathbb{P}(\\widehat{\\boldsymbol{w}}\\neq\\boldsymbol{w}^{*})\\,<\\,\\delta$ for $\\delta\\,\\in\\,(0,0.05]$ . To reach a contradiction, we analyze the distribution of $(w-w^{*})^{\\top}{\\widehat{\\theta}}$ for some $w\\ne w^{*}$ and show that with probability at least $\\delta$ it is positive. We remark that this proof follows similar techniques to that of the proof of Theorem 3 of [22]. ", "page_idx": 17}, {"type": "text", "text": "Let $\\overline{{\\lambda}}\\in\\Delta(\\mathcal{Z})$ represent the empirical sampling distribution of the algorithm $\\boldsymbol{\\mathcal{A}}$ , which is defined such that $\\begin{array}{r}{\\overline{{\\lambda}}_{z}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{1}\\{z_{I_{t}}=z\\}}\\end{array}$ foreach $z\\in{\\mathcal{Z}}$ .Moreover, define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\rho^{*}(\\overline{{\\lambda}}):=\\operatorname*{max}_{w\\in\\mathcal{W}\\backslash\\{w^{*}\\}}\\frac{\\|w^{*}-w\\|_{A(\\overline{{\\lambda}},\\Gamma)^{-1}}^{2}}{\\langle w^{*}-w,\\theta\\rangle^{2}}\\quad\\mathrm{and}\\quad\\widetilde{w}\\in\\operatorname*{arg\\,max}_{w\\in\\mathcal{W}\\backslash\\{w^{*}\\}}\\frac{\\|w^{*}-w\\|_{A(\\overline{{\\lambda}},\\Gamma)^{-1}}^{2}}{\\langle w^{*}-w,\\theta\\rangle^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that $\\rho^{*}(\\overline{{\\lambda}})\\geq\\rho^{*}$ and observe by definition, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{A(\\overline{{\\lambda}},\\Gamma)^{-1}}{T}:=\\frac{(\\sum_{z\\in\\mathcal{Z}}\\overline{{\\lambda}}_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma)^{-1}}{T}=\\Big(\\sum_{t=1}^{T}\\Gamma^{\\top}z_{I_{t}}z_{I_{t}}\\Gamma\\Big)^{-1}:=\\bar{A}(Z_{T},\\Gamma)^{-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, by Eq. (8), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{\\mathtt{0r a c l e}}-\\theta\\sim\\mathcal{N}\\Big(0,\\sigma^{2}\\cdot\\frac{A(\\overline{{\\lambda}},\\Gamma)^{-1}}{T}\\Big),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{(\\widetilde{w}-w^{*})^{\\top}(\\widehat{\\theta}_{0\\mathrm{racle}}-\\theta)}{\\langle w^{*}-\\widetilde{w},\\theta\\rangle}\\sim\\!\\mathcal{N}\\Big(0,\\sigma^{2}\\cdot\\frac{\\|w^{*}-\\widetilde{w}\\|_{A(\\overline{{\\lambda}},\\Gamma)^{-1}}^{2}}{T\\cdot\\langle w^{*}-\\widetilde{w},\\theta\\rangle^{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, by the definition of $\\rho^{*}(\\overline{{\\lambda}})$ , the assumption $T\\leq\\sigma^{2}\\rho^{*}\\log\\left(1/\\delta\\right)/2$ , and the fact $\\rho^{*}({\\overline{{\\lambda}}})\\geq$ $\\rho^{*}$ , we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{V}\\Big(\\frac{(\\widetilde w-w^{*})^{\\top}(\\widehat\\theta_{\\mathrm{0racle}}-\\theta)}{\\langle w^{*}-\\widetilde w,\\theta\\rangle}\\Big)=\\sigma^{2}\\cdot\\frac{\\Vert w^{*}-\\widetilde w\\Vert_{A(\\overline{\\lambda},\\Gamma)^{-1}}^{2}}{T\\cdot\\langle w^{*}-\\widetilde w,\\theta\\rangle^{2}}:=\\sigma^{2}\\cdot\\frac{\\rho^{*}(\\overline{\\lambda})}{T}\\geq\\frac{2}{\\log\\left(1/\\delta\\right)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, consider a random variable $W\\,\\sim\\,{\\mathcal{N}}(0,1)$ . Proposition 2.1.2 of Vershynin [34] gives an anti-concentration result showing that for all $\\zeta>0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}(W\\ge\\zeta)\\ge\\Big(\\frac1\\zeta-\\frac{1}{\\zeta^{3}}\\Big)\\frac{1}{\\sqrt{2\\pi}}e^{-\\zeta^{2}/2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We apply this result to the quantity $(\\widetilde{w}-w^{*})^{\\top}(\\widehat{\\theta}_{0\\mathrm{racle}}-\\theta)/\\langle w^{*}-\\widetilde{w},\\theta\\rangle$ to conclude that $\\widetilde{(w\\mathrm{~-~}}$ $w^{*})^{\\top}{\\widehat{\\theta}}_{\\mathtt{0r a c l e}}>0$ with probability at least $\\delta$ . Toward doing so, let $c\\in(1,1.15]$ be a constant and define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{W}\\sim\\mathcal{N}\\Big(0,\\frac{2}{\\log\\bigl(1/\\delta\\bigr)}\\Big)\\quad\\mathrm{and}\\quad W:=\\frac{\\widetilde{W}}{\\sqrt{2/\\log\\bigl(1/\\delta\\bigr)}}\\quad\\mathrm{and}\\quad\\gamma:=\\frac{c}{\\sqrt{2/\\log\\bigl(1/\\delta\\bigr)}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Observe that $W\\sim\\mathcal{N}(0,1)$ . The following analysis holds for $\\delta\\in(0,0.05]$ given that $c\\in(1,1.15]$ as assumed: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{P}\\Big(\\frac{(\\widetilde w-w^{*})^{\\top}(\\hat{\\theta}_{\\mathrm{oxtcle}}-\\theta)}{\\langle w^{*}-\\hat{w},\\theta\\rangle}\\geq c\\Big)\\geq\\mathbb{P}(\\widetilde W\\geq c)}&{}&{\\quad{\\mathrm{(By~Eq.~9)}}}\\\\ &{=\\mathbb{P}\\Big(\\frac{\\widetilde W}{\\sqrt{2/\\log(1/\\delta)}}\\geq\\frac{c}{\\sqrt{2/\\log(1/\\delta)}}\\Big)}\\\\ &{:=\\mathbb{P}(W\\geq\\gamma)}\\\\ &{\\geq\\Big(\\frac{1}{\\gamma}-\\frac{1}{\\gamma^{3}}\\Big)\\frac{1}{\\sqrt{2\\pi}}e^{-\\gamma^{2}/2}}&{\\:{\\mathrm{(Proposition~2.1.2~Vexhynin~34)}}}\\\\ &{=\\Big(\\frac{\\sqrt{2}}{c\\sqrt{\\log(1/\\delta)}}-\\frac{\\sqrt{2}^{3}}{c^{3}\\sqrt{\\log(1/\\delta)}}\\Big)\\frac{1}{\\sqrt{2\\pi}}\\delta^{\\alpha^{2}/4}}\\\\ &{>\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The final inequality can be verified computationally. Thus, with probability at least $\\delta$ for $\\delta\\in(0,0.05]$ weobtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\widetilde w-w^{*})^{\\top}\\widehat{\\theta}_{0x{\\mathrm{acle}}}\\geq c(w^{*}-\\widetilde w)^{\\top}\\theta+(\\widetilde w-w^{*})^{\\top}\\theta}\\\\ &{\\qquad\\qquad\\qquad=c(w^{*}-\\widetilde w)^{\\top}\\theta-(w^{*}-\\widetilde w)^{\\top}\\theta}\\\\ &{\\qquad\\qquad\\qquad=(c-1)(w^{*}-\\widetilde w)^{\\top}\\theta}\\\\ &{\\qquad\\qquad\\qquad\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Observe that the final inequality holds since $c>1$ and $(\\boldsymbol{w}^{*}-\\boldsymbol{\\widetilde{w}})^{\\top}\\boldsymbol{\\theta}>0$ by definition. ", "page_idx": 19}, {"type": "text", "text": "This result directly implies that with probability at least $\\delta$ for $\\delta\\in(0,0.05]$ ,thevector $\\widehat{w}$ returned by algorithm $\\boldsymbol{\\mathcal{A}}$ is not $w^{*}$ . This is a contradiction, so we conclude that if the non-adaptive oracle algorithm $\\boldsymbol{\\mathcal{A}}$ acquires $T\\leq\\sigma^{2}\\rho^{*}\\log\\left(1/\\delta\\right)/2$ samples, then $\\mathbb{P}(\\widehat{\\boldsymbol{w}}\\neq\\boldsymbol{w}^{*})\\ge\\delta$ for $\\delta\\in(0,0.05]$ ", "page_idx": 19}, {"type": "text", "text": "F.2 Proof of Corollary D.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Corollary $D.2$ There exists a problem instance characterized by $\\mathcal{W}\\subset\\mathbb{R}^{d},\\mathcal{Z}\\subset\\mathbb{R}^{d},\\Gamma\\in\\mathbb{R}^{d\\times d}$ , and $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ with a noise process satisfying Assumption 1 such that if the non-adaptive oracle algorithm acquires $T\\leq\\operatorname*{max}\\{d\\|\\theta\\|_{2}^{2},\\sqrt{d}\\|\\theta\\|_{2}\\}\\rho^{*}\\log(1/\\delta)/2$ samples, then $\\mathbb{P}(\\widehat{w}\\neq w^{*})\\ge\\delta$ for $\\delta\\in(0,0.05]$ ", "page_idx": 19}, {"type": "text", "text": "Proof. To begin, consider the specifications of Theorem D.1 and its result. That is, a problem an arbitrary instance characterized by $\\mathcal{W}\\,\\subset\\,\\mathbb{R}^{d},\\mathcal{Z}\\,\\subset\\,\\mathbb{R}^{d}$ \uff0c $\\Gamma\\in\\mathbb{R}^{d\\times d}$ , and $\\theta\\in\\mathbb{R}^{d}$ where the noise process is jointly Gaussian and defined by $\\gamma:=[\\eta\\quad\\varepsilon]\\sim\\mathcal{N}(0,\\Sigma)$ where $\\Sigma\\in\\mathbb{R}^{(d+1)\\times(d+1)}$ is an arbitrary correlation matrix. Observe that the noise process defined by $\\gamma$ satisfies Assumption 1. The result states that if the non-adaptive oracle algorithm acquires $T\\stackrel{{\\,.\\,}}{\\leq}\\sigma^{2}\\rho^{*}\\log\\left(1/\\delta\\right)/2$ samples on the problem instance where $\\sigma^{2}:=v^{\\top}\\Sigma v$ and $v:=\\left[\\theta\\ \\ 1\\right]\\in\\mathbb{R}^{d+1}$ , then $\\mathbb{P}(\\widehat{\\boldsymbol{w}}\\neq\\boldsymbol{w}^{*})\\ge\\delta$ for $\\delta\\in(0,0.05]$ . From this point, we show that there exists a parameter $\\theta$ and correlation matrix $\\Sigma$ such that $\\sigma^{2}:=v^{\\top}\\Sigma v\\geq\\operatorname*{max}\\{d\\|\\theta\\|_{2}^{2},\\sqrt{d}\\|\\theta\\|_{2}\\}$ in order to reach the stated conclusion. ", "page_idx": 19}, {"type": "text", "text": "Notation. Let $\\zeta_{\\eta_{i},\\varepsilon}\\in[-1,1]$ denote the correlation between $\\eta_{i}$ and $\\varepsilon$ for $i\\in\\{1,\\ldots,d\\}$ . Similarly, let $\\zeta_{\\eta_{i},\\eta_{j}}\\,=\\,\\zeta_{\\eta_{j},\\eta_{i}}\\,\\in\\,[-1,1]$ denote the correlation between $\\eta_{i}$ and $\\eta_{j}$ for $i\\neq j\\in\\{1,\\ldots,d\\}$ Note that the crrelation of $\\eta_{i}$ with itsefor $i\\in\\{1,\\ldots,d\\}$ .sm $\\sigma_{\\eta_{i}}^{2}=\\zeta_{\\eta_{i},\\eta_{i}}=1$ and similarlythe correlation of $\\varepsilon$ with itself is $\\sigma_{\\varepsilon}^{2}=\\zeta_{\\varepsilon,\\varepsilon}=1$ . The correlation matrix $\\Sigma$ is then given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Sigma=\\left[\\!\\!\\begin{array}{c c c c c}{\\!1}&{\\!\\zeta_{\\eta_{2},\\eta_{1}}}&{\\cdot\\cdot\\cdot}&{\\zeta_{\\eta_{d},\\eta_{1}}}&{\\zeta_{\\varepsilon,\\eta_{1}}}\\\\ {\\!\\zeta_{\\eta_{1},\\eta_{2}}}&{\\!1}&{\\cdot\\cdot\\cdot}&{\\zeta_{\\eta_{d},\\eta_{2}}}&{\\zeta_{\\varepsilon,\\eta_{2}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {\\!\\zeta_{\\eta_{1},\\eta_{d}}}&{\\zeta_{\\eta_{2},\\eta_{d}}}&{\\cdot\\cdot\\cdot}&{\\!1}&{\\zeta_{\\varepsilon,\\eta_{d}}}\\\\ {\\zeta_{\\eta_{1},\\varepsilon}}&{\\zeta_{\\eta_{2},\\varepsilon}}&{\\cdot\\cdot\\cdot}&{\\zeta_{\\eta_{d},\\varepsilon}}&{\\;1}\\end{array}\\!\\!\\right]:=\\left[\\!\\!\\begin{array}{c c}{\\!\\Sigma_{\\theta}}&{\\!\\zeta_{\\eta,\\varepsilon}\\!}\\\\ {\\!\\zeta_{\\eta,\\varepsilon}^{\\top}}&{\\!1}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{\\theta}=\\left[\\begin{array}{c c c c c}{1}&{\\zeta_{\\eta_{2},\\eta_{1}}}&{\\cdot\\cdot\\cdot}&{\\zeta_{\\eta_{d},\\eta_{1}}}\\\\ {\\zeta_{\\eta_{1},\\eta_{2}}}&{1}&{\\cdot\\cdot\\cdot}&{\\zeta_{\\eta_{d},\\eta_{2}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\zeta_{\\eta_{1},\\eta_{d}}}&{\\zeta_{\\eta_{2},\\eta_{d}}}&{\\cdot\\cdot\\cdot}&{1}\\end{array}\\right]\\in[-1,1]^{d\\times d}\\quad\\mathrm{and}\\quad\\zeta_{\\eta,\\varepsilon}=\\left[\\begin{array}{c}{\\zeta_{\\eta_{1},\\varepsilon}}\\\\ {\\zeta_{\\eta_{2},\\varepsilon}}\\\\ {\\vdots}\\\\ {\\zeta_{\\eta_{d},\\varepsilon}}\\end{array}\\right]\\in[-1,1]^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, we use the notation $\\Sigma_{\\theta,i}^{\\top}\\in\\mathbb{R}^{d}$ to denote the $i\\cdot$ -th row of $\\Sigma_{\\theta}^{\\top}$ , or equivalently the $i$ -th column of $\\Sigma_{\\theta}$ , for any $i\\in\\{1,\\ldots,d\\}$ ", "page_idx": 19}, {"type": "text", "text": "Lower Bounding Noise Variance. Given the above notation, we now work toward lower bounding $\\sigma^{2}:=v^{\\top}\\Sigma v$ . Observe that by algebraic manipulations, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{\\top}\\Sigma v:=\\left[\\!\\!\\begin{array}{l}{\\theta}\\\\ {1}\\end{array}\\!\\!\\right]^{\\top}\\left[\\!\\!\\begin{array}{l l l l l}{\\Sigma_{0}}&{\\zeta_{\\eta,\\varepsilon}}\\\\ {\\zeta_{\\eta,\\varepsilon}^{\\top}}&{1}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{l}{\\theta}\\\\ {1}\\end{array}\\!\\!\\right]}\\\\ &{\\qquad\\quad=\\left[\\!\\!\\theta^{\\top}\\Sigma_{\\theta,1}^{\\top}+\\zeta_{\\eta_{1},\\varepsilon}\\quad\\theta^{\\top}\\Sigma_{\\theta,2}^{\\top}+\\zeta_{\\eta_{2},\\varepsilon}\\quad\\cdot\\quad\\theta^{\\top}\\Sigma_{\\theta,d}^{\\top}+\\zeta_{\\eta_{d},\\varepsilon}\\quad\\theta^{\\top}\\zeta_{\\eta,\\varepsilon}+1\\right]\\left[\\!\\!\\begin{array}{l}{\\theta}\\\\ {1}\\end{array}\\!\\!\\right]}\\\\ &{\\qquad=\\theta^{\\top}\\displaystyle\\sum_{i=1}^{d}\\Sigma_{\\theta,i}^{\\top}\\theta_{i}+\\displaystyle\\sum_{i=1}^{d}\\zeta_{\\eta_{i},\\varepsilon}\\theta_{i}+\\theta^{\\top}\\zeta_{\\eta,\\varepsilon}+1}\\\\ &{\\qquad=\\theta^{\\top}\\Sigma_{\\theta}\\theta+2\\theta^{\\top}\\zeta_{\\eta,\\varepsilon}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\Sigma_{\\theta}$ is a real symmetric matrix, an eigendecomposition exists such that $\\Sigma_{\\theta}=Q\\Lambda Q^{\\top}$ where $\\Lambda=\\mathrm{diag}(\\lambda_{1},\\ldots,\\dot{\\lambda_{d}})\\in\\mathbb{R}^{d\\times d}$ is a diagonal matrix containing the eigenvalues of $\\Sigma_{\\theta}$ and $\\dot{Q}\\in\\mathbb{R}^{d\\times d}$ is an orthogonal matrix with columns corresponding to the eigenvectors of $\\Sigma_{\\theta}$ . Let $q_{i}\\;:=\\;Q_{i}^{\\top}$ denotecolumn $i$ of the matrix $Q$ for $i=\\{1,\\bar{\\dots},d\\}$ , which is equivalently eigenvector $i$ of $\\Sigma_{\\theta}$ for $i=\\{1,\\ldots,d\\}$ . Without loss of generality, assume that the eigenvectors are of unit length so that $\\|q_{i}\\|_{2}=1$ for all $i=\\{1,\\ldots,d\\}$ ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Given this information, suppose that the parameter $\\theta$ in the instance is equal to a scalar multiple of the eigenvector of $\\Sigma_{\\theta}$ corresponding to the maximum eigenvalue. Note this is equivalent to the statement that $\\theta$ is equal to some scalar multiple of the column $q_{*}\\in\\mathbb{R}^{d}$ of the matrix $Q$ where $q_{*}$ is the eigenvector of $\\Sigma_{\\theta}$ corresponding to the maximum eigenvalue $\\lambda^{*}$ . Thus, we take $\\theta=c\\cdot q_{*}$ for some $c\\in\\mathbb R$ and observe that $\\lVert\\theta\\rVert_{2}=c$ . Toward quantifying the value of $v^{\\top}\\Sigma v$ for the problem instance, we begin by characterizing $\\theta^{\\top}\\Sigma_{\\theta}\\theta$ for the choice of $\\theta$ . Consider the following analysis: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}&{\\theta^{\\top}\\Sigma_{\\theta}\\theta=\\theta^{\\top}Q\\Lambda Q^{\\top}\\theta}\\\\ &{\\qquad\\quad:=\\left(c q_{*}\\right)^{\\top}Q\\Lambda Q^{\\top}\\bigl(c q_{*}\\bigr)}&&{}&&{(\\theta:=c q_{*})}\\\\ &{\\qquad=\\|\\theta\\|_{2}^{2}q_{*}^{\\top}\\left[q_{1}\\quad\\cdots\\quad q_{*}\\quad\\cdots\\quad q_{d}\\right]\\Lambda\\bigl[q_{1}\\quad\\cdots\\quad q_{*}\\quad\\cdots\\quad q_{d}\\right]^{\\top}q_{*}}&&{}\\\\ &{\\qquad=\\|\\theta\\|_{2}^{2}\\bigl[0\\quad\\cdots\\quad\\|q_{*}\\|_{2}^{2}\\quad\\cdots\\quad0\\bigr]\\mathrm{diag}\\bigl(\\lambda_{1},\\ldots,\\lambda^{*},\\ldots,\\lambda_{1}\\bigr)\\bigl[0\\quad\\cdots\\quad\\|q_{*}\\|_{2}^{2}\\quad\\cdots\\quad0\\bigr]^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\quad}&&{}&&{(q_{i}^{\\top}q_{j}=0\\,\\forall i\\neq j)}\\\\ &{\\qquad=\\|\\theta\\|_{2}\\lambda^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, in general for this choice of $\\theta$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{\\top}\\Sigma_{\\theta}v=\\lVert\\theta\\rVert_{2}^{2}\\lambda^{*}+2\\theta^{\\top}\\zeta_{\\eta,\\varepsilon}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To conclude, take $\\Sigma_{\\theta}:=\\mathbf{1}_{d}\\mathbf{1}_{d}^{\\top}$ where $\\mathbf{1}_{d}$ represents the $d$ -dimensional vector of all ones. Since this is a rank-1 matrix, the maximum eigenvalue is $\\lambda^{*}=\\mathbf{1}_{d}^{\\top}\\mathbf{1}_{d}=d$ and the remainder of the eigenvalues are zero. Observe that $q_{*}=\\mathbf{1}_{d}/\\sqrt{d}$ is an eigenvector corresponding to the maximum eigenvalue since $\\mathbf{1}_{d}\\mathbf{1}_{d}^{\\top}q_{*}=d q_{*}$ . Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{v^{\\top}\\Sigma_{\\theta}v=\\|\\theta\\|_{2}^{2}\\lambda^{*}+2\\theta^{\\top}\\zeta_{\\eta,\\varepsilon}+1}\\\\ &{\\qquad\\quad:=\\|\\theta\\|_{2}^{2}\\lambda^{*}+2\\|\\theta\\|_{2}{\\bf1}_{d}^{\\top}{\\bf1}_{d}/\\sqrt{d}+1\\qquad}&&{(\\theta:=c q_{*}:=\\|\\theta\\|_{2}{\\bf1}_{d}/\\sqrt{d}=\\mathrm{and}\\;\\zeta_{\\eta,\\varepsilon}:={\\bf1}_{d})}\\\\ &{\\qquad\\quad=d\\|\\theta\\|_{2}^{2}+2\\sqrt{d}\\|\\theta\\|_{2}+1}\\\\ &{\\qquad\\quad\\geq\\operatorname*{max}\\{d\\|\\theta\\|_{2}^{2},\\sqrt{d}\\|\\theta\\|_{2}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This completes the proof since we have shown that there exists a parameter $\\theta$ and correlation matrix $\\Sigma$ such that $\\sigma^{2}:=v^{\\top}\\Sigma v\\geq\\operatorname*{max}\\{d\\|\\theta\\|_{2}^{2},\\sqrt{d}\\|\\theta\\|_{2}\\}$ , which by Theorem D.1 allows us to make the stated conclusion. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "G Proofs of the confidence interval ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "G.1 Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The first statement is an immediate consequence of Lemma G.2 and the second statement is proven in Lemma G.1. ", "page_idx": 20}, {"type": "text", "text": "Lemma G.1. In the compliance model,the noise $\\eta^{\\top}\\theta+\\varepsilon$ follows a $\\left(8\\|\\theta\\|_{2}^{2}+2\\right)$ -sub-Gaussian distribution. ", "page_idx": 20}, {"type": "text", "text": "Proof. In compliance, we have $z,x\\in\\{e_{1},\\cdot\\cdot\\cdot,e_{d}\\}$ , and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta=x-\\left(\\mathbb{P}(e_{1}\\mid z),\\cdot\\cdot\\cdot\\,,\\mathbb{P}(e_{d}\\mid z)\\right)^{\\top}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us figure out the sub-Gaussian parameter of the random vector $\\eta$ . Fix any unit vector $a$ .First,we have $\\mathbb{E}[\\tilde{\\langle\\eta,a\\rangle}]=0$ . Second, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\eta^{\\top}a\\right\\|\\leq\\left\\|\\eta\\right\\|_{2}}}&{\\quad\\mathrm{(Cauchy-Schwarz~inequality)}}\\\\ &{\\leq\\left\\|x-\\left(\\mathbb{P}(e_{1}\\mid z),\\cdots,\\mathbb{P}(e_{d}\\mid z)\\right)^{\\top}\\right\\|_{2}}\\\\ &{\\leq\\Bigg(\\|x\\|_{2}+\\left\\|\\left(\\mathbb{P}(e_{1}\\mid z),\\cdots,\\mathbb{P}(e_{d}\\mid z)\\right)^{\\top}\\right\\|_{2}\\Bigg)}\\\\ &{\\leq\\Bigg(1+\\left\\|\\left(\\mathbb{P}(e_{1}\\mid z),\\cdots,\\mathbb{P}(e_{d}\\mid z)\\right)^{\\top}\\right\\|_{1}\\Bigg)\\quad(x\\in\\left\\{e_{1},\\cdots,e_{d}\\right\\}\\mathrm{and~}\\|x\\|_{2}\\leq\\|x\\|_{1},\\forall x)}\\\\ &{\\leq\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, $\\eta^{\\top}a$ is bounded and zero-mean and thus $2^{2}$ -sub-Gaussian. This implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall\\beta,\\operatorname*{max}_{a:\\|a\\|\\leq1}\\mathbb{E}[\\exp\\bigl(\\beta\\langle\\eta,a\\rangle\\bigr)]\\leq\\exp\\left(\\frac{\\beta^{2}2^{2}}{2}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and thus $\\eta$ is a $2^{2}$ -sub-Gaussan random vector. Then, $\\eta^{\\top}\\theta$ is $(2\\|\\theta\\|)^{2}$ -sub-Gaussian. ", "page_idx": 21}, {"type": "text", "text": "Using Lemma G.2, we have that $\\eta^{\\top}\\theta+\\varepsilon$ is $2(4\\|\\theta\\|^{2}+1)$ -sub-Gaussian. ", "page_idx": 21}, {"type": "text", "text": "Lemma G.2. Let $A$ and $B$ random variables that are each $\\sigma_{A}^{2}$ and $\\sigma_{B}^{2}$ -sub-Gaussian but are correlated. Then, $A+B$ .s $2(\\sigma_{A}^{2}+\\sigma_{B}^{2})$ -sub-Gaussian. ", "page_idx": 21}, {"type": "text", "text": "Proof. By definition of sub-Gaussian, we have for any $\\gamma\\in\\mathbb{R}$ ", "page_idx": 21}, {"type": "text", "text": "(Cauchy-Schwarz) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[\\exp\\bigl(\\gamma(A+B)\\bigr)\\Big]=\\mathbb{E}\\big[\\exp(\\gamma A)\\exp(\\gamma B)\\big]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\mathbb{E}\\big[\\exp(2\\gamma A)\\big]}\\sqrt{\\mathbb{E}\\big[\\exp(2\\gamma B)\\big]}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\exp\\bigl(2\\gamma^{2}\\sigma_{A}^{2}\\bigr)}\\sqrt{\\exp\\bigl(2\\gamma^{2}\\sigma_{B}^{2}\\bigr)}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp\\Bigl(2\\gamma^{2}(\\sigma_{A}^{2}+\\sigma_{B}^{2})\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "G.2Proof of Lemma 2.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 2.2. Suppose that $T$ observations are collected non-adaptively from the structural equation model in Eqs. (1) and $\\Gamma\\in\\mathbb{R}^{d\\times d}$ is known. Then, with probability at least $1-\\delta$ for $\\delta\\in(0,1)$ and $w\\in\\mathbb{R}^{d}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n|w^{\\top}(\\widehat{\\theta}_{\\mathrm{oracle}}-\\theta)|\\leq\\sqrt{2\\sigma_{\\nu}^{2}\\|w\\|_{\\bar{A}(Z_{T},\\Gamma)^{-1}}\\log\\bigl(2/\\delta\\bigr)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\sigma_{\\nu}^{2}$ is the sub-Gaussian parameter of the noise process $\\boldsymbol{\\nu}:=\\boldsymbol{\\eta}^{\\top}\\boldsymbol{\\theta}+\\varepsilon$ as characterized in Lemma 2.1. ", "page_idx": 21}, {"type": "text", "text": "Proof. Given the knowledge of $\\Gamma$ , we have the oracle 2SLS estimator ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{\\sf o r a c l e}=\\left(\\sum_{t=1}^{T}z_{s}\\left(\\Gamma^{\\top}z_{s}\\right)^{\\top}\\right)^{-1}\\sum_{t=1}^{T}z_{s}y_{t}=\\left(\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\Gamma\\right)^{-1}\\sum_{t=1}^{T}z_{s}y_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\ny_{t}=x_{t}^{\\top}\\theta+\\varepsilon_{t}=\\Big(\\Gamma^{\\top}z_{t}\\Big)^{\\top}\\theta+\\eta_{t}^{\\top}\\theta+\\varepsilon_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Denote $\\nu_{t}:=\\eta_{t}^{\\top}\\theta+\\varepsilon_{t}$ . For any $w\\in\\mathcal{W}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg\\langle\\widehat{\\theta}_{n e a k}-\\theta,w\\bigg\\rangle=\\Bigg\\langle\\Bigg(\\displaystyle\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\Bigg)^{-1}\\sum_{t=1}^{T}z_{s}y_{t}-\\theta,w\\Bigg\\rangle}\\\\ {=\\Bigg\\langle\\left(\\displaystyle\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\right)^{-1}\\sum_{t=1}^{T}z_{s}\\left(z_{s}^{\\top}\\nabla\\theta+\\nu_{t}\\right)-\\theta,w\\Bigg\\rangle}\\\\ {=\\Bigg\\langle\\left(\\displaystyle\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\right)^{-1}\\left(\\displaystyle\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\nabla\\theta+\\displaystyle\\sum_{t=1}^{T}z_{s}z_{s}\\nu_{t}\\right)-\\theta,w\\Bigg\\rangle}\\\\ {=\\Bigg\\langle\\left(\\displaystyle\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\right)^{-1}\\sum_{t=1}^{T}z_{s}z_{s}\\nu_{s}w\\Bigg\\rangle}\\\\ {=\\displaystyle\\frac{1}{\\sigma_{\\mathrm{pr}}^{2}}\\Bigg\\langle\\left(\\displaystyle\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\right)^{-1}z_{s}\\nu_{s}\\Bigg\\rangle\\nu_{\\sigma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma G.1, the noise $\\nu_{t}$ is $\\sigma_{\\nu}^{2}$ -sub-Gaussian, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Biggl\\langle\\left(\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\Gamma\\right)^{-1}z_{q},w\\Biggr\\rangle\\nu_{q}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "1S $\\begin{array}{r}{\\bigg\\langle\\Big(\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\Gamma\\Big)^{-1}z_{q},w\\Bigg\\rangle^{2}\\sigma_{\\nu}^{2}.}\\end{array}$ -sub-Gaussian. Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\langle\\widehat{\\theta}_{\\mathrm{oracle}}-\\theta,w\\right\\rangle\\geq\\sqrt{2\\sum_{q=1}^{t}\\biggl\\langle\\left(\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\Gamma\\right)^{-1}z_{q},w\\biggr\\rangle^{2}\\sigma_{\\nu}^{2}\\log\\left(\\frac{1}{\\delta}\\right)}\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Concisely, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{q=1}{\\overset{t}{\\sum}}\\Bigg\\langle\\left(\\underset{t=1}{\\overset{T}{\\sum}}z_{s}z_{s}^{\\top}\\Gamma\\right)^{-1}z_{q},w\\right\\rangle^{2}=w^{\\top}\\left(\\underset{t=1}{\\overset{T}{\\sum}}z_{s}z_{s}^{\\top}\\Gamma\\right)^{-1}\\left(\\underset{q=1}{\\overset{t}{\\sum}}z_{q}z_{q}^{\\top}\\right)\\left(\\left(\\underset{t=1}{\\overset{T}{\\sum}}z_{s}z_{s}^{\\top}\\Gamma\\right)^{-1}\\right)^{\\top}w}\\\\ &{=w^{\\top}\\Gamma^{-1}\\left(\\left(\\underset{t=1}{\\overset{T}{\\sum}}z_{s}z_{s}^{\\top}\\Gamma\\right)^{-1}\\right)^{\\top}w}\\\\ &{=w^{\\top}\\left(\\Gamma^{\\top}\\left(\\underset{t=1}{\\overset{T}{\\sum}}z_{s}z_{s}^{\\top}\\right)\\Gamma\\right)^{-1}w.}&{(11)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\langle\\widehat{\\theta}_{\\mathrm{oracle}}-\\theta,w\\right\\rangle\\geq\\sqrt{2w^{\\top}\\left(\\Gamma^{\\top}\\left(\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\right)\\Gamma\\right)^{-1}w\\sigma_{\\nu}^{2}\\log\\left(\\frac{1}{\\delta}\\right)}\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can further write it as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\langle\\widehat{\\theta}_{\\mathrm{oracle}}-\\theta,w\\right\\rangle\\geq\\sqrt{2\\left\\|w\\right\\|_{\\left(\\Gamma^{\\top}\\left(\\sum_{t=1}^{T}z_{s}z_{s}^{\\top}\\right)\\Gamma\\right)^{-1}}\\sigma_{\\nu}^{2}\\log\\left(\\frac{1}{\\delta}\\right)}\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By taking a union bound over, we have the confidence interval for the absolute value as in the statement of the lemma. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "G.3Proof of Theorem 2.3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 2.3. Suppose that $\\widehat{\\Gamma}$ is estimated through a design matrix $Z_{T_{1}}\\,\\in\\,\\mathbb{R}^{T_{1}\\times d}$ and $\\widehat{\\theta}_{\\mathrm{P}-2\\mathrm{SLS}}$ .s estimated through a design matrix $Z_{T_{2}}\\in\\mathbb{R}^{T_{2}\\times d}$ . Then, for any $w\\in\\mathcal{W}$ , with probability at least $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n|w^{\\top}(\\widehat{\\theta_{\\mathtt{P}-2\\mathrm{SIS}}}-\\theta)|\\leq\\|w\\|_{\\tilde{A}(Z_{T_{2}},\\widehat{\\Gamma})^{-1}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\biggl(\\frac{4}{\\delta}\\biggr)}+\\|w\\|_{\\tilde{A}(Z_{T_{1}},\\widehat{\\Gamma})^{-1}}\\|\\theta\\|_{2}\\sqrt{\\sigma_{\\eta}^{2}\\overline{{\\log\\bigl(Z_{T_{1}},\\delta/4\\bigr)}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\sigma_{\\nu}^{2}$ is the sub-Gaussian parameter of the noise $\\nu:=\\eta^{\\top}\\theta+\\varepsilon,\\sigma_{\\eta}^{2}$ is the sub-Gaussian parameter of the noise $\\eta$ , and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\overline{{\\log}}(Z_{T},\\delta):=8d\\ln\\left(1+\\frac{2T L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(Z_{T_{1}}^{\\top}Z_{T_{1}}))}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta}\\cdot\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(Z_{T_{1}}^{\\top}Z_{T_{1}})}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For the pseudo 2SLS estimator, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\theta}_{p-2\\boxtimes\\lambda}-\\theta=\\left(\\displaystyle\\sum_{t=1}^{T_{2}}{z_{I}z_{I}\\underline{{\\tau}}_{\\bar{I}}\\hat{\\tau}}\\right)^{-1}\\displaystyle\\sum_{t=1}^{T_{2}}{z_{I}z_{I}}y_{I}-\\theta}\\\\ &{=\\left(\\displaystyle\\sum_{t=1}^{T_{2}}{z_{I}z_{I}\\underline{{\\tau}}_{\\bar{I}}\\hat{\\tau}}\\right)^{-1}\\displaystyle\\sum_{t=1}^{T_{2}}{z_{I}z_{I}\\left(z_{I}^{T}\\Gamma\\theta+\\nu_{t}\\right)}-\\theta}\\\\ &{=\\left(\\displaystyle\\sum_{t=1}^{T_{2}}{z_{I}z_{I}\\underline{{\\tau}}_{\\bar{I}}\\hat{\\tau}}\\right)^{-1}\\left(\\displaystyle\\sum_{t=1}^{T_{2}}{z_{I}z_{I}\\underline{{\\tau}}_{\\bar{I}}^{T}\\left[\\theta+\\displaystyle\\sum_{t=1}^{T_{2}}{z_{I}\\nu_{t}}\\right]}-\\theta\\right.}\\\\ &{=\\left(\\displaystyle\\sum_{t=1}^{T_{2}}{z_{I}z_{I}\\underline{{\\tau}}_{\\bar{I}}\\hat{\\tau}}\\right)^{-1}\\displaystyle\\sum_{t=1}^{T_{2}}{z_{I}z_{I}\\nu_{t}}+\\left(\\hat{\\Gamma}^{-1}\\Gamma-I\\right)\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For any $w\\in\\mathcal{W}$ ,we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\widehat{\\theta}_{r>2\\mathrm{t}{s}-\\theta,\\,w}\\right\\rangle=\\left\\langle\\left(\\sum_{t=1}^{T_{2}}\\tau_{I,t}\\frac{\\tau}{L_{t}\\Gamma}\\right)^{-1}\\sum_{t=1}^{T_{2}}\\tau_{I,t}\\nu_{t}+\\left(\\widehat{\\Gamma}^{-1}\\Gamma-I\\right)\\theta,w\\right\\rangle}\\\\ &{\\quad=\\left\\langle\\left(\\sum_{t=1}^{T_{2}}z_{I,t}\\frac{\\tau}{L_{t}\\Gamma}\\right)^{-1}\\sum_{t=1}^{T_{2}}z_{I,t}\\nu_{t},w\\right\\rangle+\\left\\langle\\left(\\widehat{\\Gamma}^{-1}\\Gamma-I\\right)\\theta,w\\right\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{q=1}^{T_{2}}\\left\\langle\\left(\\sum_{t=1}^{T_{2}}z_{I,t}\\frac{\\tau}{L_{t}\\Gamma}\\right)^{-1}z_{I,t},w\\right\\rangle\\nu_{q}+\\left\\langle\\left(\\widehat{\\Gamma}^{-1}\\Gamma-I\\right)\\theta,w\\right\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{q=1}^{T_{2}}\\left\\langle\\left(\\sum_{t=1}^{T_{2}}z_{I,t}\\frac{\\tau}{L_{t}\\Gamma}\\right)^{-1}z_{I,t},w\\right\\rangle\\nu_{q}+\\left\\langle\\left(\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}\\right)\\Gamma\\theta,w\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We upper bound the first term and the second term separately.  For the first term, by Lemma 2.1, we know $\\nu_{t}$ $\\sigma_{\\nu}^{2}$ subGaussian,we have $\\begin{array}{r}{\\sum_{q=1}^{T_{2}}\\biggl\\langle\\Bigl(\\sum_{t=1}^{T_{2}}z_{I_{t}}z_{I_{t}}^{\\top}\\widehat\\Gamma\\Bigl)^{-1}z_{I_{q}},w\\biggr\\rangle\\nu_{q}}\\end{array}$ $\\begin{array}{r}{\\sum_{q=1}^{T_{2}}\\biggl\\langle\\Bigl(\\sum_{t=1}^{T_{2}}z_{I_{t}}z_{I_{t}}^{\\top}\\widehat\\Gamma\\Bigl)^{-1}z_{I_{q}},w\\biggr\\rangle^{2}\\sigma_{\\nu}^{2}}\\end{array}$ -subGaussian. Thus by the concentration inequality of sub", "page_idx": 23}, {"type": "text", "text": "Gaussian random variables, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n>\\left(\\sum_{q=1}^{T_{2}}\\Biggl\\langle\\left(\\sum_{t=1}^{T_{2}}z_{I_{t}}z_{I_{t}}^{\\top}\\widehat\\Gamma\\right)^{-1}z_{I_{q}},w\\Biggr\\rangle\\nu_{q}\\geq\\sqrt{2\\sum_{q=1}^{T_{2}}\\Biggl\\langle\\left(\\sum_{t=1}^{T_{2}}z_{I_{t}}z_{I_{t}}^{\\top}\\widehat\\Gamma\\right)^{-1}z_{I_{q}},w\\Biggr\\rangle^{2}\\sigma_{\\nu}^{2}\\log\\left(\\frac{2}{\\delta}\\right)}\\Biggr)\\leq\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Bysimilaallatons weavewith prbabiltya t $\\textstyle1-{\\frac{\\delta}{2}}$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{q=1}{\\overset{T_{2}}{\\sum}}\\Bigg\\langle\\left(\\underset{t=1}{\\overset{T_{2}}{\\sum}}z_{I_{t}}z_{I_{t}}^{\\top}\\widehat\\Gamma\\right)^{-1}z_{I_{q}},w\\Bigg\\rangle\\nu_{q}\\leq\\sqrt{2w^{\\top}\\left(\\underset{t=1}{\\overset{T_{2}}{\\sum}}z_{I_{t}}z_{I_{t}}^{\\top}\\right)\\underset{\\Gamma}{\\cap}\\right)^{-1}w\\sigma_{\\nu}^{2}\\log\\left(\\frac{2}{\\delta}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{2\\|w\\|_{\\bar{A}(Z_{T_{2}},\\widehat\\Gamma)^{-1}}^{2}\\sigma_{\\nu}^{2}\\log\\left(\\frac{2}{\\delta}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\langle\\widehat{\\theta}_{\\mathrm{P-2SLS}}-\\theta,w\\right\\rangle\\leq\\|w\\|_{\\bar{A}(Z_{T_{2}},\\widehat{\\Gamma})^{-1}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\left(\\frac{2}{\\delta}\\right)}+\\left\\langle\\left(\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}\\right)\\Gamma\\theta,w\\right\\rangle\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By TheoremG3, we have with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bigg\\langle\\Big(\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}\\Big)\\Gamma\\theta,w\\bigg\\rangle\\leq\\|w\\|_{\\bar{A}(Z_{T_{1}},\\widehat{\\Gamma})^{-1}}\\|\\theta\\|\\sqrt{\\sigma_{\\eta}^{2}\\overline{{\\log}}\\big(Z_{T_{1}},\\delta/2\\big)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining (13) and (14), we have with probability at least $1-\\delta$ , for any $w\\in\\mathscr{W}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\langle\\widehat{\\theta}_{\\mathtt{P-2S L S}}-\\theta,w\\right\\rangle\\leq\\|w\\|_{\\bar{A}(Z_{T_{2}},\\widehat{\\Gamma})^{-1}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\biggl(\\frac{2}{\\delta}\\biggr)}+\\|w\\|_{\\bar{A}(Z_{T_{1}},\\widehat{\\Gamma})^{-1}}\\|\\theta\\|\\sqrt{\\sigma_{\\eta}^{2}\\overline{{\\log\\bigl(Z_{T_{1}},\\delta/2\\bigr)}}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By a union bound, we have the confidence interval for the absolute value of the inner product, ", "page_idx": 24}, {"type": "equation", "text": "$$\n|w^{\\top}(\\widehat{\\theta_{\\mathtt{P}-2\\mathrm{SIS}}}-\\theta)|\\leq\\|w\\|_{\\bar{A}(Z_{T_{2}},\\widehat{\\Gamma})^{-1}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\biggl(\\frac{4}{\\delta}\\biggr)}+\\|w\\|_{\\bar{A}(Z_{T_{1}},\\widehat{\\Gamma})^{-1}}\\|\\theta\\|\\sqrt{\\sigma_{\\eta}^{2}\\overline{{\\log\\bigl(Z_{T_{1}},\\delta/4\\bigr)}}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Theorem G.3. Suppose that the least square estimator $\\widehat{\\Gamma}$ is estimated through a design matrix $Z_{T_{1}}\\in\\mathbb{R}^{T_{1}\\times d}$ , then it satisfes, with probability at least $1-\\delta_{i}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\langle\\left(\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}\\right)\\Gamma\\theta,w\\right\\rangle\\leq\\|w\\|_{\\bar{A}(Z_{T_{1}},\\widehat{\\Gamma})^{-1}}\\|\\theta\\|\\sqrt{\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for any $w\\in\\mathscr{W}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Define $V=Z_{T_{1}}^{\\top}Z_{T_{1}}$ \uff0c $S\\in\\mathbb{R}^{T_{1}\\times d}$ as the matrix with $i$ -th row being $\\eta_{i}^{\\top}$ , the stacked noise, i.e. the data collection process of the design matrix $Z_{T_{1}}$ is $X=Z_{T_{1}}\\Gamma+S$ . Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\Bigl(\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}\\Bigr)\\Gamma\\theta,w\\right\\rangle=w^{\\top}\\Bigl(\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}\\Bigr)\\Gamma\\theta}\\\\ &{\\qquad\\qquad\\qquad\\qquad=w^{\\top}\\Bigl(\\Gamma+V^{-1}Z_{T_{1}}^{\\top}S\\Bigr)^{-1}V^{-1}Z_{T_{1}}^{\\top}S\\Gamma^{-1}\\Gamma\\theta}\\\\ &{\\qquad\\qquad\\qquad\\qquad=w^{\\top}\\widehat{\\Gamma}^{-1}V^{-1/2}V^{-1/2}Z_{T_{1}}^{\\top}S\\theta}\\\\ &{\\qquad\\qquad\\qquad\\quad\\leq\\Bigl\\|w^{\\top}\\widehat{\\Gamma}^{-1}V^{-1/2}\\Bigr\\|\\Bigl\\|V^{-1/2}Z_{T_{1}}^{\\top}S\\theta\\Bigr\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|w\\|_{\\tilde{\\cal A}(Z_{T_{1}},\\widehat{\\Gamma})^{-1}}\\Bigr\\|V^{-1/2}Z_{T_{1}}^{\\top}S\\Bigr\\|_{\\infty}\\|\\theta\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|w\\|_{\\tilde{\\cal A}(Z_{T_{1}},\\widehat{\\Gamma})^{-1}}\\|\\theta\\|\\sqrt{\\sigma_{q}^{2}\\overline{{\\log\\left(Z_{T_{1}},\\delta\\right)}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality is due to Lemma G.4. ", "page_idx": 24}, {"type": "text", "text": "Lemma G.4. Suppose we have $z_{1},\\dots,z_{T}\\,\\,\\,\\in\\,\\,\\,\\mathbb{R}^{d}$ and $\\eta_{1},\\dots,\\eta_{T}\\quad\\in\\ \\ \\mathbb{R}^{d}$ such that $\\eta_{T}~\\vert$ $z_{1},\\eta_{1},\\dots,z_{T-1},\\eta_{T-1},z_{T}$ .s $\\sigma_{\\eta}^{2}$ -sub-Gaussian vector (defined in Assumption $^{\\,l}$ ). Let $Z,S\\in\\mathbb{R}^{T\\times d}$ be matrices whose $t$ -th row is $z_{t}^{\\top}$ and $\\eta_{t}^{\\top}$ respectively. Suppose $\\|z_{t}\\|\\leq L_{z},\\forall t$ Let $V=Z^{\\top}Z$ Then, $\\forall\\delta\\in(0,1)$ ,we have, with probability at least $1-\\delta$ \uff0c ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|V^{-1/2}Z_{T}^{\\top}S\\right\\|_{o p}\\leq\\sigma_{\\eta}\\sqrt{8d\\ln\\left(1+\\frac{2T L_{z}^{2}}{d\\left(2\\wedge\\sigma_{\\operatorname*{min}}(V)\\right)}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta}\\cdot\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(V)}\\right)\\right)}.}\\\\ &{\\textit{F a b r e v i a t e}\\overline{{\\log}}(Z_{T},\\delta):=8d\\ln\\left(1+\\frac{2T L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(V))}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta}\\cdot\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(V)}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. By the definition of operator norm, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|V^{-1/2}Z_{T}^{\\top}S\\right|\\right|_{\\mathrm{op}}=\\underset{\\left\\{x|\\|x\\|_{2}=1\\right\\}}{\\operatorname*{sup}}\\left\\|V^{-1/2}Z_{T}^{\\top}S x\\right\\|_{2}}\\\\ &{\\qquad\\qquad=\\underset{\\left\\{x|\\|x\\|_{2}=1\\right\\}}{\\operatorname*{sup}}\\sqrt{x^{\\top}S^{\\top}Z_{T}V^{-1}Z_{T}^{\\top}S x}}\\\\ &{\\qquad\\qquad=\\underset{\\left\\{x|\\|x\\|_{2}=1\\right\\}}{\\operatorname*{sup}}\\left\\|Z_{T}^{\\top}S x\\right\\|_{V^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Considering a fixed $w\\in\\mathbb{R}^{d}$ , by Lemma G.5, we have with probability at least $1-\\delta$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\Vert Z_{T}^{\\top}S x\\right\\Vert_{V^{-1}}\\leq\\sqrt{2}\\sigma_{\\eta}\\sqrt{d\\ln\\left(1+\\frac{2T L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(V))}\\right)+2\\ln\\left(\\frac{2}{\\delta}{\\cdot}\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(V)}\\right)\\right)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma G.6 and a union bound, for the $\\varepsilon$ -covering $\\mathcal{C}_{\\varepsilon}$ , we have the following event happens with probability no more than $\\delta$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\circ\\left\\{\\exists x\\in\\mathcal{C}_{\\varepsilon},\\left\\|Z_{T}^{\\top}S x\\right\\|_{V^{-1}}\\ge\\sqrt{2}\\sigma_{\\eta}\\sqrt{d\\ln\\left(1+\\frac{2T L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(V))}\\right)+2\\ln\\left(\\frac{2|\\mathcal{C}_{\\varepsilon}|}{\\delta}.\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(V)}\\right)\\right)}\\right.}\\\\ &{\\left.\\mathrm{abbreviate}\\;\\;\\widehat{\\log}(Z_{T},\\delta)\\;\\;:=\\;\\;\\sqrt{2}\\sqrt{d\\ln\\left(1+\\frac{2T L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(V))}\\right)+2\\ln\\left(\\frac{2|\\mathcal{C}_{\\varepsilon}|}{\\delta}.\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(V)}\\right)\\right)}\\right.}\\\\ &{\\begin{array}{r l}{.}\\end{array}}\\end{array}\n$$We ", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "When $\\mathcal{E}$ does not happen, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|V^{-1/2}Z^{\\top}S\\right\\|_{\\infty}=\\textstyle\\operatorname*{sup}_{\\textstyle\\left\\{\\pi\\left\\|x\\right\\|=c_{r}\\right\\}}\\left\\|Z_{T}^{\\top}S x\\right\\|_{V^{-1}}}}\\\\ &{=\\textstyle\\operatorname*{sup}_{\\textstyle\\left\\{\\pi\\left\\|x\\right\\|=1\\right\\}}\\operatorname*{min}_{\\textstyle\\left\\|y\\right\\|=c_{r}}\\left\\|Z_{T}^{\\top}S(x-y+y)\\right\\|_{V^{-1}}}\\\\ &{\\leq\\textstyle\\operatorname*{sup}_{\\textstyle\\left\\{\\pi\\left\\|x\\right\\|=1\\right\\}}\\left\\{\\left\\|Z_{T}^{\\top}S(x-y)\\right\\|_{V^{-1}}+\\left\\|Z_{T}^{\\top}S y\\right\\|_{V^{-1}}\\right\\}}\\\\ &{\\leq\\textstyle\\operatorname*{sup}_{\\textstyle\\left\\{\\pi\\left\\|y\\right\\|=c_{r}\\right\\}}\\left(\\left\\|V^{-1/2}Z^{\\top}S\\right\\|_{\\infty}\\!\\!\\!\\left\\|_{V^{-1}}+\\left\\|Z_{T}^{\\top}S y\\right\\|_{V^{-1}}\\right)}\\\\ &{\\leq\\textstyle\\operatorname*{sup}_{\\textstyle\\left\\{\\pi\\left\\|x\\right\\|=1\\right\\}}\\left\\{\\left\\|V^{-1/2}Z^{\\top}S\\right\\|_{\\infty}\\!\\!\\left\\|_{\\infty}\\!\\left\\|x-y\\right\\|_{2}+\\sigma_{\\eta}\\sqrt{\\log}(Z_{T},\\delta)\\right\\}}\\\\ &{\\leq\\varepsilon\\left\\|V^{-1/2}Z_{T}^{\\top}S\\right\\|_{\\infty}+\\sigma_{\\eta}\\widehat{\\log}(Z_{T},\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|V^{-1/2}Z_{T}^{\\top}S\\right\\|_{\\mathrm{op}}\\leq\\frac{\\sigma_{\\eta}}{1-\\varepsilon}\\widehat{\\log}(Z_{T},\\delta).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By choosing $\\varepsilon=\\textstyle{\\frac{1}{2}}$ and Lemma G.6, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|V^{-1/2}Z_{T}^{\\top}S\\right\\|_{\\mathrm{op}}\\leq\\sigma_{\\eta}\\sqrt{8d\\ln\\left(1+\\frac{2T L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(V))}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta}{\\cdot}\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(V)}\\right)\\right)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma G.5. [Self-Normalized Bound for Vector-Valued Martingales] Suppose we have $z_{1},\\dotsc..\\ ,z_{t}\\in$ $\\mathbb{R}^{d}$ and $\\eta_{1},\\dots,\\eta_{t}\\in\\mathbb{R}^{d}$ such that $\\eta_{t}\\mid z_{1},\\eta_{1},\\ldots,z_{t-1},\\eta_{t-1},z_{t}$ $\\sigma_{\\eta}^{2}$ sub-Gaussianvector(defined in Assumption $^{\\,I}$ ). Let $Z$ $\\dot{\\mathbf{\\eta}},S\\in\\mathbb{R}^{t\\times d}$ be matrices whose $s$ -th row is $z_{s}^{\\top}$ and $\\eta_{s}^{\\top}$ respectively. Suppose $\\|z_{s}\\|\\le L_{z},\\forall s$ Let $V_{t}=Z^{\\top}Z$ Then, $\\forall b\\in\\mathbb{R}^{t},\\delta\\in(0,1)$ wehave,withprobabilityat least $1-\\delta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left.\\begin{array}{l}{\\displaystyle{\\mathcal{\\nmid}}\\geq1,\\left\\|Z^{\\top}S b\\right\\|_{V_{t}^{-1}}\\leq\\sqrt{2}\\|b\\|_{2}\\sigma_{\\eta}\\sqrt{d\\ln\\left(1+\\frac{2t L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(V_{t}))}\\right)+2\\ln\\left(\\frac{2}{\\delta}{\\cdot}\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(V_{t})}\\right)\\right)}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Since each row of $S$ is a $\\sigma_{\\eta}^{2}$ subGaussan vector wehave at $(S b)_{i}/\\Vert b\\Vert$ .s $\\sigma_{\\eta}^{2}$ subGaussian. Using Lemma G.7 with $\\varepsilon_{s}=(S b)_{i}/\\vert\\vert b\\vert\\vert$ completes the proof. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Lemma G.6. [26][Lemma 20.1] There exists a set $\\mathcal{C}_{\\varepsilon}\\subset\\mathbb{R}^{d}$ with $\\begin{array}{r}{|\\mathcal{C}_{\\varepsilon}|\\leq\\left(\\frac{3}{\\varepsilon}\\right)^{d}}\\end{array}$ such that for any $x\\in\\left\\{x\\mid x\\in\\mathbb{R}^{d},\\|x\\|_{2}=1\\right\\}$ there exists a $y\\in{\\mathcal{C}}_{\\varepsilon}$ suchthat $\\left\\|x-y\\right\\|_{2}\\leq\\varepsilon$ ", "page_idx": 26}, {"type": "text", "text": "Lemma G.7. Let $z_{1},z_{2},.\\,.\\,.\\in\\{z\\in\\mathbb{R}^{d}:\\|z\\|_{2}\\leq L_{z}\\}$ and $\\varepsilon_{1},\\varepsilon_{2},\\ldots\\in\\mathbb{R}$ be random variables such that $\\varepsilon_{k}\\mid z_{1},\\varepsilon_{1},\\ldots,z_{t-1},\\varepsilon_{t-1},z_{t}$ .s $\\sigma_{\\varepsilon}^{2}$ sub-Gaussian Let $\\begin{array}{r}{V_{t}=\\sum_{s=1}^{t}z_{s}z_{s}^{\\top}}\\end{array}$ . Then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\cdot-\\delta\\leq\\mathbb{P}\\left(\\forall t\\geq1,\\left\\|\\displaystyle\\sum_{s=1}^{t}z_{s}\\varepsilon_{s}\\right\\|_{V_{t}^{-1}}\\leq\\sqrt{2}\\sigma_{\\varepsilon}\\sqrt{d\\ln\\left(1+\\frac{2t L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(V_{t}))}\\right)+2\\ln\\left(\\frac{2}{\\delta}\\cdot\\log_{2}^{2}\\left(\\frac{\\pi}{2\\wedge\\sigma_{\\operatorname*{max}}(V_{t})}\\right)\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Let $Z_{t}\\in\\mathbb{R}^{t\\times d}$ be the design matrix and define $\\varepsilon_{t}:=(\\varepsilon_{1},\\ldots,\\varepsilon_{t})^{\\top}\\in\\mathbb{R}^{t}$ . Let us omit the subscript $t$ from $Z_{t}$ \uff0c $\\varepsilon_{t}$ and $V_{t}$ . Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|Z^{\\top}\\varepsilon\\|_{V^{-1}}=\\|Z^{\\top}\\varepsilon\\|_{(\\frac{1}{2}V+\\frac{1}{2}V)^{-1}}}&{\\leq\\|Z^{\\top}\\varepsilon\\|_{(\\frac{1}{2}V+\\frac{1}{2}\\sigma_{\\operatorname*{min}}(V)I)^{-1}}\\leq\\sqrt{2}\\|Z^{\\top}\\varepsilon\\|_{(V+\\sigma_{\\operatorname*{min}}(V)I)^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Itremainsto bound $\\|Z^{\\top}\\varepsilon\\|_{(V+\\sigma_{\\operatorname*{min}}(V)I)^{-1}}$ We use union bound with the standard self-normalized inequality of Lattimore and Szepesvari [26][Theorem 20.4 and Note 20.2]. Specifically, let $\\lambda_{k}=$ 2-k+1 for k \u2265 1. Then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\delta\\leq\\mathbb{P}\\left(\\mathcal{E}:=\\left\\{\\forall k\\in\\mathbb{N}_{+},\\forall t\\geq1,\\|Z^{\\top}\\varepsilon\\|_{(V+\\lambda_{k}I)^{-1}}\\leq\\sigma_{\\varepsilon}\\sqrt{d\\ln\\left(1+\\frac{t L^{2}}{d\\lambda_{k}}\\right)+2\\ln\\left(\\frac{(\\pi^{2}/6)k^{2}}{\\delta}\\right)}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Under the event $\\mathcal{E}$ , we have two cases: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Z^{\\top}\\varepsilon\\|_{(V+\\sigma_{\\operatorname*{min}}(V)I)^{-1}}\\leq\\|Z^{\\top}\\varepsilon\\|_{(V+I)^{-1}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\sigma_{\\varepsilon}\\sqrt{d\\ln\\left(1+\\displaystyle\\frac{t L^{2}}{d}\\right)+2\\ln\\left(\\displaystyle\\frac{\\left(\\pi^{2}/6\\right)}{\\delta}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "\u00b7 $\\sigma_{\\mathrm{min}}(V)<1$ : choose $k=\\lceil\\log_{2}(2\\sigma_{\\operatorname*{min}}(V)^{-1})\\rceil\\geq2$ , which is the $k$ that satisfies $\\lambda_{k}\\,\\leq$ $\\sigma_{\\operatorname*{min}}(V)<\\lambda_{k-1}$ . Then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|Z^{\\top}\\varepsilon\\|_{(V+\\sigma_{\\operatorname*{min}}(V)I)^{-1}}\\leq\\|Z^{\\top}\\varepsilon\\|_{(V+\\lambda_{k}I)^{-1}}}}\\\\ &{\\leq\\sigma_{\\varepsilon}\\sqrt{d\\ln\\left(1+\\frac{t L^{2}}{d\\lambda_{k}}\\right)+2\\ln\\left(\\frac{(\\pi^{2}/6)k^{2}}{\\delta}\\right)}}\\\\ &{\\leq\\sigma_{\\varepsilon}\\sqrt{d\\ln\\left(1+\\frac{t L^{2}}{d\\sigma_{\\operatorname*{min}}(V)/2}\\right)+2\\ln\\left(\\frac{(\\pi^{2}/6)\\lceil\\log_{2}\\left(2\\sigma_{\\operatorname*{min}}(V)^{-1}\\right)\\rceil^{2}}{\\delta}\\right)}\\,.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad(\\lambda_{k}=\\frac{1}{2}\\lambda_{k-1}>\\frac{1}{\\sigma}\\sigma_{\\operatorname*{min}}(V);\\,\\mathrm{def}^{\\top}\\mathfrak{n}\\,\\mathrm{of}\\,k)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Altogether, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nZ^{\\top}\\varepsilon\\|_{(V+\\sigma_{\\operatorname*{min}}(V)I)^{-1}}\\leq\\sigma_{\\varepsilon}\\sqrt{d\\ln\\left(1+\\frac{t L^{2}(1\\vee2\\sigma_{\\operatorname*{min}}^{-1}(V))}{d}\\right)+2\\ln\\left(\\frac{(\\pi^{2}/6)(1\\vee\\lceil\\log_{2}(2\\sigma_{\\operatorname*{min}}^{-1}(V))\\rceil)}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We conclude the proof by simplifying the RHS ", "page_idx": 26}, {"type": "text", "text": "H  Proofs of sample complexity when given I ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "H.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Algorithm 5 Confounded pure exploration with known I Input $\\mathcal{Z},\\mathcal{W},\\Gamma,\\delta,\\varepsilon,L_{\\nu}\\geq\\sigma_{\\nu}^{2}$ Initialize: $k=1,\\hat{\\mathcal{W}}_{k}=\\mathcal{W}$ Deef(,,,=wT1 while $\\left|\\hat{\\mathcal{W}}_{k}\\right|>1$ do $\\begin{array}{r l}&{\\lambda_{k}=\\underset{\\rho(\\mathcal{W}_{k})=0}{\\operatorname{arg}\\operatorname*{min}}\\lambda_{\\mathsf{A}\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}f(w,w^{\\prime},\\Gamma,\\lambda)}\\\\ &{\\rho(\\mathcal{W}_{k})=\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}f(w,w^{\\prime},\\Gamma,\\lambda)}\\\\ &{\\zeta_{k}=2^{-k}}\\\\ &{N_{k}=\\left\\lceil2(1+\\omega)\\zeta_{k}^{-2}\\rho(\\mathcal{W}_{k})L_{\\nu}\\log\\Bigl(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\Bigr)\\right\\rceil\\vee r(\\omega)}\\\\ &{Z_{N_{k}}=\\mathsf{R O U N D}(\\lambda_{k},N_{k})}\\end{array}$ Pull arms in $Z_{N_{k}}$ and observe $Y_{N_{k}}$ $\\begin{array}{r l}&{\\mathrm{Compute}\\;\\widehat{\\theta}_{\\mathrm{oracle}}^{k}=\\Big(Z_{N_{k}}^{\\top}Z_{N_{k}}\\Gamma\\Big)^{-1}Z_{N_{k}}^{\\top}Y_{N_{k}}}\\\\ &{\\mathcal{W}_{k+1}=\\mathcal{W}_{k}\\backslash\\left\\{w\\in\\mathcal{W}_{k}\\mid\\exists w^{\\prime}\\in\\mathcal{W}_{k},\\mathrm{s.t.},\\Big\\langle w^{\\prime}-w,\\widehat{\\theta}_{\\mathrm{oracle}}^{k}\\Big\\rangle>\\zeta_{k}\\right\\}}\\\\ &{k\\underset{k\\not=-k,\\ i}{\\sim}\\quad1}\\end{array}$ k=k+1 end while Output: $\\mathcal{W}_{k}$ ", "page_idx": 27}, {"type": "text", "text": "Theorem 3.1. Algorithm 1 is $\\delta$ -PAC for the CPET-LB problem and terminates in at most $c(1+$ $\\omega)L_{\\nu}\\rho^{*}\\log\\left(1/\\delta\\right)+c r(\\omega)$ samples, where $c$ hides logarithmic factors of $\\Delta$ and $|\\mathcal{W}|$ ", "page_idx": 27}, {"type": "text", "text": "Proof. Part 1 $\\delta$ -PAC By the confidence interval in Lemma 2.2, we have, with probability at least $\\begin{array}{r}{1-\\frac{\\delta}{2k^{2}|\\mathcal{W}|}}\\end{array}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\widehat{\\theta}_{\\mathrm{oracle}}^{k}-\\theta,w-w^{*}\\right\\rangle\\bigg|\\leq\\|w-w^{*}\\|_{\\left(\\Gamma^{\\top}\\left(\\sum_{s=1}^{N_{k}}z_{t},\\tau_{t}\\right)\\Gamma\\right)}-1\\sqrt{2\\sigma_{\\nu}^{2}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)}}\\\\ &{\\leq\\frac{\\sqrt{1+\\omega}\\|w-w^{*}\\|_{\\left(\\Gamma^{\\top}\\lambda_{z}z:\\tau\\right)^{-1}}}{\\sqrt{N_{k}}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)}}\\\\ &{\\leq\\frac{\\sqrt{1+\\omega}\\|w-w^{*}\\|_{\\left(\\Gamma^{\\top}\\lambda_{z}z:\\tau\\right)^{-1}}}{\\sqrt{\\left[2(1+\\omega)\\zeta_{k}^{-2}\\rho(\\mathcal{W}_{k})L_{\\nu}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)\\right]\\vee r(\\omega)}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)}}\\\\ &{\\leq{\\zeta_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Define a good event $\\mathcal{E}_{k,w}$ for each $k$ and $\\mathcal{W}_{k}$ as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{k,w}=\\left\\{\\left|\\left\\langle\\widehat{\\theta}_{\\mathrm{oracle}}^{k}-\\theta,w-w^{*}\\right\\rangle\\right|\\leq\\zeta_{k}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We claim that with probability at least $1-\\delta$ , the event $\\bigcap_{k=1}^{\\infty}\\bigcap_{w\\in\\mathcal{W}_{k}}\\mathcal{E}_{k,w}$ holds. It can be proved by a union bound. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}\\!\\left(\\left(\\bigcap_{k=1}^{\\infty}\\bigcap_{w\\in\\mathcal{W}_{k}}\\mathcal{E}_{w,k}\\right)^{c}\\right)\\leq\\!\\!\\!\\displaystyle\\sum_{k=1}^{\\infty}\\sum_{w\\in\\mathcal{W}_{k}}\\mathbb{P}\\!\\left(\\mathcal{E}_{k,w}^{c}\\right)}\\\\ &{}&{\\qquad\\leq\\!\\!\\!\\displaystyle\\sum_{k=1}^{\\infty}\\sum_{w\\in\\mathcal{W}_{k}}\\frac{\\delta}{2k^{2}|\\mathcal{W}|}}\\\\ &{}&{\\qquad\\leq\\!\\!\\!\\displaystyle\\frac{\\delta}{2}\\sum_{k=1}^{\\infty}\\frac{1}{k^{2}}}\\\\ &{}&{\\qquad<\\!\\!\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$\\begin{array}{r}{\\sum_{k=1}^{\\infty}{\\frac{1}{k^{2}}}={\\frac{\\pi^{2}}{6}}<2}\\end{array}$ $\\bigcap_{k=1}^{\\infty}\\bigcap_{w\\in\\mathcal{W}_{k}}\\mathcal{E}_{w,k}$ \uff0c\uff0c$\\dot{w}\\in\\mathcal{W}_{k}$", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\langle w-w^{*},\\widehat{\\theta}_{\\mathrm{oracle}}^{k}\\right\\rangle=\\left\\langle w-w^{*},\\widehat{\\theta}_{\\mathrm{oracle}}^{k}-\\theta\\right\\rangle+\\left\\langle w-w^{*},\\theta\\right\\rangle}&{}\\\\ {\\qquad\\qquad\\leq\\left\\langle w-w^{*},\\widehat{\\theta}_{\\mathrm{oracle}}^{k}-\\theta\\right\\rangle}&{}\\\\ {\\qquad\\qquad\\leq\\zeta_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus the best arm $w^{*}$ never satisfies the elimination condition. Next we show that at the end of stage $k$ , any suboptimal arm $w$ thatsatisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\langle\\theta,w^{*}-w\\rangle\\geq2\\zeta_{k}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "is eliminated. To show this, we need to show that $w$ satisfies the elimination condition, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\langle\\widehat{\\theta}_{\\mathrm{oracle}}^{k},w^{\\prime}-w\\right\\rangle\\geq\\!\\left\\langle\\widehat{\\theta}_{\\mathrm{oracle}}^{k},w^{*}-w\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\!\\left\\langle\\widehat{\\theta}_{\\mathrm{oracle}}^{k}-\\theta,w^{*}-w\\right\\rangle+\\langle\\theta,w^{*}-w\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq-\\,\\zeta_{k}+2\\zeta_{k}}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\!\\zeta_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This implies that with probability at least $1-\\delta,\\,w^{*}$ always survives. ", "page_idx": 28}, {"type": "text", "text": "Part 2 sample complexity ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Define $S_{k}~=~\\left\\{w\\in\\mathcal{W}\\mid\\langle w^{*}-w,\\theta\\rangle\\leq4\\zeta_{k}\\right\\}$ .Thus with probability at least $1\\,-\\,\\delta$ wehave $\\bigcap_{k}\\left\\{\\mathcal{W}_{k}\\subseteq S_{k}\\right\\}$ . This implies the following is true with probability at least $1-\\delta$ for all $k$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho(\\mathcal{W}_{k})=\\underset{\\lambda\\in\\Delta(\\mathcal{Z})}{\\operatorname*{min}}\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\big\\|w-w^{\\prime}\\big\\|_{\\left(\\sum_{z\\in\\mathcal{Z}}\\Gamma^{\\top}\\lambda_{z}z z^{\\top}\\Gamma\\right)^{-1}}^{2}}\\\\ &{\\qquad\\leq\\underset{\\lambda\\in\\Delta(\\mathcal{Z})}{\\operatorname*{min}}\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\big\\|w-w^{\\prime}\\big\\|_{\\left(\\sum_{z\\in\\mathcal{Z}}\\Gamma^{\\top}\\lambda_{z}z z^{\\top}\\Gamma\\right)^{-1}}^{2}}\\\\ &{\\qquad=\\rho(S_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Define $\\Delta$ to be the minimum gap between $w^{*}$ and any other $\\begin{array}{r l r}{w}&{{}\\in}&{\\mathcal{W}}\\end{array}$ i.e., $\\Delta\\ :=$ $\\operatorname*{min}_{w\\in\\mathcal{W}\\setminus\\{w^{*}\\}}\\langle w^{*}-w,\\theta\\rangle$ . Then for $k\\,\\geq\\,\\left\\lceil\\log\\bigl(4\\Delta^{-1}\\bigr)\\right\\rceil$ , we have $S_{k}\\,=\\,\\{w^{*}\\}$ with probability ", "page_idx": 28}, {"type": "text", "text": "at least $1-\\delta$ . The total sample complexity is the summation of the number of samples in each round, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\left|\\log\\left(\\Delta^{-1}\\right)\\right|}{\\sum_{k=1}^{\\infty}}}\\\\ &{=\\underbrace{\\left|\\log\\left(4\\Delta^{-1}\\right)\\right|}_{k=1}\\left(\\left[4(1+\\omega)\\zeta_{k}^{-2}\\rho(W_{k})L_{\\nu}\\log\\left(\\frac{4k^{2}\\left|W\\right|}{\\delta}\\right)\\right]\\vee r(\\omega)\\right)}\\\\ &{\\leq\\underbrace{\\left|\\log\\left(4\\Delta^{-1}\\right)\\right|}_{k=1}\\left(8(1+\\omega)2^{2k}\\rho(W_{k})L_{\\nu}\\log\\left(\\frac{4k^{2}\\left|W\\right|}{\\delta}\\right)\\vee r(\\omega)\\right)}\\\\ &{\\leq\\underbrace{\\left|\\log\\left(4\\Delta^{-1}\\right)\\right|}_{k=1}\\left(8(1+\\omega)2^{2k}\\rho(S_{k})L_{\\nu}\\log\\left(\\frac{4k^{2}\\left|W\\right|}{\\delta}\\right)\\vee r(\\omega)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "On the other hand, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\rho^{*}=}&{\\frac{\\operatorname*{min}}{\\lambda\\in\\Delta(\\mathcal{L})}\\operatorname*{max}_{i\\in\\mathcal{N}(k^{*})}\\frac{\\|w^{*}-w^{*}\\|_{\\mathcal{L}(\\mathcal{L}^{n})}^{2}\\Gamma^{*}\\lambda_{\\mathcal{A}^{\\pi/1}}\\Gamma^{*}\\lambda}{(w^{*}-w^{*})(w^{*}-w^{*})^{2}}}\\\\ &{=\\frac{\\operatorname*{min}}{\\lambda\\in\\Delta(\\mathcal{L})}\\operatorname*{max}_{i\\in\\Delta(\\mathcal{R})}\\frac{\\|w^{*}-w^{*}\\|_{\\mathcal{L}(\\mathcal{L}^{n})}^{2}\\Gamma^{*}\\lambda_{\\mathcal{A}^{\\pi/1}}\\Gamma^{*}\\lambda}{\\left[\\operatorname*{sup}(w^{*}-w^{*})(w^{*}-w^{*})^{2}\\right]^{2}}}\\\\ &{\\geq\\frac{1}{\\left[\\operatorname*{sup}(4\\Delta^{-1})\\right]}\\operatorname*{min}_{i\\in\\Delta(\\mathcal{L})}\\frac{\\|w^{*}-w^{*}\\|_{\\mathcal{L}(\\mathcal{L}^{n})}^{2}\\Gamma^{*}\\lambda_{\\mathcal{A}^{\\pi/1}}\\Gamma^{*}\\lambda}{\\operatorname*{max}_{i\\in\\Delta(\\mathcal{L}^{n})}\\frac{\\|w^{*}-w^{*}\\|_{\\mathcal{L}(\\mathcal{L}^{n})}^{2}\\Gamma^{*}\\lambda_{\\mathcal{A}^{\\pi/1}}\\Gamma^{*}\\lambda}{\\left[w^{*}-w^{*}-w^{*}\\right]_{\\mathcal{L}(\\mathcal{L}^{n})}^{2}}}\\\\ &{\\geq\\frac{1}{2\\left[\\operatorname*{sup}(4\\Delta^{-1})\\right]}\\left[\\operatorname*{sup}(4\\Delta^{-1})\\right]^{2}}\\\\ &{\\geq\\frac{1}{16\\left[\\operatorname*{sup}(4\\Delta^{-1})\\right]}\\sum_{i=1}^{2^{n}}\\frac{\\operatorname*{min}}{2^{n}\\lambda(\\lambda(\\lambda))\\cdot\\operatorname*{min}(\\Delta^{-1})}\\|w^{*}-w^{*}\\|_{\\mathcal{L}(\\mathcal{L}^{n})}^{2}\\Gamma^{*}\\lambda_{\\mathcal{A}^{\\pi}}\\Gamma^{*}\\lambda^{\\pi}}\\\\ &{\\geq\\frac{1}{64 \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality is by the triangle inequality, i.e. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\gamma,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{(\\sum_{z\\in\\mathcal{Z}}\\Gamma^{\\top}\\lambda_{z}z z^{\\top}\\Gamma)^{-1}}^{2}\\leq\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\left(\\|w-w^{*}\\|_{(\\sum_{z\\in\\mathcal{Z}}\\Gamma^{\\top}\\lambda_{z}z z^{\\top}\\Gamma)^{-1}}+\\left\\|w^{*}-w^{\\prime}\\right\\|_{(\\sum_{z\\in\\mathcal{Z}}\\Gamma^{\\top}\\lambda_{z}z z^{\\top}\\Gamma)^{-1}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq4\\underset{w\\in S_{k}}{\\operatorname*{max}}\\|w-w^{*}\\|_{(\\sum_{z\\in\\mathcal{Z}}\\Gamma^{\\top}\\lambda_{z}z z^{\\top}\\Gamma)^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining (16) and (17), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\left\\lceil\\log\\left(4\\Delta^{-1}\\right)\\right\\rceil}N_{k}\\leq c(1+\\omega)L_{\\nu}\\log\\left(4\\Delta^{-1}\\right)\\log\\left(\\frac{\\log\\left(4\\Delta^{-1}\\right)^{2}|\\mathcal{W}|}{\\delta}\\right)\\rho^{*}+\\log\\Bigl(4\\Delta^{-1}\\Bigr)r(\\omega).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Input $\\mathcal{Z},\\mathcal{W},\\widehat{\\Gamma},\\delta,\\varepsilon,L_{\\nu}\\geq\\sigma_{\\nu}^{2},L_{\\eta}\\geq\\sigma_{\\eta}^{2}$   \nInitialize: $k=1,\\hat{\\mathcal{W}}_{k}=\\mathcal{W}$ , calculate $\\gamma$ using (18) $f(w,w^{\\prime},\\Gamma,\\lambda):=\\Vert w-w^{\\prime}\\Vert_{(\\sum_{z\\in\\mathcal{Z}}\\Gamma^{\\top}\\lambda_{z}z z^{\\top}\\Gamma)^{-1}}^{2}$   \nwhile $\\exists w,w^{\\prime}\\in\\mathcal{W}_{k}$ , s.t., $\\left\\langle w^{\\prime}-w,\\widehat{\\theta_{\\mathsf{o r a c l e}}^{k}}\\right\\rangle>4\\gamma$ Or Sk>do   \n$\\begin{array}{r l}&{\\lambda_{k}=\\underset{\\rho(\\mathcal{W}_{k})}{\\operatorname{arg\\,min}}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}f(w,^{'}w^{\\prime},\\widehat{\\Gamma},\\lambda)}\\\\ &{\\rho(\\mathcal{W}_{k})=\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}f(w,w^{\\prime},\\widehat{\\Gamma},\\lambda)}\\\\ &{\\zeta_{k}=2^{-k}}\\\\ &{N_{k}=\\left\\lceil2(1+\\omega)\\Big(\\zeta_{k}^{-2}\\wedge\\frac{1}{\\gamma^{2}}\\Big)\\rho(\\mathcal{W}_{k})L_{\\nu}\\log\\Big(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\Big)\\right\\rceil\\vee r(\\omega)}\\\\ &{Z_{N_{k}}=\\mathsf{R O U N D}(\\lambda_{k},N_{k})}\\end{array}$   \nPullarms in ZNk and observeYNg1   \n$\\begin{array}{r l}&{\\mathrm{Compute~}\\widehat{\\theta}_{\\mathrm{p-2SLS}}^{k}=\\left(Z_{N_{k}}^{\\top}Z_{N_{k}}\\widehat{\\Gamma}\\right)^{-1}Z_{N_{k}}^{\\top}Y_{N_{k}}}\\\\ &{\\mathcal{W}_{k+1}=\\mathcal{W}_{k}\\backslash\\left\\{w\\in\\mathcal{W}_{k}\\mid\\exists w^{\\prime}\\in\\mathcal{W}_{k},\\mathrm{s.t.},\\left\\langle w^{\\prime}-w,\\widehat{\\theta}_{\\mathrm{p-2SLS}}^{k}\\right\\rangle>\\zeta_{k}+\\gamma\\right\\}}\\\\ &{k=k+1}\\end{array}$   \nendwhile   \nOutput:any $w\\in\\mathcal{W}_{k}$ ", "page_idx": 30}, {"type": "text", "text": "Theorem I.1. Suppose that we have $\\widehat{\\Gamma}$ that is an OLS estimate from an offine dataset $\\{Z_{T},X_{T}\\}$ collected non-adaptively through a fixed design $\\xi$ and the efficient rounding procedure ROUND,as Well as T \u2265 mi(A(E,r) $\\begin{array}{r}{T\\ge\\frac{4\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\xi,\\Gamma)\\big)}\\overline{{\\log}}\\big(Z_{T},\\delta/2\\big)}\\end{array}$ .Algorithm $6$ guarantees that with probability at least $1-\\delta$ a $6\\gamma$ -good arm is returned, where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\gamma:=\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}}\\mathopen{}\\mathclose\\bgroup\\left\\|w-w^{\\prime}\\aftergroup\\egroup\\right\\|_{\\bar{A}(Z_{T},\\widehat{\\Gamma})^{-1}}\\|\\theta\\|_{2}\\sqrt{L_{\\eta}\\overline{{\\log}}(Z_{T},\\delta)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Also, the algorithm terminates in at most ", "page_idx": 30}, {"type": "equation", "text": "$$\nc(1+\\omega)L_{\\nu}\\log\\bigl(1/\\delta\\bigr)\\rho^{*}(\\gamma)+c(1+\\omega)^{2}L_{\\nu}\\log\\bigl(1/\\delta\\bigr)\\frac{\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T},\\delta)}{\\sigma_{\\operatorname*{min}}\\bigl(A(\\xi,\\Gamma)\\bigr)}\\frac{\\rho(\\xi,\\gamma)}{T}\\vee c r(\\omega)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "samples, where $c$ hides logarithmic factors of $\\Delta$ and $\\vert\\mathcal{W}\\vert$ and $\\gamma$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\rho(\\lambda,\\gamma):=\\operatorname*{max}_{w\\in\\mathcal{W}\\backslash\\{w^{*}\\}}\\frac{\\|w^{*}-w\\|_{A(\\lambda,\\Gamma)^{-1}}^{2}}{\\langle w^{*}-w,\\theta\\rangle^{2}\\vee\\gamma^{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\rho^{*}(\\gamma)=\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w\\in\\mathcal{W}\\backslash\\{w^{*}\\}}\\frac{\\|w^{*}-w\\|_{A(\\lambda,\\Gamma)^{-1}}^{2}}{\\langle w^{*}-w,\\theta\\rangle^{2}\\vee\\gamma^{2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. The proof can be divided into four steps: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The best arm $w^{*}$ is never eliminated.   \n\u00b7 At the end of stage $k$ , any suboptimal arm $w$ that satisfies $\\langle\\theta,w^{*}-w\\rangle\\,\\geq\\,2\\zeta_{k}+2\\gamma$ is eliminated.   \n\u00b7 The stopping condition is met in finite time.   \n\u00b7 When the stopping condition is met, there are only $6\\gamma.$ -good arms left. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The upper bound of the sample complexity. ", "page_idx": 31}, {"type": "text", "text": "Step 1: The best arm $w^{*}$ is never eliminated. ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "By the confidence interval in Theorem 2.3, we have, with probability at least $\\begin{array}{r}{1-\\frac{\\delta}{2k^{2}|\\mathcal{W}|}}\\end{array}$ 2k2/wl, for any k and $w\\in\\mathcal{W}_{k}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\widehat{\\theta}_{\\mathrm{p-2SLS}}^{k}-\\theta,w-w^{*}\\right\\rangle\\bigg|\\leq\\|w-w^{*}\\|_{\\tilde{A}(Z_{N_{k}},\\widehat{\\Gamma})^{-1}}\\sqrt{2\\sigma_{v}^{2}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)}+\\gamma}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{1+\\omega}\\|w-w^{*}\\|_{A(\\lambda_{k},\\widehat{\\Gamma})^{-1}}\\sqrt{2\\sigma_{v}^{2}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)}}{\\sqrt{N_{k}}}+\\gamma}\\\\ &{\\qquad\\qquad\\leq\\frac{\\sqrt{1+\\omega}\\|w-w^{*}\\|_{A(\\lambda_{k},\\widehat{\\Gamma})^{-1}}}{\\sqrt{\\sqrt{2(1+\\omega)\\zeta_{k}^{-2}\\rho(\\mathcal{W}_{k})L_{v}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)}\\left|\\vee\\,r(\\omega)\\right|}}\\sqrt{2\\sigma_{v}^{2}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)}\\,.}\\\\ &{\\qquad\\qquad\\leq\\zeta_{k}+\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Define a good event $\\mathcal{E}_{k,w}$ for each $k$ and $w\\in\\mathcal{W}_{k}$ as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k,w}=\\left\\{\\left|\\left\\langle\\widehat{\\theta}_{\\mathtt{P}-2\\mathtt{S L S}}^{k}-\\theta,w-w^{*}\\right\\rangle\\right|\\leq\\zeta_{k}+\\gamma\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By_the same calculation as (15), we claim that with probability at least $1\\ -\\ \\delta$ \uff0cthe event $\\bigcap_{k=1}^{\\infty}\\bigcap_{w\\in\\mathcal{W}_{k}}\\mathcal{E}_{k,w}$ holds,Undertheevent $\\bigcap_{k=1}^{\\infty}\\bigcap_{\\stackrel{w\\in\\mathcal{W}_{k}}{\\zeta}}\\bar{\\mathcal{E}}_{w,k}$ to show that the best arm is never eliminated, it suffices to show that for any sub-optimal arm $w\\in\\mathcal{W}_{k}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle w-w^{*},\\widehat{\\theta}_{\\mathtt{P-2S L S}}^{k}\\right\\rangle=\\left\\langle w-w^{*},\\widehat{\\theta}_{\\mathtt{P-2S L S}}^{k}-\\theta\\right\\rangle+\\langle w-w^{*},\\theta\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\langle w-w^{*},\\widehat{\\theta}_{\\mathtt{P-2S L S}}^{k}-\\theta\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq\\zeta_{k}+\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus the best arm $w^{*}$ never satisfies the elimination condition. ", "page_idx": 31}, {"type": "text", "text": "Step 2: At the end of stage $k$ , any suboptimal arm $w$ that satisfies $\\langle\\theta,w^{*}-w\\rangle\\geq2\\zeta_{k}+2\\gamma$ is eliminated. ", "page_idx": 31}, {"type": "text", "text": "To prove this, we show that such arm $w$ must satisfy the elimination condition, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\langle\\widehat{\\theta}_{\\mathrm{P-2SLS}}^{k},w^{\\prime}-w\\right\\rangle\\geq\\!\\left\\langle\\widehat{\\theta}_{\\mathrm{P-2SLS}}^{k},w^{*}-w\\right\\rangle}&{}\\\\ {=\\!\\left\\langle\\widehat{\\theta}_{\\mathrm{P-2SLS}}^{k}-\\theta,w^{*}-w\\right\\rangle+\\langle\\theta,w^{*}-w\\rangle}&{}\\\\ {\\quad\\geq-\\zeta_{k}-\\gamma+2\\zeta_{k}+2\\gamma}&{}\\\\ {=\\!\\zeta_{k}+\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus the arm $w$ is eliminated. ", "page_idx": 31}, {"type": "text", "text": "Step 3: The stopping condition is met in finite time. ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Given the result in Step 2 and the fact that $\\zeta_{k}$ is an exponentially decreasing sequence, we know that all of the arms $w$ satisfying $\\langle\\theta,w^{*}-w\\rangle>2\\gamma$ will be eliminated in finite time. This means that only the arms $w$ satisfying $\\langle\\theta,w^{*}-w\\rangle\\,\\leq\\,2\\gamma$ will remain. We need to show that $\\forall w,w^{\\prime}\\in$ $\\begin{array}{r}{\\mathcal{W}_{k},\\left\\langle\\boldsymbol{w}^{\\prime}-\\boldsymbol{w},\\widehat{\\theta}_{\\mathrm{P-2SLS}}^{k}\\right\\rangle\\leq4\\gamma}\\end{array}$ can be achieved in fnite tme. Whn $\\zeta_{k}\\leq\\gamma$ (which will happen in ", "page_idx": 31}, {"type": "text", "text": "finite time), ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phantom{\\frac{1}{\\mu^{\\prime}}}w^{\\prime}-w,\\widehat{\\theta}_{\\mathtt{p}-2\\mathtt{S L S}}^{k}\\Big\\rangle=\\biggr\\langle w^{\\prime}-w+w^{\\ast}-w^{\\ast},\\widehat{\\theta}_{\\mathtt{p}-2\\mathtt{S L S}}^{k}\\biggr\\rangle}&{}\\\\ {\\phantom{\\frac{1}{\\mu^{\\prime}}}=\\biggr\\langle w^{\\ast}-w,\\widehat{\\theta}_{\\mathtt{p}-2\\mathtt{S L S}}^{k}\\biggr\\rangle+\\biggr\\langle w^{\\prime}-w^{\\ast},\\widehat{\\theta}_{\\mathtt{p}-2\\mathtt{S L S}}^{k}\\biggr\\rangle}&{}\\\\ {\\phantom{\\frac{1}{\\mu^{\\prime}}}=\\biggr\\langle w^{\\ast}-w,\\widehat{\\theta}_{\\mathtt{p}-2\\mathtt{S L S}}^{k}-\\theta+\\theta\\biggr\\rangle+\\biggr\\langle w^{\\prime}-w^{\\ast},\\widehat{\\theta}_{\\mathtt{p}-2\\mathtt{S L S}}^{k}-\\theta+\\theta\\biggr\\rangle}&{}\\\\ {\\phantom{\\frac{1}{\\mu^{\\prime}}}=\\biggr\\langle w^{\\ast}-w,\\widehat{\\theta}_{\\mathtt{p}-2\\mathtt{S L S}}^{k}-\\theta\\biggr\\rangle+\\langle w^{\\ast}-w,\\theta\\rangle+\\biggr\\langle w^{\\prime}-w^{\\ast},\\widehat{\\theta}_{\\mathtt{p}-2\\mathtt{S L S}}^{k}-\\theta\\biggr\\rangle+\\biggr\\langle w^{\\prime}-w^{\\ast},\\theta}&{}\\\\ {\\phantom{\\frac{1}{\\mu^{\\prime}}}\\leq\\zeta_{k}+\\gamma+2\\gamma+\\zeta_{k}+\\gamma}&{}\\\\ {\\phantom{\\frac{1}{\\mu^{\\prime}}}=4\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Step 4: When the stopping condition is met, there are only $6\\gamma$ good arms left. ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "For any $w\\in\\mathcal{W}_{k}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle w^{*}-w,\\theta\\rangle=\\!\\Big\\langle w^{*}-w,\\theta-\\widehat{\\theta}_{\\mathtt{P}-2\\mathrm{SLS}}^{k}\\Big\\rangle+\\Big\\langle w^{*}-w,\\widehat{\\theta}_{\\mathtt{P}-2\\mathrm{SLS}}^{k}\\Big\\rangle}\\\\ &{\\qquad\\qquad\\quad\\le\\!\\zeta_{k}+\\gamma+4\\gamma}\\\\ &{\\qquad\\qquad=\\!6\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Step 5: The upper bound of the sample complexity. ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Define $\\mathcal{W}(2\\gamma)$ as the set of $2\\gamma$ -good arms, i.e., $\\mathcal{W}(2\\gamma)\\,:=\\,\\left\\{w\\in\\mathcal{W}\\mid\\langle\\theta,w^{*}-w\\rangle\\leq2\\gamma\\right\\}$ .Then the best arm in the set $\\mathcal{W}\\backslash\\mathcal{W}(2\\gamma)$ has a suboptimality gap $\\begin{array}{r}{\\operatorname*{min}_{w\\in\\mathcal{W}\\backslash\\mathcal{W}(2\\gamma)}\\langle\\theta,w^{*}-w\\rangle.}\\end{array}$ 'We define $\\Delta_{\\operatorname*{min}}(2\\gamma)~:=~\\operatorname*{min}_{w\\in\\mathcal{W}\\backslash\\mathcal{W}(2\\gamma)}\\langle\\theta,w^{*}-w\\rangle\\,-\\,2\\gamma$ .By the result in Step 2, we know that after $k^{*}\\ :=\\ \\left\\lceil\\log\\Bigl(4\\Delta_{\\operatorname*{min}}(2\\gamma)^{-1}\\Bigr)\\right\\rceil$ stages, all of the arms in $\\mathcal{W}\\backslash\\mathcal{W}(2\\gamma)$ are eliminated. Define $S_{k}\\;=\\;\\left\\{w\\in\\mathcal{W}\\;\\vert\\;\\langle w^{*}-w,\\theta\\rangle\\leq4\\zeta_{k}+2\\gamma\\right\\}$ . Thus with probability at least $1-\\delta$ ,we have $\\bigcap_{k}\\left\\{\\mathcal{W}_{k}\\subseteq\\dot{S}_{k}\\right\\}$ ", "page_idx": 32}, {"type": "text", "text": "The sample complexity of the algorithm is the total number of samples pulled, which is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c l r}{{}}&{{\\displaystyle\\sum_{k=1}^{k^{*}}N_{k}}}\\\\ {{}}&{{\\displaystyle=\\sum_{k=1}^{k^{*}}\\left(\\left[2(1+\\omega)\\left(\\zeta_{k}^{-2}\\wedge\\frac{1}{\\gamma^{2}}\\right)\\rho(\\mathcal{W}_{k})L_{\\nu}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)\\right]\\,\\nabla\\,r(\\omega)\\right)}}\\\\ {{}}&{{\\displaystyle\\le\\sum_{k=1}^{k^{*}}\\left(3(1+\\omega)\\Big(2^{2k}\\wedge\\frac{1}{\\gamma^{2}}\\Big)\\rho(\\mathcal{W}_{k})L_{\\nu}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)\\,\\nabla\\,r(\\omega)\\right)}}\\\\ {{}}&{{\\displaystyle\\le\\sum_{k=1}^{k^{*}}\\left(3(1+\\omega)\\Big(2^{2k}\\wedge\\frac{1}{\\gamma^{2}}\\Big)\\rho(\\mathcal{S}_{k})L_{\\nu}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)\\,\\nabla\\,r(\\omega)\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that the factor of $\\begin{array}{r}{\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in S_{k}}\\|w\\ -\\ w^{\\prime}\\|_{A(\\lambda,\\Gamma)^{-1}}^{2}}\\end{array}$ has the underlying true $\\Gamma$ in it, while the algorithm uses the plugged-in $\\widehat{\\Gamma}$ We need to relate it to $\\rho(S_{k})\\;\\;:=\\;$ $\\begin{array}{r}{\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in S_{k}}\\|w-w^{\\prime}\\|_{A(\\lambda,\\widehat{\\Gamma})^{-1}}^{2}}\\end{array}$ By Lemma J9, and defning $\\lambda_{z}^{*}(S_{k})$ as the optimal design for $S_{k}$ , i.e., ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\lambda_{k}^{*}=\\underset{\\lambda\\in\\Delta(\\mathcal{Z})}{\\arg\\operatorname*{min}}\\ \\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\,\\|w-w^{\\prime}\\|_{A(\\lambda,\\Gamma)^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Define $\\begin{array}{r}{\\lambda_{k}^{\\prime}=\\frac{1}{2}\\lambda_{k}^{*}+\\frac{1}{2}\\xi}\\end{array}$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in W_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda,\\widehat{\\Gamma})^{-1}}^{2}}\\\\ &{=\\underset{\\lambda\\in\\Delta(\\mathcal{Z})}{\\operatorname*{min}}\\underset{w,w^{\\prime}\\in W_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda,\\widehat{\\Gamma})^{-1}}^{2}}\\\\ &{\\leq\\underset{\\lambda\\in\\Delta(\\mathcal{Z})}{\\operatorname*{min}}\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda,\\widehat{\\Gamma})^{-1}}^{2}}\\\\ &{\\leq\\underset{\\lambda\\in\\Delta(\\mathcal{Z})}{\\operatorname*{min}}\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}3\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda,\\Gamma)^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})(w-w^{\\prime})\\right\\|_{A(\\lambda,I)^{-1}}^{2}}\\\\ &{\\leq\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}3\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{\\prime},\\Gamma)^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})(w-w^{\\prime})\\right\\|_{A(\\lambda_{k}^{\\prime},I)^{-1}}^{2}}\\\\ &{\\leq\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}6\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{\\prime},\\Gamma)^{-1}}^{2}+\\frac{16\\sigma_{\\eta}^{2}\\log\\left(Z_{T}\\right)}{\\sigma\\operatorname*{min}}\\left(\\mathbb{Z}_{T},\\delta/2\\right)\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi,\\Gamma)^{-1}}^{2}\\frac{1}{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality is due to ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{k}^{\\prime},\\Gamma)^{-1}}^{2}\\leq\\big\\|w-w^{\\prime}\\big\\|_{A\\left(\\frac{1}{2}\\lambda_{k}^{*}+\\frac{1}{2}\\xi,\\Gamma\\right)^{-1}}^{2}}}\\\\ &{\\leq\\big\\|w-w^{\\prime}\\big\\|_{A\\left(\\frac{1}{2}\\lambda_{k}^{*},\\Gamma\\right)^{-1}}^{2}}\\\\ &{=2\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "as wellas by Lemma .1 hen $\\begin{array}{r}{T\\ge\\frac{4\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\xi,\\Gamma)\\big)}\\overline{{\\log}}\\big(Z_{T},\\delta/2\\big)}\\end{array}$ wehave with probabity a least $1-\\delta/2$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})(w-w^{\\prime})\\right\\|_{A(\\lambda_{k}^{\\prime},I)^{-1}}^{2}}\\\\ &{\\leq\\!\\frac{4\\sigma_{\\eta}^{2}\\!\\log\\left(Z_{T},\\delta/2\\right)}{\\sigma_{\\operatorname*{min}}\\left(A(\\lambda_{k}^{\\prime},\\Gamma)\\right)}\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\|w-w^{\\prime}\\|_{A(\\xi,\\Gamma)^{-1}}^{2}\\frac{1}{T}}\\\\ &{\\leq\\!\\frac{4\\sigma_{\\eta}^{2}\\!\\log\\left(Z_{T},\\delta/2\\right)}{\\sigma_{\\operatorname*{min}}\\left(A(\\alpha\\lambda_{k}^{\\ast}+(1-\\alpha)\\xi,\\Gamma)\\right)}\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\|w-w^{\\prime}\\|_{A(\\xi,\\Gamma)^{-1}}^{2}\\frac{1}{T}}\\\\ &{\\leq\\!\\frac{8\\sigma_{\\eta}^{2}\\!\\log\\left(Z_{T},\\delta/2\\right)}{\\sigma_{\\operatorname*{min}}\\left(A(\\xi,\\Gamma)\\right)}\\underset{w,w^{\\prime}\\in S_{k}}{\\operatorname*{max}}\\|w-w^{\\prime}\\|_{A(\\xi,\\Gamma)^{-1}}^{2}\\frac{1}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can lower bound $\\rho^{*}(\\gamma)$ by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho^{*}(\\gamma)=\\underset{\\lambda\\in\\Delta(2)}{\\operatorname*{min}}\\underset{\\mathbf{w}\\in\\Delta^{\\mathbb{N}}\\setminus\\{\\mathbf{0}\\}}\\frac{\\|w^{*}-w\\|_{\\mathcal{A}(\\cdot)\\Gamma}^{2}-1}{\\lambda(w^{*}-w)\\theta^{2}\\sqrt{\\gamma^{2}}}}\\\\ &{=\\underset{\\lambda\\in\\Delta(2)}{\\operatorname*{min}}\\underset{\\mathbf{b}}{\\operatorname*{max}}\\underset{\\mathbf{w}\\in\\Delta_{\\mathbb{N}}\\setminus\\{\\mathbf{0}\\}}\\frac{\\|w^{*}-w\\|_{\\mathcal{A}(\\cdot)\\Gamma}^{2}-1}{\\left\\|w^{*}-w\\right\\|_{\\mathcal{A}(\\cdot)\\Gamma}^{2}-1}}\\\\ &{\\geq\\frac{1}{k^{*}\\lambda(\\leq\\Lambda)}\\underset{\\lambda\\in\\Delta(2)}{\\operatorname*{min}}\\underset{\\mathbf{b}}{\\sum}\\quad\\underset{w\\in\\Delta_{\\mathbb{N}}\\setminus\\{\\mathbf{0}\\}}\\frac{\\|w^{*}-w\\|_{\\mathcal{A}(\\cdot)\\Gamma}^{2}-1}{\\left\\|w^{*}-w\\right\\|_{\\mathcal{A}(\\cdot)\\Gamma}^{2}-1}}\\\\ &{\\geq\\frac{1}{16k^{*}}\\underset{\\lambda\\in\\Delta(2)}{\\sum}\\underset{k=1}{\\overset{k}{\\sum}}\\left(2^{2k}\\wedge\\frac{1}{\\gamma^{2}}\\right)\\underset{\\lambda\\in\\Delta(2)}{\\operatorname*{min}}\\underset{w\\in\\Delta_{\\mathbb{N}}\\setminus\\{\\mathbf{0}\\}}\\frac{\\operatorname*{max}}{\\left\\|w^{*}-w\\right\\|_{\\mathcal{A}(\\cdot)\\Gamma}^{2}-1}}\\\\ &{\\geq\\frac{1}{16k^{*}}\\underset{k=1}{\\overset{k}{\\sum}}\\left(2^{2k}\\wedge\\frac{1}{\\gamma^{2}}\\right)\\underset{\\lambda\\in\\Delta(2)}{\\operatorname*{min}}\\underset{w^{*}\\in\\Delta_{\\mathbb{N}}\\setminus\\{\\mathbf{0}\\}}\\|w^{*}-w\\|_{\\mathcal{A}(\\cdot)\\Gamma}^{2}-1}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Given (19), (20) and (21), we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{k^{*}}N_{k}\\leq c(1+\\omega)L_{\\nu}\\log\\bigl(1/\\delta\\bigr)\\rho^{*}(\\gamma)+c(1+\\omega)^{2}L_{\\nu}\\log\\bigl(1/\\delta\\bigr)\\frac{\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T},\\delta)}{\\sigma_{\\operatorname*{min}}\\bigl(A(\\xi,\\Gamma)\\bigr)}\\frac{\\rho(\\xi,\\gamma)}{T}+c r(\\omega),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $c$ hides logarithmic factors of $\\Delta$ and $\\vert\\mathcal{W}\\vert$ and $\\gamma$ ", "page_idx": 33}, {"type": "text", "text": "Aigoritnm / Opumai design witn unknown $\\Gamma$ Input $\\mathcal{Z},\\mathcal{W},\\delta,L_{\\nu}\\geq\\sigma_{\\nu}^{2},L_{\\eta}\\geq\\sigma_{\\eta}^{2},\\omega,\\gamma_{\\operatorname*{min}}\\leq\\lambda_{\\operatorname*{min}}(\\Gamma),\\lambda_{E},\\kappa_{0}$ Initialize: $k=1,\\mathcal{W}_{1}=\\mathcal{W},\\widehat{\\Gamma}_{0}=\\perp,\\zeta_{1}=1$ Define $f(w,w^{\\prime},\\Gamma,\\lambda)\\,:=\\,\\|w-w^{\\prime}\\|_{(\\sum_{z\\in\\mathcal{Z}}\\Gamma^{\\top}\\lambda_{z}z z^{\\top}\\Gamma)^{-1}}^{2}$ \uff0c $\\begin{array}{r}{M:=\\frac{32L_{\\eta}}{\\gamma_{\\mathrm{min}}^{2}\\sigma_{\\mathrm{min}}\\left(A(\\lambda_{E},I)\\right)}\\vee1,\\,\\delta_{k,\\ell}\\::=}\\end{array}$ $\\frac{\\delta}{4k^{2}\\ell^{2}}$ $|\\mathcal{W}_{k}|>1$ b $\\begin{array}{r l r}&{\\widehat{\\Gamma}_{k}=\\Gamma-\\mathrm{estimator}\\Big(\\mathcal{W}_{k},\\widehat{\\Gamma}_{k-1},\\zeta_{k},\\delta,k,\\omega,\\lambda_{E},M,L_{\\eta}\\Big)\\quad}&{\\triangleright\\mathrm{Step~1:update~}\\widehat{\\Gamma}}\\\\ &{\\widehat{\\theta}_{\\mathtt{p-2S L S}}^{k}=\\theta-\\mathrm{estimator}\\Big(\\mathcal{W}_{k},\\delta,\\zeta_{k},\\widehat{\\Gamma}_{k},\\omega,L_{\\nu},k\\Big)\\quad}&{\\triangleright\\mathrm{Step~2:update~}\\widehat{\\theta}}\\\\ &{\\mathcal{W}_{k+1}=\\mathcal{W}_{k}\\Big\\backslash\\bigg\\{w\\in\\mathcal{W}_{k}\\ \\vert\\ \\exists w^{\\prime}\\in\\mathcal{W}_{k},\\mathrm{s.t.},\\Big\\langle w^{\\prime}-w,\\widehat{\\theta}_{\\mathtt{p-2S L S}}^{k}\\Big\\rangle>\\zeta_{k}\\bigg\\}\\quad}&{\\triangleright\\mathrm{Step~3:elimination}}\\\\ &{k\\leftarrow k+1,\\zeta_{k}=2^{-k}\\quad}&\\end{array}$ end while Output: $\\mathcal{W}_{k}$ ", "page_idx": 34}, {"type": "text", "text": "The algorithms of $\\Gamma-$ estimator and $\\theta-$ estimator we present below are slightly different from the one in the main text. In the main text, we omit the phase index $k$ in the algorithm for simplicity. ", "page_idx": 34}, {"type": "text", "text": "Algorithm 8 T - estimator ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Input $\\mathcal{W}_{k},\\widehat{\\Gamma}_{k-1},\\zeta_{k},\\delta,k,\\omega,\\lambda_{E},M,L_{\\eta}$   \nDefine $\\begin{array}{r l}&{\\mathsf{S t o p}(\\mathcal{W},Z,\\Gamma,\\delta):=\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}}\\Bigl\\|w-w^{\\prime}\\Bigr\\|_{\\bar{A}(Z,\\Gamma)^{-1}}\\|\\theta\\|_{2}\\sqrt{L_{\\eta}\\overline{{\\log}}(Z,\\delta)}\\quad}\\\\ &{\\mathsf{i z e}\\ \\ell=1,\\,N_{k,0,0}=0\\quad}&{\\mathsf{v~d o u b l i n g~t r a p~.}}\\end{array}$   \nInitialize ick initialization   \nif $k=1$ then   \nwhile $\\ell=1$ or $\\mathtt{S t o p}\\Big(\\mathcal{W}_{k},Z_{k,0,\\ell},\\widehat{\\Gamma}_{k},\\delta_{k,\\ell}\\Big)>1$ do get $\\begin{array}{r}{2^{\\ell-1}\\Big(r(\\omega)\\vee\\frac{2}{\\kappa_{0}}\\Big)}\\end{array}$ samples denoed as $\\{Z_{k,0,\\ell},X_{k,0,\\ell},Y_{k,0,\\ell}\\}$ per desgn $\\lambda_{E}\\triangleright$ via ROUND Update $\\widehat{\\Gamma}_{k}$ by OLS on $\\left\\{Z_{k,0,\\ell},X_{k,0,\\ell}\\right\\}$ \uff0c $\\ell\\gets\\ell+1$ end while   \nelse   \n$\\begin{array}{r l}&{\\tilde{\\lambda}_{k}^{*}=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}f(w,w^{\\prime},\\widehat{\\Gamma}_{k-1},\\lambda)}\\\\ &{N^{\\prime}=\\bigg\\lfloor4g d M\\ln\\Big(1+2M\\big(d+L_{z}^{2}\\big)+2M2g d M\\Big)+8M\\ln\\Big(\\frac{2\\cdot6^{d}}{\\delta_{k,1}}\\Big)\\vee r(\\omega)\\bigg\\rfloor}\\\\ &{\\mathbf{while}\\,\\ell=1\\;\\mathrm{or}\\;\\mathrm{}\\mathrm{Stop}\\Big(\\mathcal{W}_{k},Z_{k,0,\\ell}\\cup Z_{k,1,\\ell},\\widehat{\\Gamma}_{k},\\delta_{k,\\ell}\\Big)>\\zeta_{k}\\;\\mathbf{do}}\\end{array}$ $N_{k,1,\\ell}=2^{\\ell}N^{\\prime}$ $\\triangleright$ doubling trick update get $N_{k,1,\\ell}$ samples per $\\tilde{\\lambda}_{k}$ denoted as $\\{Z_{k,1,\\ell},X_{k,1,\\ell},Y_{k,1,\\ell}\\}$ $\\triangleright$ via ROUND $\\begin{array}{r}{N_{k,0,\\ell}=\\left\\lceil2g d M\\ln\\left(M\\big(d+N_{k,1,\\ell}+L_{z}^{2}\\big)\\right)+4M\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta_{k,\\ell}}\\right)\\vee r(\\omega)\\vee\\frac{2}{\\kappa_{0}}\\right\\rceil}\\end{array}$ get $(N_{k,0,\\ell}~-~N_{k,0,\\ell-1})$ samples per $\\lambda_{E}$ augmented to $\\left\\{Z_{k,0,\\ell-1},X_{k,0,\\ell-1}\\right\\}$ and get   \n$\\left\\{Z_{k,0,\\ell},X_{k,0,\\ell}\\right\\}$ Update $\\widehat{\\Gamma}_{k}$ by OLS on $\\{Z_{k,0,\\ell}\\cup Z_{k,1,\\ell},X_{k,0,\\ell}\\cup X_{k,1,\\ell}\\},\\ell\\gets\\ell+1$   \nend while   \nend if   \nOutput: $\\widehat{\\Gamma}_{k}$ ", "page_idx": 34}, {"type": "text", "text": "$\\begin{array}{r l}&{\\mathbf{Input}\\,\\mathcal{W}_{k},\\delta,\\zeta_{k},\\widehat{\\Gamma}_{k},\\omega,L_{\\nu},k}\\\\ &{\\widehat{\\lambda}_{k}=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}f\\big(w,w^{\\prime},\\widehat{\\Gamma}_{k},\\lambda\\big)}\\\\ &{\\rho\\big(\\mathcal{W}_{k}\\big)=\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}f\\big(w,w^{\\prime},\\widehat{\\Gamma}_{k},\\lambda\\big)}\\\\ &{N_{k,2}=\\Bigg\\lceil2\\big(1+\\omega\\big)\\zeta_{k}^{-2}\\rho\\big(\\mathcal{W}_{k}\\big)L_{\\nu}\\log\\Big(\\frac{4k^{2}\\lvert\\mathcal{W}\\rvert}{\\delta}\\Big)\\Bigg\\rceil\\vee r\\big(\\omega\\big)}\\end{array}$ get $N_{k,2}$ samples per design $\\widehat{\\lambda}_{k}$ denoted as $\\left\\{Z_{k,2},X_{k,2},Y_{k,2}\\right\\}$ update $\\widehat{\\theta}_{\\mathrm{P-2SLS}}^{k}=(\\widehat{\\Gamma}_{k}^{\\top}Z_{k,2}^{\\top}Z_{k,2}\\widehat{\\Gamma}_{k})^{-1}\\widehat{\\Gamma}_{k}^{\\top}Z_{k,2}^{\\top}Y_{k,2}$ Output: B-L ", "page_idx": 35}, {"type": "text", "text": "Lemma J.1. Algorithm 3 and $N_{k,1,\\ell},N_{k,0,\\ell}$ guarantees three properties: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 Property 1: $N_{k,1,\\ell}\\geq N_{k,0,\\ell}.$ \u00b7 Property 2: $\\begin{array}{r}{\\frac{1}{2}\\big(N_{k,0,\\ell}+N_{k,1,\\ell}\\big)\\leq N_{k,0,\\ell-1}+N_{k,1,\\ell-1}.}\\end{array}$ $\\begin{array}{r}{r o p e r t y\\ 3\\colon\\frac{N_{k,1,1}}{8d\\ln\\left(1+\\frac{N_{k,1,1}L_{z}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d_{k}2}}{\\delta}\\right)}\\leq M\\ln(d M).}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "Proof. For Property 1, recall that $N_{k,0,\\ell}$ and $N_{k,1,\\ell}$ are defined as ", "page_idx": 35}, {"type": "equation", "text": "$$\nN_{k,0,\\ell}=\\left\\lceil2g d M\\ln\\left(M\\Big(d+N_{k,1,\\ell}+L_{z}^{2}\\Big)\\right)+4M\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta_{k,\\ell}}\\right)\\vee r(\\omega)\\right\\rceil\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\nN_{k,1,\\ell}=2^{\\ell}\\left\\lfloor4g d M\\ln\\left(1+2M\\Big(d+L_{z}^{2}\\Big)+2M2g d M\\right)+8M\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta_{k,1}}\\right)\\vee r(\\omega)\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We prove the result by induction. For the result for $\\ell=1$ , note that $N_{k,1,1}$ is of the form $N_{k,1,1}=$ $2a\\bar{+}\\,2b\\ln(1+2c+2b d)$ and $N_{k,0,1}$ is of the form $N_{k,0,1}=a+b\\ln(c+d r)$ ,where ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ a=4M\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta_{k,1}}\\right)}\\\\ &{\\bullet\\ b=2g d M}\\\\ &{\\bullet\\ c=M\\bigl(d+L_{z}^{2}\\bigr)}\\\\ &{\\bullet\\ d=M}\\\\ &{\\bullet\\ r=N_{k,1,\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By the contraposition of Lemma K.5, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nr>2a+2b\\ln(1+2c+2b d)\\implies r>a+b\\ln(c+d r).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus we have $N_{k,1,1}\\geq N_{k,0,1}$ . Now we assume the result holds for $\\ell$ , i.e., $N_{k,1,\\ell}\\ge N_{k,0,\\ell}$ and prove that it holds for $\\ell+1$ .We have ", "page_idx": 35}, {"type": "equation", "text": "$$\nN_{k,1,\\ell+1}=2N_{k,1,\\ell}\\geq2N_{k,0,\\ell}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It suffices to prove that ", "page_idx": 35}, {"type": "equation", "text": "$$\n2N_{k,0,\\ell}\\geq N_{k,0,\\ell+1}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We have ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{2N_{k,0,\\ell}}\\\\ &{=2{\\Biggl[}2g d M\\ln\\left(M\\left(d+N_{k,1,\\ell}+L_{s}^{2}\\right)\\right)+4M\\ln\\left({\\frac{2.6\\ell^{4}}{\\delta_{k,\\ell}}}\\right){\\,\\mathrm{\\boldmath{vr}}}(\\omega){\\Biggr]}}\\\\ &{\\geq2g d M\\ln\\left(M^{2}\\left(d+N_{k,1,\\ell}+L_{s}^{2}\\right)^{2}\\right)+8M\\ln\\left({\\frac{2.6\\ell^{2}d^{2}}{\\delta_{k,1,\\ell}}}\\right){\\boldsymbol{\\mathrm{vr}}}(\\omega)}\\\\ &{\\geq2g d M\\ln\\left(M^{2}\\left(d+N_{k,1,\\ell}+L_{s}^{2}\\right)^{2}\\right)+8M\\left({\\frac{2.6\\ell^{4}}{\\delta_{k,1,\\ell}}}\\right)+2\\ln(\\ell){\\Biggl\\backslash}{\\boldsymbol{\\mathrm{vr}}}(\\omega)}\\\\ &{\\geq2g d M\\ln\\left(M^{2}\\left(d+N_{k,1,\\ell}+L_{s}^{2}\\right)^{2}\\right)+4M\\left({\\frac{1}{\\ln\\left(2.6\\ell^{4}\\right)}}+2\\ln(\\ell+1)\\right){\\boldsymbol{\\mathrm{vr}}}(\\omega)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(2\\ln\\ell\\geq\\ln(\\ell+1)){\\mathrm{\\boldmath{vr}}}(\\omega)}\\\\ &{\\geq2g d M\\ln\\left(M\\left(d+2N_{k,1,\\ell}+L_{s}^{2}\\right)\\right)+4M\\ln\\left({\\frac{2.6\\ell^{4}\\left(\\ell+1\\right)^{2}}{\\delta_{k,1,\\ell}}}\\right){\\boldsymbol{\\mathrm{vr}}}(\\omega)}\\\\ &{\\qquad\\qquad\\times.}\\end{array}}\n$$$\\ell\\geq2$ ", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For Property 2, it suffices to prove that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{1}{2}N_{k,0,\\ell}\\leq N_{k,0,\\ell-1}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This is equivalent to prove that ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}{\\big(}a+b\\ln(c+2d r){\\big)}\\leq a+b\\ln(c+d r).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\frac{1}{2}\\big(a+b\\ln(c+2d r)\\big)\\leq a+b\\ln(c+d r)}\\\\ &{\\Longleftrightarrow\\frac{1}{2}b\\ln(c+2d r)\\leq\\frac{1}{2}a+b\\ln(c+d r)}\\\\ &{\\Longleftrightarrow b\\ln\\left(\\sqrt{c+2d r}\\right)\\leq\\frac{1}{2}a+b\\ln(c+d r)}\\\\ &{\\Longleftarrow\\sqrt{c+2d r}\\leq c+d r}\\\\ &{\\Longleftrightarrow c+2d r\\leq c^{2}+2c d r+d^{2}r^{2}}\\\\ &{\\Longleftrightarrow d r(d r+2c-2)+c^{2}-c\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The last inequality holds because $c=M>1$ ", "page_idx": 36}, {"type": "text", "text": "For Property 3, we have, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{4d M\\ln{\\left(1+2M\\left(d+L_{z}^{2}\\right)+2M d M\\right)}+8M\\ln{\\left(\\frac{2\\cdot6^{d}k^{2}}{\\delta}\\right)}}{8d\\ln{\\left(1+\\frac{N_{k,1,1}L_{z}^{2}}{d}\\right)}+16\\ln{\\left(\\frac{2\\cdot6^{d}k^{2}}{\\delta}\\right)}}}\\\\ &{\\leq\\frac{4d M\\ln{\\left(1+2M\\left(d+L_{z}^{2}\\right)+2M d M\\right)}}{8d\\ln{\\left(1+\\frac{N_{k,1,1}L_{z}^{2}}{d}\\right)}+16\\ln{\\left(\\frac{2\\cdot6^{d}k^{2}}{\\delta}\\right)}}+\\frac{8M\\ln{\\left(\\frac{2\\cdot6^{d}k^{2}}{\\delta}\\right)}}{8d\\ln{\\left(1+\\frac{N_{k,1,1}L_{z}^{2}}{d}\\right)}+16\\ln{\\left(\\frac{2\\cdot6^{d}k^{2}}{\\delta}\\right)}}}\\\\ &{\\leq\\frac{M\\ln{\\left(1+2M\\left(d+L_{z}^{2}\\right)+2M d M\\right)}}{2\\ln{\\left(1+M L_{z}^{2}\\right)}}+\\frac{M}{2}\\qquad\\qquad\\qquad\\qquad(\\mathrm{loosely\\;apply\\;}N_{k,1,1}\\geq d M)}\\\\ &{\\leq M\\ln(d M).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Theorem J.2. Algorithm 3 guarantees that with probability at least $1-\\delta$ the best arm is returned, and the algorithm terminates in at most ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1+\\omega)\\Bigg(\\bigg(L_{\\nu}\\log\\bigl(1/\\delta\\bigr)+L_{\\eta}\\|\\theta\\|_{2}^{2}\\Big(d+\\log\\bigl(1/\\delta\\bigr)\\Big)\\bigg)\\rho^{*}+\\Big(d+\\log\\bigl(1/\\delta\\bigr)\\Big)\\left(L_{\\eta}\\|\\theta\\|_{2}^{2}\\rho_{0}+M\\right)\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "pulls, ignoring both of the additive and multiplicative logarithms of $\\Delta,|\\mathcal{W}|,\\rho^{*},\\rho_{0},M$ where ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\rho^{*}=\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w\\in W\\backslash\\{w^{*}\\}}\\frac{\\|w^{*}-w\\|_{(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma)^{-1}}^{2}}{\\langle w^{*}-w,\\theta\\rangle^{2}},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\rho_{0}=\\operatorname*{max}_{w\\in\\mathcal{W}\\backslash\\{w^{*}\\}}\\|w^{*}-w\\|_{(\\sum_{z\\in{\\mathcal{Z}}}\\lambda_{E,z}\\Gamma^{\\top}z z^{\\top}\\Gamma)^{-1}}^{2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and ", "page_idx": 37}, {"type": "equation", "text": "$$\nM=\\frac{32L_{\\eta}}{\\gamma_{\\operatorname*{min}}^{2}\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},I)\\big)}\\vee1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Note that $\\rho_{0}$ does not get hurt by $\\left\\langle w^{*}-w,\\theta\\right\\rangle$ .It comes from the fact that in the first phase, we initialize that algorithm with $E$ -optimal design. ", "page_idx": 37}, {"type": "text", "text": "Proof. Part 1: correctness of the algorithm ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The idea of the proof is similar to the proof of Theorem 3.1. ", "page_idx": 37}, {"type": "text", "text": "Recall that the confidence interval of P-2SLS can be break down into two terms (12) ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\biggl\\langle\\widehat{\\theta}_{\\mathrm{P-2SLS}}-\\theta,w\\biggr\\rangle=\\sum_{q=1}^{T_{2}}\\biggl\\langle\\left(\\sum_{t=1}^{T_{2}}z_{I_{t}}z_{I_{t}}^{\\top}\\widehat{\\Gamma}\\right)^{-1}z_{I_{q}},w\\biggr\\rangle\\nu_{q}+\\biggl\\langle\\left(\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}\\right)\\Gamma\\theta,w\\biggr\\rangle.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Given $\\widehat{\\Gamma}$ , for a $w\\in\\mathscr{W}$ with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ the frst term satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{q=1}^{T_{2}}\\biggl\\langle\\left(\\sum_{t=1}^{T_{2}}z_{I_{t}}z_{I_{t}}^{\\top}\\widehat\\Gamma\\right)^{-1}z_{I_{q}},w\\biggr\\rangle\\nu_{q}\\leq\\|w\\|_{\\bar{A}(Z_{T_{2}},\\widehat\\Gamma)^{-1}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\biggl(\\frac{4}{\\delta}\\biggr)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For any $w\\in\\mathcal{W}$ , with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ , the second term satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Biggl\\langle\\Bigl(\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}\\Bigr)\\Gamma\\theta,w\\Biggr\\rangle\\leq\\|w\\|_{\\bar{A}(Z_{T_{1}},\\widehat{\\Gamma})^{-1}}\\|\\theta\\|_{2}\\sqrt{\\sigma_{\\eta}^{2}\\overline{{\\log}}\\bigl(Z_{T_{1}},\\delta/4\\bigr)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Note that by Lemma G.4, the above inequality holds for all $w\\in\\mathcal{W}$ , and the RHS is essentially a result of ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\|V^{-1/2}Z_{T}^{\\top}S\\right\\|_{\\mathrm{op}}\\leq\\sqrt{\\sigma_{\\eta}^{2}\\overline{{\\log}}\\big(Z_{T},\\delta/4\\big)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the vanilla form of the confidence of P-2SLS, we can define good events as ", "page_idx": 37}, {"type": "text", "text": "\u00b7 for the first term, for any $w\\in\\mathcal{W}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{q=1}^{T_{2}}\\biggl\\langle\\left(\\sum_{t=1}^{T_{2}}z_{I_{t}}z_{I_{t}}^{\\top}\\widehat\\Gamma\\right)^{-1}z_{I_{q}},w\\biggr\\rangle\\nu_{q}\\leq\\|w\\|_{\\bar{A}(Z_{T_{2}},\\widehat\\Gamma)^{-1}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\left(16k^{2}|W|/\\delta\\right)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "\u00b7 for the secon term, $\\left\\|V^{-1/2}Z_{T}^{\\top}S\\right\\|_{\\mathrm{op}}\\leq\\sqrt{\\sigma_{\\eta}^{2}\\overline{{\\log}}\\big(Z_{T},\\delta/4\\big)}.$ ", "page_idx": 37}, {"type": "text", "text": "For our algorithm design, since we use the doubling trick for the first sub-phase, we need to define the good event for the first sub-phase as the samples from each doubling trick iteration satisfies the self-normalized concentration inequality of Lemma G.4. ", "page_idx": 38}, {"type": "text", "text": "We define the good event for $\\ell$ -th doubling trick iteration in the first sub-phase of phase $k$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k,\\ell}^{1}=\\left\\{\\left\\Vert V_{k,\\ell}^{-1/2}Z_{k,\\ell}^{\\top}S_{k,\\ell}\\right\\Vert_{\\mathrm{op}}^{2}\\leq\\sigma_{\\eta}^{2}\\overline{{\\log}}\\bigg(Z_{k,\\ell},\\frac{\\delta}{4k^{2}(\\ell+1)^{2}}\\bigg)\\right\\},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $Z_{k,\\ell}=Z_{k,0,\\ell}\\cup Z_{k,1,\\ell}$ $V_{k,\\ell}\\,=\\,Z_{k,\\ell}^{\\top}Z_{k,\\ell}$ and $S_{k,\\ell}$ is stacked noise matrix during collcting samples $Z_{k,\\ell}$ per the model $X=Z\\Gamma+S$ . By a union bound, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left(\\bigcap_{k=1}^{\\infty}\\bigcap_{\\ell=1}^{\\infty}\\mathcal{E}_{k,\\ell}^{1}\\right)^{c}\\right)\\leq\\sum_{k=1}^{\\infty}\\sum_{\\ell=1}^{\\infty}\\mathbb{P}\\Big(\\mathcal{E}_{k,\\ell}^{1}\\Big)\\leq\\delta/2.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the second sub-phase in phase $k$ , we define the good event for the second sub-phase in phase $k$ and $w\\in\\mathscr{W}$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k,w}^{2}=\\left\\{\\sum_{q=1}^{N_{k,2}}\\biggl\\langle\\left(\\sum_{t=1}^{N_{k,2}}z_{I_{t}}z_{I_{t}}^{\\top}\\widehat{\\Gamma}\\right)^{-1}z_{I_{q}},w\\right\\rangle\\nu_{q}\\leq\\|w\\|_{\\bar{A}(Z_{k,2},\\widehat{\\Gamma})^{-1}}\\sqrt{2\\sigma_{\\nu}^{2}\\log\\left(16k^{2}|\\mathcal{W}|/\\delta\\right)}\\right\\}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By a union bound, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left(\\bigcap_{k=1}^{\\infty}\\bigcap_{w\\in\\mathcal{W}_{k}}\\mathcal{E}_{k,w}^{2}\\right)^{c}\\right)\\leq\\sum_{k=1}^{\\infty}\\sum_{w\\in\\mathcal{W}}\\mathbb{P}\\Big(\\mathcal{E}_{k,w}^{2}\\Big)\\leq\\delta/2.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Under the good event $\\begin{array}{r}{\\left(\\bigcap_{k=1}^{\\infty}\\bigcap_{\\ell=1}^{\\infty}\\mathcal{E}_{k,\\ell}^{1}\\right)\\bigcap\\left(\\bigcap_{k=1}^{\\infty}\\bigcap_{w\\in\\mathcal{W}_{k}}\\mathcal{E}_{k,w}^{2}\\right)}\\end{array}$ wehavewith proabiltyat ast $1-\\delta$ , for all $k$ and $w\\in\\mathscr{W}$ \uff0c ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left\\langle\\widehat{\\theta}_{\\mathrm{P-2SLS}}^{k}-\\theta,w-w^{*}\\right\\rangle\\right|\\leq\\zeta_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The rest of proof is same as the proof of Theorem 3.1. ", "page_idx": 38}, {"type": "text", "text": "Part 2: sample complexity of algorithm Sample complexity for first sub-phase ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\lambda_{E}\\,=\\,\\arg\\operatorname*{max}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\sigma_{\\operatorname*{min}}\\Bigl(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}z z^{\\top}\\Bigr)}\\end{array}$ is the $\\boldsymbol{\\mathrm E}$ -optimal design to maximize the minimum singular value of $\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}z z^{\\top}$ and $\\begin{array}{r}{\\kappa_{0}\\,=\\,\\operatorname*{max}_{\\lambda}\\sigma_{\\operatorname*{min}}\\bigl(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}z z^{\\top}\\bigr)}\\end{array}$ is the maximum minimum singular value of $\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}z z^{\\top}$ .At the beginning of first sub-phase in phase $k$ the algorithm first samples $N_{k,0,0}$ arms according to $\\lambda_{E}$ ", "page_idx": 38}, {"type": "text", "text": "Before we proceed to the main proof of the sample complexity, we first address a minor technique issue to avoid cumbersomeness. For the logarithmic term that appears in the algorithm and confidence interval, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\overline{{\\log}}(Z_{T},\\delta):=8d\\ln\\left(1+\\frac{2T L_{z}^{2}}{d(2\\wedge\\sigma_{\\operatorname*{min}}(Z_{T}^{\\top}Z_{T}))}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta}\\cdot\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}(Z_{T}^{\\top}Z_{T})}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "when $2\\wedge\\sigma_{\\mathrm{min}}(V)=2$ , it is equivalent to ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\widetilde{\\log}(T,\\delta):=8d\\ln\\left(1+{\\frac{T L_{z}^{2}}{d}}\\right)+16\\ln\\left({\\frac{2.6^{d}}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Our algorithm design guarantees that $2\\wedge\\sigma_{\\mathrm{min}}(V)=2$ is always true whenever we need to use the logarithmic term $\\overline{{\\log}}(Z_{T},\\delta)$ , given that the samples of our interest always includes the E-optimal ", "page_idx": 38}, {"type": "text", "text": "design samples $Z_{k,0,\\ell}$ and the number of samples from the E-optimal design $|Z_{k,0,\\ell}|$ is always larger than $\\frac{2}{\\kappa_{0}}$ . So for the remaining part of the proof, we willuse $\\widetilde{\\log}(N,\\delta)$ instead of $\\overline{{\\log}}(Z_{T},\\delta)$ ", "page_idx": 39}, {"type": "text", "text": "Denote the samples of E-optimal design that mixed into the samples from $\\ell$ -th doubling trick iteration in the first sub-phase of phase $k$ as $Z_{k,0,\\ell}$ and $\\left\\vert Z_{k,0,\\ell}\\right\\vert=N_{k,0,\\ell}$ . By Lemma J.3, our choice of $N_{k,0,\\ell}$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathrm{V}_{k,0,\\ell}\\geq\\frac{64g d\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\left(A(\\lambda_{E},\\Gamma)\\right)}\\ln\\left(\\frac{32g\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\left(A(\\lambda_{E},\\Gamma)\\right)}\\left(d+N_{k,1,\\ell}L_{z}^{2}+L_{z}^{2}\\right)\\right)+\\frac{128g\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\left(A(\\lambda_{E},\\Gamma)\\right)}\\ln\\left(\\frac{2\\cdot6\\left(\\lambda_{E}+\\lambda_{E}\\right)}{\\delta\\left(\\lambda_{E}+\\lambda_{E}\\right)}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "is a sufficient condition to guarantee ", "page_idx": 39}, {"type": "equation", "text": "$$\nN_{k,0,\\ell}\\geq\\frac{4g\\sigma_{\\eta}^{2}\\widetilde{\\log}\\big(N_{k,0,\\ell}+N_{k,1,\\ell},\\delta_{k,\\ell}\\big)}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},\\Gamma)\\big)}\\vee r(\\omega).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Multiply both sides of (22) by Nk.0,g+ Nk.1.e and we have Nk,0, ", "page_idx": 39}, {"type": "equation", "text": "$$\nN_{k,0,\\ell}+N_{k,1,\\ell}\\geq\\left(\\frac{4g\\sigma_{\\eta}^{2}\\widetilde{\\log}\\bigl(N_{k,0,\\ell}+N_{k,1,\\ell},\\delta_{k,\\ell}\\bigr)}{\\sigma_{\\operatorname*{min}}\\bigl(A(\\lambda_{E},\\Gamma)\\bigr)}\\vee r(\\omega)\\right)\\frac{N_{k,0,\\ell}+N_{k,1,\\ell}}{N_{k,0,\\ell}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By Property 1 of Lemma J.1, we have Qk, = Ne0.&+N.1.e , then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{k,0,\\ell}+N_{k,1,\\ell}\\geq\\cfrac{1}{\\alpha_{k,\\ell}}\\left(\\cfrac{4g\\sigma_{\\eta}^{2}\\widetilde{\\log}\\left(N_{k,0,\\ell}+N_{k,1,\\ell},\\delta_{k,\\ell}\\right)}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},\\Gamma)\\big)}\\vee r(\\omega)\\right)}\\\\ &{\\qquad\\qquad=\\cfrac{4g\\sigma_{\\eta}^{2}\\widetilde{\\log}\\left(N_{k,0,\\ell}+N_{k,1,\\ell},\\delta_{k,\\ell}\\right)}{\\sigma_{\\operatorname*{min}}\\big(A(\\alpha_{k,\\ell}\\lambda_{E},\\Gamma)\\big)}\\vee\\cfrac{r(\\omega)}{\\alpha_{k,\\ell}}}\\\\ &{\\qquad\\qquad\\geq\\cfrac{4g\\sigma_{\\eta}^{2}\\widetilde{\\log}\\left(N_{k,0,\\ell}+N_{k,1,\\ell},\\delta_{k,\\ell}\\right)}{\\sigma_{\\operatorname*{min}}\\big(A(\\alpha_{k,\\ell}\\lambda_{E}+(1-\\alpha_{k,\\ell})\\widetilde{\\lambda}_{k},\\Gamma)\\big)}\\vee r(\\omega).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "These condition on $N_{k,0,\\ell}$ and $N_{k,0,\\ell}+N_{k,1,\\ell}$ are needed for the proof. ", "page_idx": 39}, {"type": "text", "text": "Denote the total number of doubling trick iterations as $L_{k}$ forphase $k$ .In thecase of $L_{k}\\,=\\,1$ the samples from the first doubling trick iteration satisfies stopping condition of the first sub-phase already, and the algorithm will not enter the second doubling trick iteration. Thus the total number of samples for the first sub-phase is ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{N_{k,0,1}+N_{k,1,1}\\le2N_{k,1,1}\\le}&{(\\mathrm{Property~1~of~Lemma~})}\\\\ &{}&{\\le8g d M\\ln\\left(1+2M\\Big(d+L_{z}^{2}\\Big)+2M2g d M\\right)+16M\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta_{k,1}}\\right)\\lor r(\\omega).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In the case of $L_{k}>1$ , for $\\ell\\in\\{1,\\cdots\\,,L_{k}\\}$ , denote $\\widehat{\\Gamma}^{\\ell}$ as the estimate of $\\Gamma$ at the end of the $\\ell$ -th doubling trick iteration. With these notations, we have ", "page_idx": 39}, {"type": "text", "text": "\u00b7At the end of $L_{k}$ -th doubling trick iteration, the stopping condition is satisfied, i.e., ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\bigl\\|w-w^{\\prime}\\bigr\\|_{\\bar{A}(Z_{k,0,L_{k}}\\cup Z_{k,1,L_{k}},\\widehat\\Gamma^{L_{k}})^{-1}}^{2}\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log}\\bigl(N_{k,0,L_{k}}+N_{k,1,L_{k}},\\delta_{k,L_{k}}\\bigr)\\leq\\zeta_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "\u00b7At the end of $(L_{k}-1)$ -th doubling trick iteration, the stopping condition is not satisfied, i.e., ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{\\bar{A}(Z_{k,0,L_{k}-1}\\cup Z_{k,1,L_{k}-1},\\widehat\\Gamma^{L_{k}-1})^{-1}}^{2}\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log_{k,L_{k}-1}}>\\zeta_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Denote $\\xi_{L_{k}}$ as the empirical distribution of $Z_{k,0,L_{k}}\\cup Z_{k,1,L_{k}}$ . Then above two conditions imply that the number of samples for $L_{k}$ -th and $(L_{k}-1)$ -th doubling trick iterations respectively satisfy ", "page_idx": 40}, {"type": "equation", "text": "$$\nN_{k,0,L_{k}}+N_{k,1,L_{k}}\\geq\\frac{\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log_{k,L_{k}}}}{\\zeta_{k}^{2}}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\bigl\\|w-w^{\\prime}\\bigr\\|_{A(\\xi_{L_{k}},\\widehat{\\Gamma}^{L_{k}})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\nN_{k,0,L_{k}-1}+N_{k,1,L_{k}-1}<\\frac{\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log_{k,L_{k}-1}}}{\\zeta_{k}^{2}}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\xi_{L_{k}-1},\\widehat{\\Gamma}^{L_{k}-1})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Note that by Property 2 of Lemma J.1, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\big(N_{k,0,L_{k}}+N_{k,1,L_{k}}\\big)\\le N_{k,0,L_{k}-1}+N_{k,1,L_{k}-1}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus ", "page_idx": 40}, {"type": "equation", "text": "$$\nN_{k,0,L_{k}}+N_{k,1,L_{k}}<\\frac{2\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log_{k,L_{k}-1}}}{\\zeta_{k}^{2}}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{L_{k}-1},\\widehat{\\Gamma}^{L_{k}-1})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Forany $\\ell\\in\\{1,\\ldots,L_{k}\\}$ by Lemma J.7,the faetorof $\\begin{array}{r}{\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{\\ell},\\widehat{\\Gamma}^{\\ell})^{-1}}^{2}}\\end{array}$ can e uper bounded by ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi\\ell,\\widehat{\\Gamma}^{\\ell})^{-1}}^{2}}\\\\ &{\\le3\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi\\ell,\\widehat{\\Gamma}^{L_{k-1}})^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|((\\widehat{\\Gamma}^{\\ell})^{-\\top}-(\\widehat{\\Gamma}^{L_{k-1}})^{-\\top})\\big(w-w^{\\prime}\\big)\\right\\|_{\\left(\\sum_{z}\\xi\\ell z z^{\\top}\\right)^{-1}}^{2}}\\\\ &{\\le3\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi\\ell,\\widehat{\\Gamma}^{L_{k-1}})^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{\\ell})^{-\\top})\\big(w-w^{\\prime}\\big)\\right\\|_{\\left(\\sum_{z}\\xi\\ell z^{\\top}\\right)^{-1}}^{2}}\\\\ &{\\ \\ +\\,2\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{L_{k-1}})^{-\\top})\\big(w-w^{\\prime}\\big)\\right\\|_{\\left(\\sum_{z}\\xi\\ell z z^{\\top}\\right)^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We will upper bound the three terms in the RHS of (29) separately. ", "page_idx": 40}, {"type": "text", "text": "For the first term, by Lemma J.6, ", "text_level": 1, "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{\\ell},\\widehat{\\Gamma}^{L_{k-1}})^{-1}}^{2}\\leq4\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\tilde{\\lambda}_{k},\\widehat{\\Gamma}^{L_{k-1}})^{-1}}^{2},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\tilde{\\lambda}_{k}$ is the optimal design for $\\mathcal{W}_{k}$ in the first sub-phase of phase $k-1$ (based on the last doubling trick iteration), i.e., ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\widetilde{\\lambda}_{k}=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda,\\widehat{\\Gamma}^{L_{k-1}})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then for any $\\lambda$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\tilde{\\lambda}_{k},\\widehat{\\Gamma}^{L_{k-1}})^{-1}}^{2}}\\\\ &{\\leq3\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda,\\Gamma)^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{L_{k-1}})^{-\\top})(w-w^{\\prime})\\right\\|_{A(\\lambda,I)^{-1}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{b_{1}}{\\leq}3\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{\\prime},\\Gamma)^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{L_{k-1}})^{-\\top})(w-w^{\\prime})\\right\\|_{\\left(\\sum_{z}\\lambda_{k}^{\\prime}z z^{\\top}\\right)^{-1}}^{2}}\\\\ &{\\overset{b_{2}}{\\leq}6\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{L_{k-1}})^{-\\top})(w-w^{\\prime})\\right\\|_{\\left(\\sum_{z}\\lambda_{k}^{\\prime}z z^{\\top}\\right)^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where for $\\left(b_{1}\\right)$ , we plug in $\\lambda_{k}^{\\prime}:=\\alpha_{k}^{*}\\lambda_{E}+(1-\\alpha_{k}^{*})\\lambda_{k}^{*}$ , with $\\alpha_{k}^{*}\\leq1/2$ will be defined later and $\\lambda_{k}^{*}$ is the optimal design for $\\mathcal{W}_{k}$ given $\\Gamma$ , i.e., ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\lambda_{k}^{*}=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\big|\\big|w-w^{\\prime}\\big|\\big|_{A(\\lambda,\\Gamma)^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "$\\left(b_{2}\\right)$ is due to the fact that $\\alpha_{k}^{*}\\leq1/2$ . For the second term in the RHS of (31), given the condition (23), i.e., ", "page_idx": 41}, {"type": "equation", "text": "$$\nN_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}}\\geq\\frac{4g L_{\\eta}\\widetilde{\\log_{k-1,L_{k-1}}}}{\\sigma_{\\operatorname*{min}}\\mathopen{}\\mathclose\\bgroup\\left(A\\mathopen{}\\mathclose\\bgroup\\left(\\xi_{L_{k-1}},\\Gamma\\aftergroup\\egroup\\right)\\aftergroup\\egroup\\right)}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "by Lemma J.11 we have, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|(\\Gamma^{-\\tau}-(\\tilde{\\Gamma}^{L_{\\lambda-1}})^{-\\tau})(w-w^{\\prime})\\right\\|_{(\\sum_{k=\\lambda_{l}}\\leq\\pi^{\\tau})^{-1}}^{2}}\\\\ &{\\leq\\frac{2L_{w}^{2}\\widetilde{\\Theta}_{\\tilde{\\Lambda}_{k-1},L_{w-1}}}{\\sigma_{\\operatorname*{min}}(\\tilde{A}(\\lambda_{k},\\Gamma))}\\frac{1}{N_{\\lambda-1,0,k-1}+N_{\\lambda-1}\\jmath_{k-1}}\\|w-w^{\\prime}\\|_{A(\\xi_{k-1},\\Gamma)^{-1}}^{2}}\\\\ &{\\leq\\frac{1}{N_{\\lambda-1,0,k-1}}\\frac{2L_{w}\\widetilde{\\Theta}_{\\tilde{\\Lambda}_{k-1},L_{w-1}}}{\\sigma_{\\operatorname*{min}}(\\tilde{A}(\\lambda_{k},\\Gamma))}\\frac{N_{\\lambda-1,0,k-1}}{N_{\\lambda-1,0,k-1}+N_{\\lambda-1}\\jmath_{k-1}}\\|w-w^{\\prime}\\|_{A(\\xi_{k-1},\\Gamma)^{-1}}^{2}}\\\\ &{\\leq\\frac{1}{N_{\\lambda-1,0,k-1}}\\frac{2L_{w}\\widetilde{\\Theta}_{\\tilde{\\Lambda}_{k},\\Gamma}}{\\sigma_{\\operatorname*{min}}(\\tilde{A}(\\lambda_{k},\\Gamma_{1}))}\\frac{N_{\\lambda-1,0,k-1}}{N_{\\lambda-1,0,k-1}}\\frac{\\mathrm{D}_{\\lambda-1,0,k-1}}{N_{\\lambda-1,0,k-1}}\\|w-w^{\\prime}\\|_{A(\\xi_{k-1},\\Gamma)^{-1}}^{2}}\\\\ &{\\leq\\frac{1}{N_{\\lambda-1,0,k-1}}\\frac{2L_{w}\\widetilde{\\Theta}_{\\tilde{\\Lambda}_{k},\\Gamma}}{\\sigma_{\\operatorname*{min}}(\\tilde{A}(\\lambda_{k},\\Gamma_{1}))}\\frac{N_{\\lambda-1,0,k-1}}{N_{\\lambda-1,0,k-1}}\\frac{(N_{\\lambda}-1)L_{w}}{(N_{\\lambda}-1)^{\\lambda-1}}\\|w-w^{\\prime}\\|_{A(\\xi_{k-1},\\Gamma)^{-1}}^{2}}\\\\ &{\\leq\\frac{1}{N_{\\\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where for $\\left(b_{1}\\right)$ , we use the condition (22) on $N_{k-1,0,L_{k-1}}$ , and $\\left(b_{2}\\right)$ is due to Lemma J.5. Plug (32) into (31), we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\eta,w^{\\prime}\\in\\mathcal{W}_{k}}}\\bigl\\|w-w^{\\prime}\\bigr\\|_{A(\\tilde{\\lambda}_{k},\\hat{\\Gamma}^{L_{k-1}})^{-1}}^{2}\\leq6\\operatorname*{max}_{\\substack{w,w^{\\prime}\\in\\mathcal{W}_{k}}}\\bigl\\|w-w^{\\prime}\\bigr\\|_{A(\\tilde{\\lambda}_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{3}{g}\\operatorname*{max}_{\\substack{w,w^{\\prime}\\in\\mathcal{W}_{k}}}\\bigl\\|w-w^{\\prime}\\bigr\\|_{A(\\xi_{L_{k-1}},\\hat{\\Gamma}^{L_{k-1}})^{2}}^{2}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Plug (33) into (30), we have the frst term in the RHS of (29) can be upper bounded by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{\\ell},\\widehat{\\Gamma}^{L_{k-1}})^{-1}}^{2}}\\\\ &{\\leq24\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{12}{g}\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{L_{k-1}},\\widehat{\\Gamma}^{L_{k-1}})^{-1}}^{2}}\\\\ &{\\leq24\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{12}{g}\\frac{\\zeta_{k-1}^{2}}{\\|\\theta\\|_{2}^{2}L_{\\eta}\\widehat{\\log_{k-1,L_{k-1}}}}(N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where for the last inequality we use the fact that the stopping condition is satisfied at the end of $\\left(L_{k-1}\\right)$ -th doubling trick iteration for phase $k-1$ per (26). ", "page_idx": 41}, {"type": "text", "text": "For the second term in the RHS of (29), by Lemma J.10, when the condition (23) is satisfied, i.e. when ", "page_idx": 41}, {"type": "equation", "text": "$$\nN_{k,0,\\ell}+N_{k,1,\\ell}\\geq\\frac{4g L_{\\eta}\\widetilde{\\log}_{k,\\ell}}{\\sigma_{\\operatorname*{min}}\\big(A(\\xi_{\\ell},\\Gamma)\\big)},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{\\ell})^{-\\top})\\big(w-w^{\\prime}\\big)\\right\\|_{\\left(\\sum_{z}\\xi_{\\ell}z_{\\mathcal{Z}}\\tau\\right)^{-1}}^{2}\\le\\frac{1}{g}\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{\\ell},\\Gamma)^{-1}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\frac{6}{g}\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{\\ell},\\widehat{\\Gamma}^{\\ell})^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the last inequality is due to Lemma J.5. ", "page_idx": 41}, {"type": "text", "text": "For the third term in the RHS of (29), by Lemma J.11, when the condition (23) is satisfied, i.e., when ", "page_idx": 42}, {"type": "equation", "text": "$$\nN_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}}\\geq\\frac{4g L_{\\eta}\\widetilde{\\log_{k-1,L_{k-1}}}}{\\sigma_{\\operatorname*{min}}\\mathopen{}\\mathclose\\bgroup\\left(A\\mathopen{}\\mathclose\\bgroup\\left(\\xi_{L_{k-1}},\\Gamma\\aftergroup\\egroup\\right)\\aftergroup\\egroup\\right)}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{v=w_{m+1}}{\\operatorname*{max}}\\bigg\\|(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{L_{k-1}})^{-\\top})(w-w^{*})\\bigg\\|_{(\\sum_{k}\\in z:z^{*})^{-1}}^{2}}\\\\ &{\\leq\\frac{2I_{\\nabla}\\tilde{p}\\tilde{p}\\xi_{k-1,L_{k-1}}}{\\sigma_{\\operatorname*{min}}\\big(A(\\xi,\\Gamma)\\big)}\\frac{1}{N-1,0,L_{k-1}+N_{k-1,1,L_{k-1}}\\cdot w,w\\ast_{\\mathbb{R}}}\\frac{\\operatorname*{max}}{|w-w^{*}||_{A(\\xi_{k-1},\\Gamma)}^{2}-1}}\\\\ &{\\leq\\frac{2I_{\\nabla}\\tilde{p}\\tilde{p}\\xi_{k-1,L_{k-1}}}{\\sigma_{\\operatorname*{min}}\\big(A(\\alpha_{k},\\xi_{k})\\big)}\\frac{1}{N-1,0,L_{k-1}+N_{k-1,1,L_{k-1}}\\cdot w,w\\ast_{\\mathbb{R}}}\\frac{\\operatorname*{max}}{|w-w^{*}||_{A(\\xi_{k-1},\\xi_{k-1})}^{2}-1}}\\\\ &{\\overset{v_{1}}{\\leq}\\frac{N_{k,0,\\xi}+N_{k,1,L_{k}}}{N_{k,0,\\xi}}\\frac{1}{N-1,0,L_{k-1}+N_{k-1,1,L_{k-1}}}\\frac{2\\tilde{p}\\xi_{k-1,L_{k-1}}}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{k},\\Gamma)\\big)}(N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}})\\frac{1}{|\\|\\tilde{p}|}}\\\\ &{\\leq\\frac{N_{k,0,\\xi}+N_{k,1,L_{k}}}{N_{k,0,\\xi}}\\frac{2}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{k},\\Gamma)\\big)}\\frac{\\zeta_{k-1}^{2}}{\\|\\theta\\|_{2}^{2}}}\\\\ &{\\overset{b_{2}}{\\leq}\\frac{N_{k,0,\\xi}+N_{k,1,\\xi}\\zeta_{k-1}^{2}}{4\\mu_{\\operatorname*{min}}\\big(A(\\lambda_{k},\\xi_{k})\\big)}\\\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\left(b_{1}\\right)$ is due to (26), $\\left(b_{2}\\right)$ is due to (22). Plug (34), (35) and (36) into (29), we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{\\ell},\\widehat{\\Gamma}^{\\ell})^{-1}}^{2}}\\\\ &{\\le72\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{36}{g}\\frac{\\zeta_{k-1}^{2}}{\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log_{k-1,L_{k-1}}}}(N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}})}\\\\ &{\\ \\ +\\frac{12}{g}\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{\\ell},\\widehat{\\Gamma}^{\\ell})^{-1}}^{2}+\\frac{2}{g}\\frac{N_{k,0,\\ell+1}+N_{k,1,\\ell+1}}{L_{\\eta}\\widetilde{\\log_{k,\\ell}}}\\frac{\\zeta_{k-1}^{2}}{\\|\\theta\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "With $\\ell=L_{k}-1$ and the fact that $g>24$ whose exact value willbe set later, we can rearrange (37) as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{L_{k}-1},\\hat{\\Gamma}^{L_{k}-1})^{-1}}^{2}}\\\\ &{\\leq144\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{72}{g}\\frac{\\zeta_{k-1}^{2}}{\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log_{k-1,L_{k-1}}}\\left(N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}}\\right)}}\\\\ &{\\quad+\\frac{1}{g}\\frac{N_{k,0,L_{k}}+N_{k,1,L_{k}}}{L_{\\eta}\\widetilde{\\log_{k,L_{k-1}}}}\\frac{\\zeta_{k-1}^{2}}{\\|\\theta\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Note that the LHS of above can be lower bounded by (28), ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{\\zeta_{k}^{2}}{2\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log_{k,L_{k}-1}}}(N_{k,0,L_{k}}+N_{k,1,L_{k}})<\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\bigl\\|w-w^{\\prime}\\bigr\\|_{A(\\xi_{L_{k}-1},\\hat{\\Gamma}^{L_{k}-1})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Rearrange the terms in (39) and (38) and setting $g$ to be larger enough and using the fact that $\\zeta_{k}=\\zeta_{k-1}/2$ ,wehave ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{V}_{k,0,L_{k}}+N_{k,1,L_{k}}\\le\\frac{576}{\\zeta_{k}^{2}}\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log}_{k,L_{k}-1}\\operatorname*{max}_{w,w^{\\prime}\\in W_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{72}{g}\\frac{\\widetilde{\\log}_{k,L_{k}-1}}{\\widetilde{\\log}_{k-1,L_{k-1}}}(N_{k-1,0,L_{k}}-N_{k-1})\\|\\widetilde{\\log}_{k}\\|_{L_{\\infty}^{2}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Note that by definition $\\widetilde{\\log}_{k,L_{k}-1}<\\widetilde{\\log}_{k,L_{k}}$ , thus ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{V}_{k,0,L_{k}}+N_{k,1,L_{k}}\\le\\frac{576}{\\zeta_{k}^{2}}\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log}_{k,L_{k}}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{72}{g}\\frac{\\widetilde{\\log}_{k,L_{k}}}{\\widetilde{\\log}_{k-1,L_{k-1}}}(N_{k-1,0,L_{k-1}}+N_{k-1})^{-1}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Denote $D_{k}:=N_{k,0,L_{k}}+N_{k,1,L_{k}}$ and divide both sides of above by $\\widetilde{\\log}_{k,L_{k}}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{D_{k}}{\\widetilde{\\log}_{k,L_{k}}}\\leq\\frac{576}{\\zeta_{k}^{2}}\\|\\theta\\|_{2}^{2}L_{\\eta}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{72}{g}\\frac{D_{k-1}}{\\widetilde{\\log}_{k-1,L_{k-1}}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "In the case of $L_{k}=1$ , by Property 3 of Lemma J.1, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{D_{k}}{\\widetilde{\\log}_{k,L_{k}}}=\\frac{N_{k,1,1}}{8d\\ln\\left(1+\\frac{N_{k,1,1}L_{z}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}k^{2}}{\\delta}\\right)}\\leq M\\ln(d M).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thereby we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{D_{k}}{\\widetilde{\\log}_{k,L_{k}}}\\leq\\operatorname*{max}\\left\\{M\\ln(d M),\\frac{576}{\\zeta_{k}^{2}}\\|\\theta\\|_{2}^{2}L_{\\eta}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\bigl\\|w-w^{\\prime}\\bigr\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{72}{g}\\frac{D_{k-1}}{\\widetilde{\\log}_{k-1,L_{k-1}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Taking a summation over $k$ on both sides of (40), we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=1}^{K^{*}}\\frac{D_{k}}{|\\overline{{\\Theta}}_{\\mathbb{E},L_{1}}}=\\frac{D_{1}}{|\\overline{{\\Theta}}_{\\mathbb{E},L_{1}}+\\sum_{k=2}^{K^{*}}}\\frac{D_{k}}{|\\overline{{\\Theta}}_{\\mathbb{E},L_{1}}}}&{\\quad{\\mathrm{(41)}}}\\\\ &{\\leq\\displaystyle\\frac{D_{1}}{|\\overline{{\\Theta}}_{\\mathbb{E},1,L_{1}}}+\\sum_{k=2}^{K^{*}}\\operatorname*{max}\\left\\{M\\ln(d M),\\frac{576}{\\zeta_{k}^{2}}\\|\\theta\\|_{2}^{2}L_{\\eta}\\operatorname*{max}_{\\omega^{\\prime}\\in W_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{72}{g}\\frac{1}{\\mathrm{\\tilde{k}}}}\\\\ &{\\quad+\\displaystyle\\frac{D_{1}}{|\\overline{{\\Theta}}_{\\mathbb{E},1,L_{1}}+\\sum_{k=2}^{K}}M\\ln(d M)+\\sum_{k=2}^{K^{*}}\\left(\\frac{576}{\\zeta_{k}^{2}}\\|\\theta\\|_{2}^{2}L_{\\eta}\\operatorname*{max}_{\\omega^{\\prime}\\in W_{k}}\\|w-w^{\\prime}\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\frac{72}{g}\\frac{1}{\\mathrm{\\tilde{k}}}\\right)}\\\\ &{\\leq\\displaystyle\\frac{D_{1}}{|\\overline{{\\Theta}}_{\\mathbb{E},1,L_{1}}+\\sum_{k=2}^{K}}M\\ln(d M)+\\sum_{k=2}^{K^{*}}\\frac{576}{\\zeta_{k}^{2}}\\|\\theta\\|_{2}^{2}L_{\\eta}\\operatorname*{max}_{\\omega^{\\prime}\\in W_{k}}\\|w-w^{\\prime}\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\sum_{k=2}^{K^{*}}\\frac{72}{g}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus by setting $g=72\\times2$ and rearranging the terms, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K^{*}}\\frac{D_{k}}{\\widetilde{\\log}_{k,L_{k}}}\\leq\\frac{2D_{1}}{\\widetilde{\\log}_{1,L_{1}}}+2\\sum_{k=2}^{K^{*}}M\\ln(d M)+2\\sum_{k=2}^{K^{*}}\\frac{576}{\\zeta_{k}^{2}}\\|\\theta\\|_{2}^{2}L_{\\eta}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\|w-w^{\\prime}\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For $D_{1}$ , which corresponds to the first sub-phase where we use $\\boldsymbol{\\mathrm E}$ -optimal design with doubling trick, we have the stopping condition, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{1}}\\bigl|\\bigl|w-w^{\\prime}\\bigr|\\bigr|_{\\bar{A}(Z_{1,0,L_{1}},\\widehat{\\Gamma}^{L_{1}})^{-1}}^{2}\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log_{1,L_{1}}}\\leq\\zeta_{1}^{2}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This implies that when the stopping condition is met ", "page_idx": 43}, {"type": "equation", "text": "$$\nN_{1,0,L_{1}}\\geq\\frac{\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log}_{1,L_{1}}}{\\zeta_{1}^{2}}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{1}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\xi_{L_{1}},\\widehat{\\Gamma}^{L_{1}})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and ", "page_idx": 43}, {"type": "equation", "text": "$$\nN_{1,0,L_{1}-1}<\\frac{2\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log}_{1,L_{1}-1}}{\\zeta_{1}^{2}}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{1}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\xi_{L_{1}-1},\\widehat{\\Gamma}^{L_{1}-1})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "S $\\big\\|\\boldsymbol{w}-\\boldsymbol{w}^{\\prime}\\big\\|_{A(\\xi_{L_{1}-1},\\widehat{\\Gamma}^{L_{1}-1})^{-1}}^{2}$ can be upper bounded by ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{L_{1}-1},\\widehat{\\Gamma}^{L_{1}-1})^{-1}}^{2}}\\\\ &{\\leq3\\big\\|w-w^{\\prime}\\big\\|_{A(\\xi_{L_{1}-1},\\Gamma)^{-1}}^{2}+2\\Big\\|(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{L_{1}-1})^{-\\top})\\big(w-w^{\\prime}\\big)\\Big\\|_{\\left(\\sum_{z}\\xi_{L_{1}-1}z z^{\\top}\\right)^{-1}}^{2}}\\\\ &{\\leq3\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{E},\\Gamma)^{-1}}^{2}+2\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{E},\\Gamma)^{-1}}^{2}}\\\\ &{\\leq6\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{E},\\Gamma)^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Plug (48) into (47), and use the fact that $2N_{1,0,L_{1}-1}=N_{1,0,L_{1}},\\zeta_{1}=1$ ,we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{V}_{1,0,L_{1}}\\leq24\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log}_{1,L_{1}-1}\\underset{w,w^{\\prime}\\in\\mathcal{W}_{1}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{E},\\Gamma)^{-1}}^{2}\\leq24\\|\\theta\\|_{2}^{2}L_{\\eta}\\widetilde{\\log}_{1,L_{1}}\\underset{w,w^{\\prime}\\in\\mathcal{W}_{1}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{E},\\Gamma)^{-1}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus we have, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{D_{1}}{\\widetilde{\\log}_{1,L_{1}}}\\leq\\frac{2N_{1,0,L_{1}}}{\\widetilde{\\log}_{1,L_{1}}}\\leq48\\|\\theta\\|_{2}^{2}L_{\\eta}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{1}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{E},\\Gamma)^{-1}}^{2}=:\\rho_{1}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "By the same calculation as (17) and (19), we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|\\theta\\|_{2}^{2}L_{\\eta}\\sum_{k=2}^{K^{*}}\\frac{1}{\\zeta_{k}^{2}}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\bigl\\|w-w^{\\prime}\\bigr\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}\\leq c\\|\\theta\\|_{2}^{2}L_{\\eta}K^{*}\\rho^{*}=:\\rho_{2},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $c$ is an absolute constant. Next we lower bound the left hand side of (45). To do this, we first upper bound $\\widetilde{\\log}_{k,L_{k}}$ as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\log}_{k,L_{k}}=8d\\ln\\left(1+\\frac{D_{k}L_{z}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}k^{2}L_{k}^{2}}{\\delta}\\right)}\\\\ &{\\qquad\\qquad\\leq8d\\ln\\left(1+\\frac{D_{k}L_{z}^{2}}{d}\\right)+32\\ln(L_{k})+16\\ln\\left(\\frac{2\\cdot6^{d}k^{2}}{\\delta}\\right)}\\\\ &{\\qquad\\leq8d\\ln\\left(1+\\frac{D_{k}L_{z}^{2}}{d}\\right)+32\\ln\\left(\\log_{2}\\!\\left(\\frac{D_{k}}{d}\\right)\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}k^{2}}{\\delta}\\right)}\\\\ &{\\qquad\\leq32d\\ln\\left(1+\\frac{D_{k}L_{z}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}k^{2}}{\\delta}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the inequality above uses the fact that $L_{k}$ is the index of the last doubling trick iteration for phase $k$ , by the design of the doublin rck, we have $\\begin{array}{r}{L_{k}\\leq\\log_{2}\\!\\left(\\frac{D_{k}}{d}\\right)}\\end{array}$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{k=1}^{K^{*}}{\\frac{D_{k}}{\\mathrm{log}_{k,L_{k}}}}=\\displaystyle\\sum_{k=1}^{K^{*}}{\\frac{D_{k}}{\\mathrm{sidn}\\left(1+\\frac{D_{k}L_{k}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\phi^{d(k,L_{k}^{2})}}{\\delta}\\right)}}}}\\\\ {{\\displaystyle\\sum_{k=1}^{K^{*}}{\\frac{\\kappa^{*}}{32d\\ln\\left(1+\\frac{D_{k}L_{k}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\phi^{d(k,L_{k}^{2})}}{\\delta}\\right)}}}}\\\\ {{\\displaystyle\\sum_{k=1}^{K^{*}}{\\frac{D_{k}}{32d\\ln\\left(1+\\frac{\\sum_{k=1}^{K^{*}}D_{k}L_{k}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\phi^{d(k,L_{k}^{2})}}{\\delta}\\right)}}}}\\\\ {{\\displaystyle=\\frac{1}{32d\\ln\\left(1+\\frac{\\sum_{k=1}^{K^{*}}D_{k}L_{k}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\phi^{d(k,L_{k}^{2})}}{\\delta}\\right)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Denote $\\rho_{3}:=K^{*}M\\ln(d M)$ , looking back at (44), we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K^{*}}D_{k}\\leq\\left(32d\\ln\\left(1+\\frac{\\sum_{k=1}^{K^{*}}D_{k}L_{z}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}K^{*2}}{\\delta}\\right)\\right)(\\rho_{1}+\\rho_{2}+\\rho_{3}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "By Lemma K.5, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K^{*}}D_{k}\\leq32(\\rho_{1}+\\rho_{2}+\\rho_{3})\\ln\\left(\\frac{2\\cdot6^{d}K^{*2}}{\\delta}\\right)+64d(\\rho_{1}+\\rho_{2}+\\rho_{3})\\ln\\left(3+(\\rho_{1}+\\rho_{2}+\\rho_{3})\\frac{2L_{z}^{2}}{d}\\right)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Sample complexity for second sub-phase ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "The design for the second sub-phase of phase $k$ is based on $\\widehat{\\Gamma}^{L_{k}}$ , the estimate of $\\Gamma$ at the end of the $L_{k}$ -th doubling trick iteration in the first sub-phase of phase $k$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\hat{\\lambda}_{k}=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda,\\widehat{\\Gamma}^{L_{k}})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then for any $\\lambda$ , by Lemma J.7 we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\hat{\\lambda}_{k},\\hat{\\Gamma}^{L_{k}})^{-1}}^{2}}\\\\ &{\\leq3\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda,\\Gamma)^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\left\\|(\\Gamma^{-\\top}-(\\widehat\\Gamma^{L_{k}})^{-\\top})\\big(w-w^{\\prime}\\big)\\right\\|_{A(\\lambda,I)^{-1}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We plug in $\\lambda_{k}^{\\prime\\prime}:=\\alpha_{k}^{*}\\lambda_{E}+(1-\\alpha_{k}^{*})\\lambda_{k}^{*}$ with $\\alpha_{k}^{*}\\leq1/2$ will be defined later and $\\lambda_{k}^{*}$ is the optimal design for $\\mathcal{W}_{k}$ \uff0c ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\hat{\\lambda}_{k},\\widehat{\\Gamma}^{L_{k}})^{-1}}^{2}}\\\\ &{\\leq3\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{k}^{\\prime\\prime},\\Gamma)^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\Big\\|\\big(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{L_{k}})^{-\\top}\\big)\\big(w-w^{\\prime}\\big)\\Big\\|_{\\left(\\sum_{z}\\lambda_{k}^{\\prime\\prime}z\\geq\\tau\\right)^{-1}}^{2}}\\\\ &{\\overset{\\leq6}{\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+2\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{\\operatorname*{max}}\\Big\\|\\big(\\Gamma^{-\\top}-(\\widehat{\\Gamma}^{L_{k}})^{-\\top}\\big)\\big(w-w^{\\prime}\\big)\\Big\\|_{\\left(\\sum_{z}\\lambda_{k}^{\\prime\\prime}z z\\geq\\tau\\right)^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the last inequality is due to the fact that $\\alpha_{k}^{*}\\leq1/2$ . For the second term in the RHS of above, by Lemma J.11 we have, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|(\\Gamma^{-7}-(\\widetilde{\\Gamma}^{k})^{-7})(w-w^{\\prime})\\right\\|_{(\\sum_{k,\\ell}\\leq\\ell_{2}^{\\prime})^{-1}}^{2}}\\\\ &{\\leq\\frac{4L_{w}^{5}\\beta_{\\ell k,\\ell_{3}}}{\\sigma_{\\eta\\eta}\\ln\\left(A_{\\ell}(\\lambda_{k}^{\\prime},\\Gamma_{k})\\right)}\\left\\|w-w^{\\prime}\\right\\|_{A_{\\ell}(\\lambda_{k}^{\\prime},\\ell_{2}^{\\prime},\\ell_{1}^{\\prime})^{-1}}^{2}}\\\\ &{\\leq\\frac{1}{\\mathcal{N}_{k,\\ell,\\ell}}\\frac{1}{\\alpha\\ln\\left(B_{\\ell}(\\frac{\\lambda_{k}^{\\prime}\\beta_{\\ell}}{\\ell_{1}^{\\prime}})\\right)}\\widetilde{w}_{k}\\widetilde{w}_{k}h_{\\ell k,\\ell_{1}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\ell_{1},\\ell_{2}^{\\prime})^{-1}}^{2}}\\\\ &{\\leq\\frac{4L_{w}^{5}\\beta_{\\ell k,\\ell_{3}}}{\\mathcal{N}_{k,\\ell,\\ell}\\ln\\left(A_{\\ell}(\\lambda_{k}^{\\prime},\\Gamma_{k})\\right)}\\frac{N_{\\ell k}\\beta_{\\ell k,\\ell_{1}}}{N_{\\ell k,\\ell_{1}}\\Delta_{\\ell_{1}}+N_{\\ell k,\\ell_{1}}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\ell_{1},\\ell_{2}^{\\prime})^{-1}}^{2}}\\\\ &{\\leq\\frac{1}{\\mathcal{N}_{k,\\ell,\\ell}}\\frac{4L_{w}|\\phi_{k}|_{\\ell_{2}^{\\prime},\\ell_{1}^{\\prime}}}{\\sigma_{\\eta\\eta}\\ln\\left(A_{\\ell}(\\lambda_{k}^{\\prime},\\Gamma_{k})\\right)}\\Big\\|w-w^{\\prime}\\Big\\|_{A(\\ell_{1},\\ell_{1}^{\\prime})^{-1}}^{2}}\\\\ &{\\overset{()}{\\leq}\\frac{1}{\\mathcal{N}_{k,\\ell,\\ell}}\\frac{4L_{w}|\\phi_{k}|_{\\ell_{2}^{\\prime}}}{\\sigma_{\\eta}\\ln\\left(B_{\\ell}(\\lambda_{k}^{\\prime},\\Gamma_{k \n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where for $\\left(b_{1}\\right)$ , we have $\\begin{array}{r}{N_{k,0,L_{k}}\\geq\\frac{4g L_{\\eta}\\widetilde{\\log_{k,L_{k}}}}{\\sigma_{\\operatorname*{min}}\\left(A(\\lambda_{E},\\Gamma)\\right)}}\\end{array}$ , and $\\left(b_{2}\\right)$ is due to Lemma J.5. Plug (53) into (52), we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\operatorname*{max}_{\\substack{\\eta,w^{\\prime}\\in\\mathcal{W}_{k}}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\hat{\\lambda}_{k},\\hat{\\Gamma}^{L_{k}})^{-1}}^{2}\\leq6\\displaystyle\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\displaystyle\\frac{2}{g}\\displaystyle\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\xi_{L_{k}},\\hat{\\Gamma}^{L_{k}})^{-1}}^{2}}\\\\ {\\leq6\\displaystyle\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}+\\displaystyle\\frac{2}{g}\\displaystyle\\frac{\\zeta_{k}^{2}}{\\|\\theta\\|_{2}^{2}L_{\\eta}\\overline{{\\log_{k,L_{k}}}}}(N_{k,0,L_{k}}+N_{k,1,2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the last inequality is due to (26). According to the algorithm design, the number of samples in the second sub-phase of phase $k$ is defined as ", "page_idx": 46}, {"type": "equation", "text": "$$\nN_{k,2}=\\left\\lceil2(1+\\omega)\\zeta_{k}^{-2}\\rho(\\mathcal{W}_{k})L_{\\nu}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)\\right\\rceil\\vee r(\\omega),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with $\\begin{array}{r}{\\rho(\\mathcal{W}_{k})=\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\|w-w^{\\prime}\\|_{A(\\lambda,\\widehat{\\Gamma}^{L_{k}})^{-1}}^{2}}\\end{array}$ Then we have, by sting $g\\geq4$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathrm{V}_{k,2}\\underset{\\approx}{\\lesssim}(1+\\omega)\\zeta_{k}^{-2}L_{\\nu}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}_{k}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}\\mathrm{log}\\Bigg(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\Bigg)+(1+\\omega)(N_{k,0,L_{k}}+N_{k,1,L_{k}}),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\because$ hides logarithmic factors of $\\vert\\mathcal{W}\\vert$ for the second term and constants for simplicity. Plug (54) into the above inequality. Also note that by Lemma 2.1, we can always set $L_{\\nu}=2(\\|\\theta\\|_{2}^{2}L_{\\eta}+1)$ Thus, ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=1}^{\\mathbb{K}^{*}}N_{k,2}\\,\\underset{\\Tilde{\\approx}}{\\lesssim}(1+\\omega)\\sum_{k=1}^{K^{*}}\\zeta_{k}^{-2}L_{\\nu\\underset{w,w^{\\prime}\\in\\mathcal{W}_{k}}{w}}\\big\\|w-w^{\\prime}\\big\\|_{A(\\lambda_{k}^{*},\\Gamma)^{-1}}^{2}\\log\\left(\\frac{4k^{2}|\\mathcal{W}|}{\\delta}\\right)+(1+\\omega)\\sum_{k=1}^{K^{*}}(N_{k,0,L_{k}}+1)\\frac{\\|w\\|_{\\infty}^{2}}{k}}\\\\ &{\\qquad\\qquad\\lesssim(1+\\omega)K^{*}L_{\\nu}\\rho^{*}\\log\\left(\\frac{4K^{*2}|\\mathcal{W}|}{\\delta}\\right)+(1+\\omega)\\sum_{k=1}^{K^{*}}(N_{k,0,L_{k}}+N_{k,1,L_{k}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "This essentially means that the sample complexity for the second sub-phase $\\sum N_{k,2}$ canbe upper bounded by summation of the sample complexity we pay for Algorithm 1 and the sample complexity of the first sub-phase $\\begin{array}{r}{\\sum_{k=1}^{K^{*}}(N_{k,0,L_{k}}+\\bar{N_{k,1,L_{k}}})}\\end{array}$ . Combine (51) and (55), we conclude the result. ", "page_idx": 46}, {"type": "text", "text": "Lemma J.3. Denote ", "text_level": 1, "page_idx": 46}, {"type": "equation", "text": "$$\n\\widetilde{\\log}\\big(N_{k,0},\\delta_{k,0}\\big)=8d\\ln\\left(1+{\\frac{N_{k,0}L_{z}^{2}}{d}}\\right)+16\\ln\\left({\\frac{2\\cdot6^{d}}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "A sufficient condition for ", "page_idx": 46}, {"type": "equation", "text": "$$\nN_{k,0,\\ell}\\geq\\frac{g\\sigma_{\\eta}^{2}\\widetilde{\\log}\\big(N_{k,0,\\ell}+N_{k,1,\\ell},\\delta_{k,\\ell}\\big)}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},\\Gamma)\\big)}\\vee r(\\omega).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "to hold is ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathrm{V}_{k,0,\\ell}\\geq\\frac{16g d\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\left(A(\\lambda_{E},\\Gamma)\\right)}\\ln\\left(\\frac{8g}{\\sigma_{\\operatorname*{min}}\\left(A(\\lambda_{E},\\Gamma)\\right)}\\left(d+N_{k,1,\\ell}L_{z}^{2}+L_{z}^{2}\\right)\\right)+\\frac{32g\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\left(A(\\lambda_{E},\\Gamma)\\right)}\\ln\\left(\\frac{2\\cdot6\\left(\\lambda_{E}+\\lambda_{E}\\right)}{\\delta\\left(\\lambda_{E}+\\lambda_{E}\\right)}\\right),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. Given ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\widetilde{\\log}\\big(N_{k,0,\\ell}+N_{k,1,\\ell},\\delta_{k,\\ell}\\big)=8d\\ln\\left(1+\\frac{(N_{k,0,\\ell}+N_{k,1,\\ell})L_{z}^{2}}{d}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "By Lemma J.4, for the formula ", "page_idx": 46}, {"type": "equation", "text": "$$\nX\\geq A\\ln\\left(D+B X\\right)+C\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\,\\,A=\\frac{8g d\\sigma_{\\eta}^{2}}{\\sigma_{\\mathrm{min}}\\left(A(\\lambda_{E},\\Gamma)\\right)},}\\\\ &{\\bullet\\,\\,B=\\frac{L_{z}^{2}}{d},}\\\\ &{\\bullet\\,\\,C=\\frac{16g\\sigma_{\\eta}^{2}}{\\sigma_{\\mathrm{min}}\\left(A(\\lambda_{E},\\Gamma)\\right)}\\ln{\\left(\\frac{2\\cdot6^{d}}{\\delta}\\right)},}\\\\ &{\\bullet\\,\\,D=1+\\frac{N_{k,1,\\ell}L_{z}^{2}}{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus a suffcient condition for the inequality to hold is ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{V}_{k,0,\\ell}\\geq\\frac{16g d\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},\\Gamma)\\big)}\\ln\\left(\\frac{8g d\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},\\Gamma)\\big)}\\left(1+\\frac{N_{k,1,\\ell}L_{z}^{2}}{d}+\\frac{L_{z}^{2}}{d}\\right)\\right)+\\frac{32g\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},\\Gamma)\\big)}\\ln\\left(\\frac{2\\cdot6^{d}}{4}\\right)}\\\\ {=\\frac{16g d\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},\\Gamma)\\big)}\\ln\\left(\\frac{8g\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},\\Gamma)\\big)}\\left(d+N_{k,1,\\ell}L_{z}^{2}+L_{z}^{2}\\right)\\right)+\\frac{32g\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda_{E},\\Gamma)\\big)}\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Lemma J.4. Let $X\\geq1,A,B\\geq0,$ then a sufficient condition for $X\\geq A\\ln\\left(D+B X\\right)+C$ is ", "page_idx": 47}, {"type": "equation", "text": "$$\nX\\geq2A\\ln(A D+A B)+2C.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. The proof is motivated by Gales et al. [16]. Let $f\\in(0,1)$ , then ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\,X\\geq A\\ln\\left(D+B X\\right)+C}\\\\ &{\\Leftrightarrow X\\geq A\\ln\\Biggl(\\frac{f X}{A}\\Biggr)+A\\ln\\Biggl(\\frac{A}{f}\\Biggr(\\frac{D}{X}+B\\Biggr)\\Biggr)+C}\\\\ &{\\Leftarrow X\\geq A\\Biggl(\\frac{f X}{A}-1\\Biggr)+A\\ln\\Biggl(\\frac{A}{f}\\Biggr(\\frac{D}{X}+B\\Biggr)\\Biggr)+C}\\\\ &{\\Leftarrow X(1-f)\\geq A\\ln\\Biggl(\\frac{A}{2f}\\Biggr(\\frac{D}{X}+B\\Biggr)\\Biggr)+C}\\\\ &{\\Leftrightarrow X\\geq\\frac{1}{1-f}A\\ln\\Biggl(\\frac{A}{2f}\\Biggr(\\frac{D}{X}+B\\Biggr)\\Biggr)+\\frac{c}{1-f}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Set $f=1/2$ and by the fact $X\\geq1$ , we have ", "page_idx": 47}, {"type": "equation", "text": "$$\nX\\geq2A\\ln(A D+A B)+2C.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Lemma J.5. Suppose that we have a data set $\\{Z_{T},X_{T}\\}$ . Denote the empirical distribution of $Z_{T}$ as $\\xi$ .The number of samples satisfies ", "page_idx": 47}, {"type": "equation", "text": "$$\nT\\geq\\frac{8\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\xi,\\Gamma)\\big)}\\overline{{\\log}}(Z_{T},\\delta)\\vee r(\\omega).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "$\\widehat{\\Gamma}$ is theOLS estimate ofT based on $\\{Z_{T},X_{T}\\}$ Then ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\|w\\right\\|_{A(\\xi,\\Gamma)^{-1}}^{2}\\leq6\\big\\|w\\big\\|_{A(\\xi,\\widehat{\\Gamma})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. By Lemma J.7, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|w\\right\\|_{A(\\xi,\\Gamma)^{-1}}^{2}\\leq3\\|w\\|_{A(\\xi,\\widehat{\\Gamma})^{-1}}^{2}+2\\Big\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})w\\Big\\|_{A(\\xi,I)^{-1}}^{2}}\\\\ &{\\qquad\\qquad\\leq3\\|w\\|_{A(\\xi,\\widehat{\\Gamma})^{-1}}^{2}+\\displaystyle\\frac{1}{2}\\|w\\|_{A(\\xi,\\Gamma)^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where the last inequality is due to Lemma J.10 with $g=2$ . Rearranging the terms, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\|w\\right\\|_{A(\\xi,\\Gamma)^{-1}}^{2}\\leq6\\big\\|w\\big\\|_{A(\\xi,\\widehat{\\Gamma})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Lemma J.6. Suppose that we use ROUND to sample $N_{0}$ arms according to $\\lambda_{E}$ denoted as $Z_{0}$ and $N_{1}$ arms accordingto $\\lambda_{1}$ denoted as $Z_{1}$ ,with $N_{1}\\geq N_{0}$ .Denote the empirical distribution of all the collected samples as $\\xi$ then ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\|w\\right\\|_{A(\\xi,\\Gamma)}^{2}\\leq4\\big\\|w\\big\\|_{A(\\lambda_{1},\\Gamma)^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof. Denote the empirical distribution of $Z_{0}$ as $\\xi_{0}$ , and the empirical distribution of $Z_{1}$ as $\\xi_{1}$ , then wehave ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\pi_{\\mathrm{S},(a,b),\\varepsilon,\\mathbf{D}^{\\prime}}^{\\mathrm{\\uparrow\\uparrow}}}&{=\\mathrm{sup}\\left(\\underset{N\\to0}{\\sum}\\mathrm{rexp}\\right)^{-1}\\mathrm{sup}}\\\\ &{\\quad\\times\\mathrm{sup}\\left(\\underset{N\\to0}{\\sum}\\mathrm{rexp}\\right)^{-1}\\mathrm{sp}\\left(\\underset{N\\to0}{\\sum}\\mathrm{rexp}\\right)^{-1}\\mathrm{sup}}\\\\ &{\\quad=\\mathrm{sup}\\left(\\underset{N\\to0}{\\sum}\\mathrm{sf}_{N}\\mathrm{rexp}^{-1}\\mathrm{rexp}\\right)\\mathrm{sp}}\\\\ &{\\quad=\\mathrm{sup}\\left(\\underset{N\\to0}{\\sum}\\mathrm{sf}_{N}\\mathrm{rexp}^{-1}\\mathrm{rexp}\\right)\\mathrm{sp}\\left(\\underset{N\\to0}{\\sum}\\mathrm{rexp}^{-1}\\mathrm{rexp}\\right)^{-1}\\mathrm{sp}}\\\\ &{\\quad-\\frac{1}{N+N}\\mathrm{sp}^{-1}\\left(\\underset{N\\to0}{\\sum}\\mathrm{sgn}\\underset{N\\to0}{\\sum}\\mathrm{sf}_{N}\\mathrm{rexp}^{-1}\\mathrm{rexp}\\right)\\mathrm{s}+N_{1}\\sum_{i\\in\\partial\\Omega}\\mathrm{rexp}\\left(\\underset{N}{\\sum}\\mathrm{rexp}^{-1}\\right)^{-1}\\mathrm{sp}}\\\\ &{\\quad\\times\\frac{1}{N+N}\\mathrm{sp}^{\\prime}\\left(\\underset{N\\to0}{\\sum}\\mathrm{rexp}\\right)\\left(\\underset{N\\to0}{\\sum}\\mathrm{rexp}\\right)^{-1}\\mathrm{sp}}\\\\ &{\\quad\\times\\frac{2}{N+N}\\mathrm{sp}^{\\prime}\\left(\\underset{N\\to0}{\\sum}\\mathrm{rexp}\\right)\\left(\\underset{N\\to0}{\\sum}\\mathrm{rexp}^{-1}\\mathrm{rexp}\\right)^{-1}\\mathrm{sp}}\\\\ &{\\quad\\times\\frac{4}{N+N}\\mathrm{sp}^{\\prime}\\left(\\underset{N\\to0}{\\sum}\\mathrm{app}\\right)\\left(\\underset{N\\to0}{\\sum}\\mathrm{app}\\right)}\\\\ &{\\quad\\times\\frac{2}{N+N}\\mathrm{sp}^{\\prime}\\left(\\underset{N\\to0}{\\sum}\\mathrm{app}\\right)\\left(\\underset{N\\to0}{\\sum}\\mathrm{app}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The result follows by noting that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\|w\\|_{\\bar{A}(Z_{0}\\cup Z_{1},\\Gamma)^{-1}}^{2}=\\frac{1}{N_{0}+N_{1}}\\|w\\|_{A(\\xi,\\Gamma)}^{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Lemma J.7. Suppose that we have an estimate $\\widehat{\\Gamma}$ that is invertible, for any $x\\in\\mathbb{R}^{d}$ and covariance matrix $V$ ,wehave ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|x\\|_{(\\Gamma^{\\top}V\\Gamma)^{-1}}^{2}\\leq3\\|x\\|_{(\\widehat{\\Gamma}^{\\top}V\\widehat{\\Gamma})^{-1}}^{2}+2\\Big\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x\\Big\\|_{V^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof. Suppose we have an estimate $\\widehat{\\Gamma}$ . Then, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|x\\|_{(\\Gamma^{\\top}V\\Gamma)^{-1}}^{2}=\\|x\\|_{(\\widehat{\\Gamma}^{\\top}V\\widehat{\\Gamma})^{-1}}^{2}+\\|x\\|_{(\\Gamma^{\\top}V\\Gamma)^{-1}}^{2}-\\|x\\|_{(\\widehat{\\Gamma}^{\\top}V\\widehat{\\Gamma})^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Note that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\Gamma^{\\top}V\\Gamma)^{-1}-(\\widehat\\Gamma^{\\top}V\\widehat\\Gamma)^{-1}=\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}-\\widehat\\Gamma^{-1}V^{-1}\\widehat\\Gamma^{-\\top}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad=\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}-\\widehat\\Gamma^{-1}V^{-1}\\widehat\\Gamma^{-\\top}+\\Gamma^{-1}V^{-1}\\widehat\\Gamma^{-\\top}-\\Gamma^{-1}V^{-1}\\widehat\\Gamma^{-\\top}}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\Gamma^{-1}V^{-1}(\\Gamma^{-\\top}-\\widehat\\Gamma^{-\\top})+(\\Gamma^{-1}-\\widehat\\Gamma^{-1})V^{-1}\\widehat\\Gamma^{-\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Thus, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x\\|_{(\\Gamma^{\\top}V\\Gamma)^{-1}}^{2}=\\|x\\|_{(\\widehat{\\Gamma}^{\\top}V\\widehat{\\Gamma})^{-1}}^{2}+x^{\\top}\\Gamma^{-1}V^{-1}(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x+x^{\\top}(\\Gamma^{-1}-\\widehat{\\Gamma}^{-1})V^{-1}\\widehat{\\Gamma}^{-\\top}x}\\\\ &{\\qquad\\qquad\\leq\\|x\\|_{(\\widehat{\\Gamma}^{\\top}V\\widehat{\\Gamma})^{-1}}^{2}+\\|x\\|_{(\\Gamma^{\\top}V\\Gamma)^{-1}}\\left\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x\\right\\|_{V^{-1}}+\\left\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x\\right\\|_{V^{-1}}\\|x\\|_{(\\widehat{\\Gamma}^{\\top}}}\\\\ &{\\qquad\\leq\\|x\\|_{(\\widehat{\\Gamma}^{\\top}V\\widehat{\\Gamma})^{-1}}^{2}+\\frac{1}{2}\\|x\\|_{(\\Gamma^{\\top}V\\Gamma)^{-1}}^{2}+\\frac{1}{2}\\left\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x\\right\\|_{V^{-1}}^{2}+\\frac{1}{2}\\left\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x\\right\\|_{V^{-1}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This implies that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{x}\\right\\|_{(\\Gamma^{\\top}V\\Gamma)^{-1}}^{2}\\leq3\\|x\\|_{(\\widehat{\\Gamma}^{\\top}V\\widehat{\\Gamma})^{-1}}^{2}+2\\Big\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x\\Big\\|_{V^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Lemma J.8. Suppose $\\lambda_{f}=\\arg\\operatorname*{min}f(\\lambda)$ and $\\lambda_{g}=\\arg\\operatorname*{min}g(\\lambda)$ and $f(\\lambda)\\leq g(\\lambda)+h(\\lambda),$ then ", "page_idx": 49}, {"type": "equation", "text": "$$\nf(\\lambda_{f})\\leq g(\\lambda_{g})+h(\\lambda_{g}).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof. ", "page_idx": 49}, {"type": "equation", "text": "$$\nf(\\lambda_{f})\\leq\\operatorname*{min}_{\\lambda}\\bigl(g(\\lambda)+h(\\lambda)\\bigr)\\leq g(\\lambda_{g})+h(\\lambda_{g}).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Lemma J.9. Define $\\lambda_{z}^{*}$ and $\\tilde{\\lambda}_{z}$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\lambda_{z}^{*}:=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}}\\bigl\\|w-w^{\\prime}\\bigr\\|_{A(\\lambda_{z},\\Gamma)^{-1}}^{2},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\widetilde{\\lambda}_{z}:=\\arg\\operatorname*{min}_{\\lambda\\in\\Delta(\\mathcal{Z})}\\operatorname*{max}_{w,w^{\\prime}\\in\\mathcal{W}}\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{z},\\widehat{\\Gamma})^{-1}}^{2}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "as the optimal design regarding $\\Gamma$ and that regarding its estimate $\\widehat{\\Gamma}$ respectively. Then, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\|w-w^{\\prime}\\|_{A(\\tilde{\\lambda}_{z},\\tilde{\\Gamma})^{-1}}^{2}\\leq\\operatorname*{max}_{w,w^{\\prime}\\in W}3\\|w-w^{\\prime}\\|_{A(\\lambda_{z}^{*},\\Gamma)^{-1}}^{2}+\\operatorname*{max}_{w,w^{\\prime}\\in W}2\\Big\\|(\\Gamma^{-\\top}-\\widehat\\Gamma^{-\\top})(w-w^{\\prime})\\Big\\|_{(\\sum_{z}\\lambda_{z}^{*}z z)}^{2}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof. By Lemma J.7, for any $w,w^{\\prime}\\in\\mathcal{W}$ and $\\lambda_{z}\\in\\Delta z$ \uff0c ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{z},\\widehat{\\Gamma})^{-1}}^{2}\\leq3\\left\\|w-w^{\\prime}\\right\\|_{A(\\lambda_{z},\\Gamma)^{-1}}^{2}+2\\Big\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})\\big(w-w^{\\prime}\\big)\\Big\\|_{(\\sum_{z}\\lambda_{z}z z^{\\top})^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Thus ", "text_level": 1, "page_idx": 49}, {"type": "equation", "text": "$$\n\\underset{\\in\\mathcal{W}}{\\operatorname{xx}}\\|w-w^{\\prime}\\|_{A(\\lambda_{z},\\widehat{\\Gamma})^{-1}}^{2}\\leq\\underset{w,w^{\\prime}\\in\\mathcal{W}}{\\operatorname*{max}}3\\|w-w^{\\prime}\\|_{A(\\lambda_{z},\\Gamma)^{-1}}^{2}+\\underset{w,w^{\\prime}\\in\\mathcal{W}}{\\operatorname*{max}}2\\Big\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}_{k}^{-\\top})(w-w^{\\prime})\\Big\\|_{(\\sum_{z}\\lambda_{z})^{\\frac{3}{2}}}^{2}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "By Lemma J.8, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\|w-w^{\\prime}\\|_{A(\\tilde{\\lambda}_{z},\\tilde{\\Gamma})^{-1}}^{2}\\leq\\operatorname*{max}_{w,w^{\\prime}\\in W}3\\|w-w^{\\prime}\\|_{A(\\lambda_{z}^{*},\\Gamma)^{-1}}^{2}+\\operatorname*{max}_{w,w^{\\prime}\\in W}2\\Big\\|(\\Gamma^{-\\top}-\\widehat\\Gamma^{-\\top})(w-w^{\\prime})\\Big\\|_{(\\sum_{z}\\lambda_{z}^{*}z z)}^{2}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Lemma J.10. Suppose that we have $\\widehat{\\Gamma}$ that is an OLS estimate from an offine dataset $\\{Z_{T_{1}},X_{T_{1}}\\}$ collected non-adaptively through a fixed design $\\lambda$ and the efficient rounding procedure ROUND. Let $V=Z_{T_{1}}^{\\top}Z_{T_{1}}$ . Then, for any $x\\in\\mathbb{R}^{d}$ and $g\\geq1$ , we have, with probability $1-\\delta$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\left\\lVert(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x\\right\\rVert_{V^{-1}}^{2}\\leq\\frac1g\\|x\\|_{\\bar{A}(Z_{T_{1}},\\Gamma)^{-1}}^{2},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "when ", "page_idx": 50}, {"type": "equation", "text": "$$\nT_{1}\\geq\\frac{4g\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda,\\Gamma)\\big)}\\overline{{\\log}}(Z_{T_{1}},\\delta)\\vee2p,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $p$ is the cardinality of support of $\\lambda$ ", "page_idx": 50}, {"type": "text", "text": "Proof. We first show that ", "text_level": 1, "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Gamma^{-1}-\\frac{\\kappa}{\\gamma}\\Gamma_{g_{1}}^{-1}\\right\\|_{\\mathcal{F}_{n-1}}^{2}}\\\\ &{=\\left\\|\\Gamma^{-1}\\mathcal{S}^{\\prime}\\mathcal{Z}_{\\gamma_{1}}V^{-1}(\\Gamma+V^{-1}\\mathcal{Z}_{\\gamma_{1}}^{\\gamma}\\mathcal{S})^{-\\gamma}\\right\\|_{\\mathcal{F}_{n-1}}^{2}}\\\\ &{\\overset{\\mathrm{()}}{=}\\left\\|\\Gamma^{-1}\\Gamma^{-1}\\mathcal{S}^{\\prime}\\mathcal{Z}_{\\gamma_{2}}V^{-1}(\\Gamma+V^{-1}\\mathcal{Z}_{\\gamma_{1}}^{\\gamma}\\mathcal{S})^{-\\gamma}\\right\\|_{\\mathcal{F}_{n-1}}^{2}}\\\\ &{\\overset{\\mathrm{()}}{=}\\left\\|\\Gamma^{-1}\\Gamma^{-1}\\mathcal{S}^{\\prime}\\mathcal{Z}_{\\gamma_{1}}V^{-1}\\right\\|_{\\mathcal{F}_{n-1}}^{2}\\left\\|\\Gamma^{-1}(\\Gamma+V^{-1}\\mathcal{Z}_{\\gamma_{1}}^{\\gamma}\\mathcal{S})^{-\\gamma}\\right\\|_{\\mathcal{F}_{n-1}}^{2}}\\\\ &{\\leq\\left\\|V^{-1}\\Gamma^{-1}\\mathcal{S}^{\\prime}\\mathcal{Z}_{\\gamma_{2}}V^{-1}\\right\\|_{\\mathcal{F}_{n-1}}^{2}\\left\\|\\Gamma^{-1}(\\Gamma+V^{-1}\\mathcal{Z}_{\\gamma_{1}}^{\\gamma}\\mathcal{S})^{-\\gamma}\\right\\|_{\\mathcal{F}_{n-1}}^{2}}\\\\ &{\\leq\\left\\|V^{-1}\\Gamma^{-1}\\Gamma^{-1}\\mathbb{I}_{\\varphi_{1}}^{2}\\left\\|\\mathcal{S}^{\\prime}\\mathcal{Z}_{\\gamma_{1}}V^{-1}\\right\\|_{\\mathcal{F}_{n-1}}^{2}\\left\\|V^{-1}(\\Gamma+V^{-1}\\mathcal{Z}_{\\gamma_{1}}^{\\gamma}\\mathcal{S})^{-\\gamma}\\right\\|_{\\mathcal{F}_{n-1}}^{2}}\\\\ &{=\\left\\|\\Gamma^{-1}V^{-1}\\Gamma^{-1}\\right\\|_{\\varphi_{1}}\\left\\|\\Gamma^{-1/2}\\mathcal{Z}_{\\gamma_{1}}^{\\gamma}\\right\\|_{\\mathcal{F\n$$\uff09", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We can upper bund the the term $\\begin{array}{r}{\\left\\|V^{-1/2}\\Gamma^{-\\top}\\Big(V^{-1}Z_{T_{1}}^{\\top}S\\Big)^{\\top}\\Big(\\Gamma+V^{-1}Z_{T_{1}}^{\\top}S\\Big)^{-\\top}x\\right\\|_{2}^{2}=:\\mathfrak{V}\\,\\mathfrak{b}.}\\end{array}$ noticing that it appears in both of $a_{1},a_{2}$ above. Thus we have the inequality ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathfrak{V}\\le\\left\\|\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}\\right\\|_{\\mathrm{op}}\\left\\|V^{-1/2}Z_{T_{1}}^{\\top}S\\right\\|_{\\mathrm{op}}^{2}\\biggl(\\left\\|V^{-1/2}\\Gamma^{-\\top}x\\right\\|_{2}^{2}+\\mathfrak{V}\\biggr).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "By rearranging the terms, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\vartheta\\leq\\frac{1}{1-\\left\\|\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}\\right\\|_{\\mathrm{op}}\\left\\|V^{-1/2}Z_{T_{1}}^{\\top}S\\right\\|_{\\mathrm{op}}^{2}}\\left\\|\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}\\right\\|_{\\mathrm{op}}\\left\\|V^{-1/2}Z_{T_{1}}^{\\top}S\\right\\|_{\\mathrm{op}}^{2}\\left\\|V^{-1/2}\\Gamma^{-\\top}x\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "By Lemma G.4, with probability $1-\\delta$ , we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\left\\|V^{-1/2}Z_{T_{1}}^{\\top}S\\right\\|_{\\mathrm{op}}^{2}\\leq\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Thus, with probability $1-\\delta$ , we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathfrak{M}\\leq\\frac{1}{1-\\Vert\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}\\Vert_{\\mathrm{op}}\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta)}\\Bigl\\Vert\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}\\Bigl\\Vert_{\\mathrm{op}}\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta)\\Bigl\\Vert V^{-1/2}\\Gamma^{-\\top}x\\Bigr\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "To further upper bound $\\mathfrak{V}$ \uff0c we first find a sufficient condition on $T_{1}$ such that $\\begin{array}{r}{\\bigl\\|\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}\\bigr\\|_{\\mathrm{op}}^{\\top}\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta)\\leq\\frac{1}{2g},g\\geq1}\\end{array}$ By LemmaJ.14, when $T_{1}\\geq2p$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\left\\|\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}\\right\\|_{\\mathrm{op}}\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta)\\leq\\frac{2\\sigma_{\\eta}^{2}}{T_{1}\\sigma_{\\operatorname*{min}}\\left(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma\\right)}\\overline{{\\log}}(Z_{T_{1}},\\delta).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "To upper bound the right hand side by $\\textstyle{\\frac{1}{2g}}$ , we need ", "page_idx": 51}, {"type": "equation", "text": "$$\nT_{1}\\geq\\frac{4g\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\left(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma\\right)}\\overline{{\\log}}(Z_{T_{1}},\\delta).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "With this condition (58), we have with probability $1-\\delta$ \uff0c ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{A}\\le\\frac{1}{1-\\left\\|\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}\\right\\|_{\\mathrm{op}}\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta)}\\left\\|\\Gamma^{-1}V^{-1}\\Gamma^{-\\top}\\right\\|_{\\mathrm{op}}\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta)\\left\\|V^{-1/2}\\Gamma^{-\\top}x\\right\\|_{2}^{2}}\\\\ &{\\quad\\le\\frac{1}{1-\\frac{1}{2g}}\\displaystyle\\frac{1}{2g}\\left\\|V^{-1/2}\\Gamma^{-\\top}x\\right\\|_{2}^{2}}\\\\ &{\\quad\\le\\frac{1}{g}\\left\\|V^{-1/2}\\Gamma^{-\\top}x\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma J.11. Suppose that we have $\\widehat{\\Gamma}$ that is an OLS estimate from an offine dataset $\\{Z_{T_{1}},X_{T_{1}}\\}$ collected non-adaptively through a fixed design $\\lambda$ and the effcient roundingprocedureRouND.Let $\\dot{V}$ be any positive definite matrix. Then, for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we have, with probability $1-\\delta$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x\\right\\|_{\\dot{V}^{-1}}^{2}\\leq2\\Big\\|\\Gamma^{-1}\\dot{V}^{-1}\\Gamma^{-\\top}\\Big\\|_{\\mathrm{op}}\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta)\\|x\\|_{\\bar{A}(Z_{T_{1}},\\Gamma)^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "when ", "page_idx": 51}, {"type": "equation", "text": "$$\nT_{1}\\geq\\frac{4\\sigma_{\\eta}^{2}}{\\sigma_{\\operatorname*{min}}\\big(A(\\lambda,\\Gamma)\\big)}\\overline{{\\log}}(Z_{T_{1}},\\delta)\\vee2p,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where p is the cardinality of support of $\\lambda$ ", "page_idx": 51}, {"type": "text", "text": "Proof. ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Gamma^{-1}-\\frac{\\kappa}{\\gamma}\\Gamma_{g_{1}}^{1}\\right\\|_{\\L{p}^{*}}^{2}}\\\\ &{=\\left\\|\\Gamma^{-1}\\delta^{*}\\Gamma^{-1}(\\Gamma+V^{-1}Z_{T_{1}}^{*})^{-\\gamma}Z_{T_{1}}^{1}\\right\\|_{\\L{p}^{*}-1}^{2}}\\\\ &{\\frac{2}{3}\\left\\|\\tilde{\\Gamma}^{-1}\\Gamma^{-1}C^{3}Z_{T_{1}}\\Gamma^{-1}(\\Gamma+V^{-1}Z_{T_{1}}^{*})^{-\\gamma}Z_{T_{1}}^{2}\\right\\|_{\\L{p}^{*}-1}^{2}}\\\\ &{\\leq\\left\\|\\tilde{\\Gamma}^{-1}\\Gamma^{-1}C^{3}Z_{T_{2}}V^{-1}\\right\\|_{\\L{p}^{*}-1}^{2}\\left\\|\\frac{\\mathrm{d}}{\\mathrm{d}\\Gamma}\\Gamma^{-1}(\\Gamma+V^{-1}Z_{T_{1}}^{*})^{-\\gamma}Z_{T_{1}}^{2}\\right\\|_{\\L{p}^{2}}^{2}}\\\\ &{\\leq\\left\\|\\tilde{\\Gamma}^{-1}\\Gamma^{-1}C^{3}\\tau^{2}Z_{T_{1}}V^{-1}\\right\\|_{\\L{p}^{2}}^{2}\\left\\|\\Gamma^{-1}(\\Gamma+V^{-1}Z_{T_{1}}^{*})^{-\\gamma}Z_{T_{1}}^{2}\\right\\|_{\\L{p}^{2}}^{2}}\\\\ &{\\leq\\left\\|\\tilde{\\Gamma}^{-1}\\Gamma^{-1}\\right\\|_{\\L{p}^{2}}^{2}\\left\\|\\mathcal{S}^{2}\\tau_{T_{1}}V^{-1}\\right\\|_{\\L{p}^{2}}^{2}\\left\\|\\Gamma^{-1}(\\Gamma+V^{-1}Z_{T_{1}}^{*})^{-\\gamma}Z_{T_{1}}^{2}\\right\\|_{\\L{p}^{2}}^{2}}\\\\ &{=\\left\\|\\Gamma^{-1}\\tilde{\\Gamma}^{-1}\\Gamma^{-1}\\right\\|_{\\L{p}^{2}}\\left\\|\\Gamma^{-1/2}\\overline{{\\Gamma}}_{\\mathfrak{p}}^{2}\\right\\|_{\\L{p}^{2}}^{2}\\left\\\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Given the condition (58) holds, we have with probability $1-\\delta$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|(\\Gamma^{-\\top}-\\widehat{\\Gamma}^{-\\top})x\\right\\|_{\\dot{V}^{-1}}^{2}}\\\\ &{\\stackrel{a_{1}}{\\leq}\\left\\|\\Gamma^{-1}\\dot{V}^{-1}\\Gamma^{-\\top}\\right\\|_{\\mathrm{op}}\\left\\|V^{-1/2}Z_{T_{1}}^{\\top}S\\right\\|_{\\mathrm{op}}^{2}\\left(\\left\\|V^{-1/2}\\Gamma^{-\\top}x\\right\\|_{2}^{2}+\\left\\|V^{-1/2}\\Gamma^{-\\top}x\\right\\|_{2}^{2}\\right)}\\\\ &{=2\\left\\|\\Gamma^{-1}\\dot{V}^{-1}\\Gamma^{-\\top}\\right\\|_{\\mathrm{op}}\\left\\|V^{-1/2}Z_{T_{1}}^{\\top}S\\right\\|_{\\mathrm{op}}^{2}\\left\\|V^{-1/2}\\Gamma^{-\\top}x\\right\\|_{2}^{2}}\\\\ &{\\leq2\\left\\|\\Gamma^{-1}\\dot{V}^{-1}\\Gamma^{-\\top}\\right\\|_{\\mathrm{op}}\\sigma_{\\eta}^{2}\\overline{{\\log}}(Z_{T_{1}},\\delta)\\left\\|x\\right\\|_{(\\Gamma^{\\top}V\\Gamma)^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\left(a_{1}\\right)$ is due to (59) and setting $g=1$ ", "page_idx": 52}, {"type": "text", "text": "Lemma J.12. For a least square estimate $\\widehat{\\Gamma}$ that is estimated through a design matrix $Z$ and $\\widehat{\\Gamma}$ is invertible, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}=-\\Big(\\Gamma+V^{-1}Z^{\\top}S\\Big)^{-1}V^{-1}Z^{\\top}S\\Gamma^{-1}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof. Since $\\widehat{\\Gamma}$ is a least square estimator, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\widehat{\\Gamma}=\\Gamma+V^{-1}Z^{\\top}S.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "By Lemma J.13, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\Gamma}^{-1}-\\Gamma^{-1}=\\left(\\Gamma+V^{-1}Z^{\\top}S\\right)^{-1}-\\Gamma^{-1}}\\\\ &{\\qquad\\qquad\\quad=-\\left(\\Gamma+V^{-1}Z^{\\top}S\\right)^{-1}\\!V^{-1}Z^{\\top}S\\Gamma^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma J.13. For two invertible matrixes $A,B\\in\\mathbb{R}^{d\\times d}$ we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\left(A+B\\right)^{-1}=A^{-1}-\\left(A+B\\right)^{-1}B A^{-1}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof. We have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(A+B\\right)^{-1}=A^{-1}+\\left(A+B\\right)^{-1}-A^{-1}}\\\\ &{\\qquad\\qquad=A^{-1}+\\Bigl(\\left(A+B\\right)^{-1}A-I\\Bigr)A^{-1}}\\\\ &{\\qquad\\qquad=A^{-1}+\\left(A+B\\right)^{-1}\\!\\left(A-\\left(A+B\\right)\\right)A^{-1}}\\\\ &{\\qquad=A^{-1}-\\left(A+B\\right)^{-1}\\!B A^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma J.14. Suppose that we have a design matrix $Z_{T}$ that is sampled from a distribution $\\lambda\\in\\Delta_{\\mathcal{Z}}$ \uff0c with the efficient rounding procedure ROUND. Let $p$ represent the cardinality of support of $\\lambda$ We have, $i f T\\geq2p$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\Gamma^{\\top}Z_{T}^{\\top}Z_{T}\\Gamma\\right)^{-1}\\right\\|_{\\mathrm{op}}\\leq\\frac{2}{T\\sigma_{\\operatorname*{min}}\\left(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma\\right)}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\sigma_{\\mathrm{min}}(\\cdot)$ is the smallest singular value of a matrix. ", "page_idx": 52}, {"type": "text", "text": "Proof. Suppose that each arm $z\\ \\in\\ {\\mathcal{Z}}$ is sampled $t_{z}$ times, the empirical distribution of $Z_{T}$ is $\\xi:=\\left(\\frac{t_{z}}{T}\\right)_{z\\in\\mathcal{Z}}^{\\star}$ . Thus, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|\\left(\\Gamma^{\\top}Z_{T}^{\\top}Z_{T}\\Gamma\\right)^{-1}\\right\\|_{\\mathrm{op}}=\\frac{1}{\\sigma_{\\operatorname*{min}}\\left(\\Gamma^{\\top}Z_{T}^{\\top}Z_{T}\\Gamma\\right)}\\ }\\\\ {\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\frac{1}{\\sigma_{\\operatorname*{min}}\\left(T\\sum_{z\\in\\mathcal{Z}}\\xi_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma\\right)}}\\\\ {\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\frac{1}{T\\sigma_{\\operatorname*{min}}\\left(\\sum_{z\\in\\mathcal{Z}}\\xi_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "By Fiez et al. [15, Proposition 2], we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\sum_{z\\in\\mathcal{Z}}\\xi_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma\\succeq\\alpha\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma,\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\alpha}&{{}\\in}&{[\\frac{T}{T+2p},1]}\\end{array}$ when $T~~\\geq~~2p$ .Given the fact that both of $\\textstyle\\sum_{z\\in{\\mathcal{Z}}}\\xi_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma$ and $\\begin{array}{r}{\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma}\\end{array}$ are positive definite, we have $\\begin{array}{r l}{\\sigma_{\\operatorname*{min}}\\left(\\sum_{z\\in{\\mathcal{Z}}}\\xi_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma\\right)}&{{}\\geq}\\end{array}$ $\\begin{array}{r}{\\alpha\\sigma_{\\mathrm{min}}\\left(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma\\right)}\\end{array}$ . Thus, we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\Gamma^{\\top}Z_{T}^{\\top}Z_{T}\\Gamma\\right)^{-1}\\right\\|_{\\mathrm{op}}\\leq\\frac{1}{\\alpha T\\sigma_{\\operatorname*{min}}\\left(\\sum_{z\\in\\mathcal{Z}}\\lambda_{z}\\Gamma^{\\top}z z^{\\top}\\Gamma\\right)}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "When $T\\geq2p$ , we have $\\alpha\\geq1/2$ , which implies the result. ", "page_idx": 53}, {"type": "text", "text": "K Estimating $\\lambda_{\\operatorname*{min}}(\\Gamma)$ ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In this section, we introduce a simple adaptive procedure that finds a high probability lower bound on $\\gamma_{\\mathrm{min}}^{\\ast}:=\\lambda_{\\mathrm{min}}(\\Gamma)$ that is sufficiently accurate (i.e., within a constant factor of $\\gamma_{\\mathrm{min}}^{\\ast}$ ).For simplicity, weassume $\\|z_{t}\\|\\leq1,\\forall t$ in this section. ", "page_idx": 53}, {"type": "text", "text": "Our algorithm leverages confidence bounds to adaptively determine how many samples we like to take. Let $\\hat{\\Gamma}_{t}:=(Z^{\\top}\\bar{Z})^{-1}Z^{\\top}X$ be the least square estimate of $\\Gamma$ after sampling $t$ times to obtain $\\{(z_{s},x_{s})\\}_{s=1}^{t}$ where $Z\\in\\mathbb{R}^{t\\times d}$ and $X\\in\\mathbb{R}^{t\\times d}$ are the design matrices. Let $\\begin{array}{r}{V_{t}:=\\sum_{s=1}^{t}z_{s}z_{s}^{\\top}}\\end{array}$ T.We define the lower and upper confidence bound for $\\gamma_{\\mathrm{min}}^{\\ast}$ as follows: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbf{LCB}(t):=\\lambda_{\\operatorname*{min}}(\\hat{\\Gamma}_{t})-\\sqrt{\\frac{\\psi_{t}}{t}},\\quad\\mathbf{UCB}(t):=\\lambda_{\\operatorname*{min}}(\\hat{\\Gamma}_{t})+\\sqrt{\\frac{\\psi_{t}}{t}}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\psi_{t}=\\sigma_{\\mathrm{min}}^{-1}\\left({\\frac{1}{t}}V_{t}\\right)\\cdot\\left(8d\\ln\\left(1+{\\frac{2t}{d(2\\wedge\\sigma_{\\mathrm{min}}(V_{t}))}}\\right)+16\\ln\\left({\\frac{2\\cdot6^{d}}{\\delta}}\\cdot\\log_{2}^{2}\\left({\\frac{4}{2\\wedge\\sigma_{\\mathrm{min}}(V_{t})}}\\right)\\right)\\right)\\ .\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "and $\\mathrm{LCB}(0):=-\\infty$ and $\\operatorname{UCB}(0):=\\infty$ ", "page_idx": 53}, {"type": "text", "text": "The following lemma shows that $\\mathbf{LCB}(t)$ aind $\\mathbf{UCB}(t)$ form a valid anytime confidence bound for $\\gamma_{\\mathrm{min}}^{\\ast}$ ", "page_idx": 53}, {"type": "text", "text": "Lemma K.1. (Correctness of the confidence bounds) ", "page_idx": 53}, {"type": "equation", "text": "$$\n1-\\delta\\leq\\mathbb{P}(\\forall t\\geq1,\\mathrm{LCB}(t)\\leq\\gamma_{\\operatorname*{min}}^{*}\\leq\\mathrm{UCB}(t))\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Equiped with the confidene bounds, we ae now ready to describe or algorithm for larming $\\gamma_{\\mathrm{min}}^{\\ast}$ (see Algorithm 10). Since the tightness of the confidence bounds depends on the smallest eigenvalue of $V_{t}$ , it is natural to use the $\\boldsymbol{\\mathrm E}$ -optimal design as defined in Section 3.2. Recall that the solution of the E-optimal design is $\\lambda_{E}^{*}$ and $\\kappa_{0}$ is the smallest singular value achieved by $\\lambda_{E}^{*}$ . We take in a rounding procedure for the E-optimal design $\\mathrm{ROUND}_{E}(\\lambda,t)$ that takes in $t$ samples and design $\\lambda$ and outputs integer sample count assignments $\\{N_{z}\\}_{z\\in\\mathcal{Z}}$ so that if we sample according to these counts then we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}^{-1}(\\frac{1}{t}V_{t})\\leq(1+\\omega)\\kappa_{0}^{-1}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "After determining the base sample counts $\\{m_{z}\\}_{z\\in\\mathcal{Z}}$ by $\\mathrm{ROUND}(\\lambda_{E}^{*},\\lceil r(\\omega)\\rceil,\\omega)$ , we start doubling the sample size until we satisfy the condition $\\operatorname{LCB}(t)>{\\textstyle{\\frac{1}{2}}}\\operatorname{UCB}(t)$ . Note that the sampling scheme in the while loop is designed such that the total number of samples collected up to (and including) $j$ -th iteration is $2^{j-1}\\lceil r(\\omega)\\rceil$ . Once the loop stops, we return $\\mathbf{LCB}(t)$ as the claimed lower approximation of the min' ", "page_idx": 53}, {"type": "text", "text": "Let $N_{w}$ be the total number of samples we used in Algorithm 10. Then, the next theorem shows that the estimate returned by our algorithm is both a valid lower bound to $\\gamma_{\\mathrm{min}}^{\\ast}$ and sufficiently accurate. ", "page_idx": 53}, {"type": "text", "text": "Algorithm 10 Learning $\\lambda_{\\mathrm{min}}(\\Gamma)$ ", "page_idx": 54}, {"type": "text", "text": "Input: Arm set $\\mathcal{Z}$ , rounding procedure ${\\mathrm{ROUND}}_{E}$ for the E-optimal design, rounding accuracy $\\omega$ Initialize: $j=1$ $t=0$   \nCompute the E-optimal design $\\lambda_{E}^{*}$ for $\\mathcal{Z}$   \nCompute $\\{m_{z}\\}_{z\\in\\mathcal{Z}}$ by $\\mathrm{ROUND}_{E}^{-}(\\lambda_{E}^{*},\\lceil r(\\omega)\\rceil)$ \uff1a   \nwhile $\\begin{array}{r}{\\mathrm{LCB}(t)\\leq\\frac{1}{2}\\mathrm{UCB}(t)}\\end{array}$ do   \n$t\\gets2^{j-1}\\lceil r(\\omega)\\rceil$   \nFor each arm $z\\in{\\mathcal{Z}}$ , sample $2^{j-1}m_{z}-\\mathbb{1}\\left\\{j\\neq1\\right\\}2^{j-2}m_{z}$ times.   \nEstimate $\\hat{\\Gamma}_{t}$ using all samples collected so far (total $t$ data points)   \n$j\\leftarrow j+1$   \nend while   \nOutput: $\\mathbf{LCB}(t)$ ", "page_idx": 54}, {"type": "text", "text": "Theorem K.2. (Correctness of Algorithm $I O$ )The total number of samples denoted by $N_{w}$ usedin Algorithm 10 satisfies that, with probability at least $1-\\delta$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{\\gamma_{\\operatorname*{min}}^{*}}{2}<\\mathrm{LCB}(N_{w})\\leq\\gamma_{\\operatorname*{min}}^{*}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "We next analyze the sample complexity of the algorithm, which essentially shows the scaling of $\\frac{1}{(\\gamma_{\\operatorname*{min}}^{*})^{2}\\kappa_{0}}$ even if the algorithm does not ned knowledge of $\\gamma_{\\mathrm{min}}^{\\ast}$ ", "page_idx": 54}, {"type": "text", "text": "Theorem K.3. (Sample complexity of Algorithm $I O$ )Then,with $\\omega=1$ we have, with probability at least $1-\\delta$ ", "page_idx": 54}, {"type": "equation", "text": "$$\nN_{w}=O\\left(r(1)+(\\gamma_{\\mathrm{min}}^{*})^{-2}\\kappa_{0}^{-1}.\\left(d\\mathrm{{polylog}}((\\gamma_{\\mathrm{min}}^{*})^{-2},\\kappa_{0}^{-1},d)+\\ln\\bigl(2/\\delta\\bigr)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "We remark that Allen-Zhu et al. [2] provides a rounding procedure with $r(\\varepsilon)=O(d/\\varepsilon^{2})$ ", "page_idx": 54}, {"type": "text", "text": "Proof of Lemma K.1. Note that $\\hat{\\Gamma}=\\Gamma+V_{t}^{-1}Z^{\\top}S$ where $Z,S\\in\\mathbb{R}^{t\\times d}$ is the design matrices with $s$ -th row being $z_{s}^{\\top}$ and $\\eta_{s}^{\\top}$ respectively. Using Lemma G.4, we have, with probability at least $1-\\delta$ \uff0c ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t\\geq1,\\left\\Vert\\hat{\\mathbf{Y}}_{t}-\\mathbf{I}\\right\\Vert_{\\infty}^{2}=\\left\\Vert V_{t}^{-1}Z^{\\top}S\\right\\Vert_{\\infty}^{2}}\\\\ &{\\qquad\\qquad\\leq\\left\\Vert V_{t}^{-1/2}\\right\\Vert_{\\infty}^{2}\\left\\Vert V_{t}^{-1/2}Z^{\\top}S\\right\\Vert_{\\infty}^{2}}\\\\ &{\\qquad\\qquad=\\frac{1}{t o\\mathrm{min}\\frac{\\left\\Vert\\hat{\\mathbf{V}}_{t}\\right\\Vert_{\\infty}}{\\left\\Vert\\hat{\\mathbf{V}}_{t}\\right\\Vert_{\\infty}}}\\left\\Vert V_{t}^{-1/2}Z^{\\top}S\\right\\Vert_{\\infty}^{2}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{t o\\mathrm{min}\\left(\\frac{1}{t}V_{t}\\right)}\\cdot\\left(8d\\ln\\left(1+\\frac{2t}{d\\left(2\\wedge\\sigma_{\\operatorname*{min}}\\left(V_{t}\\right)\\right)}\\right)+16\\ln\\left(\\frac{2\\cdot6^{d}}{\\delta}\\!\\!\\cdot\\!\\log_{2}^{2}\\left(\\frac{4}{2\\wedge\\sigma_{\\operatorname*{min}}\\left(\\log_{2}\\left(\\frac{4}{\\delta}\\right)\\right)}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(\\mathrm{Lemma~G.})}\\\\ &{\\qquad=\\frac{1}{t}\\psi_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "The well-known Weyl's theorem implies that $\\mathrm{max}_{k}\\left|\\lambda_{k}(\\hat{\\Gamma}_{t})-\\lambda_{k}(\\Gamma)\\right|\\leq\\left\\|\\hat{\\Gamma}_{t}-\\Gamma\\right\\|_{\\mathrm{op}}$ where $\\lambda_{k}(A)$ is the $k$ -th largest singular value of the matrix $A$ . Choosing $k=d$ and combining it with the display above conclude the proof. \u53e3 ", "page_idx": 54}, {"type": "text", "text": "Proof of Theorem K.2. We assume $\\forall t\\geq1,\\mathrm{LCB}(t)\\leq\\gamma_{\\operatorname*{min}}^{*}\\leq\\mathrm{UCB}(t)$ , which happens with probability at least 1 - 8. Then,itis trivial to se that LCB(Nu) min ", "page_idx": 54}, {"type": "text", "text": "For the other inequality, we use the fact that the stopping condition was satisfied with $N_{w}$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\operatorname{LCB}(N_{w})>{\\frac{1}{2}}\\mathrm{UCB}(N_{w})\\geq{\\frac{\\gamma_{\\operatorname*{min}}^{*}}{2}}~.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 54}, {"type": "text", "text": "Proofof TheoremK.3.We assume $\\forall t\\geq1,\\mathrm{LCB}(t)\\leq\\lambda_{\\operatorname*{min}}(\\Gamma)\\leq\\mathrm{UCB}(t)$ ,which happens with probability at least $1-\\delta$ . In this proof, we let $\\hat{\\gamma}_{\\mathrm{min}}:=\\lambda_{\\mathrm{min}}(\\hat{\\Gamma}_{N_{w}})$ and $\\gamma_{\\mathrm{min}}^{\\ast}:=\\lambda_{\\mathrm{min}}(\\Gamma)$ for brevity. If $N_{w}=\\lceil r(1)\\rceil$ , then there is nothing to prove. Otherwise, the loop in the algorithm was iterated more than once. Then, since the stopping condition was satisfied with $N_{w}$ , we have that in the previous iteration where $t=N_{w}/2$ the stopping condition was not satisfied. Thus, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2\\mathrm{LCB}(N_{w}/2)\\leq\\mathrm{UCB}(N_{w}/2)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Using the following two inequalities: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2{\\bf L C B}(N_{w}/2)\\ge2\\left(\\hat{\\gamma}_{\\mathrm{min}}-\\sqrt{\\frac{\\psi_{N_{w}/2}}{N_{w}/2}}\\right)\\ge2\\left(\\gamma_{\\mathrm{min}}^{*}-2\\sqrt{\\frac{\\psi_{N_{w}/2}}{N_{w}/2}}\\right)}\\\\ &{\\quad{\\bf U C B}(N_{w}/2)\\le\\hat{\\gamma}_{\\mathrm{min}}+\\sqrt{\\frac{\\psi_{N_{w}/2}}{N_{w}/2}}\\le\\gamma_{\\mathrm{min}}^{*}+2\\sqrt{\\frac{\\psi_{N_{w}/2}}{N_{w}/2}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\gamma_{\\mathrm{min}}^{*}\\le6\\sqrt{\\frac{\\psi_{N_{w}/2}}{N_{w}/2}}\\implies N_{w}\\le\\frac{72}{(\\gamma_{\\mathrm{min}}^{*})^{2}}\\psi_{N_{w}/2}\\;.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "On the other hand, with the rounding procedure, we have ", "page_idx": 55}, {"type": "equation", "text": "$$\nr_{\\mathrm{min}}^{-1}\\left(\\sum_{t=1}^{N}z_{t}z_{t}^{\\top}\\right)=\\frac{1}{N}\\sigma_{\\mathrm{min}}^{-1}\\left(\\frac{1}{N}\\sum_{t=1}^{N}z_{t}z_{t}^{\\top}\\right)=\\frac{1}{N}\\sigma_{\\mathrm{max}}\\left(\\left(\\frac{1}{N}\\sum_{t=1}^{N}z_{t}z_{t}^{\\top}\\right)^{-1}\\right)\\leq\\frac{1}{N}(1+\\omega)\\kappa_{0}^{-1}=\\frac{2}{N}\\sigma_{\\mathrm{max}}^{-1}\\left(\\frac{1}{N}\\sum_{t=1}^{N}z_{t}z_{t}^{\\top}\\right),\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "since $\\omega=1$ . Using this and the fact that $N_{w}\\geq d$ , it is easy to see that there exists an absolute constant $c_{1}$ such that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\psi_{N_{w}/2}\\leq c_{1}\\kappa_{0}^{-1}\\left(d\\ln\\left(1+N_{w}+\\kappa_{0}^{-1}\\right)+\\ln\\left(\\frac{2}{\\delta}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Then, there exists an absolute constant $c_{2}$ such that ", "page_idx": 55}, {"type": "equation", "text": "$$\nN_{w}\\leq\\left(\\gamma_{\\operatorname*{min}}^{*}\\right)^{-2}\\cdot c_{2}\\kappa_{0}^{-1}\\left(d\\ln\\left(1+N_{w}+\\kappa_{0}^{-1}\\right)+\\ln\\left(\\frac{2}{\\delta}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Wehave $N_{w}$ on both sides. We invoke Lemma K.5 with $r=1+N_{w}$ to obtain ", "page_idx": 55}, {"type": "equation", "text": "$$\nN_{w}\\leq1+2c_{2}(\\gamma_{\\mathrm{min}}^{*})^{-2}\\kappa_{0}^{-1}\\left(d\\ln\\Bigl(1+2\\kappa_{0}^{-1}(1+c_{2}d(\\gamma_{\\mathrm{min}}^{*})^{-2})\\Bigr)+\\ln\\left(\\frac2\\delta\\right)\\right)\\ .\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Lemma K.4. Let $A,\\Gamma\\in\\mathbb{R}^{d\\times d}$ where $A$ is symmetric positive semi-definite. Then, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}\\left(\\Gamma^{\\top}A\\Gamma\\right)\\geq\\sigma_{\\mathrm{min}}(\\Gamma)^{2}\\sigma_{\\mathrm{min}}(A)\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Proof. ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\sigma_{\\operatorname*{min}}(\\Gamma^{\\top}A\\Gamma))^{-1}=\\left\\lVert\\Gamma^{-1}A^{-1}\\Gamma^{-T}\\right\\rVert_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\leq\\left\\lVert\\Gamma^{-1}\\right\\rVert_{\\mathrm{op}}\\left\\lVert A^{-1}\\right\\rVert_{\\mathrm{op}}\\left\\lVert\\Gamma^{-T}\\right\\rVert_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad=\\sigma_{\\operatorname*{min}}(\\Gamma)^{-2}\\sigma_{\\operatorname*{min}}(A)^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "(submultiplicity of the operator norm) ", "page_idx": 55}, {"type": "text", "text": "Taking the inverse on both sides concludes the proof. ", "page_idx": 55}, {"type": "text", "text": "K.1 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Define $\\begin{array}{r}{\\Lambda=\\sum_{i=1}^{d}\\lambda_{i}e_{i}e_{i}^{\\top}}\\end{array}$ , i.e. the diagonal matrix with $\\lambda$ on the diagonal. Note that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\lambda}\\operatorname*{max}_{i,j}(e_{i}^{\\top}-e_{j})^{\\top}(\\Gamma\\Lambda\\Gamma^{\\top})^{-1}(e_{i}-e_{j})=\\operatorname*{min}_{\\lambda}\\operatorname*{max}_{i,j}\\sum_{k=1}^{d}\\frac{(\\Gamma_{k,i}^{-1}-\\Gamma_{k,j}^{-1})^{2}}{\\lambda_{k}}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "So for an upper bound, we consider $\\lambda=1/d1$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\lambda}\\operatorname*{max}_{i,j}(e_{i}^{\\top}-e_{j})^{\\top}(\\Gamma\\Lambda\\Gamma^{\\top})^{-1}(e_{i}-e_{j})\\leq d\\operatorname*{max}_{i,j}\\|\\Gamma^{-1}(e_{i}-e_{j})\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The result about $\\rho^{*}$ follows immediately. ", "page_idx": 56}, {"type": "text", "text": "When $\\Gamma=(1-\\varepsilon)/d11^{\\top}+\\varepsilon I$ , a computation using Sherman-Morrison shows that $\\Gamma^{-1}=1/\\varepsilon[I-$ $(1-\\varepsilon)/d11^{\\top}]$ . Thus ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\lambda}{\\operatorname*{min}}\\,\\underset{i,j}{\\operatorname*{max}}(e_{i}^{\\top}-e_{j})^{\\top}(\\Gamma\\Lambda\\Gamma^{\\top})^{-1}(e_{i}-e_{j})=\\varepsilon^{-2}\\underset{\\lambda}{\\operatorname*{min}}\\,\\underset{i,j}{\\operatorname*{max}}(e_{i}-e_{j})^{\\top}\\Lambda^{-1}(e_{i}-e_{j})}\\\\ {=\\varepsilon^{-2}\\underset{\\lambda}{\\operatorname*{min}}\\,\\underset{i,j}{\\operatorname*{max}}\\,e_{i}\\top\\Lambda^{-1}e_{i}+e_{j}\\Lambda^{-1}e_{j}}\\\\ {=\\varepsilon^{-2}\\underset{\\lambda}{\\operatorname*{min}}\\,\\underset{i,j}{\\operatorname*{max}}\\,e_{i}\\top\\Lambda^{-1}e_{i}+e_{j}\\Lambda^{-1}e_{j}}\\\\ {\\geq\\varepsilon^{-2}\\underset{\\lambda}{\\operatorname*{min}}\\,\\underset{i}{\\operatorname*{max}}\\,e_{i}\\top\\Lambda^{-1}e_{i}}&{=\\varepsilon^{-2}d}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the last line follows from the Kiefer-Wolfowitz Theorem [26]. ", "page_idx": 56}, {"type": "text", "text": "K.2  lemma for solving $\\mathbf{X}$ less than $\\mathbf{ln}(\\mathbf{x})$ ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Lemma K.5. Let $a,b,c,d>0$ Then, for every $r$ ", "page_idx": 56}, {"type": "equation", "text": "$$\nr\\leq a+b\\ln(c+d r)\\implies r\\leq2a+2b\\ln(1+2c+2b d)\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof. If $d r\\leq c$ then ", "page_idx": 56}, {"type": "equation", "text": "$$\nr\\le a+b\\ln(2c)\\le a+b\\ln(1+2c)\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "If $d r>c$ , then, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r\\leq a+b\\ln(2d r)}\\\\ &{\\quad=a+b\\ln\\biggl(2d\\frac{r}{2b}\\cdot2b\\biggr)}\\\\ &{\\quad\\leq a+b\\left(\\frac{r}{2b}-1+\\ln(4b d)\\right)}\\\\ &{\\implies r\\leq2a+2b\\ln\\left(\\frac{4}{e}b d\\right)}\\\\ &{\\quad\\leq2a+2b\\ln(2b d)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "equation", "text": "$$\n(\\ln(x)\\leq x-1)\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Either case, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\nr\\leq2a+2b\\ln\\bigl((1+2c)\\vee2b d\\bigr)\\leq2a+2b\\ln(1+2c+2b d)\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Lemma K.6. Let $\\alpha,\\beta>0$ Then, for any $r$ \uff0c ", "page_idx": 56}, {"type": "equation", "text": "$$\nr\\leq\\alpha\\ln(1+r)+\\beta\\implies r\\leq2\\alpha\\ln(e+\\alpha)+2\\beta\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof. ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r\\leq\\alpha\\ln\\Biggl(r(\\frac{1}{r}+1)\\Biggr)+\\beta}\\\\ &{\\quad\\leq\\alpha\\ln\\Biggl(\\frac{r}{2\\alpha}\\cdot2\\alpha(\\frac{1}{r}+1)\\Biggr)+\\beta}\\\\ &{\\quad\\leq\\alpha\\left(\\frac{r}{2\\alpha}-1+\\ln\\Biggl(2\\alpha(\\frac{1}{r}+1)\\Biggr)\\right)+\\beta}\\\\ &{\\implies r\\leq2\\alpha\\ln\\Biggl(\\frac{2}{e}\\alpha(\\frac{1}{r}+1)\\Biggr)+\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "equation", "text": "$$\n(\\forall x,\\ln(x)\\leq x-1)\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "If $r\\leq2\\alpha$ , then there is nothing to prove. If $r>2\\alpha$ then $r\\le2\\alpha\\ln(1+\\alpha)+2\\beta$ . Either case, we have $r\\le2\\alpha\\ln(e+\\alpha)+2\\beta$ \u53e3 ", "page_idx": 57}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: Abstract and introduction summarize our main theorems. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 58}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: For example, we explain our limitations on lower bound. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 58}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The assumptions are given is the beginning of the paper and referred later when necessary. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 59}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: Details on the experiment setup are given. It is mainly a theoretical paper. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 59}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: It is mainly a theoretical paper. We attach the code used for the simulation. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 60}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: It is mainly a theoretical paper with some simulations in appendix. No deep learning or other complicated training involved. But we attach our code for reference. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 60}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Justification: Error bar is reported, e.g., Fig 2b. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 60}, {"type": "text", "text": "", "page_idx": 61}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 61}, {"type": "text", "text": "Answer: [No] ", "page_idx": 61}, {"type": "text", "text": "Justification: It is mainly a theoretical paper. Some simulations are given in the appendix.   \nNo need for heavy computation resource. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 61}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper conforms all the code of Ethics. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 61}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: we discuss it in the appendix. It is mainly a theoretical and algorithmic paper, without any sensitive data, very unlikely to have negative impacts. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 61}, {"type": "text", "text": "", "page_idx": 62}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: the paper poses no such risks, no such data or model involved. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 62}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: We do not use any existing assets. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 62}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 63}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: We do not produce any new assets. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 63}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: We do not use anything like crowdsourcing. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 63}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: We do not use anything like crowdsourcing. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 63}]