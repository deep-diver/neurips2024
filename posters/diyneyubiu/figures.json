[{"figure_path": "diYnEYUbIU/figures/figures_1_1.jpg", "caption": "Figure 1: Panoramic segmentation problem can be re-formulated to the estimation of over-sampled segments: floor, ceiling and under-sampled segments: chair, table, bookcase, window, etc.", "description": "This figure illustrates the proposed method's approach to panoramic semantic segmentation.  The input is a panoramic image. The problem is broken down into two sub-problems: estimating over-sampled segments (which are large, easily identifiable features such as floor and ceiling) and estimating under-sampled segments (smaller, more complex features such as chairs, tables, etc.).  The final segmentation combines the results of these two sub-problems to provide a complete semantic segmentation of the entire scene.  This approach allows the model to handle the challenges of size imbalance and distortion common in panoramic images more effectively.", "section": "1 Introduction"}, {"figure_path": "diYnEYUbIU/figures/figures_3_1.jpg", "caption": "Figure 2: The proposed framework consists of three main modules: an encoder for extracting image features, a branch that estimates over-sampled segments alongside dense depth estimation, and a hybrid decoder for estimating under-sampled segments before a merging process to obtain the final segmentation result.", "description": "This figure illustrates the overall architecture of the proposed method for indoor panoramic semantic segmentation. It consists of three main parts: an encoder that processes the input image and generates feature maps, a branch focusing on over-sampled segments (like floors and ceilings) which also estimates depth, and a hybrid decoder that combines high-resolution features with a transformer-based context module to estimate the under-sampled segments. The process concludes by merging the segment estimations to create the final segmentation map.", "section": "3 Method"}, {"figure_path": "diYnEYUbIU/figures/figures_4_1.jpg", "caption": "Figure 1: Panoramic segmentation problem can be re-formulated to the estimation of over-sampled segments: floor, ceiling and under-sampled segments: chair, table, bookcase, window, etc.", "description": "This figure illustrates the proposed method for panoramic semantic segmentation. The input is a panoramic image, which is divided into two groups: over-sampled segments (floor and ceiling), and under-sampled segments (other scene elements such as chairs, tables, etc.). The over-sampled segments are enhanced using a depth estimation task, and the under-sampled segments are estimated using a transformer-based context module that leverages the geometric properties of the scene. Finally, the estimated segments are merged to produce the final segmentation result.", "section": "1 Introduction"}, {"figure_path": "diYnEYUbIU/figures/figures_5_1.jpg", "caption": "Figure 4: Transformer based context module", "description": "This figure shows the architecture of the transformer-based context module, a key component of the hybrid decoder in the proposed framework for Indoor Panoramic Semantic Segmentation.  The module integrates multiple geometric representations of the scene, including the global image feature (Fimg), hidden features from the over-sampled segment estimation (Fhid), 3D point cloud (Fpc), vertical relative distances (Fdist), and predicted floor-ceiling masks (Fm). These features are concatenated and fed into a stack of transformer encoder layers to capture contextual relationships and geometric information. The output of the context module is then combined with features from a high-resolution branch to produce refined semantic masks for the under-sampled segments.", "section": "3 Method"}, {"figure_path": "diYnEYUbIU/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative comparison of semantic segmentation results from Trans4PASS+ [35], SGAT4PASS [18], and ours using the Stanford2D3D dataset. Black boxes highlight the improvements. Zoom for the better view.", "description": "This figure displays a qualitative comparison of semantic segmentation results on the Stanford2D3D dataset.  It shows the results from three different methods: Trans4PASS+, SGAT4PASS, and the authors' proposed method. The ground truth segmentation is also included for comparison. Black boxes highlight the areas where the authors' method shows improvement over the other two methods. The images show that the authors' method achieves better segmentation, especially in areas with complex geometric structures and smaller objects.", "section": "4.2 Experiment Results"}, {"figure_path": "diYnEYUbIU/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparison of semantic segmentation results from Trans4PASS+ [35], SGAT4PASS [18], and ours using the Stanford2D3D dataset. Black boxes highlight the improvements. Zoom for the better view.", "description": "This figure compares the qualitative results of semantic segmentation on the Stanford2D3DS dataset between three different methods: Trans4PASS+, SGAT4PASS, and the proposed method in the paper.  The figure shows several example images alongside their respective segmentation masks generated by each method. The black boxes highlight areas where the proposed method shows improvement over the other two. The comparison emphasizes the ability of the proposed method to handle more complex and diverse indoor scenes by accurately segmenting smaller and more intricate objects compared to baseline methods.", "section": "4.2 Experiment Results"}, {"figure_path": "diYnEYUbIU/figures/figures_9_1.jpg", "caption": "Figure 5: Qualitative comparison of semantic segmentation results from Trans4PASS+ [35], SGAT4PASS [18], and ours using the Stanford2D3D dataset. Black boxes highlight the improvements. Zoom for the better view.", "description": "This figure shows a qualitative comparison of semantic segmentation results on the Stanford2D3D dataset, comparing the proposed method against Trans4PASS+ and SGAT4PASS.  Each row presents an input image and the corresponding segmentation masks generated by each method, along with the ground truth. Black boxes highlight areas where the proposed method shows improvements. The figure demonstrates the effectiveness of the proposed approach in handling challenging scenarios, such as differentiating between objects with similar appearances (e.g., boards and walls) and accurately segmenting smaller objects, unlike previous methods which tend to focus on larger segments.", "section": "4.2 Experiment Results"}, {"figure_path": "diYnEYUbIU/figures/figures_13_1.jpg", "caption": "Figure 8: Convert equirectangular image to spherical image. Image is adjusted from Ai et al. [1].", "description": "This figure illustrates the conversion process from an equirectangular image to a spherical image.  It shows how a pixel (u,v) in the 2D equirectangular image projection maps to a point (\u03c1,\u03b8,\u03c6) in 3D spherical coordinates. This conversion is a crucial step in the proposed method, allowing for the use of geometric information in the panoramic semantic segmentation task. The figure also shows the relationship between the width (W) and height (h) of both the equirectangular and spherical images and how these dimensions relate to the spherical coordinate system.", "section": "A.1 Projection of equiretangular image to 3D points in Cartesian coordinates using depths."}, {"figure_path": "diYnEYUbIU/figures/figures_14_1.jpg", "caption": "Figure 2: The proposed framework consists of three main modules: an encoder for extracting image features, a branch that estimates over-sampled segments alongside dense depth estimation, and a hybrid decoder for estimating under-sampled segments before a merging process to obtain the final segmentation result.", "description": "This figure illustrates the overall architecture of the proposed method for indoor panoramic semantic segmentation. It comprises three main modules: an encoder to extract image features, a branch for simultaneously estimating over-sampled segments (floor and ceiling) and dense depth, and a hybrid decoder (combining a high-resolution branch and a transformer-based context module) for estimating under-sampled segments. Finally, the results from both branches are merged to produce the final segmentation.", "section": "3 Method"}, {"figure_path": "diYnEYUbIU/figures/figures_14_2.jpg", "caption": "Figure 2: The proposed framework consists of three main modules: an encoder for extracting image features, a branch that estimates over-sampled segments alongside dense depth estimation, and a hybrid decoder for estimating under-sampled segments before a merging process to obtain the final segmentation result.", "description": "This figure shows the overall architecture of the proposed method for indoor panoramic semantic segmentation.  It's composed of three main modules: an encoder to extract image features, a branch to concurrently estimate over-sampled segments (floor and ceiling) and dense depth, and a hybrid decoder which combines a high-resolution branch and a transformer-based context module to estimate under-sampled segments. Finally, a merging step combines the over-sampled and under-sampled segment estimations to generate the final segmentation result.", "section": "3 Method"}, {"figure_path": "diYnEYUbIU/figures/figures_15_1.jpg", "caption": "Figure 12: More examples of distance to ceiling and distance to floor masks, where light to dark colors represent distances from far to near.", "description": "This figure shows four example images from the dataset and their corresponding ceiling and floor masks, along with the dense depth prediction and the calculated distances to ceiling and floor.  The color gradients in the distance maps visually represent the distances, ranging from light (far) to dark (near). This illustrates how the proposed method effectively estimates these distances, which is crucial for differentiating between objects located at different heights in the scene. This information is utilized in the semantic segmentation process to improve performance, particularly in areas with objects positioned closely to the floor or ceiling.", "section": "A.4 More examples of distance to ceiling and distance to floor masks"}, {"figure_path": "diYnEYUbIU/figures/figures_15_2.jpg", "caption": "Figure 13: a) Input image. b) Predicted depth. c) Ceiling mask before softmax. d) Floor mask before softmax. e) Different views of pointcloud constructed from predicted depth. f) Ceiling and floor in 3D visualization. g) Planes of ceiling and floor in 3D coordinates after applying least square method. h) Distance of 3D points to ceiling plane. i) Distance of 3D points to floor plane. k) Final segmentation.", "description": "This figure visualizes the step-by-step process of the proposed method. It starts with an input image (a), then shows the predicted depth (b), ceiling mask before softmax (c), floor mask before softmax (d), and different views of the point cloud (e). Next, it shows the ceiling and floor in 3D visualization (f), the planes of the ceiling and floor in 3D coordinates (g), the distance of 3D points to the ceiling plane (h), the distance of 3D points to the floor plane (i), and finally the final segmentation (k).", "section": "3 Method"}]