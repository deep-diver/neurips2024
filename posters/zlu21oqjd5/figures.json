[{"figure_path": "zLU21oQjD5/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Average accuracy on six mathematical benchmarks. We compare with models fine-tuned on the best, public instruction tuning datasets for mathematical problem-solving: MetaMath (Yu et al., 2024) with 395k examples, MMIQC (Liu et al., 2024a) with 2.3 million examples, as well as vanilla rejection tuning (VRT) with 590k examples. Both DART-Math (Uniform) and DART-Math (Prop2Diff) use 590k training examples. Right: Number of responses for each query descending by difficulty across 3 synthesis strategies. Queries are from the MATH training split (Hendrycks et al., 2021). VRT is the baseline biased towards easy queries, while Uniform and Prop2Diff are proposed in this work to balance and bias towards difficult queries respectively. Points are slightly shifted and downsampled for clarity.", "description": "The figure presents a comparison of the average accuracy of different models on six mathematical benchmarks (left panel) and an analysis of the number of responses generated for queries of varying difficulty using three different synthesis strategies (right panel). The left panel shows that DART-Math models outperform other models, including those trained on larger datasets. The right panel illustrates the impact of different sampling strategies on the coverage of difficult queries, highlighting the advantage of the proposed difficulty-aware rejection tuning.", "section": "1 Introduction"}, {"figure_path": "zLU21oQjD5/figures/figures_2_1.jpg", "caption": "Figure 2: Left: Number of queries in the MATH training set and the MetaMathQA-MATH-AnsAug set across 5 difficulty levels annotated by humans. MetaMathQA-MATH-AnsAug is generated through rejection sampling from the original training queries. We annotate the query coverage ratio of MetaMathQA. While the most difficult queries (Level 5) are predominant in the original set, synthetic examples bias towards easier queries, dropping over 50% of the most difficult queries. Middle: Total number of responses for queries across different difficulty levels in MetaMathQA-MATH-AnsAug. The most difficult queries represent the smallest proportion, only accounting for 10.5% of all the samples. Right: pass@k accuracy of different DeepSeekMath (DSMath) models and temperatures (t) on MATH500 (Lightman et al., 2024), a subset of MATH test set. With enough trials, models are actually able to sample out answer-correct responses to most (>99%) queries.", "description": "This figure shows the bias of rejection-based data synthesis towards easy queries. The left panel shows that the proportion of difficult queries (Level 5) decreases significantly in the synthetic dataset MetaMathQA compared to the original MATH dataset. The middle panel shows that the number of responses for difficult queries is also much smaller in MetaMathQA. The right panel shows that a strong model (DeepSeekMath-7B-RL) can generate correct responses for most queries given enough trials, suggesting that the bias in rejection-based data synthesis is not due to the inherent difficulty of the queries but rather to the sampling strategy.", "section": "Biases in Rejection-Based Data Synthesis"}, {"figure_path": "zLU21oQjD5/figures/figures_7_1.jpg", "caption": "Figure 3: Scaling curves of MATH test performance against number of training samples synthesized from MATH training queries, training is on three base models.", "description": "This figure shows the scaling curves of MATH test performance for three different base models (Mistral-7B, Llama3-8B, and DeepSeekMath-7B) as the number of training samples increases.  The x-axis represents the number of training samples (log scale), and the y-axis represents the accuracy on the MATH test set. Three lines are plotted for each model, representing the performance of vanilla rejection tuning (VRT), DART with uniform sampling, and DART with difficulty-proportional sampling.  The figure demonstrates that DART, particularly the difficulty-proportional version, consistently outperforms VRT across all three models and across a wide range of training data sizes, highlighting the effectiveness of the difficulty-aware rejection tuning technique.", "section": "4.3 Analysis"}, {"figure_path": "zLU21oQjD5/figures/figures_8_1.jpg", "caption": "Figure 4: From Left to Right, (1) and (2): Scaling curves studying the effect of one-response coverage. \"Prop2Diff (-Cover)\" denotes DARS-Prop2Diff without enforcing at least one synthetic response for each query, while \"VRT (+Cover)\" denotes vanilla rejection sampling enforcing at least one synthetic response for each query. (3) and (4): The total number of raw samples needed, and the actual ratio (r) of queries achieving the desiderata of the two DARS synthesis strategy for 585k-sized dataset curation respectively, when we vary the maximum allowable raw samples per query (nmax).", "description": "This figure analyzes the impact of ensuring at least one synthetic response for each query during data synthesis.  It shows scaling curves for MATH and GSM8K benchmarks comparing vanilla rejection tuning (VRT) with and without the one-response constraint, and DARS-Prop2Diff with and without the constraint. Additionally, it illustrates the total number of raw samples needed and the ratio of queries achieving the desired number of correct responses for both DARS strategies, varying the maximum number of raw samples per query.", "section": "4.3 Analysis"}]