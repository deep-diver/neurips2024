[{"figure_path": "zLU21oQjD5/tables/tables_4_1.jpg", "caption": "Table 1: Comparison between our DART-Math datasets and previous mathematical instruction tuning datasets. Most of previous datasets are constructed with ChatGPT, and many of them are not open-source, especially for ones of the best performance.", "description": "This table compares the DART-Math datasets with other existing mathematical instruction tuning datasets.  It highlights key differences such as the number of samples, the model used for synthesis (many use proprietary models like GPT-4, while DART uses an open-weight model), and whether the dataset is publicly available (open-source).  The table shows that DART-Math is significantly smaller than most other datasets, yet still achieves state-of-the-art performance,  making it a more efficient and accessible resource.", "section": "1 Introduction"}, {"figure_path": "zLU21oQjD5/tables/tables_6_1.jpg", "caption": "Table 2: Main results on mathematical benchmarks. College, DM, Olympiad, Theorem denote the CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA benchmarks respectively. We annotate the absolute accuracy change compared to the VRT baseline within the same base model. Bold means the best score within the respective base model. ICL, MetaMath, MMIQC, and VRT baselines are from our own runs, while other numbers are copied from the respective papers or reports. For WizardMath and Xwin-Math, we take the public model checkpoints and evaluate ourselves using their official CoT prompt. : For Xwin-Math, we take the best public models that are based on Llama2 (Touvron et al., 2023), which is not a very fair comparison with others.", "description": "This table presents the main experimental results comparing the performance of DART-Math models with various baselines across six mathematical benchmarks (two in-domain and four out-of-domain).  It shows the accuracy on each benchmark for different models (varying in size and architecture), including those fine-tuned with different datasets (MetaMath, MMIQC, KPMath-Plus, Xwin-Math, Vanilla Rejection Tuning, and DART-Math with Uniform and Prop2Diff strategies).  The table highlights the improvements achieved by DART-Math, especially its superior performance on challenging out-of-domain benchmarks despite using smaller datasets.  It also indicates the difference between DART-Math models trained using the Uniform and Prop2Diff sampling strategies.", "section": "4.2 Main Results"}, {"figure_path": "zLU21oQjD5/tables/tables_14_1.jpg", "caption": "Table 3: Comparison between datasets synthesized by methods based on non-vanilla rejection sampling. \"RPQ\" means the average number of responses per query. The ToRA and MARIO datasets here are implemented by us according to their papers' descriptions, since the official implementations have not been open-sourced.", "description": "This table compares the number of responses per query (RPQ) for datasets created using ToRA, MARIO, and DART-Math methods.  It shows the RPQ for GSM8K queries and for different difficulty levels (1-5) within the MATH dataset.  The MATH Coverage indicates the percentage of queries in the MATH dataset for which at least one response was generated.  The table highlights that DART-Math generates a significantly larger number of responses, particularly for the more difficult MATH queries (levels 3-5), demonstrating its ability to overcome the bias towards easier queries that other methods suffer from.", "section": "A Comparison to Methods Based on Non-vanilla Rejection Sampling"}, {"figure_path": "zLU21oQjD5/tables/tables_15_1.jpg", "caption": "Table 4: MATH training set coverage rates across all the difficulty levels of different synthetic datasets. The numbers of ToRA-Corpus-16k-MATH are from their OpenReview page\u00b3. The two DART-Math-* datasets have the same coverage because of the \"Cover\" operation, which tries to ensure there is at least one correct response for each query.", "description": "This table compares the coverage of different difficulty levels of MATH training queries across four different synthetic datasets: ToRA-Corpus-16k-MATH, MetaMathQA-MATH-AnsAug, a Vanilla Rejection Tuning (VRT) baseline, and the two DART-Math datasets (Uniform and Prop2Diff).  It shows the percentage of queries covered at each difficulty level (1-5, with 5 being the most difficult). The DART-Math datasets achieve significantly higher coverage, especially at the most difficult level, demonstrating their effectiveness in addressing the class imbalance in mathematical problem-solving datasets.", "section": "4.3 Analysis"}, {"figure_path": "zLU21oQjD5/tables/tables_16_1.jpg", "caption": "Table 2: Main results on mathematical benchmarks. College, DM, Olympiad, Theorem denote the CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA benchmarks respectively. We annotate the absolute accuracy change compared to the VRT baseline within the same base model. Bold means the best score within the respective base model. ICL, MetaMath, MMIQC, and VRT baselines are from our own runs, while other numbers are copied from the respective papers or reports. For WizardMath and Xwin-Math, we take the public model checkpoints and evaluate ourselves using their official CoT prompt. : For Xwin-Math, we take the best public models that are based on Llama2 (Touvron et al., 2023), which is not a very fair comparison with others.", "description": "This table presents the main results of the experiments conducted on six mathematical benchmarks.  The table compares the performance of DART-Math models (using different base models and data synthesis strategies) against several baselines (Vanilla Rejection Tuning and state-of-the-art models from other papers). Both in-domain and out-of-domain benchmarks are included.  Performance is measured by average accuracy, and improvements compared to the VRT baseline are highlighted.", "section": "4.2 Main Results"}, {"figure_path": "zLU21oQjD5/tables/tables_17_1.jpg", "caption": "Table 5: Examples of training time cost.", "description": "This table shows the training time cost for different models on the DART-Math-Hard dataset. The training time cost varies depending on the model size and the hardware used. For example, training DeepSeekMath-7B on 8 A100 GPUs takes 3 hours per epoch, while training Llama3-70B on 32 A100 GPUs takes 6 hours per epoch.", "section": "4.1 General Setup"}, {"figure_path": "zLU21oQjD5/tables/tables_18_1.jpg", "caption": "Table 6: MATH performance across all the domains. Macro average assigns equal weights to each domain, while micro average assigns equal weights to each query, which is the same to the whole-benchmark score. The full names of the domains are Counting & Probability, Prealgebra, Number Theory, Intermediate Algebra, Algebra, Precalculus, Geometry, respectively. Bold means the best score within the respective base model.", "description": "This table presents a detailed breakdown of the performance of different models on six mathematical domains within the MATH benchmark.  It compares the vanilla rejection tuning (VRT) baseline with the DART-Math models using both uniform and difficulty-proportional sampling strategies.  The results are shown for both micro and macro averages, providing insights into the model's performance across different query types and overall.", "section": "4.2 Main Results"}, {"figure_path": "zLU21oQjD5/tables/tables_18_2.jpg", "caption": "Table 7: Performance by DART and RL on DeepSeekMath-7B. College, DM, Olympiad, Theorem denote the CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA benchmarks respectively. Bold means the best score within the respective base model.", "description": "This table compares the performance of DART-Math models with a reinforcement learning (RL) model, DeepSeekMath-7B-RL, on six mathematical benchmarks.  It shows that DART-Math, despite being a supervised fine-tuning (SFT) method, achieves comparable performance to DeepSeekMath-7B-RL, a reinforcement learning model. The results highlight that DART-Math's performance is competitive, even when compared to approaches utilizing RL.", "section": "4.2 Main Results"}]