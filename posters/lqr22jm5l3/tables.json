[{"figure_path": "LQR22jM5l3/tables/tables_3_1.jpg", "caption": "Table 1: Examples of causal Bayesian networks with undesired dependencies between Y and Z displayed by red edges. Light gray indicates unobserved variables. Xyz = 0 in (a-b) and there is no entanglement between Y and Z via X. In (c), we expand the system to include V \u2208 U and its influence on X, which is given by Xv. For each Causal Bayesian Network considered, we display when data balancing leads to a risk-invariant and/or optimal model. We compare these with regularization following Veitch et al. [74] and suggest next steps.", "description": "This table presents four causal Bayesian networks illustrating different scenarios of undesired dependencies between variables Y and Z.  Each network shows whether data balancing leads to a risk-invariant and/or optimal model, comparing the results with regularization methods and proposing further steps to mitigate any remaining issues.", "section": "3 Can we predict when data balancing fails?"}, {"figure_path": "LQR22jM5l3/tables/tables_4_1.jpg", "caption": "Table 2: Model performance on semi-synthetic data, for the tasks in Figure 1. 'Acc' refers to accuracy, 'Worst Grp' to worst group accuracy, 'Encoding' to confounder encoding as measured by transfer learning and 'Equ. Odds' refers to equalized odds between Z subgroups. \u2191(resp.\u2193) means the higher (resp. lower), the better.", "description": "This table presents the performance of models trained on semi-synthetic data generated for four different scenarios (purely spurious correlation, additional confounder, entangled signals, and causal task with spurious correlation) depicted in Figure 1.  The performance is evaluated on both the original training distribution (Pt) and a modified distribution (P0) where the undesired correlation is removed. The metrics reported are accuracy (Acc), worst-group accuracy (Worst Grp), confounder encoding (Encoding), and equalized odds (Equ. Odds).  The arrows indicate whether higher or lower values are better for each metric.", "section": "3 Can we predict when data balancing fails?"}, {"figure_path": "LQR22jM5l3/tables/tables_8_1.jpg", "caption": "Table 3: VGG model performance on CelebA, when trained on Pt, on Q, with ImageNet pre-training ('Pre-trained') on Q, with MMD ('MMD') on Pt with regularizer=5. All models are evaluated on Q.", "description": "This table shows the performance of different models trained on the CelebA dataset.  The models used are a VGG network and various sized Vision Transformers.  The models were trained using either the original, imbalanced data (Pt), balanced data (Q), pre-trained on balanced data (Pre-trained on Q), or trained on imbalanced data with MMD regularization (MMD on Pt).  The table reports accuracy, worst-group accuracy, confounder encoding, and equalized odds, all evaluated on the balanced dataset (Q).  This allows for a comparison of the effect of different training methods on the performance and fairness of the models.", "section": "6 Case study: distinguishing between failure modes in CelebA"}, {"figure_path": "LQR22jM5l3/tables/tables_26_1.jpg", "caption": "Table 1: Examples of causal Bayesian networks with undesired dependencies between Y and Z displayed by red edges. Light gray indicates unobserved variables. Xyz = 0 in (a-b) and there is no entanglement between Y and Z via X. In (c), we expand the system to include V \u2208 U and its influence on X, which is given by Xv. For each Causal Bayesian Network considered, we display when data balancing leads to a risk-invariant and/or optimal model. We compare these with regularization following Veitch et al. [74] and suggest next steps.", "description": "This table presents four causal Bayesian networks illustrating scenarios with undesired dependencies between Y and Z. Each scenario shows if data balancing leads to risk-invariant and/or optimal models. The impact of regularization strategies is also compared, suggesting next steps for each case.", "section": "Can we predict when data balancing fails?"}]