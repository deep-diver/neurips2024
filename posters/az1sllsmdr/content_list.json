[{"type": "text", "text": "Elucidating the Design Space of Dataset Condensation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shitong Shao\u2020\u2662, Zikai $\\mathbf{Zhou}^{\\dagger}\\diamond$ , Huanran Chen\u2020\u2021, Zhiqiang Shen\u2020\u2217 ", "page_idx": 0}, {"type": "text", "text": "\u2020 Mohamed bin Zayed University of AI, \u2021 Tsinghua University $\\diamondsuit$ The Hong Kong University of Science and Technology (Guangzhou) {1090784053sst,choukai003}@gmail.com, huanran_chen@outlook.com zhiqiang.shen@mbzuai.ac.ae, $^*$ : Corresponding author ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dataset condensation, a concept within data-centric learning, efficiently transfers critical attributes from an original dataset to a synthetic version, maintaining both diversity and realism. This approach significantly improves model training efficiency and is adaptable across multiple application areas. Previous methods in dataset condensation have faced challenges: some incur high computational costs which limit scalability to larger datasets (e.g., MTT, DREAM, and TESLA), while others are restricted to less optimal design spaces, which could hinder potential improvements, especially in smaller datasets (e.g., $\\mathrm{SRe^{2}L}$ , G-VBSM, and RDED). To address these limitations, we propose a comprehensive design framework that includes specific, effective strategies like implementing soft category-aware matching and adjusting the learning rate schedule. These strategies are grounded in empirical evidence and theoretical backing. Our resulting approach, Elucidate Dataset Condensation (EDC), establishes a benchmark for both small and largescale dataset condensation. In our testing, EDC achieves state-of-the-art accuracy, reaching $48.6\\%$ on ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a compression ratio of $0.78\\%$ . This performance exceeds those of $\\mathrm{SRe^{2}L}$ , G-VBSM, and RDED by margins of $27.3\\%$ , $17.2\\%$ , and $6.6\\%$ , respectively. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dataset condensation, also known as dataset distillation, has emerged in response to the everincreasing training demands of advanced deep learning models (He et al., 2016a,b; Brown et al., 2020). This approach addresses the challenge of requiring high-precision models while also managing substantial resource constraints (Dosovitskiy et al., 2020; Shao et al., 2024). In this method, the original dataset acts as a \u201cteacher\u201d, distilling and preserving essential information into a smaller, surrogate \u201cstudent\u201d dataset. The ultimate goal of this technique is to achieve performance comparable to the original by training models from scratch with the condensed dataset. This approach has become popular in various downstream applications, including continual learning Masarczyk and Tautkute (2020); Sangermano et al. (2022); Zhao and Bilen (2021), neural architecture search Such et al. (2020); Zhao and Bilen (2023); Zhao et al. (2021), and training-free network slimming Liu et al. (2017). ", "page_idx": 0}, {"type": "text", "text": "Unfortunately, the prohibitively expensive bi-level optimization paradigm limits the effectiveness of traditional dataset distillation methods, such as those presented in previous studies (Cazenavette et al., 2022; Sajedi et al., 2023; Liu et al., 2023a), particularly when applied to large-scale datasets like ImageNet-1k (Russakovsky et al., 2015). In response, the uni-level optimization paradigm has gained significant attention as a potential solution, with recent contributions from the research community (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023) highlighting its utility. These methods primarily leverage the rich and extensive information from static, pre-trained observer models, facilitating a more streamlined optimization process on a condensed dataset without the need to adjust other parameters (e.g., those within the observer models). While uni-level optimization has demonstrated remarkable performance on large datasets, it has yet to achieve the benchmark accuracy levels seen with classical methods on datasets like CIFAR-10/100 (Krizhevsky et al., 2009). Moreover, the newly developed training-free RDED (Sun et al., 2024) significantly outperforms previous methods in efficiency and maintains effectiveness, yet it overlooks the potential information loss due to the lack of image optimization. Additionally, some simple but promising techniques (e.g., smoothing the learning rate schedule) that could enhance performance have been underexplored in existing literature. For instance, applying these techniques allowed RDED to achieve a performance improvement of $16.2\\%$ . ", "page_idx": 0}, {"type": "image", "img_path": "az1SLLsmdR/tmp/4e0970d7b79ef05c9c70361a7c97caf98f03d66a1cd3923d664b101458187ee4.jpg", "img_caption": ["Figure 1: Illustration of Elucidating Dataset Condensation (EDC). Left: The overall of our better design choices in dataset condensation on ImageNet-1k. Right: The evaluation performance and data synthesis required time of different configurations on ResNet-18 with IPC 10. Our integral EDC refers to $\\mathbb{C}\\mathbb{O}\\mathbb{N}\\mathbb{F}\\mathbb{G}$ G. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "These drawbacks show the constraints of previous methods in several respects, highlighting the need for a thorough investigation and assessment of these issues. In contrast to earlier strategies that targeted specific improvements, our approach systematically examines all viable facets and integrates them into our novel framework. To establish a comprehensive model, we carefully analyze deficiencies during the data synthesis, soft label generation, and post-evaluation stages of dataset condensation, resulting in an extensive exploration of the design space on the large-scale ImageNet-1k. As a result, we introduce Elucidate Dataset Condensation (EDC), which includes a range of detailed and effective enhancements (refer to Fig. 1). For instance, soft category-aware matching ${\\widehat{\\pmb{\\bigotimes}}},$ ) ensures consistent category representation between the original and condensed dataset batches for more precise matching. Importantly, EDC not only achieves state-of-the-art performance on CIFAR-10, CIFAR-100, Tiny-ImageNet, ImageNet-10, and ImageNet-1k, at half the computational expense compared to the baseline G-VBSM, but it also provides in-depth empirical and theoretical insights that affirm the soundness of our design decisions. ", "page_idx": 1}, {"type": "text", "text": "2 Dataset Condensation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Preliminary. Dataset condensation involves generating a synthetic dataset $\\mathcal{D}^{s}:=\\{\\mathbf{x}_{i}^{S},\\mathbf{y}_{i}^{S}\\}_{i=1}^{|\\mathcal{D}^{s}|}$ consisting of images and labels , designed to be as informative as the original dataset $\\mathcal{D}^{\\mathcal{T}}:=\\{\\mathbf{x}_{i}^{\\mathcal{T}},\\mathbf{y}_{i}^{\\mathcal{T}}\\}_{i=1}^{|\\mathcal{D}^{\\mathcal{T}}|}$ , which includes images $\\chi^{\\mathcal{T}}$ and labels $y^{\\mathcal{T}}$ . The synthetic dataset $\\mathcal{D}^{S}$ is substantially smaller in size than $\\mathcal{D}^{\\mathcal{T}}\\left(\\lvert\\mathcal{D}^{\\mathcal{S}}\\rvert\\ll\\lvert\\mathcal{D}^{\\mathcal{T}}\\rvert\\right)$ . The goal of this process is to maintain the critical attributes of $\\mathcal{D}^{\\mathcal{T}}$ to ensure robust or comparable performance during evaluations on test protocol $\\mathcal{P}_{\\mathcal{D}}$ . ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\arg\\operatorname*{min}\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{P}_{D}}[\\ell_{\\mathrm{exa}}(\\mathbf{x},\\mathbf{y},\\phi^{*})],\\;\\;\\mathrm{where}\\;\\phi^{*}=\\arg\\operatorname*{min}_{\\phi}\\mathbb{E}_{(\\mathbf{x}_{i}^{s},\\mathbf{y}_{i}^{s})\\sim\\mathcal{D}^{S}}[\\ell(\\phi(\\mathbf{x}_{i}^{S}),\\mathbf{y}_{i}^{S})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $\\ell_{\\mathbf{eval}}(\\cdot,\\cdot,\\phi^{*})$ represents the evaluation loss function, such as cross-entropy loss, which is parameterized by the neural network $\\phi^{*}$ that has been optimized from the distilled dataset $\\mathcal{D}^{S}$ . The data synthesis process primarily determines the quality of the distilled datasets, which transfers desirable knowledge from $\\bar{\\mathcal{D}}^{\\mathcal{T}}$ to $\\mathcal{D}^{\\tilde{S}}$ through various matching mechanisms, such as trajectory matching (Cazenavette et al., 2022), gradient matching (Zhao et al., 2021), distribution matching (Zhao and Bilen, 2023) and generalized matching (Shao et al., 2023). ", "page_idx": 1}, {"type": "text", "text": "Small-scale vs. Large-scale Dataset Condensation/Distillation. Traditional dataset condensation algorithms, as referenced in studies such as (Wang et al., 2018; Cazenavette et al., 2022; Cui et al., ", "page_idx": 1}, {"type": "text", "text": "2023; Wang et al., 2022; Nguyen et al., 2020), encounter computational challenges and are generally confined to small-scale datasets like CIFAR-10/100 (Krizhevsky et al., 2009), or larger datasets with limited class diversity, such as ImageNette (Cazenavette et al., 2022) and ImageNet-10 (Kim et al., 2022). The primary inefficiency of these methods stems from their reliance on a bi-level optimization framework, which involves alternating updates between the synthetic dataset and the observer model utilized for distillation. This approach not only heavily depends on the model but also limits the versatility of the distilled datasets in generalizing across different architectures. In contrast, the uni-level optimization strategy, noted for its efficiency and enhanced performance on the regular $224\\!\\times\\!224$ scale of ImageNet-1k in recent research (Yin et al., 2023; Shao et al., 2023; Yin and Shen, 2024), shows reduced effectiveness in smaller-scale datasets due to the many iterations required in the data synthesis process without a direct connection to actual data. Recent new methods in training-free distillation paradigms, such as in (Sun et al., 2024; Zhou et al., 2023), offer advancements in efficiency. Nevertheless, these methods compromise data privacy and do not leverage statistical information from observer models, thereby restraining their potential in a training-free environment. ", "page_idx": 2}, {"type": "text", "text": "Generalized Data Synthesis Paradigm. We consistently describe algorithms (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023; Sun et al., 2024) that efficiently conduct data synthesis on ImageNet-1k as \u201cgeneralized data synthesis\u201d. This approach avoids the inefficient bi-level optimization and includes both image and label synthesis phases. Note that several recent works (Zhang et al., 2024a,b; Deng et al., 2024), particularly DANCE (Zhang et al., 2024a), effectively apply to ImageNet-1k; however, these methods lack enhancements in soft label generation and post-evaluation. Specifically, generalized data synthesis involves initially generating highly condensed images followed by acquiring soft labels through predictions from a pre-trained model. The evaluation process resembles knowledge distillation Hinton et al. (2015), aiming to transfer knowledge from a teacher to a student model whenever feasible (Gou et al., 2021; Hinton et al., 2015). The primary distinction between the training-dependent (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023) and training-free paradigms (Sun et al., 2024) centers on their approach to data synthesis. In detail, the training-dependent paradigm employs Statistical Matching (SM) to extract pertinent information from the entire dataset. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathtt{s p n}}=||p(\\mu|\\mathcal{X}^{S})-p(\\mu|\\mathcal{X}^{T})||_{2}+||p(\\sigma^{2}|\\mathcal{X}^{S})-p(\\sigma^{2}|\\mathcal{X}^{T})||_{2},\\,s.t.\\,\\mathcal{L}_{\\mathtt{s p n}}\\sim\\operatorname{\\mathrm{\\&}}_{\\operatorname{mach}},}\\\\ &{\\mathcal{X}^{S*}=\\underset{\\mathcal{X}^{S}}{\\mathrm{arg\\,min}}\\,\\mathbb{E}_{\\mathcal{L}_{\\mathtt{s p n}}\\sim\\mathbb{S}_{\\operatorname{mach}}}[\\mathcal{L}_{\\mathtt{s p n}}(\\mathcal{X}^{S},\\mathcal{X}^{T})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{S}_{\\mathrm{match}}$ represents the extensive collection of statistical matching operators, which operate across a variety of network architectures and layers as described by (Shao et al., 2023). Here, $\\mu$ and $\\sigma^{2}$ are defined as the mean and variance, respectively. For more detailed theoretical insights, please refer to Definition 3.1. The training-free approach, as discussed in (Sun et al., 2024; Zhou et al., 2023), employs a direct reconstruction method for the original dataset, aiming to generate simplified representations of images. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{X}}^{S}=\\bigcup_{i=1}^{\\mathrm{C}}\\boldsymbol{\\mathcal{X}}_{i}^{S},\\;\\boldsymbol{\\mathcal{X}}_{i}^{S}=\\{\\mathbf{x}_{j}^{i}=\\mathrm{concat}(\\{\\tilde{\\mathbf{x}}_{k}\\}_{k=1}^{N}\\subset\\boldsymbol{\\mathcal{X}}_{i}^{T})\\}_{j=1}^{|\\mathrm{PC}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{C}$ denotes the number of classes, concat $(\\cdot)$ represents the concatenation operator, $\\mathcal{X}_{i}^{S}$ signifies the set of condensed images belonging to the $i$ -th class, and $\\mathcal{X}_{i}^{\\mathcal{T}}$ corresponds to the set of original images of the $i$ -th class. It is important to note that the default settings for $N$ are 1 and 4, as specified in the works (Zhou et al., 2023) and (Sun et al., 2024), respectively. Using one or more observer models, denoted as $\\{\\phi_{i}\\}_{i=1}^{N}$ , we then derive the soft labels $\\dot{\\mathcal{V}}^{s}$ from the condensed image set $\\mathcal{X}^{s}$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{V}}^{s}=\\bigcup_{\\mathbf{x}_{i}^{s}\\subset\\mathcal{X}^{s}}\\frac{1}{N}\\sum_{i=1}^{N}\\phi_{i}(\\mathbf{x}_{i}^{s}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This plug-and-play component, as outlined in $\\mathrm{SRe^{2}L}$ (Yin et al., 2023) and IDC (Kim et al., 2022), plays a crucial role for enhancing the generalization ability of the distilled dataset $\\mathcal{D}^{S}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Improved Design Choices ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Design choices in data synthesis, soft label generation, and post-evaluation significantly influence the generalization capabilities of condensed datasets. Effective strategies for small-scale datasets are well-explored, yet these approaches are less examined for large-scale datasets. We first delineate ", "page_idx": 2}, {"type": "image", "img_path": "az1SLLsmdR/tmp/9e91d30fdd667a837487d4fa3b0317b6d980c89c2a13f6495cd68b664f073065.jpg", "img_caption": ["Figure 2: (a): Illustration of soft category-aware matching ( $\\left(\\widehat{\\pmb{\\mathscr{Q}}}\\right)$ using a Gaussian distribution in $\\mathbb{R}^{2}$ . (b): The effect of employing smoothing LR schedule ( $\\left(\\widehat{\\pmb{\\mathscr{Q}}}\\right)$ on loss landscape sharpness reduction. (c) top: The role of flatness regularization $(\\widehat\\mathbf{\\Phi}(\\widehat\\mathbf{D})$ in reducing the Frobenius norm of the Hessian matrix driven by data synthesis iteration. (c) bottom: Cosine similarity comparison between local gradients (obtained from original and distilled datasets via random batch selection) and the global gradient (obtained from gradient accumulation). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "the limitations of existing algorithms\u2019 design choices on ImageNet-1k. We then propose solutions, showcasing experimental results as shown in Fig. 1. For most design choices, we offer both theoretical analysis and empirical insights to facilitate a thorough understanding, as detailed in Sec. 3.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Limitations of Prior Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Lacking Realism (solved by $\\circledcirc$ ). Training-dependent condensation algorithms for datasets, particularly those employed for large-scale datasets, typically initiate the optimization process using Gaussian noise inputs (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023). This initial choice complicates the optimization process and often results in the generation of synthetic images that do not exhibit high levels of realism. The limitations in visualization associated with previous approaches are detailed in Appendix F. ", "page_idx": 3}, {"type": "text", "text": "Coarse-grained Matching Mechanism (solved by $\\circledcirc$ ). The Statistical Matching (SM)-based pipeline (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023) computes the global mean and variance by aggregating samples across all categories and uses these statistical parameters for matching purposes. However, this method exhibits two critical drawbacks: it does not account for the domain discrepancies among different categories, and it fails to preserve the integrity of category-specific information across the original and condensed samples within each batch. These limitations result in a coarse-grained matching approach that diminishes the accuracy of the matching process. ", "page_idx": 3}, {"type": "text", "text": "Overly Sharp of Loss Landscape (solved by $\\@$ and $\\circledcirc$ ). The optimization objective ${\\mathcal{L}}(\\theta)$ can be expanded through a second-order Taylor expansion as $\\mathcal{L}(\\theta^{*})+(\\theta-\\theta^{*})^{\\mathrm{T}}\\nabla_{\\theta}\\mathcal{L}(\\theta^{*})+(\\theta-\\theta^{*})^{\\mathrm{T}}\\mathbf{H}(\\theta-$ $\\theta^{*}$ ), with an upper bound of $\\mathcal{L}(\\boldsymbol{\\theta}^{*})+\\lvert\\lvert\\mathbf{H}\\rvert\\rvert_{\\mathbf{F}}\\mathbb{E}[\\lvert\\rvert\\rvert\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\rvert\\rvert_{2}^{2}]$ upon model convergence (Chen et al., 2024). However, earlier training-dependent condensation algorithms neglect to minimize the Frobenius norm of the Hessian matrix $\\mathbf{H}$ to obtain a flat loss landscape for enhancing its generalization capability through sharpness-aware minimization theory (Foret et al., 2020; Chen et al., 2022). Please see Appendix $\\textrm{C}$ for more formal information. ", "page_idx": 3}, {"type": "text", "text": "Irrational Hyperparameter Settings (solved by $\\circledcirc$ , $\\circledcirc$ , $\\circledcirc$ , $\\circledcirc$ and $\\circledcirc$ ). RDED (Sun et al., 2024) adopts a smoothing LR schedule $\\left(\\circledcirc\\right)$ and (Liu et al., 2023b; Yin and Shen, 2024; Sun et al., 2024) use a reduced batch size $\\left(\\mathbb{O}\\left(\\widehat{\\mathbf{o}}\\right)\\right)$ for post-evaluation on the full $224\\!\\times\\!224$ ImageNet-1k. These changes, although critical, lack detailed explanations and impact assessments in the existing literature. Our empirical analysis highlights a remarkable impact on performance: absent these modifications, RDED achieves only $25.8\\%$ accuracy on ResNet18 with IPC 10. With these modifications, however, accuracy jumps to $42.0\\%$ . In contrast, $\\mathrm{SRe^{2}L}$ and G-VBSM do not incorporate such strategies in their experimental frameworks. This work aims to fill the gap by providing the first comprehensive empirical analysis and ablation study on the effects of these and similar improvements in the field. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Our Solutions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To address these limitations described previously, we explore the design space and elaborately present a range of optimal solutions at both empirical and theoretical levels, as illustrated in Fig. 1. ", "page_idx": 4}, {"type": "text", "text": "Real Image Initialization $\\left(\\circledast\\right)$ . Intuitively, using real images instead of Gaussian noise for data initialization during the data synthesis phase is a practical and effective strategy. As shown in Fig. 3, this method significantly improves the realism of the condensed dataset and simplifies the optimization process, thus enhancing the synthesized dataset\u2019s ability to generalize in post-evaluation tests. Additionally, we incorporate considerations of information density and efficiency by employing a training-free condensed dataset (typically via RDED) for initialization at the start of the synthesis protheory, the cost of transporting from a Gaussian distribution to the original data distribution is higher than using the training-free condensed distribution as the initial reference. This advantage also allows us to reduce the number of iterations needed to achieve results to half of those required by our baseline G-VBSM model, significantly boosting synthesis efficiency. ", "page_idx": 4}, {"type": "image", "img_path": "az1SLLsmdR/tmp/d08f4ec083a88862d58c5cf5f740f00f1731a50b604b60164a324326ecb83b37.jpg", "img_caption": ["Figure 3: Comparison between real image initialization and random initialization. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. (proof in Appendix B.1) Considering samples $\\mathcal{X}_{r e a l}^{S},\\,\\mathcal{X}_{f r e e}^{S},$ and $\\mathcal{X}_{r a n d o m}^{S}$ from the original data, training-free condensed (e.g., RDED), and Gaussian distributions, respectively, let us assume $a$ cost function defined in optimal transport theory that satisfies $\\mathbb{E}[c(a-b)]\\stackrel{*}{\\propto}1/I(L a w(a),L a w(b))$ . Under this assumption, it follows that $\\mathbb{E}[c(\\mathcal{X}_{r e a l}^{S}-\\mathcal{X}_{f r e e}^{S})]\\overset{\\cdot}{\\leq}\\mathbb{E}[c(\\dot{\\mathcal{X}}_{r e a l}^{S}-\\dot{\\mathcal{X}}_{r a n d o m}^{S})]$ . ", "page_idx": 4}, {"type": "text", "text": "Soft Category-Aware Matching $\\left(\\widehat{\\pmb{\\mathscr{Q}}}\\right)$ . Previous dataset condensation methods (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023) based on the Statistical Matching (SM) framework have shown satisfactory results predominantly when the data follows a unimodal distribution (e.g., a single Gaussian). This limitation is illustrated with a simple example in Fig. 2 (a). Typically, datasets consist of multiple classes with significant variations among their class distributions. Traditional SM-based methods compress data by collectively processing all samples, thus neglecting the differences between classes. As shown in the top part of Fig. 2 (a), this method enhances information density but also creates a big mismatch between the condensed source distribution $\\mathcal{X}^{s}$ and the target distribution $\\chi^{\\tau}$ . To tackle this problem, we propose the use of a Gaussian Mixture Model (GMM) to effectively approximate any complex distribution. This solution is theoretically justifiable by the Tauberian Theorem under certain conditions (detailed proof provided in Appendix B.2). In light of this, we define two specific approaches to Statistical Matching: ", "page_idx": 4}, {"type": "text", "text": "Sketch Definition 3.1. (formal definition in Appendix $B.2$ ) Given $N$ random samples $\\{x_{i}\\}_{i=1}^{N}$ with an unknown distribution $p_{m i x}(x)$ , we define two forms to statistical matching. Form $(I)$ : involves synthesizing $M$ distilled samples $\\{y_{i}\\}_{i=1}^{M}$ , where $M\\ll N$ , ensuring that the variances and means of both $\\{x_{i}\\}_{i=1}^{N}$ and $\\{y_{i}\\}_{i=1}^{M}$ are consistent. Form (2): treats $p_{m i x}(x)$ as a GMM with $\\mathbf{C}$ components. For random samples $\\begin{array}{r}{\\{x_{i}^{j}\\}_{i=1}^{N_{j}}\\left(\\sum_{j}N_{j}=N\\right)}\\end{array}$ within each component $c_{j}$ , we synthesize $\\begin{array}{r}{M_{j}\\left(\\sum_{j}M_{j}=M\\right)}\\end{array}$ distilled samples $\\{y_{i}^{j}\\}_{i=1}^{M_{j}}$ , where $M_{j}\\ll N_{j}$ , to maintain the consistency of variances and means between $\\{x_{i}^{j}\\}_{i=1}^{N_{j}}$ and $\\{y_{i}^{j}\\}_{i=1}^{M_{j}}$ . ", "page_idx": 4}, {"type": "text", "text": "In general, $\\mathrm{SRe^{2}L}$ , CDA, and G-VBSM are all categorized under Form (1), as shown in Fig. 2 (a) at the top, leading to coarse-grained matching. According to Fig. 2 (a) at the bottom, transitioning to Form (2) is identified as a practical and appropriate alternative. However, our empirical result indicates that exclusive reliance on Form (1) yields a synthesized dataset that lacks sufficient information density. Consequently, we propose for a hybrid method that effectively integrates Form (1) and Form (2) using a weighted average, which we term soft category-aware matching. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\mathcal{L}_{\\mathtt{s p n}}^{\\prime}=\\alpha||p(\\mu|\\mathcal{X}^{S})-p(\\mu|\\mathcal{X}^{T})||_{2}+||p(\\sigma^{2}|\\mathcal{X}^{S})-p(\\sigma^{2}|\\mathcal{X}^{T})||_{2}\\quad\\#\\mathrm{Form~(1)}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}+\\left(1-\\alpha\\right)\\sum_{i}^{\\mathbf{C}}p(c_{i})\\left[||p(\\mu|\\mathcal{X}^{S},c_{i})-p(\\mu|\\mathcal{X}^{T},c_{i})||_{2}+||p(\\sigma^{2}|\\mathcal{X}^{S},c_{i})-p(\\sigma^{2}|\\mathcal{X}^{T},c_{i})||_{2}\\right],}}&{{\\#\\mathrm{Form~(1)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{C}$ represents the total number of components, $c_{i}$ indicates the $i$ -th component within a GMM, and $\\alpha$ is a coefficient for adjusting the balance. The modified loss function $\\mathcal{L}_{\\mathrm{syn}}^{\\prime}$ is designed to effectively regulate the information density of $\\mathcal{X}^{s}$ and to align the distribution of $\\mathcal{X}^{S}$ with that of $\\chi^{\\tau}$ . Operationally, each category in the original dataset is mapped to a distinct component in the GMM framework. Particularly, when $\\alpha=1$ , the sophisticated category-aware matching described by $\\mathcal{L}_{\\mathrm{syn}}^{\\prime}$ in Eq. 5 simplifies to the basic statistical matching defined by $\\mathcal{L}_{\\mathrm{syn}}$ in Eq. 2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. (proofs in Theorems B.5, B.7, B.8 and Corollary B.6) Given the original data distribution $p_{m i x}(x)$ , and define condensed samples as $x$ and $y$ in Form $(I)$ and Form (2) with their distributions characterized by $P$ and $Q$ . Subsequently, it follows that (i) $\\mathbb{E}[x]\\,\\equiv\\,\\mathbb{E}[y].$ , (ii) $\\begin{array}{r l}&{[x]\\,\\equiv\\,\\mathbb{D}[y],\\,\\,(i i i)\\,\\,\\mathcal{H}(P)\\,-\\,\\frac{1}{2}\\left[\\log(\\mathbb{E}[\\mathbb{D}[y^{j}]]\\right)^{\\!\\!\\!-\\!\\!}\\mathbb{D}[\\mathbb{E}[y^{\\hat{j}}]])-\\mathbb{E}[\\log(\\mathbb{D}[y^{j}])]\\right]\\,\\,\\le\\,\\mathcal{H}(\\dot{Q})\\,\\le\\,\\tilde{\\mathcal{H}}(P)\\,+}\\\\ &{\\mathbb{E}_{(i,j)\\sim\\prod[\\mathbf{C},\\mathbf{C}]}\\left[\\frac{(\\mathbb{E}[y^{i}]-\\mathbb{E}[y^{j}])^{2}(\\mathbb{D}[y^{i}]+\\mathbb{D}[y^{j}])}{\\mathbb{D}[y^{i}]\\mathbb{D}[y^{j}]}\\right]\\,a n d\\,(i\\nu)\\,D_{K L}[p_{m i x}||P]\\le\\mathbb{E}_{i\\sim\\mathcal{U}[1,\\dots,\\mathbf{C}]}\\mathbb{E}_{j\\sim\\mathcal{U}[1,\\dots,\\mathbf{C}]}\\frac{\\mathbb{E}[y^{j}]^{2}}{\\mathbb{D}[y^{i}]}}\\\\ &{n d\\,\\,D_{\\nu},\\lceil n,\\,.\\rceil|\\,\\mathcal{O}\\rceil=0}\\end{array}$ and . ", "page_idx": 5}, {"type": "text", "text": "We further analyze the properties of distributions $P$ and $Q$ as in Form $^{(I)}$ and Form (2). According to parts (i) and (ii) of Theorem 3.2, $Q$ retains the same variance and mean as $P$ . Regarding diversity, part (iii) of Theorem 3.2 states that the entropy $\\mathcal{H}(\\cdot)$ of $P$ and $Q$ is equivalent, $\\mathbf{\\bar{\\mathcal{H}}}(P)\\;\\equiv\\;\\mathcal{H}(Q)$ , provided the mean and variance of all components in the GMM are uniform, suggesting a single Gaussian proflie. Absent this condition, there is no guarantee that $\\mathcal{H}(P)$ and $\\mathcal{H}(Q)$ will consistently increase or decrease. These findings underscore the advantages of using GMM, especially when the initial data conforms to a unimodal distribution, thus aligning the mean, variance, and entropy of distributions $P$ and $Q$ in the reduced dataset. Moreover, even in diverse scenarios, the mean, variance, and entropy of $Q$ tend to remain stable. Furthermore, when the original dataset exhibits a more complex bimodal distribution and the parameters of the Gaussian components are precisely estimated, utilizing GMM can effectively reduce the Kullback-Leibler divergence between the mixed original distribution $p_{\\mathrm{mix}}$ and $Q$ to near zero. In contrast, the divergence $D_{\\mathrm{KL}}[p_{\\mathrm{mix}}||P]$ always maintains a non-zero upper bound, as noted in part (iv) of Theorem 3.2. Therefore, by modulating the weight $\\alpha$ in Eq. 5, we can derive an optimally balanced solution that minimizes loss in data characteristics while maximizing fidelity between the synthesized and original distributions. ", "page_idx": 5}, {"type": "text", "text": "Flatness Regularization ( $\\mathbf{\\widehat{\\Pi}}(\\widehat{\\mathbf{O}})$ and EMA-based Evaluation $\\left(\\circledast\\right)$ . Choices $\\@$ and $\\circledast$ are utilized to ensure flat loss landscapes during the stages of data synthesis and post-evaluation, respectively. ", "page_idx": 5}, {"type": "text", "text": "During the data synthesis phase, the use of sharpness-aware minimization (SAM) algorithms is beneficial for reducing the sharpness of the loss landscape, as presented in prior research (Foret et al., 2020; Du et al., 2022; Bahri et al., 2021). Nonetheless, traditional SAM approaches, as detailed in Eq. 29 in the Appendix, generally double the computational load due to their two-stage parameter update process. This increase in computational demand is often impractical during data synthesis. Inspired by MESA (Du et al., 2022), which achieves sharpness-aware training without additional computational overhead through self-distillation, we introduce a lightweight flatness regularization approach for implementing SAM during data synthesis. This method utilizes a teacher dataset, $\\chi_{\\mathrm{EMA}}^{S}$ , maintained via exponential moving average (EMA). The newly formulated optimization goal aims to foster a flat loss landscape in the following manner: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{FR}}=\\mathbb{E}_{\\mathcal{L}_{\\mathrm{Syn}}\\sim\\mathbb{S}_{\\mathrm{mach}}}[\\mathcal{L}_{\\mathrm{syn}}(\\mathcal{X}^{S},\\mathcal{X}_{\\mathrm{EMA}}^{S})],\\;\\mathcal{X}_{\\mathrm{EMA}}^{S}=\\beta\\mathcal{X}_{\\mathrm{EMA}}^{S}+(1-\\beta)\\mathcal{X}^{S},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta$ is the weighting coefficient, which is empirically set to 0.99 in our experiments. The detail derivation of Eq. 7 is in Appendix E. And the critical theoretical result is articulated as follows: ", "page_idx": 5}, {"type": "text", "text": "Tmihneiomriezamt i3o.n3 .w i(tphrion oaf $\\rho$ - bAapllp feonrd iexa $E$ h)  pTohien to apltiomnigz aa tsitorna iogbhjte cptaitvhe $\\mathcal{L}_{F R}$ eceann $\\mathcal{X}^{S}$ uarned $\\chi_{E M A}^{S^{\\,^{\\prime}}}$ n.ess-aware ", "page_idx": 5}, {"type": "text", "text": "This indicates that the primary optimization goal of $\\mathcal{L}_{\\mathbf{FR}}$ deviates somewhat from that of traditional SAM-based algorithms, which are designed to achieve a flat loss landscape around $\\mathcal{X}^{s}$ . The constraint on flatness needs to ensure that the first-order term of the Taylor expansion equals zero, indicating normal model convergence. However, our exploratory experiments found that despite the good performance of EDC, the loss of statistical matching at the end of data synthesis still fluctuated significantly and did not reach zero. As a result, we choose to apply flatness regularization exclusively to the logits of the observer model, since the cross-entropy loss for these can more straightforwardly reach zero. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{FR}}^{\\prime}=D_{\\mathrm{KL}}(\\mathrm{softmax}(\\phi(\\mathcal{X}^{S})/\\tau)||\\mathrm{softmax}(\\phi(\\mathcal{X}_{\\mathrm{EMA}}^{S})/\\tau)),\\;\\mathcal{X}_{\\mathrm{EMA}}^{S}=\\beta\\mathcal{X}_{\\mathrm{EMA}}^{S}+(1-\\beta)\\mathcal{X}^{S},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "az1SLLsmdR/tmp/c418df6251d7803bbbb79d3cf34499b9d116c99ed97872aed4e6d524b4eb766a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "az1SLLsmdR/tmp/56116a8720c3f9699d6b085b5a5b830cbe4738634700655f58b1ce4faaaf40ab.jpg", "table_caption": ["Table 1: Comparison with the SOTA baseline dataset condensation methods. $\\mathrm{SRe^{2}L}$ and RDED utilize ResNet-18 for data synthesis, whereas G-VBSM and EDC leverage various backbones for this purpose. ", "Table 2: Cross-architecture generalization comparison with different IPCs on ImageNet-1k. RDED refers to the latest SOTA method on ImageNet-1k and $+\\Delta$ stands for the improvement for each architecture. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "where softmax $(\\cdot)$ , $\\tau$ and $\\phi$ represent the softmax operator, the temperature coefficient and the pretrained observer model, respectively. As illustrated in Fig. 2 (c) top, it is evident that $\\mathcal{L}_{\\mathbf{FR}}^{\\prime}$ significantly lowers the Frobenius norm of the Hessian matrix relative to standard training, thus confirming its efficacy in pushing a flatter loss landscape. ", "page_idx": 6}, {"type": "text", "text": "In post-evaluation, we observe that a method analogous to $\\mathcal{L}_{\\mathbf{FR}}^{\\prime}$ employing SAM does not lead to appreciable performance improvements. This result is likely due to the limited sample size of the condensed dataset, which hinders the model\u2019s ability to fully converge post-training, thereby undermining the advantages of flatness regularization. Conversely, the integration of an EMA-updated model as the validated model markedly stabilizes performance variations during evaluations. We term this strategy EMA-based evaluation and apply it across all benchmark experiments. ", "page_idx": 6}, {"type": "text", "text": "Smoothing Learning Rate (LR) Schedule $\\left(\\circledast\\right)$ and Smaller Batch Size $\\left(\\mathbb{O}\\left(\\widehat{\\mathbf{o}}\\right)\\right)$ . Here, we introduce two effective strategies for post-evaluation analysis. Initially, it is crucial to distinguish between standard and conventional deep model training and post-evaluation in the context of dataset condensation. Specifically, (1) the limited number of samples in $\\mathcal{X}^{S}$ results in fewer training iterations per epoch, typically leading to underfitting; and (2) the gradient of a random batch from $\\mathcal{X}^{S}$ aligns more closely with the global gradient than that from a random batch in ${\\boldsymbol{\\mathcal{X}}}^{\\mathcal{T}}$ . To support the latter observation, we utilize a ResNet-18 model with randomly initialized parameters to calculate the gradient of a random batch and assess the cosine similarity with the global gradient of $\\chi^{\\mathcal{T}}$ . After conducting over 100 iterations of this procedure, the average cosine similarity is consistently higher between $\\bar{\\chi}^{s}$ and the global gradient than with $\\chi^{\\tau}$ , indicating a greater similarity and reduced sensitivity to batch size fluctuations. Our findings further illustrate that the gradient from a random batch in $\\dot{\\boldsymbol{\\chi}}^{s}$ effectively approximates the global gradient, as shown in Fig. 2 (c) bottom. Given this, the inaccurate gradient direction problem introduced by the small batch size becomes less problematic. Instead, using a small batch size effectively increases the number of iterations, thereby helps prevent model under-convergence. ", "page_idx": 6}, {"type": "text", "text": "To optimize the training with condensed samples, we implement a smoothed LR schedule that moderates the learning rate reduction throughout the training duration. This approach helps avoid early convergence to suboptimal minima, thereby enhancing the model\u2019s generalization capabilities. The mathematical formulation of this schedule is given by $\\begin{array}{r}{\\mu(i)=\\frac{1+\\cos(i\\pi/\\zeta N)}{2}}\\end{array}$ , where $i$ represents the current epoch, $N$ is the total number of epochs, $\\mu(i)$ is the learning rate for the $i$ -th epoch, and $\\zeta$ is the deceleration factor. Notably, a $\\zeta$ value of 1 corresponds to a typical cosine learning rate schedule, whereas setting $\\zeta$ to 2 improves performance metrics from $34.4\\%$ to $38.7\\%$ and effectively moderates loss landscape sharpness during post-evaluation. ", "page_idx": 6}, {"type": "table", "img_path": "az1SLLsmdR/tmp/5dc2b014418cad230a2e9ccf24ca03a882025daf1795667a2e31bfbdbbb67e7c.jpg", "table_caption": [], "table_footnote": ["Table 3: Ablation studies on ImageNet-1k with IPC 10. Left: Explore the influence of the slowdown coefficient $\\zeta$ with CONFIG C. Right: Evaluate the effectiveness of real image initialization $\\left(\\circledast\\right)$ , smoothing LR schedule $\\left(\\circledcirc\\right)$ and smaller batch size $(\\oplus(\\oplus)$ with $\\zeta=2$ . "], "page_idx": 7}, {"type": "table", "img_path": "az1SLLsmdR/tmp/e3fbe7079e473a9563af6838cb829ec5fc9e25a87d03ae7b3532ba88853bd77b.jpg", "table_caption": [], "table_footnote": ["Table 4: Ablation studies on ImageNet-1k with IPC 10. Investigate the potential effects of several factors, including loss type, loss weight, $\\beta$ , and $\\tau$ , amid flatness regularization $(\\widehat{\\pmb{\\uptheta}})$ . "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Weak Augmentation $\\left(\\pmb{\\mathscr{O}}\\right)$ and Better Backbone Choice $\\left(\\circledcirc\\right)$ . The principal role of these two design decisions is to address the flawed settings in the baseline G-VBSM. The key finding reveals that the minimum area threshold for cropping during data synthesis was too restrictive, thereby diminishing the quality of the condensed dataset. To rectify this, we implement mild augmentations to increase this minimum cropping threshold, thereby improving the dataset condensation\u2019s ability to generalize. Additionally, we substitute the computationally demanding EfficientNet-B0 with more streamlined AlexNet for generating soft labels on ImageNet-1k, a change we refer to as an improved backbone selection. This modification maintains the performance without degradation. More details on the ablation studies for mild augmentation and improved backbone selection are in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To validate the effectiveness of our proposed EDC, we conduct comparative experiments across various datasets, including ImageNet-1k (Russakovsky et al., 2015), ImageNet-10 (Kim et al., 2022), Tiny-ImageNet (Tavanaei, 2020), CIFAR-100 (Krizhevsky et al., 2009), and CIFAR-10 (Krizhevsky et al., 2009). Additionally, we explore cross-architecture generalization and ablation studies on ImageNet-1k. All experiments are conducted using $4\\times$ RTX 4090 GPUs. Due to space constraints, detailed descriptions of the hyperparameter settings, additional ablation studies, and visualizations of synthesized images are provided in the Appendix A.1, G, and H, respectively. ", "page_idx": 7}, {"type": "text", "text": "Network Architectures. Following prior dataset condensation work (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023; Sun et al., 2024), our comparison uses ResNet-{18, 50, 101} (He et al., 2016a) as our verified models. We also extend our evaluation to include MobileNet-V2 (Sandler et al., 2018) in Table 1 and explore cross-architecture generalization further with recently advanced backbones such as DeiT-Tiny (Touvron et al., 2021) and Swin-Tiny (Liu et al., 2021) (detailed in Table 2). ", "page_idx": 7}, {"type": "text", "text": "Baselines. We compare our work with several recent state-of-the-art methods, including $\\mathrm{SRe^{2}L}$ (Yin et al., 2023), G-VBSM (Shao et al., 2023), and RDED (Sun et al., 2024) to assess broader practical impacts. It is important to note that we have omitted several traditional methods (Cazenavette et al., 2022; Liu et al., 2023a; Cui et al., 2023) from our analysis. This exclusion is due to their inadequate performance on the large-scale ImageNet-1k and their lesser effectiveness when applied to practical networks such as ResNet, MobileNet-V2, and Swin-Tiny (Liu et al., 2021). For instance, the MTT method (Cazenavette et al., 2022) encounters an out-of-memory issue on ImageNet-1k, and ResNet18 achieves only a $46.4\\%$ accuracy on CIFAR-10 with IPC 10, which is significantly lower than the $79.1\\%$ accuracy reported for our EDC in Table 1. ", "page_idx": 7}, {"type": "table", "img_path": "az1SLLsmdR/tmp/6cd7aa7ad49679fec6ebaba48313438f9bd21df4735cdfe47a96fc7187654ff4.jpg", "table_caption": [], "table_footnote": ["Table 5: Ablation studies on ImageNet-1k with IPC 10. Evaluate the effectiveness of several design choices, including soft category-aware matching $\\left(\\circledcirc\\right)$ , weak augmentation $\\left(\\pmb{\\mathscr{G}}\\right)$ ) and EMA-based evaluation $\\left(\\circledast\\right)$ . "], "page_idx": 8}, {"type": "text", "text": "4.1 Main Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experimental Comparison. Our integral EDC, represented as $\\mathbb{C}\\mathbb{O}\\mathbb{N}\\mathbb{F}\\mathbb{I}\\mathbb{G}$ G in Fig. 1, provides a versatile solution that outperforms other approaches across various dataset sizes. The results in Table 1 affirm its ability to consistently deliver substantial performance gains across different IPCs, datasets, and model architectures. Particularly notable is the performance leap in the highly compressed IPC 1 scenario using ResNet-18, where EDC markedly outperforms the latest state-of-the-art method, RDED. Performance rises from $22.9\\%$ , $11.0\\%$ , $7.0\\%$ , $24.9\\%$ , and $6.6\\%$ to $32.6\\%$ , $39.7\\%$ , $39.2\\%$ , $45.2\\%$ , and $12.8\\%$ for CIFAR-10, CIFAR-100, Tiny-ImageNet, ImageNet-10, and ImageNet-1k, respectively. These improvements clearly highlight EDC\u2019s superior information encapsulation and enhanced generalization capability, attributed to the efficiently synthesized condensed dataset. ", "page_idx": 8}, {"type": "text", "text": "Cross-Architecture Generalization. To verify the generalization ability of our condensed datasets, it is essential to assess their performance across various architectures such as ResNet-{18, 50, 101} (He et al., 2016a), MobileNet-V2 (Sandler et al., 2018), EfficientNet-B0 (Tan and Le, 2019), DeiTTiny (Touvron et al., 2021), Swin-Tiny (Liu et al., 2021), ConvNext-Tiny (Liu et al., 2022) and ShuffleNet-V2 (Zhang et al., 2018). The results of these evaluations are presented in Table 2. During cross-validation that includes all IPCs and the mentioned architectures, our EDC consistently achieves higher accuracy than RDED, demonstrating its strong generalization capabilities. Specifically, EDC surpasses RDED by significant margins of $8.2\\%$ and $14.42\\%$ on DeiT-Tiny and ShuffleNet-V2, respectively. ", "page_idx": 8}, {"type": "text", "text": "Application. Our condensed dataset not only serves as a versatile training resource but also enhances the adaptability of models across various downstream tasks. We demonstrate its effectiveness by employing it in scenarios such as data-free network slimming (Liu et al., 2017) (w.r.t., parameter pruning (Srinivas and Babu, 2015)) and class-incremental continual learning (Prabhu et al., 2020) outlined in DM (Zhao and Bilen, 2023). Fig. 4 shows the wide applicability ", "page_idx": 8}, {"type": "image", "img_path": "az1SLLsmdR/tmp/2ef54d72da6a37a5a87afd6d370e34f8770631fce5107f418c9be8c27f9b210e.jpg", "img_caption": ["Figure 4: Application on ImageNet-1k. We evaluate the effectiveness of data-free network slimming and continual learning using VGG11-BN and ResNet-18, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "network slimming and class-incremental continual learning. It substantially outperforms $\\mathrm{SRe^{2}L}$ and G-VBSM, achieving significantly better results. ", "page_idx": 8}, {"type": "text", "text": "4.2 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Real Image Initialization $\\left(\\circledast\\right)$ , Smoothing LR Schedule $(\\widehat{\\pmb{\\mathscr{Q}}})$ and Smaller Batch Size $\\left(\\mathbb{O}\\left(\\widehat{\\mathbf{o}}\\right)\\right)$ . As shown in Table 3 (left), these design choices, with zero additional computational cost, sufficiently enhance the performance of both G-VBSM and RDED. Furthermore, we investigate the influence of $\\zeta$ within smoothing LR schedule in Table 3 (right), concluding that a smoothing learning rate decay is worthwhile for the condensed dataset\u2019s generalization ability and the optimal $\\zeta$ is model-dependent. ", "page_idx": 8}, {"type": "text", "text": "Flatness Regularization $\\left(\\widehat{\\pmb{\\uptheta}}\\right)$ . The results in Table 4 demonstrate the effectiveness of flatness regularization, while requiring a well-designed setup. Specifically, attempting to minimize sharpness across all statistics (i.e., $\\mathcal{L}_{\\mathbf{FR}})$ ) proves ineffective, instead, it is more effective to apply this regularization exclusively to the logit (i.e., $\\mathcal{L}_{\\mathbf{F}\\mathbf{R}}^{\\prime})$ . Setting the loss weights $\\beta$ and $\\tau$ at 0.25, 0.99, and 4, respectively, yields the best accuracy of $39.5\\%$ , $44.1\\%$ , and $45.9\\%$ for ResNet-18, ResNet-50, and DenseNet-121. Moreover, our design of $\\mathcal{L}_{\\mathbf{FR}}^{\\prime}$ surpasses the performance of the vanilla SAM, while requiring only half the computational resources. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Soft Category-Aware Matching $(\\widehat{\\pmb{\\mathscr{Q}}})$ , Weak Augmentation $\\left(\\circledcirc\\right)$ and EMA-based Evaluation ( $\\circledcirc$ ). Table 5 illustrates the effectiveness of weak augmentation and EMA-based evaluation, with EMA evaluation also playing a crucial role in minimizing performance fluctuations during assessment. The evaluation of soft category-aware matching primarily involves exploring the effect of parameter $\\alpha$ across the range [0, 1]. The results in Table 5 suggest that setting $\\alpha$ to 0.5 yields the best results based on our empirical analysis. This finding not only confirms the utility of soft category-aware matching but also emphasizes the importance of ensuring that the condensed dataset maintains a high level of information density and bears a distributional resemblance to the original dataset. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have conducted an extensive exploration and analysis of the design possibilities for scalable dataset condensation techniques. This comprehensive investigation helped us pinpoint a variety of effective and flexible design options, ultimately leading to the construction of a novel framework, which we call EDC. We have extensively examined EDC across five different datasets, which vary in size and number of classes, effectively proving EDC\u2019s robustness and scalability. Our results suggest that previous dataset distillation methods have not yet reached their full potential, largely due to suboptimal design decisions. We aim for our findings to motivate further research into developing algorithms capable of efficiently managing datasets of diverse sizes, thus advancing the field of dataset condensation task. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "K. He, X. Zhang, and S. Ren, \u201cDeep residual learning for image recognition,\u201d in Computer Vision and Pattern Recognition. Las Vegas, NV, USA: IEEE, Jun. 2016, pp. 770\u2013778. 1, 8, 9   \nK. He, X. Zhang, S. Ren, and J. Sun, \u201cIdentity mappings in deep residual networks,\u201d in European Conference on Computer Vision. Amsterdam, North Holland, The Netherlands: Springer, Oct. 2016, pp. 630\u2013645. 1   \nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage models are few-shot learners,\u201d Advances in neural information processing systems, vol. 33, pp. 1877\u20131901, 2020. 1   \nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in International Conference on Learning Representations. Event Virtual: OpenReview.net, May 2020. 1   \nS. Shao, Z. Shen, L. Gong, H. Chen, and X. Dai, \u201cPrecise knowledge transfer via flow matching,\u201d arXiv preprint arXiv:2402.02012, 2024. 1   \nW. Masarczyk and I. Tautkute, \u201cReducing catastrophic forgetting with learning on synthetic data,\u201d in Computer Vision and Pattern Recognition Workshops. Virtual Event: IEEE, Jun. 2020, pp. 252\u2013253. 1   \nM. Sangermano, A. Carta, A. Cossu, and D. Bacciu, \u201cSample condensation in online continual learning,\u201d in International Joint Conference on Neural Networks. Padua, Italy: IEEE, Jul. 2022, pp. 1\u20138. 1   \nB. Zhao and H. Bilen, \u201cDataset condensation with differentiable siamese augmentation,\u201d in International Conference on Machine Learning, M. Meila and T. Zhang, Eds., vol. 139. Virtual Event: PMLR, 2021, pp. 12 674\u201312 685. 1   \nF. P. Such, A. Rawal, J. Lehman, K. O. Stanley, and J. Clune, \u201cGenerative teaching networks: Accelerating neural architecture search by learning to generate synthetic training data,\u201d in International Conference on Machine Learning, vol. 119. Virtual Event: PMLR, Jul. 2020, pp. 9206\u20139216. 1   \nB. Zhao and H. Bilen, \u201cDataset condensation with distribution matching,\u201d in Winter Conference on Applications of Computer Vision. Waikoloa, Hawaii: IEEE, Jan. 2023, pp. 6514\u20136523. 1, 2, 9, 19   \nB. Zhao, K. R. Mopuri, and H. Bilen, \u201cDataset condensation with gradient matching,\u201d in International Conference on Learning Representations. Virtual Event: OpenReview.net, May 2021. 1, 2, 19 ", "page_idx": 9}, {"type": "text", "text": "Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, \u201cLearning efficient convolutional networks through network slimming,\u201d in International Conference on Computer Vision. IEEE, 2017, pp. 2736\u20132744. 1, 9 ", "page_idx": 10}, {"type": "text", "text": "G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J. Zhu, \u201cDataset distillation by matching training trajectories,\u201d in Computer Vision and Pattern Recognition. New Orleans, LA, USA: IEEE, Jun. 2022. 1, 2, 3, 8, 19, 25 ", "page_idx": 10}, {"type": "text", "text": "A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis, \u201cDatadam: Efficient dataset distillation with attention matching,\u201d in International Conference on Computer Vision. Paris, France: IEEE, Oct. 2023, pp. 17 097\u201317 107. 1, 33 ", "page_idx": 10}, {"type": "text", "text": "Y. Liu, J. Gu, K. Wang, Z. Zhu, W. Jiang, and Y. You, \u201cDREAM: efficient dataset distillation by representative matching,\u201d arXiv preprint arXiv:2302.14416, 2023. 1, 8 ", "page_idx": 10}, {"type": "text", "text": "O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., \u201cImagenet large scale visual recognition challenge,\u201d International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, 2015. 1, 8 ", "page_idx": 10}, {"type": "text", "text": "Z. Yin, E. P. Xing, and Z. Shen, \u201cSqueeze, recover and relabel: Dataset condensation at imagenet scale from A new perspective,\u201d in Neural Information Processing Systems. NeurIPS, 2023. 1, 3, 4, 5, 8 ", "page_idx": 10}, {"type": "text", "text": "Z. Yin and Z. Shen, \u201cDataset distillation in large data era,\u201d 2024. [Online]. Available: https: //openreview.net/forum?id=kpEz4Bxs6e 1, 3, 4, 5, 8, 22, 32 ", "page_idx": 10}, {"type": "text", "text": "S. Shao, Z. Yin, X. Zhang, and Z. Shen, \u201cGeneralized large-scale data condensation via various backbone and statistical matching,\u201d arXiv preprint arXiv:2311.17950, 2023. 1, 2, 3, 4, 5, 8, 15, 19, 22 ", "page_idx": 10}, {"type": "text", "text": "A. Krizhevsky, G. Hinton et al., \u201cLearning multiple layers of features from tiny images,\u201d 2009. 2, 3, 8 ", "page_idx": 10}, {"type": "text", "text": "P. Sun, B. Shi, D. Yu, and T. Lin, \u201cOn the diversity and realism of distilled dataset: An efficient dataset distillation paradigm,\u201d in Computer Vision and Pattern Recognition. IEEE, 2024. 2, 3, 4, 8, 15 ", "page_idx": 10}, {"type": "text", "text": "T. Wang, J.-Y. Zhu, A. Torralba, and A. A. Efros, \u201cDataset distillation,\u201d arXiv preprint arXiv:1811.10959, 2018. 2 ", "page_idx": 10}, {"type": "text", "text": "J. Cui, R. Wang, S. Si, and C. Hsieh, \u201cScaling up dataset distillation to imagenet-1k with constant memory,\u201d in International Conference on Machine Learning, vol. 202. Honolulu, Hawaii, USA: PMLR, 2023, pp. 6565\u20136590. 2, 8 ", "page_idx": 10}, {"type": "text", "text": "K. Wang, B. Zhao, X. Peng, Z. Zhu, S. Yang, S. Wang, G. Huang, H. Bilen, X. Wang, and Y. You, \u201cCafe: Learning to condense dataset by aligning features,\u201d in Computer Vision and Pattern Recognition. New Orleans, LA, USA: IEEE, Jun. 2022, pp. 12 196\u201312 205. 3 ", "page_idx": 10}, {"type": "text", "text": "T. Nguyen, Z. Chen, and J. Lee, \u201cDataset meta-learning from kernel ridge-regression,\u201d arXiv preprint arXiv:2011.00050, 2020. 3, 25 ", "page_idx": 10}, {"type": "text", "text": "J. Kim, J. Kim, S. J. Oh, S. Yun, H. Song, J. Jeong, J. Ha, and H. O. Song, \u201cDataset condensation via efficient synthetic-data parameterization,\u201d in International Conference on Machine Learning, vol. 162. Baltimore, Maryland, USA: PMLR, Jul. 2022, pp. 11 102\u201311 118. 3, 8 ", "page_idx": 10}, {"type": "text", "text": "D. Zhou, K. Wang, J. Gu, X. Peng, D. Lian, Y. Zhang, Y. You, and J. Feng, \u201cDataset quantization,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 17 205\u201317 216. 3, 15 ", "page_idx": 10}, {"type": "text", "text": "H. Zhang, S. Li, F. Lin, W. Wang, Z. Qian, and S. Ge, \u201cDance: Dual-view distribution alignment for dataset condensation,\u201d arXiv preprint arXiv:2406.01063, 2024. 3, 33 ", "page_idx": 10}, {"type": "text", "text": "H. Zhang, S. Li, P. Wang, D. Zeng, and S. Ge, \u201cM3d: Dataset condensation by minimizing maximum mean discrepancy,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 8, 2024, pp. 9314\u20139322. 3, 33, 34 ", "page_idx": 10}, {"type": "text", "text": "W. Deng, W. Li, T. Ding, L. Wang, H. Zhang, K. Huang, J. Huo, and Y. Gao, \u201cExploiting inter-sample and inter-feature relations in dataset distillation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 17 057\u201317 066. 3, 33, 34 ", "page_idx": 10}, {"type": "text", "text": "G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d 2015. [Online]. Available: https://arxiv.org/abs/1503.02531 3 ", "page_idx": 10}, {"type": "text", "text": "J. Gou, B. Yu, S. J. Maybank, and D. Tao, \u201cKnowledge distillation: A survey,\u201d International Journal of Computer Vision, vol. 129, no. 6, pp. 1789\u20131819, 2021. 3 ", "page_idx": 10}, {"type": "text", "text": "H. Chen, Y. Zhang, Y. Dong, and J. Zhu, \u201cRethinking model ensemble in transfer-based adversarial attacks,\u201d in International Conference on Learning Representations. Vienna, Austria: OpenReview.net, May 2024. 4, 19, 20   \nP. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, \u201cSharpness-aware minimization for efficiently improving generalization,\u201d in International Conference on Learning Representations, 2020. 4, 6, 20, 21   \nH. Chen, S. Shao, Z. Wang, Z. Shang, J. Chen, X. Ji, and X. Wu, \u201cBootstrap generalization ability from loss landscape perspective,\u201d in European Conference on Computer Vision. Springer, 2022, pp. 500\u2013517. 4, 20, 24   \nH. Liu, T. Xing, L. Li, V. Dalal, J. He, and H. Wang, \u201cDataset distillation via the wasserstein metric,\u201d arXiv preprint arXiv:2311.18531, 2023. 4   \nJ. Du, D. Zhou, J. Feng, V. Tan, and J. T. Zhou, \u201cSharpness-aware training for free,\u201d in Advances in Neural Information Processing Systems, vol. 35. New Orleans, Louisiana, USA: NeurIPS, Dec. 2022, pp. 23 439\u2013 23 451. 6, 20, 21, 22   \nD. Bahri, H. Mobahi, and Y. Tay, \u201cSharpness-aware minimization improves language model generalization,\u201d arXiv preprint arXiv:2110.08529, 2021. 6, 20   \nA. Tavanaei, \u201cEmbedded encoder-decoder in convolutional networks towards explainable AI,\u201d vol. abs/2007.06712, 2020. [Online]. Available: https://arxiv.org/abs/2007.06712 8   \nM. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen, \u201cMobilenetv2: Inverted residuals and linear bottlenecks,\u201d in Computer Vision and Pattern Recognition. Salt Lake City, UT, USA: IEEE, Jun. 2018, pp. 4510\u20134520. 8, 9   \nH. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou, \u201cTraining data-efficient image transformers & distillation through attention,\u201d in International Conference on Machine Learning, M. Meila and T. Zhang, Eds., vol. 139. Virtual Event: PMLR, Jul. 2021, pp. 10 347\u201310 357. 8, 9   \nZ. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in International Conference on Computer Vision, 2021, pp. 10 012\u2013 10 022. 8, 9   \nM. Tan and Q. Le, \u201cEfficientnet: Rethinking model scaling for convolutional neural networks,\u201d in International conference on machine learning. PMLR, 2019, pp. 6105\u20136114. 9   \nZ. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, \u201cA convnet for the 2020s,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 11 976\u201311 986. 9   \nX. Zhang, X. Zhou, M. Lin, and J. Sun, \u201cShufflenet: An extremely efficient convolutional neural network for mobile devices,\u201d in Computer Vision and Pattern Recognition, 2018, pp. 6848\u20136856. 9   \nS. Srinivas and R. V. Babu, \u201cData-free parameter pruning for deep neural networks,\u201d arXiv preprint arXiv:1507.06149, 2015. 9   \nA. Prabhu, P. H. S. Torr, and P. K. Dokania, \u201cGdumb: A simple approach that questions our progress in continual learning,\u201d in European Conference on Computer Vision. Springer, Jan 2020, p. 524\u2013540. 9   \nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d in Neural Information Processing Systems, Vancouver, BC, Canada, Dec. 2019. 14   \nB. Ostle et al., \u201cStatistics in research.\u201d Statistics in research., no. 2nd Ed, 1963. 16   \nM. Zhou, Z. Yin, S. Shao, and Z. Shen, \u201cSelf-supervised dataset distillation: A good compression is all you need,\u201d arXiv preprint arXiv:2404.07976, 2024. 22   \nY. Wu, J. Du, P. Liu, Y. Lin, W. Cheng, and W. Xu, \u201cDd-robustbench: An adversarial robustness benchmark for dataset distillation,\u201d arXiv preprint arXiv:2403.13322, 2024. 32   \nH. Kim, \u201cTorchattacks: A pytorch repository for adversarial attacks,\u201d arXiv preprint arXiv:2010.01950, 2020. 32 ", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Here, we complement both the hyperparameter settings and the backbone choices utilized for the comparison and ablation experiments in the main paper. ", "page_idx": 12}, {"type": "text", "text": "A.1 Hyperparameter Settings ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "az1SLLsmdR/tmp/77783619c0ebd93d590ae6aaf6119390de989289168bdbe8001005a0a888170b.jpg", "table_caption": ["(a) Data Synthesis "], "table_footnote": ["Table 6: Hyperparameter setting on ImageNet-1k "], "page_idx": 12}, {"type": "table", "img_path": "az1SLLsmdR/tmp/8fdd101bf95e3a4fcab09acdedf52c3cd905214441ae6546ff31170c497b4ebd.jpg", "table_caption": ["(b) Soft Label Generation and Post-Evaluation "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "az1SLLsmdR/tmp/937fface00bd2c6995ef7916b6bfb4b104e4969847469ea6a9aec07e308ed662.jpg", "table_caption": ["(a) Data Synthesis "], "table_footnote": ["Table 7: Hyperparameter setting on ImageNet-10. "], "page_idx": 12}, {"type": "table", "img_path": "az1SLLsmdR/tmp/4e29c8030bd7ec24e884678690de54f3fde7f9ed6096dd822cfef813050a5abc.jpg", "table_caption": ["(b) Soft Label Generation and Post-Evaluation "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "az1SLLsmdR/tmp/f77aaa2f499501f217d23c92faaba6cdeea24c4f9bb9fb0d0618901898645490.jpg", "table_caption": ["(a) Data Synthesis ", "Table 8: Hyperparameter setting on Tiny-ImageNet. "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "az1SLLsmdR/tmp/cfee2956b1c2b5162c90754369779218e708b8530e258cd7996c9214b8935a86.jpg", "table_caption": ["(b) Soft Label Generation and Post-Evaluation "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "We detail the hyperparameter settings of EDC for various datasets, including ImageNet-1k, ImageNet10, Tiny-ImageNet, CIFAR-100, and CIFAR-10, in Tables 6, 7, 8, 9, and 10, respectively. For epochs, a critical factor affecting computational cost, we utilize strategies from $\\mathrm{SRe^{2}L}$ , G-VBSM, and RDED for ImageNet-1k and follow RDED for the other datasets. In the data synthesis phase, we reduce the iteration count of hyperparameters by half compared to those used in $\\dot{\\mathrm{SRe}}^{2}\\mathrm{L}$ and G-VBSM. ", "page_idx": 12}, {"type": "text", "text": "A.2 Network Architectures on Different Datasets ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section outlines the specific configurations of the backbones employed in the data synthesis and soft label generation phases, details of which are omitted from the main paper. ", "page_idx": 12}, {"type": "table", "img_path": "az1SLLsmdR/tmp/8853bfa0077df35dbf49a5446675e66bcf7d463a04fd898913c10e0882a7646c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "az1SLLsmdR/tmp/09c4ea42bc4a2b4184840347c220c8503315459b011381b8b1de5ea75337b299.jpg", "table_caption": ["(b) Soft Label Generation and Post-Evaluation "], "table_footnote": ["Table 9: Hyperparameter setting on CIFAR-100. "], "page_idx": 13}, {"type": "table", "img_path": "az1SLLsmdR/tmp/97260d47f079bd70670783f7d873a34a2de55527ff7a1e83f28c9be5755ed860.jpg", "table_caption": ["(a) Data Synthesis ", "Table 10: Hyperparameter setting on CIFAR-10. "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "az1SLLsmdR/tmp/68f14e357ac0d833c91c6d2f47e5dc8198459ff0c1d0ad770de421a4f4feefc1.jpg", "table_caption": ["(b) Soft Label Generation and Post-Evaluation "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "ImageNet-1k. We utilize pre-trained models {ResNet-18, MobileNet-V2, ShuffleNet-V2, EfficientNet-V2, AlexNet} from torchvision (Paszke et al., 2019) as observer models in data synthesis. To reduce computational load, we exclude EfficientNet-V2 from the soft label generation process, a decision in line with our strategy of selecting more efficient backbones, a concept referred to as better backbone choice in the main paper. An extensive ablation analysis is available in Appendix G. ", "page_idx": 13}, {"type": "text", "text": "ImageNet-10. Prior to data synthesis, we train {ResNet-18, MobileNet-V2, ShuffleNet-V2, EfficientNet-V2} from scratch for 20 epochs and save their respective checkpoints. Subsequently, these pre-trained models are consistently employed for both data synthesis and soft label generation. ", "page_idx": 13}, {"type": "text", "text": "Tiny-ImageNet. We adopt the same backbone configurations as G-VBSM, specifically utilizing {ResNet-18, MobileNet-V2, ShuffleNet-V2, EfficientNet-V2} for both data synthesis and soft label generation. Each of these models has been trained on the original dataset with 50 epochs. ", "page_idx": 13}, {"type": "text", "text": "CIFAR-10&CIFAR-100. For small-scale datasets, we enhance the baseline G-VBSM model by incorporating three additional lightweight backbones. Consequently, the backbones utilized for data synthesis and soft label generation comprise {ResNet-18, ConvNet-W128, MobileNet-V2, WRN-16- 2, ShuffleNet-V2, ConvNet-D1, ConvNet-D2, ConvNet-W32}. To demonstrate the effectiveness of our approach, we conduct comparative experiments and present results in Table 11, which illustrates that G-VBSM achieves improved performance with this enhanced backbone configuration. ", "page_idx": 13}, {"type": "table", "img_path": "az1SLLsmdR/tmp/3e70088e8671758a915a3d3c9b449f0a6507e7a6769636522754b176b0e9da5a.jpg", "table_caption": [], "table_footnote": ["Table 11: Ablation studies on CIFAR-10 with IPC 10. With the remaining settings are the same as those of G-VBSM, our new backbone setting achieves better performance. "], "page_idx": 13}, {"type": "text", "text": "B Theoretical Derivations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here, we give a detailed statement of the definitions, assumptions, theorems, and corollaries relevant to this paper. ", "page_idx": 13}, {"type": "text", "text": "B.1 Random Initialization vs. Real Image Initialization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the data synthesis phase, random initialization involves using Gaussian noise, while real image initialization uses condensed images derived from training-free algorithms, such as RDED. aSnpde X S ,ll ry,e swpeec tdievneloyt.e  tFhoer  dcaotuapsleitns gi $(\\mathcal{X}_{\\bf r a n d o m}^{S},\\;\\mathcal{X}_{\\bf r e a l}^{S})$ ,n dwohmer ea $\\chi_{\\mathrm{random}}^{S}\\,\\sim\\,\\pi_{\\mathrm{random}}$ ,o $\\mathcal{X}_{\\mathbf{real}}^{S}\\sim\\ \\pi_{\\mathbf{real}}$ $\\mathcal{X}_{\\mathrm{random}}^{S}$ and satisfies $\\bar{p(\\pi_{\\mathbf{random}},\\pi_{\\mathbf{real}})}=\\bar{p(\\pi_{\\mathbf{random}})}\\bar{p(\\pi_{\\mathbf{real}})}$ , we have the mutual information (MI) between $\\pi_{\\mathbf{random}}$ and $\\pi_{\\mathbf{real}}$ is 0, a.k.a., $I(\\pi_{\\mathbf{random}},\\pi_{\\mathbf{real}})\\;=\\;0$ . By contrast, training-free algorithms (Sun et al., 2024; Zhou et al., 2023) synthesize the compressed data $\\mathcal{X}_{\\mathrm{free}}^{S}:=\\,\\breve{\\phi}(\\mathcal{X}_{\\mathrm{real}}^{S})$ via $\\mathcal{X}_{\\mathbf{real}}^{S}$ , satisfying $p(\\mathcal{X}_{\\mathbf{free}}^{S}|\\mathcal{X}_{\\mathbf{real}}^{S})>0$ . When the cost function $\\mathbb{E}[c(a-b)]\\,\\propto\\,1/I(\\mathrm{Law}(a),\\mathrm{Law}(b))$ , we have $\\mathbb{E}[c(\\mathcal{X}_{\\mathrm{real}}^{S}-\\mathcal{X}_{\\mathrm{free}}^{S})]\\leq\\mathbb{E}[c(\\mathcal{X}_{\\mathrm{real}}^{S}-\\mathcal{X}_{\\mathrm{random}}^{S})]$ ", "page_idx": 14}, {"type": "text", "text": "Proof. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[c(\\mathcal{X}_{\\mathrm{real}}^{S}-\\mathcal{X}_{\\mathrm{ree}}^{S})]=k/I(\\mathrm{Law}(\\mathcal{X}_{\\mathrm{real}}^{S}),\\mathrm{Law}(\\mathcal{X}_{\\mathrm{free}}^{S}))}&{}\\\\ {=k/\\mathrm{D}_{\\mathrm{KL}}\\big(p(\\pi_{\\mathrm{real}},\\pi_{\\mathrm{free}})\\big)||p(\\pi_{\\mathrm{real}})p(\\pi_{\\mathrm{free}})\\big)}&{}\\\\ {=k/\\big[H\\big(\\pi_{\\mathrm{real}})-H\\big(\\pi_{\\mathrm{real}}|\\pi_{\\mathrm{free}}\\big)\\big]}&{}\\\\ {\\leq k/\\big[H\\big(\\pi_{\\mathrm{real}}\\big)\\big]}&{}\\\\ {=k/[H\\big(\\pi_{\\mathrm{real}}\\big)-H\\big(\\pi_{\\mathrm{real}}|\\pi_{\\mathrm{random}}\\big)\\big]}&{}\\\\ {=k/I(\\mathrm{Law}(\\mathcal{X}_{\\mathrm{real}}^{S}),\\mathrm{Law}(\\mathcal{X}_{\\mathrm{random}}^{S}))}&{}\\\\ {=\\mathbb{E}[c(\\mathcal{X}_{\\mathrm{real}}^{S}-\\mathcal{X}_{\\mathrm{random}}^{S})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $k\\in\\mathbb{R}^{+}$ denotes a constant. And $D_{\\mathrm{KL}}(\\cdot||\\cdot)$ and $H(\\cdot)$ stand for Kullback-Leibler divergence and entropy, respectively. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "From the theoretical perspective described, it becomes evident that initializing with real images enhances MI more significantly than random initialization between the distilled and the original datasets at the start of the data synthesis phase. This improvement substantially alleviates the challenges inherent in data synthesis. Furthermore, our exploratory experiments demonstrate that the generalized matching loss (Shao et al., 2023) for real image initialization remains consistently lower compared to that of random initialization throughout the data synthesis phase. ", "page_idx": 14}, {"type": "text", "text": "B.2 Theoretical Derivations of Soft Category-Aware Matching ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition B.1. (Statistical Matching) Assume that we have $N\\emph{D}$ -dimensional random samples $\\{x_{i}\\in\\mathcal{R}^{D}\\}_{i=1}^{N}$ with an unknown distribution $p_{m i x}(x)$ , we define two forms of statistical matching for dataset distillation: ", "page_idx": 14}, {"type": "text", "text": "Form $^{(I)}$ : Estimate the mean $\\mathbb{E}[x]$ and variance $\\mathbb{D}[x]$ of samples $\\{x_{i}\\in\\mathcal{R}^{D}\\}_{i=1}^{N}$ . Then, synthesize $M$ $(M\\ll N)$ distilled samples $\\{y_{i}\\in\\mathcal{R}^{D}\\}_{i=1}^{M}$ such that the absolute differences between the variances $(|\\mathbb{D}[x]-\\mathbb{D}[y]|)$ and means $(|\\mathbb{E}[x]-\\mathbb{E}[y]|)$ of the original and distilled samples are $\\le\\epsilon$ . ", "page_idx": 14}, {"type": "text", "text": "Form (2): Consider $p_{m i x}(x)$ to be a linear combination of multiple subdistributions, expressed as $\\begin{array}{r}{p_{m i x}(x)\\,=\\,\\int_{\\bf C}p(x|c_{i})p(c_{i})d c_{i},}\\end{array}$ , where $c_{i}$ denotes a component of the original distribution. Given Assumption B.4, we can treat $p_{m i x}(x)$ as a GMM, with each component $p(x|c_{i})$ following a Gaussian distribution. For each component, estimate the mean $\\mathbb{E}[x^{j}]$ and variance $\\mathbb{D}[x^{j}]$ using $N_{j}$ samples $\\{x_{i}^{j}\\}_{i=1}^{N_{j}}$ , ensuring that $\\begin{array}{r}{\\sum_{j=1}^{\\mathbf{C}}N_{j}=N}\\end{array}$ . Subsequently, synthesize $M$ $'M\\ll N,$ ) distilled samples across all components $\\bigcup_{j=1}^{\\bar{\\mathbf{C}}}\\{y_{i}^{j}\\}_{i=1}^{M_{j}}$ , where $\\begin{array}{r}{\\sum_{j=1}^{\\mathbf{C}}M_{j}\\,=\\,M}\\end{array}$ . This process aims to ensure that for each component, the absolute differences between the variances $(|\\mathbb{D}[x^{j}]-\\mathbb{D}[y^{j}]|)$ and means $(|\\mathbb{E}[x^{j}]-\\mathbb{E}[\\bar{y^{j}}]|)$ of the original and distilled samples $\\leq\\epsilon$ . ", "page_idx": 14}, {"type": "text", "text": "Based on Definition B.1, here we provide several relevant theoretical conclusion. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. Consider a sample set $\\mathbb{S}_{}$ , where each sample $\\mathcal{X}$ within $\\mathbb{S}$ belongs to $\\mathcal{R}^{D}$ . Assume any two variables $x_{i}$ and $x_{j}$ in $\\mathbb{S}$ satisfies $p(x_{i},x_{j})=p(x_{i})p(x_{j})$ . This set $\\mathbb{S}$ comprises $c$ disjoint subsets $\\{\\mathbb{S}_{I},\\mathbb{S}_{2},\\ldots,\\mathbb{S}_{C}\\}$ , ensuring that for any $1\\leq i<j\\leq C$ , the intersection $\\mathbb{S}_{i}\\cap\\mathbb{S}_{j}=\\emptyset$ and the union $\\begin{array}{r}{\\bigcup_{k=1}^{C}\\mathbb{S}_{k}=\\mathbb{S}}\\end{array}$ . Consequently, the expected value over the variance within the subsets, denoted as $\\mathbb{E}_{\\mathbb{S}_{s u b}\\sim\\{\\mathbb{S}_{I},\\dots,\\mathbb{S}_{C}\\}}\\mathbb{D}_{\\mathcal{X}\\sim\\mathbb{S}_{s u b}}[\\mathcal{X}].$ , is smaller than or equal to the variance within the entire set, $\\mathbb{D}_{\\mathcal{X}\\sim\\mathcal{S}}[\\mathcal{X}]$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\hat{S}_{\\mathrm{sub}}\\sim\\{S_{1},\\dots,S_{c}\\}}\\mathbb{D}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]}\\\\ &{=\\mathbb{E}_{\\hat{S}_{\\mathrm{sub}}\\sim\\{S_{1},\\dots,S_{c}\\}}(\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}\\circ X]-\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]\\circ\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}])}\\\\ &{=\\mathbb{E}_{X\\sim\\hat{S}}[\\mathcal{X}\\circ X]-\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]\\circ\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]+\\mathbb{E}_{X\\sim\\hat{S}}[\\mathcal{X}]\\circ\\mathbb{E}_{X\\sim\\hat{S}}[\\mathcal{X}]}\\\\ &{\\quad-\\mathbb{E}_{\\hat{S}_{\\mathrm{sub}}\\sim\\{S_{1},\\dots,S_{c}\\}}\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]\\circ\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]}\\\\ &{=\\mathbb{D}_{X\\sim\\hat{S}}[\\mathcal{X}]-\\mathbb{E}_{\\mathrm{sub}}[\\mathcal{X}_{1},\\dots,S_{c}]\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]\\circ\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]}\\\\ &{\\quad+\\mathbb{E}_{X\\sim\\delta}[\\mathcal{X}]\\circ\\mathbb{E}_{X\\sim\\hat{S}}[\\mathcal{X}]}\\\\ &{=\\mathbb{D}_{X\\sim\\hat{S}}[\\mathcal{X}]-\\mathbb{E}_{\\hat{S}_{\\mathrm{sub}}\\sim\\{S_{1},\\dots,S_{c}\\}}\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]\\circ\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]}\\\\ &{\\quad+\\mathbb{E}_{\\hat{S}_{\\mathrm{sub}}\\sim\\{S_{1},\\dots,S_{c}\\}}\\mathbb{E}_{X\\sim\\hat{S}_{\\mathrm{sub}}}[\\mathcal{X}]\\circ\\mathbb{E}_{\\hat{\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.3. Consider a Gaussian Mixture Model (GMM) $p_{m i x}(x)$ comprising $\\mathbf{C}$ components (i.e., swuebi-ghGtas,u sdseinaont eddi satsr $\\{\\mu_{i}\\}_{i=1}^{\\mathbf{C}}$ ,. $\\{\\sigma_{i}^{2}\\}_{i=1}^{\\mathbf{C}}$ ,o amnpdo $\\{\\omega_{i}\\}_{i=1}^{\\mathbf{C}}$ ,e  rcehsapreactcitveerliyz.e Td hbey  mtheeainr $\\mathbb{E}[x]$ nasn, dv varairaiannccees, $\\mathbb{D}[x]$ of the distribution are given by $\\textstyle\\sum_{i=1}^{\\mathbf{C}}\\omega_{i}\\mu_{i}$ and $\\begin{array}{r}{\\sum_{i=1}^{\\mathbf{C}}\\omega_{i}(\\mu_{i}^{2}\\!+\\!\\sigma_{i}^{2})\\!-\\!(\\sum_{i=1}^{\\mathbf{C}}\\omega_{i}\\mu_{i})^{2}}\\end{array}$ , respectively (Ostle et al., 1963). ", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}[\\varepsilon]=\\int_{\\Phi}x\\sum_{i=1}^{K}\\sin\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}e^{-\\frac{(\\cos{\\varphi})^{2}}{2\\sigma_{i}^{2}}}}\\quad}&{}\\\\ &{=\\sum_{i=1}^{K}\\left\\langle\\int_{\\mathcal{O}}x\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}e^{-\\frac{(\\cos{\\varphi})^{2}}{2\\sigma_{i}^{2}}}\\right\\rangle}\\\\ &{=\\sum_{i=1}^{K}\\omega_{i},}\\\\ {\\mathbb{D}[\\varepsilon]=\\mathbb{E}[\\varepsilon^{2}]-\\mathbb{E}[x^{2}]}\\\\ &{=\\int_{\\Phi}x^{2}\\sum_{i=1}^{K}\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}e^{-\\frac{(\\cos{\\varphi})^{2}}{2\\sigma_{i}^{2}}}-\\mathbb{E}[\\varepsilon]^{2}}\\\\ &{=\\sum_{i=1}^{K}\\sin\\left[\\int_{\\mathcal{O}}x^{2}\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}e^{-\\frac{(\\cos{\\varphi})^{2}}{2\\sigma_{i}^{2}}}\\right]-\\mathbb{E}[\\varepsilon]^{2}}\\\\ &{=\\sum_{i=1}^{K}\\left\\langle\\int_{\\mathcal{O}}x^{2}\\sqrt{2\\pi}\\sigma_{i}\\ e^{-\\frac{(\\cos{\\varphi})^{2}}{2\\sigma_{i}^{2}}}\\right\\rangle-\\mathbb{E}[\\varepsilon]^{2}}\\\\ &{=\\sum_{i=1}^{K}[\\mu_{i}^{2}+\\sigma_{i}^{2}]-(\\sum_{i}\\mu_{i}\\mu_{i})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Assumption B.4. For any distribution $Q$ , there exists a constant $\\mathbf{C}$ enabling the approximation of $Q$ by a Gaussian Mixture Model $P$ with $\\mathbf{C}$ components. More generally, this is expressed as the existence of a C such that the distance between $P$ and $Q$ , denoted by the distance metric function $\\ell(P,Q)$ , is bounded above by an infinitesimal $\\epsilon$ . ", "page_idx": 15}, {"type": "text", "text": "Sketch Proof. The Fourier transform of a Gaussian function does not possess true zeros, indicating that such a function, $f(x)$ , along with its shifted variant, $f(x+a)$ , densely populates the function space through the Tauberian Theorem. In the context of $L^{2}$ , the space of all square-integrable functions, where Gaussian functions form a subspace denoted as $G$ , any linear functional defined on $G$ \u2014such as convolution operators\u2014can be extended to all of $L^{2}$ through the application of the Hahn-Banach Theorem. This extension underscores the completeness of Gaussian Mixture Models (GMM) within $L^{2}$ spaces. ", "page_idx": 15}, {"type": "text", "text": "Remarks. The proof presents two primary limitations: firstly, it relies solely on shift, which allows the argument to remain valid even when the variances of all components within GMM are identical (a relatively loose condition). Secondly, it imposes an additional constraint by requiring that the coefficients $\\omega_{i}>0$ and $\\textstyle\\sum_{i}\\omega_{i}=1$ in GMM. Accordingly, this study proposes, rather than empirically demonstrates, that GMM can approximate any specified distribution. ", "page_idx": 15}, {"type": "text", "text": "Theorem B.5. Given Assumption B.4 and Definition B.1, the variances and means of $x$ and $y$ , estimated through maximum likelihood, remain consistent across scenarios Form $^{(I)}$ and Form (2). ", "page_idx": 16}, {"type": "text", "text": "Proof. The maximum likelihood estimation mean $\\mathbb{E}[x]$ and variance $\\mathbb{D}[x]$ of samples $\\{x_{i}\\}_{i=1}^{N}$ within a Gaussian distribution are calculated as $\\frac{\\sum_{i=1}^{N}x_{i}}{N}$ i and iN=1(xi\u2212E[x])2, respectively. These estimations enable us to characterize the distribution\u2019s behavior across different scenarios as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F o r m\\left(I\\right)\\colon P\\!\\left(x\\right)\\sim\\mathcal{N}\\left(\\frac{\\sum_{i=1}^{N}x_{i}}{N},\\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\frac{\\sum_{i=1}^{N}x_{i}}{N}\\right)^{2}}{N}\\right).}}\\\\ {{F o r m\\left(2\\right)\\colon Q\\!\\left(y\\right)\\sim\\sum_{i}\\frac{N_{i}}{\\sum_{j=1}^{C}N_{j}}\\mathcal{N}\\left(\\frac{\\sum_{k=1}^{N_{i}}x_{k}^{i}}{N_{i}},\\frac{\\sum_{k=1}^{N_{i}}\\left(x_{k}^{i}-\\frac{\\sum_{k=1}^{N_{i}}x_{k}^{i}}{N_{i}}\\right)^{2}}{N_{i}}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Intuitively, the distilled samples $\\{y_{i}\\}_{i=1}^{M}$ will obey distributions $P(x)$ and $Q(y)$ in scenarios Form $(I)$ and Form (2), respectively. Then, the difference of the means between Form $^{(I)}$ and Form (2) can be derived as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{\\Theta}\\left[x P(x)d x-x Q(x)d x\\right]=\\frac{\\sum_{i=1}^{N}x_{i}}{N}-\\sum_{i}\\frac{N_{i}}{\\sum_{j=1}^{\\mathbf{C}}N_{j}}\\frac{\\sum_{k=1}^{N_{i}}x_{k}^{i}}{N_{i}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To further enhance the explanation on proving the consistency of the variance, the setup introduces two sample sets, $\\{x_{i}\\}_{i=1}^{N}$ and $\\textstyle\\bigcup_{j=1}^{\\mathbf{C}}\\{y_{i}^{j}\\}_{i=1}^{N_{j}}$ , each drawn from their respective distributions, $P(x)$ and $Q(y)$ . After that, we can acquire: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{D}[x]-\\mathbb{D}[y]=\\mathbb{D}[x]-\\displaystyle\\sum_{i=1}^{\\mathbf{C}}\\displaystyle\\sum_{i=1}^{N_{i}}(\\mathbb{E}[y^{j}]^{2}+\\mathbb{D}[y^{j}])+\\left(\\displaystyle\\sum_{i=1}^{\\mathbf{C}}\\displaystyle\\frac{N_{i}}{\\sum_{j}N_{j}}\\mathbb{E}[y^{j}]\\right)^{2}}&{\\quad\\mathrm{\\#~}L e m m a~B.3}\\\\ {=\\mathbb{D}[x]-\\mathbb{E}[\\mathbb{E}[y^{j}]^{2}]-\\mathbb{E}[\\mathbb{D}[y^{j}]]+\\mathbb{E}[\\mathbb{E}[y^{j}]]^{2}}&{}\\\\ {=(\\mathbb{D}[x]-\\mathbb{E}[\\mathbb{D}[y^{j}]])-\\mathbb{E}[\\mathbb{E}[y^{j}]^{2}]+\\mathbb{E}[\\mathbb{E}[y^{j}]]^{2}}&{}\\\\ {=\\mathbb{D}[\\mathbb{E}[y^{j}]]-\\mathbb{E}[\\mathbb{E}[y^{j}]^{2}]+\\mathbb{E}[\\mathbb{E}[y^{j}]]^{2}}&{\\quad\\mathrm{\\#~}L e m m a~B.2}\\\\ {=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Corollary B.6. The mean and variance obtained from maximum likelihood for any split form $\\{c_{1},c_{2},\\ldots,c_{\\bf C}\\}$ in Form (2) remain consistent. ", "page_idx": 16}, {"type": "text", "text": "Sketch Proof. According to Theorem B.5 the mean and variance obtained from maximum likelihood for each split form in Form (2) remain consistent within Form $(I)$ , so that any split form $\\{c_{1},c_{2},\\ldots,c_{\\bf C}\\}$ in Form (2) remain consistent. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.7. Based on Definition B.1, the entropy\u2014pertaining to diversity\u2014of the distributions characterized as $\\mathcal{H}(P)$ from Form $(I)$ and $\\mathcal{H}(Q)$ from Form (2), which are estimated through maximum likelihood, exhibits the subsequent relationship: $\\begin{array}{r l r}{\\mathcal{H}(P)\\;-\\;\\frac{1}{2}\\left[\\log(\\mathbb{E}[\\mathbb{D}[y^{j}]]+\\mathbb{D}[\\mathbb{E}[y^{j}]])-\\mathbb{E}[\\log(\\mathbb{D}[y^{j}])]\\right]}&{{}\\le}&{\\mathcal{H}(Q)\\;\\;\\stackrel{\\cdot}{\\;\\le}\\;\\;\\;\\mathcal{H}(P)\\;\\;+\\;\\mathcal{H}(P)\\;\\;\\mathcal{H}(P)\\;\\;\\le\\;\\;\\mathcal{H}(P)\\;\\;\\mathcal{H}(P)\\;.}\\end{array}$ $\\begin{array}{r}{\\frac14\\mathbb{E}_{(i,j)\\sim\\prod[\\mathbf{C},\\mathbf{C}]}\\left[\\frac{(\\mathbb{E}[y^{i}]-\\mathbb{E}[y^{j}])^{2}(\\mathbb{D}[y^{i}]+\\mathbb{D}[y^{j}])}{\\mathbb{D}[y^{i}]\\mathbb{D}[y^{j}]}\\right]}\\end{array}$ . The two-sided equality (i.e., $\\begin{array}{r l r}{\\mathcal{H}(P)}&{{}\\equiv}&{\\mathcal{H}(Q))}\\end{array}$ holds if and only if both the variance and the mean of each component are consistent. ", "page_idx": 16}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "text", "text": "#Lower bound: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}^{1}\\{-\\log(P(x))-\\mathbb{E}_{t}-\\log(Q(y))\\}}\\\\ &{=\\int_{-\\infty}^{\\infty}\\log(P(x))P(x)\\alpha+\\int_{\\infty}^{\\infty}\\log(P(y))P(y)\\,\\mathrm{d}y}\\\\ &{=\\frac{1}{2}\\log(2\\pi|y|)+\\frac{1}{2}+\\int_{\\infty}^{\\infty}\\log(\\int_{P}y)^{\\frac{1}{\\sqrt{2}}}\\frac{e^{-\\frac{|x|^{2}y|^{2}}{2}}}{\\sqrt{2\\pi|y|}}\\,\\mathrm{d}z\\int_{P}\\frac{(x)^{\\frac{1}{\\sqrt{2}}}}{\\sqrt{2\\pi|y|}}\\,\\mathrm{d}z\\int_{P}\\frac{(x-\\frac{1}{2}\\log(P))^{2}}{\\sqrt{2\\pi|y|}}\\,\\mathrm{d}y}\\\\ &{=\\frac{1}{2}\\log(2\\pi|y|)+\\frac{1}{2}+\\int_{\\infty}^{\\infty}\\log(\\frac{1}{2}\\sqrt{\\frac{1}{\\sqrt{2\\pi|y|}}}\\,\\mathrm{e}^{-\\frac{(x-\\frac{1}{2}\\log(P))^{2}}{2}})\\,\\mathrm{d}z\\,\\sqrt{\\frac{1}{2}\\exp\\frac{e^{-\\frac{|x|^{2}y|^{2}}{2}}}{\\sqrt{2\\pi|y|}}}\\,\\mathrm{d}y}\\\\ &{\\geq\\frac{1}{2}\\log(2\\pi|y|)+\\frac{1}{2}+\\int_{\\infty}^{\\infty}\\log(\\frac{1}{2}\\sqrt{2\\pi|y|})^{\\frac{1}{\\sqrt{2}}}\\frac{(x-\\log(\\frac{1}{2}\\log(P))^{2})}{\\sqrt{2\\pi|y|}}\\,\\mathrm{e}^{-\\frac{(x-\\frac{1}{2}\\log(P))^{2}}{2}}\\,\\mathrm{d}y}\\\\ &{=\\frac{1}{2}\\log(2\\pi|z|)+\\frac{1}{2}+\\mathbb{C}_{(\\delta)+\\delta-1}\\log\\left(\\int_{\\log(\\frac{1}{2}\\pi|y|)}^{\\infty}\\mathrm{e}^{-\\frac{(x-\\log(\\frac{1}{2}\\log(P)))^{2}}{2\\sqrt{\\pi|y|}}}\\,\\mathrm{e}^{-\\frac{(x-\\log(\\frac{1}{2}\\log(P)))^{2}}{ \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "#Upper bound: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[-\\log(P(x))]-\\mathbb{E}[-\\log(Q(y))]}\\\\ &{=\\int_{\\Theta}-\\log(P(x))P(x)d x+\\int_{\\Theta}\\log(P(y))P(y)d y}\\\\ &{=\\int_{\\Theta}-\\log(P(x))P(x)d x+\\int_{\\Theta}\\log(\\mathbb{E}[\\frac{1}{\\sqrt{2\\pi\\mathbb{D}\\left[y\\right]^{\\rho}}}e^{\\frac{(y-\\mathbb{E}[y^{j}])^{2}}{-2\\mathbb{D}\\left[y^{j}\\right]}}])\\mathbb{E}[\\frac{1}{\\sqrt{2\\pi\\mathbb{D}\\left[y^{j}\\right]}}e^{\\frac{(y-\\mathbb{E}[y^{j}])^{2}}{-2\\mathbb{D}\\left[y^{j}\\right]}}]d y}\\\\ &{\\leq\\int_{\\Theta}-\\log(P(x))P(x)d x+\\mathbb{E}[\\int_{\\Theta}\\log(\\frac{1}{\\sqrt{2\\pi\\mathbb{D}\\left[y^{j}\\right]}}e^{\\frac{(y-\\mathbb{E}[y^{j}])^{2}}{-2\\mathbb{D}\\left[y^{j}\\right]}})\\frac{1}{\\sqrt{2\\pi\\mathbb{D}\\left[y^{j}\\right]}}e^{\\frac{(y-\\mathbb{E}[y^{j}])^{2}}{-2\\mathbb{D}\\left[y^{j}\\right]}}d y]}\\\\ &{=\\frac{1}{2}\\log(2\\pi\\mathbb{D}[x])-\\mathbb{E}[\\frac{1}{2}\\log(2\\pi\\mathbb{D}[y^{j}])]}\\\\ &{=\\frac{1}{2}\\left[\\log(\\mathbb{E}[\\mathbb{D}\\{y^{j}\\}]+\\mathbb{D}[\\mathbb{E}[y^{j}]])-\\mathbb{E}[\\log(\\mathbb{D}[y^{j}])]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem B.8. Based on Definition B.1, if the original distribution is $p_{m i x}$ , the Kullback-Leibler divergence DKL[pmix||Q] has a upper bound Ei\u223cU[1,...,C]Ej\u223cU[1,...,C]ED[[yyi]] and $D_{K L}[p_{m i x}||P]=0$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\ak}\\left[\\boldsymbol{Q}\\vert\\vert\\boldsymbol{P}\\right]}\\\\ &{=D_{\\mathrm{{kL}}}\\left[\\sum_{i}\\frac{N_{i}}{\\sum_{j=1}^{\\mathbf{C}}N_{j}}\\boldsymbol{N}\\left(\\frac{\\sum_{k=1}^{N_{i}}x_{k}^{i}}{N_{i}},\\frac{\\sum_{k=1}^{N_{i}}\\left(x_{k}^{i}-\\frac{\\sum_{k=1}^{N_{i}}x_{k}^{i}}{N_{i}}\\right)^{2}}{N_{i}}\\right)\\Bigg\\Vert\\boldsymbol{N}\\left(\\frac{\\sum_{i=1}^{N}x_{i}}{N},\\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\frac{\\sum_{k=1}^{N}x_{i}}{N}\\right)^{2}}{N}\\right)}\\\\ &{\\leq\\sum_{i}\\frac{N_{i}}{\\sum_{j=1}^{\\mathbf{C}}N_{j}}D_{\\mathrm{{kL}}}\\left[\\boldsymbol{N}\\left(\\frac{\\sum_{k=1}^{N_{i}}x_{k}^{i}}{N_{i}},\\frac{\\sum_{k=1}^{N_{i}}\\left(x_{k}^{i}-\\frac{\\sum_{k=1}^{N_{i}}x_{k}^{i}}{N_{i}}\\right)^{2}}{N_{i}}\\right)\\Bigg\\Vert\\boldsymbol{N}\\left(\\frac{\\sum_{i=1}^{N}x_{i}}{N},\\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\frac{\\sum_{k=1}^{N}x_{i}}{N}\\right)^{2}}{N}\\right)\\Bigg]\\Bigg\\Vert\\boldsymbol{N}\\left(\\frac{\\sum_{i=1}^{N}x_{i}}{N},\\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\frac{\\sum_{k=1}^{N}x_{i}}{N}\\right)^{2}}{N}\\right)}\\\\ &{\\leq\\sum_{i}\\frac{\\sum_{j=1}^{\\mathbf{C}}N_{j}}{\\sum_{j=1}^{\\mathbf{C}}N_{j}}D_{\\mathrm{{kL}}}\\left[\\boldsymbol{N}\\left(\\frac{\\sum_{k=1}^{N_{i}}x_{k}^{i}}{N_{i}},\\frac{\\sum_{k=1}^{N_{i}}\\left(x_{k}^{i}-\\frac{\\sum_{k=1}^{N_{i}}x_{k}^{i}}{N_{i}}\\right)^{2}} \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By applying the notations from Lemma B.3 for convenience, we obtain: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log_{k}[Q|P]}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{\\infty}\\omega_{i}\\left[\\frac{1}{2}\\log\\left(\\frac{\\sum_{j=1}^{\\bf C}\\omega_{j}[\\mu_{j}^{2}+\\sigma_{j}^{2}]-(\\sum_{j=1}^{\\bf C}\\omega_{j}[\\mu_{j}])^{2}}{\\sigma_{i}^{2}}\\right)+\\frac{\\sum_{j=1}^{\\bf C}\\omega_{j}[\\mu_{j}^{2}+\\sigma_{j}^{2}]-(\\sum_{j=1}^{\\bf C}\\omega_{j}\\mu_{j})^{2}}{2\\sigma_{i}^{2}}\\right]-\\frac{1}{2}}\\\\ &{\\leq\\displaystyle\\frac{1}{2}\\log\\left(\\sum_{i}\\omega_{i}\\frac{\\sum_{j=1}^{\\bf C}\\omega_{j}[\\mu_{j}^{2}+\\sigma_{j}^{2}]-(\\sum_{j=1}^{\\bf C}\\omega_{j}\\mu_{j})^{2}}{\\sigma_{i}^{2}}\\right)+\\frac{1}{2}\\sum_{i}\\omega_{i}\\frac{\\sum_{j=1}^{\\bf C}\\omega_{j}[\\mu_{j}^{2}+\\sigma_{j}^{2}]-(\\sum_{j=1}^{\\bf C}\\omega_{j}\\mu_{j})^{2}}{\\sigma_{i}^{2}}-}\\\\ &{\\leq\\displaystyle\\frac{1}{2}\\log\\left(1+\\sum_{i}\\omega_{i}\\frac{\\sum_{j=1}^{\\bf C}\\omega_{j}[\\mu_{j}^{2}-(\\sum_{j=1}^{\\bf C}\\omega_{j}\\mu_{j})^{2}}{\\sigma_{i}^{2}}\\right)+\\frac{1}{2}\\sum_{i}\\omega_{i}\\frac{\\sum_{j=1}^{\\bf C}\\omega_{j}[\\mu_{j}^{2}-(\\sum_{j=1}^{\\bf C}\\omega_{j}\\mu_{j})^{2}}{\\sigma_{i}^{2}}-}\\\\ &{\\leq\\frac{1}{2}\\log\\left(1+\\sum_{i}\\sum_{j}\\omega_{i}\\omega_{j}\\frac{\\mu_{j}^{2}}{\\sigma_{i}^{2}}\\right)+\\frac{1}{2}\\sum_{i}\\sum_{j}\\omega_{i}\\omega_{j}\\frac{\\mu_{j}^{2}}{\\sigma_{i}^{2}}}\\\\ &{\\leq\\mathbb{E}_{i\\geq\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When the sample size is sufficiently large, the original distribution aligns with $Q$ . Consequently, we obtain DKL[pmix||P] \u2264Ei\u223cU[1,...,C]Ej\u223cU[1,...,C]ED[[yyi]] and establish that $D_{\\mathrm{KL}}[p_{\\mathrm{mix}}||Q]=0$ . ", "page_idx": 18}, {"type": "text", "text": "C Decoupled Optimization Objective of Dataset Condensation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we demonstrate that the training objective, as defined in Eq. 2, can be decoupled into two components\u2014flatness and closeness\u2014using a second-order Taylor expansion, under the assumption that $\\mathcal{L}_{\\mathrm{syn}}\\in\\mathbf{C}^{2}(\\mathbf{I},\\mathbb{R})$ . We define the closest optimization point $\\mathbf{o}_{i}$ for $\\mathcal{X}^{s}$ in relation to the $i$ -th matching operator $\\mathcal{L}_{\\mathrm{syn}}^{i}(\\cdot,\\cdot)$ . This framework can accommodate all matchings related to $f^{i}(\\cdot)$ , including gradient matching(Zhao et al., 2021), trajectory matching (Cazenavette et al., 2022), distribution matching (Zhao and Bilen, 2023), and statistical matching (Shao et al., 2023). Consequently, we derive the dual decoupling of flatness and closeness as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\dot{\\boldsymbol{\\cdot}}}\\mathbf{p}_{\\mathrm{m}}=\\mathbb{E}_{\\boldsymbol{Z}_{\\le\\mathbf{p}_{\\mathrm{m}}(\\cdot,\\cdot)}\\sim\\mathsf{S}_{\\mathrm{mach}}}[\\mathcal{L}_{\\mathrm{spn}}(\\boldsymbol{\\chi}^{S},\\boldsymbol{\\chi}^{T})]=\\frac{1}{|\\mathrm{S}_{\\mathrm{mach}}|}\\sum_{i=1}^{|\\mathcal{S}_{\\mathrm{mach}}|}[\\mathcal{L}_{\\mathrm{spn}}^{i}(\\boldsymbol{\\chi}^{S},\\boldsymbol{\\chi}^{T})]}\\\\ &{\\quad=\\frac{1}{|\\mathrm{S}_{\\mathrm{mach}}|}\\sum_{i=1}^{|\\mathcal{S}_{\\mathrm{mach}}|}[\\mathcal{L}_{\\mathrm{spn}}^{i}(\\mathbf{0}_{i},\\boldsymbol{\\chi}^{T})+(\\boldsymbol{\\chi}^{S}-\\mathbf{0}_{i})\\nabla_{\\boldsymbol{\\chi}^{S}}\\mathcal{L}_{\\mathrm{spn}}^{i}(\\mathbf{0}_{i},\\boldsymbol{\\chi}^{T})+(\\boldsymbol{\\chi}^{S}-\\mathbf{0}_{i})^{T}\\mathrm{H}^{i}(\\boldsymbol{\\chi}^{S}-\\mathbf{0}_{i})]+\\mathcal{O}((\\boldsymbol{\\chi}^{S}-\\mathbf{0}_{i})^{T})}\\\\ &{\\quad=\\frac{1}{|\\mathrm{S}_{\\mathrm{mach}}|}\\sum_{i=1}^{|\\mathcal{S}_{\\mathrm{mach}}|}[\\mathcal{L}_{\\mathrm{spn}}^{i}(\\mathbf{0}_{i},\\boldsymbol{\\chi}^{T})+(\\boldsymbol{\\chi}^{S}-\\mathbf{0}_{i})^{T}\\mathrm{H}^{i}(\\boldsymbol{\\chi}^{S}-\\mathbf{0}_{i})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathrm{H}^{i}$ refers to the Hessian matrix of $\\mathcal{L}_{\\mathrm{syn}}^{i}(\\cdot,\\mathcal{X}^{\\tau})$ at the closest optimization point $\\mathbf{o}_{i}$ . Note that as the optimization method for deep learning typically involves gradient descent-like approaches (e.g., SGD and AdamW), the first-order derivative $\\dot{\\nabla_{\\mathcal{X}^{s}}}\\mathcal{L}_{\\mathbf{syn}}^{i}(\\mathbf{o}_{i},\\boldsymbol{\\chi}^{\\tau})$ can be directly discarded. After that, scanning the two terms in Eq. 16, the first one necessarily reaches an optimal solution, while the second one allows us to obtain an upper definitive bound on the Hessian matrix and Jacobi matrix through Theorem 3.1 outlined in Chen et al. (2024). Here, we give a special case under the $\\ell_{2}$ -norm to discard the assumption that $\\mathrm{H}^{i}$ and $(\\mathcal{X}^{s}-\\mathbf{o}_{i})$ are independent: ", "page_idx": 18}, {"type": "text", "text": "Theorem C.1. (improved from Theorem 3.1 in (Chen et al., 2024)) $\\begin{array}{r}{\\frac{1}{|\\mathbb{S}_{m a t c h}|}\\sum_{i=1}^{|\\mathbb{S}_{m a t c h}|}(\\chi^{S}\\ -}\\end{array}$ $\\begin{array}{r}{\\mathbf{o}_{i})^{T}{\\mathrm{H}}^{i}(\\mathcal{X}^{S}\\mathrm{~-~}\\mathbf{o}_{i})\\,\\leq\\,|\\mathbb{S}_{m a t c h}|\\,\\cdot\\,\\mathbb{E}[||\\mathrm{H}^{i}||_{\\mathrm{F}}]\\mathbb{E}[||\\mathcal{X}^{S}\\mathrm{~-~}\\mathbf{o}_{i}||_{2}^{2}],}\\end{array}$ , where $\\mathbb{E}[||\\mathrm{H}^{i}||_{\\mathrm{F}}]$ and $\\mathbb{E}[||\\boldsymbol{\\mathcal{X}}^{s}\\,-\\,\\mathbf{o}_{i}||_{2}^{2}]$ denote flatness and closeness, respectively. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\frac{1}{|\\mathrm{S}_{\\mathrm{matel}}|}\\displaystyle\\sum_{i=1}^{|\\mathrm{S}_{\\mathrm{matel}}|}(\\boldsymbol{X}^{\\mathcal{S}}-\\mathbf{o}_{i})^{T}\\mathrm{H}^{i}(\\boldsymbol{X}^{\\mathcal{S}}-\\mathbf{o}_{i})\\leq\\frac{1}{|\\mathrm{S}_{\\mathrm{matel}}|}\\displaystyle\\sum_{i=1}^{|\\mathrm{S}_{\\mathrm{matel}}|}[\\|(\\boldsymbol{X}^{\\mathcal{S}}-\\mathbf{o}_{i})\\|_{2}|\\|\\boldsymbol{\\mathrm{H}}^{i}(\\boldsymbol{X}^{\\mathcal{S}}-\\mathbf{o}_{i})\\|_{2}]}&&{\\mathrm{\\#~H\\deltalder^{\\mathcal{S}}~}}\\\\ &{=\\frac{1}{|\\mathrm{S}_{\\mathrm{matel}}|}\\displaystyle\\sum_{i=1}^{|\\mathrm{S}_{\\mathrm{matel}}|}[\\|(\\boldsymbol{X}^{\\mathcal{S}}-\\mathbf{o}_{i})\\|_{2}|\\|\\boldsymbol{\\mathrm{H}}^{i}\\|_{2,2}|](\\boldsymbol{X}^{\\mathcal{S}}-\\mathbf{o}_{i})\\|_{2}]}&&{\\mathrm{\\#~Definition~of~matrix~norm}}\\\\ &{\\leq|\\mathrm{S}_{\\mathrm{matel}}|\\cdot\\mathbb{E}[\\|\\mathbf{H}^{i}\\|_{2,2}]\\mathbb{E}[\\|\\boldsymbol{X}^{\\mathcal{S}}-\\mathbf{o}_{i}|]_{2}^{2}|\\leq|\\mathrm{S}_{\\mathrm{matel}}|\\cdot\\mathbb{E}[\\|\\mathbf{H}^{i}|\\vert\\boldsymbol{\\mathrm{H}}^{\\mathcal{S}}[\\vert\\boldsymbol{\\mathrm{H}}^{\\mathcal{S}}-\\mathbf{o}_{i}]\\|_{2}^{2}]}&&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Actually, flatness can be ensured by convergence in a flat region through sharpness-aware minimization (SAM) theory (Foret et al., 2020; Bahri et al., 2021; Du et al., 2022; Chen et al., 2022). Specifically, a body of work on SAM has established a connection between the Hessian matrix and the flatness of the loss landscape (i.e., the curvature of the loss trajectory), with a series of empirical studies demonstrating the theory\u2019s reliability. Meanwhile, the specific implementation of flatness is elaborated upon in Sec. E. By contrast, the concept of closeness was first introduced in Chen et al. (2024), where it is observed that utilizing more backbones for ensemble can result in a smaller generalization error during the evaluation phase. In fact, closeness has been implicitly implemented since our baseline G-VBSM uses a sequence optimization mechanism akin to the official implementation in Chen et al. (2024). Therefore, this paper will not elucidate on closeness and its specific implementation. ", "page_idx": 19}, {"type": "text", "text": "D Traditional Sharpness-Aware Minimization Optimization Approach ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For the comprehensive of our paper, let us give a brief yet formal description of sharpness-aware minimization (SAM). The applicable SAM algorithm was first proposed in Foret et al. (2020), which aims to solve the following maximum minimization problem: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\operatorname*{max}_{\\epsilon:||\\epsilon||\\leq\\rho}L_{\\mathbb{S}}(f_{\\theta+\\epsilon}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $L_{\\mathbb{S}}(f_{\\theta}),\\epsilon,\\rho,$ , and $\\theta$ refer to the loss $\\begin{array}{r}{\\frac{1}{|\\mathbb{S}|}\\sum_{x_{i},y_{i}\\sim\\mathbb{S}}\\ell(f_{\\theta}(x_{i}),y_{i})}\\end{array}$ , the perturbation, the pre-defined flattened region, and the model parameter, respectively. Let us define the final optimized model parameters as $\\theta^{*}$ , then the optimization objective can be rewritten as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}R_{\\mathrm{S}}(f_{\\theta})+L_{\\mathrm{S}}(f_{\\theta}),\\mathrm{~where~}R_{\\mathrm{S}}(f_{\\theta})=\\operatorname*{max}_{\\epsilon:||\\epsilon||\\le\\rho}L_{\\mathrm{S}}(f_{\\theta+\\epsilon})-L_{\\mathrm{S}}(f_{\\theta}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By expanding $L_{\\mathbb{S}}(f_{\\theta+\\epsilon})$ at $\\theta$ and by solving the classical dual norm problem, the first maximization objective can be solved as (In the special case of the $\\ell_{2}$ -norm) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\epsilon^{*}=\\underset{\\epsilon:\\,|\\,\\epsilon|\\,\\leq\\rho}{\\arg\\operatorname*{max}}\\,L_{\\mathbb{S}}(f_{\\theta+\\epsilon})\\approx\\rho\\frac{\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})}{||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The specific derivation is as follows: ", "page_idx": 19}, {"type": "text", "text": "Proof. Subjecting $L_{\\mathbb{S}}(f_{\\theta+\\epsilon})$ to a Taylor expansion and retaining only the first-order derivatives: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{\\mathbb{S}}(f_{\\theta})=L_{\\mathbb{S}}(f_{\\theta+\\epsilon})-L_{\\mathbb{S}}(f_{\\theta})\\approx L_{\\mathbb{S}}(f_{\\theta})+\\epsilon^{T}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})-L_{\\mathbb{S}}(f_{\\theta})=\\epsilon^{T}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we can get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\boldsymbol{\\epsilon}^{*}=\\underset{\\epsilon:||\\epsilon||\\leq\\rho}{\\arg\\operatorname*{max}}\\,L_{\\mathbb{S}}(f_{\\theta+\\epsilon})-L_{\\mathbb{S}}(f_{\\theta})=\\underset{\\epsilon:||\\epsilon||\\leq\\rho}{\\arg\\operatorname*{max}}~\\left[\\epsilon^{T}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we base our solution on the solution of the classical dual norm problem, where the above equation can be written as $\\lVert\\nabla_{\\theta}L_{\\mathrm{S}}(f_{\\theta})\\rVert_{*}$ . Firstly, H\u00f6lder\u2019s inequality gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon^{T}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})=\\displaystyle\\sum_{i=1}^{n}\\epsilon_{i}^{T}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})_{i}\\leq\\displaystyle\\sum_{i=1}^{n}|\\epsilon_{i}^{T}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})_{i}|}\\\\ &{\\leq||\\epsilon^{T}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{1}\\leq||\\epsilon^{T}||_{p}||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{q}\\leq\\rho||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So, we just need to find a $\\epsilon$ that makes all the above inequality signs equal. Define $m$ as ${\\mathrm{sign}}(\\nabla_{\\theta}{\\bar{L}}_{\\mathbb{S}}(f_{\\theta}))|\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})|^{q-1}$ , then we can rewritten Eq. 23 as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\epsilon^{T}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})=\\displaystyle\\sum_{i=1}^{n}\\mathrm{sign}(\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})_{i})|\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})_{i}|^{q-1}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})_{i}}}\\\\ {{=\\displaystyle\\sum_{i=1}^{n}|\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})_{i}||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})_{i}|^{q-1}}}\\\\ {{=||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{q}^{q}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "And we also get ", "page_idx": 20}, {"type": "equation", "text": "$$\n||\\epsilon||_{p}^{p}=\\sum_{i=1}^{n}|\\epsilon|^{p}=\\sum_{i=1}^{n}|\\mathrm{sign}(\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta}))|\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})|^{q-1}|^{p}=||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{q}^{q},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $1/p+1/q=1$ . We choose a new $\\epsilon$ , defined as $y=\\rho\\frac{\\epsilon}{||\\boldsymbol{\\epsilon}||_{p}}$ , which satisfies: $||y||_{p}=\\rho$ , and substitute into $\\epsilon^{T}\\nabla_{\\theta}L_{\\mathrm{S}}(f_{\\theta})$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\ny^{T}\\nabla_{\\theta}L_{5}(f_{\\theta})=\\sum_{i=1}^{n}y_{i}\\nabla_{\\theta}L_{5}(f_{\\theta})_{i}=\\sum_{i=1}^{n}{\\frac{\\rho\\nabla_{\\theta}L_{5}(f_{\\theta})_{i}}{||\\nabla_{\\theta}L_{5}(f_{\\theta})||_{p}}}\\nabla_{\\theta}L_{5}(f_{\\theta})_{i}={\\frac{\\rho}{||\\epsilon||_{p}}}\\sum_{i=1}^{n}\\epsilon_{i}\\nabla_{\\theta}L_{5}(f_{\\theta})_{i}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Due to $||\\epsilon||_{p}\\,=\\,||\\nabla_{\\theta}L_{\\mathrm{{S}}}(f_{\\theta})_{i}||_{q}^{q/p}$ and $\\epsilon^{T}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})\\,=\\,||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{q}^{q}$ , we can further derive and obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\rho}{||\\epsilon||_{p}}\\sum_{i=1}^{n}\\epsilon_{i}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})_{i}=\\frac{\\rho}{||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{q}^{q/p}}\\sum_{i=1}^{n}\\epsilon_{i}\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})_{i}=\\rho||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{q}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, $y$ can be rewritten as: ", "page_idx": 20}, {"type": "equation", "text": "$$\ny=\\rho\\frac{\\mathrm{sign}(\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta}))|\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})|^{q-1}}{||\\mathrm{sign}(\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta}))|\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})|^{q-1}||_{p}}=\\rho\\frac{\\mathrm{sign}(\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta}))|\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})|^{q-1}}{||\\nabla_{\\theta}L_{\\mathbb{S}}(f_{\\theta})||_{q}^{q-1}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The above derivation is partly derived from Foret et al. (2020), to which we have added another part. To solve the SAM problem in deep learning (Foret et al., 2020), had to require two iterations to complete a single SAM-based gradient update. Another pivotal aspect to note is that within the context of dataset condensation, $\\theta$ transitions from representing the model parameter $f_{\\theta}$ to denoting the synthesized dataset $\\mathcal{X}^{s}$ . ", "page_idx": 20}, {"type": "text", "text": "E Implementation of Flatness Regularization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As proved in Sec. D, the optimal solution $\\epsilon^{*}$ is denoted as $\\rho\\frac{\\nabla_{\\theta}L_{\\mathrm{8}}(f_{\\theta})}{||\\nabla_{\\theta}L_{\\mathrm{8}}(f_{\\theta})||_{2}}$ . Analogously, in the dataset condensation scenario, the joint optimization objective is given by $\\begin{array}{r}{\\sum_{i=1}^{|\\mathbb{S}_{\\mathrm{match}}|}[\\mathcal{L}_{\\mathrm{syn}}^{i}(\\mathcal{X}^{S},\\mathcal{X}^{T})]}\\end{array}$ . There exists an optimal $\\epsilon^{*}$ , which can be written as $\\rho\\frac{\\nabla_{\\boldsymbol{x}}\\boldsymbol{s}\\sum_{i=1}^{|\\mathbb{S}_{\\mathrm{match}}|}[\\mathcal{L}_{\\mathrm{syn}}^{i}(\\boldsymbol{\\mathcal{X}}^{\\mathcal{S}},\\boldsymbol{\\mathcal{X}}^{\\mathcal{T}})]}{||\\nabla_{\\boldsymbol{x}}\\boldsymbol{s}\\sum_{i=1}^{|\\mathbb{S}_{\\mathrm{match}}|}[\\mathcal{L}_{\\mathrm{syn}}^{i}(\\boldsymbol{\\mathcal{X}}^{\\mathcal{S}},\\boldsymbol{\\mathcal{X}}^{\\mathcal{T}})]||_{2}}$ ||iS=m1atch||[Lsiyn(X S,X T )] . Thus, a dual-stage approach of flatness regularization is shown below: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi_{\\mathrm{new}}^{S}\\leftarrow\\chi^{S}+\\frac{\\rho}{\\vert\\vert\\nabla_{\\boldsymbol{x}^{S}}\\sum_{i=1}^{\\vert\\mathbb{S}_{\\mathrm{math}}\\vert}\\vert\\mathcal{L}_{\\mathrm{spn}}^{i}(\\mathcal{X}^{S},\\mathcal{X}^{T})\\vert\\vert_{2}}\\left(\\nabla_{\\boldsymbol{x}^{S}}\\sum_{i=1}^{\\vert\\mathbb{S}_{\\mathrm{math}}\\vert}[\\mathcal{L}_{\\mathrm{spn}}^{i}(\\mathcal{X}^{S},\\mathcal{X}^{T})]\\right)}\\\\ &{\\chi_{\\mathrm{new}}^{S}\\leftarrow\\chi_{\\mathrm{new}}^{S}-\\eta\\left(\\nabla_{\\boldsymbol{x}_{\\mathrm{new}}^{S}}\\sum_{i=1}^{\\vert\\mathbb{S}_{\\mathrm{math}}\\vert}[\\mathcal{L}_{\\mathrm{spn}}^{i}(\\chi_{\\mathrm{new}}^{S},\\mathcal{X}^{T})]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\eta$ and $\\mathcal{X}_{\\mathbf{next}}^{S}$ denote the learning rate and the synthesized dataset in the next iteration, respectively. However, this optimization approach significantly increases the computational burden, thus reducing its scalability. Enlightened by Du et al. (2022), we consider a single-stage optimization strategy implemented via exponential moving average (EMA). Given an EMA-updated synthesized dataset X ESM $\\dot{\\mathcal{X}}_{\\mathrm{EMA}}^{S}=\\beta\\mathcal{X}_{\\mathrm{EMA}}^{S}\\dot{+}\\,(1-\\beta)\\mathcal{X}^{S}$ , where $\\beta$ is typically set to 0.99 in our experiments. The trajectories of the synthesized datasets updated via gradient descent (GD) and EMA can be represented as $\\{\\theta_{\\mathbf{GD}}^{0},\\theta_{\\mathbf{GD}}^{1},\\cdot\\cdot\\cdot,\\theta_{\\mathbf{GD}}^{N}\\}$ and $\\{\\theta_{\\mathbf{EMA}}^{0},\\theta_{\\mathbf{EMA}}^{1},\\cdot\\cdot\\cdot,\\theta_{\\mathbf{EMA}}^{N}\\}$ , respectively. Assume that $\\begin{array}{r}{\\mathbf{g}_{j}=\\nabla_{\\boldsymbol{\\chi}^{s}}\\sum_{i=1}^{|\\mathbb{S}_{\\mathrm{match}}|}[\\mathcal{L}_{\\mathbf{syn}}^{i}(\\boldsymbol{\\chi}^{s},\\boldsymbol{\\chi}^{\\tau})]}\\end{array}$ at the $j$ -th iteration, then $\\begin{array}{r}{\\theta_{\\mathbf{EMA}}^{j}=\\theta_{\\mathbf{GD}}^{j}+\\sum_{i=1}^{j-1}\\beta^{j-i}\\mathbf{g}_{i}}\\end{array}$ with the condition $1\\le j\\le N^{1}$ , as outlined in Du et al. (2022). Consequently, we can provide the EMA-based SAM algorithm and applied to backbone sequential optimization in dataset condensation as follows: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathtt{F R}}=\\sum_{i=1}^{|\\mathcal{S}_{\\mathrm{mach}}|}[{\\mathcal{L}}_{\\mathtt{s p n}}^{i}({\\mathcal{X}}^{S},{\\mathcal{X}}_{\\mathtt{E M A}}^{S})]=\\sum_{i=1}^{|\\mathcal{S}_{\\mathrm{mach}}|}[{\\mathcal{L}}_{\\mathtt{s p n}}^{i}(\\theta_{\\mathtt{G D}}^{j},\\theta_{\\mathtt{E M A}}^{j})],\\qquad{\\mathrm{at~the~}}j{\\mathrm{-th~iteration}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the vast majority of dataset distillation algorithms (Yin and Shen, 2024; Shao et al., 2023; Zhou et al., 2024), the metric function used in matching is set to mean squared error (MSE) loss. Based on this phenomenon, we can rewrite Eq. 30 to Eq. 31, which guarantees flatness. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\nabla_{\\phi_{(k)}}\\frac{1}{\\sum_{i=0}^{N}\\rho_{k}}[C_{\\phi_{(k)}}(\\theta_{k}^{*},X^{T})-C_{\\phi_{(k)}}(\\theta_{\\sin,X}^{*})^{T}]}\\\\ &{=\\nabla_{\\phi_{(k)}}\\frac{1}{\\sum_{i=0}^{N}\\rho_{k}}[C_{\\phi_{(k)}}(\\theta_{k}^{*},X^{T})-C_{\\phi_{(k)}}(\\theta_{\\sin,+}^{*}\\frac{\\sum_{i=0}^{N}\\rho_{k}}{\\sum_{i=0}^{N}\\rho_{k}})]}\\\\ &{=\\nabla_{\\phi_{(k)}}\\frac{1}{\\sum_{i=0}^{N}\\rho_{k}}[C_{\\phi_{(k)}}(\\theta_{k,i}^{*},X^{T})-C_{\\phi_{(k)}}(\\theta_{\\sin,+}^{*}\\theta^{*}-\\theta_{(k,X}^{*})^{T})+\\dots}\\\\ &{\\quad+C_{\\phi_{(k)}}(\\theta_{\\sin,+}^{*}\\sum_{i=0}^{N}\\rho_{k},X^{T})-C_{\\phi_{(k)}}(\\theta_{\\sin,+}^{*}\\sum_{i=0}^{i-1}\\theta_{\\sin,X}^{*})]}\\\\ &{=\\nabla_{\\phi_{(k)}}\\frac{1}{\\sum_{i=0}^{N}\\rho_{k}}[(\\theta^{*}-\\theta_{(k)})\\nabla_{\\phi_{(k)}}\\frac{1}{\\sum_{i=0}^{N}\\rho_{k}}(\\theta_{\\sin,X}^{*})^{T}]\\|\\theta_{+}\\|^{2}}\\\\ &{\\quad+(\\lambda^{2}\\theta_{(k)})[\\nabla_{\\phi_{(k)}}\\frac{1}{\\sum_{i=0}^{N}\\rho_{k}}(\\theta_{\\sin,}^{*}\\theta_{\\sin,}^{*})(\\theta_{\\sin,}^{*}+\\sum_{i=0}^{N}\\theta_{\\sin,X}\\theta_{\\sin,}^{*})]}\\\\ &{\\quad+(\\lambda^{2}\\theta_{(k)})[\\nabla_{\\phi_{(k)}}\\frac{1}{\\sum_{i=0}^{N}\\rho_{k}}(\\theta_{\\sin,}^{*}\\sum_{i=0}^{\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we can further obtain a SAM-like presentation. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{\\lambda^{5/5}}\\sum_{i=1}^{|\\mathcal{S}_{\\mathrm{matel}}|}[\\mathcal{L}_{\\mathrm{sp}}^{i}(\\theta_{\\mathrm{GD}}^{i},\\theta_{\\mathrm{EMA}}^{j})],}}&{\\quad\\mathrm{at~the~}j\\mathrm{-th~iteration}}\\\\ &{}&{\\quad\\times\\frac{|\\mathcal{S}_{\\mathrm{matel}}|}{\\lambda^{5}}[\\mathbb{E}_{(\\theta_{1},\\theta_{2})\\sim\\mathrm{Un}\\mathbb{I}(\\theta_{\\mathrm{GD}}^{j},\\theta_{\\mathrm{GD}}^{j}+\\beta^{j-1}\\mathbf{g}_{11},\\cdots,\\theta_{\\mathrm{GD}}^{j}+\\sum_{k=1}^{j-1}\\beta^{j-k}\\mathbf{g}_{k k})}||\\nabla_{\\theta_{1}}\\mathcal{L}_{\\mathrm{sp}}^{i}(\\theta_{1},\\mathcal{X}^{T})||_{2}||\\nabla_{\\theta_{2}}\\mathcal{L}_{\\mathrm{sp}}^{i}(\\theta_{2},\\mathcal{X}^{T})||_{2}]}\\\\ &{}&{\\quad\\times\\frac{|\\mathcal{S}_{\\mathrm{matel}}|}{\\lambda^{5}}[\\operatorname*{max}_{i\\in|\\mathcal{S}_{\\mathrm{mateD}}}\\mathbb{E}_{(\\theta\\sim\\beta\\theta_{\\mathrm{GD}}^{j}+(1-\\beta)\\theta_{\\mathrm{EM}}^{j},\\beta\\sim\\mathcal{U}[0,1])}\\mathcal{L}_{\\mathrm{sp}}^{i}(\\theta+\\epsilon,\\mathcal{X}^{T})].}\\\\ &{}&{\\quad\\times\\frac{|\\mathcal{S}_{\\mathrm{matel}}|}{\\lambda^{5}}[\\operatorname*{max}_{i\\in|\\mathcal{S}_{\\mathrm{mateD}}}\\mathbb{E}_{(\\theta\\sim\\beta\\theta_{\\mathrm{GD}}^{j}+(1-\\beta)\\theta_{\\mathrm{EM}}^{j},\\beta\\sim\\mathcal{U}[0,1])}\\mathcal{L}_{\\mathrm{sp}}^{i}(\\theta+\\epsilon,\\mathcal{X}^{T})].}\\\\ &{}&{\\quad\\times\\frac{|\\mathcal{S}_{\\mathrm{matel}}|}{\\lambda^{6}}[\\operatorname*{max}_{i\\in|\\mathcal{S}_{\\mathrm{mateD}}}\\mathbb{E}_{(\\theta\\sim\\beta\\theta_{\\mathrm{GD}}^{j}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consequently, optimizing Eq. 30 effectively addresses the SAM problem during the data synthesis phase, which results in a flat loss landscape. Additionally, Eq. 32 presents a variant of the SAM algorithm that slightly differs from the traditional form. This variant is specifically designed to ensure sharpness-aware minimization within a $\\rho$ -ball for each point along a straight path between $\\theta_{\\mathbf{GD}}^{j}$ and $\\theta_{\\mathrm{EMA}}^{j}$ ", "page_idx": 21}, {"type": "text", "text": "F Visualization of Prior Dataset Condensation Methods ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Fig. 5, we present the visualization results of previous training-dependent dataset condensation methods. These approaches, which optimize starting from Gaussian noise, tend to produce synthetic images that lack realism and fail to convey clear semantics to the naked eye. ", "page_idx": 22}, {"type": "image", "img_path": "az1SLLsmdR/tmp/6b4afc0f8a7ddfb38761501203090b8069a4a4afbb28eadb9c8e59b5e5143340.jpg", "img_caption": ["Figure 5: Visualization of the synthetic images of prior training-dependent dataset condensation methods. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "G More Ablation Experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we present a series of ablation studies to further validate the design choices outlined in the main paper. ", "page_idx": 22}, {"type": "text", "text": "G.1 Backbone Choices of Data Synthesis on ImageNet-1k ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "az1SLLsmdR/tmp/c4176959e07e321e7c4fc3b4104fba00b1f8056caf659ad6318af16b93473b8d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 12: Ablation studies on ImageNet-1k with IPC 10. Verify the influence of backbone choices on data synthesis with CONFI $\\mathbb{G}$ C $\\zeta=1.5)$ . ", "page_idx": 22}, {"type": "text", "text": "The results in Table 12 demonstrate the significant impact of backbone architecture selection on the performance of dataset distillation. This study employs the optimal configuration, which includes ResNet-18, MobileNet-V2, EfficientNet-B0, ShuffleNet-V2, and AlexNet. ", "page_idx": 22}, {"type": "table", "img_path": "az1SLLsmdR/tmp/55898f6812f25cc6f9f48235daf13871fc988f5b98cfba4a25cb3d7dc9179710.jpg", "table_caption": ["G.2 Backbone Choices of Soft Label Generation on ImageNet-1k "], "table_footnote": ["Table 13: Ablation studies on ImageNet-1k with IPC 1. Verify the influence of backbone choice on soft label generation with CONFI $\\mathbb{G}$ G $\\zeta=2,$ ). "], "page_idx": 22}, {"type": "text", "text": "Our strategy better backbone choice, which focuses on utilizing lighter backbone combinations for soft label generation, significantly enhances the generalization capabilities of the condensed dataset. Empirical studies conducted with IPC 1, and the results detailed in Table 13, show that optimal performance is achieved by using ResNet-18, MobileNet-V2, EfficientNet-B0, ShuffleNet-V2, and AlexNet for data synthesis. For soft label generation, the combination of ResNet-18, MobileNet-V2, ShuffleNet-V2, and AlexNet demonstrates most effective. ", "page_idx": 22}, {"type": "image", "img_path": "az1SLLsmdR/tmp/0f913d11b2f054d248cbfe7080acfbcb6727f0b693346c76e506056512f88924.jpg", "img_caption": ["Figure 6: The visualization of SSRS and smoothing LR schedule. ", "Table 15: Ablation studies on ImageNet-1k with IPC 10. Verify the effectiveness of ALRS in post-evaluation. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "G.3 Smoothing LR Schedule Analysis ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "az1SLLsmdR/tmp/215ed50bf100a768d04982d97f3dfba7fb30d90b839e779072530414fa864749.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "az1SLLsmdR/tmp/6452d1889bf5700f06a3b99000a1b99e54fcac8288a221533aa0ee6b3da6e722.jpg", "table_caption": ["Table 14: Ablation studies on ImageNet-1k with IPC 10. Additional experimental result of the slowdown coefficient $\\zeta$ on the verified model MobileNet-V2. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Due to space limitations in the main paper, the experimental results for MobileNet-V2, which are not included in Table 3 Left, are presented in Table 14. Additionally, we investigate Adaptive Learning Rate Scheduler (ALRS), an algorithm that adjusts the learning rate based on training loss. Although ALRS did not produce effective results, it provides valuable insights for future research. This scheduler was first introduced in (Chen et al., 2022) and is described as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mu(i)=\\mu(i-1)\\gamma^{\\frac{\\parallel\\boldsymbol{L}_{i}-\\boldsymbol{L}_{i-1}\\mid}{|\\boldsymbol{L}_{i}|}\\leq h_{1}\\mathrm{~and~}\\left|L_{i}-L_{i-1}\\right|\\leq h_{2}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, $\\gamma$ represents the decay rate, $L_{i}$ is the training loss at the $i$ -th iteration, and $h_{1}$ and $h_{2}$ are the first and second thresholds, respectively, both set by default to 0.02. We list several values of $\\gamma$ that demonstrate the best empirical performance in Table 15. These results allow us to conclude that our proposed smoothing LR schedule outperforms ALRS in the dataset condensation task. ", "page_idx": 23}, {"type": "text", "text": "Ultimately, we introduce a learning rate scheduler superior to the traditional smoothing LR schedule in scenarios with high IPC. This enhanced strategy, named early Smoothing-later Steep Learning Rate Schedule (SSRS), integrates the smoothing LR schedule with MultiStepLR. It intentionally implements a significant reduction in the learning rate during the final epochs of training to accelerate model convergence. The formal definition of SSRS is as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu(i)=\\left\\{\\frac{1+\\cos(i\\pi/\\zeta N)}{2}\\qquad\\qquad\\qquad,i\\le\\frac{5N}{6},}\\\\ {\\frac{1+\\cos(5\\pi/\\zeta6)}{2}\\frac{(6N-6i)}{6N}\\quad,i>\\frac{5N}{6}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "table", "img_path": "az1SLLsmdR/tmp/a4f12e4aed96a282e3cad711d793a5fed9b473a949d643056f3b32048e9fa6ca.jpg", "table_caption": ["Table 16: Ablation studies on ImageNet-1k with IPC 40. Verify the effectiveness of SSRS in post-evaluation. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Note that the visualization of SSRS can be found in Fig. 6. Meanwhile, the comparative experimental results of SSRS and the smoothing LR schedule are detailed in Table 16. Notably, SSRS enhances the verified model\u2019s performance without incurring additional overhead. ", "page_idx": 24}, {"type": "text", "text": "G.4 Understanding of EMA-based Evaluation ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "az1SLLsmdR/tmp/67b5e4c3577ac7abcada26a245633f68e8b6c9799296596d71ea6077634fd8ce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 17: Ablation studies on ImageNet-1k with IPC 10. Verify the effect of EMA Rate in EMA-based Evaluation. ", "page_idx": 24}, {"type": "text", "text": "The EMA Rate, a crucial hyperparameter governing the EMA update rate during post-evaluation, significantly influences the final results. Additional experimental outcomes, presented in Table 17, reveal that the EMA Rate 0.99 we adopt in the main paper yields optimal performance. ", "page_idx": 24}, {"type": "text", "text": "G.5 Ablation Studies on CIFAR-10 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section details the process of deriving hyperparameter configurations for CIFAR-10 through exploratory studies. The demonstrated superiority of our EDC method over traditional approaches, as detailed in our main paper, suggests that conventional dataset condensation techniques like MTT (Cazenavette et al., 2022) and KIP (Nguyen et al., 2020) are not the sole options for achieving superior performance on small-scale datasets. ", "page_idx": 24}, {"type": "table", "img_path": "az1SLLsmdR/tmp/0d9f58edd9ff6451e1c20758e57a876b948a14d459da0bf669e230317a6c4325.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "az1SLLsmdR/tmp/f6ddaa07131a13ece708b3998d06439e3b1c40409ab1973c5e360dbd82e80a01.jpg", "table_caption": ["Table 18: Ablation studies on CIFAR-10 with IPC 10. We employ ResNet-18 exclusively for data synthesis and soft label generation, examining the impact of iteration count during post-evaluation and adhering to RDED\u2019s consistent hyperparameter settings. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "az1SLLsmdR/tmp/8ac4dfd43e107b69b7aa465946a0b21f6466fbb2e7980313f87bb7c3de065985.jpg", "table_caption": ["Table 19: Ablation studies on CIFAR-10 with IPC 10. Hyperparameter settings follow those in Table 10, excluding the scheduler and batch size, which are set to smoothing LR schedule $\\zeta=2$ ) and 50, respectively. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Our quantitative experiments, detailed in Table 18, pinpoint 75 iterations as the empirically optimal count. This finding highlights that, for smaller datasets with limited samples and fewer categories, fewer iterations are required to achieve superior results. ", "page_idx": 24}, {"type": "table", "img_path": "az1SLLsmdR/tmp/47d7cdae3fb2f82e31fb0995154ebb01e8e29bac98bab5a0030d6655888306f7.jpg", "table_caption": [], "table_footnote": ["Table 21: Ablation studies on CIFAR-10 with IPC 10. Explore the influence of various scheduler in postevaluation. Hyperparameter settings follow those in Table 10. "], "page_idx": 25}, {"type": "text", "text": "Subsequently, we evaluate the effectiveness of using a pre-trained model on ImageNet-1k for dataset condensation on CIFAR-10. Our study differentiates two training pipelines: the first involves 100 epochs of pre-training followed by 10 epochs of fine-tuning (denoted as \u2018w/ pre-train\u2019), and the second comprises training from scratch for 10 epochs (denoted as \u2018w/o pre-train\u2019). The results, presented in Table 19, indicate that pre-training on ImageNet-1k does not significantly enhance dataset distillation performance. ", "page_idx": 25}, {"type": "text", "text": "We further explore how batch size and EMA Rate affect the generalization abilities of the condensed dataset. Results in Table 20 show that a reduced batch size of 25 enhances performance on CIFAR-10. ", "page_idx": 25}, {"type": "text", "text": "In our final set of experiments, we compare MultiStepLR and smoothing LR schedules. As detailed in Table 21, MultiStepLR is superior for ResNet-18 and ResNet-50, whereas the smoothing LR schedule is more effective for ResNet-101 and MobileNet-V2. ", "page_idx": 25}, {"type": "text", "text": "H Synthesized Image Visualization ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The visualization of the condensed dataset is showcased across Figs. 7 to 11. Specifically, Figs. 7, 9, 10, and 11 present the datasets synthesized from ImageNet-1k, Tiny-ImageNet, CIFAR-100, and CIFAR-10, respectively. ", "page_idx": 25}, {"type": "text", "text": "I Ethics Statement ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our research utilizes synthetic data to avoid the use of actual personal information, thereby addressing privacy and consent issues inherent in datasets with identifiable data. We generate synthetic data using a methodology that distills from real-world data but maintains no direct connection to individual identities. This method aligns with data protection laws and minimizes ethical risks related to confidentiality and data misuse. However, it is important to note that models trained on synthetic data may not achieve the same accuracy levels as those trained on the full original dataset. ", "page_idx": 25}, {"type": "text", "text": "J Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The paper offers an extensive examination of the design space for dataset condensation, but it might still miss some potentially valuable strategies due to the broad scope. Additionally, as the IPC count grows, the performance of the described approach converges with that of the baseline RDED. ", "page_idx": 25}, {"type": "image", "img_path": "az1SLLsmdR/tmp/b7e033b4b0dbd9ea6b4bc4f8d9277cde42081d37ed30ff6f020baed77e5c06d2.jpg", "img_caption": ["Figure 7: Synthetic data visualization on ImageNet-1k randomly selected from EDC. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "az1SLLsmdR/tmp/225b812ae0e128b5bb5a28b4671c5a676fd2073e5adb35d4431aa9f1962fc896.jpg", "img_caption": ["Figure 8: Synthetic data visualization on ImageNet-10 randomly selected from EDC. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "az1SLLsmdR/tmp/f567fad906d6355282a7b84c6811f5c4d565c2971066d38ff2d548ff15715c38.jpg", "img_caption": ["Figure 9: Synthetic data visualization on Tiny-ImageNet randomly selected from EDC. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "az1SLLsmdR/tmp/3d1f2222a70afd51a3d47c833b503ce6d33d908c597c8aaa5e0be6f6564c161a.jpg", "img_caption": ["Figure 10: Synthetic data visualization on CIFAR-100 randomly selected from EDC. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "az1SLLsmdR/tmp/1ae274167787bd21ff39eb8932bff69b0376fdfa4244103a65474adc525ad4e7.jpg", "img_caption": ["Figure 11: Synthetic data visualization on CIFAR-10 randomly selected from EDC. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "K Additional Experiments, Theories and Descriptions (Rebuttal Stage Supplement) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Here we add some experiments, theories and explanations that we think it is necessary to add. ", "page_idx": 31}, {"type": "text", "text": "K.1 Scalability on ImageNet-21k ", "text_level": 1, "page_idx": 31}, {"type": "table", "img_path": "az1SLLsmdR/tmp/6404bc5ea1038c2080265e55043bfef18425ca13cf94452e63129b4a34050b40.jpg", "table_caption": [], "table_footnote": ["Table 22: Comparison of Different Methods on ImageNet-21k. "], "page_idx": 31}, {"type": "text", "text": "We conduct experiments on a larger scale dataset ImageNet-21k-P with IPC 10. The results in Table 22 indicate that our method outperforms the state-of-the-art method CDA (Yin and Shen, 2024) on this dataset, demonstrating that EDC can scale to larger datasets. ", "page_idx": 31}, {"type": "text", "text": "K.2 Complexity of Implementation ", "text_level": 1, "page_idx": 31}, {"type": "table", "img_path": "az1SLLsmdR/tmp/33cd7820af7a989e1bce3d0968029acab1abdbd93cd235fc6eb76c5c0bb26409.jpg", "table_caption": [], "table_footnote": ["Table 23: Comparison of computational resources on 4 RTX 4090. "], "page_idx": 31}, {"type": "text", "text": "Here we present Table 23 to complement the computational overhead in Fig. 1 in the main paper. EDC is an efficient algorithm as it reduces the number of iterations by half, compared to the baseline G-VBSM. As illustrated in the table above, although transitioning from CONFIG A to CONFIG G adds small GPU memory overhead, it is minor compared to the reduction in time spent. Additionally, introducing EDC to other tasks often requires significant effort for tuning hyper-parameters or even redesigning statistical matching, which is a challenge EDC should address. ", "page_idx": 31}, {"type": "text", "text": "K.3 Robustness Evaluation ", "text_level": 1, "page_idx": 31}, {"type": "table", "img_path": "az1SLLsmdR/tmp/148bae7f43ae2478d5e16ba7a1af53cdfaa2e7318654e2858243bb216687c6fe.jpg", "table_caption": [], "table_footnote": ["Table 24: Comparison on DD-RobustBench. "], "page_idx": 31}, {"type": "text", "text": "We follow the pipeline in Wu et al. (2024) to evaluate the robustness of models trained on condensed datasets, utilizing the well-known adversarial attack library available at Kim (2020). As illustrared in Table 24. Our experiments are conducted on Tiny-ImageNet with IPC 50, with the test accuracy presented in the table above. Evidently, EDC demonstrates significantly higher robustness compared to other methods. We attribute this to improvements in post-evaluation techniques, such as EMAbased evaluation and smoothing LR schedule, which help reduce the sharpness of the loss landscape. ", "page_idx": 31}, {"type": "text", "text": "K.4 Theoretical Explanation of Irrational Hyperparameter Setting (Sketch!!) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The smoothing LR schedule is designed to address suboptimal solutions that arise due to the scarcity of sample sizes in condensed datasets. Additionally, the use of small batch size is implemented ", "page_idx": 31}, {"type": "text", "text": "because the gradient of the condensed dataset more closely resembles the global gradient of the original dataset, as illustrated at the bottom of Fig. 2. Against the latter, we can propose a complete chain of theoretical derivation: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\mathcal{L}_{s y n}=\\mathbb{E}_{c_{i}\\sim C}\\|p_{\\theta}(\\mu|X^{S},c_{i})-p(\\mu|X^{T},c_{i})\\|_{2}}\\\\ &{\\qquad\\qquad+\\|p_{\\theta}(\\sigma^{2}|X^{S},c_{i})-p(\\theta^{2}|X^{T},c_{i})\\|_{2}\\quad\\#\\mathrm{~(Our~statistical~matching)}}\\\\ &{\\qquad\\qquad\\partial\\theta=\\displaystyle\\int_{c_{i}}(\\partial L_{s y n}/\\partial p_{\\theta}(\\cdot|X^{S},c_{i}))(\\partial p_{\\theta}(\\cdot|X^{S},c_{i})/\\partial\\theta)d c_{i}}\\\\ &{\\qquad\\qquad\\approx\\displaystyle\\int_{c_{i}}([p_{\\theta}(\\mu|X^{S},c_{i})-p(\\mu|X^{T},c_{i})]+[p_{\\theta}(\\sigma^{2}|X^{S},c_{i})-p(\\sigma^{2}|X^{T},c_{i})])(\\partial p_{\\theta}(\\cdot|X^{S},c_{i})/\\partial\\theta)\\mathrm{d}\\theta}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $p_{\\theta}(|X^{S},c_{i})$ and $p(|X^{T},c_{i})$ refer to a Gaussian component in the Gaussian Mixture Model. Consider post-evaluation, We can derive the gradient of the MSE loss as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial\\mathbb{E}_{x_{i}\\sim X^{S}}\\|f_{\\theta}(x_{i})-y_{i}\\|_{2}^{2}/\\partial\\theta=2\\mathbb{E}_{x_{i}\\sim X^{S}}[(f_{\\theta}(x_{i})-y_{i})(\\partial f_{\\theta}(x_{i})/\\partial\\theta)]}\\\\ &{\\;=2\\mathbb{E}_{x_{i}\\sim X^{S}}[(f_{\\theta}(x_{i})-y_{i})\\displaystyle\\int_{c_{i}}(\\partial f_{\\theta}(x_{i})/\\partial p_{\\theta}(\\cdot|X^{S},c_{i}))(\\partial p_{\\theta}(\\cdot|X^{S},c_{i})/\\partial\\theta)d c_{i}]}\\\\ &{\\approx2\\mathbb{E}_{(x_{j},x_{i})\\sim(X^{S},X^{T})}[(f_{\\theta}(x_{j})-y_{j})\\displaystyle\\int_{c_{i}}(\\partial f_{\\theta}(x_{i})/\\partial p_{\\theta}(\\cdot|X^{T},c_{i}))(\\partial p_{\\theta}(\\cdot|X^{T},c_{i})/\\partial\\theta)d c_{i}]}\\\\ &{\\approx\\partial\\mathbb{E}_{x_{i}\\sim X^{T}}||f_{\\theta}(x_{i})-y_{i}||_{2}^{2}/\\partial\\theta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\theta$ stands for the model parameter. The right part of the penultimate row results from the loss $\\mathcal{L}_{\\mathrm{syn}}$ , which ensures the consistency of $p(\\cdot|X^{\\overline{{T}}},c_{i})$ and $p(\\cdot|\\dot{X}^{S},c_{i})$ . If the model initialization during training is the same, the left part of the penultimate row is a scalar and has little influence on the direction of the gradient. Since $\\bar{X}^{T}$ is the complete original dataset with a global gradient, the gradient of $X^{S}$ approximates the global gradient of $X^{T}$ , thus enabling the use of small batch size. ", "page_idx": 32}, {"type": "text", "text": "K.5 Additional Related Work ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We additionally discuss the differences between published related papers (Sajedi et al., 2023; Zhang et al., 2024b; Deng et al., 2024) and our work. ", "page_idx": 32}, {"type": "text", "text": "DataDAM (Sajedi et al., 2023) vs. EDC. Both DataDAM and EDC do not require model parameter updates during training. However, DataDAM struggles to generalize effectively to ImageNet-1k because it relies on randomly initialized models for distribution matching. As noted in $\\mathrm{SRe^{2}\\bar{L}}$ , models trained for fewer than 50 epochs can experience significant performance degradation. DataDAM does not explore the soft label generation and post-evaluation phases as EDC does, limiting its competitiveness. ", "page_idx": 32}, {"type": "text", "text": "DANCE (Zhang et al., 2024a) vs. EDC. DANCE is a DM-based algorithm that, unlike traditional distribution matching, does not require model updates during data synthesis. Instead, it interpolates between pre-trained and randomly initialized models, using this interpolated model for distribution matching. Similarly, EDC also does not need to update the model parameters, but it uses a pre-trained model with a different architecture and does not incorporate random interpolation. The \u201crandom interpolation\u201d technique was not adopted because it did not yield performance gains on ImageNet-1k. Although DANCE considers both intra-class and inter-class perspectives, it limits inter-class analysis to the logit level and intra-class analysis to the feature map level. In contrast, EDC performs both intra-class and inter-class matching at the feature map level, where inter-class matching is crucial. To support this, last year, $\\mathrm{SRe^{2}L}$ focused solely on inter-class matching at the feature map level and still achieved state-of-the-art performance on ImageNet-1k. EDC is the first dataset distillation algorithm to simultaneously improve data synthesis, soft label generation, and post-evaluation stages. In contrast, DANCE only addresses the data synthesis stage. While DANCE can be effectively applied to ImageNet-1k, the introduction of soft label generation and post-evaluation improvements is essential for DANCE to achieve more competitive results. ", "page_idx": 32}, {"type": "text", "text": "M3D (Zhang et al., 2024b) vs. EDC. M3D is a DM-based algorithm, but its data synthesis paradigm aligns with DataDAM by relying solely on randomly initialized models, which limits its generalization to ImageNet-1k. M3D, similar to $\\mathrm{SRe^{2}L}$ , G-VBSM, and EDC, takes into account second-order information (variance), but this is not a unique contribution of EDC. The key contributions of EDC in data synthesis are real image initialization, flatness regularization, and the consideration of both intra-class and inter-class matching. ", "page_idx": 33}, {"type": "text", "text": "Deng et al. (Deng et al., 2024) vs. EDC. Deng et al. (Deng et al., 2024) is a DM-based algorithm, but its data synthesis paradigm is consistent with M3D and DataDAM, as it considers only randomly initialized models, which cannot be generalized to ImageNet-1k. Deng et al. (Deng et al., 2024) considers both interclass and intraclass information, similar to EDC. However, while EDC obtains interclass information by traversing the entire training set, Deng et al. (Deng et al., 2024) derives interclass information from only one batch, making its information richness inferior to that of EDC. Deng et al. (Deng et al., 2024) only explores data synthesis and does not explore soft label generation or post-evaluation. Additionally, Deng et al. (Deng et al., 2024) only shares some similarity with Soft Category-Aware Matching among the 10 design choices in EDC. ", "page_idx": 33}, {"type": "text", "text": "K.6 Implementation of Cropping ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The implementation of this crop operation refers to torchvision.transforms.RandomResizedCrop, where the minimum area threshold is controlled by the parameter scale[0]. The default value is 0.08, meaning that the cropped image can be as small as $8\\%$ of the original image. Since 0.08 is too small for the model to extract complete semantic information during data synthesis, increasing the value to 0.5 resulted in a significant performance gain. ", "page_idx": 33}, {"type": "table", "img_path": "az1SLLsmdR/tmp/e364269da935a2b2f4632394f59fa3b99830ef0aec868e81f0a21226ca37fc6b.jpg", "table_caption": ["K.7 Comprehensive Comparison Experiment "], "table_footnote": ["Table 25: Comparison with the SOTA baseline dataset condensation methods. MTT, TESLA, $\\overline{{\\mathrm{SRe^{2}L}}}$ , CDA, WMDD and RDED utilize ResNet-18 for data synthesis, whereas G-VBSM and EDC leverage various backbones for this purpose. "], "page_idx": 33}, {"type": "text", "text": "Due to space constraints in the main paper and for aesthetic reasons, we have not fully presented the experimental results of other methods. However, since the benchmark for dataset distillation is uniform and well-recognized, the performance of other algorithms can be found in their respective papers. We present the related experimental results of the popular convolutional architecture ResNet18 in Table 25. ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: In the introduction and abstract, we state a comprehensive design framework for dataset condensation, incorporating specific and effective strategies supported by empirical evidence and theoretical foundations. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Please see Sec. J. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Please see Sec. B in Appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our supplemental materials contain the reproducible code. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Code has been provided in supplemental materials. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The details have been presented in Appendix A.1. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Please see Table 1. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: All experiments are conducted using $4\\times$ RTX 4090 GPUs, as detailed in the experiment section. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Please see Sec. I. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Please see Sec. I. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: There are no risk factors present here. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: In our paper and accompanying code, we have carefully cited and credited the works of G-VBSM and RDED, which form the foundation of our implementation. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have attached our code and user instructions in the supplementary materials. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper does not have any experiments or research relevant to human subjects. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]