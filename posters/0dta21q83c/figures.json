[{"figure_path": "0dtA21q83C/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the DeNetDM framework: In Stage 1, an ensemble of shallow and deep branches produces outputs linearly combined and trained as a product of experts. The cross-entropy loss with depth modulation aids in separating biases and identifying target attributes. In Stage 2, we further introduce a target branch with the desired architecture, which also requires debiasing. This phase exclusively focuses on refining the target branch's feature extractor ($t) and classifier head (ft) while leveraging knowledge from the initial stages.", "description": "This figure illustrates the DeNetDM framework, which consists of two stages. Stage 1 uses a product of experts approach with deep and shallow branches to separate bias and core attributes.  Stage 2 introduces a target branch with the desired architecture, using knowledge distillation from the previous stage to refine its feature extractor and classifier head.  The goal is to create a debiased model without relying on explicit bias annotations or data augmentation.", "section": "3 Debiasing by Network Depth Modulation"}, {"figure_path": "0dtA21q83C/figures/figures_4_1.jpg", "caption": "Figure 2: Exploring the effect of depth modulation: (a) illustrates how the linear decodability of features decreases as neural network depth increases, while (b) dives into the training dynamics of MLPs with varying depths under ERM.", "description": "This figure explores how network depth affects feature decodability.  Part (a) shows that as network depth increases, the linear decodability of both digit (core attribute) and color (bias attribute) features decreases, but the core attribute's decodability drops more steeply. Part (b) examines the training dynamics, showing that during early training, both attributes' decodability increases, but the bias attribute\u2019s improvement is more pronounced, especially in deeper networks. This suggests that deeper networks are more likely to focus on spurious correlations (bias attributes).", "section": "3.2 Effect of Depth Modulation"}, {"figure_path": "0dtA21q83C/figures/figures_4_2.jpg", "caption": "Figure 2: Exploring the effect of depth modulation: (a) illustrates how the linear decodability of features decreases as neural network depth increases, while (b) dives into the training dynamics of MLPs with varying depths under ERM.", "description": "This figure shows two graphs that explore the relationship between network depth and feature decodability. Graph (a) shows how the linear decodability of both digit (core attribute) and color (bias attribute) decreases with increasing network depth. Graph (b) displays the training dynamics of MLPs with varying depths. This graph shows how the linear decodability of the features changes during the training process under empirical risk minimization (ERM).", "section": "3.2 Effect of Depth Modulation"}, {"figure_path": "0dtA21q83C/figures/figures_8_1.jpg", "caption": "Figure 2: Exploring the effect of depth modulation: (a) illustrates how the linear decodability of features decreases as neural network depth increases, while (b) dives into the training dynamics of MLPs with varying depths under ERM.", "description": "This figure shows two graphs that explore the effect of network depth on feature decodability.  Graph (a) demonstrates how the linear decodability of both digit and color features decreases as the network depth increases in an untrained MLP. Graph (b) shows the training dynamics of MLPs with varying depths; specifically, how the linear decodability of these features changes during training under Empirical Risk Minimization (ERM).", "section": "3.2 Effect of Depth Modulation"}, {"figure_path": "0dtA21q83C/figures/figures_17_1.jpg", "caption": "Figure 2: Exploring the effect of depth modulation: (a) illustrates how the linear decodability of features decreases as neural network depth increases, while (b) dives into the training dynamics of MLPs with varying depths under ERM.", "description": "This figure shows two plots. Plot (a) shows that the linear decodability of both digit and color attributes decreases with increasing network depth in an untrained network. Plot (b) illustrates how the linear decodability of digit and color features change during training for MLPs with 3 and 5 layers, showing that the 3-layer model maintains higher decodability of digit attributes (core attributes) compared to the 5-layer model throughout training. This observation supports the claim that deeper networks prioritize bias attributes while shallower networks retain more core attributes.", "section": "3.2 Effect of Depth Modulation"}, {"figure_path": "0dtA21q83C/figures/figures_22_1.jpg", "caption": "Figure 1: Illustration of the DeNetDM framework: In Stage 1, an ensemble of shallow and deep branches produces outputs linearly combined and trained as a product of experts. The cross-entropy loss with depth modulation aids in separating biases and identifying target attributes. In Stage 2, we further introduce a target branch with the desired architecture, which also requires debiasing. This phase exclusively focuses on refining the target branch's feature extractor ($t) and classifier head (ft) while leveraging knowledge from the initial stages.", "description": "The figure illustrates the DeNetDM framework, which consists of two stages. In Stage 1, a deep and a shallow branch are trained simultaneously using a product of experts approach to separate bias and core attributes.  In Stage 2, a target branch with the desired architecture is trained using knowledge distillation from Stage 1 to refine the feature extractor and classifier, focusing solely on debiasing. ", "section": "3 Debiasing by Network Depth Modulation"}]