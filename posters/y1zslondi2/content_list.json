[{"type": "text", "text": "Soft ascent-descent as a stable and flexible alternative to flooding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matthew J. Holland\u2217 Osaka University ", "page_idx": 0}, {"type": "text", "text": "Kosuke Nakatani Osaka University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As a heuristic for improving test accuracy in classification, the \u201cflooding\u201d method proposed by Ishida et al. (2020) sets a threshold for the average surrogate loss at training time; above the threshold, gradient descent is run as usual, but below the threshold, a switch to gradient ascent is made. While setting the threshold is nontrivial and is usually done with validation data, this simple technique has proved remarkably effective in terms of accuracy. On the other hand, what if we are also interested in other metrics such as model complexity or average surrogate loss at test time? As an attempt to achieve better overall performance with less fine-tuning, we propose a softened, pointwise mechanism called SoftAD (soft ascent-descent) that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect of flooding, with no additional computational overhead. We contrast formal stationarity guarantees with those for flooding, and empirically demonstrate how SoftAD can realize classification accuracy competitive with flooding (and the more expensive alternative SAM) while enjoying a much smaller loss generalization gap and model norm. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Modern machine learning makes use of sophisticated models that are trained through optimization of non-convex objective functions, which typically admit numerous local minima that make for natural candidates when taken at face value. While many such candidates are indeed essentially \u201coptimal\u201d from the viewpoint of classification error rates or other average losses incurred at training time, these often turn out to be highly sub-optimal in terms of performance at test time. It goes without saying that understanding and closing this gap is the problem of \u201cgeneralization\u201d that underlies most machine learning research (Jiang et al., 2020; Dziugaite et al., 2020; Johnson and Zhang, 2023). ", "page_idx": 0}, {"type": "text", "text": "When we are faced with multiple candidates which are essentially optimal and thus indistinguishable in terms of some \u201cbase\u201d objective function (e.g., the average loss) at training time, one of best-known heuristics for identifying good candidates is that of the \u201clandscape\u201d or \u201cgeometry\u201d of the base objective in a neighborhood around each candidate. Roughly speaking, one expects that candidates in regions which are in some sense \u201cflat\u201d (often said to be less \u201csharp\u201d) tend to perform better at test time. Strictly speaking, flatness is not necessary for generalization (Dinh et al., 2017), but our intuition can often be empirically verified to be correct, as good generalization is regularly observed in flat regions where the eigenvalues of the Hessian are mostly concentrated near zero (Chaudhari et al., 2017). The spectral density of the Hessian can in principle be used to evaluate sharpness, and has well-known links to norms that can be used for explicit regularization (Karakida et al., 2019), but for large-scale neural network training in practice, first-order approximations have shown the greatest utility. In particular, the sharpness-aware minimization (SAM) algorithm of Foret et al. (2021), extended for scale invariance by Kwon et al. (2021) and later captured as a special case of the gradient norm penalization (GNP) scheme of Zhao et al. (2022), has shown state-of-the-art performance on a variety of deep learning tasks. All of these first-order procedures can be cast as (forward) finite-difference approximations of the curvature (Karakida et al., 2023), requiring at least double the computational cost of vanilla gradient descent (GD) at each iteration. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As an alternative approach, the \u201cflooding\u201d technique of Ishida et al. (2020) is worthy of attention for surprising improvements in test accuracy despite its apparent simplicity. Flooding is done as follows: fix a threshold $\\theta$ before training and run vanilla GD until the average loss goes below $\\theta$ , and while below this threshold, run gradient ascent rather than descent (see $\\S2.1$ for details). Flooding appeared before SAM in the literature, but near the threshold, flooding can iterate between optimizing the empirical risk and the squared gradient norm (penalizing sharpness), establishing the former as an inexpensive alternative to the latter. On the other hand, it is not at all obvious how the flooding threshold $\\theta$ should be set given a particular data distribution and model class, and at present there is no methodology for settings which are \u201coptimal\u201d or at least \u201csufficient\u201d from the viewpoint of test accuracy. More importantly, what if we are interested in performance criteria going beyond that of classification accuracy? Flooding just says \u201cmake the average loss as close to $\\theta$ as possible,\u201d and we hypothesize that this requirement is too weak to encourage low model complexity and/or good generalization in terms of losses, while also keeping test accuracy high. ", "page_idx": 1}, {"type": "text", "text": "In this work, we investigate the validity of this hypothesis, and consider the impact of making a stronger requirement, namely to ask the algorithm to \u201cmake sure the loss distribution is wellconcentrated near $\\theta$ .\u201d We show in $\\S3$ that this can be implemented by introducing a smooth wrapper, applicable to any loss, which penalizes both over-performing and under-performing examples in a per-point fashion, instead of applying a hard threshold to the whole (mini-)batch as in flooding. We call this proposed procedure \u201csoft ascent-decent\u201d (SoftAD), and provide a detailed comparison with the flooding technique, highlighting the smoothness of SoftAD with implications in terms of formal stationarity guarantees, and emphasize how our mechanism leads to update directions that are qualitatively distinct from those used in flooding. Through rigorous empirical tests using both simulated and real-world benchmark classification datasets, featuring neural networks both large and small, we discover that compared with ERM, SAM, and flooding, the proposed SoftAD achieves far and away the smallest generalization error in terms of the base loss, while maintaining competitive accuracy and small model norms, without any explicit regularization. ", "page_idx": 1}, {"type": "text", "text": "Before diving into the main results just described, we introduce notation and background concepts in $\\S2$ . SoftAD is introduced in $\\S3$ , where we make basic empirical comparisons to flooding in $\\S3.1$ , and contrast formal guarantees of stationarity for these two methods in $\\S3.2$ . Our main empirical test results are given in $\\S4$ , with discussion and concluding remarks wrapping up the paper in $\\S5$ . All detailed proofs and supplementary empirical results are relegated to the appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To begin, we formulate performance metrics characterizing the problem of interest. Let $\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ parameterize our hypothesis class, let $\\mathcal{Z}$ denote the set to which individual data points $z$ belong. Let Z represent a random (test) data point with distribution $\\mu$ over $\\mathcal{Z}$ . For classification, where our data takes the form $\\mathsf Z=(\\mathsf X,\\mathsf Y)$ and for each $w\\in\\mathscr{W}$ we let $\\widehat{\\mathsf{Y}}(w)$ denote the predicted label given $\\mathsf{X}$ , the traditional performance metric of interest is the error probability at test time, denoted by ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathcal E}(w):={\\mathbf P}\\left\\{{\\widehat{\\mathsf Y}}(w)\\neq{\\mathsf Y}\\right\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "When we refer to test accuracy, we mean the probability of correct prediction, namely $1-\\mathcal{E}(w)$ . Even when high accuracy is desired, it is standard to make use of computationally congenial surrogate loss functions for training. Let $\\ell:\\mathbb{R}^{d}\\times\\mathcal{Z}\\to\\mathbb{R}$ denote a generic loss function to be used for training. For example, if $z\\,=\\,(x,y)$ represents (input, label) pairs, then a typical choice of $\\ell$ would be the cross-entropy loss. While the model is left abstract in our notation, note that for non-linear models such as neural networks, the mapping $w\\mapsto\\ell(w;z)$ may be non-convex and non-smooth over $\\mathcal{W}$ . As with the error probability (1), the expected loss (or \u201crisk\u201d) ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{R}_{\\mu}(w):=\\mathbf{E}_{\\mu}\\,\\ell(w;{\\cal Z})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "is also an important indicator of classifier performance. Both (1) and (2) are ideal quantities since $\\mu$ is unknown and Z is not observed at training time. We assume the learning algorithm has access to an independent sample of $n$ observations from $\\mu$ , denoted by $\\mathbf Z_{n}:=\\bar{(}Z_{1},\\bar{.}\\ldots,Z_{n})$ for convenience. Traditional machine learning algorithms are driven by the empirical risk, denoted here by $\\begin{array}{r}{\\mathsf{R}_{n}(w):=(1/n)\\sum_{i=1}^{n}\\ell(w;Z_{i})}\\end{array}$ , in that they seek out (local) minima of $\\mathsf{R}_{n}$ . In this paper, we use the term empirical risk minimization $(E R M)$ to refer to algorithms that directly apply an optimizer to $\\mathsf{R}_{n}$ . Under sophisticated models, the usual ERM objective $\\mathsf{R}_{n}(\\cdot)$ tends to admit a complex landscape. As discussed in $\\S1$ , numerous alternatives to ERM have been proposed, with the aim of minimizing $\\mathcal E(\\cdot)$ more reliably; next we take a closer look at one such technique, called \u201cflooding.\u201d ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.1 Flooding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The basic intuition underlying the proposal of Ishida et al. (2020) is that while minimizing $\\mathsf{R}_{n}(\\cdot)$ may be sufficient for maximizing the training accuracy, it need not be necessary, and from the perspective of optimizing $\\mathcal E(\\cdot)$ , it may be worth it to sacrifice $\\mathsf{R}_{n}(\\cdot)$ and even $\\operatorname{R}_{\\mu}(\\cdot)$ . Fixing a threshold $\\theta\\in\\mathbb{R}$ , the \u201cflooded\u201d objective is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{F}_{n}(w;\\theta):=\\theta+|\\mathsf{R}_{n}(w)-\\theta|.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This objective can be implemented as a simple wrapper around typical loss functions, to which off-the-shelf gradient-based optimizers can be applied; running vanilla sub-gradient descent yields a sequence $(w_{1},w_{2},\\ldots)$ generated using the update ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{t+1}=w_{t}-\\alpha\\,\\mathrm{sign}\\,(\\mathsf{R}_{n}(w_{t})-\\theta)\\,\\nabla\\mathsf{R}_{n}(w_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for all $t\\geq1$ , where $\\mathrm{sign}(x):=x/|x|$ for all $x\\neq0$ and $\\mathrm{sign}(0):=0$ , and $\\alpha\\,>\\,0$ is a fixed step size. The update (4) characterizes what we call the Flooding algorithm. From the above definitions, $\\mathsf{F}_{n}(w;\\theta)$ is minimal if and only if $\\mathsf{R}_{n}(w)=\\theta$ . That is, Flooding seeks out $w$ such that the training loss distribution induced by $(\\ell(w;Z_{1}),\\dots,\\ell(w;Z_{n}))$ has a mean which is close to $\\theta$ ; nothing more, nothing less. ", "page_idx": 2}, {"type": "text", "text": "2.2 Links to sharpness ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assuming for now that the loss is differentiable, it has been appreciated for some time that the distribution of the loss gradients $\\nabla\\ell(w;\\mathbf{Z})$ can convey important information about generalization (Zhang et al., 2020), and in particular the role of gradient regularization, both implicit and explicit, is receiving significant attention (Barrett and Dherin, 2021; Smith et al., 2021).2 As a concrete example of explicit regularization, consider modifying the ERM objective using the squared Euclidean norm as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{\\mathsf{R}}_{n}(w;\\lambda):=\\mathsf{R}_{n}(w)+\\frac{\\lambda}{2}\\|\\nabla\\mathsf{R}_{n}(w)\\|^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\lambda\\geq0$ controls the degree of penalization. If one is to minimize this objective in $w$ directly using gradient descent, this involves computing ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla\\widetilde{\\mathsf{R}}_{n}(w;\\lambda)=\\nabla\\mathsf{R}_{n}(w)+\\lambda\\nabla^{2}\\mathsf{R}_{n}(w)\\left(\\nabla\\mathsf{R}_{n}(w)\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and thus doing matrix multiplication using a $d\\times d$ Hessian ${\\nabla}^{2}{\\mathsf{R}}_{n}(w)$ , an unattractive proposition when $d$ is large. A linear approximation to the expensive term can be obtained via ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\nabla\\mathsf{R}_{n}(w+a u)-\\nabla\\mathsf{R}_{n}(w)}{a}\\approx\\nabla^{2}\\mathsf{R}_{n}(w)\\,(u)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $u\\in\\mathbb{R}^{d}$ is arbitrary and $|a|$ is small; see Zhao et al. $(2022,\\,\\S3.3)$ for example. Applying this to approximate $\\nabla\\widetilde{\\mathsf{R}}_{n}(w;\\lambda)$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla\\mathsf{R}_{n}(w)+\\frac{\\lambda}{a}\\left(\\nabla\\mathsf{R}_{n}(w+a\\nabla\\mathsf{R}_{n}(w))-\\nabla\\mathsf{R}_{n}(w)\\right)\\approx\\nabla\\widetilde{\\mathsf{R}}_{n}(w;\\lambda).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The iterative update directions used by the $S A M$ algorithm of Foret et al. (2021) are captured by setting $a=\\lambda$ , offering a nice link between loss-based sharpness control and gradient regularization. The extension of SAM in GNP (Zhao et al., 2022) can be expressed by an analogous derivation, replacing the squared norm $\\lVert\\cdot\\rVert^{2}$ in (5) with $\\lVert\\cdot\\rVert$ . Using updates of the form given in (6) with $a>0$ is called a \u201cforward\u201d finite-difference (FD) approach to explicit gradient regularization (GR) (henceforth, FD-GR), and clearly requires two gradient calls per update.3 Better precision is available using \u201ccentered\u201d FD, at the cost of additional gradient calls (Karakida et al., 2023). How does this all relate to Flooding? In repeating the update (4), once the empirical risk $\\mathsf{R}_{n}$ goes below $\\theta$ and the algorithm switches from ascent to descent, it is straightforward to show conditions where this leads to iteration between minimizing $\\mathsf{R}_{n}$ and $\\|\\nabla\\mathsf{R}_{n}(w)\\|^{2}$ . We give more details in $\\S B.1$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Soft ascent-descent ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "With the context of $\\S2$ in place, we consider making two qualitative changes to the Flooding update (4), described as follows. ", "page_idx": 3}, {"type": "text", "text": "1. Pointwise thresholds: Invert the order of applying $\\mathsf{R}_{n}(\\cdot)$ and $\\mathrm{{sign}(\\cdot)}$ , i.e., do summation over data points after per-loss truncation.   \n2. Soft truncation: Replace the hard threshold $\\mathrm{{sign}(\\cdot)}$ with a continuous, bounded, monotonic (increasing) function $\\phi(\\cdot)$ satisfying $\\phi(0)=0$ . ", "page_idx": 3}, {"type": "text", "text": "The reason for making the thresholds pointwise is to allow the algorithm to view \u201cascent\u201d and \u201cdescent\u201d from the perspective of individual losses (rather than bundled up in $\\mathsf{R}_{n}$ ), making it possible to utilize a sum of both ascent and descent update directions.4 To make this sum a weighted sum, the soft truncation using $\\phi$ plays a key role. Keeping $\\phi$ bounded limits the impact of errant loss values, while the other assumptions allow for both ascent and descent, with \u201cborderline\u201d points near the threshold given less weight. Written explicitly as an empirical objective function, we use ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{S}_{n}(w;\\theta):=\\theta+\\frac{1}{n}\\sum_{i=1}^{n}\\rho(\\ell(w;\\mathsf{Z}_{i})-\\theta)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where once again $\\theta\\in\\mathbb{R}$ is a fixed threshold, and we set $\\rho(x):=\\sqrt{x^{2}+1}-1$ . Running vanilla GD on ${\\sf S}_{n}(\\cdot;\\theta)$ in (7) yields the update ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}-\\frac{\\alpha}{n}\\sum_{i=1}^{n}\\phi(\\ell(w_{t};Z_{i})-\\theta)\\nabla\\ell(w_{t};Z_{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\phi(x):=\\rho^{\\prime}(x)\\,=\\,x/\\sqrt{x^{2}+1}$ , and $\\alpha\\,>\\,0$ is once again a fixed step size. For convenience, we use soft ascent-descent (or SoftA $D$ for short) to refer to the algorithm implied by the iterative update (8). Note that there is nothing particularly special about our choice of $\\rho$ here; it is just a simple algebraic function whose derivative also takes a simple form; note that the function $\\phi$ resulting from our choice of $\\rho$ satisfies the desired properties of continuity, boundedness, monotonicity, and $\\phi(0)=\\rho^{\\prime}(0)=0.$ .5 Note that for each point being summed over, $\\ell(w_{t};\\mathbf{Z}_{i})\\,>\\,\\theta$ implies descent while $\\ell(w_{t};\\mathsf Z_{i})<\\theta$ implies ascent, and borderline points with $\\ell(w_{t};\\mathbf{Z}_{i})\\approx\\theta$ have a smaller relative impact. In contrast with Flooding, SoftAD requires that the training loss distribution induced by $(\\ell(\\bar{w};Z_{1}),\\ldots,\\ell(w;Z_{n}))$ be well-concentrated around $\\theta$ , where the degree of dispersion is measured in a symmetric fashion using $\\rho$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1 (Comparison with other variants of Flooding). During the review phase for this work, we were made aware of another recent related work by Xie et al. (2022). Their proposed method is known as iFlood, and it is essentially a middle ground between our proposal above and the original Flooding procedure. They use pointwise thresholds as we do in SoftAD, but retain the hard ascent-descent switch as in Flooding. More concisely, replacing our soft truncator $\\phi(x)$ in (8) with the absolute value $|x|$ yields the iFlood update. We have added empirical test results for iFlood to complement our original experiments at the end of $\\S C.2$ . Another very recent variant is AdaFlood (Bae et al., 2023), which sets the $\\theta$ threshold level individually for each point based on \u201cdifficulty\u201d as evaluated using an auxiliary model. ", "page_idx": 3}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/7821f7d3e63996a60a373c18e5e060bb414821abe10351d611a0fc9253069c82.jpg", "img_caption": ["Figure 1: The left-most figure simply plots the graph of $f(x)=x^{2}/2$ over $x\\in\\left\\lceil-2,2\\right\\rceil$ . The two remaining figures show plots of the graphs of $\\bar{f^{\\prime}}(\\bar{x})=x$ (dashed black line) and $\\dot{\\phi}\\big((f(x)-\\theta)\\dot{/}\\sigma\\big)f^{\\prime}\\bar{(}x\\big)$ for the same range of $_x$ values, with colors corresponding to modified values of $\\sigma$ (middle plot; $\\theta=0.5$ fixed) and $\\theta$ (right-most plot; $\\sigma=1.0$ fixed) respectively. Thick dotted lines are $\\phi=\\mathrm{sign}$ , thin solid lines are $\\phi=\\rho^{\\prime}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/c53f7eed5a9294079c978501a6ec837f28584aa6db1f7a2a55e3e8176012a827.jpg", "img_caption": ["Deterministic comparison $(x_{0}=-2.0$ , $\\theta=0.5$ , $\\sigma=1.0$ ) ", "Figure 2: Gradient descent on the quadratic example from Figure 1. The horizontal axis denotes iteration number, and we plot sequences of iterates $\\left(\\boldsymbol{x}_{t}\\right)$ and function values $\\textstyle(f(x_{t}))$ for each method. Here \u201cGD\u201d denotes vanilla gradient descent, with \u201cFlood\u201d and \u201cSoftAD\u201d corresponding to (4) and (8) respectively. Step size is $\\alpha=0.1$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Remark 2 (Difference from OCE-like criteria). At first glance, our objective ${\\sf S}_{n}(w;\\theta)$ in (7) may appear similar to the criteria used in OCE risk minimization (Lee et al., 2020; Li et al., 2021) and some varieties of DRO (Duchi and Namkoong, 2021). Aside from the obvious difference that $\\theta$ is fixed, rather than optimized alongside $w$ , the critical difference here is that our $\\rho(\\cdot)$ is not monotonic. Losses which are too large and too small are both penalized. It is precisely this bi-directional property that allows for switching between ascent and descent; this is impossible to achieve with monotonic OCE and DRO risks (Holland, 2023; Holland and Tanabe, 2023; Hu et al., 2023; Royset, 2024). This bi-directional criterion can also be used as a straightforward method to provably avoid unintentional \u201ccollapse\u201d into ERM solutions (Holland, 2024). ", "page_idx": 4}, {"type": "text", "text": "3.1 Initial comparison with Flooding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To develop some intuition for our SoftAD (8) and the Flooding update (4), we carry out a few illustrative numerical experiments. To start, let us consider a simple, non-stochastic example in one dimension. Letting $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ be some differentiable function, we consider how the transformed gradient $\\phi(f(x)-\\theta)f^{\\prime}(x)$ behaves under $\\phi=\\mathrm{sign}$ and $\\phi=\\rho^{\\prime}$ . In Figure 1, we give a numerical example using a quadratic function. The softened nature of the transformed gradients used in SoftAD is clear when compared with the hard switching mechanism underlying the Flooding update. In Figure 2, we continue the quadratic function example, looking at sequences $(x_{1},x_{2},\\ldots)$ generated based on the Flooding and SoftAD procedures. That is, instead of $n$ points from which to compute losses, we have just one \u201closs,\u201d namely $f(x)=x^{2}/2$ . Both procedures realize an effective \u201cflood level\u201d of sorts (i.e., a buffer around the loss minimizer), but as expected, the Flooding procedure tends to be far more \u201cjagged\u201d in its trajectory. ", "page_idx": 4}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/e057ac8ac7cc3d111f274ac6721b22b78a884d1cad416471d863417ec61ef5e9.jpg", "img_caption": ["Figure 3: Left: We randomly sample $n\\,=\\,8$ poin\u221ats (black dots) from the 2D Gaussian distribution, zero mean, zero correlations, with standard deviation $2\\sqrt{2}$ in each coordinate. The two candidates are denoted by square-shaped points (red and green), and the minimizer of $\\mathsf{R}_{n}$ is given by a gold star. Center: The Flooding updates (colored arrows) via (4) for each candidate. Right: Analogous SoftAD update vectors via (8), with per-point transformed gradients (semi-transparent arrows) for reference. Throughout, we have fixed $\\theta=1.5\\ \\bar{\\times}\\ \\mathrm{min}_{w}\\,\\mathsf{R}_{n}(w)$ and $\\alpha=0.75$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Finally, a simple example to illustrate how the per-point soft thresholding of SoftAD leads to distinct gradient-based update directions when compared to the Flooding strategy. Here we consider a dataset of $n$ data points $z_{1},\\dots,z_{n}\\,\\in\\,\\mathbb{R}^{2}$ , and use the squared Euclidean norm as a loss, i.e., $\\ell(w;z)=\\|w-z\\|^{2}$ . This is a natural extension of the quadratic example in the previous paragraph, to multiple points and two dimensions. In Figure 3, we look at two candidate points, and compute the Flooding and SoftAD update directions that arise at each candidate under a randomly generated dataset. We can clearly see how the Flooding update plunges directly towards the minimizer of $\\mathsf{R}_{n}$ , unless it is too close (given threshold $\\theta$ ), in which case it goes in the opposite direction. In contrast, the SoftAD update is composed of per-point update directions, some which attract toward the minimum, and some which repel from the minimum, with borderline points down-weighted (shorter update arrows). Since the final update averages over these, movement both toward and away from the minimum is clearly \u201csoftened\u201d when compared with the Flooding updates. ", "page_idx": 5}, {"type": "text", "text": "3.2 Comparison of convergence properties ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With the Flooding and SoftAD methods in mind, next we consider concrete conditions under which stochastic gradient-based learning algorithms can be given guarantees of (approximate) stationarity. Throughout this section, we assume that the loss $w\\mapsto\\ell(w;z)$ is locally Lipschitz on $\\mathcal{W}$ for each $z\\in{\\mathcal{Z}}$ , but convexity will not be used. Let us assume for simplicity that $({\\dot{Z}}_{1},{\\bar{Z}}_{2},\\ldots)$ is a sequence of independent random variables with distribution $\\mu$ , the same as our test data point $Z\\sim\\mu$ . ", "page_idx": 5}, {"type": "text", "text": "Starting with SoftAD, we are interested in procedures fuelled by stochastic gradients of the form ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathsf{G}_{t}(w):=\\phi(\\ell(w;\\mathsf{Z}_{t})-\\theta)\\nabla\\ell(w;\\mathsf{Z}_{t})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all integers $t\\geq1$ and $w\\in\\mathcal{W}$ , with threshold $\\theta\\in\\mathbb{R}$ fixed in advance, and $\\phi=\\rho^{\\prime}$ as before. Recalling the empirical SoftAD objective $\\mathsf{S}_{n}$ in (7), the underlying population objective is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{S}_{\\mu}(w):=\\theta+\\mathbf{E}_{\\mu}\\,\\rho(\\ell(w;\\mathbf{Z})-\\theta).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By design, we do not expect SoftAD to approach a stationary point of the original $\\mathrm{R}_{\\mu}$ . Note that under mild assumptions on the data distribution, we have an unbiased estimator with $\\mathbf{E}_{\\mu}\\,\\dot{\\mathbf{G}}_{t}(w)=\\nabla\\mathrm{S}_{\\mu}(w)$ , suggesting stationarity in terms of $\\mathrm{S}_{\\mu}(\\cdot)$ as a natural goal. Assuming the losses are $L_{\\ell}$ -smooth in expectation, one can readily confirm that the objective (10) is $L_{\\mathrm{AD}}$ -smooth, with ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{\\mathrm{AD}}:=\\mathbf{E}_{\\mu}\\left[\\operatorname*{sup}_{w\\in\\mathcal{W}}\\|\\nabla\\ell(w;\\mathbf{Z})\\|^{2}\\right]+L_{\\ell}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A more detailed derivation is given in $\\S B.5$ . Assuming that second-order moments are finite in a uniform sense over $\\mathcal{W}$ ensures that $L_{\\mathrm{AD}}<\\infty$ , and naturally implies pointwise variance bounds, allowing us to seek out stationarity guarantees using a combination of gradient norm control and momentum in the fashion of Cutkosky and Mehta (2021). ", "page_idx": 5}, {"type": "text", "text": "Proposition 3 (Stationarity for SoftAD, smooth case). Starting with an arbitrary $w_{1}\\in\\mathcal{W}$ , update using $w_{t+1}=w_{t}-\\alpha\\mathsf{M}_{t}/\\|\\mathsf{M}_{t}\\|$ , where $\\mathsf{M}_{t}:=\\,b\\mathsf{M}_{t-1}+(1-b)\\bar{\\mathsf{G}}_{t}(w_{t})$ for $t\\geq1$ , with ${\\sf M}_{0}:=0$ and $\\overline{{\\mathsf{G}}}_{t}(\\cdot)\\,:=\\,\\mathsf{G}_{t}(\\cdot)\\operatorname*{min}\\{1,\\gamma/\\|\\mathsf{G}_{t}(\\cdot)\\|\\},$ , taking each gradient $\\mathsf{G}_{t}(\\cdot)$ b\u221aased on (9). Assuming we make $T\\mathrm{~-~}1$ updates, set the momentum parameter to $b\\,=\\,1\\,-\\,1/{\\sqrt{T}}$ , the norm threshold to $\\gamma\\,=\\,\\sqrt{(L_{\\mathrm{AD}}-L_{\\ell})/(1-b)}$ , and the step size to $\\alpha=1/T^{3/4}$ . The stationarity of this sequence $(w_{1},w_{2},\\ldots)$ , assumed to be in $\\mathcal{W}$ , in terms of the modified objective $(I O)$ can be bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\lVert\\nabla\\mathrm{S}_{\\mu}(w_{t})\\rVert\\le\\frac{1}{T^{1/4}}\\left(\\mathrm{S}_{\\mu}(w_{1})-\\mathrm{S}_{\\mu}(w_{T+1})+\\frac{3L_{\\mathrm{AD}}}{2}+2\\sqrt{L_{\\mathrm{AD}}-L_{\\ell}}\\left(1+C_{\\delta}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "using confidence-dependent factor $C_{\\delta}:=10\\log(3T/\\delta)+4\\sqrt{\\log(3T/\\delta)}+1,$ , with probability no less than $1-\\delta$ over the draw of $(Z_{1},Z_{2},\\ldots)$ . ", "page_idx": 6}, {"type": "text", "text": "For comparison, we next consider stationarity of the Flooding algorithm in the same context of smooth losses. The argument used in Proposition 3 critically depends on the smoothness of the underlying objective (10). Unfortunately, this smoothness cannot be leveraged when we consider the population objective underlying the Flooding procedure, namely the function ", "page_idx": 6}, {"type": "equation", "text": "$$\nw\\mapsto\\theta+|\\mathrm{R}_{\\mu}(w)-\\theta|.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Further complicating things is the fact that stochastic gradients of the form (9) with $\\phi(\\cdot)$ replaced by $\\mathrm{{sign}(\\cdot)}$ do not yield unbiased sub-gradient estimates for (12), but rather for an upper bound $\\theta+\\mathbf{E}_{\\mu}|\\ell(w;\\mathbf{Z})-\\theta|$ that follows via Jensen\u2019s inequality. A lack of smoothness means we cannot establish stationarity in terms of (12) nor the upper bound just given, but it is possible using a smoothed approximation (the Moreau envelope) of this bound: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{F}}}_{\\mu}(w):=\\operatorname*{inf}_{v\\in\\mathcal{W}}\\left[\\theta+\\mathbf{E}_{\\mu}|\\ell(v;\\mathsf{Z})-\\theta|+\\frac{1}{2\\beta}\\|v-w\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The parameter $\\beta\\,>\\,0$ controls the degree of smoothness. The objective in (13) can be linked to \u201cgradients\u201d in (9) with $\\phi=\\mathrm{sign}$ , and leveraging the Lipschitz continuity of $\\left|\\cdot\\right|$ along with a sufficiently smooth loss, it is possible to show that this non-smooth objective satisfies a weak form of convexity, and using the techniques of Davis and Drusvyatskiy (2019) it is possible to show that stochastic gradient algorithms enjoy stationarity guarantees, albeit not in terms of the objective (12), but rather the smoothed upper bound (13). ", "page_idx": 6}, {"type": "text", "text": "Proposition 4 (Stationarity for Flooding, smooth case). Letting $\\mathcal{W}$ be closed and convex, take an initial point $w_{1}\\in\\mathcal{W}$ , and make $T-1$ updates using $w_{t+1}=\\Pi_{\\mathcal{W}}[w_{t}-\\alpha\\mathsf{G}_{t}(w_{t})].$ , where $\\Pi_{\\mathcal{W}}[\\cdot]$ denotes projection to $\\mathcal{W}_{i}$ , and each $\\mathsf{G}_{t}(\\cdot)$ is computed using (9) with $\\phi=$ sign. Assuming the loss $w\\mapsto\\ell(w;z)$ is $L_{\\ell}^{*}$ -smooth on $\\mathcal{W}$ for all $z\\in{\\mathcal{Z}}$ , and taking a step size of ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\alpha^{2}=\\frac{\\Delta}{T L_{\\ell}^{*}(L_{\\mathrm{AD}}-L_{\\ell})},\\ u s i n g\\ \\Delta\\;s u c h\\;t h a t\\ \\Delta\\geq\\bar{\\mathrm{F}}_{\\mu}(w_{1})-\\operatorname*{inf}_{w\\in\\mathcal{W}}\\bar{\\mathrm{F}}_{\\mu}(w),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $L_{\\mathrm{AD}}$ and $L_{\\ell}$ as in $(I I)$ , the expected squared stationarity in terms of the smoothed upper bound $(I3)$ at smoothness level $\\beta=1/(2L_{\\ell}^{*})$ can be controlled as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac1T\\sum_{t=1}^{T}\\mathbf{E}\\|\\nabla\\overline{{\\mathrm{F}}}_{\\mu}(w_{t})\\|^{2}\\leq\\sqrt{\\frac{2L_{\\ell}^{*}(L_{\\mathrm{AD}}-L_{\\ell})\\Delta}{T}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with expectation taken over the draw of $(Z_{1},Z_{2},\\ldots)$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 5 (Comparing rates and assumptions). Considering the preceding Propositions 3 and 4, one common point is that learning algorithms based on both the SoftAD and Flooding gradients (of mini-batch size 1) can be shown to be approximately stationary in terms of functions of a similar form (i.e., (10) and (12)), differing only in how they measure deviations from the threshold $\\theta$ . The rates of decrease (as a function of $T$ ) are essentially the same, noting that the bounds in Proposition 4 are in terms of squared norms. That said, a lack of smoothness means the Flooding guarantees only hold for a smoothed variant, plus they require a stronger form of smoothness in the loss (over all $z$ vs. in expectation). In addition, the SoftAD guarantees hold with high probability over the data sample, and can be readily strengthened to hold for an individual iterate (instead of summing over $T$ iterates), using for example the technique of Cutkosky and Mehta (2021, Thm. 3). ", "page_idx": 6}, {"type": "table", "img_path": "Y1ZsLONDI2/tmp/7e7a23f4e3449004714820990cda3de8012b6aab96a54fc199627743dc76de2a.jpg", "table_caption": ["Table 1: Generalization gap (test - training) for trial-averaged cross entropy loss after final epoch. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 Empirical study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we apply the proposed SoftAD procedure to a variety of classification tasks using neural network models, leading to losses that are non-convex and non-smooth. Our goal here is to compare and contrast the behavior and performance (accuracy, average loss, model norm) of SoftAD with three natural alternatives: ERM, Flooding, and SAM.6 ", "page_idx": 7}, {"type": "text", "text": "4.1 Overview of experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our core experiments are centered around re-creating the tests done by Ishida et al. (2020, $\\S4.1$ , $\\S4.2)$ and Foret et al. (2021, $\\S3.1)$ to include all four methods of interest. There are two main parts: simulation-based tests and real benchmark-based tests. We briefly describe the setup of each below. ", "page_idx": 7}, {"type": "text", "text": "Non-linear binary classification on the plane We use three synthetic data-generators (\u201ctwo Gaussians,\u201d \u201csinusoid,\u201d and \u201cspiral,\u201d see Figure 7) to create a dataset on the 2D plane that is not linearly separable, but separable using relatively simple non-linear models. We treat the underlying model as unknown, and approximate it using a shallow feedforward neural network. All four methods of interest (ERM, Flooding, SAM, and SoftAD) are driven by the Adam optimizer, with the cross-entropy loss used as the base loss function. Complete experimental details are provided in $\\mathrm{\\&C.1}$ . ", "page_idx": 7}, {"type": "text", "text": "Image classification from scratch Our second set of experiments utilizes four well-known benchmark datasets for multi-class image classification. Compared to the synthetic experiments, the classification task is more difficult (much larger inputs, variation within classes, more classes), and so we utilize more sophisticated neural network models to tackle the classification task. That said, as the sub-section title indicates, this training is done \u201cfrom scratch,\u201d i.e., no pre-trained models are used. The datasets we use are all standard benchmarks in the machine learning community: CIFAR-10, CIFAR-100, FashionMNIST, and SVHN. Model choice essentially mirrors that of Ishida et al. (2020, $\\S4.2)$ . For FashionMNIST, we flatten each image into a vector, and use a simple feedforward neural network with one hidden layer. For SVHN, we use ResNet-18 as implemented in torchvision.models, without any pre-trained weights. Finally, for both CIFAR-10 and CIFAR100, we use ResNet-34 (again in torchvision.models) without pre-training. For the optimizer, we use vanilla SGD with a fixed step size. Full details are given in $\\S C.2$ in the appendix. ", "page_idx": 7}, {"type": "text", "text": "4.2 Main findings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Uniformly small loss generalization gap One of the most lucid results we obtained is that SoftAD shows the smallest loss generalization gap of all the methods studied, across all models and datasets used. In Table 1, we show the gaps incurred under each dataset. More precisely, for each trial and each epoch, we compute the average cross-entropy loss on test and training (less validation) datasets, and then respective average both of these over all trials. The difference of these two values (i.e., trial-averaged test loss minus trial-averaged training loss) after the final epoch of training is the value shown in each cell of the table. ", "page_idx": 7}, {"type": "text", "text": "Balance of accuracy and loss on real data In the previous paragraph we noted that SoftAD has superior loss gaps, but this is not much to celebrate if performance in terms of the key metrics of interest (i.e., test loss and test accuracy) is poor. In Figures 4\u20135, we show the trajectory of loss and accuracy (for both training and test data) over epochs run (averaged over trials). All four methods are comparable in terms of accuracy, with SAM (at double the gradient cost) coming out slightly ahead. On the other hand, there is significant divergence between the different methods in terms of test loss. For each dataset, SoftAD achieves a superior test loss, often converging faster than any of the other methods; this may be a natural by-product of the fact that SoftAD is designed to ensure losses are well-concentrated around threshold $\\theta$ , instead of just asking that their mean get close to $\\theta$ (as in Flooding). While there is clearly a major difference between ERM and the other three methods, the stable nature of the \u201cdouble descent\u201d in SoftAD is quite stark compared with Flooding and SAM. ", "page_idx": 7}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/c7b37dfd88d00c4fa7fb40bc6126a6c13843c0afd6bdd0d19d0a10637fd73e4c.jpg", "img_caption": ["Figure 4: Trajectories over epochs for average test loss (top row) and test accuracy (bottom row). Horizontal axis is epoch number. Columns are associated with the CIFAR-10 and CIFAR-100 datasets (left to right). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/dae75df333f4e0605ecd280c2093e49aa0c5040118e835c43df9f738a766f578.jpg", "img_caption": ["Figure 5: Analogous to Figure 4, but with FashionMNIST and SVHN datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Uniformly smaller model norms We do not do any explicit model regularization (e.g., L2 norm penalization) in our experiments here, and we only use fixed step-size parameters for Adam and SGD, so as we run for many iterations, the norm of the model weight parameters tends to grow. While this property holds across all methods tested here, we find that under all datasets and models tested, SoftAD uniformly results in the smallest model norm; see Figure 6 for trajectories over epochs for each benchmark dataset. The \u201cModel norm\u201d values plotted here are the L2 norm of all the model parameters (neural network weights) concatenated into a single vector, and these norm values are averaged over trials. Completely analogous trends hold for the simulated datasets as well. ", "page_idx": 8}, {"type": "text", "text": "Trends in hyperparameter selection Aside from ERM, the three key methods of interest (Flooding, SAM, SoftAD) each have one hyperparameter. Flooding and SoftAD have the threshold $\\theta$ as described in $\\S2{-}\\S3$ , and SAM has the radius parameter (denoted by \u201c $\\dot{\\rho}$ \u201d in the original paper). In all tests, we select a representative candidate for each method with hyperparameters by using validation data held out from the training data, and in Table 2 we show the average and standard deviation of the validation-based hyperparameters over randomized trials (see $\\S C$ for exact hyperparameter grid values). One clear take-away from this table is that the \u201cbest\u201d value of $\\theta$ (in terms of accuracy) for SoftAD tends to be larger than that for Flooding, and this trend is uniform across all datasets, both simulated and real. In particular for the real benchmark datasets, it is interesting to note that while a larger threshold $\\theta$ (applied to loss distribution) is selected for SoftAD, the resulting test loss value achieved is actually smaller/better than that achieved by Flooding (top row of Figures 4-5). ", "page_idx": 8}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/d0369a11f4128e1a0dfdd89018a32572f763020899977ee6bef7b00cb316c548.jpg", "img_caption": ["Figure 6: Model norm trajectories over epochs for each dataset in Figures 4\u20135. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "Y1ZsLONDI2/tmp/9ff7aab1d5ba16faf9f3ea6de37f614e8398ddb086e4fd90db5f2085bb02909b.jpg", "table_caption": ["Table 2: Hyperparameters selected by validation for each method (averaged over trials). Flooding and SoftAD have threshold $\\theta$ ; SAM has radius parameter. Standard deviation (over trials) is given in small-text parentheses. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Limitations and concluding remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While previous work had already shown that it is possible to sacrifice performance in terms of losses to improve accuracy, the nature of that tradeoff was left totally unexplored, and in $\\S1$ we put forward the hypothesis that simply asking the empirical loss mean to get close to a non-zero threshold $\\theta$ , as in Flooding, would not be enough to realize a competitive tradeoff over varied learning tasks (datasets, models). Our main take-away is that we have empirical evidence that the slightly stronger requirement of \u201closses well-concentrated around $\\theta^{\\bullet}$ (implemented as SoftAD) can result in an appealing balance of average test loss and accuracy, with the added benefit of a strong (implicit) regularization effect, likely due to the soft dampening effect on borderline points. A more formal theoretical understanding of this regularization effect is of interest, as are empirical studies going far beyond the limited choice of loss functions used here. Our biggest limitation is that the question of \u201chow to set the threshold $\\theta?^{\\bullet}$ still remains without an answer. Any meaningful answer will likely require some user judgement regarding tradeoffs between performance metrics. One potential first approach would be to leverage recent techniques for estimating the Bayes error (Ishida et al., 2023), combined with existing surrogate theory (Bartlett et al., 2006) to reverse-engineer a loss threshold given a user-specified \u201ctolerable\u201d drop in accuracy, for example. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by JST PRESTO (grant number JPMJPR21C6) and a grant from the SECOM Science and Technology Foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Arora, S., Ge, R., Neyshabur, B., and Zhang, Y. (2018). Stronger generalization bounds for deep nets via a compression approach. In Proceedings of the 35th International Conference on Machine Learning (ICML), volume 80 of Proceedings of Machine Learning Research, pages 254\u2013263.   \nBae, W., Ren, Y., Ahmed, M. O., Tung, F., Sutherland, D. J., and Oliveira, G. L. (2023). AdaFlood: Adaptive flood regularization. arXiv preprint arXiv:2311.02891.   \nBaker, A. (2022). Simplicity. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Summer 2022 edition.   \nBarrett, D. G. T. and Dherin, B. (2021). Implicit gradient regularization. In The 9th International Conference on Learning Representations (ICLR).   \nBarron, J. T. (2019). A general and adaptive robust loss function. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4331\u20134339.   \nBartlett, P. L. (1998). The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE Transactions on Information Theory, 44(2):525\u2013536.   \nBartlett, P. L., Jordan, M. I., and McAuliffe, J. D. (2006). Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138\u2013156.   \nChaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C., Chayes, J., Sagun, L., and Zecchina, R. (2017). Entropy-SGD: Biasing gradient descent into wide valleys. In International Conference on Learning Representations.   \nClaeskens, G. and Hjort, N. L. (2008). Model Selection and Model Averaging. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.   \nCutkosky, A. and Mehta, H. (2021). High-probability bounds for non-convex stochastic optimization with heavy tails. In Advances in Neural Information Processing Systems 34 (NeurIPS 2021), pages 4883\u20134895.   \nCutkosky, A., Mehta, H., and Orabona, F. (2023). Optimal stochastic non-smooth non-convex optimization through online-to-non-convex conversion. arXiv preprint arXiv:2302.03775v2.   \nDavis, D. and Drusvyatskiy, D. (2019). Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization, 29(1):207\u2013239.   \nDenker, J. and LeCun, Y. (1990). Transforming neural-net output levels to probability distributions. In Advances in Neural Information Processing Systems 3 (NIPS 1990), volume 3.   \nDevroye, L., Gy\u00f6rf,i L., and Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition, volume 31 of Stochastic Modelling and Applied Probability. Springer.   \nDinh, L., Pascanu, R., Bengio, S., and Bengio, Y. (2017). Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning (ICML), volume 70 of Proceedings of Machine Learning Research, pages 1019\u20131028.   \nDrucker, H. and Le Cun, Y. (1992). Improving generalization performance using double backpropagation. IEEE Transactions on Neural Networks, 3(6):991\u2013997.   \nDrusvyatskiy, D. and Paquette, C. (2019). Efficiency of minimizing compositions of convex functions and smooth maps. Mathematical Programming, 178:503\u2013558.   \nDuchi, J. C. and Namkoong, H. (2021). Learning models with uniform performance via distributionally robust optimization. The Annals of Statistics, 49(3):1378\u20131406.   \nDziugaite, G. K., Drouin, A., Neal, B., Rajkumar, N., Caballero, E., Wang, L., Mitliagkas, I., and Roy, D. M. (2020). In search of robust measures of generalization. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pages 11723\u201311733.   \nEfron, B. and Hinkley, D. V. (1978). Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information. Biometrika, 65(3):457\u2013483.   \nFeldman, V. (2016). Generalization of ERM in stochastic convex optimization: The dimension strikes back. In Advances in Neural Information Processing Systems 29 (NIPS 2016), pages 3576\u20133584.   \nFlaxman, A. D., Kalai, A. T., and McMahan, H. B. (2004). Online convex optimization in the bandit setting: gradient descent without a gradient. arXiv preprint arXiv:cs/0408007v1.   \nForet, P., Kleiner, A., Mobahi, H., and Neyshabur, B. (2021). Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations.   \nGoodfellow, I. J., Vinyals, O., and Saxe, A. M. (2014). Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544.   \nGr\u00fcnwald, P. D. (2007). The Minimum Description Length Principle. MIT Press.   \nHinton, G. E. and van Camp, D. (1993). Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the 6th Annual Conference on Computational Learning Theory, pages 5\u201313.   \nHochreiter, S. and Schmidhuber, J. (1994). Simplifying neural nets by discovering flat minima. In Advances in Neural Information Processing Systems 7 (NIPS 1994).   \nHochreiter, S. and Schmidhuber, J. (1997). Flat minima. Neural Computation, 9(1):1\u201342.   \nHolland, M. J. (2022). Learning with risks based on M-location. Machine Learning, 111:4679\u20134718.   \nHolland, M. J. (2023). Flexible risk design using bi-directional dispersion. In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS), volume 206 of Proceedings of Machine Learning Research, pages 1586\u20131623.   \nHolland, M. J. (2024). Criterion collapse and loss distribution control. In Proceedings of the 41st International Conference on Machine Learning (ICML), volume 235 of Proceedings of Machine Learning Research, pages 18547\u201318567.   \nHolland, M. J. and Tanabe, K. (2023). A survey of learning criteria going beyond the usual risk. Journal of Artificial Intelligence Research, 73:781\u2013821.   \nHu, S., Wang, X., and Lyu, S. (2023). Rank-based decomposable losses in machine learning: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45.   \nIshida, T., Yamane, I., Charoenphakdee, N., Niu, G., and Sugiyama, M. (2023). Is the performance of my deep network too good to be true? A direct approach to estimating the Bayes error in binary classification. In The 11th International Conference on Learning Representations (ICLR).   \nIshida, T., Yamane, I., Sakai, T., Niu, G., and Sugiyama, M. (2020). Do we need zero training loss after achieving zero training error? In Proceedings of the 37th International Conference on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pages 4604\u20134614.   \nJia, Z. and Su, H. (2020). Information-theoretic local minima characterization and regularization. In Proceedings of the 37th International Conference on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pages 4773\u20134783.   \nJiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S. (2020). Fantastic generalization measures and where to find them. In International Conference on Learning Representations. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Johnson, R. and Zhang, T. (2023). Inconsistency, instability, and generalization gap of deep neural network training. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023), pages 9479\u20139505. ", "page_idx": 12}, {"type": "text", "text": "Karakida, R., Akaho, S., and Amari, S.-i. (2019). Universal statistics of Fisher information in deep neural networks: Mean field approach. In 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), volume 89 of Proceedings of Machine Learning Research, pages 1032\u20131041. ", "page_idx": 12}, {"type": "text", "text": "Karakida, R., Takase, T., Hayase, T., and Osawa, K. (2023). Understanding gradient regularization in deep learning: Efficient finite-difference computation and implicit bias. In Proceedings of the 40th International Conference on Machine Learning (ICML), volume 202 of Proceedings of Machine Learning Research, pages 15809\u201315827.   \nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. (2017). On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations.   \nKullback, S. (1968). Information Theory and Statistics. Dover.   \nKwon, J., Kim, J., Park, H., and Choi, I. K. (2021). ASAM: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In Proceedings of the 38th International Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pages 5905\u20135914.   \nLee, J., Park, S., and Shin, J. (2020). Learning bounds for risk-sensitive learning. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pages 13867\u201313879.   \nLi, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. (2018). Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems 31 (NIPS 2018), pages 3576\u20133584.   \nLi, T., Beirami, A., Sanjabi, M., and Smith, V. (2021). Tilted empirical risk minimization. In The 9th International Conference on Learning Representations (ICLR).   \nMacKay, D. J. C. (1992). A practical Bayesian framework for backpropagation networks. Neural Computation, 4(3):448\u2013472.   \nMontavon, G., Orr, G. B., and M\u00fcller, K.-R., editors (2012). Neural Networks: Tricks of the Trade, volume 7700 of Lecture Notes in Computer Science. Springer, 2nd edition.   \nNesterov, Y. and Spokoiny, V. (2017). Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527\u2013566.   \nNeyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017). Exploring generalization in deep learning. In Advances in Neural Information Processing Systems 30 (NIPS 2017).   \nRoyset, J. O. (2024). Risk-adaptive approaches to stochastic optimization: A survey. arXiv preprint arXiv:2212.00856v3.   \nSchmidt, R. M., Schneider, F., and Hennig, P. (2021). Descending through a crowded valley \u2014 benchmarking deep learning optimizers. In Proceedings of the 38th International Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pages 9367\u20139376.   \nSejnowski, T. J. (2020). The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National Academy of Sciences, 117(48):30033\u201330038.   \nShalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K. (2010). Learnability, stability and uniform convergence. Journal of Machine Learning Research, 11:2635\u20132670.   \nSmith, S. L., Dherin, B., Barrett, D., and De, S. (2021). On the origin of implicit regularization in stochastic gradient descent. In International Conference on Learning Representations.   \nSun, K. and Nielsen, F. (2021). A geometric modeling of Occam\u2019s razor in deep learning. arXiv preprint arXiv:1905.11027v4.   \nWu, L., Zhu, Z., and E, W. (2017). Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239.   \nXie, Y., Wang, Z., Li, Y., Zhang, C., Zhou, J., and Ding, B. (2022). iFlood: A stable and effective regularizer. In International Conference on Learning Representations.   \nXie, Z., Sato, I., and Sugiyama, M. (2020). A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima. arXiv preprint arXiv:2002.03495.   \nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations.   \nZhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. (2020). Why are adaptive methods good for attention models? In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pages 15383\u201315393.   \nZhao, Y., Zhang, H., and Hu, X. (2022). Penalizing gradient norm for efficiently improving generalization in deep learning. In Proceedings of the 39th International Conference on Machine Learning (ICML), volume 162 of Proceedings of Machine Learning Research, pages 26982\u201326992. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Bibliographic notes ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide additional references intended to complement those in the main body of the paper. ", "page_idx": 14}, {"type": "text", "text": "A.1 Broad overview ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The following questions succinctly summarize key aspects of the problem of generalization:7 ", "page_idx": 14}, {"type": "text", "text": "QP. What properties at training time are reliable indicators of performance at test time? QA. How can we efficiently find candidates with such desirable properties? ", "page_idx": 14}, {"type": "text", "text": "The easy answer to these questions is, of course, \u201cit depends.\u201d There is no fixed procedure that can guarantee arbitrarily good performance on all statistical learning problems, even if restricted to binary classification tasks.8 A more subtle answer involves characterizing the problems on which abstract learning algorithms such as empirical risk minimization (ERM) yield tight bounds on tractable criteria of interest (e.g., the expected loss).9 Even more difficult is refining our understanding of the learning problems on which concrete algorithms used in practice can be reliably expected to perform well.10 ", "page_idx": 14}, {"type": "text", "text": "For conceptual grounding, we make use of the two questions, QP (the \u201cproperty\u201d question) and QA (the \u201calgorithm\u201d question), particularly within the context of non-linear models such as neural networks. Broadly speaking, in the machine learning literature over the past three decades, most answers to the property question $\\mathbb{Q P}$ come in the form of quantifying some notion of \u201csimplicity,\u201d a property of candidate $w$ . It goes without saying that the underlying heuristic is that all else equal (at training time), a \u201ccomplex\u201d candidate seems intuitively less likely to perform well at test time.11 As for the algorithm question, there are numerous \u201cworkhorse\u201d procedures of machine learning that are computationally convenient, have high-quality software available, and tend to generalize very well in practice, providing a partial answer to $\\mathtt{Q A}$ . That said, the design principles underlying these procedures are often only very loosely related to the properties that satisfy QP.12 With this in mind, a large body of research can be understood as trying to develop new connections between answers to QP and $\\mathtt{Q A}$ , either through post hoc analysis using existing concepts, or by introducing new properties and deriving algorithms in a more unified fashion. ", "page_idx": 14}, {"type": "text", "text": "A.2 Notions of model complexity ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Model complexity is a concept that has a long history in the context of statistical model selection (Claeskens and Hjort, 2008), with well-established ties to information theory (Kullback, 1968). Assuming the quality of \u201cfti\u201d is measured using negative log-likelihood, the second derivative of this objective function (Hessian matrix in multi-dimensional case) is known as the Fisher information (matrix).13 In the context of neural networks, it is common to use their outputs to model probabilities (Denker and LeCun, 1990), and thus at least conceptually, much of the existing statistical methodology can be inherited. For early work in the context of backprop-driven neural networks, MacKay (1992) looks at designing objective criteria for comparing and choosing between models (including norm regularization parameters). MacKay introduces a form of Bayesian \u201cevidence\u201d for candidate models using a Gaussian approximation that requires evaluating the (inverse) Hessian of the base objective function. More generally, the Hessian describes the curvature of the objective function, and is closely related to geometric notions of \u201cflat regions\u201d on the surface induced by the objective function (Goodfellow et al., 2014; Li et al., 2018). ", "page_idx": 14}, {"type": "text", "text": "The notion of model complexity has also played an central role in statistical learning theory. It has long been known that even when the number of parameters far outnumber the number of training samples, a small weight norm can be used to guarantee off-sample generalization for empirical risk minimizers (Bartlett, 1998). Of course, due to the high expressive power of neural network models, even with strong weight regularization it is possible to perfectly fti random labels (Zhang et al., 2017), leading to a gap between the test error (chance level) and training error (zero) that is methodologically unsatisfactory. This has motivated a variety of new approaches to measure off-sample generalization (Jiang et al., 2020), as well as to quantify model complexity, such as the degree to which a model can be meaningfully compressed (Arora et al., 2018). ", "page_idx": 15}, {"type": "text", "text": "The notion of \u201cflat minima\u201d is seen in the early work of Hochreiter and Schmidhuber (1994, 1997), which considers both how to measure sharpness, and heuristics for actually finding candidates in flat regions. The basic underlying notion is that of measuring \u201cvolume,\u201d namely the idea that a \u201cflat\u201d point is one from which we need to go far in most (if not all) directions for the objective function to increase a certain fixed amount. See more recent work by Wu et al. (2017) for related notions of volume in this context. These notions of sharpness are intimately related to properties of the Hessian matrix of the underlying objective function, even when the loss is not based on negative log-likelihood, and an active line of research is centered around the eigenvalue distribution of this Hessian. See for example Chaudhari et al. (2017) and Karakida et al. (2019) for representative work. For sufficiently \u201cregular\u201d models, the determinant of the Fisher information matrix plays a central role in the complexity term used to implement the minimum description length (MDL) principle (Gr\u00fcnwald, 2007); see also early work from Hinton and van Camp (1993) and more recent work by Jia and Su (2020) in the context of neural networks. More generally, however, many neural networks do not satisfy these regularity conditions, and new technical innovations based on the Fisher information have been explored to bridge this gap in recent years (Sun and Nielsen, 2021). ", "page_idx": 15}, {"type": "text", "text": "A.3 Algorithms that generalize well ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The empirical effectiveness of deep learning goes well beyond what we would expect based purely on learning theoretical insights (Sejnowski, 2020). This success is driven by a handful of workhorse stochastic gradient-based solvers (Schmidt et al., 2021), often coupled with explicit norm-based regularization and a number of techniques used to stabilize learning and effectively constrain the model candidate which is selected by the learning algorithm.14 A rich line of research has developed over the past decade looking at why a certain algorithmic \u201crecipe\u201d tends to generalize well. The tendency for stochastic gradient descent to \u201cescape\u201d from regions near undesirable critical points is one key theme; see Xie et al. (2020) for example. For influential work on relating sharpness, minibatch size, and (weight) norms to off-sample generalization, see Keskar et al. (2017) and Neyshabur et al. (2017). In both papers, the notion of measuring sharpness by a worst-case perturbation appears, and this is pursued even further by Foret et al. (2021) in the well-known sharpness-aware minimization (SAM) algorithm, and extensions due to Kwon et al. (2021) and Zhao et al. (2022). These algorithms, as well as the Fisher information-based procedure of Jia and Su (2020), all involve a forward-difference implementation of explicit gradient regularization (using squared Euclidean norm), and recent work from Karakida et al. (2023) compares this approach with that of direct back-propagation approach. Barrett and Dherin (2021) look at both implicit and explicit gradient regularization. The implicit side contrasts the path of \u201ccontinuous\u201d gradient-based updates with the \u201cdiscrete\u201d updates made in practice (continuous/discrete with respect to time), saying that the discrete updates, even when computed based on an unregularized objective function, tend to move closer to the (continuous) path of a regularized objective, where regularization is in terms of the (squared) gradient norms. Inspired by this finding, they also consider explicit GR in the same way; see also Smith et al. (2021). ", "page_idx": 15}, {"type": "text", "text": "B Technical appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 More details on Flooding and sharpness ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "When the empirical risk goes below the threshold $\\theta$ , the Flooding update (4) attempts to push it back up above $\\theta$ . Consider the case in which this occurs in a single step, i.e., the situation in which at some step $t$ , the pair of sequential iterates $(w_{t},w_{t+1})$ satisfy the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{R}_{n}(w_{t})<\\theta\\mathrm{~and~}\\mathsf{R}_{n}(w_{t+1})>\\theta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When condition (14) holds, some basic algebra immediately shows us that running two iterations of the Flooding update (4) yields the equality ", "page_idx": 16}, {"type": "equation", "text": "$$\nw_{t+2}=w_{t}-\\alpha^{2}\\left(\\frac{\\nabla\\mathsf{R}_{n}(w_{t}+\\alpha\\nabla\\mathsf{R}_{n}(w_{t}))-\\nabla\\mathsf{R}_{n}(w_{t})}{\\alpha}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "telling us that the result is equivalent to running one iteration of FD descent with step size $\\alpha^{2}$ on the GR penalty $\\|\\nabla\\mathsf{R}_{n}(\\cdot)\\|^{2}$ at $w_{t}$ , using the forward FD approximation described earlier in $\\S2.2$ . To the best of our knowledge, this link was first highlighted by Karakida et al. (2023, $\\S5.1)$ ). In a sense, this is a natural complement to the strategy employed in (6); instead of tackling the GR objective $\\widetilde{\\mathsf{R}}_{n}(w;\\lambda)$ in (5) directly, the Flooding algorithm can iterate back and forth between optimizing the empirical risk and the squared gradient norm. The GR effect is thus constrained to regions in $\\mathcal{W}$ with $\\theta$ -small empirical risk, but all updates outside this region enjoy the same per-step computational complexity as vanilla GD. ", "page_idx": 16}, {"type": "text", "text": "B.2 Non-smooth loss setting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All of the analysis in $\\S3.2$ relies heavily on smoothness of the underlying loss function. Here we consider the case in which the loss itself may not even be differentiable. All we ask is that the losses be $L$ -Lipschitz on $\\mathcal{W}$ in expectation, i.e., $\\begin{array}{r}{\\mathbf{E}_{\\mu}|\\ell(w_{1};\\mathbf{Z})-\\ell(w_{2};\\mathbf{Z})|\\,\\leq\\,L\\|w_{1}-w_{2}\\|}\\end{array}$ for all $w_{1},w_{2}\\,\\in\\,\\mathcal{W}$ . As an explicit objective function, we start with $\\mathrm{S}_{\\mu}$ as given in (10), but with the understanding that $\\rho$ can\u221a actually be any 1-Lipschitz function, capturing the two special cases of interest, namely $\\rho(x)=\\sqrt{x^{2}+1}-1$ for SoftAD and $\\rho(x)=|x|$ for Flooding. Shifting our focus to function values (rather than gradients), we will also need to assume a second moment bound ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bf E}_{\\mu}\\left(\\theta+\\rho(\\ell(w;Z)-\\theta)\\right)^{2}\\leq V<\\infty\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "that holds over $w\\in\\mathcal{W}$ . With these basic assumptions in place, stationarity guarantees are available via a smooth approximation of $\\mathrm{S}_{\\mu}$ . ", "page_idx": 16}, {"type": "text", "text": "Proposition 6 (Stationarity, non-smooth case). Choosing an initial value $w_{1}\\in\\mathcal{W}_{1}$ , run the algorithm described in Proposition $^3$ , but re-defining the core gradients used for updating as ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\sf G}_{t}(w):=\\frac{d}{r}\\left(\\theta+\\rho(\\ell(w+r\\mathsf{U}_{t};\\mathsf{Z}_{t})-\\theta)\\right){\\sf U}_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(\\mathsf{U}_{1},\\mathsf{U}_{2},\\ldots)$ is a sequence of independent vectors sampled uniformly at random from the unit sphere, and $r\\,>\\,0$ sets the smoothing radius. In addition, the norm threshold is set as $\\gamma=$ $\\sqrt{d L/((1-b)r)}$ (with b unchanged). Stationarity of the resulting sequence $(w_{1},w_{2},\\ldots)$ , assumed to be in $\\mathcal{W}$ , can be controlled with probability $1-\\delta$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\lVert\\nabla\\bar{\\mathrm{{S}}}_{\\mu}(w_{t};r)\\rVert\\le\\frac{1}{T^{1/4}}\\left(\\overline{{\\mathrm{R}}}_{\\mu}(w_{1})-\\overline{{\\mathrm{R}}}_{\\mu}(w_{T+1})+\\frac{3d L}{2r}+\\frac{2d}{r}\\sqrt{V}\\left(1+C_{\\delta}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\bar{\\mathrm{S}}_{\\mu}(w;r):=\\mathrm{\\bfE}[\\mathrm{S}_{\\mu}(w+r\\mathrm{V})]$ is the $r$ -smoothed approximation of the objective $\\mathrm{S}_{\\mu}$ , with $\\vee$ distributed uniformly over the unit ball. Probability is taken over the random draw of $(\\bar{Z_{1}},\\bar{Z}_{2},\\ldots)$ and $(\\mathsf{U}_{1},\\mathsf{U}_{2},\\ldots)$ , and the confidence factor $C_{\\delta}$ matches that given in Proposition 3. ", "page_idx": 16}, {"type": "text", "text": "Remark 7 (Stationarity in the original objective). When the original objective $\\mathrm{S}_{\\mu}(\\cdot)$ is sufficiently wellbehaved, e.g., differentiable almost everywhere and Lipschitz, then stationarity guarantees in terms of the $r$ -smoothed objective $\\bar{\\mathrm{S}}_{\\mu}(\\cdot;r)$ can be easily translated into analogous guarantees for $\\widetilde{\\mathrm{R}}_{\\mu}(\\cdot)$ . In addition, recent work by Cutkosky et al. (2023) shows how a modified algorithmic approach can be used to achieve faster rates under such congenial (but still non-convex and non-smooth) conditions. ", "page_idx": 16}, {"type": "text", "text": "B.3 Additional proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 3. The machinery of Cutkosky and Mehta (2021, Thm. 2) gives us the ability to control the stationarity of sequences generated using the described procedure (norm-clipping, momentum, normalization), just assuming the \u201craw\u201d stochastic gradients (here, $\\mathsf{G}_{t}$ ) are unbiased estimators of a smooth function. As such, we just need to ensure the assumptions underlying their Theorem 2 (henceforth, CHT2) are met; the key points have already been described in the main text, so we just flil in the details here. The \u201cunbiased estimator\u201d property we refer to means that we want ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mu}\\,\\mathsf{G}_{t}(w)=\\nabla\\mathrm{S}_{\\mu}(w)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "to hold for all $w\\in\\mathcal{W}$ . Fortunately, this holds under very weak assumptions; the running assumption that $L_{\\mathrm{AD}}<\\infty$ is more than sufficient.15 In addition, finite $L_{\\mathrm{AD}}$ also implies that the objective (10) is smooth in the sense that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\nabla\\mathrm{S}_{\\mu}(w_{1})-\\nabla\\mathrm{S}_{\\mu}(w_{2})\\|\\leq L_{\\mathrm{AD}}\\|w_{1}-w_{2}\\|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $w_{1},w_{2}\\in\\mathcal{W}$ ; this is proved in $\\S B.5$ . In addition, uniform second moment bounds naturally imply pointwise bounds, so we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mu}\\|\\nabla\\ell(w;\\mathbf{Z})\\|^{2}\\leq\\mathbf{E}_{\\mu}\\left[\\operatorname*{sup}_{w\\in\\mathcal{W}}\\|\\nabla\\ell(w;\\mathbf{Z})\\|^{2}\\right]\\leq L_{\\mathrm{AD}}-L_{\\ell}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for each $w\\,\\in\\,\\mathcal{W}$ . Taken with the construction of sequence $(w_{1},w_{2},\\ldots)$ in the hypothesis, the properties (17)\u2013(19) ensure all the basic requirements of CHT2 are met (with their ${\\bf\\ddot{p}}^{,\\ast}$ at 2). Noting that we assume $\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ using the standard norm and inner product on Euclidean space, the Banach space generality in CHT2 is not needed (their \u201c $C^{\\bullet}$ and \u201c $\\dot{p}$ \u201d can be fixed to 1 and 2 respectively). For reference, the complete upper bound implied by CHT2 is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{S}_{\\mu}(w_{1})-\\mathrm{S}_{\\mu}(w_{T+1})}{T\\alpha}+\\frac{\\alpha L_{\\mathrm{AD}}}{2}+\\frac{2b\\sqrt{L_{\\mathrm{AD}}-L_{\\ell}}}{(1-b)T}+\\frac{2b\\alpha L_{\\mathrm{AD}}}{(1-b)}+2\\sqrt{(1-b)(L_{\\mathrm{AD}}-L_{\\ell})}C_{\\delta}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where for readability the coefficient in the right-most summand is defined by ", "page_idx": 17}, {"type": "equation", "text": "$$\nC_{\\delta}:=10\\log(3T/\\delta)+4\\sqrt{\\log(3T/\\delta)}+1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have simplified all the terms in CHT2 involving $\\operatorname*{max}\\{1,\\log(3T/\\delta)\\}$ , since as long as $T>0$ and $0<\\delta<1$ , we trivially have $3T/\\mathrm{e}\\geq1>\\delta$ and thus $\\log(3T/\\delta)\\geq1$ . Furthermore, their free parameters $\\,^{\\bullet}b^{\\bullet}$ (different from our $b$ ) and $\"s\"$ are both set to 1, with\u221aout loss of generality. Plugging in our settings of $\\alpha$ and $b$ to the bound in (20) and bounding $(1-1/\\sqrt{T})\\leq1$ for readability yields the desired upper bound. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 4. Here we leverage the projected sub-gradient analysis done by Davis and Drusvyatskiy (2019), in particular their Theorem 3 (henceforth, $D D T3$ ). The core of their argument relies upon a weak convexity property held by a rather large class of composite functions, namely compositions of the form $f\\ =\\ h\\circ g$ , where $g$ is smooth and $h$ is both convex and Lipschitz. Considering the non-smooth objective function ", "page_idx": 17}, {"type": "equation", "text": "$$\nw\\mapsto\\theta+\\mathbf{E}_{\\mu}|\\ell(w;\\mathbf{Z})-\\theta|,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "it can be taken as a compound function by writing $\\ensuremath{\\mathbf{E}_{\\mu}}\\,f(w;Z)$ with $f(w;z):=h(g(w;z))$ , where $g(w;z):=\\ell(w;z)$ and $\\bar{h}(x):=\\theta+|x-\\overset{\\cdot}{-}\\theta|$ . Fixing $z\\in{\\mathcal{Z}}$ for now, clearly $h$ is 1-Lipschitz and convex. By assumption, we have that $g(\\cdot;z)$ is $L_{\\ell}^{*}$ -smooth and locally Lipschitz. Then, using standard arguments, it is straightforward to show that $f(\\cdot;z)$ is $L_{\\ell}^{*}$ -weakly convex.16 Since the weak convexity parameter $L_{\\ell}^{*}$ does not depend on the arbitrary choice of $z$ , it follows that the function (21) is $L_{\\ell}^{*}$ -weakly convex. In the setting of DDT3, their \u201c $f(\\cdot)^{,}$ is $\\theta+\\mathbf{E}_{\\mu}|\\ell(w\\cdot;\\mathbf{Z})-\\theta|$ , and their \u201c $\\mathcal{X}^{\\bullet}$ is $\\mathcal{W}$ here. The bound on the expected squared stochastic gradient norms (their $\\bullet\\,L^{2}{}^{,}$ ) is our $L_{\\mathrm{AD}}-L_{\\ell}$ just as in the proof of Proposition 3. Finally, the key \u201cunbiased estimator\u201d property in this case deals with sub-differentials, namely we require that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mu}\\,\\mathsf{G}_{t}(w)\\in\\partial\\,\\mathbf{E}_{\\mu}|\\ell(w{\\cdot};\\mathsf{Z})-\\theta|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $w\\in\\mathscr{W}$ . Fortunately this basic property holds under very weak assumptions that are trivially satisfied when $L_{\\mathrm{AD}}$ is finite.17 With these facts in place, we simple apply DDT3, in particular their inequality (3.5), with their $\"\\rho'$ corresponding to our $L_{\\ell}^{*}$ here, and their $\\stackrel{*}{\\varphi_{\\lambda}}\\stackrel{,,}{\\dots}$ corresponding to our (13), with \u201c $\\acute{\\lambda}$ as our $\\beta$ . The desired bound follows by applying their result to the specified procedure over $T-1$ updates (instead of their $T$ updates). \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Proof of Proposition $6$ . To begin, under the assumptions given, the objective $\\mathrm{S}_{\\mu}$ clearly inherits the Lipschitz property that the losses have in expectation; since $\\rho$ is 1-Lipschitz, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\mathrm{S}_{\\mu}(w_{1})-\\mathrm{S}_{\\mu}(w_{2})|\\leq\\mathbf{E}_{\\mu}|\\ell(w_{1};\\mathsf{Z})-\\ell(w_{2};\\mathsf{Z})|\\leq L\\|w_{1}-w_{2}\\|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We proceed by using a standard technique for function smoothing.18 If we let $\\vee$ be a random vector distributed over the unit ball $\\{x\\in\\mathbb{R}^{d}:\\}|x||\\leq1\\}$ , then regardless of whether $\\mathrm{S}_{\\mu}(\\cdot)$ is differentiable or not, one can obtain a smooth approximation by averaging over random $r$ -length perturbations, namely ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\mathrm{S}}_{\\mu}(w;r):=\\mathbf{E}\\left[\\mathrm{S}_{\\mu}(w+r\\mathrm{V})\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A critical property of the function given in (24) is that it is differentiable and its gradient can be represented explicitly in terms of the function it is trying to smooth, namely we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla{\\bar{\\mathrm{S}}}_{\\mu}(w;r)={\\frac{d}{r}}\\,\\mathbf{E}\\left[\\mathrm{S}_{\\mu}(w+r\\mathsf{U})\\mathsf{U}\\right]={\\frac{d}{r}}\\,\\mathbf{E}\\left[(\\theta+\\mathbf{E}_{\\mu}\\,\\rho(\\ell(w+r\\mathsf{U})-\\theta))\\,\\mathsf{U}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any choice of $r>0$ and $w\\in\\mathscr{W}$ , where $\\mathsf{U}$ is uniformly distributed on the unit sphere $\\{x\\in\\mathbb{R}^{d}:$ $\\|x\\|=1\\}$ (Flaxman et al., 2004, Lem. 1).19 This means that Lipschitz properties on the original function translate to smoothness properties for the new function. Making this more explicit, using the equality (25), note that for any choice of $w_{1},w_{2}\\in\\mathcal{W}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla\\bar{\\mathrm{S}}_{\\mu}(w_{1};r)-\\nabla\\bar{\\mathrm{S}}_{\\mu}(w_{2};r)=\\frac{d}{r}\\,\\mathbf{E}\\left[\\left(\\mathrm{S}_{\\mu}(w_{1}+r\\mathsf{U})-\\mathrm{S}_{\\mu}(w_{2}+r\\mathsf{U})\\right)\\mathsf{U}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking norms and using the Lipschitz property (23) of $\\mathrm{S}_{\\mu}$ , we observe that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|\\nabla\\bar{\\mathrm{S}}_{\\mu}(w_{1};r)-\\nabla\\bar{\\mathrm{S}}_{\\mu}(w_{2};r)|\\leq\\displaystyle\\frac{d}{r}\\,\\mathbf{E}\\|U\\|\\vert\\mathrm{S}_{\\mu}(w_{1}+r\\mathsf{U})-\\mathrm{S}_{\\mu}(w_{2}+r\\mathsf{U})\\|}\\\\ {\\displaystyle\\leq\\frac{d L}{r}\\|w_{1}-w_{2}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and thus have that the smoothed function $\\bar{\\mathrm{S}}_{\\mu}(\\cdot;r)$ is $(d L/r)$ -smooth over $\\mathcal{W}$ . This means the function is analogous to the objective function (10) used in Proposition 3, except with unbiased stochastic gradients taking the form ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\sf G}_{t}(w):=\\frac{d}{r}\\left(\\theta+\\rho(\\ell(w+r\\mathsf{U}_{t};\\mathsf{Z}_{t})-\\theta)\\right){\\sf U}_{t}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $t\\geq1$ , where each $\\mathsf{U}_{t}$ is an independent copy of $\\mathsf{U}$ from (25). From this point, the remainder of the proof is basically identical to that of Proposition 3; the only remaining changes are the smoothness factor and the second moment bound. For the former, we use $d L/r$ in place of $L_{\\mathrm{AD}}$ , which also impacts the norm clipping radius $\\gamma$ . For the latter, since we are assuming $w_{t}+r\\mathsf{U}_{t}\\in\\mathcal{W}$ for each $t$ , and using the bound (16), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{E}\\|\\mathsf{G}_{t}(w)\\|^{2}\\leq\\operatorname*{sup}_{w\\in\\mathcal{W}}\\left(\\frac{d}{r}\\right)^{2}\\mathbf{E}\\|\\mathsf{U}_{t}\\|^{2}\\left(\\theta+\\rho(\\ell(w;\\mathsf{Z}_{t})-\\theta)\\right)^{2}\\leq\\left(\\frac{d}{r}\\right)^{2}V.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging in these two remaining modified factors to the bounds obtained in Proposition 3 yields the desired result. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.4 Gradient of GR objective ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "With $w\\,=\\,(w_{1},\\hdots,w_{d})\\,\\in\\,\\mathbb{R}^{d}$ , we will frequently use $\\partial_{j}$ to denote partial derivatives taken with respect to $w_{j}$ , i.e., for a differentiable function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , we write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\partial_{j}f(w):=\\operatorname*{lim}_{|a|\\to0}{\\frac{f(w_{1},\\dots,w_{j}+a,\\dots,w_{d})-f(w)}{a}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with analogous definitions for all $j=1,\\ldots,d$ . With the above notation in place, note that basic calculus gives us ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\partial_{j}\\left\\Vert\\nabla\\mathsf{R}_{n}(w)\\right\\Vert^{2}=\\sum_{k=1}^{d}\\partial_{j}\\left(\\partial_{k}\\mathsf{R}_{n}(w)\\right)^{2}=2\\sum_{k=1}^{d}\\left(\\partial_{k}\\mathsf{R}_{n}(w)\\right)\\left(\\partial_{j}\\partial_{k}\\mathsf{R}_{n}(w)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As such, the gradient takes the form ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla\\|\\nabla\\mathsf{R}_{n}(w)\\|^{2}=2\\nabla^{2}\\mathsf{R}_{n}(w)\\left(\\nabla\\mathsf{R}_{n}(w)\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\nabla^{2}{\\mathsf{R}}_{n}$ denotes the $d\\times d$ Hessian matrix of $\\mathsf{R}_{n}$ , and ${\\boldsymbol{\\nabla}}\\mathsf{R}_{n}({\\boldsymbol{w}})$ is taken as a column vector (a $d\\times1$ matrix) for the purpose of this multiplication. ", "page_idx": 19}, {"type": "text", "text": "B.5 Smoothness check ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let us consider a simple, non-stochastic example in one dimension. Letting $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ be some differentiable function, we cons\u221aider how the transformed gradient $\\phi(f(x)-\\theta)f^{\\prime}(x)$ behaves under $\\phi=\\mathrm{sign}$ and $\\phi=\\rho^{\\prime}(x)=x/\\sqrt{x^{2}+1}$ . As we have seen visually in Figure 1, the soft threshold of SoftAD makes it possible to have Lipschitz gradients, which impacts iterative optimization procedures. For arbitrary values $x_{1}$ and $x_{2}$ , taking the difference of transformed gradients using arbitrary $\\phi$ , we can write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lefteqn{\\left(f(x_{1})-\\theta\\right)f^{\\prime}(x_{1})-\\phi(f(x_{2})-\\theta)f^{\\prime}(x_{2})}}\\\\ &{=\\left(\\phi(f(x_{1})-\\theta)-\\phi(f(x_{2})-\\theta)\\right)f^{\\prime}(x_{1})+\\phi(f(x_{2})-\\theta)\\left(f^{\\prime}(x_{1})-f^{\\prime}(x_{2})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that even if $|x_{1}-x_{2}|<\\varepsilon$ for some arbitrarily small $\\varepsilon>0$ , if for example the threshold is such that $f(x_{1})<\\theta<f(x_{2})$ , then under $\\phi=\\mathrm{{sign}}$ , the difference multiplying $f^{\\prime}(x_{1})$ cannot be arbitrarily small, even if $f$ is Lipschitz. On the other hand, such a property follows easily when $\\phi=\\rho^{\\prime}$ , since $\\rho^{\\prime}$ itself is 1-Lipschitz. ", "page_idx": 19}, {"type": "text", "text": "Returning to our more general learning setup, let us denote the modified gradients concisely as $g(w;z):=\\phi(\\ell(w;z)\\!-\\!\\theta)\\nabla\\ell(w;z)$ . With random variable $Z\\sim\\mu$ , taking any two points $w_{1},w_{2}\\in\\mathcal{W}$ , based on the equality (28), the normed difference of the gradient expectations can be bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{E}_{\\mu}\\,g(w_{1};\\mathsf{Z})-\\mathbf{E}_{\\mu}\\,g(w_{2};\\mathsf{Z})\\|\\leq B_{1}+B_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $B_{1}$ and $B_{2}$ defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}:={\\bf E}_{\\mu}||\\nabla\\ell(w_{1};\\mathsf{Z})|||\\phi(\\ell(w_{1};\\mathsf{Z})-\\theta)-\\phi(\\ell(w_{2};\\mathsf{Z})-\\theta)|}\\\\ &{B_{2}:={\\bf E}_{\\mu}|\\phi(\\ell(w_{2};\\mathsf{Z})-\\theta)|||\\nabla\\ell(w_{1};\\mathsf{Z})-\\nabla\\ell(w_{2};\\mathsf{Z})||.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Bounding each of these terms is trivial when the functions $\\ell$ and $\\phi$ are smooth enough. First, note that if $\\phi$ is $L_{\\phi}$ -Lipschitz and $\\mathcal{W}$ is a convex subset of $\\mathbb{R}^{d}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}\\leq L_{\\phi}\\,\\mathbf{E}_{\\mu}\\|\\nabla\\ell(w_{1};\\mathsf{Z})\\||\\ell(w_{1};\\mathsf{Z})-\\ell(w_{2};\\mathsf{Z})|}\\\\ &{\\qquad\\leq L_{\\phi}\\|w_{1}-w_{2}\\|\\,\\mathbf{E}_{\\mu}\\|\\nabla\\ell(w_{1};\\mathsf{Z})\\|\\underbrace{\\operatorname*{sup}_{0\\leq a<1}\\|\\nabla\\ell(a w_{1}+(1-a)w_{2};\\mathsf{Z})\\|}_{0<a<1}}\\\\ &{\\qquad\\leq L_{\\phi}\\|w_{1}-w_{2}\\|\\,\\mathbf{E}_{\\mu}\\left[\\displaystyle\\operatorname*{sup}_{w\\in\\mathcal{W}}\\|\\nabla\\ell(w;\\mathsf{Z})\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "noting that the second inequality uses the mean value theorem on differentiable $\\ell(\\cdot;z)$ , applied pointwise in $z\\in{\\mathcal{Z}}$ , and the last inequality uses convexity of $\\mathcal{W}$ . This bounds $B_{1}$ . Moving on to $B_{2}$ , note that if $|\\phi(x)|$ is bounded by $B_{\\phi}$ and the losses are $L_{\\ell}$ -smooth in expectation, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{2}\\leq B_{\\phi}\\,{\\bf E}_{\\mu}\\|\\nabla\\ell(w_{1};{\\sf Z})-\\nabla\\ell(w_{2};{\\sf Z})\\|}\\\\ &{\\qquad\\leq B_{\\phi}L_{\\ell}\\|w_{1}-w_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking these new bounds together, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\mathbf{E}_{\\mu}\\,g(w_{1};\\mathcal{Z})-\\mathbf{E}_{\\mu}\\,g(w_{2};\\mathcal{Z})\\|\\leq\\left(L_{\\phi}\\,\\mathbf{E}_{\\mu}\\left[\\operatorname*{sup}_{w\\in\\mathcal{W}}\\|\\nabla\\ell(w;\\mathcal{Z})\\|^{2}\\right]+B_{\\phi}L_{\\ell}\\right)\\|w_{1}-w_{2}\\|,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "namely a Lipschitz property in expectation for the modified gradients. ", "page_idx": 19}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/7fbe3ad46c0fdd32aab4eacf72a802b75fe717c3ca01759f7e54dbc8e0cced9d.jpg", "img_caption": ["Figure 7: Synthetic dataset examples. From left to right: \u201ctwo Gaussians,\u201d \u201csinusoid,\u201d and \u201cspiral.\u201d "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Empirical appendix ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we provide additional details and results related to the empirical tests described in $\\S4$ . ", "page_idx": 20}, {"type": "text", "text": "Software and hardware All of the experiments done in this section have been implemented using PyTorch 2, utilizing three machines each using a single-GPU implementation, i.e., there is no parallelization across multiple machines or GPUs. Two units are equipped with an NVIDIA A100 (80GB), and the remaining machine uses an NVIDIA RTX 6000 Ada. We use the MLflow library for storing and retrieving metrics and experiment details. Our coding of SAM follows that of David Samuel (https://github.com/davda54/sam), which is the PyTorch implementation acknowledged in the original SAM paper of Foret et al. (2021). ", "page_idx": 20}, {"type": "text", "text": "C.1 Non-linear binary classification on the plane ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Data The three types of synthetic data that we generate here differ chiefly in the degree of nonlinearity; see Figure 7 for an example. The \u201ctwo Gaussians\u201d dataset is almost linearly separable, save for some overlap of the two distributions. The \u201csinusoid\u201d data is separated by a simple curve, easily approximated by a low-order polynomial, but the curves in the \u201cspiral\u201d data are a bit more complicated. Exact implementation details, plus historical references, are given by Ishida et al. (2020, $\\S4.1)$ . For each trial, we generate training and validation data of size 100, and test data of size 20000. All methods see the same data in each trial. ", "page_idx": 20}, {"type": "text", "text": "Model For each dataset, we use the same model, namely a simple feedforward neural network with four hidden layers, 500 units per layer, using batch normalization and ReLU activations at each layer.20 ", "page_idx": 20}, {"type": "text", "text": "Algorithms In line with the experiments we are trying to replicate, all methods (ERM, Flooding, SAM, and SoftAD) are driven by the Adam optimizer, using a fixed learning rate of 0.001, with no momentum or weight decay. All methods use the multi-class logistic loss as their base loss (i.e., nn.CrossEntropyLoss in PyTorch), and are run for 500 epochs. We use mini-batch size of 50 here, but key trends remain the same for full-batch (of size 100) runs. ", "page_idx": 20}, {"type": "text", "text": "Hyperparameter selection ERM has no hyperparameters, but all the other methods have one each. Flooding and SoftAD both have the threshold parameter $\\theta$ seen in $\\S2{-}\\S3$ , and SAM has a radius parameter (denoted $\\cdot\\rho^{,,}$ in the original paper). For each of these methods, in each trial, we select from a grid of 40 points spaced linearly between 0.01 and 2.0. Selection is based on classification accuracy on validation data. ", "page_idx": 20}, {"type": "text", "text": "C.2 Image classification from scratch ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our second set of experiments utilizes four well-known benchmark datasets for multi-class image classification. Compared to the synthetic experiments done in $\\S C.1$ , the classification task is more difficult (much larger inputs, variation within classes, more classes), and so we utilize more sophisticated neural network models to tackle the classification task. That said, as the sub-section title indicates, this training is done \u201cfrom scratch,\u201d i.e., no pre-trained models are used. ", "page_idx": 20}, {"type": "text", "text": "Data The datasets we use are all standard benchmarks in the machine learning community: CIFAR10, CIFAR-100, FashionMNIST, and SVHN. All of these datasets are collected using classes defined in the torchvision.datasets module, with raw training/test splits left as-is with default settings. As such, across all trials the test set is constant, but in each trial we randomly select $80\\%$ of the raw training data to be used for actual training, with the remaining $20\\%$ used for validation. We normalize all pixel values in the image data to the unit interval [0, 1]; this is done separately for training, validation, and testing data. ", "page_idx": 21}, {"type": "text", "text": "Models Unlike the previous sub-section, here we use different models for different data sets. Model choice essentially mirrors that of Ishida et al. (2020, $\\S4.2)$ . For FashionMNIST, we flatten each image into a vector, and use a simple feedforward neural network composed of a single hidden layer with 1000 units, batch normalization, and ReLU activation before the output transformation. For SVHN, we use ResNet-18 as implemented in torchvision.models, without any pre-trained weights. Finally, for both CIFAR-10 and CIFAR-100, we use ResNet-34 (again in torchvision.models) without pre-training. Both of the ResNet models used do not flatten the images, but rather take each RGB image as-is. ", "page_idx": 21}, {"type": "text", "text": "Algorithms Just as in $\\mathrm{\\SC.1}$ , we are testing ERM, Flooding, SAM, and SoftAD. Again we use the cross entropy loss, and run for 500 epochs. However, instead of Adam as the base optimizer, here we use vanilla SGD with a fixed step size of 0.1, and momentum parameter of 0.9. For all datasets, we use a mini-batch size of 200. All these settings match the experimental setup of Ishida et al. (2020, \u00a74.2).21 ", "page_idx": 21}, {"type": "text", "text": "Hyperparameter selection Once again we select hyperparameters for Flooding, SoftAD, and SAM from a grid of candidate values, such that the classification accuracy on validation data is maximized. Unlike $\\mathrm{\\&C.1}$ however, here we use different grids for each method. For Flooding, we follow the setup of the original paper, choosing from ten values: $\\{0.01,0.02,\\dots,0.1\\}$ . For SAM, once again we follow the original paper (their $\\S3.1\\rangle$ ), which for analogous tests utilized the set $\\left\\{0.01,\\bar{0}.02,0.05,0.1,0.2,0.5\\right\\}$ . Finally, for SoftAD we match the set size used by Flooding (i.e., ten) by taking the union of $\\{0.15,0.25,0.35,0.75\\}$ and the set used by SAM. ", "page_idx": 21}, {"type": "text", "text": "Comparison with iFlood As mentioned in Remark 1, during the review phase of this work, the iFlood method of Xie et al. (2022) was brought to our attention, and we have run additional tests analogous to those described in the preceding paragraphs, but this time comparing ERM, iFlood, and SoftAD. The results are given below in Table 3, Figures 8\u201310, and Table 4, in that order. ", "page_idx": 22}, {"type": "table", "img_path": "Y1ZsLONDI2/tmp/34dd1796329e8febb8138315bac6c2f4130a7fc61c513ed816b9cf6250949412.jpg", "table_caption": ["Table 3: Generalization gap (test - training) for trial-averaged cross entropy loss after final epoch. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/4d0def220d669fbced9b43ce8e3d2cc74451d382d02d67a93f1be7c6fe35e450.jpg", "img_caption": ["Figure 8: Trajectories over epochs for average test loss (top row) and test accuracy (bottom row). Horizontal axis is epoch number. Columns are associated with the CIFAR-10 and CIFAR-100 datasets (left to right). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/338a85bbf9ed76d7417b0db17da5eb4b68092b6cf88db354fbec4dd67e39cc68.jpg", "img_caption": ["Figure 9: Analogous to Figure 8, but with FashionMNIST and SVHN datasets. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/0c595507b980222378184135d4c24db3acf581772ff43540e52cb7bb151b886b.jpg", "img_caption": ["Figure 10: Model norm trajectories over epochs for each dataset in Figures 8\u20139. ", "Table 4: Hyperparameters selected by validation for each method (averaged over trials). Flooding and SoftAD have threshold $\\theta$ ; SAM has radius parameter. Standard deviation (over trials) is given in small-text parentheses. "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Y1ZsLONDI2/tmp/77e683ca5d2baf853cf3468ce28446342e4041b08e656a3195afa04cebf6acf4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.3 Linear binary classification ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Figure 11, we compare ERM with Flood and SoftAD run with a common threshold level of $\\theta=\\bar{0}.25$ , using the \u201ctwo Gaussians\u201d and \u201csinusoid\u201d data described in $\\mathrm{\\&C.1}$ , and a simple linear model, i.e., a feed-forward neural network with no hidden layers. Training and test sizes match those described in $\\mathrm{\\&C.1}$ . Even with a very simple linear model, it is clear that SoftAD can be used to achieve competitive accuracy at much larger loss levels. Note that in the case of \u201csinusoid,\u201d the average loss does not reach the threshold $\\theta$ , and thus Flooding is identical to ERM. These basic trends hold over a range of thresholds $\\theta$ and re-scaling parameters $\\sigma$ (i.e., using $\\phi((x-\\theta)/\\sigma)$ with $\\sigma\\neq1$ ). These trends are captured by the heatmaps given in Figure 12, where for each setting of $\\theta$ (for SoftAD and Flooding) and $\\sigma$ (for SoftAD only), we generate a fresh dataset. Clearly taking the threshold level far too high leads to arbitrarily bad performance, but below a certain level, similar performance is observed over a wide range of values. It is interesting to note how while test loss changes in a rather predictable continuous fashion as a function of $\\theta$ , the test accuracy drops in a much sharper manner when $\\theta$ is set too high in the case of SoftAD, whereas this drop is smoother in the case of Flooding. That said, these trends are only within the confines of this very simple linear model example using full batch, and tend to change (even with the same model) as we modify the mini-batch size. ", "page_idx": 24}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/1cd1a599512e813dcd80375971fb798bdd92dc0309fdde46958453491d132511.jpg", "img_caption": ["Linear binary classification (gaussian, trial 0) ", "Linear binary classification (sinusoid, trial 0) "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/f34a9388cdc15ab05f47bd5f3a212aa062adc2a9a56c0e71cb3405dd32bb047f.jpg", "img_caption": ["Figure 11: Average cross entropy loss and accuracy over epochs (full batch) for each method. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "Y1ZsLONDI2/tmp/b87980909b1218f34978f52c5b23a747530003f7994516e001ca13c330587ff5.jpg", "img_caption": ["Figure 12: Test loss and accuracy heatmaps for Flooding and SoftAD, depending on threshold level (denoted \u201ctheta\u201d) and scaling parameter (denoted \u201csigma\u201d). "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We are very clear that if our interest is purely in improving the test accuracy, the existing Flooding algorithm tends to work quite well. Our contributions are set in the context of learning tasks where accuracy is important, but average loss and/or model norms are also of importance; our new method SoftAD is shown to achieve an appealing balance between these metrics compared with Flooding (and ERM/SAM), as we claim. Furthermore, we make no claims that our method can circumvent the issue of how to set the threshold level $\\theta$ ; as we discuss in $\\S5$ , this is a broad question that underlies both Flooding and SoftAD, and the pursuit of an answer is stated as future work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: As discussed above under \u201cClaims,\u201d we emphasize the strong points of our method (smoothness, stability, good balance across loss/accuracy/norms), but are completely open about the fact that our method, just as with Flooding, still has a free parameter $\\theta$ whose setting is by no means trivial (though empirically, simple validation is shown to work well). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 26}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: All assumptions are stated clearly, either in the main text or in the body of propositions/theorems, and complete proofs are given in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: All experimental results can be recovered using code and Jupyter notebooks that we have already prepared in a GitHub repository. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See the above answer; code is provided on GitHub (after the review phase). As for data, all datasets are public, clearly described, and accessible via the outlets we have mentioned. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In the main body of the paper we give a broad summary, but complete details are given in $\\S C$ , a fact that we clearly mention in the main body. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: For results with relatively large discrepancy between methods (namely hyperparameter selection results in Table 2), we give standard deviation (over randomized trials) in addition to averages. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: At the start of $\\S C$ , we describe the hardware used in our experimental setup.   \nActual computation time is not a key factor in our results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Datasets and software are all credited, both within the paper itself and within our code made available on GitHub. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The only relevant new \u201cassets\u201d are code for re-creating the experiments, and this is all well-documented on our GitHub repository. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]