[{"figure_path": "Y1ZsLONDI2/figures/figures_4_1.jpg", "caption": "Figure 1: The left-most figure simply plots the graph of f(x) = x\u00b2/2 over x \u2208 [\u22122,2]. The two remaining figures show plots of the graphs of f'(x) = x (dashed black line) and $((f(x) \u2013 \u03b8)/\u03c3)f'(x) for the same range of x values, with colors corresponding to modified values of \u03c3 (middle plot; \u03b8 = 0.5 fixed) and \u03b8 (right-most plot; \u03c3 = 1.0 fixed) respectively. Thick dotted lines are $ = sign, thin solid lines are $ = \u03c1'.", "description": "This figure compares the gradients of the quadratic function f(x) = x\u00b2/2 with its modified version, where the modification involves scaling and shifting by parameters \u03b8 and \u03c3. The leftmost plot shows the quadratic function. The middle plot compares the gradients for different values of \u03c3, while keeping \u03b8 fixed. The rightmost plot compares the gradients for different values of \u03b8, while keeping \u03c3 fixed. In each case, the effect of the SoftAD's smooth truncation function \u03c6(x) = x/\u221ax\u00b2+1 is demonstrated, showing how it contrasts with the sharp thresholding used in the original Flooding algorithm.", "section": "Initial comparison with Flooding"}, {"figure_path": "Y1ZsLONDI2/figures/figures_4_2.jpg", "caption": "Figure 2: Gradient descent on the quadratic example from Figure 1. The horizontal axis denotes iteration number, and we plot sequences of iterates (xt) and function values (f(xt)) for each method. Here \u201cGD\u201d denotes vanilla gradient descent, with \u201cFlood\u201d and \u201cSoftAD\u201d corresponding to (4) and (8) respectively. Step size is \u03b1 = 0.1.", "description": "This figure compares the performance of vanilla gradient descent (GD), Flooding, and SoftAD on a simple quadratic function.  The x-axis represents the iteration number, and the y-axes show the iterates (x<sub>t</sub>) and function values (f(x<sub>t</sub>)) respectively.  The plot demonstrates that SoftAD converges more smoothly than Flooding, which oscillates near the minimum, while GD approaches the minimum directly.", "section": "Initial comparison with Flooding"}, {"figure_path": "Y1ZsLONDI2/figures/figures_5_1.jpg", "caption": "Figure 3: Left: We randomly sample n = 8 points (black dots) from the 2D Gaussian distribution, zero mean, zero correlations, with standard deviation 2\u221a2 in each coordinate. The two candidates are denoted by square-shaped points (red and green), and the minimizer of Rn is given by a gold star. Center: The Flooding updates (colored arrows) via (4) for each candidate. Right: Analogous SoftAD update vectors via (8), with per-point transformed gradients (semi-transparent arrows) for reference. Throughout, we have fixed \u03b8 = 1.5 \u00d7 minw Rn(w) and \u03b1 = 0.75.", "description": "This figure compares the update directions of Flooding and SoftAD.  In the left panel, eight data points are sampled from a 2D Gaussian distribution, with two candidate points (red and green squares) and the empirical risk minimizer (gold star). The center panel shows the Flooding update vectors for each candidate, illustrating a direct movement towards or away from the minimizer. The right panel demonstrates SoftAD updates, which involve per-point update directions (semi-transparent arrows) that are weighted and aggregated, resulting in a smoother and more nuanced update compared to Flooding.", "section": "Comparison of convergence properties"}, {"figure_path": "Y1ZsLONDI2/figures/figures_8_1.jpg", "caption": "Figure 4: Trajectories over epochs for average test loss (top row) and test accuracy (bottom row). Horizontal axis is epoch number. Columns are associated with the CIFAR-10 and CIFAR-100 datasets (left to right).", "description": "This figure shows the training curves for test loss and accuracy using four different methods: ERM, Flooding, SAM, and SoftAD.  The curves are plotted against the number of training epochs. The figure is split into two columns, one for the CIFAR-10 dataset and the other for CIFAR-100 dataset. Each column has two subplots, the upper one for loss and the lower one for accuracy. The plots visualize the performance of each method over the course of training, highlighting the differences in their convergence behavior and final performance.", "section": "4 Empirical study"}, {"figure_path": "Y1ZsLONDI2/figures/figures_8_2.jpg", "caption": "Figure 10: Model norm trajectories over epochs for each dataset in Figures 8-9.", "description": "This figure shows the trajectories of model norms over epochs for each dataset used in Figures 8 and 9.  The model norm is calculated as the L2 norm of all model parameters concatenated into a single vector. The plots show the average model norm across multiple trials for each method (ERM, iFlood, SoftAD).  The x-axis represents the epoch number, and the y-axis represents the L2 norm of model parameters.", "section": "C.2 Image classification from scratch"}, {"figure_path": "Y1ZsLONDI2/figures/figures_9_1.jpg", "caption": "Figure 10: Model norm trajectories over epochs for each dataset in Figures 8\u20139.", "description": "This figure shows the trajectory of model norm (L2 norm of all model parameters) over training epochs for four different datasets (CIFAR-10, CIFAR-100, FashionMNIST, and SVHN) using three methods: ERM, iFlood, and SoftAD.  Each line represents one of these methods for a particular dataset. The figure helps to visualize the model norm growth and compare the methods' effect on model complexity over time. In general, the results show that SoftAD helps to maintain smaller model norms compared to other methods.", "section": "C.2 Image classification from scratch"}, {"figure_path": "Y1ZsLONDI2/figures/figures_20_1.jpg", "caption": "Figure 7: Synthetic dataset examples. From left to right: \u201ctwo Gaussians\u201d, \u201csinusoid\u201d, and \u201cspiral\u201d.", "description": "This figure shows three synthetic datasets used for binary classification experiments in the paper. Each plot represents a 2D dataset with two classes, shown as circles and crosses. The 'two Gaussians' dataset shows two overlapping Gaussian distributions. The 'sinusoid' dataset shows data points separated by a sinusoidal curve. The 'spiral' dataset displays data points forming two intertwined spirals.", "section": "4.1 Overview of experiments"}, {"figure_path": "Y1ZsLONDI2/figures/figures_22_1.jpg", "caption": "Figure 8: Trajectories over epochs for average test loss (top row) and test accuracy (bottom row). Horizontal axis is epoch number. Columns are associated with the CIFAR-10 and CIFAR-100 datasets (left to right).", "description": "This figure shows the training curves for test loss and accuracy of ERM, iFlood, and SoftAD methods on CIFAR-10 and CIFAR-100 datasets.  The top row displays the test loss for each method over training epochs, while the bottom row shows the corresponding test accuracy.  It visually demonstrates the performance of each method throughout the training process on these two benchmark image classification datasets.", "section": "4.2 Main findings"}, {"figure_path": "Y1ZsLONDI2/figures/figures_22_2.jpg", "caption": "Figure 9: Analogous to Figure 8, but with FashionMNIST and SVHN datasets.", "description": "This figure displays the training trajectories for average test loss and test accuracy over epochs for FashionMNIST and SVHN datasets.  It compares four different methods: ERM, iFlood, and SoftAD.  The plots show how the training loss and accuracy evolve over time for each method on these datasets, allowing for a visual comparison of their performance characteristics.", "section": "Comparison with iFlood"}, {"figure_path": "Y1ZsLONDI2/figures/figures_23_1.jpg", "caption": "Figure 6: Model norm trajectories over epochs for each dataset in Figures 4\u20135.", "description": "This figure shows the L2 norm of all model parameters (neural network weights) concatenated into a single vector for each dataset (CIFAR-10, CIFAR-100, FashionMNIST, SVHN) over epochs.  The norms are averaged over multiple trials for each method (ERM, Flooding, SAM, SoftAD).  The figure illustrates that SoftAD consistently maintains smaller model norms compared to the other methods, even without explicit regularization.", "section": "4 Empirical study"}, {"figure_path": "Y1ZsLONDI2/figures/figures_24_1.jpg", "caption": "Figure 11: Average cross entropy loss and accuracy over epochs (full batch) for each method.", "description": "This figure compares the performance of ERM, Flooding, and SoftAD on two datasets using a linear model.  The left panel shows the average cross-entropy loss over epochs, while the right panel displays the test accuracy.  The results indicate that SoftAD can achieve competitive accuracy even at significantly higher loss values than ERM.", "section": "C.3 Linear binary classification"}, {"figure_path": "Y1ZsLONDI2/figures/figures_24_2.jpg", "caption": "Figure 11: Average cross-entropy loss and accuracy over epochs (full batch) for each method.", "description": "This figure compares the performance of ERM, Flooding, and SoftAD on two datasets using a simple linear model.  It shows the average cross-entropy loss and accuracy over 200 epochs.  The results illustrate how SoftAD achieves competitive accuracy while maintaining lower average loss compared to ERM and Flooding.", "section": "C.3 Linear binary classification"}, {"figure_path": "Y1ZsLONDI2/figures/figures_25_1.jpg", "caption": "Figure 12: Test loss and accuracy heatmaps for Flooding and SoftAD, depending on threshold level (denoted \"theta\") and scaling parameter (denoted \"sigma\").", "description": "This figure shows heatmaps of test loss and accuracy for both Flooding and SoftAD methods. The heatmaps visualize how the test loss and accuracy change depending on different values of two hyperparameters: the threshold (\u03b8) and the scaling parameter (\u03c3).  Each cell in the heatmap represents a combination of \u03b8 and \u03c3 values, and its color intensity indicates the corresponding test loss or accuracy. This visualization helps to understand the impact of the hyperparameters on the performance of the two methods. The heatmaps are generated separately for two synthetic datasets: \"gaussian\" and \"sinusoid\".", "section": "C Empirical appendix"}]