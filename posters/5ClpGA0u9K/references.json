{"references": [{"fullname_first_author": "G. An", "paper_title": "Direct Preference-based Policy Optimization without Reward Modeling", "publication_date": "2023-12-01", "reason": "This paper proposes a novel preference-based policy optimization algorithm without reward modeling, which is closely related to the ERA algorithm proposed in the current paper."}, {"fullname_first_author": "M. G. Azar", "paper_title": "A General Theoretical Paradigm to Understand Learning from Human Preferences", "publication_date": "2023-11-01", "reason": "This paper provides a general theoretical framework for understanding learning from human preferences, which is relevant to the theoretical analysis of the ERA algorithm."}, {"fullname_first_author": "Y. Bai", "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "publication_date": "2022-04-01", "reason": "This paper introduces reinforcement learning from human feedback (RLHF) as a method for aligning large language models, which is an important context for the ERA algorithm."}, {"fullname_first_author": "R. Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-01", "reason": "This paper introduces direct preference optimization (DPO), a method for aligning language models without explicit reward modeling, which is compared against the ERA algorithm in the current paper."}, {"fullname_first_author": "J. Schulman", "paper_title": "Proximal Policy Optimization Algorithms", "publication_date": "2017-08-01", "reason": "This paper introduces the Proximal Policy Optimization (PPO) algorithm, a reinforcement learning algorithm that is closely related to the ERA algorithm, and its theoretical properties are compared in the current paper."}]}