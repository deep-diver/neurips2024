[{"type": "text", "text": "Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1   \n2   \n3   \n4   \n5   \n6   \n7   \n8   \n9   \n10   \n11   \n12   \n13   \n14   \n15   \n16   \n17   \n18   \n19   \n20   \n21 ", "page_idx": 0}, {"type": "text", "text": "Searching through chemical space is an exceptionally challenging problem because the number of possible molecules grows combinatorially with the number of atoms. Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties. This molecular search problem closely resembles the \u201calignment\u201d problem for large language models, though for many chemical tasks we have a specific and easily evaluable reward function. Here, we introduce an algorithm called energy rank alignment (ERA) that leverages an explicit reward function to produce a gradient-based objective that we use to optimize autoregressive policies. We show theoretically that this algorithm is closely related to proximal policy optimization (PPO) and direct preference optimization (DPO), but has a minimizer that converges to an ideal Gibbs-Boltzmann distribution with the reward playing the role of an energy function. Furthermore, this algorithm is highly scalable, does not require reinforcement learning, and performs well relative to DPO when the number of preference observations per pairing is small. We deploy this approach to align molecular transformers to generate molecules with externally specified properties and find that it does so robustly, searching through diverse parts of chemical space. While our focus here is on chemical search, we also obtain excellent results on an AI supervised task for LLM alignment, showing that the method is scalable and general. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 Large language models (LLMs) are trained on large corpora of text to autoregressively generate   \n24 outputs. These models strongly reflect the distribution of the data on which they are trained [21],   \n25 and controlling the outputs to reflect externally imposed preferences is an increasingly important   \n26 challenge for deployment. The aforementioned task, often called \u201calignment\u201d, requires either careful   \n27 curation of training data or large sets of human preference data\u2014both options are labor-intensive [9].   \n28 Reinforcement learning from human feedback (RLHF), a family of algorithms that employs these   \n29 human preference datasets, has been widely employed to align instruction and chat models [21, 5],   \n30 but it is both expensive to acquire the training data and difficult to carry out in practice [9]. Recent   \n31 algorithmic developments, such as direct preference optimization (DPO) [25], simplify the alignment   \n32 framework by making the reward function implicit, but still require human preference data. While   \n33 these algorithms succeed in constraining outputs, many \u201calignment\u201d-like tasks require evaluation that   \n34 would be difficult for human evaluators.   \n35 Generative sampling problems seeking to optimize a reward are common in chemistry, where   \n36 comparing small molecules using a particular functional assay or computationally accessible property   \n37 is often far easier than searching chemical space to identify novel compounds. Recent efforts to build   \n38 large, domain-specific models for chemistry [10] have shown promising performance on both property   \n39 prediction and reaction prediction tasks. Nevertheless, just as with LLMs, leveraging these models   \n40 for molecule optimization requires first guiding \u201cunaligned\u201d models to favor important properties   \n41 like synthetic accessibility or solubility. Here, we seek to productively search chemical space using   \n42 transformers by introducing a new preference optimization algorithm, which we call energy rank   \n43 alignment.   \n44 Our contribution: We formulate a generic alignment algorithm that we call Energy Rank Alignment   \n45 or ERA that leverages an explicit reward function to guide autoregressive sampling while targeting   \n46 specific properties or preferences. Unlike reward maximization in RL-based algorithms, the policy   \n47 that minimizes our objective is designed to sample fluctuations around a maximal reward value to   \n48 promote sample diversity. Our algorithm enables direct gradient-based optimization of a policy to   \n49 match the ideal preference distribution and converges asymptotically to an optimal distribution with   \n50 tuneable entropy and controllable regularization, which we show theoretically. The minimizers of our   \n51 objective are closely related to the minimizer of PPO and DPO, but we have more direct control over   \n52 the influence of the regularization relative to fluctuations around the maximum reward. In numerical   \n53 experiments, we demonstrate that this algorithm successfully aligns a molecule transformer model to   \n54 identify a highly diverse set of chemicals with properties favored by our choice of reward. Finally, we   \n55 also show that we obtain competitive performance with ERA on benchmark LLM alignment tasks,   \n56 but emphasize that the chemical applications are the main focus of this paper. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/07fba75ad5492c01ec400ef6d0c4205343a5f0535d2ee66d8fd32360e467f81c.jpg", "img_caption": ["Figure 1: Energy rank alignment (ERA) enables targeting low-energy, high-reward regions with controllable fluctuations. Optimal policy approaches Boltzmann distribution with low regularization $(\\gamma\\rightarrow0)$ ) and reference policy with high regularization $(\\gamma\\rightarrow\\infty)$ (left). Aligned models can be used to sample molecules with desired chemical properties (right). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "57 1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "58 Inverse molecular design tasks have a long history [17] and many recent works have sought to apply   \n59 machine learning to facilitate this difficult search problem [27, 12, 13]. While reinforcement learning   \n60 has proved a popular strategy for molecular optimization [39, 27], several recent studies have sought   \n61 to use transformers [34] trained on large databases of molecules represented with the text-based   \n62 SMILES syntax [10, 30, 35, 4] for such tasks. Schwaller et al. [31] utilized an atom-wise tokenization,   \n63 which we also employ, to train a transformer for the downstream task of reaction prediction. These   \n64 \u201cchemical language models\u201d have been studied for applications on downstream tasks, including   \n65 property prediction [4, 10] and reaction prediction [23, 30].   \n66 Building scalable strategies for alignment has attracted enormous attention because of the high cost   \n67 and complexity of constraining LLM outputs. Much of the current paradigm is built on reinforcement   \n68 learning from human feedback (RLHF) [21]. Within this framework, human preferences provided in   \n69 the form of pairwise rankings are first used to train a reward model, and subsequently that reward   \n70 model is used to optimize a policy using, for example, proximal policy optimization (PPO) [29].   \n71 Rafailov et al. [25] demonstrated that the reward model can be treated implicitly using a scheme   \n72 that maximizes the likelihood of the preferences given an offline dataset. Because this approach   \n73 does not require training a reward model, it has been named Direct Preference Optimization (DPO).   \n74 Our work differs from both strategies; first, unlike RLHF, we do not employ reinforcement learning   \n75 and instead develop an explicit, gradient-based objective for the optimal policy. Secondly, unlike   \n76 DPO, we leverage an explicit reward function and add regularization transparently, both of which   \n77 help to avoid greedy policies [3]. However, like both approaches, we assume that the Bradley-Terry   \n78 model [7] of preference data is appropriate for the underlying target distribution.   \n79 Many recent works have built upon the ideas of RLHF and DPO, including studies on the effect   \n80 of point-wise sampling of preference distributions [3], investigations into the theoretical basis for   \n81 contrastive methods for unlearning target datasets [38], and alternatives to the Bradley-Terry pairwise   \n82 preference model [20, 2]. One recent study explores alignment in the context of inverse molecular   \n83 design: Park et al. [22] applies DPO to SMILES generators to increase the probability of activity   \n84 for generated compounds against a drug target. However, they indicate that many preferences in   \n85 chemistry are expressed as continuous signals, which is not suitable for DPO. Overcoming this   \n86 limitation while maintaining the advantages of a direct gradient-based policy optimization strategy is   \n87 a central goal of our current work. Our analysis and methodology directly addresses issues related   \n88 to point-wise sampling because the explicit reward function eliminates overly greedy assignments   \n89 of preference probabilities. Indeed, as discussed in Sec. 4, we see that DPO mode collapses where   \n90 ERA shifts the policy towards the target distribution. While non-transitive preferences may arise   \n91 in some settings, leading to a breakdown of the Bradley-Terry preference distribution model, by   \n92 construction our target rewards are determined by quantitative evaluations of properties, and are   \n93 therefore transitive. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "94 2 Energy rank alignment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "95 A policy is a conditional probability distribution $\\pi(\\cdot|{\\pmb x}):\\mathbb{y}\\rightarrow\\mathbb{R}$ ; we generate an output $\\textit{\\textbf{y}}$ from   \n96 prompt $\\textbf{\\em x}$ . The spaces $\\boldsymbol{\\wp}$ and $\\mathcal{X}$ are discrete and finite, corresponding to sequences of tokenized   \n97 outputs of the model with a maximum length. In alignment tasks, we begin with a pre-trained   \n98 reference policy $\\pi_{\\mathrm{ref}}$ and seek to optimize a parametric, trainable policy $\\pi_{\\theta}$ to adapt the conditional   \n99 sampling for a particular task or constraint.   \n100 Consider a prompt $x\\in\\mathcal{X}$ and model outputs $\\pmb{y},\\pmb{y}^{\\prime}\\in\\mathcal{Y}$ and a collection of preferences $\\begin{array}{r}{\\mathcal{D}=\\{(\\pmb{y}_{i}\\succ}\\end{array}$   \n101 $\\pmb{y}_{i}^{\\prime};\\pmb{x}_{i})\\}_{i=1}^{n}$ ; the notation $\\succ$ indicates that $\\pmb{y}_{i}$ is preferred to $\\pmb{y}_{i}^{\\prime}$ . The conditional probability that   \n102 $\\pmb{y}\\succ\\pmb{y}^{\\prime}$ given $\\textbf{\\em x}$ can be modeled as a pairwise Boltzmann ranking within the Bradley-Terry model,   \n103 i.e., ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\pmb{y}\\succ\\pmb{y}^{\\prime}|\\pmb{x})=\\frac{e^{-\\beta U(\\pmb{x},\\pmb{y})}}{e^{-\\beta U(\\pmb{x},\\pmb{y})}+e^{-\\beta U(\\pmb{x},\\pmb{y}^{\\prime})}}\\equiv\\sigma\\big(\\beta U(\\pmb{x},\\pmb{y}^{\\prime})-\\beta U(\\pmb{x},\\pmb{y})\\big).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "104 Here $\\beta\\,>\\,0$ is a constant, $\\sigma(x)\\,=\\,(1+e^{-x})^{-1}$ and we refer to $U:\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ as an energy   \n105 function to make clear the connection to statistical physics, but it is the negative reward within the RL   \n106 framework for alignment. ", "page_idx": 2}, {"type": "text", "text": "107 To impose the preferences we minimize the objective ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ(\\pi)=\\mathbb{E}_{x\\sim\\nu}\\left[\\int U(\\pmb{x},y)\\mathrm{d}\\pi(y|\\pmb{x})+\\beta^{-1}\\int(1+\\gamma)\\log\\pi(y|\\pmb{x})-\\gamma\\log(\\pi_{\\mathrm{ref}}(y|\\pmb{x}))\\mathrm{d}\\pi(y|\\pmb{x})\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "108 where $\\beta^{-1}$ is a parameter controlling the magnitude of the entropic term, $\\gamma$ sets the scale of the   \n109 Kullback-Leibler regularization compared with the energy term, and $\\nu$ is a probability distribution   \n110 over the prompts $\\nu\\in\\mathcal{P}(\\mathcal{X})$ . A proximal scheme for gradient descent on this objective corresponds   \n111 to a gradient flow on $J$ [28, 19]; the functional can be viewed as a free energy, and the corresponding   \n112 flow is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\partial_{t}\\pi_{t}=\\nabla\\cdot\\left(\\pi_{t}\\nabla\\delta_{\\pi}J[\\pi_{t}]\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "113 and $\\delta_{\\pi}$ denotes the Fr\u00e9chet derivative with respect to $\\pi$ . Assuming that $\\pi_{0}$ has full support on $\\mathcal X\\times\\mathcal X$ ,   \n114 the optimization converges asymptotically to stationary policy which satisfies ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla\\delta_{\\pi}J[\\pi_{\\star}]=0\\;\\Longleftrightarrow\\;\\pi_{\\star}\\propto e^{-\\frac{\\beta}{1+\\gamma}U+\\frac{\\gamma}{\\gamma+1}\\log\\pi_{\\mathrm{ref}}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "115 and this minimizer is globally optimal. In the context of LLM alignment, a representation of the   \n116 energy function $U:\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ is learned as a \u201creward model\u201d, though we also consider tasks   \n117 in which $U$ is an easily evaluated function of the pair $\\left({\\pmb x},{\\pmb y}\\right)$ . The optimal distribution $\\pi_{\\star}$ is a   \n118 Gibbs-Boltzmann measure ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{\\star}({\\pmb y}|{\\pmb x})=Z^{-1}({\\pmb x})\\exp\\left[-\\frac{\\beta}{1+\\gamma}\\big(U({\\pmb x},{\\pmb y})-\\beta^{-1}\\gamma\\log\\pi_{\\mathrm{ref}}({\\pmb y}|{\\pmb x})\\big)\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "119 where $Z(x)$ is the $\\textbf{\\em x}$ -dependent normalization constant. This expression makes clear the effect of $\\beta$ :   \n120 when $\\beta\\to\\infty$ (low temperature), the reward dominates and fluctuations around the maximal reward   \n121 are small, which could lead to \u201cmode-seeking\u201d; when $\\beta\\rightarrow0$ (high physical temperature) fluctuations   \n122 around the maximal reward increase and the regularization term favors proximity to $\\pi_{\\mathrm{ref}}$ . Similarly,   \n123 $\\gamma\\to0$ recovers a Gibbs-Boltzmann distribution proportional to $e^{-\\beta U}$ at inverse temperature $\\beta$ , while   \n124 $\\gamma\\to\\infty$ is dominated by the reference policy.   \n125 Loss functions for $\\pi_{\\theta}$ : Proximal Policy Optimization (PPO) optimizes an indirect, proximal   \n126 objective to minimize an objective closely related to (2) (cf. Appendix A). Direct Preference   \n127 Optimization (DPO) treats the negative reward function $U$ implicitly and directly maximizes the   \n128 likelihood of $p(\\pmb{y}\\succ\\pmb{y}^{\\prime}|\\pmb{x})$ . Our objectives differ from both approaches: like DPO, we directly   \n129 optimize the policy using an explicit, gradient-based objective, but, in contrast, we use a reward   \n130 function directly in our objective. The losses we build are thus amenable to both offline (samples   \n131 from $\\pi_{\\mathrm{ref.}}$ ) and online (samples from $\\pi_{\\theta}$ ) policy alignment, as explained below. Choosing to optimize   \n132 the objective online has been shown to have important consequences on performance [32], though we   \n133 focus here on the setting where samples are drawn offline.   \n134 We directly optimize the Kullback-Leibler divergence between the entropy-regularized preference   \n135 distribution $p_{\\gamma}(\\pmb{y}\\succ\\pmb{y}^{\\prime}|\\pmb{x})$ and the corresponding parametric preference distribution $p_{\\theta}(\\boldsymbol{y}\\succ\\boldsymbol{y}^{\\prime}|\\boldsymbol{x})$ .   \n136 Explicitly, using the fact that conditional preference distribution is normalized, we obtain ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}^{(y,y^{\\prime})}(p_{\\gamma}|p_{\\theta})=p_{\\gamma}(y\\succ y^{\\prime}|x)\\log\\frac{p_{\\gamma}(y\\succ y^{\\prime}|x)}{p_{\\theta}(y\\succ y^{\\prime}|x)}+p_{\\gamma}(y^{\\prime}\\succ y|x)\\log\\frac{p_{\\gamma}(y^{\\prime}\\succ y|x)}{p_{\\theta}(y^{\\prime}\\succ y|x)},}\\\\ &{\\qquad\\qquad\\qquad=p_{\\gamma}(y\\succ y^{\\prime}|x)\\log\\frac{p_{\\gamma}(y\\succ y^{\\prime}|x)}{p_{\\theta}(y\\succ y^{\\prime}|x)}+\\left(1-p_{\\gamma}(y\\succ y^{\\prime}|x)\\right)\\log\\frac{1-p_{\\gamma}(y\\succ y^{\\prime}|x)}{1-p_{\\theta}(y\\succ y^{\\prime}|x)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 where ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\gamma}:=\\sigma\\left(\\frac{\\beta}{1+\\gamma}\\left[(U(\\pmb{x},\\pmb{y}^{\\prime})-U(\\pmb{x},\\pmb{y}))+\\beta^{-1}\\gamma\\log\\frac{\\pi_{\\mathrm{ref}}(\\pmb{y}|\\pmb{x})}{\\pi_{\\mathrm{ref}}(\\pmb{y}^{\\prime}|\\pmb{x})}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "138 This quantity is a well-defined KL divergence and is hence non-negative; the quantity vanishes when   \n139 $p_{\\gamma}=p_{\\theta}$ on the observations ${\\pmb y},{\\pmb y}^{\\prime}$ . Furthermore, with access to an explicit reward model, all terms   \n140 in (6) can be computed directly and ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(y\\sim y^{\\prime}|x^{\\prime})=\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta}(y|x)+\\pi_{\\theta}(y^{\\prime}|x)}=\\sigma\\left(\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta}(y^{\\prime}|x)}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "141 To obtain a minimizer of the regularized objective defined in (2) we optimize ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\mathrm{ERA}}(\\pi_{\\pmb{\\theta}})=\\mathbb{E}_{\\boldsymbol{x}\\sim\\mathcal{D}}\\mathbb{E}_{\\pmb{y},\\pmb{y}^{\\prime}\\sim\\pi_{\\mathrm{ref}}(\\cdot|\\pmb{x})}D_{\\mathrm{KL}}^{(\\pmb{y},\\pmb{y}^{\\prime})}(p_{\\gamma}|p_{\\pmb{\\theta}});}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 If the current policy overlaps with the target preference distribution, it may be useful to sample   \n143 directly from the partially aligned policy, i.e., to use the \u201con-policy\u201d formulation, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{on}}^{\\mathrm{ERA}}(\\pi_{\\theta})=\\mathbb{E}_{\\pmb{x}\\sim\\mathcal{D}}\\mathbb{E}_{\\pmb{y},\\pmb{y}^{\\prime}\\sim\\pi_{\\theta}(\\pmb{y}|\\pmb{x})}D_{\\mathrm{KL}}^{(\\pmb{y},\\pmb{y}^{\\prime})}(p_{\\gamma}|p_{\\pmb{\\theta}})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "144 instead of (9). One issue that arises with this scheme is that differentiation with respect to the   \n145 parameters of the policy $\\pmb{\\theta}$ because $\\textit{\\textbf{y}}$ and $\\pmb{y}^{\\prime}$ are decoded into discrete tokens, an operation that is not   \n146 differentiable. To remedy this, we importance sample with a reference policy ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{on}}^{\\mathrm{ERA}}(\\pi_{\\theta})=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\mathbb{E}_{\\mathbf{y},\\mathbf{y}^{\\prime}\\sim\\pi_{\\mathrm{ref}}(\\mathbf{y}|\\mathbf{x})}\\frac{\\pi_{\\theta}(\\mathbf{y}|\\mathbf{x})\\pi_{\\theta}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{\\pi_{\\mathrm{ref}}(\\mathbf{y}|\\mathbf{x})\\pi_{\\mathrm{ref}}(\\mathbf{y}^{\\prime}|\\mathbf{x})}D_{\\mathrm{KL}}^{(\\mathbf{y},\\mathbf{y}^{\\prime})}(p_{\\gamma}|p_{\\theta}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 This reweighting is straightforward and the importance weights should generally be appreciable,   \n148 especially early in training when $\\pi_{\\theta}$ has not drifted far from $\\pi_{\\mathrm{ref}}$ . It is, of course, also natural to   \n149 iteratively update $\\pi_{\\theta}$ using a previous iterate as the reference policy. In this work, we only use (9) as   \n150 an objective and leave the on-policy objectives to future work. ", "page_idx": 3}, {"type": "text", "text": "151 3 Theoretical Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "152 To understand the ERA loss function and its connection to the entropy regularized objective (2), we   \n153 first establish that the minimizers of (6) are of the form (5). We first define the notion of equivalence   \n154 precisely.   \n155 Definition 3.1 The conditional probability measures $\\pi(\\cdot|x)$ and $\\pi^{\\prime}(\\cdot|x)$ are conditionally equivalent   \n156 $i f\\forall x\\in\\mathcal{X}$ , $\\pi$ and $\\pi^{\\prime}$ are such that $\\begin{array}{r}{\\operatorname*{sup}_{y\\in\\mathcal{Y}}|\\pi(y|x)-\\pi^{\\prime}(y|x)|=0}\\end{array}$ .   \n157 We remark that this strong form of equivalence is appropriate on the finite, discrete spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$   \n158 we consider here.   \n159 Lemma 3.1 If $\\pi$ is conditionally equivalent to $\\pi^{\\prime}$ , then $\\pi_{g}^{\\prime}(\\cdot|{\\pmb x})\\,\\propto\\,\\pi^{\\prime}(\\cdot|{\\pmb x})e^{g({\\pmb x})}$ is conditionally   \n160 equivalent to $\\pi$ for all functions $g:\\mathcal{X}\\to\\mathbb{R}$ such that $\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\left|e^{g(\\mathbf{x})}\\right|<+\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "161 We prove Lemma 3.1 in Appendix A and use this simple lemma to prove the following result. ", "page_idx": 4}, {"type": "text", "text": "162 Proposition 3.2 Suppose $\\pi(\\cdot|x)\\in\\mathcal{P}(\\mathcal{P})$ and that $\\mathrm{supp}(\\pi)=\\mathrm{supp}(\\pi_{\\mathrm{ref}})$ . Let $\\beta>0$ , $\\gamma\\geq0$ and   \n163 that the reward model is such that $\\operatorname*{sup}_{\\pmb{x},\\pmb{y}\\in\\mathcal{X}\\times\\mathcal{Y}}|e^{-U(\\pmb{x},\\pmb{y})}|<+\\infty$ . Then, the minimizer of $\\mathcal{L}^{\\mathrm{ERA}}$ is   \n164 conditionally equivalent to $\\pi_{\\star}$ .   \n165 First, we verify that any probability measure $\\begin{array}{r l r}{\\pi_{g}(\\pmb{y}|\\pmb{x})}&{{}\\propto}&{\\exp(-\\frac{\\beta}{1+\\gamma}\\left(U(\\pmb{x},\\pmb{y})\\right.\\right.\\left.-\\right.}\\end{array}$   \n166 $\\beta^{-1}\\gamma\\log\\pi_{\\mathrm{ref}}(\\pmb{y}|\\pmb{x})\\bigr)\\,+\\,g(\\pmb{x}))$ minimizes the objective. Because $\\mathcal{L}^{\\mathrm{ERA}}$ is non-negative, it suf  \n167 fices to show that for all pairs $y,y^{\\prime},\\;D_{\\mathrm{KL}}^{({\\pmb y},{\\pmb y^{\\prime}})}(p_{\\gamma}|p_{\\pmb\\theta})\\;\\equiv\\;0$ . This follows immediately from the   \n168 cancellation in the preference probability $p_{\\gamma}$ of $e^{g(\\pmb{x})}$ after factorization in (5). Now, suppose that   \n169 $\\begin{array}{r}{\\pi(y|x)\\neq\\exp\\left(-\\frac{\\bar{\\beta}}{1+\\gamma}\\big(U(\\pmb{x},\\pmb{\\dot{y}})-\\beta^{-1}\\gamma\\log\\pi_{\\mathrm{ref}}(\\pmb{y}|\\pmb{x})\\big)\\right)}\\end{array}$ where we have taken $g(\\pmb{x})\\,=\\,0$ without   \n170 loss of generality and $\\pi:=\\pi_{g}$ . Assume that for all pairs ${\\pmb y},{\\pmb y}^{\\prime}$ , the divergence $D_{\\mathrm{KL}}^{(\\pmb{y},\\pmb{y}^{\\prime})}(p_{\\gamma}|p_{\\pmb{\\theta}})\\equiv0$   \n171 which is required of a minimizer. Equivalently, it must be the case that for all $\\boldsymbol{y},\\bar{\\boldsymbol{y}^{\\prime}}$ , ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\pi(y|x)}{\\pi(y|x)+\\pi(y^{\\prime}|x)}=\\frac{\\pi_{\\star}(y|x)}{\\pi_{\\star}(y|x)+\\pi_{\\star}(y^{\\prime}|x)}\\implies\\frac{\\pi(y^{\\prime}|x)}{\\pi(y|x)}=\\frac{\\pi_{\\star}(y^{\\prime}|x)}{\\pi_{\\star}(y|x)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "172 from which we see that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi(y|x)=\\frac{\\pi\\left(y^{\\prime}\\middle|x\\right)}{e^{-\\frac{\\beta}{1+\\gamma}\\left(U(x,y^{\\prime})-\\beta^{-1}\\gamma\\log\\pi_{\\mathrm{ref}}(y^{\\prime}|x)\\right)}}e^{-\\frac{\\beta}{1+\\gamma}\\left(U(x,y)-\\beta^{-1}\\gamma\\log\\pi_{\\mathrm{ref}}(y|x)\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "173 By construction, $\\pi(\\pmb{y}|\\pmb{x})$ does not depend on $\\pmb{y}^{\\prime}$ so the prefactor must be purely a function of $\\textbf{\\em x}$ , which   \n174 completes the proof, using Lemma 3.1.   \n175 Gradients of $\\mathcal{L}^{\\mathbf{ERA}}$ . One advantage of the ERA framework is that the objective is amenable to direct,   \n176 gradient-based optimization. We remark that establishing global convergence for the optimization of   \n177 $\\pmb{\\theta}$ using (9) requires establishing convexity with respect to the parameters, which is not obviously the   \n178 case for our objective, nor those used in PPO and DPO. However, one can still glean some insight   \n179 into the optimization by examining the gradients on a samplewise basis. Using the compact notation   \n180 $p_{\\pmb{\\theta}}(\\pmb{y}\\succ\\pmb{\\bar{y^{\\prime}}}|\\pmb{x})\\equiv\\sigma_{\\pmb{\\theta}}$ and $p_{\\gamma}(\\pmb{y}\\succ\\bar{\\pmb{y}}^{\\prime}|\\pmb{x})\\stackrel{\\cdot}{\\equiv}\\sigma_{\\star}$ , ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{L}^{\\mathrm{ERA}}=\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y,y^{\\prime}\\sim\\pi_{\\mathrm{ref}}}\\left(\\frac{1-\\sigma_{\\star}}{1-\\sigma_{\\theta}}-\\frac{\\sigma_{\\star}}{\\sigma_{\\theta}}\\right)\\nabla_{\\theta}\\sigma_{\\theta}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "181 The gradient is straightforward to interpret on a particular pair ${\\pmb y},{\\pmb y}^{\\prime}$ : if $p_{\\theta}(\\pmb{y}\\succ\\pmb{y}^{\\prime}|\\pmb{x})$ is larger than   \n182 $p_{\\gamma}(\\pmb{y}\\succ\\pmb{y}^{\\prime}|\\pmb{x})$ then the preference gradient is positive and gradient descent lowers the probability that   \n183 $\\pmb{y}\\succ\\pmb{y}^{\\prime}$ . The opposite occurs whenever $p_{\\theta}(\\pmb{y}\\bar{\\succ}\\pmb{y}^{\\prime}|\\pmb{x})$ is smaller than $p_{\\gamma}(\\pmb{y}\\succ\\pmb{y}^{\\prime}|\\pmb{x})$ . The magnitude   \n184 of the gradient is scaled by the degree of misspecification of the preference probability.   \n185 This calculation highlights one key difference between the approach we use and DPO. When the data   \n186 only contains one observation of $\\pmb{y}\\succ\\pmb{y}^{\\prime}$ for a given $\\textbf{\\em x}$ , the DPO objective\u2019s implicit reward model   \n187 assigns zero probability to $\\boldsymbol{y}^{\\prime}\\succ\\boldsymbol{y}$ . This pushes the policy towards extremal values, which can lead   \n188 to undesired behavior, as discussed in Azar et al. [3]. In our formulation, this behavior occurs only   \n189 when the reward model assigns an energy of $\\pm\\infty$ , which is prohibited by construction in most tasks.   \n190 We further discuss differences between ERA and DPO in Appendix A.2. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "191 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "192 We test ERA on both chemical and language tasks to shed light on the following questions: 1) Can   \n193 we use ERA to robustly fine-tune our model to generate samples according to a desired distribution?   \n194 2) What is the effect of changing the inverse-temperature $\\beta$ during ERA? 3) Do we maintain sample   \n195 diversity (and validity) without regularizing to remain close to a reference policy, and what is the   \n196 effect of increased regularization? 4) Can we simultaneously target multiple properties with high   \n197 fidelity, and how can we trade off between desired properties? 5) Can we carry out ERA on higher   \n198 capacity models with \u201cweak\u201d signals from smaller models? ", "page_idx": 4}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/1499a6ddc610e8f43686ab743b0977b5358776337ef6368e5da479850cacc262.jpg", "img_caption": ["Figure 2: Unprompted molecular generator alignment. Distributions of different chemical properties for molecules sampled from aligned and unaligned policies. The center of the harmonic potential, $\\mu$ , is varied for MR $(\\beta=1.0)$ ), Ring Count $\\beta=1.0)$ ), and LogP $\\beta=10.0)$ , while $\\beta$ is varied for QED. All experiments were run with no regularization to the reference policy $(\\gamma=0)$ ). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "199 4.1 Generating molecules with desired properties ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "200 We use a decoder-only representation for the molecular generator [4], where the generator has 2 layers,   \n201 an embedding dimension of 512, a vocabulary of 324 tokens, and totals 3.5M parameters. Starting   \n202 from a random initialization, we carry out pretraining on a dataset of 2.4M small molecules from the   \n203 ChEMBL database [37] for 180 epochs. This version of the model is not conditioned on a prompt   \n204 and generates a small molecule given just a start-of-sequence token. We use this pretrained model as   \n205 our reference policy for all unprompted molecular alignment tasks (Sec. 4.1.1). In Sec. 4.1.2, we   \n206 generate molecules conditioned on a prompt using a generator that was trained to carry out sampling   \n207 with a prompt molecule.   \n208 Central to ERA is, of course, access to a computable energy function. As a proof-of-concept, here   \n209 we consider 5 different properties for which the corresponding energy function is easily evaluable:   \n210 Quantitative Estimate of Drug-Likeness (QED) [6], Wildman-Crippen LogP (LogP) [36], Ring Count,   \n211 Molar Refractivity (MR) [36], and Tanimoto Similarity [26]. Briefly, LogP is a measure of the   \n212 hydrophobicity of a molecule, MR is a measure of the polarizability of the molecule, and Tanimoto   \n213 similarity is a measure of the similarity between two molecules (see Appendix C.2). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "214 4.1.1 Unprompted molecular alignment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "215 First, we independently target four different properties using ERA with an unprompted   \n221167 ernefe y nrcaen ny,m enwte o ng e, rawthee rae tias sienti t $\\mathcal{D}=$   \n$\\{\\pmb{y}_{1}^{(i)},\\pmb{y}_{2}^{(i)},\\bar{U}(\\pmb{y}_{1}^{(i)}),U(\\pmb{y}_{2}^{(i)})\\}_{i=1}^{N}$ $\\pi_{\\theta}$ $\\pi_{\\theta}$   \n218 using the weights of $\\pi_{\\mathrm{ref}}$ . Here, $y_{1},y_{2}\\sim\\pi_{\\mathrm{ref}}$ $\\textit{\\textbf{y}}$ $U(\\boldsymbol{y})$ denote the generated molecule and   \n219 its corresponding energy, respectively. For MR, Ring Count, and LogP, we define the energy $U$ to be   \n220 a harmonic potential centered at a target value. For QED, we define the energy to be the negative   \n221 logarithm of QED and vary $\\beta$ to assess its impact on alignment (see Table 1, 2). In Fig. 2, we see that   \n222 we successfully shift the distribution to target means that are both greater and lower than the average   \n223 value of MR, Ring Count, and LogP under the reference policy. Furthermore, in the alignment of   \n224 QED, we observe the effect of changing $\\beta$ on the learned policy; with increased $\\beta$ , the learned policy   \n225 concentrates around low-energy samples (i.e. near $\\mathrm{QED}=1!$ ), and with lower $\\beta$ , the learned policy   \n226 samples a greater range of QED values, as expected. We note that for each of these four experiments,   \n227 we did not regularize towards the reference policy (i.e. $\\gamma=0$ ). Even so, we were able to maintain   \n228 both sample diversity and maintain appreciable sample validity (see Fig. 7 and Table 3).   \n229 Many molecular design tasks require balancing multiple properties, and designing an objective for   \n230 multi-property alignment is straightforward within the ERA framework. To demonstrate this, we   \n231 generate molecules with both high QED and LogP using ERA with an energy function weighted by   \n232 property-specific $\\beta$ : $:U=\\beta_{\\mathrm{QED}}U_{\\mathrm{QED}}+\\beta_{\\mathrm{LogP}}U_{\\mathrm{LogP}}$ (see Table 1, 4 for details on energy function).   \n233 We carry out ERA with different pairs of $\\boldsymbol{\\beta}_{\\mathrm{QED}}$ , $\\beta_{\\mathrm{LogP}}\\$ using the same procedure as above, and   \n234 from Fig. 3, we see that we target multiple properties with varying fidelity by simply modulating the   \n235 value of property-specific $\\beta$ . Ultimately, increasing the $\\beta$ for an individual property enables us to   \n236 favor higher values of that property in multi-property alignment setting. In this case, we also do not   \n237 regularize with the KL-divergence to the reference policy and again maintain sample diversity and   \n238 validity (see Fig. 8 and Table 4) ", "page_idx": 5}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/b09cde9a4d085b209940b8bb3499cb6ffb83fae97e79a6dfca969efe6b11f614.jpg", "img_caption": ["Figure 3: Unprompted multi-property molecular generator alignment. 2D histograms of LogP versus QED for different combinations of property-specific $\\beta$ illustrating a clear trade-off when performing multi-property alignment. Relative increases in $\\beta$ for a given property target higher values for that property. All experiments were run with no regularization to the reference policy $(\\gamma=0)$ ). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "239 4.1.2 Prompted molecular alignment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "240 Inspired by the task of lead optimization in drug discovery efforts [16], we ask whether we can use   \n241 ERA to train a molecular generator that can sample a molecule that is both similar to the prompt   \n242 molecule and also exhibits some desired property.   \n243 First, we fine-tune the pretrained molecular generator to enable prompted molecular generation (see   \n244 Appendix C.3.2) and use this fine-tuned model as our reference policy for all prompted molecular   \n245 alignment tasks. This reference policy disproportionately samples molecules that are identical (i.e. a   \n246 Tanimoto similarity of 1.0) to the prompt molecule (see Fig. 4), so we carry out multi-property align  \n247 ment on this reference policy to generate molecules that are similar\u2014but not identical\u2014to the prompt   \n248 molecule and also have a high drug-likeness as measured by QED. Using ERA, we optimize the refer  \n249 ence policy with a generated dataset $\\mathcal{D}=\\{(\\pmb{y}_{1}^{(i)},\\pmb{x}^{(i)}),(\\pmb{y}_{2}^{(i)},\\pmb{x}^{(i)}),U(\\pmb{y}_{1}^{(i)},\\pmb{x}^{(i)}),U(\\pmb{y}_{2}^{(i)},\\pmb{x}^{(i)})\\}_{i=1}^{N},$   \n250 where we sample four molecules for each prompt molecule from the reference policy and consider all   \n251 possible preference pairs for a total of six preference pairs per prompt molecule (see Appendix C.2   \n252 for full details on energy used).   \n253 We observe that the per-prompt average QED under the optimized policy for a given prompt is higher   \n254 than the corresponding average under the reference policy (Fig. 4). Furthermore, we see that we are   \n255 able to sample a diverse set of molecules that are chemically similar to the prompt molecule, and   \n256 also chemically valid (see Figure 9, Table 5). We repeat the experiment with a related objective of   \n257 generating molecules similar to the prompt molecule with a high LogP instead and again observe   \n258 that we increase the per-prompt average LogP under the optimized policy relative to the reference   \n259 policy without degrading sample diversity and validity. For both of these experiments, we required   \n260 regularization to the reference policy. With no regularization, the aligned generator would almost   \n261 exclusively sample sequences that were chemically invalid $(<25\\%$ chemical validity). Finally, we   \n262 note that the increases in QED and LogP in Fig. 4 are smaller relative to the increases in Fig. 2   \n263 because the samples are now conditioned to remain proximal to the prompt molecule, which restricts   \n264 the chemical space that can be explored. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/37b7f282ffb6b6006c37241186a84e6074ebdbfcefc42699c4d84a63d08ddd3a.jpg", "img_caption": ["Figure 4: Prompted multi-property molecular generator alignment. From left to right: Tanimoto similarities computed between the prompt and sampled molecules for both aligned and unaligned policies (QED and Tanimoto alignment), per-prompt difference in the average QED under aligned and unaligned policies (QED and Tanimoto alignment), Tanimoto similarities computed between the prompt and sampled molecules for both aligned and unaligned policies (LogP and Tanimoto alignment), and per-prompt difference in the average LogP under aligned and unaligned policies (LogP and Tanimoto alignment). With alignment, we target higher QED and LogP values, while still sampling molecules chemically similar\u2014but not identical\u2014to prompt molecule. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/cc6915f3c723ce11010e8daaf2d0732183570020fecf370753fe9bff65dc33a2.jpg", "img_caption": ["Figure 5: AI-guided alignment of LLMs. Average sentiment of responses from aligned GPT-2 model across all prompts. (left). Proportion of unsafe content relative to unaligned model of responses aligned LLaMA2-13B model across all prompts (right). $5.4\\%$ of all responses from unaligned model were classified as unsafe. Error bars too small to be shown. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "265 4.2 AI-guided alignment of large language models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "266 We test the generality of ERA by applying it to align large language models (LLMs). Similar to the   \n267 experiments in [25], we first carry out ERA on a GPT-2 model [24] fine-tuned on movies reviews   \n268 from IMDb [18]. We use a pretrained sentiment classifier [14] to evaluate the energies\u2014where   \n269 lower energies correspond to more positive sentiments\u2014of sampled responses from the reference   \n270 policy and carry out ERA using the same approach as in Section 4.1.2 (see Appendix D.1). We   \n271 vary the regularization strength $\\gamma$ and inverse-temperature $\\beta$ on the average sentiment and observe   \n272 that across all regularization strengths, with increasing $\\beta$ , the average sentiment becomes more   \n273 positive. Increasing regularization also elicits more positive sentiments. Qualitatively, with lower   \n274 regularization, we observe that text quality degrades and becomes less coherent, likely resulting in   \n275 lower average sentiment predictions by the sentiment model. Regularization here is important to   \n276 ensure high quality text samples.   \n277 We next leverage a \u201cweak\u201d AI supervisor to carry out LLM alignment, a task sometimes called   \n278 \u201csuperalignment\u201d [8]. In the present context, we order \u201cweak\u201d vs. \u201cstrong\u201d models based on their   \n279 parameter count (within the same family) and empirical performance; i.e., LLaMA2-7B is weaker   \n280 than LLaMA2-13B. Here, the weak model does not necessarily contain the complexity of the stronger   \n281 model but can weakly discern between different outputs of a stronger model. Given a sample   \n282 $\\pmb{y}_{i}\\sim\\pi_{\\mathrm{strong}}(\\pmb{y}|\\pmb{x})$ , we define the energy using the weak model $U(y_{i}|\\bar{\\pmb{x}})=-\\log\\pi_{\\mathrm{weak}}(y_{i}|\\pmb{x})$ .   \n283 We test weak-to-strong alignment using a previously aligned LLaMA2-7B  \n284 Chat (meta-llama/Llama-2-7b-chat) to optimize an unaligned LLaMA2-13B   \n285 (meta-llama/Llama-2-13b) model [33]. Using prompts from the Anthropic Helpful and   \n286 Harmless dialogue dataset [5], we first carry out a short supervised fine-tuning step of LLaMA2-13B   \n287 to ensure it can output text in a chat-like format (see Appendix D.2). Using this reference policy,   \n288 we generate a dataset with energies computed from the smaller LLaMA2-7B-Chat model and carry   \n289 out ERA as above, again across varying $\\gamma$ and $\\beta$ . We evaluate the \u201csafety\u201d of generated samples   \n290 using Meta LLama Guard 2 (meta-llama/Meta-Llama-Guard-2-8B) [15]. We observe that as we   \n291 increase $\\beta$ , the proportion of unsafe content relative to the unaligned, reference model decreases, with   \n292 over a $90\\%$ drop between the unaligned model and the models aligned with the highest $\\beta$ across all $\\gamma$ .   \n293 For these experiments, we observe that varying regularization strengths has a minimal effect and that   \n294 we are in fact able to generate coherent sentences with no regularization, with strong regularization   \n295 hurting performance for $\\beta=0.1$ . Finally, we compare ERA and DPO in Appendix D.2 and observe   \n296 that with our implementation of DPO, we are able to generate lower energy samples, but that it is   \n297 prone to mode collapse. We caution that our implementation of DPO is likely not optimal and that   \n298 we did not exhaustively tune the hyperparameters of DPO due to resource constraints. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "299 5 Conclusions and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "300 This paper introduces energy rank alignment, a simple and effective algorithm for policy optimization   \n301 with an explicit reward model. We find that ERA is stable without extensive hyperparameter tuning,   \n302 and sufficiently general to successfully align both application-specific transformers for chemical   \n303 search problems as well as generative pre-trained transformers for language. The algorithm exhibits   \n304 strong performance with a variety of reward models, even ones with relatively weak signal, such as   \n305 the AI feedback of LLaMA2-7B-Chat. Interestingly, with this approach we are able to reduce unsafe   \n306 content by more than $90\\%$ with no human preference data.   \n307 We analyze the minimizers of the ERA objective and find that they differ from the minimizers of   \n308 popular policy alignment algorithms DPO and PPO in an important way: unlike PPO, the strength of   \n309 regularization to the reference policy that we add is controlled by a parameter $\\gamma$ , while the entropy   \n310 of the target distribution is independently tuned by a distinct parameter $\\beta$ . This means that we can   \n311 avoid greedy policies by keeping $\\beta$ small\u2014amplifying fluctuations around the optimum of the reward   \n312 model $-U$ \u2014while reducing the influence of the reference policy by taking $\\gamma$ small. Our objective   \n313 leads to easily interpretable sample-wise gradients which highlight the importance of a reward model   \n314 relative to DPO in the sampled objective. Similar observations about the inadequacy of the DPO   \n315 objective for finite preference observations were also made theoretically in Azar et al. [3].   \n316 Limitations: First, our approach requires a reward model, which can be difficult to train or design,   \n317 especially for complex tasks. While we observed that ERA makes an appreciable impact even with   \n318 weak supervision from an AI chat model, this sort of proxy may not be available for more complex   \n319 tasks. For example, optimizing small molecules for high binding affinity to a target protein would   \n320 require expensive and noisy evaluations of a reward model, which likely limits the scope of molecular   \n321 design to problems where the reward can be computed somewhat efficiently. A second limitation of   \n322 our present work is that we do not train the molecular transformer to favor synthetic accessibility   \n323 nor do we explicitly seek to obtain molecules that are easily synthesized experimentally. There are   \n324 models that seek to evaluate synthesizability computationally that could be used in our rewards,   \n325 which we plan to explore in future work [11]. A final limitation of our current work is the moderate   \n326 scale of our numerical experiments due to our limited compute resources, including the inadequate   \n327 hyperparameter tuning for the DPO baseline for Fig. 5.   \n329 [1] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/   \n330 blob/main/MODEL_CARD.md.   \n331 [2] G. An, J. Lee, X. Zuo, N. Kosaka, K.-M. Kim, and H. O. Song. Direct Preference-based Policy   \n332 Optimization without Reward Modeling. Advances in Neural Information Processing Systems,   \n333 36:70247\u201370266, Dec. 2023.   \n334 [3] M. G. Azar, M. Rowland, B. Piot, D. Guo, D. Calandriello, M. Valko, and R. Munos. A General   \n335 Theoretical Paradigm to Understand Learning from Human Preferences, Nov. 2023.   \n336 [4] V. Bagal, R. Aggarwal, P. K. Vinod, and U. D. Priyakumar. MolGPT: Molecular Generation   \n337 Using a Transformer-Decoder Model. Journal of Chemical Information and Modeling, 62(9):   \n338 2064\u20132076, May 2022. ISSN 1549-9596. doi: 10.1021/acs.jcim.1c00600.   \n339 [5] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,   \n340 T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield  \n341 Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson,   \n342 D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training   \n343 a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Apr.   \n344 2022.   \n345 [6] G. R. Bickerton, G. V. Paolini, J. Besnard, S. Muresan, and A. L. Hopkins. Quantifying   \n346 the chemical beauty of drugs. Nature Chemistry, 4(2):90\u201398, Feb. 2012. ISSN 1755-4330,   \n347 1755-4349. doi: 10.1038/nchem.1243.   \n348 [7] R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of   \n349 paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952. ISSN 0006-3444. doi: 10.2307/   \n350 2334029.   \n351 [8] C. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,   \n352 M. Joglekar, J. Leike, I. Sutskever, and J. Wu. Weak-to-strong generalization: Eliciting strong   \n353 capabilities with weak supervision, Dec. 2023.   \n354 [9] S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak,   \n355 D. Lindner, P. Freire, T. Wang, S. Marks, C.-R. Segerie, M. Carroll, A. Peng, P. Christoffersen,   \n356 M. Damani, S. Slocum, U. Anwar, A. Siththaranjan, M. Nadeau, E. J. Michaud, J. Pfau,   \n357 D. Krasheninnikov, X. Chen, L. Langosco, P. Hase, E. B\u0131y\u0131k, A. Dragan, D. Krueger, D. Sadigh,   \n358 and D. Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning   \n359 from human feedback, Sept. 2023.   \n360 [10] S. Chithrananda, G. Grand, and B. Ramsundar. ChemBERTa: Large-Scale Self-Supervised   \n361 Pretraining for Molecular Property Prediction. In Machine Learning for Molecules Workshop at   \n362 NeurIPS, 2020.   \n363 [11] C. W. Coley, L. Rogers, W. H. Green, and K. F. Jensen. SCScore: Synthetic Complexity Learned   \n364 from a Reaction Corpus. Journal of Chemical Information and Modeling, 58(2):252\u2013261, Feb.   \n365 2018. ISSN 1549-9596. doi: 10.1021/acs.jcim.7b00622.   \n366 [12] P. S. Gromski, A. B. Henson, J. M. Granda, and L. Cronin. How to explore chemical space   \n367 using algorithms and automation. Nature Reviews Chemistry, 3(2):119\u2013128, 2019.   \n368 [13] R. G\u00f3mez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hern\u00e1ndez-Lobato, B. S\u00e1nchez-Lengeling,   \n369 D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Auto  \n370 matic chemical design using a data-driven continuous representation of molecules. ACS Central   \n371 Science, 4(2):268\u2013276, Feb. 2018. ISSN 2374-7943. doi: 10.1021/acscentsci.7b00572.   \n372 [14] J. Hartmann, M. Heitmann, C. Siebert, and C. Schamp. More than a feeling: Accuracy   \n373 and application of sentiment analysis. International Journal of Research in Marketing, 40   \n374 (1):75\u201387, 2023. doi: https://doi.org/10.1016/j.ijresmar.2022.05.005. URL https://www.   \n375 sciencedirect.com/science/article/pii/S0167811622000477.   \n376 [15] H. Inan, K. Upasani, J. Chi, R. Rungta, K. Iyer, Y. Mao, M. Tontchev, Q. Hu, B. Fuller,   \n377 D. Testuggine, and M. Khabsa. Llama Guard: LLM-based Input-Output Safeguard for Human  \n378 AI Conversations, Dec. 2023.   \n379 [16] G. M. Keser\u00fc and G. M. Makara. The influence of lead discovery strategies on the properties of   \n380 drug candidates. Nature Reviews Drug Discovery, 8(3):203\u2013212, Mar. 2009. ISSN 1474-1776,   \n381 1474-1784. doi: 10.1038/nrd2796.   \n382 [17] R. K. Lindsay, B. G. Buchanan, E. A. Feigenbaum, and J. Lederberg. Dendral: A case study of   \n383 the first expert system for scientific hypothesis formation. Artificial Intelligence, 61(2):209\u2013261,   \n384 June 1993. ISSN 00043702. doi: 10.1016/0004-3702(93)90068-M.   \n385 [18] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning Word Vectors   \n386 for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for   \n387 Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon,   \n388 USA, June 2011. Association for Computational Linguistics.   \n389 [19] J. Maas. Gradient flows of the entropy for finite Markov chains. Journal of Functional Analysis,   \n390 261(8):2250\u20132292, Oct. 2011. ISSN 0022-1236. doi: 10.1016/j.jfa.2011.06.009.   \n391 [20] R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland, Z. D. Guo, Y. Tang, M. Geist,   \n392 T. Mesnard, A. Michi, M. Selvi, S. Girgin, N. Momchev, O. Bachem, D. J. Mankowitz, D. Precup,   \n393 and B. Piot. Nash Learning from Human Feedback, Dec. 2023.   \n394 [21] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,   \n395 K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,   \n396 P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with   \n397 human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,   \n398 editors, Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744.   \n399 Curran Associates, Inc., 2022.   \n400 [22] R. Park, R. Theisen, N. Sahni, M. Patek, A. Cichon\u00b4ska, and R. Rahman. Preference Optimization   \n401 for Molecular Language Models, Oct. 2023.   \n402 [23] G. Pesciullesi, P. Schwaller, T. Laino, and J.-L. Reymond. Transfer learning enables the   \n403 molecular transformer to predict regio- and stereoselective reactions on carbohydrates. Nature   \n404 Communications, 11(1):4874, Sept. 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-18671-7.   \n405 [24] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are   \n406 unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n407 [25] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference   \n408 optimization: Your language model is secretly a reward model. In A. Oh, T. Neumann,   \n409 A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information   \n410 Processing Systems, volume 36, pages 53728\u201353741. Curran Associates, Inc., 2023.   \n411 [26] D. J. Rogers and T. T. Tanimoto. A Computer Program for Classifying Plants: The computer is   \n412 programmed to simulate the taxonomic process of comparing each case with every other case.   \n413 Science, 132(3434):1115\u20131118, Oct. 1960. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.   \n414 132.3434.1115.   \n415 [27] B. Sanchez-Lengeling and A. Aspuru-Guzik. Inverse molecular design using machine learning:   \n416 Generative models for matter engineering. Science, 361(6400):360\u2013365, July 2018. doi:   \n417 10.1126/science.aat2663.   \n418 [28] F. Santambrogio. {Euclidean, Metric, and Wasserstein} gradient flows: An overview. Bul  \n419 letin of Mathematical Sciences, 7(1):87\u2013154, Apr. 2017. ISSN 1664-3615. doi: 10.1007/   \n420 s13373-017-0101-1.   \n421 [29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal Policy Optimization   \n422 Algorithms, Aug. 2017.   \n423 [30] P. Schwaller, T. Gaudin, D. L\u00e1nyi, C. Bekas, and T. Laino. \u201cFound in Translation\u201d: Predicting   \n424 outcomes of complex organic chemistry reactions using neural sequence-to-sequence models.   \n425 Chemical Science, 9(28):6091\u20136098, 2018. doi: 10.1039/C8SC02339E.   \n426 [31] P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter, C. Bekas, and A. A. Lee. Molecular   \n427 transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS Central   \n428 Science, 5(9):1572\u20131583, Sept. 2019. ISSN 2374-7943, 2374-7951. doi: 10.1021/acscentsci.   \n429 9b00576.   \n430 [32] F. Tajwar, A. Singh, A. Sharma, R. Rafailov, J. Schneider, T. Xie, S. Ermon, C. Finn, and   \n431 A. Kumar. Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data,   \n432 Apr. 2024.   \n433 [33] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,   \n434 P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,   \n435 J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,   \n436 R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.   \n437 Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,   \n438 I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.   \n439 Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,   \n440 I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and   \n441 T. Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.   \n442 [34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and   \n443 I. Polosukhin. Attention is All you Need. In Advances in Neural Information Processing   \n444 Systems, volume 30. Curran Associates, Inc., 2017.   \n445 [35] S. Wang, Y. Guo, Y. Wang, H. Sun, and J. Huang. SMILES-BERT: Large Scale Unsupervised   \n446 Pre-Training for Molecular Property Prediction. In Proceedings of the 10th ACM International   \n447 Conference on Bioinformatics, Computational Biology and Health Informatics, BCB \u201919, pages   \n448 429\u2013436, New York, NY, USA, Sept. 2019. Association for Computing Machinery. ISBN   \n449 978-1-4503-6666-3. doi: 10.1145/3307339.3342186.   \n450 [36] S. A. Wildman and G. M. Crippen. Prediction of Physicochemical Parameters by Atomic   \n451 Contributions. Journal of Chemical Information and Computer Sciences, 39(5):868\u2013873, Sept.   \n452 1999. ISSN 0095-2338, 1520-5142. doi: 10.1021/ci990307l.   \n453 [37] B. Zdrazil, E. Felix, F. Hunter, E. J. Manners, J. Blackshaw, S. Corbett, M. de Veij, H. Ioannidis,   \n454 D. M. Lopez, J. F. Mosquera, M. P. Magarinos, N. Bosc, R. Arcila, T. Kizil\u00f6ren, A. Gaulton,   \n455 A. P. Bento, M. F. Adasme, P. Monecke, G. A. Landrum, and A. R. Leach. The ChEMBL   \n456 Database in 2023: A drug discovery platform spanning multiple bioactivity data types and   \n457 time periods. Nucleic Acids Research, 52(D1):D1180\u2013D1192, Jan. 2024. ISSN 0305-1048,   \n458 1362-4962. doi: 10.1093/nar/gkad1004.   \n459 [38] R. Zhang, L. Lin, Y. Bai, and S. Mei. Negative Preference Optimization: From Catastrophic   \n460 Collapse to Effective Unlearning, Apr. 2024.   \n461 [39] Z. Zhou, J. Liu, C. Yang, J. Shao, Y. Liu, X. Yue, W. Ouyang, and Y. Qiao. Beyond One  \n462 Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization, Dec. 2023. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "463 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "464 1. Claims   \n465 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n466 paper\u2019s contributions and scope?   \n467 Answer: [Yes]   \n468 Justification: The abstract clearly outlines the problem of searching over vast chemical   \n69 spaces as an alignment problem, gives a high-level overview of our method, highlights   \n470 the key differences between our method and similar approaches, and summarizes the key   \n471 advantages of our algorithm. We clearly define the current scope of our investigation by   \n472 describing our experiments.   \n473 Guidelines:   \n474 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n475 made in the paper.   \n476 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n477 contributions made in the paper and important assumptions and limitations. A No or   \n478 NA answer to this question will not be perceived well by the reviewers.   \n479 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n480 much the results can be expected to generalize to other settings.   \n481 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n82 are not attained by the paper.   \n83 2. Limitations ", "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: We address several potential limitations of our work at the end of Section 5. Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "514 3. Theory Assumptions and Proofs ", "page_idx": 12}, {"type": "text", "text": "15 Question: For each theoretical result, does the paper provide the full set of assumptions and 16 a complete (and correct) proof? 17 Answer: [Yes] 18 Justification: Section 3 provides a thorough theoretical analysis of the ERA algorithm, and 19 further details are in Section A of the Appendix. 20 Guidelines:   \n21 \u2022 The answer NA means that the paper does not include theoretical results. 22 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross23 referenced.   \n24 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. 25 \u2022 The proofs can either appear in the main paper or the supplemental material, but if 26 they appear in the supplemental material, the authors are encouraged to provide a short 27 proof sketch to provide intuition. 28 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented 29 by formal proofs provided in appendix or supplemental material. 30 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n531 4. Experimental Result Reproducibility 32 Question: Does the paper fully disclose all the information needed to reproduce the main ex", "page_idx": 13}, {"type": "text", "text": "perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Justification: Between Section 4 (Experiments) and Sections C and $\\mathrm{D}$ of the Appendix (Details for molecular generator experiments and Details for LLM experiments, respectively), we describe the experimental procedure used to obtain each result. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 13}, {"type": "text", "text": "569 some way (e.g., to registered users), but it should be possible for other researchers   \n570 to have some path to reproducing or verifying the results.   \n571 5. Open access to data and code   \n572 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n573 tions to faithfully reproduce the main experimental results, as described in supplemental   \n574 material?   \n575 Answer: [Yes]   \n576 Justification: We will release the code as open source upon submission of the paper. The   \n577 models used for the LLM experiments, such as LLaMA2 and GPT-2, are already available   \n578 online via Huggingface.   \n579 Guidelines:   \n580 \u2022 The answer NA means that paper does not include experiments requiring code.   \n581 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n582 public/guides/CodeSubmissionPolicy) for more details.   \n583 \u2022 While we encourage the release of code and data, we understand that this might not be   \n584 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n585 including code, unless this is central to the contribution (e.g., for a new open-source   \n586 benchmark).   \n587 \u2022 The instructions should contain the exact command and environment needed to run to   \n588 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n589 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n590 \u2022 The authors should provide instructions on data access and preparation, including how   \n591 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n592 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n593 proposed method and baselines. If only a subset of experiments are reproducible, they   \n594 should state which ones are omitted from the script and why.   \n595 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n596 versions (if applicable).   \n597 \u2022 Providing as much information as possible in supplemental material (appended to the   \n598 paper) is recommended, but including URLs to data and code is permitted.   \n599 6. Experimental Setting/Details   \n600 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n601 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n602 results?   \n603 Answer: [Yes]   \n604 Justification: The exact settings and hyperparameters used for training and testing are   \n605 provided in descriptions of the experiments in Section 4 and in Sections C and $\\mathrm{D}$ of the   \n606 Appendix.   \n607 Guidelines:   \n608 \u2022 The answer NA means that the paper does not include experiments.   \n609 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n610 that is necessary to appreciate the results and make sense of them.   \n611 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n612 material.   \n613 7. Experiment Statistical Significance   \n614 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n615 information about the statistical significance of the experiments?   \n616 Answer: [Yes]   \n617 Justification: The statistical significance of all the results is discussed throughout the body   \n618 of the paper in Section 4 and in Sections C and $\\mathrm{D}$ of the Appendix.   \n619 Guidelines:   \n620 \u2022 The answer NA means that the paper does not include experiments.   \n621 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n622 dence intervals, or statistical significance tests, at least for the experiments that support   \n623 the main claims of the paper.   \n624 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n625 example, train/test split, initialization, random drawing of some parameter, or overall   \n626 run with given experimental conditions).   \n627 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n628 call to a library function, bootstrap, etc.)   \n629 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n630 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n631 of the mean.   \n632 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n633 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n634 of Normality of errors is not verified.   \n635 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n636 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n637 error rates).   \n638 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n639 they were calculated and reference the corresponding figures or tables in the text.   \n640 8. Experiments Compute Resources   \n641 Question: For each experiment, does the paper provide sufficient information on the com  \n642 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n643 the experiments?   \n644 Answer: [Yes]   \n645 Justification: Computational resources used are discussed in Section E of the Appendix.   \n646 Guidelines:   \n647 \u2022 The answer NA means that the paper does not include experiments.   \n648 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n649 or cloud provider, including relevant memory and storage.   \n650 \u2022 The paper should provide the amount of compute required for each of the individual   \n651 experimental runs as well as estimate the total compute.   \n652 \u2022 The paper should disclose whether the full research project required more compute   \n653 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n654 didn\u2019t make it into the paper).   \n655 9. Code Of Ethics   \n656 Question: Does the research conducted in the paper conform, in every respect, with the   \n657 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n658 Answer: [Yes]   \n659 Justification: Yes, the work is compliant with the NeurIPS Code of Ethics.   \n660 Guidelines:   \n661 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n662 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n663 deviation from the Code of Ethics.   \n664 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n665 eration due to laws or regulations in their jurisdiction).   \n666 10. Broader Impacts   \n667 Question: Does the paper discuss both potential positive societal impacts and negative   \n668 societal impacts of the work performed?   \n669 Answer: [Yes]   \n671 Guidelines:   \n672 \u2022 The answer NA means that there is no societal impact of the work performed.   \n673 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n674 impact or why the paper does not address societal impact.   \n675 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n676 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n677 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n678 groups), privacy considerations, and security considerations.   \n679 \u2022 The conference expects that many papers will be foundational research and not tied   \n680 to particular applications, let alone deployments. However, if there is a direct path to   \n681 any negative applications, the authors should point it out. For example, it is legitimate   \n682 to point out that an improvement in the quality of generative models could be used to   \n683 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n684 that a generic algorithm for optimizing neural networks could enable people to train   \n685 models that generate Deepfakes faster.   \n686 \u2022 The authors should consider possible harms that could arise when the technology is   \n687 being used as intended and functioning correctly, harms that could arise when the   \n688 technology is being used as intended but gives incorrect results, and harms following   \n689 from (intentional or unintentional) misuse of the technology.   \n690 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n691 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n692 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n693 feedback over time, improving the efficiency and accessibility of ML).   \n694 11. Safeguards   \n695 Question: Does the paper describe safeguards that have been put in place for responsible   \n696 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n697 image generators, or scraped datasets)?   \n698 Answer: [Yes]   \n699 Justification: We use public data and models for all LLM experiments. There are no safety   \n700 concerns with the chemical alignment models.   \n701 Guidelines:   \n702 \u2022 The answer NA means that the paper poses no such risks.   \n703 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n704 necessary safeguards to allow for controlled use of the model, for example by requiring   \n705 that users adhere to usage guidelines or restrictions to access the model or implementing   \n706 safety filters.   \n707 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n708 should describe how they avoided releasing unsafe images.   \n709 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n710 not require this, but we encourage authors to take this into account and make a best   \n711 faith effort.   \n712 12. Licenses for existing assets   \n713 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n714 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n715 properly respected?   \n716 Answer: [Yes]   \n717 Justification: Yes, we cite the appropriate works throughout the main text and the Appendix.   \n718 Guidelines:   \n719 \u2022 The answer NA means that the paper does not use existing assets.   \n720 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n721 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n722 URL.   \n723 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n724 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n725 service of that source should be provided.   \n726 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n727 package should be provided. For popular datasets, paperswithcode.com/datasets   \n728 has curated licenses for some datasets. Their licensing guide can help determine the   \n729 license of a dataset.   \n730 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n731 the derived asset (if it has changed) should be provided.   \n732 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n733 the asset\u2019s creators.   \n734 13. New Assets   \n735 Question: Are new assets introduced in the paper well documented and is the documentation   \n736 provided alongside the assets?   \n737 Answer: [Yes]   \n738 Justification: The code will be released as open-source and will have adequate documentation   \n739 for reproducing experiments.   \n740 Guidelines:   \n741 \u2022 The answer NA means that the paper does not release new assets.   \n742 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n743 submissions via structured templates. This includes details about training, license,   \n744 limitations, etc.   \n745 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n746 asset is used.   \n747 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n748 create an anonymized URL or include an anonymized zip file.   \n749 14. Crowdsourcing and Research with Human Subjects   \n750 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n751 include the full text of instructions given to participants and screenshots, if applicable, as   \n752 well as details about compensation (if any)?   \n753 Answer: [NA]   \n754 Justification: We did not use human subjects.   \n755 Guidelines:   \n756 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n757 human subjects.   \n758 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n759 tion of the paper involves human subjects, then as much detail as possible should be   \n760 included in the main paper.   \n761 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n762 or other labor should be paid at least the minimum wage in the country of the data   \n763 collector.   \n764 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n765 Subjects   \n766 Question: Does the paper describe potential risks incurred by study participants, whether   \n767 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n768 approvals (or an equivalent approval/review based on the requirements of your country or   \n769 institution) were obtained?   \n770 Answer: [NA]   \n771 Justification: We did not use human subjects in this work.   \n772 Guidelines:   \n773 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n774 human subjects. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "text", "text": "784 Set-up, notation, and assumptions Let $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ be discrete spaces; each element of one of these   \n785 spaces is a finite-length sequence of tokens within a fixed dictionary on which an autoregressive   \n786 generative model is trained. The resulting models yield \u201cpolicies\u201d, which are conditional probability   \n787 distributions $\\pi(\\cdot|\\pmb{x})\\;\\in\\;\\mathcal{P}(\\mathcal{P})$ for each $\\textbf{\\em x}\\in\\textbf{\\em x}$ . Throughout, we assume that our policies have   \n788 full support on $\\boldsymbol{\\wp}$ for each $\\textbf{\\em x}$ , meaning that $\\operatorname*{inf}_{y,x\\in{\\mathcal{Y}}\\times{\\mathcal{X}}}\\pi(y|x)\\;>\\;0$ . Because the spaces are   \n789 discrete, we make no strong restrictions on the regularity or coerciveness of the reward model   \n790 $-U\\,:\\,\\mathcal{X}\\,\\times\\,\\mathcal{Y}\\,\\rightarrow\\,\\mathbb{R}$ . The only requirement to ensure the existence of an optimal probability   \n791 distribution is that supx,y\u00d7X\u00d7Y |e\u2212U(x,y)| < +\u221e, which maintains full support of the distribution.   \n792 Though it plays little role in theoretical analysis, we also denote by $\\nu\\,\\in\\,\\mathscr{P}(\\mathcal{X})$ the probability   \n793 distribution over the prompts $\\textbf{\\em x}$ .   \n794 Goals of the analysis presented here The main purpose of this section is to establish that globally   \n795 minimizing the loss (9) yields a global minimizer of the regularized policy objective (2). A secondary   \n796 goal is to clearly articulate the theoretical advantages of ERA compared with PPO and DPO.   \n797 To understand the ERA loss function and its connection to the entropy regularized objective (2), we   \n798 first establish that the minimizer of (6) are of the form (5). We first define the notion of equivalence   \n799 precisely.   \n800 Definition A.1 The conditional probability measures $\\pi(\\cdot|x)$ and $\\pi^{\\prime}(\\cdot|x)$ in $\\mathscr{P}(\\mathscr{y})$ are conditionally   \n801 equivalent $i f\\forall x\\in\\mathcal{X}$ , $\\pi$ and $\\pi^{\\prime}$ are such that $\\begin{array}{r}{\\operatorname*{sup}_{\\pmb{y}\\in\\mathcal{Y}}|\\pi(\\pmb{y}|\\pmb{x})-\\pi^{\\prime}(\\pmb{y}|\\pmb{x})|=0}\\end{array}$ .   \n802 This is a strong form of equivalence for probability measures, but it is appropriate on the discrete   \n803 spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ we consider here. For more general continuous spaces, one could relax this condition   \n804 to weak equivalence of the conditional measures. We use this notion to emphasize that a shift of   \n805 the distribution of the \u201cprompts\u201d $x\\in\\mathcal{X}$ , which we denote $\\nu\\in\\mathcal{P}(\\mathcal{X})$ , does not impact conditional   \n806 equivalence and hence establishes an equivalence class of conditional probability measures that   \n807 minimize (2).   \n808 Lemma A.1 If $\\pi$ is conditionally equivalent to $\\pi^{\\prime}$ , then $\\pi_{g}^{\\prime}(\\cdot|\\pmb{x})\\,\\propto\\,\\pi^{\\prime}(\\cdot|\\pmb{x})e^{g(\\pmb{x})}$ is conditionally   \n809 equivalent to $\\pi$ for all functions $g:\\mathcal{X}\\to\\mathbb{R}$ such that $\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\left|e^{g(\\mathbf{x})}\\right|<+\\infty$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "810 Assume that $\\pi^{\\prime}$ is a normalized probability distribution. This requires that, ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ^{\\prime}(x)=\\sum_{y\\in\\mathcal{Y}}\\pi^{\\prime}(y|x)=1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "811 If $g$ is such that ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ_{g}^{\\prime}(x)=\\sum_{\\pmb{y}\\in\\mathcal{D}}\\pi^{\\prime}(\\pmb{y}|\\pmb{x})e^{g(\\pmb{x})}\\neq1,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "812 then the normalized policy $\\pi_{g}^{\\prime}$ is clearly defined by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{Z_{g}^{\\prime}(\\pmb{x})}\\pi^{\\prime}(\\pmb{y}|\\pmb{x})e^{g(\\pmb{x})}\\equiv\\pi^{\\prime}(\\pmb{y}|\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "813 because $Z_{g}^{\\prime}(x)=e^{g({\\pmb x})}$ . By the assumption that $\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\big|e^{g(\\mathbf{x})}\\big|<+\\infty$ , all terms in these calcula  \n814 tions remain finite.   \n815 Using Lemma A.1 it is straightforward to prove the result in the main text Proposition 3.2. For   \n816 completeness, we re-state that result here and refer the reader to the main text for the complete   \n817 argument.   \n818 Proposition A.2 Suppose $\\pi(\\cdot|x)\\in\\mathcal{P}(\\mathcal{P})$ and that $\\mathrm{supp}(\\pi)=\\mathrm{supp}(\\pi_{\\mathrm{ref}})$ . Let $\\beta>0$ , $\\gamma\\geq0$ and   \n819 that the reward model is such that $\\operatorname*{sup}_{\\pmb{x},\\pmb{y}\\in\\mathcal{X}\\times\\mathcal{Y}}|e^{-U(\\pmb{x},\\pmb{y})}|<+\\infty$ . Then, the minimizer of $\\mathcal{L}^{\\mathrm{ERA}}$ is   \n820 conditionally equivalent to $\\pi_{\\star}$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "821 This proposition establishes that a policy minimizing the objective ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{\\mathrm{ERA}}(\\pi_{\\theta})=\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y,y^{\\prime}\\sim\\pi_{\\mathrm{ref}}(\\cdot|x)}D_{\\mathrm{KL}}^{(y,y^{\\prime})}(p_{\\beta}|p_{\\theta});}\\\\ &{\\quad\\quad\\quad p_{\\theta}:=\\sigma\\left(\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta}(y^{\\prime}|x)}\\right)}\\\\ &{\\quad\\quad\\quad p_{\\gamma}:=\\sigma\\left(\\frac{\\beta}{1+\\gamma}\\left[(U(\\pmb{x},y^{\\prime})-U(\\pmb{x},y))+\\beta^{-1}\\gamma\\log\\frac{\\pi_{\\mathrm{ref}}(y|\\pmb{x})}{\\pi_{\\mathrm{ref}}(y^{\\prime}|x)}\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "822 has the form ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi_{\\star}(\\pmb{y}|\\pmb{x})=Z^{-1}(\\pmb{x})\\exp\\left[-\\frac{\\beta}{1+\\gamma}\\big(U(\\pmb{x},\\pmb{y})-\\beta^{-1}\\gamma\\log\\pi_{\\mathrm{ref}}(\\pmb{y}|\\pmb{x})\\big)\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "823 We do not, however, prove that gradient descent of $\\pmb{\\theta}$ on (18) converges to the global minimizer (19)   \n824 because such an argument requires additional assumptions about the parametric class of policies and   \n825 the convexity of the objective with respect to the parameters, neither of which are straightforward to   \n826 establish. ", "page_idx": 20}, {"type": "text", "text": "827 A.1 Comparison with PPO Objective ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "828 The free energy functional for a policy under the energy rank alignment framework can be written as   \n829 an expectation ", "page_idx": 20}, {"type": "equation", "text": "$$\nJ_{\\mathrm{ERA}}[\\pi]=\\mathbb{E}_{\\mathbf{x}\\sim\\nu}\\left[\\int U(\\pmb{x},y)\\mathrm{d}\\pi(y|\\pmb{x})+\\beta^{-1}\\int(1+\\gamma)\\log\\pi(y|\\pmb{x})-\\gamma\\log(\\pi_{\\mathrm{ref}}(y|\\pmb{x})\\mathrm{d}\\pi(y|\\pmb{x})\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "830 involving an energetic term and an entropic term. The additional regularization acts as an effective   \n831 energetic bias. Solving for the extremum of this functional by setting Fr\u00e9chet derivative with respect   \n832 to $\\pi$ equal to zero, one obtains the formal solution (19) for the minimizer. This objective differs from   \n833 the regularized reward loss conventionally used for PPO, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\mathrm{PPO}}(\\pi)=\\mathbb{E}_{\\boldsymbol{x}}\\left[\\int U(\\pmb{x},\\pmb{y})\\mathrm{d}\\pi(\\pmb{y}|\\pmb{x})+\\gamma\\beta^{-1}\\int\\log\\frac{\\pi(\\pmb{y}|\\pmb{x})}{\\pi_{\\mathrm{ref}}(\\pmb{y}|\\pmb{x})}\\mathrm{d}\\pi(\\pmb{y}|\\pmb{x})\\right],}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{\\pmb{x}}\\left[\\int U(\\pmb{x},\\pmb{y})\\mathrm{d}\\pi(\\pmb{y}|\\pmb{x})+\\gamma\\beta^{-1}D_{\\mathrm{KL}}\\big(\\pi(\\cdot|\\pmb{x})|\\pi_{\\mathrm{ref}}(\\cdot|\\pmb{x})\\big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "834 The minimizer of the PPO objective (21) is also a Gibbs-Boltzmann measure, explicitly, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi_{\\star}^{(\\mathrm{PPO})}\\propto\\exp\\left[-\\frac{\\beta}{\\gamma}U(\\pmb{x},\\pmb{y})+\\log\\pi_{\\mathrm{ref}}(\\pmb{y}|\\pmb{x})\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "835 Here, the KL-regularization corresponds to an energy shift, as in our objective, but there is no limit in   \n836 which the ideal distribution $\\pi\\propto\\stackrel{\\mathbf{\\Lambda}}{e}^{-\\beta U}$ is obtained for the PPO objective. This is in stark contrast   \n837 to our approach, which recovers the ideal distribution as $\\gamma\\to0$ . Furthermore, while our approach   \n838 allows for a direct gradient-based optimization using (18), PPO is implemented using an actor-critic   \n839 framework that is difficult to tune [25, 9]. Finally, we emphasize that for ERA in the $\\gamma\\to0$ , finite   \n840 $\\beta>0$ , the distribution has positive entropy and is not manifestly mode-seeking; there can still be   \n841 appreciable fluctuations in the output. Eliminating the effect of regularization in (22), on the other   \n842 hand, requires taking $\\beta/\\gamma\\rightarrow\\infty$ , which eliminates fluctuations in the distribution. ", "page_idx": 20}, {"type": "text", "text": "843 A.2 Comparison with DPO Objective ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "844 The DPO approach also seeks to optimize the objective (21). The algorithm does so by first using (22)   \n845 to define an implicit reward model by solving for the $U$ that reflects the observed preference probabil  \n846 ities. This elegant idea has had a significant impact and has already been deployed in state-of-the-art   \n847 models [1]. In many cases, the observed preference probabilities will be sampled and only perhaps   \n848 only one observation of $\\boldsymbol{y}\\succ\\boldsymbol{y}^{\\prime}$ will be available for each $\\textbf{\\em x}$ in the dataset. When the preference   \n849 dataset only has one observation $\\pmb{y}\\succ\\pmb{y}^{\\prime}$ per prompt $\\textbf{\\em x}$ , the optimal policy requires that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi_{\\star}^{\\mathrm{DPO}}(y|x)=1\\quad\\mathrm{and}\\quad\\pi_{\\star}^{\\mathrm{DPO}}(y^{\\prime}|x)=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "850 The sampled gradients of the objective used for DPO are proportional to the implicit reward discrep  \n851 ancy, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\hat{\\mathcal{L}}^{\\mathrm{DPO}}(y,y^{\\prime},x)=\\sigma\\left(\\beta^{-1}\\gamma\\left[\\log\\frac{\\pi_{\\theta}(y^{\\prime}|x)}{\\pi_{\\mathrm{ref}}(y^{\\prime}|x)}-\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right]\\right)\\nabla_{\\theta}\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta}(y^{\\prime}|x)},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "852 which when $\\pi_{\\theta}(y^{\\prime}|x)\\to0$ , could lead to instability as $-\\log\\pi_{\\pmb{\\theta}}(\\pmb{y}^{\\prime}|\\pmb{x})\\rightarrow\\infty$ . On the other hand, the   \n853 ERA gradients are scaled by the relative preference discrepancy, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{L}^{\\mathrm{ERA}}(y,y^{\\prime},x)=\\left(\\frac{1-\\sigma_{\\star}(y\\succ y^{\\prime}|x)}{1-\\sigma_{\\theta}(y\\succ y^{\\prime}|x)}-\\frac{\\sigma_{\\star}(y\\succ y^{\\prime}|x)}{\\sigma_{\\theta}(y\\succ y^{\\prime}|x)}\\right)\\nabla_{\\theta}\\sigma_{\\theta}(y\\succ y^{\\prime}|x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "854 The advantage of a reward model becomes apparent because ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sigma_{\\star}(y\\succ y^{\\prime}|x)=p_{\\gamma}(y\\succ y^{\\prime}|x)=\\sigma\\left(\\frac{\\beta}{1+\\gamma}\\left[(U(\\pmb{x},y^{\\prime})-U(\\pmb{x},y))+\\beta^{-1}\\gamma\\log\\frac{\\pi_{\\mathrm{ref}}(y|x)}{\\pi_{\\mathrm{ref}}(y^{\\prime}|x)}\\right]\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "855 and hence the optimum of $\\mathcal{L}^{\\mathrm{ERA}}$ will not lead to policies in which $\\operatorname{supp}(\\pi_{\\theta})$ degrades unless the   \n856 energy becomes infinite. Choosing an appropriate reward model, hence, gives the flexibility to control   \n857 instability if it becomes problematic. ", "page_idx": 21}, {"type": "text", "text": "858 B ERA implementation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "859 Implementing energy rank alignment is straightforward to implement within existing code bases. We   \n860 provide sample PyTorch code for the ERA loss function below. ", "page_idx": 21}, {"type": "text", "text": "import torch.nn as nn from torch.nn.functional import logsigmoid def era_loss(pi_logps_1, pi_logps_2, ref_logps_1, ref_logps_2, energies_1, energies_2, beta, gamma): \"\"\" pi_logps_1: logprob under policys model of first sequence in pair (B,) pi_logps_2: logprob under policys model of second sequence in pair (B,) ref_logps_1: logprob under reference model of first sequence in pair (B,) ref_logps_2: logprob under reference model of second sequence in pair (B,) energies_1: energies of first sequence in pair (B,) energies_2: energies of second sequence in pair (B,) beta: inverse temperature gamma: regularization controlling strength of KL penalty \"\"\" beta_prime $=$ (beta / (1 + gamma)) gamma_prime $=$ (gamma / ( $1+$ gamma)) logp $=$ logsigmoid(policy_logps_y2 - policy_logps_y1) logp_prime $=$ logsigmoid(policy_logps_y1 - policy_logps_y2) logp_star $=$ logsigmoid(-beta_prime $^*$ (energies_y2 - energies_y1) $^+$ gamma_prime $^*$ (ref_logps_y2 - ref_logps_y1)) logp_star_prime $=$ logsigmoid(-beta_prime $^*$ (energies_y1 - energies_y2) $^+$ gamma_prime $^*$ (ref_logps_y1 - ref_logps_y2)) era_loss $=$ (torch.exp(logp_star) $^*$ (logp_star - logp) $^+$ torch.exp(logp_star_prime) $^*$ (logp_star_prime - logp_prime)) return era_loss.mean() ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "861 C Details for molecular generator experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "862 C.1 Pretraining details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "863 In this work, we represent all molecules as SMILES strings and tokenize SMILES strings according   \n864 to the approach in [30]. Our dataset consisted of all small-molecules from the ChEMBL database   \n865 that were of length 500 tokens or less. Ultimately, this token limit filtered out approximately $0.1\\%$   \n866 of the small-molecules in the original ChEMBL dataset. The alphabet generated from this curated   \n867 dataset consists of 324 tokens, which we augmented with start, stop, and padding tokens.   \n868 We first pretrained a model according to a next-token prediction, self-supervised learning approach.   \n869 We trained a model using the standard cross entropy loss ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CE}}=-\\sum_{t=1}^{T}\\log p_{\\theta}(\\pmb{x}_{t+1}|\\pmb{x}_{1:t}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "870 Our trained molecular generator consisted of just the encoder block of a standard multi-head attention   \n871 transformer [34]. Finally, the model had 2 layers, 8 heads, and a width of 512. For pretraining,   \n872 we used an Adam optimizer with a learning rate of $1.0*10^{-5}$ . We emphasize that this pretrained   \n873 generator samples molecules in an unprompted fashion; given just a start-of-sequence token, we can   \n874 autoregressively generate a sequence of tokens. Moreover, it is possible that this sequence of tokens   \n875 corresponds to a molecule that is not chemically valid, and we find that around $88\\%$ of all generated   \n876 molecules are chemically valid. Lastly, we measure the diversity of the pretrained molecular generator   \n877 by first generating 1500 molecules and then computing the Tanimoto similarity between every pair   \n878 of molecules. We plot the distribution of all pairwise Tanimoto similarities from this sample and   \n879 from all pariwise Tanimoto similarities from 1500 randomly sampled molecules from the original   \n880 dataset in Fig. 6. We observe that we can generate molecules that are quite distinct (i.e. low Tanimoto   \n881 similarity) in comparison with all other molecules. ", "page_idx": 22}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/9a01e52747627514684ce9b23fc56ac390232aa68b9314c68fbe193a458c6acb.jpg", "img_caption": ["Figure 6: Chemical diversity of samples from training dataset and from unprompted molecular generator (unaligned) as measured by pairwise Tanimoto similarities. Lower Tanimoto similarities correspond to more chemically dissimilar molecules. ", "Table 1: Definitions of energy functions (in reduced units) used for each of the five chemical properties investigated in this work. Here $\\textit{\\textbf{y}}$ refers to the generated molecule. "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "5ClpGA0u9K/tmp/468a5318f6d0319882c3659f78764fb812cb37d73d06962358ab4d2259a697dd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "882 C.2 Chemical properties ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "883 We investigated aligning the molecule generator to several target chemical properties, which we detail   \n884 below. All of the properties can be easily computed using the RDKit package. We list the energy   \n885 function and parameters used for the corresponding energy functions for each of these properties in   \n886 Table 1.   \n887 Tanimoto similarity is a measure of chemical and structural properties between two molecules and   \n888 ranges from 0 to 1, where higher values correspond to more similar molecules [26]. Quantitative   \n889 estimation of drug-likeness (QED) is evaluated by taking the geometric mean of a set of \u201cdesirability   \n890 functions\u201d for different molecular descriptors and also ranges continuously from values of 0 to 1 [6],   \n891 where higher values correspond to more drug-like molecules. The octanol-water parition coefficient   \n892 (Wildman-Crippen LogP) is a measure of hydrophobicity frequently employed in medicinal chemistry   \n893 applications [36]. Molecules with more positive values are more hydrophobic (i.e. more soluble   \n894 in octanol relative to water), whereas molecules with more negative values are more hydrophilic   \n895 (i.e. more soluble in water relative to octanol). Molar refractivity is similarly calculated as a linear   \n896 combination of atomic contributions, and is a positive number that serves as a measure for molecular   \n897 size and polarizability [36]. A higher molar refractivity corresponds to larger and more polarizable   \n898 molecules. Finally, ring count corresponds to the number of rings in a molecule.   \n899 Under the definitions of the energy functions in Table 1, it is possible for a generated sequence to   \n900 not be chemically valid. For these cases, we manually define energies that are sufficiently high to   \n901 penalize that outcome and we report these values in Table 2. Furthermore, when the computed QED   \n902 or Tanimoto Similarity is 0, the energy is infinite, and to ensure numerical stability, we set the value of   \n903 the energies to be 4.5 and 10 respectively. Finally, in the prompted molecular generator experiments   \n904 in Section 4.1.2, we assign an energy of 3.5 to the setting where Tanimoto similarity between the ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "5ClpGA0u9K/tmp/3e7ed83d49f938467f084e2154e6ecb0a2886f2256cffcf3ccf2b076c310e672.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 2: Property-specific energy values (in reduced units) used to treat chemically invalid sequences. ", "page_idx": 24}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/a205441f9698a3eda56ae74807762fca680b9748ad712ba748b0eaa0e6fbae78.jpg", "img_caption": ["Tanimoto Similarity "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 7: Chemical diversity of samples from unprompted molecular generator after alignment as measured by pairwise Tanimoto similarities. (See Fig. 2, Section 4.1.1) ", "page_idx": 24}, {"type": "text", "text": "905 generated and prompt molecule is 1.0 (i.e they are the same) in order to penalize this outcome. Here,   \n906 all energy and $\\beta$ values are reported in reduced units. ", "page_idx": 24}, {"type": "text", "text": "907 C.3 Molecular alignment details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "908 C.3.1 Unprompted molecular generation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "909 We first investigated aligning the unprompted molecular generator to sample small-molecules with   \n910 desired properties. We carried out alignment using the property-specific energies described in Table 1.   \n911 All alignment properties were initialized with the weights of the pretrained model and trained using an   \n912 Adam optimizer with learning rate $1.0*10^{-6}$ . We tabulate the chemical validity for single-property   \n913 alignment in Table 3 and for multi-property alignment in Table 4. While we do see a drop in chemical   \n914 validity after alignment, we see that a majority of the samples we generate post-alignment are still   \n915 chemically valid despite no regularization to a reference policy. We measure the chemical diversity ", "page_idx": 24}, {"type": "table", "img_path": "5ClpGA0u9K/tmp/c9fbf8c766edb7bd0a5a15b704f6def08387d1676d07563c37284af981e3632f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 3: Percentage of generated sequences that were chemically valid for samples from unprompted molecular generator after alignment. (See Fig. 2, Section 4.1.1). ", "page_idx": 24}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/2d7d57ad0b46af0e5482a33fd43c95d8bdd270f927abe763d6917f5e71eb5b52.jpg", "img_caption": ["Tanimoto Similarity "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 8: Chemical diversity of samples from unprompted molecular generator after multi-property alignment as measured by pairwise Tanimoto similarities. (See Fig. 3, Section 4.1.1). ", "page_idx": 25}, {"type": "table", "img_path": "5ClpGA0u9K/tmp/8410a13e037bdb3f74562a178400a6f9e689080b0e8933d69f6f7c4b20467515.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 4: Percentage of generated sequences that were chemically valid for samples from unprompted molecular generator after multi-property alignment. (See Fig. 3, Section 4.1.1). ", "page_idx": 25}, {"type": "text", "text": "916 for these experiments by computing all pairwise Tanimoto similarities from all chemically valid   \n917 predictions of 1500 generated molecules. We visualize the chemical diversity for single-property   \n918 experiments in Fig. 7 and multi-property experiments in Fig. 8. We observe that the samples are still   \n919 highly diverse chemically after alignment. All plots in Fig. 2 and Fig. 3 were computed using 1500   \n920 generated molecules per experiment. ", "page_idx": 25}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/b034b3d848d8958f8eccd6da15ba77780268fed65f787b269fb64877b99ec61f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 9: Sample molecules from prompted molecular generator after multi-property alignment experiments: QED and Tanimoto (left) and LogP and Tanimoto (right). With alignment, generated molecules are diverse, while still chemically similar to prompt molecule. ", "page_idx": 25}, {"type": "table", "img_path": "5ClpGA0u9K/tmp/de17a0c9400dfcd1dd434028121d6033ae5617524e9ac59710ae4f1893c00aee.jpg", "table_caption": [], "table_footnote": ["Table 5: Percentage of generated sequences that were chemically valid for samples from prompted molecular generator after multi-property alignment. (See Fig. 4, Section 4.1.2). "], "page_idx": 26}, {"type": "text", "text": "921 C.3.2 Prompted molecular generation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "922 Next, we generate small-molecules with desired properties conditioned on a prompt, where the   \n923 prompt is itself another molecule. In the experiments here, we consider the setting where we generate   \n924 molecules that are chemically similar to the prompt molecule. With this in mind, we first carry out a   \n925 fine-tuning step using a synthetic dataset $\\bar{\\mathcal{D}}\\bar{=}\\,\\{(\\bar{\\pmb{x}_{1}},\\pmb{y}_{1}),\\dots,(\\pmb{x}_{n},\\pmb{y}_{n})\\}_{i=1}^{N}$ , where $\\textbf{\\em x}$ corresponds to   \n926 the SMILES string of a prompt molecule and $\\textit{\\textbf{y}}$ corresponds to the SMILES string of the conditionally   \n927 generated molecule. To curate this dataset, we consider all molecules in our original flitered ChEMBL   \n928 dataset to be a prompt molecules and for each prompt molecule $\\pmb{x}_{i}$ , we generate a response molecule   \n929 $\\pmb{y}_{i}$ by simply perturbing a random token from $\\pmb{x}_{i}$ . If the perturbed sequence was chemically invalid,   \n930 we repeated the random perturbation until a valid molecule was generated. The prompted generator   \n931 was the same size as the unprompted molecular generator, and we initialized the weights using those   \n932 of the pre-trained unprompted molecular generator. We then carried out supervised fine-tuning using   \n933 an Adam optimizer with learning rate $1.\\bar{0}*10^{-5}$ and used this generator as our reference policy   \n934 for all prompted alignment experiments. All plots in Fig. 4 were computed using 100 generated   \n935 molecules per prompt, where we carried inference over 500 prompts per experiment. ", "page_idx": 26}, {"type": "text", "text": "936 D Details for LLM experiments ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "937 D.1 GPT-2 seniment alignment ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "938 Similar to the experiments run in [25], we carried out alignment of a GPT-2 model fine-tuned on a   \n939 dataset of IMDb reviews to a pretrained sentiment model. For this experiment, we first carried out   \n940 supervised fine-tuning of gpt2-large using an 80/20 train/validation split of the 25000 reviews in   \n941 (stanfordnlp/imdb)[18].   \n942 Next, we carried out alignment of this fine-tuned model supervised by a sentiment classifier $p_{\\mathrm{sent}}$   \n943 siebert/sentiment-roberta-large-english [14]. Here, $p_{\\mathrm{sent}}$ corresponds to the probability   \n944 that the sentiment is a positive one. For each of the 25000 reviews, we considered the first 8 tokens   \n945 as a \u201cprompt,\u201d and for each of these prompts, sampled four completions with maximum length 256   \n946 tokens. We evaluated the energy of these completions under the sentiment classifier, where the energy   \n947 $U_{\\mathrm{sent}}=-\\log{p_{\\mathrm{sent}}}$ . We used all 6 preference pairs for each of the 25000 prompts to carry out energy   \n948 rank alignment for 3 epochs.   \n949 Finally, using the aligned models, we carried out inference on 7500 prompts of length 8 tokens   \n950 that were held out during the fine-tuning and alignment steps. For each prompt, we sampled four   \n951 responses with a maximum length of 256 tokens and plot the mean sentiment across all prompts in   \n952 Fig. 5 and the energies in Fig. 10. We include sample responses from one of the prompts in Table 6. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "953 D.2 LLaMA2 weak-to-strong alignment ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "954 We carried out \u201csuperalignment\u201d of a 13B LLaMA model (meta-llama/Llama-2-13b-hf) super  \n955 vised by a 7B LLaMA model (meta-llama/Llama-2-7b-chat-hf) [15]. Importantly, the 13B   \n956 model we use here has only been pretrained using self-supervised learning and has not been further   \n957 optimized using strategies such as supervised fine-tuning and RLHF. The 7B model here has been   \n958 further optimized with supervised fine-tuning and RLHF and is designed for chat applications. Here,   \n959 for a completion $\\textit{\\textbf{y}}$ given a prompt $\\textbf{\\em x}$ , we define the energy of $U(\\pmb{y},\\pmb{x})=-\\log\\pi_{\\mathrm{weak}}(\\pmb{y}|\\pmb{x})$ , where   \n960 $\\pi_{\\mathrm{weak}}(y|\\bar{\\boldsymbol{x}})$ is evaluated as the probability using LLaMA2-7B-chat.   \n961 We first carried out a short supervised fine-tuning step of the 13B model to ensure that it could   \n962 respond appropriately to chat style prompts. Using 15000 prompts from the Anthropic Helpful and   \n963 Harmless dataset (Anthropic/hh-rlhf), we generated a synthetic dataset of suitable responses   \n964 using zero-temperature samples from LLaMA-7B-chat and carried out supervised fine-tuning for 3   \n965 epochs. All responses generated had a maximum length of 128 tokens.   \n966 We note that we first attempted to carry out supervised fine-tuning directly using responses from the   \n967 Anthropic HH dataset. However, the evaluated energies of responses generated using the resulting   \n968 model were significantly high energy, making alignment infeasible. With this synthetic dataset, we   \n969 were able to fine-tune LLaMA2-13B to generate more responses in a chat-style format with more   \n970 reasonable energies.   \n971 We emphasize that in a real-life setting, one would have access to a dataset of high quality responses   \n972 to carry out fine-tuning and the strategy we used was merely a proxy to generate a comparable dataset.   \n973 Furthermore, we note that by using zero-temperature sampling, we obtained samples from the modes   \n974 of our target distribution and did not directly carry out supervised fine-tuning on samples from our   \n975 target distribution.   \n976 Upon fine-tuning LLaMA2-13B, for each of the 15000 prompts, we generated 4 responses and carried   \n977 out ERA using all 6 preference pairs for one epoch. All responses generated had a maximum length   \n978 of 128 tokens.   \n979 Using 7500 prompts held out during the fine-tuning and alignment steps, we generated 4 responses   \n980 also with a maximum length of 128 tokens. Using these generated responses, we evaluated the safety   \n981 using Meta LLaMA Guard (see Fig. 5).   \n982 We also carried out alignment using DPO, where we used $\\beta_{\\mathrm{DPO}}=0.1$ and for a given pair, preference   \n983 was assigned to the lower-energy sample. Here, $\\beta_{\\mathrm{DPO}}$ is defined as in [25] and is different from   \n984 the $\\beta$ used in the definition of ERA. We observe that, with our implementation, DPO is prone to   \n985 mode collapse. While it generates very low energy samples, these samples are not sentence-like and   \n986 practically not useful responses. We provide visualizations of the energy distribution in Fig. 11 and   \n987 sample responses in Table 7. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/f3cbf75ed8a89a72b6218b821eee9e31da6d058c70fc1942f2bea3115121cd92.jpg", "img_caption": ["Figure 10: Distribution of energies evaluated by sentiment model for aligned GPT-2 models across varying $\\beta$ and $\\gamma$ . "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "988 E Computational resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "989 For all chemical alignment experiments, we trained on an in-house cluster with 8 Nvidia 4080 GPUs.   \n990 For LLM experiments, we used resources of the National Energy Research Scientific Computing   \n991 Center (NERSC), a Department of Energy Office of Science User Facility. Jobs run on NERSC used   \n992 at most 4 Nvidia A100 GPUs (either 40GB or 80GB depending on what was allocated). ", "page_idx": 27}, {"type": "text", "text": "993 F Societal and broader impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "994 The ERA algorithm we have introduced in this work is a powerful and scalable approach towards gen  \n995 erating outputs targeting some desired combination of properties. In this work we have demonstrated   \n996 the efficacy of this method in both a chemical context and a language context. There is potential for   \n997 intentional misuses of the alignment strategy, where models are aligned to generate harmful content   \n998 or toxic chemicals. ", "page_idx": 27}, {"type": "table", "img_path": "5ClpGA0u9K/tmp/4b67de50a0693802bdfb0a4fce2cca94b1efe415882068b1d27304221b4f3119.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "image", "img_path": "5ClpGA0u9K/tmp/101056f259395ff0b33bad4e953cdd9d89578c2c09d90e335d1f6f9e1bb51f9c.jpg", "img_caption": ["Figure 11: Distribution of energies evaluated by LLaMA2-7B-Chat for aligned LLaMA2-13B models across varying $\\beta$ and $\\gamma$ . "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "5ClpGA0u9K/tmp/68bd63e173fce7172d5b4499b1fd62f18bb9090dc95cc851d9d5b5fe6e4eae45.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 7: Sample responses for aligned LLaMA2-13B model for both ERA and DPO. With our implementation of DPO, alignment collapses onto a low-energy but incoherent response, while ERA outputs meaningful responses across varying $\\beta$ even with no regularization. ", "page_idx": 29}]