{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that is extensively used and adapted in the current work."}, {"fullname_first_author": "Harsh Agrawal", "paper_title": "NoCaps: novel object captioning at scale", "publication_date": "2019-10-01", "reason": "This paper provides the NoCaps dataset, a crucial resource for evaluating vision-language tasks, particularly image captioning, which is one of the downstream tasks explored in this paper."}, {"fullname_first_author": "Jia Deng", "paper_title": "ImageNet: A large-scale hierarchical image database", "publication_date": "2009-06-01", "reason": "This paper introduces the ImageNet dataset, a benchmark for image classification that the authors use for experiments and comparisons."}, {"fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft COCO: common objects in context", "publication_date": "2014-09-01", "reason": "The COCO dataset, introduced in this paper, is used for data and is fundamental to the work on vision-language tasks."}, {"fullname_first_author": "Sarah Pratt", "paper_title": "What does a platypus look like? Generating customized prompts for zero-shot image classification", "publication_date": "2023-10-01", "reason": "This paper introduces CuPL, a method for generating prompts using LLMs which is adapted and improved in the current paper"}]}