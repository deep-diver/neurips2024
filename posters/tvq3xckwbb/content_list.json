[{"type": "text", "text": "Enriching Disentanglement: From Logical Definitions to Quantitative Metrics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yivan Zhang Masashi Sugiyama The University of Tokyo, RIKEN AIP RIKEN AIP, The University of Tokyo Tokyo, Japan Tokyo, Japan yivanzhang@ms.k.u-tokyo.ac.jp sugi@k.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Disentangling the explanatory factors in complex data is a promising approach for generalizable and data-efficient representation learning. While a variety of quantitative metrics for learning and evaluating disentangled representations have been proposed, it remains unclear what properties these metrics truly quantify. In this work, we establish algebraic relationships between logical definitions and quantitative metrics to derive theoretically grounded disentanglement metrics. Concretely, we introduce a compositional approach for converting a higher-order predicate into a real-valued quantity by replacing (i) equality with a strict premetric, (ii) the Heyting algebra of binary truth values with a quantale of continuous values, and (iii) quantifiers with aggregators. The metrics induced by logical definitions have strong theoretical guarantees, and some of them are easily differentiable and can be used as learning objectives directly. Finally, we empirically demonstrate the effectiveness of the proposed metrics by isolating different aspects of disentangled representations. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In supervised learning, we usually use a real-valued cost function $\\ell:Y\\times Y\\to\\mathbb{R}_{>0}$ to measure how close an output $f(x)$ of a function $f:X\\to Y$ is to a target label $y$ , i.e., $\\ell(f(x),{\\overline{{y}}})$ , to quantify the cost of inaccurate prediction. Then, we can use the total cost over a collection of input-output pairs to measure the performance of this function. From a functional perspective, this construction induces a quantitative metric $L:[X,Y]\\times[X,Y]\\to\\mathbb{R}_{\\geq0}$ between functions:1 ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(f,g):=\\sum_{x\\in X}\\ell(f(x),g(x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $g:X\\to Y$ is a \u201cground-truth function\u201d that maps each input $x$ to its target label $y$ . This metric can be used as both learning objective and evaluation metric for the learning model $f:X\\to Y$ . What does $L(f,g)$ quantify? It quantifies the extent to which two functions $f$ and $g$ are equal: ", "page_idx": 0}, {"type": "equation", "text": "$$\n(f=_{[X,Y]}g):=\\forall x\\in X.\\ f(x)=_{Y}g(x).^{2}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Considering the equality as a predicate, we can observe a parallel between (binary-valued) equality $=_{Y}\\colon Y\\times Y\\rightarrow\\{\\top,\\bot\\}$ and (real-valued) cost $\\ell:Y\\times Y\\rightarrow\\mathbb{R}_{\\geq0}$ , $\\mathsf{I}$ universal quantifier (\u201cfor all\u201d) $\\forall x\\in X$ and summation $\\sum_{x\\in X}$ , and $\\mid$ function equality $=_{[X,Y]}$ : $[X,Y]\\times[X,Y]\\to\\{\\top,\\bot\\}$ a nd total cost $L:[X,Y]\\times[X,Y]\\to\\mathbb{R}_{\\geq0}$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "We would like to ask: $I s$ it possible to measure and optimize other properties in the same way? ", "page_idx": 0}, {"type": "text", "text": "In representation learning [Bengio et al., 2013], measuring and optimizing the performance of a learning model becomes a non-trivial task. The quality of a model cannot always be measured by how close it is to a fixed ground truth. Instead, we often need to consider the properties of the model architecture or learned representation itself, such as convexity [Amos et al., 2017], uniformity [Wang and Isola, 2020], invariance [Kvinge et al., 2022], and equivariance [Lee et al., 2019, Brehmer et al., 2023]. A proper comprehension of what constitutes good representations and how to assess their quality is important for designing suitable models, learning objectives, and evaluation metrics. ", "page_idx": 1}, {"type": "text", "text": "Disentangled representation learning: definitions, metrics, and methods Disentanglement is an important property in representation learning, which intuitively means that different explanatory factors in data should be encoded separately [Bengio et al., 2013]. However, disentanglement has no universally agreed-upon formal definition [Higgins et al., 2018, Suter et al., 2019, Shu et al., 2020, Fumero et al., 2021], and it is typically viewed not as a single property but rather as a combination of several requirements [Ridgeway and Mozer, 2018, Eastwood and Williams, 2018, Do and Tran, 2020, Tokui and Sato, 2022]. While many metrics for measuring disentanglement have been proposed [Carbonneau et al., 2022], it remains unclear what properties these metrics truly quantify and how they can be optimized directly. Often, a new evaluation metric is introduced along with a new learning method, but it is usually unproven that the method can optimize the new metric [Higgins et al., 2017, Kim and Mnih, 2018, Chen et al., 2018, Li et al., 2020]. This lack of theoretical understanding makes it difficult to design learning models that can effectively learn disentangled representations. ", "page_idx": 1}, {"type": "text", "text": "A logical and algebraic approach to defining and measuring disentangled representations Recently, Zhang and Sugiyama [2023] proposed a general and abstract definition of disentanglement, shedding light on the common structures underlying the algebraic, statistical, and topological definitions of disentanglement. It was shown that the abstract concept of product [Mac Lane, 1978] underlies an essential property of disentanglement called modularity [Ridgeway and Mozer, 2018], and other properties of learning models, such as informativeness [Eastwood and Williams, 2018], can also be defined abstractly using only the composition and identity of morphisms. Following this algebraic approach, we aim to derive theoretically grounded quantitative metrics of disentanglement from the logical definitions of the desired properties, extending the parallel between Eqs. (1) and (2). ", "page_idx": 1}, {"type": "text", "text": "Contributions In this paper, we focus on logically defined properties of disentangled representation learning, such as modularity and informativeness (Section 2). We introduce a compositional approach to converting a higher-order equational predicate into a real-valued quantity (Table 1), which serves as a quantitative metric of the extent to which a function satisfies the predicate (Section 3). Our analysis on the relationship between the logical definitions and the induced quantitative metrics provides theoretical guarantee on the properties of the optimal functions (Theorem 1). Then, we demonstrate the usefulness of this conversion method by deriving quantitative metrics for measuring properties of disentangled representations, and we analyze these metrics in terms of computation, optimization, and differentiability (Section 4). Lastly, we compare the derived metrics with several existing ones in a fully controlled experiment and demonstrate that the proposed metrics are able to isolate different aspects of disentangled representations (Section 5). ", "page_idx": 1}, {"type": "text", "text": "2 Logical definitions of disentangled representations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, let us first take a closer look at the logical definitions of two properties of disentangled representation learning \u2014 informativeness [Eastwood and Williams, 2018] and modularity [Ridgeway and Mozer, 2018], which are arguably more important than other properties [Carbonneau et al., 2022]. We limit our discussion to sets and functions, but the generalization to other morphisms, such as equivariant, stochastic, or continuous functions, is straightforward. ", "page_idx": 1}, {"type": "text", "text": "2.1 Informativeness: injectivity or retractability of a learning model ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Being informative, expressive, faithful, or useful is a basic requirement for learned representations [Bengio et al., 2013]. We want a representation learning model to preserve explanatory factors in data that are informative to the downstream tasks. For functions, this criterion could be formulated as follows: If two factors $y$ and $y^{'}$ are different, then their representations $m(y)$ and $m(y^{\\prime})$ extracted by a function $m:Y\\rightarrow Z$ should be different too. This means that the function $m$ should be injective: ", "page_idx": 1}, {"type": "text", "text": "Definition 1 (Injective function). A function $m:Y\\rightarrow Z$ is injective if ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\mathrm{injective}}(m:Y\\rightarrow Z):=\\forall y\\in Y.\\,\\forall y^{\\prime}\\in Y.\\,(m(y)=_{Z}m(y^{\\prime}))\\rightarrow(y=_{Y}y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Alternatively, because injective functions are precisely functions with retractions (left inverses) [Lawvere and Rosebrugh, 2003, Chapter 2], we can measure the retractability instead: ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Retractable function). A function $m:Y\\rightarrow Z$ has a retraction $h:Z\\to Y$ if ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\mathrm{retractable}}(m:Y\\to Z):=\\exists h:Z\\to Y.\\ h\\circ m=_{[Y,Y]}{\\mathrm{id}}_{Y}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that these properties are predicates pinjec ${\\mathrm{ive}},p_{\\mathrm{retractable}}:[Y,Z]\\to\\{\\top,\\bot\\}$ on the set $[Y,Z]$ of all functions from $Y$ to $Z$ . Analogous to using the total cost in Eq. (1) to measure function equality in Eq. (2), if we want to measure the injectivity in Eq. (3) or retractability in Eq. (4), we need to find quantitative counterparts of the implication $\\rightarrow$ , universal quantifier $\\forall$ , and existential quantifier \u2203used in their logical definitions. Generally, it is desirable to extend the parallel between Eqs. (1) and (2) to other predicates by finding quantitative operations corresponding to logical connectives and quantifiers. This correspondence allows us to construct and analyze quantitative metrics for machine learning models in a compositional manner [Boole, 1854]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Modularity: product structure preserved by a learning model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Modularity [Ridgeway and Mozer, 2018] is an essential property of disentangled representation learning, which means that the explanatory factors in data, such as the color and shape of an object, are separated into independent components in the learned representation [Bengio et al., 2013]. ", "page_idx": 2}, {"type": "text", "text": "As shown in Fig. 1, modularity can be defined as follows. We assume that data with multiple explanatory factors (e.g., color and shape) is generated via a function $g:Y\\rightarrow X$ from a product $Y:=Y_{1}\\times Y_{2}$ of factors. An encoder $f:X\\to Z$ is a function to a product $Z\\,:=\\,Z_{1}\\,\\times\\,Z_{2}$ of codes. Then, an encoder is said to be modular if it can reconstruct the product structure, such that the composition $m:=f\\circ g:$ $Y\\,\\rightarrow\\,Z$ of the generator $g$ and the encoder $f$ is a product function. ", "page_idx": 2}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/606b33c2181a9a77f2081351777c3544f950c57e47864307885cde955e2ee062.jpg", "img_caption": ["Figure 1: Disentangled representation learning "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Formally, being a product function is also a property that can be represented as a predicate: ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Product function). Let $Y:=Y_{1}\\times Y_{2}$ and $Z:=Z_{1}\\times Z_{2}$ be products of sets. A function $m:Y\\rightarrow Z$ is a product function if ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\mathrm{product}}(m:Y\\rightarrow Z):=\\exists m_{1,1}:Y_{1}\\rightarrow Z_{1}.\\,\\exists m_{2,2}:Y_{2}\\rightarrow Z_{2}.\\,m=_{[Y,Z]}\\,m_{1,1}\\times m_{2,2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Example 1. Let us compare the following two functions from $Y:=\\{0,1\\}^{2}$ to $Z:=\\mathbb{R}^{2}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nm:=\\left\\{\\begin{array}{l l}{(0,0)\\mapsto(1,2)}\\\\ {(0,1)\\mapsto(3,4)}\\\\ {(1,0)\\mapsto(5,6)}\\\\ {(1,1)\\mapsto(7,8)}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\nm^{\\prime}:=\\left\\{\\!\\!\\begin{array}{l l}{(0,0)\\mapsto(a,c)}\\\\ {(0,1)\\mapsto(a,d)}\\\\ {(1,0)\\mapsto(b,c)}\\\\ {(1,1)\\mapsto(b,d)}\\end{array}\\!\\!\\right.=\\underbrace{\\!\\!\\left\\{0\\mapsto a\\!\\!\\!\\begin{array}{l l}{\\!\\!\\leq}&{}\\\\ {\\!\\!\\!-b\\!\\!\\!}\\\\ {\\!\\!\\!m_{1,1}}\\end{array}\\right\\}}_{m_{2,2}}\\underbrace{\\!\\!\\left\\{0\\mapsto c\\!\\!\\!\\begin{array}{l l}{\\!\\!\\leq}&{}\\\\ {\\!\\!\\!-b\\!\\!\\!}\\\\ {\\!\\!\\!m_{2,2}}\\end{array}\\right.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $a,b,c,$ and $d$ are arbitrary real numbers. According to Definition 3, only $m^{\\prime}=m_{1,1}\\times m_{2,2}$ is a product function, whose first/second output depends only on the first/second input. ", "page_idx": 2}, {"type": "text", "text": "In Example 1, although $m$ is not a product function, we want to address the following questions: ", "page_idx": 2}, {"type": "text", "text": "(Metric) Can we quantify the extent to which it resembles a product function? (Approximation) Can we find a product function that is closest to it? (Differentiability) Can we make it slightly closer to a product function? ", "page_idx": 2}, {"type": "text", "text": "Answers to these questions will be given in the following sections. ", "page_idx": 2}, {"type": "text", "text": "3 Enrichment: from logic to metric ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In Appendices A and B, we describe in detail the theory of converting a higher-order predicate into a real-valued quantity. In this section, we only introduce the conversion procedure using concrete examples and present the theoretical results. A summary of the conversion is given in Table 1. ", "page_idx": 3}, {"type": "text", "text": "First of all, let us clarify the terms predicate and quantity. In the realm of classical logic, a predicate $p:A\\,\\rightarrow\\,\\{\\top,\\bot\\}$ on a set $A$ is a function from the set $A$ to the set $\\{\\top,\\bot\\}$ of binary truth values. For example, the predicates pinjective, $p_{\\mathrm{retractable}}$ , and $p_{\\mathrm{product}}$ in Definitions 1 to 3 are functions from the set $[Y,Z]$ of functions to the set $\\{\\top,\\bot\\}$ . They are logical definitions of some properties of functions. On the other hand, in this work, a quantity $q:A\\rightarrow[0,\\infty]$ on a set $A$ is defined as a function to the set $[0,\\infty]$ of extended non-negative real numbers. The quantities associated with a predicate will serve as quantitative metrics for the property defined by the predicate. ", "page_idx": 3}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/5a6aef37beab88b174442d22188a0ffc6f221aec789be2fe1f9b82c63f12d0dc.jpg", "table_caption": ["Table 1: From logic to metric "], "table_footnote": ["\u2217truncated subtraction: $b\\div a:=\\operatorname*{max}\\{b-a,0\\}$ \u2217\u2217 e.g., maximum, sum, mean, and mean square "], "page_idx": 3}, {"type": "text", "text": "3.1 From equality predicate to strict premetric ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, a predicate of central importance is the equality predicate $=_{A}$ : $A\\times A\\to\\{\\top,\\bot\\}$ [Mazur, 2008]. A quantity associated with the equality predicate should be a strict premetric: ", "page_idx": 3}, {"type": "text", "text": "Definition 4 (Strict premetric). A strict premetric on a set $A$ is a function $d_{A}:A\\times A\\to[0,\\infty]$ that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall a\\in A.\\;\\forall a^{\\prime}\\in A.\\;(d_{A}(a,a^{\\prime})=0)\\leftrightarrow(a=_{A}a^{\\prime}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 From logical operation to quantitative operation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Next, let us have a look at the logical connectives and quantifiers used in the definitions of properties. The product of sets and functions plays a significant role in this work. Two functions $f,g:C\\rightarrow A{\\times}B$ to a product are equal if and only if all their component functions are equal: ", "page_idx": 3}, {"type": "equation", "text": "$$\n(f=_{[C,A\\times B]}g):=(f_{1}=_{[C,A]}g_{1})\\wedge(f_{2}=_{[C,B]}g_{2}).^{5}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that the conjunction ${\\wedge:\\{\\top,\\bot\\}\\times\\{\\top,\\bot\\}\\to\\{\\top,\\bot\\}}$ , a logical connective, is used in Eq. (9). To obtain a corresponding quantity, we replace it with the addition $+:[0,\\infty]\\times[0,\\infty]\\to[0,\\bar{\\infty}]$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{[C,A\\times B]}(f,g):=d_{[C,A]}(f_{1},g_{1})+d_{[C,B]}(f_{2},g_{2}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The universal quantifier on a set $A$ is a specific (second-order) predicate $\\forall_{A}:\\{\\top,\\bot\\}^{A}\\to\\{\\top,\\bot\\}$ on the set $\\{\\top,\\bot\\}^{A}$ of predicates. We can replace it with the supremum sup : $\\left[0,\\infty\\right]^{A}\\to\\left[0,\\infty\\right]$ . We can also choose a function from the (i) maximum, (ii) sum, (iii) mean, and (iv) mean square when the set $A$ is finite. More generally, we can replace it with a quantity $\\nabla_{A}:[0,\\infty]^{A}\\to[{\\bar{0,}}\\infty]$ on the set $\\lbrack0,\\infty]^{A}$ of quantities that satisfies some conditions, which we refer to as a (universal) aggregator. Intuitively, a universal aggregator should output 0 if and only if all inputs are 0. Therefore, the median, mode, and range are non-examples. Different choices of aggregators yield metrics with different characteristics in computation and optimization. For example, the function equality predicate ", "page_idx": 3}, {"type": "equation", "text": "$$\n(f=_{[A,B]}g):=\\forall a\\in A.\\ f(a)=_{B}g(a)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "converts to a quantity whose aggregator $\\bigtriangledown$ is not limited to the sum (cf. Eqs. (1) and (2)): ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{[A,B]}(f,g):=\\operatorname{\\nabla}_{a\\in A}d_{B}(f(a),g(a)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Dually, we also need to consider the disjunction $\\vee$ and the existential quantifier \u2203. We replace them with the minimum and the infimum, respectively. Lastly, we replace the implication $a\\rightarrow b$ with the (truncated) subtraction $b\\stackrel{.}{-}a:=\\operatorname*{max}\\{b-a,0\\}$ . These operations are illustrated in Fig. 2. ", "page_idx": 3}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/5137972c284f3ba10eb0070c2598e7b2265c7d876d44c52fc07c0aef210640b2.jpg", "img_caption": ["Figure 2: From predicates and logical operations to quantities and quantitative operations "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 From compound predicate to compound quantity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following Table 1, we can convert any compound predicate defined using equational predicates and logical operations into a corresponding compound quantity defined using strict premetrics and quantitative operations. Our main result on their relationship is as follows: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $p:A\\to\\{\\top,\\bot\\}$ be a predicate on a set $A$ , and let $q:A\\rightarrow[0,\\infty]$ be a quantity converted from $p$ according to Table 1. Then, for any $a\\in A$ , $q(a)=0$ implies $p(a)=\\top$ . Conversely, for any $a\\in A$ , $p(a)=\\top$ implies $q(a)=0$ if and only if p does not contain the implication. ", "page_idx": 4}, {"type": "text", "text": "The implication is special because we must sacrifice logical equivalence for the sake of continuity, which is necessary for gradient-based optimization. We will explore this through a concrete example regarding injectivity in Section 4.3 and discuss it in detail in Appendices B and D. ", "page_idx": 4}, {"type": "text", "text": "3.4 (Sub)homomorphism from metric to logic ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Finally, for readers interested in the theoretical background, we briefly introduce the following algebraic concepts and a proof sketch underlying Table 1 and Theorem 1. ", "page_idx": 4}, {"type": "text", "text": "Definition 5 (Zero predicate). The zero predicate $\\zeta:[0,\\infty]\\to\\{\\top,\\bot\\}:=x\\mapsto(x=0)$ is a function that maps 0 to true $\\intercal$ and any positive value to false $\\perp$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 6 ((Sub)homomorphism from a quantity to a predicate). Let $A$ be a set. A quantity $q:A\\rightarrow[0,\\infty]$ on the set $A$ is homomorphic to a predicate $p:A\\to\\{\\top,\\bot\\}$ via the zero predicate $\\zeta:[0,\\infty]\\to\\{\\top,\\bot\\}$ if $\\zeta\\circ q=p$ , and is subhomomorphic to $p$ if $\\zeta\\circ q\\to p$ .6 ", "page_idx": 4}, {"type": "text", "text": "Definition 7 ((Sub)homomorphism from a quantitative operation to a logical operation). Let $n\\in\\mathbb N$ be a natural number. An $n$ -ary quantitative operation $\\alpha:[0,\\infty]^{n}\\rightarrow[0,\\bar{\\infty}]$ is homomorphic to a logical operation $\\beta:\\{\\top,\\bot\\}^{n}\\rightarrow\\{\\bar{\\top},\\bot\\}$ via the zero predicate $\\zeta:[0,\\infty]\\to\\{\\top,\\bot\\}$ if $\\zeta\\circ\\bar{\\alpha}=\\beta\\circ\\zeta^{n}$ , and is subhomomorphic to $\\beta$ if $\\zeta\\circ\\alpha\\rightarrow\\beta\\circ\\zeta^{n}$ .7 ", "page_idx": 4}, {"type": "text", "text": "Definition 8 ((Sub)homomorphism from an aggregator to a quantifier). Let $A$ be a set. An aggregator $\\alpha_{A}:[0,\\infty]^{A}\\to[0,\\infty]$ on the set $A$ is homomorphic to a quantifier $\\beta_{A}:\\{\\top,\\bot\\}^{A}\\rightarrow\\{\\top,\\bot\\}$ via the zero predicate $\\zeta:[0,\\infty]\\rightarrow\\{\\top,\\bot\\}$ if $\\zeta\\circ\\alpha_{A}\\,=\\,\\beta_{A}\\circ\\zeta^{A}$ , and is subhomomorphic to $\\beta_{A}$ if \u03b6 \u25e6\u03b1A \u2192\u03b2A \u25e6\u03b6A.8 ", "page_idx": 4}, {"type": "text", "text": "Homomorphic quantities, quantitative operations, and aggregators can be illustrated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{A\\xrightarrow[\\![0,\\infty]]{\\quad\\mathrm{id}_{A}}\\to A}&{\\quad[0,\\infty]^{n}\\xrightarrow{\\zeta^{n}}\\{\\top,\\bot\\}^{n}}&{\\quad[0,\\infty]^{A}\\xrightarrow{\\zeta^{A}}\\{\\top,\\bot\\}^{A}}\\\\ {\\nsim[\\![0,\\infty]\\!]{\\quad}\\quad}&{\\quad\\times\\!\\!\\!\\!\\bigcup_{A}}&{\\quad\\mathord{\\downarrow,}}&{\\quad\\stackrel{\\mathrm{\\scriptsize~|}\\,\\beta\\quad}{\\longrightarrow}\\cdots\\not\\!\\!\\!\\bigcup_{A}}\\\\ {\\{0,\\infty]\\xrightarrow{\\zeta}\\{\\top,\\bot\\}}&{\\quad[0,\\infty]\\xrightarrow{\\zeta}\\{\\top,\\bot\\}}&{\\quad[0,\\infty]\\xrightarrow{\\zeta}\\{\\top,\\bot\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "6We use the infix notation, so $\\zeta\\circ q=p$ means that $\\forall a\\in A$ . $(q(a)=0)\\leftrightarrow p(a)$ , and $\\zeta\\circ q\\to p$ means that $\\forall a\\in A$ . $(q(a)=0)\\to p(a)$ . ${}^{7}\\zeta^{n}:[0,\\infty]^{n}\\rightarrow\\{\\top,\\bot\\}^{n}:=(q_{1},\\dots,q_{n})\\mapsto(q_{1}=0,\\dots,q_{n}=0)$ is the $_n$ -fold product of the zero predicate $\\zeta:[0,\\infty]\\to\\{\\top,\\bot\\}$ . $^{8}\\zeta^{A}:[0,\\infty]^{A}\\to\\{\\top,\\bot\\}^{A}:=\\zeta\\circ(-)$ is the postcomposition with the zero predicate $\\zeta:[0,\\infty]\\to\\{\\top,\\bot\\}$ that maps a quantity $q:A\\rightarrow[0,\\infty]$ to the predicate $\\zeta\\circ q:A\\rightarrow\\{\\top,\\bot\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Based on these algebraic concepts, we can say that strict premetrics are homomorphic to equality predicates, addition is homomorphic to conjunction (since the sum is zero if and only if both addends are zero), minimum is homomorphic to disjunction, truncated subtraction is subhomomorphic to implication, and universal aggregators are homomorphic to the universal quantifier. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 means that any compound quantity is (sub)homomorphic to the corresponding compound predicate if each component (quantities, quantitative operations, aggregators) is (sub)homomorphic to the corresponding component (predicates, logical operations, quantifiers). For implication, we use the truncated subtraction, which is only subhomomorphic, since there is no continuous operation that is homomorphic to implication (see also Appendix D). ", "page_idx": 5}, {"type": "text", "text": "More abstractly and concisely, we can say that we replace the Heyting algebra of truth values $\\{\\top,\\bot\\}$ with a quantale of extended non-negative real numbers $[0,\\infty]$ , and we replace the quantifiers $\\forall$ and \u2203 with aggregators $\\bigtriangledown$ and inf (see also Appendix B). In this way, we can derive quantitative metrics for any logically defined properties of learning models compositionally. ", "page_idx": 5}, {"type": "text", "text": "4 Quantitative metrics of disentangled representations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we demonstrate how to apply the conversion method introduced above to derive quantitative metrics for measuring the modularity (Definition 3) and informativeness (Definitions 1 and 2) of disentangled representations. ", "page_idx": 5}, {"type": "text", "text": "In Sections 4.1 and 4.2, we introduce modularity metrics based on two approaches and discuss their differences in terms of computation and optimization. We point out that the main obstacle lies in the optimization step, resulting from the existential quantifiers in the definition. Then, we show that we can derive easily computable and differentiable metrics from a logically equivalent definition. In Section 4.3, we introduce informativeness metrics and present a result of Theorem 1. ", "page_idx": 5}, {"type": "text", "text": "4.1 Modularity metrics via product approximation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We begin with modularity, which is an essential property of disentangled representation learning. Recall that modularity can be defined using the product function (Definition 3). For easier reference, we provide the following diagram, which shows the domains and codomains of the functions involved in the upcoming discussion: ", "page_idx": 5}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/89e48acc4929c862540d05b4729850c031558288a87fbb039d97edbf7f8cd881.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "From Definition 3, we can derive the following metric: ", "page_idx": 5}, {"type": "text", "text": "Definition 9 (Product approximation). Let $m:Y\\rightarrow Z$ be a function from a product $Y:=Y_{1}\\times Y_{2}$ of sets to another product $Z:=Z_{1}\\times Z_{2}$ of sets. The extent to which $m$ resembles a product function can be measured by a distance between $m$ and its best product function approximation: ", "page_idx": 5}, {"type": "equation", "text": "$$\nq_{\\mathrm{product}}(m:Y\\rightarrow Z):=\\operatorname*{inf}_{m_{1,1}\\in[Y_{1},Z_{1}]}\\operatorname*{inf}_{m_{2,2}\\in[Y_{2},Z_{2}]}d_{[Y,Z]}(m,m_{1,1}\\times m_{2,2}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The derivation of $q_{\\mathrm{product}}$ from $p_{\\mathrm{product}}$ follows the conversion described in Table 1: replacing the equality $=_{[Y,Z]}\\colon[Y,Z]\\times[Y,Z]\\stackrel{}{\\rightarrow}\\{\\top,\\bot\\}$ with a strict premetric $d_{[Y,Z]}:[Y,Z]\\times[Y,Z]\\rightarrow[0,\\infty]$ and the existential quantifiers $\\exists$ with the infimum operators inf. ", "page_idx": 5}, {"type": "text", "text": "This modularity metric can be interpreted as a distance from a point $m~\\in~[Y,Z]$ to a subset $\\{m_{1,1}\\times m_{2,2}\\ |\\ m_{1,1}\\in[Y_{1},Z_{1}],m_{2,2}\\lnot\\in[Y_{2},Z_{2}]\\}\\subset[Y,Z]$ of product functions (cf. the Hausdorff distance [Lawvere, 1986, Tuzhilin, 2016]). Following from Theorem 1, $q_{\\mathrm{product}}(m)=0$ if and only if $p_{\\mathrm{product}}(m)=\\top$ . This means that the minimizers of this metric are precisely product functions. ", "page_idx": 5}, {"type": "text", "text": "However, we still face two obstacles: the product operation and the minimization problem. For the product operation, we can employ Eqs. (10) and (12) to rewrite $q_{\\mathrm{product}}$ into a more computable form: ", "page_idx": 6}, {"type": "text", "text": "Proposition 2. The quantity $q_{\\mathrm{product}}(m:Y\\rightarrow Z)$ equals ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underbrace{\\nabla}_{y_{1}\\in Y_{1}}\\underbrace{\\nabla}_{y_{2}\\in Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),m_{1,1}^{*}(y_{1}))+\\underbrace{\\nabla}_{y_{2}\\in Y_{2}}\\underbrace{\\nabla}_{y_{1}\\in Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{2}),m_{2,2}^{*}(y_{2})),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the functions $m_{1,1}^{*}:Y_{1}\\to Z_{1}$ and $m_{2,2}^{*}:Y_{2}\\rightarrow Z_{2}$ are given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{1,1}^{*}:Y_{1}\\to Z_{1}:=y_{1}\\mapsto\\arg\\operatorname*{inf}_{z_{1}\\in Y_{2}}\\mathcal{\\big{A_{Z_{1}}}}(m_{1}(y_{1},y_{2}),z_{1}),}\\\\ &{m_{2,2}^{*}:Y_{2}\\to Z_{2}:=y_{2}\\mapsto\\arg\\operatorname*{inf}_{z_{2}\\in Z_{2}}\\operatorname*{sup}_{y_{1}\\in Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{2}),z_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A detailed derivation can be found in Appendix C. Note that we can obtain the optimal product function approximation $m_{1,1}^{*}\\times m_{2,2}^{*}$ explicitly via Eqs. (17) and (18). Intuitively, we need to find an approximation of a (multi)set of codes with one factor fixed and other factors varying, and then we use the aggregation of all the approximation errors as a modularity metric. ", "page_idx": 6}, {"type": "text", "text": "The second obstacle \u2014 the minimization problem \u2014 still needs to be addressed. Since the code spaces $Z_{1}$ and $Z_{2}$ can be infinite sets, the minimization problem may not have a closed-form minimizer or even an exact solver. Even if an exact solver exists, the solution may not be differentiable with respect to the inputs. Let us examine some concrete examples of $q_{\\mathrm{product}}$ by choosing different aggregators $\\bigtriangledown$ in Eqs. (17) and (18). In the following three examples, we assume that the code spaces $Z_{1}$ and $Z_{2}$ are Euclidean spaces equipped with the usual Euclidean distances. ", "page_idx": 6}, {"type": "text", "text": "Example 2. If the aggregator $\\bigtriangledown$ is the supremum, the best approximation is the center of the smallest bounding sphere [Megiddo, 1983], and the approximation error is the radius. ", "page_idx": 6}, {"type": "text", "text": "This metric has the advantage of being definable even when the factor spaces $Y_{1}$ and $Y_{2}$ are infinite sets, and it can be computed using either randomized [Welzl, 1991] or exact [Fischer et al., 2003] algorithms. However, it is not easy to calculate its gradient. Thus, we cannot use it as a learning objective and directly optimize it using gradient-based optimization. ", "page_idx": 6}, {"type": "text", "text": "Example 3. If the aggregator $\\bigtriangledown$ is the mean, the best approximation is the (geometric) median [Weiszfeld, 1937], and the approximation error is the mean absolute deviation around the median. ", "page_idx": 6}, {"type": "text", "text": "It is known that there is no exact algorithm for obtaining the geometric median [Cockayne and Melzak, 1969], but it can be effectively approximated using convex optimization [Cohen et al., 2016]. The geometric median has found applications in robust estimation in the fields of statistics and machine learning [Meer et al., 1991, Minsker, 2015, Pillutla et al., 2022, Guerraoui et al., 2023]. ", "page_idx": 6}, {"type": "text", "text": "Example 4. If the aggregator $\\bigtriangledown$ is the mean square, the best approximation is the mean, and the approximation error is the variance. In this case, $q_{\\mathrm{product}}(m)$ can be simplified to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{mean}_{y_{1}\\in Y_{1}}\\operatorname{var}_{y_{2}\\in Y_{2}}m_{1}(y_{1},y_{2})+\\operatorname*{mean}_{y_{2}\\in Y_{2}}\\operatorname{var}_{y_{1}\\in Y_{1}}m_{2}(y_{1},y_{2}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The variance is easier to compute and differentiate than the radius of the smallest bounding sphere and the mean absolute deviation around the median, but it is also more susceptible to outliers and noise. Further work could explore the theoretical implications of these metrics, especially in cases where only partial combinations of factors or noisy annotations are available. ", "page_idx": 6}, {"type": "text", "text": "Then, let us revisit our motivating example in Example 1: ", "page_idx": 6}, {"type": "text", "text": "Example 5. Let us consider the function $m:\\left\\{0,1\\right\\}^{2}\\rightarrow\\mathbb{R}^{2}$ in Eq. (6). Its best product function approximation is ", "page_idx": 6}, {"type": "equation", "text": "$$\nm^{*}:\\{0,1\\}^{2}\\rightarrow\\mathbb{R}^{2}:=\\left\\{\\stackrel{(0,0)\\mapsto(2,4)}{(0,1)\\mapsto(2,6)}=\\underbrace{\\left\\{0\\mapsto2\\atop1\\mapsto6\\right}}_{m_{1,1}^{*}}\\times\\underbrace{\\left\\{0\\mapsto4\\right.}_{m_{2,2}^{*}}}_{m_{2,2}^{*}}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "because $m_{1,1}^{*}(0)=2$ is the center/median/mean of the set $\\{m_{1}(0,0)=1,m_{1}(0,1)=3\\}$ , and so on.   \nThe modularity metric is a distance between $m$ and $m^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Modularity metrics via constancy ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Upon analyzing the metrics above, it becomes evident that what we need is not the best approximation itself (e.g., the mean) but rather the approximation error (e.g., the variance) \u2014 a measure of the constancy of a set of codes. Following this insight, our next objective is to formulate a modularity metric that eliminates the need for an optimization step. Zhang and Sugiyama [2023] have proved that a function is a product function if and only if the curried functions of its component functions are constant, as shown in the following example: ", "page_idx": 7}, {"type": "text", "text": "Example 6. Consider the functions $m,m^{\\prime}:\\,\\{0,1\\}^{2}\\,\\rightarrow\\,\\mathbb{R}^{2}$ in Eqs. (6) and (7) and the curried functions of their second component functions $m_{2},m_{2}^{\\prime}:\\{0,1\\}^{2}\\rightarrow\\mathbb{R}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nm_{2}=\\left\\{\\begin{array}{l l}{(0,0)\\mapsto2}\\\\ {(0,1)\\mapsto4}\\\\ {(1,0)\\mapsto6}\\\\ {(1,1)\\mapsto8}\\end{array}\\right\\}0\\mapsto\\left\\{\\begin{array}{l l}{0\\mapsto2}\\\\ {1\\mapsto4}\\\\ {1\\mapsto6}\\\\ {1\\mapsto8}\\end{array}\\right.\\quad(21)\\quad\\quad m_{2}^{\\prime}=\\left\\{\\begin{array}{l l}{(0,0)\\mapsto c}\\\\ {(0,1)\\mapsto d}\\\\ {(1,0)\\mapsto c}\\\\ {(1,1)\\mapsto d}\\end{array}\\right.\\right\\}0\\mapsto\\left\\{\\begin{array}{l l}{0\\mapsto c}\\\\ {1\\mapsto d}\\\\ {1\\mapsto d}\\\\ {1\\mapsto\\left\\{0\\mapsto d}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The curried function $\\widehat{m_{2}}:\\{0,1\\}\\to[\\{0,1\\},\\mathbb{R}]$ is not constant, while $\\widehat{m_{2}^{\\prime}}$ is constant with value $\\{0\\mapsto c,1\\mapsto d\\}\\in[\\{0,1\\},\\mathbb{R}]$ (and so is $\\widehat{m_{1}^{\\prime}}$ ), indicating that $m^{\\prime}$ is a product function. ", "page_idx": 7}, {"type": "text", "text": "Based on this fact, we propose an alternative approach for measuring modularity: ", "page_idx": 7}, {"type": "text", "text": "Definition 10 (Constancy of curried function). Let $m_{1}$ and $m_{2}$ be the component functions of a function $m:Y\\rightarrow Z$ from a product $Y:=Y_{1}\\times Y_{2}$ of sets to another product $Z:=Z_{1}\\times Z_{2}$ of sets. The extent to which $m$ resembles a product function can be measured by the constancy of the curried functions of $m_{1}$ and $m_{2}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nq_{\\mathrm{const-curry}}(m:Y\\rightarrow Z):=q_{\\mathrm{const}}(\\widehat{m_{1}})+q_{\\mathrm{const}}(\\widehat{m_{2}}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $q_{\\mathrm{const}}$ is a quantity for constant functions. ", "page_idx": 7}, {"type": "text", "text": "To complete this construction, we adopt the following definition and metric of the constant function: Definition 11 (Constant function). A function $f:A\\rightarrow B$ is constant if ", "page_idx": 7}, {"type": "equation", "text": "$$\np_{\\mathrm{const}}(f:A\\to B):=\\forall a\\in A.\\;\\forall a^{\\prime}\\in A.\\;f(a)=_{B}f(a^{\\prime}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which can be measured by ", "page_idx": 7}, {"type": "equation", "text": "$$\nq_{\\mathrm{const}}(f:A\\to B):=\\bigtriangledown_{a\\in A\\,a^{'}\\in A}d_{B}(f(a),f(a^{'})).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This constancy metric $q_{\\mathrm{const}}$ only needs to compute pairwise distances between the outputs, requiring $\\left|A\\right|^{2}$ times distance computation but no optimization. Incorporating $q_{\\mathrm{const}}$ into $q_{\\mathrm{const-curry}}$ , we can get the following metric: ", "page_idx": 7}, {"type": "text", "text": "Proposition 3. The quantity $q_{\\mathrm{const-curry}}(m:Y\\rightarrow Z)$ equals ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\nabla\\underbrace{\\nabla\\quad\\nabla\\quad}_{\\!\\!\\!y_{1}\\in Y_{1}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),m_{1}(y_{1},y_{2}^{\\prime}))}\\\\ &{}&{\\quad\\quad y_{1}\\in Y_{1}\\;y_{2}\\!\\in\\!Y_{2}\\;y_{2}^{\\prime}\\!\\in\\!Y_{2}}\\\\ &{}&{\\quad\\quad+\\underbrace{\\nabla\\quad\\quad\\nabla\\quad}_{\\!\\!\\!y_{2}\\in Y_{2}\\;y_{1}\\in Y_{1}\\;y_{1}^{\\prime}\\in Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{2}),m_{2}(y_{1}^{\\prime},y_{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here are two examples of $q_{\\mathrm{const}}$ and $q_{\\mathrm{const-curry}}$ using different aggregators $\\bigtriangledown$ in Eqs. (25) and (26). ", "page_idx": 7}, {"type": "text", "text": "Example 7. If the aggregator $\\bigtriangledown$ is the maximum, $q_{\\mathrm{const}}$ is the diameter (the maximum pairwise distance) of the outputs. In this case, $q_{\\mathrm{const-curry}}(m)$ can be simplified to ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y_{1}\\in Y_{1}}\\dim m_{2}(y_{1},y_{2})+\\operatorname*{max}_{y_{2}\\in Y_{2}}\\dim m_{2}(y_{1},y_{2}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Example 8. If the aggregator $\\bigtriangledown$ is the mean square, $q_{\\mathrm{const}}$ is the mean pairwise squared distance, which equals the variance. In this case, $q_{\\mathrm{const-curry}}(m)$ coincides with Eq. (19). ", "page_idx": 7}, {"type": "text", "text": "In summary, Eqs. (19) and (27) are easily computable and differentiable metrics, and their minimizers are precisely product functions. They do not contain any hyperparameters or stochastic components and thus can serve as both learning objectives and evaluation metrics. ", "page_idx": 7}, {"type": "text", "text": "4.3 Informativeness metrics ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "If an encoder $f:X\\to Z$ is constant, mapping everything to the same value, according to Definition 3, it is perfectly modular. However, a constant encoder is also completely useless. In this subsection, we shift our focus to the property of informativeness \u2014 a measurement of usefulness. ", "page_idx": 8}, {"type": "text", "text": "Informativeness is not a unique requirement for disentangled representations. Other representation learning paradigms, such as contrastive learning [Jaiswal et al., 2020, Wang and Isola, 2020] and metric learning [Musgrave et al., 2020], also emphasize the importance of mapping dissimilar data to far-apart locations in the representation space. While one could integrate this requirement into a single disentanglement score (e.g., [Higgins et al., 2017, Kim and Mnih, 2018]), we argue that it is better to evaluate the usefulness of representations separately for a more fine-grained assessment [Carbonneau et al., 2022]. ", "page_idx": 8}, {"type": "text", "text": "One straightforward way to measure informativeness is to measure how much we can invert the encoding process: ", "page_idx": 8}, {"type": "text", "text": "Definition 12 (Retraction approximation). Let $m:Y\\rightarrow Z$ be a function. The extent to which $m$ is retractable can be measured by a distance between the composition of $m$ and its best retraction approximation and the identity function: ", "page_idx": 8}, {"type": "equation", "text": "$$\nq_{\\mathrm{retractable}}(m:Y\\rightarrow Z):=\\operatorname*{inf}_{h\\in[Z,Y]}d_{[Y,Y]}(h\\circ m,\\operatorname{id}_{Y})=\\operatorname*{inf}_{h\\in[Z,Y]}\\operatorname{\\bigtriangledown\\big|}_{y\\in Y}d_{Y}(h(m(y)),y).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This metric $q_{\\mathrm{retractable}}$ is derived from Definition 2 following the conversion procedure in Table 1. This informativeness metric also involves an optimization step similar to the modularity metric $q_{\\mathrm{product}}$ , potentially introducing randomness or higher computation costs. Note that we may use a parameterized subset of the set $[Z,Y]$ of all functions from codes $Z$ to factors $Y$ , such as the set of linear functions. Then, the problem becomes a regression/classification problem, and the metric is the performance of the predictor. A number of existing works adopted this approach and used the accuracy, the area under the ROC curve (AUC-ROC), or the mean squared error (MSE) to measure the informativeness [Ridgeway and Mozer, 2018, Eastwood and Williams, 2018, Eastwood et al., 2023]. However, such metrics necessitate additional hyperparameter tuning and are more likely to exhibit varying behavior across different implementations [Carbonneau et al., 2022]. ", "page_idx": 8}, {"type": "text", "text": "It raises the question of whether we can measure the informativeness of an encoder without approximating its retraction. We propose to measure informativeness by directly measuring the injectivity of the encoding process: ", "page_idx": 8}, {"type": "text", "text": "Definition 13 (Contraction). Let $m:Y\\rightarrow Z$ be a function. The extent to which $m$ is injective can be measured by how much $m$ contracts pairs of inputs: ", "page_idx": 8}, {"type": "equation", "text": "$$\nq_{\\mathrm{injective}}(m:Y\\rightarrow Z):=\\underset{y\\in Y}{\\nabla}\\underset{y^{\\prime}\\in Y}{\\nabla}\\,d_{Y}(y,y^{\\prime})\\dot{}\\dot{}+d_{Z}(m(y),m(y^{\\prime})).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This metric $q_{\\mathrm{injective}}$ is derived from Definition 1 following the conversion procedure in Table 1. According to Theorem 1, we know that $q_{\\mathrm{retractable}}(m)=0$ if and only if $m$ is retractable. However, $q_{\\mathrm{injective}}(\\bar{m})=0$ implies the injectivity of $m$ but not the other way around: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{(p_{\\mathrm{retractable}}(m)=\\top)\\xrightarrow[\\log\\mathrm{ically}]{+}(p_{\\mathrm{injective}}(m)=\\top)}\\\\ &{\\mathrm{{Theorem}~}1\\bigg\\lbrack\\underbrace[]{e\\mathrm{quivalent}}}&{\\quad\\quad\\quad\\uparrow\\underbrace[]{\\downarrow\\mathrm{Theorem}\\,1}}\\\\ &{(q_{\\mathrm{retractable}}(m)=0)}&{(q_{\\mathrm{injective}}(m)=0)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In other words, a minimizer of $q_{\\mathrm{injective}}$ is required to be non-contractive, which is a stronger condition than being injective. For example, let us consider the function $m:[0,1]\\rightarrow\\mathbb{R}:=y\\mapsto0.01\\times y$ Although it is injective, its outputs are less distinguishable from each other in terms of the Euclidean distance. Therefore, $q_{\\mathrm{injective}}$ still assign a non-zero value to this function. ", "page_idx": 8}, {"type": "text", "text": "Although not all injective functions necessarily minimize $q_{\\mathrm{injective}}$ , according to Theorem 1, we can guarantee that minimizing $q_{\\mathrm{injective}}$ will not lead to non-injective functions. Moreover, $q_{\\mathrm{injective}}$ does not require training regressors or classifiers to approximate the retraction. Consequently, it does not need any time-consuming hyperparameter tuning or cross-validation like existing informativeness metrics [Eastwood and Williams, 2018, Ridgeway and Mozer, 2018]. ", "page_idx": 8}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/aeb0d80aeb662c09ffb9edd5a062d54da473cbd804df341ce9367f1a468b9377.jpg", "table_caption": ["Table 2: Supervised disentanglement metrics "], "table_footnote": ["a [Higgins et al., 2017] b [Kim and Mnih, 2018] c [Chen et al., 2018] d [Eastwood and Williams, 2018] "], "page_idx": 9}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we empirically demonstrate the effectiveness of the proposed metrics. Following Carbonneau et al. [2022], we did not learn representations on datasets but directly defined functions $m:Y\\rightarrow Z$ from factors to codes, which allows us to capture typical failure patterns. ", "page_idx": 9}, {"type": "text", "text": "We evaluated modularity metrics based on (i) the radius of the smallest bounding sphere, (ii) mean absolute deviation (MAD) around the median, (iii) variance, (iv) diameter, and (v) mean pairwise distance (MPD) introduced in Sections 4.1 and 4.2. We evaluated informativeness metrics based on retraction approximation using the maximum error (ME), mean absolute error (MAE), and mean squared error (MSE), and we calculated the contraction discussed in Section 4.3. To compare with existing metrics [Higgins et al., 2017, Kim and Mnih, 2018, Chen et al., 2018, Eastwood and Williams, 2018], we transformed the results isomorphically using $e^{-x}:[0,\\infty]\\to[0,1]$ , meaning that 1 is the perfect score. The results are shown in Table 2, and our observations are as follows. ", "page_idx": 9}, {"type": "text", "text": "If a representation is given a perfect score by a proposed metric, it must satisfy the property that the metric quantifies, which confirms our theoretical result. In Table 2, the light cells show that the proposed metrics can assign a perfect score when the function truly satisfies the properties, which is indicated by $\\checkmark$ and $\\pmb{x}$ . The dark cells are supposed to be perfect scores, but they fall short due to the limited expressiveness of the linear models used for the approximation. Meanwhile, some existing metrics that only provide a single score may entangle modularity and informativeness. ", "page_idx": 9}, {"type": "text", "text": "The metrics derived from equivalent definitions may differ in terms of computation cost and differentiability. Concretely, the radius, MAD, ME, MAE, and MSE are not differentiable due to the inner optimization problem, while the variance, diameter, MPD, and max/mean contraction are differentiable. In terms of computation, they are much faster than metrics requiring hyperparameter tuning, such as DCI [Eastwood and Williams, 2018]. Further comparisons are provided in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "Different metrics may rank imperfect representations differently, even though they have exactly the same minimizers. This difference can lead to differences in risk preferences, sensitivity to outliers, and learning dynamics when these metrics are used as learning objectives. Illustrations and further discussion can be found in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "Further, the proposed metrics can be used in weakly supervised or fine-grained evaluation. See Appendix E for detailed data configuration and further experimental results. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we developed a systematic and rigorous method for converting logical definitions of properties of representation learning models into quantitative metrics (Table 1). We applied this method to assess two important and distinct properties of disentangled representations: modularity and informativeness. We derived two families of metrics for each property based on their logically equivalent definitions. We theoretically analyzed the minimizers of these metrics (Theorem 1) and compared their differences in terms of computation cost and differentiability. Future research could compare metrics derived from different aggregators and design appropriate models to optimize these metrics with minimal supervision. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Paolo Perrone for generously sharing insights on Markov categories enriched in divergence spaces. We thank Ken Sakayori for reviewing an earlier version of Appendices A and B and providing constructive suggestions. We thank Zhiyuan Zhan for insightful discussions on metrics, topology, measures, and optimization of function properties, and for reviewing and proofreading some parts of Appendix D. We thank Masahiro Negishi for valuable discussions on disentanglement metrics and weakly supervised disentanglement. We thank Jingwen Fu for checking the algebraic concepts in Section 3. We also thank Johannes Ackermann, Xin-Qiang Cai, and Tongtong Fang for their valuable feedback on the manuscript. ", "page_idx": 10}, {"type": "text", "text": "YZ was supported by JSPS KAKENHI Grant Number 22KJ0880. MS was supported by JST CREST Grant Number JPMJCR18A2 and a grant from Apple, Inc. Any views, opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and should not be interpreted as reflecting the views, policies or position, either expressed or implied, of Apple Inc. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Ji\u02c7r\u00ed Ad\u00e1mek, Horst Herrlich, and George Strecker. Abstract and Concrete Categories: The Joy of Cats. John Wiley and Sons, 1990. URL http://www.tac.mta.ca/tac/reprints/ar ticles/17/tr17abs.html. A.1   \nK Amer. Equationally complete classes of commutative monoids with monus. Algebra Universalis, 18:129\u2013131, 1984. URL https://doi.org/10.1007/BF01182254. 18   \nBrandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference on Machine Learning, 2017. URL http://proceedings.mlr.press/v70/amos17b. html. 1, D.1   \nSteve Awodey. Category Theory. Oxford University Press, 2010. URL https://doi.org/10 .1093/acprof:oso/9780198568612.001.0001. A.1   \nGiorgio Bacci, Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Propositional logics for the Lawvere quantale. Electronic Notes in Theoretical Informatics and Computer Science, 3, 2023. URL https://doi.org/10.46298/entics.12292. https://arxiv.org/abs/ 2302.01224. B.9, D.2   \nGiorgio Bacci, Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Polynomial Lawvere logic. arXiv preprint, 2024. URL https://arxiv.org/abs/2402.03543. D.2   \nSamy Badreddine, Artur d\u2019Avila Garcez, Luciano Serafini, and Michael Spranger. Logic tensor networks. Artificial Intelligence, 303:103649, 2022. URL https://doi.org/10.1016/j. artint.2021.103649. https://arxiv.org/abs/2012.13635. D.2   \nNikita Balabin, Daria Voronkova, Ilya Trofimov, Evgeny Burnaev, and Serguei Barannikov. Disentanglement learning via topology. In International Conference on Machine Learning, 2024. D.1   \nHan Bao and Masashi Sugiyama. Calibrated surrogate maximization of linear-fractional utility in binary classification. In International Conference on Artificial Intelligence and Statistics, pages 2337\u20132347, 2020. URL http://proceedings.mlr.press/v108/bao20a.html. D.2   \nHan Bao, Clay Scott, and Masashi Sugiyama. Calibrated surrogate losses for adversarially robust classification. In Conference on Learning Theory, pages 408\u2013451, 2020. URL http: //proceedings.mlr.press/v125/bao20a.html. D.2   \nMichael Barr and Charles Wells. Category Theory for Computing Science, volume 1. Prentice Hall New York, 1990. URL https://www.math.mcgill.ca/triples/Barr-Wells-ctc s.pdf. A.1   \nSugato Basu, Arindam Banerjee, and Raymond J Mooney. Semi-supervised clustering by seeding. In International Conference on Machine Learning, 2002. URL https://dl.acm.org/doi/1 0.5555/645531.656012. E.2   \nJens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and J\u00f6rn-Henrik Jacobsen. Invertible residual networks. In International Conference on Machine Learning, 2019. URL https://proceedings.mlr.press/v97/behrmann19a.html. D.1   \nIta\u00ef Ben Yaacov. On the expressive power of quantifiers in continuous logic. arXiv preprint, 2022. URL https://arxiv.org/abs/2207.01863. D.2   \nIta\u00ef Ben Yaacov and Alexander Usvyatsov. Continuous first order logic and local stability. Transactions of the American Mathematical Society, 362(10):5213\u20135259, 2010. URL https: //doi.org/10.1090/S0002-9947-10-04837-3. https://arxiv.org/abs/08 01.4303. D.2   \nIta\u00ef Ben Yaacov, Alexander Berenstein, C. Ward Henson, and Alexander Usvyatsov. Model Theory for Metric Structures, page 315\u2013427. London Mathematical Society Lecture Note Series. Cambridge University Press, 2008. URL https://doi.org/10.1017/CBO9780511735219.011. D.2   \nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8): 1798\u20131828, 2013. URL https://doi.org/10.1109/TPAMI.2013.50. https: //arxiv.org/abs/1206.5538. 1, 1, 2.1, 2.2, D.1   \nMerrie Bergmann. An Introduction to Many-Valued and Fuzzy Logic: Semantics, Algebras, and Derivation Systems. Cambridge University Press, 2008. URL https://doi.org/10.1017/ CBO9780511801129. B.1, D.2   \nMikhail Bilenko, Sugato Basu, and Raymond J Mooney. Integrating constraints and metric learning in semi-supervised clustering. In International Conference on Machine Learning, 2004. URL https://dl.acm.org/doi/10.1145/1015330.1015360. E.2   \nGeorge Boole. An Investigation of the Laws of Thought: On Which Are Founded the Mathematical Theories of Logic and Probabilities. Cambridge University Press, 1854. URL https: //doi.org/10.1017/CBO9780511693090. 2.1, D.2   \nTai-Danae Bradley, John Terilla, and Yiannis Vlassopoulos. An enriched category theory of language: From syntax to semantics. La Matematica, 1(2):551\u2013580, 2022. URL https://doi.org/10 .1007/s44007-022-00021-2. https://arxiv.org/abs/2106.07890. A.3   \nJohann Brehmer, Pim De Haan, Phillip Lippe, and Taco Cohen. Weakly supervised causal representation learning. In Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id $=$ dz79MhQXWvg. D.2   \nJohann Brehmer, Pim De Haan, S\u00f6nke Behrends, and Taco Cohen. Geometric algebra transformer. In Neural Information Processing Systems, 2023. URL https://openreview.net/forum ?id $=$ M7r2CO4tJC. 1, D.1   \nLeo Breiman, Jerome Friedman, Richard A. Olshen, and Charles J. Stone. Classification and Regression Trees. Routledge, 1984. URL https://doi.org/10.1201/9781315139470. D.1   \nChris Burgess and Hyunjik Kim. 3D shapes dataset, 2018. https://github.com/deepmin d/3d-shapes (Apache License 2.0). 6, 5, E.3, 7, 10   \nMatteo Capucci. On quantifiers for quantitative reasoning. arXiv preprint, 2024. URL https: //arxiv.org/abs/2406.04936. D.2   \nMarc-Andr\u00e9 Carbonneau, Julian Zaidi, Jonathan Boilard, and Ghyslain Gagnon. Measuring disentanglement: A review of metrics. IEEE Transactions on Neural Networks and Learning Systems, 2022. URL https://doi.org/10.1109/TNNLS.2022.3218982. https://arxiv.org/abs/2012.09276. 1, 2, 4.3, 4.3, 5, D.1   \nHugo Caselles-Dupr\u00e9, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled representation learning requires interaction with environments. In Neural Information Processing Systems, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/36e 729ec173b94133d8fa552e4029f8b-Abstract.html. D.1 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Chen Chung Chang. Algebraic analysis of many valued logics. Transactions of the American Mathematical society, 88(2):467\u2013490, 1958. URL https://doi.org/10.2307/1993227. D.2 ", "page_idx": 12}, {"type": "text", "text": "Chen Chung Chang and H Jerome Keisler. Continuous Model Theory, volume 58 of Annals of Mathematics Studies. Princeton University Press, 1966. URL https://doi.org/10.1515/ 9781400882052. D.2 ", "page_idx": 12}, {"type": "text", "text": "Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in variational autoencoders. In Neural Information Processing Systems, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/1ee3dfcd8a064 5a25a35977997223d22-Abstract.html. 1, 2, 5, D.1, 5, E.3, 6, 7 ", "page_idx": 12}, {"type": "text", "text": "Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation. The Journal of Machine Learning Research, 21(245):1\u201371, 2020. URL http://jmlr.org/p apers/v21/20-163.html. D.1 ", "page_idx": 12}, {"type": "text", "text": "Yiting Chen, Zhanpeng Zhou, and Junchi Yan. Going beyond neural network feature similarity: The network feature complexity and its interpretation using category theory. In International Conference on Learning Representations, 2024. URL https://openreview.net/forum ?id=4bSQ3lsfEV. A.1 ", "page_idx": 12}, {"type": "text", "text": "Kenta Cho and Bart Jacobs. Disintegration and Bayesian inversion via string diagrams. Mathematical Structures in Computer Science, 29(7):938\u2013971, 2019. URL https://doi.org/10.1017/ S0960129518000488. https://arxiv.org/abs/1709.00322. A.1 ", "page_idx": 12}, {"type": "text", "text": "Simon Cho. Categorical semantics of metric spaces and continuous logic. The Journal of Symbolic Logic, 85(3):1044\u20131078, 2020. URL https://doi.org/10.1017/jsl.2020.44. D.2 ", "page_idx": 12}, {"type": "text", "text": "Ernest J Cockayne and Zdzislaw A Melzak. Euclidean constructibility in graph-minimization problems. Mathematics Magazine, 42(4):206\u2013208, 1969. URL https://doi.org/10.108 0/0025570X.1969.11975961. 4.1 ", "page_idx": 12}, {"type": "text", "text": "Michael B Cohen, Yin Tat Lee, Gary Miller, Jakub Pachocki, and Aaron Sidford. Geometric median in nearly linear time. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 9\u201321, 2016. URL https://doi.org/10.1145/2897518.2897647. 4.1 ", "page_idx": 12}, {"type": "text", "text": "Taco Cohen. Equivariant convolutional networks. PhD thesis, University of Amsterdam, 2021. URL https://hdl.handle.net/11245.1/0f7014ae-ee94-430e-a5d8-37d03d8d1 0e6. D.1 ", "page_idx": 12}, {"type": "text", "text": "Taco Cohen and Max Welling. Learning the irreducible representations of commutative Lie groups. In International Conference on Machine Learning, 2014. URL https://proceedings.ml r.press/v32/cohen14.html. D.1 ", "page_idx": 12}, {"type": "text", "text": "Taco Cohen and Max Welling. Transformation properties of learned visual representations. In International Conference on Learning Representations, 2015. URL http://arxiv.org/ab s/1412.7659. D.1 ", "page_idx": 12}, {"type": "text", "text": "Taco Cohen and Max Welling. Group equivariant convolutional networks. In International Conference on Machine Learning, 2016. URL http://proceedings.mlr.press/v48/cohenc16 .html. D.1 ", "page_idx": 12}, {"type": "text", "text": "Geoffrey SH Cruttwell, Bruno Gavranovic\u00b4, Neil Ghani, Paul Wilson, and Fabio Zanasi. Categorical foundations of gradient-based learning. In Programming Languages and Systems, pages 1\u201328. Springer International Publishing, 2022. URL https://doi.org/10.1007/978-3-030 -99336-8_1. https://arxiv.org/abs/2103.01931. A.1 ", "page_idx": 12}, {"type": "text", "text": "Haskell B. Curry. Some philosophical aspects of combinatory logic. In Studies in Logic and the Foundations of Mathematics, volume 101, pages 85\u2013101. Elsevier, 1980. URL https: //doi.org/10.1016/S0049-237X(08)71254-0. 9 ", "page_idx": 12}, {"type": "text", "text": "Francesco Dagnino and Fabio Pasquali. Logical foundations of quantitative equality. In Logic in Computer Science, 2022. URL https://doi.org/10.1145/3531130.3533337. D.2 ", "page_idx": 13}, {"type": "text", "text": "Hennie Daniels and Marina Velikova. Monotone and partially monotone neural networks. IEEE Transactions on Neural Networks, 21(6):906\u2013917, 2010. URL https://doi.org/10.110 9/TNN.2010.2044803. D.1 ", "page_idx": 13}, {"type": "text", "text": "Artur S. d\u2019Avila Garcez, Krysia B. Broda, and Dov M. Gabbay. Neural-Symbolic Learning Systems. Springer, 2002. URL https://doi.org/10.1007/978-1-4471-0211-3. D.2 ", "page_idx": 13}, {"type": "text", "text": "Pim de Haan, Taco Cohen, and Max Welling. Natural graph networks. In Neural Information Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/2517756c5a9be6ac007fe9bb7fb92611-Abstract.html. A.1, D.1 ", "page_idx": 13}, {"type": "text", "text": "Andrea Dittadi, Frederik Tr\u00e4uble, Francesco Locatello, Manuel Wuthrich, Vaibhav Agrawal, Ole Winther, Stefan Bauer, and Bernhard Sch\u00f6lkopf. On the transfer of disentangled representations in realistic settings. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=8VXvj1QNRl1. D.1 ", "page_idx": 13}, {"type": "text", "text": "Kien Do and Truyen Tran. Theory and evaluation metrics for learning disentangled representations. In International Conference on Learning Representations, 2020. URL https://openreview .net/forum?id $=$ HJgK0h4Ywr. 1, D.1 ", "page_idx": 13}, {"type": "text", "text": "Andrew Dudzik. Quantales and Hyperstructures: Monads, Mo\u2019Problems. PhD thesis, University of California, Berkeley, 2017. URL https://arxiv.org/abs/1707.09227. B.9 ", "page_idx": 13}, {"type": "text", "text": "Andrew Joseph Dudzik and Petar Velic\u02c7kovic\u00b4. Graph neural networks are dynamic programmers. In Neural Information Processing Systems, 2022. URL https://openreview.net/forum?i ${\\mathsf{d}}=$ wu1Za9dY1GY. A.1 ", "page_idx": 13}, {"type": "text", "text": "Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disentangled representations. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id $=$ By-7dz-AZ. 1, 1, 2, 4.3, 4.3, 2, 5, D.1, 5, 6, 7, E.4, E.5 ", "page_idx": 13}, {"type": "text", "text": "Cian Eastwood, Andrei Liviu Nicolicioiu, Julius Von K\u00fcgelgen, Armin Keki\u00b4c, Frederik Tr\u00e4uble, Andrea Dittadi, and Bernhard Sch\u00f6lkopf. DCI-ES: An extended disentanglement framework with connections to identifiability. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $=$ 462z-gLgSht. 4.3, D.1 ", "page_idx": 13}, {"type": "text", "text": "Ronald Fagin, Ryan Riegel, and Alexander Gray. Foundations of reasoning with uncertainty via real-valued logics. Proceedings of the National Academy of Sciences, 121(21), 2024. URL https://doi.org/10.1073/pnas.2309905121. D.2 ", "page_idx": 13}, {"type": "text", "text": "Daniel Figueroa and Benno van den Berg. A topos for continuous logic. Theory and Applications of Categories, 38(28), 2022. URL http://www.tac.mta.ca/tac/volumes/38/28/3 8-28abs.html. D.2 ", "page_idx": 13}, {"type": "text", "text": "Kaspar Fischer, Bernd G\u00e4rtner, and Martin Kutz. Fast smallest-enclosing-ball computation in high dimensions. In European Symposium on Algorithms, pages 630\u2013641. Springer, 2003. URL https://doi.org/10.1007/978-3-540-39658-1_57. 4.1 ", "page_idx": 13}, {"type": "text", "text": "Brendan Fong and David I Spivak. An Invitation to Applied Category Theory: Seven Sketches in Compositionality. Cambridge University Press, 2019. URL https://doi.org/10.1017/ 9781108668804. https://arxiv.org/abs/1803.05316. A.1, B.1, B.6, B.8 ", "page_idx": 13}, {"type": "text", "text": "Tobias Fritz. A synthetic approach to Markov kernels, conditional independence and theorems on sufficient statistics. Advances in Mathematics, 370:107239, 2020. URL https://doi.org/ 10.1016/j.aim.2020.107239. https://arxiv.org/abs/1908.07021. A.1 ", "page_idx": 13}, {"type": "text", "text": "Soichiro Fujii. Ordered semirings and subadditive morphisms. arXiv preprint, 2023. URL https://arxiv.org/abs/2311.03862. B.9 ", "page_idx": 13}, {"type": "text", "text": "Marco Fumero, Luca Cosmo, Simone Melzi, and Emanuele Rodol\u00e0. Learning disentangled representations via product manifold projection. In International Conference on Machine Learning, 2021. URL http://proceedings.mlr.press/v139/fumero21a.html. 1, D.1 ", "page_idx": 14}, {"type": "text", "text": "Bruno Gavranovi\u00b4c, Paul Lessard, Andrew Joseph Dudzik, Tamara von Glehn, Jo\u00e3o Guilherme Madeira Ara\u00fajo, and Petar Veli\u02c7ckovi\u00b4c. Position: Categorical deep learning is an algebraic theory of all architectures. In International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id $=$ EIcxV7T0Sy. A.1 ", "page_idx": 14}, {"type": "text", "text": "Muhammad Waleed Gondal, Manuel Wuthrich, Djordje Miladinovic, Francesco Locatello, Martin Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Sch\u00f6lkopf, and Stefan Bauer. On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset. In Neural Information Processing Systems, 2019. URL https://proceedings.neurips.cc /paper/2019/hash/d97d404b6119214e4a7018391195240a-Abstract.html. https://github.com/rr-learning/disentanglement_dataset (Creative Commons Attribution 4.0 International License). 6, 5, E.3, 7, E.6, 11 ", "page_idx": 14}, {"type": "text", "text": "Ian Goodfellow, Honglak Lee, Quoc Le, Andrew Saxe, and Andrew $\\mathrm{Ng}$ . Measuring invariances in deep networks. In Neural Information Processing Systems, 2009. URL https://procee dings.neurips.cc/paper/2009/hash/428fca9bc1921c25c5121f9da7815cd e-Abstract.html. D.1 ", "page_idx": 14}, {"type": "text", "text": "Rachid Guerraoui, Nirupam Gupta, and Rafael Pinot. Byzantine machine learning: A primer. ACM Computing Surveys, 2023. URL https://doi.org/10.1145/3616537. 4.1 ", "page_idx": 14}, {"type": "text", "text": "Petr H\u00e1jek. Metamathematics of Fuzzy Logic, volume 4 of Trends in Logic. Springer, 1998. URL https://doi.org/10.1007/978-94-011-5300-3. D.2 ", "page_idx": 14}, {"type": "text", "text": "Charles R. Harris, K. Jarrod Millman, St\u00e9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez del R\u00edo, Mark Wiebe, Pearu Peterson, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357\u2013362, 2020. URL https://doi.org/10.1038/s415 86-020-2649-2. https://numpy.org. D.6 ", "page_idx": 14}, {"type": "text", "text": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id $=$ Sy2fzU9gl. 1, 4.3, 2, 5, D.1, D.2, 5, E.3, E.4, 6, 7, E.6 ", "page_idx": 14}, {"type": "text", "text": "Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint, 2018. URL https://arxiv.org/abs/1812.02230. 1, D.1 ", "page_idx": 14}, {"type": "text", "text": "Aapo Hyv\u00e4rinen and Erkki Oja. Independent component analysis: Algorithms and applications. Neural networks, 13(4):411\u2013430, 2000. URL https://doi.org/10.1016/S0893-608 0(00)00026-5. D.1 ", "page_idx": 14}, {"type": "text", "text": "Isao Ishikawa, Takeshi Teshima, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama. Universal approximation property of invertible neural networks. Journal of Machine Learning Research, 24(287):1\u201368, 2023. URL https://www.jmlr.org/papers/v24/22-0384. html. D.1 ", "page_idx": 14}, {"type": "text", "text": "Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A survey on contrastive self-supervised learning. Technologies, 9(1):2, 2020. URL https://doi.org/10.3390/technologies9010002. https://arxiv.org/ab s/2011.00362. 4.3 ", "page_idx": 14}, {"type": "text", "text": "Peter Johnstone. Sketches of an Elephant: A Topos Theory Compendium. Oxford University Press, 2002. A.2 ", "page_idx": 14}, {"type": "text", "text": "Max Kelly. Basic concepts of enriched category theory. In London Mathematical Society Lecture Note Series, volume 64. Cambridge University Press, 1982. URL http://www.tac.mta.ca /tac/reprints/articles/10/tr10.html. A.3   \nMaurice G. Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81\u201393, 1938. URL https://doi.org/10.2307/2332226. E.4   \nHamza Keurti, Hsiao-Ru Pan, Michel Besserve, Benjamin F Grewe, and Bernhard Sch\u00f6lkopf. Homomorphism AutoEncoder \u2013 learning group structured representations from observed transitions. In International Conference on Machine Learning, 2023. URL https://pr oceedings.mlr.press/v202/keurti23a.html. D.1   \nHyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on Machine Learning, 2018. URL http://proceedings.mlr.press/v80/kim18b.html. 1, 4.3, 2, 5, D.1, D.2, 5, E.3, E.4, 6, 7   \nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. URL https://openreview.net/forum?id $=$ 33X9 fd2-9FyZd. http://arxiv.org/abs/1312.6114. E.3   \nJonas K\u00f6hler, Leon Klein, and Frank No\u00e9. Equivariant flows: exact likelihood generative learning for symmetric densities. In International Conference on Machine Learning, 2020. URL https://proceedings.mlr.press/v119/kohler20a.html. D.1   \nDaphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT press, 2009. URL https://mitpress.mit.edu/9780262013192/. D.1   \nSolomon Kullback and Richard A. Leibler. On information and sufficiency. The Annals of Mathematical Statistics, 22(1):79\u201386, 1951. URL https://doi.org/10.1214/aoms /1177729694. https://www.jstor.org/stable/2236703. D.2   \nHenry Kvinge, Tegan Emerson, Grayson Jorgenson, Scott Vasquez, Timothy Doster, and Jesse Lew. In what ways are deep neural networks invariant and how should we measure this? In Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=SC D0hn3kMHw. 1, D.1, D.2   \nF. William Lawvere. Adjointness in foundations. Dialectica, pages 281\u2013296, 1969. URL https://doi.org/10.1111/j.1746-8361.1969.tb01194.x. http: //www.tac.mta.ca/tac/reprints/articles/16/tr16abs.html. B.10   \nF. William Lawvere. Metric spaces, generalized logic, and closed categories. Rendiconti del seminario mat\u00e9matico e fisico di Milano, 43:135\u2013166, 1973. URL https://doi.org/10.1007/BF02 924844. http://www.tac.mta.ca/tac/reprints/articles/1/tr1abs.html. A.3, A.3, B.1, B.9, D.2   \nF. William Lawvere. Taking categories seriously. Revista colombiana de matematicas, 20(3-4):147\u2013 178, 1986. http://www.tac.mta.ca/tac/reprints/articles/8/tr8abs.html. 4.1   \nF William Lawvere and Robert Rosebrugh. Sets for Mathematics. Cambridge University Press, 2003. URL https://doi.org/10.1017/CBO9780511755460. 2.1, A.2   \nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, 2019. URL http://proceedings.mlr.pr ess/v97/lee19d. 1, D.1   \nTom Leinster. An informal introduction to topos theory. arXiv preprint, 2010. URL https: //arxiv.org/abs/1012.5647. A.2   \nTom Leinster. Basic Category Theory. Cambridge University Press, 2014. URL https://doi. org/10.1017/CBO9781107360068. https://arxiv.org/abs/1612.09375. A.1   \nZhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali, and Linwei Wang. Progressive learning and disentanglement of hierarchical representations. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJxpsx rYPS. 1, D.1   \nFrancesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Sch\u00f6lkopf, and Olivier Bachem. On the fairness of disentangled representations. In Neural Information Processing Systems, 2019a. URL https://proceedings.neurips.cc/paper/2019/hash/1b4 86d7a5189ebe8d8c46afc64b0d1b4-Abstract.html. D.1   \nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In International Conference on Machine Learning, 2019b. URL https://proceedings.mlr.press/v97/locatello19a.html. D.1, 14, E.3   \nFrancesco Locatello, Ben Poole, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In International Conference on Machine Learning, 2020. URL http://proceedings.mlr.press/v119/locatel lo20a.html. D.2   \nJan \u0141ukasiewicz. O logice tr\u00f3jwarto\u00b4sciowej (On three-valued logic). In Selected Works, volume 11 of Studies in Logic, pages 87\u201388. North-Holland Publishing Company, 1920. D.2   \nJan \u0141ukasiewicz and Alfred Tarski. Untersuchungen \u00fcber den Aussagenkalk\u00fcl (Investigations on the propositional calculus). Comptes Rendus des S\u00e9ances de la Soci\u00e9t\u00e9 des Sciences et des Lettres de Varsovie, Class III, 23, 1930. D.2   \nSaunders Mac Lane. Categories for the Working Mathematician. Springer, 1978. URL https: //doi.org/10.1007/978-1-4757-4721-8. 1, A.1, A.1   \nSaunders Mac Lane and Ieke Moerdijk. Sheaves in Geometry and Logic: A First Introduction to Topos Theory. Springer, 1994. URL https://doi.org/10.1007/978-1-4612-0927-0. A.2   \nLouis Mahon, Lei Shah, and Thomas Lukasiewicz. Correcting flaws in common disentanglement metrics. arXiv preprint, 2023. URL https://arxiv.org/abs/2304.02335. D.7   \nGregorz Malinowski. Many-valued logic and its philosophy. In Handbook of the History of Logic, volume 8, pages 13\u201394. Elsevier, 2007. URL https://doi.org/10.1016/S1874-585 7(07)80004-5. D.2   \nRobin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. DeepProbLog: Neural probabilistic logic programming. In Neural Information Processing Systems, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/dc5d637ed 5e62c36ecb73b654b05ba2a-Abstract.html. D.2   \nRadu Mardare, Prakash Panangaden, and Gordon Plotkin. Quantitative algebraic reasoning. In Logic in Computer Science, 2016. URL https://doi.org/10.1145/2933575.2934518. D.2   \nRadu Mardare, Prakash Panangaden, and Gordon Plotkin. Fixed-points for quantitative equational logics. In Logic in Computer Science, 2021. URL https://doi.org/10.1109/LICS52 264.2021.9470662. D.2   \nHaggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id $=$ Syx72jC9tm. D.1   \nLoic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing sprites dataset, 2017. https://github.com/deepmind/dsprites-dataset (Apache License 2.0). 6, 5, E.3, 7, 9   \nBarry Mazur. When is one thing equal to some other thing? In Proof and Other Dilemmas: Mathematics and Philosophy, page 221\u2013242. Mathematical Association of America, 2008. URL https://doi.org/10.5948/UPO9781614445050.015. https://people.math. harvard.edu/\\~mazur/preprints/when_is_one.pdf. 3.1 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Peter Meer, Doron Mintz, Azriel Rosenfeld, and Dong Yoon Kim. Robust regression methods for computer vision: A review. International journal of computer vision, 6:59\u201370, 1991. URL https://doi.org/10.1007/BF00127126. 4.1 ", "page_idx": 17}, {"type": "text", "text": "Nimrod Megiddo. The weighted Euclidean 1-center problem. Mathematics of Operations Research, 8(4):498\u2013504, 1983. URL https://doi.org/10.1287/moor.8.4.498. 2 ", "page_idx": 17}, {"type": "text", "text": "Stanislav Minsker. Geometric median and robust estimation in Banach spaces. Bernoulli, 21(4): 2308\u20132335, 2015. URL https://doi.org/10.3150/14-BEJ645. 4.1 ", "page_idx": 17}, {"type": "text", "text": "Takeru Miyato, Masanori Koyama, and Kenji Fukumizu. Unsupervised learning of equivariant structure from sequences. In Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id $=$ 7b7iGkuVqlZ. D.1 ", "page_idx": 17}, {"type": "text", "text": "Milton L. Montero, Jeffrey Bowers, Rui Ponte Costa, Casimir JH Ludwig, and Gaurav Malhotra. Lost in latent space: Examining failures of disentangled models at combinatorial generalisation. In Neural Information Processing Systems, 2022. URL https://openreview.net/forum?i ${\\mathsf{d}}=$ 7yUxTNWyQGf. D.7 ", "page_idx": 17}, {"type": "text", "text": "Milton Llera Montero, Casimir JH Ludwig, Rui Ponte Costa, Gaurav Malhotra, and Jeffrey Bowers. The role of disentanglement in generalisation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $=$ qbH974jKUVy. D.1, D.7 ", "page_idx": 17}, {"type": "text", "text": "Christopher J. Mulvey. &. Supplemento ai Rendiconti del Circolo Matem\u00e0tico di Palermo. Serie II, 12:99\u2013104, 1986. URL https://zbmath.org/0633.46065. B.9 ", "page_idx": 17}, {"type": "text", "text": "Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A metric learning reality check. In European Conference on Computer Vision, pages 681\u2013699, 2020. URL https://doi.org/10.1007/ 978-3-030-58595-2_41. 4.3 ", "page_idx": 17}, {"type": "text", "text": "Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, and Haggai Maron. Equivariant architectures for learning in deep weight spaces. In International Conference on Machine Learning, 2023. URL https://proceedings.mlr.press/v202/navon23a. html. D.1 ", "page_idx": 17}, {"type": "text", "text": "Chenri Ni, Nontawat Charoenphakdee, Junya Honda, and Masashi Sugiyama. On the calibration of multiclass classification with rejection. In Neural Information Processing Systems, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/571d3a9420bfd9219 f65b643d0003bf4-Abstract.html. D.2 ", "page_idx": 17}, {"type": "text", "text": "Matthew Painter, Adam Prugel-Bennett, and Jonathon Hare. Linear disentangled representations and unsupervised action estimation. In Neural Information Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/9a02387b02ce7de2d ac4b925892f68fb-Abstract.html. D.1 ", "page_idx": 17}, {"type": "text", "text": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, highperformance deep learning library. In Neural Information Processing Systems, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2 bfa9f7012727740-Abstract.html. https://pytorch.org. D.6, E.3 ", "page_idx": 17}, {"type": "text", "text": "Edward Pearce-Crump. Categorification of group equivariant neural networks. arXiv preprint, 2023. URL https://arxiv.org/abs/2304.14144. A.1 ", "page_idx": 17}, {"type": "text", "text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011. URL https://jmlr.org/papers/v12/pedr egosa11a.html. https://scikit-learn.org. E.4, E.5 ", "page_idx": 17}, {"type": "text", "text": "Paolo Perrone. Markov categories and entropy. IEEE Transactions on Information Theory, 2023. URL https://doi.org/10.1109/TIT.2023.3328825. https://arxiv.org/ abs/2212.11719. A.1, D.2   \nDavid Pfau, Irina Higgins, Alex Botev, and S\u00e9bastien Racani\u00e8re. Disentangling by subspace diffusion. In Neural Information Processing Systems, 2020. URL https://proceedings.neurips. cc/paper/2020/hash/c9f029a6a1b20a8408f372351b321dd8-Abstract.ht ml. D.1   \nKrishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. IEEE Transactions on Signal Processing, 70:1142\u20131154, 2022. URL https: //doi.org/10.1109/TSP.2022.3153135. 4.1, 13   \nJean-Eric Pin. Tropical semirings. In Idempotency, pages 50\u201369. Cambridge University Press, 1998. URL https://doi.org/10.1017/CBO9780511662508.004. B.7   \nRobin Quessard, Thomas Barrett, and William Clements. Learning disentangled representations and group structure of dynamical environments. In Neural Information Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/e449b9317dad9 20c0dd5ad0a2a2d5e49-Abstract.html. D.1   \nScott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In Neural Information Processing Systems, 2015. URL https://proceedings.neurips.cc/pap er/2015/hash/e07413354875be01a996dc560274708e-Abstract.html. 6, 5, E.3, 7, 8   \nMark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning Research, 11(83):2387\u20132422, 2010. URL http://jmlr.org/papers/v11/reid10a.ht ml. D.2   \nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning, 2015. URL https://proceedings.mlr.press/v37/ rezende15.html. D.1   \nKarl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the f-statistic loss. In Neural Information Processing Systems, 2018. URL https://proceedings.neur ips.cc/paper/2018/hash/2b24d495052a8ce66358eb576b8912c8-Abstract. html. 1, 1, 2, 2.2, 4.3, 4.3, D.1, D.2, E.2   \nKarsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt. Disentanglement of correlated factors via hausdorff factorized support. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $=$ OKcJ hpQiGiX. D.7   \nBernhard Sch\u00f6lkopf and Julius von K\u00fcgelgen. From statistical to causal learning. In International Congress of Mathematicians. EMS Press, 2022. URL https://doi.org/10.4171/icm2 022/173. https://arxiv.org/abs/2204.00607. D.1   \nPrithviraj Sen, Breno WSR de Carvalho, Ryan Riegel, and Alexander Gray. Neuro-symbolic inductive logic programming with logical neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022. URL https://doi.org/10.1609/aaai.v36i8.20795. D.2   \nDan Shiebler, Bruno Gavranovi\u00b4c, and Paul Wilson. Category theory in machine learning. arXiv preprint, 2021. URL https://arxiv.org/abs/2106.07032. A.1   \nDaniel Shiebler. Compositionality and Functorial Invariants in Machine Learning. PhD thesis, University of Oxford, 2023. URL http://doi.org/10.5287/bodleian:DE1aDx4Zw. A.1   \nRui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised disentanglement with guarantees. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id $=$ HJgSwyBKvr. 1, D.2, E.2   \nJoseph Sill. Monotonic networks. In Neural Information Processing Systems, 1997. URL https://proceedings.neurips.cc/paper/1997/hash/83adc9225e4de b67d7ce42d58fe5157c-Abstract.html. D.1   \nIngo Steinwart. How to compare different loss functions and their risks. Constructive Approximation, 26:225\u2013287, 2007. URL https://doi.org/10.1007/s00365-006-0662-3. D.2   \nRaphael Suter, Djordje Miladinovic, Bernhard Sch\u00f6lkopf, and Stefan Bauer. Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness. In International Conference on Machine Learning, 2019. URL http://proceedings.mlr.press/v97/ suter19a.html. 1   \nSeiya Tokui and Issei Sato. Disentanglement analysis with partial information decomposition. In International Conference on Learning Representations, 2022. URL https://openreview.n et/forum?id $=$ pETy-HVvGtt. 1, D.1   \nLoek Tonnaer, Luis A P\u00e9rez Rey, Vlado Menkovski, Mike Holenderski, and Jacobus W Portegies. Quantifying and learning linear symmetry-based disentanglement. In International Conference on Machine Learning, 2022. URL https://proceedings.mlr.press/v162/tonna er22a.html. D.1   \nFrederik Tr\u00e4uble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Sch\u00f6lkopf, and Stefan Bauer. On disentangled representations learned from correlated data. In International Conference on Machine Learning, 2021. URL https: //proceedings.mlr.press/v139/trauble21a.html. D.7, D.7   \nTodd Trimble. An elementary approach to elementary topos theory, 2019. URL https: //ncatlab.org/toddtrimble/published/An+elementary+approach+to+ elementary+topos+theory. A.2   \nAlexey A. Tuzhilin. Who invented the Gromov-Hausdorff distance? arXiv preprint, 2016. URL https://arxiv.org/abs/1612.00728. 4.1   \nElise van der Pol, Herke van Hoof, Frans A Oliehoek, and Max Welling. Multi-agent MDP homomorphic networks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $=$ H7HDG--DJF0. D.1   \nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental algorithms for scientific computing in Python. Nature methods, 17(3):261\u2013272, 2020. URL https://doi.org/10.1038/s41592-019-068 6-2. https://scipy.org. E.4   \nKiri Wagstaff, Claire Cardie, Seth Rogers, and Stefan Schr\u00f6dl. Constrained k-means clustering with background knowledge. In International Conference on Machine Learning, 2001. URL https://dl.acm.org/doi/10.5555/645530.655669. E.2   \nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning, 2020. URL https://proceedings.mlr.press/v119/wang20k.html. 1, 4.3   \nEndre Weiszfeld. Sur le point pour lequel la somme des distances de n points donn\u00e9s est minimum (on the point for which the sum of the distances to n given points is minimum). Tohoku Mathematical Journal, First Series, 43:355\u2013386, 1937. URL https://doi.org/10.1007/s10479-0 08-0352-z. 3   \nEmo Welzl. Smallest enclosing disks (balls and ellipsoids). In New Results and New Trends in Computer Science, pages 359\u2013370. Springer, 1991. URL https://doi.org/10.1007/BF b0038202. 4.1, 12 Zhenlin Xu, Marc Niethammer, and Colin A Raffel. Compositional generalization in unsupervised compositional representation learning: A study on disentanglement and emergent language. In Neural Information Processing Systems, 2022. URL https://openreview.net/forum?i d=ZEQ5Gf8DiD. D.1 Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. Towards building a group-based unsupervised representation disentanglement framework. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $=$ YgPq Nctmyd. D.1 Yang Yuan. On the power of foundation models. In International Conference on Machine Learning,   \n2023. URL https://proceedings.mlr.press/v202/yuan23b.html. A.1 Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In Neural Information Processing Systems, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/f22e4747da1aa27e3   \n63d86d40ff442fe-Abstract.html. D.1 Sharon Zhang, Amit Moscovich, and Amit Singer. Product manifold learning. In International Conference on Artificial Intelligence and Statistics, 2021. URL http://proceedings.mlr. press/v130/zhang21j.html. D.1 Yivan Zhang and Masashi Sugiyama. A category-theoretical meta-analysis of definitions of disentanglement. In International Conference on Machine Learning, 2023. URL https: //proceedings.mlr.press/v202/zhang23ak.html. 1, 4.2, A.1, D.1 Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y Ng, Gunnar Carlsson, and Stefano Ermon. Evaluating the disentanglement of deep generative models through manifold topology. In International Conference on Learning Representations, 2020. URL https://openrevi ew.net/forum?id=djwS0m4Ft_A. D.1 ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "2 Logical definitions of disentangled representations 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "2.1 Informativeness: injectivity or retractability of a learning model 2   \n2.2 Modularity: product structure preserved by a learning model 3 ", "page_idx": 21}, {"type": "text", "text": "3 Enrichment: from logic to metric ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "3.1 From equality predicate to strict premetric . . 4   \n3.2 From logical operation to quantitative operation . . . 4   \n3.3 From compound predicate to compound quantity . . . . 5   \n3.4 (Sub)homomorphism from metric to logic 5 ", "page_idx": 21}, {"type": "text", "text": "4 Quantitative metrics of disentangled representations 6 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "4.1 Modularity metrics via product approximation . . . 6   \n4.2 Modularity metrics via constancy . . . . . 8   \n4.3 Informativeness metrics . 9 ", "page_idx": 21}, {"type": "text", "text": "5 Experiments 10 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "6 Conclusion 10 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Bibliography 11 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "A Preliminaries 24 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "A.1 Basic category theory . . 24   \nA.2 Elementary topos theory 25   \nA.3 Enriched category theory . . . 25 ", "page_idx": 21}, {"type": "text", "text": "B Theory 26 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.1 Subobject quantifier and quantizer 26 ", "page_idx": 21}, {"type": "text", "text": "B.2 Equality and premetric . . 28 B.3 Preorder . . 28 B.4 Operation: algebra over the product endofunctor . . 282931323435363738 B.5 Negation . . . . . .   \nB.6 Conjunction . . . .   \nB.7 Disjunction   \nB.8 Implication   \nB.9 Heyting algebra, quantale, and ordered semiring . . .   \nB.10 Quantifier: algebra over the exponentiation endofunctor . .   \nB.11 Enrichment . .   \nB.12 Summary 39 ", "page_idx": 21}, {"type": "text", "text": "C Proofs 41 C.1 Product function . 41 C.2 Constant curried function 41 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D Discussions 42 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Background . 42 D.2 Related work 43 DD..34 ICmopnlsitcaantti ofnu nacntido enq u.i v.a l.e .n c.e . 4467 D.5 Rank of imperfect representations   \nD.6 Implementation   \nD.7 Limitations 48 D.8 Broader impact . . . 48 ", "page_idx": 21}, {"type": "text", "text": "E Experiments 49 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Synthetic data . . 49   \nE.2 Weakly supervised modularity metrics 50   \nE.3 Evaluation of existing models on image datasets . 51   \nE.4 Kendall tau distance between metrics . 51   \nE.5 Computation time of metrics . . . 52   \nE.6 Factor-wise modularity metrics . 53 ", "page_idx": 21}, {"type": "text", "text": "List of Figures ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1 Disentangled representation learning . . . . . 3   \n2 From predicates and logical operations to quantities and quantitative operations . 5   \n3 Negation . . . . . . 31   \n4 Conjunction 32   \n5 Disjunction \u00b7\u00b7\u00b7 32   \n6 Implication \u00b7\u00b7\u00b7\u00b7 32   \n7 Equivalence . . . . 32   \n8 Quantitative operations for implication . . . 45   \n9 Quantitative operations for equivalence 4456   \n10 Constancy metrics . . .   \n11 Radius and variance . . . . 47   \n12 Rank of imperfect representations 47   \n13 Illustration of synthetic data . . . 49   \n14 Entanglement of a distribution . . 49 ", "page_idx": 22}, {"type": "text", "text": "List of Tables ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1 From logic to metric 4   \n2 Supervised disentanglement metrics 10   \n3 Supervised modularity metrics . . 50   \n4 Weakly supervised modularity metrics . . 50   \n5 Supervised disentanglement metrics on image datasets 51   \n6 Average Kendall tau rank distances bewteen disentanglement metrics . 52   \n7 Computation time (seconds) of supervised disentanglement metrics on image datasets 52   \n8 Factor-wise modularity metrics on 3D Cars [Reed et al., 2015] . . 53   \n9 Factor-wise modularity metrics on dSprites [Matthey et al., 2017] 54   \n10 Factor-wise modularity metrics on 3D Shapes [Burgess and Kim, 2018] 55   \n11 Factor-wise modularity metrics on MPI3D [Gondal et al., 2019] 56 ", "page_idx": 22}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this paper, we used abstract mathematical tools such as category theory and topos theory to develop a theory of the relationship between logical definitions and quantitative metrics. However, this level of abstraction may be unfamiliar or even intimidating to some readers, and sometimes unnecessary for machine learning practitioners. Therefore, we have used only the most basic algebraic concepts, such as homomorphism, in the main text. For readers interested in the mathematical background, we provide a brief introduction to the basic categorical concepts in this section. ", "page_idx": 23}, {"type": "text", "text": "A.1 Basic category theory ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Category theory is a branch of mathematics that studies mathematical structures in an abstract way, which is suitable for identifying and organizing common patterns across various fields of mathematics [Mac Lane, 1978, Ad\u00e1mek et al., 1990, Awodey, 2010]. It has found applications in many fields, including computer science [Barr and Wells, 1990], probability theory [Cho and Jacobs, 2019, Fritz, 2020, Perrone, 2023], and machine learning [de Haan et al., 2020, Shiebler et al., 2021, Cruttwell et al., 2022, Dudzik and Veli\u02c7ckovi\u00b4c, 2022, Shiebler, 2023, Yuan, 2023, Pearce-Crump, 2023, Chen et al., 2024, Gavranovic\u00b4 et al., 2024]. ", "page_idx": 23}, {"type": "text", "text": "The most fundamental concept is that of a category: ", "page_idx": 23}, {"type": "text", "text": "Definition 14. A category $\\mathbf{C}=(\\mathrm{Obj},\\mathrm{Hom},\\circ,\\mathrm{id})$ consists of ", "page_idx": 23}, {"type": "text", "text": "a collection Obj of objects,   \n$\\boxed{\\boxed{\\begin{array}{r l}\\end{array}}$ a set ${\\mathrm{Hom}}(A,B)$ of morphisms between objects,10   \n$\\boxed{\\boxed{\\begin{array}{r l}\\end{array}}$ a composition function $\\bar{\\circ}:\\mathrm{Hom}(B,C)\\times\\dot{\\mathrm{Hom}}(A,B)\\rightarrow\\mathrm{Hom}(A,C)$ for each triple of objects, and   \n$\\blacktriangledown$ an identity morphism $\\operatorname{id}_{A}\\in\\operatorname{Hom}(A,A)$ for each object, ", "page_idx": 23}, {"type": "text", "text": "subject to ", "page_idx": 23}, {"type": "text", "text": "$\\boxed{\\boxed{\\begin{array}{r l}\\end{array}}$ associativity: $(h\\circ g)\\circ f=h\\circ(g\\circ f)$ and $\\pmb{\\mathrm{\\Pi}}$ identity: $\\operatorname{id}_{B}\\circ f=f=f\\circ\\operatorname{id}_{A}$ . ", "page_idx": 23}, {"type": "text", "text": "A crucial example is the category Set of sets and functions. However, what is of particular interest is not the category itself but its relationships with other categories. Building on the concepts of the functor and natural transformation, whose definitions are omitted here, we can develop tools to better understand the properties of a category. ", "page_idx": 23}, {"type": "text", "text": "Moreover, we can define objects in terms of their relations with other objects, employing what is known as universal construction. For example, a terminal object 1 in a category is an object such that for any object $A$ , there exists a unique morphism $e_{A}:A\\rightarrow1$ to it, which we call a terminal morphism. In Set, any set $\\{*\\}$ with only one element is a terminal object. Based on the concept of the terminal object, a global element of an object $B$ is defined to be a morphism $b:1\\rightarrow B$ from a terminal object. We write $b_{A}:A\\rightarrow B$ as an abbreviation for the constant morphism $b\\circ e_{A}:A\\;{\\xrightarrow{\\ e_{A}}}\\;1\\;{\\xrightarrow{\\ b}}\\;B$ with value $b:1\\rightarrow B$ . These concepts will be used to develop our theory in Appendix B. Other important universal constructions include the product, pullback, and exponential. The concept of the product is of great importance in disentangled representation learning [Zhang and Sugiyama, 2023]. ", "page_idx": 23}, {"type": "text", "text": "Regarding the pullback, we need to mention the following useful lemma: ", "page_idx": 23}, {"type": "text", "text": "Lemma 4 (Pullback lemma). Suppose that in the following commutative diagram, the right square is a pullback. ", "page_idx": 23}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/3d956ea5d8b61f3add411d98ec4227bacd9eea99464c02b89e3f1361f5cf6a58.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Then, the left square is a pullback if and only if the outer rectangle is a pullback. ", "page_idx": 23}, {"type": "text", "text": "This lemma is usually left as an exercise in textbooks [e.g., Mac Lane, 1978, p. 72, Exercise 8, Leinster, 2014, Exercise 5.1.35]. A proof can be found in Fong and Spivak [2019, Proposition 7.3]. We need to use this lemma to (de)compose pullbacks. As a side note, we use the asterisk $f^{\\ast}g$ to denote the pullback of $g$ along $f$ . ", "page_idx": 23}, {"type": "text", "text": "A.2 Elementary topos theory ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Topos theory studies categories that, in some sense, exhibit behavior akin to the category of sets and functions [Lawvere and Rosebrugh, 2003]. Topos theory has found applications in geometry, topology, and logic [Mac Lane and Moerdijk, 1994, Johnstone, 2002, Leinster, 2010, Trimble, 2019]. In this work, we only explore its relation to logic. ", "page_idx": 24}, {"type": "text", "text": "To formally define a topos, two essential concepts are those of the subobject and subobject classifier. A subobject of an object $C$ is simply a monomorphism $b:B\\mapsto C$ to the object $C$ . The subobject classifier is defined as follows: ", "page_idx": 24}, {"type": "text", "text": "Definition 15. In a finitely complete category, a subobject classifier is a universal subobject $\\mathsf{T}:1\\mapsto\\Omega$ such that for every subobject $b:B\\mapsto C$ , there exists a unique morphism $\\chi_{b}:C\\to\\Omega$ such that $b$ is a pullback of $\\intercal$ along $\\chi_{b}$ . The morphism $\\chi_{b}$ is called the classifying morphism of $b$ . ", "page_idx": 24}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/755acc8c90f9e99333eff5b60dc2e9ea11091c060690b379c4db02843d8acc16.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Alternatively, we can state that ", "page_idx": 24}, {"type": "text", "text": "Proposition 5. A subobject classifier is precisely a terminal object in the category of monomorphisms and pullbacks. ", "page_idx": 24}, {"type": "text", "text": "Then, we can study the morphisms to the object $\\Omega$ : ", "page_idx": 24}, {"type": "text", "text": "Definition 16. In a category with a subobject classifier $\\mathsf{T}:1\\mapsto\\Omega$ , a predicate on an object $C$ is a morphism $p:C\\to\\Omega$ . ", "page_idx": 24}, {"type": "text", "text": "Based on Definition 15, we can state that subobjects of an object $C$ are classified by predicates on $C$ . ", "page_idx": 24}, {"type": "text", "text": "For example, in Set, subobjects are subsets, a function from a singleton $\\{*\\}$ to a two-element set is a subobject classifier, which is usually denoted by $\\top:\\{*\\}\\rightarrow\\{\\bar{\\top},\\bot\\}$ , a predicate on a set $C$ is a function $p:C\\to\\{\\top,\\bot\\}$ , and a subset precisely corresponds to its indicator function. ", "page_idx": 24}, {"type": "text", "text": "Among various equivalent definitions of a topos, a concise one is as follows: ", "page_idx": 24}, {"type": "text", "text": "Definition 17. An elementary topos is a finitely complete and cartesian closed category with a subobject classifier. ", "page_idx": 24}, {"type": "text", "text": "Despite its concise definition, a great number of logical structures can be derived from it, which will be explored in Appendix B. ", "page_idx": 24}, {"type": "text", "text": "A.3 Enriched category theory ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Enriched category theory generalizes the concept of the category by replacing the sets of morphisms with objects in a suitable category [Kelly, 1982]. It has been used to better understand a wide range of domains, from metric spaces [Lawvere, 1973] to language [Bradley et al., 2022]. ", "page_idx": 24}, {"type": "text", "text": "Let us dive into the definition of an enriched category: ", "page_idx": 24}, {"type": "text", "text": "Definition 18. A category $\\mathbf{C}=(\\mathrm{Obj},\\mathrm{Hom},\\circ,\\mathrm{id})$ enriched in a monoidal category $(\\mathbf{V},\\otimes,I)$ consists of ", "page_idx": 24}, {"type": "text", "text": "a collection Obj of objects,   \na hom-object $\\operatorname{Hom}(A,B)\\in\\operatorname{Obj}_{\\mathbf{v}}$ between objects,   \na composition morphism $\\circ:{\\mathrm{Hom}}(B,C)\\otimes{\\mathrm{Hom}}(A,B)\\to{\\mathrm{Hom}}(A,C)$ for each triple of objects, and   \nan identity element $\\operatorname{id}_{A}:I\\to\\operatorname{Hom}(A,A)$ for each object, ", "page_idx": 24}, {"type": "text", "text": "subject to associativity and identity. ", "page_idx": 24}, {"type": "text", "text": "Comparing Definitions 14 and 18, we can say that a (locally small) category is a category enriched in the category Set of sets and functions. Enrichment is a way to describe the additional structures of morphisms and the properties that need to be respected by composition. ", "page_idx": 24}, {"type": "text", "text": "An example is a preorder, which can be seen as a category enriched in the category of boolean values. Another example is a Lawvere metric space [Lawvere, 1973], which is a set with a premetric that satisfies the triangle inequality. Note that the transitivity of a preorder and the triangle inequality of a Lawvere metric are described by their composition morphisms, respectively. ", "page_idx": 25}, {"type": "text", "text": "In this work, we use enrichment to describe the association of a set of morphisms with additional operations, such as a strict premetric and aggregators. ", "page_idx": 25}, {"type": "text", "text": "B Theory ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we detail the theory of converting logical definitions into their corresponding quantitative metrics based on elementary topos theory and enriched category theory. ", "page_idx": 25}, {"type": "text", "text": "B.1 Subobject quantifier and quantizer ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Since our main goal is to develop a multi-valued (possibly continuous and differentiable) quantification of properties defined by a certain type of logic, we begin with a category $\\mathbf{E}$ that has sufficient structures to allow the desired logical operations and build the quantification upon these structures. ", "page_idx": 25}, {"type": "text", "text": "Firstly, recall that a subobject classifier $\\Omega$ , if exists, is the representing object of the subobject functor $\\operatorname{Sub}_{\\mathbf{E}}$ , such that a subobject $b:B\\longmapsto C$ corresponds to a unique classifying morphism $\\chi_{b}:C\\to\\Omega$ , and an external operation on the set $\\operatorname{Sub}_{\\mathbf{E}}(C)$ of subobjects that is natural in the object $C$ corresponds to an internal operation. ", "page_idx": 25}, {"type": "text", "text": "For example, the intersection ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\cap_{C}:\\operatorname{Sub}_{\\mathbf{E}}(C)\\times\\operatorname{Sub}_{\\mathbf{E}}(C)\\to\\operatorname{Sub}_{\\mathbf{E}}(C)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "corresponds a natural transformation ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Hom}_{\\mathbf{E}}(-,\\Omega)\\times\\mathrm{Hom}_{\\mathbf{E}}(-,\\Omega)\\Rightarrow\\mathrm{Hom}_{\\mathbf{E}}(-,\\Omega),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which, because the hom-functor $\\operatorname{Hom}_{\\mathbf{E}}$ preserves limits, is isomorphic to a natural transformation between hom-functors ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Hom}_{\\mathbf{E}}(-,\\Omega\\times\\Omega)\\Rightarrow\\mathrm{Hom}_{\\mathbf{E}}(-,\\Omega),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which, by the Yoneda lemma, is isomorphic to an internal operation on the subobject classifier $\\Omega$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\wedge:\\Omega\\times\\Omega\\rightarrow\\Omega.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that the subobject classifier, the classifying morphisms, and those internal operations are determined uniquely up to isomorphism. However, in order to obtain a multi-valued quantification, the requirement for uniqueness might be too restrictive. Thus, we propose to study a weakened concept instead: ", "page_idx": 25}, {"type": "text", "text": "Definition 19. In a finitely complete category, a subobject quantifier is a subobject $o:1\\mapsto\\Psi$ such that for every subobject $b:B\\mapsto C$ , there exists at least one morphism $\\phi_{b}:C\\to\\Psi$ such that $b$ is a pullback of $o$ along $\\phi_{b}$ . The morphism $\\phi_{b}$ is called a quantifying morphism of $b$ . If the category has a subobject classifier $\\mathsf{T}:1\\mapsto\\Omega$ , the quantizer $\\kappa:\\Psi\\to\\Omega$ of the subobject quantifier $o$ is the classifying morphism of $o$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{b\\!\\!\\!\\!}_{b\\!\\!\\!\\!\\!}\\underbrace{\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-}_{\\!\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "More succinctly, we can state that (cf. Proposition 5) ", "page_idx": 25}, {"type": "text", "text": "Proposition 6. A subobject quantifier is a weakly terminal object in the category of monomorphisms and pullbacks. ", "page_idx": 25}, {"type": "text", "text": "Thus, there is a unique morphism $\\chi_{b}:C\\to\\Omega$ classifying a subobject $b:B\\mapsto C$ of an object $C$ , but there could be multiple morphisms $\\phi_{b}:C\\to\\Psi$ quantifying this subobject. ", "page_idx": 26}, {"type": "text", "text": "It is provable that the domain of a terminal object $\\intercal$ in the category of monomorphisms and pullbacks (Proposition 5) must be a terminal object 1 in the category $\\mathbf{E}$ , but not all weakly terminal objects in the category of monomorphisms and pullbacks (Proposition 6) are monomorphisms out of a terminal object. We choose Definition 19 because we want only one global element $o:1\\mapsto\\Psi$ to be designated to the truth value $\\mathsf{T}:1\\mapsto\\Omega$ . ", "page_idx": 26}, {"type": "text", "text": "Here, we give two examples to motivate this definition of subobject quantifier. One is related to three-valued logic [Bergmann, 2008, Fong and Spivak, 2019, Exercise 2.34]: ", "page_idx": 26}, {"type": "text", "text": "Example 9. In Set, the function ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathsf{y e s:\\{*\\}}\\to\\{\\mathsf{n o,m a y b e,y e s}\\},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which maps the element $^*$ in a singleton set $\\{*\\}$ (a terminal object in Set) to an element yes in a three-element set $\\{\\mathsf{n o,m a y b e,y e s}\\}$ is a subobject quantifier. For any subset $B$ of a set $C$ , a quantifying morphism $\\phi_{b}:C\\to\\Psi$ is a function that maps all elements in the subset $B$ to yes and all other elements to either maybe or no. ", "page_idx": 26}, {"type": "text", "text": "The other is related to metric spaces [Lawvere, 1973] and will be our running example in the following subsections: ", "page_idx": 26}, {"type": "text", "text": "Example 10. In Set, the function $0:\\{*\\}\\rightarrow[0,\\infty]$ selecting the number 0 out of the set $[0,\\infty]$ of extended non-negative real numbers is a subobject quantifier. The quantizer is a function ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\kappa:[0,\\infty]\\rightarrow\\{\\top,\\bot\\}:=n\\mapsto{\\left\\{\\begin{array}{l l}{\\top}&{n=0,}\\\\ {\\bot}&{n>0,}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which maps 0 to $\\intercal$ and any non-zero number to $\\perp$ . ", "page_idx": 26}, {"type": "text", "text": "Intuitively, with a subobject quantifier, there is only one way to be true, but there may be many ways to be false. In Set, a quantizer $\\kappa$ maps multiple \u201cdegrees of truth\u201d from a potentially large, even infinite set $\\Psi$ to a smaller set $\\Omega$ of truth values, hence the name. ", "page_idx": 26}, {"type": "text", "text": "Next, we define the counterpart of the concept of predicate (Definition 16): ", "page_idx": 26}, {"type": "text", "text": "Definition 20. In a category with a subobject quantifier $o:1\\mapsto\\Psi$ , a quantity on an object $C$ is a morphism $q:C\\to\\Psi$ . ", "page_idx": 26}, {"type": "text", "text": "Since we weakened the requirement for uniqueness, there is no one-to-one correspondence between subobjects and quantities. However, they are still related as follows: ", "page_idx": 26}, {"type": "text", "text": "Lemma 7. In a category with a subobject classifier $\\top:1\\mapsto\\Omega,$ , a subobject quantifier $o:1\\mapsto\\Psi$ , and a quantizer $\\kappa:\\Psi\\to\\Omega$ , a quantity $q:C\\to\\Psi$ on an object $C$ is a quantifying morphism of $a$ subobject $q^{*}o$ of the object $C$ , which is isomorphic to a subobject $\\left(\\kappa\\circ q\\right)^{\\ast}\\mathsf{T}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. $q^{*}o$ and $\\left(\\kappa\\circ q\\right)^{\\ast}\\mathsf{T}$ are both subobjects of $C$ because pullbacks preserve subobjects. Their isomorphism follows from the pullback lemma. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Lemma 8. In a category with a subobject classifier $\\mathsf{T}:1\\mapsto\\Omega_{\\!}$ , a subobject quantifier $o:1\\mapsto\\Psi$ , and a quantizer $\\kappa:\\Psi\\to\\Omega$ , a quantity $q:C\\to\\Psi$ on an object $C$ is a quantifying morphism of $a$ subobject $b:B\\mapsto C$ if and only if $\\kappa\\circ q=\\chi_{b}$ , where $\\chi_{b}$ is the classifying morphism of the subobject $b$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Necessity follows from the pullback lemma and the uniqueness of the classifying morphism; sufficiency follows from Lemma 7. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "The following relationship between a quantity and the quantizer is also useful: ", "page_idx": 26}, {"type": "text", "text": "Lemma 9. In a category with a subobject classifier $\\mathsf{T}:1\\mapsto\\Omega$ , a subobject quantifier $o:1\\mapsto\\Psi$ , and a quantizer $\\kappa:\\Psi\\to\\Omega,$ , for any quantity $q:C\\to\\Psi$ on an object $C$ , $q=o_{C}$ if and only $i f$ $\\kappa\\circ q=\\kappa\\circ o_{C}=\\intercal_{C}$ , where $o_{C}$ is the constant morphism $o\\circ e_{C}:C\\ {\\xrightarrow{e_{C}}}\\ 1\\ {\\xrightarrow{o}}\\ \\Psi$ with value $o:1\\rightarrow\\Psi$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. This is due to the universal property of pullback $o$ of $\\intercal$ along $\\kappa$ . ", "page_idx": 26}, {"type": "text", "text": "We can see that the hom-functor on the quantizer ${\\mathrm{Hom}}_{\\mathbf{E}}(-,\\kappa):{\\mathrm{Hom}}_{\\mathbf{E}}(-,\\Psi)\\Rightarrow{\\mathrm{Hom}}_{\\mathbf{E}}(-,\\Omega)$ is a natural transformation which maps the quantities ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Psi)$ on an object $C$ to the predicates ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Omega)$ on $C$ , which are precisely subobjects of $C$ . ", "page_idx": 26}, {"type": "text", "text": "B.2 Equality and premetric ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Next, we take a closer look at a concrete and important predicate \u2014 equality \u2014 and its corresponding quantities. ", "page_idx": 27}, {"type": "text", "text": "Definition 21. In a category with a subobject classifier ${\\top}:1\\ \\rightarrow\\ \\Omega$ , the equality predicate $\\mathbf{\\equiv}_{C}$ : $C\\;\\times\\;C\\;\\rightarrow\\;\\Omega$ on an object $C$ is the classifying morphism of the diagonal morphism $\\Delta_{C}:C\\longmapsto C\\times C:=\\langle{\\mathrm{id}}_{C},{\\mathrm{id}}_{C}\\rangle$ . ", "page_idx": 27}, {"type": "text", "text": "By Lemma 8, a quantity $d_{C}:\\,C\\times C\\,\\to\\,\\Psi$ is a quantifying morphism of $\\Delta_{C}$ if and only if $\\kappa\\circ d_{C}==_{C}$ , depicted in the following diagram: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{{\\displaystyle C\\sim}}&{{\\stackrel{\\textstyle()}{\\sim}}}\\\\ {{\\displaystyle\\Delta_{C}\\!\\!\\!\\int^{\\;\\;\\;-}}}&{{\\!\\!\\!o\\!\\!\\!\\int^{\\;\\;\\;-}}}\\\\ {{\\displaystyle{\\cal C}\\times{\\cal C}\\:\\!\\!\\!\\!\\!\\!\\!\\!}}&{{\\stackrel{\\textstyle()}{\\sim}}}\\\\ {{\\displaystyle\\cdots\\!\\!\\!\\!\\cdots\\!\\!\\!\\!\\cdots\\!\\!\\!\\!\\cdots\\!\\!\\!\\!\\cdots\\!\\!\\!\\!\\cdots}}&{{}}\\\\ {{\\displaystyle=}}&{{\\cdots}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In Set, we have the following definitions: ", "page_idx": 27}, {"type": "text", "text": "Definition 22. A premetric on a set $C$ is a binary function $d_{C}:C\\times C\\to[0,\\infty]$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall c\\in C.~d_{C}(c,c)=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Or equivalently, $d_{C}\\circ\\Delta_{C}=0_{C}$ , depicted in the following diagram: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{C\\xrightarrow{}\\left\\{*\\right\\}}}\\\\ {\\lefteqn{C\\succ{C\\xrightarrow[]{}}\\sum_{d_{C}}\\left[0,\\infty\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Definition 23. A strict premetric on a set $C$ is a premetric $d_{C}:C\\times C\\to[0,\\infty]$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall c_{1}\\in C.\\;\\forall c_{2}\\in C.\\;(d_{C}(c_{1},c_{2})=0)\\to(c_{1}=_{C}c_{2})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Or equivalently, $\\Delta_{C}$ is a pullback of 0 along $d_{C}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\~~~C\\xrightarrow{}\\Bigl\\{*\\Bigl\\}}\\\\ &{~~\\Delta c\\Biggr\\}^{\\mathrm{~\\!~\\!~-~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!\\!~\\!~\\!~\\!\\!~\\!~\\!\\!~\\!~\\!\\!~\\!\\!~\\!~\\!\\!~\\!\\!~C\\times\\!\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!~\\!\\!\\!~\\!\\!~\\!\\!~\\!\\!\\!~\\!\\!~\\!\\!~\\!\\!\\!~\\!\\!~\\!\\!\\!~\\!\\!~\\!\\!\\!~\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!\\!~\\!\\!\\!\\!~\\!\\!\\!\\!~\\!\\!\\!\\!\\!~\\!\\!\\!\\!\\!\\!\\!\\!~\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In other words, strict premetrics are precisely quantifying morphisms of the diagonal morphism $\\Delta_{C}$ in the category Set with $0:\\{*\\}\\rightarrow[0,\\infty]$ as a subobject quantifier. ", "page_idx": 27}, {"type": "text", "text": "Note that the symmetry ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall c_{1}\\in C.\\;\\forall c_{2}\\in C.\\;d_{C}(c_{1},c_{2})=d_{C}(c_{2},c_{1})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the triangle inequality ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall c_{1}\\in C.\\;\\forall c_{2}\\in C.\\;\\forall c_{3}\\in C.\\;d_{C}(c_{1},c_{2})+d_{C}(c_{2},c_{3})\\geq d_{C}(c_{1},c_{3}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which make $d_{C}$ a metric, are not required. The addition $^+$ and the order $\\geq$ on the set $[0,\\infty]$ are not needed to define a strict premetric. However, they are necessary for defining other operations and properties, which will be discussed in the next subsection. ", "page_idx": 27}, {"type": "text", "text": "B.3 Preorder ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "There is a preorder $\\subseteq_{C}$ of inclusion defined on the set $\\operatorname{Sub}_{\\mathbf{E}}(C)$ of subobjects of an object $C$ : for two subobjects $a:A\\mapsto C$ and $b:B\\mapsto C$ , $a\\subseteq_{C}b$ if and only if there exists a morphism $f:A\\rightarrow B$ such that $a=b\\circ f$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\underset{a\\searrow\\atop C}{\\underbrace{A\\,{\\xrightarrow{\\quad f\\quad}}\\,B}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This preorder on the subobjects $\\operatorname{Sub}_{\\mathbf{E}}(C)$ induces a preorder on the predicates ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Omega)$ via the isomorphism. We can generalize this construction and define a preorder on other hom-sets: ", "page_idx": 27}, {"type": "text", "text": "Definition 24. In a category $\\mathbf{E}$ with pullbacks, the inclusion preorder $\\subseteq_{C}$ on the set $\\operatorname{Sub}_{\\mathbf{E}}(C)$ of subobjects of an object $C$ and a subobject $m:S\\,\\mapsto\\,T$ induce a preorder $\\preceq_{C}^{m}$ on the hom-set ${\\mathrm{Hom}}_{\\mathbf{E}}(C,T)$ via pullback of $m$ : for any two morphisms $f_{1},f_{2}:C\\to T$ , $f_{1}\\preceq_{C}^{m}f_{2}$ if and only if $f_{1}^{*}m\\subseteq_{C}f_{2}^{*}m$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f^{*}S\\xrightarrow{}S}&{{}}\\\\ {f^{*}m\\biggr\\downarrow\\;\\;{\\stackrel{\\rightharpoonup}{C}}\\xrightarrow{\\;\\;\\;\\,\\,\\,\\,}{\\biggl\\uparrow}{\\stackrel{\\rightharpoonup}{T}}}\\\\ {C\\xrightarrow{\\qquad\\,\\;\\,\\,\\,}{\\longrightarrow}T}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Based on this definition, we can explore the preorders on any hom-sets. From now, we assume that $\\mathbf{E}$ is a category with necessary structures that we need. ", "page_idx": 28}, {"type": "text", "text": "Then, the preorder of predicates is $\\preceq_{C}^{\\top}$ on ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Omega)$ , and the preorder of quantities is $\\preceq_{C}^{o}$ on $\\mathrm{Hom}_{\\mathbf{E}}(C,\\mathbf{\\bar{\\Psi}})$ . By Lemma 7, we know that for two quantities $q_{1},q_{2}:C\\to\\Psi$ , $q_{1}\\preceq_{C}^{o}\\ q_{2}$ if and only if $(\\kappa\\circ q_{1})\\preceq_{C}^{\\top}(\\kappa\\circ q_{2})$ , which means that ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\kappa)$ is an order-preserving function from $(\\mathrm{Hom}_{\\mathbf{E}}(C,\\Psi),\\preceq_{C}^{o})$ to $(\\mathrm{Hom}_{\\mathbf{E}}(C,\\Omega),\\boldsymbol{\\preceq_{C}^{\\top}})$ . ", "page_idx": 28}, {"type": "text", "text": "Next, we will explore the structures of ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Omega)$ and ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Psi)$ . To begin with, ${\\top}_{C}$ is a top in ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Omega)$ , and $o_{C}$ is a top in ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Psi)$ , because $\\mathrm{id}_{C}$ is a top in $\\operatorname{Sub}_{\\mathbf{E}}(C)$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overbrace{\\ensuremath{\\mathrm{id}}_{C}\\!\\!\\!\\int}^{C\\;\\;\\;\\mathrm{.......}\\star\\frac{\\gamma}{2}\\mathrm{~\\bf1~}.}}\\\\ {\\overbrace{C\\underbrace{\\mathrm{\\boldmath~\\mu~}^{\\circ}\\!\\!\\!-\\!\\!\\!\\frac{\\gamma}{\\delta C}\\!\\!\\!\\!-\\!\\!\\!\\frac{\\gamma}{\\delta\\!\\!\\!-\\!\\!\\!\\frac{\\gamma}{\\delta\\!\\!\\!-\\!\\!\\!\\frac{\\gamma}{\\delta\\!\\!\\!-\\!\\!\\!\\frac{\\gamma}{\\delta\\!\\!-\\!\\!\\!\\frac{\\gamma}{\\delta\\!\\!-\\!\\!\\!\\frac{\\gamma}{\\delta\\!\\!-\\!\\!\\!\\frac{\\gamma}{\\delta\\!}}}}}}}}}^{\\overbrace{\\textstyle\\mathrm{\\boldmath~\\mu~}^{\\circ}}\\cdots\\cdots\\cdots\\cdots}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The inclusion preorder on $\\operatorname{Sub}_{\\mathbf{E}}(C)$ also has a bottom \u2014 the initial morphism $i_{C}:0\\longmapsto C$ from an initial object 0 in the category $\\mathbf{E}$ to the object $C$ . Then, its classifying morphism $\\perp_{C}:C\\to\\Omega$ is a bottom in ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Omega)$ . It can be proven that $\\perp_{C}$ is a constant morphism with value $\\perp:1\\rightarrow\\Omega$ , which is the classifying morphism of the initial/terminal morphism $0\\rightarrow1$ . Any quantifying morphism $\\psi_{C}:C\\to\\Psi$ of $i_{C}$ is a bottom in ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Psi)$ , but it is not necessarily a constant morphism. ", "page_idx": 28}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/bf98944146d116f58d6e2695a357e4abf066bb5bad0be3728ea7f2b1fc29cce4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "The preorder on the global elements plays a special role: ", "page_idx": 28}, {"type": "text", "text": "Example 11. In Set, $\\preceq_{1}^{\\top}$ , also denoted by $\\vdash$ , is a preorder on the set $\\{\\top,\\bot\\}$ with only one nonidentity relation $\\bot\\vdash\\top$ . ", "page_idx": 28}, {"type": "text", "text": "Example 12. In Set, $\\preceq_{1}^{0}$ is a preorder on the set $[0,\\infty]$ where $n\\preceq_{1}^{0}0$ for any number $n$ , and $m\\preceq_{1}^{0}n$ and $n\\preceq_{1}^{0}m$ for any positive numbers $m$ and $n$ . ", "page_idx": 28}, {"type": "text", "text": "By definition, $(\\{top,\\bot\\},\\vdash)$ and $([0,\\infty],\\preceq_{1}^{0})$ are equivalent. However, we can consider a suborder of $([0,\\infty],\\preceq_{1}^{0})$ , e.g., the usual \u201cgreater than or equal $t o^{*}\\ge$ total order, to further differentiate positive numbers. Note that 0 remains the top in this suborder $([0,\\infty],\\geq)$ . ", "page_idx": 28}, {"type": "text", "text": "B.4 Operation: algebra over the product endofunctor ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In an elementary topos $\\mathbf{E}$ , the subobjects $\\operatorname{Sub}_{\\mathbf{E}}(C)$ of an object $C$ not only forms a preorder by inclusion but also are equipped with certain set operations (e.g., intersection and disjoint union), which are defined in terms of the universal properties of their corresponding order operations (e.g., meet and join). Further, these operations are reflected in the structures of the subobject classifier (e.g., conjunction and disjunction). ", "page_idx": 28}, {"type": "text", "text": "Here, we establish a link between the structures of the subobject classifier and those of a subobject quantifier. Our primary result is as follows: ", "page_idx": 28}, {"type": "text", "text": "Theorem 10. Consider a category with a subobject classifier $\\top:1\\mapsto\\Omega$ , a subobject quantifier $o:1\\mapsto\\Psi$ , and a quantizer $\\kappa:\\Psi\\to\\Omega$ . ", "page_idx": 29}, {"type": "text", "text": "Let n be a natural number. Let $\\beta:\\Omega^{n}\\to\\Omega$ be an $n$ -ary logical operation on $\\Omega$ , and let $\\alpha:\\Psi^{n}\\rightarrow\\Psi$ be an $n$ -ary quantitative operation on $\\Psi$ . ", "page_idx": 29}, {"type": "text", "text": "For $i\\in\\{1,\\ldots,n\\}_{\\!}$ , let $p_{i}:C\\to\\Omega$ be a predicate on an object $C$ , and let $q_{i}:C\\to\\Psi$ be a quantity on the object $C$ such that $p_{i}=\\kappa\\circ q_{i}$ . Let $p=\\langle p_{1},\\dots,p_{n}\\rangle$ be the tupling of the predicates, and let $q=\\langle q_{1},\\dots,q_{n}\\rangle$ be the tupling of the quantities. Let $b:B\\mapsto C:=(\\beta\\stackrel{.}{\\circ}p)^{*}\\top$ be the subobject classified by $\\beta\\circ p$ , and let $a:A\\stackrel{*}{\\longrightarrow}C:=\\stackrel{*}{(\\alpha\\circ q)^{*}}o$ be the subobject quantified by $\\alpha\\circ q$ . ", "page_idx": 29}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/70bf0c8e150d10671e3424ec82c0058200f7de77973edcb9de1377783d67a022.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Then, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\colon^{n}\\circ q=p}\\\\ &{\\colon(\\beta\\circ\\kappa^{n}\\circ\\alpha^{*}o=\\top_{\\alpha^{*}1})\\to((\\beta\\circ p)\\circ a=\\top_{A})}\\\\ &{\\colon(\\beta\\circ\\kappa^{n}=\\kappa\\circ\\alpha)\\to(\\beta\\circ\\kappa^{n}\\circ\\alpha^{*}o=\\top_{\\alpha^{*}1})}\\\\ &{\\colon(\\beta\\circ\\kappa^{n}=\\kappa\\circ\\alpha)\\to(\\kappa\\circ(\\alpha\\circ q)=\\beta\\circ p)}\\\\ &{\\colon(\\beta\\circ\\kappa^{n}=\\kappa\\circ\\alpha)\\to((\\alpha\\circ q)\\circ b=o_{B})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For convenience, we call an $n$ -ary operation $\\alpha:\\Psi^{n}\\rightarrow\\Psi$ (as an algebra over the product endofunctor $(-)^{n})$ homomorphic to $\\beta:\\Omega^{n}\\to\\Omega$ via a morphism $\\kappa:\\Psi\\to\\Omega$ if ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\beta\\circ\\kappa^{n}=\\kappa\\circ\\alpha\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and subhomomorphic to $\\beta:\\Omega^{n}\\to\\Omega$ if it satisfies the condition ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\beta\\circ\\kappa^{n}\\circ\\alpha^{*}o=\\mathsf{T}_{\\alpha^{*}1}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. (i) follows from the property of tupling and product: $\\kappa^{n}\\circ q\\,=\\,\\langle\\kappa\\circ q_{1},...\\,,\\kappa\\circ q_{n}\\rangle\\,=$ $\\langle p_{1},\\cdot\\cdot\\cdot,p_{n}\\rangle=p$ . ", "page_idx": 29}, {"type": "text", "text": "(ii) states that if $\\alpha$ is subhomomorphic to $\\beta$ , then $\\beta\\circ p$ is the classifying morphism of the subobject quantified by $\\alpha\\circ q$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\beta\\circ p\\circ a}\\\\ {=\\beta\\circ\\kappa^{n}\\circ q\\circ a}\\\\ {=\\beta\\circ\\kappa^{n}\\circ\\alpha^{*}o\\circ(\\alpha^{*}o)^{*}q}\\\\ {=\\top_{\\alpha^{*}1}\\circ({\\alpha^{*}o})^{*}q}\\\\ {=\\top_{A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(subhomomorphism) (57) ", "page_idx": 29}, {"type": "text", "text": "(composition) (58) ", "page_idx": 29}, {"type": "text", "text": "(iii) means that $\\alpha$ being homomorphic to $\\beta$ is a stronger condition than being merely subhomomorphic to $\\beta$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\beta\\circ\\kappa^{n}\\circ\\alpha^{*}o}\\\\ {=\\kappa\\circ\\alpha\\circ\\alpha^{*}o}\\\\ {=\\kappa\\circ o\\circ o^{*}\\alpha}\\\\ {=\\top_{\\alpha^{*}1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{(homomorphism)}}\\\\ {\\mathrm{(pullback)}}\\\\ {\\mathrm{(composition)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(62) ", "page_idx": 29}, {"type": "text", "text": "(iv) shows the relationship between the predicate $\\beta\\circ p$ and the quantity $\\alpha\\circ q$ when $\\alpha$ is homomorphic to $\\beta$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\kappa\\circ\\alpha\\circ q}\\\\ {=\\beta\\circ\\kappa^{n}\\circ q}\\\\ {=\\beta\\circ p}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(homomorphism) (64) ", "page_idx": 29}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/645c169cb14af36b9e01c5f7be1f05b4f29368765c64562bffe11b884bf7ae5c.jpg", "img_caption": ["Figure 3: Negation "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "(v) means that if $\\alpha$ is homomorphic to $\\beta$ , then $\\alpha\\circ q$ is a quantifying morphism of $b$ . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\kappa\\circ\\alpha\\circ q\\circ b}\\\\ {=\\beta\\circ p\\circ b}\\\\ {=\\top_{B}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In summary, if $\\alpha$ is subhomomorphic to $\\beta$ , then $a$ is included in $b$ ((ii)); if $\\alpha$ is homomorphic to $\\beta$ , then $a$ and $b$ are isomorphic ((iii) and (v)). We consider this weaker condition because subhomomorphic but non-homomorphic operations may exhibit favorable properties in other aspects, such as continuity. We will discuss several concrete examples in the following subsections. ", "page_idx": 30}, {"type": "text", "text": "B.5 Negation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "First, let us take a closer look at a unary logical operation \u2014 negation $\\neg:\\Omega\\rightarrow\\Omega$ , which is defined as the classifying morphism of $\\perp:1\\rightarrow\\Omega$ . Recall that $\\bot$ is the classifying morphism of $0\\rightarrow1$ . ", "page_idx": 30}, {"type": "text", "text": "Let us consider a unary quantitative operation $\\sim:\\Psi\\,\\rightarrow\\,\\Psi$ . If $\\sim$ is homomorphic to $\\neg$ via the quantizer $\\kappa$ , it means that $\\neg\\circ\\kappa=\\kappa\\circ\\sim$ , or the following diagram commutes: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\Psi\\xrightarrow{\\ \\sim\\ }\\Psi}\\\\ {\\Omega\\xrightarrow{\\ \\sim\\ }\\Psi}\\\\ {\\Omega\\xrightarrow{\\ \\sim\\ }\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let us consider the set $[0,\\infty]$ in Set. A quantitative operation $\\sim:[0,\\infty]\\rightarrow[0,\\infty]$ homomorphic to the negation $\\neg:\\{\\top,\\bot\\}\\rightarrow\\{\\top,\\bot\\}$ is a function ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\sim}(n):=[n=0]\\times n_{0}={\\binom{n_{0}}{0}}\\quad n=0,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which maps 0 to a non-zero number $n_{0}$ and any non-zero number to 0.11 However, this function is discontinuous at 0. On the other hand, if we consider a quantitative operation $\\sim$ subhomomorphic to the negation $\\neg$ , then the only requirement is that for all $n$ , $\\sim\\!(n)\\,=\\,0$ implies $n\\,>\\,0$ , or, by contraposition, ${\\sim}(0)>0$ . Continuous choices include the hinge function $n\\mapsto1{\\overset{.}{-}}n=\\operatorname*{max}\\{1{-}n,0\\}$ , reciprocal function $\\textstyle n\\mapsto{\\frac{1}{n}}$ , and exponential decay function $n\\mapsto e^{-n}$ (see Fig. 3). Note that the latter ", "page_idx": 30}, {"type": "equation", "text": "$$\n{}^{11}[-]:\\{\\top,\\bot\\}\\rightarrow[0,\\infty]:=\\left\\{{\\frac{\\bot\\mapsto0}{\\top\\mapsto1.}}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/4d6dc9bdf042499165f20bbecd3cb887c9e62ee4a1e22485f2b60a800df0ea2a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "two are actually homomorphic to the constant false $\\perp$ because their outputs are always non-zero. The hinge function $1\\div q$ , as discussed later, can be seen as derived from the implication $p\\rightarrow\\bot$ . ", "page_idx": 31}, {"type": "text", "text": "In this way, if we have a quantity $q$ for a predicate $p$ , we can obtain a quantity ${\\sim}q$ for the negation $\\neg p$ of the predicate as well. If the quantitative operation $\\sim$ is subhomomorphic but not homomorphic to the logical operation $\\neg$ , we can guarantee that for any $x$ , $,\\sim\\!q(x)=0$ implies $\\neg p(x)=\\top$ , but not vice versa. ", "page_idx": 31}, {"type": "text", "text": "B.6 Conjunction ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Next, let us move on to an important binary logical operation \u2014 conjunction $\\wedge:\\Omega\\times\\Omega\\to\\Omega$ , which is defined as the classifying morphism of $\\langle\\top,\\top\\rangle:1\\mapsto\\Omega\\times\\Omega$ . Similarly, we consider a binary quantitative operation $\\otimes:\\Psi\\times\\Psi\\rightarrow\\Psi$ homomorphic to the conjunction $\\wedge$ via the quantizer $\\kappa$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Psi\\times\\Psi\\xrightarrow{\\quad\\otimes\\quad}\\Psi}\\\\ &{\\quad\\kappa\\times\\kappa\\biggr\\downarrow}\\\\ &{\\quad\\Omega\\times\\Omega\\xrightarrow[\\quad\\triangle\\quad]{\\quad\\quad\\quad}\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By abuse of notation, the conjunction $\\wedge$ also denotes a binary operation on the set ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Omega)$ of predicates, such that for any two predicates $p_{1},p_{2}\\in\\mathrm{Hom}_{\\mathbf{E}}(\\bar{C},\\bar{\\Omega})$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\np_{1}\\wedge p_{2}:=\\wedge\\circ\\,\\langle p_{1},p_{2}\\rangle.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The same goes for the quantitative operation $\\otimes$ . ", "page_idx": 32}, {"type": "text", "text": "For the conjunction, we can prove a stronger result: ", "page_idx": 32}, {"type": "text", "text": "Theorem 11. Consider a category with a subobject classifier $\\top:1\\mapsto\\Omega$ , a subobject quantifier $o:1\\mapsto\\Psi$ , and a quantizer $\\kappa:\\Psi\\to\\Omega$ . ", "page_idx": 32}, {"type": "text", "text": "Let the conjunction $\\wedge:\\Omega\\times\\Omega\\to\\Omega$ be the classifying morphism of $\\langle\\top,\\top\\rangle:1\\longmapsto\\Omega\\times\\Omega,$ , and let $\\otimes:\\Psi\\times\\Psi\\rightarrow\\Psi$ be a binary operation on $\\Psi$ homomorphic to the conjunction $\\wedge$ via the quantizer $\\kappa$ . ", "page_idx": 32}, {"type": "text", "text": "Let $p_{1},p_{2}:C\\to\\Omega$ be two predicates on an object $C$ , and let $\\jmath_{1},q_{2}:C\\to\\Psi$ be two quantities on the object $C$ such that $p_{1}=\\kappa\\circ q_{1}$ and $p_{2}=\\kappa\\circ q_{2}$ . Let $p=\\langle p_{1},p_{2}\\rangle$ be the pairing of the predicates, and let $q=\\langle q_{1},q_{2}\\rangle$ be the pairing of the quantities. Let $b:B\\longrightarrow C:=(p_{1}\\wedge p_{2})^{*}\\top$ be the subobject classified by p1 \u2227p2. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{B}_{b\\Bigg\\downarrow}\\underbrace{\\rightarrow\\uparrow}_{\\langle o,o\\rangle\\downarrow}\\underbrace{\\rightarrow\\downarrow}_{\\mathrm{~\\rightmoon~}}}\\\\ {\\underbrace{b\\Bigg\\downarrow}_{C}\\underbrace{\\rightarrow\\uparrow}_{\\langle q_{1},q_{2}\\rangle}\\underbrace{\\rightarrow\\Psi\\times\\Psi}\\xrightarrow[\\kappa\\times\\kappa]{\\rightarrow}\\Psi}\\\\ {\\underbrace{C}\\underbrace{\\dots\\Psi\\times\\Psi}_{\\langle p_{1},p_{2}\\rangle}\\rightarrow\\Omega\\times\\Omega\\xrightarrow[\\Lambda]{\\rightarrow}\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, $\\otimes$ is a quantifying morphism of $\\langle o,o\\rangle$ , and $b$ is a pullback of $\\langle o,o\\rangle$ along $\\langle q_{1},q_{2}\\rangle$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. If $q_{1}\\otimes q_{2}=o_{C}$ , then $\\left(\\kappa\\circ q_{1}\\right)\\wedge\\left(\\kappa\\circ q_{2}\\right)=\\top_{C}$ , which leads to $\\langle\\kappa\\circ q_{1},\\kappa\\circ q_{2}\\rangle=\\langle\\top,\\top\\rangle\\circ e_{C}=$ $\\langle\\top_{C},\\top_{C}\\rangle$ due to the universal property of pullback. Then, we have $\\kappa\\circ q_{1}=\\kappa\\circ q_{2}=\\intercal_{C}$ due to the universal property of pairing, and consequently $q_{1}=q_{2}=o_{C}$ according to Lemma 9, i.e., $\\langle q_{1},q_{2}\\rangle=\\langle o,o\\rangle\\,\\stackrel{\\_}{\\circ}e_{C}$ . Therefore, $\\langle o,o\\rangle$ is a pullback of $o$ along $\\otimes$ . According to Theorem 10, $b$ is a pullback of $o$ along $q_{1}\\otimes q_{2}$ . Then, following the pullback lemma, $b$ is a pullback of $\\langle o,o\\rangle$ along $\\langle q_{1},q_{2}\\rangle$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "In other words, the pullback square symbols in Eq. (73) are unambiguous \u2014 the top row, the left column, the right column, the top-left square, and the top-right square are all pullbacks. ", "page_idx": 32}, {"type": "text", "text": "Note that for any quantities $a,b,c:C\\to\\Psi$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\kappa\\circ((a\\otimes b)\\otimes c)=\\kappa\\circ(a\\otimes(b\\otimes c)),}}\\\\ {{\\kappa\\circ(a\\otimes b)=\\kappa\\circ(b\\otimes a),}}\\\\ {{\\kappa\\circ(o_{C}\\otimes a)=\\kappa\\circ a,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "due to the associativity, commutativity, and unitality of the conjunction, but the quantitative operation $\\otimes$ is not required to satisfy these properties, i.e., it is possible that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(a\\otimes b)\\otimes c\\neq a\\otimes(b\\otimes c),}}\\\\ {{a\\otimes b\\neq b\\otimes a,}}\\\\ {{o_{C}\\otimes a\\neq a.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "However, in the following examples, we mainly consider quantitative operations $\\otimes$ such that these properties are satisfied. In such cases, $(\\mathrm{Hom}_{\\mathbf{E}}(C,\\Psi),\\otimes,o_{C})$ forms a commutative monoid, and consequently ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\kappa)$ is a monoid homomorphism from it to $(\\mathrm{Hom}_{\\mathbf{E}}(C,\\Omega),\\Lambda,\\top_{C})$ . ", "page_idx": 32}, {"type": "text", "text": "In Appendix B.3, we introduced preorder structures on the predicates ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Omega)$ and the quantities ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Psi)$ . The conjunction $\\wedge$ is a commutative monoidal structure compatible with the preorder $(\\mathrm{Hom}_{\\mathbf{E}}(C,\\Omega),\\boldsymbol{\\preceq_{C}^{\\top}})$ because it is the meet operation. For the set ${\\mathrm{Hom}}_{\\mathbf{E}}(C,\\Psi)$ of quantities, we can choose the quantitative operation $\\otimes$ to be the meet operation as well. Alternatively, we can only require it to be compatible with the preorder, in the sense that the monoid product is order-preserving: for any quantities $\\boldsymbol{q}_{1}^{\\prime},\\boldsymbol{q}_{1}^{\\prime},\\boldsymbol{q}_{2},\\boldsymbol{q}_{2}^{\\prime}\\in\\mathrm{Hom}_{\\mathrm{E}}(\\boldsymbol{C},\\boldsymbol{\\Psi})$ , if $q_{1}\\preceq_{C}^{o}q_{1}^{\\prime}$ and $q_{2}\\preceq_{C}^{o}q_{2}^{\\lambda}$ , then $q_{1}\\otimes q_{2}\\stackrel{\\bullet}{\\longrightarrow}_{C}^{o}q_{1}^{\\prime}\\otimes q_{2}^{\\daleth}$ . In other words, we require $(\\mathrm{Hom}_{\\mathbf{E}}(C,\\Psi),\\preceq_{C}^{o},\\otimes,o_{C})$ to be a symmetric monoidal preorder [Fong and Spivak, 2019, Definition 2.2]. ", "page_idx": 32}, {"type": "text", "text": "Example 13. Let us consider two commutative monoidal structures on the preorder $([0,\\infty],\\geq)$ . The max operation max $:[0,\\infty]\\times[0,\\infty]\\rightarrow[0,\\infty]$ is the meet, i.e., cartesian product, while the addition $+:[0,\\infty]\\times[0,\\infty]\\stackrel{!}{\\rightarrow}[0,\\bar{\\infty}]$ is a monoidal product. They are both semicartesian because the top 0 is the unit. ", "page_idx": 33}, {"type": "text", "text": "Note that $([0,\\infty],+,0)$ , $([0,1],\\times,1)$ , and $([1,\\infty],\\times,1)$ are isomorphic to each other with the following isomorphisms: ", "page_idx": 33}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/be272e5662a5586c85b960c366dc8fab28a5c14b042bc2980e5bb2763a706c13.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "We can also induce a monoidal structure on a set if it is isomorphic to a monoid: ", "page_idx": 33}, {"type": "text", "text": "Lemma 12. Let $f:A\\rightleftarrows B:g$ be a pair of bijections between sets $A$ and $B$ . If $(B,\\otimes_{B},I_{B})$ is $a$ monoid, then $(A,\\otimes_{A}:=g\\circ\\otimes_{B}\\circ(f\\times f),I_{A}:=g\\circ I_{B})$ is also a monoid. ", "page_idx": 33}, {"type": "text", "text": "Proof. Associativity: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(a\\otimes_{A}b)\\otimes_{A}c}}\\\\ {{=g((f(a)\\otimes_{B}f(b))\\otimes_{B}f(c))}}\\\\ {{=g(f(a)\\otimes_{B}(f(b)\\otimes_{B}f(c)))}}\\\\ {{=a\\otimes_{A}(b\\otimes_{A}c)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Left unitality: ", "page_idx": 33}, {"type": "text", "text": "(85) ", "page_idx": 33}, {"type": "text", "text": "(86) ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad I_{A}\\otimes_{A}a}\\\\ &{=g(I_{B})\\otimes_{A}a}\\\\ &{=g(f(g(I_{B}))\\otimes_{B}f(a))}\\\\ &{=g(I_{B}\\otimes_{B}f(a))}\\\\ &{=g(f(a))}\\\\ &{=a}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Right unitality can be proven similarly. ", "page_idx": 33}, {"type": "text", "text": "In this way, we can obtain a richer choice of monoidal structures on the set $[0,\\infty]$ beyond the addition (see Fig. 4): ", "page_idx": 33}, {"type": "text", "text": "Example 14 (Semicartesian monoidal product). ", "page_idx": 33}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r l}&{e^{x}-1:([0,\\infty],\\otimes,0)\\rightleftarrows([0,\\infty],+,0):\\log(1+x)\\qquad}&&{a\\otimes b:=\\log(e^{a}+e^{b}-1)}\\\\ &{\\qquad x^{2}:([0,\\infty],\\otimes,0)\\rightleftarrows([0,\\infty],+,0):{\\sqrt{x}}}&&{a\\otimes b:={\\sqrt{a^{2}+b^{2}}}}\\\\ &{\\qquad{\\sqrt{x}}:([0,\\infty],\\otimes,0)\\rightleftarrows([0,\\infty],+,0):x^{2}}&&{a\\otimes b:=a+b+2{\\sqrt{a b}}}\\\\ &{x+1:([0,\\infty],\\otimes,0)\\rightleftarrows([1,\\infty],\\times,1):x-1}&&{a\\otimes b:=a+b+a b}\\end{array}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In summary, the max operation on $[0,\\infty]$ can be regarded as a continuous conjunction, whereas a semicartesian monoidal product, such as the addition, can be viewed as a soft max. ", "page_idx": 33}, {"type": "text", "text": "B.7 Disjunction ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Dually, the disjunction $\\vee:\\Omega\\times\\Omega\\rightarrow\\Omega$ reflects the join of the inclusion preorder of subobjects. Similarly, we want to find a quantitative operation $\\oplus:\\Psi\\times\\Psi\\rightarrow\\Psi$ homomorphic to the disjunction $\\vee$ via the quantizer $\\kappa$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Psi\\times\\Psi\\xrightarrow[\\quad\\forall]{\\quad\\oplus\\quad}\\Psi}\\\\ {\\kappa\\times\\kappa\\bigg\\downarrow\\quad\\quad\\quad\\quad\\quad\\bigg\\downarrow_{\\kappa}}\\\\ {\\Omega\\times\\Omega\\xrightarrow[\\quad\\forall]{\\quad\\quad}\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using the same technique as in Lemma 12, we can obtain several monoidal structures on the set $[0,\\infty]$ homomorphic to the disjunction (see Fig. 5): ", "page_idx": 33}, {"type": "text", "text": "Example 15 (Semicocartesian monoidal product). ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\frac{1}{x}}:([0,\\infty],\\oplus,\\infty)\\rightleftarrows([0,\\infty],+,0):{\\frac{1}{x}}}\\\\ &{{\\frac{1}{x^{2}}}:([0,\\infty],\\oplus,\\infty)\\rightleftarrows([0,\\infty],+,0):{\\frac{1}{\\sqrt{x}}}}\\\\ &{1-e^{-x}:([0,\\infty],\\oplus,\\infty)\\rightleftarrows([0,1],\\times,1):-\\log(1-x)}&{a\\oplus b:=-\\log(e^{-a}+e^{-b}-e^{-(a+b)})}\\\\ &{{\\mathrm{tanh}}:([0,\\infty],\\oplus,\\infty)\\rightleftarrows([0,1],\\times,1):\\arctan{\\mathrm{h}}}&{a\\oplus b:=\\mathrm{arctanh}(\\operatorname{tanh}(a)\\operatorname{tanh}(b))}\\end{array}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "However, we usually choose the quantitative operation $\\bigoplus$ to be the join operation min of the preorder $([0,\\infty],\\geq)$ . In this way, $\\otimes$ distributes over $\\bigoplus$ , i.e., for any quantities $a,b,c:C\\to\\Psi$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\na\\otimes(b\\oplus c)=(a\\otimes b)\\oplus(a\\otimes c).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "A typical example is the min-plus semiring $([0,\\infty],\\operatorname*{min},+)$ [Pin, 1998]. ", "page_idx": 34}, {"type": "text", "text": "B.8 Implication ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Finally, let us construct a quantitative counterpart of the implication $\\to:\\Omega\\times\\Omega\\to\\Omega$ , which is right adjoint to the conjunction $\\wedge$ . For global elements $a,b,c:1\\rightarrow\\Omega$ , this means that ", "page_idx": 34}, {"type": "equation", "text": "$$\nc\\wedge a\\vdash b\\;{\\mathrm{if~and~only~if~}}c\\vdash a\\to b.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "There are two ways to construct a quantitative operation $-\\circ:\\Psi\\times\\Psi\\rightarrow\\Psi$ corresponding to the implication $\\rightarrow$ . One way is to find a quantitative operation \u22b8homomorphic to the implication $\\rightarrow$ via the quantizer $\\kappa$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Psi\\times\\Psi\\xrightarrow{\\quad\\to\\;\\Psi}\\Psi}\\\\ {\\kappa\\times\\kappa\\biggr\\downarrow\\quad\\quad\\quad\\quad\\biggr\\downarrow_{\\kappa}}\\\\ {\\Omega\\times\\Omega\\xrightarrow[\\rightarrow\\rightarrow\\Omega]{\\quad\\to\\;\\Omega}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Example 16. For the set $[0,\\infty]$ , a quantitative operation $-\\circ:[0,\\infty]\\times[0,\\infty]\\rightarrow[0,\\infty]$ homomorphic to the implication $\\rightarrow:\\{\\bar{\\top},\\bot\\}\\times\\{\\top,\\bot\\}\\rightarrow\\{\\top,\\bot\\}$ is a function ", "page_idx": 34}, {"type": "equation", "text": "$$\n(a,b)\\mapsto[a=0]\\times[b>0]\\times f(b)={\\binom{f(b)}{0}}\\quad a=0{\\mathrm{~and~}}b>0,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $f:(0,\\infty]\\rightarrow(0,\\infty]$ is an arbitrary function to non-zero numbers. Note that this function is discontinuous at the line $a=0$ and $b>0$ (cf. Appendix B.5). ", "page_idx": 34}, {"type": "text", "text": "The other way is to find a quantitative operation $\\multimap$ right adjoint to an operation $\\otimes$ homomorphic to the conjunction $\\wedge$ , i.e., the internal hom of the monoidal closed preorder [Fong and Spivak, 2019, Definition 2.79]. ", "page_idx": 34}, {"type": "text", "text": "Example 17. For the meet-semilattice $([0,\\infty],\\geq,\\operatorname*{max},0)$ , the function ", "page_idx": 34}, {"type": "equation", "text": "$$\n(a,b)\\mapsto[a<b]\\times b=\\left\\{0\\quad a\\geq b,\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "is right adjoint to the max because ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{c,a\\}\\geq b\\,{\\mathrm{if~and~only~if~}}c\\geq{\\left\\{\\begin{array}{l l}{0}&{a\\geq b,}\\\\ {b}&{a<b.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "While this function is subhomomorphic to the implication, it is still discontinuous at the line $a=b$ . ", "page_idx": 34}, {"type": "text", "text": "Example 18. For the monoidal preorder $([0,\\infty],\\geq,+,0)$ , the truncated subtraction $\\doteq$ (a.k.a. monus [Amer, 1984]) ", "page_idx": 34}, {"type": "equation", "text": "$$\nb\\div a:=\\operatorname*{max}\\{b-a,0\\}={\\binom{0}{b-a}}\\leq b,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "is right adjoint to the addition because ", "page_idx": 35}, {"type": "equation", "text": "$$\nc+a\\geq b{\\mathrm{~if~and~only~if~}}c\\geq b\\therefore a.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The truncated subtraction is continuous and subhomomorphic to the implication. Note that the hinge function $n\\mapsto\\operatorname*{max}\\{1-n,0\\}=1\\div n$ for the negation can be interpreted as the quantitative operation derived from $\\neg n=n\\rightarrow\\bot$ , where 1 is homomorphic to the constant false $\\bot$ . ", "page_idx": 35}, {"type": "text", "text": "Similarly to Lemma 12, if two symmetric monoidal preorders are isomorphic and one of them is closed, we can induce that the other is also closed: ", "page_idx": 35}, {"type": "text", "text": "Lemma 13. Let $f~:~A~\\rightleftarrows~B~:~g$ be a pair of isomorphisms between symmetric monoidal preorders $(A,\\preceq_{A},\\otimes_{A},I_{A})$ and $(B,\\preceq_{B},\\otimes_{B},I_{B})$ . If $(B,\\preceq_{B},\\otimes_{B},I_{B},\\to_{B})$ is closed, then $(A,\\preceq_{A}$ , $\\otimes_{A},I_{A},-\\circ_{A}:=g\\circ-\\circ_{B}\\circ(f\\times f))$ is also closed. ", "page_idx": 35}, {"type": "text", "text": "Proof. For all $a,b,c\\in A$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad c\\preceq_{A}(a-\\circ_{A}b)}\\\\ &{\\equiv c\\preceq_{A}g(f(a)\\to_{B}f(b))}\\\\ &{\\equiv f(c)\\preceq_{B}f(a)\\to_{B}f(b)}\\\\ &{\\equiv f(c)\\otimes_{B}f(a)\\preceq_{B}f(b)}\\\\ &{\\equiv c\\otimes_{A}a\\preceq_{A}b}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This means that $-\\circ_{A}$ is right adjoint to $\\otimes_{A}$ ", "page_idx": 35}, {"type": "text", "text": "In this way, we can find the internal homs corresponding to the monoidal products discussed in Appendix B.6 (see Fig. 6). ", "page_idx": 35}, {"type": "text", "text": "As a side note, we can use the quantitative operations discussed above to define quantitative operations for other logical connectives. For example, the logical equivalence $a\\leftrightarrow b$ can be represented as $(a\\to b)\\land({\\bar{b}}\\to a)$ (bi-implication), $(\\neg{\\bar{a}}\\lor b)\\land({\\bar{\\neg}}b\\lor a)$ (conjunctive normal form (CNF)), or $(a\\wedge b)\\vee(\\neg a\\wedge\\neg b)$ (disjunctive normal form (DNF)), and its quantitative operations can be defined accordingly. Some examples are shown in Fig. 7. ", "page_idx": 35}, {"type": "text", "text": "B.9 Heyting algebra, quantale, and ordered semiring ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Now, having constructed the logical operations and their corresponding quantitative operations, we can compare the structures of the subobject classifier with those of a subobject quantifier. ", "page_idx": 35}, {"type": "text", "text": "It is known that the global elements of the subobject classifier with the logical operations form a Heyting algebra $(\\Omega,\\vdash,\\top,\\bot,\\land,\\lor,\\rightarrow)$ : ", "page_idx": 35}, {"type": "text", "text": "Definition 25. A Heyting algebra is a cartesian closed bounded lattice. ", "page_idx": 35}, {"type": "text", "text": "In categorical terms, $\\intercal$ is the terminal object, $\\bot$ is the initial object, $\\wedge$ is the product, $\\vee$ is the coproduct, and $\\rightarrow$ is the exponential in the preorder $\\left(\\Omega,\\vdash\\right)$ (as a thin category). ", "page_idx": 35}, {"type": "text", "text": "We weakened the requirements to construct the algebraic structures of the subobject quantifier, which usually forms what is called a quantale $(\\Psi,\\preceq,1,0,\\otimes,\\oplus,-\\circ)$ [Mulvey, 1986, Dudzik, 2017]: ", "page_idx": 35}, {"type": "text", "text": "Definition 26. A (unital) quantale is a monoidal closed suplattice. ", "page_idx": 35}, {"type": "text", "text": "This means that we can consider a preorder $(\\Psi,\\preceq)$ on the subobject quantifier as a thin category, where 1 is the terminal object, 0 is the initial object, $\\otimes$ is a monoidal product and not necessarily the product, $\\bigoplus$ is still the coproduct, and $\\multimap$ is the internal hom right adjoint to the monoidal product $\\otimes$ . An example is the Lawvere quantale $([0,\\infty],\\geq,0,\\infty,+,\\operatorname*{min},\\,\\dot{-})$ [Lawvere, 1973, Bacci et al., 2023]. Then, the quantizer $\\kappa:\\Psi\\to\\Omega$ is a homomorphism preserving some or all the structures. ", "page_idx": 35}, {"type": "text", "text": "Note that due to the adjoint functor theorem and the fact that left adjoints preserve colimits, the product distributes over the coproduct in a Heyting algebra, and the monoidal product distributes over the coproduct in a quantale. We can further relax the requirement for $\\oplus$ to be the coproduct and instead consider a monoidal product (Appendix B.7). If we still require the distributivity for the two monoidal structures $\\otimes$ and $\\oplus$ , the algebraic structure is an ordered semiring [Fujii, 2023]. Further investigation is left for future work. ", "page_idx": 35}, {"type": "text", "text": "B.10 Quantifier: algebra over the exponentiation endofunctor ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Up to this point, our focus has been on $n$ -ary operations in propositional logic (Appendix B.4). Next, we introduce the universal quantification $\\forall$ and existential quantification $\\exists$ used in predicate logic and their quantitative counterparts. ", "page_idx": 36}, {"type": "text", "text": "Externally, the universal quantification and the existential quantification are right and left adjoint to the pullback of projection, respectively [Lawvere, 1969]. Internally, the universal quantifier $\\forall_{D}:\\Bar{\\Omega^{D}}\\rightarrow\\Omega$ and existential quantifier $\\exists_{D}:\\Omega^{D}\\to\\Omega$ are given by morphisms from the power object $\\Omega^{D}$ of an object $D$ to the subobject classifier $\\Omega$ . ", "page_idx": 36}, {"type": "text", "text": "Recall that the power object $\\Omega^{D}$ is also an exponential object into the subobject classifier $\\Omega$ , so the universal quantifier $\\forall_{D}$ and existential quantifier $\\exists_{D}$ can also be viewed as algebras over the exponentiation endofunctor $\\left(-\\right)^{D}$ of exponentiation on the subobject classifier $\\Omega$ . Then, we can consider algebras on the subobject quantifier $\\Psi$ homomorphic to them, which serve as quantitative counterparts of these quantifiers. ", "page_idx": 36}, {"type": "text", "text": "Our main result is as follows (cf. Theorem 10): ", "page_idx": 36}, {"type": "text", "text": "Theorem 14. Consider an elementary topos with a subobject classifier $\\top:1\\rightarrow\\Omega$ , a subobject quantifier $o:1\\mapsto\\Psi$ , and a quantizer $\\kappa:\\Psi\\rightarrow\\Omega$ .   \nLet $p:C\\times D\\to\\Omega$ be a predicate on a product, and let $q:C\\times D\\to\\Psi$ be a quantity such that $p=\\kappa\\circ q$ . Let $\\widehat{p}:C\\to{\\Omega}^{D}$ and $\\widehat{q}:C\\to\\Psi^{D}$ be their exponential transposes.   \nLet $\\beta_{D}:\\Omega^{D}\\to\\Omega$ be a predicate, and let $\\alpha_{D}:\\Psi^{D}\\to\\Psi$ be a quantity homomorphic to $\\beta_{D}$ via the quantizer $\\kappa_{;}$ , i.e., $\\beta_{D}\\circ\\kappa^{D}=\\kappa\\circ\\alpha_{D}$ .   \nLet $b:B\\mapsto C:=(\\beta_{D}\\circ\\widehat{p})^{*}\\intercal$ be the subobject classified by $\\beta_{D}\\circ{\\widehat{p}},$ and let $a:A\\mapsto C:={\\big(}\\alpha_{D}\\circ{\\widehat{q}}{\\big)}^{*}o$ ", "page_idx": 36}, {"type": "text", "text": "be the subobject quantifie d by $\\alpha_{D}\\circ{\\widehat{q}}.$ . ", "page_idx": 36}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/f1314d86f3b7b24eeb5b046edbc4ef50b4168265bf107f7125f375d8e55b2442.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Then, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\boldmath~\\Gamma~}(i)\\;\\;\\kappa^{D}\\circ\\widehat{q}=\\widehat{\\boldsymbol{p}}}\\\\ &{\\mathrm{~\\}(i i)\\;\\;\\kappa\\circ\\left(\\alpha_{D}\\circ\\widehat{q}\\right)=\\beta_{D}\\circ\\widehat{\\boldsymbol{p}}}\\\\ &{\\mathrm{~\\}(i i i)\\;\\;(\\beta_{D}\\circ\\widehat{p})\\circ a=\\overline{{\\mathbf{\\Gamma}}}_{A}}\\\\ &{\\mathrm{~\\}(i\\nu)\\;\\;(\\alpha_{D}\\circ\\widehat{q})\\circ b=o_{B}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. (i) follows from the property of exponential. ", "page_idx": 36}, {"type": "text", "text": "(ii) shows the relationship between the predicate $\\beta_{D}\\circ{\\widehat{p}}$ and the quantity $\\alpha_{D}\\circ{\\widehat{q}}$ when $\\alpha_{D}$ is homomorphic to $\\beta_{D}$ . ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\kappa\\circ\\alpha_{D}\\circ\\widehat{q}}}\\\\ {{=\\beta_{D}\\circ\\kappa^{D}\\circ\\widehat{q}}}\\\\ {{=\\beta_{D}\\circ\\widehat{p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(iii) means that $\\beta_{D}\\circ{\\widehat{p}}$ is a classifying morphism of $a$ . ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\beta_{D}\\circ\\widehat{p}\\circ a}\\\\ &{=\\kappa\\circ\\alpha_{D}\\circ\\widehat{q}\\circ a}\\\\ &{=\\kappa\\circ o_{A}}\\\\ &{=\\top_{A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(composition) (120) ", "page_idx": 36}, {"type": "text", "text": "(iv) means that $\\alpha_{D}\\circ{\\widehat{q}}$ is a quantifying morphism of $b$ . ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\kappa\\circ\\alpha_{D}\\circ\\widehat{q}\\circ b}\\\\ &{=\\beta_{D}\\circ\\widehat{p}\\circ b}\\\\ &{=\\top_{B}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "$\\alpha_{D}\\circ\\widehat{q}\\circ b=o_{B}$ follows from Lemma 9. ", "page_idx": 37}, {"type": "text", "text": "Definition 27 (Universal aggregator). A universal aggregator $\\nabla_{D}:\\Psi^{D}\\to\\Psi$ is a quantity that is homomorphic to the universal quantifier $\\forall_{D}:\\Omega^{D}\\to\\bar{\\Omega}$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Psi^{D}\\xrightarrow[\\hphantom{\\Psi^{D}}]{\\quad\\nabla_{D}\\quad\\nabla_{D}}\\Psi}}\\\\ {{\\kappa^{D}\\bigg\\downarrow}}\\\\ {{\\Omega^{D}\\xrightarrow[\\forall\\hphantom{\\Psi^{D}}]{\\quad\\nabla_{D}\\quad\\quad}\\ddots}\\ \\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Definition 28 (Existential aggregator). An existential aggregator $\\Delta_{D}:\\Psi^{D}\\to\\Psi$ is a quantity that is homomorphic to the existential quantifier $\\exists_{D}:\\Omega^{D}\\to{\\bar{\\Omega}}$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Psi^{D}\\xrightarrow[]{\\Delta_{D}}\\Psi}}\\\\ {{\\kappa^{D}\\downarrow}}\\\\ {{\\Omega^{D}\\xrightarrow[]{\\Delta_{D}}\\Omega}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Example 19. For the set $[0,\\infty]$ , the canonical choices of universal aggregator $\\nabla_{D}$ and existential aggregator $\\triangle_{D}$ are sup and inf. If the set $D$ is finite, we can also use sum and mean as the universal aggregator. Non-examples of universal aggregator include median and mode, which are not homomorphic to the universal quantifier. ", "page_idx": 37}, {"type": "text", "text": "Note that the universal quantifier and existential quantifier are commutative up to isomorphism: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall_{A\\times B}\\cong\\forall_{B}\\circ\\forall_{A}^{B}\\cong\\forall_{A}\\circ\\forall_{B}^{A}\\cong\\forall_{B\\times A},}\\\\ &{\\exists_{A\\times B}\\cong\\exists_{B}\\circ\\exists_{A}^{B}\\cong\\exists_{A}\\circ\\exists_{B}^{A}\\cong\\exists_{B\\times A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "However, we are free to choose different aggregators for different objects that are not necessarily commutative. For example, the sum of max is usually not equal to the max of sum. ", "page_idx": 37}, {"type": "text", "text": "B.11 Enrichment ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Lastly, we describe the conversion based on enrichment. ", "page_idx": 37}, {"type": "text", "text": "First, let us define the enriching category: ", "page_idx": 37}, {"type": "text", "text": "Definition 29. Let $(\\Psi,\\preceq,\\otimes,\\oplus,-\\circ)$ be an internal quantale object in Set. We define $\\Psi$ -Set to be a category whose objects are tuples consisting of a set $C$ , a $\\Psi$ -valued strict premetric $d_{C}$ on $C$ , and universal and existential aggregators on $C$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n(C,d_{C}:C\\times C\\to\\Psi,\\nabla_{C},\\Delta_{C}:\\Psi^{C}\\to\\Psi),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and morphisms from $(A,d_{A},\\nabla_{A},\\triangle_{A})$ to $(B,d_{B},\\nabla_{B},\\Delta_{B})$ are functions $f:A\\rightarrow B$ . ", "page_idx": 37}, {"type": "text", "text": "Definition 30. A bifunctor $\\boxtimes$ on $\\Psi$ -Set is given by the Cartesian product of sets and functions, together with the following products of strict premetrics and aggregators: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{A}\\boxtimes d_{B}:\\!(A\\times B)\\times(A\\times B)\\cong(A\\times A)\\times(B\\times B)\\xrightarrow{d_{A}\\times d_{B}}\\Psi\\times\\Psi\\xrightarrow{\\otimes}\\Psi.}\\\\ &{\\nabla_{A}\\boxtimes\\nabla_{B}:\\!\\Psi^{A\\times B}\\cong(\\Psi^{A})^{B}\\xrightarrow{\\nabla_{A}^{B}}\\Psi^{B}\\xrightarrow{\\nabla_{B}}\\Psi,}\\\\ &{\\Delta_{A}\\boxtimes\\triangle_{B}:\\!\\Psi^{A\\times B}\\cong(\\Psi^{A})^{B}\\xrightarrow{\\Delta_{A}^{B}}\\Psi^{B}\\xrightarrow{\\Delta_{B}}\\Psi.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proposition 15. The product $d_{A}\\boxtimes d_{B}$ of strict premetrics given in Definition $30$ is again a strict premetric. ", "page_idx": 37}, {"type": "text", "text": "Proof. This is a result of Theorem 11. Consider the following diagram: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A\\times B\\xrightarrow{\\operatorname{id}_{A\\times B}}A\\times B\\xrightarrow{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad1}1}\\\\ &{\\Delta_{A\\times B}\\bigg\\downarrow\\quad\\xrightarrow{\\quad\\quad\\quad\\Delta_{A}\\times\\Delta_{B}\\bigg\\downarrow\\quad\\quad\\quad\\quad\\quad\\quad\\langle o,o\\rangle\\bigg\\downarrow^{-1}}\\bigg\\downarrow_{o}}\\\\ &{(A\\times B)^{2}\\xrightarrow{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad A^{2}\\times B^{2}\\xrightarrow{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\Psi\\times\\Psi\\xrightarrow{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\Psi}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "$\\Delta_{A}\\times\\Delta_{B}$ is a pullback of $o$ along $\\otimes\\circ(d_{A}\\times d_{B})$ because $\\langle o,o\\rangle$ is a pullback of $o$ along $\\otimes$ according to Theorem 11, $\\Delta_{A}\\times\\Delta_{B}$ is a pullback of $\\langle o,o\\rangle$ along $d_{A}\\times d_{B}$ , and we can apply the pullback lemma. $\\Delta_{A}\\times\\Delta_{B}$ is isomorphic to $\\Delta_{A\\times B}$ , which means that $\\otimes\\circ(d_{A}\\times d_{B})$ is a strict premetric. Based on this definition, we can show that ", "page_idx": 38}, {"type": "equation", "text": "$$\n(d_{A}\\boxtimes d_{B})\\boxtimes d_{C}\\cong d_{A}\\boxtimes(d_{B}\\boxtimes d_{C}),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "because $\\times$ and $\\otimes$ are associative up to isomorphism. ", "page_idx": 38}, {"type": "text", "text": "Further, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigl(\\nabla_{A}\\boxtimes\\nabla_{B}\\bigr)\\boxtimes\\nabla_{C}\\cong\\nabla_{A}\\boxtimes\\bigl(\\nabla_{B}\\boxtimes\\nabla_{C}\\bigr),}\\\\ &{\\bigl(\\triangle_{A}\\boxtimes\\triangle_{B}\\bigr)\\boxtimes\\triangle_{C}\\cong\\triangle_{A}\\boxtimes\\bigl(\\triangle_{B}\\boxtimes\\triangle_{C}\\bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "because composition is associative. ", "page_idx": 38}, {"type": "text", "text": "The singleton $(\\{*\\},o_{\\{*\\}}\\!\\times\\!\\{*\\},\\mathrm{id_{\\{*\\}},i d_{\\{*\\}}})$ is the unit of the bifunctor $\\boxtimes$ . In this way, $(\\Psi\\mathbf{-Set},\\boxtimes,\\{\\ast\\})$ forms a monoidal category. ", "page_idx": 38}, {"type": "text", "text": "However, note that the bifunctor $\\boxtimes$ is not symmetric because $\\nabla_{A}\\boxtimes\\nabla_{B}$ and $\\triangle_{A}\\boxtimes\\triangle_{B}$ are not necessarily symmetric, and $d_{A}\\!\\boxtimes d_{B}\\cong d_{B}\\!\\boxtimes d_{A}$ if and only if the monoidal product $\\otimes$ of the quantale $\\Psi$ is commutative. ", "page_idx": 38}, {"type": "text", "text": "Since $\\Psi$ -Set is a monoidal category, we can consider a strict monoidal functor $F_{\\Psi}:\\mathbf{Set}\\rightarrow\\Psi.$ -Set, which equips products of sets with product strict premetrics and product aggregators: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{A\\times B}:=d_{A}\\boxtimes d_{B},}\\\\ &{\\nabla_{A\\times B}:=\\nabla_{A}\\boxtimes\\nabla_{B},}\\\\ &{\\Delta_{A\\times B}:=\\triangle_{A}\\boxtimes\\triangle_{B}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Such a monoidal functor induces a functor from a (Set-enriched) category to a $\\Psi$ -Set-enriched category, called the base change of enriching category. This means that we have a systematic way to equip a set $[A,B]$ of morphisms with a strict premetric ", "page_idx": 38}, {"type": "equation", "text": "$$\nd_{[A,B]}:[A,B]\\times[A,B]\\rightarrow\\Psi,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "a universal aggregator ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\nabla_{[A,B]}:\\Psi^{[A,B]}\\rightarrow\\Psi,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and an existential aggregator ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\triangle_{[A,B]}:\\Psi^{[A,B]}\\rightarrow\\Psi,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which is compatible with the product. ", "page_idx": 38}, {"type": "text", "text": "Then, Theorem 1 is a special case of this enrichment. The relationship between predicates and quantities follows from Theorems 10 and 14. ", "page_idx": 38}, {"type": "text", "text": "B.12 Summary ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Finally, to accommodate readers without a background in category theory, we present the instantiated definitions and theoretical results free of categorical terminology. The non-categorical proofs are omitted. We will be using the following functions: ", "page_idx": 38}, {"type": "text", "text": "$\\blacktriangledown$ the zero predicate $\\zeta:[0,\\infty]\\to\\{\\top,\\bot\\}:=x\\mapsto(x=0),$ $\\pmb{\\mathrm{\\Pi}}$ the product $\\tilde{\\mathfrak{s}}^{n}:[0,\\infty]^{n}\\to\\{\\top,\\bot\\}^{n}:(q_{1},\\dots,q_{n})\\mapsto(q_{1}=0,\\dots,q_{n}=0)$ ), and $\\blacktriangledown$ the postcomposition $\\zeta^{A}:[0,\\infty]^{A}\\to\\{\\top,\\bot\\}^{A}:=q\\mapsto\\zeta\\circ q$ of the zero predicate. ", "page_idx": 38}, {"type": "text", "text": "Definition 31 (Quantity). A quantity $q:A\\rightarrow[0,\\infty]$ is homomorphic to a predicate $p:A\\to\\{\\top,\\bot\\}$ if $p=\\zeta\\circ q$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\forall a\\in A.\\;(q(a)=0)\\leftrightarrow p(a).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "A quantity $q$ is subhomomorphic to a predicate $p$ if $\\zeta\\circ q\\to p$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\forall a\\in A.\\;(q(a)=0)\\to p(a).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Example 20. A strict premetric $d_{A}:A\\times A\\to[0,\\infty]$ is a quantity on the product set $A\\times A$ homomorphic to the equality predicate $=_{A}\\colon A\\times A\\stackrel{}{\\rightarrow}\\{\\bar{\\top},\\bot\\}$ . ", "page_idx": 39}, {"type": "text", "text": "Definition 32 (Quantitative operation). Let $n\\in\\mathbb N$ be a natural number. A quantitative operation $\\alpha:[0,\\infty]^{n}\\to[\\bar{0},\\infty]$ is homomorphic to a logical operation $\\beta:\\{\\top,\\bot\\}^{n}\\rightarrow\\{\\top,\\bot\\}$ via the zero predicate $\\zeta$ if $\\zeta\\,\\bar{\\circ}\\,\\alpha=\\bar{\\beta}\\circ\\zeta^{n}$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\forall(q_{1},\\ldots,q_{n})\\in[0,\\infty]^{n}.\\;(\\alpha(q_{1},\\ldots,q_{n})=0)\\leftrightarrow\\beta(q_{1}=0,\\ldots,q_{n}=0).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "A quantitative operation $\\alpha$ is subhomomorphic to a logical operation $\\beta$ via the zero predicate if $\\zeta\\circ{\\bar{\\alpha}}\\rightarrow\\beta\\circ\\zeta^{n}$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\forall(q_{1},\\ldots,q_{n})\\in[0,\\infty]^{n}.\\;(\\alpha(q_{1},\\ldots,q_{n})=0)\\to\\beta(q_{1}=0,\\ldots,q_{n}=0).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The relationship between quantitative operations and logical operations is as follows (Theorem 10): ", "page_idx": 39}, {"type": "text", "text": "Proposition 16. Let $n\\in\\mathbb{N}$ be a natural number. For $i\\in\\{1,\\ldots,n\\}$ , let $p_{i}:A\\to\\{\\top,\\bot\\}$ be $a$ predicate, and let $q_{i}:A\\rightarrow[0,\\infty]$ be a quantity. Let $p:{\\dot{A}}\\rightarrow\\{\\top,\\bot\\}^{n}:=\\langle p_{1},\\dots,p_{n}\\rangle$ be the tupling of the p redicates, and let $q^{\\stackrel{\\cdot}{\\cdot}}A\\rightarrow^{\\stackrel{\\cdot}{\\textstyle}}[0,\\infty]^{n}:=\\langle\\bar{q}_{1},\\dots,q_{n}\\rangle$ be the t upling of the quantities. Let $\\alpha:[0,\\infty]^{n}\\rightarrow[0,\\infty]$ be a quantitative operation, and let $\\beta:\\{\\top,\\bot\\}^{n}\\rightarrow\\{\\top,\\bot\\}$ be a logical operation. Assume that for all $i\\in\\{1,\\ldots,n\\}$ , $q_{i}$ is homomorphic to $p_{i}$ . Then, ", "page_idx": 39}, {"type": "text", "text": "if $\\alpha$ is homomorphic to $\\beta$ , $\\alpha\\circ q$ homomorphic to $\\beta\\circ p,$ ; and $\\mathbf{\\bar{\\rho}}$ if \u03b1 is subhomomorphic to $\\beta$ , $\\alpha\\circ q$ subhomomorphic to $\\beta\\circ p$ . ", "page_idx": 39}, {"type": "text", "text": "In fact, we can also show that if for all $i\\;\\in\\;\\{1,\\ldots,n\\}$ , $q_{i}$ is subhomomorphic to $p_{i}$ , and $\\alpha$ is subhomomorphic to $\\beta$ , then $\\alpha\\circ q$ is subhomomorphic to $\\beta\\circ p$ . ", "page_idx": 39}, {"type": "text", "text": "Definition 33 (Aggregator). An aggregator $\\alpha_{A}:[0,\\infty]^{A}\\to[0,\\infty]$ is homomorphic to a quantifier $\\beta_{A}:\\{\\top,\\bot\\}^{A}\\rightarrow\\{\\top,\\bot\\}$ via the zero predicate $\\zeta$ if $\\zeta\\circ\\alpha_{A}=\\beta_{A}\\circ\\zeta^{A}$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\forall q\\in[0,\\infty]^{A}.\\,\\left(\\l_{a\\in A}\\,q(a)\\right)=0\\right)\\leftrightarrow\\left(\\l_{a\\in A}(q(a)=0)\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Example 21. A universal aggregator $\\nabla_{A}:[0,\\infty]^{A}\\to[0,\\infty]$ is a function such that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\forall q\\in[0,\\infty]^{A}.\\left(\\binom{\\nabla}{a\\in A}\\,q(a)\\right)=0\\right)\\leftrightarrow(\\forall a\\in A.\\ q(a)=0).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Example 22. An existential aggregator $\\triangle_{A}:[0,\\infty]^{A}\\to[0,\\infty]$ is a function such that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\forall q\\in[0,\\infty]^{A}.\\left(\\big(\\underset{a\\in A}{\\Delta}q(a)\\big)=0\\right)\\leftrightarrow(\\exists a\\in A.\\ q(a)=0).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The relationship between aggregators and quantifiers is as follows (Theorem 14): ", "page_idx": 39}, {"type": "text", "text": "Proposition 17. Let $p:A\\times B\\,\\to\\,\\{\\top,\\bot\\}$ be a predicate, and let $q:A\\times B\\,\\rightarrow\\,[0,\\infty]$ be $a$ quantity. Let ${\\widehat{p}}:B\\to\\{\\top,\\bot\\}^{A}$ and $\\widehat{q}:B\\to[0,\\infty]^{A}$ be the exponential transposes of $p$ and $q$ . Let $\\alpha_{A}:[0,\\infty]^{A}\\to[0,\\infty]$ be an aggregator, and let $\\mathring{\\beta_{A}}:\\{\\top,\\bot\\}^{A}\\rightarrow\\{\\top,\\bot\\}$ be a quantifier. Then, $i f$ $q$ is homomorphic to $p_{\\mathrm{:}}$ , and $\\alpha$ is homomorphic to $\\beta$ , then $\\alpha_{A}\\circ{\\widehat{q}}$ is homomorphic to $\\beta_{A}\\circ{\\widehat{p}}$ . ", "page_idx": 39}, {"type": "text", "text": "We have a compositional way to assign a strict premetric and an aggregator to a product of sets: ", "page_idx": 39}, {"type": "text", "text": "Proposition 18. Let $d_{A}:A\\times A\\to[0,\\infty]$ and $d_{B}:B\\times B\\to[0,\\infty]$ be strict premetrics. Then, ", "page_idx": 39}, {"type": "equation", "text": "$$\nd_{A\\times B}:(A\\times B)\\times(A\\times B)\\to[0,\\infty]:=((a,b),(a^{\\prime},b^{\\prime}))\\mapsto d(a,a^{\\prime})+d(b,b^{\\prime})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "is a strict premetric on the product set $A\\times B$ . ", "page_idx": 39}, {"type": "text", "text": "Proposition 19. Let $\\nabla_{A}:[0,\\infty]^{A}\\to[0,\\infty]$ and $\\nabla_{B}:\\left[0,\\infty\\right]^{B}\\rightarrow\\left[0,\\infty\\right]$ be universal aggregators. Then, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\underset{A\\times B}{\\nabla}:[0,\\infty]^{A\\times B}\\rightarrow[0,\\infty]:=q\\mapsto\\underset{b\\in B}{\\nabla}\\underset{a\\in A}{\\nabla}q(a,b)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "is a universal aggregator on the product set $A\\times B$ . ", "page_idx": 39}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "C.1 Proposition 2 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proof. ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{\\mathrm{polace}}(m:Y\\rightarrow Z)}\\\\ {=}&{\\underset{\\{1,1\\}\\in[Y,\\bar{Z}_{1}]}{\\operatorname*{inf}}\\underset{m_{2},\\bar{\\tau}\\in\\mathcal{Y}_{2}}{\\operatorname*{inf}}z_{2}\\mathrm{d}|Y\\rangle\\mathrm{,}\\mathcal{Z}|(m,m_{1,1}\\times m_{2,2})}\\\\ {=}&{\\underset{m_{1},1\\in[Y,\\bar{Z}_{1}]}{\\operatorname*{inf}}\\underset{m_{2},\\bar{\\tau}\\in\\mathcal{Y}_{2}}{\\operatorname*{inf}}\\underset{\\bar{\\tau}\\in\\mathcal{Z}_{2}}{\\operatorname*{inf}}z_{2}(d|Y,\\bar{z}_{1}|m_{1},m_{1},\\bar{\\tau}\\rangle)+d|\\gamma_{\\bar{Z},\\bar{z}_{2}}|(m_{2},m_{2,2}\\circ p_{2})\\rangle}\\\\ &{=\\underset{m_{1},1\\in[Y,\\bar{Z}_{1}]}{\\operatorname*{inf}}\\underset{\\bar{\\tau}\\in\\mathcal{Z}_{1}}{\\operatorname*{inf}}2|\\gamma_{Z_{2}\\cap\\left(m_{1},m_{1},\\Omega_{1}\\right)}+\\underset{m_{2}\\in[Y,\\bar{Z}_{2}]}{\\operatorname*{inf}}2|\\gamma_{Z_{2}\\cap\\left(m_{2},m_{2},2\\circ p_{2}\\right)}}\\\\ {=}&{\\underset{m_{1},1\\in[Y,\\bar{Z}_{1}]}{\\operatorname*{inf}}\\underset{y\\in\\mathcal{Y}_{2}}{\\operatorname*{inf}}\\underset{y\\in\\mathcal{Y}_{2}}{\\operatorname*{inf}}(m_{1}(m_{1}),m_{1}(y_{1}))+\\underset{m_{2}\\in[Y,\\bar{Z}_{2}]}{\\operatorname*{inf}}\\underset{y\\in\\mathcal{Y}_{2}}{\\operatorname*{inf}}z_{2}|y\\mathcal{I}_{2}\\,d_{2}(m_{2}(y),m_{2}(y_{2}))}\\\\ {=}&{\\underset{m_{1},1\\in[Y,\\bar{Z}_{1}]}{\\operatorname*{inf}}\\underset{y\\in\\mathcal{Y}_{2}}{\\operatorname*{inf}}\\underset{y\\in\\mathcal{Y}_{2}}{\\operatorname*{inf}}2|\\gamma_{1}(m_{1}(y_{1},y_{2}),m_{1,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "$\\begin{array}{r l}&{m_{1,1}^{*}:=\\underset{m_{1,1}\\in[Y_{1},Z_{1}]}{\\arg\\operatorname*{inf}}\\ \\underset{y_{1}\\in Y_{1}}{\\nabla}\\underset{y_{2}\\in Y_{2}}{\\nabla}d_{Z_{1}}\\big(m_{1}(y_{1},y_{2}),m_{1,1}(y_{1})\\big)}\\\\ &{\\qquad=y_{1}\\mapsto\\underset{z_{1}\\in Z_{1}}{\\arg\\operatorname*{inf}}\\ \\underset{y_{2}\\in Y_{2}}{\\nabla}d_{Z_{1}}\\big(m_{1}(y_{1},y_{2}),z_{1}\\big),}\\\\ &{m_{2,2}^{*}:=\\underset{m_{2,2}\\in[Y_{2},Z_{2}]}{\\arg\\operatorname*{inf}}\\ \\underset{y_{2}\\in Y_{2}}{\\nabla}\\ \\underset{y_{1}\\in Y_{1}}{\\nabla}d_{Z_{2}}\\big(m_{2}(y_{1},y_{2}),m_{2,2}(y_{2})\\big)}\\\\ &{\\qquad=y_{2}\\mapsto\\underset{z_{2}\\in Z_{2}}{\\arg\\operatorname*{inf}}\\ \\underset{y_{1}\\in Y_{1}}{\\nabla}d_{Z_{2}}\\big(m_{2}(y_{1},y_{2}),z_{2}\\big).}\\end{array}$ (158) (159) (160) (161) ", "page_idx": 40}, {"type": "text", "text": "C.2 Proposition 3 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad q_{\\mathrm{coast\\,scal}}(m;Y\\rightarrow Z)}\\\\ &{=q_{\\mathrm{coast}(\\overline{{m}}_{1})}+q_{\\mathrm{coast}(\\overline{{m}}_{2})}}\\\\ &{=\\frac{\\sqrt{\\gamma}}{y_{1}\\cdots y_{1}^{\\sqrt{6}}}q_{1}\\gamma_{1,z_{1}}(\\overline{{m}}_{1}(y_{2}),\\overline{{m}}_{1}(y_{2}^{\\prime}))+\\underbrace{\\nabla}_{y_{1}\\cdots y_{1}^{\\sqrt{6}}}\\underbrace{\\eta}_{1\\vee(Y_{1})}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "D Discussions ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "D.1 Background ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Defining and measuring the properties of learning models is a core topic in machine learning, especially representation learning [Bengio et al., 2013]. A proper comprehension of what constitutes good representations and how to assess their quality is important for developing suitable learning objectives and evaluation metrics. To define these properties, many important concepts are given by equational predicates, such as independence of random variables, extensively used in statistical learning and causal learning [Hyv\u00e4rinen and Oja, 2000, Koller and Friedman, 2009, Sch\u00f6lkopf and von K\u00fcgelgen, 2022], and equivariance of learning models, reflecting the symmetries and structures of the data [Cohen and Welling, 2016, Zaheer et al., 2017, Higgins et al., 2018, Maron et al., 2019, de Haan et al., 2020, Cohen, 2021, van der Pol et al., 2022, Navon et al., 2023]. ", "page_idx": 41}, {"type": "text", "text": "Considerable efforts have been put into designing model architectures that perfectly satisfy specific properties, such as monotonicity [Sill, 1997, Daniels and Velikova, 2010], invertibility [Rezende and Mohamed, 2015, Behrmann et al., 2019, Ishikawa et al., 2023], convexity [Amos et al., 2017], and equivariance [Lee et al., 2019, Brehmer et al., 2023]. However, hard-coding multiple properties into a model by design could be challenging [K\u00f6hler et al., 2020]. Hence, it is desirable to devise quantitative metrics to directly measure these properties, even if the models do not have the properties built-in [Goodfellow et al., 2009, Chen et al., 2020, Kvinge et al., 2022]. Ideally, these metrics should be easily computable or even differentiable, allowing us to directly optimize the properties. ", "page_idx": 41}, {"type": "text", "text": "Disentangled representation learning [Bengio et al., 2013], our main focus of this paper, is such a field where defining and measuring the desired properties are not straightforward tasks [Carbonneau et al., 2022, Zhang and Sugiyama, 2023]. It has been suggested that disentangling the underlying explanatory factors in complex data is a promising approach for reliable, interpretable, generalizable, and data-efficient representation learning [Locatello et al., 2019a,b, Montero et al., 2021, Dittadi et al., 2021, Xu et al., 2022]. However, in contrast to the wealth of results regarding invariant and equivariant layers, the exploration of designing a \u201cdisentangled layer\u201d has been relatively limited. One reason is that disentanglement was not considered a singular property but rather a combination of several requirements. The absence of a clear definition and appropriate metrics for disentanglement has created a gap between the learning objectives and evaluation metrics. A new evaluation metric is often introduced along with a new representation learning method [Carbonneau et al., 2022], but it is usually unproven that the method can optimize the new metric, and the metric truly quantifies the alleged property [Higgins et al., 2017, Kim and Mnih, 2018, Chen et al., 2018, Li et al., 2020]. ", "page_idx": 41}, {"type": "text", "text": "To formally define disentanglement, a line of research utilized group theory and representation theory [Cohen and Welling, 2014, 2015, Higgins et al., 2018], with a focus on the direct product of groups. Thanks to the rich algebraic structure, it becomes possible to derive various model architectures and learning objectives from the equational requirements of the product and equivariance [Caselles-Dupr\u00e9 et al., 2019, Pfau et al., 2020, Quessard et al., 2020, Painter et al., 2020, Miyato et al., 2022, Yang et al., 2022, Tonnaer et al., 2022, Keurti et al., 2023]. Another approach adopted a topological perspective, using concepts such as the product manifold to define disentanglement [Zhou et al., 2020, Fumero et al., 2021, Zhang et al., 2021, Balabin et al., 2024]. However, theoretically comparing different approaches has been a challenging task. ", "page_idx": 41}, {"type": "text", "text": "To quantitatively measure disentanglement, Ridgeway and Mozer [2018] proposed three concepts called modularity, compactness, and explicitness, which were defined verbally but not mathematically. Eastwood and Williams [2018] proposed similar three criteria called disentanglement, completeness, and informativeness and corresponding evaluation metrics. However, it was unclear what properties these metrics truly quantify. Additionally, due to the necessity for additional training of classifiers along with hyperparameter tuning and the involvement of non-differentiable regressors such as the random forest [Breiman et al., 1984], it is impossible to directly optimize these metrics using gradient-based optimization. Recently, Eastwood et al. [2023] extended this framework with two new metrics called explicitness/ease-of-use and size based on the functional capacity. Do and Tran [2020] introduced metrics for informativeness, separability, independence, and interpretability from an information-theoretic perspective, while Tokui and Sato [2022] introduced a new metric in terms of uniqueness, redundancy, and synergy based on partial information decomposition. These metrics have been mainly used during the evaluation stage, after a model is trained with other learning objectives. ", "page_idx": 41}, {"type": "text", "text": "D.2 Related work ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Equivariance The work by Kvinge et al. [2022] might be the closest to our approach in spirit. They directly converted equivariance, an equational predicate, to a quantitative metric and analyzed their relationship (Proposition 3.2). In contrast, based on our proposed conversion method, we can use the following definition and metric: ", "page_idx": 42}, {"type": "text", "text": "Definition 34 (Equivariant function). Let $A,B$ , and $C$ be sets. A function $f:A\\rightarrow B$ is equivariant to actions (any binary functions) ${\\mathbf{\\Omega}}_{A}:C\\times A\\to A$ and ${\\bf\\mu}_{B}:C\\times B\\to B$ if ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{p_{\\mathrm{equivariant}}(f:A\\to B):=\\forall c\\in C.\\ f\\circ(c\\,\\cdot\\,_{A}-)=_{[A,B]}(c\\,\\cdot\\,_{B}-)\\circ f}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\forall c\\in C.\\ \\forall a\\in A.\\ f(c\\,\\cdot\\,_{A}\\,a)=_{B}c\\,\\cdot\\,_{B}f(a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which can be measured by ", "page_idx": 42}, {"type": "equation", "text": "$$\nq_{\\mathrm{equivariant}}(f:A\\to B):=\\underset{c\\in C}{\\nabla}\\underset{a\\in A}{\\nabla}d_{B}(f(c\\cdot_{A}a),c\\cdot_{B}f(a)).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Calibration More broadly, the study of the relationship between different metrics in statistical learning is called calibration analysis [Steinwart, 2007, Reid and Williamson, 2010, Ni et al., 2019, Bao and Sugiyama, 2020, Bao et al., 2020]. Our work can be seen as an extension of the concept of the calibration to a wider range of properties defined by equational predicates. ", "page_idx": 42}, {"type": "text", "text": "Disentanglement metric In disentangled representation learning, metrics similar to Eq. (26) have been proposed by Higgins et al. [2017], Kim and Mnih [2018]. Their metrics also fix one factor and vary all others and calculate some constancy metrics (the mean pairwise distance in Higgins et al. [2017] and the variance in Kim and Mnih [2018]). However, both studies took an indirect approach, involving the training of a classifier to predict the fixed factor. Consequently, the resulting metrics are not differentiable anymore and entangle modularity and informativeness. In this work, we argue that it is better to measure these two properties separately. ", "page_idx": 42}, {"type": "text", "text": "Weakly supervised disentanglement Ridgeway and Mozer [2018] proposed and investigated the similarity supervision and argued that such supervision is easy to obtain via crowdsourcing. Shu et al. [2020] further studied this type of supervision based on distribution matching and referred it as match pairing. Other weaker forms of supervision were also investigated, such as the number of changed factors [Locatello et al., 2020] or paired data with unknown intervention [Brehmer et al., 2022]. Given that our theory can establish connections between logical definitions and quantitative metrics, it holds promise for deriving disentanglement metrics for various types of weak supervision based on logical inference. ", "page_idx": 42}, {"type": "text", "text": "Multi-valued logic Aristotelian logic assumes that every proposition is either true or false, adhering to the principle of bivalence. The law of Aristotelian logic can be algebraically represented on the set $\\{0,1\\}$ of binary truth values [Boole, 1854], known as the two-element Boolean algebra. The exploration of non-Aristotelian logic involves investigating logical systems that relax or modify this strict binary valuation, allowing for a broader range of truth values and accommodating various forms of uncertainty, vagueness, or context-dependence in reasoning [H\u00e1jek, 1998, Malinowski, 2007, Bergmann, 2008]. ", "page_idx": 42}, {"type": "text", "text": "The mathematical study of multi-valued logic can date back to the seminal work by \u0141ukasiewicz in 1920, who introduced a third truth value interpreted as \u201cpossibility\u201d and symbolized by $\\frac{1}{2}$ . \u0141ukasiewicz [1920] examined several principles in this three-valued logic such as the principles of identity, implication, syllogism, and contradiction, and discussed its theoretical and practical importance in indeterministic philosophy and deductive sciences. Later, \u0141ukasiewicz and Tarski [1930] proposed propositional calculus, a theory of propositions with values from the real interval $[0,1]$ , which is now also commonly known as \u0141ukasiewicz logic. \u0141ukasiewicz logic involves new continuous logical connectives such as strong/weak conjunction and disjunction. ", "page_idx": 42}, {"type": "text", "text": "Furthermore, Chang [1958] studied the algebraic systems for many-valued logic, called MV-algebras. Chang and Keisler [1966] then proposed continuous model theory, also referred to as compact-valued logic [Ben Yaacov, 2022], where the truth values can be in arbitrary compact Hausdorff spaces and a wide variety of quantifiers was studied. Later, Ben Yaacov et al. [2008] studied model theory for metric structures and proposed (real-valued) continuous first-order logic [Ben Yaacov and Usvyatsov, ", "page_idx": 42}, {"type": "text", "text": "2010], where the space of truth values is a closed, bounded interval of real numbers with the order topology (e.g., $[0,1]\\rangle$ , and suggested that we only need two canonical quantifiers sup and inf. From a categorical perspective, Cho [2020] developed categorical semantics of metric spaces and continuous logic by introducing the notion of continuous subobject classifier, and Figueroa and van den Berg [2022] studied a topos of continuous logic using the notion of hyperdoctrine. ", "page_idx": 43}, {"type": "text", "text": "On the other hand, from a categorical perspective, Lawvere [1973] showed that a generalized metric space, also known as a Lawvere metric space, is a category enriched over what is now commonly called the Lawvere quantale $([0,\\infty],\\geq,+,0)$ , i.e., the set $[0,\\infty]$ of extended non-negative real numbers equipped with addition $^+$ as a (semicartesian) monoidal product and truncated subtraction $\\doteq$ as the internal hom. In other words, a Lawvere metric space is a set $A$ equipped with a function $d:A\\times A\\to[0,\\infty]$ such that for all $a\\in A$ , we have $0\\geq\\bar{d}(a,a)$ or $d(a,a)=0$ (identity), which makes $d$ a premetric, and for all $a,b,c\\in A$ , we have $d(b,c)+d(a,b)\\geq d(a,c)$ (composition), which means that $d$ satisfies the triangle inequality. ", "page_idx": 43}, {"type": "text", "text": "Recently, Mardare et al. [2016] took an equational approach to quantitative algebraic reasoning, which was later also referred to as quantitative equational logic [Mardare et al., 2021], by introducing approximate equality predicates $=_{\\varepsilon}$ indexed by rational numbers $\\varepsilon$ (i.e., $a=_{\\varepsilon}b$ if $a$ and $b$ are at most $\\varepsilon$ apart), and suggested that this approach essentially involves working with enriched Lawvere theory. Dagnino and Pasquali [2022] provided a logical ground to quantitative reasoning in the categorical language of Lawvere\u2019s doctrines by viewing distances as equality predicates in linear logic. Bacci et al. [2023] further studied the natural deduction systems of propositional logics for the Lawvere quantale and introduced what was later called affine Lawvere logic, including \u0141ukasiewicz logic and Ben Yaacov\u2019s continuous propositional logic. Bacci et al. [2024] extended affine Lawvere logic to polynomial Lawvere logic by allowing multiplication as an extra logical connective. These studies are on propositional logic and do not involve predicates and quantifiers. Recently, Capucci [2024] studied a spectrum of quantifiers in $[0,\\infty]$ -valued quantitative predicate logic. ", "page_idx": 43}, {"type": "text", "text": "In the context of machine learning, these relatively recently developed logics have yet to prove their practical importance. While these innovative approaches often hold theoretical promise, they need to demonstrate tangible benefits in real-world applications. Key areas where these new logics might eventually make an impact include neuro-symbolic reasoning and logic/probabilistic programming [d\u2019Avila Garcez et al., 2002, Manhaeve et al., 2018, Sen et al., 2022, Badreddine et al., 2022, Fagin et al., 2024], which hold promise for integrating low-level perception with high-level reasoning, improving model interpretability, enhancing training efficiency, and enabling more robust decisionmaking processes. However, widespread adoption and validation through practical use cases are necessary to establish their true value and effectiveness in the machine learning landscape. ", "page_idx": 43}, {"type": "text", "text": "Under this background, let us contextualize our proposed methodology for deriving $[0,\\infty]$ -valued quantitative metrics from logical definitions. We highlight three characteristics of our framework: ", "page_idx": 43}, {"type": "text", "text": "We allowed not only metrics, but strict premetrics, such as the relative entropy (Kullback\u2013Leibler divergence) [Kullback and Leibler, 1951, Perrone, 2023] widely used in machine learning, as the real-valued counterparts for the equality predicates;   \nWe focused on whether the metrics are zero or not and the (differentiable) optimization of the derived metrics, because our main goal is to guarantee that the minimizers of the derived metrics satisfy the predicate;   \nWe included a wide range of real-valued quantifiers (or aggregators in our terms) beyond sup and inf, such as mean and mean square, as long as they are homomorphic to the two-valued quantifiers, because the derived metrics may have nicer properties or even analytical solutions. ", "page_idx": 43}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/3e83ebc7af7d98711a0d0a27f7f96942b815443aa0fbe29255bf6658457bbbd2.jpg", "img_caption": ["Figure 8: Quantitative operations for implication "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/81f77788bde4b23072bddd93cff77075048735a46b55ad820caab9664fe71405.jpg", "img_caption": ["Figure 9: Quantitative operations for equivalence "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "D.3 Implication and equivalence ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In Section 3, we only used the truncated subtraction \u00b4 as a quantitative operation for the implication $\\rightarrow$ (Table 1). In Theorem 1, we noted that the implication is special because if a predicate involves the implication, then not all elements satisfying the predicate minimize the corresponding quantities (Fig. 2). In Appendix B.8, we discussed other possible quantitative operations $\\multimap$ corresponding to the implication $\\rightarrow$ . ", "page_idx": 44}, {"type": "text", "text": "In Fig. 8, we showed eight alternative quantitative operations for the implication. In the first row, the first one is homomorphic to the implication, which means that its minimizers are exactly those that satisfy the predicate (Example 16); the second one is right adjoint to the max (Example 17); and the other two are variants of the truncated subtraction (Example 18), which have more or less coverage. In the second row, we used the logical equivalence between $a\\rightarrow b$ and $\\neg a\\lor b$ to define quantitative operations for the implication using quantitative operations for the negation and disjunction. For example, we can use the hinge function $1\\div n$ and min, which lead to $\\operatorname*{min}\\{1\\to a,b\\}$ , or the reciprocal function n1 andaa+bb, which lead to 1 $\\begin{array}{r}{\\frac{\\frac{1}{a}b}{\\frac{1}{a}+b}=\\frac{b}{1+a b}}\\end{array}$ . ", "page_idx": 44}, {"type": "text", "text": "Similarly, we can use logically equivalent expressions of the logical equivalence $a\\leftrightarrow b$ , such as $(a\\to b)\\land(b\\to a)$ (bi-implication), $(\\neg a\\lor b)^{\\overline{{\\prime}}}\\land(\\neg b\\lor a)$ (conjunctive normal form (CNF)), and $(a\\wedge b)\\vee(\\neg a\\wedge\\neg b)$ (disjunctive normal form (DNF)), to derive quantitative operations for the equivalence, shown in Fig. 9. ", "page_idx": 44}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/8d4dabcdbbbbbdefe1d2d726544d4f835312643a7090c3c95c03fc539c768ca2.jpg", "img_caption": ["(a) central point "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/fcb8caf7bb64c74e31c1c642fd3fe83e57da780bf67cc72fdc0f8db5e029d00c.jpg", "img_caption": ["Figure 10: Two approaches for measuring the constancy of a set in $\\mathbb{R}^{2}$ : (a) finding a central point, such as the center of the smallest bounding sphere, the geometric median, or the mean, and then measuring the dispersion around this point; and (b) aggregating pairwise distances between points. ", "(b) pairwise distance "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Note that a quantitative operation homomorphic to the implication cannot be continuous everywhere, which is undesirable for gradient-based optimization. For example, the following quantity also measures the injectivity of a function $m:Y\\rightarrow Z$ : ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{y\\in Y}\\underset{y^{\\prime}\\in Y}{\\nabla}[d_{Z}(m(y),m(y^{\\prime}))=0]\\times[d_{Y}(y,y^{\\prime})>0]\\times d_{Y}(y,y^{\\prime})}\\\\ &{=\\underset{y\\in Y}{\\nabla}\\underset{y^{\\prime}\\in Y}{\\nabla}[m(y)=_{Z}m(y^{\\prime})]\\times[y\\neq_{Y}y^{\\prime}]\\times d_{Y}(y,y^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This quantity aggregates distances between pairs of different inputs mapped to the same outputs. However, unlike $q_{\\mathrm{injective}}$ introduced in Section 4.3, it is not differentiable with respect to the function $m:Y\\rightarrow Z$ . Thus, we cannot use it to improve the injectivity of a function by gradient descent. ", "page_idx": 45}, {"type": "text", "text": "D.4 Constant function ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Note that there are two logically equivalent definitions of a constant function (to a non-empty set). One is based on the equality between all pairs, as in Definition 11. The other is based on the constant output value (see Fig. 10): ", "page_idx": 45}, {"type": "text", "text": "Definition 35 (Constant function with value). A function $f:A\\rightarrow B$ is a constant function with value $b\\in B$ if ", "page_idx": 45}, {"type": "equation", "text": "$$\np_{\\mathrm{const-v}}(f:A\\to B):=\\exists b\\in B.\\;\\forall a\\in A.\\;(f(a)=_{B}b),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which can be measured by ", "page_idx": 45}, {"type": "equation", "text": "$$\nq_{\\mathrm{const-v}}(f:A\\to B):=\\operatorname*{inf}_{b\\in B}\\nabla_{a\\in A}d_{B}(f(a),b).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This quantity $q_{\\mathrm{const-v}}$ finds a central point in the codomain that best approximates all the outputs of a function, which is similar to the approach we discussed in Section 4.1. In fact, we can prove that if we use $q_{\\mathrm{const-v}}$ in $q_{\\mathrm{const-curry}}$ , we will end up with the same quantity $q_{\\mathrm{product}}$ . ", "page_idx": 45}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/66fa20532806250b80cdfa950d8ccda6522e43dd69b59d2f744cde01ef09518e.jpg", "img_caption": ["Figure 11: Metrics may rank imperfect representations differently. "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/745423e2bb7f4d35f8e1dcbbc17c236300e86577bf02d77718392c87f0c768d6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "Figure 12: For a pair of constancy metrics (each column), we can find two sets of points in $\\mathbb{R}^{2}$ ranked differently by these metrics, except for the radius and diameter, because for a subset $A_{0}$ in a set $A$ , we have $\\begin{array}{r}{\\operatorname*{inf}_{a_{0}\\in A}\\operatorname*{sup}_{a\\in A_{0}}d_{A}(a_{0},a)\\leq\\operatorname*{inf}_{a_{0}\\in A_{0}}\\operatorname*{sup}_{a\\in A_{0}}d_{A}(a_{0},a)\\leq\\operatorname*{sup}_{a_{0}\\in A_{0}}\\operatorname*{sup}_{a\\in A_{0}}d_{A}(a_{0},a).}\\end{array}$ $\\operatorname*{sup}_{a\\in A_{0}}d_{A}(a_{0},a)$ . ", "page_idx": 46}, {"type": "text", "text": "D.5 Rank of imperfect representations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "It is worth noting that Theorem 1 only guarantees that the minimizers of different quantitative metrics derived from the same logical definition are the same, but imperfect representations, whose evaluation results are non-zero, may be ranked differently by different metrics. ", "page_idx": 46}, {"type": "text", "text": "For example, Fig. 11 illustrates two constancy metrics, the radius of the smallest bounding sphere and the variance, on two sets of points in $\\mathbb{R}^{2}$ , where one set has a small radius but a large variance, while the other has a large radius but a small variance. More examples are presented in Fig. 12, and such results can also be observed in Table 2. This difference can lead to differences in risk preferences, sensitivity to outliers, and learning dynamics when these metrics are used as learning objectives. Further investigation of the characteristics of these metrics for imperfect representations is left for future work. ", "page_idx": 46}, {"type": "text", "text": "D.6 Implementation ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Thanks to advanced indexing (e.g., NumPy [Harris et al., 2020] and PyTorch [Paszke et al., 2019]) and analytical solutions to some optimization problems (e.g., Eq. (19)), some of the proposed metrics can be easily implemented, even as Python one-liners. ", "page_idx": 47}, {"type": "text", "text": "For example, the following function implements a family of modularity metrics: ", "page_idx": 47}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/65b262747840d4d61ce02fa8c64fd47e3f3eb77e604f243a2464b4219fa97b98.jpg", "table_caption": [], "table_footnote": [], "page_idx": 47}, {"type": "text", "text": "Here, y and z are NumPy arrays of shape (factor, index); aggregate can be max, mean, or sum; deviation can be a function calculating the radius of the smallest bounding sphere,12 mean absolute deviation around the geometric median,13variance, diameter, or mean pairwise distance. Please note, however, that the deviation function can be computationally expensive, depending on the dimension of the codes. ", "page_idx": 47}, {"type": "text", "text": "D.7 Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Lastly, we discuss several aspects that are not covered in this work and potential directions for future research. ", "page_idx": 47}, {"type": "text", "text": "Function equality A collection of input-out pairs $\\{(x_{i},y_{i})\\}_{i=1}^{n}\\in(X\\times Y)^{n}$ may not define a function $g\\,:\\,X\\,\\rightarrow\\,Y$ for two reasons: First, the set of all inputs $X_{0}:=\\{x_{i}\\}_{i=0}^{n}$ is unlikely to enumerate all possible inputs (i.e., $X_{0}\\subsetneq X;$ , especially when the cardinality of the domain $X$ is infinite (e.g., $\\mathbb{R}$ ), so the data may only define a partial function $g:X\\to Y$ or a function from a smaller domain $g_{0}:X_{0}\\rightarrow Y$ . Second, the inputs may not be distinct, e.g., when an input is given multiple labels by different annotators, so the data may define a multi-valued function. The extension from functions to relations or stochastic maps is an important future direction of our work. ", "page_idx": 47}, {"type": "text", "text": "Partial combinations A more general issue is learning and evaluating disentangled representations given only a subset of all combinations of factors, which is common when dealing with a large number of factors [Tr\u00e4uble et al., 2021, Montero et al., 2021, 2022, Roth et al., 2023]. It is crucial to evaluate and justify whether a metric computed on partial combinations of factors is a reliable proxy for the performance of the model on unseen combinations. ", "page_idx": 47}, {"type": "text", "text": "Unknown projections Another common scenario is when the extracted representation is not properly aligned with the underlying factors. For example, a model may extract a three-dimensional representation $z\\in\\mathbb{R}^{3}$ for two factors $y\\in[0,1]^{2}$ , and it can project to $((z_{1},z_{2}),z_{3})$ or $(z_{1},(z_{2},z_{3}))$ . How can we determine which is better, without enumerating all possible projections? Finding the optimal assignment [Mahon et al., 2023] and correcting a pre-trained model post hoc [Tr\u00e4uble et al., 2021] based on the proposed metrics are interesting future directions. ", "page_idx": 47}, {"type": "text", "text": "D.8 Broader impact ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "This paper focuses on the theoretical aspects of disentangled representation learning, and we do not foresee any immediate negative societal consequences. However, we would acknowledge that disentanglement is closely related to data-efficiency and fairness, potentially sparking discussions on ethical considerations. Besides, the application of category theory may facilitate the transfer and integration of knowledge across disciplines, fostering closer connections between various fields of study, even beyond the machine learning community. ", "page_idx": 47}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/4940347672fca5703d5b115c83ead896cdcdd30475b44125d87caf45de7f84ab.jpg", "img_caption": ["Figure 13: (a) a set of factors $Y$ represented by the RGB color model; (b) a set of entangled codes $Z$ extracted by an encoder $m:Y\\rightarrow Z$ ; (c) a product function approximation; and (d) a linear approximation of the retraction $h:Z\\to Y$ of the encoder. "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "tvQ3XCKWbB/tmp/556a98f89b136ce6f3aa4c557c9a29cf9e4d8f158228f17c26682c3bc6b659be.jpg", "img_caption": [], "img_footnote": [], "page_idx": 48}, {"type": "text", "text": "Figure 14: Entangling a multivariate distribution via probability integral transform, inverse transform, and orthogonal transformation of standard normal distribution [Locatello et al., 2019b, Theorem 1]. ", "page_idx": 48}, {"type": "text", "text": "E Experiments ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "In this section, we provide the detailed data configuration used in Section 5 and further experimental results. ", "page_idx": 48}, {"type": "text", "text": "E.1 Synthetic data ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We used a simple synthetic setup to simulate entanglement of factors and common failures patterns. Concretely, we used a Cartesian product $Y:=\\{0,0.1,\\ldots,1\\}^{3}$ of three sets as the underlying factors (Fig. 13a). We used a random rotation matrix $R$ to entangle factors and componentwise exponential as a non-linear transformation. We composited this procedure twice and used an affine transformation to normalize the outputs (Fig. 13b). That is, we used the following function as the data generating process: ", "page_idx": 48}, {"type": "equation", "text": "$$\ng:Y\\rightarrow X:y\\mapsto a\\cdot\\exp(R\\cdot\\exp(R\\cdot y))+b.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Note that this function is injective but not a product or linear. ", "page_idx": 48}, {"type": "text", "text": "We used the following functions as the function $m:Y\\rightarrow Z$ : ", "page_idx": 48}, {"type": "text", "text": "entanglement $\\begin{array}{r c l}{(y_{1},y_{2},y_{3})}&{\\mapsto}&{g(y_{1},y_{2},y_{3})}\\\\ {(y_{1},y_{2},y_{3})}&{\\mapsto}&{R\\cdot(y_{1},y_{2},y_{3})}\\\\ {(y_{1},y_{2},y_{3})}&{\\mapsto}&{((y_{1},y_{2},y_{3}),(y_{1},y_{2},y_{3}),y_{3})}\\\\ {(y_{1},y_{2},y_{3})}&{\\mapsto}&{((y_{2},y_{3}),(y_{1},y_{3}),(y_{1},y_{2}))}\\\\ {(y_{1},y_{2},y_{3})}&{\\mapsto}&{(y_{2},y_{3},y_{1})}\\\\ {(y_{1},y_{2},y_{3})}&{\\mapsto}&{((y_{1},-y_{1}),y_{2},y_{3})}\\\\ {(y_{1},y_{2},y_{3})}&{\\mapsto}&{0.01\\times(y_{1},y_{2},y_{3})}\\\\ {(y_{1},y_{2},y_{3})}&{\\mapsto}&{(y_{1}^{2},y_{2}^{2},y_{3}^{2})}\\\\ {(y_{1},y_{2},y_{3})}&{\\mapsto}&{(0,0,0)}\\end{array}$   \nrotation   \nduplicate   \ncomplement   \nmisalignment   \nredundancy   \ncontraction   \nnonlinear   \nconstant ", "page_idx": 48}, {"type": "text", "text": "The rotation operation entangles factors but can be (linearly) inverted. The duplicate encoder has a modular decoder (projections), but itself is not modular. The redundancy encoder is both modular and informative, but not all codes can be decoded. The constant encoder is perfectly modular but not informative. ", "page_idx": 48}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/4af3b615de4f16781c51f746fe94e5218e9e2c2b8971cbc8947c7c2d4577e4aa.jpg", "table_caption": ["Table 3: Supervised modularity metrics "], "table_footnote": [], "page_idx": 49}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/66535614b937e5913c104f3dc32868b860178a448c96ac577a62ea70afa7d0a6.jpg", "table_caption": ["Table 4: Weakly supervised modularity metrics "], "table_footnote": [], "page_idx": 49}, {"type": "text", "text": "E.2 Weakly supervised modularity metrics ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "We briefly comment on the possibility of employing weak supervision for measuring disentangled representations. ", "page_idx": 49}, {"type": "text", "text": "For supervised disentanglement metrics we discussed in Section 4, the necessary data consists of observation-factor pairs $(x,y)$ , representing a generator $g:Y\\rightarrow X$ . In order to evaluate an encoder $f:X\\to Z$ , we compose it with a generator and study the properties of the composition m : Y \u2192Z := f \u25e6g. ", "page_idx": 49}, {"type": "text", "text": "It is worth noting that $p_{\\mathrm{product}}$ and $p_{\\mathrm{injective}}$ are equational predicates, which means that they are invariant to bijections. Similarly, $q_{\\mathrm{product}}$ and $q_{\\mathrm{injective}}$ are invariant to isometries. This implies that the exact values of the factors are not important; we only need to know if two factors are equal or not. Hence, we only need weak supervision of the form $(\\dot{x},x^{\\prime},y_{i}=_{Y_{i}}y_{i}^{\\prime})$ so that we can construct some equivalence classes of factors, and we can still calculate or approximate Eqs. (19) and (27). Note that this type of supervision has been partially investigated by Ridgeway and Mozer [2018], Shu et al. [2020]. ", "page_idx": 49}, {"type": "text", "text": "For example, suppose we have an object A (e.g., red circle) and an object B (e.g., red triangle), and all we know is that objects A and B have the same color. Based on such weak information, we can still construct an equivalence class containing objects with the same color as object A. Then, we can regularize an encoder $f:X\\to Z$ by minimizing the variance of the color representations over this equivalence class (Eq. (19)). In this way, the modularity of the encoder can be improved. The challenge arises when there is noise or only partial combinations, which is an interesting future work direction. In such cases, we may need to use semi-supervised clustering to group the data [Wagstaff et al., 2001, Basu et al., 2002, Bilenko et al., 2004]. ", "page_idx": 49}, {"type": "text", "text": "To validate this idea, we conducted experiments where we only used a random sample of pairs and their similarities. Table 3 is an excerpt of Table 2, showing only the proposed modularity metrics, and Table 4 shows these metrics calculated using only similarity supervision. We reported the mean values of 10 random samples of pairs, and the variances are negligible. Comparing Tables 3 and 4, we can observe that weakly supervised metrics may overestimate imperfect representations, but they can still maintain the ranks. This observation suggests the potential utility of employing weak supervision for both learning and evaluating disentangled representations using the proposed metrics. ", "page_idx": 49}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/ff28b4514103c02579f214402cff0975f00a61f2284482545d028750ffabb012.jpg", "table_caption": ["Table 5: Supervised disentanglement metrics on image datasets "], "table_footnote": ["a [Higgins et al., 2017] b [Kim and Mnih, 2018] c [Chen et al., 2018] d [Eastwood and Williams, 2018] "], "page_idx": 50}, {"type": "text", "text": "E.3 Evaluation of existing models on image datasets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We also report the results of several widely used unsupervised disentangled representation learning methods (VAE [Kingma and Welling, 2014], $\\beta$ -VAE [Higgins et al., 2017], FactorVAE [Kim and Mnih, 2018], and $\\beta$ -TCVAE [Chen et al., 2018]) evaluated on four image datasets (3D Cars [Reed et al., 2015], dSprites [Matthey et al., 2017], 3D Shapes [Burgess and Kim, 2018], and MPI3D [Gondal et al., 2019]) in Table 5. ", "page_idx": 50}, {"type": "text", "text": "We used a public PyTorch implementation [Paszke et al., 2019] of these methods and used the same encoder/decoder architecture with the default hyperparameters described in Locatello et al. [2019b] for all methods for a fair comparison. We used linear projection to find the most informative representations for each factor. The experiments were conducted on a NVIDIA Tesla V100 GPU. ", "page_idx": 50}, {"type": "text", "text": "Before analyzing these results, it is important to note that the evaluation of these learning models is not meant to be a proof of the correctness of the proposed metrics, since we cannot tell whether a bad result is due to the insufficiency of a learning method, to the quality of the datasets, or to the problem of the evaluation, if we have no theoretical guarantee for the metrics. We can trust the results of the proposed metrics because the properties of their minimizers are guaranteed by Theorem 1. ", "page_idx": 50}, {"type": "text", "text": "From Table 5 we can observe that the considered learning methods do not exhibit significant difference in terms of modularity and informativeness. This result supports the theoretical finding of Locatello et al. [2019b] that unsupervised learning of disentangled representations by matching the distributions of observations is fundamentally impossible (see also Fig. 14) as well as their empirical finding that there is no evidence that learning disentangled representations in an unsupervised manner is reliable. ", "page_idx": 50}, {"type": "text", "text": "E.4 Kendall tau distance between metrics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "To analyze the relationship between these metrics, we report the Kendall tau distance [Kendall, 1938, Virtanen et al., 2020] averaged over experimental settings in Table 6. The Kendall tau distance is a correlation measure for ordinal data valued in $[-1,1]$ which counts the number of pairwise disagreements between two ranking lists. Values close to 1 indicate strong agreement, and values close to $-1$ indicate strong disagreement. ", "page_idx": 50}, {"type": "text", "text": "From Table 6 we can observe that even though different metrics derived from the same logical definition may rank imperfect representations differently (see also Fig. 12), they still have positive correlations with each other, indicating that they measure the same property. The metrics proposed by Higgins et al. [2017] and Kim and Mnih [2018] have the highest correlations with each other (except for themselves), and we hypothesize that this is because they are both based on the pairwise distance approach. The DCI disentanglement metric [Eastwood and Williams, 2018] weakly agrees with the modularity metrics. However, the DCI informativeness metric [Eastwood and Williams, 2018] weakly disagrees with the informativeness metrics. It is possible that this is because of the different regressors (sklearn.ensemble.GradientBoostingClassifier, sklearn.linear_model.LinearRegression, and sklearn .linear_model.QuantileRegressor [Pedregosa et al., 2011]) used in predicting factors from codes, showing that random seeds and hyperparameters of the metrics may matter more than the models when additional predictors need to be trained to evaluate the learning methods. This result indicates the advantage of qinjective over qretractable. ", "page_idx": 50}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/8f78e6fca688069d95dc2a82d8ce3e5123810701871f06a2052a699c28ce4ef0.jpg", "table_caption": ["Table 6: Average Kendall tau rank distances bewteen disentanglement metrics "], "table_footnote": ["[Higgins et al., 2017]  [Kim and Mnih, 2018]  [Chen et al., 2018]  [Eastwood and Williams, 2018] "], "page_idx": 51}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/787740d2761f5f1c30d8d7e647eea66b31435079bf97dfe6e10acc6090713d28.jpg", "table_caption": ["Table 7: Computation time (seconds) of supervised disentanglement metrics on image datasets "], "table_footnote": ["a [Higgins et al., 2017] b [Kim and Mnih, 2018] c [Chen et al., 2018] d [Eastwood and Williams, 2018] "], "page_idx": 51}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "However, it is important to note the limitations of these experimental results. Since the representations were learned from data and not fully controlled, it is possible that such results are due to the choices of datasets, learning algorithms, hyperparameters, and optimization errors. A high rank correlation coefficient between two metrics in this specific setting cannot guarantee that these metrics always measure the same property, or that they rank imperfect representations similarly in other settings. To gain a deeper understanding of these metrics, it is preferable to analyze their minimizers theoretically (Theorem 1) or test them in a fully controlled environment (Section 5). ", "page_idx": 51}, {"type": "text", "text": "E.5 Computation time of metrics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Finally, we report the computation time of the considered metrics in Table 7 to support our claim that the proposed metrics are much faster than those that require training additional predictors and hyperparameter tuning. We can see that, in an extreme case, the calculation of the DCI metrics [Eastwood and Williams, 2018] using GradientBoostingClassifier [Pedregosa et al., 2011] takes around 15 minutes, while other metrics can be calculated within seconds. This computation time may be acceptable if the metrics are only used in the evaluation phase, but it is not feasible to use them as learning objectives even in derivative-free optimization. ", "page_idx": 51}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/786a68caf506d4ef07d73777e131fbb46d45dc3412855b27ce5249b0e525b73d.jpg", "table_caption": ["Table 8: Factor-wise modularity metrics on 3D Cars [Reed et al., 2015] "], "table_footnote": [], "page_idx": 52}, {"type": "text", "text": "E.6 Factor-wise modularity metrics ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "An advantage of the proposed modularity metrics is that we can even evaluate each factor separately, which is impossible for those metrics that entangle modularity and informativeness. The results were reported in Tables 8 to 11. We found that some learning methods may outperform others on one factor but underperform on others, and different modularity metrics may rank learning methods differently (see also Appendix D.5). ", "page_idx": 52}, {"type": "text", "text": "For example, on MPI3D [Gondal et al., 2019] (Table 11), $\\beta$ -VAE [Higgins et al., 2017] has the highest scores (radius, MAD, variance, diameter, and MPD) on the horizontal and vertical axis factors, but the lowest scores (radius and diameter) on the object size, camera height, and background color factors. However, as measured by the MAD and MPD, it still has the highest scores on these factors. This means that $\\beta$ -VAE may generally encode the object size, camera height, and background color well compared to other considered methods, but has a small number of outliers. We believe that such fine-grained evaluation can guide the design of learning objectives, data collection, and further refinement of trained representation learning models. ", "page_idx": 52}, {"type": "text", "text": "Table 9: Factor-wise modularity metrics on dSprites [Matthey et al., 2017] ", "text_level": 1, "page_idx": 53}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/db02da055d69f0ed4c96c038ac4c8d937abce3f764ed56bcea1ad158bbe82292.jpg", "table_caption": [], "table_footnote": [], "page_idx": 53}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/2a179c75b38fb3b7a346d7900c49aeb5db7b9c35598efd69061e9f8e781cfefa.jpg", "table_caption": ["Table 10: Factor-wise modularity metrics on 3D Shapes [Burgess and Kim, 2018] "], "table_footnote": [], "page_idx": 54}, {"type": "table", "img_path": "tvQ3XCKWbB/tmp/fd0401c8e1dd2d9eae25cc52f82a33fffa6c10bb3c56f853f45055fc585ba268.jpg", "table_caption": ["Table 11: Factor-wise modularity metrics on MPI3D [Gondal et al., 2019] "], "table_footnote": [], "page_idx": 55}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Justification: \u201cA theoretical connection between logical definitions of disentanglement and quantitative metrics\u201d was detailed in Appendices A and B. \u201cA systematic approach for converting a first-order predicate into a real-valued quantity\u201d was introduced in Section 3. \u201cThe metrics induced by logical definitions,\u201d which were introduced in Section 4, \u201chave strong theoretical guarantees,\u201d which was supported by Theorem 1. \u201cThe effectiveness of the proposed metrics\u201d was empirically validated in Section 5 \u201cby isolating different aspects of disentangled representations\u201d and further investigated in Appendix E. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "The answer NA means that the abstract and introduction do not include the claims made in the paper.   \nThe abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \nThe claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \nIt is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 56}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: We discussed the limitations in Appendix D.7. Another limitation is that we only studied the properties of the minimizers of the metrics (Theorem 1). Research on the behavior of these metrics on imperfect representations is limited (Appendix D.5). We claimed that some proposed metrics can serve as differentiable learning objectives, but empirical evaluation is out of the scope of this work. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \nThe authors are encouraged to create a separate \"Limitations\" section in their paper.   \nThe paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \nThe authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \nThe authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \nThe authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \nIf applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \nWhile the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 56}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Justification: All definitions of the proposed terms, theoretical results, and detailed proofs were provided in Appendices A to C. The main theoretical result was stated in Theorem 1 and instantiated in Section 4. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "The answer NA means that the paper does not include theoretical results.   \nAll the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \nAll assumptions should be clearly stated or referenced in the statement of any theorems.   \nThe proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \nInversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \nTheorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 57}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: The contribution is primarily the new theoretical framework. For experiments, we provided code snippets in Appendix D.6. We provided the detailed data configuration used in Section 5 in Appendix E. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "The answer NA means that the paper does not include experiments.   \nIf the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \nIf the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \nDepending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \nWhile NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 57}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 58}, {"type": "text", "text": "Justification: We provided the code to reproduce the modularity metrics in Table 2.   \nGuidelines:   \nThe answer NA means that paper does not include experiments requiring code.   \nPlease see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \nWhile we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \nThe instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \nThe authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \nThe authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \nAt submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \nProviding as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 58}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 58}, {"type": "text", "text": "Justification: See Appendix E.   \nGuidelines:   \nThe answer NA means that the paper does not include experiments.   \nThe experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \nThe full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 58}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The results of the proposed metrics reported in Table 2 are deterministic and do not contain randomness. We reported the mean values of 10 random trials in Table 4, and the variances are negligible. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "The answer NA means that the paper does not include experiments.   \nThe authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \nThe factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \nThe method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \nThe assumptions made should be given (e.g., Normally distributed errors).   \nIt should be clear whether the error bar is the standard deviation or the standard error of the mean.   \nIt is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \nFor asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \nIf error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 58}, {"type": "text", "text": "", "page_idx": 59}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?   \nAnswer: [Yes]   \nJustification: See Appendix E.   \nGuidelines:   \nThe answer NA means that the paper does not include experiments.   \nThe paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \nThe paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \nThe paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 59}, {"type": "text", "text": "", "page_idx": 59}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: We reviewed the NeurIPS Code of Ethics.   \nGuidelines:   \nThe answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \nIf the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \nThe authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 59}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?   \nAnswer: [Yes]   \nJustification: See Appendix D.8.   \nGuidelines:   \nThe answer NA means that there is no societal impact of the work performed.   \nIf the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \nExamples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \nThe conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \nThe authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 59}, {"type": "text", "text": "", "page_idx": 59}, {"type": "text", "text": "If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 60}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 60}, {"type": "text", "text": "Justification: This paper mainly focused on the theory of disentangled representation learning.   \nGuidelines:   \nThe answer NA means that the paper poses no such risks.   \nReleased models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \nDatasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \nWe recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 60}, {"type": "text", "text": "", "page_idx": 60}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?   \nAnswer: [Yes]   \nJustification: See Appendix E.   \nGuidelines:   \nThe answer NA means that the paper does not use existing assets.   \nThe authors should cite the original paper that produced the code package or dataset.   \nThe authors should state which version of the asset is used and, if possible, include a URL.   \nThe name of the license (e.g., CC-BY 4.0) should be included for each asset.   \nFor scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \nIf assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \nFor existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \nIf this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 60}, {"type": "text", "text": "", "page_idx": 60}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation   \nprovided alongside the assets?   \nAnswer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: This paper does not release new assets. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "The answer NA means that the paper does not release new assets.   \nResearchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \nThe paper should discuss whether and how consent was obtained from people whose asset is used.   \nAt submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 60}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 60}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects.   \nGuidelines:   \nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \nIncluding this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \nAccording to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 61}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA]   \nJustification: This paper does not involve crowdsourcing nor research with human subjects.   \nGuidelines:   \nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \nDepending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \nWe recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \nFor initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 61}]