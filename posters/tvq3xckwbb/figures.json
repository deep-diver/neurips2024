[{"figure_path": "tvQ3XCKWbB/figures/figures_2_1.jpg", "caption": "Figure 1: Disentangled representation learning", "description": "This figure illustrates the concept of disentangled representation learning.  It shows data with multiple explanatory factors (e.g., color and shape) being generated via a function g from a product of factor spaces (Y = Y\u2081 \u00d7 Y\u2082). An encoder f maps the observations (X) to a product of code spaces (Z = Z\u2081 \u00d7 Z\u2082).  A modular encoder preserves this product structure, meaning that the composition m = f o g of the generator and encoder is a product function.  The example shows different combinations of color and shape as inputs, and how a well-disentangled model would represent them as separate factors in the codes.", "section": "Modularity: product structure preserved by a learning model"}, {"figure_path": "tvQ3XCKWbB/figures/figures_4_1.jpg", "caption": "Figure 2: From predicates and logical operations to quantities and quantitative operations", "description": "This figure illustrates the compositional approach for converting higher-order predicates into real-valued quantities. It shows how predicates and logical operations (conjunction, disjunction, implication) are converted into quantities and quantitative operations using strict premetrics and quantitative operations. The figure visually represents the relationship between logical definitions and quantitative metrics.", "section": "Enrichment: from logic to metric"}, {"figure_path": "tvQ3XCKWbB/figures/figures_5_1.jpg", "caption": "Figure 1: Disentangled representation learning", "description": "The figure illustrates the concept of disentangled representation learning.  It shows data with multiple explanatory factors (e.g., color and shape) being generated by a function g: Y \u2192 X, where Y is a product of factor spaces Y = Y\u2081 \u00d7 Y\u2082. An encoder f: X \u2192 Z maps the data to a product of code spaces Z = Z\u2081 \u00d7 Z\u2082.  The encoder is considered modular if the composition m := f \u00b0 g: Y \u2192 Z preserves the product structure, meaning m can be decomposed into separate functions m\u2081,\u2081: Y\u2081 \u2192 Z\u2081 and m\u2082,\u2082: Y\u2082 \u2192 Z\u2082.", "section": "2 Logical definitions of disentangled representations"}, {"figure_path": "tvQ3XCKWbB/figures/figures_23_1.jpg", "caption": "Figure 1: Disentangled representation learning", "description": "This figure illustrates the concept of disentangled representation learning using a diagram. It shows how multiple explanatory factors in data (e.g., color and shape) are represented separately in the learned representation.  The data is generated via a function g from a product of factors Y. An encoder f maps the observations X to a product of codes Z, where the composition m = f o g is a product function.  This demonstrates how disentangled representations preserve the product structure and separate the underlying factors.", "section": "Logical definitions of disentangled representations"}, {"figure_path": "tvQ3XCKWbB/figures/figures_24_1.jpg", "caption": "Figure 1: Disentangled representation learning", "description": "This figure illustrates the concept of disentangled representation learning.  Data with multiple explanatory factors (e.g., color and shape) is generated by a function g from a product Y of factors. An encoder f is a function that maps the data X into a product Z of codes.  A modular encoder is one which preserves the structure of the product (m = f o g).", "section": "Logical definitions of disentangled representations"}, {"figure_path": "tvQ3XCKWbB/figures/figures_28_1.jpg", "caption": "Figure 1: Disentangled representation learning", "description": "The figure illustrates the concept of disentangled representation learning. It shows that the data with multiple explanatory factors (e.g., color and shape) are generated via a function g: Y \u2192 X from a product Y := Y\u2081 \u00d7 Y\u2082 of factors.  An encoder f: X \u2192 Z is a function to a product Z := Z\u2081 \u00d7 Z\u2082 of codes.  Then, an encoder is said to be modular if it can reconstruct the product structure, such that the composition m := f \u25e6 g: Y \u2192 Z of the generator g and the encoder f is a product function.", "section": "2 Logical definitions of disentangled representations"}, {"figure_path": "tvQ3XCKWbB/figures/figures_29_1.jpg", "caption": "Figure 1: Disentangled representation learning", "description": "The figure illustrates the disentangled representation learning model. The data with multiple explanatory factors (e.g., color and shape) is generated via a function g: Y \u2192 X from a product Y:= Y\u2081 \u00d7 Y2 of factors. An encoder f: X \u2192 Z is a function to a product Z := Z1 \u00d7 Z2 of codes. Then, an encoder is said to be modular if it can reconstruct the product structure, such that the composition m := f\u00b0g: Y \u2192 Z of the generator g and the encoder f is a product function.", "section": "2 Logical definitions of disentangled representations"}, {"figure_path": "tvQ3XCKWbB/figures/figures_30_1.jpg", "caption": "Figure 3: Negation", "description": "This figure shows four different functions that could be used as a quantitative operation for negation. The functions are plotted against their input values (n). The red line represents the function [n = 0], which outputs 1 when n is 0 and 0 otherwise. The orange line represents the function 1 \u00f7 n, which approaches 1 when n is close to 0 and 0 when n approaches \u221e. The green line represents the function 1 \u00f7 n, which shows a similar behavior to the orange line. The blue line represents the function e\u207b\u207f, which decays exponentially as n increases. The choice of which function to use will depend on the specific application and desired properties of the negation operation.", "section": "3.2 From logical operation to quantitative operation"}, {"figure_path": "tvQ3XCKWbB/figures/figures_31_1.jpg", "caption": "Figure 2: From predicates and logical operations to quantities and quantitative operations", "description": "This figure illustrates the compositional approach of converting higher-order predicates into real-valued quantities.  It shows how predicates and logical operations (conjunction, disjunction, implication) are converted into quantities and their corresponding quantitative operations by replacing equality with a strict premetric, binary truth values with continuous values, and quantifiers with aggregators. The figure visually demonstrates the transformation process for each logical operation, highlighting the relationship between the logical and quantitative domains.", "section": "3 Enrichment: from logic to metric"}, {"figure_path": "tvQ3XCKWbB/figures/figures_33_1.jpg", "caption": "Figure 1: Disentangled representation learning", "description": "This figure illustrates the concept of disentangled representation learning.  The data, represented by observations (X), is generated by a function (g) that combines multiple explanatory factors (Y1, Y2). An encoder function (f) then transforms the observations into codes (Z), which ideally should maintain the structure of the factors (e.g., as separate components Z1 and Z2). The composition of g and f, denoted as m, which is the path from factors to codes is highlighted, and should also maintain the structure of Y and produce a product function.", "section": "Logical definitions of disentangled representations"}, {"figure_path": "tvQ3XCKWbB/figures/figures_36_1.jpg", "caption": "Figure 1: Disentangled representation learning", "description": "The figure illustrates a conceptual model of disentangled representation learning.  It shows how multiple explanatory factors (e.g., color and shape) in data are represented separately in a learned representation.  The data is initially generated from a product of factors (Y) via a generator (g) resulting in observations (X).  An encoder (f) maps these observations to a product of codes (Z). The encoder is considered modular if the composition of the generator and encoder (m) preserves the product structure of the factors.  This indicates disentanglement, where different explanatory factors are encoded separately.", "section": "Logical definitions of disentangled representations"}, {"figure_path": "tvQ3XCKWbB/figures/figures_44_1.jpg", "caption": "Figure 8: Quantitative operations for implication", "description": "This figure illustrates eight different ways to define a quantitative operation for implication in the context of converting logical definitions into quantitative metrics.  Each sub-figure shows a 3D plot representing the quantitative operation's behavior with respect to the truth values of two predicates (pB(a) and pC(a)) and demonstrates the relationship between these predicates and the quantitative outcomes.  The plots show various approaches, from those homomorphic to implication to those using other logical connectives like negation and disjunction, and highlight the differences in their behavior and coverage of possible outcomes.", "section": "3.3 From compound predicate to compound quantity"}, {"figure_path": "tvQ3XCKWbB/figures/figures_44_2.jpg", "caption": "Figure 9: Quantitative operations for equivalence", "description": "This figure shows four different quantitative operations for the logical equivalence, which is defined as (a \u2192 b) ^ (b \u2192 a) (bi-implication), (\u00aca \u2228 b) \u2227 (\u00acb\u2228 a) (conjunctive normal form (CNF)), (a\u2227b) \u2228 (\u00aca\u2227 \u00acb) (disjunctive normal form (DNF)). The last one is the fraction operation. Each subfigure shows a 3D plot that visualizes how the value of the quantitative operation changes as the values of the two input quantities vary. The plots showcase the different ways to approximate the logical operation quantitatively.", "section": "From compound predicate to compound quantity"}, {"figure_path": "tvQ3XCKWbB/figures/figures_45_1.jpg", "caption": "Figure 10: Two approaches for measuring the constancy of a set in R\u00b2: (a) finding a central point, such as the center of the smallest bounding sphere, the geometric median, or the mean, and then measuring the dispersion around this point; and (b) aggregating pairwise distances between points.", "description": "This figure shows two different approaches to quantify the constancy of a set of points. The first approach involves finding a central point (such as the mean, median, or center of the smallest bounding sphere), and then computing the dispersion around that point. The second approach involves aggregating all pairwise distances between the points in the set.  Both approaches aim to quantify how consistent the data points are; high constancy indicates points are clustered tightly, while low constancy suggests a more scattered distribution.", "section": "4.2 Modularity metrics via constancy"}, {"figure_path": "tvQ3XCKWbB/figures/figures_45_2.jpg", "caption": "Figure 10: Two approaches for measuring the constancy of a set in R\u00b2: (a) finding a central point, such as the center of the smallest bounding sphere, the geometric median, or the mean, and then measuring the dispersion around this point; and (b) aggregating pairwise distances between points.", "description": "This figure illustrates two different approaches to quantify the constancy of a set of points in a two-dimensional space. The first approach (a) involves identifying a central point (e.g., mean, median, or center of the smallest bounding sphere) and then measuring the dispersion of the points around this central point.  The second approach (b) focuses on calculating and aggregating the pairwise distances between all pairs of points in the set.", "section": "4.2 Modularity metrics via constancy"}, {"figure_path": "tvQ3XCKWbB/figures/figures_46_1.jpg", "caption": "Figure 11: Metrics may rank imperfect representations differently.", "description": "This figure shows two sets of points. The left one has a small radius and large variance, while the right one has a large radius and small variance. This illustrates how different metrics can rank imperfect representations differently, highlighting the need to consider the characteristics of each metric when choosing one for a specific application.", "section": "D.5 Rank of imperfect representations"}, {"figure_path": "tvQ3XCKWbB/figures/figures_46_2.jpg", "caption": "Figure 11: Metrics may rank imperfect representations differently.", "description": "The figure illustrates that different metrics may rank imperfect representations differently, even if they are derived from the same logical definition. This is because metrics may have different sensitivities to aspects like variance and radius.  The top row shows a set of points with a small bounding radius but high variance, while the bottom row shows a set with a larger bounding radius but lower variance.  This emphasizes the importance of considering the nuances and characteristics of specific metrics when evaluating disentangled representations.", "section": "D.5 Rank of imperfect representations"}, {"figure_path": "tvQ3XCKWbB/figures/figures_48_1.jpg", "caption": "Figure 13: (a) a set of factors Y represented by the RGB color model; (b) a set of entangled codes Z extracted by an encoder m: Y \u2192 Z; (c) a product function approximation; and (d) a linear approximation of the retraction h: Z \u2192 Y of the encoder.", "description": "This figure shows four 3D plots visualizing the transformation of factors to entangled codes and then to approximations.  (a) displays the true factors represented using an RGB color cube. (b) shows the entangled codes resulting from an encoder function m: Y \u2192 Z, exhibiting a deviation from the perfectly disentangled cube. (c) illustrates a product function approximation attempting to reconstruct a disentangled structure from the entangled codes. Finally, (d) shows a linear approximation of the retraction function h: Z \u2192 Y used to reconstruct the factors from the entangled codes.", "section": "5 Experiments"}, {"figure_path": "tvQ3XCKWbB/figures/figures_48_2.jpg", "caption": "Figure 14: Entangling a multivariate distribution via probability integral transform, inverse transform, and orthogonal transformation of standard normal distribution [Locatello et al., 2019b, Theorem 1].", "description": "This figure shows how a standard normal distribution can be transformed into a uniform distribution and vice-versa using probability integral transforms. It further demonstrates how an orthogonal transformation can be used to modify the correlation between variables in the distribution. This illustrates the challenges in disentangling representations, as different transformations can lead to the same or similar distribution.", "section": "Experiments"}]