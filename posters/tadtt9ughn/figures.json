[{"figure_path": "TADTT9ughN/figures/figures_0_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM against random sampling and BALD in active preference learning on two datasets: Reddit TL;DR and CNN/DM. The x-axis represents the number of data points acquired, and the y-axis represents the log-likelihood of the learned preference models. The results show that BAL-PM significantly outperforms both baselines, requiring fewer data points to achieve the same level of log-likelihood.  The shaded areas represent the standard error across multiple runs, indicating the statistical significance of the results.  The figure highlights the efficiency gain of BAL-PM, particularly a 33% reduction on Reddit TL;DR's test set and a 68% improvement on the CNN/DM dataset.", "section": "5 Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_1_1.jpg", "caption": "Figure 2: An illustration of how BAL-PM works. For each tuple (x, y1, y2) \u2208 Dpool, we obtain features for the prompt and prompt-completion pairs by computing the last layer embeddings of the base LLM. We leverage the prompt feature space to estimate the entropy score of the acquired prompt distribution, \u0124(Xtrain \u222a {x}). Similarly, we use the prompt-completion features as input for the Bayesian Preference Model, which is used to estimate task-dependent epistemic uncertainty scores, \u00db(x, y1, y2). BAL-PM selects the tuple that maximizes the linear combination of both scores.", "description": "This figure illustrates the BAL-PM algorithm's workflow.  For each prompt-completion pair in the pool, the last layer embeddings from a base LLM are used to represent the prompt and completion in a feature space. The algorithm then estimates two scores: (1) the entropy of the prompt distribution in the feature space, which promotes diversity, and (2) the epistemic uncertainty of the prompt-completion pair according to the Bayesian preference model. BAL-PM selects the pair that maximizes the sum of these two scores, weighted by a hyperparameter \u03b2.", "section": "4 Bayesian Active Learner for Preference Modeling"}, {"figure_path": "TADTT9ughN/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of entropy estimators. The green point maximizes the entropy estimation of the prompt distribution (according to the employed estimator). Dashed lines represent its k-NN distance. In (a), the KL estimator (Equation 4) does not account for the available prompts in the pool (in red) and underestimates the density in regions not covered by the acquired set (in blue). In (b), the KSG estimator (Equation 10) uses all data points, leading to better estimation and effectively selecting the point that maximizes the true entropy.", "description": "This figure illustrates the difference between two entropy estimation methods: KL and KSG.  KL estimator underestimates density in low-density regions because it only considers acquired points, while KSG utilizes all data points (acquired and available) for more accurate entropy estimation and better diversity in selected samples.", "section": "Feature Space Entropy Estimation"}, {"figure_path": "TADTT9ughN/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM against random sampling and BALD baselines on two datasets: Reddit TL;DR and CNN/DM.  The y-axis represents the log-likelihood of the learned preference model, indicating its accuracy. The x-axis represents the number of acquired data points (human feedback). The results show that BAL-PM consistently outperforms the other methods, requiring significantly fewer data points to achieve comparable performance. The shaded regions represent the standard error across multiple experimental runs, illustrating the reliability of the results.", "section": "5 Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM against baseline methods (random sampling and BALD) in two datasets: Reddit TL;DR (test split) and CNN/DM.  The y-axis represents the log-likelihood of the learned preference models, indicating model performance.  The x-axis represents the number of data points (acquired data) used for training.  BAL-PM consistently outperforms the baselines, achieving a significant reduction in the number of samples needed to reach comparable model performance. The shaded regions illustrate the standard error across five different trials, showing the consistency of the results.", "section": "Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_8_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM against random sampling and BALD in active preference modeling on two datasets: Reddit TL;DR and CNN/DM. The plots show the log-likelihood of the learned preference models as a function of the number of acquired data points.  BAL-PM significantly outperforms the baselines, requiring fewer data points to achieve similar performance, indicating its efficiency in data acquisition for preference learning.", "section": "5 Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_8_2.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM against random sampling and BALD in active preference learning using two datasets: Reddit TL;DR and CNN/DM.  The x-axis represents the number of data points acquired, and the y-axis shows the log-likelihood of the learned preference models.  The plots demonstrate that BAL-PM achieves significantly higher log-likelihoods with considerably fewer samples compared to the baseline methods, showcasing its efficiency in reducing the amount of human feedback needed for preference modeling. The shaded regions represent the standard errors across multiple runs, indicating the reliability of the results.", "section": "Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_17_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM against random sampling and BALD baselines in two active preference modeling tasks: Reddit TL;DR and CNN/DM. The x-axis represents the number of acquired data points, and the y-axis shows the log-likelihood of the learned preference models. The results demonstrate that BAL-PM requires significantly fewer data points (33% fewer for Reddit TL;DR test set and 68% for CNN/DM) to achieve similar or better performance compared to the baselines. The shaded regions represent the standard error computed across multiple runs, showcasing the consistency of the results.", "section": "5 Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_18_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM against two baseline methods (random sampling and BALD) in two different datasets (Reddit TL;DR and CNN/DM).  The x-axis represents the number of data points acquired, and the y-axis shows the log-likelihood of the learned preference model.  The results demonstrate that BAL-PM significantly outperforms the baselines, requiring substantially fewer samples to achieve comparable performance. The shaded areas indicate the standard error across multiple runs, highlighting the consistency of the results.  The figure showcases the efficiency of the BAL-PM model in reducing the need for human feedback during preference labeling.", "section": "Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_18_2.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "The figure shows the comparison of the proposed BAL-PM method against two baseline methods (random sampling and BALD) for two datasets, Reddit TL;DR and CNN/DM, in active preference learning.  The x-axis represents the number of acquired data points, and the y-axis represents the log-likelihood of the learned preference model, a measure of model performance.  BAL-PM consistently outperforms the baseline methods across both datasets, demonstrating a significant reduction in the number of samples required to achieve comparable performance.  The shaded region indicates standard error across multiple runs, highlighting the statistical significance of the results.", "section": "5 Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_20_1.jpg", "caption": "Figure 11: Ratio of entropy and preference model uncertainty scores. This plot represents the normalized contributions from the terms of Equation 5 on the first selected point of each batch. BAL-PM automatically adjusts the contribution based on the information available in the pool set.", "description": "This figure shows how BAL-PM balances the contributions of task-dependent and task-agnostic epistemic uncertainty during active learning. The ratio of entropy and preference model uncertainty scores is plotted against the number of acquired data points.  Initially, the entropy score (task-agnostic) dominates, promoting diversity in the acquired prompt distribution. As more data is acquired, the relevance of the preference model uncertainty (task-dependent) increases, leading to a shift towards exploiting the model's knowledge.", "section": "F BAL-PM Objective \u2013 Empirical Analysis"}, {"figure_path": "TADTT9ughN/figures/figures_21_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM with random sampling and BALD in two datasets: Reddit TL;DR and CNN/DM.  The y-axis represents the log-likelihood of the learned preference models, indicating the quality of the model. The x-axis shows the number of acquired data points. BAL-PM consistently outperforms the baselines, demonstrating a significant reduction in the amount of data needed to achieve a high-quality preference model. The shaded regions represent standard error across multiple trials.", "section": "Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_23_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM with random sampling and BALD across two datasets: Reddit TL;DR and CNN/DM.  The x-axis represents the number of acquired data points. The y-axis represents the log-likelihood of the learned preference model.  BAL-PM significantly outperforms the baseline methods by requiring considerably fewer samples to achieve comparable or better log-likelihood scores. The shaded regions represent the standard error, indicating the consistency of the results across multiple trials.", "section": "5 Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_24_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM with random sampling and BALD in two datasets: Reddit TL;DR and CNN/DM.  The x-axis represents the number of data points acquired, and the y-axis shows the log-likelihood of the learned preference model.  The results demonstrate that BAL-PM requires significantly fewer data points (33% fewer for Reddit TL;DR test set and 68% for CNN/DM) to achieve similar or better log-likelihood compared to the baselines, indicating its higher efficiency in preference modeling. Shaded regions represent standard errors across multiple runs.", "section": "5 Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_24_2.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM against random sampling and BALD in active preference modeling using two datasets: Reddit TL;DR and CNN/DM. The results show that BAL-PM significantly reduces the number of samples needed to achieve comparable performance, indicating its efficiency. The shaded regions represent standard errors across multiple runs.", "section": "5 Experiments and Discussion"}, {"figure_path": "TADTT9ughN/figures/figures_25_1.jpg", "caption": "Figure 4: Comparison with baseline methods in Active Preference Modeling. BAL-PM considerably reduces the number of samples required for preference modeling, achieving 33% and 68% of reduction in the Reddit TL;DR test split and CNN/DM News datasets, respectively. The shaded area corresponds to the standard error computed over five seeds.", "description": "This figure compares the performance of BAL-PM against two baseline methods (random sampling and BALD) in active preference learning on two datasets: Reddit TL;DR and CNN/DM.  The x-axis represents the number of acquired data points, and the y-axis represents the log-likelihood of the learned preference model.  The results demonstrate that BAL-PM significantly outperforms the baselines, requiring substantially fewer data points to achieve similar log-likelihood values, showcasing its efficiency in active preference modeling.", "section": "5 Experiments and Discussion"}]