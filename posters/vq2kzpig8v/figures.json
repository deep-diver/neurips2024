[{"figure_path": "Vq2kzpig8v/figures/figures_5_1.jpg", "caption": "Figure 1: (a) The first number in each cell denotes the reward received by the agent taking the row action, and the second the reward received by the agent taking the column action, where C: cooperate (stay silent) and D: defect (confess). (b) Two agents (red and blue) are tasked with collecting randomly spawning coins. If an agent collects its own coin, it receives a reward of +1 (left). If an agent collects another's coin, then it receives a reward of +1 but the other agent receives a punishment of -2.", "description": "This figure illustrates the reward structures for two sequential social dilemmas used in the paper's experiments. (a) shows the reward matrix for the Iterated Prisoner's Dilemma (IPD), a classic game theory scenario.  (b) depicts the reward structure for the Coins game, where agents compete to collect coins, receiving a reward for their own coins and a penalty for collecting their opponent's coins. ", "section": "5.1 Sequential Social Dilemmas"}, {"figure_path": "Vq2kzpig8v/figures/figures_6_1.jpg", "caption": "Figure 2: Representative run of a Reciprocator vs. an NL in IPD-Rollout. Average reciprocal reward per step (left axis) and probability of cooperation (right axis) over the course of an episode.", "description": "This figure shows the results of a single game between a Reciprocator agent and a Naive Learner (NL) agent in an Iterated Prisoner's Dilemma (IPD) with rollout-based evaluation. The left y-axis displays the average reciprocal reward received by the Reciprocator agent at each step of the game, while the right y-axis shows the probability that the Reciprocator agent chooses to cooperate (C) at each step. The x-axis represents the episode number. The plot demonstrates the oscillatory pattern of the reciprocal reward and cooperation probability, showing how the Reciprocator agent's behavior changes over the course of the game in response to the NL agent's actions. The oscillations reflect the interplay between the intrinsic reciprocal reward and the extrinsic reward obtained from the game. The Reciprocator initially cooperates but may defect depending on the NL's actions, leading to fluctuations in the reciprocal reward and cooperation probability.", "section": "5.2 Baselines"}, {"figure_path": "Vq2kzpig8v/figures/figures_7_1.jpg", "caption": "Figure 3: Shaping an NL in Coins. Proportion of own coins collected by NL during training when facing each opponent (left) and coin counts by type for Reciprocator vs. NL (right). Reciprocator and NL-PPO results are plotted on a scale of single episodes (bottom axis) whereas MFOS results are plotted on a scale of meta-episodes, where one meta-episode contains 16 episodes (top axis).", "description": "This figure displays the results of training a naive learner (NL) agent in the Coins game against three different opponents: a Reciprocator, MFOS, and another NL-PPO. The left panel shows the proportion of its own coins collected by the NL during training against each opponent. The right panel shows the number of coins collected (own, other, and total) by both the Reciprocator and NL-PPO agents over training episodes. Note that the time scale differs between MFOS and the other two agents, with MFOS using meta-episodes (16 episodes each).", "section": "5.1 Sequential Social Dilemmas"}, {"figure_path": "Vq2kzpig8v/figures/figures_8_1.jpg", "caption": "Figure 4: Head-to-head results in symmetric Coins (two agents of the same kind). Total (extrinsic) reward per episode (left), proportion of own coins collected (right). Again, Reciprocator and NL-PPO results are plotted on a scale of episodes and MFOS results are plotted on a scale of meta-episodes.", "description": "This figure displays the results of a head-to-head competition between two Reciprocator agents in the Coins game, as well as their performance against MFOS and NL-PPO agents.  The left panel shows the total extrinsic reward per episode, illustrating the Reciprocator's ability to achieve higher rewards than the baselines. The right panel shows the proportion of own coins collected by each agent type.  This highlights the Reciprocator's ability to successfully collect more of its own coins than the baselines, demonstrating cooperative behavior.", "section": "6.2 Coins"}, {"figure_path": "Vq2kzpig8v/figures/figures_14_1.jpg", "caption": "Figure 4: Head-to-head results in symmetric Coins (two agents of the same kind). Total (extrinsic) reward per episode (left), proportion of own coins collected (right). Again, Reciprocator and NL-PPO results are plotted on a scale of episodes and MFOS results are plotted on a scale of meta-episodes.", "description": "This figure compares the performance of Reciprocators against MFOS and NL-PPO agents in a symmetric Coins game where both agents are of the same type. The left panel shows the total extrinsic reward per episode, while the right panel shows the proportion of own coins collected by each agent type.  It demonstrates that Reciprocators achieve higher rewards and collect a greater proportion of their own coins compared to the other approaches, highlighting their superior cooperative behavior even in a competitive setting. The difference in scaling between the x-axes (episodes vs. meta-episodes) reflects the distinct training methodologies employed by MFOS, which uses meta-learning across multiple episodes.", "section": "6.2 Coins"}, {"figure_path": "Vq2kzpig8v/figures/figures_14_2.jpg", "caption": "Figure 3: Shaping an NL in Coins. Proportion of own coins collected by NL during training when facing each opponent (left) and coin counts by type for Reciprocator vs. NL (right). Reciprocator and NL-PPO results are plotted on a scale of single episodes (bottom axis) whereas MFOS results are plotted on a scale of meta-episodes, where one meta-episode contains 16 episodes (top axis).", "description": "This figure shows the results of training a naive learner (NL) in the Coins game against three different opponents: a Reciprocator, MFOS, and another NL-PPO. The left panel shows the proportion of own coins collected by the NL over time when facing each opponent. The right panel displays the number of coins collected by type (own coins, other coins, and total coins) for both the Reciprocator and the NL-PPO over time.  The x-axis represents the episode number, but note that MFOS results are shown in terms of meta-episodes (16 episodes each). This visualizes how the Reciprocator shapes the NL's behavior towards collecting more of its own coins, contrasting with the other opponents.", "section": "5.1 Sequential Social Dilemmas"}]