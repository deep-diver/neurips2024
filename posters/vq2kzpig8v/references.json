{"references": [{"fullname_first_author": "John N. Foerster", "paper_title": "Learning with Opponent-Learning Awareness", "publication_date": "2018-09-01", "reason": "This paper introduces the LOLA algorithm, a foundational work in opponent shaping that directly inspires the Reciprocator's approach to influencing opponent learning."}, {"fullname_first_author": "Jakob Foerster", "paper_title": "Counterfactual Multi-Agent Policy Gradients", "publication_date": "2017-12-01", "reason": "This paper introduces the COMA algorithm, providing a key counterfactual baseline estimation method for the Reciprocator's value influence calculation."}, {"fullname_first_author": "C. Lu", "paper_title": "Model-Free Opponent Shaping", "publication_date": "2022-06-01", "reason": "This paper presents the MFOS algorithm, a leading model-free opponent-shaping method that serves as a major comparative baseline for the Reciprocator."}, {"fullname_first_author": "Arnu Pretorius", "paper_title": "Approximate Markov Tit-for-Tat", "publication_date": "2018-01-01", "reason": "This paper introduces the amTFT algorithm, which inspires the Reciprocator's influence balance mechanism for tracking cumulative influence over extended time scales."}, {"fullname_first_author": "Zheng Wang", "paper_title": "Influence-Based Multi-Agent Exploration", "publication_date": "2020-04-01", "reason": "This paper introduces the VoI reward, which the Reciprocator adapts as a foundation for its reciprocal reward function by modifying it to focus on one-step value influence."}]}