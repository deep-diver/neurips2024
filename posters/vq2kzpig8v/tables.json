[{"figure_path": "Vq2kzpig8v/tables/tables_6_1.jpg", "caption": "Table 1: IPD-Analytic round robin results", "description": "This table presents the average reward per episode achieved by each agent (rows) against every other agent (columns) in the IPD-Analytic setting.  The results are from a round-robin tournament where each agent plays against every other agent.  Lower values indicate worse performance.  The results show that Reciprocators achieve a comparable performance against all other baselines.", "section": "5.2 Baselines"}, {"figure_path": "Vq2kzpig8v/tables/tables_13_1.jpg", "caption": "Table 2: General PPO parameters.", "description": "This table lists the hyperparameters used for training the Proximal Policy Optimization (PPO) agents in the IPD-Rollout and Coins experiments.  It details the network architecture (number and size of convolutional and linear layers, and GRUs), training parameters (episode length, Adam learning rate, PPO epochs per episode, PPO-clip epsilon), and reward parameters (discount factor and entropy coefficient). These settings were used for both the Reciprocator and baseline agents in the respective experiments.", "section": "A Experimental Details"}, {"figure_path": "Vq2kzpig8v/tables/tables_13_2.jpg", "caption": "Table 3: Reciprocator-specific parameters.", "description": "This table lists the hyperparameters specific to the Reciprocator agent,  including buffer sizes for experience replay, batch sizes for training, target update periods, and the weight given to the reciprocal reward.  Different values are shown for the IPD-Analytic, IPD-Rollout, and Coins experiments, indicating parameter tuning for each environment.", "section": "5.3 Implementation Details"}]