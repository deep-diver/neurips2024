[{"figure_path": "ffNrpcBpi6/figures/figures_0_1.jpg", "caption": "Figure 1: Performance improvements (%) of our GFSA when integrated with different Transformer backbones in various domains. We achieve these results with only tens to hundreds of additional parameters to Transformers.", "description": "This figure shows the performance improvement achieved by integrating the proposed Graph Filter-based Self-Attention (GFSA) with various Transformer models across different tasks. The improvements are presented as percentages, indicating the increase in performance obtained by adding GFSA compared to the original models.  Noteworthy is that these improvements were achieved with the addition of only a small number of parameters (tens to hundreds) to the base Transformers, highlighting the efficiency of the proposed method.", "section": "1 Introduction"}, {"figure_path": "ffNrpcBpi6/figures/figures_1_1.jpg", "caption": "Figure 2: Filter frequency response, cosine similarity, and singular values on ImageNet-1k for DeiT-S and DeiT-S + GFSA. Details and more visualizations are in Appendices C and D.", "description": "This figure visualizes the impact of GFSA on DeiT-S (a small version of DeiT) performance on ImageNet-1k.  It presents three sub-figures:\n(a) Filter response: Shows the frequency response of the original DeiT-S and DeiT-S with GFSA in the frequency domain.  GFSA shows a wider range of frequencies used compared to vanilla DeiT-S, suggesting improved ability to capture diverse features.\n(b) Cosine similarity: Shows the cosine similarity between feature vectors across layers for both DeiT-S models.  GFSA helps mitigate the oversmoothing problem, maintaining feature distinguishability across layers.\n(c) Singular value: Shows the singular values of the feature matrix of the last block. GFSA helps mitigate the dimensionality collapse in Transformer-based models by retaining more significant singular values.", "section": "2.3 Oversmoothing in GCNs and Transformers"}, {"figure_path": "ffNrpcBpi6/figures/figures_9_1.jpg", "caption": "Figure 3: Effectiveness of our selective layer strategy on ImageNet-1k. This shows out strategy's ability to maintain accuracy benefits while mitigating runtime increases.", "description": "This figure demonstrates the effectiveness of selectively applying GFSA to even-numbered layers in a 12-layer DeiT-S model trained on ImageNet-1k.  The bar chart shows that applying GFSA to all layers (+\"GFSA\") improves top-1 accuracy from 79.8% to 81.1%, but significantly increases the runtime per epoch from ~551s to ~814s.  In contrast, applying GFSA only to even-numbered layers (+\"GFSAeven\") maintains a similar improvement in top-1 accuracy (81.0%) while significantly reducing the runtime increase to ~595s.  This highlights the strategy's success in balancing improved accuracy and reduced computational cost.", "section": "Discussion on Runtime Overheads"}, {"figure_path": "ffNrpcBpi6/figures/figures_9_2.jpg", "caption": "Figure 4: Performance (x-axis), runtime (y-axis), and GPU usage (circle sizes) of various Transformers and integrated GFSA on Long-Range benchmark", "description": "This figure compares the performance, runtime, and GPU usage of different Transformer models, both with and without the proposed GFSA (Graph Filter-based Self-Attention) on the Long-Range Arena benchmark.  The x-axis represents the accuracy achieved, the y-axis represents the runtime (in seconds per 1000 steps), and the size of the circles represents the GPU memory usage in gigabytes.  The results show that GFSA improves the accuracy across models while maintaining relatively low runtime especially when implemented with Efficient Attention. Note the significant reduction in runtime for Efficient Attention + GFSA compared to the Transformer + GFSA.", "section": "6 Discussion on Runtime Overheads"}, {"figure_path": "ffNrpcBpi6/figures/figures_18_1.jpg", "caption": "Figure 2: Filter frequency response, cosine similarity, and singular values on ImageNet-1k for DeiT-S and DeiT-S + GFSA. Details and more visualizations are in Appendices C and D.", "description": "This figure visualizes the effects of GFSA on DeiT-S (a small version of DeiT) for ImageNet-1k image classification.  It uses three sub-figures to show:\n\n(a) **Filter response:** The frequency response of the self-attention mechanism in DeiT-S, with and without GFSA. This demonstrates how GFSA enriches the frequency information processed by the model, addressing the oversmoothing issue.\n(b) **Cosine similarity:** Shows the cosine similarity between token representations across different layers of the model. The lower the cosine similarity, the more distinct the representations, indicating that GFSA reduces oversmoothing.\n(c) **Singular value:** Shows the distribution of singular values, which represent the importance of different components in the representation. It shows that GFSA is able to capture more diverse feature information.\nThe Appendices C and D contain additional visualizations and details.", "section": "2.3 Oversmoothing in GCNs and Transformers"}, {"figure_path": "ffNrpcBpi6/figures/figures_19_1.jpg", "caption": "Figure 2: Filter frequency response, cosine similarity, and singular values on ImageNet-1k for DeiT-S and DeiT-S + GFSA. Details and more visualizations are in Appendices C and D.", "description": "This figure visualizes the effects of GFSA on DeiT-S, a small vision transformer model, trained on the ImageNet-1k dataset.  It presents three sub-figures:\n(a) Filter response: Shows the frequency response of the self-attention mechanism in DeiT-S (blue) and DeiT-S with GFSA (orange). The difference highlights how GFSA enhances higher-frequency information compared to the original self-attention, which predominantly focuses on low frequencies (oversmoothing).\n(b) Cosine similarity: This illustrates the cosine similarity between the representations of different layers in the model. The graph indicates that GFSA reduces the increase of similarity between layers as the network deepens, thereby mitigating the oversmoothing problem.\n(c) Singular value: Presents the singular value distribution of the features in the last block.  GFSA's impact is shown by its more diverse singular values, implying that it prevents the collapse of feature representations which is often associated with oversmoothing.  In essence, this analysis reveals that GFSA enriches the self-attention mechanism by preserving a wider range of frequencies and preventing representation collapse.", "section": "2.3 Oversmoothing in GCNs and Transformers"}, {"figure_path": "ffNrpcBpi6/figures/figures_19_2.jpg", "caption": "Figure 2: Filter frequency response, cosine similarity, and singular values on ImageNet-1k for DeiT-S and DeiT-S + GFSA. Details and more visualizations are in Appendices C and D.", "description": "This figure visualizes the effects of GFSA on DeiT-S for ImageNet-1k image classification.  It shows three subfigures: (a) demonstrates the frequency response of the filter, highlighting how GFSA enriches the self-attention mechanism by incorporating a wider range of frequencies; (b) illustrates the cosine similarity between representations across different layers, showcasing GFSA's mitigation of over-smoothing; (c) presents the singular value distribution, further emphasizing the improved representation learning of GFSA.  Appendices C and D contain further details and visualizations.", "section": "3 Graph Filter-based Self-Attention Layers"}, {"figure_path": "ffNrpcBpi6/figures/figures_30_1.jpg", "caption": "Figure 2: Filter frequency response, cosine similarity, and singular values on ImageNet-1k for DeiT-S and DeiT-S + GFSA. Details and more visualizations are in Appendices C and D.", "description": "This figure visualizes the effects of GFSA on DeiT-S (a small vision transformer model) trained on the ImageNet-1k dataset. It consists of three sub-figures: (a) shows the frequency response of the filter, illustrating how GFSA modifies the frequency components; (b) displays the cosine similarity between representations across different layers, indicating that GFSA mitigates the oversmoothing problem; and (c) presents the singular value distribution, showing how GFSA preserves more distinct features. Appendices C and D provide further details and additional visualizations.", "section": "3 Graph Filter-based Self-Attention Layers"}]