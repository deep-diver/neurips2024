[{"figure_path": "ffNrpcBpi6/tables/tables_6_1.jpg", "caption": "Table 1: Results comparison on GLUE benchmark. Avg denotes the average performance.", "description": "This table presents the results of experiments conducted on the GLUE benchmark, a widely used collection of datasets for evaluating natural language understanding systems.  The table compares the performance of three different pre-trained large language models (BERT, ALBERT, and RoBERTa) with and without the proposed Graph Filter-based Self-Attention (GFSA) method.  The results are broken down by individual GLUE task (e.g., CoLA, SST-2, MRPC, etc.) and show the average performance across all tasks.  It also includes a comparison against a related method called ContraNorm.", "section": "5.1 Experiments on Natural Language Understanding"}, {"figure_path": "ffNrpcBpi6/tables/tables_6_2.jpg", "caption": "Table 2: Results comparison on GPT-2 finetuned with GFSA. Avg denotes the average performance.", "description": "This table presents the results of experiments comparing the performance of a standard GPT-2 model against a GPT-2 model enhanced with the Graph Filter-based Self-Attention (GFSA) mechanism.  The comparison is made across three different datasets for causal language modeling: Penn Treebank (PTB), WikiText-2, and WikiText-103. The table shows the perplexity scores for each model on each dataset, along with the average perplexity across all three datasets. Lower perplexity indicates better performance.", "section": "5.2 Experiments on Causal Language Modeling"}, {"figure_path": "ffNrpcBpi6/tables/tables_7_1.jpg", "caption": "Table 3: Results comparison on ImageNet-1k. Our full results with other models are in Appendix M.4.", "description": "This table presents the Top-1 accuracy results on the ImageNet-1k benchmark for various Vision Transformer (ViT) backbones, including DeiT, CaiT, and Swin.  It compares the performance of these base models against versions modified with different enhancement techniques: AttnScale, FeatScale, ContraNorm, and the authors' proposed GFSA.  The table shows the impact of these techniques on accuracy for both 12-layer and 24-layer versions of some models, indicating how GFSA contributes to improved performance compared to the existing state-of-the-art methods.", "section": "Experiments on Vision Transformers"}, {"figure_path": "ffNrpcBpi6/tables/tables_8_1.jpg", "caption": "Table 4: Results on ZINC", "description": "This table presents the Mean Absolute Error (MAE) achieved by the Graphormer model and the Graphormer model enhanced with GFSA (Graph Filter-based Self-Attention) on the ZINC dataset.  Lower MAE values indicate better performance. The results demonstrate that incorporating GFSA improves the model's accuracy in predicting molecular properties.", "section": "Experiments on Graph-level Tasks"}, {"figure_path": "ffNrpcBpi6/tables/tables_8_2.jpg", "caption": "Table 6: Experimental evaluation of GFSA plugged into GPS and Graph-ViT. Results marked with \u2020 indicate settings where we conducted our own experiments due to unavailable Hadamard self-attention performance in He et al. [30]'s paper.", "description": "This table presents the results of experiments comparing the performance of GPS and Graph-ViT models with and without the GFSA method on several graph-level datasets.  It shows the average precision (AP), mean absolute error (MAE), and area under the ROC curve (ROCAUC) for each dataset.  The use of GFSA leads to improved performance across multiple metrics on most datasets.  Note that the results for Graph-ViT on MNIST and CIFAR10 are denoted with a \u2020 to indicate that the authors conducted their own experiments due to a lack of available data for the standard Hadamard self-attention method in the referenced work.", "section": "Experiments on Graph-level Tasks"}, {"figure_path": "ffNrpcBpi6/tables/tables_8_3.jpg", "caption": "Table 7: Results for ASR training on LibriSpeech 100h and 960h with GFSA", "description": "This table presents the Word Error Rate (WER) results for Automatic Speech Recognition (ASR) experiments conducted on the LibriSpeech dataset (both 100-hour and 960-hour subsets).  The WER is broken down for test-clean and test-other subsets for each model. The models compared are a vanilla Transformer, a Transformer with the proposed GFSA (Graph Filter-based Self-Attention), a Branchformer model, and a Branchformer model also incorporating GFSA. The table shows the impact of GFSA on the WER, indicating its effectiveness in improving ASR performance.", "section": "5.5 Experiments on Automatic Speech Recognition"}, {"figure_path": "ffNrpcBpi6/tables/tables_8_4.jpg", "caption": "Table 8: Results on code classification. The number in (\u2191) indicates the improvement rate.", "description": "This table presents the results of code classification experiments using different transformer models with and without the proposed GFSA.  It shows the accuracy achieved by each model, highlighting the improvement gained by integrating GFSA.  The improvement is presented as a percentage increase in accuracy. The models compared include ROBERTa, CodeBERT, PLBART, and two versions of CodeT5 (small and base).", "section": "5.6 Experiments on Code Classification"}, {"figure_path": "ffNrpcBpi6/tables/tables_22_1.jpg", "caption": "Table 1: Results comparison on GLUE benchmark. Avg denotes the average performance.", "description": "This table compares the performance of different models on the GLUE benchmark, a collection of tasks designed to evaluate the performance of natural language understanding systems.  The models include the base BERT, ALBERT, and RoBERTa models, as well as versions of these models that incorporate the proposed GFSA (Graph Filter-based Self-Attention) mechanism and the ContraNorm method.  The table shows the performance of each model on several individual GLUE tasks, such as CoLA, SST-2, MRPC, and others, as well as an average performance across all the tasks.  This allows for a comparison of the effectiveness of GFSA in improving performance compared to the base models and a competing technique.", "section": "Experiments on Natural Language Understanding"}, {"figure_path": "ffNrpcBpi6/tables/tables_22_2.jpg", "caption": "Table 1: Results comparison on GLUE benchmark. Avg denotes the average performance.", "description": "This table compares the performance of different models on the GLUE benchmark, a standard dataset for evaluating natural language understanding systems.  It shows the results for several models (BERT, ALBERT, and RoBERTa) both with and without the proposed GFSA method (Graph Filter-based Self-Attention).  The table presents scores for various GLUE subtasks and an average score across all tasks.  The \"#Params\" column shows the number of parameters in each model.  It highlights the performance improvement achieved by integrating GFSA.", "section": "5.1 Experiments on Natural Language Understanding"}, {"figure_path": "ffNrpcBpi6/tables/tables_24_1.jpg", "caption": "Table 10: Sensitivity results on various K with BERTBASE finetuned on GLUE tasks", "description": "This table presents the results of a sensitivity analysis on the hyperparameter K (polynomial order) used in the GFSA method.  The analysis was performed using BERTBASE models fine-tuned on various GLUE tasks. The table shows the performance (measured by metrics specific to each task) for different values of K, ranging from 2 to 10, to evaluate the robustness of the GFSA's performance with regard to changes in K.  The goal is to determine the optimal K value for each GLUE task.", "section": "K.2 Sensitivity to K"}, {"figure_path": "ffNrpcBpi6/tables/tables_25_1.jpg", "caption": "Table 11: Results comparison on GPT-2 finetuned with GFSA", "description": "This table shows the perplexity results on three datasets (PTB, WikiText-2, WikiText-103) for different values of the hyperparameter K in GFSA.  It demonstrates the impact of varying the polynomial order in the proposed GFSA on the performance of causal language modeling.", "section": "L Causal Language Modeling"}, {"figure_path": "ffNrpcBpi6/tables/tables_25_2.jpg", "caption": "Table 3: Results comparison on ImageNet-1k. Our full results with other models are in Appendix M.4.", "description": "This table presents the Top-1 accuracy results on the ImageNet-1k benchmark for various Vision Transformer (ViT) backbones (DeiT-S, CaiT-S, Swin-S) with and without the proposed Graph Filter-based Self-Attention (GFSA) mechanism.  It compares the performance of different ViT models with varying depths (12 and 24 layers), highlighting the improvement achieved by incorporating GFSA.  The table also includes a comparison with other state-of-the-art methods (AttnScale, FeatScale, and ContraNorm) for context.", "section": "Experiments on Vision Transformers"}, {"figure_path": "ffNrpcBpi6/tables/tables_26_1.jpg", "caption": "Table 13: Sensitivity to K for 12-layer DeiT-S + GFSA", "description": "This table presents the sensitivity analysis of the 12-layer DeiT-S model with GFSA to different values of the hyperparameter K, which controls the order of the matrix polynomial in the GFSA filter.  The table shows the Top-1 accuracy on the ImageNet-1k dataset for each value of K (2, 3, 4, and 5).  It demonstrates how the model's performance varies with different choices of K, indicating the optimal value or the range of values that yield the best performance.", "section": "5.3 Experiments on Vision Transformers"}, {"figure_path": "ffNrpcBpi6/tables/tables_26_2.jpg", "caption": "Table 14: Sensitivity to K for 24-layer CaiT-S + GFSA", "description": "This table presents the sensitivity analysis results for the 24-layer CaiT-S model with GFSA applied. It shows the Top-1 accuracy achieved for different values of the hyperparameter K (polynomial order in GFSA).  The results demonstrate how the model's performance varies with different values of K, indicating the optimal K for achieving the best accuracy.", "section": "5.3 Experiments on Vision Transformers"}, {"figure_path": "ffNrpcBpi6/tables/tables_26_3.jpg", "caption": "Table 3: Results comparison on ImageNet-1k. Our full results with other models are in Appendix M.4.", "description": "This table compares the Top-1 accuracy of various Vision Transformer models on the ImageNet-1k dataset.  The models include several base ViT architectures (DeiT-S, CaiT-S, Swin-S) with different numbers of layers, and various modifications applied to improve performance, such as AttnScale, FeatScale, ContraNorm. The table also shows the results obtained by integrating the proposed Graph Filter-based Self-Attention (GFSA) into these models and highlights the performance gains achieved using GFSA.", "section": "Experiments on Vision Transformers"}, {"figure_path": "ffNrpcBpi6/tables/tables_27_1.jpg", "caption": "Table 16: Compared with state-of-the-art models on ImageNet-1k", "description": "This table compares the performance of DeiT-S with GFSA against other state-of-the-art models on the ImageNet-1k dataset.  It shows the input size, number of layers, number of parameters, Top-1 accuracy, and Top-5 accuracy for each model.  The results demonstrate that DeiT-S with GFSA achieves a higher Top-1 accuracy than other comparable models, highlighting the effectiveness of the proposed method.", "section": "5.3 Experiments on Vision Transformers"}, {"figure_path": "ffNrpcBpi6/tables/tables_27_2.jpg", "caption": "Table 17: Experiment results on ImageNet-1k", "description": "This table compares the performance of DeiT-T and DeiT-S models with and without ContraNorm and GFSA on the ImageNet-1k dataset.  The results are broken down by the number of layers (12, 16, and 24) in the model architecture.  It demonstrates how GFSA improves the top-1 accuracy of both DeiT-T and DeiT-S models across different model depths.", "section": "M Image Classification"}, {"figure_path": "ffNrpcBpi6/tables/tables_28_1.jpg", "caption": "Table 13: Sensitivity to K for 12-layer DeiT-S + GFSA", "description": "This table presents the sensitivity analysis of the performance of DeiT-S with GFSA on ImageNet-1K when varying the hyperparameter K (polynomial order) from 2 to 5. It shows the Top-1 accuracy for each value of K, allowing for a direct comparison of the model's performance with different high-order terms.", "section": "5.3 Experiments on Vision Transformers"}, {"figure_path": "ffNrpcBpi6/tables/tables_29_1.jpg", "caption": "Table 1: Results comparison on GLUE benchmark. Avg denotes the average performance.", "description": "This table presents the results of experiments conducted on the GLUE benchmark, a widely used standard dataset for evaluating natural language understanding models.  The table compares the performance of several models: BERT, ALBERT, and RoBERTa, both with and without the proposed GFSA modification.  Each model is evaluated on multiple sub-tasks within the GLUE benchmark and the average score across those tasks is provided. The table also includes the number of parameters for each model, showcasing the minimal parameter overhead of GFSA.  The performance improvements shown highlight GFSA's effectiveness in enhancing natural language understanding.", "section": "5.1 Experiments on Natural Language Understanding"}, {"figure_path": "ffNrpcBpi6/tables/tables_31_1.jpg", "caption": "Table 20: Experimental results and number of parameters on ZINC", "description": "This table presents the Mean Absolute Error (MAE) achieved by Graphormer and Graphormer enhanced with GFSA (Graph Filter-based Self-Attention) on the ZINC dataset.  The results show that adding GFSA improves the MAE, indicating better performance in the task of predicting molecular properties.", "section": "O Graph-level Tasks"}, {"figure_path": "ffNrpcBpi6/tables/tables_31_2.jpg", "caption": "Table 5: Results on PCQM4M and PCQM4Mv2", "description": "This table presents the results of the PCQM4M and PCQM4Mv2 experiments.  It compares the Mean Absolute Error (MAE) achieved by Graphormer and Graphormer enhanced with GFSA (Graph Filter-based Self-Attention).  Results are shown for both training and validation sets, indicating the performance of each model on seen and unseen data.  The number of parameters (#Params) for each model is also provided.", "section": "Experiments on Graph-level Tasks"}, {"figure_path": "ffNrpcBpi6/tables/tables_33_1.jpg", "caption": "Table 1: Results comparison on GLUE benchmark. Avg denotes the average performance.", "description": "This table compares the performance of different models on the GLUE benchmark.  The models include BERT, ALBERT, and RoBERTa, both with and without the proposed GFSA method, and also includes ContraNorm as a comparison.  The table shows the performance on each individual task within the GLUE benchmark (CoLA, SST-2, MRPC, QQP, STS-B, MNLI-m, MNLI-mm, QNLI, RTE) as well as the average performance across all tasks. The number of parameters for each model is also shown.", "section": "5.1 Experiments on Natural Language Understanding"}, {"figure_path": "ffNrpcBpi6/tables/tables_33_2.jpg", "caption": "Table 23: Training time (seconds per epoch) on GLUE tasks. s denotes the abbreviation for second. Avg denotes the average training time across all tasks.", "description": "This table shows the training time in seconds per epoch for each of the GLUE tasks for various language models, both with and without the GFSA enhancement.  It provides a comparison of the computational overhead introduced by GFSA and shows that the increase is relatively small, even for larger models.", "section": "5.1 Experiments on Natural Language Understanding"}, {"figure_path": "ffNrpcBpi6/tables/tables_33_3.jpg", "caption": "Table 24: Training time (seconds per epoch) on causal language modeling tasks.", "description": "This table shows the training time in seconds per epoch for three different causal language modeling datasets (PTB, WikiText-2, and WikiText-103) for both the original GPT2 model and the GPT2 model with the GFSA (Graph Filter-based Self-Attention) layer added. The results demonstrate that adding the GFSA layer increases the training time but provides improved performance, as shown in Table 2 in the main text of the paper.", "section": "L Causal Language Modeling"}, {"figure_path": "ffNrpcBpi6/tables/tables_34_1.jpg", "caption": "Table 3: Results comparison on ImageNet-1k. Our full results with other models are in Appendix M.4.", "description": "This table compares the Top-1 accuracy of various Vision Transformer models on the ImageNet-1k dataset.  It shows the performance of DeiT, CaiT, and Swin Transformer models, both with and without the addition of GFSA (Graph Filter-based Self-Attention). The table also includes input size, number of layers and number of parameters for each model, providing a comprehensive comparison of the models' performance and efficiency.", "section": "5.3 Experiments on Vision Transformers"}, {"figure_path": "ffNrpcBpi6/tables/tables_34_2.jpg", "caption": "Table 26: Training time (seconds per epoch) on graph-level tasks", "description": "This table shows the training time in seconds per epoch for three graph-level tasks (ZINC, PCQM4M, and PCQM4Mv2) using two methods: Graphormer and Graphormer with the proposed GFSA.  It highlights the additional computational cost introduced by GFSA, while also showing that the increase is relatively small.", "section": "5.4 Experiments on Graph-level Tasks"}, {"figure_path": "ffNrpcBpi6/tables/tables_34_3.jpg", "caption": "Table 7: Results for ASR training on LibriSpeech 100h and 960h with GFSA", "description": "This table presents the Word Error Rate (WER) results for Automatic Speech Recognition (ASR) experiments conducted on the LibriSpeech dataset.  Two different Transformer-based models, a vanilla Transformer and Branchformer [59], were used as baselines.  The WER is reported for both the 100-hour and 960-hour subsets of the LibriSpeech dataset, for both clean and \"other\" test sets.  The table also shows the WER results when GFSA is integrated into both base models.  The results demonstrate the improvement achieved by incorporating GFSA.", "section": "5.5 Experiments on Automatic Speech Recognition"}, {"figure_path": "ffNrpcBpi6/tables/tables_34_4.jpg", "caption": "Table 28: Training time (seconds per epoch) on the code defect prediction task", "description": "This table shows the training time for various code classification models, both with and without GFSA.  It demonstrates that while GFSA slightly increases the training time, the increase is relatively small, especially considering the performance improvements it provides.", "section": "5.6 Experiments on Code Classification"}, {"figure_path": "ffNrpcBpi6/tables/tables_35_1.jpg", "caption": "Table 1: Results comparison on GLUE benchmark. Avg denotes the average performance.", "description": "This table compares the performance of different models on the GLUE benchmark, a widely used dataset for evaluating natural language understanding.  It shows the results for several models, including the baseline models (BERT, ALBERT, RoBERTa) and those same models enhanced with the proposed GFSA method. The metrics used to measure performance vary across the different GLUE tasks. The table also shows the number of parameters for each model, providing context on the model size and complexity. The \"Avg\" column represents the average score across all GLUE tasks.  The comparison helps demonstrate the improvement in performance achieved by incorporating GFSA into the models.", "section": "5.1 Experiments on Natural Language Understanding"}, {"figure_path": "ffNrpcBpi6/tables/tables_35_2.jpg", "caption": "Table 11: Results comparison on GPT-2 finetuned with GFSA", "description": "This table compares the perplexity scores achieved by the original GPT2 model and the GPT2 model enhanced with GFSA across three datasets: Penn Treebank (PTB), WikiText-2, and WikiText-103.  The results show the impact of applying GFSA on the causal language modeling performance.  Different values of K (the hyperparameter for GFSA) were tested to evaluate sensitivity.", "section": "5.2 Experiments on Causal Language Modeling"}, {"figure_path": "ffNrpcBpi6/tables/tables_35_3.jpg", "caption": "Table 31: Inference time on ImageNet-1k", "description": "This table shows the inference time in seconds for various Vision Transformer backbones (DeiT, CaiT, Swin) with and without GFSA.  It demonstrates the minimal increase in inference time caused by adding GFSA, highlighting its efficiency.", "section": "M Image Classification"}, {"figure_path": "ffNrpcBpi6/tables/tables_36_1.jpg", "caption": "Table 32: Inference time on graph-level tasks", "description": "This table shows the inference time of Graphormer and Graphormer with GFSA on three graph-level datasets: ZINC, PCQM4M, and PCQM4Mv2.  The inference time is measured in seconds.  The results show that adding GFSA to Graphormer increases the inference time, although the increase is relatively small (less than 20 seconds for all datasets).", "section": "O Graph-level Tasks"}, {"figure_path": "ffNrpcBpi6/tables/tables_36_2.jpg", "caption": "Table 7: Results for ASR training on LibriSpeech 100h and 960h with GFSA", "description": "This table compares the Word Error Rate (WER) achieved by different models on two subsets of the LibriSpeech dataset: 100 hours and 960 hours.  The models compared are a vanilla Transformer, a Transformer enhanced with Graph Filter-based Self-Attention (GFSA), a Branchformer, and a Branchformer enhanced with GFSA. WER is reported for both clean and other test sets. This table demonstrates the effectiveness of GFSA in improving ASR performance on both large and small models.", "section": "5.5 Experiments on Automatic Speech Recognition"}, {"figure_path": "ffNrpcBpi6/tables/tables_36_3.jpg", "caption": "Table 34: Inference time on the code defect prediction task", "description": "This table presents the inference time, in seconds, for various code classification models with and without the proposed GFSA (Graph Filter-based Self-Attention) method.  It shows the inference time for ROBERTA, CodeBERT, PLBART, and CodeT5 (small and base versions) to highlight the impact of GFSA on inference speed in a code defect detection task.  The results demonstrate that while adding GFSA increases inference time slightly, the gains in accuracy outweigh the increased computational cost in most cases.", "section": "5.6 Experiments on Code Classification"}, {"figure_path": "ffNrpcBpi6/tables/tables_37_1.jpg", "caption": "Table 1: Results comparison on GLUE benchmark. Avg denotes the average performance.", "description": "This table compares the performance of different models on the GLUE benchmark.  The models tested include BERT, ALBERT, and RoBERTa, both with and without the GFSA modification.  The table shows the performance on various subtasks of the GLUE benchmark, including CoLA, SST-2, MRPC, QQP, STS-B, MNLI-m/mm, QNLI, and RTE. The \"Avg\" column represents the average performance across all subtasks.  The comparison highlights the improvement in performance achieved by incorporating GFSA into the base Transformer models.", "section": "5.1 Experiments on Natural Language Understanding"}, {"figure_path": "ffNrpcBpi6/tables/tables_37_2.jpg", "caption": "Table 1: Results comparison on GLUE benchmark. Avg denotes the average performance.", "description": "This table compares the performance of different models on the GLUE benchmark.  The models include the baseline BERT, ALBERT, and RoBERTa, as well as versions of these models with ContraNorm and GFSA applied. The table shows the performance on several different tasks within the GLUE benchmark (CoLA, SST-2, MRPC, QQP, STS-B, MNLI-m/mm, QNLI, RTE), as well as the average performance across all tasks.  The number of parameters (#Params) for each model is also listed. GFSA shows significant performance improvements across the board, often exceeding that of ContraNorm.", "section": "5.1 Experiments on Natural Language Understanding"}, {"figure_path": "ffNrpcBpi6/tables/tables_37_3.jpg", "caption": "Table 2: Results comparison on GPT-2 finetuned with GFSA. Avg denotes the average performance.", "description": "This table compares the performance of three different models on three different datasets related to causal language modeling.  The first model is GPT2, a pre-trained large language model. The second model is GPT2 with the addition of Graph Filter-based Self-Attention (GFSA). The third model is GPT2 with GFSA applied only to even-numbered layers (GFSA_even). The datasets used are Penn Treebank (PTB), WikiText-2, and WikiText-103. The table shows the perplexity score for each model on each dataset, as well as the average perplexity across all three datasets.  Perplexity is a measure of how well a language model predicts a sequence of words.", "section": "5.2 Experiments on Causal Language Modeling"}, {"figure_path": "ffNrpcBpi6/tables/tables_37_4.jpg", "caption": "Table 11: Results comparison on GPT-2 finetuned with GFSA", "description": "This table compares the perplexity scores achieved by the original GPT-2 model and GPT-2 models enhanced with GFSA across three different datasets: Penn Treebank (PTB), WikiText-2, and WikiText-103.  The table shows the perplexity for each model and dataset, along with the average perplexity across the three datasets. The results demonstrate the improved performance of GPT-2 with GFSA on these causal language modeling tasks.", "section": "L Causal Language Modeling"}, {"figure_path": "ffNrpcBpi6/tables/tables_37_5.jpg", "caption": "Table 3: Results comparison on ImageNet-1k. Our full results with other models are in Appendix M.4.", "description": "This table presents the comparison of Top-1 accuracy on ImageNet-1k using different Vision Transformers such as DeiT, CaiT, and Swin with and without GFSA.  It shows the impact of GFSA on various model depths (#Layers) and sizes (#Params) by comparing their Top-1 accuracy and runtime.", "section": "Experiments on Vision Transformers"}, {"figure_path": "ffNrpcBpi6/tables/tables_38_1.jpg", "caption": "Table 40: Comparison of accuracy (%), runtime (s per 1,000 steps) and GPU usage (GB) on Long Range Arena benchmark", "description": "This table presents a comparison of the performance of various transformer models on the Long Range Arena benchmark.  The benchmark includes two datasets: ListOps (with 2K samples) and Image (with 4K samples).  The table shows the accuracy, runtime (seconds per 1000 steps), and GPU usage (in GB) for each model. The models compared include the standard Transformer, the Transformer with the proposed GFSA, Linformer, YOSO-E, Efficient Attention, and Efficient Attention with GFSA. The results demonstrate the impact of GFSA on both accuracy and resource usage.  Specifically, the GFSA generally increases accuracy, but with increased runtime and GPU usage, although the improvements are not uniform across models and datasets.", "section": "U GFSA in Linear Transformers"}]