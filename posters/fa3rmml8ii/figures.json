[{"figure_path": "fA3RMMl8ii/figures/figures_1_1.jpg", "caption": "Figure 1: Our method leverages tactile sensing to improve the existing 3D generation pipeline. Left: Given a text prompt, we first generate an image using SDXL [12] and then run Wonder3D [13] to generate mesh from the image. This process often results in a mesh with an overly smooth surface. Right: Our method takes a text prompt and several tactile patches as input and generates high-fidelity coherent visual and tactile textures that can be transferred to different meshes. Our method can easily adapt to image-to-3D tasks, as shown in the rightmost column, with the reference image's thumbnail displayed at the bottom right corner. Please visit our webpage for video results.", "description": "This figure demonstrates the Tactile DreamFusion method. The left side shows the results of existing 3D generation methods from a text prompt (an avocado and a beanie), which lack fine geometric details. The right side illustrates how the proposed method incorporates tactile sensing to significantly improve the realism of generated 3D models by adding high-fidelity textures (avocado texture on a mug and phone case, woven texture on a beanie, toy flower, and miffy bunny). It highlights the ability to transfer these textures to different meshes and adapt to both text-to-3D and image-to-3D generation.", "section": "1 Introduction"}, {"figure_path": "fA3RMMl8ii/figures/figures_2_1.jpg", "caption": "Figure 2: Tactile data capture. We collect one patch by pressing GelSight Mini on an object surface. We use Poisson integration to estimate the contact depth from the sensor output, apply high-pass filtering to extract the high-frequency texture information, and then run the 2D texture synthesis method of Image Quilting [77] to obtain an initial texture map. Finally, we convert the height map back to a normal map.", "description": "This figure illustrates the process of tactile data acquisition and pre-processing. It starts with using GelSight Mini to capture a tactile image from an object's surface. The raw sensor output undergoes depth estimation using Poisson integration. High-pass filtering is then applied to extract high-frequency texture information.  This filtered data is used for 2D texture synthesis using the Image Quilting algorithm, generating an initial texture map. Finally, the height map is converted into a normal map, providing both depth and surface normal information.", "section": "3 Tactile Data Acquisition"}, {"figure_path": "fA3RMMl8ii/figures/figures_3_1.jpg", "caption": "Figure 3: TouchTexture dataset. We collect tactile normal data from 16 daily objects featuring diverse tactile textures. To demonstrate the local geometric intricacies, we show the tactile normal map and a 3D height map for each object. Please refer to the supplement for the full set of our data.", "description": "This figure showcases the TouchTexture dataset used in the Tactile DreamFusion paper.  It displays six example objects from the dataset (avocado, strawberry, canvas bag, striped steel, corn, and rubber) along with their corresponding tactile normal maps and 3D height maps. Each image shows a GelSight sensor capturing a tactile patch from the object surface, illustrating the high-resolution tactile data collected. The normal maps highlight the fine-grained surface geometry details captured by the tactile sensor, while the 3D height maps provide a visual representation of the surface texture. The variety of textures shows the breadth of the dataset.", "section": "3 Tactile Data Acquisition"}, {"figure_path": "fA3RMMl8ii/figures/figures_3_2.jpg", "caption": "Figure 4: Method overview. Given an input image or a text prompt, our method generates a mesh with high-quality visual and normal texture. We first generate a base mesh with albedo texture using a text- or image-to-3D method. We use a 3D texture field with hash encoding to represent albedo and tactile normal textures and train it with loss functions on rendered images. To capture the scale differences between visual and tactile modalities, we sample distinct camera views, P for visual rendering and Pt for tactile rendering. For texture refinement, we train the texture field with a visual matching loss La, to ensure fidelity to the input mesh, and a visual guidance loss with normal-conditioned ControlNet, LVG, to enhance photorealism and cross-modal alignment. We further apply a tactile matching loss, Lt, and a tactile guidance loss, LTG, using a customized Texture Dreambooth, to achieve high-quality geometric details aligned with the distribution of tactile input V* texture exemplars.", "description": "This figure provides a detailed overview of the proposed method's pipeline. It starts with generating a base mesh using existing text-to-3D or image-to-3D methods.  Then, it incorporates tactile data by training a 3D texture field to co-optimize visual and tactile textures using several loss functions. These losses ensure consistency between visual and tactile details, enhance photorealism, and maintain alignment between the two modalities. The process includes refining textures using visual and tactile guidance losses and utilizes a customized Texture Dreambooth to incorporate high-resolution tactile information.", "section": "4 Method"}, {"figure_path": "fA3RMMl8ii/figures/figures_6_1.jpg", "caption": "Figure 5: 3D generation with a single texture. For each object, we show generated albedo (top), normal (middle), and full color (bottom) renderings from two viewpoints. Our method works for both text-to-3D (corn and football) and image-to-3D (potato and strawberry), generating realistic and coherent visual textures and geometric details. (We use roughness=0.5 when rendering color views in Blender for Figure 1, Figure 5, Figure 6, and Figure 7.)", "description": "This figure shows the results of 3D object generation using a single texture, comparing the generated albedo, normal, and full-color renderings from two different viewpoints for four different objects (corn, American football, potato, and strawberry).  The results demonstrate the effectiveness of the method in generating realistic textures and geometric details, and its applicability to both text-to-3D and image-to-3D generation tasks.", "section": "5 Experiment"}, {"figure_path": "fA3RMMl8ii/figures/figures_6_2.jpg", "caption": "Figure 6: Diverse textures with the same object. With additional texture cues from tactile data, we can synthesize diverse textures with the same coarse shape for customized designs.", "description": "This figure shows the results of applying different tactile textures to the same 3D model of a coffee cup.  The top row displays the generated albedo, normal maps, and full-color renderings from two different viewpoints for each texture.  The bottom row shows the same renderings but with a more neutral color scheme, highlighting the geometry details generated from different tactile inputs. This demonstrates the ability of the Tactile DreamFusion method to produce diverse textures while maintaining the same underlying geometry.", "section": "5 Experiment"}, {"figure_path": "fA3RMMl8ii/figures/figures_7_1.jpg", "caption": "Figure 7: Multi-part texture generation. Our method allows users to specify an object (via text or image) and its two parts to assign different textures (color-coded text prompts correspond to text descriptions for two textures). We show paired results for albedo, normal, and full-color renderings. The zoom-in patches demonstrate the generated normal textures on different parts.", "description": "This figure demonstrates the results of the proposed method's ability to generate 3D objects with multiple textures assigned to different parts. Three examples are shown: a cactus in a pot, a goat sculpture, and a lamp.  Each example shows the text prompt used (including the specification of textures for different parts), the generated albedo, the generated normal map, and the rendered full color image from two viewpoints. The zoom-in patches highlight the detail and consistency of the generated normal textures on each part of the objects.  The color coding in the text prompts corresponds to the actual textures applied to the various parts of each object.", "section": "4.3 Multi-Part Textures"}, {"figure_path": "fA3RMMl8ii/figures/figures_8_1.jpg", "caption": "Figure 8: Baseline comparison. Compared to the SOTA image-to-3D (Wonder3D and DreamGaussian) and text-to-3D (DreamCraft3D) baselines, our method produces significantly more plausible low-level geometry. For a fair comparison, we use the same input image for the first three rows.", "description": "This figure compares the results of the proposed method against three state-of-the-art baselines for image-to-3D and text-to-3D generation.  The comparison shows that the proposed method generates more realistic and detailed low-level geometric textures compared to the baselines, especially concerning fine-grained surface details. The same input images are used for a fair comparison in the first three rows.", "section": "5 Experiment"}, {"figure_path": "fA3RMMl8ii/figures/figures_9_1.jpg", "caption": "Figure 9: Ablation study regarding texture refinement. We ablate our method regarding the tactile guidance loss LTG and visual guidance loss LVG. Removing LTG results in fewer details of the generated tactile texture. Removing the visual guidance introduces misaligned visual and tactile normal textures. For example, the bumps in the normal map are misaligned with the locations of white seeds in the albedo rendering, as shown in the zoomed-in patches. Our method encourages the generated color to be consistent with the tactile normal using ControlNet-guided visual refinement loss while also enhancing the details in the tactile texture.", "description": "This figure shows an ablation study on the tactile DreamFusion model. Three rows show results for the full model, a model without tactile guidance, and a model without visual guidance.  The results demonstrate the importance of both visual and tactile guidance for generating high-fidelity geometric details and color alignment between visual and tactile modalities.  Without tactile guidance, the resulting tactile texture lacks detail.  Without visual guidance, there is a misalignment between the visual and tactile textures, indicating a lack of proper alignment between visual and tactile normal maps. The image highlights the impact of each component on the overall quality and realism of the generated 3D assets.", "section": "5 Experiment"}, {"figure_path": "fA3RMMl8ii/figures/figures_9_2.jpg", "caption": "Figure 10: Ablation study regarding tactile preprocessing. Without tactile data preprocessing including high-pass filtering and contact area cropping, the generated geometric details tend to be flat and unrealistic.", "description": "This figure shows an ablation study on the effect of tactile data preprocessing. The top row displays the results using the proposed method with preprocessing steps such as high-pass filtering and contact area cropping, which helps to generate realistic geometric details. The bottom row shows the results without preprocessing, demonstrating that the geometric details become flat and less realistic without these steps.  This highlights the importance of proper preprocessing of tactile data for achieving high-fidelity 3D generation.", "section": "4 Method"}, {"figure_path": "fA3RMMl8ii/figures/figures_9_3.jpg", "caption": "Figure 11: Ablation study regarding tactile input. We remove the tactile input while keeping the refinement loss. Without the tactile information, the generated 3D assets fail to capture fine-grained geometric details.", "description": "This figure demonstrates the importance of tactile input in generating high-fidelity 3D models with fine-grained geometric details.  By comparing the results of the proposed method with and without tactile input, it highlights how the tactile information significantly improves the realism and accuracy of the generated 3D assets' textures. The absence of tactile input results in overly smooth surfaces, lacking the intricate detail captured when tactile data is used.", "section": "Experiment"}, {"figure_path": "fA3RMMl8ii/figures/figures_16_1.jpg", "caption": "Figure 12: Illustration of diffusion-based multi-parts segmentation. At each iteration, we run a forward pass of the diffusion model for the target albedo image, aggregate its self-attention and cross-attention maps, and compute KL distance to merge the masks into N parts. We show an example of \"a cactus in a pot\", where we extract the masks corresponding to two tokens \"cactus\" and \"pot\", shown as Mask A and Mask B. The segmentation masks are then used to supervise the label field training to enable multi-part synthesis.", "description": "This figure illustrates the process of multi-part segmentation using diffusion models.  The input is an image of a cactus in a pot.  The model processes the image using its attention maps to create two separate masks\u2014one for the cactus and one for the pot. These masks are then used to guide a label field training process, allowing the model to generate different textures for each part of the object (cactus and pot).", "section": "4.3 Multi-Part Textures"}, {"figure_path": "fA3RMMl8ii/figures/figures_16_2.jpg", "caption": "Figure 3: TouchTexture dataset. We collect tactile normal data from 16 daily objects featuring diverse tactile textures. To demonstrate the local geometric intricacies, we show the tactile normal map and a 3D height map for each object. Please refer to the supplement for the full set of our data.", "description": "This figure shows 16 daily objects with their corresponding tactile normal maps and 3D height maps, showcasing the dataset used in the study called TouchTexture.  Each object displays a different surface texture to highlight the diversity of tactile information captured.  The dataset serves as input to the proposed method, enabling the 3D model to generate detailed surface textures and geometry.", "section": "3 Tactile Data Acquisition"}, {"figure_path": "fA3RMMl8ii/figures/figures_17_1.jpg", "caption": "Figure 1: Our method leverages tactile sensing to improve the existing 3D generation pipeline. Left: Given a text prompt, we first generate an image using SDXL [12] and then run Wonder3D [13] to generate mesh from the image. This process often results in a mesh with an overly smooth surface. Right: Our method takes a text prompt and several tactile patches as input and generates high-fidelity coherent visual and tactile textures that can be transferred to different meshes. Our method can easily adapt to image-to-3D tasks, as shown in the rightmost column, with the reference image's thumbnail displayed at the bottom right corner. Please visit our webpage for video results.", "description": "This figure demonstrates the core idea of the Tactile DreamFusion method.  The left side shows the limitations of existing text-to-3D methods, which produce overly smooth surfaces. The right side showcases the improvement achieved by incorporating tactile sensing, resulting in higher fidelity and more realistic geometric details in the generated 3D models.  Both text-to-3D and image-to-3D applications are depicted.", "section": "1 Introduction"}, {"figure_path": "fA3RMMl8ii/figures/figures_18_1.jpg", "caption": "Figure 1: Our method leverages tactile sensing to improve the existing 3D generation pipeline. Left: Given a text prompt, we first generate an image using SDXL [12] and then run Wonder3D [13] to generate mesh from the image. This process often results in a mesh with an overly smooth surface. Right: Our method takes a text prompt and several tactile patches as input and generates high-fidelity coherent visual and tactile textures that can be transferred to different meshes. Our method can easily adapt to image-to-3D tasks, as shown in the rightmost column, with the reference image's thumbnail displayed at the bottom right corner. Please visit our webpage for video results.", "description": "This figure shows a comparison between the output of a standard 3D generation pipeline and the proposed method. The standard pipeline produces a smooth 3D model from a text or image input. The proposed method incorporates tactile data to generate a model with much finer details and more realistic textures. The method's adaptability to image-to-3D generation is also showcased.", "section": "1 Introduction"}, {"figure_path": "fA3RMMl8ii/figures/figures_19_1.jpg", "caption": "Figure 1: Our method leverages tactile sensing to improve the existing 3D generation pipeline. Left: Given a text prompt, we first generate an image using SDXL [12] and then run Wonder3D [13] to generate mesh from the image. This process often results in a mesh with an overly smooth surface. Right: Our method takes a text prompt and several tactile patches as input and generates high-fidelity coherent visual and tactile textures that can be transferred to different meshes. Our method can easily adapt to image-to-3D tasks, as shown in the rightmost column, with the reference image's thumbnail displayed at the bottom right corner. Please visit our webpage for video results.", "description": "This figure shows a comparison of 3D object generation methods. The left side shows the results of a standard text-to-3D pipeline, resulting in a smooth object. The right side shows the results of the proposed method, which incorporates tactile sensing to generate high-fidelity geometric details. The figure demonstrates the method's ability to generate realistic textures and its adaptability to image-to-3D tasks.", "section": "1 Introduction"}, {"figure_path": "fA3RMMl8ii/figures/figures_19_2.jpg", "caption": "Figure 1: Our method leverages tactile sensing to improve the existing 3D generation pipeline. Left: Given a text prompt, we first generate an image using SDXL [12] and then run Wonder3D [13] to generate mesh from the image. This process often results in a mesh with an overly smooth surface. Right: Our method takes a text prompt and several tactile patches as input and generates high-fidelity coherent visual and tactile textures that can be transferred to different meshes. Our method can easily adapt to image-to-3D tasks, as shown in the rightmost column, with the reference image's thumbnail displayed at the bottom right corner. Please visit our webpage for video results.", "description": "This figure shows a comparison between the traditional 3D generation pipeline (left) and the proposed method that incorporates tactile sensing (right). The traditional method often produces overly smooth surfaces, while the proposed method generates high-fidelity meshes with realistic geometric details by incorporating tactile information.  The figure demonstrates the application of the method to both text-to-3D and image-to-3D tasks.", "section": "1 Introduction"}]