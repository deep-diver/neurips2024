[{"type": "text", "text": "Why the Metric Backbone Preserves Community Structure ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Maximilien Dreveton Charbel Chucri EPFL EPFL maximilien.dreveton@epfl.ch charbel.chucri@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Matthias Grossglauser Patrick Thiran EPFL EPFL matthias.grossglauser@epfl.ch patrick.thiran@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The metric backbone of a weighted graph is the union of all-pairs shortest paths. It is obtained by removing all edges $(u,v)$ that are not on the shortest path between $u$ and $v$ . In networks with well-separated communities, the metric backbone tends to preserve many inter-community edges, because these edges serve as bridges connecting two communities, but tends to delete many intra-community edges because the communities are dense. This suggests that the metric backbone would dilute or destroy the community structure of the network. However, this is not borne out by prior empirical work, which instead showed that the metric backbone of real networks preserves the community structure of the original network well. In this work, we analyze the metric backbone of a broad class of weighted random graphs with communities, and we formally prove the robustness of the community structure with respect to the deletion of all the edges that are not in the metric backbone. An empirical comparison of several graph sparsification techniques confirms our theoretical finding and shows that the metric backbone is an efficient sparsifier in the presence of communities. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph clustering partitions the vertex set of a graph into non-overlapping groups, so that the vertices of each group share some typical pattern or property. For example, each group might be composed of vertices that interact closely with each other. Graph clustering is one of the main tasks in the statistical analysis of networks [4, 30]. ", "page_idx": 0}, {"type": "text", "text": "In many scenarios, the observed pairwise interactions are weighted. In a proximity graph, these weights measure the degree of similarity between edge endpoints (e.g., frequency of interaction in a social network), whereas in a distance graph, they measure dissimilarity instead (for example, the length of segments in a road network or travel times in a flight network). To avoid confusion, we refer to the weights in a distance graph as costs, so that the cost of a path will be naturally defined as the sum of the edge costs on this path.1 ", "page_idx": 0}, {"type": "text", "text": "Distance graphs obtained from real-world data typically violate the triangle inequality. More precisely, the shortest distance between two vertices in the graph is not always equal to the cost of the direct edge, but rather equal to the cost of an indirect path via other vertices. For example, the least expensive flight between two cities is often a flight including one or more layovers. An edge whose endpoints are connected by an indirect shorter path is called semi-metric; otherwise, it is called metric. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We obtain the metric backbone of a distance graph by removing all its semi-metric edges. It has been experimentally observed that the metric backbone retains only a small fraction of the original edges, typically between $10\\%$ and $30\\%$ in social networks [35]. Properties of the original graph that depend only on the shortest paths (such as connectivity, diameter, and betweenness centrality) are preserved by its metric backbone. Moreover, experiments on contact networks indicate that other properties (such as spreading processes and community structures) are also empirically well approximated by computing them on the metric backbone, rather than on the original graph [8]. ", "page_idx": 1}, {"type": "text", "text": "The preservation of these properties by the backbone highlights a well-known empirical feature of complex networks: redundancy. This is the basis for graph sparsification: the task of building a sparser graph from a given graph so that important structural properties are approximately preserved while reducing storage and computational costs. Many existing network sparsifiers identify the statistically significant edges of a graph by comparing them to a null-model [34, 43]. These methods typically require hyperparameters and can leave some vertices isolated (by removing all edges from a given vertex). Spectral sparsification aims at preserving the spectral structure of a graph but also relies on a hyperparameter, and the edges in the sparsified graph might have different weights than in the original graph [37]. In contrast, the metric backbone is parameter-free and automatically preserves all properties linked to the shortest path structure. Moreover, all-pairs shortest paths can be efficiently computed [14, 33]. This makes the metric backbone an appealing mechanism for sparsification. ", "page_idx": 1}, {"type": "text", "text": "Among all the properties empirically shown to be well preserved by the metric backbone, the community structure is perhaps the most surprising. Indeed, if a network contains dense communities that are only loosely connected by a small number of inter-community edges, then all shortest paths between vertices in different communities must go through one of these edges. This suggests that these \"bridge\" edges are less likely to be semi-metric than intra-community edges, where the higher density provides shorter alternative paths.2 This in turn implies that metric sparsification should thin out intra-community edges more than inter-community edges, thereby diluting the community structure. The central contribution of this paper is to show that this intuition is wrong. ", "page_idx": 1}, {"type": "text", "text": "To do so, we formally characterize the metric backbone of a weighted stochastic block model (wSBM). We assume that the vertices are separated into $k$ blocks (also referred to as clusters or communities). An edge between two vertices belonging to blocks $a$ and $b$ is present with probability $p_{a b}$ and is independent of the presence or absence of other edges. This edge, if present, has a weight sampled from a distribution with cdf $F_{a b}$ , and this weight represents the cost of traveling through this edge. We denote by $p_{a b}^{\\mathrm{mb}}$ the probability that an edge between a vertex in block $a$ and a vertex in block $b$ is present in the backbone.3 Under loose assumptions on the costs distributions $F_{a b}$ and on the probabilities $p_{a b}$ , we show that $p_{a b}^{\\mathrm{mb}}/p_{c d}^{\\mathrm{mb}}=(1+\\bar{o}(1))p_{a b}/p_{c d}$ for every $a,b,c,d\\in[k]$ . This shows that the metric backbone thins the edge set approximately uniformly and, therefore, preserves the community structure of the original graph. Moreover, we also prove that a spectral algorithm recovers almost exactly the communities. ", "page_idx": 1}, {"type": "text", "text": "We also conduct numerical experiments with several graph clustering algorithms and datasets to back up these theoretical results. We show that all clustering algorithms achieve similar accuracy on the metric backbone as on the original network. These simulations, performed on different types of networks and with different clustering algorithms, generalize the experiments of [8], which are restricted to contact networks and the Louvain algorithm. Another type of graph we consider are graphs constructed from data points in a Euclidean space, typically by a kernel similarity measure between $q$ -nearest neighbors $\\mathit{\\Delta}_{\\mathit{g}}$ -NN) [15]. This is an important technique for graph construction, with applications in non-linear embedding and clustering. Although $q$ controls the sparsity of this embedding graph, this procedure is not guaranteed to produce only metric edges, and varying $q$ often impacts the clustering performance. By investigating the metric backbone of $q$ -NN graphs, we notice that it makes clustering results more robust against the value of $q$ . Consequently, leveraging graph sparsification alongside $q$ -NN facilitates graph construction. ", "page_idx": 1}, {"type": "text", "text": "The paper is structured as follows. We introduce the main definitions and theoretical results on the metric backbone of wSBMs in Section 2. We discuss these results in Section 3 and compare them with the existing literature. Sections 4 and 5 are devoted to numerical experiments and applications. Finally, we conclude in Section 6. ", "page_idx": 2}, {"type": "text", "text": "Code availability We provide the code used for the experiments: https://github.com/ Charbel-11/Why-the-Metric-Backbone-Preserves-Community-Structure ", "page_idx": 2}, {"type": "text", "text": "Notations The notation $1_{n}$ denotes the vector of size $n\\times1$ whose entries are all equal to one. For any vector $\\pi~\\in~\\mathbb{R}^{n}$ and any matrix $\\textit{B}\\in\\ \\mathbb{R}^{n\\times m}$ , we denote by $\\pi_{\\mathrm{min}}\\;=\\;\\operatorname*{min}_{a\\in[n]}\\,\\pi_{a}$ and $B_{\\mathrm{min}}=\\operatorname*{min}_{a,b}B_{a b}$ , and similarly for $\\pi_{\\mathrm{max}}$ and $B_{\\mathrm{max}}$ . Given two matrices $A$ and $B$ of the same size, we denote by $A\\odot B$ the entry-wise matrix product (i.e., their Hadamard product). $A^{T}$ is the transpose of a matrix $A$ . For a vector $\\pi$ , we denote $\\mathrm{diag}(\\pi)$ the diagonal matrix whose diagonal element $(a,a)$ is $\\pi_{a}$ . ", "page_idx": 2}, {"type": "text", "text": "The indicator of an event $A$ is denoted $\\mathbb{1}\\{A\\}$ . Binomial and exponential random variables are denoted by $\\operatorname{Bin}(n,p)$ and $\\mathrm{Exp}(\\lambda)$ , respectively. The uniform distribution over an interval $[a,b]$ is denoted ${\\mathrm{Unif}}(a,b)$ . Finally, given a cumulative distribution function $F$ , we write $X\\sim F$ for a random variable $X$ sampled from $F$ , i.e., $\\mathbb{P}(X\\leq x)=F(x)$ , and we denote by $f$ the pdf of this distribution. We write whp (with high probability) for events with probability tending to 1 as $n\\to\\infty$ . ", "page_idx": 2}, {"type": "text", "text": "2 The Metric Backbone of Weighted Stochastic Block Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Definitions and Main Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $G=(V,E,c)$ be an undirected weighted graph, where $V=[n]$ is the set of vertices, $E\\subseteq V\\times V$ is the set of undirected edges, and $c\\colon E\\to\\mathbb{R}_{+}$ is the cost function.The cost of a path $(u_{1},\\cdot\\cdot\\cdot,u_{p})$ is naturally defined as $\\begin{array}{r}{\\sum_{q=1}^{p-1}c(u_{q},u_{q+1})}\\end{array}$ , and a shortest path between $u$ and $v$ is a path of minimal cost starting from vertex $u$ and finishing at vertex $v$ . We define the metric backbone as the union of all shortest paths of $G$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1. The metric backbone of a weighted graph $G=(V,E,c)$ where $c$ represents the edge cost is the subgraph $G^{\\mathrm{mb}}\\,=\\,(V,E^{\\mathrm{mb}},c^{\\mathrm{mb}})$ of $G$ , where $E^{\\mathrm{mb}}\\subseteq E$ is such that $e\\in E^{\\mathrm{mb}}$ if and only if $e$ belongs to a shortest path from two vertices in $G$ , and $c^{\\mathrm{mb}}\\colon E^{\\mathrm{mb}}\\rightarrow\\mathbb{R}_{+};e\\mapsto c(e)$ is the restriction of $c$ to $E^{\\mathrm{mb}}$ . ", "page_idx": 2}, {"type": "text", "text": "We investigate the structure of the metric backbone of weighted random graphs with community structure. We generate these graphs as follows. Each vertex $u\\in[n]$ is randomly assigned to the cluster $a\\in[k]$ with probability $\\pi_{a}$ . We denote by $z_{u}\\in[k]$ the cluster of vertex $u$ . Conditioned on $z_{u}$ and $z_{v}$ , an edge $(u,v)$ is present with probability $p_{z_{u}\\,z_{v}}$ , independently of the presence or absence of other edges. If an edge $(u,v)$ is present, it is assigned a cost $c(u,v)$ . The cost $c(u,v)$ is sampled from $F_{z_{u}z_{v}}$ where $F\\,=\\,(F_{a b})_{1\\leq a,b\\leq k}$ denotes a collection of cumulative distribution functions such that $F_{a b}=F_{b a}$ . This defines the weighted stochastic block model, and we denote $(z,G)\\sim\\mathrm{wSBM}(n,\\pi,p,F)$ with $G=([n],E,c)$ , $z\\in[k]^{n}$ and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{P}\\left(z\\right)\\,=\\,\\displaystyle\\prod_{u=1}^{n}{\\pi_{z_{u}}},}\\\\ {\\mathbb{P}\\left(E\\left|\\,z\\right)\\,=\\,\\displaystyle\\prod_{1\\le u<v\\le n}p_{z_{u}z_{v}}^{1\\{\\{u,v\\}\\in E\\}}\\left(1-p_{z_{u}z_{v}}\\right)^{1\\{\\{u,v\\}\\notin E\\}},}\\\\ {\\mathbb{P}\\left(c\\,|\\,E,z\\right)\\,=\\,\\displaystyle\\prod_{\\{u,v\\}\\in E}\\mathbb{P}\\left(c(u,v)\\,|\\,z_{u},z_{v}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and $c(u,v)\\,|\\,z_{u}=a,z_{v}=b$ is sampled from $F_{a b}$ . ", "page_idx": 2}, {"type": "text", "text": "The wSBM is a direct extension of the standard (non-weighted) SBM into the weighted setting. SBM is the predominant benchmark for studying community detection and establishing theoretical guarantees of recovery by clustering algorithms [1]. Despite its shortcomings, such as a low clustering coefficient, the SBM is analytically tractable and is a useful model for inference purposes [32]. ", "page_idx": 2}, {"type": "text", "text": "Throughout this paper, we will make the following assumptions. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (Asymptotic scaling). The edge probabilities $p_{a b}$ between blocks $a$ and $b$ can depend on $n$ , such that $p_{a b}=B_{a b}\\rho_{n}$ with $\\bar{\\rho}_{n}=\\omega(n^{\\bar{-}1}\\log n)$ and $B_{a b}$ is independent of $n$ . Furthermore, the number of communities $k$ , the matrix $B$ , the probabilities $\\pi_{a}$ and the cdf $F_{a b}$ are all fixed (independent of $n$ ). We also assume $\\pi_{\\mathrm{min}}>0$ and $B_{\\mathrm{min}}>0$ . ", "page_idx": 3}, {"type": "text", "text": "To rule out some issues, such as edges with a zero cost,4 we also assume that the probability distributions of the costs have no mass at 0. More precisely, we require that the cumulative distribution functions $F_{a b}$ verify $F_{a b}(0)=0$ and $F_{a b}^{\\prime}(0)=\\bar{\\lambda}_{a b}>0$ , where $F^{\\prime}$ denotes the derivative of $F$ (i.e., the pdf). The first condition ensures that the distribution has support $\\mathbb{R}_{+}$ and no mass at 0, and the second ensures that, around a neighborhood of 0, $F_{a b}$ behaves as the exponential distribution $\\mathrm{Exp}(\\lambda_{a b})$ (or as the uniform distribution $\\mathrm{Unif}([0,\\lambda_{a b}^{-1}])$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Condition on the cost distributions). The costs are sampled from continuous distributions, and there exists $\\Lambda=(\\lambda_{a b})_{a,b}$ with $\\lambda_{a b}>0$ such that $F_{a b}(0)=0$ and $F_{a b}^{\\prime}(0)=\\lambda_{a b}$ . ", "page_idx": 3}, {"type": "text", "text": "We define the following matrix ", "page_idx": 3}, {"type": "equation", "text": "$$\nT\\;=\\;[\\Lambda\\odot B]\\:\\mathrm{diag}(\\pi),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B$ and $\\Lambda$ are defined in Assumptions 1 and 2. Finally, we denote by $\\tau_{\\mathrm{min}}$ and $\\tau_{\\mathrm{max}}$ the minimum and maximum entries of the vector $\\tau=T1_{k}$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1. Assume that $\\Lambda=\\lambda1_{k}1_{k}^{T}$ with $\\lambda>0$ . We notice that $\\begin{array}{r}{\\tau_{a}=\\lambda\\sum_{b}\\pi_{b}B_{a b}}\\end{array}$ . Denote by $\\bar{d}=(\\bar{d}_{1},\\cdot\\cdot\\cdot\\,,\\bar{d}_{k})$ the vector whose $a$ -entry $\\bar{d}_{a}$ is the expected degree of a vertex in community $a$ . We have $\\begin{array}{r}{\\bar{d}_{a}=n\\sum_{b}\\pi_{b}p_{a b}}\\end{array}$ . Then $\\tau_{\\mathrm{min}}=\\lambda\\bar{d}_{\\mathrm{min}}(n\\rho_{n})^{-1}$ and $\\tau_{\\mathrm{max}}\\overset{\\bullet}{=}\\lambda\\bar{d}_{\\mathrm{max}}(n\\rho_{n})^{-1}$ , where $\\bar{d}_{\\mathrm{min}}$ and $\\bar{d}_{\\mathrm{max}}$ are the minimum and maximum entries of $\\bar{d}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Cost of Shortest Paths in wSBMs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a path $\\left(u_{1},\\cdot\\cdot\\cdot,u_{p}\\right)$ , recall that its cost is qp=\u221211 c(uq, uq+1), and the hop count is the number of edges composing the path (that is, the hop count of $\\left(u_{1},\\cdot\\cdot\\cdot,u_{p}\\right)$ is $p-1)$ . ", "page_idx": 3}, {"type": "text", "text": "For two vertices $u,v\\in V$ , we denote by $C(u,v)$ the cost of the shortest path from $u$ to $v$ .5 The following proposition provides asymptotics for the cost of shortest paths in wSBMs. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Let $(z,G)\\sim\\mathrm{wSBM}(n,\\pi,p,F)$ . Suppose that Assumptions 1 and 2 hold and let $\\tau_{\\mathrm{min}}$ and $\\tau_{\\mathrm{max}}$ be defined following Equation (2.1). Then, for two vertices u and v chosen uniformly at random in blocks a and $b$ , respectively. We have whp ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(\\tau_{\\mathrm{max}}\\right)^{-1}\\;\\leq\\;\\frac{n\\rho_{n}}{\\log n}C(u,v)\\;\\leq\\;\\left(\\tau_{\\mathrm{min}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To prove Proposition 1, in the first stage, we simplify the problem by assuming exponentially distributed weights. We then analyze two first passage percolation (FPP) processes, originating from vertices $u$ and $v$ , respectively. Using the memoryless property of the exponential distribution, we analyze two first passage percolation (FPP) processes, originating from vertex $u$ and $v$ , respectively. Each FP\u221aP explores the nearest neighbors of its starting vertex until it reaches $q$ neighbors. As long as $q=o({\\sqrt{n}})$ , the two FPP processes remain disjoint (with high probability). Thus, the cost $C(u,v)$ is lower-bounded by the sum of (i) the cost of the shortest path from $u$ to its $q$ -nearest neighbor an\u221ad of (ii) the cost of the shortest path from $v$ to its $q$ -nearest neighbor. On the contrary, when $q=\\omega({\\sqrt{n}})$ , the two FPPs intersect, revealing a path from $u$ to $v$ , and the cost of this path upper-bounds the cost $C(u,v)$ of the shortest path from $u$ to $v$ . In the second stage, we extend the result to general weight distributions by noticing that the edges belonging to the shortest paths have very small costs. Moreover, Assumption 2 yields that the weight distributions behave as an exponential distribution in a neighborhood of 0. We can thus adapt the coupling argument of [23] to show that the edge weights distributions do not need to be exponential, as long as Assumption 2 is verified. The proof of Proposition 1 is provided in Section A. ", "page_idx": 3}, {"type": "text", "text": "2.3 Metric Backbone of wSBMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let $(z,G)\\sim\\mathrm{wSBM}(n,\\pi,p,F)$ and denote by $G^{\\mathrm{mb}}$ the metric backbone of $G$ . Choose two vertices $u$ and $v$ uniformly at random, and notice that the probability that the edge $(u,v)$ is present in $G^{\\mathrm{mb}}$ depends only on $z_{u}$ and $z_{v}$ , and not on $z_{w}$ for $w\\notin\\{u,v\\}$ . Denote by $p_{a b}^{\\mathrm{mb}}$ the probability that an edge between a vertex in community $a$ and a vertex in community $b$ appears in the metric backbone, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{a b}^{\\mathrm{mb}}\\,=\\,\\mathbb{P}\\left((u,v)\\in G^{\\mathrm{mb}}\\,|\\,z_{u}=a,z_{v}=b\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The following theorem shows that the ratio $\\frac{p_{a b}^{\\mathrm{mb}}}{p_{a b}}$ scales as $\\Theta\\left(\\frac{\\log n}{n\\rho_{n}}\\right)$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $(z,G)\\,\\sim\\,\\mathrm{wSBM}(n,\\pi,p,F)$ and suppose that Assumptions $^{\\,l}$ and 2 hold. Let $\\tau_{\\mathrm{min}}$ and $\\tau_{\\mathrm{max}}$ be defined after Equation (2.1). Then ", "page_idx": 4}, {"type": "equation", "text": "$$\n(1+o(1))\\frac{\\lambda_{a b}}{\\tau_{\\mathrm{max}}}\\;\\leq\\;\\frac{n\\rho_{n}}{\\log n}\\frac{p_{a b}^{\\mathrm{mb}}}{p_{a b}}\\;\\leq\\;(1+o(1))\\frac{\\lambda_{a b}}{\\tau_{\\mathrm{min}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We prove Theorem 1 in Appendix B.1. Theorem 1 shows that the metric backbone maintains the same proportion of intra- and inter-community edges as in the original graph. We illustrate the theorem with two important examples. ", "page_idx": 4}, {"type": "text", "text": "Example 1. Consider a weighted version of the planted partition model, where for all $a,b\\in[k]$ we have $\\pi_{a}=1/k$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\nB_{a b}\\;=\\;\\binom{p_{0}}{q_{0}}\\quad\\mathrm{if}\\;a=b,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p_{\\mathrm{0}}$ and $q_{0}$ are constant. Assume that $\\Lambda=\\lambda1_{k}1_{k}^{T}$ with $\\lambda>0$ . Using Remark 1, we have $\\tau_{\\operatorname*{min}}=\\tau_{\\operatorname*{max}}=\\lambda k^{-1}\\left(p_{0}+(k-1)q_{0}\\right)$ , and Theorem 1 states that ", "page_idx": 4}, {"type": "equation", "text": "$$\np^{\\mathrm{mb}}\\,=\\,(1+o(1))\\frac{k p_{0}}{p_{0}+(k-1)q_{0}}\\frac{\\log n}{n}\\quad\\mathrm{~and~}\\quad q^{\\mathrm{mb}}\\,=\\,(1+o(1))\\frac{k q_{0}}{p_{0}+(k-1)q_{0}}\\frac{\\log n}{n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In particular, $\\begin{array}{r}{\\frac{p^{\\mathrm{mb}}}{q^{\\mathrm{mb}}}\\;=\\;(1+o(1))\\frac{p_{0}}{q_{0}}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Example 2. Consider a stochastic block model with edge probabilities $p_{a b}=B_{a b}\\rho_{n}$ such that the vertices of different communities have the same expected degree $\\bar{d}$ . If $\\Lambda=\\lambda1_{k}1_{k}^{T}$ , then ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{a b}^{\\mathrm{mb}}\\;=\\;(1+o(1))\\frac{B_{a b}}{\\bar{d}}\\frac{\\log n}{n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2.4 Recovering Communities from the Metric Backbone ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we prove that a spectral algorithm on the (weighted) adjacency matrix of the metric backbone of a wSBM asymptotically recovers the clusters whp. Given an estimate $\\hat{z}\\in[k]^{n}$ of the clusters $z\\in[k]^{n}$ , we define the loss as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{loss}(z,\\hat{z})\\;=\\;\\frac{1}{n}\\operatorname*{inf}_{\\sigma\\in\\mathrm{Sym}(k)}\\operatorname{Ham}\\left(z,\\sigma\\circ\\hat{z}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where Ham denotes the Hamming distance and $\\operatorname{Sym}(k)$ is the set of all permutations of $[k]$ . ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Spectral Clustering on the weighted adjacency matrix of the metric backbone Input: Graph $G$ , number of clusters $k$ Output: Predicted community memberships $\\hat{z}\\in[k]^{n}$   \n1 Denote $W^{\\mathrm{mb}}\\in\\mathbb{R}_{+}^{n\\times n}$ the weighted adjacency matrix of the metric backbone $G^{\\mathrm{mb}}$ of $G$   \n2 Ldete $\\begin{array}{r}{W^{\\mathrm{mb}}=\\sum_{i=1}^{n}\\sigma_{i}u_{i}u_{i}^{T}}\\end{array}$ p aonsidt ieoing eonf $W^{\\mathrm{mb}}$ ,s ordered in $(|\\sigma_{1}|\\geq\\vdots\\cdot\\cdot\\geq|\\sigma_{n}|)$ $u_{1},\\cdot\\cdot\\cdot\\mathrm{\\Omega},u_{n}\\in\\mathbb{R}^{n}$   \n3 Denote $U=[u_{1},\\cdot\\cdot\\cdot\\,,u_{k}]\\in\\mathbb{R}^{n\\times k}$ and $\\Sigma=\\mathrm{diag}{(\\sigma_{1},\\cdot\\cdot\\cdot\\cdot,\\sigma_{k})}$   \n4 Let $\\hat{z}\\in[k]^{n}$ be a $(1+\\epsilon)$ -approximate solution of $k$ -means performed on the rows of $U\\in\\mathbb{R}^{n\\times k}$ ", "page_idx": 4}, {"type": "text", "text": "The following theorem states that, as long as the matrix $T$ defined in (2.1) is invertible, the loss of spectral clustering applied on the metric backbone asymptotically vanishes whp. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let $(z,G)\\,\\sim\\,\\mathrm{wSBM}(n,\\pi,p,F)$ and suppose that Assumptions 1 and 2 hold. Let \u00b5 be the minimal absolute eigenvalue of the matrix $T$ defined in (2.1). Moreover, assume that $\\tau_{\\operatorname*{max}}=\\tau_{\\operatorname*{min}}$ and $\\mu\\neq0$ . Then, the output $\\hat{z}$ of Algorithm $^{\\,l}$ on $G$ verifies whp ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\cos(z,\\hat{z})\\:=\\:{\\cal O}\\left(\\frac{1}{\\mu^{2}\\,\\log n}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We prove Theorem 2 in Appendix B.2. We saw in Example 1 and 2 that the condition $\\tau_{\\operatorname*{max}}=\\tau_{\\operatorname*{min}}$ is verified in several important settings. The additional assumption $\\mu\\neq0$ (equivalent to $T$ being invertible) also often holds: in the planted partition model of Example 1, $T$ is invertible if $p_{0}\\neq q_{0}$ . ", "page_idx": 5}, {"type": "text", "text": "3 Comparison with Previous Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The metric backbone has been introduced under different names, such as the essential subgraph [29], the transport overlay network [41], or simply the union of shortest path trees [39]. In this section, we discuss our contribution with respect to closely related earlier works. ", "page_idx": 5}, {"type": "text", "text": "3.1 Computing the Metric Backbone ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Computing the metric backbone requires solving the All Pairs Shortest Path (APSP) problem, a classic and fundamental problem in computer science. Simply running Dijkstra\u2019s algorithm on each vertex of the graph solves the APSP in $O^{\\pm}(n m+n^{2}\\log n)$ worst-case time, where $m=|E|$ is the number of edges in the original graph [14], whereas [29] proposed an algorithm running in $O(n m^{\\prime}+n^{2}\\log n)$ worst-case time, where $m^{\\prime}$ is the number of edges in the metric backbone. APSP has also been studied in weighted random graphs in which the weights are independent and identically distributed [20, 16]. In particular, the APSP can be solved in $O(n^{2})$ time with high probability on complete weighted graphs whose weights are drawn independently and uniformly at random from $[0,1]$ [33]. ", "page_idx": 5}, {"type": "text", "text": "However, practical implementations of APSP can achieve faster results. For example, in [24], an empirical observation regarding the low hop count of shortest paths is leveraged to compute the metric backbone efficiently. Although exact time complexity is not provided, the implementation scales well for massive graphs, such as a Facebook graph with 190 million nodes and 49.9 billion edges, and the empirical running time appears to be linear with the number of edges [24, Table 1 and Figure 5]. Additionally, our simulations reveal that some popular clustering algorithms such as spectral and subspace clustering have higher running times than computing the metric backbone. ", "page_idx": 5}, {"type": "text", "text": "3.2 First-Passage Percolation in Random Graphs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To study the metric backbone theoretically, we need to understand the structure of the shortest path between vertices in a random graph. This classical and fundamental topic of probability theory is known as first-passage percolation (FPP) [19]. The paper [23] originally studied the weights and hop counts of shortest paths on the complete graph with iid weights. This was later generalized to Erdo\u02dds-R\u00e9nyi graphs and configuration models (see, for example, [28, 12] and references therein). ", "page_idx": 5}, {"type": "text", "text": "Closer to our setting, [25] studied the FPP on inhomogeneous random graphs. Indeed, SBMs are a particular case of inhomogeneous random graphs, for which the set of vertex types is discrete (we refer to [7] for general statements on inhomogeneous random graphs). Assuming that the edge weights are independent and $\\mathrm{Exp}(\\lambda)$ -distributed, [25] established a central limit theorem of the weight and hop count of the shortest path between two vertices chosen uniformly at random among all vertices. Using the notation of Section 2, this result implies that $\\frac{n\\rho_{n}}{\\log n}C(u,v)$ converges in probability to $\\tilde{\\tau}^{-1}$ , where $\\tilde{\\tau}$ is the Perron-Frobenius eigenvalue of $\\lambda{\\cal B}\\,\\mathrm{diag}(\\pi)$ . ", "page_idx": 5}, {"type": "text", "text": "The novelty of our work is two-fold. First, we allow different cost distributions for each pair of communities, whereas all previous works in FPP on random graphs assume that the costs are sampled from a single distribution. Furthermore, we examine the cost of a path between two vertices, $u$ and $v$ , chosen uniformly at random among vertices in block a and in block $b$ , respectively. This differs from previous work (and, in particular, [25]) in which vertices $u$ and $v$ are selected uniformly at random among all vertices. As a result, even for a single cost distribution, Proposition 1 cannot be obtained directly from [25, Theorem 1.2]. This difference is key, as this proposition is required to establish Theorem 1. ", "page_idx": 5}, {"type": "text", "text": "The closest result to Theorem 1 appearing in the literature is [39, Corollary 1]; it establishes a formula for the probability $p_{u v}^{\\mathrm{mb}}$ that an edge between two vertices $u$ and $v$ exists in the metric backbone of a random graph whose edge costs are iid. This previous work does not focus on community structure, so the costs are sampled from a single distribution. More importantly, the expression of $p_{u v}^{\\mathrm{mb}}$ given by [39, Theorem 2 and Corollary 1] is mainly of theoretical interest (and we use it in the proof of Theorem 1). Indeed, understanding the asymptotic behavior of $p_{u v}^{\\mathrm{mb}}$ requires a complete analysis of the cost $C(u,v)$ of the shortest path between $u$ and $v$ . [39] propose such an analysis only in one simple scenario (namely, a complete graph with iid exponentially distributed costs). ", "page_idx": 6}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we test whether the metric backbone preserves a graph community structure in various real networks for which a ground truth community structure is known (see Table 2 in Appendix C.1 for an overview). ", "page_idx": 6}, {"type": "text", "text": "As in many weighted networks, such as social networks, the edge weights represent a measure (e.g., frequency) of interaction between two entities over time, we need to preprocess these proximity graphs into distance graphs. More precisely, given the original (weighted or unweighted) graph $G=(V,E,s)$ , where the weights measure the similarities between pairs of vertices, we define the proximity $p(u,v)$ of vertices $u$ and $v$ as the weighted Jaccard similarity between the neighborhoods of $u$ and $v,i.e.$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\np(u,v)\\;=\\;\\frac{\\sum_{w\\in\\mathrm{Nei}(u)\\cap\\mathrm{Nei}(v)}\\operatorname*{min}\\{s(u,w),s(v,w)\\}}{\\sum_{u\\in\\mathrm{Nei}(u)\\cup\\mathrm{Nei}(v)}\\operatorname*{max}\\{s(u,w),s(v,w)\\}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ${\\mathrm{Nei}}(u)=\\{w\\in V\\colon(u,w)\\in E\\}$ denotes the neighborhood of $u,i.e.$ ., the vertices connected to $u$ by an edge. If $G$ is unweighted $(s(e)=1$ for all $e\\in E$ ), we simply recover the Jaccard index ||NNeeii((uu))\u2229\u222aNNeeii((vv))||. We note that other choices for normalization could have been made, such as the Adamic-Adar index [2]. We refer to [10] for an in-depth discussion of similarity and distance indices. ", "page_idx": 6}, {"type": "text", "text": "Once the proximity graph $G\\,=\\,(V,E,p)$ has been computed, we construct the distance graph $D=(V,E,c)$ where $c\\colon E\\to\\mathbb{R}_{+}$ is such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall e\\in E:\\;c(e)\\;=\\;\\frac{1}{p(e)}-1.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This is the simplest and most commonly used method for converting a similarity to a distance [36]. ", "page_idx": 6}, {"type": "text", "text": "We then compute the set $E^{\\mathrm{mb}}$ of metric edges of the distance graph $D$ , and let $G^{\\mathrm{mb}}=(V,E^{\\mathrm{mb}},p^{\\mathrm{mb}})$ where $p^{\\mathrm{mb}}\\colon E^{\\mathrm{mb}}\\to[0,1];e\\mapsto p(e)$ is the restriction of $p$ to $E^{\\mathrm{mb}}$ . ", "page_idx": 6}, {"type": "text", "text": "We will also consider the two following sparsifications (with the corresponding restrictions of $c$ to the sparsified edge sets) to compare the resulting community structure: ", "page_idx": 6}, {"type": "text", "text": "\u2022 the threshold graph $G^{\\theta}=(V,E^{\\theta},p^{\\theta})$ , where an edge $e\\in E$ is kept in $E^{\\theta}$ iff $p(e)\\geq\\theta$ ; \u2022 the graph $G^{\\mathrm{ss}}=(V,E^{\\mathrm{ss}},p^{\\mathrm{ss}})$ obtained by spectral sparsification on $G$ . We use the SpielmanSrivastava sparsification [37], implemented in the PyGSP package [13]. ", "page_idx": 6}, {"type": "text", "text": "For both threshold and spectral sparsification, we tune the hyperparameters so that the number of edges kept is the same as in the metric backbone: $|E^{\\mathrm{mb}}|=|{\\dot{E}}^{\\hat{\\theta}}|\\stackrel{>}{=}|E^{\\mathrm{ss}}|$ . We provide in Table 3 (in Appendix) some statistics on the percentage of edges remaining in the sparsified graphs. ", "page_idx": 6}, {"type": "text", "text": "For each proximity graph $G$ and its three sparsifications $G^{\\mathrm{mb}}$ , $G^{\\theta}$ , and $G^{\\mathrm{ss}}$ , we run a graph clustering algorithm to obtain the respective predicted clusters $\\hat{z}$ , $\\hat{z}^{\\mathrm{mb}}$ , $\\hat{z}^{\\theta}$ and $\\hat{z}^{\\mathrm{ss}}$ . We show, in Figure 1, the adjusted Rand index6 (ARI) obtained between the ground truth communities and the predicted clusters for three widely used graph clustering algorithms: Bayesian algorithm [31], Leiden algorithm [38] and spectral clustering [40]. We use the graph-tool implementation for the Bayesian algorithm, with an exponential prior for the weight distributions. The Leiden algorithm is implemented at https://github.com/vtraag/leidenalg. For spectral clustering, we assume that the algorithm knows the correct number of clusters in advance, and we use the implementation from scikit-learn. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "Kx8I0rP7w2/tmp/d107fcfbe2c215249895f8e3543a1b3c42180dadcfddafc033fce015c7069ba1.jpg", "img_caption": ["Figure 1: Effect of sparsification on the performance of clustering algorithms on various data sets. We observe that the metric backbone and the spectral sparsification retain equally well the community structure across all data sets and for all clustering algorithms tested. Thresholding often yields several disconnected components of small sizes, impacting the performance of clustering algorithms on $G^{\\theta}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We highlight the difference between the metric backbone and the threshold subgraph of the Primary school data set in Figure 2. We observe, in Figure 2a, that the edges in red (which are present in the backbone but not in the threshold graph) are mostly inter-community edges. On the contrary, in Figure 2b, the blue edges (which are present in the threshold graph but not in the backbone) are mostly intra-community edges. Despite this difference, the metric backbone retains the information about the community structure, as shown in Figure 1. ", "page_idx": 7}, {"type": "image", "img_path": "Kx8I0rP7w2/tmp/84be740df66c1c5da00343e6460f4880108e75aa7e598294ea0be92db60706c8.jpg", "img_caption": ["(b) Threshold Subgraph "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Graphs obtained from Primary school data set, after taking the metric backbone (Figure 2a) and after thresholding (Figure 2b), are drawn using the same layout. Vertex colors represent the true clusters. Edges present in the metric backbone but not in the threshold graph are highlighted in red. Edges present in the threshold graph, but not in the metric backbone, are highlighted in blue. ", "page_idx": 7}, {"type": "text", "text": "5 Application to Graph Construction Using $q$ -NN ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In a large number of machine-learning tasks, data does not originally come from a graph structure, but instead from a cloud of points $x_{1},\\cdot\\cdot\\cdot\\,,x_{n}$ where each $x_{u}$ belongs to a metric space (say $\\mathbb{R}^{d}$ for simplicity). The goal of graph construction is to discover a proximity graph $G=([n],E,p)$ from the original data points. Graph construction is commonly done through the $q$ -nearest neighbors $(q{-}\\Nu\\Nu)$ . Given a similarity function sim: $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}$ that quantifies the resemblance between two data points, we define the set $\\mathcal{N}(u,q)$ of $q$ -nearest neighbors of $u\\in[n]$ . More precisely, $\\mathcal{N}(u,q)$ is the subset of $[n]\\backslash\\{u\\}$ with cardinality $q$ such that for all $v\\in\\mathcal{N}(u,q)$ and for all $w\\not\\in\\mathcal{N}(u,\\dot{q})$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sin(x_{u},x_{v})\\;\\geq\\;\\sin(x_{u},x_{w}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The edge set $E$ is composed of all pairs of vertices $(u,v)$ such that $u\\in\\mathcal{N}(v,q)$ or $v\\in\\mathcal{N}(u,q)$ ,8 and the proximity $p_{u v}$ associated with the edge $(u,v)$ is $(s_{u v}+s_{v u})/2$ , where ", "page_idx": 8}, {"type": "equation", "text": "$$\ns_{u v}\\;=\\;\\left\\{{\\begin{array}{l l}{\\sin(x_{u},x_{v})}&{{\\mathrm{~if~}}v\\in{\\mathcal{N}}(u,q),}\\\\ {0}&{{\\mathrm{~otherwise.}}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In the following, we use the Gaussian kernel similarity $\\begin{array}{r}{\\sin(x_{u},x_{v})=\\exp\\left(-\\frac{\\|x_{u}-x_{v}\\|^{2}}{d_{K}^{2}(x_{u})}\\right)}\\end{array}$ , where $d_{K}(x_{i})$ is the Euclidean distance between $x_{u}$ and its $q$ -NN. In Appendix C.2, we provide results using another similarity measure. ", "page_idx": 8}, {"type": "text", "text": "We investigate the effect of graph sparsification on graphs built by $q$ -NN. We sample $n=10000$ images from MNIST [26] and FashionMNIST [42], and use the full HAR [3] dataset $n=10299)$ ). From the sampled data points, we build the $q{-}\\Nu\\Nu$ graph $G_{q}$ , as well as its metric backbone $G_{q}^{\\mathrm{mb}}$ and its spectral sparsification $G_{q_{-}}^{\\mathrm{ss}}$ . We then compare the performance of two clustering algorithms, spectral clustering and Poisson learning. Poisson learning is a semi-supervised graph clustering algorithm and was recently shown to outperform other graph-based semi-supervised algorithms [9]. Results of spectral clustering using another similarity measure are presented in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "We compare the ARI of clustering algorithms on $q$ -NN graphs and its sparsifications (metric backbone and spectral sparsification) for various choices of the number of nearest neighbors $q$ . The results are shown in Figure 3 (for spectral clustering) and Figure 4 (for Poisson-learning). Unlike the spectral sparsifier, the metric backbone retains a high ARI across all choices of $q$ . Interestingly, the performance on the original graph often decreases with $q$ , which is a hyperparameter of the graph construction step. Applying a clustering algorithm on the metric backbone comes with the two advantages of significantly reducing the number of edges in the graph and of making its performance robust against the choice of the hyperparameter $q$ . Indeed, a larger value of $q$ creates more edges but with a higher distance (cost), which are therefore more likely to be non-metric. ", "page_idx": 8}, {"type": "image", "img_path": "Kx8I0rP7w2/tmp/7bcce71326fe9b50441eb739198c137ca9c5bbf268bf01ccacc7cb4518d2208e.jpg", "img_caption": ["Figure 3: Performance of spectral clustering on subsets of MNIST, FashionMNIST datasets, and on the HAR dataset. The ARI is averaged over 10 trials; error bars show the standard error of the mean. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Kx8I0rP7w2/tmp/bf2fc916f001f0117650e1a6fe22b8f12d214eea2ef7cfec0dbf3c36d1713231.jpg", "img_caption": ["Figure 4: Performance of Poisson learning on subsets of MNIST, FashionMNIST datasets, and the HAR dataset. The ARI is averaged over 100 trials, and error bars show the standard error of the mean. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Finally, we compare in Table 1 the ARI obtained on the $q$ -nearest neighbor graph using $q=10$ (as it is a common default choice) with the metric backbone graph of a $q$ -nearest neighbor graph with ", "page_idx": 8}, {"type": "table", "img_path": "Kx8I0rP7w2/tmp/4533cedacbdad39c05acb7b632794d5978fa94eb6a8f4b94d50866d01b82f47d.jpg", "table_caption": ["$q=\\sqrt{n}/2$ . Moreover, we compute an approximation of the metric backbone by sampling uniformly at random $2\\log n$ vertices and taking the union of the $2\\log n$ shortest-path trees rooted at each one of them instead of the union of all $n$ shortest-path trees. This produces a graph $\\tilde{G}_{\\sqrt{n}/2}^{\\mathrm{mb}}$ whose edge set is a subset of the edges of the true metric backbone $G_{\\sqrt{n}/2}^{\\mathrm{mb}}$ 2. We observe that G\u02dcm\u221abn/ 2 retains the community structure albeit being typically twice sparser than the 10-nearest neighbor graph $G_{10}$ . "], "table_footnote": ["Table 1: Comparison of clustering on the full data sets. $G_{10}$ deno\u221ates the 10-nearest neighbors graph, and G\u02dcm\u221ab denotes the approximate metric backbone of the $\\sqrt{n}/2$ -nearest neighbor graph. We approximate the metric backbone by sampling only $2\\log n$ shortest-path trees. "], "page_idx": 9}, {"type": "text", "text": "Additional discussion The performance of clustering algorithms on the $q$ -nearest neighbor graph $G_{q}$ tends to decrease when $q$ increases. Indeed, a larger $q$ introduces many low-similarity edges, which can act as noise. Spectral sparsification preserves the spectral properties of the graph, but this becomes ineffective if the spectral properties of $G_{q}$ are insufficient to recover the communities (as attested by the poor performance of spectral clustering for large values of $q$ in Figures 3a and 8a). However, when the performance of spectral clustering on $G_{q}$ remains stable as $q$ is varied, so does the performance of spectral clustering on the spectral sparsified graph $G_{q}^{\\mathrm{ss}}$ (as seen in Figures 3b and 8b). In contrast, the metric backbone preserves the shortest paths rather than spectral properties. Because the shortest paths are robust to the addition of numerous low-similarity edges,9the performance of clustering algorithms on the metric backbone Gqmb remains stable when $q$ increases. This holds regardless of whether the performance on the original graph $G_{q}$ is stable or decreases with increasing $q$ . Finally, sparsified graphs obtained by metric sparsification are more consistent across different values of $q$ than those obtained via spectral sparsification. For instance, on the MNIST dataset with Gaussian kernel similarity, the metric backbone $G_{\\mathrm{30}}^{\\mathrm{mb}}$ a nd G4m0b have both approximately 70k edges, with edges in common. In contrast, the spectrally sparsified graphs $G_{30}^{\\mathrm{ss}}$ and $G_{40}^{\\mathrm{ss}}$ , also with around 70k edges each, share only $22\\mathbf{k}$ edges in common. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The metric backbone plays a crucial role in preserving several essential properties of a network. Notably, the metric backbone effectively preserves the network community structure, although many inter-community edges belong to shortest paths. In this paper, we have specifically proven that the metric backbone preserves the community structure in weighted stochastic block models. Moreover, our numerical experiments emphasize the performance of the metric backbone as a powerful graph sparsification tool. Furthermore, the metric backbone can serve as a preprocessing step for graph construction employing $q$ -nearest neighbors, alleviating the sensitivity associated with selecting the hyperparameter $q$ and producing sparser graphs. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of Machine Learning Research, 18(1):6446\u20136531, 2017.   \n[2] Lada A Adamic and Eytan Adar. Friends and neighbors on the web. Social networks, 25(3):211\u2013 230, 2003.   \n[3] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis Reyes-Ortiz. A public domain dataset for human activity recognition using smartphones. In 21st European Symposium on Artificial Neural Networks (ESANN\u201913), 2013. [4] Konstantin Avrachenkov and Maximilien Dreveton. Statistical Analysis of Networks. BostonDelft: now publishers, 10 2022.   \n[5] Afonso S. Bandeira and Ramon van Handel. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. The Annals of Probability, 44(4):2479 \u2013 2506, 2016.   \n[6] Shankar Bhamidi, Remco van der Hofstad, and Gerard Hooghiemstra. First passage percolation on random graphs with finite mean degrees. The Annals of Applied Probability, 20(5):1907 \u2013 1965, 2010.   \n[7] B\u00e9la Bollob\u00e1s, Svante Janson, and Oliver Riordan. The phase transition in inhomogeneous random graphs. Random Structures & Algorithms, 31(1):3\u2013122, 2007. [8] Rion Brattig Correia, Alain Barrat, and Luis M Rocha. Contact networks have small metric backbones that maintain community structure and are primary transmission subgraphs. PLoS Computational Biology, 19(2):e1010854, 2023.   \n[9] Jeff Calder, Brendan Cook, Matthew Thorpe, and Dejan Slepcev. Poisson learning: Graph based semi-supervised learning at very low label rates. In International Conference on Machine Learning, pages 1306\u20131316. PMLR, 2020.   \n[10] Shihyen Chen, Bin Ma, and Kaizhong Zhang. On the similarity metric and the distance metric. Theoretical Computer Science, 410(24-25):2365\u20132376, 2009.   \n[11] Gabor Csardi and Tamas Nepusz. The igraph software package for complex network research. InterJournal, Complex Systems:1695, 2006.   \n[12] Fraser Daly, Matthias Schulte, and Seva Shneer. First passage percolation on Erd\u02ddos\u2013R\u00e9nyi graphs with general weights. arXiv preprint arXiv:2308.12149, 2023.   \n[13] Micha\u00ebl Defferrard, Lionel Martin, Rodrigo Pena, and Nathana\u00ebl Perraudin. PyGSP: Graph signal processing in Python, October 2017.   \n[14] Edsger Wybe Dijkstra. A note on two problems in connexion with graphs. Journal of the ACM (Numerische Mathematik, 1:269\u2013271, 1959.   \n[15] Wei Dong, Charikar Moses, and Kai Li. Efficient $k$ -nearest neighbor graph construction for generic similarity measures. In Proceedings of the 20th international conference on World wide web, pages 577\u2013586, 2011.   \n[16] Alan M Frieze and Geoffrey R Grimmett. The shortest-path problem for graphs with random arc-lengths. Discrete Applied Mathematics, 10(1):57\u201377, 1985.   \n[17] Michelle Girvan and Mark EJ Newman. Community structure in social and biological networks. Proceedings of the national academy of sciences, 99(12):7821\u20137826, 2002.   \n[18] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semidefinite programming. IEEE Transactions on Information Theory, 62(5):2788\u20132797, 2016.   \n[19] John M Hammersley and Dominic JA Welsh. First-passage percolation, subadditive processes, stochastic networks, and generalized renewal theory. In Bernoulli 1713, Bayes 1763, Laplace 1813: Anniversary Volume. Proceedings of an International Research Seminar Statistical Laboratory University of California, Berkeley 1963, pages 61\u2013110. Springer, 1965.   \n[20] Refael Hassin and Eitan Zemel. On shortest paths in graphs with random weights. Mathematics of Operations Research, 10(4):557\u2013564, 1985.   \n[21] Reinhard Heckel and Helmut B\u00f6lcskei. Robust subspace clustering via thresholding. IEEE Transactions on Information Theory, 61(11):6320\u20136342, 2015.   \n[22] Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2:193\u2013218, 1985.   \n[23] Svante Janson. One, two and three times $\\log n/n$ for paths in a complete graph with random weights. Combinatorics, Probability and Computing, 8(4):347\u2013361, 1999.   \n[24] Vasiliki Kalavri, Tiago Simas, and Dionysios Logothetis. The shortest path is not always a straight line: leveraging semi-metricity in graph analysis. Proceedings of the VLDB Endowment, 9(9):672\u2013683, 2016.   \n[25] Istv\u00e1n Kolossv\u00e1ry and J\u00falia Komj\u00e1thy. First passage percolation on inhomogeneous random graphs. Advances in Applied Probability, 47(2):589\u2013610, 2015.   \n[26] Yann LeCun, Corinna Cortes, and Christopher JC Burges. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.   \n[27] Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block models. Annals of Statistics, 43(1):215\u2013237, 2015.   \n[28] Lasse Leskel\u00e4 and Hoa Ngo. First passage percolation on sparse random graphs with boundary weights. Journal of Applied Probability, 56(2):458\u2013471, 2019.   \n[29] Catherine C. McGeoch. All-pairs shortest paths and the essential subgraph. Algorithmica, 13:426\u2013441, 1995.   \n[30] Mark EJ Newman. Networks. Oxford University Press, 07 2018.   \n[31] Tiago P Peixoto. Nonparametric weighted stochastic block models. Physical Review E, 97(1):012306, 2018.   \n[32] Tiago P. Peixoto and Alec Kirkley. Implicit models, latent compression, intrinsic biases, and cheap lunches in community detection. Phys. Rev. E, 108:024309, Aug 2023.   \n[33] Yuval Peres, Dmitry Sotnikov, Benny Sudakov, and Uri Zwick. All-pairs shortest paths in $O(n^{2})$ time with high probability. Journal of the ACM (JACM), 60(4):1\u201325, 2013.   \n[34] M \u00c1ngeles Serrano, Mari\u00e1n Bogun\u00e1, and Alessandro Vespignani. Extracting the multiscale backbone of complex weighted networks. Proceedings of the national academy of sciences, 106(16):6483\u20136488, 2009.   \n[35] Tiago Simas, Rion Brattig Correia, and Luis M Rocha. The distance backbone of complex networks. Journal of Complex Networks, 9(6), 2021.   \n[36] Tiago Simas and Luis M Rocha. Distance closures on complex networks. Network Science, 3(2):227\u2013268, 2015.   \n[37] Daniel A. Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM Journal on Computing, 40(6):1913\u20131926, 2011.   \n[38] Vincent A Traag, Ludo Waltman, and Nees Jan Van Eck. From Louvain to Leiden: guaranteeing well-connected communities. Scientific reports, 9(1):5233, 2019.   \n[39] Piet Van Mieghem and Huijuan Wang. The observable part of a network. IEEE/ACM Transactions on Networking, 17(1):93\u2013105, 2009.   \n[40] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17:395\u2013416, 2007.   \n[41] Huijuan Wang, Javier Martin Hernandez, and Piet Van Mieghem. Betweenness centrality in a weighted network. Physical Review E, 77(4):046105, 2008.   \n[42] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[43] Ali Yassin, Abbas Haidar, Hocine Cherif,i Hamida Seba, and Olivier Togni. An evaluation tool for backbone extraction techniques in weighted complex networks. Scientific Reports, 13(1):17000, 2023.   \n[44] Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis\u2013kahan theorem for statisticians. Biometrika, 102(2):315\u2013323, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proof of Proposition 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let us set up some notations before proving Proposition 1. We let $(z,G)\\,\\sim\\,\\mathrm{wSBM}(n,\\pi,p,f)$ where the model parameters verify Assumptions 1 and 2, and we denote by $c$ and $\\tilde{c}$ the cost and the extended cost associated with $G$ , respectively. We let $\\Gamma_{1},\\cdot\\cdot\\cdot,\\Gamma_{k}$ be the $k$ communities, $i.e.$ , $\\Gamma_{a}\\ =\\ \\{w\\in[n]\\colon z_{w}=a\\}$ , and we denote by $n_{1},\\cdot\\cdot\\cdot\\,,n_{k}$ their respective sizes, i.e., $n_{a}=|\\Gamma_{a}|$ . As $k=\\Theta(1)$ , the concentration of multinomial distributions ensures that $n_{a}=(1+o(1))\\pi_{a}n$ whp. ", "page_idx": 13}, {"type": "text", "text": "A.1 Particular case of exponentially distributed costs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ for exponentially distributed costs. We firstly assume that $F_{a b}\\sim\\mathrm{Exp}(\\lambda_{a b})$ with $\\lambda_{a b}>0$ . It is immediate to notice that $(f_{a b})_{a b}$ verify Assumption 2. ", "page_idx": 13}, {"type": "text", "text": "Let $u,v$ be two vertices chosen uniformly at random in $\\Gamma_{a}$ and $\\Gamma_{b}$ respectively, so that $z_{u}=a$ and $z_{v}=b$ . To explore the neighborhood of vertex $u$ , we consider a first passage percolation (FPP) on $G$ starting from $u$ . More precisely, for any $t>0$ , we denote by $\\mathcal{B}(u,t)\\stackrel{}{=}\\{\\bar{w}\\in V\\colon C(u,w)\\leq t\\}$ the set of vertices within a cost $t$ of $u$ , and by ", "page_idx": 13}, {"type": "equation", "text": "$$\nC_{u}(q)\\;=\\;\\operatorname*{min}\\left\\{t\\ge0\\colon\\,|\\mathcal{B}(u,t)|\\ge q+1\\right\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "the cost going from $u$ to its $q_{\\mathrm{~\\,~}}$ -th nearest neighbor (with the convention that $\\operatorname*{min}\\varnothing=\\infty)$ ). In particular, $B(u,C_{u}\\bar{(}q))$ is the set of the $q$ nearest neighbors of $u$ . Let $U(q)\\,=\\,(U_{1}(q),\\cdot\\cdot\\cdot\\,,U_{k}(\\bar{q}))$ be the collection of sets such that $U_{\\ell}(q)$ is the set of vertices that are in the $q$ nearest neighborhood of $u$ and in community $\\ell,i.e.$ ., ", "page_idx": 13}, {"type": "equation", "text": "$$\nU_{\\ell}(q)\\;=\\;{\\mathcal{B}}(u,C_{u}(q))\\cap\\Gamma_{\\ell}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, we denote by $S(u,q)$ the matrix whose entry $S_{\\ell\\ell^{\\prime}}(u,q)$ is equal to the number of edges going from $\\bar{U}_{\\ell}(q)$ to $\\Gamma_{\\ell^{\\prime}}\\backslash U_{\\ell^{\\prime}}(\\dot{q})$ . ", "page_idx": 13}, {"type": "text", "text": "The outline of the proof is as follows. In a first s\u221atep (i), we study the FPP starting \u221afrom vertex $u$ to obtain upper and lower bounds on the cost $C_{u}({\\sqrt{n\\log n}})$ for going from $u$ to its $\\sqrt{n\\log n}$ -nearest neighbor10. More precisely, we will establish that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{n\\rho_{n}}{\\log n}C_{u}\\left(\\sqrt{n\\log n}\\right)\\,\\leq\\,\\frac{1+o(1)}{2\\tau_{\\operatorname*{min}}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds whp. By symmetry, the same upper bound also holds for $C_{v}({\\sqrt{n\\log n}})$ ", "page_idx": 13}, {"type": "text", "text": "Next, we will show in step (ii) that $\\mathcal{B}(u,\\sqrt{n\\log n})\\cap\\mathcal{B}(v,\\sqrt{n\\log n})\\neq\\emptyset$ whp. Together with the upper bound (A\u221a.1), we can then u\u221apper bound the cost $C(u,v)$ of the shortest path from $u$ to $v$ . Indeed, let $w_{1}\\in\\mathcal{B}(u,\\sqrt{n\\log n})\\cap\\mathcal{B}(v,\\sqrt{n\\log n})$ (such a $w_{1}$ exists whp by step (ii)), and consider the path $\\mathcal{P}=u\\to w_{1}\\to v$ , where $u\\rightarrow w_{1}$ and $w_{1}\\to v$ denote the shortest path from $u$ to $w_{1}$ and from $w_{1}$ to $v$ , respectively. Because the cost $C(u,v)$ of the shortest path from $u$ to $v$ is upper-bounded by the cost of the path $\\mathcal{P}$ , we have whp ", "page_idx": 13}, {"type": "equation", "text": "$$\nC(u,v)\\ \\leq\\ C(u,w_{1})+C(w_{1},v)\\ \\leq\\ \\frac{1+o(1)}{\\tau_{\\mathrm{min}}}\\frac{\\log n}{n\\rho_{n}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the second inequality holds because $w_{1}\\in\\mathcal{B}(u,\\sqrt{n\\log n})\\cap\\mathcal{B}(v,\\sqrt{n\\log n})$ , which enables to use the upper bound (A.1). This establishes the desired upper bound on $C(u,v)$ . ", "page_idx": 13}, {"type": "text", "text": "To lower-bound $C(u,v)$ , we will establish that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1+o(1)}{2\\tau_{\\operatorname*{max}}}\\:\\leq\\:\\frac{n\\rho_{n}}{\\log n}C_{u}\\left(\\sqrt{\\frac{n}{\\log n}}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds whp. Again, a similar bound holds for $C_{v}\\left(\\sqrt{n/\\log n}\\right)$ by symmetry. ", "page_idx": 13}, {"type": "text", "text": "Finally, we will show in step (iii) that $\\mathcal{B}\\left(u,C_{u}(\\sqrt{n/\\log n})\\right)\\cap\\mathcal{B}\\left(v,C_{v}(\\sqrt{n/\\log n}\\right)\\,=\\,\\emptyset$ whp. Combining this with the lower bound (A.2), we conclude then that whp ", "page_idx": 13}, {"type": "equation", "text": "$$\nC(u,v)\\geq\\frac{1+o(1)}{\\tau_{\\operatorname*{max}}}\\frac{\\log n}{n\\rho_{n}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(i) Upper and lower bounds on $C_{u}(\\sqrt{n/\\log n})$ and $C_{u}({\\sqrt{n\\log n}})$ . In this paragraph, we will establish (A.1) and (A.2). ", "page_idx": 14}, {"type": "text", "text": "A key property of the FPP is to notice that, conditioned on $S(u,\\cdot)=(S(u,1),\\cdot\\cdot\\cdot,S(u,n))$ , the random numbers $C_{u}(q+1)-C_{u}(q)$ are independent and exponentially distributed, such that ", "page_idx": 14}, {"type": "equation", "text": "$$\nC_{u}(q+1)-C_{u}(q)\\mid S(u,\\cdot)\\;\\sim\\;\\mathrm{Exp}\\left(\\sum_{\\ell,\\ell^{\\prime}}\\lambda_{\\ell\\ell^{\\prime}}S_{\\ell\\ell^{\\prime}}(u,q)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Indeed, $C_{u}(q+1)-C_{u}(q)$ is the difference in the cost of traveling from $u$ to its $(q+1)$ -th nearestneighbor minus the cost from $u$ and its $q$ -th nearest neighbor. In other words, ", "page_idx": 14}, {"type": "equation", "text": "$$\nC_{u}(q+1)-C_{u}(q)\\mid S(u,\\cdot)\\;=\\;\\operatorname*{min}_{w\\in{U_{c}(q)}\\atop w^{\\prime}\\in\\Gamma_{c^{\\prime}}\\backslash{U_{c^{\\prime}}(q)}}c_{w w^{\\prime}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $c_{w w^{\\prime}}\\sim\\mathrm{Exp}(\\lambda_{c c^{\\prime}})$ . Statement (A.3) follows because $\\operatorname*{min}{\\left\\{\\mathrm{Exp}(\\lambda),\\mathrm{Exp}(\\mu)\\right\\}}\\sim\\mathrm{Exp}(\\lambda+\\mu)$ . Let $q\\,\\leq\\,{\\sqrt{n\\log n}}$ . The number of edges $S_{\\ell\\ell^{\\prime}}(u,q)$ between the sets $U_{\\ell}(q)$ and $\\Gamma_{\\ell^{\\prime}}\\backslash U_{\\ell^{\\prime}}(q)$ is binomially distributed such that ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{\\ell\\ell^{\\prime}}(u,q)\\;\\sim\\;\\mathrm{Bin}\\left(|U_{\\ell}(q)|\\times|\\Gamma_{\\ell^{\\prime}}\\backslash U_{\\ell^{\\prime}}(q)|\\,,p_{c c^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The number of vertices in $\\Gamma_{\\ell^{\\prime}}$ is $n_{\\ell^{\\prime}}=\\Theta(n)$ and the number of vertices in $|U_{\\ell^{\\prime}}(q)|$ verifies $|U_{\\ell^{\\prime}}(q)|\\le$ $q\\ll n$ . Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[S_{\\ell\\ell^{\\prime}}(u,q)\\right]\\;=\\;|U_{\\ell}(q)|\\left(n_{\\ell^{\\prime}}-|U_{\\ell^{\\prime}}(q)|\\right)p_{\\ell\\ell^{\\prime}}\\;=\\;\\left(1+o(1)\\right)|U_{\\ell}(q)|\\;n_{\\ell^{\\prime}}p_{\\ell\\ell^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence $\\mathbb{E}\\left[S_{\\ell\\ell^{\\prime}}(u,q)\\right]\\gg1$ , and the concentration of binomial distributions ensure that whp ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{\\ell\\ell^{\\prime}}(u,q)\\;=\\;(1+o(1))\\,|U_{\\ell}(q)|\\,(n_{\\ell^{\\prime}}-|U_{\\ell}(q)|)p_{\\ell\\ell^{\\prime}}\\;=\\;(1+o(1))\\,|U_{\\ell}(q)|\\,n_{\\ell^{\\prime}}p_{\\ell\\ell^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, using $n_{\\ell^{\\prime}}=(1+o(1))\\pi_{\\ell^{\\prime}}n$ and $p_{\\ell\\ell^{\\prime}}=B_{\\ell\\ell^{\\prime}}\\rho_{n}$ , we have from (A.4) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[C_{u}(q+1)-C_{u}(q)\\,|\\,S(u,\\cdot)\\right]\\;=\\;\\frac{1+o(1)}{\\sum_{\\ell}|U_{\\ell}(q)|\\sum_{\\ell^{\\prime}}n_{\\ell^{\\prime}}\\lambda_{\\ell\\ell^{\\prime}}p_{\\ell\\ell^{\\prime}}}}&{}\\\\ {\\;=\\;\\frac{1+o(1)}{n\\rho_{n}\\sum_{\\ell}|U_{\\ell}(q)|\\sum_{\\ell^{\\prime}}\\pi_{\\ell^{\\prime}}\\lambda_{\\ell\\ell^{\\prime}}B_{\\ell\\ell^{\\prime}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We notice that $\\begin{array}{r}{\\sum_{l}|U_{\\ell}(q)|\\sum_{\\ell^{\\prime}}\\pi_{\\ell^{\\prime}}\\lambda_{\\ell\\ell^{\\prime}}B_{\\ell\\ell^{\\prime}}=U^{T}(q)T1_{k}}\\end{array}$ where $T=(\\Lambda\\odot B)\\,\\mathrm{diag}(\\pi)$ is the operator defined in (2.1 ). Then, us ing $\\sum_{\\ell}|U_{\\ell}(q)|=q$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tau_{\\mathrm{min}}q\\;\\leq\\;\\sum_{\\ell}|U_{\\ell}(q)|\\sum_{\\ell^{\\prime}}\\pi_{\\ell^{\\prime}}\\lambda_{\\ell\\ell^{\\prime}}B_{\\ell\\ell^{\\prime}}\\;\\leq\\;\\tau_{\\mathrm{max}}q.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1+o(1)}{n\\rho_{n}\\tau_{\\mathrm{max}}q}\\;\\le\\;\\mathbb{E}\\left[C_{u}(q+1)-C_{u}(q)\\,|\\,S(u,\\cdot)\\right]\\;\\le\\;\\frac{1+o(1)}{n\\rho_{n}\\tau_{\\mathrm{min}}q}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This boun\u221ad is uniform in $S(u,\\cdot)$ . Thus, using the total law of probability and summing over all $1\\leq q\\leq\\sqrt{n\\log n}$ leads to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1+o(1)}{2\\tau_{\\mathrm{max}}}\\;\\leq\\;\\frac{n\\rho_{n}}{\\log n}\\mathbb{E}C_{u}(\\sqrt{n\\log n})\\;\\leq\\;\\frac{1+o(1)}{2\\tau_{\\mathrm{min}}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we used $\\textstyle\\sum_{q=1}^{\\sqrt{n\\log n}}q^{-1}=2^{-1}(\\log n+\\log\\log n)+\\Theta(1).$ ", "page_idx": 14}, {"type": "text", "text": "Let us now upper-bound the variance of $C_{u}\\left(\\sqrt{n\\log n}\\right)$ . By the law of total variance, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Var}\\left[C_{u}(\\sqrt{n\\log n})\\right]\\;=\\;\\mathbb{E}\\left[\\mathrm{Var}\\left[C_{u}(\\sqrt{n\\log n})\\,|\\,S(u,\\cdot)\\right]\\right]+\\mathrm{Var}\\left[\\mathbb{E}\\left[C_{u}(\\sqrt{n\\log n})\\,|\\,S(u,\\cdot)\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The first term on the right-hand side of (A.9) can be upper bounded by proceeding similarly as for the expectation. Indeed, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left[C_{u}(q+1)-C_{u}(q)\\,|\\,\\cal S(u,\\cdot)\\right]\\;\\leq\\;\\frac{1+o(1)}{n^{2}\\rho_{n}^{2}\\tau_{\\mathrm{min}}^{2}q^{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the independence of $C_{u}(q+1)-C_{u}(q)$ conditioned on $S(u,\\cdot)$ leads to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Var}\\left[C_{u}(\\sqrt{n\\log n})\\mid S(u,\\cdot)\\right]\\;=\\;}&{\\displaystyle\\sum_{q=1}^{\\sqrt{n\\log n}-1}\\mathrm{Var}\\left(C_{u}(q+1)-C_{u}(q)\\mid S(u,\\cdot)\\right)}\\\\ &{\\le\\;\\frac{1+o(1)}{n^{2}\\rho_{n}^{2}\\tau_{\\mathrm{min}}^{2}}\\displaystyle\\sum_{q=1}^{\\sqrt{n\\log n}}\\;q^{-2}}\\\\ &{\\le\\;\\frac{1+o(1)}{n^{2}\\rho_{n}^{2}\\tau_{\\mathrm{min}}^{2}}\\displaystyle\\frac{\\pi^{2}}{6}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To upper bound the second term on the right-hand side of (A.9), we notice that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Var}\\left[\\mathbb{E}\\left[C_{u}(\\sqrt{n\\log n})\\,|\\,S(u,\\cdot)\\right]\\right]\\;=\\;\\mathrm{Var}\\left[\\sum_{q=1}^{\\sqrt{n\\log n}-1}\\mathbb{E}\\left[C_{u}(q+1)-C_{u}(q)\\,|\\,S(u,\\cdot)\\right]\\right]}\\\\ {\\;=\\;\\sum_{q=1}^{\\sqrt{n\\log n}-1}\\mathrm{Var}\\left[\\mathbb{E}\\left[C_{u}(q+1)-C_{u}(q)\\,|\\,S(u,\\cdot)\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first line holds by the linearity of the conditional expectation, and the second one by the independence of $C_{u}(q+1)-C_{u}(q)$ conditioned on $S(u,\\cdot)$ . Moreover, recall that $\\mathrm{Var}(X)\\leq$ $(b-a)^{2}{\\bar{/}}4$ if $a\\leq X\\leq b$ . Hence, using the upper and lower bound on $C_{u}(q+1)-C_{u}(q)$ obtained in (A.7) leads to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{\\mathrm{Var}\\left[\\mathbb{E}\\left[C_{u}(q+1)-C_{u}(q)\\,|\\,S(u,\\cdot)\\right]\\right]\\;\\leq\\;\\frac{1+o(1)}{4}\\left(\\frac{1}{n\\rho_{n}\\tau_{\\mathrm{min}}q}-\\frac{1}{n\\rho_{n}\\tau_{\\mathrm{max}}q}\\right)^{2}}\\\\ &{}&{\\leq\\;\\frac{1+o(1)}{4}\\left(\\frac{1}{n\\rho_{n}\\tau_{\\mathrm{min}}q}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and therefore ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left[\\mathbb{E}\\left[C_{u}(\\sqrt{n\\log n})\\,|\\,S(u,\\cdot)\\right]\\right]\\;\\leq\\;\\frac{1+o(1)}{4n^{2}\\rho_{n}^{2}\\tau_{\\mathrm{min}}^{2}}\\frac{\\pi^{2}}{6}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining (A.10) and (A.11) into (A.9) provides ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left[C_{u}(\\sqrt{n\\log n})\\right]\\ \\leq\\ \\frac{5}{4}\\frac{(1+o(1))\\pi^{2}}{6\\tau_{\\mathrm{min}}^{2}n^{2}\\rho_{n}^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This upper bound (A.12) ensures that $\\operatorname{Var}\\left[C_{u}({\\sqrt{n\\log n}})\\right]=o\\left(\\left(\\mathbb{E}\\left[C_{u}({\\sqrt{n\\log n}})\\right]\\right)^{2}\\right)$ , and therefore an application of Chebyshev\u2019s inequality ensures that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1+o(1)}{2\\tau_{\\mathrm{max}}}\\:\\leq\\:\\frac{n\\rho_{n}}{\\log n}C_{u}\\left(\\sqrt{n\\log n}\\right)\\:\\leq\\:\\frac{1+o(1)}{2\\tau_{\\mathrm{min}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with high probability. Likewise, by symmetry for a FPP starting from $v$ instead of $u$ , we find ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1+o(1)}{2\\tau_{\\mathrm{max}}}\\;\\leq\\;\\frac{n\\rho_{n}}{\\log n}C_{v}\\left(\\sqrt{n\\log n}\\right)\\;\\leq\\;\\frac{1+o(1)}{2\\tau_{\\mathrm{min}}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This establishes (A.1). Moreover, we establish (A.2) by doing a \u221aslight modification of this proof. More precisely, we sum over all $q\\leq\\sqrt{n/\\log n}$ instead of all $q\\leq{\\sqrt{n\\log n}}$ in step (A.8). We obtain the same bound as in (A.8) because $\\textstyle\\sum_{q=1}^{\\sqrt{n/\\log n}}q^{-1}=2^{-1}(\\log n-\\log\\log n)+\\Theta(1).$ ", "page_idx": 15}, {"type": "text", "text": "(ii) $\\mathcal{B}(u,C_{u}(\\sqrt{n\\log n}))\\cap B(v,C_{v}(\\sqrt{n\\log n}))\\neq\\emptyset$ whp. For ease of notations, let us shorten by $\\mathscr{B}_{u}=\\mathscr{B}(u,C_{u}(\\sqrt{n\\log n}))$ the set of the $\\sqrt{n\\log n}$ nearest neighbours of $u$ and by $B_{v}(q)$ the set of the $q$ -nearest neighbours of $v$ . We also denote by $w_{q}$ the $q$ -nearest neighbor of $v$ . A key property is that the two FPPs (starting from vertex $u$ and starting from vertex $v$ ) are independent of each other as long as they do not intersect. To make this rigorous, we denote by $Q$ the random variable counting the number of steps made by the FPP starting from $v$ without intersecting with $B_{u}$ , i.e., ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nQ\\ =\\ \\operatorname*{min}\\left\\{q\\in[n]\\colon B_{u}\\cap B_{v}(q)\\neq\\emptyset\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We \u221anow show that $\\mathbb{P}\\left(Q>q\\right)\\ =\\ o(1)$ whenever $q~\\gg~\\sqrt{n/\\log n}$ , which implies that $B_{u}\\cap$ $B_{v}(\\sqrt{n\\log n})\\neq\\varnothing$ . Using [6, Lemma B.1], we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(Q>m\\right)\\,=\\,\\mathbb{E}\\left[\\prod_{q=1}^{m}\\mathbb{Q}^{(q)}\\left(Q>q\\,|\\,Q>q-1\\right)\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbb{Q}^{(q)}$ denotes the conditional distribution given $B_{u}$ and $B_{v}(q)$ . We further notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{Q}^{\\left(q\\right)}\\left(Q>q\\,|\\,Q>q-1\\right)\\,=\\,1-\\mathbb{Q}^{\\left(q\\right)}\\left(Q=q\\,|\\,Q>q-1\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and that the event $\\{Q>q-1\\}$ is equivalent to the event $\\{w_{1}\\notin B_{u},\\cdots,w_{q-1}\\notin B_{u}\\}$ , i.e., the FPP starting from $v$ has not yet collided with the one starting from $u$ . Conditionally on this event, all vertices within the same block have an equal probability of being chosen at the $q$ -th step of the FPP. Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{Q}^{(q)}\\left(Q=q\\,|\\,Q>q-1\\right)\\;=\\;\\sum_{\\ell\\in[k]}\\mathbb{P}\\left(w_{q}\\in\\mathcal{B}_{u}\\,|\\,w_{q}\\in\\Gamma_{\\ell},Q>q-1\\right)\\mathbb{P}\\left(w_{q}\\in\\Gamma_{\\ell}\\right)}}\\\\ &{}&{=\\;\\displaystyle\\sum_{\\ell\\in[k]}\\frac{|\\mathcal{B}_{u}\\cap\\Gamma_{\\ell}|}{|\\Gamma_{\\ell}|}\\mathbb{P}\\left(w_{q}\\in\\Gamma_{\\ell}\\right).\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Because $\\begin{array}{r}{\\sum_{\\ell\\in[k]}|\\mathcal B_{u}\\cap\\Gamma_{\\ell}|=\\sqrt{n\\log n}}\\end{array}$ , we have that11 $\\operatorname*{max}_{\\ell}|\\mathcal{B}_{u}\\cap\\Gamma_{\\ell}|\\geq k^{-1}\\sqrt{n\\log n}$ . Moreover, $\\vert\\Gamma_{\\ell}\\vert\\le(1+o(\\dot{1}))\\pi_{\\mathrm{max}}n$ whp and $\\begin{array}{r}{\\sum_{\\ell\\in[k]}\\mathbb{P}\\left(w_{q}\\in\\Gamma_{\\ell}\\right)=1}\\end{array}$ . Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{Q}^{(q)}\\left(Q=q\\,\\vert\\,Q>q-1\\right)\\,\\ge\\,\\frac{\\operatorname*{max}_{\\ell\\in[k]}\\,\\vert\\mathcal{B}_{u}\\cap\\,\\Gamma_{\\ell}\\vert}{\\operatorname*{min}_{\\ell\\in[k]}\\,\\vert\\Gamma_{\\ell}\\vert}\\sum_{\\ell\\in[k]}\\mathbb{P}\\left(w_{q}\\in\\Gamma_{\\ell}\\right)}}\\\\ &{}&{\\ge\\,\\frac{1+o(1)}{\\pi_{\\operatorname*{max}}}\\sqrt{\\frac{\\log n}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Going back to (A.13), this implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(Q>m\\right)\\;\\leq\\;\\left(1-\\frac{1+o(1)}{\\pi_{\\operatorname*{max}}}\\sqrt{\\frac{\\log n}{n}}\\right)^{m},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which indeed goes to 0 when $m\\gg\\sqrt{n/\\log n}$ ", "page_idx": 16}, {"type": "text", "text": "(iii) $\\mathscr{B}(u,C_{u}(\\sqrt{n/\\log n}))\\cap B(v,C_{v}(\\sqrt{n/\\log n}))=\\emptyset$ whp. We proceed similarly to step (ii) by considering the FPP starting from $v$ , and denote by $w_{q}$ the $q$ -nearest neighbor of $v$ . For ease of notations, let us denote in this paragraph $\\mathcal{B}_{u}=\\mathcal{B}\\left(u,C_{u}\\,\\overline{{\\left(\\sqrt{n/\\log n}\\right)}}\\right)$ and $\\boldsymbol{B}_{v}(\\boldsymbol{q})=\\boldsymbol{B}\\left(v,C_{v}\\left(q\\right)\\right)$ . Note tha\u221at, in contrast to step (ii), we now look at the FPP up to step $\\sqrt{n/\\log n}$ instead of the FPP up to step $\\sqrt{n\\log n}$ . We define ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ\\ =\\ \\operatorname*{min}\\{q\\in[n]\\colon B_{u}\\cap B_{v}(q)\\neq\\emptyset\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(Q=q\\right)\\,=\\,\\mathbb{P}\\left(w_{q}\\in\\mathcal{B}_{u}\\,|\\,Q>q-1\\right)\\mathbb{P}\\left(Q>q-1\\right)}\\\\ &{=\\,\\displaystyle\\sum_{\\ell\\in[k]}\\frac{|\\mathcal{B}_{u}\\cap\\Gamma_{\\ell}|}{|\\Gamma_{\\ell}|}\\mathbb{P}\\left(w_{q}\\in\\Gamma_{\\ell}\\,|\\,Q>q-1\\right)\\mathbb{P}\\left(Q>q-1\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "11Recall that if $x_{1}+\\cdots+x_{k}=m$ with $x_{\\ell}\\ge0$ then max\u2113 $x_{\\ell}\\geq k^{-1}m$ . ", "page_idx": 16}, {"type": "text", "text": "where, as in step (ii), the second equality holds because all vertices within the same block have the same probability of being chosen at the next step of the FPP as long as the two FPP have not collided. Using $\\vert\\Gamma_{\\ell}\\vert\\geq(1+o(1))\\pi_{\\mathrm{min}}n$ and $|\\mathcal{B}_{u}\\cap\\Gamma_{\\ell}|\\le|\\mathcal{B}_{u}|\\le\\sqrt{n/\\log n}$ leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(Q=q\\right)\\,\\leq\\,\\frac{1}{\\pi_{\\operatorname*{min}}}\\frac{1+o(1)}{\\sqrt{n\\log n}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left({\\mathcal{B}}_{u}\\cap{\\mathcal{B}}_{v}\\neq\\emptyset\\right)\\ =\\ \\sum_{q=1}^{\\sqrt{n/\\log n}}\\mathbb{P}\\left(Q=q\\right)\\ \\leq\\ \\frac{1+o(1)}{\\pi_{\\operatorname*{min}}\\log n}=o(1).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof of Proposition 1 when $F_{a b}\\sim\\mathrm{Exp}(\\lambda_{a b})$ . ", "page_idx": 17}, {"type": "text", "text": "A.2 General case ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ with non-exponentially distributed costs. In this section, the probability densities $F_{a b}$ are regular (see Assumption 2) but not necessarily exponentially distributed. We adapt the argument provided at the beginning of Section 2 of [23]. The strategy is to transform the graph $G=\\bar{(}V,E,c)$ generated from a $\\mathrm{wSBM}(n,\\pi,F)$ into a graph $G_{\\mathrm{exp}}=\\bar{(}V,E,c_{\\mathrm{exp}})$ with the same vertices and edge set but where the costs $c_{\\mathrm{exp}}$ are obtained from $F$ and are exponentially distributed so that $G_{\\mathrm{exp}}$ has the same distribution as a wSBM $(n,\\pi,(\\mathrm{Exp}(\\lambda_{a b}))_{a b})$ . We will then show that the shortest paths in $G_{\\mathrm{exp}}$ and $G$ are the same. ", "page_idx": 17}, {"type": "text", "text": "Let $f_{a b}$ be the density function of the cdf $F_{a b}$ , and remember that $\\lambda_{a b}~=~F_{a b}^{\\prime}(0)$ . Let $F_{a b}^{-1}$ be the generalized inverse function12 of $F_{a b}$ . By regularity of $F_{a b}$ (Assumption 2), we have $\\operatorname*{lim}_{t\\to0}F_{a b}(t)/t=\\lambda_{a b}$ and thus $\\begin{array}{r}{\\operatorname*{lim}_{t\\to0}F_{a b}^{-1}(t)/t=\\lambda_{a b}^{-1}}\\end{array}$ . We also denote by $g_{a b}(x)=\\lambda_{a b}e^{-\\lambda_{a b}x}$ and by $G_{a b}(x)=1-e^{-\\lambda_{a b}x}$ the density and cumulative distribution functions of $\\mathrm{Exp}(\\lambda_{a b})$ . ", "page_idx": 17}, {"type": "text", "text": "We first show that Proposition 1 holds for uniformly distributed edge weights, i.e., when $f_{a b}(x)=$ $\\lambda_{a b}1(x\\in[0,\\lambda_{a b}^{-1}])$ . Denote by $F_{a b}$ the cdf associated to $f_{a b}$ , and let $(z,G)\\sim\\mathrm{wSBM}(n,\\pi,p,F)$ where $G=(\\bar{V_{*}}\\bar{E},c)$ . We construct the graph $G_{\\mathrm{exp}}=(V,E,c_{\\mathrm{exp}})$ such that for all $(w,w^{\\prime})\\in E$ with $z_{w}=\\ell$ and $z_{w^{\\prime}}=\\ell^{\\prime}$ we have $c_{\\mathrm{exp}}(w,w^{\\prime})=G_{\\ell\\ell^{\\prime}}^{-1}(\\lambda_{\\ell\\ell^{\\prime}}c(w,w^{\\prime}))$ . Note that the (unweighted) edges of $G$ and $G_{\\mathrm{exp}}$ are the same, only the costs $c$ and $c_{\\mathrm{exp}}$ differ. Since $\\lambda_{\\ell\\ell^{\\prime}}c(w,w^{\\prime})\\sim\\mathrm{Unif}(0,1)$ we have $c_{\\mathrm{exp}}(w,w^{\\prime})\\sim\\mathrm{Exp}(\\lambda_{\\ell\\ell^{\\prime}})$ . In particular, $G_{\\mathrm{exp}}$ has the same distribution of a weighted SBM whose costs are exponentially distributed, i.e., $(z,G_{\\mathrm{exp}}^{\\ \\cdot})\\sim\\mathrm{wSBM}(n,\\pi,p,G)$ with $\\bar{G}=(G_{a b})_{a,b}$ . ", "page_idx": 17}, {"type": "text", "text": "Let $\\textstyle\\mathcal{P}(u,v)$ (resp., $\\mathcal{P}_{\\exp}(u,v))$ be the shortest path from $u$ to $v$ in $G$ (resp., in $G_{\\mathrm{exp.}}$ ), and denote by $C(u,v)$ (resp., by $C_{\\exp}(u,v))$ its cost. We know from Section A.1 that $\\begin{array}{r}{C_{\\exp}(u,v)=\\Theta\\left(\\frac{\\log n}{n\\rho_{n}}\\right)}\\end{array}$ whp. ", "page_idx": 17}, {"type": "text", "text": "Suppose that the edge $(w,w^{\\prime})\\in E$ belongs to $\\mathcal{P}_{\\exp}(u,v)$ . From $c_{\\mathrm{exp}}(w,w^{\\prime})\\leq C_{\\mathrm{exp}}(u,v)$ we notice that $\\begin{array}{r}{c_{\\mathrm{exp}}(w,w^{\\prime})=O(\\frac{\\log n}{n\\rho_{n}})}\\end{array}$ , and hence by Assumption 1 we have $c_{\\mathrm{exp}}(w,w^{\\prime})=o(1)$ . Moreover, by definition of $c_{\\mathrm{exp}}$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{c(w,w^{\\prime})}{c_{\\mathrm{exp}}(w,w^{\\prime})}=\\frac{1}{\\lambda_{\\ell\\ell^{\\prime}}}\\frac{G_{\\ell\\ell^{\\prime}}(c_{\\mathrm{exp}}(w,w^{\\prime}))}{c_{\\mathrm{exp}}(w,w^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to0}G_{\\ell\\ell^{\\prime}}(t)/t=\\lambda_{\\ell\\ell^{\\prime}}.}\\end{array}$ . Thus, we have for any $\\epsilon>0$ and for $n$ large enough, $1-\\epsilon<$ cecx(pw(,ww,\u2032w)\u2032) < 1 + \u03f5. This holds for any edge (w, w\u2032) belonging to Pexp(u, v). Therefore, the sum of the costs $c(w,w^{\\prime})$ over $\\mathcal{P}_{\\exp}(u,v)$ is at most $(1+\\epsilon)C_{\\exp}(u,v)$ , and hence ", "page_idx": 17}, {"type": "equation", "text": "$$\nC(u,v)\\;<\\;(1+\\epsilon)C_{\\mathrm{exp}}(u,v).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, if $(w,w^{\\prime})\\in\\mathcal{P}(u,v)$ , then the upper bound (A.14) together with Assumption 1 imply that $C(u,v)=o(1)$ . This in turn implies $c(w,w^{\\prime})=o(1)$ and hence $\\begin{array}{r}{1-\\epsilon<\\frac{c(w,w^{\\prime})}{c_{\\mathrm{exp}}(w,w^{\\prime})}<1+\\epsilon}\\end{array}$ . Thus, summing over all edges in $\\textstyle\\mathcal{P}(u,v)$ leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\n(1-\\epsilon)C_{\\exp}(u,v)\\;<\\;C(u,v).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining (A.14) with (A.15) shows that Proposition 1 holds for uniformly distributed edges weights. ", "page_idx": 18}, {"type": "text", "text": "Finally, if the costs $c(w,w^{\\prime})$ are sampled from general distributions $F_{a b}$ verifying Assumption 2, then we construct the graph $G_{\\mathrm{unif}}\\,=\\,(V,E,c_{\\mathrm{unif}})$ where $c_{\\mathrm{unif}}(u,v)\\,=\\,\\lambda_{z_{u}z_{v}}^{-1}\\bar{F_{z_{u}z_{v}}}\\bar{(c(u,v))}$ . We have $c_{\\mathrm{unif}}(u,v)\\sim\\mathrm{Unif}\\big([0,\\lambda_{z_{u}z_{v}}^{-1}]\\big)$ and we apply the previous reasoning (by replacing the exponential distributions with uniform distributions) to conclude. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B Proof of Sections 2.3 and 2.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . Let $G\\,=\\,(V,E,c)$ be a wSBM, and let $u,v\\in V$ be two arbitrary distinct vertices such that $z_{u}=a$ and $z_{v}=b$ . Then, adapting the proof13 of [39, Corollary 1], we can write ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{a b}^{\\mathrm{mb}}\\;=\\;-\\int_{0}^{\\infty}c_{u v}^{*}(x)\\log(1-p_{a b}F_{a b}(x))\\,\\mathrm{d}x,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $c_{u v}^{*}(x)$ is the probability density function of the weight of the shortest path between $u$ and $v$ and $\\begin{array}{r}{F_{a b}\\bar{(}x\\bar{)}=\\int_{0}^{x}f_{a b}(x)d x}\\end{array}$ is the cumulative distribution function of the length of an edge between two vertices belonging to communities $a$ and $b$ . ", "page_idx": 18}, {"type": "text", "text": "ti tiiso lno w1e re nasnudr eusp ptehra tb,o uwnitdhe dh ibgyh $\\frac{1\\!+\\!o(1)}{\\tau_{\\operatorname*{max}}}\\,\\frac{\\log n}{n\\rho_{n}}$ y,a ntdh $\\frac{1\\!+\\!o(1)}{\\tau_{\\operatorname*{min}}}\\,\\frac{\\log n}{n\\rho_{n}}$ $C(u,v)$ sipse cat irvaenlyd. oItms l ef uwnchtioosne $c_{u v}^{*}(x)$ tends therefore to zero outside these two bounds, and hence by setting the $\\log(\\cdot)$ Equation (B.1) to the lower and, respectively, the upper bound of the support of $C(u,v)$ , and by integrating next the pdf $c_{u v}^{*}(x)$ over the whole interval, Equation (B.1) implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n-\\log\\left(1-p_{a b}F_{a b}\\left(\\frac{1+o(1)}{\\tau_{\\operatorname*{max}}}\\frac{\\log n}{n\\rho_{n}}\\right)\\right)\\;\\leq\\;p_{a b}^{\\operatorname*{mb}}\\;\\leq\\;-\\log\\left(1-p_{a b}F_{a b}\\left(\\frac{1+o(1)}{\\tau_{\\operatorname*{min}}}\\frac{\\log n}{n\\rho_{n}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We finish the proof using $\\begin{array}{r}{F_{a b}\\left(\\frac{(1+o(1))\\log n}{\\tau_{\\operatorname*{min}}n\\rho_{n}}\\right)=(1+o(1))\\lambda_{a b}\\frac{\\log n}{\\tau_{\\operatorname*{min}}n\\rho_{n}}}\\end{array}$ and $\\begin{array}{r l}{F_{a b}\\left(\\frac{\\left(1+o\\left(1\\right)\\right)\\log n}{\\tau_{\\operatorname*{max}}n\\rho_{n}}\\right)=}&{{}}\\end{array}$ (1 + o(1))\u03bbab\u03c4mlaoxg nn\u03c1n (Assumption 2). \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 2. Let $G=(V,E,c)$ be the original graph and $G^{\\mathrm{mb}}=(V,E^{\\mathrm{mb}},c^{\\mathrm{mb}})$ its metric backbone. Let $E^{\\theta}\\subseteq E$ be the subset of edges whose cost is no more than $\\theta$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n(u,v)\\in E^{\\theta}\\iff c(u,v)\\ \\leq\\ \\theta,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and which is the edge set of the corresponding threshold graph $G^{\\theta}=(V,E^{\\theta},c^{\\theta})$ , where $c^{\\theta}$ is the restriction of $c$ to E\u03b8. ", "page_idx": 18}, {"type": "text", "text": "Denote by $W,W^{\\mathrm{mb}},W^{\\theta}\\in\\mathbb{R}_{+}^{n\\times n}$ the adjacency matrices of $G,G^{\\mathrm{mb}}$ , and $G^{\\theta}$ , respectively. ", "page_idx": 18}, {"type": "text", "text": "Overview of the proof. To prove Theorem 2, the key idea is to choose a threshold $\\theta$ large enough such that the threshold graph contains the metric backbone (i.e., $E^{\\mathrm{mb}}\\subseteq E^{\\theta})$ , but not too large so that the adjacency matrices $\\dot{W}^{\\mathrm{mb}}$ and $W^{\\theta}$ are not too different. ", "page_idx": 18}, {"type": "text", "text": "Lemma 1 ensures that, for any $\\epsilon>0$ , we have $\\begin{array}{r}{\\mathbb{P}\\left(\\operatorname*{max}_{1\\leq u,v\\leq n}C(u,v)\\leq\\frac{3+\\epsilon}{\\tau_{\\operatorname*{min}}}\\frac{\\log n}{n\\rho_{n}}\\right)=1-o(1).}\\end{array}$ . Hence, $E^{\\mathrm{mb}}\\subseteq E^{\\theta}$ whp as soon as $\\begin{array}{r}{\\theta\\ge\\frac{3+\\epsilon}{\\tau_{\\mathrm{min}}}\\frac{\\log n}{n\\rho_{n}}}\\end{array}$ . We choose $\\begin{array}{r}{\\theta=\\frac{4}{\\tau_{\\mathrm{min}}}\\frac{\\log n}{n\\rho_{n}}}\\end{array}$ , and we proceed by conditioning on the event $E^{\\mathrm{mb}}\\subseteq E^{\\theta}$ , which occurs with high probability given our choice of . ", "page_idx": 18}, {"type": "text", "text": "We will first prove that the clusters can exactly be recovered using the eigenvectors of $\\mathbb{E}W^{\\mathrm{mb}}$ . Then, using Davis-Kahan\u2019s Theorem [44, Theorem 2], we show that the clusters can also be recovered from the adjacency matrix $W^{\\mathrm{mb}}$ , provided that $\\lVert W^{\\mathrm{mb}}-\\mathbb{E}W^{\\mathrm{mb}}\\rVert$ is small enough. More precisely, we obtain an upper-bound on $\\mathrm{loss}(z,\\tilde{z})$ that depends on $\\lVert W^{\\mathrm{mb}}-\\mathbb{E}W^{\\mathrm{mb}}\\rVert$ . The main ingredients of the proof are thus (i) the justification of the choice of $\\theta$ in Lemma 1 and (ii) the careful upper-bounding of $\\lVert W^{\\mathrm{mb}}-\\mathbb{E}\\dot{W}^{\\mathrm{mb}}\\rVert$ . ", "page_idx": 18}, {"type": "text", "text": "Starting point: eigenstructure of the expected adjacency matrix $\\mathbb{E}W^{\\mathrm{mb}}$ and Davis-Kahan. Let $Z\\in\\{0,\\bar{1}\\}^{n\\times k}$ be the one-hot encoding of the true community structure $z\\in[k]^{n}$ , i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall u\\in[n],\\ \\forall a\\in[k]\\colon\\ Z_{u a}\\ =\\ {\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{~if~}}z_{u}=a,}\\\\ {0}&{{\\mathrm{~otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "FLoetr $u,v$ bWeleo hnagivneg to communities $a$ and $b$ , respectively, we write $\\mathbb{E}W_{u v}^{\\mathrm{mb}}=m_{a b}$ $M=(m_{a b})\\in\\mathbb{R}^{k\\times k}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}W^{\\mathrm{mb}}\\;=\\;Z M Z^{T}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Denote by $|\\bar{\\sigma}_{1}|\\;\\geq\\;\\cdots\\;\\geq\\;|\\bar{\\sigma}_{k}|$ the $k$ eigenvalues of $\\mathbb{E}W^{\\mathrm{mb}}$ , and by $\\bar{u}_{1},\\cdots,\\bar{u}_{k}$ their associated eigenvectors. Let $\\bar{\\Sigma}=\\mathrm{diag}(\\bar{\\sigma}_{1},\\cdot\\cdot\\cdot\\ ,\\bar{\\sigma}_{k})\\in\\mathbb{R}^{k\\times k}$ and $\\bar{U}=[\\bar{u}_{1},\\cdot\\cdot\\cdot\\ ,\\bar{u}_{k}]\\in\\mathbb{R}^{n\\times k}$ . We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}W^{\\mathrm{mb}}\\;=\\;\\bar{U}\\bar{\\Sigma}\\bar{U}^{T}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\Delta=\\mathrm{diag}(\\sqrt{n\\pi})\\in\\mathbb{R}^{k\\times k}$ be the diagonal matrix whose diagonal elements are ${\\sqrt{n\\pi_{1}}},\\cdot\\cdot\\cdot{\\sqrt{n\\pi_{k}}}$ We have ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ M Z^{T}\\:=\\:\\left(Z\\Delta^{-1}\\right)\\Delta M\\Delta\\left(Z\\Delta^{-1}\\right)^{T}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Notice that $Z\\Delta^{-1}\\in\\mathbb{R}^{n\\times k}$ has orthonormal rows (indeed $\\left(Z\\Delta^{-1}\\right)^{T}\\left(Z\\Delta^{-1}\\right)=\\Delta^{-1}Z^{T}Z\\Delta^{-1}=$ $I_{K}$ because $Z^{T}Z=\\Delta^{2}$ ). Let $O D O^{T}$ be an eigendecomposition of the symmetric real-valued matrix $\\Delta M\\Delta$ (that is, $D\\in\\mathbb{R}^{k\\times k}$ is a diagonal matrix whose diagonal elements are in decreasing order (in absolute value) and $O\\in\\mathbb{R}^{k\\times k}$ is an orthonormal matrix). Then ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ M Z^{T}\\;=\\;\\left(Z\\Delta^{-1}O\\right)D\\left(Z\\Delta^{-1}O\\right)^{T}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is an eigendecomposition of $Z M Z^{T}$ (because $Z\\Delta^{-1}O$ is orthonormal). Hence, going back to (B.3), we have $\\bar{\\Sigma}=D$ and $\\bar{U}=Z\\Delta^{-1}O$ for some orthonormal matrix $O\\in\\mathbb{R}^{k\\times k}$ . ", "page_idx": 19}, {"type": "text", "text": "Because $\\bar{U}\\,=\\,Z\\Delta^{-1}O$ , two vertices are in the same cluster if and only if their corresponding rows in $\\bar{U}$ are the same. In other words, the spectral embedding of the expected graph $\\bar{\\mathbb{E}}\\bar{W}^{\\mathrm{mb}}$ is condensed into $k$ points $(\\Delta^{-1}O)_{1\\cdot},\\cdot\\cdot\\cdot\\,,(\\Delta^{-1}\\bar{O})_{k}$ \u00b7 of $\\mathbb{R}^{k}$ . Consequently, $k$ -means on $\\bar{U}$ recovers the true clusters (up to a permutation). ", "page_idx": 19}, {"type": "text", "text": "Next, [27, Lemma 5.3] ensures that any $(1+\\epsilon)$ solution $\\tilde{z}$ of the $k$ -means problem on $U\\Sigma$ verifies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{loss}(z,\\tilde{z})\\;\\leq\\;4(4+2\\epsilon)\\operatorname*{min}_{O\\in{\\bf O}_{k\\times k}}\\|U O-\\bar{U}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{O}_{k\\times k}$ denotes the group of orthonormal $k$ -by- $k$ matrices and $\\|\\cdot\\|_{F}$ is the Frobenius norm. Davis-Kahan\u2019s Theorem [44, Theorem 2] ensures the existence of an orthogonal matrix $O\\in\\mathbf{O}_{k\\times k}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|U O-\\bar{U}\\|_{F}\\;\\leq\\;2^{3/2}k^{1/2}\\frac{\\|W^{\\mathrm{mb}}-\\mathbb{E}W^{\\mathrm{mb}}\\|}{|\\bar{\\sigma}_{k}|},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\|\\cdot\\|$ denotes the matrix operator norm. ", "page_idx": 19}, {"type": "text", "text": "Let us now establish an expression of $\\left|\\bar{\\boldsymbol{\\sigma}}_{k}\\right|$ . Observe firstly that $\\Delta M\\Delta$ and $M\\Delta^{2}$ have the same eigenvalues.14 From Lemma 4, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{2\\tau_{\\mathrm{max}}^{2}}(\\Lambda\\odot B)_{a b}\\frac{\\log^{2}n}{n^{2}\\rho_{n}}\\ \\leq\\ m_{a b}\\ \\leq\\ \\frac{1}{2\\tau_{\\mathrm{min}}^{2}}(\\Lambda\\odot B)_{a b}\\frac{\\log^{2}n}{n^{2}\\rho_{n}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The definition of $T=[\\Lambda\\odot B]\\,\\mathrm{diag}(\\pi)$ in (2.1) and the fact that $\\Delta^{2}=n\\,\\mathrm{diag}(\\pi)$ further imply that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{2\\tau_{\\mathrm{max}}^{2}}T_{a b}\\frac{\\log^{2}n}{n\\rho_{n}}\\;\\leq\\;\\left(M\\Delta^{2}\\right)_{a b}\\;\\leq\\;\\frac{1}{2\\tau_{\\mathrm{min}}^{2}}T_{a b}\\frac{\\log^{2}n}{n\\rho_{n}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\theta\\,=\\,\\frac{4}{\\tau_{\\mathrm{min}}}\\frac{\\log n}{n\\rho_{n}}}\\end{array}$ and $\\mu$ is the smallest (in absolute value) eigenvalue of $T$ . Using the assumption $\\tau_{\\operatorname*{min}}=\\tau_{\\operatorname*{max}}$ , we obtain that $\\begin{array}{r}{M\\Delta^{2}=\\frac{\\theta\\log n}{8\\tau_{\\operatorname*{min}}}T}\\end{array}$ \u03b8 log nT and thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{\\sigma}_{k}\\;=\\;\\frac{\\mu}{8\\tau_{\\operatorname*{min}}}\\theta\\log n.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, combining (B.4), (B.5) and (B.6), we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{loss}(z,\\tilde{z})\\;\\leq\\;(4+2\\epsilon)2^{5}k\\left(\\frac{8\\tau_{\\mathrm{min}}}{\\mu}\\cdot\\frac{\\|W^{\\mathrm{mb}}-\\mathbb{E}W^{\\mathrm{mb}}\\|}{\\theta\\log n}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Core of the proof: concen\u221atration of $W^{\\mathrm{mb}}$ around $\\mathbb{E}W^{\\mathrm{mb}}$ . We finish the proof by showing that $\\lVert W^{\\mathrm{mb}}-\\mathbb{E}\\bar{W}^{\\mathrm{mb}}\\rVert=O\\left(\\theta\\sqrt{\\log n}\\right)$ whp. First, by a triangle inequality, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|W^{\\mathrm{mb}}-\\mathbb{E}W^{\\mathrm{mb}}\\|\\;\\leq\\;\\|W^{\\mathrm{mb}}-W^{\\theta}-\\mathbb{E}\\left[W^{\\mathrm{mb}}-W^{\\theta}\\right]\\|+\\|W^{\\theta}-\\mathbb{E}W^{\\theta}\\|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For ease of the exposition, we will upper-bound the term of (B.8) in the following order: (i) the second term $\\lVert W^{\\theta}-\\mathbb{E}W^{\\theta}\\rVert$ , and then (ii) the first term $\\lVert W^{\\mathrm{mb}}-\\dot{W^{\\theta}}-\\mathbb{E}\\left[W^{\\mathrm{mb}}-\\check{W}^{\\theta}\\right]\\rVert$ . ", "page_idx": 20}, {"type": "text", "text": "(i) Let us first study $\\lVert W^{\\theta}-\\mathbb{E}W^{\\theta}\\rVert$ . The matrix $X=W^{\\theta}/\\theta$ is symmetric with zero-diagonal, whose entries $\\{X_{u v},u<v\\}$ are independent $[0,1]$ -valued random variables. Moreover, Lemma 3 shows that E $\\left[\\dot{X}_{u v}\\,\\right|z_{u}=a,\\dot{z}_{v}=b\\right]=p_{a b}\\lambda_{a b}\\bar{\\theta}=\\dot{\\Theta}(\\log n/n)$ . Thus, [18, Theorem 5] ensures that for any $c>0$ there exists $c^{\\prime}>0$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\|W^{\\theta}-\\mathbb{E}W^{\\theta}\\|\\ge c^{\\prime}\\,\\theta\\sqrt{\\log n}\\right)\\;\\le\\;n^{-c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(ii) Next, let us study $\\lVert W^{\\mathrm{mb}}-W^{\\theta}-\\mathbb{E}\\left[W^{\\mathrm{mb}}-W^{\\theta}\\right]\\rVert_{2}$ . Denote $Y=-\\left(W^{\\mathrm{mb}}-W^{\\theta}\\right)/\\theta$ . By fLoer malml $\\theta$ .h aHveen $E^{\\mathrm{mb}}\\subseteq E^{\\theta}$ (for $n$ large enough). Moreover, $W_{u v}^{\\theta}=W_{u v}^{\\mathrm{mb}}$ $\\{u,v\\}\\in E^{\\mathrm{mb}}\\cap E^{\\theta}=E^{\\mathrm{mb}}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\cal Y}_{u v}\\;=\\;\\left\\{\\frac{W_{u v}^{\\theta}}{\\theta}\\right.\\quad\\mathrm{if~}\\{u,v\\}\\in{\\cal E}^{\\theta}\\backslash{\\cal E}^{\\mathrm{mb}},\\quad\\mathrm{~and~}\\quad{\\mathbb{E}}{\\cal Y}_{u v}\\;=\\;\\left\\{\\frac{{\\mathbb{E}}W_{u v}^{\\theta}}{\\theta}\\quad\\mathrm{if~}\\{u,v\\}\\in{\\cal E}^{\\theta}\\backslash{\\cal E}^{\\mathrm{mb}},\\quad\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can thus rewrite the matrices $Y$ and $\\mathbb{E}Y$ as $Y=R\\odot W^{\\theta}/\\theta$ and $\\mathbb{E}Y=R\\odot\\mathbb{E}W^{\\theta}/\\theta$ , where $\\odot$ denote the Hadamard (entry-wise) matrix product and $R\\in\\{0,1\\}^{n\\times n}$ is defined by ", "page_idx": 20}, {"type": "equation", "text": "$$\nR_{u v}\\;=\\;R_{v u}\\;=\\;\\left\\{\\begin{array}{l l}{{1}}&{{\\mathrm{~if~}\\{u,v\\}\\in E^{\\theta}\\backslash E^{\\mathrm{mb}},}}\\\\ {{0}}&{{\\mathrm{~otherwise.}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\mathcal{E}_{c_{1}}$ be the event that all rows of $R$ have at most $c_{1}\\log n$ non-zero entries (with $c_{1}>0$ ), i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{E}_{c_{1}}\\;=\\;\\left\\{\\forall u\\in[n]\\colon\\sum_{v=1}^{n}\\mathbb{1}\\{R_{u v}\\neq0\\}\\;\\leq\\;c_{1}\\log n\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Because $E^{\\mathrm{mb}}\\subseteq E^{\\theta}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{1}\\{R_{u v}\\neq0\\}\\;=\\;R_{u v}\\;=\\;\\mathbb{1}\\{(u,v)\\in E^{\\theta}\\backslash E^{\\mathrm{mb}}\\}\\;\\leq\\;\\mathbb{1}\\{(u,v)\\in E^{\\theta}\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, $\\mathcal{E}_{c_{1},\\theta}\\subseteq\\mathcal{E}_{c_{1}}$ , where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{E}_{c1,\\theta}\\ =\\ \\left\\{\\forall u\\in[n]\\colon\\sum_{v=1}^{n}\\mathbb{1}\\{(u,v)\\in E^{\\theta}\\}\\ \\leq\\ c_{1}\\log n\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, $\\mathbb{P}(\\mathcal{E}_{c_{1}})\\ge\\mathbb{P}(\\mathcal{E}_{c_{1},\\theta})$ . Recall also that $\\mathbb{1}\\{(u,v)\\in E^{\\theta}\\}=\\mathbb{1}\\{c(u,v)\\le\\theta\\}$ . Thus, by Lemma 2, for any $c_{0}>0$ there exists a $c_{1}>0$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathcal{E}_{c_{1}}\\right)\\;\\geq\\;1-n^{-c_{0}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Conditioned on the high probability event $\\mathcal{E}_{c_{1}}$ , every row of the matrix $R$ has at most $c_{1}\\log n$ non-zero elements. Moreover, $W/\\theta$ is symmetric and has bounded (hence sub-gaussian) entries. Therefore [5, Corollary 3.9] ensures the existence of constants $C,C^{\\prime}>0$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(R\\odot\\left(W^{\\theta}-\\mathbb{E}W^{\\theta}\\right)/\\theta\\;\\ge\\;C\\sqrt{\\log n}+t\\Big|\\mathcal{E}_{c_{1}}\\right)\\;\\le\\;e^{-C^{\\prime}t^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using $t\\,=\\,C^{\\prime\\prime}{\\sqrt{\\log n}}$ , and because $Y-\\mathbb{E}Y\\,=\\,R\\odot\\left(W^{\\theta}-\\mathbb{E}W^{\\theta}\\right)/\\theta$ , we obtain the following statement: for any $c>0$ , there exists $c^{\\prime}>0$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|Y-\\mathbb{E}Y\\|_{2}\\;\\geq\\;c^{\\prime}\\sqrt{\\log n}\\,|\\,\\mathcal{E}_{c_{1}}\\right)\\;\\leq\\;n^{-c}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using (B.10), we finally obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|W^{\\mathrm{mb}}-W^{\\theta}-\\mathbb{E}\\left[W^{\\mathrm{mb}}-W^{\\theta}\\right]\\|\\;\\geq\\;c^{\\prime}\\theta\\sqrt{\\log n}\\right)\\;\\leq\\;2n^{-c}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Conclusion Using (B.8), (B.9) and (B.11), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lVert W^{\\mathrm{mb}}-\\mathbb{E}W^{\\mathrm{mb}}\\rVert\\;\\leq\\;2c^{\\prime}\\theta\\sqrt{\\log n},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with probability at least $1-3n^{-c}$ . The proof ends by combining (B.12) with (B.7). ", "page_idx": 21}, {"type": "text", "text": "B.3 Additional Lemmas ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 1. Let $(z,G)\\,\\sim\\,\\mathrm{wSBM}(n,\\pi,p,F)$ and suppose that Assumptions $^{\\,I}$ and 2 hold. Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{u,v\\in[n]}C(u,v)\\ \\leq\\ (1+o(1))\\frac{3}{\\tau_{\\operatorname*{min}}}\\frac{\\log n}{n\\rho_{n}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We proceed as in the proof of Proposition 1 by firstly assuming that $F_{a b}\\sim\\mathrm{Exp}(\\lambda_{a b})$ . The extension to more general weight distributions follows from the same coupling argument presented in Section A.2 and is thus omitted. ", "page_idx": 21}, {"type": "text", "text": "We use the same notations as in the proof of Proposition 1. Re\u221acall in particular the n\u221aotations $C_{u}(q)$ , $U_{\\ell}(q)$ and $X_{\\ell\\ell^{\\prime}}(u,q)$ . Because we established that $\\mathcal{B}(u,C_{u}(\\sqrt{n\\log n}))\\cap B(v,C_{v}(\\sqrt{n\\log n}))\\neq\\hat{\\emptyset}$ whp, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nC(u,v)\\;\\leq\\;C_{u}(\\sqrt{n\\log n})+C_{v}(\\sqrt{n\\log n}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{u,v}{\\operatorname*{max}}\\,C(u,v)\\ \\leq\\ \\underset{u,v}{\\operatorname*{max}}\\,\\Big(C_{u}(\\sqrt{n\\log n})+C_{v}(\\sqrt{n\\log n})\\Big)}\\\\ &{=\\ 2\\operatorname*{max}_{u}C_{u}(\\sqrt{n\\log n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using a union bound, we obtain for any $t>0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}\\left(\\operatorname*{max}_{u}C_{u}(\\sqrt{n\\log n})\\geq t\\right)\\;\\leq\\;\\displaystyle\\sum_{u=1}^{n}\\mathbb{P}\\left(C_{u}(\\sqrt{n\\log n})\\geq t\\right)}\\\\ &{}&{=\\;n\\mathbb{P}\\left(C_{u^{*}}(\\sqrt{n\\log n})\\geq t\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\boldsymbol{u}^{*}$ denotes an arbitrary vertex chosen in $[n]$ . For any $s>0$ , we have (by Chernoff bounds) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(C_{u^{*}}(\\sqrt{n\\log n})\\geq t\\right)\\ \\leq\\ e^{-s t}\\mathbb{E}\\left[e^{s C_{u^{*}}(\\sqrt{n\\log n})}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall from (A.3) that $C_{u^{*}}(q\\!+\\!1)\\!-\\!C_{u^{*}}(q)\\mid S(u,\\cdot)$ are i.i.d. $\\operatorname{Exp}(\\theta_{q})$ with $\\begin{array}{r}{\\theta_{q}=\\sum_{\\ell,\\ell^{\\prime}}\\lambda_{\\ell\\ell^{\\prime}}S_{\\ell\\ell^{\\prime}}(u,q)}\\end{array}$ . As for $X\\sim\\operatorname{Exp}(\\theta)$ we have $\\mathbb{E}[e^{s X}]=\\theta/(\\theta-s)=1+s/(\\theta-s)$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[e^{s C_{\\mathfrak u^{+}}\\left(\\sqrt{n\\log n}\\right)}\\mid S(u,\\cdot)\\right]}&{=\\displaystyle\\prod_{q=1}^{\\sqrt{n\\log n}}\\mathbb{E}\\left[e^{s(C_{\\mathfrak u^{+}}(q+1)-C_{\\mathfrak u^{+}}(q))}\\mid S(u,\\cdot)\\right]}\\\\ &{=\\displaystyle\\prod_{q=1}^{\\sqrt{n\\log n}}\\left(1+\\frac{s}{\\theta_{q}-s}\\right)}\\\\ &{=\\displaystyle\\exp\\left(\\sum_{q=1}^{\\sqrt{n\\log n}}\\log\\left(1+\\frac{s}{\\theta_{q}-s}\\right)\\right)}\\\\ &{\\leq\\displaystyle\\exp\\left(\\sum_{q=1}^{\\sqrt{n\\log n}}\\frac{s}{\\theta_{q}-s}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $0<\\epsilon<1/6$ . From Equations (A.5) and (A.6), we have whp (for $n$ large enough) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\theta_{q}\\;\\geq\\;(1-\\epsilon)n\\rho_{n}\\tau_{\\mathrm{min}}q.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Choose $s=(1-2\\epsilon)n\\rho_{n}\\tau_{\\mathrm{min}}$ and let $\\alpha_{n}$ be a diverging sequence verifying $\\alpha_{n}=o(\\sqrt{n\\log n})$ to be chosen later. We split the sum as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{q=1}^{\\sqrt n\\log n}\\frac{s}{\\theta_{q}-s}\\;=\\;\\sum_{q=1}^{\\alpha_{n}}\\frac{s}{\\theta_{q}-s}+\\sum_{q=\\alpha_{n}+1}^{\\sqrt n\\log n}\\frac{s}{\\theta_{q}-s}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We have for any $1\\leq q\\leq\\alpha_{n}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\theta_{q}-s\\;\\ge\\;n\\rho_{n}\\tau_{\\mathrm{min}}q\\left(1-\\epsilon-{\\frac{1-2\\epsilon}{q}}\\right)\\;\\ge\\;n\\rho_{n}\\tau_{\\mathrm{min}}q\\epsilon,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "while for $\\alpha_{n}\\geq q$ we have $(1-2\\epsilon)\\alpha_{n}\\leq\\epsilon$ (for $n$ large enough) and thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{I}_{q}-s\\;\\geq\\;n\\rho_{n}\\tau_{\\mathrm{min}}q\\left(1-\\epsilon-\\frac{1-2\\epsilon}{q}\\right)\\;\\geq\\;n\\rho_{n}\\tau_{\\mathrm{min}}q\\left(1-\\epsilon-\\frac{1-2\\epsilon}{\\alpha_{n}}\\right)\\;\\geq\\;n\\rho_{n}\\tau_{\\mathrm{min}}q(1-2\\epsilon),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{q=1}^{\\sqrt{n\\log n}}\\frac{s}{\\theta_{q}-s}\\;\\leq\\;(1-2\\epsilon)\\left[\\frac{1}{\\epsilon}\\sum_{q=1}^{\\alpha_{n}}\\frac{1}{q}+\\frac{1}{1-2\\epsilon}\\sum_{q=\\alpha_{n}+1}^{\\sqrt{n\\log n}}\\frac{1}{q}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Recalling that $\\begin{array}{r}{\\sum_{q=m+1}^{n}q^{-1}\\le\\log(n/m)}\\end{array}$ for any $m\\geq1$ (by integration) further leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{q=1}^{\\sqrt{n\\log n}}\\frac{s}{\\theta_{q}-s}\\;\\leq\\;(1-2\\epsilon)\\left[\\frac{1}{\\epsilon}(1+\\log\\alpha_{n})+\\frac{1}{1-2\\epsilon}\\log\\left(\\frac{\\sqrt{n\\log n}}{\\alpha_{n}}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{s C_{u^{*}}(\\sqrt{n\\log n})}\\,|\\,S(u,\\cdot)\\right]\\;\\leq\\;\\exp\\left((1-2\\epsilon)\\left[\\frac{1}{\\epsilon}(1+\\log\\alpha_{n})+\\frac{1}{1-2\\epsilon}\\log\\left(\\frac{\\sqrt{n\\log n}}{\\alpha_{n}}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Because the right-hand side of this last upper bound does not depend on $S(u,\\cdot)$ , we have by the total law of probabilities, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{s C_{u^{*}}(\\sqrt{n\\log n})}\\right]\\;\\leq\\;\\exp\\left((1-2\\epsilon)\\left[\\frac{1}{\\epsilon}(1+\\log\\alpha_{n})+\\frac{1}{1-2\\epsilon}\\log\\left(\\frac{\\sqrt{n\\log n}}{\\alpha_{n}}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We choose $\\alpha_{n}=\\lceil{\\sqrt{\\log n}}\\rceil$ . Because $\\alpha_{n}=\\omega(1)$ , $\\alpha_{n}=o(\\log n)$ , and $\\epsilon$ is fixed, we have for $n$ large enough, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{1}{\\epsilon}(1+\\log\\alpha_{n})\\,\\leq\\,\\epsilon\\frac{\\log n}{2}}\\\\ {\\displaystyle\\frac{1}{1-2\\epsilon}\\log\\left(\\frac{\\sqrt{n\\log n}}{\\alpha_{n}}\\right)\\,\\leq\\,(1+3\\epsilon)\\frac{\\log n}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality uses $\\epsilon<1/6$ . Thus, we can rewrite the inequality (B.15) (for $n$ large enough) as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{s C_{u^{*}}(\\sqrt{n\\log n})}\\right]\\ \\leq\\ \\exp\\left((1-2\\epsilon)(1+4\\epsilon)\\frac{\\log n}{2}\\right)\\ \\leq\\ \\exp\\left((1+2\\epsilon)\\frac{\\log n}{2}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $t=(3+\\epsilon^{\\prime})\\log n/(2n\\rho_{n}\\tau_{\\mathrm{min}})$ with $\\epsilon^{\\prime}=15\\epsilon$ . Recall that $s=(1-2\\epsilon)n\\rho_{n}\\tau_{\\mathrm{min}}$ . From (B.14) we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(C_{u^{*}}(\\sqrt{n\\log n})\\ge t\\right)\\,\\le\\,e^{-(1-2\\epsilon)(3+\\epsilon^{\\prime})\\frac{\\log n}{2}+(1+2\\epsilon)\\frac{\\log n}{2}}}&{}\\\\ {=\\,e^{-\\frac{\\log n}{2}\\left(2-8\\epsilon-2\\epsilon\\epsilon^{\\prime}+\\epsilon^{\\prime}\\right)}}&{}\\\\ {\\,\\le\\,e^{-\\log n(1+\\epsilon)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where in the last li\u221ane we uses $-8\\epsilon-2\\epsilon\\epsilon^{\\prime}+\\epsilon^{\\prime}=6\\epsilon-30\\epsilon^{2}\\geq\\epsilon$ (using $\\epsilon<1/6$ ). Because $1+\\epsilon>1$ , we have $\\mathbb{P}\\left(C_{u^{*}}(\\sqrt{n\\log n})\\geq t\\right)=o(n^{-1})$ , and we conclude using (B.13). \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma 2. Let $(z,G)\\sim\\mathrm{wSBM}(n,\\pi,p,F)$ and suppose Assumptions $^{\\,l}$ and 2 hold. Let $c_{0},c_{1}>0$ be two constants (independent of n), and let \u03b8 = c0lnog\u03c1 nn . Denote ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{E}_{c_{1},\\theta}\\;=\\;\\left\\{\\forall u\\in[n]\\colon\\sum_{v=1}^{n}\\mathbb{1}\\{c(u,v)\\le\\theta\\}\\;\\le\\;\\left(4B_{\\operatorname*{max}}\\lambda_{\\operatorname*{max}}c_{0}+c_{1}\\right)\\log n\\right\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, $\\mathbb{P}\\left(\\mathcal{E}_{c_{1},\\theta}\\right)\\ \\geq\\ 1-n^{-c_{1}}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. For $u\\neq v\\in[n]$ such that $z_{u}=a$ and $z_{v}=b$ , we have (recall that the probability of having an edge $(u,v)\\in E$ is $p_{a b}$ , and the cost of this edge is sampled from $F_{a b}$ ) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(c(u,v)\\leq\\theta\\,|\\,z_{u}=a,z_{v}=b\\right)\\,=\\,p_{a b}F_{a b}(\\theta).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recall also that, by Assumption 1, we have $p_{a b}\\,=\\,B_{a b}\\rho_{n}$ and by Assumption 1 $F_{a b}(x)\\,=\\,(1\\,+$ $o(1))\\lambda_{a b}x$ . Let $\\epsilon>0$ . Using the law of total probability, we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(c(u,v)\\leq\\theta\\right)\\,\\leq\\,(1+o(1))B_{\\operatorname*{max}}\\lambda_{\\operatorname*{max}}\\theta\\rho_{n},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $B_{\\mathrm{max}}=\\mathrm{max}_{1\\leq a,b\\leq k}\\,B_{a b}$ and $\\lambda_{\\operatorname*{max}}=\\operatorname*{max}_{1\\leq a,b\\leq k}\\lambda_{a b}$ . Thus, for $n$ large enough we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(c(u,v)\\le\\theta\\right)\\ \\le\\ 2B_{\\operatorname*{max}}\\lambda_{\\operatorname*{max}}\\theta\\rho_{n},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For ease of notations, let $p_{\\theta}=2B_{\\mathrm{max}}\\lambda_{\\mathrm{max}}\\theta\\rho_{n}$ and $\\tilde{c}=4B_{\\mathrm{max}}\\lambda_{\\mathrm{max}}c_{0}+1+c_{1}$ . ", "page_idx": 23}, {"type": "text", "text": "For $u\\in[n]$ we denote $\\begin{array}{r}{d_{u}=\\sum_{v=1}^{n}\\mathbb{1}\\{c(u,v)\\leq\\theta\\}}\\end{array}$ . We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{max}_{u\\in[n]}d_{u}\\ge\\tilde{c}\\log n\\right)\\le\\sum_{u\\in[n]}\\mathbb{P}\\left(d_{u}\\ge\\tilde{c}\\log n\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, for any $t>0$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{t d_{u}}\\right]\\ \\leq\\ \\left(1-p_{\\theta}+p_{\\theta}e^{t}\\right)^{n}\\ \\leq\\ e^{n p_{\\theta}\\left(e^{t}-1\\right)},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second inequality used $\\log(1+x)\\leq x$ . Let $\\tilde{c}>0$ . Using Chernoff\u2019s bounds, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(d_{u}\\ge\\tilde{c}\\log n\\right)\\;\\le\\;e^{-t\\tilde{c}\\log n+n p_{\\theta}(e^{t}-1)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $t=1$ . Because $e^{1}-1\\leq2$ and $n p_{\\theta}=2B_{\\mathrm{max}}\\lambda_{\\mathrm{max}}c_{0}\\log n$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(d_{u}\\ge\\tilde{c}\\log n\\right)\\;\\le\\;e^{-\\log n(\\tilde{c}\\log n-4B_{\\operatorname*{max}}\\lambda_{\\operatorname*{max}}c_{0})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We finish the proof by letting $\\tilde{c}=4B_{\\mathrm{max}}\\lambda_{\\mathrm{max}}c_{0}+1+c_{1}$ and using (B.16). ", "page_idx": 23}, {"type": "text", "text": "Lemma 3. Let $(z,G)\\sim\\mathrm{wSBM}(n,\\pi,p,F)$ and suppose Assumptions $^{\\,I}$ and 2 hold. Let $\\theta=c{\\frac{\\log n}{n\\rho_{n}}}$ Denote $W$ the weighted adjacency matrix of $G$ , and $W^{\\theta}$ the weighted adjacency matrix of the threshold graph. Let $u,v\\in[n]$ such that $z_{u}=a$ and $z_{v}=b$ . We have E $[W_{u v}^{\\bar{\\theta^{}}}]=p_{a b}^{\\;\\;\\;}\\lambda_{a b}\\theta^{2}/2$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. We have $W_{u v}^{\\theta}=W_{u v}\\mathbb{1}\\big\\{W_{u v}\\leq\\theta\\big\\}$ . Moreover, $W_{u v}\\mid W_{u v}\\neq0$ is sampled from $F_{a b}$ . Thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[W_{u v}^{\\theta}\\right]\\;=\\;p_{a b}\\mathbb{E}\\left[W_{u v}\\mathbb{1}\\{W_{u v}\\leq\\theta\\}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Denote also $f_{a b}$ the pdf of $F_{a b}$ . Recall by Assumption 2 that $f_{a b}(x)=\\lambda_{a b}+O(x)$ . Because $\\theta\\ll1$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[W_{u v}^{\\theta}\\right]\\;=\\;p_{a b}\\lambda_{a b}\\frac{\\theta^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 4. Let $(z,G)\\sim\\mathrm{wSBM}(n,\\pi,p,F)$ and suppose Assumptions 1 and 2 hold. Let $\\theta=c{\\frac{\\log n}{n\\rho_{n}}}$ Denote $W$ the weighted adjacency matrix of $G_{i}$ , and $W^{\\mathrm{mb}}$ the weighted adjacency matrix of the metric backbone of $G$ . Let $u,v\\in[n]$ such that $z_{u}=a$ and $z_{v}=b$ . We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{2\\tau_{\\operatorname*{max}}^{2}}(\\Lambda\\odot B)_{a b}\\frac{\\log^{2}n}{n^{2}\\rho_{n}}\\ \\leq\\ \\mathbb{E}\\left[W_{u v}^{\\mathrm{mb}}\\right]\\ \\leq\\ \\frac{1}{2\\tau_{\\operatorname*{min}}^{2}}(\\Lambda\\odot B)_{a b}\\frac{\\log^{2}n}{n^{2}\\rho_{n}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $u,v\\in[n]$ be two arbitrary vertices such that $z_{u}\\,=\\,a$ and $z_{v}\\,=\\,b$ . We have $W_{u v}^{\\mathrm{mb}}=$ $c(u,v)\\mathbb{1}\\{(u,v)\\in E^{\\mathrm{mb}}\\}$ . Recall from Proposition 1 that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\tau_{\\mathrm{max}}\\right)^{-1}\\;\\leq\\;\\frac{n\\rho_{n}}{\\log n}C(u,v)\\;\\leq\\;\\left(\\tau_{\\mathrm{min}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "whp, where $C(u,v)$ is the cost of the shortest path from $u$ to $v$ . Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[W_{u v}^{\\mathrm{mb}}\\right]\\;=\\;\\mathbb{E}\\left[c(u,v)\\cap(u,v)\\in E^{\\mathrm{mb}}\\right]}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\;p_{\\mathrm{min}}\\lambda_{a b}\\displaystyle{\\frac{1}{2}}\\left(\\frac{\\log n}{\\tau_{\\mathrm{min}}n\\rho_{n}}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used Lemma 3 with $\\begin{array}{r}{\\theta=\\frac{1}{\\tau_{\\operatorname*{min}}}\\frac{\\log n}{n\\rho_{n}}}\\end{array}$ 1 log n. Similarly, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[W_{u v}^{\\mathrm{mb}}\\right]\\;\\ge\\;\\mathbb{E}\\left[c(u,v)\\mathbb{1}\\left\\{c(u,v)\\le\\frac{1}{\\tau_{\\operatorname*{max}}}\\frac{\\log n}{n\\rho_{n}}\\right\\}\\right]}\\\\ &{=\\;p_{a b}\\lambda_{a b}\\frac{1}{2}\\left(\\frac{\\log n}{\\tau_{\\operatorname*{max}}n\\rho_{n}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recalling that $p_{a b}=B_{a b}\\rho_{n}$ , we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{2\\tau_{\\operatorname*{max}}^{2}}(\\Lambda\\odot B)_{a b}\\frac{\\log^{2}n}{n^{2}\\rho_{n}}\\ \\leq\\ \\mathbb{E}\\left[W_{u v}^{\\mathrm{mb}}\\right]\\ \\leq\\ \\frac{1}{2\\tau_{\\operatorname*{min}}^{2}}(\\Lambda\\odot B)_{a b}\\frac{\\log^{2}n}{n^{2}\\rho_{n}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C Additional Numerical Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 Additional Material for Section 4 ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "Kx8I0rP7w2/tmp/6c5b795b28fd12ce5dce393245e595f5dc29f10b10ec5754b935eee57bab26cc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 2: Dimensions of networks considered. High school and primary school data sets are taken from http://www.sociopatterns.org, where the weights represent the number of interactions between students. $D B L P$ and Amazon data sets are unweighted and taken from https://snap. stanford.edu/data/. $\\bar{d}$ denotes the average unweighted degree, i.e., $2|E|/n$ . ", "page_idx": 24}, {"type": "table", "img_path": "Kx8I0rP7w2/tmp/b04a6e5b92ed9cb3c899dcf343c0e130edcc42e7c43935806428148a9c816b66.jpg", "table_caption": ["Table 3: Ratio of edges kept by the metric backbone in various real graphs. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "We highlight the difference between the metric backbone and the spectral sparsification of the Primary school data set in Figure 5. Whereas Figure 2 highlights a clear pattern between the metric backbone and the threshold graph, finding from Figure 5 any pattern in the edges present in the metric backbone but not in the spectral sparsification (and vice-versa) is much harder. Although both sparsified graphs preserve the community structure very well, they do so by keeping a very different set of edges. ", "page_idx": 24}, {"type": "image", "img_path": "Kx8I0rP7w2/tmp/e8260cf96485474aa783e152e4e67adcc78b467aa59b8fa8325d8480d2d88ff1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 5: Graphs obtained from Primary school data set, after taking the metric backbone (Figure 5a) and after spectral sparsification (Figure 5b), drawn using the same layout. Vertex colors represent the true clusters. Edges present in the metric backbone but not in the spectral sparsifier graph are highlighted in red. Similarly, edges present in the spectral sparsifier graph, but not in the metric backbone, are highlighted in blue. ", "page_idx": 25}, {"type": "text", "text": "We preprocessed the DBLP and Amazon datasets by only keeping components where the second eigenvalue of the normalized Laplacian of each component that is kept is larger than 0.1. This ensures that we do not consider communities that are not well-connected. ", "page_idx": 25}, {"type": "text", "text": "We additionally show in Figure 6 the performance of the three clustering algorithms (Bayesian, Leiden, and spectral clustering) on four other data sets. As we do not have any ground truth for these additional data sets, we computed the ARI between the clustering obtained on the original network and on the sparsified network. We observe that the largest ARI is almost always obtained for the metric backbone. This shows that the metric backbone is the sparsification that best preserves the community structures of the original networks. ", "page_idx": 25}, {"type": "image", "img_path": "Kx8I0rP7w2/tmp/cea1de364968111f75095f95f42432d82e0304752056dcda992e7a90e8cf063b.jpg", "img_caption": ["Figure 6: Effect of sparsification on the performance of clustering algorithms compared to the results of the same clustering algorithms ran on the original graph. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "C.2 Additional Material for Section 5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Number of edges in the $q$ -nearest neighbor graph and its metric backbone We show in Figure 7 the number of edges in $G_{q}$ and $G_{q}^{\\mathrm{mb}}$ (we do not show the number of edges of the spectral sparsified $G_{g}^{\\mathrm{ss}}$ because the hyperparameters of $G_{q}^{\\mathrm{ss}}$ are chosen such that $G_{q}^{\\mathrm{mb}}$ and $G_{q}^{\\mathrm{ss}}$ have the same number of edges). ", "page_idx": 25}, {"type": "text", "text": "Exploring another similarity measure Finally, we provide additional evidence supporting the robustness of the metric backbone with respect to the number of nearest neighbors $q$ by providing the results of another clustering algorithm, namely threshold-based subspace clustering (TSC). TSC is a subspace clustering algorithm and was shown to succeed under very general conditions on the high-dimensional data set to be clustered [21]. TSC performs spectral clustering on a $q$ -nearest neighbors graph obtained using the following similarity measure ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sin(x_{u},x_{v})=\\exp\\left(-2\\operatorname{arccos}\\left(<x_{u},x_{v}>\\right)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "image", "img_path": "Kx8I0rP7w2/tmp/b716714b904187cafdce5780935eec269734a5f2565d7b16a82cc450653eb5ad.jpg", "img_caption": ["Figure 7: Number of edges in the $q$ -nearest neighbour graph (built using Gaussian kernel similarity) and its metric backbone. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Because in Section 5 we showed the performance of spectral clustering using the Gaussian kernel similarity, these additional experiments also show the robustness of our results with respect to the choice of the similarity measure. Figure 8 shows the performance of TSC on $G_{q}$ and its sparsified versions Gqmb and $G_{q}^{\\mathrm{ss}}$ , and Figure 9 shows the number of edges (before and after sparsification). ", "page_idx": 26}, {"type": "image", "img_path": "Kx8I0rP7w2/tmp/a8b963c18e3d1b66eef5c2aff2e1495962aa36b642d769ab8dc44388b67939fc.jpg", "img_caption": ["Figure 8: Performance of TSC on subsets of MNIST and FashionMNIST datasets, and on the HAR dataset. The plots show the ARI averaged over 10 trials, and the error bars show the standard error. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "Kx8I0rP7w2/tmp/5944112f3b5924675c813500fbb6c59e87fbd248ed50bfa6ce82164e8c4edc65.jpg", "img_caption": ["Figure 9: Number of edges remaining in each graph, using dot product similarity. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Computing Resources All experiments were run on a standard laptop with 16GB of RAM and 12th Gen Intel(R) Core(TM) i7-1250U CPU. ", "page_idx": 26}, {"type": "text", "text": "To compute the full metric backbone, we used the igraph distances implementation [11]. It takes around 7 minutes for graphs with 10, 000 vertices and $q=10$ . For the same graph, computing the spectral sparsification takes around 15 minutes. In general, computing the spectral sparsification was 2 to 3 times slower than computing the metric backbone. Computing the approximate metric backbone takes around 100 seconds for 70, 000 vertices and $q=132$ on our $c{++}$ implementation. ", "page_idx": 26}, {"type": "text", "text": "For spectral clustering and TSC algorithms, the bottleneck is the clustering part: running the scikitlearn implementation of spectral clustering takes 3.5 hours for MNIST and 2 hours for FashionMNIST while we only needed 100 seconds to obtain the metric backbone approximation. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The abstract and introduction clearly state all the theoretical and numerical claims made in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our work points out the assumptions made in the different theorems (especially Assumption 1 and 2). The numerical section (done only using real data) highlights that the results are robust to model misspecification (by going from weighted SBM to real data sets). We used several types of data sets (such as social networks, interaction networks, images), and several clustering algorithms to show how the method generalises well. Computational efficiency is discussed in Section 3. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All the theorems, formulas, and proofs in the paper are numbered and crossreferenced. All assumptions are clearly stated or referenced in the statement of all the theorems. The proofs are in the supplemental material, but we provide a proof sketch of the main results in the main text. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: we release all the code and data used to perform the experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 28}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: we release all the code and data used to perform the experiments. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: we present in the core of the paper all the detail necessary to appreciate the numerical results. The supplement (Appendix C) contains additional information (such as network statistics). Finally, we also release the code. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: the results are accompanied by error bars, showing the standard error. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Details about compute resources are included in the appendix C. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We respect the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: there is no direct societal impact of the work performed. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: the paper poses no such risks. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: we cited all the original papers that produced the code of the various algorithms, packages and datasets used. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: the paper does not release new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]