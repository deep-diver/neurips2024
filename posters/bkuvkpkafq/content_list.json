[{"type": "text", "text": "ChatQA: Surpassing GPT-4 on Conversational QA and RAG ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zihan Liu \u22171 Wei Ping \u22171 Rajarshi Roy 1 Peng Xu 1   \nChankyu Lee 1 Mohammad Shoeybi 1 Bryan Catanzaro 1 ", "page_idx": 0}, {"type": "text", "text": "1 NVIDIA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the CHATRAG BENCH, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the CHATRAG BENCH, without relying on any synthetic data from OpenAI GPT models. Notably, Llama3- ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09 by a margin. These results demonstrate the exceptional quality of the proposed ChatQA recipe. To advance research in this field, we open-sourced the model weights, instruction tuning data, CHATRAG BENCH, and retriever for the community: https://chatqa-project.github.io/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Most recently, ChatGPT (OpenAI, 2022) and its follow ups (OpenAI, 2023; Anthropic, 2023b; Google, 2023) have led to the paradigm shift of building question answering (QA) and retrievalaugmented generation (RAG) system in production and research community. In particular, the following aspects of the models are preferred: i) The users can interact with the QA models in a conversational way, thus one can easily raise follow-up questions. ii) The models are capable of integrating retrieved chunks of evidence in both open-domain or long document settings, where the provided context is much longer than the context window of LLM (e.g., Anthropic, 2023a; Xu et al., 2023b). iii) The generalist models can answer any questions with respect to table, arithmetic calculation in zero-shot manner without dataset-specific fine-tuning, while matching the accuracies of fine-tuned models. To this end, we focus on building the state-of-the-art model with all these key capabilities that are essentially important for many real-world applications. ", "page_idx": 0}, {"type": "text", "text": "However, building such a model that can match the accuracy of the state-of-the-art proprietary models, e.g., GPT-4 (OpenAI, 2023), is still a grand challenge for the research community. In this work, we introduce ChatQA, a family of open-sourced models that can outperform GPT-4 while utilizing relatively weak open-weights foundation model. We also open-source our training data, detailing techniques for synthetic data generation, along with alternative human annotation processes aimed at eliminating reliance on OpenAI GPT models for open research purpose. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Specifically, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a two-stage instruction tuning method and design a dataset curation recipe that can largely enhance LLM\u2019s capability of integrating user provided or retrieved context for conversational QA and RAG tasks. We demonstrate that the proposed instruction tuning method significantly outperforms strong alignment baselines or RLHF-based recipes (e.g., Llama2-Chat, Llama3-Instruct) on RAG and various conversational QA tasks.   \n2. For retrieval, we show that fine-tuning the single-turn QA retriever on human-annotated data or synthetic multi-turn QA dataset works as well as utilizing the state-of-the-art LLM-based query rewriting model, i.e., GPT-3.5-Turbo (OpenAI, 2022). Our result also highlights the promising direction of utilizing synthetic data generation for training customized retriever.   \n3. We introduce CHATRAG BENCH, a comprehensive benchmark with ten conversational QA datasets, including five datasets with long documents that need retrieval and three datasets with tabular data and arithmetic calculation. We apply ChatQA training recipe on different text foundation models, and show the superb generalization capability of the proposed methods. In terms of average score on CHATRAG BENCH, our ChatQA-1.0-70B (54.14) based on Llama2 can outperform GPT-4-0613 (53.90) and GPT-4-Turbo-2024-04-09 (54.03) without utilizing any synthetic data from ChatGPT models. Notably, much smaller Llama3-ChatQA-1.5-8B can perform comparable with GPT-4 models, while Llama3-ChatQA-1.5-70B outperforms GPT-4-Turbo-2024-04-09 by a margin.   \n4. We study the \u201cunanswerable\u201d scenario, where the LLM needs to generate \u201ccannot answer\u201d to avoid hallucination. We show that incorporating a small amount of \u201cunanswerable\u201d samples significantly enhances model\u2019s capability to handle it. Our ChatQA-1.0-70B outperforms GPT-3.5-Turbo in this regard, while has a slight gap compared to GPT-4-0613 (around $3.5\\%$ ). ", "page_idx": 1}, {"type": "text", "text": "We discuss related work in $\\S~2$ . We introduce the two-stage instruction tuning method and data curation for ChatQA in $\\S\\ 3$ , and study retrieval in conversational QA in $\\S\\ 4$ . We present the experimental setup in $\\S\\ S$ , results in $\\S\\ 6$ , and conclude the paper in $\\S\\ 7$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Conversational QA and RAG ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Question answering in a conversational way naturally improves user experiences by addressing follow-up questions. The model can also raise clarification questions for users if necessary, which can reduce hallucination. Thus, it becomes the default format of deploying QA models in production (e.g. OpenAI, 2022; Google, 2023; Anthropic, 2023b). In contrast to the latest LLM-based generalist solution (e.g., OpenAI, 2022), most of the previous studies focus on fine-tuned expert models on specific domains or datasets (Feng et al., 2020; Izacard & Grave, 2021; Chen et al., 2022a; Gao et al., 2022; Nakamura et al., 2022; Adlakha et al., 2022; Wu et al., 2023). ", "page_idx": 1}, {"type": "text", "text": "In recent years, many conversational QA datasets have been introduced. The models are asked to answer questions based on provided context or documents, which involves retrieval-augmented generation (RAG) if the provided documents are longer than the context window of LLM. The provided context or documents can be: i) text-only documents from various domains (Feng et al., 2020; Anantha et al., 2021; Saeidi et al., 2018; Adlakha et al., 2022; Aliannejadi et al., 2021; Reddy et al., 2019; Qu et al., 2020; Wu et al., 2023; Deng et al., 2022; Guo et al., 2021; Choi et al., 2018; Campos et al., 2020), or ii) documents comprising plain text along with tables (Pasupat & Liang, 2015; Nakamura et al., 2022; Chen et al., 2022a). ", "page_idx": 1}, {"type": "text", "text": "2.2 Retrieval for Multi-Turn QA ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "RAG is critically important for conversational QA in open-domain setting, e.g., utilizing update-todate information from search engine, or when the proprietary documents are longer than the context window of LLM. The dense retrievers are usually trained to retrieve the top- $.k$ relevant chunks given a single question (e.g., Lin et al., 2023a; Wang et al., 2022a; Izacard et al., 2022). In conversational QA, the follow-up questions (e.g., with pronouns referring to entities mentioned in the previous conversation) may have insufficient information for retrieval, while feeding them along with all of the dialogue history can be redundant, thus leading to sub-optimal results. ", "page_idx": 1}, {"type": "image", "img_path": "bkUvKPKafQ/tmp/87a81711f933ab5b7fa153d3cef221fb5b11e3abbd011782d644f9c9568376c3.jpg", "img_caption": ["Figure 1: Two-stage instruction tuning framework for ChatQA. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Conversational Query Rewriting Most of the previous solutions are query rewriting methods. The latest turn of question is rewritten to be a standalone query without additional information from previous dialogue history (Vakulenko et al., 2021a; Ye et al., 2023; Mo et al., 2023), so it can be directly used by retrieval model to retrieve relevant context (Vakulenko et al., 2021b; Mele et al., 2021; Raposo et al., 2022; Mo et al., 2023). Many datasets have been collected to facilitate this line of research (Elgohary et al., 2019; Chu et al., 2020; Qu et al., 2020; Anantha et al., 2021; Brabant et al., 2022), alongside multiple proposed query rewriting methods (Ishii et al., 2022; Yu et al., 2020; Wu et al., 2022; Del Tredici et al., 2021; Chen et al., 2022b; Galimzhanova et al., 2023). For example, Wu et al. (2022) and Chen et al. (2022b) proposed to use reinforcement learning methods for the query rewriting. Yu et al. (2020) investigated few-shot generative models like GPT-2 for query rewriting. Galimzhanova et al. (2023) studied instruction tuned GPT-3.5-Turbo and showed that it achieved state-of-the-art results for conversational query rewriting. ", "page_idx": 2}, {"type": "text", "text": "Fine-tuning Retriever for multi-turn QA Some previous work fine-tune a single-turn query retriever on in-domain conversational query and context pairs (Feng et al., 2020; Gao et al., 2022; Adlakha et al., 2022; Wu et al., 2023), so it can directly take a concatenation of dialog history and current query as input. In this work, we focus on the zero-shot evaluation. We fine-tune a single-turn query retriever on a high-quality multi-turn dataset. Then, we evaluate zero-shot capability of the fine-tuned retriever on five benchmark datasets. Surprisingly, we find this simple approach can obtain comparable zero-shot results as the state-of-the-art query rewriting model, i.e., GPT-3.5-Turbo. ", "page_idx": 2}, {"type": "text", "text": "2.3 Instruction Tuning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The goal of instruction tuning is to equip LLMs with the capability to follow natural language instructions (Wei et al., 2022a; Sanh et al., 2022; Mishra et al., 2022; Iyer et al., 2022; Du et al., 2022; Ouyang et al., 2022; Wang et al., 2023; Zhang et al., 2023b; Gao et al., 2023; Chung et al., 2022; Muennighoff et al., 2022; Xu et al., 2023a; Wang et al., 2022c; Zhou et al., 2023; Albalak et al., 2024). There has been a surge in the development of high-quality instruction tuning datasets, including FLAN (Chung et al., 2022), Self-Instruct (Wang et al., 2022b), unnatural Instructions (Honovich et al., 2022), Dolly (Conover et al., 2023b), and OpenAssistant (K\u00f6pf et al., 2023). ", "page_idx": 2}, {"type": "text", "text": "Although numerous research on instruction tuning has been conducted, a few works focused on improving RAG or context awareness generation for QA. Lin et al. (2023b) introduced a retrievalaugmented instruction tuning method, which appends top- $.k$ retrieved chunks for LLM fine-tuning. Wang et al. (2024) applied instruction tuning after retrieval-augmented pretraining. In contrast, we propose a two-stage instruction tuning method to improve generation with retrieval or provided context. We find that appending top- $\\cdot k$ retrieved chunks for LLM fine-tuning does not help for a wide range of conversation QA tasks (see Appendix $\\S\\mathrm{A.l}$ for details). Similar to the latest work (Zhang et al., 2023a), we show that adding a small amount of \u201cunanswerable\u201d samples in instruction tuning prompts the model to respond \u201ccannot answer\u201d when needed, significantly reducing hallucination. ", "page_idx": 2}, {"type": "text", "text": "3 ChatQA ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we propose a two-stage instruction tuning method for ChatQA. See Figure 1 for an illustration. Our method starts with a pretrained LLM base model. At stage-1, we apply supervised fine-tuning (SFT) as in Ouyang et al. (2022) on a blend of instruction-following and dialog datasets. After that, our model exhibits good capability to follow instructions. However, its capability for contextualized or RAG-based QA remains limited. Hence, we introduce a subsequent stage, called context-enhanced instruction tuning, which is designed specifically for enhancing our model\u2019s capability for context-aware or retrieval-augmented generation in conversational QA. ", "page_idx": 2}, {"type": "image", "img_path": "bkUvKPKafQ/tmp/52895ef5e9c8462fab6780158e6164695beb37eda97d24105b15abba62ec11a7.jpg", "img_caption": ["Figure 2: Illustration of fine-tuning retriever for multi-turn QA. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Stage-1: Supervised Fine-tuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To construct a large and comprehensive supervised fine-tuning (SFT) dataset, we follow Xu et al. (2023b), Wang et al. (2024) and gather a combined set of 128K SFT samples from high-quality instruction tuning datasets. It consists of 1) a social dialogue dataset Soda (Kim et al., 2022), 2) a long-form QA dataset ELI5 containing elaborate answers (Fan et al., 2019), 3) FLAN and chain-of-thought datasets (Wei et al., 2022b; Chung et al., 2022; Longpre et al., 2023), 4) LLM synthetic instruction tuning datasets, including Self-Instruct (Wang et al., 2022b) and Unnatural Instructions (Honovich et al., 2022), and 5) a private crowd-sourced conversational dataset, as well as two public human-written conversation datasets: OpenAssistant (K\u00f6pf et al., 2023), and Dolly (Conover et al., 2023a). ", "page_idx": 3}, {"type": "text", "text": "We unify the structure of all the SFT data in a conversational format. We first add a \u201cSystem\u201d role at the beginning to set up a general instruction guiding LLM to provide polite and helpful answers. We also add \u201cUser\u201d and \u201cAssistant\u201d roles to incorporate instruction and response pairs from the SFT dataset. We apply fine-tuning using this unified format on an LLM foundation model. ", "page_idx": 3}, {"type": "text", "text": "3.2 Stage-2: Context-Enhanced Instruction Tuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To further enhance the model\u2019s conversational QA capability over a given context, we conduct a second stage instruction tuning, which integrates contextualized QA datasets into the instruction tuning blend. One of the key elements for stage-2 is to obtain a high-quality document-grounded conversational QA dataset. We leverage two strategies of collecting such dataset. One is to leverage GPT-3.5-Turbo to generate a synthetic dataset (referred to as SyntheticConvQA). Another is to collect human-annotated dataset (referred to as HumanAnnotatedConvQA), which enables us to avoid the reliance on OpenAI models. For both datasets, we also collect unanswerable data samples the answers cannot be found within the given context. More details can be found in the Appendix B. ", "page_idx": 3}, {"type": "text", "text": "To boost the QA capability in handling tabular documents and arithmetic calculation, we add the TATQA dataset (Zhu et al., 2021) which contains both elements. In addition, we integrate contextualized single-turn QA datasets to further strengthen the QA capability of our model. We also retain the stage-1 SFT dataset in the training blend to maintain the model\u2019s instruction-following capability. ", "page_idx": 3}, {"type": "text", "text": "Finally, the training blend for stage-2 consists of: 1) A conversational QA dataset: HumanAnnotatedConvQA or SyntheticConvQA, 2 2) single-turn QA datasets: DROP (Dua et al., 2019), NarrativeQA (Koc\u02c7isky\\` et al., 2018), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD1.1 (Rajpurkar et al., 2016), SQuAD2.0 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), TATQA (Zhu et al., 2021), and 3) all of SFT datasets from stage-1. As for the training of Llama3-ChatQA1.5, we additionally incorporate HybriDial (Nakamura et al., 2022) and our collected around 2K QA pairs within the financial domain to further improve our model\u2019s capability in tabular understanding and arithmetic calculations. We follow the similar template as in stage-1 to unify all single-turn QA and conversational QA datasets. The differences are in two parts: 1) Following the system role, we append relevant context for the single-turn question or the multi-turn conversation, and 2) Just before the single-turn question or multi-turn conversation, we integrate further instruction based on the answer types of different QA datasets (e.g., short answer, long answer, arithmetic calculation). We use the format for SFT dataset from stage-1. 3 ", "page_idx": 3}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/55a69031c7513da798068132132b5f4953536fa3ce808f1c55b0299e226731a2.jpg", "table_caption": [], "table_footnote": ["Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. \\*Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. \u2020The inputs for these two models are a concatenation of the dialogue history and the current query. \u2021The input for this model is the rewritten query. \u2662denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. \u25b3The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning). "], "page_idx": 4}, {"type": "text", "text": "4 Retrieval for Multi-Turn QA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In conversational QA tasks, when a document becomes too lengthy to feed directly into LLMs, a retriever that can handle conversational queries becomes essential. This conversational retriever encodes the concatenation of the dialogue history and the current query, and then retrieve relevant context from documents. After that, only the relevant context will be used as inputs for LLMs. The state-of-the-art retrievers, e.g., Dragon (Lin et al., 2023a), are optimized for single-turn queries, resulting in a limited generalization capability for multi-turn conversational queries. In Figure 2, we depict our retriever fine-tuning method to alleviate this issue. We propose to use conversational query and context pairs for further fine-tuning a single-turn query retriever to cope with multi-turn queries. ", "page_idx": 4}, {"type": "text", "text": "An alternative solution is conversational query rewriting method which uses a query rewriter to rewrite the current question based on the conversational history. The rewritten query is then directly used as the input to a single-turn query retriever for retrieving relevant context. In addition to the embedding and search cost, the query rewriting model introduces a large amount of extra computational expense to generate the rewritten query. ", "page_idx": 4}, {"type": "text", "text": "4.1 Fine-tuning Retriever for Multi-turn QA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To build a high-quality fine-tuning dataset, we leverage the conversational QA dataset from either HumanAnnotatedConvQA or SyntheticConvQA to construct conversational query and context pairs. For the HumanAnnotatedConvQA, we directly take the annotations of the conversational query and context pairs, and use them to further fine-tune a single-turn query retriever. For the SyntheticConvQA, we first cut each document in the conversational QA dataset into different chunks. Then, we calculate the 4-gram recall score between agent\u2019s answer and each chunk. After that, we consider the chunk that has the highest recall score as the gold chunk for the current user\u2019s question. Finally, the constructed conversational query and context pairs are used to fine-tune a single-turn query retriever. ", "page_idx": 4}, {"type": "text", "text": "4.2 Conversational Query Rewriting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To build powerful conversational query rewriting model, we take GPT-3.5-Turbo as the rewriter given that Galimzhanova et al. (2023) demonstrated the state-of-the-art query rewriting results using GPT-3.5-Turbo. Similar to Galimzhanova et al. (2023), we not only provide GPT-3.5-Turbo with the rewriting task instruction, but also give it few-shot rewriting examples to enhance the quality of rewriting results. More details can be found in Appendix E.1. ", "page_idx": 4}, {"type": "text", "text": "4.3 Comparisons ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Table 1, we compare the query rewriting and fine-tuning methods across five datasets in the zero-shot setting. More details about these datasets can be found in $\\S5.2$ . We conduct experiments on a state-of-the-art retriever, Dragon (Lin et al., 2023a), and a strong unsupervised retriever, E5- unsupervised (Wang et al., 2022a), which is not finetuned on MS MARCO (Nguyen et al., 2016). In terms of the experiments on Dragon, we find that fine-tuning performs marginally worse than query rewriting in average top-1 recall by $1.74\\%$ , while it achieves better results on average top-5 recall by $0.54\\%$ . It demonstrates the effectiveness of the fine-tuning approach for the conversational retrieval. ", "page_idx": 4}, {"type": "text", "text": "In addition, we observe that the results are comparable between using HumanAnnotatedConvQA and SyntheticConvQA for fine-tuning. This highlights that our human-annotated dataset is in high-quality, and we do not rely on ChatGPT models for building the state-of-the-art multi-turn query retriever. ", "page_idx": 5}, {"type": "text", "text": "Surprisingly, fine-tuning performs significantly better than rewriting on E5-unsupervised. We conjecture that since E5-unsupervised does not use human-annotated query and context pairs in the pre-training stage, it leads to weak generalization for the high-quality rewritten query. In contrast, using a high-quality dataset to fine-tune E5-unsupervised brings a giant boost, with more than a $15\\%$ improvement on both average top-1 and top-5 recall scores. ", "page_idx": 5}, {"type": "text", "text": "Therefore, fine-tuning a good single-turn retriever on high-quality conversational query context pairs performs on par with leveraging the state-of-the-art rewriter. However, rewriting method requires extra computational time for autoregressive generation process and probably also API cost for using powerful models like GPT-3.5-Turbo. In contrast, our proposed multi-turn fine-tuning bypasses these issues. For the QA evaluations across these five datasets, we consistently use the retrieved top-5 results from the fine-tuning approach for all the QA models. We put more results on comparisons between rewriting and fine-tuning methods in the Appendix E.2. ", "page_idx": 5}, {"type": "text", "text": "5 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Baselines ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We develop ChatQA models based on our in-house GPT-{8B, 22B} base models (pretrained with 3.5 trillion tokens), Llama2-{7B, 13B, 70B} base models (Touvron et al., 2023), and Llama3-{8B, 70B} base models (Meta, 2024). We compare ChatQA models against Llama2-Chat-{7B, 13B, 70B} and Llama3-Instruct-{8B, 70B}, which are shown to possess strong instruction following and conversational QA capabilities (Touvron et al., 2023). In addition, we compare against a powerful RAG model, Command $\\mathbf{R+}$ , which has 104 billion parameters, as well as three very strong OpenAI models: GPT-3.5-Turbo-0613, GPT-4-0613, and GPT-4-Turbo-2024-04-09. For fair comparison, when retrieval is needed, we use the same top- $.k$ retrieved chunks from our best retriever as the context for all baselines and our ChatQA models. Note that we have carefully tuned the instructions for all the baselines to ensure they achieve as good as possible results.4 ", "page_idx": 5}, {"type": "text", "text": "5.2 CHATRAG BENCH: Evaluation Benchmarks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To evaluate the model\u2019s capability on conversational QA and RAG, we construct CHATRAG BENCH, a collection of 10 datasets covering a wide range of documents and question types, which require models to generate responses from (retrieved) context, comprehend and reason over tables, conduct arithmetic calculations, and indicate when questions cannot be found within the context. ", "page_idx": 5}, {"type": "text", "text": "Long Document Datasets We collect five conversational QA datasets with long documents: Doc2Dial (D2D) (Feng et al., 2020); QuAC (Choi et al., 2018), QReCC (Anantha et al., 2021), TopiOCQA (TCQA) (Adlakha et al., 2022), and INSCIT (Wu et al., 2023). One can find details of these datasets in Appendix F. Since the documents of these datasets cannot be directly fitted into LLMs with a sequence length of 4K or 8K tokens. Hence, we run our best multi-turn query retriever to get top-k relevant chunks as the inputs. For Doc2Dial, QuAC, and QReCC, we segment documents into around 300-word chunks, and we retrieve top-5 relevant chunks as context for each user question. For TopiOCQA and INSCIT, we follow their original segmentation, resulting in smaller chunks. Hence, we retrieved top-20 chunks to obtain similar context length to the first three datasets (experiments can be found in $\\S4.3\\$ ). ", "page_idx": 5}, {"type": "text", "text": "Short Document Datasets To increase the diversity of document lengths, we collect five conversational QA datasets with short documents (less than 1.5K words): CoQA (Reddy et al., 2019), a dataset covering a wide range of domains; DoQA (Campos et al., 2020), a dataset covering cooking, travel and movie domains with unanswerable cases; ConvFinQA (Chen et al., 2022a), a dataset based on Financial domain with tabular document and requiring arithmetic calculation and complex numerical reasoning; SQA (Pasupat & Liang, 2015); and HybriDial (HDial) (Nakamura et al., 2022). Both SQA and HybriDial have tabular documents from Wikipedia and require reasoning for complex questions.5 ", "page_idx": 5}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/835a060bceca25a3638137b9e4d5815d6abdecd403906ba25e78d9609f873278.jpg", "table_caption": [], "table_footnote": ["Table 2: Zero-shot results on CHATRAG BENCH. ChatQA-1.0 are built on Llama2 base models, while Llama3-ChatQA-1.5 are built on Llama-3 base models. Note that Llama3-ChatQA-1.5 used training samples from the HDial dataset. Hence, we also compare average scores excluding HDial for the fair comparison. "], "page_idx": 6}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/94be59361c1e0cfa4cc4b8b6cfad3bfc7d63d9cb2fe5a7437b6e1420d4aa9b63.jpg", "table_caption": [], "table_footnote": ["Table 3: Ablation studies on ChatQA training. \u2662denotes that the HumanAnnotatedConvQA is replaced with the SyntheticConvQA. \u2020 denotes that the stage-1 (SFT) or stage-2 tuning is removed from the ChatQA training. \\* denotes that the single-turn QA datasets are removed from the stage-2 training blends. \u2021 denotes that the HumanAnnotatedConvQA is removed from the stage-2 training blends. In both \u201cw/o stage-1\u201d and \u201cw/o single-turn\u201d settings, the SyntheticConvQA data is used. "], "page_idx": 6}, {"type": "text", "text": "5.3 Evaluation Metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given that F1 score is the most commonly used automatic metric to assess QA models, we use it for all datasets except for ConvFinQA. In ConvFinQA, we follow Chen et al. (2022a) to use exact match metric since the answers in ConvFinQA are about extracting numbers from documents as well as arithmetic calculations. Hence, the answer only makes sense when it is exactly the same as the answer. When models generate the arithmetic formula, we will calculate its final result based on a calculator and compare it with the gold answer. In addition, we also conduct human evaluations to assess the correctness of generated answers between our model and GPT-4-0613. ", "page_idx": 6}, {"type": "text", "text": "6 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Overview In Table 2, we compare different model variants and OpenAI models across 10 conversational QA datasets. ChatQA-1.0-7B/70B greatly surpass Llama2-Chat counterparts, and Llama3- ChatQA-1.5-8B/70B greatly outperform Llama3-Instruct counterparts. For example, ChatQA-1.0- 70B outperforms Llama2-Chat-70B by an average score of 8.93, and Llama3-ChatQA-1.5-70B surpasses Llama3-Instruct-70B by an average score of 4.19 (Avg. w/o HDial). In addition, ChatQA1.0-70B slightly outperforms GPT-4-0613 and GPT-4-Turbo, and greatly surpasses Command $\\mathbf{R+}$ , a powerful RAG model that has 104 billion parameters, as well as GPT-3.5-Turbo (Avg. All). Compared to state-of-the-art OpenAI models (i.e., GPT-4-0613 and GPT-4-Turbo), Llama3-ChatQA-1.5-8B achieves comparable results, and Llama3-ChatQA-1.5-70B greatly outperforms both of them. 6 ", "page_idx": 6}, {"type": "text", "text": "Furthermore, we compare our models and OpenAI models across different dataset types (e.g., documents with table or solely text) on CHATRAG BENCH. We observe that Llama3-ChatQA-1.5- 70B surpasses GPT-4-0613 and GPT-4-Turbo in all categories. Detail results are in the Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "Importance of Stage-1 SFT and Stage-2 Tuning In Table 3, we find that removing the stage-1 SFT from the ChatQA training makes the average score drops 1.9 (from 54.08 to 52.18). Aside from the SQA, removing stage-1 makes the model consistently perform worse on other datasets. Results indicate that stage-1 still plays an important role. It enhances the instruction-following capability of ChatQA which is beneficial for stage-2 tuning. In addition, we find that removing ChatQA stage-2 tuning makes the average scores greatly drop by 10.92 (from 54.14 to 43.22). This is because the stage-2 tuning (i.e., context-enhanced instruction fine-tuning) enables the model to learn how to effectively leverage information from retrieved or relevant context. ", "page_idx": 6}, {"type": "image", "img_path": "bkUvKPKafQ/tmp/169342cdef34cac870e6d27ca891a17c38d92d3953adb624d7ada31c6be37c40.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Single-Turn Data To investigate how single-turn QA datasets affect model\u2019s multi-turn QA capability, we conduct an ablation study by removing them from the ChatQA-1.0-70B training blends in stage-2. As shown in Table 3, incorporating single-turn QA datasets in the stage-2 training blends generally make the scores increase across all benchmark datasets, leading to an average improvement of 1.83 score. Interestingly, we observe improvement in ConvFinQA, SQA, and HybriDial (table-based datasets), despite the added single-turn QA datasets not having tabular data in the documents. These results align with our intuitions. Adding single-turn QA datasets improves the model\u2019s capability to effectively leverage relevant context for the answers, resulting in better scores on CHATRAG BENCH. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Conversational QA Data We further explore the how conversational QA data affect the model\u2019s multi-turn QA capability by removing HumanAnnotatedConvQA data from the ChatQA stage-2 training blends. As illustrated in Table 3, \u201cw/o ConvQAData\u201d makes the results significantly worse than ChatQA-1.0-70B (average scores degrading from 54.08 to 48.97). We observe large degradation in datasets with text-based documents, such as QuAC, QReCC, and DoQA. However, the degradation in datasets with table-based documents (e.g., ConvFinQA, SQA) are small, thanks to having TAT-QA (Zhu et al., 2021) in the training blends. ", "page_idx": 7}, {"type": "text", "text": "Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data In Table 3, we also compare our ChatQA models using the 7k GPT-3.5-Turbo synthetic dataset (SyntheticConvQA) and our collected 7k human-annotated dataset (HumanAnnotatedConvQA). First, we find that both achieve comparable results in terms of average scores, which suggests that we do not need to rely on synthetic data from OpenAI models to build the state-of-the-art conversational QA models. Second, we find that using human-annotated data achieved significant improvements on QuAC and DoQA datasets. This can be attributed to the fact that the human-annotated data have higher quality on unanswerable cases which exists in QuAC and DoQA datasets. Eventually, it leads to the overall improvements on these two datasets. Detail results and analyses on unanswerable cases can be found in $\\S6.2$ . ", "page_idx": 7}, {"type": "text", "text": "Human Evaluation Despite F1 scores being the most commonly used metrics for evaluating the quality of QA models, there are often multiple ways to answer questions, which makes the automatic metrics less than perfect. Therefore, we use human evaluations to further compare our ChatQA-1.0-70B with GPT-4-0613. In this human evaluation, we ask annotators to verify the facts in ChatQA-1.0-70B and GPT-4\u2019s outputs and determine which model provides a more accurate response to the question. The human evaluation results averaged over the 10 datasets in CHATRAG BENCH are shown in Figure 3. We find that our ChatQA-1.0-70B and GPT-4 are tie most of the time $(69.1\\%)$ , and GPT-4 achieves slightly higher win rate $(3.3\\%)$ than ours. This further confirms our model has powerful capability to produce correct answers. 7 ", "page_idx": 7}, {"type": "text", "text": "Additional Ablation Studies $\\&$ Case Studies We conduct ablation studies on ChatQA training in terms of using top- $.k$ chunks as context for stage-2 tuning (See Appendix A.1). Furthermore, we also study how different factors affect the model\u2019s generation quality on the inference stage, in terms of: 1) the number of retrieved context/chunks; 2) context ordering; and 3) different retrievers (See Appendix A.3). In addition to quantitative results, we perform detailed qualitative case studies, and compare the output of ChatQA-1.0-70B, GPT-3.5-Turbo-0613, and GPT-4-0613 in Appendix I. ", "page_idx": 7}, {"type": "text", "text": "6.2 Evaluation of Unanswerable Case ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Evaluation Setup In this section, we study another aspect of the model\u2019s capability, which is to discern if a question can be answered within the provided context. Generating an answer in unanswerable case will lead to hallucination. To allow this evaluation, we require the model to indicate it when no answer can be found in the given context. ", "page_idx": 7}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/857a39137128906f9209cfce0a929981cd092704d682dcdb71d6bd1d9912ba23.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/b74d2638c6a0bef6b80b545c4e5ee07077ff2d8bc53417e53a8dd9fad6787f7b.jpg", "table_caption": ["Table 4: Accuracies on answerable and unanswerable samples across QuAC and DoQA datasets. Avg-Both is the averaged score between QuAC and DoQA. ChatRAG is the average score on the CHATRAG BENCH. \\* \u201cno\u201d and \u201cyes\u201d denote unanswerable and answerable samples, respectively. ", "Table 5: Zero-shot exact match scores on Natural Questions (NQ), TriviaQA, and HotpotQA, which were evaluated using the data split from the KILT Benchmark (Petroni et al., 2021). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We use QuAC and DoQA datasets which have such unanswerable cases to evaluate such capability. Specifically, for unanswerable case, we consider the model indicating that the question cannot be answered as correct8, and as for answerable cases, we consider the model not indicating the question is unanswerable as correct (i.e., the model giving an answer). Note that for answerable cases, we only select the samples where correct context is retrieved. In the end, we calculate the average accuracy score of unanswerable and answerable cases as the final metric. We consider this average accuracy as the evaluation metric since it is in the same spirit of F1 metric which measures the harmonic mean of precision and recall scores. ", "page_idx": 8}, {"type": "text", "text": "Results In Table 4, we compare our models with OpenAI models across QuAC and DoQA datasets. We observe that OpenAI models show powerful capability in this task, especially for GPT-4-0613. Compared to them, our best model (ChatQA-1.0-70B) achieved a significantly better average accuracy than GPT-3.5-Turbo, while we still has a slight gap compared to GPT-4 and GPT-4-Turbo (around $3.5\\%$ and $3.2\\%$ , respectively). Furthermore, we conduct ablation studies in terms of the number of unanswerable samples for training. We find that using a small amount of unanswerable samples (e.g., $1.5\\mathrm{k})$ is able to achieve remarkable results on the unanswerable evaluation and incorporating more unanswerable samples does not necessarily lead to higher accuracy scores in the unanswerable evaluation (Detail results are in the Appendix H.2). ", "page_idx": 8}, {"type": "text", "text": "6.3 Evaluation on Single-Turn QA and RAG Benchmark ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In addition to CHATRAG BENCH, we further evaluate Llama3-ChatQA-1.5 models on knowledgeintensive single-turn QA datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and HotpotQA (Yang et al., 2018), and compare them against frontier RAG models. We use the Dragon retriever (Lin et al., 2023a) to extract the top- $\\cdot\\mathbf{k}$ most relevant contexts, which are then used as the inputs for Llama3-Instruct and Llama3-ChatQA-1.5. We report the best results obtained from using the top-15, top-20, and top-25 contexts. In Table 5, we show that, despite its significantly smaller model size, Llama3-ChatQA-1.5-8B performs better than the state-of-the-art RA-DIT (65B) model. Llama3-ChatQA-1.5-70B remarkably outperforms existing frontier RAG models. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we build a family of models that surpass GPT-4 on conversational QA and RAG. We introduce CHATRAG BENCH, a collection of 10 conversational QA datasets covering comprehensive evaluations on RAG, table-based reasoning, arithmetic calculations, and unanswerable scenarios. Our ChatQA-1.0-70B model built on Llama2 can slightly outperform GPT-4-0613 and GPT-4- Turbo without using any synthetic data from OpenAI GPT models. Remarkably, Llama3-ChatQA1.5-70B even surpasses GPT-4-Turbo in all categories of CHATRAG BENCH. In addition, we demonstrate that fine-tuning a single-turn query retriever using our curated conversational QA data performs comparably to the state-of-the-art LLM-based query rewriting model, without incurring extra computational time and potential API costs associated with rewriting. Furthermore, we show that incorporating a small amount of \u201cunanswerable\u201d samples can significantly enhance our model\u2019s capability to handle scenarios where answers are unavailable. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Impact Statement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we discuss potential positive and negative social impacts that could arise from our ChatQA models. ", "page_idx": 9}, {"type": "text", "text": "Positive Impacts First, ChatQA enables users to interact with their data, including documents and tables, and provides accurate responses to their questions. This enhances productivity and efficient learning. Second, ChatQA can improve customer support services by quickly retrieving information about products, services, and common issues, allowing customer queries to be addressed more efficiently. Third, ChatQA can assist professionals, such as those in the medical field, by gathering relevant information and improving decision-making. ", "page_idx": 9}, {"type": "text", "text": "Potential Negative Impacts First, although our ChatQA models demonstrate impressive accuracy in generating answers, they still have the potential to provide factually inaccurate responses. Second, LLMs can inadvertently learn and perpetuate biases present in their pre-training data, which may lead to biased responses in our QA models as they are further fine-tuned based on these LLMs. Third, there is a risk of malicious use of our ChatQA models, such as gathering personal information by inputting documents containing relevant personal content. ", "page_idx": 9}, {"type": "text", "text": "Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "ChatQA models are optimized for RAG and conversational QA across various types of questions and documents, making them very useful in many real-world applications. However, they may not perform well on code-related tasks or math reasoning tasks compared to GPT-4 and other versatile frontier models, because ChatQA\u2019s instruction-tuning blend does not include any code generation data. In the future, we plan to include more code-related and math reasoning data to support potential use cases. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Adlakha, V., Dhuliawala, S., Suleman, K., de Vries, H., and Reddy, S. Topiocqa: Open-domain conversational question answering with topic switching. TACL, 2022. ", "page_idx": 9}, {"type": "text", "text": "Albalak, A., Elazar, Y., Xie, S. M., Longpre, S., Lambert, N., Wang, X., Muennighoff, N., Hou, B., Pan, L., Jeong, H., et al. A survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024.   \nAliannejadi, M., Kiseleva, J., Chuklin, A., Dalton, J., and Burtsev, M. Building and evaluating open-domain dialogue corpora with clarifying questions. In EMNLP, 2021.   \nAnantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman, S., and Chappidi, S. Open-domain question answering goes conversational via question rewriting. In NAACL, 2021.   \nAnthropic. Introducing 100k context windows, 2023a.   \nAnthropic. Introducing Claude, 2023b. ", "page_idx": 9}, {"type": "text", "text": "Brabant, Q., Lecorv\u00e9, G., and Barahona, L. M. R. Coqar: Question rewriting on coqa. In LREC, 2022. ", "page_idx": 9}, {"type": "text", "text": "Campos, J. A., Otegi, A., Soroa, A., Deriu, J. M., Cieliebak, M., and Agirre, E. Doqa-accessing domain-specific faqs via conversational qa. In ACL, 2020.   \nChen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., and Wang, W. Y. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. In EMNLP, 2022a.   \nChen, Z., Zhao, J., Fang, A., Fetahu, B., Rokhlenko, O., and Malmasi, S. Reinforced question rewriting for conversational question answering. In EMNLP, 2022b.   \nChoi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi, Y., Liang, P., and Zettlemoyer, L. Quac: Question answering in context. In EMNLP, 2018.   \nChu, Z., Chen, M., Chen, J., Wang, M., Gimpel, K., Faruqui, M., and Si, X. How to ask better questions? a large-scale multi-domain dataset for rewriting ill-formed questions. In AAAI, 2020.   \nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instruction-finetuned language models. arXiv preprint arXiv: 2210.11416, 2022.   \nConover, M., Hayes, M., Mathur, A., Meng, X., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., et al. Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm, 2023a.   \nConover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R. Free Dolly: Introducing the world\u2019s first truly open instruction-tuned llm, 2023b.   \nDai, Z., Chaganty, A. T., Zhao, V., Amini, A., Green, M., Rashid, Q., and Guu, K. Dialog inpainting: Turning documents to dialogs. In ICML, 2022.   \nDasigi, P., Liu, N. F., Marasovic\u00b4, A., Smith, N. A., and Gardner, M. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In EMNLP, 2019.   \nDel Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and de Gispert, A. Question rewriting for open-domain conversational qa: Best practices and limitations. In CIKM, 2021.   \nDeng, Y., Lei, W., Zhang, W., Lam, W., and Chua, T.-S. Pacific: Towards proactive conversational question answering over tabular and textual data in finance. In EMNLP, 2022.   \nDu, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. Glm: General language model pretraining with autoregressive blank infilling. In ACL, 2022.   \nDua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL, 2019.   \nElgohary, A., Peskov, D., and Boyd-Graber, J. Can you unpack that? learning to rewrite questions-incontext. In EMNLP, 2019.   \nFan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and Auli, M. Eli5: Long form question answering. In ACL, 2019.   \nFeng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and Lastras, L. doc2dial: A goal-oriented document-grounded dialogue dataset. In EMNLP, 2020.   \nGalimzhanova, E., Muntean, C. I., Nardini, F. M., Perego, R., and Rocchietti, G. Rewriting conversational utterances with instructed large language models. In IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology, 2023.   \nGao, C., Zhang, W., and Lam, W. Unigdd: A unified generative framework for goal-oriented document-grounded dialogue. In ACL, 2022.   \nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.   \nGoogle. Introducing bard, 2023.   \nGuo, M., Zhang, M., Reddy, S., and Alikhani, M. Abg-coqa: Clarifying ambiguity in conversational question answering. In AKBC, 2021.   \nHonovich, O., Scialom, T., Levy, O., and Schick, T. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.   \nHuang, J., Ping, W., Xu, P., Shoeybi, M., Chang, K. C.-C., and Catanzaro, B. Raven: In-context learning with retrieval augmented encoder-decoder language models. arXiv preprint arXiv:2308.07922, 2023.   \nIshii, E., Xu, Y., Cahyawijaya, S., and Wilie, B. Can question rewriting help conversational question answering? In Proceedings of the Third Workshop on Insights from Negative Results in NLP, 2022.   \nIyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S., et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.   \nIzacard, G. and Grave, \u00c9. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, 2021.   \nIzacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022.   \nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 2023.   \nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, 2017.   \nKim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y., Zhou, P., Bras, R. L., Alikhani, M., Kim, G., Sap, M., et al. Soda: Million-scale dialogue distillation with social commonsense contextualization. arXiv preprint arXiv:2212.10465, 2022.   \nKoc\u02c7isky\\`, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The narrativeqa reading comprehension challenge. TACL, 2018.   \nK\u00f6pf, A., Kilcher, Y., von R\u00fctte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyf,i R., et al. Openassistant conversations\u2013democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.   \nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: A benchmark for question answering research. TACL, 2019.   \nK\u00f6pf, A., Kilcher, Y., von R\u00fctte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyf,i R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A. Openassistant conversations - democratizing large language model alignment. arXiv preprint arXiv: 2304.07327, 2023.   \nLin, K., Tafjord, O., Clark, P., and Gardner, M. Reasoning over paragraph effects in situations. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, 2019.   \nLin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y., Yih, W.-t., and Chen, X. How to train your dragon: Diverse augmentation towards generalizable dense retrieval. arXiv preprint arXiv:2302.07452, 2023a.   \nLin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et al. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint arXiv:2310.01352, 2023b. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Lin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., Zettlemoyer, L., and tau Yih, W. RA-DIT: Retrieval-augmented dual instruction tuning. In ICLR, 2024. URL https://openreview.net/forum?id=22OTbutug9. ", "page_idx": 12}, {"type": "text", "text": "Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. ", "page_idx": 12}, {"type": "text", "text": "Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. ", "page_idx": 12}, {"type": "text", "text": "Mele, I., Muntean, C. I., Nardini, F. M., Perego, R., Tonellotto, N., and Frieder, O. Adaptive utterance rewriting for conversational search. Information Processing & Management, 2021. ", "page_idx": 12}, {"type": "text", "text": "Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. ", "page_idx": 12}, {"type": "text", "text": "Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-task generalization via natural language crowdsourcing instructions. In ACL, 2022. ", "page_idx": 12}, {"type": "text", "text": "Mo, F., Mao, K., Zhu, Y., Wu, Y., Huang, K., and Nie, J.-Y. Convgqr: Generative query reformulation for conversational search. arXiv preprint arXiv:2305.15645, 2023. ", "page_idx": 12}, {"type": "text", "text": "Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Scao, T. L., Bari, M. S., Shen, S., Yong, Z.-X., Schoelkopf, H., et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022. ", "page_idx": 12}, {"type": "text", "text": "Nakamura, K., Levy, S., Tuan, Y.-L., Chen, W., and Wang, W. Y. Hybridialogue: An informationseeking dialogue dataset grounded on tabular and textual data. In Findings of ACL, 2022. ", "page_idx": 12}, {"type": "text", "text": "Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. Ms marco: A human generated machine reading comprehension dataset. choice, 2016. ", "page_idx": 12}, {"type": "text", "text": "OpenAI. Introducing ChatGPT, 2022. ", "page_idx": 12}, {"type": "text", "text": "OpenAI. GPT-4, 2023. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. NeurIPS, 2022. ", "page_idx": 12}, {"type": "text", "text": "Pasupat, P. and Liang, P. Compositional semantic parsing on semi-structured tables. In ACL, 2015. ", "page_idx": 12}, {"type": "text", "text": "Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De Cao, N., Thorne, J., Jernite, Y., Karpukhin, V., Maillard, J., et al. Kilt: a benchmark for knowledge intensive language tasks. In NAACL, 2021. ", "page_idx": 12}, {"type": "text", "text": "Qu, C., Yang, L., Chen, C., Qiu, M., Croft, W. B., and Iyyer, M. Open-retrieval conversational question answering. In SIGIR, 2020. ", "page_idx": 12}, {"type": "text", "text": "Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: $100{,}000{+}$ questions for machine comprehension of text. In EMNLP, 2016. ", "page_idx": 12}, {"type": "text", "text": "Rajpurkar, P., Jia, R., and Liang, P. Know what you don\u2019t know: Unanswerable questions for squad. In ACL, 2018. ", "page_idx": 12}, {"type": "text", "text": "Raposo, G., Ribeiro, R., Martins, B., and Coheur, L. Question rewriting? assessing its importance for conversational question answering. In ECIR, 2022. ", "page_idx": 12}, {"type": "text", "text": "Reddy, S., Chen, D., and Manning, C. D. Coqa: A conversational question answering challenge. TACL, 2019. ", "page_idx": 12}, {"type": "text", "text": "Saeidi, M., Bartolo, M., Lewis, P., Singh, S., Rockt\u00e4schel, T., Sheldon, M., Bouchard, G., and Riedel, S. Interpretation of natural language rules in conversational machine reading. In EMNLP, 2018. ", "page_idx": 12}, {"type": "text", "text": "Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. In ICLR, 2022. ", "page_idx": 12}, {"type": "text", "text": "Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.   \nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nTrischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K. Newsqa: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, 2017.   \nVakulenko, S., Longpre, S., Tu, Z., and Anantha, R. Question rewriting for conversational question answering. In WSDM, 2021a.   \nVakulenko, S., Voskarides, N., Tu, Z., and Longpre, S. A comparison of question rewriting methods for conversational passage retrieval. In ECIR, 2021b.   \nWang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro: Instruction tuning post retrieval-augmented pretraining. In ICML, 2024.   \nWang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022a.   \nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022b.   \nWang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S., Arunkumar, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on $1600+$ nlp tasks. In EMNLP, 2022c.   \nWang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K. R., Wadden, D., MacMillan, K., Smith, N. A., Beltagy, I., et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.   \nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In ICLR, 2022a.   \nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-ofthought prompting elicits reasoning in large language models. NeurIPS, 2022b.   \nWu, Z., Luan, Y., Rashkin, H., Reitter, D., Hajishirzi, H., Ostendorf, M., and Tomar, G. S. Conqrr: Conversational query rewriting for retrieval with reinforcement learning. In EMNLP, 2022.   \nWu, Z., Parish, R., Cheng, H., Min, S., Ammanabrolu, P., Ostendorf, M., and Hajishirzi, H. Inscit: Information-seeking conversations with mixed-initiative interactions. TACL, 2023.   \nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023a.   \nXu, F., Shi, W., and Choi, E. RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation. In ICLR, 2024. URL https://openreview.net/forum?id= mlJLVigNHp.   \nXu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025, 2023b.   \nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In EMNLP, 2018.   \nYe, F., Fang, M., Li, S., and Yilmaz, E. Enhancing conversational search: Large language model-aided informative query rewriting. In EMNLP, 2023.   \nYu, S., Liu, J., Yang, J., Xiong, C., Bennett, P., Gao, J., and Liu, Z. Few-shot generative conversational query rewriting. In SIGIR, 2020.   \nZhang, H., Diao, S., Lin, Y., Fung, Y. R., Lian, Q., Wang, X., Chen, Y., Ji, H., and Zhang, T. R-tuning: Teaching large language models to refuse unknown questions. arXiv preprint arXiv:2311.09677, 2023a.   \nZhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et al. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792, 2023b.   \nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.   \nZhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., and Chua, T.-S. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. In ACL, 2021. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Additional Results & Analyses ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/585bc7851b0f82bc4478bc90122266767da00bfd8b1080486a637bfa44948c3c.jpg", "table_caption": ["A.1 Using Top- $\\pmb{k}$ Chunks as Context for Stage-2 Instruction Tuning "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 6: Ablation study on using \u201ctop-5\u201d retrieved chunk as the context for the stage-2 instruction tuning. We report average scores on CHATRAG BENCH, five retrieval datasets (Avg-ret) and five non-retrieval datasets (Avg-nonret). ", "page_idx": 15}, {"type": "text", "text": "For all the datasets used in stage-2 tuning, the context is provided as continuous paragraphs or documents containing the answer. In contrast, the model needs to handle the top- $.k$ retrieved chunks during inference for long documents. To address this train/test mismatch, we investigate whether replacing some continuous paragraphs with retrieved top- $k$ chunks will enhance the model\u2019s robustness. ", "page_idx": 15}, {"type": "text", "text": "We use NarrativeQA for this study, since each question has a corresponding long document. Initially, we use a summary of the long document as the context, which contains the answer. To incorporate discontinuous contexts, we first cut the long document into 300-word chunks. Then, we use the Dragon retriever to retrieve the top-4 chunks related to the question as additional context. Finally, we consider the retrieved four chunks along with the summary of the long document as the \u201ctop- $.5^{\\circ}$ chunks. 9 We use this reconstructed NarrativeQA to replace the original one for the stage-2 instruction tuning. ", "page_idx": 15}, {"type": "text", "text": "In Table 6, we observe that using \u201ctop-5\u201d chunks as the context for training leads to improvements on the datasets that require retrieval. But it degrades the performance on non-retrieval datasets. Overall, these two models perform comparable. It is because incorporating \u201ctop- $.5^{\\circ}$ retrieved chunks in stage-2 tuning aligns with the inference stage where retrieval is needed, which improves the Avg-ret score. However, mixing continuous and discontinuous documents could make the stage-2 tuning less stable, leading to sub-optimal results on non-retrieval datasets. We believe more future work can be conducted regarding the balance of incorporating continuous context and top- $.k$ retrieved chunks in stage-2 tuning. ", "page_idx": 15}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/8b97644822f5314de9f16c44c042b8225c713e861401502c8b97551f68fe7e56.jpg", "table_caption": ["A.2 Fine-grained Analyses "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 7: Fine-grained studies on average scores of different dataset types. Avg-text covers datasets where the documents only have text, including Doc2Dial, QuAC, QReCC, CoQA, DoQA, TopiOCQA, and INSCIT. Avg-table covers datasets with table in the documents, including ConvFinQA, SQA, and HybriDial. Avg-ret covers datasets with long documents requiring retrieval, including Doc2Dial, QuAC, QReCC, TopiOCQA, and INSCIT. Avg-nonret covers datasets with short documents which do not require retrieval, including CoQA, DoQA, ConvFinQA, SQA, and HybriDial. ", "page_idx": 15}, {"type": "text", "text": "In Table 7, we further compared our models and OpenAI models across different dataset types in our conversational QA benchmarks. ChatQA-1.0-70B achieves better results in text-only documents (Avgtext), while GPT-4-0613 shows better QA capability in tabular data given (Avg-table). Regarding the datasets that require or do not require retrieval (Avg-ret and Avg-nonret), ChatQA-1.0-70B and GPT-4-0613 are comparable. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Furthermore, Llama3-ChatQA-1.5-70B surpasses GPT-4-0613 and GPT-4-Turbo in all categories. ", "page_idx": 16}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/80c3e8e1a91ad3590f99f6ba5143415f4911b6ba2871003196b9d2c44d95e09e.jpg", "table_caption": ["A.3 Ablation Studies on Inference Stage "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 8: Ablation studies on input context across datasets that require retrieval. All models use SyntheticConvQA. We study the number of contexts used in inputs (# of ctx), context ordering (reverse, swing, random), and the use of retrieved context from the original Dragon. In comparison, ChatQA-1.0-70B (default setting) uses \u201cDragon $^+$ Fine-tune\u201d to retrieve the top-5 contexts, and arranges them sequentially from the first to the fifth context in top-5. ", "page_idx": 16}, {"type": "text", "text": "In Table 8, we show ablation studies on how the number of retrieved context/chunks, context ordering, and different retrievers affect the conversational QA and RAG results. ", "page_idx": 16}, {"type": "text", "text": "First, we find that using more contexts as inputs do not always improve the results. Utilizing top-5 contexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively, more contexts have higher probability to contain correct answers (better recall scores). As a result, using top-5 contexts achieves better results than using top-3. However, as the number of contexts further increases, the model may suffer from \u201clost in the middle\u201d phenomenon (Liu et al., 2023) and the difficulty of extracting answers from the provided context could also increase, which leads to inferior results by using top-10 contexts. ", "page_idx": 16}, {"type": "text", "text": "Second, we study how using different orderings of top-5 contexts affects the results. We compare sequential ordering (from 1st context to 5th context) to reverse ordering (from 5th to 1st context), swing ordering (given the \u201clost in the middle\u201d phenomenon, we arrange the most relevant context to appear at the beginning and the end of the input context. Hence, the ordering becomes {1st, 3rd, 5th, 4th, 2nd}), and random ordering (random shuffle the top-5 contexts). We find using sequential ordering is comparable to using reverse and swing orderings, and random shuffling is slightly worse. Results indicate that our model excels in extracting the correct answer from lengthy contexts, regardless of the answer\u2019s location. This is because, during the ChatQA fine-tuning, the answer\u2019s location is randomly distributed within the context. ", "page_idx": 16}, {"type": "text", "text": "Third, we observe that when we replace \u201cDragon $^+$ Fine-tune\u201d with the original non-finetuned Dragon retriever, the average score drops by 1.81 (from 42.31 to 40.50). In addition, the score drops significantly in INSCIT dataset (from 33.98 to 27.87) due to the large performance gap between the two retrievers (as shown in Table 1). Basically, when the quality of retrieval improves, it directly enhances the performance of question answering. ", "page_idx": 16}, {"type": "text", "text": "B Conversational QA Data Collection for Stage-2 Instruction Tuning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Human Annotated Data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In addition to the public available datasets, one of the key elements for stage-2 is to obtain a highquality document-grounded conversational QA dataset. We create a human-annotated conversational QA (called HumanAnnotatedConvQA) dataset only consisting of 7k dialogues. To build this dataset, we first collected 7k documents covering diverse topics from the Internet. Then, we instruct the annotator to act both as a curious user asking questions (and follow-up questions) about the document and as an agent to give the answers. We create a multi-turn conversation for each document, resulting in a total of $7\\mathbf{k}$ conversational QA dialogues with an average of 5 user-agent turns per dialogue. Details of data collection guideline can be found in the Appendix J.2. ", "page_idx": 17}, {"type": "text", "text": "To reduce hallucinated answers in unanswerable cases, we aim to empower our model to explicitly indicate it when the answer cannot be found within the given context. To obtain these unanswerable data samples, we requested annotators to identify all parts of the context locations that are relevant to the user\u2019s question. Hence, it enabled us to construct unanswerable scenarios by deleting the text from the corresponding locations in the context. After deleting the relevant text to the question, we use a sentence, \u201cSorry. I cannot find the answer based on the context\u201d, as the response for the unanswerable question. Finally, we construct another $1.5\\mathrm{k}$ user-agent turns with unanswerable annotations, which provides a good trade-off for answerable and unanswerable cases. Detailed results can be found in the Appendix H.2. ", "page_idx": 17}, {"type": "text", "text": "B.2 Synthetic Data Generation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To validate the quality of the HumanAnnotatedConvQA, we leverage GPT-3.5-Turbo to generate a synthetic conversational QA dataset given its powerful instruction-following and text generation capability. Note that large-scale synthetic data for conversational QA has also been explored in Dai et al. (2022). In this work, we focus on midsize high-quality synthetic data for LLM fine-tuning. One can find the details of synthetic data generation at Appedix D. ", "page_idx": 17}, {"type": "text", "text": "Similar to the HumanAnnotatedConvQA, we construct another $1.5\\mathrm{k}$ user-agent turns with unanswerable annotations in this synthetic dataset. Since there are no annotations of the context location for the agent\u2019s answer, we construct synthetic unanswerable samples from SyntheticConvQA. Specifically, we first cut the document (for each dialogue) into different chunks. Then, we consider it as a valid unanswerable sample only when there are chunks that have \u201chigh overlaps\u201d with the agent\u2019s answer to be removed and the rest of the chunks show \u201clow overlaps\u201d with the agent\u2019s answer. We use the 4-gram recall score between each chunk and the agent\u2019s answer (which measures the ratios of the answer\u2019s 4-gram phrases are within each chunk) as the metric to measure their overlaps, and consider it higher than 0.5 as \u201chigh overlaps\u201d and lower than 0.1 as \u201clow overlaps\u201d. ", "page_idx": 17}, {"type": "text", "text": "C ChatQA Instruction Tuning Templates ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Stage-1: Supervised Fine-tuning ", "page_idx": 18}, {"type": "text", "text": "The format template of LLM inputs in stage-1 is as follows: ", "page_idx": 18}, {"type": "text", "text": "System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\u2019s questions based on the context. The assistant should also indicate when the answer cannot be found in the context. ", "page_idx": 18}, {"type": "text", "text": "User: {Question 1}   \nAssistant: {Answer 1}   \nUser: {Latest Question}   \nAssistant:   \nWe use the {Latest Answer} from Assistant as the supervision for model outputs. ", "page_idx": 18}, {"type": "text", "text": "C.2 Stage-2: Context-Enhanced Instruction Tuning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Based on the stage-1 format template, the LLM inputs in stage-2 adds {Context for Latest Question} and {Instruction} from User, as depicted below: ", "page_idx": 18}, {"type": "text", "text": "System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\u2019s questions based on the context. The assistant should also indicate when the answer cannot be found in the context. ", "page_idx": 18}, {"type": "text", "text": "{Context for Latest Question} User: {Instruction} $^+$ {Question 1} Assistant: {Answer 1} User: {Latest Question} ", "page_idx": 18}, {"type": "text", "text": "Assistant: ", "page_idx": 18}, {"type": "text", "text": "Same as Stage-1, we use the $\\{{\\mathrm{Latest~~Answer}}\\}$ from Assistant as the supervision for model outputs. ", "page_idx": 18}, {"type": "text", "text": "As for the $\\{{\\tt I n s t r u c t i o n}\\}$ , we use different instructions for different datasets based on the answer types. Details can be found below: ", "page_idx": 18}, {"type": "text", "text": "\u201cPlease give a full and complete answer for the question.\u201d This is for datasets with long answers. We use it for the HumanAnnotatedConvQA or SyntheticConvQA. ", "page_idx": 18}, {"type": "text", "text": "\u201cAnswer the following question with a short span. The answer needs to be just in a few words.\u201d This is for datasets with short answers. We use it for SQuAD1.1, SQuAD2.0, NarrativeQA, DROP, ROPES, NewsQA, and Quoref. ", "page_idx": 18}, {"type": "text", "text": "\u201cAnswer the following question with a number from context or the math arithmetic using +, -, \\*, or /.\u201d This is for datasets using arithmetic calculation or extracting numbers from the context. We use it for the TAT-QA dataset where questions need the arithmetic calculation or extracting numbers from the context. ", "page_idx": 18}, {"type": "text", "text": "\u201cAnswer the following question with a short span, or a full and complete answer.\u201d This is for datasets with both short and long answers. The model needs to tell whether to generate short or long answers based on the questions. We use it for the TAT-QA dataset where questions do not need the arithmetic calculation, since we observe the lengths of the corresponding answers can be both short and long. ", "page_idx": 19}, {"type": "text", "text": "For the stage-1 SFT data used in stage-2 instruction tuning, we keep the same format template as in stage-1. In other words, the {Context for Latest Question} and {Instruction} are empty. ", "page_idx": 19}, {"type": "text", "text": "D Synthetic Data Generation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The instruction for GPT-3.5-Turbo comprises three parts: 1) system role guiding the model to provide helpful answers, 2) examples of conversational QA indicating the required data types, and 3) a document that directs the model to generate conversational QA based on its content. We collect $7\\mathbf{k}$ documents (average ${\\sim}1\\mathrm{k}$ words per document) from common crawl, which cover a wide range of domains. Each document is used for generation of a single conversational QA sample, which leads to a total of $7\\mathbf{k}$ multi-turn QA dialogues with an average of 4.4 user-agent turns per dialogue (called SyntheticConvQA). ", "page_idx": 19}, {"type": "text", "text": "We use a two-step prompting on GPT-3.5-Turbo for obtaining a high-quality multi-turn QA conversation. In step-1, we prompt GPT-3.5-Turbo to generate a multi-turn QA conversation based on a given document. However, we find that the generated conversation lacks pronouns or common nouns to refer back to words or phrases in dialogue history. Hence, in step-2, we prompt GPT-3.5- Turbo to revise the generated conversation by adding pronouns or common nouns. For both stages, we give GPT-3.5-Turbo few-shot examples to ensure a high-quality generation. ", "page_idx": 19}, {"type": "text", "text": "The in-context few-shot prompt in step-1 is as follows (aims to generate multi-turn QA conversation): ", "page_idx": 19}, {"type": "text", "text": "You are a helpful, pattern-following assistant. ", "page_idx": 19}, {"type": "text", "text": "role-user:   \nPersonA has access to information in the following passage:   \n###There are certain important aspects of college preparation. The student preparing for college should define their goals and interests. The student should also plan financially and academically. We have a checklist that helps students get ready financially and academically.###   \nGenerate a conversation of around four turns between PersonA and PersonU where PersonU needs information from PersonA that is in the passage. Use only information found in the passage.   \nPersonU starts the conversation and only asks questions. Most of PersonU\u2019s questions are follow-up or continuations of previous questions.   \nConversation: role-assistant:   \nPersonU: I need to know how to prepare for college.   \nPersonA: You should first define your goals and interest and be sure to know the costs and how to plan financially and academically for college.   \nPersonU: Is there something I can use as a guide to get ready for it?   \nPersonA: you can use our checklist to get ready financially and academically. PersonU: Any other tip?   \nPersonA: Be sure to check out the reasons to attend a College or Career schools. PersonU: Ok, that sounds good. Can you give me more tips?   \nPersonA: Another thing to take into consideration is to explore your career options.   \nrole-user:   \nPersonA has access to information in the following passage:   \n###There are several disability compensation benefits. One of the benefits is the   \nannual clothing allowance. This allowance covers the cost of purchasing new clothes   \nup to a limit of 200 dollars annually.###   \nGenerate a conversation of around two turns between PersonA and PersonU where PersonU needs information from PersonA that is in the passage. Use only information found in the passage.   \nPersonU starts the conversation and only asks questions. Most of PersonU\u2019s questions are follow-up or continuations of previous questions.   \nConversation: ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "role-assistant: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "PersonU: I heard about a disability benefit for clothing. What is it about? PersonA: This is a disability compensation benefit known as an annual clothing allowance.   \nPersonU: I am quite interested about such benefits.   \nPersonA: Would you like to find out if you are eligible? ", "page_idx": 20}, {"type": "text", "text": "role-user: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "PersonA has access to information in the following passage: ###Vascepa (icosapent ethyl) is a prescription fish oil medication approved for treating adults with extremely high triglycerides, defined as a triglyceride level of 500 mg/dL or higher. While using this medicine, you may need frequent blood tests at your doctor\u2019s office. Vascepa is only part of a treatment program that may also include diet, exercise, and weight control. Follow your doctor\u2019s instructions very closely. You should not use Vascepa if you are allergic to icosapent. To make sure Vascepa is safe for you, tell your doctor if you have: 1 liver disease; 2 diabetes; 3 a thyroid disorder; 4 problems with your pancreas; 5 a bleeding or blood-clotting disorder; 6 if you are allergic to fish or shellfish; or. 7 if you drink large amounts of alcohol, Vascepa (icosapent) works in the liver and bloodstream to reduce very low-density triglycerides.### ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Generate a conversation of around three turns between PersonA and PersonU where PersonU needs information from PersonA that is in the passage. Use only information found in the passage. ", "page_idx": 20}, {"type": "text", "text": "PersonU starts the conversation and only asks questions. Most of PersonU\u2019s questions are follow-up or continuations of previous questions.   \nConversation: ", "page_idx": 20}, {"type": "text", "text": "role-assistant: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "PersonU: I heard about a medication called Vascepa. What is it used for? ", "page_idx": 20}, {"type": "text", "text": "PersonA: Vascepa is a prescription fish oil medication used for treating adults with extremely high triglyceride levels.   \nPersonU: Are there any side effects or precautions I should be aware of?   \nPersonA: While using Vascepa, you may need frequent blood tests, and it\u2019s important to follow your doctor\u2019s instructions closely. It\u2019s important to talk to your doctor about any allergies or existing conditions you have.   \nPersonU: Is it effective in lowering cholesterol levels?   \nPersonA: Vascepa is used together with a low-fat diet and other treatments to lower total cholesterol in people with severe high cholesterol. ", "page_idx": 20}, {"type": "text", "text": "role-user: PersonA has access to information in the following passage: ", "page_idx": 20}, {"type": "text", "text": "{Document} ", "page_idx": 20}, {"type": "text", "text": "Generate a conversation of around three turns between PersonA and PersonU where PersonU needs information from PersonA that is in the passage. Use only information found in the passage.   \nPersonU starts the conversation and only asks questions. Most of PersonU\u2019s questions are follow-up or continuations of previous questions.   \nConversation: ", "page_idx": 20}, {"type": "text", "text": "role-assistant: ", "page_idx": 20}, {"type": "text", "text": "The in-context few-shot prompt in step-2 is as follows (aims to revise the multi-turn QA conversation by adding pronouns and common nouns): ", "page_idx": 21}, {"type": "text", "text": "You are a helpful, pattern-following assistant. ", "page_idx": 21}, {"type": "text", "text": "role-user:   \nGiven the following conversation between PersonU and PersonA:   \n###PersonU: I\u2019ve heard about adobo sauce in Mexican cuisine. Can you tell me more about it?   \nPersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and vinegar. It\u2019s used as a marinade and cooking sauce in Mexican and Southwestern cuisines.   \nPersonU: What are the main ingredients in adobo sauce?   \nPersonA: The main ingredients are typically chile peppers, garlic, and vinegar. However, there can be variations in the recipe.   \nPersonU: How can I make adobo sauce at home?   \nPersonA: One way to make adobo sauce at home is by using smoked jalapeno peppers called chipotles. You can stew the chipotles in a sauce with tomatoes, garlic, vinegar, salt, and spices. It\u2019s also possible to find canned adobo sauce in the Hispanic foods section of many grocery stores.   \nPersonU: Are there any other uses for adobo sauce?   \nPersonA: Adobo sauce can be used as a condiment or as a marinade for various dishes. It enhances the flavor of meats and can be used as a cooking and serving sauce. Additionally, adobo is also used in a stew-like presentation of meat prepared in this sauce.   \nPersonU: Is adobo sauce easy to preserve?   \nPersonA: Yes, you can preserve adobo sauce by freezing or canning it when you\u2019re done making it. This allows you to store it for up to a year and enjoy its flavors whenever you want.###   \nModify PersonU\u2019s questions to use pronouns or common nouns to refer back to words or phrases in PersonU\u2019s previous questions or PersonA\u2019s previous responses. role-assistant:   \nPersonU: I\u2019ve heard about adobo sauce in Mexican cuisine. Can you provide more information about it?   \nPersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and vinegar. It\u2019s used as a marinade and cooking sauce in Mexican and Southwestern cuisines. PersonU: What are the main ingredients in the sauce?   \nPersonA: The main ingredients are typically chile peppers, garlic, and vinegar. However, there can be variations in the recipe.   \nPersonU: How can I make the sauce at home?   \nPersonA: One way to make it at home is by using smoked jalapeno peppers called chipotles. You can stew them in a sauce with tomatoes, garlic, vinegar, salt, and spices. It\u2019s also possible to find canned adobo sauce in the Hispanic foods section of many grocery stores.   \nPersonU: Are there any other uses for it?   \nPersonA: It can be used as a condiment or as a marinade for various dishes. It enhances the flavor of meats and can be used as a cooking and serving sauce.   \nAdditionally, it is also used in a stew-like presentation of meat prepared in this sauce.   \nPersonU: Is it easy to preserve?   \nPersonA: Yes, you can preserve it by freezing or canning it when you\u2019re done making it. This allows you to store it for up to a year and enjoy its flavors whenever you want. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "role-user: Given the following conversation between PersonU and PersonA: ###PersonU: I\u2019ve heard about a test called urinalysis. What is it used for? PersonA: Urinalysis is a test that evaluates a sample of your urine. It is used to detect and assess a wide range of disorders, such as urinary tract infection, kidney disease, and diabetes. It examines the appearance, concentration, and content of urine. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "PersonU: Can it help diagnose bladder cancer? ", "page_idx": 22}, {"type": "text", "text": "PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for bladder cancer. One way to test for bladder cancer is to check for blood in the urine, which is called hematuria. This can be done during a urinalysis by examining the urine cells under a special microscope. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "PersonU: What other conditions or diseases can be detected through urinalysis? PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections, evaluate causes of kidney failure, screen for the progression of chronic conditions like diabetes mellitus and high blood pressure, and it can be used in combination with other tests to diagnose certain diseases. It is a versatile test that helps detect abnormalities in the urinary tract. ", "page_idx": 22}, {"type": "text", "text": "PersonU: Where can I get a urinalysis done? ", "page_idx": 22}, {"type": "text", "text": "PersonA: Urinalysis can be performed in various healthcare settings, including doctors\u2019 offices, urgent care facilities, laboratories, and hospitals. You can provide a urine sample in a specimen cup, and the sample can be analyzed on-site or sent to a laboratory for testing. ", "page_idx": 22}, {"type": "text", "text": "PersonU: How much urine is typically needed for a urinalysis? ", "page_idx": 22}, {"type": "text", "text": "PersonA: Usually, only small amounts of urine, ranging from 30 to $60\\,\\mathrm{~m1}$ , are required for urinalysis testing. It\u2019s a non-invasive and relatively simple test that provides valuable information about your health.### ", "page_idx": 22}, {"type": "text", "text": "Modify PersonU\u2019s questions to use pronouns or common nouns to refer back to words or phrases in PersonU\u2019s previous questions or PersonA\u2019s previous responses. ", "page_idx": 22}, {"type": "text", "text": "role-assistant: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "PersonU: I\u2019ve heard about a test called urinalysis. What is it used for? ", "page_idx": 22}, {"type": "text", "text": "PersonA: Urinalysis is a test that evaluates a sample of your urine. It\u2019s used t detect and assess a wide range of disorders, such as urinary tract infection, kidney disease, and diabetes. It examines the appearance, concentration, and content of urine. ", "page_idx": 22}, {"type": "text", "text": "PersonU: Can it help diagnose bladder cancer? ", "page_idx": 22}, {"type": "text", "text": "PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for bladder cancer. One way to test for bladder cancer is to check for blood in the urine, which is called hematuria. This can be done during a urinalysis by examining the urine cells under a special microscope. ", "page_idx": 22}, {"type": "text", "text": "PersonU: What other conditions or diseases can be detected through this test? ", "page_idx": 22}, {"type": "text", "text": "PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections, evaluate causes of kidney failure, screen for the progression of chronic conditions like diabetes mellitus and high blood pressure, and it can be used in combination with other tests to diagnose certain diseases. It is a versatile test that helps detect abnormalities in the urinary tract. ", "page_idx": 22}, {"type": "text", "text": "PersonU: Where can I go to get this test done? ", "page_idx": 22}, {"type": "text", "text": "PersonA: Urinalysis can be performed in various healthcare settings, including doctors\u2019 offices, urgent care facilities, laboratories, and hospitals. You can provide a urine sample in a specimen cup, and the sample can be analyzed on-site or sent to a laboratory for testing. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "PersonU: How much urine is typically needed for the test? ", "page_idx": 22}, {"type": "text", "text": "PersonA: Usually, only small amounts of urine, ranging from 30 to $60\\,\\mathrm{~m1}$ , are required for urinalysis testing. It\u2019s a non-invasive and relatively simple test that provides valuable information about your health. ", "page_idx": 22}, {"type": "text", "text": "role-user: Given the following conversation between PersonU and PersonA: ", "page_idx": 22}, {"type": "text", "text": "{multi-turn QA conversation} ", "page_idx": 22}, {"type": "text", "text": "Modify PersonU\u2019s questions to use pronouns or common nouns to refer back to words ", "page_idx": 22}, {"type": "text", "text": "role-assistant: ", "page_idx": 23}, {"type": "text", "text": "E More Details and Results for Retrieval in Conversational QA ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Query Rewriting Prompts for GPT-3.5-Turbo ", "page_idx": 23}, {"type": "text", "text": "The in-context few-shot query rewriting prompt we use for GPT-3.5-Turbo is as follows: ", "page_idx": 23}, {"type": "text", "text": "You are a helpful, pattern-following assistant. ", "page_idx": 23}, {"type": "text", "text": "role-user:   \nGiven the following conversation between PersonU and PersonA:   \nPersonU: Hello, I would like to know what to do if I do not agree with any decision. PersonA: disagree with our decision about your monthly income adjustment amounts? PersonU: no. Where can I find my SHIP contact information?   \nPersonA: You can find your local SHIP contact information in the back of your Medicare & You 2020 Handbook online.   \nPersonU: and how do they calculate the adjustments?   \nInstead of having this entire conversation, how can PersonU get what he or she is looking for using a single question? Respond with that question. ", "page_idx": 23}, {"type": "text", "text": "role-assistant: How is the calculation for adjustments made by SHIP determined? ", "page_idx": 23}, {"type": "text", "text": "role-user:   \nGiven the following conversation between PersonU and PersonA:   \nPersonU: I need to know how to prepare for college.   \nPersonA: You should first define your goals and interest and be sure to know the costs and how to plan financially and academically for college.   \nPersonU: Is there something I can use as a guide to get ready for it?   \nInstead of having this entire conversation, how can PersonU get what he or she is looking for using a single question? Respond with that question. ", "page_idx": 23}, {"type": "text", "text": "role-assistant: What resources or guides can I use to help me prepare for college? ", "page_idx": 23}, {"type": "text", "text": "role-user: Given the following conversation between PersonU and PersonA: ", "page_idx": 23}, {"type": "text", "text": "{Dialogue History $^+$ Latest Question} ", "page_idx": 23}, {"type": "text", "text": "Instead of having this entire conversation, how can PersonU get what he or she is looking for using a single question? Respond with that question. ", "page_idx": 23}, {"type": "text", "text": "role-assistant: ", "page_idx": 23}, {"type": "text", "text": "E.2 More Results for Retrieval in Conversational QA ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Table 9, we show comprehensive comparisons between fine-tuning and rewriting methods. ", "page_idx": 23}, {"type": "text", "text": "Interestingly, we find that compared to only using the rewritten query as input (Dragon $^+$ Rewrite (w/ single query only)), giving additional dialog history (Dragon $^+$ Rewrite (w/ dialog history)) makes the average scores significantly drop. This is because Dragon is originally pretrained on single-turn queries, it will naturally have better generalization ability when a single-turn rewritten query instead of a multi-turn conversation is provided. And the rewritten query already contains sufficient information from the dialog history. ", "page_idx": 23}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/84b1576ea674f410f931a15542e1c11787716496d9808686a872496c039f2e8e.jpg", "table_caption": ["Table 9: Comprehensive multi-turn retrieval results across five datasets. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "In addition, we observe that \u201cDragon $^+$ Fine-tune\u201d performs on par with \u201cDragon $^+$ Fine-tune $^+$ Rewrite\u201d. In other words, for the multi-turn fine-tuning method, replacing original query with rewritten query as inputs yields comparable results. This is because rewritten query will not provide much additional information for the model since the dialog history has already been provided, and it might even cause negative effects (e.g., results on the TopiOCQA dataset) since it makes the whole conversation input not natural. This once again demonstrates how effective the fine-tuning method is in equipping the model with the capability to comprehend the multi-turn context. ", "page_idx": 24}, {"type": "text", "text": "F Details of CHATRAG BENCH ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "F.1 Dataset Statistics and Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Doc2Dial (D2D)(Feng et al., 2020), a dataset covering DMV, SSA, VA, and Student Aid domains with an average document length of 101K words. We use the test set of Doc2Dial for the evaluation. It consists of 719 dialogs with 3939 user-agent turns. License of this dataset is \u201cApache License $2.0^{\\circ}$ . ", "page_idx": 24}, {"type": "text", "text": "QuAC (Choi et al., 2018), an open-domain dataset with an average document length of 15K words and unanswerable cases. We use the validation set of QuAC for the evaluation since its test set cannot be directly obtained. Its validation set consists of 1000 dialogs with 7354 user-agent turns. Among these 7354 user-agent turns, there are 1486 (around $20.2\\%$ ) unanswerable questions. For the combined evaluation of answerable and unanswerable questions, we set the ground truth response for the unanswerable question as \u201cSorry. I cannot find the answer based on the context.\u201d, same as the one we set in our stage-2 tuning. For the fair comparison, we replace the unanswerable responses in the baseline models (i.e., Llama2-SFT/Chat, GPT-3.5-Turbo, GPT-4) with the same sentence (Details can be found in Appendix H). License of this dataset is \u201cCC BY-SA $4.0^{\\circ}$ . ", "page_idx": 24}, {"type": "text", "text": "QReCC (Anantha et al., 2021), an open-domain dataset with an average document length of 5K words and maximum document length of 20K words. We use the test set of QReCC for the evaluation. Its test set includes some dialogue samples source from the QuAC dataset. To avoid overlap with other benchmark datasets, we remove those QuAC-sourced samples, resulting in 2805 user-agent turns. License of this dataset is \u201cApache License $2.0^{\\circ}$ . ", "page_idx": 24}, {"type": "text", "text": "TopiOCQA (TCQA)(Adlakha et al., 2022) uses the whole Wikipedia as the documents. We use the validation set of TopiOCQA since its test set is not available yet. Its validation set consists of 205 dialogs with 2514 user-agent turns. Each question has a total of four human-written answers. It has unanswerable cases. However, we do not include this dataset in unanswerable case evaluation because we only find 46 questions with no answer being found, which is not sufficient enough for the unanswerable case evaluation. Given that this dataset use the whole Wikipedia as the retrieval corpus, we leverage its topic information for multi-turn retrieval since we focused more on conversational QA over documents which typically narrow down to a specific topic or domain. License of this dataset is \u201cAttribution-NonCommercial-ShareAlike 4.0 International\u201d. ", "page_idx": 24}, {"type": "text", "text": "INSCIT (Wu et al., 2023) use the whole Wikipedia as the documents. We use the validation set of TopiOCQA since its test set is not available yet. Its validation set consists of 86 dialogues with 502 user-agent turns. Each question has an average of 1.9 human-written answers. It has unanswerable response for a few questions. However, we do not include this dataset in unanswerable case evaluation because every question always has at least one answerable response, which means every question is answerable. INSCIT also use the whole Wikipedia as the retrieval corpus. Hence, we also leverage its topic information for multi-turn retrieval for the same reason mentioned in TopiOCQA. License of this dataset is \u201carXiv.org perpetual, non-exclusive license\u201d. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "CoQA We use the validation set of CoQA since its test set cannot be directly obtained. Its validation set consists of 500 dialogues with 7983 user-agent turns. Each question has a total of four humanwritten answers. It has unanswerable cases. However, we do not include this dataset in unanswerable case evaluation for the same reason as in the TopiOCQA dataset. We only find 13 questions with no answer being found, which is not sufficient enough for the unanswerable case evaluation. CoQA contains passages from seven domains. Five of these public ones are under the following licenses: Literature and Wikipedia passages are shared under \u201cCC BY-SA 4.0 license\u201d; Children\u2019s stories are collected from MCTest which comes with \u201cMSR-LA license\u201d; Middle/High school exam passages are collected from RACE which comes with its own license; and News passages are collected from the DeepMind CNN dataset which comes with \u201cApache license\u201d. ", "page_idx": 25}, {"type": "text", "text": "DoQA We use the test set of DoQA for the evaluation. Its test set consists of 1200 dialogues with 5394 user-agent turns across Cooking, Travel and Movie domains. Among 5394 user-agent turns, there are 1479 (around $27.4\\%$ ) unanswerable questions. We use the same strategy as the one mentioned in QuAC dataset to incorporate unanswerable samples into the evaluation. License of this dataset is \u201cCC BY-SA $4.0^{\\circ}$ . ", "page_idx": 25}, {"type": "text", "text": "ConvFinQA We use the validation set of ConvFinQA since its test set cannot be directly obtained. Its validation set consists of 421 dialogues with 1490 user-agent turns. License of this dataset is \u201cMIT License\u201d. ", "page_idx": 25}, {"type": "text", "text": "SQA We use the test set of SQA for the evaluation. Its test set consists of 1025 dialogues with 3100 user-agent turns. License of this dataset is \u201cMicrosoft Research Data License Agreement\u201d. ", "page_idx": 25}, {"type": "text", "text": "HybriDial We use the test set of HybriDial for the evaluation. Its test set consists of 243 dialogues with 1111 user-agent turns. License of this dataset is \u201carXiv.org - Non-exclusive license\u201d. ", "page_idx": 25}, {"type": "text", "text": "F.2 Details of Incorporating Table as Context ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We use the markdown format to process the table in the document (i.e., using \u201c|\u201d to separate each cell in the table). However, as the number of rows in a table increases, it is harder for the model to link the values of cells in the last few rows to their corresponding headers in the table. To address this issue, we append the headers to the corresponding cell values in each row, so as to make it easier for the model to capture the connections between the cells and their respective headers. For example, a original markdown table ", "page_idx": 25}, {"type": "text", "text": "| Boat | Crew | Nation | Date | Meet | Location | | M1x | Mahe | FR | 2009 | | Poznan | | M2- | Hamish | US | 2012 | Olympics | Lucerne | | $^{82+}$ | Igor | DE | 1994 | | Indianapolis | ", "page_idx": 25}, {"type": "text", "text": "will be converted to ", "page_idx": 25}, {"type": "text", "text": "| M1x (Boat) | Crew: Mahe | Nation: FR | Date: 2009 | Meet: | Location: Poznan | | M2- (Boat) | Crew: Hamish | Nation: US | Date: 2012 | Meet: Olympics | Location: Paris | $^{82+}$ (Boat) | Crew: Igor | Nation: DE | Date: 1994 | Meet: | Location: Indianapolis | ", "page_idx": 25}, {"type": "text", "text": "We apply this table processing strategy to all tabular datasets. ", "page_idx": 25}, {"type": "text", "text": "F.3 Prompts for the Benchmarks ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We adopt the prompt format mentioned in $\\mathrm{\\SC}.2$ for benchmarking ChatQA, Llama2-Chat, Llama3- Instruct, GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo. For Llama2-Chat and Llama3-Instruct, we modify the prompt based on their own templates, and we convert the prompt into the chat completion template for OpenAI models. We keep the {Context for Latest Question} the same for all models. We adjust the {Instruction} for the baselines (i.e., Llama2-Chat, GPT-3.5-Turbo and GPT4) to ensure they do not get sub-optimal results. We use different instructions for test benchmarks with different answer types (e.g., long answer, short answer, arithmetic calculation). The {Instruction} for all models on the benchmarks are as follows: ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "F.3.1 ChatQA ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We keep the {Instruction} consistent between the data blends in stage-2 and test benchmarks based on different answer types. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cPlease give a full and complete answer for the question.\u201d for DoQA, INSCIT, HybriDial, Doc2Dial, QuAC, and QReCC, since these datasets generally have long answers for the questions. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cAnswer the following question with a short span, or a full and complete answer.\u201d for SQA and TopiOCQA, since these datasets have both short and long answers based on the questions. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cAnswer the following question with a short span. The answer needs to be just in a few words.\u201d for CoQA, since it generally has short answers for the questions. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cAnswer the following question with a number from context or the math arithmetic using $^+$ , -, \\*, or /.\u201d for ConvFinQA, since this dataset requires the model to either extract numbers from the context or do arithmetic calculation. We will calculate the number based on the arithmetic formula the model generates and compare it with the gold answer. ", "page_idx": 26}, {"type": "text", "text": "F.3.2 Llama2-Chat & Llama3-Instruct ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Note that we tried original Llama2-Chat prompt template10, which gives slightly worse results compared to using the one in $\\S C.2$ . We have tried several {Instruction} for Llama2-Chat. We find the ones below works the best. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cPlease give an answer in just one sentence.\u201d for DoQA, INSCIT, HybriDial, Doc2Dial, QuAC, and QReCC, since these datasets generally have long answers but within one sentence. We notice that it is important to give Llama2-Chat specific instruction like \u201cone sentence\u201d instead of \u201cfull and complete answer\u201d to prevent the model from generating very long answers. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cAnswer the following question with a short span, or one sentence.\u201d for TopiOCQA, since this dataset has both short and long answers based on the questions, and the long answers are generally within one sentence. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cAnswer the following questions with one or a list of entities.\u201d for SQA, since the answer for this dataset always consists of one or a list of entities from the context. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cAnswer the following question with a short span. The answer needs to be just in a few words.\u201d for CoQA, since it generally has short answers for the questions. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cAnswer the following question with just a number from context or just the math arithmetic using $^+$ , -, \\*, or /.\u201d for ConvFinQA, since this dataset requires the model to either extract numbers from the context or do arithmetic calculation. We extract the arithmetic formula generated by the model and use a calculator to get its final result. ", "page_idx": 26}, {"type": "text", "text": "F.3.3 GPT-3.5-Turbo & GPT-4 & GPT-4-Turbo ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We have tried several {Instruction} for GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo, we find the ones below works the best. ", "page_idx": 26}, {"type": "text", "text": "We use \u201cPlease give an answer in just one sentence.\u201d for DoQA, INSCIT, HybriDial, Doc2Dial, QuAC, and QReCC, since these datasets generally have long answers but within one sentence. Similar to Llama2-Chat, we also find OpenAI models tend to generate quite long answers given the instruction of \u201cfull and complete answer\u201d. Hence, we make the instruction more specific (i.e., \u201cone sentence\u201d) to prevent the model from generating very long answers. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "We use \u201cAnswer the following questions in JUST a few words or one sentence.\u201d for TopiOCQA, since this dataset has both short and long answers based on the questions, and the long answers are generally within one sentence. ", "page_idx": 27}, {"type": "text", "text": "We use \u201cAnswer the following questions with one or a list of entities. Do not give a detailed explanation. Answer needs to be as short as possible.\u201d for SQA. We find that OpenAI models often give detailed explanation for SQA dataset unless specifically instructed not to. ", "page_idx": 27}, {"type": "text", "text": "We use \u201cAnswer the following question with a short span. The answer needs to be just in a few words.\u201d for CoQA, since it generally has short answers for the questions. ", "page_idx": 27}, {"type": "text", "text": "We use \u201cAnswer the following questions with just a number from context or just the math arithmetic using $^+$ , -, \\*, or /.\u201d for ConvFinQA, since this dataset requires the model to either extract numbers from the context or do arithmetic calculation. We extract the arithmetic formula generated by the model and use a calculator to get its final result. ", "page_idx": 27}, {"type": "text", "text": "G Human Evaluation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "G.1 Evaluation Setup ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "bkUvKPKafQ/tmp/82340fbc1919eef5a37b1ced5c85629dcaace3e88cade5ef07b8fa299fa34ff3.jpg", "img_caption": ["Figure 4: Human evaluation instructions and the interface for annotators. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "We conduct human evaluation across the ten test benchmark datasets. We randomly select 60 samples for each dataset, and each sample is labelled by three annotators, which results in a total of 1800 annotations. ", "page_idx": 27}, {"type": "text", "text": "We ask the annotators to verify the facts in models\u2019 outputs and determine which model provides a more accurate response to the question. We use Amazon Mechanical Turk platform to conduct the human evaluation. The details of our human evaluation instructions and the interface shown to annotators are illustrated in Figure 4. ", "page_idx": 27}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/e72549108765f1a6e82da28b577eb64114b43dc3bbd21830b725836a121c526a.jpg", "table_caption": [], "table_footnote": ["Table 10: Human evaluation (A/B testing) comparing our ChatQA-1.0-70B to GPT-4-0613 on CHATRAG BENCH. In terms of average scores, our model and GPT-4 are tie most of the time $(\\sim\\!69.1\\%)$ , and GPT-4-0613 achieves slightly higher win rate $(\\sim\\!3.3\\%)$ than ours. "], "page_idx": 28}, {"type": "text", "text": "G.2 Full Human Evaluation Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The full human evaluation results over the 10 datasets on CHATRAG BENCH are shown in Table 10. We first find that our ChatQA-1.0-70B and GPT-4 are tie most of the time $(\\sim\\!69.1\\%)$ , and GPT-4 achieves slightly higher win rate $(\\sim\\!3.3\\%)$ than ours. This further confirms our model has powerful capability to produce correct answers. Second, we find that our model achieves slightly better win rate than GPT-4 in ConvFinQA, which indicates the strong arithmetic calculation ability of our model. Third, we find that GPT-4 achieves significantly better win rate in SQA task, which suggests that there is still a gap between our model and GPT-4 on tabular reasoning task. ", "page_idx": 28}, {"type": "text", "text": "H Unanswerable Case Evaluation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "H.1 Evaluation Setup ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For unanswerable case, we consider the model indicating that the question cannot be answered as correct. For answerable cases, we consider the model not indicating the question is unanswerable as correct (i.e., the model giving an answer). We find that ChatQA always generates \u201cSorry. I cannot find the answer based on the context\u201d when it finds the question is unanswerable, since we use this sentence as a response for unanswerable questions in the stage-2 tuning. As for Llama2-Chat, Llama3-Instruct, GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo, their outputs for unanswerable questions usually follow some specific patterns. Below, we list the heuristic matching patterns derived from all generated samples in DoQA and QuAC datasets to determine if the model suggests that the question is unanswerable. When we find the generated output contains the pattern listed below, we interpret it as an indication that the question is unanswerable, and we will replace its answer with \u201cSorry. I cannot find the answer based on the context.\u201d for the F1 evaluation. ", "page_idx": 28}, {"type": "text", "text": "cannot find, can\u2019t find, not able to, unable to, does not provide, cannot provide, cannot answer, couldnot answer, can\u2019t answer, couldn\u2019t answer, cannot be found, cannot be determined, do not have, couldn\u2019t find, no information, does not mention, doesn\u2019t mention, not explicitly mentioned, not explicitly explain, can not find, could not find, does not specify, doesn\u2019t provide, doesn\u2019t specify, there is no, not mentioned, don\u2019t have, don\u2019t know, does not include, doesn\u2019t include, does not contain, doesn\u2019t contain, not provided, does not indicate, doesn\u2019t indicate, does not disclose, doesn\u2019t disclose ", "page_idx": 28}, {"type": "text", "text": "H.2 Ablation Studies on the Number of Unanswerable Samples for Training ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In Table 11, we find that compared to using SyntheticConvQA, using HumanAnnotatedConvQA significantly increase the average accuracies on both QuAC and DoQA. It is because the unanswerable annotations are in higher quality for human-annotated data, which significantly improve the accuracy for unanswerable cases. Furthermore, we conduct ablation studies in terms of the number of unanswerable samples in HumanAnnotatedConvQA. We find that using a small amount of unanswerable samples (e.g., 1.5k) is able to achieve remarkable results on the unanswerable evaluation, as well as the overall CHATRAG BENCH score. Interestingly, incorporating more unanswerable samples does not necessarily lead to higher accuracy scores in the unanswerable evaluation. ", "page_idx": 28}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/509d06029fb5f1612938bac525351406f17a78c488534c60c1261ada3fc0da2d.jpg", "table_caption": [], "table_footnote": ["Table 11: Accuracies on answerable and unanswerable samples across QuAC and DoQA datasets. Avg-Both is the averaged score between QuAC and DoQA, and ChatRAG is the average score on the CHATRAG BENCH. \u2662denotes that the HumanAnnotatedConvQA is replaced with the SyntheticConvQA. \\* \u201cno\u201d denotes the unanswerable samples, while \u201cyes\u201d denotes the answerable samples. \u2020We conduct ablation studies in terms of the number of unanswerable samples in HumanAnnotatedConvQA. We use $1.5\\mathrm{k}$ unanswerable samples for our final ChatQA models, as it produces both high-quality generation and less hallucination. "], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/f0764316a92e09b181d2636129446db0e6e7aae3712fb0ea6b8d688ba7c1e714.jpg", "table_caption": ["Table 12: Case studies on ChatQA-1.0-70B and GPT-4-0613. We use blue color to highlight the relevant context for the last user question. And we use green and red color to highlight the correct and incorrect parts in the assistant responses. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "In Table 12, we show four examples of ChatQA-1.0-70B and GPT-4-0613 outputs. More examples can be found in Appendix I. The first example (about Board hearing) is a simple information seeking problem, and both ChatQA-1.0-70B and GPT-4 answer correctly. In the second example (about Arya), the model needs to find implicit information (highlighted in blue) for giving the answer. GPT-4 tends to be safe on providing an answer, and it replies that the context does not provide exact information about the age which is also correct. ", "page_idx": 30}, {"type": "text", "text": "Both the third and fourth examples require the model to have good tabular understanding and reasoning ability. In the third example (about Massif), ChatQA-1.0-70B gives correct answer by comparing the size of preserved area against 3100 ha, while GPT-4 fails to do so. In the fourth example (about John B. England), ChatQA-1.0-70B correctly lists three dates but misses one date, while GPT-4 correctly answers the question. ", "page_idx": 30}, {"type": "text", "text": "More examples on ChatQA-1.0-13B, ChatQA-1.0-70B, GPT-3.5-Turbo-0613, and GPT-4-0613 can be found in Table 13. We use blue color to highlight the relevant context for the last user question. And we use green and red color to highlight the correct and incorrect parts in the assistant responses. ", "page_idx": 30}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/c5abe2ba25c43bb00ce8365a75749e9b25c0b20ed7596b9fa32b5736a6cbddbc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/2bdcf110f5a630b38f51980c41a41131b47a0c1370788b6f3a8c3f375aa5da7a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/08b4f284a26b83a7d7d2161406e08d890f34d78b422c7fcc45e08d31430dee94.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Table 13: Case study for ChatQA-1.0-13B, ChatQA-1.0-70B, GPT-3.5-turbo-0613, and GPT-4-0613. We use blue color to highlight the relevant context for the last user question. And we use green and red color to highlight the correct and incorrect parts in the assistant responses. ", "page_idx": 33}, {"type": "text", "text": "J Human Annotated Conversational QA Data ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "J.1 Statistics of User-Agent Turns ", "page_idx": 34}, {"type": "image", "img_path": "bkUvKPKafQ/tmp/7d40145a861fa426ef4f2dcce65ef4311dfccabb93f84bf954d95fdff525c874.jpg", "img_caption": ["Figure 5: Statistics regarding the number of user-agent turns for each dialogue ", ""], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 5 illustrates the statistics regarding the number of user-agent turns per dialogue over the $7\\mathbf{k}$ human annotated conversational QA data. ", "page_idx": 34}, {"type": "text", "text": "J.2 Guidelines for Conversational QA Data Collection ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we demonstrate the guidelines we provide for the conversational QA data collection. We request the annotators to act as both user and agent to construct the conversational QA sample based on the given document, which makes the annotation more efficient. The guidelines consists of three parts: 1) What does conversational QA samples look like; 2) What kinds of conversational QA samples we need; and 3) What we need to annotate. ", "page_idx": 34}, {"type": "text", "text": "J.2.1 What does conversational QA samples look like ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "One conversational QA sample is based on a document that we provide. The user and agent behaviors are as follows: ", "page_idx": 34}, {"type": "text", "text": "\u2022 User behavior: 1) Ask questions to agent based on the given document; 2) Answer questions from agent when agent wants to clarify something. \u2022 Agent behavior: 1) Answer questions from user based on the document; 2) Ask questions to the user when the user\u2019s question is not clear, or too general/broad. ", "page_idx": 34}, {"type": "text", "text": "J.2.2 What kinds of multi-turn QA samples we need ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We list the requirements for user\u2019s questions and agent\u2019s responses below. ", "page_idx": 34}, {"type": "text", "text": "User\u2019s Questions ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 User\u2019s questions can refer to his/her previous (or multiple turns before) question.   \n\u2022 User\u2019s questions can also refer to the previous (or multiple turns before) answer from the agent.   \n\u2022 Try to use pronouns or common nouns to replace the entities mentioned before.   \n\u2022 Try to make the user\u2019s questions diverse. For the same type of questions, try to use different ways to express it in different dialogue annotations.   \n\u2022 Other user question types we need   \n\u2013 Given the answer from agent, ask for extra information (e.g., What else ...; any other ...; is there more ...).   \n\u2013 Switch the topic and starting a new thread in conversation.   \n\u2013 Ask two questions at the same time. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "Agent\u2019s Response ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 Try to make the agent\u2019s answer within 1-2 sentences. If the answer has to be long, try to make it concise.   \n\u2022 Try not to directly copy the whole relevant context from the document as the answer. Instead, try to construct the answer by paraphrasing the chosen context.   \n\u2022 Try to come up with a small proportion of cases where agent asks questions to user to clarify something. Specifically, when the user\u2019s question is too broad or not clear, the agent needs to narrow down the scope by asking clarification questions to see which specific aspects that the user cares more about. ", "page_idx": 35}, {"type": "text", "text": "J.2.3 What we need to annotate ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We list what we need to annotate for each conversation below. ", "page_idx": 35}, {"type": "text", "text": "\u2022 For each document, we need to annotate the user\u2019s questions and corresponding agent\u2019s responses. The average number of user-agent turns per conversation needs to be around five. \u2022 For each user\u2019s question, we need to annotate all the relevant context within the document. ", "page_idx": 35}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/484d5de10a9d21ddbb68e9c5cd20ddcc7a7277151eff44e224fc2829e972a81b.jpg", "table_caption": ["K Results of All Models on CHATRAG BENCH "], "table_footnote": ["Table 14: Full zero-shot results on CHATRAG BENCH. \\* SFT denotes that only stage-1 supervised fine-tuning is applied. We add Llama2-13B and our in-house 8B/22B models. "], "page_idx": 36}, {"type": "table", "img_path": "bkUvKPKafQ/tmp/665e13033cfa909bd16e7bdac85cd07b85821901a8ea334054d40298d4733bd7.jpg", "table_caption": [], "table_footnote": ["Table 15: Studies on directly applying stage-2 instruction tuning to the powerful Llama3-Instruct models. "], "page_idx": 36}, {"type": "text", "text": "In Table 14, we report zero-shot results of all ChatQA models on CHATRAG BENCH. ChatQA models generally give great improvements compared to the corresponding SFT/Chat/Instruct counterparts. ", "page_idx": 36}, {"type": "text", "text": "In Table 15, we study the effectiveness of directly applying stage-2 instruction tuning to the powerful Llama3-Instruct models. Llama3-Instruct models can be considered as those that have already finished the Stage-1 SFT training. They use a much larger dataset than the stage-1 training for Llama3- ChatQA-1.5. However, we find that \u201cLlama3-Instruct- $\\mathrm{\\cdot8B/70B+Stage{-2}^{\\circ}}$ performs comparable to Llama3-ChatQA-1.5-8B/70B. This further suggests that the stage-2 instruction tuning plays a key role in improving model\u2019s capability in conversational QA and RAG. ", "page_idx": 36}, {"type": "text", "text": "L Training Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "For all ChatQA models, in stage-1 SFT, we use a learning rate of 5e-6, and train 1000 iterations with a global batch size of 128; and in stage-2 instruction tuning, we use a learning rate of 3e-7, and train 3300 iterations with a global batch size of 64. We use 256 NVIDIA A100 GPUs for training ChatQA-1.0-70B and Llama3-ChatQA-1.5-70B models, and it takes around three hours for stage-1 training and around six hours for the stage-2 training. We use 64 NVIDIA A100 GPUs for training ChatQA-1.0-7B and Llama3-ChatQA-1.5-8B models, and it takes around one and half hours for stage-1 training and around three hours for stage-2 training. ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have comprehensive experimental results in $\\S\\ 6$ . ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We discuss the limitations of our models at the beginning of the Appendix ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This work does not propose any theory assumptions and does not include theoretical results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide all the details of the data collection for training and evaluations, which can be found in section 3, 4, 5 and appendix C, D, E and H. We also open-source model weights, training data, evaluation data, and scripts for reproducing our results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We open-source model weights, training data, evaluation data, and scripts for reproducing our results. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Dataset details can be found in section 5 and Appendix F. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: All results are zero-shot and deterministic (generations are using greedy search, retrieval scores are also deterministic). Hence, the results will not have any variances. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We provide the compute resources information in the training details section of the Appendix. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our research is conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We discuss both potential societal impacts and negative impacts at the beginning of the Appendix. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We include licenses of the datasets in the Appendix F. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We provide details explanations of how we collect human-annotated data and synthetic data in the Appendix D and Appendix J. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ", "page_idx": 42}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]