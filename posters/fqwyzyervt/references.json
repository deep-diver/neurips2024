{"references": [{"fullname_first_author": "Martin Abadi", "paper_title": "Deep learning with differential privacy", "publication_date": "2016-10-24", "reason": "This paper is foundational for the privacy-preserving techniques used in FeT, providing a rigorous framework for differential privacy in machine learning."}, {"fullname_first_author": "Borja Balle", "paper_title": "Improving the gaussian mechanism for differential privacy: Analytical calibration and optimal denoising", "publication_date": "2018-07-01", "reason": "This paper enhances the Gaussian mechanism, a core component of FeT's privacy-preserving framework, improving its accuracy and calibration."}, {"fullname_first_author": "Keith Bonawitz", "paper_title": "Practical secure aggregation for privacy-preserving machine learning", "publication_date": "2017-10-30", "reason": "This paper presents a practical secure aggregation protocol crucial for FeT's multi-party privacy framework, enabling collaborative model training without directly sharing raw data."}, {"fullname_first_author": "Jakub Kone\u010dn\u1ef3", "paper_title": "Federated learning: Strategies for improving communication efficiency", "publication_date": "2016-10-17", "reason": "This paper is foundational for federated learning, which FeT extends to the multi-party, vertically partitioned setting, and proposes strategies to improve communication efficiency."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-03", "reason": "This paper introduces the transformer architecture, which is the basis for the FeT model's design, enabling parallel processing and superior performance on long-range dependencies."}]}