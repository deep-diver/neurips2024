[{"figure_path": "FqWyzyErVT/figures/figures_1_1.jpg", "caption": "Figure 1: Real application of multi-party fuzzy VFL: travel cost prediction in a city", "description": "The figure illustrates a real-world application of multi-party fuzzy Vertical Federated Learning (VFL) for travel cost prediction.  Different transportation companies (taxi, bus, car, bike) each possess a unique set of features related to their services (e.g., price, number of passengers, gas cost, availability).  However, they share common route identifiers (start and end coordinates, which might be slightly different or fuzzy due to measurement errors or different mapping systems). VFL allows these companies to collaboratively train a model to predict travel costs without directly sharing their private data, leveraging the common, albeit fuzzy, route identifiers to link relevant data.", "section": "1 Introduction"}, {"figure_path": "FqWyzyErVT/figures/figures_4_1.jpg", "caption": "Figure 2: Structure of federated transformer (PE: multi-dimensional positional encoding)", "description": "This figure illustrates the architecture of the Federated Transformer (FeT) model, which is a transformer-based architecture designed for multi-party fuzzy VFL. It shows how the model encodes keys into data representations using multi-dimensional positional encoding and employs a dynamic masking module to filter out incorrectly linked data records.  The figure also depicts the secure multi-party summation process used during the aggregation of outputs from the encoders at the secondary parties and the usage of the party dropout strategy to improve performance and reduce communication costs.", "section": "5.1 Model Structure"}, {"figure_path": "FqWyzyErVT/figures/figures_5_1.jpg", "caption": "Figure 3: Learned dynamic masks of different samples: Each figure displays one sample (red star) from the primary party fuzzily linked with 4900 samples (circles) from 49 secondary parties. The position indicates the sample\u2019s identifier, and colors reflect learned dynamic mask values. Larger mask values signify higher importance in attention layers.", "description": "This figure visualizes the output of the trainable dynamic masking module in the Federated Transformer (FeT) model.  Each plot shows a single data point from the primary party (red star) and its 4900 nearest neighbors from 49 secondary parties (blue circles). The x and y axes represent the key identifiers, while the color intensity represents the learned dynamic mask value.  Larger, warmer colors indicate stronger attention weights assigned by the model to these data points during training, effectively filtering out less relevant data points with weaker connections.", "section": "5.1 Model Structure"}, {"figure_path": "FqWyzyErVT/figures/figures_5_2.jpg", "caption": "Figure 4: Misalignment of positional encoding (P): primary party; P\u2081 ~ P3: secondary parties)", "description": "This figure visualizes the misalignment issue of positional encoding in the Federated Transformer model.  It shows scatter plots illustrating the correlation between input and output distances of positional encoding across different parties.  Panel (a) displays a low correlation between the primary party's positional encodings and those of secondary parties (P\u2081-P\u2083). Panel (b) shows a high positive correlation between input and output positional encodings within the primary party. Panel (c) shows a negligible correlation between the primary party's inputs and the encodings of a specific secondary party (P\u2083). This highlights the problem of positional encoding misalignment across parties which the proposed positional encoding averaging technique aims to solve. ", "section": "5.1 Model Structure"}, {"figure_path": "FqWyzyErVT/figures/figures_6_1.jpg", "caption": "Figure 2: Structure of federated transformer (PE: multi-dimensional positional encoding)", "description": "This figure shows the architecture of the Federated Transformer (FeT) model.  It illustrates how the data flows between the primary party (holding labels) and the secondary parties (without labels) during the training process. The primary party has both an encoder and a decoder, while each secondary party has only an encoder.  Key information is integrated into feature vectors using multi-dimensional positional encoding. The outputs from the secondary parties' encoders are aggregated and fed into the decoder of the primary party.  The figure also highlights components like multi-head attention, feed-forward layers, layer normalization, and dynamic masking, which are key elements of the FeT's design to improve performance and privacy.", "section": "5.1 Model Structure"}, {"figure_path": "FqWyzyErVT/figures/figures_7_1.jpg", "caption": "Figure 5: Differentially private split-sum neural network", "description": "This figure illustrates the architecture of SplitAvg, a multi-party privacy-preserving VFL framework that integrates differential privacy (DP), secure multi-party computation (MPC), and norm clipping to enhance the privacy of representations.  It shows how the secondary parties' representations are processed with norm clipping and distributed Gaussian noise before secure multi-party averaging with the primary party. The resulting aggregated data is then used by the decoder in the FeT model.", "section": "6 Privacy"}, {"figure_path": "FqWyzyErVT/figures/figures_8_1.jpg", "caption": "Figure 6: Effect of Different Number of Neighbors K on FeT Performance", "description": "This figure displays the impact of varying the number of neighbors (K) on the performance of the Federated Transformer (FeT) model.  The results are shown across three different real-world datasets (house, bike, and hdb).  The performance is measured using Root Mean Squared Error (RMSE). The figure reveals that FeT's performance generally improves as K increases, but there is a point of diminishing returns. Importantly, FeT consistently outperforms baseline models (Top1Sim and FedSim) at larger K values, highlighting its effectiveness in fuzzy Vertical Federated Learning (VFL) scenarios.", "section": "7.2 Performance"}, {"figure_path": "FqWyzyErVT/figures/figures_9_1.jpg", "caption": "Figure 7: Impact of number of parties on FeT performance", "description": "This figure displays the impact of the number of parties on the performance of the Federated Transformer (FeT) model and its baselines (Solo, FedSim, and Top1Sim) for both MNIST and gisette datasets.  It shows how the test accuracy of each model changes as the number of parties increases from 10 to 50.  The results illustrate FeT's superiority in multi-party scenarios.", "section": "7.2 Performance"}, {"figure_path": "FqWyzyErVT/figures/figures_9_2.jpg", "caption": "Figure 8: Impact of noise scale \u03c3 on FeT accuracy and relationship between \u03c3 and \u03b5 under 10-party fuzzy VFL (RDP: without MPC, privacy loss calculated by R\u00e9nyi differential privacy)", "description": "This figure analyzes the performance of FeT under different privacy levels controlled by noise scale (\u03c3) and sampling rate on secondary parties.  The left two subfigures show accuracy vs. noise scale (\u03c3) on gisette and MNIST datasets, while the right two subfigures show the relationship between privacy parameter (\u03b5) and noise scale (\u03c3).  It demonstrates FeT's robustness to increased privacy constraints, even outperforming RDP (without MPC) methods, and highlights the effectiveness of the SplitAvg privacy framework.", "section": "7.3 Privacy"}, {"figure_path": "FqWyzyErVT/figures/figures_16_1.jpg", "caption": "Figure 9: Effect of party dropout rate on FeT", "description": "This figure shows the effect of the party dropout rate on the performance of the Federated Transformer (FeT) model. The left subplot shows the accuracy on the MNIST dataset, while the right subplot shows the accuracy on the gisette dataset.  The x-axis represents the party dropout rate, ranging from 0 to 1, while the y-axis represents the test accuracy. The results indicate that a moderate party dropout rate (around 0.6) improves the model's generalization ability without significantly reducing the accuracy.", "section": "C Ablation Study"}, {"figure_path": "FqWyzyErVT/figures/figures_17_1.jpg", "caption": "Figure 10: Effect of frequency of positional encoding averaging", "description": "This figure displays the results of an ablation study on the effect of positional encoding averaging frequency on the performance of the Federated Transformer (FeT) model.  The x-axis represents the average positional encoding frequency, and the y-axis represents accuracy. Separate lines show the performance on two datasets (MNIST and gisette) across various numbers of secondary parties (2, 5, 20, and 50). It demonstrates the impact of positional encoding averaging on model accuracy for different numbers of parties and datasets.", "section": "C.3 Positional Encoding"}, {"figure_path": "FqWyzyErVT/figures/figures_17_2.jpg", "caption": "Figure 7: Impact of number of parties on FeT performance", "description": "This figure shows the impact of the number of parties on the performance of FeT and baseline models on the MNIST and gisette datasets.  The accuracy is plotted against the number of parties, revealing FeT's superior performance, especially with a larger number of parties.  The underperformance of FeT and other models compared to Solo on the Gisette dataset with k=10 is attributed to overfitting due to the small size of the Gisette dataset.", "section": "7.2 Performance"}, {"figure_path": "FqWyzyErVT/figures/figures_19_1.jpg", "caption": "Figure 13: Impact of noise scale \u03c3 on FeT performance", "description": "This figure shows the results of an experiment evaluating the performance of the Federated Transformer (FeT) model under different noise scales (\u03c3) and secondary sampling rates.  The x-axis represents the noise scale (\u03c3), and the y-axis represents the Root Mean Squared Error (RMSE) for regression tasks or accuracy for classification tasks. Separate graphs are shown for three different datasets: house, taxi, and hdb.  Each graph shows three lines, corresponding to different secondary sampling rates (dpSample = 0.1, 0.5, and 1.0).  Error bars are included. The performance of a baseline model (Solo) is also shown for comparison, highlighting the impact of privacy-preserving techniques on model performance in multi-party fuzzy Vertical Federated Learning (VFL) settings.", "section": "7.3 Privacy"}, {"figure_path": "FqWyzyErVT/figures/figures_20_1.jpg", "caption": "Figure 14: Relationship between \u03b5 and noise \u03c3", "description": "This figure illustrates the relationship between the privacy parameter \u03b5 and the noise scale \u03c3 for different real-world datasets (house, taxi, hdb) using the Federated Transformer (FeT) and R\u00e9nyi Differential Privacy (RDP). It shows how the required noise scale \u03c3 changes with different \u03b5 values to maintain a certain level of privacy.  The plots demonstrate that FeT requires less noise than RDP to achieve the same level of privacy, showcasing its enhanced privacy-preserving capability.", "section": "7.3 Privacy"}, {"figure_path": "FqWyzyErVT/figures/figures_20_2.jpg", "caption": "Figure 15: Performance on feature split with different level of imbalance", "description": "This figure shows the performance of FeT and baseline methods (Solo and Top1Sim) on the MNIST dataset with varying levels of feature imbalance. The x-axis represents the imbalance factor (\u03b1), ranging from 0.1 to 50, where higher values indicate more balanced feature splits across parties.  The y-axis shows the test accuracy. The results demonstrate that FeT consistently achieves competitive or superior performance compared to the baseline methods across all levels of imbalance.", "section": "G Performance on Imbalanced Split"}]