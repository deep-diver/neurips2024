[{"type": "text", "text": "On the Sparsity of the Strong Lottery Ticket Hypothesis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Emanuele Natale Davide Ferr\u00b4e Giordano Giambartolomei Universit\u00b4e C\u02c6ote d\u2019Azur, Universite\u00b4 Co\u02c6te d\u2019Azur, Department of Informatics, CNRS, Inria, I3S, France CNRS, Inria, I3S, France King\u2019s College London ", "page_idx": 0}, {"type": "text", "text": "Fre\u00b4de\u00b4ric Giroire Universite\u00b4 Co\u02c6te d\u2019Azur, CNRS, Inria, I3S, France ", "page_idx": 0}, {"type": "text", "text": "Frederik Mallmann-Trenn Department of Informatics, King\u2019s College London ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Considerable research efforts have recently been made to show that a random neural network $N$ contains subnetworks capable of accurately approximating any given neural network that is sufficiently smaller than $N$ , without any training. This line of research, known as the Strong Lottery Ticket Hypothesis (SLTH), was originally motivated by the weaker Lottery Ticket Hypothesis, which states that a sufficiently large random neural network $N$ contains sparse subnetworks that can be trained efficiently to achieve performance comparable to that of training the entire network $N$ . Despite its original motivation, results on the SLTH have so far not provided any guarantee on the size of subnetworks. Such limitation is due to the nature of the main technical tool leveraged by these results, the Random Subset Sum (RSS) Problem. Informally, the RSS Problem asks how large a random i.i.d. sample $\\Omega$ should be so that we are able to approximate any number in $[-1,1]$ , up to an error of $\\varepsilon$ , as the sum of a suitable subset of $\\Omega$ . ", "page_idx": 0}, {"type": "text", "text": "We provide the first proof of the SLTH in classical settings, such as dense and equivariant networks, with guarantees on the sparsity of the subnetworks. Central to our results, is the proof of an essentially tight bound on the Random Fixed-Size Subset Sum Problem (RFSS), a variant of the RSS Problem in which we only ask for subsets of a given size, which is of independent interest. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Lottery Ticket Hypothesis (LTH) is a research direction that has attracted considerable attention over the years, stemming from the empirical contrast between the fact that, while large neural networks can be successfully trained to achieve good performance on a given task and successively pruned to a great level of sparsity without compromising their performance, researchers have struggled to train sparse neural networks from scratch. The authors of [12] observed that, using a simple pruning strategy (namely Iterative Magnitude Pruning while rewinding the original weights of the remaining edges to their value at initialization), starting from a sufficiently large random neural networks, it is possible to identify sparse subnetworks that can be trained to achieve the performance achievable by the starting network (see Figure 2 in the appendix for an illustration). The previous statement, namely the LTH, soon gave rise to an even stronger one, corroborated by empirical works [29, 26] which proposed \u201ctraining-by-pruning\u201d algorithms (see Section 2 for details), providing evidence that starting from a sufficiently large random neural networks, it is possible to identify sparse subnetworks that exhibit good performance as they are, without changing the original weights (see Figure 3 in the appendix for an illustration). By removing the need to analyze the dynamics of training, the last statement, namely the Strong Lottery Ticket Hypothesis (SLTH), allowed a fruitful series of rigorous proofs for increasingly more general architectures (see Section 2 for an overview). Such rigorous results can informally be stated as follows: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Theorem 1 (Informal statement of previous SLTH results). With high probability, a random artificial neural network $N_{\\Omega}$ with m parameters can be pruned so that the resulting subnetwork $N_{S}$ $\\varepsilon$ -approximates (i.e., approximates up to an error $\\varepsilon$ ) any target artificial neural network $N_{t}$ with $O\\left(\\Bar{m}/\\log_{2}(1/\\varepsilon)\\right)$ parameters. ", "page_idx": 1}, {"type": "text", "text": "It is important to note that, to this day, we only have proofs on the existence of such subnetworks, also called winning tickets, but it remains an open question how to find them reliably. ", "page_idx": 1}, {"type": "text", "text": "All theoretical results on the SLTH however have so far not investigated the interplay between the sparsity of the winning ticket $N_{S}$ and the size of the random neural network $N_{\\Omega}$ . This is in contrast to the original motivation of the LTH and to the practical application of the aforementioned training-by-pruning algorithms that motivated the SLTH, such as [15, 14]. In fact, to approximate target networks with $O\\left(m/\\log_{2}(1/\\varepsilon)\\right)$ parameters, essentially all winning tickets $N_{S}$ have $\\Theta(m)$ parameters (see Appendix A), thus being roughly of the same size of the original network $N_{\\Omega}$ . We thus ask the following natural question: ", "page_idx": 1}, {"type": "text", "text": "If we want to $\\varepsilon$ -approximate a family of target artificial neural networks with $m_{t}$ parameters by pruning a fraction $\\alpha$ , called sparsity, of the $m$ parameters of a random artificial neural network $N_{\\Omega}$ , how big should $m$ be? ", "page_idx": 1}, {"type": "text", "text": "We are particularly interested in the regime in which the density parameter $\\gamma=1-\\alpha$ vanishes as the size of the network increases, so that the size of the winning ticket $N_{S}$ is $\\gamma m=o(m)$ . ", "page_idx": 1}, {"type": "text", "text": "The above question has so far remained unanswered as a consequence of the limitation inherited from the core technical tool that has been leveraged so far to prove SLTH results, namely the Random Subset Sum (RSS) Problem [17]. Informally, the RSS asks how large a random i.i.d. sample $\\Omega$ should be so that we are able to approximate any number in $[-1,1]$ as the sum of a suitable subset of $\\Omega$ . The applicability of RSS to the SLTH was first recognized by [24] within the proof strategy previously developed in [20]. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We answer the aforementioned question by introducing and proving a refined variant of the RSS Problem, namely the Random Fixed-Size Subset Sum Problem (RFSS), in which the approximation of the target values should be achieved by only considering subsets of fixed size $k$ from a set of $n$ samples (Theorem 2). We focus on subsets of fixed size $k$ rather than subsets of size up to $k$ for two main reasons. From a theoretical point of view, it is a stronger requirement, and practically speaking, using fixed-size subsets enables us to achieve SLTH results where the layers of the lottery ticket exhibit a uniform structure, potentially offering a computational advantage in their implementation. ", "page_idx": 1}, {"type": "text", "text": "In Section 4, we show how the density $\\gamma$ impacts the overparameterization, i.e., the ratio $\\left(m/{m_{t}}\\right)$ between the number of parameters of the original network $N_{\\Omega}$ and that of the class of target networks $N_{t}$ that can be $\\varepsilon$ -approximated by pruning $N_{\\Omega}$ down to a subnetwork $N_{S}$ with $\\gamma m$ parameters. In our analysis, we also compare and recover as special cases previous SLTH results such as [24, 20, 8, 3, 9]. For instance, when $\\gamma m\\;=\\;\\Theta(m)$ , we recover up to a logarithmic factor the result of [24], which states that the overparameterization needed is $O(\\log_{2}\\left({m_{t}^{2}}/{\\varepsilon^{2}}\\right))$ ). In the case of Dense Neural Networks, Theorem 3 thus bridges the gap between the two extreme cases of $\\gamma m=\\Theta(m_{t})$ and $\\gamma m\\,=\\,\\Theta(m)$ considered in [20] and [24], respectively. It is worth noting that [24] is often considered an improvement over [20], as it exponentially reduces the overparameterization, albeit at the cost of a trivial sparsity level. Finally, we prove that our bounds on the overparameterization as a function of the subnetwork sparsity are essentially tight. ", "page_idx": 1}, {"type": "text", "text": "Organization of the paper. After reviewing the literature on the SLTH in Section 2, we introduce the Random Fixed-Size Subset Sum Problem in Section 3. In Section 4, we explore some applications of the RFSS Problem to the SLTH, and finally draw our conclusions in Section 5. Some limitations of our work, along with its potential impact, are discussed in Section 6. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The SLTH is named after the LTH, which was introduced by Frankle and Carbin in [12]. At the time of writing, this paper has received over 3,300 citations, attesting to the significance and impact of the research topic. Surveying the LTH is thus besides the scope of this work, and we defer the reader to dedicated surveys such as [16]. ", "page_idx": 2}, {"type": "text", "text": "The SLTH was empirically motivated by work investigating training-by-pruning algorithms such as [29, 26], namely algorithms that leverage the gradient of the network parameters to learn a good mask of the edges to be retained (i.e., a good subnetwork, called the winning ticket). [29] achieves this by learning a probability associated to each edge, which is then used to sample the edges that should be included in the subnetwork. [26] gets rid of the stochasticity involved in the aforementioned strategies by learning a score associated to each edge; the subnetwork is then determined by including the edges with the highest score. Such strategies are leveraged in [15, 14] in a federated learning setting, in order to improve the communication cost of distributed training by communicating the sampled masks of a fixed shared network, rather than the entire weights. However, these training-by-pruning algorithms are generally not computationally less expensive than classical training, since they also make use of backpropagation to update scores and are applied to a sufficiently large network to find a winning ticket. To reduce the computational cost of finding a good subnetwork, [13] shows, both theoretically and experimentally, that randomly pre-pruning the source network before looking for a winning ticket can be an effective approach. In [23], on top of randomly pruning the source network, some parameters are also frozen. Frozen parameters are forced to be part of the winning ticket and they do not have an associated score, which effectively reduces the search space for the training-by-pruning algorithms. ", "page_idx": 2}, {"type": "text", "text": "The first rigorous proof of the SLTH in the case of dense neural networks has been provided by [20], which establishes a framework that was inherited by the subsequent works. [24] crucially shows that the framework in [20] allows the application of the RSS analysis in [17], proving that, with no constraint on the size of the subnetworks, a random network with $m$ -parameters can be pruned to approximate target networks with $m/\\log(1/\\varepsilon)$ parameters (we defer the reader to Theorem 3 for details on further constraints on the parameters). An alternative proof of the result in [24] was simultaneously shown in [22]. [8] and [3] successively extended [24] and [22] to convolutional neural networks (CNNs). By leveraging multidimensional generalizations of RSS [7, 2], [6] further extended the SLTH to structured pruning of CNNs and, as a special case, dense networks. Finally, [9] provided a general framework that proves the SLTH for equivariant networks. ", "page_idx": 2}, {"type": "text", "text": "As for refinements and generalizations of the above results, [4] shows that, at the cost of a quadratic overhead in the overparameterization w.r.t. [24], the number of layers of the random network $N_{\\Omega}$ can be reduced to $\\ell+1$ , where $\\ell$ is the number of layers of the target networks $N_{z}$ ; furthermore, while previous results only considered networks with ReLU activation, [4] shows how to extend the proof in [24] to a more general class of activations functions. [5] introduces the notion of universal lottery ticket, and show that it is possible to prune a sufficiently overparameterized random network so that the resulting subnetwork (the lottery ticket) can approximate certain class of functions up to an affine transformation of the output of the subnetwork (in this sense being universal). [11] shows how to extend the proof in [24] when neurons have random biases, and adapts the trainingby-pruning algorithm of [26] to find a strong lottery ticket with a desired sparsity level. Motivated by theoretical insights on the existence of sparse strong lottery tickets, [10] develops a framework to plant the latter in large random network and investigates training-by-pruning algorithms, providing evidence that sparse strong lottery tickets typically exists for common machine learning tasks, and the difficulty to find them is of algorithmic nature. ", "page_idx": 2}, {"type": "text", "text": "Our proof of the RFSS Problem in Section 3 is based on the second moment method approach first explored by [18], and which has recently been refined to prove multidimensional generalizations of RSS by [7] and [2]. ", "page_idx": 2}, {"type": "text", "text": "3 Fixed-Size Random Subset Sum ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section we present our technical contributions on the RFSS, which are the foundation of our proofs regarding the sparsity of the SLTH. ", "page_idx": 2}, {"type": "text", "text": "sLeett $\\Omega=\\{X_{1},...,X_{n}\\}$ cainndg  as osemt eo fn oitnadtiicoens. $S\\subseteq[n]$ owtee  bdye $[n]$ $\\Sigma_{S}^{\\Omega}=\\dot{\\textstyle\\sum}_{i\\in S}X_{i}$ $\\{1,\\ldots,n\\}$ ,a nfod r $n\\in\\mathbb N$ .t $\\Omega$ ivwehne na clear from the context. We now define a class of distributions for w hich our RFSS result holds. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (sum-bounded). We say that a probability density function $f$ is sum-bounded if there exist positive constants $c_{l}$ and $c_{u}$ such that, for all $k\\in\\mathbb{N}$ , given $k$ independent samples $X_{1},...,X_{k}$ with density $f$ , the density of their sum $f_{\\Sigma_{[k]}}$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{c_{l}}{\\sqrt{k}}\\leq f_{\\Sigma_{[k]}}\\left(x\\right)\\leq\\frac{c_{u}}{\\sqrt{k}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with the lower bound holding for all $x\\in\\left[-{\\sqrt{k}},{\\sqrt{k}}\\right]$ and the upper bound holding for all $x\\in\\mathbb R$ . ", "page_idx": 3}, {"type": "text", "text": "At first, our definition of sum-bounded could look as a weaker version of a classical local limit theorem on the sum of random variables (e.g., see [25, Chapter VII, Theorem 7]). However, that is not the case, since we require a lower bound on the sum for any $k$ , which is needed to prove our main result. ", "page_idx": 3}, {"type": "text", "text": "Denote, for all $x\\in[0,1]$ , the binary entropy as ", "page_idx": 3}, {"type": "equation", "text": "$$\nH_{2}(x)=-x\\log_{2}x-(1-x)\\log_{2}(1-x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our main technical result is the following proof of a fixed-size subset variant of the RSS Problem. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. Let $0<\\varepsilon<1$ , $c_{h y p}\\geq1$ , k, n be integers with $1\\leq k\\leq\\frac{n}{2}$ , and let $\\Omega=\\{X_{1},...,X_{n}\\}$ where the $X_{i}$ \u2019s are i.i.d. random variables with sum-bounded density. There exists a constant $c_{t h m}$ such that, $i f$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nn\\geq c_{h y p}{\\frac{\\log_{2}{\\frac{k}{\\varepsilon}}}{H_{2}\\left({\\frac{k}{n}}\\right)}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "then for every fixed $z\\in\\left[-{\\sqrt{k}},{\\sqrt{k}}\\right]$ it holds that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\exists S\\subset[n]\\,,|S|=k:|\\Sigma_{S}-z|<\\varepsilon\\right)\\geq c_{t h m}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark. The proof of Theorem 2 is given in Section 3.1, and it actually holds for any $1\\leq k\\leq\\lambda n$ , for an arbitrary $\\lambda\\,\\in\\,[1/n,1)$ . We state the theorem this way for readability and because we are primarily interested in high-sparsity settings (i.e., small size $k$ of the subsets), so considering values of $k\\geq\\textstyle{\\frac{n}{2}}$ does not add much to our analysis. The same remark also holds for Corollary 1. ", "page_idx": 3}, {"type": "text", "text": "The sum-bounded condition of Definition 1 is easily verified for distributions such as the Gaussian distribution. Previous SLTH results rely on a classical resampling argument by [17, Corollary 3.3], which shows how RSS results for $\\mathrm{Uniform}[-1,1]$ independent random variables naturally extend to independent random variables that contains a uniform distribution, in the sense that they can be expressed as the mixture of distributions one of which is Uniform $[-1,1]$ with constant probability.1 The next lemma thus proves that the Uniform $[-1,1]$ distribution is sum-bounded2. A detailed proof is provided in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. The Uniform $[-1,1]$ probability density function is sum-bounded, i.e., given a set ${\\mathcal{U}}_{n}\\,=\\,\\{U_{i}\\}_{i\\in[n]}$ of i.i.d. variables $U_{i}$ with Uniform $![-1,1]$ probability density function, there exist constants $c_{l}$ and $c_{u}$ such that the probability density function $f(x,n)$ of the sum $\\Sigma_{[n]}^{\\mathcal{U}_{n}}$ of these variables, for all $n\\in\\mathbb{N}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{c_{l}}{\\sqrt{n}}\\leq f(x,n)\\leq\\frac{c_{u}}{\\sqrt{n}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with the lower bound holding for all $x\\in[-{\\sqrt{n}},{\\sqrt{n}}]$ , and the upper bound holding for all $x\\in\\mathbb R$ . ", "page_idx": 3}, {"type": "text", "text": "Finally, in our proofs on the Sparse SLTH in Section 4, we make use of the following c\u221aorol\u221alary of Theorem 2, which ensures a uniform high probability of hitting any target $z\\;\\in\\;[-\\sqrt{k},\\sqrt{k}]$ , considering independent random variables that contain a uniform distribution. ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. Let $0~<~p~\\leq~1$ and $\\varepsilon\\ \\in\\ (0,1/2)$ be constants, $k,n$ with $1\\ \\leq\\ k\\ \\leq\\ {\\frac{n}{2}}$ , and let $\\Omega\\,=\\,\\{X_{1},...,X_{n}\\}$ be i.i.d. random variables whose density is a mixture of a Unif $)r\\bar{m}([-1,1])$ with probability $p_{i}$ , and some other density otherwise. There exists a positive constant $c_{a m p}$ that only depends on $p$ such that, if ", "page_idx": 4}, {"type": "equation", "text": "$$\nn\\geq c_{a m p}{\\frac{\\log_{2}^{2}{\\frac{k}{\\varepsilon}}}{H_{2}\\left({\\frac{k}{n}}\\right)}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\forall z\\in\\left[-\\sqrt{k},\\sqrt{k}\\right],\\exists S_{z}\\subset[n],|S_{z}|=k:|\\Sigma_{S_{z}}-z|<\\varepsilon\\right)\\geq1-\\varepsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof Idea. The corollary follows from three arguments. First, by a standard sampling argument, we can assume that a constant fraction of the sample follows a $\\mathrm{Uniform}[-1,1]$ distribution. Secondly, by Lemma 1, the uniform probability density function is sum-bounded. We can thus apply Theorem 2, which guarantees a success probability of $c_{\\mathrm{thm}}$ for approximating a given target. Finally, by a standard probability amplification argument and a union bound applied to Theorem 2, by paying an extra factor $\\log_{2}(\\dot{k_{}^{\\prime}}/\\varepsilon)$ in Eq. 1, the constant $c_{\\mathrm{thm}}$ can be a\u221assum\u221aed to be $1-\\varepsilon$ , and the existence of a suitable subset $S_{z}$ holds simultaneously for all $z\\in[-{\\sqrt{k}},{\\sqrt{k}}]$ . Details are given in Appendix D. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "For $k$ big enough, we can get rid of the squared logarithmic dependency on $k$ in the right hand side of Equation 3, as shown in the following Corollary, whose proof can be found in Appendix E. ", "page_idx": 4}, {"type": "text", "text": "Corollary 2. Let $0\\,<\\,p\\,\\leq\\,1$ and $\\varepsilon\\;\\in\\;(0,1/2)$ be constants, $k,n$ be integers with $1\\ \\leq\\ k\\ \\leq\\ {\\frac{n}{2}}$ and $k\\geq2c_{a m p}\\left(\\log_{2}^{2}k+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}\\right)$ . Let $\\Omega=\\{X_{1},...,X_{n}\\}$ be i.i.d. random variables whose density is a mixture of a Uniform $([-1,1])$ with probability $p_{i}$ , and some other density otherwise. There exists a positive constant $c_{a m p}$ that only depends on $p$ such that, $i f$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nn\\geq2c_{a m p}\\frac{\\log_{2}^{2}\\frac{1}{\\varepsilon}}{H_{2}\\left(\\frac{k}{n}\\right)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left(\\forall z\\in\\left[-\\sqrt{k},\\sqrt{k}\\right],\\exists S_{z}\\subset[n],|S_{z}|=k:|\\Sigma_{S_{z}}-z|<\\varepsilon\\right)\\geq1-\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As customary in conference versions of papers, our proofs adopt the convention of taking ceilings and floors as suitable for non integer fractional terms. This is done in the interest of the reader (and ours), and does not impact the results in any significant way. ", "page_idx": 4}, {"type": "text", "text": "3.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Proof of Theorem 2. For simplicity, throughout the proof we will often\u221a use\u221a $c$ to denote any positive constant. Let $S_{k}=\\left\\{S\\subset[n]\\,|\\,|S|=k\\right\\}$ and define, for a fixed $z\\in[-\\sqrt{k},\\sqrt{k}]$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nY=Y(z)=\\sum_{S\\in S_{k}}Z_{S}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $Z_{S}\\,=\\,Z_{S}(z)\\,=\\,{\\bf1}_{\\{|\\Sigma_{S}-z|<\\varepsilon\\}}$ . Following [18], we exploit the second moment method for RFSS, generalising it to arbitrary $k$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{Pr}\\left(Y>0\\right)\\geq{\\frac{\\left(\\mathbb{E}\\left[Y\\right]\\right)^{2}}{\\mathbb{E}\\left[Y^{2}\\right]}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "it thus suffices to prove that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[Y^{2}\\right]\\leq c\\left(\\mathbb{E}\\left[Y\\right]\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We first rewrite Eq. 5 in a more convenient form. Let $\\tilde{S}$ and ${\\tilde{S}}^{\\prime}$ be two independently and uniformly at random chosen subsets of $[n]$ of size $k$ , and denote $H_{S}(z)$ as the event that $\\Sigma_{S}\\ \\varepsilon.$ -approximates $z$ , namely ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{S}=H_{S}(z)=\\left\\{|\\Sigma_{S}-z|<\\varepsilon\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y]=\\sum_{S\\in{S_{k}}}\\mathbb{E}[Z_{S}]=\\sum_{S\\in{S_{k}}}\\operatorname*{Pr}(H_{S})={\\binom{n}{k}}\\operatorname*{Pr}\\left(H_{\\tilde{S}}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}[Y^{2}]=\\mathbb{E}\\left[\\left(\\sum_{S\\in\\mathcal{S}_{k}}Z_{S}\\right)\\left(\\sum_{S^{\\prime}\\in\\mathcal{S}_{k}}Z_{S^{\\prime}}\\right)\\right]=\\sum_{S,S^{\\prime}\\in\\mathcal{S}_{k}}\\mathbb{E}\\left[Z_{S}Z_{S^{\\prime}}\\right]}}\\\\ &{}&{=\\sum_{S,S^{\\prime}\\in\\mathcal{S}_{k}}\\mathrm{Pr}\\left(H_{S}\\wedge H_{S^{\\prime}}\\right)=\\left(\\!\\!\\!\\begin{array}{l}{n}\\\\ {k}\\end{array}\\!\\!\\right)^{2}\\mathrm{Pr}\\left(H_{\\tilde{S}}\\wedge H_{\\tilde{S}^{\\prime}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using Eqs. 7 and 8 we can rewrite the r.h.s. of Eq. 5 as follows ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\left(\\mathbb{E}\\left[Y\\right]\\right)^{2}}{\\mathbb{E}\\left[Y^{2}\\right]}=\\frac{\\left[\\operatorname*{Pr}\\left(H_{\\tilde{S}}\\right)\\right]^{2}}{\\operatorname*{Pr}\\left(H_{\\tilde{S}}\\wedge H_{\\tilde{S}^{\\prime}}\\right)}=\\frac{\\operatorname*{Pr}\\left(H_{\\tilde{S}}\\right)}{\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\left|\\,H_{\\tilde{S}}\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Eq. 6 thus becomes ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\mid H_{\\tilde{S}}\\right)\\leq c\\operatorname*{Pr}\\left(H_{\\tilde{S}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $I_{i}$ denote the event $\\{|\\tilde{S}\\cap\\tilde{S}^{\\prime}|=i\\}$ and $I_{a,b}$ the event $\\cup_{a\\leq i\\leq b}I_{i}$ . Fix $\\mu\\in(\\lambda,1)$ . By the law of total probability and independence of $I_{i}$ and $H_{\\tilde{S}}$ , we rewrite the l.h.s. of Eq. 9 as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\mid H_{\\tilde{S}}\\right)}\\\\ &{=\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\wedge I_{k}\\mid H_{\\tilde{S}}\\right)+\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\wedge I_{\\mu k,k-1}\\mid H_{\\tilde{S}}\\right)+\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\wedge I_{0,\\mu k-1}\\mid H_{\\tilde{S}}\\right)}\\\\ &{=\\operatorname*{Pr}\\left(I_{k}\\right)\\cdot\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\mid H_{\\tilde{S}},I_{k}\\right)}\\\\ &{\\qquad+\\operatorname*{Pr}\\left(I_{\\mu k,k-1}\\right)\\cdot\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\mid H_{\\tilde{S}},I_{\\mu k,k-1}\\right)}\\\\ &{\\qquad+\\displaystyle\\sum_{i=0}^{\\mu k-1}\\left(\\operatorname*{Pr}\\left(I_{i}\\right)\\cdot\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\mid H_{\\tilde{S}},I_{i}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To conclude the proof, it suffices to show that each addendum in Eqs. 10, 11 and 12 are upperbounded by some constant multiple of $\\varepsilon/{\\sqrt{k}}$ , since the lower bound in Definition 1 ensures that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\varepsilon}{\\sqrt{k}}\\leq c\\operatorname*{Pr}\\left(H_{\\tilde{S}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As for the first addendum (Eq. 10), since $\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\,|\\,H_{\\tilde{S}},I_{k}\\right)=1$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(I_{k}\\right)\\cdot\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\,|\\,H_{\\tilde{S}},I_{k}\\right)=\\operatorname*{Pr}\\left(I_{k}\\right)=\\frac{1}{{\\binom{n}{k}}}\\overset{(a)}{\\leq}\\sqrt{\\frac{8k(n-k)}{n}}2^{-n H_{2}\\left(\\frac{k}{n}\\right)}}\\\\ &{\\overset{(b)}{\\leq}\\sqrt{\\frac{8k(n-k)}{n}}2^{-\\alpha_{\\mathrm{bp}}\\log_{2}\\frac{k}{\\varepsilon}}\\overset{(c)}{\\leq}2\\sqrt{2}\\frac{\\varepsilon}{\\sqrt{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where inequality $(a)$ in Eq. 14 is a standard lower bound on $\\binom{n}{k}$ holding for all $k\\,\\leq\\,n\\,-\\,1$ ; in inequality $(b)$ in Eq. 14 we used Eq. 1, namely $\\begin{array}{r}{n H_{2}\\left(\\frac{k}{n}\\right)\\geq c_{\\mathrm{hyp}}\\log_{2}\\frac{k}{\\varepsilon}}\\end{array}$ ; in inequality $(c)$ in Eq. 14 we used that $c_{\\mathrm{hyp}}\\geq1$ . ", "page_idx": 5}, {"type": "text", "text": "As for the second addendum (Eq. 11), we next show that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(I_{\\mu k,k-1}\\right)\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\mid H_{\\tilde{S}},I_{\\mu k,k-1}\\right)\\leq c\\frac{\\varepsilon}{\\sqrt{k}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "by proving that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(I_{\\mu k,k-1}\\right)\\leq{\\frac{c}{\\sqrt{k}}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\,|\\,H_{\\tilde{S}},I_{\\mu k,k-1}\\right)\\leq c\\varepsilon.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "First, observe that $I=|\\tilde{S}\\cap\\tilde{S}^{\\prime}|$ follows a Hypergeometric $(n,k,k)$ distribution, thus by Chebyshev\u2019s inequality ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(I_{\\mu k,k-1}\\right)\\leq\\operatorname*{Pr}\\left(I\\geq\\mu k\\right)=\\operatorname*{Pr}\\left(I-\\frac{k^{2}}{n}\\geq\\mu k-\\frac{k^{2}}{n}\\right)\\leq\\frac{\\operatorname{Var}\\left[I\\right]}{\\mu^{2}k^{2}\\left(1-\\frac{k}{\\mu n}\\right)^{2}}}\\\\ &{\\qquad\\qquad\\leq c^{\\prime}\\frac{\\frac{k^{2}}{n}\\frac{n-k}{n}\\frac{n-k}{n-1}}{k^{2}}\\leq\\frac{c}{\\sqrt{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "having set $c^{\\prime}=\\mu^{2}(1-\\lambda/\\mu)^{2}>0$ , thus proving Eq. 16. The proof of Eq. 17 is given in Appendix F, concluding the proof of Eq. 15. ", "page_idx": 6}, {"type": "text", "text": "As for the third addendum (Eq. 12), in Appendix $\\boldsymbol{\\mathrm F}$ we show that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\mu k-1}\\operatorname*{Pr}\\left(I_{i}\\right)\\cdot\\operatorname*{Pr}\\left(H_{\\tilde{S}^{\\prime}}\\mid H_{\\tilde{S}},I_{i}\\right)\\leq c\\frac{\\varepsilon}{\\sqrt{k}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The three bounds on the addenda in Eqs. 10, 11, and 12 (Eqs. 14, 15, and 18, respectively), combined with Eq. 13, conclude the proof. ", "page_idx": 6}, {"type": "text", "text": "4 Sparse Strong Lottery Ticket Hypothesis (SSLTH) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now apply our results on the RFSS problem to the SLTH and obtain guarantees on the sparsity of winning tickets for Dense Neural Networks (DNNs, Theorem 3) and Equivariant NNs (Theorem 4). ", "page_idx": 6}, {"type": "text", "text": "The next theorem essentially interpolates between the two extremes of [20][Theorem 2.1] (where $\\gamma m=\\Theta(m_{t}))$ and [24][Theorem 1] (where $\\gamma m=\\Theta(m))$ , where we recall that $m$ and $m_{t}$ represent the number of parameters of the overparameterized and the target networks, respectively, and $\\gamma$ is the density of the winning ticket. ", "page_idx": 6}, {"type": "text", "text": "We use $\\sigma(\\cdot)$ to denote the ReLU activation function, i.e., $\\sigma(x)=x\\cdot{\\mathbf{1}}_{x\\geq0}$ , and $\\lVert\\mathbf{W}\\rVert$ to denote the spectral norm of the matrix $\\mathbf{W}$ . Let $\\mathcal{F}$ to be a set of target ReLU neural networks $f:{\\mathbf{R}}^{d_{0}}\\rightarrow{\\mathbf{R}}^{d_{l}}$ of depth $l$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\left\\{f:f(\\mathbf{x})=\\mathbf{W}_{l}\\sigma(\\mathbf{W}_{l-1}\\ldots\\sigma(\\mathbf{W}_{1}\\mathbf{x})),\\,\\forall i\\,\\,\\mathbf{W}_{i}\\in\\mathbb{R}^{d_{i}\\times d_{i-1}}\\mathrm{~and~}\\|\\mathbf{W}_{i}\\|\\leq1\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For a given $f\\in\\mathcal F$ , for all $i\\in[\\ell]$ , let $\\rho_{i}=\\operatorname*{max}\\{d_{i-1}/d_{i},\\,d_{i}/d_{i-1}\\}$ , and $\\rho=\\operatorname*{max}_{i}\\rho_{i}$ . Then, recalling that $c_{\\mathrm{amp}}$ is the constant defined in Corollary 1, we have the following result. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (SSLTH for DNNs). Let g be a randomly initialized feed-forward $2\\ell$ -layer neural network, in which each weight is drawn from a Uniform $[-1,1]$ distribution, of the following form: ", "page_idx": 6}, {"type": "equation", "text": "$$\ng(\\mathbf{x})=\\mathbf{M}_{2l}\\sigma(\\mathbf{M}_{2l-1}\\ldots\\sigma(\\mathbf{M}_{1}\\mathbf{x})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Let $\\gamma^{\\prime}=\\gamma^{\\prime}(\\varepsilon)\\in(0,1),$ , $\\mathbf{M}_{2i}\\in\\mathbb{R}^{d_{i}\\times2d_{i-1}n_{i}^{*}}$ and $\\mathbf{M}_{2i-1}\\in\\mathbb{R}^{2d_{i-1}n_{i}^{*}\\times d_{i-1}}$ , with $n_{i}^{*}$ satisfying ", "page_idx": 6}, {"type": "equation", "text": "$$\nn_{i}^{*}=c_{a m p}\\frac{\\log_{2}^{2}\\left(\\frac{2\\ell d_{i-1}d_{i}\\gamma^{\\prime}n_{i}^{*}}{\\varepsilon}\\right)}{H_{2}(\\gamma^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With probability at least $1-\\varepsilon,$ , for every $f\\in\\mathcal F$ , where $\\mathcal{F}$ is defined as in Eq. 19, $g$ can be pruned to obtain a subnetwork of sparsity at least $\\alpha=1-\\gamma$ that approximates $f$ up to an error $\\varepsilon$ , having defined $\\gamma=\\rho\\gamma^{\\prime}$ . ", "page_idx": 6}, {"type": "text", "text": "Proof Idea. The theorem follows from a slight variation of the same approach detailed in [24], in which we use our Corollary 1 instead of [17][Corollary 2.5] when pruning $g$ , allowing us to have control over the size of the pruned network. A detailed proof is provided in Appendix G. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "To illustrate a simple example of how Theorem 3 addresses the main question asked in the introduction, consider the case where we want to approximate a target network with $m_{t}$ parameters and $\\ell$ layers, each of width $d$ (so $\\rho\\,=\\,1$ and $\\gamma^{\\prime}\\,=\\,\\gamma$ ), by pruning an overparameterized network to achieve a desired sparsity level of $\\alpha=1-\\gamma$ . The condition expressed by Equation 20 in Theorem 3 ", "page_idx": 6}, {"type": "image", "img_path": "aBMESB1Ajx/tmp/85330b6acc2322b4cbe1bdad8fa0b3d7736daf7944a54007b814b52d0cc61395.jpg", "img_caption": ["Density y of winning ticket "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 1: A qualitative plot showing the relationship between the density $\\gamma$ of a winning ticket and the overparameterization required by Theorem 3 for a target network with $m_{t}$ parameters. Earlier results from Pensia et al. [24] and Malach et al. [20] are shown for comparison. ", "page_idx": 7}, {"type": "text", "text": "comes from the use of Corollary 1 when pruning network $g$ , as shown in the proof. If, instead of Corollary 1, we use its simplified variant Corollary 2, it is easy to observe that Equation 20 would become ", "page_idx": 7}, {"type": "equation", "text": "$$\nn_{i}^{*}=c_{\\mathrm{amp}}\\frac{\\log_{2}^{2}\\left(\\frac{2\\ell d_{i-1}d_{i}}{\\varepsilon}\\right)}{H_{2}(\\gamma^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using this condition, Theorem 3 then tells us that we need to prune a randomly initialized network with twice as many layers and a number of parameters of the order of d2 loHg2( \u03b3\u2113d\u03b5)2 . ", "page_idx": 7}, {"type": "text", "text": "We will now clarify the connection between Theorem 3 and the earlier results from [20] and [24].   \nFigure 1 provides a quick visual comparison. ", "page_idx": 7}, {"type": "text", "text": "Malach et al.[20]. When all layers have the same width $d$ , [20] showed that any target network with $l$ layers and a total of $m_{t}\\,\\stackrel{.}{=}\\,d^{2}l$ parameters can be $\\varepsilon$ -approximated by pruning a randomly initialized network with $2l$ layers. The overparameterization of this network, relative to the target network, is $\\begin{array}{r}{O\\left(\\frac{m_{t}^{2}}{\\varepsilon^{2}}\\log_{2}\\frac{m_{t}}{\\varepsilon}\\right)=\\tilde{O}\\left(\\frac{m_{t}^{2}}{\\varepsilon^{2}}\\right)}\\end{array}$ . More specifically, the winning ticket found after pruning has a parameter count of the same order as the target network, resulting in a density of $\\begin{array}{r}{\\gamma=\\tilde{O}\\left(\\frac{\\varepsilon^{2}}{m_{t}^{2}}\\right)}\\end{array}$ . Notably, this density $\\gamma$ is the inverse of the overparameterization, as the size of the winning ticket matches that of the target network. ", "page_idx": 7}, {"type": "text", "text": "Next, we show that Theorem 3 also yields a density that is polynomial in $\\frac{\\varepsilon}{m_{t}}$ , when using an overparametrization of $\\Theta\\left({\\frac{m_{t}^{2}}{\\varepsilon^{2}}}\\right)$ . Let $\\begin{array}{r}{z=\\left(\\frac{m_{t}}{\\varepsilon}\\right)}\\end{array}$ , and note that $\\gamma^{\\prime}=\\gamma$ in Theorem 3, since all layers have the same width. As $n_{i}^{*}$ in Theorem 3 represents the overparametrization with respect to the target network, let us set $n_{i}^{*}=c z^{2}$ , for some constant $c$ . Equation 20 then becomes ", "page_idx": 7}, {"type": "equation", "text": "$$\nc z^{2}\\geq c_{\\mathrm{amp}}\\frac{\\log_{2}^{2}(c z^{3}\\gamma)}{H(\\gamma)}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We show that the inequality cz2 \u2265campl\u03b3o lgo2g(2c(z1/\u03b3\u03b3)) holds for some big enough constant $c$ when setting $\\begin{array}{r}{\\gamma=\\frac{\\varepsilon}{m_{t}}=\\frac{1}{z}}\\end{array}$ , which implies that Equation 22 is also satisfied. We get $c z\\ge c_{\\mathrm{amp}}\\frac{\\log_{2}^{2}(c z^{2})}{\\log_{2}(z)}$ , which is satisfied for a big enough constant (see Appendix I). Overall, when using an overparametrization $\\Theta\\left({\\frac{m_{t}^{2}}{\\varepsilon^{2}}}\\right)$ , we find a winning ticket with density $\\frac{\\varepsilon}{m_{t}}$ , as shown in Figure 1. ", "page_idx": 7}, {"type": "text", "text": "Pensia et al.[24]. For simplicity, let us still consider target networks where all layers have the same width $d$ , and we apply Theorem 3 using the simplified condition from Equation 21. When $\\gamma m=\\Theta(m)$ , i.e. the density $\\gamma$ is a constant as in [24] (see Appendix A), the entropy term $H_{2}(\\gamma^{\\prime})$ in the right-side of Equation 21 also becomes a constant. In this setting, we indeed recover the result shown in [24][Theorem 1], up to a logarithmic factor, as shown in Figure 1. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Quite similarly to Theorem 3, the next result essentially generalizes [9] up to a factor $\\log_{2}{\\frac{1}{\\varepsilon}}$ . The theorem is stated with the understanding that for $G$ -equivariant networks, in order to preserve $G$ - equivariance, pruning is best done not with respect to the parameters expressing the network in the canonical basis (i.e. directly on the weights of the network), but with respect to the equivariant parameters, that is those coefficients expressing the linear layers of the network as a linear combination of the elements of the corresponding equivariant basis [9]. For simplicity, due to the technical set-up, we assume all feature spaces being $\\bar{\\mathbb{F}}\\stackrel{=}{=}(\\mathbb{R}^{d},\\sigma)$ , with $\\sigma$ the linear representation of the group $G$ , and the same number $n$ of such feature spaces being stacked in each layer. A $G$ -equivariant linear map from the ith feature space to the $i+1\\mathrm{st}$ can be decomposed in a corresponding equivariant basis denoted $B_{i\\to i+1}=B$ . Since all feature spaces are the same, we omit the layers\u2019 indices. When stacking $n$ feature spaces in the input and output of the $i$ th layer, the full equivariant basis is denoted $k_{n\\rightarrow n}$ , and finally the basis of the $G$ -equivariant maps from $\\mathbb{F}^{n}$ to $\\mathbb{F}^{n}$ can be written as the Kronecker product $k_{n\\rightarrow n}\\otimes B$ . For any basis $\\bar{B}\\,\\,\\bar{=}\\,\\,\\lbrace b_{1},\\ldots,\\bar{b_{p}}\\rbrace$ , we denote its cardinality $p=|\\boldsymbol{\\mathcal{B}}|$ and define $\\begin{array}{r}{\\|\\mathcal{B}\\|=\\operatorname*{max}_{\\|\\beta\\|_{\\infty}}\\|\\sum_{k=1}^{p}\\beta_{k}b_{k}\\|}\\end{array}$ , with $\\|\\cdot\\|$ in the r.h.s. being the operator norm inherited from the $\\ell_{p}$ norm. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4 (SSLTH for Equivariant Networks). Let h be a random $2\\ell$ -layer $G$ -equivariant network where all equivariant parameters are drawn from a Uniform $[-1,1]$ distribution, every odd layer expressed in the associated equivariant basis $k_{\\tilde{n}\\to n}\\otimes B$ and every even layer expressed in the associated equivariant basis $k_{n\\rightarrow\\tilde{n}}\\otimes B$ . Let $\\gamma=\\gamma(\\varepsilon)\\in(0,1)$ , with $\\tilde{n}$ satisfying ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tilde{n}=c_{a m p}\\frac{\\log_{2}^{2}\\left(\\frac{2\\ell n^{2}\\operatorname*{max}\\{|\\mathcal{B}|,\\|\\mathcal{B}\\|\\}\\gamma\\tilde{n}}{\\varepsilon}\\right)}{H_{2}\\left(\\gamma\\right)}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "With probability at least $1\\mathrm{~-~}\\varepsilon,$ , for every $\\ell$ -layer $G$ -equivariant neural network $f$ , with all layers expressed in the associated equivariant basis $k_{n\\rightarrow n}\\otimes B$ , $h$ can be pruned to obtain a $G$ -equivariant subnetwork of sparsity at least $\\alpha=1-\\gamma$ that approximates $f$ up to an error $\\varepsilon$ . ", "page_idx": 8}, {"type": "text", "text": "The proof, which we omit, is analogous to that of Theorem 3, since [9][Theorem 1] exploits the exact same pruning strategy of [24], except for the fact that it is applied not to the original parameters of the equivariant network, but to the network expressed in terms of its equivariant basis (the sparsity $\\alpha$ is here also intended with respect to the equivariant parameters count). This allows the construction to apply without losing the property of equivariance in the pruned approximating subnetwork obtained. The crucial step is when Corollary 1 is applied in [9][Lemma 1], instead of [17][Corollary 2.5]. This is done in parallel, multiple times, across non-overlapping coefficients of the equivariant basis. Thanks to the careful preprocessing devised by the authors, this preserves equivariance and at the same time ensures that each application of Corollary 1 is independent of the others. ", "page_idx": 8}, {"type": "text", "text": "To conclude the section, we mention that Theorem 4 applies in particular to vanilla CNNs, which are a special case of equivariant neural networks where the group is $G\\;=\\;(\\mathbb{Z}^{2},+)$ , recovering previous SLTH results on CNN [8, 3]. Furthermore, we remark that Theorem 3 can be revisited through the improvement upon the $2\\ell$ -depth overparameterization devised in [4], i.e., it is possible to provide sparsity guarantees also for overparameterizations requiring depth $\\ell\\!+\\!1$ only. The analysis is more technical and we omit it, but the ideas are analogous to what shown in [4]. An analogous improvement is suggested as future work in [9]. ", "page_idx": 8}, {"type": "text", "text": "4.1 Lower bound on the required overparameterization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now adapt the lower bound of [24] in order to almost match the required overparameterization of our Theorem 3, considering the simple scenario in which we want to approxima\u221ate the family $\\mathcal{F}$ of all linear networks with weights forming a matrix having spectral norm less than $\\sqrt{k}$ ; more formally ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\mathcal{F}}:=\\{h_{W}:W\\in\\mathbb{R}^{d\\times d},\\|W\\|\\leq{\\sqrt{k}}\\},\\quad{\\mathrm{~where~}}\\quad h_{W}(x)=W x.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The formal claim states that, if a network with $n$ parameters can approximate every $h_{W}\\in{\\mathcal{F}}$ with probability at least $^{1/2}$ (after it is pruned down to $k$ parameters), then the hypothesis of Theorem 2 in Eq. 1 must hold.3 ", "page_idx": 8}, {"type": "text", "text": "Theorem 5. Let $n,k\\in\\mathbb{N},$ , with $1\\leq k\\leq\\lambda n$ , having set $\\lambda=1-1/2\\pi\\approx0.84$ . Consider a neural network $g$ with $n$ parameters, and let $\\mathcal{G}_{k}$ be the set of neural networks that can be formed by pruning $g$ down to $k$ parameters. Let $\\mathcal{F}$ be as defined in Eq. 23. If it holds that, for some $\\varepsilon<1/16$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\forall h_{W}\\in\\mathcal{F},\\mathbb{P}\\left(\\exists g^{\\prime}\\in\\mathcal{G}_{k}:\\operatorname*{max}_{\\mathbf{x}:\\|x\\|\\leq1}\\|h_{W}(x)-g^{\\prime}(x)\\|<\\varepsilon\\right)\\geq\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "then it holds that ", "page_idx": 9}, {"type": "equation", "text": "$$\nn\\geq\\frac{d^{2}}{2}\\frac{\\log_{2}\\frac{k}{\\varepsilon}}{H_{2}\\left(\\frac{k}{n}\\right)}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The theorem follows by adapting the packing argument of [24]. A detailed proof is provided in Appendix $\\mathrm{H}$ . ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have extended previous results on the Strong Lottery Ticket Hypothesis by quantifying the required overparameterization as a function of the sparsity of the subnetworks. Central to our results is a proof of the Random Fixed-size Subset Sum (RFSS) Problem, a refinement of the seminal Random Subset Sum (RSS) Problem in which the subsets have a required fixed size. ", "page_idx": 9}, {"type": "text", "text": "A challenging open problem is to extend our analysis of RFSS to the multidimensional case, in which the random samples and targets are vectors in $\\mathbb{R}^{d}$ . Previous extension of RSS to the Multidimensional RSS have indeed allowed to prove structured-pruning version of the SLTH [8]. A Multidimensional RFSS result would then allow to quantify, in the structured pruning case, the dependency of the overparameterization w.r.t. the sparsity of the (structured) subnetworks. ", "page_idx": 9}, {"type": "text", "text": "Another future direction is to refine our analysis of the RFSS in Theorem 2 in order to improve the probability of success to $1\\,-\\,\\varepsilon$ rather than constant, thus allowing to avoid shaving off the extra factor $\\log_{2}(1/\\varepsilon)$ in our corollaries w.r.t. our lower bound, which is due to the amplification done in Corollary 1 to get to probability $1-\\varepsilon$ . ", "page_idx": 9}, {"type": "text", "text": "Finally, an important future direction is to improve training-by-pruning methods such as [29, 26, 11, 10, 23] or to develop new ones, in order to allow to efficiently find strong lottery tickets of a desired sparsity, thus empirically validating our theoretical predictions. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations Similar to all the research conducted on the LTH and the SLTH, this work only proves the existence of lottery tickets. To this date, it is not clear if these subnetworks can be found reliably (no formal proof exists) in an efficient manner - however, empirical evidence suggests that efficient algorithms exist (e.g., [29, 26]). ", "page_idx": 9}, {"type": "text", "text": "Impact The contribution of this work is primarily theoretical and not confined to a specific domain. Its potential societal impact would, therefore, be closely tied to the particular scenarios to which it is applied. It could be interesting to compare the environmental impact of finding lottery tickets inside overparameterized networks. We also believe that our work has the potential to have a strong environmental impact as sparse NNs have massively reduced inference costs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research is supported by the EPSRC grant EP/W005573/1, and by the France 2030 program, managed by the French National Research Agency under grant agreements No. ANR-23-PECL0003 and and ANR-22-PEFT-0002. It was also funded in part by the European Network of Excellence dAIEDGE under Grant Agreement Nr. 101120726, by SmartNet and LearnNet, and by the French government National Research Agency (ANR) through the UCA JEDI (ANR-15-IDEX-01), EUR DS4H (ANR-17-EURE-004), and the 3IA C\u02c6ote d\u2019Azur Investments in the Future project with the reference number ANR-19-P3IA-0002. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Giampietro Allasia. \u201cApproximation of the normal distribution functions by means of a spline function\u201d. In: Statistica 41.2 (1981), pp. 325\u2013332. [2] Sander Borst et al. \u201cOn the Integrality Gap of Binary Integer Programs with Gaussian Data\u201d. In: Mathematical Programming 197.2 (Feb. 2023), pp. 1221\u20131263. ISSN: 1436-4646. DOI: 10.1007/s10107-022-01828-1. (Visited on 06/05/2023). [3] Rebekka Burkholz. \u201cConvolutional and Residual Networks Provably Contain Lottery Tickets\u201d. In: Proceedings of the 39th International Conference on Machine Learning. Baltimore: PMLR, July 2022, pp. 2414\u20132433. (Visited on 02/27/2023). [4] Rebekka Burkholz. \u201cMost Activation Functions Can Win the Lottery Without Excessive Depth\u201d. In: Thirty-Sixth Conference on Neural Information Processing Systems. Dec. 2022. (Visited on 02/27/2023). [5] Rebekka Burkholz et al. \u201cOn the Existence of Universal Lottery Tickets\u201d. In: International Conference on Learning Representations. virtual, Apr. 2022. (Visited on 02/27/2023). [6] Arthur da Cunha, Francesco D\u2019Amore, and Emanuele Natale. \u201cPolynomially OverParameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets\u201d. In: Thirty-Seventh Conference on Neural Information Processing Systems. Nov. 2023. (Visited on 04/23/2024). [7] Arthur Carvalho Walraven da Cunha et al. \u201cRevisiting the Random Subset Sum Problem\u201d. In: 31st Annual European Symposium on Algorithms, ESA 2023, September 4-6, 2023, Amsterdam, The Netherlands. Ed. by Inge Li G\u00f8rtz et al. Vol. 274. LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum fu\u00a8r Informatik, 2023, 37:1\u201337:11. DOI: 10.4230/LIPICS.ESA.2023.37. URL: https://doi.org/10.4230/LIPIcs.ESA.2023.37. [8] Arthur da Cunha, Emanuele Natale, and Laurent Viennot. \u201cProving the Strong Lottery Ticket Hypothesis for Convolutional Neural Networks\u201d. In: ICLR 2022 - 10th International Conference on Learning Representations. Virtual, France, Apr. 2022. (Visited on 08/04/2022).   \n[9] Damien Ferbach et al. \u201cA General Framework For Proving The Equivariant Strong Lottery Ticket Hypothesis\u201d. In: The Eleventh International Conference on Learning Representations. Sept. 2022. (Visited on 03/04/2024).   \n[10] Jonas Fischer and Rebekka Burkholz. \u201cPlant \u2019n\u2019 Seek: Can You Find the Winning Ticket?\u201d In: International Conference on Learning Representations. Apr. 2022. (Visited on 02/27/2023).   \n[11] Jonas Fischer, Advait Gadhikar, and Rebekka Burkholz. Lottery Tickets with Nonzero Biases. June 2022. arXiv: 2110.11150 [cs]. (Visited on 05/13/2024).   \n[12] Jonathan Frankle and Michael Carbin. \u201cThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\u201d. In: International Conference on Learning Representations. Sept. 2018. (Visited on 10/20/2023).   \n[13] Advait Harshal Gadhikar, Sohom Mukherjee, and Rebekka Burkholz. \u201cWhy Random Pruning Is All We Need to Start Sparse\u201d. In: International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA. Ed. by Andreas Krause et al. Vol. 202. Proceedings of Machine Learning Research. PMLR, 2023, pp. 10542\u201310570. URL: https: //proceedings.mlr.press/v202/gadhikar23a.html.   \n[14] Berivan Isik et al. \u201cAdaptive Compression in Federated Learning via Side Information\u201d. In: Proceedings of The 27th International Conference on Artificial Intelligence and Statistics. PMLR, Apr. 2024, pp. 487\u2013495. (Visited on 05/16/2024).   \n[15] Berivan Isik et al. \u201cSparse Random Networks for Communication-Efficient Federated Learning\u201d. In: The Eleventh International Conference on Learning Representations. Sept. 2022. (Visited on 01/18/2024).   \n[16] Bohan Liu et al. A Survey of Lottery Ticket Hypothesis. Mar. 2024. DOI: 10.48550/arXiv. 2403.04861. arXiv: 2403.04861 [cs]. (Visited on 05/16/2024).   \n[17] George S. Lueker. \u201cExponantially small bounds on the expected optimum of the partition and subset sum problem\u201d. In: Ramdom Structures and Algorithms 12 (1998), pp. 51\u201362.   \n[18] George S. Lueker. \u201cOn the average difference between the solutions to linear and integer knapsack problems\u201d. In: Applied Probability - Computer Science, The Interface. Vol. 1. Birkha\u00a8user, 1982.   \n[19] Florence Jessie MacWilliams and Neil James Alexander Sloane. The Theory of ErrorCorrecting Codes. Vol. 16. North-Holland Mathematical Library. North-Holland Publishing Company, 1977.   \n[20] Eran Malach et al. \u201cProving the Lottery Ticket Hypothesis: Pruning Is All You Need\u201d. In: Proceedings of the 37th International Conference on Machine Learning. ICML\u201920. JMLR.org, July 2020, pp. 6682\u20136691. (Visited on 03/26/2023).   \n[21] James E Marengo, David L Farnsworth, Lucas Stefanic, et al. \u201cA geometric derivation of the Irwin-Hall distribution\u201d. In: International Journal of Mathematics and Mathematical Sciences 2017 (2017).   \n[22] Laurent Orseau, Marcus Hutter, and Omar Rivasplata. \u201cLogarithmic Pruning Is All You Need\u201d. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS\u201920. Red Hook, NY, USA: Curran Associates Inc., Dec. 2020, pp. 2925\u2013 2934. ISBN: 978-1-71382-954-6. (Visited on 03/26/2023).   \n[23] Hikari Otsuka et al. Partial Search in a Frozen Network Is Enough to Find a Strong Lottery Ticket. Feb. 2024. DOI: 10.48550/arXiv.2402.14029. arXiv: 2402.14029 [cs, stat]. (Visited on 05/15/2024).   \n[24] Ankit Pensia et al. \u201cOptimal lottery tickets via SUBSETSUM: logarithmic overparameterization is sufficient\u201d. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS \u201920. , Vancouver, BC, Canada, Curran Associates Inc., 2020. ISBN: 9781713829546.   \n[25] Valentin V. Petrov. Sums of Independent Random Variables. Ergebnisse der Mathematik und ihrer Grenzgebiete. 2. Folge. Springer Berlin, Heidelberg, 1975, 348 pages. DOI: https: //doi.org/10.1007/978-3-642-65809-9.   \n[26] Vivek Ramanujan et al. \u201cWhat\u2019s Hidden in a Randomly Weighted Neural Network?\u201d In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2020, pp. 11890\u201311899. DOI: 10.1109/CVPR42600.2020.01191.   \n[27] N. Shakhaidarova. \u201cUniform local and global theorems for densities\u201d. In: Izv. Akad. Nauk UzSSR Ser. Fiz-Mat. Nauk 5 (1966), pp. 90\u201391.   \n[28] Irina Shevtsova. On the absolute constants in the Berry Esseen type inequalities for identically distributed summands. 2011. arXiv: 1111.6554 [math.PR].   \n[29] Hattie Zhou et al. \u201cDeconstructing Lottery Tickets: Zeros, Signs, and the Supermask\u201d. In: Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019 (NIPS 2019). 2019, pp. 3592\u20133602. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Lower Bound on the Ticket Size in [24] ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The claim is a direct consequence of the proof of [24, Theorem 2] (Appendix B). There, in Step 3, it is shown that ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\mathcal{G}|\\geq\\frac{1}{2}\\left(\\frac{1}{2\\varepsilon}\\right)^{d^{2}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathcal{G}$ is the set of subnetworks that can be formed. Let $m$ be the number of parameters of the original network. If we consider subnetworks of size at most $\\gamma m$ $(0\\leq\\gamma\\leq1)$ ), we have4 ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\mathcal{G}|\\leq\\sum_{i=1}^{\\gamma m}\\binom{m}{\\gamma m}\\leq2^{\\gamma m\\log_{2}(\\frac{m}{\\gamma m}e)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which combined with the previous inequality implies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\gamma m\\log_{2}\\left(\\frac{e}{\\gamma}\\right)\\geq d^{2}\\log_{2}\\left(\\frac{1}{2\\varepsilon}\\right)-1\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "If we have an overparameterized network of size $\\begin{array}{r}{m=\\mathcal{O}(d^{2}\\log_{2}\\left(\\frac{1}{2\\varepsilon}\\right))}\\end{array}$ , as in [24], we need $\\gamma m=$ $\\Theta(m)$ for the last inequality to be satisfied (note that $\\begin{array}{r}{\\log_{2}\\left(\\frac{e}{\\gamma}\\right)\\leq1}\\end{array}$ , as $0\\leq\\gamma\\leq1\\rfloor$ ). ", "page_idx": 12}, {"type": "text", "text": "B Visualizations ", "text_level": 1, "page_idx": 12}, {"type": "image", "img_path": "aBMESB1Ajx/tmp/c08804e6f908f3f8f2071ab0627a9d64e7658a41e155c9c22a4e2b6e94943670.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 2: Simplified representation of the procedure for finding Lottery Tickets (LTH). A large random neural network (step 1) is trained by iterative pruning with rewind: when the loss reaches a local minimum (step 2), some weights with smallest absolute value are pruned (step 3) and the value of the remaining edges is then reset to that of the initialization (step 4); finally, training is resumed and the final network is obtained (step 5). Remarkably, the sparser subnetwork is consistently able to reach a loss not larger than that right after pruning. ", "page_idx": 12}, {"type": "image", "img_path": "aBMESB1Ajx/tmp/d99de4b04c7b6adce02d27e6686280c55f87e3c7aeae661e8c545f43cef27ec0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 3: Simplified representation of the procedure for finding Strongly Lottery Tickets (SLTH) / Training by pruning. Previous work has shown that it is possible to sparsify large random neural network in order to obtain subnetworks that achieve good performance for a task under consideration, motivating the Strong Lottery Ticket Hypothesis. No training is required. ", "page_idx": 12}, {"type": "text", "text": "C Proof of Uniform[\u22121, 1] being Sum-Bounded ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section we provide a detailed proof of Lemma 1, which states that the uniform distribution in $[-1,1]$ is sum-bounded, as stated in Definition 1. We remark that, while the proof is written for uniform random variables, it should be possible to extend it to a family of densities which are unimodal, with bounded variance, and bounded third moment. ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Note first that the distribution of the sum of $n$ i.i.d. variables in $[0,1]$ is known as the Irwin\u2013Hall distribution $I_{n}$ .5 We will use that $\\begin{array}{r}{\\mathrm{Var}[I_{n}]=\\frac{n}{12}}\\end{array}$ , where $\\mathrm{Var}[X]$ denotes the variance of the random variable . ", "page_idx": 13}, {"type": "text", "text": "For $n\\geq2$ , $f(x,n)$ can be defined as the convolution of $f(x)=f(x,1)$ and $f(x,n-1)$ , i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(x,n)=\\int_{-\\infty}^{+\\infty}f(x-\\tau,n-1)f(\\tau)d\\tau.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is straightforward to show, by induction and an elementary substitution in the integral above, which is relied upon in the inductive step, that $f(x,n)$ is symmetric about 0, that is $f(x,{\\bar{n}})=f(-x,n)$ . ", "page_idx": 13}, {"type": "text", "text": "Let us now prove by induction that $f(x,n)$ is nondecreasing on the interval $[-n,0]$ and nonincreasing over $[0,n]$ (for simplicity, since it vanishes outside $[-n,n]$ , we can consider directly the negative half and positive half of the real line, respectively, in the argument that follows). ", "page_idx": 13}, {"type": "text", "text": "The claims hold trivially for $f(x)$ ; also note that ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(\\tau)=\\left\\{\\begin{array}{l l}{\\frac{1}{2}\\qquad}&{\\mathrm{if}\\quad-1\\leq\\tau\\leq1}\\\\ {0\\qquad}&{\\mathrm{otherwise}}\\end{array}\\right.\\qquad\\Longrightarrow\\qquad f(x,n)=\\frac{1}{2}\\int_{-1}^{+1}f(x-\\tau,n-1)d\\tau.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If $x\\leq x^{\\prime}\\leq-1$ . Since $x-\\tau\\leq x^{\\prime}-\\tau\\leq0.$ , by inductive hypothesis we have that $f(x-\\tau,n-1)\\leq$ $f(x^{\\prime}-\\tau,n-1)$ over the whole interval $\\tau\\in[-1,1]$ . Taking integrals yields $f(x,n)\\leq f(x^{\\prime},n)$ . Now, consider the case when $x\\ \\leq\\ -1\\ \\leq\\ x^{\\prime}\\ \\leq\\ 0$ . If $x+1\\,\\leq\\,-x^{\\prime}-1.$ , $x\\textrm{-}\\tau\\;\\leq\\;x\\,+\\,1\\;\\leq$ $-x^{\\prime}-1\\,\\le\\,-x^{\\prime}+\\tau\\,\\le\\,-x+\\tau$ . By the symmetry about the origin, the inductive hypothesis is $f(x-\\tau,n-1)\\,=\\,f(-x+\\tau,n-1)\\,\\leq\\,f(-x^{\\prime}+\\tau,n-1)\\,=\\,f(x^{\\prime}-\\tau,n-1)$ over the whole interval $\\tau\\in[-1,1]$ , since $-1\\leq-x^{\\prime}+\\tau\\leq-x+\\tau$ . Taking integrals yields $f({\\dot{x}},n)\\leq f(x^{\\prime},n)$ . Otherwise, there exists $\\tau_{0}$ such that $x-\\tau_{0}=-x^{\\prime}-1$ , $x-\\tau>-x^{\\prime}-1$ for all $\\tau\\in[-1,\\tau_{0})$ and $x-\\tau<-x^{\\prime}-1$ for all $\\tau\\in(\\tau_{0},1]$ . By symmetry, using $-x=x^{\\prime}+1-\\tau_{0}$ , $f(x-\\tau,n-1)=$ $f(-x+\\tau,n-1)=f(x^{\\prime}+1+\\tau-\\tau_{0},n-1)$ . Thus, for all $\\tau\\in[-1,\\tau_{0}]$ , via the change of variable $\\sigma=-(1+\\tau-\\tau_{0})$ in the middle integral below, we obtain that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{-1}^{\\tau_{0}}f(x-\\tau,n-1)d\\tau=\\int_{-1}^{\\tau_{0}}f(x^{\\prime}+1+\\tau-\\tau_{0},n-1)d\\tau=\\int_{-1}^{\\tau_{0}}f(x^{\\prime}-\\sigma,n-1)d\\sigma.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For all $\\tau\\,\\in\\,(\\tau_{0},1]$ , $x-\\tau<-x^{\\prime}-1\\leq-x^{\\prime}+\\tau\\leq-x+\\tau$ , by symmetry about the origin we have that $f(x-\\tau,n-1)\\leq f(x^{\\prime}-\\tau,n-1)$ by the inductive hypothesis with the same reasoning of the case $x+1\\leq-x^{\\prime}-1$ . Taking integrals over the range $[\\tau_{0},1]$ for each term of the inductive hypothesis yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\tau_{0}}^{1}f(x-\\tau,n-1)d\\tau\\leq\\int_{\\tau_{0}}^{1}f(x^{\\prime}-\\tau,n-1)d\\tau\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Eqs. 25 and 26 imply that $f(x,n)\\leq f(x^{\\prime}n)$ . ", "page_idx": 13}, {"type": "text", "text": "Trivially, if $-1\\leq x\\leq x^{\\prime}\\leq0$ , analogous ideas are put in place as for the previous case, therefore we omit the details. We have thus shown the nondecreasing monotonicity of $f(x,n)$ on the negative half of the real line. By the symmetry of $f(x,n)$ about the origin, on the positive half of the real line the nondecreasing monotonicity turns into nonincreasing monotonicity, and the proof is complete. ", "page_idx": 13}, {"type": "text", "text": "Lower bound (first inequality in Eq. 2). The variance of $\\Sigma_{[n]}^{\\mathcal{U}_{n}}$ is $n/3$ since $\\Sigma_{[n]}^{\\mathcal{U}_{n}}=2(I_{n}(n)-n/2)$ and $\\mathrm{Var}[I_{n}(n)]\\,=\\,n/12$ . We define $\\begin{array}{r}{Z_{n}^{u}=\\frac{\\Sigma_{[n]}^{u_{n}}}{\\sqrt{n/3}}}\\end{array}$ and we note with $F_{n}$ its cumulative distribution function. $Z_{n}^{u}$ has expectation 0 and standard deviation 1. Consider the probability ", "page_idx": 13}, {"type": "equation", "text": "$$\nP_{L}(n)=\\operatorname*{Pr}({\\sqrt{n}}\\leq\\Sigma_{[n]}^{\\mathcal{U}_{n}}\\leq2{\\sqrt{n}})=\\operatorname*{Pr}({\\sqrt{3}}\\leq Z_{n}^{u}\\leq2{\\sqrt{3}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now, we use the following form of Berry\u2013Esseen inequality, discussed in [21][p.2]).6 ", "page_idx": 14}, {"type": "text", "text": "Theorem 6 (Allasia [1]). For all $n\\geq1$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n|F_{n}(z)-\\Phi(z)|\\leq{\\frac{\\sqrt{3}}{20{\\sqrt{n}}}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\Phi(z)$ is the cumulative distribution function of the standard normal distribution. ", "page_idx": 14}, {"type": "text", "text": "Theorem 6 implies ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{L}(n)\\geq\\Phi(2{\\sqrt{3}})-\\Phi({\\sqrt{3}})-2\\cdot{\\frac{\\sqrt{3}}{20{\\sqrt{n}}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $n\\geq18$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Phi(2{\\sqrt{3}})-\\Phi({\\sqrt{3}})-2\\cdot{\\frac{\\sqrt{3}}{20{\\sqrt{n}}}}\\geq\\Phi(2{\\sqrt{3}})-\\Phi({\\sqrt{3}})-2\\cdot{\\frac{\\sqrt{3}}{20{\\sqrt{18}}}}=C_{18}>0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "That is $P_{L}(n)\\geq C_{18}>0$ . When $2\\leq n<18$ , $P_{L}(n)=F_{n}(2\\sqrt{3})-F_{n}(\\sqrt{3})=c_{n}>0$ . We thus have ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{L}(n)\\geq\\operatorname*{min}\\{C_{i},\\;\\mathrm{for}\\;2\\leq i\\leq18\\}=c_{l}^{\\prime}>0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall that $P_{L}(n)=\\mathrm{Pr}(\\sqrt{n}\\leq\\Sigma_{[n]}^{\\mathcal{U}_{n}}\\leq2\\sqrt{n})$ . As the density $f(x,n)$ is decreasing on $\\mathbb{R}^{+}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{L}(n)\\leq f({\\sqrt{n}},n){\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\nf({\\sqrt{n}},n)\\geq{\\frac{P_{L}(n)}{\\sqrt{n}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $P_{L}(n)\\ge c_{l}^{\\prime}$ then for all $n\\geq2$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nf({\\sqrt{n}},n)\\geq{\\frac{c_{l}^{\\prime}}{\\sqrt{n}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $n=1$ ,\u221a the density $\\textstyle f(1,1)={\\frac{1}{2}}$ . So, by setting $c_{l}=\\operatorname*{min}(c_{l}^{\\prime},\\frac{1}{2})$ , we get that, for all $n\\geq1$ , for all $0\\leq x\\leq{\\sqrt{n}}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(x,n)\\geq f({\\sqrt{n}},n)\\geq{\\frac{c_{l}}{\\sqrt{n}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By a symmetric argument, we also have for all $n\\geq1$ , for all $-{\\sqrt{n}}\\leq x\\leq0$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(x,n)\\geq f(-\\sqrt{n},n)\\geq{\\frac{c_{l}}{\\sqrt{n}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Upper bound (second inequality in Eq. 2). Here, we bound the probability distribution function $f(x,n)$ of $\\Sigma_{[n]}^{\\mathcal{U}_{n}}\\,=\\,\\sqrt{{n}/{3}}Z_{n}$ , where we recall that $\\begin{array}{r}{Z_{n}^{u}\\,=\\,\\frac{\\Sigma_{[n]}^{\\mathcal{U}_{n}}}{\\sqrt{n/3}}}\\end{array}$ . Denoting $f_{z}$ the probability distribution function of $Z_{n}^{u}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{z}(x,n)=f\\left({\\sqrt{\\frac{n}{3}}}x,n\\right){\\sqrt{\\frac{n}{3}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We use the following local limit theorem, discussed in [25][p.214]. ", "page_idx": 14}, {"type": "text", "text": "Theorem 7 (Sahaidarova [27]). Let $\\{X_{n}\\}$ be a sequence of independent random variables with a common density $p(x)$ , such that $E[|\\dot{X_{1}}|^{3}]<\\infty,\\,E[X_{1}]=0,\\,E[X_{1}^{2}]=1$ and $\\operatorname{sup}p(x)\\leq C$ . Let $p_{n}(x)$ be the density of the random variable $\\textstyle{\\frac{1}{\\sqrt{n}}}\\sum_{j=1}^{n}{\\dot{X}}_{j}$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x}|p_{n}(x)-\\phi(x)|\\leq\\frac{A\\beta_{3}}{\\sqrt{n}}\\operatorname*{max}(1,C^{3}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\phi$ is the probability distribution function of a standard gaussian, $A$ is an absolute constant, and $\\beta_{3}=E[|\\bar{X_{1}}|^{3}]$ . ", "page_idx": 14}, {"type": "text", "text": "The theorem\u221a ca\u221an be applied to a uniform continuous distribution with density $\\begin{array}{r}{p^{u}(x)\\,=\\,\\frac{1}{2\\sqrt{3}}}\\end{array}$ in the interval $[-{\\sqrt{3}},{\\sqrt{3}}]$ , which has mean 0 and variance 1. We thus get, for every $x\\in\\mathbb R$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{z}(x,n)=p_{n}^{u}(x)\\leq\\phi(0)+\\frac{A\\beta_{3}}{\\sqrt{n}}=\\frac{1}{2\\pi}+\\frac{A\\beta_{3}}{\\sqrt{n}}\\leq\\frac{1}{2\\pi}+A\\frac{3\\sqrt{3}}{4}=c_{u}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In conclusion, setting $c_{u}=\\sqrt{3}c_{u}^{\\prime}$ , for every $x\\in\\mathbb R$ it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(x,n)=\\sqrt{\\frac{3}{n}}f_{z}\\left(\\sqrt{\\frac{3}{n}}x,n\\right)\\leq\\frac{\\sqrt{3}c_{u}^{\\prime}}{\\sqrt{n}}=\\frac{c_{u}}{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D Proof of Corollary 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Corollary $^{\\,I}$ . As anticipated, we proceed in three steps. ", "page_idx": 15}, {"type": "text", "text": "Step 1: Hoeffding bound. We start by showing, following the idea at the base of [17, Corollary 3.3], that if $n^{\\prime}$ is large enough, a standard Hoeffding bound ensures that with high probability a constant fraction of the sample follows a Uniform $[-1,1]$ distribution. Since we assumed that every $X_{i}$ is a mixture of a Uniform $[-1,1]$ distribution with probability $p$ , and another distribution with density $g$ (given by the factors $G_{i}$ ), we can rewrite $X_{i}=B_{i}\\cdot U_{i}+(1-B_{i})\\cdot G_{i}$ , with $U_{i}$ being the uniform random variable, $G_{i}$ being the random variable with density $g$ , $B_{i}$ being independent Bernoulli random variables with probability $p$ . ", "page_idx": 15}, {"type": "text", "text": "Fix $\\alpha\\,=\\,\\alpha(p)\\,\\neq\\,p$ , and assume, for now, that $n^{\\prime}$ satisfies Eq. 1, and therefore, since $\\varepsilon\\ <\\ ^{1}\\!/\\!2$ , choosing $c_{\\mathrm{hyp}}=\\bar{c}_{\\mathrm{hyp}}(p)\\geq(\\alpha-p)^{-2}$ , ensures that, defining $\\varepsilon^{\\prime}=\\varepsilon/2$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nn^{\\prime}\\geq c_{\\mathrm{hyp}}\\log_{2}\\frac{1}{\\varepsilon}\\geq\\frac{1}{2(\\alpha-p)^{2}}\\ln\\frac{1}{\\varepsilon^{\\prime}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and therefore ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\sum_{i}^{n^{\\prime}}B_{i}\\leq\\alpha n^{\\prime}\\right)\\leq e^{-2(\\alpha-p)^{2}n^{\\prime}}\\leq e^{-\\ln{\\frac{1}{\\varepsilon^{\\prime}}}}=\\varepsilon^{\\prime}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\sum_{i}^{n^{\\prime}}B_{i}>\\alpha n^{\\prime}\\right)\\geq1-\\varepsilon^{\\prime},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "that is, with high probability, there is a set of indices $I\\subseteq[n^{\\prime}]$ of size $|I|\\ge\\alpha n^{\\prime}$ , such that for each $i\\in I$ it holds $B_{i}=1$ , i.e. $X_{i}$ is uniformly distributed. ", "page_idx": 15}, {"type": "text", "text": "Step 2: Application of Theorem 2 via rejection-sampling. Lemma 1 ensures that the uniform distribution of the $|I|$ random variables selected in Step $^{\\,l}$ is sum-bounded. Conditionally on the event $\\{\\sum_{i}^{n^{\\prime}}B_{i}>\\alpha n^{\\prime}\\}$ , we can discard all random variables indexed outside $I$ and apply directly Theorem 2 to $\\alpha n^{\\prime}$ of the remaining ones, for any fixed $k$ and $z\\,\\in\\,[-{\\sqrt{k}},{\\sqrt{k}}]$ , since $\\alpha c_{\\mathrm{hyp}}\\,\\geq\\,1$ by construction. This guarantees a success probability of $c_{\\mathrm{thm}}^{\\prime}$ for approximating the given target $z$ ; thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\operatorname*{Pr}\\left(\\exists S_{z}\\subset[n],|S_{z}|=k:|\\Sigma_{S_{z}}-z|<\\varepsilon^{\\prime}\\right)\\geq}}\\\\ {{\\operatorname*{Pr}\\left(\\exists S_{z}\\subset[n],|S_{z}|=k:|\\Sigma_{S_{z}}-z|<\\varepsilon^{\\prime}\\right|\\sum_{i}^{n^{\\prime}}B_{i}>\\alpha n^{\\prime}\\right)\\operatorname*{Pr}\\left(\\sum_{i}^{n^{\\prime}}B_{i}>\\alpha n^{\\prime}\\right)\\geq}}\\\\ {{\\operatorname*{Pr}\\left(\\exists S_{z}\\subset I,|S_{z}|=k:|\\Sigma_{S_{z}}-z|<\\varepsilon^{\\prime}\\right||I|>\\alpha n^{\\prime}\\right)(1-\\varepsilon^{\\prime})\\geq c_{\\mathrm{tim}}^{\\prime}(1-\\varepsilon^{\\prime})\\geq\\frac{3}{4}c_{\\mathrm{tim}}^{\\prime}=c_{\\mathrm{tim}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Step 3: Amplification. Finally, by a standard probability amplification argument and a union bound applied to Theorem 2, by paying an extra factor $\\log_{2}(k/\\bar{\\varepsilon})$ in Eq. 1, the constant $c_{\\mathrm{thm}}$ can be amplified to $1-\\varepsilon$ , and the existence of a suitable subset $S_{z}$ holds simultaneously for all $z\\in$ $\\left[-{\\sqrt{k}},{\\sqrt{k}}\\right]$ . We now give more details on this amplification. ", "page_idx": 16}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\varepsilon^{\\prime}=\\frac{\\varepsilon}{2}}\\end{array}$ , and let $\\begin{array}{r}{c_{\\mathrm{amp}}=c_{\\mathrm{amp}}(p)=8\\frac{c_{\\mathrm{hyp}}}{c_{\\mathrm{thm}}}}\\end{array}$ and $\\begin{array}{r}{r=\\frac{4}{c_{\\mathrm{thm}}}\\ln\\frac{k}{\\varepsilon}}\\end{array}$ . By assumption, ", "page_idx": 16}, {"type": "equation", "text": "$$\nn\\geq c_{\\mathrm{amp}}\\frac{\\log_{2}^{2}\\frac{k}{\\varepsilon}}{H_{2}\\left(\\frac{k}{n}\\right)}\\geq2r c_{\\mathrm{hyp}}\\frac{\\log_{2}\\frac{k}{\\varepsilon}}{H_{2}\\left(\\frac{k}{n}\\right)}\\geq r c_{\\mathrm{hyp}}\\frac{\\log_{2}\\frac{k}{\\varepsilon^{\\prime}}}{H_{2}\\left(\\frac{k}{n}\\right)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality is ensured by $\\varepsilon<\\,^{1/2}$ . By Step 2, we can apply Theorem 2, with $\\varepsilon^{\\prime}$ and $\\begin{array}{r}{n^{\\prime}\\geq c_{\\mathrm{hyp}}\\frac{\\log_{2}\\frac{k}{\\varepsilon^{\\prime}}}{H_{2}\\left(\\frac{k}{n}\\right)}=n^{*}}\\end{array}$ , allowing us to prove that we can $\\varepsilon^{\\prime}$ -approximate any target $z$ with probability at least $c_{\\mathrm{thm}}$ . The probability of failing to approximate some given $z$ is then at most $1\\!-\\!c_{\\mathrm{thm}}$ . From the sample $\\Omega$ of sum-bounded random variables take $r$ subsamples (without replacement) of cardinality $n^{*}$ each, $\\Omega_{1},\\ldots,\\Omega_{r}$ . The probability of failing to approximate some given $z$ with subsetsums from $\\Omega$ is less than that of failing to approximate it with subsets\u221aums\u221a from within every $\\Omega_{i}$ \u2019s, and the latter probability is at most $(1-c_{\\mathrm{thm}})^{r}$ ; thus, for every $z\\in[-\\sqrt{k},\\sqrt{k}]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\nexists S_{z}\\subset[n]\\,,|S_{z}|=k:|\\Sigma_{S_{z}}-z|<\\varepsilon^{\\prime}\\right)\\leq(1-c_{\\mathrm{thm}})^{r}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By an union bound, we also have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(\\forall z\\in\\left[-\\sqrt{k},\\sqrt{k}\\right],\\exists\\mathcal{S}_{z}\\subset[n],|\\mathcal{S}_{z}|=k:|\\mathcal{D}_{s_{z}}-z|<\\varepsilon\\right)}\\\\ &{\\geq\\operatorname*{Pr}\\bigg(\\forall z\\in\\left\\{-\\sqrt{k}+i\\varepsilon^{\\prime}:i\\in\\left[\\frac{2}{\\varepsilon^{\\prime}}\\sqrt{k}\\right]\\right\\},\\exists\\mathcal{S}_{z}\\subset[n],|\\mathcal{S}_{z}|=k:|\\mathcal{D}_{s_{z}}-z|<\\varepsilon^{\\prime}\\bigg)}\\\\ &{=1-\\operatorname*{Pr}\\bigg(\\exists z\\in\\left\\{-\\sqrt{k}+i\\varepsilon^{\\prime}:i\\in\\left[\\frac{2}{\\varepsilon^{\\prime}}\\sqrt{k}\\right]\\right\\},\\exists\\mathcal{S}_{z}\\subset[n],|\\mathcal{S}_{z}|=k:|\\mathcal{S}_{s_{z}}-z|<\\varepsilon^{\\prime}\\bigg)}\\\\ &{\\geq1-\\quad\\quad\\quad\\quad\\quad\\quad\\operatorname*{Pr}\\left(\\frac{\\sqrt{k}}{2}c\\int_{0}^{\\infty}\\left[n\\right],|\\mathcal{S}_{z}|=k:|\\mathcal{D}_{s_{z}}-z|<\\varepsilon^{\\prime}\\right)}\\\\ &{\\qquad z\\in\\left\\{-\\sqrt{k+i\\varepsilon^{\\prime}+\\left[\\frac{2}{\\varepsilon^{\\prime}}\\sqrt{k}\\right]}\\right\\}}\\\\ &{\\geq1-\\frac{2}{\\varepsilon^{\\prime}}\\sqrt{k}\\left(1-c\\operatorname*{min}^{\\prime}\\right)^{r}=1-\\frac{2}{\\varepsilon^{\\prime}}\\sqrt{k}\\exp\\left(\\frac{4}{c\\operatorname*{min}}\\left(\\frac{k}{\\varepsilon}\\right)\\cdot\\ln(1-c\\operatorname*{min})\\right)}\\\\ &{\\geq1-\\frac{2}{\\varepsilon^{\\prime}}\\sqrt{k}\\exp\\left(-4\\ln\\frac{k}{\\varepsilon}\\right)=1-\\frac{2}{\\varepsilon^{\\prime}}\\sqrt{k}\\frac{\\varepsilon^{4}}{k^{4}}\\geq1-4\\varepsilon^{3}\\geq1-\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality is ensured by $\\varepsilon<{^1\\mathord{\\left/{\\vphantom{^12}}\\right.\\kern-\\nulldelimiterspace}2}$ . This completes the proof. ", "page_idx": 16}, {"type": "text", "text": "E Proof of Corollary 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Corollary 2. By definition of binary entropy, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nH_{2}\\left({\\frac{k}{n}}\\right)={\\frac{k}{n}}\\log_{2}\\left({\\frac{n}{k}}\\right)+\\left(1-{\\frac{k}{n}}\\right)\\log_{2}{\\frac{n}{n-k}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In particular, since both terms in the previous equation are positive, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\nH_{2}\\left({\\frac{k}{n}}\\right)\\geq{\\frac{k}{n}}\\log_{2}\\left({\\frac{n}{k}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now use eq. (28) to derive an upper bound for the quantity $\\frac{c_{\\mathrm{amp}}}{H_{2}\\left(\\frac{k}{n}\\right)}\\,\\frac{\\log_{2}^{2}{k}\\!+\\!2l o g_{2}k\\!\\cdot\\!l o g_{2}1/\\varepsilon}{n}$ log22 k+2log2k\u00b7log21/\u03b5, which will be used later: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{c_{\\mathrm{amp}}}{H_{2}\\left(\\frac{k}{n}\\right)}\\frac{\\log_{2}^{2}k+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}}{n}\\leq\\frac{c_{\\mathrm{amp}}}{\\frac{k}{n}\\log_{2}\\left(\\frac{n}{k}\\right)}\\frac{\\log_{2}^{2}k+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}}{n}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=c_{\\mathrm{amp}}\\frac{\\log_{2}^{2}k+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}}{k}\\frac{1}{\\log_{2}\\left(\\frac{n}{k}\\right)}}\\\\ &{\\leq c_{\\mathrm{amp}}\\frac{\\log_{2}^{2}k+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}}{k}}\\\\ &{\\leq\\frac{1}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where from eq. (29) to eq. (30) we used that $l o g_{2}n/k\\;\\geq\\;1$ for $k\\,\\leq\\,{n}/{2}$ , and then the hypothesis $k\\,\\geq\\,2c_{\\mathrm{amp}}\\left(\\log_{2}^{2}{k}+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}\\right)$ directly gives eq. (31). Let us now rewrite eq. (3) in a more convenient form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n\\frac{H_{2}\\big(\\frac{k}{n}\\big)}{c_{\\mathrm{ang}}}\\geq\\log_{2}^{\\frac{k}{\\varepsilon}}}\\\\ &{n\\frac{H_{2}\\big(\\frac{k}{n}\\big)}{c_{\\mathrm{ang}}}\\geq\\log_{2}^{2}k+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}+\\log_{2}^{2}\\frac{1}{\\varepsilon}}\\\\ &{n\\left(\\frac{H_{2}\\big(\\frac{k}{n}\\big)}{c_{\\mathrm{ang}}}-\\frac{\\log_{2}^{2}k+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}}{n}\\right)\\geq\\log_{2}^{2}\\frac{1}{\\varepsilon}}\\\\ &{n\\left(1-\\frac{c_{\\mathrm{ang}}}{H_{2}\\big(\\frac{k}{n}\\big)}\\frac{\\log_{2}^{2}k+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}}{n}\\right)\\geq\\frac{c_{\\mathrm{ang}}}{H_{2}\\big(\\frac{k}{n}\\big)}\\log_{2}^{2}\\frac{1}{\\varepsilon}}\\\\ &{n\\geq\\frac{c_{\\mathrm{ang}}}{\\left(1-\\frac{c_{\\mathrm{ang}}}{H_{2}\\big(\\frac{k}{n}\\big)}\\frac{\\log_{2}^{2}k+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}\\right)}{n}\\frac{\\log_{2}^{2}\\frac{1}{\\varepsilon}}{H_{2}\\big(\\frac{k}{n}\\big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using eq. (31) we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{c_{\\mathrm{amp}}}{\\left(1-\\frac{c_{\\mathrm{amp}}}{H_{2}\\left(\\frac{k}{n}\\right)}\\frac{\\log_{2}^{2}{k}+2l o g_{2}k\\cdot l o g_{2}\\frac{1}{\\varepsilon}}{n}\\right)}\\leq2c_{\\mathrm{amp}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To satisfy eq. (33), we can then choose $n$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\nn\\geq2c_{\\mathrm{amp}}{\\frac{\\log_{2}^{2}{\\frac{1}{\\varepsilon}}}{H_{2}\\left({\\frac{k}{n}}\\right)}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and then apply Corollary 1 to end the proof. ", "page_idx": 17}, {"type": "text", "text": "F Details for the proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Eq. 17 Define $A={\\tilde{S}}^{\\prime}\\backslash{\\tilde{S}}$ , and observe that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(H_{\\delta};H_{\\delta},I_{n,k-1}\\right)}\\\\ &{=\\displaystyle\\sum_{i=\\mu_{\\delta}}^{k-1}\\operatorname*{Pr}\\left(H_{\\delta};H_{\\delta},I_{n}\\right)\\operatorname*{Pr}\\left(I_{i}\\,|\\,H_{\\delta},I_{n,k-1}\\right)}\\\\ &{=\\displaystyle\\sum_{i=\\mu_{\\delta}}^{k-1}\\int_{-\\infty}^{\\infty}\\operatorname*{Pr}\\left(|\\sum_{k=\\left(\\delta-y\\right)}^{k}|<\\varepsilon|\\sum_{l}\\varepsilon_{l}\\right)\\operatorname*{Pr}\\left(\\Sigma_{l}=y\\right)\\operatorname*{I}{H}_{\\delta},I_{l}\\,d y}\\\\ &{\\quad\\quad\\cdot\\operatorname*{Pr}\\left(I_{i}\\,|\\,H_{\\delta},I_{n,k-1}\\right)}\\\\ &{=\\displaystyle\\sum_{i=\\mu_{\\delta}}^{k-1}\\int_{-\\infty}^{\\infty}\\operatorname*{Pr}\\left(|\\sum_{l}\\varepsilon_{l}|\\leq-\\left(z-y\\right)|<\\varepsilon|\\sum_{l}\\varepsilon_{l}\\right)\\operatorname*{Pr}\\left(\\Sigma_{l}=y\\right|\\,H_{\\delta},I_{l}\\right)d y}\\\\ &{\\quad\\quad\\cdot\\operatorname*{Pr}\\left(I_{i}\\,|\\,H_{\\delta},I_{n,k-1}\\right)}\\\\ &{\\leq\\displaystyle\\operatorname*{e}\\varepsilon\\sum_{i=\\mu_{\\delta}}^{k-1}\\int_{-\\infty}^{\\infty}\\operatorname*{Pr}\\left(\\Sigma_{l}=y\\right)\\operatorname*{I}{H}_{\\delta},I_{n}\\right)d y\\operatorname*{Pr}\\left(I_{i}\\,|\\,H_{\\delta},I_{n,k-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "\u2264c\u03b5 ", "page_idx": 18}, {"type": "text", "text": "where from Eq. 36 to Eq. 37 and from Eq. 37 to Eq. 38 we used the law of total probability;7 from Eq. 38 to Eq. 39 we dropped the redundant event $H_{\\tilde{S}}$ in the conditioning, due to conditional independence; finally, from Eq. 39 to Eq. 40 we used Definition 1 which implies that for any $i\\in\\bar{\\{\\mu k,...,k-1\\}}$ it holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left|\\Sigma_{A}-(z-y)\\right|<\\varepsilon\\,\\right|\\Sigma_{I}=y,I_{i}\\right)=\\operatorname*{Pr}\\left(\\left|\\Sigma_{[k-i]}-(z-y)\\right|<\\varepsilon\\right)\\leq c\\varepsilon.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Eq. 18 Let $A={\\tilde{S}}^{\\prime}\\backslash{\\tilde{S}}$ . Analogously to the calculations from Eq. 37 to Eq. 39, by the law of total probability we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{\\mu k-1}\\operatorname{Pr}\\left(I_{i}\\right)\\cdot\\operatorname{Pr}\\left(H_{\\delta}\\mid H_{\\delta},I_{i}\\right)}\\\\ &{\\displaystyle=\\sum_{i=0}^{\\mu k-1}\\operatorname{Pr}\\left(I_{i}\\right)\\cdot\\int_{-\\infty}^{\\infty}\\operatorname{Pr}\\left(\\left|\\sum_{k=1}^{\\infty}\\left(z-y\\right)\\right|<\\varepsilon\\left|\\sum_{I}\\eta_{I}\\right)\\operatorname{Pr}\\left(\\sum_{I}\\eta_{I}=y\\right|H_{\\delta},I_{i}\\right)d y}\\\\ &{\\displaystyle=\\sum_{i=0}^{\\mu k-1}\\operatorname{Pr}\\left(I_{i}\\right)\\cdot\\int_{-\\infty}^{\\infty}\\operatorname{Pr}\\left(\\left|\\sum_{[k-i]}-\\left(z-y\\right)\\right|<\\varepsilon\\right)\\operatorname{Pr}\\left(\\sum_{I}\\eta_{I}=y\\mid H_{\\delta},I_{i}\\right)d y}\\\\ &{\\displaystyle\\leq c\\frac{\\varepsilon}{\\sqrt{k}}\\sum_{i=0}^{\\mu k-1}\\operatorname{Pr}\\left(I_{i}\\right)\\cdot\\int_{-\\infty}^{\\infty}\\operatorname{Pr}\\left(\\sum_{I}\\eta_{I}=y\\mid H_{\\delta},I_{i}\\right)d y}\\\\ &{\\displaystyle\\leq c\\frac{\\varepsilon}{\\sqrt{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where from Eq. 41 to Eq. 42 we used Definition 1, which implies that for any $i\\in\\{0,...,\\frac{9}{10}k-1\\}$ it holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left|\\Sigma_{[k-i]}-(z-y)\\right|<\\varepsilon\\right)\\leq c^{\\prime}{\\frac{\\varepsilon}{\\sqrt{k-i}}}\\leq c{\\frac{\\varepsilon}{\\sqrt{k}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "G Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the proof we will refer to the following results, upon which [24][Theorem 1] relies (the statement below slightly differ as we fix two small typos in their notation and mixing coefficients). With the understanding that by a mixture $D$ of a distribution $D_{1}$ and $D_{2}$ with probability $p$ it is meant that the pdf (we adopt the convention that this term includes generalised functions, such as Dirac deltas for point masses) of $D$ can be written as a convex combination of the pdf of $D_{1}$ and that of $D_{2}$ , that is $f_{D}=p f_{D_{1}}+(1-p)f_{D_{2}}$ . For the unfamiliar reader, we note that in the literature this is often stated in short as $\\dot{D}=p\\dot{D}_{1}\\,\\bar{+}\\,(1-p)D_{2}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 2 ([24][Corollary 1]). Let $\\begin{array}{r l}{X}&{{}\\sim U{n}i f o r m[0,1]}\\end{array}$ (or $\\begin{array}{r l}{X}&{{}\\sim U n i f o r m[-1,0])}\\end{array}$ and $Y\\sim U n i f o r m[-1,1]$ be independent random variables. Let $P$ be the distribution of the $X Y$ and $\\delta_{0}$ the Dirac delta at 0. Let $D$ be the distribution obtained as mixture of $\\delta_{0}$ and $P$ with probability $^1\\!/\\!2$ . Then $D$ is the mixture of a Uniform $\\left[-1/2,\\,1/2\\right]$ and some distribution $Q$ with probability $\\ln(2)/{\\dot{4}}$ . Corollary 3 ([24][Corollary 2]). Let $X_{1},\\ldots,X_{n}$ be iid with distribution $D$ as defined in Lemma 2, where $n\\geq C\\ln(^{2}\\!/\\varepsilon)$ for some universal constant $C$ . Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\forall\\,z\\in[-1,1],\\,\\exists\\,S\\subset[n]\\,:|z-\\sum_{i\\in S}X_{i}|\\leq\\varepsilon\\right)\\geq1-\\varepsilon.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 3. The key idea is exploiting Corollary 1 at each step of the pruning strategy established in [24][Theorem 1], where Corollary 3 is used instead. Without loss of generality, we replace their $\\operatorname*{min}\\{\\varepsilon,\\delta\\}$ with $\\varepsilon$ . For the sake of easily following the approach adopted in [24], let us define $n^{*}(x)$ as the function ", "page_idx": 18}, {"type": "equation", "text": "$$\nn^{*}(x)=c_{\\mathrm{amp}}\\frac{\\log_{2}^{2}\\left(k x\\right)}{H_{2}\\left(\\frac{k}{n^{*}(x)}\\right)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $k=\\gamma^{\\prime}n^{*}(x)$ . In the following, we use $n^{*}$ as short for $n^{*}(1/\\varepsilon)$ , and we will only explicitely provide an argument for $n^{*}$ when it is different than $1/\\varepsilon$ . For instance, in the last step of the proof, we will use $n^{\\bar{*}}(2\\ell d_{i}d_{i-1}/\\varepsilon)$ , which matches the definition of $n_{i}^{*}$ given in Eq. 20. ", "page_idx": 19}, {"type": "text", "text": "Consider [24][Lemma 1]. When approximating a single link (that is, a weight), after the overparameterization (which creates an additional layer of width $2{n}^{*}$ in between the input and the output node) via $4n^{*}$ links, instead of pruning via Corollary 3, we prune via Corollary 1 twice in the second layer, that is we ensure that only $k=\\gamma^{\\prime}n^{*}$ edges yield the desired approximation, both in the edges corresponding to the positive part of the input weights and in those corresponding to the negative part. Thus we obtain at most $4k$ surviving edges, after the preprocessing step and the pruning mask is applied. This yields a sparsity of at least $\\alpha^{\\prime}=1-\\gamma^{\\prime}$ . Note that it is because of the preprocessing step that we go from distributions Uniform $[-1,1]$ to distributions $D$ , as defined in Lemma 2, which are shown to be a mixture with Uniform $[-1,1]$ and therefore can be also handled via Corollary 1. ", "page_idx": 19}, {"type": "text", "text": "Consider [24][Lemma 2]. When approximating a real-valued multivariate linear function, after the overparameterization (which creates an additional layer of width $2d n^{*}(d/\\varepsilon)$ in between the $d$ input nodes and the output node) one simply iterates the ideas of the previous case $d$ times. For each input node, the overparameterization surviving the preprocessing step on the weights of the input layer is $4n^{*}(d/\\varepsilon)$ . Pruning the second layer of the overparameterized link for each input via Corollary 1 with $k=\\gamma^{\\prime}n^{*}(d/\\varepsilon)$ (again, performing this both on the edges corresponding to the positive part of the input weights and in those corresponding to the negative part), instead of exploiting Corollary 3, yields that at most $4d k$ edges survive after the pruning mask is applied. This yields a sparsity of at least $\\alpha^{\\prime}=1-\\gamma^{\\prime}$ . ", "page_idx": 19}, {"type": "text", "text": "Finally, consider [24][Lemma 3]. When approximating a layer with input dimension $d_{1}$ and output dimension $d_{2}$ , after the overparameterization (which creates an additional layer of width $2\\bar{d_{1}n^{*}}(d_{1}d_{2}/\\varepsilon)$ in between the input nodes and the output nodes) one iterates the ideas of the previous case $d_{1}$ times in the input layer through the same preprocessing step, and $d_{2}$ times in the output layer, one for each of the $d_{1}$ blocks created by the preprocessing (essentially the weights in the input layer are $r e$ -used $d_{2}$ times). For each input node, the overparameterization surviving the preprocessing step is at most $2(d_{2}+1)n^{*}(d_{1}d_{2}/\\varepsilon)$ . Overall, after the preprocessing step, we have at most $2d_{1}(d_{2}+1)n^{*}(d_{1}d_{2}/\\varepsilon)$ parameters. We then use Corollary 1 (with $k\\,=\\,\\bar{\\gamma}^{\\prime}n^{*}\\bar{(}d_{1}d_{2}/\\varepsilon))$ to prune the number of parameters between the introduced additional layer and the $d_{2}$ outputs down to $\\bar{2}d_{1}d_{2}\\gamma^{\\prime}n^{*}(d_{1}d_{2}/\\varepsilon)$ . As for the edges between the $d_{1}$ inputs and the additional layer, only those that reach a neuron in the additional layer, from which there is at least one outgoing edge towards the $d_{2}$ outputs, are used; since for each of the $d_{1}$ blocks of $2n^{*}\\big(d_{1}d_{2}\\big/\\varepsilon\\big)$ neurons in the additional layer we only kept $2\\gamma^{\\prime}n^{*}(d_{1}d_{2}/\\varepsilon)$ outgoing edges to each of the $d_{2}$ output neurons, in the worst case (all the nodes involved in the subsetsums are disjoint) we keep $2d_{2}\\gamma^{\\prime}n^{*}(d_{1}d_{2}\\big/\\varepsilon)$ of them for each of the $d_{1}$ neurons. Globally, we are left with a total of at most $2d_{1}d_{2}\\gamma^{\\prime}n^{*}(d_{1}d_{2}/\\varepsilon)$ edges both in the input layer and in the output layer, thus a total of $4d_{1}d_{2}\\gamma^{\\prime}n^{*}(d_{1}d_{2}/\\varepsilon)$ edges survive the pruning. The density of the surviving edges is then less than ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{4d_{1}d_{2}\\gamma^{\\prime}n^{*}(d_{1}d_{2}/\\varepsilon)}{2d_{1}^{2}n^{*}\\big(d_{1}d_{2}/\\varepsilon\\big)+2d_{1}d_{2}n^{*}\\big(d_{1}d_{2}/\\varepsilon\\big)}=\\frac{2d_{2}\\gamma^{\\prime}}{d_{1}+d_{2}}=\\frac{(d_{1}\\frac{d_{2}}{d_{1}}+d_{2})\\gamma^{\\prime}}{d_{1}+d_{2}}\\leq\\rho_{1}\\gamma^{\\prime},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\rho_{1}=\\operatorname*{max}\\left\\lbrace d_{1}/d_{2},d_{2}/d_{1}\\right\\rbrace$ and in the last inequality we used that $d_{1}d_{2}/d_{1}+d_{2}\\leq\\rho_{1}(d_{1}+d_{2})$ since $\\rho_{1}\\geq1$ . This ensures a sparsity $\\alpha^{\\prime}\\geq1-\\rho_{1}\\bar{\\gamma}^{\\prime}$ . ", "page_idx": 19}, {"type": "text", "text": "[24][Theorem 1] consists of performing, for every $i\\in[\\ell]$ , the previous step on layer $i$ with input dimension $d_{i-1}$ and output dimension $d_{i}$ . The overparameterization creates an additional layer of nodes of width $2d_{i-1}n^{*}(2\\ell d_{i-1}d_{i}/\\varepsilon)$ in between the $d_{i-1}$ input nodes and the $d_{i}$ output nodes. Since the construction is stacked $\\ell$ times, this generates $2\\ell$ layers for the overparameterized network, which will therefore have a starting number of parameters ", "page_idx": 19}, {"type": "equation", "text": "$$\nm=\\sum_{i=1}^{\\ell}2d_{i-1}^{2}n^{*}\\mathopen{}\\mathclose\\bgroup\\left(2\\ell d_{i-1}d_{i}/\\varepsilon\\aftergroup\\egroup\\right)+2d_{i-1}d_{i}n^{*}\\mathopen{}\\mathclose\\bgroup\\left(2\\ell d_{i-1}d_{i}/\\varepsilon\\aftergroup\\egroup\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Corollary 1 applied to each stacked overparameterized layer instead of Corollary 3 as in the previous step yields that the total number of parameters left after the pruning is ", "page_idx": 19}, {"type": "equation", "text": "$$\nm_{t}\\leq\\sum_{i=1}^{\\ell}4d_{i-1}d_{i}k_{i},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $k_{i}\\,=\\,\\gamma^{\\prime}n^{*}\\bigl(2\\ell d_{i\\,-\\,1}d_{i}\\big/\\varepsilon\\bigr)$ . Recall that $\\rho\\,=\\,\\operatorname*{max}_{i}\\rho_{i}$ , where $\\rho_{i}\\,=\\,\\operatorname*{max}\\{d_{i}\\bigl/d_{i-1},d_{i-1}\\bigl/d_{i}\\bigl\\}\\,\\ge\\,1$ . Recall that $\\gamma=\\rho\\gamma^{\\prime}$ . We obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{t}\\leq\\displaystyle\\sum_{i=1}^{\\ell}2d_{i-1}\\frac{d_{i}}{d_{i-1}}d_{i-1}k_{i}+2d_{i-1}d_{i}k_{i}}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{i=1}^{\\ell}2d_{i-1}\\rho_{i}d_{i-1}k_{i}+2d_{i-1}d_{i}\\rho_{i}k_{i}}\\\\ &{\\qquad\\leq\\rho\\displaystyle\\sum_{i=1}^{\\ell}2d_{i-1}^{2}k_{i}+2d_{i-1}d_{i}k_{i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad=\\rho\\gamma^{\\prime}\\displaystyle\\sum_{i=1}^{\\ell}2d_{i-1}^{2}n^{*}(2\\ell d_{i-1}d_{i}/\\ell)+2d_{i-1}d_{i}n^{*}\\big(2\\ell^{d_{i-1}d_{i}/\\ell}\\big)=\\gamma m}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We then get that the density of the edges surviving the pruning is $m_{t}/_{m}\\le\\gamma$ , which implies a sparsity of at least $\\alpha=1-\\gamma$ . ", "page_idx": 20}, {"type": "text", "text": "H Proof of Theorem 5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 5. Consider the space $\\mathcal{W}_{k}\\;=\\;\\{W\\;\\in\\;\\mathbb{R}^{d\\times d}\\;:\\;\\|W\\|\\;\\leq\\;\\sqrt{k}\\}.$ , and let $\\mathcal{P}_{k}$ be a $2\\varepsilon$ -separated set of $\\mathcal{W}_{k}$ , i.e. a subset $\\mathcal{P}_{k}~\\subset~\\mathcal{W}_{k}$ such that for all distinct $W,\\dot{W}^{\\prime}\\,\\in\\,\\mathcal P_{k}$ it holds $\\|W-W^{\\prime}\\|>2\\varepsilon$ . We denote $\\mathcal{W}=\\mathcal{W}_{1}$ , $\\mathcal{P}=\\mathcal{P}_{1}$ , and the set of all possible subnetworks of $g$ as $\\mathcal{G}$ (note that this does not denote $\\mathcal{G}_{1}$ , the set of all subnetworks of size 1). ", "page_idx": 20}, {"type": "text", "text": "Step 1: Packing argument. In [24][Theorem 2, Step 1], it is shown that any function $g^{\\prime}$ can only approximate at most one member of $\\mathcal{P}$ for bounded input $x$ (say, $\\|x\\|\\leq1)$ ). In particular, this also applies to functions $g^{\\prime}$ representing the elements of $\\mathcal{G}_{k}$ . ", "page_idx": 20}, {"type": "text", "text": "Step 2: Relation between $|\\mathcal{G}_{k}|$ and $|\\mathcal{P}_{k}|$ . By Step $^{\\,I}$ , in [24][Theorem 2, Step 2] it is shown that $\\begin{array}{r}{\\bar{|\\mathcal{P}|^{\\bar{\\mathbf{\\alpha}}}}\\leq2\\bar{|\\mathcal{G}|}}\\end{array}$ , under the assumption of Eq. 24, with $\\mathcal{G}_{k}$ replaced by $\\mathcal{G}$ ). Therefore, also by Step $^{\\,I}$ , replacing $\\mathcal{P}$ with $\\mathcal{P}_{k}$ and $\\mathcal{G}$ with $\\mathcal{G}_{k}$ in [24][Theorem 2, Step 2], it holds that $|\\mathcal{P}_{k}|\\,\\leq\\,2|\\mathcal{G}_{k}|$ . Note that $|{\\mathcal{G}}_{k}|={\\binom{n}{k}}$ , the number of different ways in which we can select $k$ parameters out of $n$ , so we actually get ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\binom{n}{k}}>{\\frac{|{\\mathcal{P}}_{k}|}{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 3: Lower bound on $|\\mathcal{P}_{k}|$ . Let us now consider a $2\\varepsilon$ -separated set $\\mathcal{P}_{k}^{\\mathrm{max}}$ of maximal cardinality. In [24][Theorem 2, Step 3] it is shown that ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\mathcal{P}^{\\mathrm{max}}|\\geq\\frac{\\mathrm{vol}(\\mathcal{W})}{\\mathrm{Vol}(\\{W\\in\\mathcal{W}:\\|W\\|\\leq2\\varepsilon\\})}=\\left(\\frac{1}{2\\varepsilon}\\right)^{d^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here Vol is the Lebesgue measure in $\\mathbb{R}^{d\\times d}$ identified with $\\mathbb{R}^{d^{2}}$ .By the exact same argument, replacing $\\mathcal{W}$ with $\\mathcal{W}_{k}$ and thus $\\mathcal{P}^{\\mathrm{max}}$ with $\\mathcal{P}_{k}^{\\mathrm{max}}$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\mathcal{P}_{k}^{\\operatorname*{max}}|\\geq\\frac{\\mathrm{Vol}(\\mathcal{W}_{k})}{\\mathrm{Vol}(\\{W\\in\\mathcal{W}_{k}:\\|W\\|\\leq2\\varepsilon\\})}=\\left(\\frac{\\sqrt{k}}{2\\varepsilon}\\right)^{d^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining this fact with Eq. 44 applied to $\\mathcal{P}_{k}^{\\mathrm{max}}$ implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\binom{n}{k}}>{\\frac{1}{2}}\\left({\\frac{\\sqrt{k}}{2\\varepsilon}}\\right)^{d^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 4: Lower bound on $n$ . Consider the standard bound found in [19] ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\binom{n}{k}}\\leq{\\sqrt{\\frac{n}{2\\pi k(n-k)}}}2^{n H_{2}(k/n)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and combine it with with Eq. 45. It follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n2^{n H_{2}\\left(\\frac{k}{n}\\right)}\\geq\\frac{1}{2}\\sqrt{\\frac{2\\pi k(n-k)}{n}}\\left(\\frac{\\sqrt{k}}{2\\varepsilon}\\right)^{d^{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and taking the logarithm of both sides yields the sought lower bound on $n$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n H_{2}\\left(\\frac{k}{n}\\right)\\geq\\frac{1}{2}\\log_{2}\\left(\\frac{2\\pi k(n-k)}{n}\\right)+d^{2}\\log_{2}\\frac{\\sqrt{k}}{2\\varepsilon}-1}\\\\ &{\\qquad\\qquad\\qquad\\geq d^{2}\\left(\\frac{1}{2}\\log_{2}k+\\log_{2}\\frac{1}{\\varepsilon}-1\\right)-1}\\\\ &{\\qquad\\qquad\\geq\\frac{d^{2}}{2}\\log_{2}\\frac{k}{\\varepsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where from Eq. 46 to Eq. 47 we exploited the definition of $\\lambda$ , which ensures that the first term in the r.h.s. of Eq. 46 is nonnegative;8 from Eq. 47 to Eq. 48 we used that for all $\\varepsilon<1/16$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\nd^{2}\\left(\\log_{2}{\\frac{1}{\\varepsilon}}-1\\right)\\geq1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "I Details of comparison with Malach et al. [20] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We show that cz \u2265cam plologg22(c(zz2)) holds for a big enough constant c. Recall that z = $\\begin{array}{r}{z=\\frac{m_{t}}{\\varepsilon}}\\end{array}$ , so we can always assume $\\log_{2}(z)\\geq1$ . We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log_{2}^{2}(c z^{2})=(\\log_{2}(c)+2\\log_{2}(z))^{2}}\\\\ &{\\qquad\\qquad=\\log_{2}^{2}(c)+4\\log_{2}(c)\\log_{2}(z)+4\\log_{2}^{2}(z)}\\\\ &{\\qquad\\quad\\stackrel{(a)}{\\leq}6(\\log_{2}^{2}(c)+\\log_{2}^{2}(z))}\\\\ &{\\qquad\\quad\\stackrel{(b)}{\\leq}12\\log_{2}^{2}(c)\\log_{2}^{2}(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where in $(a)$ we used that $2a b\\leq a^{2}+b^{2}$ , and in $(b)$ that $a+b\\leq2a b$ for $a$ and $b$ greater than 1. We can then focus on showing that there is a big enough constant $c$ such that $\\begin{array}{l l}{{c z}}&{{\\ge}}\\end{array}$ $12c_{\\mathrm{amp}}\\log_{2}^{2}(c)\\log_{2}^{2}(z)$ . We get $c\\geq12c_{\\mathrm{amp}}\\log_{2}^{2}(c)\\frac{\\log_{2}^{2}(z)}{z}$ , and we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\log_{2}^{2}(c)\\frac{\\log_{2}^{2}(z)}{z}\\leq\\log_{2}^{2}(c)}\\\\ {\\leq\\sqrt{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We can then focus on $c\\geq12c_{\\mathrm{amp}}{\\sqrt{c}}$ , which is satisfied for $c\\geq144c_{\\mathrm{amp}}^{2}$ ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The abstract and the introduction precisely reflect our contribution. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have added a discussion on the limitations. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Assumptions for all results are accurately stated in our statements. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted confrms in every respect with the NeurIPS Code of Ehtics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: A small section discussing the potential impacts of our work has been added to the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We not releasing any data or models (apart from mathematical models) hence there is no risk of misues. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Everything is cited properly and no assests are required. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No assets are introduced. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No such research was conducted. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: There are not study participants. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]