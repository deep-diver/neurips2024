{"references": [{"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-08", "reason": "This paper is foundational to the concept of scaling in machine learning, which is central to the current paper's investigation of scaling neural network interatomic potentials."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This work highlights the success of scaling in computer vision, providing a comparative context for the challenges and opportunities in scaling NNIPs."}, {"fullname_first_author": "Xiaohua Zhai", "paper_title": "Scaling vision transformers", "publication_date": "2022-00-00", "reason": "This paper further demonstrates the benefits of scaling in the context of vision transformers, offering additional support for the approach taken in the current research."}, {"fullname_first_author": "Volker L Deringer", "paper_title": "Machine learning interatomic potentials as emerging tools for materials science", "publication_date": "2019-00-00", "reason": "This review paper establishes the significance of machine learning interatomic potentials (MLIPs) in materials science, providing a strong foundation for the current paper's focus on NNIPs."}, {"fullname_first_author": "Oliver T Unke", "paper_title": "Machine learning force fields", "publication_date": "2021-00-00", "reason": "This comprehensive review paper provides a broad overview of the field of machine learning force fields, serving as a valuable resource for understanding the context and state of the art in NNIPs."}]}