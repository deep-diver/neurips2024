{"references": [{"fullname_first_author": "Jia", "paper_title": "Direct speech-to-speech translation with discrete units", "publication_date": "2022-07-01", "reason": "This paper introduces a novel speech-to-speech translation method that uses discrete units, achieving significant performance gains and influencing the current S2ST work."}, {"fullname_first_author": "Huang", "paper_title": "Transpeech: Speech-to-speech translation with bilateral perturbation", "publication_date": "2023-01-01", "reason": "This paper proposes a novel non-autoregressive speech-to-speech translation method using bilateral perturbation, which addresses the multi-modality problem and improves translation quality."}, {"fullname_first_author": "Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-01-01", "reason": "This foundational work on diffusion probabilistic models is crucial to the proposed DIFFNORM method, which leverages the denoising objective for speech normalization."}, {"fullname_first_author": "Ghazvininejad", "paper_title": "Mask-predict: Parallel decoding of conditional masked language models", "publication_date": "2019-11-01", "reason": "This paper introduces the Conditional Masked Language Model (CMLM), a non-autoregressive transformer model adopted in this work, laying the foundation for the non-autoregressive speech-to-unit translation."}, {"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This seminal paper on the Transformer architecture is foundational to many modern sequence-to-sequence models, including those used in this work for both autoregressive and non-autoregressive speech-to-speech translation."}]}