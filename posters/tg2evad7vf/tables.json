[{"figure_path": "Tg2EVad7VF/tables/tables_3_1.jpg", "caption": "Table 2: Comparison of speech-to-speech models evaluated by quality (ASR-BLEU) and speed (units/seconds). *: Fr-En experiments are added during author response period and we leave model 7,8 for future work. + We use w = 0.5 for CG. Our NAT models achieve superior translation quality while maintaining their fast inference speed.", "description": "This table compares different speech-to-speech translation models in terms of their translation quality (measured by ASR-BLEU score) and inference speed (units per second).  It shows a comparison between autoregressive and non-autoregressive models, highlighting the improved performance and speed achieved by the proposed models (CMLM + DIFFNORM and CMLM + DIFFNORM + CG). The table also shows the speedup achieved by the non-autoregressive models compared to the autoregressive baseline.", "section": "5.2 Results"}, {"figure_path": "Tg2EVad7VF/tables/tables_5_1.jpg", "caption": "Table 1: Data statistics for CVSS benchmarks.", "description": "The table shows the data statistics for the Cross-Lingual Speech-to-Speech Translation (CVSS) benchmark used in the paper's experiments. It presents the number of samples and average length of target speech (in units) for both English-Spanish (En-Es) and English-French (En-Fr) language pairs, broken down by training, validation, and test sets.  This data informs the scale and characteristics of the datasets used to train and evaluate the proposed models.", "section": "5.1 Experimental setup"}, {"figure_path": "Tg2EVad7VF/tables/tables_6_1.jpg", "caption": "Table 2: Comparison of speech-to-speech models evaluated by quality (ASR-BLEU) and speed (units/seconds). *: Fr-En experiments are added during author response period and we leave model 7,8 for future work. + We use w = 0.5 for CG. Our NAT models achieve superior translation quality while maintaining their fast inference speed.", "description": "This table compares various speech-to-speech translation models, both autoregressive and non-autoregressive, based on their performance metrics (ASR-BLEU score, which measures the quality of the translation) and inference speed (measured as units per second, reflecting how quickly they produce results). It showcases the improvement achieved by incorporating DIFFNORM and classifier-free guidance in the non-autoregressive models, especially model 8, which significantly outperforms both autoregressive and baseline non-autoregressive models.", "section": "5.2 Results"}, {"figure_path": "Tg2EVad7VF/tables/tables_7_1.jpg", "caption": "Table 3: For different start steps T, we show corresponding noise scheduling parameter values, reconstruction quality (-Rec columns), and downstream translation quality (-Dn column). Noise injection that perturbs about 20% of units (i.e., 80% Acc-Rec) results in the best downstream S2UT performance (highlighted in bold text).", "description": "This table shows the impact of varying the start step (T) in the DIFFNORM process on reconstruction quality and downstream S2UT performance.  Different values of T correspond to different levels of noise injection during the diffusion process. The table presents the scheduling parameters (\u03b2t, \u221a\u03b1t, \u221a1\u2212\u03b1t) for each T, along with metrics for reconstruction quality (Acc-Rec, BL-Rec) and downstream translation quality (BL-Dn). The best downstream performance is observed when the noise injection perturbs approximately 20% of the units.", "section": "6 Analysis"}, {"figure_path": "Tg2EVad7VF/tables/tables_7_2.jpg", "caption": "Table 4: Reconstruction and downstream performance with small noise injection.", "description": "This table shows the reconstruction accuracy (Acc-Rec) and downstream ASR-BLEU score (BL-Rec) for different start times (T) in the diffusion model during speech normalization.  A smaller start time implies less noise injection. The results demonstrate the impact of noise level on the quality of reconstructed speech units and their subsequent performance in a downstream speech-to-text translation task. ", "section": "6.1 Effect of synthetic noise injection"}, {"figure_path": "Tg2EVad7VF/tables/tables_8_1.jpg", "caption": "Table 5: Accuray of reconstructed speech units. KL: when applied, the latent space is regularized to be Gaussian [27]. Multitask: when not applied, the latent diffusion model is trained only with Lnoise.", "description": "This table presents the results of ablation experiments on the training objectives for the DIFFNORM model. It shows the impact of using different latent dimensions (16, 32, and 128), the inclusion or exclusion of KL divergence regularization, and the use of a multitask objective (combining noise estimation, reconstruction loss, and negative log-likelihood loss) versus only a noise estimation objective, on the reconstruction accuracy of speech units at different noise levels (start steps).", "section": "6.2 Ablation on training objectives for DIFFNORM"}, {"figure_path": "Tg2EVad7VF/tables/tables_13_1.jpg", "caption": "Table 2: Comparison of speech-to-speech models evaluated by quality (ASR-BLEU) and speed (units/seconds). *: Fr-En experiments are added during author response period and we leave model 7,8 for future work. + We use w = 0.5 for CG. Our NAT models achieve superior translation quality while maintaining their fast inference speed.", "description": "This table compares various speech-to-speech translation models, including autoregressive and non-autoregressive models, based on their ASR-BLEU scores and inference speed (units per second).  It highlights the improvements achieved by the proposed DIFFNORM and classifier-free guidance methods in terms of both translation quality and inference speed. The asterisk indicates that Fr-En results were added later, and the plus sign indicates the specific hyperparameter setting used for classifier-free guidance.", "section": "5.2 Results"}, {"figure_path": "Tg2EVad7VF/tables/tables_13_2.jpg", "caption": "Table 2: Comparison of speech-to-speech models evaluated by quality (ASR-BLEU) and speed (units/seconds). *: Fr-En experiments are added during author response period and we leave model 7,8 for future work.  + We use w = 0.5 for CG. Our NAT models achieve superior translation quality while maintaining their fast inference speed.", "description": "This table compares the performance of various speech-to-speech translation models, both autoregressive and non-autoregressive.  It shows the ASR-BLEU scores (a measure of translation quality) and the decoding speed (units per second) for each model. The table highlights the significant improvements achieved by the proposed methods (DIFFNORM and Classifier-free guidance) in terms of both translation quality and speed.", "section": "5.2 Results"}, {"figure_path": "Tg2EVad7VF/tables/tables_14_1.jpg", "caption": "Table 2: Comparison of speech-to-speech models evaluated by quality (ASR-BLEU) and speed (units/seconds). *: Fr-En experiments are added during author response period and we leave model 7,8 for future work. + We use w = 0.5 for CG. Our NAT models achieve superior translation quality while maintaining their fast inference speed.", "description": "This table compares different speech-to-speech translation models, evaluating both their quality (measured by ASR-BLEU score) and inference speed (measured in units per second).  It includes autoregressive and non-autoregressive models, with and without the proposed DIFFNORM and classifier-free guidance techniques. The table highlights the significant quality improvements and speedups achieved by the proposed models compared to baselines.", "section": "5.2 Results"}, {"figure_path": "Tg2EVad7VF/tables/tables_16_1.jpg", "caption": "Table 2: Comparison of speech-to-speech models evaluated by quality (ASR-BLEU) and speed (units/seconds). *: Fr-En experiments are added during author response period and we leave model 7,8 for future work. + We use w = 0.5 for CG. Our NAT models achieve superior translation quality while maintaining their fast inference speed.", "description": "This table compares the performance of various speech-to-speech translation models, broken down by whether they are autoregressive or non-autoregressive.  It shows the ASR-BLEU scores (a measure of translation quality) and the speed in units per second. The table highlights that the non-autoregressive models using the proposed DIFFNORM and classifier-free guidance techniques achieve superior translation quality with significantly faster inference speeds compared to autoregressive baselines.", "section": "5.2 Results"}, {"figure_path": "Tg2EVad7VF/tables/tables_17_1.jpg", "caption": "Table 9: Experimental Results of different En-Es speech-to-unit translation systems.", "description": "This table presents the ASR-BLEU scores achieved by four different speech-to-unit translation models on the English-to-Spanish (En-Es) dataset of the CVSS benchmark.  The models tested include a baseline CMLM model and variations incorporating classifier-free guidance (CG) and DIFFNORM normalization.  The results are shown for different numbers of decoding iterations (3, 5, 7, 19, and 15), highlighting the performance improvements with the addition of DIFFNORM and CG.", "section": "5.2 Results"}, {"figure_path": "Tg2EVad7VF/tables/tables_17_2.jpg", "caption": "Table 10: Experimental Results of different En-Fr speech-to-unit translation systems.", "description": "This table presents the experimental results comparing four different speech-to-unit translation models on English-French (En-Fr) data.  The models are: CMLM (Conditional Masked Language Model), CMLM + CG (CMLM with Classifier-Free Guidance), CMLM + DiffNorm (CMLM with the DIFFNORM normalization strategy), and CMLM + DiffNorm + CG (CMLM with both DIFFNORM and Classifier-Free Guidance). The results are shown for different numbers of decoding iterations (3, 5, 7, 19, and 15) and measured as ASR-BLEU scores.  The table helps evaluate the impact of each method (DIFFNORM and CG) on the translation quality and the effect of the number of decoding iterations on performance. ", "section": "5.2 Results"}]