{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models from Natural Language Supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a foundational vision-language model that is extensively used and analyzed throughout the current research."}, {"fullname_first_author": "Casey Meehan", "paper_title": "Do SSL Models Have D\u00e9j\u00e0 Vu? A Case of Unintended Memorization in Self-supervised Learning", "publication_date": "2023-04-13", "reason": "This paper is highly relevant as it directly addresses the topic of memorization in representation learning models, providing a foundation for the current study."}, {"fullname_first_author": "Nicholas Carlini", "paper_title": "Extracting Training Data from Large Language Models", "publication_date": "2021-00-00", "reason": "This study investigates the memorization of training data in large language models, offering a parallel to the issues explored in the current work for vision-language models."}, {"fullname_first_author": "Amro Abbas", "paper_title": "Semdedup: Data-efficient learning at web-scale through semantic deduplication", "publication_date": "2023-03-09", "reason": "This paper is crucial because its deduplication technique was used in processing the LAION dataset, a key component of the current study's experiments."}, {"fullname_first_author": "Gabriel Ilharco", "paper_title": "OpenCLIP", "publication_date": "2021-00-00", "reason": "This paper provides the open-source implementation of CLIP, which is the primary model used in the current research for empirical evaluations."}]}