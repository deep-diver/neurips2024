[{"figure_path": "vAOgaPvgYr/figures/figures_2_1.jpg", "caption": "Figure 1: The OccamLLM system. For each autoregressive step, the language model hidden states for that token are fed into a decoder block which assigns weights to OccamNet. The system feeds the most recent numbers from the text into OccamNet, which then evaluates the sparse function specified by its weights. The decoder then determines whether to use the LLM output or the OccamNet output.", "description": "This figure illustrates the architecture of the OccamLLM system.  The system combines a large language model (LLM) with a symbolic model called OccamNet to perform arithmetic operations. For each token in the input sequence, the LLM's hidden states are used by a decoder to assign weights to OccamNet. OccamNet then uses these weights and the relevant numbers from the input text to perform the calculation. Finally, a switch decides whether to output the LLM's prediction or OccamNet's result.", "section": "3.1 OccamLLM: Combining a Language Model with a Symbolic Model"}, {"figure_path": "vAOgaPvgYr/figures/figures_3_1.jpg", "caption": "Figure 1: The OccamLLM system. For each autoregressive step, the language model hidden states for that token are fed into a decoder block which assigns weights to OccamNet. The system feeds the most recent numbers from the text into OccamNet, which then evaluates the sparse function specified by its weights. The decoder then determines whether to use the LLM output or the OccamNet output.", "description": "This figure shows a schematic of the OccamLLM system architecture. It illustrates how the hidden states of a language model are used to control a symbolic model called OccamNet for arithmetic computations. The system takes the language model's hidden states for a given token as input. These states are fed into a decoder block, which assigns weights to OccamNet based on the task at hand (e.g., addition, subtraction, etc.). OccamNet receives numerical inputs from the text and, based on the weights assigned by the decoder, performs the computation. Finally, a decision is made to use either the output from OccamNet or the output from the language model for the next autoregressive step in text generation.", "section": "3.1 OccamLLM: Combining a Language Model with a Symbolic Model"}, {"figure_path": "vAOgaPvgYr/figures/figures_7_1.jpg", "caption": "Figure 3: Accuracy of OccamLlama and baselines on mathematical problem solving tasks. Higher is better. OccamLlama 8B achieves accuracy comparable to Llama 3 8B on benchmarks with simple arithmetic, higher accuracy than GPT 40 and GPT 40 + Code on on tasks with challenging arithmetic, and accuracy above Llama 3 8B and similar to GPT 3.5 Turbo on average. OccamLlama 70B outperforms GPT 40 and GPT 40 + Code on average.", "description": "This figure compares the accuracy of OccamLlama models (8B and 70B parameters) against several baseline LLMs (Llama 2 7B, Llama 3 8B and 70B, GPT 3.5 Turbo, GPT 4, and GPT 4 with code interpreter) on six mathematical problem-solving benchmarks (AddSub, GSM8K, MultiArith, MultiArith Float, MATH401, Single Eq, and SVAMP).  The results demonstrate that OccamLlama, especially the larger 70B parameter model, significantly outperforms the baselines, particularly on tasks involving more challenging arithmetic.", "section": "4 Experiments"}, {"figure_path": "vAOgaPvgYr/figures/figures_8_1.jpg", "caption": "Figure 1: The OccamLLM system. For each autoregressive step, the language model hidden states for that token are fed into a decoder block which assigns weights to OccamNet. The system feeds the most recent numbers from the text into OccamNet, which then evaluates the sparse function specified by its weights. The decoder then determines whether to use the LLM output or the OccamNet output.", "description": "This figure illustrates the architecture of the OccamLLM system.  It shows how the hidden states of a language model are used to control a symbolic model (OccamNet) for arithmetic operations. The system performs an autoregressive step, feeding the hidden states to a decoder that determines OccamNet's weights for the operation. The numbers from the input text are then fed to OccamNet, which computes the result. Finally, a decoder decides whether to use the LLM's output or OccamNet's result for the next token.", "section": "3 Methods"}, {"figure_path": "vAOgaPvgYr/figures/figures_22_1.jpg", "caption": "Figure 3: Accuracy of OccamLlama and baselines on mathematical problem solving tasks. Higher is better. OccamLlama 8B achieves accuracy comparable to Llama 3 8B on benchmarks with simple arithmetic, higher accuracy than GPT 40 and GPT 40 + Code on on tasks with challenging arithmetic, and accuracy above Llama 3 8B and similar to GPT 3.5 Turbo on average. OccamLlama 70B outperforms GPT 40 and GPT 40 + Code on average.", "description": "This figure compares the performance of OccamLlama (8B and 70B parameter versions), Llama 2 7B Chat, Llama 3 (8B and 70B parameter versions), GPT 3.5 Turbo, GPT 4, and GPT 4 with code interpreter on six mathematical problem-solving benchmarks.  It demonstrates OccamLlama's superior performance, particularly on tasks involving complex arithmetic.", "section": "4 Experiments"}, {"figure_path": "vAOgaPvgYr/figures/figures_24_1.jpg", "caption": "Figure 1: The OccamLLM system. For each autoregressive step, the language model hidden states for that token are fed into a decoder block which assigns weights to OccamNet. The system feeds the most recent numbers from the text into OccamNet, which then evaluates the sparse function specified by its weights. The decoder then determines whether to use the LLM output or the OccamNet output.", "description": "This figure illustrates the architecture of the OccamLLM system.  It shows how the hidden states of a language model (LLM) are used to control a symbolic model called OccamNet.  The LLM's hidden states are input to a decoder which assigns weights to OccamNet, determining which arithmetic operation OccamNet performs. Numbers from the text are passed to OccamNet, which computes the result. Finally, a decision is made whether to use the output from OccamNet or the LLM, selecting the most appropriate output for that step in text generation.", "section": "3 Methods"}, {"figure_path": "vAOgaPvgYr/figures/figures_26_1.jpg", "caption": "Figure 1: The OccamLLM system. For each autoregressive step, the language model hidden states for that token are fed into a decoder block which assigns weights to OccamNet. The system feeds the most recent numbers from the text into OccamNet, which then evaluates the sparse function specified by its weights. The decoder then determines whether to use the LLM output or the OccamNet output.", "description": "This figure illustrates the architecture of the OccamLLM system.  The system uses a language model (LLM) in conjunction with a symbolic model (OccamNet) to perform arithmetic calculations within a single autoregressive step. The LLM's hidden states influence the weights assigned to OccamNet, which then processes numerical inputs from the text. A decoder decides whether to use the LLM's output or OccamNet's output for the next token generation.", "section": "3.1 OccamLLM: Combining a Language Model with a Symbolic Model"}]