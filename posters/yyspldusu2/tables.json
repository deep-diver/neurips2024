[{"figure_path": "yySpldUsU2/tables/tables_32_1.jpg", "caption": "Table 1: Sharpness of solution at convergence. We train ResNet18 on CIFAR10 and measure the maximum Hessian eigenvalue \u03bbmax and the bulk spectra measured as \u03bbmax/15.", "description": "This table presents the sharpness of the solutions obtained by training ResNet18 on CIFAR10 using three different optimization methods: SGD, SGD+USEFUL, and SAM. Sharpness is measured using two metrics: the maximum Hessian eigenvalue (\u03bbmax) and the bulk spectrum (\u03bbmax/15). Lower values for both metrics generally indicate a flatter minimum, which is associated with better generalization performance.", "section": "5.4 USEFUL's Solution has Similar Properties to SAM"}, {"figure_path": "yySpldUsU2/tables/tables_32_2.jpg", "caption": "Table 2: Average scores for two clusters on CIFAR10.", "description": "This table presents a comparison of the average forgetting score, first learned iteration, and iteration learned for two clusters of examples in CIFAR-10: fast-learnable and slow-learnable. The forgetting score measures the frequency with which an example is misclassified after being correctly classified. The first learned iteration is the epoch when a model predicts the example correctly for the first time. Iteration learned is the epoch after which the model correctly predicts the example consistently.  The results show that fast-learnable examples have significantly lower forgetting scores and are learned much earlier than slow-learnable examples, indicating that the model learns these features more effectively.", "section": "D.3 SAM & USEFUL Reduce Forgetting Scores"}, {"figure_path": "yySpldUsU2/tables/tables_32_3.jpg", "caption": "Table 3: Average scores for two clusters on CIFAR100.", "description": "This table presents a comparison of metrics for two clusters of examples in the CIFAR100 dataset: fast-learnable and slow-learnable.  The metrics compared include the forgetting score (a measure of how frequently an example is misclassified during training), the first learned iteration (the first epoch an example is correctly classified), and the iteration learned (the epoch after which an example is consistently correctly classified).  The data shows that fast-learnable examples tend to have lower forgetting scores and are learned earlier in training.", "section": "D.3 SAM & USEFUL Reduce Forgetting Scores"}, {"figure_path": "yySpldUsU2/tables/tables_33_1.jpg", "caption": "Table 4: Test classification errors for training SAM and ASAM on the original CIFAR10 and modified datasets by USEFUL. Results are averaged over 3 seeds.", "description": "This table presents the test classification error rates achieved by training with SAM and ASAM (two different sharpness-aware minimization methods) on the original CIFAR-10 dataset and the dataset modified by the USEFUL method.  It shows the error rates for SAM and ASAM alone, and with the addition of USEFUL and/or TrivialAugment (TA). The results are averaged over three different random seeds to ensure reliability.", "section": "D.4 USEFUL generalizes to other SAM variants"}, {"figure_path": "yySpldUsU2/tables/tables_33_2.jpg", "caption": "Table 5: Comparison between USEFUL and upweighting loss regarding test classification errors. Upweighting loss doubled the loss for all examples in the slow-learnable clusters found by USEFUL, which is different from dynamically upweighting examples during the training [78]. Results are averaged over 3 seeds.", "description": "This table compares the test classification errors of using USEFUL and a baseline method called \"Upweighting Loss\".  Upweighting Loss is a method that doubles the loss for examples identified as slow-learnable by USEFUL.  The results show that USEFUL outperforms Upweighting Loss on both CIFAR10 and CIFAR100 datasets for both SGD and SAM optimizers.  The key difference is that USEFUL modifies the data distribution once at the beginning, while Upweighting Loss dynamically adjusts weights during training.", "section": "D.4 USEFUL generalizes to other SAM variants"}, {"figure_path": "yySpldUsU2/tables/tables_34_1.jpg", "caption": "Table 6: Test error on long-tailed CIFAR10. BALANCING means that we upsampled small classes to make the classes balanced. Results are averaged over 3 seeds.", "description": "This table presents the test error rates achieved by different training methods (SGD, SGD with USEFUL, SAM, SAM with USEFUL) on a long-tailed CIFAR10 dataset.  Two scenarios are compared: a 1:10 class imbalance ratio and a balanced dataset achieved by upsampling the smaller classes.  The results are averaged over three different random seeds for each training method to provide a measure of reliability.", "section": "D.6 USEFUL is also effective for noisy label data"}, {"figure_path": "yySpldUsU2/tables/tables_34_2.jpg", "caption": "Table 7: Test classification errors of SGD for different partition methods. Results are averaged over 3 seeds.", "description": "This table compares the test classification errors achieved by using three different methods to partition the data for training with SGD.  The methods compared are: Quantile, Misclassification, and the authors' proposed USEFUL method.  Results are reported for both CIFAR10 and CIFAR100 datasets and are averages across three independent experimental runs.  The table shows that USEFUL achieves the lowest test errors.", "section": "D.4 USEFUL generalizes to other SAM variants"}, {"figure_path": "yySpldUsU2/tables/tables_35_1.jpg", "caption": "Table 8: Test error on label noise CIFAR10 (all methods are with MixUp). Results are averaged over 3 seeds.", "description": "This table shows the test error rates achieved by different training methods on CIFAR10 datasets with label noise. The results are obtained using MixUp and averaged over three independent runs.  The methods compared include SGD, SGD with USEFUL, SAM, and SAM with USEFUL. Two noise rates are presented, 10% and 20%.  The table demonstrates how USEFUL improves the performance of both SGD and SAM in the presence of label noise.", "section": "D.6 USEFUL is also effective for noisy label data"}, {"figure_path": "yySpldUsU2/tables/tables_35_2.jpg", "caption": "Table 9: Test errors of different simplicity bias reduction methods on CIFAR10. Results are averaged over 3 seeds.", "description": "This table presents the test errors achieved by different simplicity bias reduction methods on the CIFAR-10 dataset. The results are averages over three independent trials, providing a measure of the methods' performance consistency.  The table compares the standard SGD approach against three other techniques: EIIL, JTT, and SGD+USEFUL.  Lower test errors indicate better performance in reducing simplicity bias and improving generalization.", "section": "D.7 Comparison with simplicity bias mitigation methods"}]