[{"figure_path": "pPeXYByHNd/figures/figures_1_1.jpg", "caption": "Figure 1: (a) The illustration of MSA and (b) performance comparisons between MSAGPT and advanced baselines on three natural MSA-scarce benchmark.", "description": "Figure 1(a) demonstrates a toy example of MSA, illustrating the co-evolutionary patterns within the MSA. This helps understand the relationships between amino acid sites that influence the folding structures. Figure 1(b) presents the overall performance comparison between MSAGPT and advanced baselines on three natural MSA-scarce benchmarks.  It highlights MSAGPT's superior performance in protein structure prediction tasks, especially when limited MSA is available.", "section": "1 Introduction"}, {"figure_path": "pPeXYByHNd/figures/figures_2_1.jpg", "caption": "Figure 2: The overall framework of prompting protein structure predictions via MSA generation. Left: The challenge faced by conventional search algorithms on protein with scarce homologous sequences, resulting in suboptimal alignments. Middle-to-Right: MSAGPT generates informative and high-quality MSA for such challenging queries, presenting a promising approach to overcoming these limitations. [M] denotes the sequence separator. [S], [E] are the special tokens to represent the start or end of MSA generation.", "description": "This figure illustrates the MSAGPT framework.  It starts by showing the problem of low-quality or missing MSAs for proteins with limited homologous sequences using conventional methods.  Then, it highlights how MSAGPT generates high-quality virtual MSAs using a 2D evolutionary positional encoding and 1D zero-/few-shot MSA decoding. Finally, it shows how these generated MSAs are integrated with an MSA-based protein structure prediction algorithm (like AlphaFold2) to improve the accuracy of structure predictions.", "section": "Methodology"}, {"figure_path": "pPeXYByHNd/figures/figures_4_1.jpg", "caption": "Figure 3: Comparisons among the axial attention (exemplified by [17]) and the one in MSAGPT in a single layer. Here we focus on the information aggregated to the AA \u201cG\u201d. The 2D evolutionary position enhanced attention shows higher efficiency than the decoupled axial attentions with one-step aggregation to attain sufficient information.", "description": "This figure compares the attention mechanisms used in MSAGPT and axial attention frameworks.  It illustrates how MSAGPT's 2D evolutionary positional encoding allows for more efficient information aggregation compared to the decoupled row-wise and column-wise attentions of axial attention methods. The example focuses on the information flow to amino acid 'G', showing how MSAGPT's approach incorporates both row and column information simultaneously, unlike the two-step approach in axial attention.", "section": "4.1 2D Evolutionary Positional Encoding"}, {"figure_path": "pPeXYByHNd/figures/figures_7_1.jpg", "caption": "Figure 4: The effect of different MSA depths and selection methods. The X-axis indicates the different MSA depths. The Y-axis represents the TM-Score. The dashed line denotes the non-selection baseline.", "description": "This figure presents two subfigures showing the effects of MSA depths and selection methods on the TM-Score of protein structure prediction. Subfigure (a) demonstrates that increasing the depth of MSA improves TM-Score up to a certain point after which it starts to decline. Subfigure (b) compares various MSA selection methods, with pLDDT selection showing the best performance.", "section": "6 Experiments"}, {"figure_path": "pPeXYByHNd/figures/figures_13_1.jpg", "caption": "Figure 6: The overall training pipeline and the illustration of preference dataset construction process for SFT and DPO learning stages.", "description": "This figure illustrates the three stages of the MSAGPT training pipeline: MSA generative pre-training, Rejective Fine-tuning (RFT), and Reinforcement Learning from AlphaFold2 Feedback (RLAF).  The preference data construction is highlighted, showing how high-quality MSAs are selected based on AlphaFold2 scores. The pipeline begins with MSA generative pre-training using a large dataset. This is followed by RFT, where the model is fine-tuned on a subset of high-quality MSAs selected using AlphaFold2. Finally, RLAF uses AlphaFold2 feedback to further refine the model\u2019s ability to generate informative MSAs. The diagram effectively visualizes the iterative refinement process that leads to improved MSA generation and ultimately better protein structure prediction.", "section": "5 The Training Pipeline of MSAGPT"}, {"figure_path": "pPeXYByHNd/figures/figures_15_1.jpg", "caption": "Figure 7: The length and depth distribution of the pre-training dataset.", "description": "This figure shows two histograms visualizing the distribution of the pre-training dataset used in MSAGPT. The left histogram displays the number of sequences (Num_Seq) in each MSA, showing a right-skewed distribution with a majority of MSAs having fewer than 200 sequences. The right histogram illustrates the length of sequences (Len_Seq) in the dataset, also showing a right-skewed distribution, with the majority of sequences having lengths less than 500 amino acids. This figure helps understand the characteristics of the dataset used to pre-train MSAGPT and its potential impact on the model's performance.", "section": "A Training Settings and Hyper-parameter Studies"}, {"figure_path": "pPeXYByHNd/figures/figures_15_2.jpg", "caption": "Figure 8: The correlation between total token length (the protein sequence length multiplied by the number of generated MSAs) and the inference time (minutes). In most cases (total token length < 20K), the generation time of MSAGPT is lower than the AF2 search pipeline requiring more than 30 minutes. The result shows MSAGPT can generate substantial sequence lengths within practical time, thus affirming its scalability and efficiency.", "description": "This figure demonstrates the relationship between the total length of tokens processed by the model and the time taken for generation.  It shows that MSAGPT is faster than alternative AlphaFold2 search methods for shorter sequences, highlighting its efficiency and scalability in generating long sequences.", "section": "A.4 Inference Efficiency"}, {"figure_path": "pPeXYByHNd/figures/figures_16_1.jpg", "caption": "Figure 4: The effect of different MSA depths and selection methods. The X-axis indicates the different MSA depths. The Y-axis represents the TM-Score. The dashed line denotes the non-selection baseline.", "description": "This figure demonstrates the impact of MSA depth and selection methods on the structure prediction accuracy using MSAGPT.  The left subplot shows that increasing the MSA depth to a certain extent improves accuracy, but excessive depth may negatively impact results.  The right subplot analyzes various MSA selection strategies, comparing their performance against a baseline with no selection. Different strategies are employed to select MSA based on criteria such as similarity or diversity, with results presented for both few-shot and zero-shot scenarios.", "section": "6.3 Rethinking the MSA Selection Strategy"}, {"figure_path": "pPeXYByHNd/figures/figures_18_1.jpg", "caption": "Figure 10: The pLDDT curves across different selection methods. Dashed red line represents using all generated sequences of a given depth. Solid lines represent selecting a subset of a given depth from 48 generated sequences with a specific strategy. The curves are smoothed using the Exponential Moving Average with alpha=0.3.", "description": "This figure displays the performance of different MSA selection strategies on the pLDDT metric. The x-axis represents the number of MSAs selected, and the y-axis represents the pLDDT score.  Various selection methods (Static Similarity, Static Diversity, Dynamic Similarity, Dynamic Diversity, Trimming, pTM, pLDDT, TM) are compared to a baseline of using all generated MSAs (dashed red line). The graph shows how the pLDDT score changes as the number of selected MSAs increases for each strategy, illustrating the effectiveness of different approaches in selecting high-quality MSAs.", "section": "C Selection Strategy Details and pLDDT Evaluation"}, {"figure_path": "pPeXYByHNd/figures/figures_19_1.jpg", "caption": "Figure 11: Visualization of improved structure prediction compared with baseline models. Yellow: Ground truth; Pink: Predictions based on MSA generated by MSAGPT; Blue: Predictions from MSA generated by EvoGen; Green: Predictions utilizing MSA generated by MSA-Augmenter.", "description": "This figure visually compares the protein structure prediction results of MSAGPT against three baseline methods (EvoGen, MSA-Augmenter, and AF2 using natural MSA).  It showcases the improved accuracy of MSAGPT across diverse protein structures, highlighting its ability to accurately predict both local and global structural features. The color-coding helps distinguish between the ground truth structure (yellow), MSAGPT predictions (pink), EvoGen predictions (blue), and MSA-Augmenter predictions (green). The TM-score is provided for each prediction to quantify the level of similarity to the ground truth structure.", "section": "6.2 MSAGPT's Virtual MSAs Reflect the Co-evolutionary Information"}, {"figure_path": "pPeXYByHNd/figures/figures_20_1.jpg", "caption": "Figure 11: Visualization of improved structure prediction compared with baseline models. Yellow: Ground truth; Pink: Predictions based on MSA generated by MSAGPT; Blue: Predictions from MSA generated by EvoGen; Green: Predictions utilizing MSA generated by MSA-Augmenter.", "description": "This figure compares the protein structure prediction results of MSAGPT against several baseline methods (EvoGen and MSA-Augmenter).  It uses three different colored structures to represent the ground truth structure, the structure predicted using MSAs generated by MSAGPT, and structures predicted using MSAs from EvoGen and MSA-Augmenter, respectively. The TM-score for each prediction is shown, illustrating the improved accuracy achieved by MSAGPT.", "section": "6.2 MSAGPT's Virtual MSAs Reflect the Co-evolutionary Information"}, {"figure_path": "pPeXYByHNd/figures/figures_21_1.jpg", "caption": "Figure 13: Visualization of improved structure prediction after DPO. Yellow: Ground truth; Blue: Predictions based on MSA generated by MSAGPT; Pink: Predictions based on MSA generated by MSAGPT-DPO.", "description": "This figure shows the improved protein structure prediction results after applying the DPO (Direct Preference Optimization) method. The DPO method is a reinforcement learning technique that leverages feedback from AlphaFold2. The figure compares protein structures predicted using MSAGPT alone (blue), MSAGPT with DPO (pink), and the ground truth (yellow). The improved accuracy of the structure predictions after DPO is evident in the close similarity between the pink (MSAGPT with DPO) and yellow (ground truth) structures.", "section": "6 Experiments"}, {"figure_path": "pPeXYByHNd/figures/figures_22_1.jpg", "caption": "Figure 14: Residue Distribution of Generated MSA for 7wme_A. The red box indicates natural MSA used as prompts during generation. The blue box indicates generated MSA. Residues are colored using the clustal scheme by Jalview.", "description": "This figure shows the residue distribution of generated MSA for protein 7wme_A using MSAGPT and MSAGPT with DPO.  The top half of the figure displays the MSA generated by MSAGPT. The bottom half shows the MSA generated by MSAGPT with DPO. The red boxes highlight the natural MSA used as input prompts. The blue boxes highlight the generated MSA. The color scheme (clustal by Jalview) indicates the level of conservation at each residue position. The figure visually demonstrates how the use of DPO improves MSA generation quality by comparing the distribution of residues in the MSAs generated by each method.", "section": "6.2 MSAGPT's Virtual MSAs Reflect the Co-evolutionary Information"}, {"figure_path": "pPeXYByHNd/figures/figures_22_2.jpg", "caption": "Figure 14: Residue Distribution of Generated MSA for 7wme_A. The red box indicates natural MSA used as prompts during generation. The blue box indicates generated MSA. Residues are colored using the clustal scheme by Jalview.", "description": "This figure shows the generated multiple sequence alignment (MSA) by MSAGPT and MSAGPT with DPO for protein 7wme_A.  The red box highlights the initial, natural MSA used as input to the model, while the blue box shows the generated MSA.  The bottom sections display conservation scores, quality scores, consensus sequences, and occupancy across the alignment, providing a visual representation of sequence similarity and variation within the generated MSA.", "section": "Experimental Settings"}, {"figure_path": "pPeXYByHNd/figures/figures_23_1.jpg", "caption": "Figure 14: Residue Distribution of Generated MSA for 7wme_A. The red box indicates natural MSA used as prompts during generation. The blue box indicates generated MSA. Residues are colored using the clustal scheme by Jalview.", "description": "This figure shows the residue distribution of generated MSAs for protein 7wme_A, comparing the results from MSAGPT and MSAGPT-DPO.  The red boxes highlight the natural MSAs used as prompts, while the blue boxes show the generated MSAs.  The color scheme (clustal by Jalview) helps visualize the similarities and differences in the amino acid sequences generated by the two models. This visualization is used to demonstrate the model's ability to generate informative and high-quality MSAs, crucial for accurate protein structure prediction. The difference between the two models likely reflects the effect of the DPO (Direct Preference Optimization) fine-tuning.", "section": "6.2 MSAGPT's Virtual MSAs Reflect the Co-evolutionary Information"}]