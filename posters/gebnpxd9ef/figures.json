[{"figure_path": "GEbnPxD9EF/figures/figures_5_1.jpg", "caption": "Figure 1: (a) Architecture of wide neural network for 4-dimensional output. The first (yellow) part approximates the distribution on different connected components of the support using the results from [43]. The width and depth of each block in this part are W1 and L1. The second (blue) part transforms the distributions on the unit cube to the distributions on the support. The width and depth of each block in the blue part are W2 and L2. The third (green) part is the Gumbel-Softmax layer. It combines the distributions on different connected components of the support together and outputs the final distribution. (b) This figure demonstrates the first two parts of our architecture. Each interval in the yellow box corresponds to one coordinate of input in the left figure. We first push forward uniform distributions to different cubes. Then, using Assumption 4, we adapt the shape of the support and push the measure from unit cubes to the original support of P(U). In this way, we can approximate complicated measures by pushing forward uniform variables.", "description": "This figure shows the architecture of a neural network used to approximate probability distributions.  The architecture consists of three parts: a wide neural network for approximating distributions on connected components of the support, a transformation to map distributions from unit cubes to the original support, and a Gumbel-Softmax layer to combine the distributions.  The right panel illustrates how the first two parts work by showing how a uniform distribution is pushed forward through the network to approximate a more complex distribution.", "section": "Approximation Error of Neural Causal Models"}, {"figure_path": "GEbnPxD9EF/figures/figures_13_1.jpg", "caption": "Figure 2: An SCM example.", "description": "This figure shows a simple example of a structural causal model (SCM).  The circles represent variables, and the arrows indicate causal relationships between them.  The solid arrows denote direct causal effects, while the dotted arrows show the presence of unobserved confounders (latent variables U1-U4) influencing multiple observed variables.  This model demonstrates a complex causal structure with both direct and indirect effects and unobserved confounding that requires more sophisticated methods for causal inference.", "section": "2 Preliminary"}, {"figure_path": "GEbnPxD9EF/figures/figures_13_2.jpg", "caption": "Figure 3: The causal graph of this SCM.", "description": "This figure shows a causal graph representing a structural causal model (SCM).  The nodes represent variables, and the arrows indicate the direction of causality.  The graph is used as an example within the paper to illustrate concepts related to causal inference and the construction of neural causal models (NCMs). Specifically, it demonstrates a setting where variables may be influenced by shared latent variables (represented by bi-directed edges), highlighting the complexity of causal relationships which neural methods aim to approximate.", "section": "2 Preliminary"}, {"figure_path": "GEbnPxD9EF/figures/figures_13_3.jpg", "caption": "Figure 4: The SCM after intervention V\u2081 = t.", "description": "This figure shows the structural causal model (SCM) after applying an intervention on variable V\u2081.  The intervention sets V\u2081 to a specific value, denoted as 't'.  The resulting graph shows how the other variables V\u2082, V\u2083, V\u2084, and V\u2085 are affected by this intervention, taking into account the causal relationships between them and the latent variables U\u2081, U\u2082, U\u2083, and U\u2084.  The dotted lines indicate the presence of unobserved confounding, signifying correlations between certain latent variables that influence the observed variables.", "section": "A Illustration of Notions in Section 2"}, {"figure_path": "GEbnPxD9EF/figures/figures_14_1.jpg", "caption": "Figure 3: The causal graph of this SCM.", "description": "This figure is a causal graph showing the relationships between variables V1, V2, V3, V4, V5, and latent variables E1, E2, E3 in a structural causal model (SCM).  Arrows represent direct causal influences; for instance, V2 causally influences V1 and V3.  The dotted lines indicate the presence of unobserved confounders; for example, E1 confounds V1 and V2. The graph illustrates the complex dependencies and confounding present in the SCM, essential for understanding the challenges of causal inference in this context.", "section": "2 Preliminary"}, {"figure_path": "GEbnPxD9EF/figures/figures_14_2.jpg", "caption": "Figure 5: The canonical representation of Figure 3.", "description": "This figure shows the canonical representation of the causal graph shown in Figure 3.  The canonical representation simplifies the SCM by using only one latent variable for each C2-component of the graph. This is done by merging the original latent variables to create new latent variables, each associated with a C2-component.  This representation is useful for approximating the SCM using neural networks, as each C2-component can be treated independently.", "section": "3 Approximation Error of Neural Causal Models"}, {"figure_path": "GEbnPxD9EF/figures/figures_15_1.jpg", "caption": "Figure 7: Example: two different SCMs with the same causal graph.", "description": "This figure shows two different structural causal models (SCMs) that share the same causal graph.  The left panel depicts an SCM where a single latent variable, U1, influences V1, V2, and V3. The right panel displays an SCM where there are three latent variables (U1, U2, U3). U2 and U3 each independently influence V2 and V3 while U1 influences all three variables.  The figure highlights that multiple SCMs can generate the same observational data distribution, even if they have different underlying causal structures and latent variable configurations.", "section": "B Proof of approximation Theorems"}, {"figure_path": "GEbnPxD9EF/figures/figures_19_1.jpg", "caption": "Figure 1: Architecture of wide neural network for 4-dimensional output. The first (yellow) part approximates the distribution on different connected components of the support using the results from [43]. The width and depth of each block in this part are W1 and L1. The second (blue) part transforms the distributions on the unit cube to the distributions on the support. The width and depth of each block in the blue part are W2 and L2. The third (green) part is the Gumbel-Softmax layer. It combines the distributions on different connected components of the support together and outputs the final distribution.", "description": "This figure illustrates the architecture of a wide neural network used to approximate a probability distribution. It's composed of three main parts: a space-filling curve component (yellow), a Lipschitz homeomorphism component (light blue), and a Gumbel-Softmax layer (light green). The space-filling curve part approximates the distribution across different connected components of the support. The Lipschitz homeomorphism part transforms the distributions into the original support. Finally, the Gumbel-Softmax layer combines these distributions to provide the final output distribution.", "section": "3 Approximation Error of Neural Causal Models"}, {"figure_path": "GEbnPxD9EF/figures/figures_20_1.jpg", "caption": "Figure 9: The construction of Hilbert curve.", "description": "This figure illustrates the construction of the Hilbert space-filling curve used in the proof of Proposition 1.  The left panel shows the first two steps of the iterative process, subdividing the unit square into smaller squares and tracing a continuous curve through them.  The colors represent the order in which squares are visited by the curve. The right panel shows a more advanced stage of this iterative process, illustrating how the curve fills the space.  The method adjusts the speed of the curve to match the target probability distribution P (the original distribution in Proposition 1) over the unit square, ultimately demonstrating that any distribution satisfying Assumption 5 can be represented as the push-forward of the Lebesgue measure on [0,1] by a H\u00f6lder continuous curve.", "section": "3.1 Approximating Mixed Distributions by Wide Neural Networks"}, {"figure_path": "GEbnPxD9EF/figures/figures_32_1.jpg", "caption": "Figure 10: Instrumental Variable (IV) graph. Z is the instrumental variable, T is the treatment and Y is the outcome.", "description": "This figure shows a causal graph representing an instrumental variable model.  Z is the instrumental variable, which affects the treatment variable T, but not the outcome variable Y directly.  T in turn influences Y.  There is also an unobserved confounder, U, affecting both T and Y, indicating a confounding effect that needs to be accounted for. This structure is common in causal inference scenarios where direct observation of the causal relationship between T and Y is confounded. The instrumental variable Z allows us to overcome the confounding effect by exploiting its independent relationship with T and its effect on the outcome variable only through T.", "section": "D.1 Discrete IV [14]"}, {"figure_path": "GEbnPxD9EF/figures/figures_34_1.jpg", "caption": "Figure 11: Comparison of Lipschitz regularized and unregularized neural causal algorithm. The two figures show the results of different architectures. The figure on the left side uses a medium-sized NN (width 128, depth 3) to approximate each structural function, while the right figure uses extremely small NNs (width 3, depth 1). In all experiments, we use the projected gradient to regularize the weight of the neural network. For each sample size, we repeat the experiment 5 times and take the average of the upper (lower) bound.", "description": "This figure compares the performance of Lipschitz regularized and unregularized neural causal algorithms for estimating average treatment effects (ATEs). Two different neural network architectures are used, one with medium size (width 128, depth 3) and another with small size (width 3, depth 1). For both architectures, the projected gradient method is employed for Lipschitz regularization.  The results are shown for various sample sizes, representing the average upper and lower bounds obtained from 5 repetitions for each size. The goal is to illustrate how Lipschitz regularization affects the accuracy and consistency of ATE estimation using neural causal models. ", "section": "4.1 Experiments"}]