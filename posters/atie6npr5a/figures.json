[{"figure_path": "atIE6Npr5A/figures/figures_1_1.jpg", "caption": "Figure 1: Few-shot concept learning. Given paired task demonstration \u0442 (e.g., 'walk') and concept c (a latent representation of the task), we train a generative model Go to generate behavior from a concept. Then, given demonstrations of a new behavior 7 (e.g., 'jumping jacks') without its concept label, we aim to learn its concept representation by optimizing concept \u010d as input to frozen Ge.", "description": "This figure illustrates the two-stage process of the Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM) method.  In the pretraining stage, a generative model (G\u03b8) is trained on paired task demonstrations (T) and their corresponding concept representations (C). This model learns to map concepts to behaviors.  In the few-shot learning stage, the pretrained model is frozen, and a new concept (\u010d) is learned by backpropagation.  This is done by optimizing the input concept (\u010d) to maximize the likelihood of generating new demonstrations (\u02dcT) of an unseen task.", "section": "Few-Shot Concept Learning"}, {"figure_path": "atIE6Npr5A/figures/figures_1_2.jpg", "caption": "Figure 2: Experiment Domains. We extensively evaluate our approach for various domains.", "description": "The figure shows five different domains used to evaluate the proposed Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM) method. These domains are: Object Rearrangement, Navigation to Goal, Motion Capture (MoCap), Autonomous Driving, and Real-world Table-Top Manipulation. Each domain presents unique challenges and requires the agent to learn different types of concepts from limited demonstrations.", "section": "1 Introduction"}, {"figure_path": "atIE6Npr5A/figures/figures_3_1.jpg", "caption": "Figure 3: Diverse learned concept generation. We generate versions of the new behavior conditioned on the learned concept and (1) new initial states and (2) composed with other concepts.", "description": "This figure illustrates the diverse concept generation capabilities of the proposed FTL-IGM method.  It shows how the learned model can generate variations of a new behavior by conditioning on the learned concept representation.  The top section demonstrates generation from new initial states, showcasing the model's ability to generalize to unseen starting points. The bottom section shows the composition of the learned concept with existing concepts from the training data, demonstrating the system's capability to create novel behaviors by combining learned knowledge.", "section": "4 Few-Shot Concept Learning Based on FTL-IGM"}, {"figure_path": "atIE6Npr5A/figures/figures_5_1.jpg", "caption": "Figure 4: Object rearrangement. Training concepts are single pairwise relations (\"A right of/above B\"), and new concepts are either compositions of training concepts (\"A right of/above B\" A \"B right of/above C\") or new relations (\"A diagonal to B\", \"A, B, C on circle circumference of radius r\").", "description": "This figure illustrates the different types of concepts used in the object rearrangement experiments.  Training concepts involve simple spatial relationships between two objects (e.g., \"A is to the right of B\").  New concepts are created by combining these basic relationships to form more complex configurations (composition) or introducing entirely new spatial arrangements not seen during training (novel concepts). The diagram visually represents examples of each concept type.", "section": "5.1 Task-Concepts"}, {"figure_path": "atIE6Npr5A/figures/figures_5_2.jpg", "caption": "Figure 5: Object rearrangement new concept qualitative evaluation. Learning the new concept \u2018square diagonal to triangle\u2019 and composing it with the training concept \u2018circle right of square\u2019.", "description": "This figure shows a qualitative evaluation of the model's ability to learn a new concept, \u2018square diagonal to triangle\u2019, and to compose this new concept with an existing training concept, \u2018circle right of square\u2019. The figure presents a series of object arrangements generated by the model, illustrating the successful composition and generation of novel behaviors based on the learned concepts.", "section": "Experiments"}, {"figure_path": "atIE6Npr5A/figures/figures_6_1.jpg", "caption": "Figure 6: Object rearrangement (left) and AGENT closed loop (right) quantitative evaluation on training and few-shot novel concept learning. Accuracy of FTL-IGM (ours), BC, VAE, and In-Context over concept generation of training concepts, novel compositions, novel concepts, and new initial states. We plot the average and standard error over new task types. Full details of the evaluation metrics appear in Appendix B and for baselines implementation in Appendix C.", "description": "This figure presents a quantitative comparison of the proposed FTL-IGM method against several baselines (BC, VAE, and In-Context) on two different tasks: object rearrangement and goal-oriented navigation.  The results showcase FTL-IGM's ability to generate novel concepts, handle novel compositions of concepts, and generalize to unseen initial states, outperforming the baselines in accuracy.  Error bars represent the standard error.", "section": "5 Experiments"}, {"figure_path": "atIE6Npr5A/figures/figures_7_1.jpg", "caption": "Figure 8: Autonomous driving. A controlled agent (green) completes various driving objectives as fast as possible while sharing the road with other vehicles (blue) and avoiding collisions. Training concepts: \u2018highway\u2019, \u2018exit\u2019, \u2018merge\u2019, and \u2018intersection\u2019, new concept: \u2018roundabout\u2019.", "description": "This figure shows examples of driving scenarios used in the autonomous driving experiments. The training scenarios include driving on a highway, exiting a highway, merging onto a highway, and navigating an intersection.  The new concept, shown on the far right, involves navigating a roundabout. The green vehicle represents the controlled agent, while blue vehicles represent other traffic. The goal is for the controlled agent to successfully complete each scenario while avoiding collisions.", "section": "5 Experiments"}, {"figure_path": "atIE6Npr5A/figures/figures_8_1.jpg", "caption": "Figure 6: Object rearrangement (left) and AGENT closed loop (right) quantitative evaluation on training and few-shot novel concept learning. Accuracy of FTL-IGM (ours), BC, VAE, and In-Context over concept generation of training concepts, novel compositions, novel concepts, and new initial states. We plot the average and standard error over new task types. Full details of the evaluation metrics appear in Appendix B and for baselines implementation in Appendix C.", "description": "This figure presents a quantitative comparison of the proposed FTL-IGM method against several baselines for few-shot concept learning.  The left panel shows results for object rearrangement, while the right panel shows results for the AGENT task.  The evaluation metrics assess the accuracy of concept generation across four scenarios: training concepts, novel compositions of training concepts, novel concepts, and new initial states.  Error bars represent the standard error, showcasing the performance consistency across different trials. The figure highlights that FTL-IGM achieves competitive performance compared to other baselines, particularly in generating novel concepts and handling new initial states.", "section": "5 Experiments"}, {"figure_path": "atIE6Npr5A/figures/figures_8_2.jpg", "caption": "Figure 10: Table-top manipulation. Training concepts: pick-and-place onto elevated surfaces and table-top pushing. New concept: pushing on an elevated surface.", "description": "This figure shows a real-world robotic manipulation experiment. The left side displays training concepts which include pick-and-place actions on various surfaces (book, table) and push actions on a flat surface. The right side shows a novel concept (pushing an object on an elevated surface\u2014a book).  It illustrates the ability of the model to generalize and adapt to new tasks based on its training.", "section": "5.1 Task-Concepts"}, {"figure_path": "atIE6Npr5A/figures/figures_9_1.jpg", "caption": "Figure 11: t-SNE embeddings of new concepts that are not explicit compositions of training concepts. See interactive version for detailed labels on our website.", "description": "This figure visualizes the t-SNE embeddings of the training concepts and the learned new concept components in three different domains: Object Rearrangement, MoCap, and Driving.  The visualization shows how the learned components (red dots) are positioned relative to the training concepts (blue dots).  The spatial proximity of the learned components to the training concepts suggests that the model is learning new concepts that build upon or are related to the existing concepts, even when these new concepts are not simply compositions of the training concepts. The interactive version on the website likely provides more specific labels to help clarify the meaning of individual data points within these visualizations.", "section": "5 Experiments"}, {"figure_path": "atIE6Npr5A/figures/figures_16_1.jpg", "caption": "Figure 12: Object rearrangement new compositions analysis qualitative evaluation. \u2018circle right of triangle triangle right of square\u2019 (top) each learned component corresponds to a single composed concept: \u2018circle right of triangle\u2019 (component 1) and \u2018triangle right of square\u2019 (component 2). In \u2018square right of circle / triangle above circle\u2019 (bottom), learned component 2 corresponds to both composed concepts and component 1 to none.", "description": "This figure shows a qualitative evaluation of the model's ability to learn new concepts that are compositions of existing concepts in the object rearrangement task.  The top row displays a successful decomposition where each learned component corresponds to one of the two component concepts. The bottom row shows a case where one component captures both component concepts, demonstrating that the model can achieve more complex compositions than a simple one-to-one mapping.", "section": "A Additional Results"}, {"figure_path": "atIE6Npr5A/figures/figures_16_2.jpg", "caption": "Figure 6: Object rearrangement (left) and AGENT closed loop (right) quantitative evaluation on training and few-shot novel concept learning. Accuracy of FTL-IGM (ours), BC, VAE, and In-Context over concept generation of training concepts, novel compositions, novel concepts, and new initial states. We plot the average and standard error over new task types. Full details of the evaluation metrics appear in Appendix B and for baselines implementation in Appendix C.", "description": "This figure presents a quantitative comparison of the proposed Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM) method against several baselines (BC, VAE, and In-Context) across two different tasks: object rearrangement and goal-oriented navigation.  The evaluation metrics assess the accuracy of concept generation for training concepts, novel compositions of training concepts, completely new concepts, and scenarios with new initial states.  The results, shown as average accuracy with standard error bars, demonstrate the superior performance of the FTL-IGM method across all scenarios.", "section": "5 Experiments"}, {"figure_path": "atIE6Npr5A/figures/figures_17_1.jpg", "caption": "Figure 14: New concepts that are training concept compositions. We display successful (green frames) and unsuccessful (red frames) states generated by our model conditioned on learned concepts that are training concept compositions.", "description": "This figure shows examples of object rearrangement scenarios generated by the model.  Each row represents a new concept that is actually a combination of training concepts. Green frames indicate successful generations matching the desired concept, while red frames indicate failures.  The figure visually demonstrates the model's ability to synthesize novel behavior by combining previously learned concepts.", "section": "4.3 Generating the learned concept"}, {"figure_path": "atIE6Npr5A/figures/figures_18_1.jpg", "caption": "Figure 15: New concepts that are not explicit training concept compositions. We display successful (green frames) and unsuccessful (red frames) states generated by our model conditioned on learned concepts that are not explicit training concept compositions.", "description": "This figure shows the results of generating states using a model trained on compositions of training concepts.  The model attempts to generate states corresponding to four new concepts that are *not* simply combinations of the training concepts. Green frames indicate successful generations, while red frames show unsuccessful attempts. This demonstrates the model's ability to learn new concepts beyond simple combinations of the training data.", "section": "4.2 Few-shot concept learning"}, {"figure_path": "atIE6Npr5A/figures/figures_19_1.jpg", "caption": "Figure 16: New concept composed with training concepts. We display successful (green frames) and unsuccessful (red frames) states generated by our model conditioned on a learned concept that is not an explicit training concept composition (\u2018square diagonal to triangle') in composition with various training concepts.", "description": "This figure demonstrates the model's ability to generate diverse trajectories by composing a newly learned concept with existing training concepts. The green frames show successful generations where the model successfully combines the new and old concepts. Red frames indicate unsuccessful attempts, highlighting the challenges in seamlessly integrating new and previously learned concepts.", "section": "4 Few-Shot Concept Learning Based on FTL-IGM"}]