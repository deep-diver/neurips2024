[{"type": "text", "text": "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Niels M\u00fcndler1, Mark Niklas M\u00fcller1,2, Jingxuan $\\mathbf{H}\\mathbf{e}^{1}$ , Martin Vechev1 ", "page_idx": 0}, {"type": "text", "text": "1 Department of Computer Science, ETH Zurich 2 LogicStar.ai ", "page_idx": 0}, {"type": "text", "text": "{niels.muendler, mark.mueller, jingxuan.he, martin.vechev}@inf.ethz.ch 1 mark@logicstar.ai ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests. We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-AGENT. We release all data and code at github.com/logic-star-ai/SWT-Bench. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As the complexity of software systems increases, rigorous testing is becoming more important than ever to ensure their reliability and correctness. However, while a large portion of these tests aims to reproduce previously reported issues (Kang et al., 2023), such issue reproduction is often disliked by professional developers (Straubinger & Fraser, 2023). Therefore, automatic generation of tests reproducing such issues from informal natural language descriptions is a promising path toward improving both code quality and developer productivity. Finally, generated tests can be leveraged as formal specifications to boost the effectiveness of automatic code repair tools (Chen et al., 2023a). ", "page_idx": 0}, {"type": "text", "text": "However, while automatic code generation, in particular using Code Agents, is an extremely active research area (i.e. Yang et al. (2024); Tao et al. (2024); Zhang et al. (2024); Bouzenia et al. (2024b); OpenDevin (2024); Bouzenia et al. (2024a); Sch\u00e4fer et al. (2024); Alshahwan et al. (2024a)), there is comparatively little work investigating automatic test generation directly. Indeed, while prior work has proposed methods based on symbolic execution (Lukasczyk & Fraser, 2022), specialized transformers (Tufano et al., 2020), and general-purpose LLMs (Li et al., 2023; Alshahwan et al., 2024b; Kang et al., 2023, 2024; Chen et al., 2023b), Code Agents have not been considered in this context, and even less work is applicable to the issue reproduction setting. Finally, large-scale, diverse test-generation datasets are lacking for Python, which is one of the most popular programming languages at the time of writing (TIOBE, 2024; PYPL, 2024) and a focus of Code Agent research. ", "page_idx": 0}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/bfd112aa59a2d174df63fcdf0f583a8e779977a7b034f0c8c270d5b48f6dba4b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Evaluation of an SWT-BENCH instance. Given an issue description in natural language and the corresponding codebase, the task is to generate tests that reproduce the issue. We considered a test to reproduce the issue if it fails on the codebase before the pull request (PR) is accepted, i.e., before the golden patch is applied, but passes after. We call this a fail-to-pass test $\\langle F\\!\\to\\!P$ ). ", "page_idx": 1}, {"type": "text", "text": "A Benchmark for Test Generation In this work, we propose SWT-BENCH, a novel and comprehensive dataset for test generation with the goal of issue reproduction in Python. SWT-BENCH contains over $1\\,900$ samples, each consisting of a GitHub issue, a golden code patch resolving the issue by adjusting the code, and a set of golden reference tests, obtained by transforming the popular SWE-BENCH (Jimenez et al., 2023) from code repair to test generation. We leverage the fact that any code repair task can be transformed into a test generation task, even in the absence of golden tests, by utilizing the golden code patch for evaluation. Concretely, for every generated test, we determine whether it reproduces the described issue, by checking whether it fails on the original repository but passes after the golden patch is applied. The golden reference tests, used in SWEBENCH for the evaluation of code repair performance, are solutions in this test generation setting. We illustrate this evaluation process of SWT-BENCH in Fig. 1. Further, we report the coverage of the code modified by the golden patch as a more fine-grained evaluation metric for generated tests. ", "page_idx": 1}, {"type": "text", "text": "Benchmarking Test Generation Methods We evaluate various existing test generation approaches on SWT-BENCH, including directly prompting state-of-the-art LLMs to generate tests for the given issue, a state-of-the-art issue reproduction method LIBRO (Kang et al., 2023), and different Code Agents adapted to the task of test generation (Yang et al., 2024; Zhang et al., 2024; Aider, 2024). Interestingly, we find that despite being designed for code repair, the Code Agent SWE-AGENT outperforms non-agent methods at test generation, both reproducing more issues and achieving higher coverage, and generally find all agents to perform strongly in both areas. However, we still observe significant complementarity between the different approaches, with an ideal ensemble of the best four methods solving $71\\%$ more samples than the best single method. Further, while the performance on code repair and test generation is generally correlated, this does not hold on a per-sample basis. This indicates that reproducing an issue with a test and fixing this issue are distinct tasks of different difficulty. Finally, we find that generated tests can serve as a strong signal for the correctness of proposed code fixes, with SWE-AGENT achieving over twice the precision on fixes that pass self-generated tests that failed before the fix was applied. ", "page_idx": 1}, {"type": "text", "text": "Key Contributions Our key contributions are: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce SWT-BENCH, a new benchmark for test-based issue reproduction based on an extensive dataset of real-world software repositories, user issues, code patches, and test cases (\u00a73). \u2022 We propose to adapt Code Agents to the task of test generation for issue reproduction (\u00a74). \u2022 We provide an extensive evaluation of SWT-BENCH, and demonstrate that, while issue reproduction is generally hard, Code Agents perform well, outperforming prior methods (\u00a75). ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Code Datasets Over recent years, a variety of code datasets such as HumanEval (Chen et al., 2021), APPS (Hendrycks et al., 2021), and MBPP (Austin et al., 2021) have been proposed to assess the capabilities of code synthesis and repair systems (Lin et al., 2017; Li et al., 2022). However, these largely focus on interview-style coding challenges or function-level code synthesis and do not capture the complexity of real-world codebases. Further, they have been shown to often include insufficient test cases to properly assess the correctness of the generated code (Liu et al., 2023a). ", "page_idx": 1}, {"type": "text", "text": "Recently, a range of repository-level code-generation benchmarks (Liu et al., 2023b; Jain et al., 2024) including the popular SWE-BENCH (Jimenez et al., 2023) have emerged, as modern LLMs began to saturate the simpler function-level benchmarks. However, none of these benchmarks were designed to assess test generation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The only dataset for reproducing bugs based on real-world issues, Defects4J (Just et al., 2014), focuses on Java, is outdated, limited in size, and contains only short bug descriptions rather than detailed issue reports. In contrast, SWT-BENCH is based on Python, which is better supported by modern Code Agents, contains more recent issue reports, and is significantly larger. ", "page_idx": 2}, {"type": "text", "text": "Automated Unit Test Generation Many approaches have been suggested to automate (unit) test generation leveraging symbolic execution (Lukasczyk & Fraser, 2022), specialized transformers (Tufano et al., 2020), and general purpose LLMs (Li et al., 2023; Alshahwan et al., 2024b; Kang et al., 2023; Tufano et al., 2020; Kang et al., 2024; Sch\u00e4fer et al., 2024; Alshahwan et al., 2024a; Chen et al., 2023b). Depending on their focus, they can be used to increase test coverage (Alshahwan et al., 2024b; Sch\u00e4fer et al., 2024), find edge cases (Lukasczyk & Fraser, 2022), or reproduce reported issues (Kang et al., 2023). Issue-reproducing tests are especially interesting, as they can be used to validate automatically generated code repair candidates and thus improve the precision of code repair systems (Chen et al., 2023a). However, most test-generation approaches are not applicable to issue reproduction. We therefore evaluate the most recent applicable method, LIBRO (Kang et al., 2023), and a range of other LLM-based baselines on SWT-BENCH. ", "page_idx": 2}, {"type": "text", "text": "Code Agents Over the last year, LLMs have been equipped with tools to observe and interact with their environment over multiple turns and preserve a state across these turns (Wang et al., 2024). These so-called agents have proven successful on a range of complex tasks, including code repair and synthesis (Bouzenia et al., 2024b; OpenDevin, 2024; Zhang et al., 2024; Yang et al., 2024; Tao et al., 2024; Bouzenia et al., 2024a; Aider, 2024). Such Code Agents can typically search, read, and edit code using an agent computer interface (ACI) (Yang et al., 2024). In this work, we leverage such Code Agents for generating issue-reproducing tests by changing their instructions. ", "page_idx": 2}, {"type": "text", "text": "3 Benchmarking Test Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we outline the structure of the proposed benchmark, SWT-BENCH, and how we leverage it to measure the capabilities of LLMs and Code Agents for test generation. ", "page_idx": 2}, {"type": "text", "text": "3.1 Notation and Definitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first introduce the notation to describe codebases, their test suites, and changes to these codebases in the form of patches. We denote a codebase $R$ after applying patch $X$ as $R\\circ X$ . Several patches can be applied sequentially, i.e. $R\\circ X\\circ Y$ is the codebase $R$ after applying a first patch $X$ and then a second one $Y$ . When a code patch $X$ is applied to $R$ , a set of tests $T$ can be used to check the correctness of the applied patch. ", "page_idx": 2}, {"type": "text", "text": "A single test $s$ can either pass $({\\bf P})$ or fail (F) after we execute it within the context of codebase $R$ . We consider a test to fail if an error is thrown during its execution, e.g., an AssertionError or ValueError. Such test errors frequently occur if $R$ lacks or misimplements the functionality targeted by the test. They can also occur due to other reasons, such as incorrect syntax or formatting of the test $s$ . Conversely, a test passes when running the test triggers no error. We define this process as an execution function: $\\mathrm{exec}(s,R)\\in\\{P,F\\}$ . ", "page_idx": 2}, {"type": "text", "text": "We consider a test $s$ to reproduce a described issue $I$ of $R$ , which is resolved by patch $X$ if it fails on the original codebase (i.e. $\\mathrm{exec}(s,R)=F)$ but passes on the patched codebase (i.e. $\\mathrm{exec}(s,R\\circ$ $X)=P)$ ). We denote these fail-to-pass tests with $F\\rightarrow P$ and define $F\\rightarrow F$ , $P\\rightarrow P$ , and $P\\rightarrow F$ tests similarly. If a test transitions from failing on $R$ to any state on $R\\circ X$ , we denote it as $F\\!\\to\\!\\times$ and vice versa for $\\times\\to F$ . Further, we consider a set of tests $T$ to be successful at reproducing the issue $I$ , if it contains at least one $F\\rightarrow P$ test and no $\\times\\to F$ test, or equivalently $\\exists s\\in T,\\mathrm{exec}(s,R)=$ $F\\wedge\\forall s\\in T$ , $\\operatorname{exec}(s,R\\circ X)=P$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Benchmark Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To construct SWT-BENCH, we leverage the same underlying data as SWE-BENCH (Jimenez et al., 2023) and summarize its three-stage construction process here for completeness. ", "page_idx": 2}, {"type": "text", "text": "1. Scrape a total of $\\sim\\!90\\,000$ pull requests (PRs) from 12 popular open-source Python repositories from GitHub. ", "page_idx": 3}, {"type": "text", "text": "2. Filter PRs to only include those that were merged, resolved a GitHub issue, and made changes to at least one test file.   \n3. Filter PRs to feature at least one $F\\rightarrow P$ test, removing PRs that result in installation or runtime errors. ", "page_idx": 3}, {"type": "text", "text": "This results in 2 294 task instances, each consisting of a GitHub issue, a golden patch $X^{*}$ fixing the issue, and a set of golden reference tests T \u2217. Table 1: Characterization of different attributes of However, we find that for 311 instances, the SWT-BENCH instance. ", "page_idx": 3}, {"type": "text", "text": "golden patch can not be evaluated without errors or does not fix the described issue reliably, i.e., some tests of $T^{*}$ fail on $R\\circ X^{*}$ . The main reasons are flaky test suites, e.g., django cases where HTTP requests sometimes return 500 Internal Server Error although the related code was not changed, erroneous test suite setup, e.g., the test suite tool tox not allowing external tools invoked in the sphinx setup, and time outs, e.g., when slow tests in the sympy library are run. We exclude these, leaving a total of $1\\,983$ instances in SWT-BENCH. To enable cheaper evaluation we create SWT-BENCHLITE, a subset of 276 issues, corresponding to SWE-BENCH-LITE. ", "page_idx": 3}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/ebf4fcc4679cb4496111da1763f3e2e1528070731144bea09f62fae24c72527d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "We summarize key statistics of SWT-BENCH in Table 1 and show its repository composition in Fig. 2. While issue descriptions are on average only 318 words long, the longest one reaches 8 756 words. Generally, repository complexity is high with on average over 1 500 files and over 300 000 lines of code. Many repositories feature large test suites of $>120$ and up to $4\\,800$ tests, already covering $70\\%$ of the lines in the to-be-patched code. Most of these existing tests are unaffected by the golden patch with basically no $F\\!\\rightarrow P$ and only 1.5 $P\\rightarrow F$ tests on average. The golden tests remove on average 0.3 tests and add another 2.9 new test cases, of which roughly twothirds are $F\\rightarrow P$ . The test patches edit on average 31.8 lines in 1-2 files. Due to the filtering for unresolved issues during dataset curation, no golden tests are $F\\!\\rightarrow\\!F$ or $P\\!\\rightarrow\\!F$ . ", "page_idx": 3}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/af2600e62bbe38effe8a6bc3bd4dfcca0f775c1727b09c51c90cf40eb3c49140.jpg", "img_caption": ["Figure 2: Distribution of SWT-BENCH instances over GitHub repositories. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.3 Metrics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose two main metrics to evaluate the test generation performance of any method; Success rate $(S)$ and change coverage $(\\mathcal{C})$ , described below. We further introduce the necessary but insufficient property of patch well-formedness $(\\mathcal{W})$ . ", "page_idx": 3}, {"type": "text", "text": "Success Rate The success rate $\\boldsymbol{S}$ measures the portion of instances where the generated tests $T$ reproduced the issue according to the definition in $\\S3.1$ , i.e. at least one test in $T$ transitions from failing to passing and none fail after applying the patch. This is the most important performance measure, as the presence of $F\\rightarrow P$ and the absence of $\\times\\rightarrow F$ tests are key for test-driven development and automatic code generation. We further report the portion of instances for which at least one Fail-to-Pass $\\langle F\\rightarrow P\\rangle$ ), Fail-to-Any $(F\\to\\times)$ , and Pass-to-Pass $\\left(P\\rightarrow P\\right)$ ) was generated. While $F\\!\\to\\!\\times$ tests, i.e., all tests that fail on the original codebase, are not necessarily desirable, only $F\\!\\to\\!\\times$ tests can result in the reproducing $F\\!\\rightarrow\\!P$ test, whereas $P\\to\\times$ tests can never reproduce an issue. As $F\\!\\to\\!\\times$ can further be identified without knowledge of the golden code patch, generation methods can aim to always produce an $F\\to\\times$ test. Finally, $P\\rightarrow P$ tests indicate that the model generated well-formed and valid, but unrelated tests. ", "page_idx": 3}, {"type": "text", "text": "Change Coverage Coverage is an important metric to determine what portion of a codebase is tested. While path coverage measures this optimally, the exponential number of paths makes it infeasible in practice. We thus follow common practice, and instead measure line coverage. As we aim to specifically test the code described in the issue text, we consider only the coverage of the changes made by the golden code patch. Further, we observe that patches may include portions of non-executable lines, e.g. documentation or configuration files, and exclude them. Specifically, we consider all lines that are executed by the original test suite $S$ or the golden test suite $T^{*}$ on both $R$ and $R\\circ X^{*}$ to be executable and track coverage of such executable lines. ", "page_idx": 4}, {"type": "text", "text": "Finally, we consider both the coverage of removed (including modified) lines of code in the original codebase and added (including modified) lines of code in the patched codebase, illustrated in Fig. 3. ", "page_idx": 4}, {"type": "text", "text": "Formally, given the number of times ${\\mathcal{C}}_{S}^{R}(l)\\,\\in\\,\\mathbb{Z}^{\\geq0}$ a specific line of code $l$ was executed when running the test suite $S$ on codebase $R$ , we define the executable lines of the patch $X$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{X}_{r}^{*}=\\{l\\in X_{r}\\mid\\mathcal{C}_{S}^{R}(l)+\\mathcal{C}_{S\\cup T^{*}}^{R}(l)>0\\}}\\\\ &{\\mathcal{X}_{a}^{*}=\\{l\\in X_{a}\\mid\\mathcal{C}_{S}^{R o X}(l)+\\mathcal{C}_{S\\cup T^{*}}^{R o X}(l)>0\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $X_{r}$ and $X_{a}$ are the lines added and removed by patch $X$ , respectively, and $T^{*}$ are the golden tests. Finally, we obtain the change coverage of the generated tests $T$ as ", "page_idx": 4}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/f4363b2fa447be9c0d2f525e1408e659955ff6b082fb57b0da5841c74f3f15ae.jpg", "img_caption": ["Figure 3: Illustration of change coverage $\\Delta\\mathcal{C}$ of the generated tests $T$ , given the original code base $R$ , the golden patch $X^{*}$ , and the golden tests $T^{*}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{T}^{X}=\\frac{\\left|\\left\\{l\\in\\mathcal{X}_{r}^{*}\\mid\\mathcal{C}_{S\\cup T}^{R}(l)>\\mathcal{C}_{S}^{R}(l)\\right\\}\\right|+\\left|\\left\\{l\\in\\mathcal{X}_{a}^{*}\\mid\\mathcal{C}_{S\\cup T}^{R\\circ X}(l)>\\mathcal{C}_{S}^{R\\circ X}(l)\\right\\}\\right|}{\\left|\\mathcal{X}_{r}^{*}\\right|+\\left|\\mathcal{X}_{a}^{*}\\right|}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $X$ and $T$ are clear from context, we drop them for notational clarity. If none of the lines modified by the golden patch $X$ are executed by any test, i.e., $|\\mathcal{X}_{r}^{*}|+|\\mathcal{X}_{a}^{*}|=0$ , we exclude this instance from our coverage analysis $1\\%$ of cases). ", "page_idx": 4}, {"type": "text", "text": "Patch Well-Formedness Many LLMs struggle to generate well-formed code patch files (Jimenez et al., 2023) and the methods we investigate employ different approaches to mitigate this issue. To assess them, we additionally measure the patch applicability $\\mathcal{W}$ as the portion of instances for which a well-formed patch was generated. We define $\\mathcal{W}$ as the portion of instances for which the generated patch $X$ can be applied to the original codebase $R$ without errors. Since well-formedness is necessary for any test to be executed, it always exceeds $\\boldsymbol{S}$ , $F\\!\\rightarrow\\!P$ , and related rates. ", "page_idx": 4}, {"type": "text", "text": "4 Automatic Test Generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first discuss how the test generation task differs from code repair, before introducing a novel code diff format based on these insights that is optimized for fault tolerance. Finally, we propose a range of test generation methods based on directly querying LLMs and leveraging Code Agents. ", "page_idx": 4}, {"type": "text", "text": "4.1 Test Generation vs Code Repair ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Automatic test generation is closely related to code repair: Instead of predicting a patch $X$ that fixes the described issue and is then evaluated using a golden test $T^{*}$ , we aim to predict reproducing tests $T$ which are then evaluated on both the original state of the codebase $R$ and the state after applying the golden code patch $X^{*}$ . However, there are some key differences between the two tasks: First, adapting an existing test suite to reproduce an issue typically only requires adding new tests. Concretely, $7\\bar{1}\\%$ of golden tests in SWT-BENCH only add new test functions, with another $28\\%$ modifying existing functions, and only $1\\%$ removing functions. Second, testing permits and requires a more granular analysis. While fixed code is either correct and passes all test cases or incorrect when failing any of them, generated tests can be correct but irrelevant to the issue $(P\\!\\to\\!P)$ , call relevant code but fail to expose the precise bug (increase in coverage), reproduce the issue with varying comprehensiveness on edge cases ( $F\\!\\rightarrow\\!P$ , with varying coverage), or fail in different ways. ", "page_idx": 4}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/a8913de56de30eac2c2f85d0f6941e6278911e28e3d33e7b12ab0ca0feac62ad.jpg", "img_caption": [], "img_footnote": ["Figure 4: Comparison of the default unified diff format (left) and our fault-tolerant version (right). "], "page_idx": 5}, {"type": "text", "text": "4.2 A Code Diff Format for Automatic Test Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Code changes are typically represented in the unified diff format, i.e., in the git patch and diff format. While using this format to represent code changes is both precise and human-readable, it is very sensitive to misspecifications, requiring, e.g., the exact line numbers of code changes to be specified and specific code snippets (including all to-be-changed lines) to be repeated verbatim. As a result, many LLMs struggle to produce well-formed patch files (Jimenez et al., 2023). Even when loosening the strict diff requirements and fuzzy-matching the generated diff to a best-fit part of the code, GPT-4 only succeeded in $48\\%$ of cases, resulting in only 10 correctly reproduced issues. ", "page_idx": 5}, {"type": "text", "text": "To alleviate this issue, we propose an adjusted patch format optimized for LLM generation that is easier to adhere to and more robust. Specifically, our custom diff format allows entire functions or classes to be inserted, replaced, or deleted, given the full function or class definition and (faulttolerant) location in the code. We show an example in Fig. 4, comparing it to the unified diff format. Based on whether the model wants to rewrite an existing function or insert a new function, the provided code is then substituted or inserted at the code location. This format is particularly well suited for test generation which usually only requires adding test functions. We provide a more formal description of this format in App. A and demonstrate its effectiveness in $\\S5$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 Direct LLM Generation of Tests ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We consider four baselines for test generation: Direct zero-shot prompting with the unified patch format (ZEROSHOT), zero-shot prompting with our novel patch format (ZEROSHOTPLUS), selecting the best out of 5 patches using an oracle $(\\mathbf{P}\\mathrm{ASS}\\,\\ @\\,5)$ ), and the state-of-the-art test generation method, LIBRO (Kang et al., 2023), which uses a range of heuristics to pick the most promising among multiple generated tests. In all methods, the LLM is instructed to add tests to reproduce and cover the described issue in the codebase. We describe these methods below, deferring further details to App. E. ", "page_idx": 5}, {"type": "text", "text": "ZEROSHOT prompts the model with the issue description, a subset of the codebase retrieved using BM-25 (Robertson & Zaragoza, 2009), and instructions to generate a patch file in unified diff format. This method corresponds to the LLM-only baseline in SWE-BENCH (Jimenez et al., 2023). ", "page_idx": 5}, {"type": "text", "text": "ZEROSHOTPLUS is similar to ZEROSHOT but leverages our custom diff format, discussed in $\\S4.2$ , which is optimized for LLMs and robustness to minor specification errors. ", "page_idx": 5}, {"type": "text", "text": "$\\mathrm{PASS}\\,(\\alpha\\,5\\$ uses our ZEROSHOTPLUS prompting scheme to generate 5 proposal tests and then uses an oracle to pick the best one. While this is of course not practical in a real-world setting, it allows us to assess the potential of the LLM to generate good test cases given an effective selection mechanism. ", "page_idx": 5}, {"type": "text", "text": "LIBRO (Kang et al., 2023), is the current state-of-the-art for LLM-based test generation. Similar to $\\mathrm{PASS}\\,(\\alpha\\,5\\$ it generates multiple proposal tests using ZEROSHOTPLUS prompting. However, instead of using an oracle, it combines multiple heuristics to select the best test cases. In particular, it runs all generated tests and then selects the one inducing an error that is most similar to the problem description. This permits not only checking whether a generated diff is well-formed and the proposed test fails on the original codebase but also selecting the most relevant test case. As LIBRO was originally proposed for Java, we adapt it to our Python setting, as detailed in App. B. ", "page_idx": 5}, {"type": "text", "text": "4.4 Code Agents for Test Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "LLM-based agents are systems that take actions based on LLM-generated text, providing tools to observe and interact with their environment over multiple turns and preserve some state across these turns. In the case of Code Agents, they can typically search, read, and edit code using an agent computer interface (ACI) (Yang et al., 2024). Recent work has shown that such Code Agents are particularly effective for complex repository-level code synthesis and repair tasks, outperforming unaided LLMs by a significant margin (Bouzenia et al., 2024b; OpenDevin, 2024; Zhang et al., 2024; Yang et al., 2024; Tao et al., 2024). In this work, we leverage Code Agents for automatic test generation by adjusting their instructions. Specifically, we adapt SWE-AGENT (Yang et al., 2024), AIDER (Aider, 2024), and AUTOCODEROVER (Zhang et al., 2024). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "SWE-AGENT (Yang et al., 2024) provides the LLM direct access to (augmented) command line tools and processes the output of these tools to be more easily parseable by an LLM. In particular, they provide special tools for searching, viewing, and editing files. Beyond initial instructions, they provide little guardrails or structure for the LLM and let it interact with a limited shell environment. ", "page_idx": 6}, {"type": "text", "text": "AIDER (Aider, 2024) performs a repository indexing step to guide file selection and then includes all selected files in the next prompts. Further, model-generated summaries and reflections on previous actions are leveraged to augment the context. Before an edit is applied, it undergoes validation via static analysis and repository test cases using project-specific evaluation harnesses. ", "page_idx": 6}, {"type": "text", "text": "AUTOCODEROVER (Zhang et al., 2024) separates the code repair task into two distinct stages. In the first stage, the LLM is tasked with collecting all required context for the task at hand. To this end, it is equipped with a range of advanced code search and navigation tools, allowing it, e.g., to retrieve class signatures, function definitions, or surrounding code snippets. Once the LLM believes it has gathered sufficient context, it proceeds to the second stage. There, the LLM is tasked with generating the actual code patch in a single step, retrying only if the patch can not be applied. ", "page_idx": 6}, {"type": "text", "text": "Adapting Code Agents for Test Generation As SWE-AGENT, AIDER, and AUTOCODEROVER were designed for program repair, we adapt their system and instruction prompts to focus on creating high-quality test cases. We find that the underlying LLMs are capable of following these changed instructions and successfully generate test cases for up to $87\\%$ of issues. Typically, the instruction changes were as simple as replacing phrases like \"solve the issue\" with \"create unit tests that cover the issue\". We provide a more detailed description of the used prompts in App. E. ", "page_idx": 6}, {"type": "text", "text": "We experiment with instructing SWE-AGENT explicitly to execute the generated test cases before submitting them. We call this variant SWE-AGENT $^+$ and find that this increases the success rate $\\boldsymbol{S}$ from $15.9\\%$ to $18.5\\%$ (see Table 2). Note we do not provide any information on how to run the tests. This contrasts the LIBRO setting, in which the test execution commands are assumed to be known. ", "page_idx": 6}, {"type": "text", "text": "5 Experimental Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We leverage SWT-BENCH to compare the performance of different test generation methods and underlying LLMs (\u00a75.2), their relation with the code repair setting (\u00a75.3), and the impact of instance characteristics (\u00a75.4). We further explore hyperparameter ablations in App. C. ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider GPT-4 (gpt-4-1106-preview OpenAI 2023), GPT-4o mini (gpt-4o-mini-2024-07-18 OpenAI 2024), Claude 3.0 Haiku (Anthropic, 2023), Claude 3.5 Sonnet (Anthropic, 2024), Mistral Large 2 (Team, 2024b) (served via the Mistral AI API), and Mixtral $7\\mathrm{x}22\\mathrm{b}$ (Team 2024a served by Together AI TogetherAI 2023), as underlying LLMs, using GPT-4 unless indicated otherwise. We sample at temperature $t\\,=\\,0$ for all zero-shot methods and agents and at $t\\,=\\,0.7$ for LIBRO and $\\mathrm{PASS}\\,{\\ @\\,5}$ . For SWE-AGENT, AIDER, and AUTOCODEROVER, we use their default settings, restricting the number of API calls to 20, reflection steps to 3, and interaction rounds to 10, respectively. For LIBRO we sample 5 tests. Due to budget constraints, we focus our evaluation on SWT-BENCH-LITE. In App. C we explore and justify this choice of hyperparameters in detail. ", "page_idx": 6}, {"type": "text", "text": "5.2 Automatic Test Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Comparing Test Generation Methods We compare test generation performance in Table 2 where all methods have access only to the issue description and the original codebase. We observe that using the original git code-diff format, ZEROSHOT only generates well-formed patches for $48.6\\%$ of issues. Using our novel test-specific code-diff format (ZEROSHOTPLUS) boosts this rate to $89.5\\%$ yielding an almost $3\\mathrm{x}$ increase in success rate $(S)$ to $9.4\\%$ . While picking the best among five generated tests $(\\operatorname{PASS}\\ @5)$ even yields an $\\boldsymbol{S}$ of $20.3\\%$ , the heuristics employed by LIBRO can only convert about half of this gap into an $\\boldsymbol{S}$ of $14.1\\%$ , still beating AUTOCODEROVER and AIDER which achieve an $\\boldsymbol{S}$ of $9.1\\%$ and $12.7\\%$ respectively. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "SWE-AGENT, however, outperforms LIBRO at an $\\boldsymbol{S}$ of $15.9\\%$ , increased to $18.5\\%$ , when instructed to check its generated tests (SWE-AGENT+). This stronger performance is significant at $p<0.1\\%$ . Interestingly, SWE-AGENT produces fewer $F\\!\\to\\!x$ tests than AIDER and LIBRO despite having much higher applicability and yielding a higher $\\boldsymbol{S}$ . ", "page_idx": 7}, {"type": "text", "text": "We conclude that general-purpose Code Agents already perform as well as domain-specific test generation methods, with simple test-specific adjustments providing significant improvements. ", "page_idx": 7}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/e9390f61622babcaf43f2af1d0679f1d229cab1192fd9a9fd6f03f08ad7ab938.jpg", "table_caption": ["Table 2: Rate of well-formed patches $(\\mathcal{W})$ , successful tests $(S)$ , potentially reproducing initially failing tests $(F\\to\\times)$ , reproducing fail-to-pass tests $(F\\rightarrow P)$ ), and correct but unhelpful pass-to-pass tests $(P\\!\\to\\!P)$ ), in $\\%$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Coverage of Generated Tests We analyze the change coverage $\\Delta\\mathcal{C}$ of the generated tests, i.e., the portion of executable golden patch code that is covered by the generated tests, in Table 3. Across all methods, we observe a significantly higher coverage on successful instances $(\\Delta\\mathcal{C}^{S})$ , indicating that coverage is indeed correlated with test quality but more granular than $\\boldsymbol{S}$ . Interestingly, SWE-AGENT+ achieves notably higher coverage on successful instances than SWE-AGENT highlighting the impact of providing agents with more test-generation-specific tools to identify promising tests. Further, LIBRO achieves lower coverage than most Code Agents, most likely as a consequence of preferring shorter tests. ", "page_idx": 7}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/7327b53aacb9fe2be0b217f0fc09eddcd7d8249bad78f7d9d9ca13ef2ed65546.jpg", "table_caption": ["Table 3: Change Coverage $\\Delta{\\mathcal{C}\\ [\\%]}$ as defined in $\\S3.3$ aggregated over all instances, $\\boldsymbol{S}$ instances and non $\\boldsymbol{S}$ instances $(\\neg{S})$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Model Effect We compare the effect of different underlying LLMs for SWEAGENT in Table 4. We observe that not only $\\boldsymbol{S}$ but even applicability $(\\mathcal{W})$ is highly sensitive to the underlying LLM\u2019s performance, with Haiku, GPT-4o mini, and Mixtral achieving significantly lower performance than GPT-4. More capable models like Claude 3.5 Sonnet and Mistral Large 2 perform on par, with the latter even outperforming GPT-4. ", "page_idx": 7}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/472f5a5083553e41645157fa24b8f2dbc7b7a72cce50951fed4f39178536ffa1.jpg", "table_caption": ["Table 4: Comparison of different underlying LLMs for SWE-AGENT, all in $\\%$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Code Repair and Test Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Test Generation for a Given Code Patch To assess the effectiveness of automatic test generation at testing specific, provided fixes, we investigate the effect of providing a (possibly incorrect) code patch, the files it changed, and the test file to be modified instead of the files retrieved with BM25, reporting results in ", "page_idx": 7}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/56e254e7b827c61827a39adfca84b70bb12fcf9152be552f4a9930aa44e49806.jpg", "table_caption": ["Table 5: Performance of ZEROSHOTPLUS, given the test file to change, none (-), the golden $(\\pmb{\\mathscr{S}})$ or an incorrect $({\\pmb X})$ code patch, and the files retrieved via BM-25 $(r)$ , or modified by the golden $(\\pmb{\\mathscr{S}})$ or incorrect patch $({\\pmb X})$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 5 in $\\%$ . We use ZEROSHOTPLUS to generate incorrect patches, resampling $\\leq\\!5$ times and excluding instances where we could not generate an incorrect but applicable patch, reducing the sample size to $n=172$ . Providing the test files to change almost doubles $\\boldsymbol{S}$ from $8.1\\%$ to $15.1\\%$ , pulling even with SWE-AGENT. We observe that meanwhile providing a code patch and the files it changed has a much smaller impact, increasing $\\boldsymbol{S}$ only to $\\bar{10}.5\\%$ for both the golden patch and an incorrect patch. This highlights the importance of retrieving the correct context for generating relevant tests. Meanwhile, GPT-4 is able to leverage the correct patch, and to improve the coverage increase of the relevant lines by almost $50\\%$ , from $12.5\\%$ to $18.4\\%$ . ", "page_idx": 8}, {"type": "text", "text": "Filtering Code Fixes with Generated Tests State-of-the-art code generation methods only resolve around $20\\%$ of cases on SWE-BENCH-LITE (Jimenez et al., 2023). Without suitable tests to distinguish correct from incorrect fixes, the overhead from manual testing (Yang et al., 2008) would thus outweigh any benefits from automatic code generation. To address this issue, we use SWEAGENT to generate both bug fixes and tests, in a similar manner to Chen et al. (2023a). We then filter the generated bug fixes, retaining only those where all generated tests are $F\\rightarrow P$ or $P\\rightarrow P$ . While only achieving $\\bar{2}0\\%$ recall, this more than doubles the precision of SWE-AGENT to $47.8\\%$ , making it significantly more practically useful, highlighting the importance of test generation, and opening an avenue to transferring the results from Chen et al. (2023a) towards more complex and realistic scenarios with more expensive inference and evaluation steps. ", "page_idx": 8}, {"type": "text", "text": "Correlation of Test Generation and Code Repair We analyze the overlap between solved instances of SWE-BENCH and SWT-BENCH, showing results in Table 6. We observe that the overlap is small for both methods, with no statistical evidence of correlation (p-values of $80.4\\%$ and $72.8\\%$ for ZEROSHOTPLUS and SWE-AGENT, respectively, under the null hypothesis of independence and uniform hardness), indicating that generating tests and fixes are distinct tasks of different difficulties. We explore this relationship in more detail in App. D. ", "page_idx": 8}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/b52b1f0f0953cd1eb5ba0617ca63593eac0265cc800dcf37a6a3ea7afebf0a75.jpg", "table_caption": ["Table 6: Overlap in solved instances of SWE-BENCH and SWT-BENCH. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.4 Test Generation Success and Instace Characteristics ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effect of Issue Description Length We investigate the relationship between issue description length and test generation performance in Fig. 5. We observe a general trend that issues with longer descriptions are easier to generate tests for, with all methods achieving a higher $\\boldsymbol{S}$ for longer descriptions, however tending to slightly decrease for very long descriptions. This is likely due to the increased amount of infor", "page_idx": 8}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/ee5c456c027bbc80ee35101272645d72708a3dbb4342f935e5d5754e0d7914c7.jpg", "img_caption": ["Figure 5: Distribution of success rate $(S)$ across issue description lengths in # tokens "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "mation available in longer descriptions, while too-long descriptions may contain many distractors and make it difficult to extract relevant information for the LLM. SWE-AGENT $^+$ , which actively summarizes context, limiting file content and reducing history, is least sensitive to issue description length, achieving approximately the same $\\boldsymbol{S}$ for all but the shortest lengths. ", "page_idx": 8}, {"type": "text", "text": "Effect of Data Contamination ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As SWT-BENCH is based on historic GitHub issues, they may be contained in the pre-training data of the LLMs we use. To investigate this issue, we conducted an experiment compar", "page_idx": 8}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/652d8cc9d48e402252458311166fb14df20417a6187b1c84addc4fcead0d224c.jpg", "table_caption": ["Table 7: Performance of ZEROSHOTPLUS on PRs before/after GPT-4 knowledge cutoff ( $K C=30\\mathrm{th}$ April 2023) in $\\%$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "ing the performance of ZEROSHOTPLUS on all issues created after the Knowledge Cutoff (KC) of GPT-4 (April 2023) to a random subset of the same size of instances created before, and report the results in Table 7. While we observe a small performance difference, we can not confirm its statistical significance $(p\\approx37\\%)$ ) due to the low number of samples created after the KC. Further, all methods in Table 2 use the same underlying LLM and should thus benefit from any potential contamination to a similar degree, allowing for a fair comparison between different methods. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Method Complimentarity We consider four diverse methods from $\\S5.2$ and analyze the overlap in the instances for which they are able to generate successful tests. We show the results in Fig. 6. While the best-performing approach, SWE-AGENT+, alone is able to solve 51 instances, the combination of all four approaches is able to solve 87 instances, highlighting the benefit of exploring a variety of approaches for test generation. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While our novel SWT-BENCH covers a wide range of real-world issues, it has several limitations: It is limited to Python, which may limit the generalizability of our findings to other programming languages. Second, the dataset is based on popular GitHub repositories, which may not be representative of common software development practices and does preclude the generation of a private holdout test set. Finally, the dataset is limited to bug reproduction and issues that can be easily covered by adding test cases and does not measure edge case detection or global coverage increase. ", "page_idx": 9}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/30e64895338138c298b429546a8fc71264626e8496d85b301837210a940b2200.jpg", "img_caption": ["Figure 6: Overlap in instances solved by the four best performing methods. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Further, as discussed in $\\S5.4$ , most issues in SWT-BENCH have been created before the knowledge cutoff of state-of-the-art models, posing a risk for data contamination. One approach to address this issue is to create a rolling version of SWT-BENCH, based only on the most recently created GitHub issues. However, this comes at the cost of direct comparability of results and increased cost for reproducing results for all baselines on a changing evaluation set. ", "page_idx": 9}, {"type": "text", "text": "Addressing these limitations would be an interesting direction for future work. As concrete starting points, we found several common errors even in the best performing method SWE-AGENT $^+$ that could be addressed through specialized monitoring: Adding passing tests that do not reproduce the given issue, getting stuck in loops after generating inapplicable edit commands, failing to execute the test environment correctly and adding tests with syntax errors or using invalid variables. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed SWT-BENCH, a novel benchmark for generating reproducing tests from GitHub issue descriptions and the corresponding code bases. SWT-BENCH leverages the dataset underlying the popular SWE-BENCH which additionally contains a golden patch fixing the described issue. We judge whether a generated test reproduces the described issue by checking whether the test fails before applying this golden patch and succeeds afterward. We measure both the rate of such fail-topass tests and the coverage of the golden patch, providing a corresponding evaluation harness. We evaluated a variety of LLM-based test generation methods including Code Agents on SWT-BENCH and found that Code Agents already outperform other approaches with only minor adaptations for the test-generation task. Finally, we demonstrated the great potential of generated tests to serve as a signal for the correctness of code fixes, i.e., we double the precision of Code Agents by filtering the generated patches to only those that cause a previously failing self-generated test to pass. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Aider. Aider is SOTA for both SWE Bench and SWE Bench Lite, Jun 2024. ", "page_idx": 10}, {"type": "text", "text": "Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Beliz Gokkaya, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, and Eddy Wang. Automated unit test improvement using large language models at meta. CoRR, abs/2402.09171, 2024a. doi: 10.48550/ARXIV. 2402.09171. ", "page_idx": 10}, {"type": "text", "text": "Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Beliz Gokkaya, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, and Eddy Wang. Automated unit test improvement using large language models at meta. CoRR, abs/2402.09171, 2024b. doi: 10.48550/ARXIV. 2402.09171. ", "page_idx": 10}, {"type": "text", "text": "Anthropic. Introducing Claude, Mar 2023. ", "page_idx": 10}, {"type": "text", "text": "Anthropic. Introducing Claude 3.5 Sonnet, Jun 2024. ", "page_idx": 10}, {"type": "text", "text": "Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv preprint, abs/2108.07732, 2021. ", "page_idx": 10}, {"type": "text", "text": "Islem Bouzenia, Premkumar T. Devanbu, and Michael Pradel. Repairagent: An autonomous, llmbased agent for program repair. CoRR, abs/2403.17134, 2024a. doi: 10.48550/ARXIV.2403. 17134. ", "page_idx": 10}, {"type": "text", "text": "Islem Bouzenia, Premkumar T. Devanbu, and Michael Pradel. Repairagent: An autonomous, llmbased agent for program repair. CoRR, 2024b. ", "page_idx": 10}, {"type": "text", "text": "Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. In Proc. of ICLR, 2023a. ", "page_idx": 10}, {"type": "text", "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond\u00e9 de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, 2021. ", "page_idx": 10}, {"type": "text", "text": "Yinghao Chen, Zehao Hu, Chen Zhi, Junxiao Han, Shuiguang Deng, and Jianwei Yin. Chatunitest: A framework for llm-based test generation. arXiv e-prints, 2023b. ", "page_idx": 10}, {"type": "text", "text": "Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. In NeurIPS Datasets and Benchmarks, 2021. ", "page_idx": 10}, {"type": "text", "text": "Naman Jain, Manish Shetty, Tianjun Zhang, King Han, Koushik Sen, and Ion Stoica. R2e: Turning any github repository into a programming agent test environment. In ICLR 2024, 2024. ", "page_idx": 10}, {"type": "text", "text": "Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? ArXiv preprint, abs/2310.06770, 2023. ", "page_idx": 10}, {"type": "text", "text": "Ren\u00e9 Just, Darioush Jalali, and Michael D. Ernst. Defects4j: a database of existing faults to enable controlled testing studies for java programs. In Proc. of ISSTA, 2014. doi: 10.1145/2610384. 2628055. ", "page_idx": 10}, {"type": "text", "text": "Sungmin Kang, Juyeon Yoon, and Shin Yoo. Large language models are few-shot testers: Exploring llm-based general bug reproduction. In Proc. of ICSE, 2023. doi: 10.1109/ICSE48619.2023. 00194. ", "page_idx": 10}, {"type": "text", "text": "Sungmin Kang, Juyeon Yoon, and Shin Yoo. LLM-powered test case generation for detecting tricky bugs. ArXiv preprint, abs/2404.10304, 2024. ", "page_idx": 11}, {"type": "text", "text": "Tsz On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, Shing-Chi Cheung, and Jeff Kramer. Nuances are the key: Unlocking chatgpt to find failure-inducing tests with differential prompting. In Proc. of ASE, 2023. doi: 10.1109/ASE56229.2023.00089. ", "page_idx": 11}, {"type": "text", "text": "Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624), 2022. doi: 10.1126/science.abq1158. ", "page_idx": 11}, {"type": "text", "text": "Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. Quixbugs: a multi-lingual program repair benchmark set based on the quixey challenge. In Proc. of SPLASH, 2017. doi: 10.1145/3135932.3135941. ", "page_idx": 11}, {"type": "text", "text": "Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. In Proc. of NeurIPS, 2023a. ", "page_idx": 11}, {"type": "text", "text": "Tianyang Liu, Canwen Xu, and Julian J. McAuley. Repobench: Benchmarking repository-level code auto-completion systems. CoRR, abs/2306.03091, 2023b. doi: 10.48550/ARXIV.2306.03091. ", "page_idx": 11}, {"type": "text", "text": "Stephan Lukasczyk and Gordon Fraser. Pynguin: Automated unit test generation for python. In Proc. of ICSE, 2022. doi: 10.1145/3510454.3516829. ", "page_idx": 11}, {"type": "text", "text": "OpenAI. GPT-4 technical report. ArXiv preprint, abs/2303.08774, 2023. ", "page_idx": 11}, {"type": "text", "text": "OpenAI. GPT-4o mini: advancing cost-efficient intelligence, Jul 2024. ", "page_idx": 11}, {"type": "text", "text": "OpenDevin. Opendevin: Code less, make more, 2024. ", "page_idx": 11}, {"type": "text", "text": "PYPL. Pypl popularity of programming language index, Aug 2024. URL https://web.archive. org/web/20240806100838/https://pypl.github.io/PYPL.html. ", "page_idx": 11}, {"type": "text", "text": "Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4), 2009. doi: 10.1561/1500000019. ", "page_idx": 11}, {"type": "text", "text": "Max Sch\u00e4fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. An empirical evaluation of using large language models for automated unit test generation. IEEE Trans. Software Eng., 50(1), 2024. doi: 10.1109/TSE.2023.3334955. ", "page_idx": 11}, {"type": "text", "text": "Philipp Straubinger and Gordon Fraser. A survey on what developers think about testing. In 34th IEEE International Symposium on Software Reliability Engineering, ISSRE 2023, Florence, Italy, October 9-12, 2023, pp. 80\u201390. IEEE, 2023. doi: 10.1109/ISSRE59848.2023.00075. URL https://doi.org/10.1109/ISSRE59848.2023.00075. ", "page_idx": 11}, {"type": "text", "text": "Wei Tao, Yucheng Zhou, Wenqiang Zhang, and Yu Cheng. MAGIS: llm-based multi-agent framework for github issue resolution. CoRR, abs/2403.17927, 2024. doi: 10.48550/ARXIV.2403. 17927. ", "page_idx": 11}, {"type": "text", "text": "MistralAI Team. Cheaper, Better, Faster, Stronger - Continuing to push the frontier of AI and making it accessible to all., Apr 2024a. ", "page_idx": 11}, {"type": "text", "text": "MistralAI Team. Large enough, Jul 2024b. ", "page_idx": 11}, {"type": "text", "text": "TIOBE. Tiobe index for august 2024, Aug 2024. URL https://web.archive.org/web/ 20240807025036/https://www.tiobe.com/tiobe-index/. ", "page_idx": 11}, {"type": "text", "text": "TogetherAI. Together AI API, 2023. ", "page_idx": 11}, {"type": "text", "text": "Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. Unit test case generation with transformers. ArXiv preprint, abs/2009.05617, 2020. ", "page_idx": 11}, {"type": "text", "text": "Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large language model based autonomous agents. Frontiers Comput. Sci., 2024. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent Computer Interfaces Enable Software Engineering Language Models, 2024. Ye Yang, Mei He, Mingshu Li, Qing Wang, and Barry W. Boehm. Phase distribution of software development effort. In Proc. of ESEM, 2008. doi: 10.1145/1414004.1414016. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. CoRR, abs/2404.05427, 2024. doi: 10.48550/ARXIV.2404.05427. ", "page_idx": 12}, {"type": "text", "text": "Figure 7: The Custom Diff format for ZEROSHOTPLUS ", "page_idx": 13}, {"type": "text", "text": "A Formalization of Custom Prompt Format for ZEROSHOTPLUS ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We introduce a custom prompt format for language models to aid them with patch generation in the zero-shot setting. The format is visualized in Fig. 7 similar to how it is provided to the language model. A full example of applying the format on two files is part of the full prompt of ZEROSHOTPLUS in Figs. 11 and 12. ", "page_idx": 13}, {"type": "text", "text": "A diff block must start and end with diff and end diff respectively. The first line inside the block must specify an existing file for rewrites and may point to a new file in the case of insertion. Next, the language model specifies whether it intends to rewrite an existing function or insert a new function. If no exact match of the function name is found, we employ a fuzzy search using the line number or EOF/BOF as an indicator for where to look for the existing functions. EOF and BOF are particularly useful for inserting new functions. We note that diff blocks can be repeated an arbitrary number of times. ", "page_idx": 13}, {"type": "text", "text": "B Adapting LIBRO to our Setting ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Kang et al. (Kang et al., 2023) originally proposed LIBRO for an evaluation in a pass $@_{\\mathrm{k}}$ setting. There, it is useful to rank all generated tests to improve performance at $k>1$ . As we only consider pass $@1$ , we drop ranking components irrelevant for the top-1 test in our reimplementation. Further, LIBRO includes heuristics for importing missing dependencies and inserting tests into the correct classes. While effective in Java, this is superfluous for Python, where tests can be added outside classes and dependency imports are (empirically) correctly generated by the LLM. We thus also drop these components. ", "page_idx": 13}, {"type": "text", "text": "LIBRO clusters test cases based on whether the generated execution trace matches the issue description. As exact matching is often not applicable for the unstructured issue descriptions, we measure the similarity between the error message and the issue description by extracting the execution trace of the generated test cases and querying the same LLM used for test generation to judge whether they relate to the same issue. Depending on its answer, we obtain two clusters and choose the shortest result of the preferred cluster. ", "page_idx": 13}, {"type": "text", "text": "C Hyperparameter Ablations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Ablation on number of LIBRO samples ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We perform an ablation study by varying the number of samples used for the LIBRO baseline. The result is presented in Fig. 8a. LIBRO\u2019s performance improves as more samples are considered, however the gains of additional samples are marginal from around the 5 samples we use by default. As shown in section App. G, to enable comparison at similar cost to Code Agents, we settle for 5 samples. ", "page_idx": 13}, {"type": "text", "text": "C.2 Ablation on Interaction Rounds for Code Agents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We analyze the number of interactions required for each Agent to submit a solution and plot the results in Fig. 8b. We observe increasing interaction rounds improve performance until saturation at 5-10 iterations (we use 20 as a default) with the only exception being AutoCodeRover, which still gains performance up to the maximum of 20 iterations we consider. ", "page_idx": 13}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/832386829bb30d4e8d60225bff003d237e9a7f073ed3f09e11bec648df696804.jpg", "table_caption": ["Table 8: Comparison of ZEROSHOTPLUS for different $T$ on GPT-4 $95\\%$ CI, $n=25$ ). "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/f0e9d9e64d1aebf4a76a89bd559a857a055ef34b5a83615dd01b1f5baba22409.jpg", "img_caption": ["(a) Ablation of $\\mathcal{W}$ and $s$ by number of LIBRO samples. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/d06989017d61ab64b56cba8e468231a776b358fd0f0df785c880dd222c54afae.jpg", "img_caption": ["(b) Ablation of $\\mathcal{W}$ and $_S$ by code agent over number of needed API calls until solution submission. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 8: Ablation on the number of samples and API calls for LIBRO and code agents resp. ", "page_idx": 14}, {"type": "text", "text": "C.3 Ablation on Temperature ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We run ZEROSHOTPLUS using GPT-4 with 25 samples and analyze the performance variation for a temperature range from greedy decoding $(T=0)$ ), used for ZEROSHOT, ZEROSHOTPLUS and the agent settings, to $T=0.7$ , the setting used in LIBRO. The results are presented in Table 8. We observe a tendency of decreased performance and increasing variance in all metrics for increasing $T$ . Moreover we observe a minimal variability among several runs of the test environment at $T=0$ , however much smaller than the variability due to temperature. ", "page_idx": 14}, {"type": "text", "text": "D Distribution over Repositories ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We compare the success rate of SWEAGENT for test and fix generation across repositories in Fig. 9. We observe that, while SWE-AGENT obtains similar success rates in both settings in three repositories, success rates vary strongly on most other repositories. Indeed, there are five repositories where test generation fails entirely while code repair fails on three and on only two of these both fail. Manually inspecting instances from the repositories where test generation fails shows a variety ", "page_idx": 14}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/6cb6e5e84d0e0a471d1f4c39fd48e77f59dfefc6dd85afdaeafa7504c0802df7.jpg", "img_caption": ["Figure 9: Distribution of success rates across repositories for SWE-AGENT. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "of reasons, astropy usually features complex computations where unit test creation is difficult, requests features a highly complex code base, flask has extremely long golden test lengths indicating particularly challenging testing problems. For pylint generated tests are all $P\\!\\rightarrow\\!P$ making them correct but unhelpful. ", "page_idx": 14}, {"type": "text", "text": "E Full prompts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "ZEROSHOT, ZEROSHOTPLUS and LIBRO The full prompt for ZEROSHOT is displayed in Figs. 10 and 11. The full prompt for ZEROSHOTPLUS and LIBRO is displayed in Figs. 12 and 13. Except for the way we include files, all lines are changed with respect to the setting in SWE-BENCH. ", "page_idx": 14}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/6cb73599fcd35352c661cb09738659d0c7dd766401106246ad39f8f7ad424343.jpg", "table_caption": ["Table 9: Cost of different LLMs running SWE-AGENT on SWT-BENCH Lite in USD "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/27a64a18af808366a928b94ea3511cfb537ee5628ed0d876c39be2ab05a80a92.jpg", "table_caption": ["Table 10: Cost of running different methods on SWT-BENCH Lite using GPT-4 in USD "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "This includes in particular the demonstration of the unified diff format on an example. In the setting for Table 5 we add the lines highlighted in boldface. ", "page_idx": 15}, {"type": "text", "text": "SWE-AGENT and SWE-AGENT $^+$ The prompt for SWE-AGENT and SWE-AGENT $^+$ is shown in Fig. 14. Changes with respect to the prompt of (Jimenez et al., 2023) are highlighted in boldface. The additional changes for SWE-AGENT $^+$ are highlighted in green. ", "page_idx": 15}, {"type": "text", "text": "AIDER We only minimally adapt the provided evaluation harness for AIDER on SWE-BENCH1. In this harness, AIDER is provided with a single initial user prompt based on the user issue, while the entire agent workflow remains unchanged. We provide the entire prompt in Fig. 16 and highlight our change in boldface. ", "page_idx": 15}, {"type": "text", "text": "AUTOCODEROVER AUTOCODEROVER (Zhang et al., 2024) leverages a number of prompts that are provided to the model in different phases of the code/test generation process. We adapt the key prompts and display them in Fig. 15. Changes are highlighted in boldface. Further, we change every occurrence of \"bug location\" in the original prompts to \"relevant location\". We further add a function to the ACI that allows inserting code in new files and fetching the entire code (capped at the first 100 lines) of any file. ", "page_idx": 15}, {"type": "text", "text": "F Licenses of used Code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We adapt code from the following projects in our work and include the respective licenses: ", "page_idx": 15}, {"type": "text", "text": "1. SWE-BENCH (Jimenez et al., 2023): MIT License   \n2. SWE-AGENT (Yang et al., 2024): MIT License   \n3. AIDER (Aider, 2024): Apache License 2.0   \n4. AUTOCODEROVER (Zhang et al., 2024): GNU General Public License ", "page_idx": 15}, {"type": "text", "text": "For all licenses of the repositories used in SWT-BENCH, we refer to Jiminez et al. (Jimenez et al., 2023), which contains a detailed list with licenses for each repository included. ", "page_idx": 15}, {"type": "text", "text": "G Computational cost ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "There is cost to both running inference on Language Models and on evaluation their predictions on the test suites of the repositories. Since the evaluation can be performed on a consumer grade machine in reasonable time, we focus on the cost inferred from LLM inference. We report the cost for each setting in Tables 9 and 10, displaying the average cost of a full inference on SWT-BENCH Lite for each model and method. The difference between cost of $\\mathrm{PASS}\\,{\\ @\\,5}$ and LIBRO is just the additional filtering step incurred by LIBRO. ", "page_idx": 15}, {"type": "table", "img_path": "9Y8zUO11EQ/tmp/11ed373bee697a4959ddcf5ee9251c8c071e2401123bed461fa1fc22ace1d900.jpg", "table_caption": ["Table 11: Average execution time $t$ per instance "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "H Execution times ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We run the different methods from Table 2 on 5 instances and compute the average execution time. For all LLMs we consider, part of the execution time is directly related to the number of tokens digested and generated (see Table 10). For methods that require interaction with an execution environment however, time is usually dominated by setting up such an environment in a clean and reproducible manner (i.e. dockerized). We list results on execution times in Table 11 and observe that all methods except zero-shot inference take between 3-5 minutes per instance, where we can observe a small trade off due to many-turn interactions in Code Agents versus single-shot execution in LIBRO. Given these small differences however, we believe execution time to be of limited practical relevance as issues can be processed in the background, similar to continuous integration, in response to raised user issues ", "page_idx": 16}, {"type": "text", "text": "1 The following text contains a user issue (in <issue/> brackets) posted at a repository. Further, you are provided with file contents of several files in the repository that contain relevant code (in <code> brackets). It may be necessary to use code from third party dependencies or files not contained in the attached documents however. Your task is to identify the issue and implement a test case that verifies a proposed solution to this issue. More details at the end of this text.   \n2   \n3 <issue>   \n4 user issue comes here   \n5 </issue>   \n6   \n7 retrieval results or oracle files come here   \n8   \n9 Please generate test cases that check whether an implemented solution   \n10 resolves the issue of the user (at the top, within <issue/> brackets).   \n11 Present the test cases in unified diff formatting.   \n12   \n13 The general format of a diff is the unified output format, described as follows.   \n14 The unified output format starts with a two-line header, which looks like this:   \n15   \n16 from-file   \n17 $+++$ to-file   \n18   \n19 Next come one or more hunks of differences; each hunk shows one area where the files differ. Unified format hunks look like this:   \n20   \n21 @@ from-file-line-numbers to-file-line-numbers @@   \n22 line-from-either-file   \n23 line-from-either-file   \n24   \n25 If a hunk contains just one line, only its start line number appears. Otherwise its line numbers look like 'start,count'. An empty hunk is considered to start at the line that follows the hunk.   \n26   \n27 If a hunk and its context contain two or more lines, its line numbers look like 'start, count'. Otherwise only its end line number appears. An empty hunk is considered to end at the line that precedes the hunk.   \n28   \n29 The lines common to both files begin with a space character. The lines that actually differ between the two files have one of the following indicator characters in the left print column:   \n30   \n31 '+' A line was added here to the first file.   \n32 '-' A line was removed here from the first file.   \n33   \n34 Insertion can only be done at the end or beginning of the file, indicated by EOF or BOF respectively.   \n35   \n36 As an example for a diff, consider the following two versions of the same file, once before and once after a change.   \n37 The original version of the file was as follows.   \n38 [start of demo/test_file.py]   \n39 1 def test_euclidean(a, b):   \n40 2 assert euclidean(0, 0) == 0   \n41 3 assert euclidean(0, 1) == 1   \n42 4 assert euclidean(1, 0) == 1   \n43 5 assert euclidean(1, 1) == 1   \n44 6   \n45 7 @pytest.mark.parametrize(\"a, b, expected\", [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1) ])   \n46 8 def test_gcd(a, b):   \n47 9 assert gcd(a, b) $==$ expected   \n48 10   \n49 [end of demo/file.py] ", "page_idx": 17}, {"type": "text", "text": "1 2 The diff for fix in function euclidean and adds the function gcd is as follows. 3 This diff changes the first file into the second file. 4 \\`\\`\\`diff 5 --- a/demo/file.py 6 +++ a/demo/file.py 7 @@ -4,4 +4,5 @@ 8 assert euclidean $(\\,1\\,,\\ \\ \\Theta\\,)\\ \\ {=}{}=\\ 1$ 9 assert euclidean $(\\,1\\,,\\ \\ 1\\,)\\ \\d t=\\ 1\\$ $10\\quad+$ assert euclidean(100, 10) == 10 11 12 @pytest.mark.parametrize(\"a, b, expected\", [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1) ]) 13 @@ -9,2 +10,6 @@ 14 assert gcd(a, b) $==$ expected 15 16 +@pytest.mark.parametrize(\"a, b, expected\", [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1), (100, 10, 10)]) 17 +def test_lcm(a, b): 18 $^+$ assert lcm(a, b) $==$ expected 19 + 20 21 22 The new version of the file is as follows. 23 [start of demo/file.py] 24 1 def test_euclidean(a, b): 25 2 assert euclidean(0, 0) == 0 26 3 assert euclidean $(\\,\\Theta\\,,~~1\\,)~~=~1$ 27 4 assert euclidean $(\\,1\\,,\\ \\ \\Theta\\,)\\;\\;==\\;1$ 28 5 assert euclidean(1, 1) == 1 29 6 assert euclidean(100, 10) == 10 30 7 31 8 @pytest.mark.parametrize(\"a, b, expected\", [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1) ]) 32 9 def test_gcd(a, b): 33 10 assert gcd(a, b) $==$ expected 34 11 35 12 @pytest.mark.parametrize(\"a, b, expected\", [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1), (100, 10, 10)]) 36 13 def test_lcm(a, b): 37 14 assert lcm(a, b) $==$ expected 38 15 39 [end of demo/file.py] 40 41 As you can see, you need to indicate the approximate line numbers, function name and the path and file name you want to change, 42 but there can be as many independent blocks of changes as you need. You may also apply changes to several files. 43 Apply as much reasoning as you please and see necessary. The format of the solution is fixed and has to follow the custom diff format. 44 Make sure to implement only test cases and don't try to fix the issue itself. ", "page_idx": 18}, {"type": "text", "text": "1 The following text contains a user issue (in <issue/> brackets) posted at a repository. Further, you are provided with file contents of several files in the repository that contain relevant code (in <code> brackets). It may be necessary to use code from third party dependencies or files not contained in the attached documents however. Your task is to identify the issue and implement a test case that verifies a proposed solution to this issue. More details at the end of this text.   \n2   \n3 <issue>   \n4 user issue comes here   \n5 </issue>   \n6   \n7 The following patch has been proposed to fix the issue described in the user issue (in <issue/> brackets).The patch might give you a hint on how to write a covering test for the issue, but you should not assume that the patch is correct.It might be that the provided patch is not correct, so your test should check whether the patch resolves the issue.<patch>proposed patch</patch>   \n8   \n9 retrieval results or oracle files come here   \n10   \n11 Please generate test cases that check whether an implemented solution   \n12 resolves the issue of the user (at the top, within <issue/> brackets).   \n13 Present the test cases as a diff (custom format, explained below).   \n14   \n15 The general format of a diff is as follows.   \n16 \\`\\`\\`custom-diff   \n17 diff   \n18 <path/filename>   \n19 $<$ \"rewrite\" or \"insert\" $>$   \n$20\\_<$ rough line number / EOF / BOF >   \n$21\\_<$ insert function that should be added or rewritten $>$   \n22 end diff   \n$23\\_<$ repeat blocks of diff as necessary $>$   \n24 \\`\\`\\`   \n25 Insertion can only be done at the end or beginning of the file, indicated by EOF or BOF respectively.   \n26   \n27 As an example for a diff, consider the following two versions of the same file, once before and once after a change.   \n28 The original version of the file was as follows.   \n29 [start of demo/test_file.py]   \n30 1 def test_euclidean(a, b):   \n31 2 assert euclidean(0, 0) == 0   \n32 3 assert euclidean $(\\,\\Theta\\,,~~1\\,)~~=~1$   \n33 4 assert euclidean $(\\,1\\,,\\ \\ \\Theta\\,)\\;\\;==\\;1$   \n34 5 assert euclidean(1, 1) == 1   \n35 6   \n36 7 @pytest.mark.parametrize(\"a, b, expected\", [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1) ])   \n37 8 def test_gcd(a, b):   \n38 9 assert gcd(a, b) $==$ expected   \n39 10   \n40 [end of demo/file.py]   \n41 ", "page_idx": 19}, {"type": "text", "text": "1 The diff for fix in function euclidean and adds the function gcd is as follows.   \n2 This diff changes the first file into the second file.   \n3 \\`\\`\\`custom-diff   \n4 diff   \n5 demo/file.py   \n6 rewrite   \n7 1   \n8 def test_euclidean(a, b):   \n9 assert euclidean(0, 0) == 0   \n10 assert euclidean $(\\,\\Theta\\,,~~1\\,)~~=~1$   \n11 assert euclidean $(\\,1\\,,\\ \\ \\Theta\\,)\\ \\ {=}{}=\\ 1$   \n12 assert euclidean(1, 1) $\\c=~1$   \n13 assert euclidean(100, 10) == 10   \n14 end diff   \n15 diff   \n16 demo/file.py   \n17 insert   \n18 EOF   \n19 @ pytest.mark.parametrize(\"a, b, expected\", [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1),   \n(100, 10, 10)])   \n20 def test_lcm(a, b):   \n21 assert lcm(a, b) $==$ expected   \n22 end diff   \n23   \n24 The new version of the file is as follows.   \n25 [start of demo/file.py]   \n26 1 def test_euclidean(a, b):   \n27 2 assert euclidean $(\\,\\Theta\\,,\\ \\ \\Theta\\,)\\ \\ ==\\ \\Theta$   \n28 3 assert euclidean $(\\,\\Theta\\,,~~1\\,)~~=~1$   \n29 4 assert euclidean $(\\,1\\,,\\ \\ \\Theta\\,)\\;\\;==\\;1$   \n30 5 assert euclidean(1, 1) == 1   \n31 6 assert euclidean(100, 10) == 10   \n32 7   \n33 8 @pytest.mark.parametrize(\"a, b, expected\", [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1)   \n])   \n34 9 def test_gcd(a, b):   \n35 10 assert gcd(a, b) $==$ expected   \n36 11   \n37 12 @pytest.mark.parametrize(\"a, b, expected\", [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1,   \n1), (100, 10, 10)])   \n38 13 def test_lcm(a, b):   \n39 14 assert lcm(a, b) $==$ expected   \n40 15   \n41 [end of demo/file.py]   \n42   \n43 As you can see, you need to indicate the approximate line numbers, function name and the   \npath and file name you want to change,   \n44 but there can be as many independent blocks of changes as you need. You may also apply   \nchanges to several files.   \n45 Apply as much reasoning as you please and see necessary. The format of the solution is   \nfixed and has to follow the custom diff format.   \n46 Make sure to implement only test cases and don't try to fix the issue itself. ", "page_idx": 20}, {"type": "text", "text": "2 ISSUE:   \n3 {issue}   \n4   \n5 INSTRUCTIONS:   \n6 Now, you\u2019re going to create unit tests that cover the issue. In other words, you should wtrhiet ei susnuiet  htaess tbse etnh arte sfoalivle di.n tEhses ecnutriraelnlty ,s tyaotue\u2019 lolf  wtahnet  rteop owsriittoer yab uutn iwti ltle spta stsh awthen reproduces the described issue.   \n7 Your terminal session has started and you're in the repository's root directory. You can use any bash commands or the special interface to help you. Edit all the files you need to and run any checks or tests that you want.   \n8 Remember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME. You should always wait for feedback after every command.   \n9 When you're satisfied with all of the changes you've made, you can submit your changes to the code base by simply running the submit command.   \n10 Note however that you cannot use any interactive session commands (e.g. python, vim) in this environment, but you can write scripts and run them. E.g. you can write a python script and then run it with \\`python <script_name>.py\\`.   \n11   \n12 NOTE ABOUT THE EDIT COMMAND: Indentation really matters! When editing a file, make sure to insert appropriate indentation before each line!   \n13   \n14 IMPORTANT TIPS:   \n15 1. Always start by trying to replicate the bug that the issues discusses.   \n16 If the issue includes code for reproducing the bug, we recommend that you reimplement that in your environment, and run it to make sure you can reproduce the bug.   \n17 Then start trying to fix it.   \n18 When you think you've fixed the bug, re-run the bug reproduction script to make sure that the bug has indeed been fixed.   \n19   \n20 If the bug reproduction script does not print anything when it successfully runs, we recommend adding a print(\"Script completed successfully, no errors.\") command at the end of the file,   \n21 so that you can be sure that the script indeed ran fine all the way through.   \n22   \n23 2. If you run a command and it doesn't work, try running a different command. A command that did not work once will not work the second time unless you modify it!   \n24   \n25 3. If you open a file and need to get to an area around a specific line that is not in the first 100 lines, say line 583, don't just use the scroll_down command multiple times. Instead, use the goto 583 command. It's much quicker.   \n26   \n27 4. If the bug reproduction script requires inputting/reading a specific file, such as buggy-input.png, and you'd like to understand how to input that file, conduct a search in the existing repo code, to see whether someone else has already done that. Do this by running the command: find_file \"buggy-input.png\" If that doesn't work, use the linux 'find' command.   \n28   \n29 5. Always make sure to look at the currently open file and the current working directory (which appears right after the currently open file). The currently open file might be in a different directory than the working directory! Note that some commands, such as 'create', open files, so they might change the current open file.   \n30   \n31 6. When editing files, it is easy to accidentally specify a wrong line number or to write code with incorrect indentation. Always check the code after you issue an edit to make sure that it reflects what you wanted to accomplish. If it didn't, issue another command to fix it.   \n32   \n33 7. After having applied your changes and before submitting, make sure to run pytest and check if the code \\*fails\\* as expected due to the issue description. If it doesn\u2019t, revisit your code changes and adapt them accordingly.   \n1 You are a software developer maintaining a large project.   \n2 You are working on an issue submitted to your project.   \n3 The issue contains a description marked between <issue> and </issue>.   \n4 Your task is to invoke a few search API calls to gather information about relevant code lines, then write unit tests to capture the described behaviour in the issue.Ideally, the unit tests should fail before the bug is fixed or the requested feature is added, and pass after.Note you are not trying to solve the bug itself, but just capture the behaviour described in the issue by creating appropriate test cases.   \n1 You are a software developer maintaining a large project.   \n2 You are working on an issue submitted to your project.   \n3 The issue contains a description marked between <issue> and </issue>.   \n4 You ultimate goal is to write one or more unit tests that capture this issue.Ideally, the unit tests should fail before the bug is fixed or the requested feature is added, and pass after.Note you are not trying to solve the bug itself, but just capture the behaviour described in the issue by creating appropriate test cases. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "9Y8zUO11EQ/tmp/86762554fb38f8006b97153b9082abcf9c8d0c18ba69b3108a95c9f0b32b2851.jpg", "img_caption": ["Figure 15: The Prompt for AUTOCODEROVER on SWT-BENCH "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "1 Below is a real GitHub issue from a popular GitHub repository.   \n2 The issue was filed some time ago.   \n3 The repo has been checked out at the commit that existed at the moment the issue was   \nfiled.   \n4 If you are already familiar with this repo, be cautious!   \n5 You are working with an old version of the repo!   \n6 Filenames, directory names, file contents, etc may be different than what you're used to   \n7   \n8 Propose changes to update the repo to reproduce the problem below.   \n9 You\u2019re going to create unit tests that cover the issue. In other words, you should   \nwrite unit tests that fail in the current state of the repository   \n10 but will pass when the issue has been resolved. Essentially, you\u2019ll want to write a   \nunit test that reproduces the described issue.   \n11   \n12   \n13 {issue} ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope, as described in $\\S3{-}\\S5$ . ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss limitations thoroughly in $\\S6$ . ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There are no theoretical results included. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We describe our approach in detail and publish all code, data, and instructions required to reproduce our results. All experimental details can be found in $\\S5$ and full details on used prompts are laid out in App. E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We release all code, data, and instructions required to reproduce our results online at https://github.com/logic-star-ai/swt-bench. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We describe all key experimental details in $\\S5$ . Further details on the used prompts for LLM interaction are provided in App. E. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We add results of tests of statistical significance in the text accompanying experiments in $\\S5$ . We further provide a temperature ablation with error bars in App. C. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We outline the cost of the significant parts of our experiments (mostly dominated by the cost of LLM inference) in App. G. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our research conforms to the NeurIPS Code of Ethics. In particular, we did not conduct research involving humans, used only publicly available data open source data with permissive licenses, and believe that our research has no negative societal impact. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The creators have included the licenses of all used code in the submitted code together with the package. The code adjusted and used for the experiments in this paper was licensed with permissive licenses, that we respect and redistribute with out adjusted code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide a detailed description of our dataset creation process in $\\S3$ , and distribute this description as well as license information alongside the dataset at https: //github.com/logic-star-ai/swt-bench. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]