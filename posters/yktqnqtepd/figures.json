[{"figure_path": "yktQNqtepd/figures/figures_0_1.jpg", "caption": "Figure 1: Bounding Box vs. Occupancy. Occupancy can better represent the crane's shape than the bounding box.", "description": "This figure is a visual comparison between using bounding boxes and occupancy grids for 3D object representation.  The left panel (a) shows a crane represented by a bounding box; it's a simple cuboid that roughly encloses the object, failing to capture the detailed shape, especially the long boom extending from the crane's cab.  This results in a significant amount of unoccupied space inside the bounding box. The right panel (b) shows the same crane represented as a scene-level occupancy grid, where the 3D space is discretized into voxels, and each voxel is classified as occupied or unoccupied, effectively representing the crane's shape and structure more precisely than the bounding box.", "section": "1 Introduction"}, {"figure_path": "yktQNqtepd/figures/figures_1_1.jpg", "caption": "Figure 2: Generating occupancy from LiDAR scans is non-trivial for foreground objects due to sparsity and detection drifts.", "description": "This figure demonstrates the challenges of generating occupancy grids directly from LiDAR point clouds.  Subfigure (a) shows an example of a high-quality object-centric occupancy representation. (b) illustrates the sparsity of a single LiDAR scan for a foreground object, making it difficult to reconstruct the object's shape.  (c) shows that aggregating multiple scans using accurate ground-truth bounding boxes can create a reasonably complete occupancy representation.  However, (d) shows that using noisy bounding boxes from a detection algorithm results in a blurry and inaccurate occupancy volume due to the accumulation of errors.", "section": "1 Introduction"}, {"figure_path": "yktQNqtepd/figures/figures_3_1.jpg", "caption": "Figure 3: Occupancy grids defined in the ego-vehicle (left) and object-centric (right) coordinate systems. The object shape is jagged in the ego-vehicle occupancy grid due to coordinate misalignment.", "description": "This figure compares two different coordinate systems for representing occupancy grids: ego-vehicle and object-centric. The left panel shows an occupancy grid centered on the ego-vehicle, where the object's shape appears jagged due to coordinate misalignment.  The right panel shows the object-centric occupancy grid, where the object is centered, resulting in a smoother, more accurate representation of its shape. This highlights the advantage of object-centric occupancy for capturing object details, especially those with irregular shapes.", "section": "3.1 Object-Centric vs. Scene-Level Occupancy"}, {"figure_path": "yktQNqtepd/figures/figures_4_1.jpg", "caption": "Figure 4: Architecture overview. The network takes a noisy object sequence as input and outputs the complete object-centric occupancy volume and refined bounding box for each proposal. The notation [,] denotes the concatenation operation. 'global'/'local' indicates features from global/local coordinate system.", "description": "This figure illustrates the architecture of the object-centric occupancy completion network proposed in the paper.  The network takes as input a sequence of noisy object proposals, each with associated point clouds and bounding boxes at different timestamps. It consists of two main branches. The first (global branch) encodes global features (Zg) from the entire object sequence via a multi-layer perceptron (MLP). The second (local branch) encodes local features (Zl) from the Region of Interest (RoI) of each object proposal, also via an MLP. A causal attention mechanism integrates the global and local features to provide a robust temporal representation of the object's shape. Finally, an implicit shape decoder (D) generates the complete object-centric occupancy volume using the combined features. A detection head uses the features to refine bounding box predictions and objectness scores.", "section": "4 Sequence-based Occupancy Completion Network"}, {"figure_path": "yktQNqtepd/figures/figures_6_1.jpg", "caption": "Figure 5: Illustration for occupancy evaluations.", "description": "This figure illustrates the two-step process used to calculate the Intersection over Union (IoU) for evaluating shape completion. First, the ground truth (GT) box is transformed into the coordinate system of the Region of Interest (RoI). Then, the predicted occupancy status for each voxel center within the transformed GT box is determined. The IoU is computed by comparing the predicted occupancy volume within the GT box to the ground truth occupancy volume. Voxels outside of the RoI are considered free during the computation.", "section": "5.2 Dataset and Evaluation Metrics"}, {"figure_path": "yktQNqtepd/figures/figures_13_1.jpg", "caption": "Figure 6: Our object-centric occupancy annotation pipeline.", "description": "The figure illustrates the object-centric occupancy annotation pipeline. It starts with a ground truth (GT) object sequence, which is then processed to densify the object points.  These points are transformed into the object's coordinate system.  Voxelization then creates the object volume, where voxels are classified as occupied or unoccupied. Occlusion reasoning is performed to distinguish between free and unobserved voxels, resulting in the final object-centric occupancy grid.", "section": "3.2 Dataset Generation Pipeline"}, {"figure_path": "yktQNqtepd/figures/figures_14_1.jpg", "caption": "Figure 7: Visualization of our object-centric occupancy annotations. The first column shows the GT-aggregated LiDAR points. The second column shows our annotated object-centric occupancy volume. The last three columns respectively show the occupancy at free, occupied and unobserved status.", "description": "This figure visualizes examples from the Object-Centric Occupancy dataset.  Each row shows a different object. The first column displays the ground truth point cloud of the object generated by aggregating multiple LiDAR scans. The second column presents the corresponding object-centric occupancy grid generated by the proposed method. The remaining columns break down the occupancy grid into voxels classified as \"free\", \"occupied\", and \"unobserved\".  The \"unobserved\" voxels represent areas within the bounding box of the object that were not captured by the LiDAR due to occlusion.", "section": "3.2 Dataset Generation Pipeline"}, {"figure_path": "yktQNqtepd/figures/figures_14_2.jpg", "caption": "Figure 4: Architecture overview. The network takes a noisy object sequence as input and outputs the complete object-centric occupancy volume and refined bounding box for each proposal. The notation [,] denotes the concatenation operation. 'global'/'local' indicates features from global/local coordinate system.", "description": "This figure illustrates the architecture of the object-centric occupancy completion network. The network takes a noisy object sequence as input and outputs the complete object-centric occupancy volume and a refined bounding box for each proposal. The network consists of several components: a local encoder (Elocal) that encodes the ROI in the local coordinate system, a global encoder (Eglobal) that encodes the ROI in the global coordinate system, a causal attention module that aggregates temporal information from the global features, an implicit shape decoder (D) that predicts the complete occupancy volume from a latent embedding, a bbox head that refines the bounding box, and an occupancy head (Occ Head) that predicts the occupancy volume. The notation [,] denotes the concatenation operation, while 'global'/'local' indicates features from global/local coordinate systems.", "section": "4 Sequence-based Occupancy Completion Network"}, {"figure_path": "yktQNqtepd/figures/figures_14_3.jpg", "caption": "Figure 4: Architecture overview. The network takes a noisy object sequence as input and outputs the complete object-centric occupancy volume and refined bounding box for each proposal. The notation [,] denotes the concatenation operation. 'global'/'local' indicates features from global/local coordinate system.", "description": "This figure shows the architecture of the object-centric occupancy completion network. The network takes a noisy object sequence as input, which includes point clouds and noisy bounding boxes at different timestamps.  It processes this sequence through two branches: a global branch and a local branch. The global branch encodes the RoI (region of interest) in the global coordinate system, providing motion information. The local branch encodes the RoI in the local coordinate system. A causal attention mechanism combines information from the global branch across timestamps to enhance the local features. This combined information then feeds into an implicit shape decoder which outputs the complete object-centric occupancy volume. Finally, a detection head refines the bounding boxes and scores. ", "section": "4 Sequence-based Occupancy Completion Network"}, {"figure_path": "yktQNqtepd/figures/figures_15_1.jpg", "caption": "Figure 10: Visualization of the object-centric occupancy prediction. Different rows denote different object instances. Pink points indicate LiDAR points. Blue cubes represent the predicted occupied voxels.", "description": "This figure visualizes the results of the object-centric occupancy prediction. Each row displays a different object. Pink points show the LiDAR points used in the prediction, while blue cubes represent the predicted occupied voxels. This provides a visual representation of how the model reconstructs the shape of objects by filling in missing data points.", "section": "A.4 Visualization of the Occupancy Prediction"}, {"figure_path": "yktQNqtepd/figures/figures_16_1.jpg", "caption": "Figure 10: Visualization of the object-centric occupancy prediction. Different rows denote different object instances. Pink points indicate LiDAR points. Blue cubes represent the predicted occupied voxels.", "description": "This figure visualizes the object-centric occupancy prediction results of the proposed method. Each row represents a different object. The pink points show the actual LiDAR points, while the blue cubes represent the voxels predicted as occupied by the model. The figure demonstrates the ability of the model to predict the shape of the objects, even in cases with occlusion or sparse LiDAR data.", "section": "A.4 Visualization of the Occupancy Prediction"}, {"figure_path": "yktQNqtepd/figures/figures_16_2.jpg", "caption": "Figure 10: Visualization of the object-centric occupancy prediction. Different rows denote different object instances. Pink points indicate LiDAR points. Blue cubes represent the predicted occupied voxels.", "description": "This figure visualizes the results of the object-centric occupancy prediction. Each row shows a different object, represented by a 3D point cloud (pink points) and the predicted occupancy (blue cubes).  The visualization highlights the model's ability to predict the occupied voxels accurately, even in cases with sparse LiDAR point clouds, showcasing its effectiveness in reconstructing complex object shapes from incomplete data.", "section": "A.4 Visualization of the Occupancy Prediction"}]