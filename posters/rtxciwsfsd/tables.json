[{"figure_path": "rTxCIWsfsD/tables/tables_6_1.jpg", "caption": "Table 1: Average scores and standard errors under random and adversarial simultaneous corruptions.", "description": "This table presents the average performance scores and standard errors of different offline reinforcement learning algorithms under different corruption scenarios. The algorithms are evaluated on three MuJoCo environments (HalfCheetah, Walker2d, and Hopper).  Each environment is tested under random and adversarial simultaneous corruption,  meaning both types of corruption are applied simultaneously. The table allows for a comparison of algorithm robustness to diverse forms of data corruption.", "section": "4 Experiments"}, {"figure_path": "rTxCIWsfsD/tables/tables_6_2.jpg", "caption": "Table 1: Average scores and standard errors under random and adversarial simultaneous corruptions.", "description": "This table presents the average performance scores and their standard errors for different reinforcement learning algorithms across various environments (Halfcheetah, Walker2d, Hopper) under different corruption scenarios. The corruption scenarios involve simultaneous random and adversarial corruptions of observations, actions, rewards, and dynamics in the offline datasets. The table allows for a comparison of the robustness of different algorithms against various data corruptions. TRACER (ours) consistently outperforms other algorithms across all environments and corruption types.", "section": "4 Experiments"}, {"figure_path": "rTxCIWsfsD/tables/tables_6_3.jpg", "caption": "Table 3: Average score under diverse adversarial corruptions.", "description": "This table presents the average performance scores of different offline reinforcement learning algorithms under various adversarial corruptions. The algorithms are evaluated on three different environments (Halfcheetah, Walker2d, and Hopper) with corruptions applied to different elements of the dataset (observation, action, reward, and dynamics).  The table shows TRACER significantly outperforms other algorithms across all environments and corruption types.", "section": "4 Experiments"}, {"figure_path": "rTxCIWsfsD/tables/tables_21_1.jpg", "caption": "Table 1: Average scores and standard errors under random and adversarial simultaneous corruptions.", "description": "This table presents the average performance scores and standard errors of different offline reinforcement learning algorithms under conditions of simultaneous random and adversarial data corruptions.  The algorithms are evaluated across three different MuJoCo environments (Halfcheetah, Walker2d, and Hopper).  The results show the average performance across both random and adversarial corruptions, offering a comparison of algorithm robustness.  The table helps illustrate the performance advantage of the proposed TRACER algorithm in handling diverse data corruptions.", "section": "4 Experiments"}, {"figure_path": "rTxCIWsfsD/tables/tables_22_1.jpg", "caption": "Table 1: Average scores and standard errors under random and adversarial simultaneous corruptions.", "description": "This table presents the average scores and standard errors achieved by various offline reinforcement learning algorithms on three different environments (Halfcheetah, Walker2d, and Hopper) under simultaneous random and adversarial corruptions.  The results are compared across different corruption methods and show the performance differences between the algorithms in handling diverse data corruption scenarios.  The results highlight the impact of the various algorithms and their handling of noisy data. ", "section": "4 Experiments"}, {"figure_path": "rTxCIWsfsD/tables/tables_23_1.jpg", "caption": "Table 5: Hyper-parameters used for TRACER under the random corruption benchmark.", "description": "This table lists the hyperparameters used in the TRACER algorithm for the random corruption benchmark.  It specifies the number of samples (N), number of ensemble models (K), and alpha and kappa values for each environment (Halfcheetah, Walker2d, Hopper) and corruption type (observation, action, reward, dynamics, simultaneous). These hyperparameters control the model's behavior and learning process under different corruption scenarios.", "section": "B.2 Implementation Details for TRACER"}, {"figure_path": "rTxCIWsfsD/tables/tables_25_1.jpg", "caption": "Table 7: Average results and standard errors with 2 seeds and 64 batch sizes in Hopper-medium-replay-v2 task for hyperparameter tuning.", "description": "This table presents the average performance and standard errors of TRACER on the Hopper-medium-replay-v2 task, focusing on hyperparameter tuning.  It shows the results under four different types of corruptions (Random Dynamics with \u03ba values of 0.01, 0.1, 0.5, and 1.0; Adversarial Reward with the same \u03ba values). The table is meant to illustrate the impact of the hyperparameter \u03ba (a threshold parameter in the Huber loss function) on the model's robustness under various corruption scenarios, showing how different settings influence the average performance. The bold values highlight the best-performing hyperparameter setting.", "section": "4.3 Evaluation of TRACER under Various Corruption Levels"}, {"figure_path": "rTxCIWsfsD/tables/tables_26_1.jpg", "caption": "Table 8: Average results and standard errors with 2 seeds and 64 batch sizes in Hopper-medium-replay-v2 task under individual corruptions.", "description": "This table presents the average performance and standard errors of RIQL and TRACER (with and without entropy-based uncertainty measure) under various individual corruptions in the Hopper-medium-replay-v2 environment.  The results are averaged over two seeds and using 64 batch sizes.  It showcases the performance difference in each individual corruption type (observation, action, reward, dynamics) for both random and adversarial corruptions.  The `TRACER (New)` row indicates an improved version of TRACER.", "section": "4.3 Evaluation of TRACER under Various Corruption Levels"}, {"figure_path": "rTxCIWsfsD/tables/tables_26_2.jpg", "caption": "Table 9: The average scores and standard errors under random simultaneous corruptions.", "description": "This table presents the average scores and standard errors achieved by IQL, RIQL, and TRACER on four different benchmark tasks (AntMaze-Medium-Play-v2, AntMaze-Medium-Diverse-v2, Walker2d-Medium-Expert-v2, and Hopper-Medium-Expert-v2) under random simultaneous corruptions.  The results demonstrate TRACER's superior performance compared to the baseline methods, showcasing its effectiveness in handling simultaneous corruptions across various environments.", "section": "4 Experiments"}, {"figure_path": "rTxCIWsfsD/tables/tables_27_1.jpg", "caption": "Table 10: Results in Hopper-medium-replay-v2 under various random simultaneous corruption levels.", "description": "This table presents the average scores and standard errors obtained by RIQL and TRACER (the proposed method) on the Hopper-medium-replay-v2 task under different levels of random simultaneous data corruption. The corruption level is controlled by the `corrupt rate c`, with values ranging from 0.1 to 0.5, which correspond to approximately 34.4%, 59.0%, 76.0%, 87.0%, and 93.8% of corrupted data, respectively.  The table demonstrates TRACER's robustness against data corruption and highlights its superior performance compared to RIQL, especially at higher corruption levels.", "section": "4 Experiments"}, {"figure_path": "rTxCIWsfsD/tables/tables_27_2.jpg", "caption": "Table 1: Average scores and standard errors under random and adversarial simultaneous corruptions.", "description": "This table presents the average performance scores and standard errors of various offline reinforcement learning algorithms under conditions of simultaneous random and adversarial data corruptions.  The results are categorized by environment (Halfcheetah, Walker2d, Hopper), type of corruption (random or adversarial), and algorithm (BC, EDAC, MSG, UWMSG, CQL, IQL, RIQL, TRACER).  The table highlights the relative performance of each algorithm when dealing with corrupted data.", "section": "4 Experiments"}]