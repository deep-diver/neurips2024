[{"type": "text", "text": "Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rui Yang1,2, Jie $\\mathbf{Wang^{1,2*}}$ , Guoping $\\mathbf{W}\\mathbf{u}^{1}$ , Bin Li1 1University of Science and Technology of China 2MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition {yr0013, guoping}@mail.ustc.edu.cn {jiewangx, binli}@ustc.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Real-world offline datasets are often subject to data corruptions (such as noise or adversarial attacks) due to sensor failures or malicious attacks. Despite advances in robust offilne reinforcement learning (RL), existing methods struggle to learn robust agents under high uncertainty caused by the diverse corrupted data (i.e., corrupted states, actions, rewards, and dynamics), leading to performance degradation in clean environments. To tackle this problem, we propose a novel robust variational Bayesian inference for offline RL (TRACER). It introduces Bayesian inference for the first time to capture the uncertainty via offline data for robustness against all types of data corruptions. Specifically, TRACER first models all corruptions as the uncertainty in the action-value function. Then, to capture such uncertainty, it uses all offline data as the observations to approximate the posterior distribution of the action-value function under a Bayesian inference framework. An appealing feature of TRACER is that it can distinguish corrupted data from clean data using an entropy-based uncertainty measure, since corrupted data often induces higher uncertainty and entropy. Based on the aforementioned measure, TRACER can regulate the loss associated with corrupted data to reduce its influence, thereby enhancing robustness and performance in clean environments. Experiments demonstrate that TRACER significantly outperforms several state-of-the-art approaches across both individual and simultaneous data corruptions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offline reinforcement learning (RL) aims to learn an effective policy from a fixed dataset without direct interaction with the environment [1, 2]. This paradigm has recently attracted much attention in scenarios where real-time data collection is expensive, risky, or impractical, such as in healthcare [3], autonomous driving [4], and industrial automation [5]. Due to the restriction of the dataset, offline RL confronts the challenge of distribution shift between the policy represented in the offilne dataset and the policy being learned, which often leads to the overestimation for out-of-distribution (OOD) actions [1, 6, 7]. To address this challenge, one of the promising approaches introduce uncertainty estimation techniques, such as using the ensemble of action-value functions or Bayesian inference to measure the uncertainty of the dynamics model [8\u201311] or the action-value function [12\u201315] regarding the rewards and transition dynamics. Therefore, they can constrain the learned policy to remain close to the policy represented in the dataset, guiding the policy to be robust against OOD actions. ", "page_idx": 0}, {"type": "image", "img_path": "rTxCIWsfsD/tmp/a82df40f9e80fa26a810fd31d07eaee8724d8f1e4d901e941b9b581850cf5287.jpg", "img_caption": ["Figure 1: Graphical model of decision-making process. Nodes connected by solid lines denote data points in the offilne dataset, while the Q values (i.e., action values) connected by dashed lines are not part of the dataset. These $\\mathrm{Q}$ values are often objectives that offline algorithms aim to approximate. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Nevertheless, in the real world, the dataset collected by sensors or humans may be subject to extensive and diverse corruptions [16\u201318], e.g., random noise from sensor failures or adversarial attacks during RLHF data collection. Offilne RL methods often assume that the dataset is clean and representative of the environment. Thus, when the data is corrupted, the methods experience performance degradation in the clean environment, as they often constrain policies close to the corrupted data distribution. ", "page_idx": 1}, {"type": "text", "text": "Despite advances in robust offilne RL [2], these approaches struggle to address the challenges posed by diverse data corruptions [18]. Specifically, many previous methods on robust offline RL aim to enhance the testing-time robustness, learning from clean datasets and defending against attacks during testing [19\u201321]. However, they cannot exhibit robust performance using offline dataset with perturbations while evaluating the agent in a clean environment. Some related works for data corruptions (also known as corruption-robust offline RL methods) introduce statistical robustness and stability certification to improve performance, but they primarily focus on enhancing robustness against adversarial attacks [16, 22, 23]. Other approaches focus on the robustness against both random noise and adversarial attacks, but they often aim to address only corruptions in states, rewards, or transition dynamics [24, 17]. Based on these methods, recent work [18] extends the data corruptions to all four elements in the dataset, including states, actions, rewards, and dynamics. This work demonstrates the superiority of the supervised policy learning scheme [25, 26] for the data corruption of each element in the dataset. However, as it does not take into account the uncertainty in decisionmaking caused by the simultaneous presence of diverse corrupted data, this work still encounters difficulties in learning robust agents, limiting its applications in real-world scenarios. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose to use offline data as the observations, thus leveraging their correlations to capture the uncertainty induced by all corrupted data. Considering that (1) diverse corruptions may introduce uncertainties into all elements in the offilne dataset, and (2) each element is correlated with the action values (see dashed lines in Figure 1), there is high uncertainty in approximating the action-value function by using various corrupted data. To address this high uncertainty, we propose to leverage all elements in the dataset as observations, based on the graphical model in Figure 1. By using the high correlations between these observations and the action values [27], we can accurately identify the uncertainty of the action-value function. ", "page_idx": 1}, {"type": "text", "text": "Motivated by this idea, we propose a robust variational Bayesian inference for offilne RL (TRACER) to capture the uncertainty via offilne data against all types of data corruptions. Specifically, TRACER first models all data corruptions as uncertainty in the action-value function. Then, to capture such uncertainty, it introduces variational Bayesian inference [28], which uses all offline data as observations to approximate the posterior distribution of the action-value function. Moreover, the corrupted observed data often induce higher uncertainty than clean data, resulting in higher entropy in the distribution of action-value function. Thus, TRACER can use the entropy as an uncertainty measure to effectively distinguish corrupted data from clean data. Based on the entropy-based uncertainty measure, it can regulate the loss associated with corrupted data in approximating the action-value distribution. This approach effectively reduces the influence of corrupted samples, enhancing robustness and performance in clean environments. ", "page_idx": 1}, {"type": "text", "text": "This study introduces Bayesian inference into offilne RL for data corruptions. It significantly captures the uncertainty caused by diverse corrupted data, thereby improving both robustness and performance in offilne RL. Moreover, it is important to note that, unlike traditional Bayesian online and offilne RL methods that only model uncertainty from rewards and dynamics [29\u201335], our approach identifies the uncertainty of the action-value function regarding states, actions, rewards, and dynamics under data corruptions. We summarize our contributions as follows. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 To the best of our knowledge, this study introduces Bayesian inference into corruption-robust offline RL for the first time. By leveraging all offline data as observations, it can capture uncertainty in the action-value function caused by diverse corrupted data. \u2022 By introducing an entropy-based uncertainty measure, TRACER can distinguish corrupted from clean data, thereby regulating the loss associated with corrupted samples to reduce its influence for robustness. \u2022 Experiment results show that TRACER significantly outperforms several state-of-the-art offline RL methods across a range of both individual and simultaneous data corruptions. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Bayesian RL. We consider a Markov decision process (MDP), denoted by a tuple $\\mathcal{M}\\;=\\;$ $(S,A,\\mathcal{R},P,P_{0},\\gamma)$ , where $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space, $\\mathcal{R}$ is the reward space, $P(\\cdot|s,a)\\in\\mathcal{P}(S)$ is the transition probability distribution over next states conditioned on a stateaction pair $(s,a)$ , $P_{0}(\\cdot)\\in\\mathcal{P}(S)$ is the probability distribution of initial states, and $\\gamma\\in[0,1)$ is the discount factor. Note that ${\\mathcal{P}}(S)$ and $\\bar{\\mathcal{P}}(\\mathcal{A})$ denote the sets of probability distributions on subsets of $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ , respectively. For simplicity, throughout the paper, we use uppercase letters to refer to random variables and lowercase letters to denote values taken by the random variables. Specifically, $R(s,a)$ denotes the random variable of one-step reward following the distribution $\\rho(r|s,a)$ , and $r(s,a)$ represents a value of this random variable. We assume that the random variable of one-step rewards and their expectations are bounded by $R_{\\mathrm{max}}$ and $r_{\\mathrm{max}}$ for any $(s,a)\\in S\\times A$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Our goal is to learn a policy that maximizes the expected discounted cumulative return: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi^{*}=\\underset{\\pi\\in\\mathcal{P}(\\mathcal{A})}{\\arg\\operatorname*{max}}~\\mathbb{E}_{s_{0}\\sim P_{0},a_{t}\\sim\\pi(\\cdot|s_{t}),R\\sim\\rho(\\cdot|s_{t},a_{t}),s_{t+1}\\sim P(\\cdot|s_{t},a_{t})}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Based on the return, we can define the value function as $\\begin{array}{r}{V^{\\pi}(s)=\\mathbb{E}_{\\pi,\\rho,P}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})|s_{0}=s\\right]}\\end{array}$ , the action-value function as $Q^{\\pi}(s,a)=\\mathbb{E}_{R\\sim\\rho(\\cdot|s,a),s^{\\prime}\\sim P(\\cdot|s,a)}\\left[R(s,\\stackrel{\\cdot}{a})+\\overline{{{\\gamma}}}\\tilde{V}^{\\stackrel{\\cdot}{\\pi}}\\right.$ (s\u2032)], and the actionvalue distribution [36] as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{D^{\\pi}(s,a)=\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t}|s_{0}=s,a_{0}=a),}}&{\\mathrm{with~}s_{t+1}\\sim P(\\cdot|s_{t},a_{t}),a_{t+1}\\sim\\pi(\\cdot|s_{t+1}).}\\\\ &{}&\\\\ &{\\mathrm{te~that~}V^{\\pi}(s)=\\mathbb{E}_{a\\sim\\pi}\\left[Q^{\\pi}(s,a)\\right]=\\mathbb{E}_{a\\sim\\pi,\\rho}\\left[D^{\\pi}(s,a)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Variational Inference. Variational inference is a powerful method for approximating complex posterior distributions, which is effective for RL to handle the parameter uncertainty and deal with modelling errors [36]. Given an observation $X$ and latent variables $Z$ , Bayesian inference aims to compute the posterior distribution $p(Z|X)$ . Direct computation of this posterior is often intractable due to the high-dimensional integrals involved. To approximate the true posterior, Bayesian inference introduces a parameterized distribution $q(Z;\\phi)$ and minimizes the Kullback-Leibler (KL) divergence ${\\cal D}_{\\mathrm{KL}}\\bigl(q\\bigl(Z;\\phi\\bigr)\\|p(Z|X)\\bigr)$ . Note that minimizing the KL divergence is equivalent to maximizing the evidence lower bound (ELBO) [37, 38]: $\\operatorname{ELB}\\!{\\mathrm{0}}(\\phi)=\\mathbb{E}_{q(\\mathbf{Z};\\phi)}[\\log p(\\mathbf{X};\\mathbf{Z})-\\log q(\\mathbf{Z};\\phi)]$ . ", "page_idx": 2}, {"type": "text", "text": "Offline RL under Diverse Data Corruptions. In the real world, the data collected by sensors or humans may be subject to diverse corruption due to sensor failures or malicious attacks. Let $b$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ denotes the uncorrupted and corrupted dataset with samples $\\{(s_{t}^{i},a_{t}^{i},r_{t}^{i},s_{t+1}^{i})\\}_{i=1}^{N}$ )}iN=1, respectively. Each data in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ may be corrupted. We assume that an uncorrupted state follows a state distribution $p_{b}(\\cdot)$ , a corrupted state follows $p_{B}(\\cdot)$ , an uncorrupted action follows a behavior policy $\\pi_{b}(\\cdot|s_{t}^{i})$ , a corrupted action is sampled from $\\dot{\\pi}_{B}(\\cdot|s_{t}^{i})$ , a corrupted reward is sampled from $\\rho_{B}(\\cdot|s_{t}^{i},a_{t}^{i})$ , and a corrupted next state is drawn from $\\dot{P_{B}}(\\cdot|s_{t}^{i},a_{t}^{i})$ . We also denote the uncorrupted and corrupted empirical state-action distributions as $p_{b}(s_{t}^{i},a_{t}^{i})$ and $p_{B}(s_{t}^{i},a_{t}^{i})$ , respectively. Moreover, we introduce the notations [18, 39] as follows. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\tilde{\\mathcal{T}}Q(s,a)=\\tilde{r}(s,a)+\\mathbb{E}_{s^{\\prime}\\sim P_{B}(\\cdot\\vert s,a)}\\left[V(s^{\\prime})\\right],\\quad\\tilde{r}(s,a)=\\mathbb{E}_{r\\sim\\rho_{B}(\\cdot\\vert s,a)}[r],}\\\\ &{}&{\\tilde{\\mathcal{T}}D(s,a):=R(s,a)+\\gamma D\\left(s^{\\prime},a^{\\prime}\\right),\\quad s^{\\prime}\\sim P_{B}(\\cdot\\vert s,a),\\ a^{\\prime}\\sim\\pi_{B}(\\cdot\\vert s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for any $(s,a)\\in S\\times A$ and $Q:S\\times A\\mapsto[0,r_{\\operatorname*{max}}/(1-\\gamma)]$ , where $X:=Y$ denotes equality of probability laws, that is the random variable $X$ is distributed according to the same law as $Y$ . ", "page_idx": 3}, {"type": "text", "text": "To address the diverse data corruptions, based on IQL [26], RIQL [18] introduces quantile estimators with an ensemble of action-value functions $\\{Q_{\\theta_{i}}(\\bar{s},a)\\}_{i=1}^{K}$ and employs a Huber regression [40]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{Q}\\left(\\theta_{i}\\right)=\\mathbb{E}_{\\left(s,a,r,s^{\\prime}\\right)\\sim B}\\left[l_{H}^{\\kappa}\\left(r+\\gamma V_{\\psi}\\left(s^{\\prime}\\right)-Q_{\\theta_{i}}(s,a)\\right)\\right],\\ \\ l_{H}^{\\kappa}(x)=\\left\\{\\frac{1}{2\\kappa}x^{2},\\qquad\\middle\\vert\\mathrm{if~}\\vert x\\vert\\leq\\kappa\\right.,}\\\\ &{\\left.\\mathcal{L}_{V}(\\psi)=\\mathbb{E}_{\\left(s,a\\right)\\sim B}\\left[\\mathcal{L}_{2}^{\\nu}\\left(Q_{\\alpha}(s,a)-V_{\\psi}(s)\\right)\\right],\\quad\\mathcal{L}_{2}^{\\nu}(x)=\\left\\vert\\nu-\\mathbb{I}(x<0)\\right\\vert\\cdot x^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that $l_{H}^{\\kappa}$ is the Huber loss, and $Q_{\\alpha}$ is the $\\alpha$ -quantile value among $\\{Q_{\\theta_{i}}(s,a)\\}_{i=1}^{K}$ . RIQL then follows IQL [26] to learn the policy using weighted imitation learning with a hyperparameter $\\beta$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\pi}(\\phi)=\\mathbb{E}_{(s,a)\\sim\\mathcal{B}}\\left[\\exp(\\beta\\cdot A_{\\alpha}(s,a))\\log\\pi_{\\phi}(a|s)\\right],\\quad A_{\\alpha}(s,a)=Q_{\\alpha}(s,a)-V_{\\psi}(s).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first introduce the Bayesian inference for capturing the uncertainty caused by diverse corrupted data in Section 3.1. Then, we provide our algorithm TRACER with the entropy-based uncertainty measure in Section 3.2. Moreover, we provide the theoretical analysis for robustness, the architecture, and the detailed implementation of TRACER in Appendices A.1, B.1, and B.2, respectively. ", "page_idx": 3}, {"type": "text", "text": "3.1 Variational Inference for Uncertainty induced by Corrupted Data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We focus on corruption-robust offilne RL to learn an agent under diverse data corruptions, i.e., random or adversarial attacks on four elements of the dataset. We propose to use all elements as observations, leveraging the data correlations to simultaneously address the uncertainty. By introducing Bayesian inference framework, our aim is to approximate the posterior distribution of the action-value function. ", "page_idx": 3}, {"type": "text", "text": "At the beginning, based on the relationships between the action values and the four elements (i.e., states, actions, rewards, next states) in the offline dataset as shown in Figure 1, we define $D_{\\theta}=D_{\\theta}(S,A,R)\\sim p_{\\theta}(\\cdot|S,A,R)$ , parameterized by $\\theta$ . Building on the action-value distribution, we can explore how to estimate the posterior of $D_{\\theta}$ using the elements available in the offline data. ", "page_idx": 3}, {"type": "text", "text": "Firstly, we start from the actions $\\{a_{t}^{i}\\}_{i=1}^{N}$ following the corrupted distribution $\\pi_{B}$ and use them as observations to approximate the posterior of the action-value distribution under a variational inference. As the actions are correlated with the action values and all other elements in the dataset, the likelihood is $p_{\\varphi_{a}}(A|D,S,R,S^{\\prime})$ , parameterized by $\\varphi_{a}$ . Then, under the variational inference framework, we maximize the posterior and derive to minimize the loss function based on ELBO: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D|A}(\\theta,\\varphi_{a})=\\mathbb{E}_{B,p_{\\theta}}\\left[\\mathcal{D}_{\\mathrm{KL}}\\big(p_{\\varphi_{a}}(A|D_{\\theta},S,R,S^{\\prime})\\,\\|\\,\\pi_{B}(A|S)\\big)-\\mathbb{E}_{A\\sim p_{\\varphi_{a}}}\\left[\\log p_{\\theta}(D_{\\theta}|S,A,R)\\right]\\right]_{\\mathfrak{p}_{A}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S,R$ , and $S^{\\prime}$ follow the offline data distributions $p_{B},\\,\\rho_{B}$ , and $P_{B}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "Secondly, we apply the rewards $\\{r_{t}^{i}\\}_{i=1}^{N}$ drawn from the corrupted reward distribution $\\rho_{B}$ as the observations. Considering that the rewards are related to the states, actions, and action values, we model the likelihood as $\\bar{p}_{\\varphi_{r}}(R|D,S,A)$ , parameterized by $\\varphi_{r}$ . Therefore, we can derive a loss function by following Equation (7): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{D|R}(\\theta,\\varphi_{r})=\\mathbb{E}_{B,p_{\\theta}}\\left[\\mathcal{D}_{\\mathrm{KL}}\\big(p_{\\varphi_{r}}(R|D_{\\theta},S,A)\\,\\|\\,\\rho_{B}(R|S,A)\\big)-\\mathbb{E}_{R\\sim p_{\\varphi_{r}}}\\left[\\log p_{\\theta}(D_{\\theta}|S,A,R)\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S$ and $A$ follow the offline data distributions $p_{B}$ and $\\pi_{B}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "Finally, we employ the state $\\{s_{t}^{i}\\}_{i=1}^{N}$ in the offilne dataset following the corrupted distribution $p_{B}$ as the observations. Due to the relation of the states, we can model the likelihood as $p_{\\varphi_{s}}(S|D,A,R)$ , parameterized by $\\varphi_{r}$ . We then have the loss function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{D|S}(\\theta,\\varphi_{s})=\\mathbb{E}_{B,p_{\\theta}}\\left[\\mathcal{D}_{\\mathrm{KL}}\\big(p_{\\varphi_{s}}(S|D_{\\theta},A,R)\\,|\\,|\\,p_{B}(S)\\big)-\\mathbb{E}_{S\\sim p_{\\varphi_{s}}}\\left[\\log p_{\\theta}(D_{\\theta}|S,A,R)\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A$ and $R$ follow the offline data distributions $\\pi_{B}$ and $\\rho_{B}$ , respectively. We present the detailed derivation process in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "The goal of first terms in Equations (7), (8), and (9) is to estimate $\\pi_{B}(A|S)$ , $\\rho_{B}(R|S,A)$ , and $p_{B}(S)$ using $p_{\\varphi_{a}}(A|D_{\\theta},S,R,S^{\\prime})$ , $p_{\\varphi_{r}}(R|D_{\\theta},S,A)$ , and $p_{\\varphi_{s}}(S|D_{\\theta},A,R)$ , respectively. As we do not have the explicit expression of distributions $\\pi_{B},\\,\\rho_{B}$ , and $p_{B}$ , we cannot directly compute the KL divergence in these first terms. To address this issue, based on the generalized Bayesian inference [41], we can exchange two distributions in the KL divergence. Then, we model all the aformentioned distributions as Gaussian distributions, and use the mean $\\mu_{\\varphi}$ and standard deviation $\\Sigma_{\\varphi}$ to represent the corresponding $p_{\\varphi}$ . For implementation, we directly employ MLPs to output each $(\\dot{\\mu}_{\\varphi},\\Sigma_{\\varphi})$ using the corresponding conditions of $p_{\\varphi}$ . Then, based on the KL divergence between two Gaussian distributions, we can derive the loss function as follows. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\hat{\\cdot}_{\\mathrm{first}}(\\theta,\\varphi_{s},\\varphi_{a},\\varphi_{r})=\\frac{1}{2}\\mathbb{E}_{(s,a,r)\\sim B,D_{\\theta}\\sim p_{\\theta}}\\left[(\\mu_{\\varphi_{a}}-a)^{T}\\Sigma_{\\varphi_{a}}^{-1}(\\mu_{\\varphi_{a}}-a)+(\\mu_{\\varphi_{r}}-r)^{T}\\Sigma_{\\varphi_{r}}^{-1}(\\mu_{\\varphi_{r}}-r)\\right.}}\\\\ &{}&{\\left.+(\\mu_{\\varphi_{s}}-s)^{T}\\Sigma_{\\varphi_{s}}^{-1}(\\mu_{\\varphi_{s}}-s)+\\log|\\Sigma_{\\varphi_{a}}|\\cdot|\\Sigma_{\\varphi_{r}}|\\cdot|\\Sigma_{\\varphi_{s}}|\\right],\\quad\\quad\\quad\\quad(10)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, the goal of second terms in Equations (7), (8), and (9) is to maximize the likelihoods of $D_{\\theta}$ given samples $\\hat{s}\\sim p_{\\varphi_{s}}$ , $\\hat{a}\\sim p_{\\varphi_{a}}$ , or $\\hat{r}\\sim p_{\\varphi_{r}}$ . Thus, with $(s,a,r)\\sim B$ , we propose minimizing the distance between $D_{\\theta}(\\hat{s},a,r)$ and $D(s,a,r)$ , $D_{\\theta}(s,\\hat{a},r)$ and $D(s,a,r)$ , and $D_{\\theta}\\big(s,a,\\hat{r}\\big)$ and $D(s,a,r)$ , where $\\hat{s}\\sim p_{\\varphi_{s}}$ , $\\hat{a}\\sim p_{\\varphi_{a}}$ , and $\\hat{r}\\sim p_{\\varphi_{r}}$ . Then, based on [41], we can derive the following loss with any metric $\\ell$ to maximize the log probabilities: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{{\\mathcal{L}}_{\\mathrm{second}}(\\theta,\\varphi_{s},\\varphi_{a},\\varphi_{r})=\\mathbb{E}_{(s,a,r)\\sim{\\cal B},\\hat{s}\\sim p_{\\varphi_{s}},\\hat{a}\\sim p_{\\varphi_{a}},\\hat{r}\\sim p_{\\varphi_{r}},{\\cal D}\\sim p}\\left[\\ell\\big({\\cal D}(s,a,r),{\\cal D}_{\\theta}(s,\\hat{a},r)\\big)\\right.}\\\\ &{}&{\\quad\\left.+\\;\\ell\\big({\\cal D}(s,a,r),{\\cal D}_{\\theta}(s,a,\\hat{r})\\big)+\\ell\\big({\\cal D}(s,a,r),{\\cal D}_{\\theta}(\\hat{s},a,r)\\big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2 Corruption-Robust Algorithm with the Entropy-based Uncertainty Measure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We focus on developing tractable loss functions for implementation in this subsection. ", "page_idx": 4}, {"type": "text", "text": "Learning the Action-Value Distribution based on Temporal Difference $(T D)$ . Based on [42, 43], we introduce the quantile regression [44] to approximate the action-value distribution in the offline dataset $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ using an ensemble model $\\{\\overline{{D_{\\theta_{i}}}}\\}_{i=1}^{K}$ . We use Equation (4) to derive the loss as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D}\\left(\\boldsymbol{\\theta}_{i}\\right)=\\mathbb{E}_{\\left(s,a,r,s^{\\prime}\\right)\\sim\\mathcal{B}}\\left[\\frac{1}{N^{\\prime}}\\sum_{n=1}^{N}\\sum_{m=1}^{N^{\\prime}}\\rho_{\\tau}^{\\kappa}\\left(\\delta_{\\boldsymbol{\\theta}_{i}}^{\\tau_{n},\\tau_{m}^{\\prime}}\\right)\\right],\\quad\\delta_{\\boldsymbol{\\theta}_{i}}^{\\tau,\\tau^{\\prime}}=r+\\gamma Z^{\\tau^{\\prime}}\\left(s^{\\prime}\\right)-D_{\\boldsymbol{\\theta}_{i}}^{\\tau}\\left(s,a,r\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\rho_{\\tau}^{\\kappa}\\left(\\delta\\right)=\\left|\\tau-\\mathbb{I}\\left\\{\\delta<0\\right\\}\\right|\\cdot l_{H}^{\\kappa}\\left(\\delta\\right)$ with the threshold $\\kappa,Z$ denotes the value distribution, $\\delta_{\\theta_{i}}^{\\tau,\\tau^{\\prime}}$ is the sampled TD error based on the parameters $\\theta_{i},\\tau$ and $\\tau^{\\prime}$ are two samples drawn from a uniform distribution $U([0,1])$ $),D_{\\theta}^{\\tau}(s,a,r):=F_{D_{\\theta}(s,a,r)}^{-1}(\\tau)$ is the sample drawn from $p_{\\theta}(\\cdot|s,a,r)$ , $Z^{\\tau}(s):=$ $F_{Z(s)}^{-1}(\\tau)$ is sampled from $p(\\cdot|s),F_{X}^{-1}(\\tau)$ is the inverse cumulative distribution function (also known as quantile function) [45] at $\\tau$ for the random variable $X$ , and $N$ and $N^{\\prime}$ represent the respective number of iid samples $\\tau$ and $\\tau^{\\prime}$ . Notably, based on [43], we have $\\begin{array}{r}{Q_{\\theta_{i}}(s,a)=\\dot{\\sum_{n=1}^{N}}D_{\\theta_{i}}^{\\tau_{n}}(s,a,r)}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "In addition, if we learn the value distribution $Z$ , the action-value distribution can extract the information from the next states based on Equation (12), which is effective for capturing the uncertainty. On the contrary, if we directly use the next states in the offline dataset as the observations, in practice, the parameterized model of the action-value distribution needs to take $(s,a,r,s^{\\prime},a^{\\prime},r^{\\prime},s^{\\prime\\prime})$ as the input data. Thus, the model can compute the action values and values for the sampled TD error in Equation (12). To avoid the changes in the input data caused by directly using next states as observations in Bayesian inference, we draw inspiration from IQL and RIQL to learn a parameterized value distribution. Based on Equations (5) and (12), we derive a new objective as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{Z}(\\psi)=\\mathbb{E}_{(s,a)\\sim\\mathcal{B}}\\left[\\sum_{n=1}^{N}\\mathcal{L}_{2}^{\\nu}\\left(D_{\\alpha}^{\\tau_{n}}(s,a,r)-Z_{\\psi}^{\\tau_{n}}(s)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $D_{\\alpha}^{\\tau}$ is the $\\alpha$ -quantile value among $\\{D_{\\theta_{i}}^{\\tau}(s,a)\\}_{i=1}^{K}$ , and $\\begin{array}{r}{V_{\\psi}(s)=\\sum_{n=1}^{N}Z_{\\psi}^{\\tau_{n}}(s)}\\end{array}$ . More details are shown in Appendix B.2. Furthermore, we provide the theoretical analysis in Appendix A.1 to give a value bound between the value distributions under clean and corrupted data. ", "page_idx": 4}, {"type": "text", "text": "Updating the Action-Value Distribution based on Variational Inference for Robustness. We discuss the detailed implementation of Equations (10) and (11) based on Equations (12) and (13). As the data ", "page_idx": 4}, {"type": "text", "text": "corruptions may introduce heavy-tailed targets [18], we apply the Huber loss to replace all quadratic loss in Equation (10) and the metric $\\ell$ in Equation (11), mitigating the issue caused by heavy-tailed targets [46] for robustness. We rewrite Equation (11) as follows. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{L}_{\\mathrm{second}}(\\theta_{i},\\varphi_{s},\\varphi_{a},\\varphi_{r})=\\mathbb{E}_{(s,a,r)\\sim B,\\hat{s}\\sim p_{\\varphi_{s}},\\hat{u}\\sim p_{\\varphi_{a}},\\hat{r}\\sim p_{\\varphi_{r}},D^{\\tau}\\sim p}\\left[\\displaystyle\\sum_{n=1}^{N}l_{H}^{\\kappa}\\big(D^{\\tau_{n}}(s,a,r),D_{\\theta_{i}}^{\\tau_{n}}(s,\\hat{a},r)\\big)\\right.}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\left.l_{H}^{\\kappa}\\big(D^{\\tau_{n}}(s,a,r),D_{\\theta_{i}}^{\\tau_{n}}(s,a,\\hat{r})\\big)+l_{H}^{\\kappa}\\big(D^{\\tau_{n}}(s,a,r),D_{\\theta_{i}}^{\\tau_{n}}(\\hat{s},a,r)\\big)\\right]\\cdot\\mathrm{~(14)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, we have the whole loss function $\\mathcal{L}_{D|S,A,R}=\\mathcal{L}_{\\mathrm{first}}(\\theta_{i},\\varphi_{s},\\varphi_{a},\\varphi_{r})+\\mathcal{L}_{\\mathrm{second}}(\\theta_{i},\\varphi_{s},\\varphi_{a},\\varphi_{r})$ in the generalized variational inference framework. Moreover, based on the assumption of heavy-tailed noise in [18], we have a upper bound of action-value distribution by using the Huber regression loss. ", "page_idx": 5}, {"type": "text", "text": "Entropy-based Uncertainty Measure for Regulating the Loss associated with Corrupted Data. To further address the challenge posed by diverse data corruptions, we consider the problem: how to exploit uncertainty to further enhance robustness? ", "page_idx": 5}, {"type": "text", "text": "Considering that our goal is to improve performance in clean environments, we propose to reduce the influence of corrupted data, focusing on using clean data to learn agents. Therefore, we provide a two-step plan: (1) distinguishing corrupted data from clean data; (2) regulating the loss associated with corrupted data to reduce its influence, thus enhancing the performance in clean environments. ", "page_idx": 5}, {"type": "text", "text": "For (1), as the Shannon entropy for the measures of aleatoric and epistemic uncertainties provides important insight [47\u201349], and the corrupted data often results in higher uncertainty and entropy of the action-value distribution than the clean data, we use entropy [50] to quantify uncertainties of corrupted and clean data. Furthermore, by considering that the exponential function can amplify the numerical difference in entropy between corrupted and clean data, we propose the use of exponential entropy [51]\u2014a metric of extent of a distribution\u2014to design our uncertainty measure. ", "page_idx": 5}, {"type": "text", "text": "Specifically, based on Equation 12, we can use the quantile points $\\{\\tau_{n}\\}_{n=1}^{N}$ to learn the corresponding quantile function values $\\{D^{\\tau_{n}}\\}_{n=1}^{N}$ drawn from the action-value distribution $p_{\\theta}$ . We sort the quantile points and their corresponding function values in ascending order based on the values. Thus, we have the sorted sets $\\{\\varsigma_{n}\\}_{n=1}^{N}$ , $\\bar{\\{D^{\\varsigma_{n}}\\}}_{n=1}^{N}$ , and the estimated PDF values $\\{\\overline{{\\varsigma}}_{n}\\}_{n=1}^{N}$ , where $\\overline{{\\varsigma}}_{1}=\\varsigma_{1}$ and $\\overline{{\\mathsf{S}}}_{n}=\\mathsf{S}_{n}-\\mathsf{S}_{n-1}$ for $1<n\\leq N$ . Then, we can further estimate differential entropy following [52] (see Appendix A.3 for a detailed derivation). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}(p_{\\theta_{i}}(\\cdot|s,a,r))=-\\sum_{n=1}^{N}\\hat{\\varsigma}_{n}\\cdot\\overline{{D}}_{\\theta_{i}}^{\\varsigma_{n}}(s,a,r)\\cdot\\log\\hat{\\varsigma}_{n},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\varsigma}_{n}$ denotes $(\\overline{{\\varsigma}}_{n-1}+\\overline{{\\varsigma}}_{n})/2$ for $1<n\\leq N$ , and ${\\overline{{D}}}^{S n}$ denotes $D^{\\varsigma_{n}}\\,-\\,D^{\\varsigma_{n}-1}$ for $1<n\\leq N$ . ", "page_idx": 5}, {"type": "text", "text": "For (2), TRACER employs the reciprocal value of exponential entropy $1/\\exp(\\mathcal{H}(p_{\\theta_{i}}))$ to weight the corresponding loss of $\\theta_{i}$ in our proposed whole loss function $\\mathcal{L}_{D|S,A,R}$ . Therefore, during the learning process, TRACER can regulate the loss associated with corrupted data and focus on minimizing the loss associated with clean data, enhancing robustness and performance in clean environments. Note that we normalize entropy values by dividing the mean of samples (i.e., quantile function values) drawn from action-value distributions for each batch. In Figure 3, we show the relationship of entropy values of corrupted and clean data estimated by Equation (15) during the learning process. The results illustrate the effectiveness of the entropy-weighted technique for data corruptions. ", "page_idx": 5}, {"type": "text", "text": "Updating the Policy based on the Action-Value Distribution. We directly applies the weighted imitation learning technique in Equation (6) to learn the policy. As $Q_{\\alpha}(s,a)$ is the $\\alpha$ -quantile value among $\\begin{array}{r}{\\{Q_{\\theta_{i}}(s,a)\\}_{i=1}^{K}=\\left\\{\\sum_{n=1}^{N}D_{\\theta_{i}}^{\\tau_{n}}(s,a,r)\\right\\}_{i=1}^{K}}\\end{array}$ and $\\begin{array}{r}{V_{\\psi}(s)=\\sum_{n=1}^{N}Z_{\\psi}^{\\tau_{n}}(s)}\\end{array}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\pi}(\\phi)=\\mathbb{E}_{(s,a)\\sim\\mathcal{B}}\\left[\\exp\\left(\\beta\\cdot Q_{\\alpha}(s,a)-\\beta\\cdot\\sum_{n=1}^{N}Z_{\\psi}^{\\tau_{n}}(s)\\right)\\cdot\\log\\pi_{\\phi}(a|s)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/148489cea7ebfc907dfff6498e9c3ae1520623104198d197a4260a40d0d31da7.jpg", "table_caption": ["Table 1: Average scores and standard errors under random and adversarial simultaneous corruptions. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/7376d4f4acd593693d9a307b24cc752718cb43e0baf2bdeaebde776b99e5ac38.jpg", "table_caption": ["Table 2: Average score under diverse random corruptions. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/94ca41371975ff912b5a3fb82eb51362685887233f5bdabb24b331f14c1291b3.jpg", "table_caption": ["Table 3: Average score under diverse adversarial corruptions. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we show the effectiveness of TRACER across various simulation tasks using diverse corrupted offline datasets. Firstly, we provide our experiment setting, focusing on the corruption settings for offilne datasets. Then, we illustrate how TRACER significantly outperforms previous stateof-the-art approaches under a range of both individual and simultaneous data corruptions. Finally, we conduct validation experiments and ablation studies to show the effectiveness of TRACER. ", "page_idx": 6}, {"type": "text", "text": "4.1 Experiment Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Building upon RIQL [18], we use two hyperparameters, i.e., corruption rate $c\\in[0,1]$ and corruption scale $\\epsilon$ , to control the corruption level. Then, we introduce the random corruption and adversarial corruption in four elements (i.e., states, actions, rewards, next states) of offline datasets. The implementation of random corruption is to add random noise to elements of a $c$ portion of the offilne datasets, and the implementation of adversarial corruption follows the Projected Gradient Descent attack [53, 54] using pretrained value functions. Note that unlike other adversarial corruptions, the adversarial reward corruption multiplies $-\\epsilon$ to the clean rewards instead of using gradient optimization. We also introduce the random or adversarial simultaneous corruption, which refers to random or adversarial corruption simultaneously present in four elements of the offline datasets. We apply the corruption rate $c=0.3$ and corruption scale $\\epsilon=1.0$ in our experiments. ", "page_idx": 6}, {"type": "image", "img_path": "rTxCIWsfsD/tmp/e40b7a6fb6fb9710a2d35e6c9f87fd82ed3377b2cb65db9aceb3c4d1f9fb39cf.jpg", "img_caption": ["Figure 2: In the left, we report the means and standard deviations on CARLA under random simultaneous corruptions. In the right, we report the results with random simultaneous corruptions against different corruption levels. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We conduct experiments on D4RL benchmark [55]. Referring to RIQL, we train all agents for 3000 epochs on the \u2019medium-replay-v2\u2019 dataset, which closely mirrors real-world applications as it is collected during the training of a SAC agent. Then, we evaluate agents in clean environments, reporting the average normalized performance over four random seeds. See Appendix C for detailed information. The algorithms we compare include: (1) CQL [7] and IQL [26], offline RL algorithms using a twin Q networks. (2) EDAC [56] and MSG [57], offilne RL algorithms using ensemble Q networks (number of ensembles $>2$ ). (3) UWMSG [17] and RIQL, state-of-the-arts in corruption-robust offline RL. Note that EDAC, MSG, and UWMSG are all uncertainty-based offline RL algorithms. ", "page_idx": 7}, {"type": "text", "text": "4.2 Main results under Diverse Data Corruptions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments on MuJoCo [58] (see Tables 1, 2, and 3, which highlight the highest results) and CARLA [59] (see the left of Figure 2) tasks from D4RL under diverse corruptions to show the superiority of TRACER. In Table 1, we report all results under random or adversarial simultaneous data corruptions. These results show that TRACER significantly outperforms other algorithms in all tasks, achieving an average score improvement of $\\mathbf{\\bar{+21.1\\%}}$ . In the left of Figure 2, results on \u2019CARLA-lane_v0\u2019 under random simultaneous corruptions also illustrate the superiority of TRACER. See Appendix C.2 for details. ", "page_idx": 7}, {"type": "text", "text": "Random Corruptions. We report the results under random simultaneous data corruptions of all algorithms in Table 1. Such results demonstrate that TRACER achieves an average score gain of $\\pm{\\dot{\\mathbf{2}}}\\mathbf{2}.4\\%$ under the setting of random simultaneous corruptions. Based on the results, it is clear that many offline RL algorithms, such as EDAC, MSG, and CQL, suffer the performance degradation under data corruptions. Since UWMSG is designed to defend the corruptions in rewards and dynamics, its performance degrades when faced with the stronger random simultaneous corruption. Moreover, we report results across a range of individual random data corruptions in Table 2, where TRACER outperforms previous algorithms in 7 out of 12 settings. We then explore hyperparameter tuning on Hopper task and further improve TRACER\u2019s results, demonstrating its potential for performance gains. We provide details in Appendix C.3.1. ", "page_idx": 7}, {"type": "text", "text": "Adversarial Corruptions. We construct experiments under adversarial simultaneous corruptions to evaluate the robustness of TRACER. The results in Table 1 show that TRACER surpasses others by a significant margin, achieving an average score improvement of $+19.3\\%$ . In these simultaneous corruption, many algorithms experience more severe performance degradation compared to the random simultaneous corruption, which indicates that adversarial attacks are more damaging to the reliability of algorithms than random noise. Despite these challenges, TRACER consistently achieves significant performance gains over other methods. Moreover, we provide the results across a range of individual adversarial data corruptions in Table 3, where TRACER outperforms previous algorithms in 7 out of 12 settings. We also explore hyperparameter tuning on Hopper task and further improve TRACER\u2019s results, demonstrating its potential for performance gains. See Appendix C.3.1 for details. ", "page_idx": 7}, {"type": "image", "img_path": "rTxCIWsfsD/tmp/ba7b63cb30265c5c4b134e3b566bef24288c8f4be6b7803d10e89ac9f921a605.jpg", "img_caption": ["Figure 3: In the first column, we report the mean and standard deviation to show the superiority of using the entropy-based uncertainty measure. In the second and third columns, we report the results over three seeds to show the higher entropy of corrupted data compared to clean data during training. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Evaluation of TRACER under Various Corruption Levels ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Building upon RIQL, we further extend our experiments to include Mujoco datasets with various corruption levels, using different corruption rates $c$ and scales $\\epsilon$ . We report the average scores and standard deviations over four random seeds in the right of Figure 2, using batch sizes of 256. ", "page_idx": 8}, {"type": "text", "text": "Results in the right of Figure 2 demonstrate that TRACER significantly outperforms baseline algorithms in all tasks under random simultaneous corruptions with various corruption levels. It achieves an average score improvement of $+33.6\\%$ . Moreover, as the corruption levels increase, the slight decrease in TRACER\u2019s results indicates that while TRACER is robust to simultaneous corruptions, its performance depends on the extent of corrupted data it encounters. We also evaluate TRACER in different scales of corrupted data and provide the results in Appendix C.4.1. ", "page_idx": 8}, {"type": "text", "text": "4.4 Evaluation of the Entropy-based Uncertainty Measure ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate the entropy-based uncertainty measure in action-value distributions to show: (1) is the uncertainty caused by corrupted data higher than that of clean data? (2) is regulating the loss associated with corrupted data effective in improving performance? ", "page_idx": 8}, {"type": "text", "text": "For (1), we first introduce labels indicating whether the data is corrupted. Importantly, these labels are not used by agents during the training process. Then, we estimate entropy values of labelled corrupted and clean data in each batch based on Equation (15). Thus, we can compare entropy values to compute results, showing how many times the entropy of the corrupted data is higher than that of clean data. Specifically, we evaluate the accuracy every 50 epochs over 3000 epochs. For each evaluation, we sample 500 batches to compute the average entropy of corrupted and clean data. Each batch consists of 32 clean and 32 corrupted data. We illustrate the curves over three seeds in the second and third columns of Figure 3, where each point shows how many of the 500 batches have higher entropy for corrupted data than that of clean data. ", "page_idx": 8}, {"type": "text", "text": "Figure 3 indicates an oscillating upward trend of TRACER\u2019s measurement accuracy using entropy (TRACER Using Entro) under simultaneous corruptions, demonstrating that using the entropy-based uncertainty measure can effectively distinguish corrupted data from clean data. These curves also reveal that even in the absence of any constraints on entropy (TRACER NOT using Entro), the entropy associated with corrupted data tends to exceed that of clean data. ", "page_idx": 8}, {"type": "text", "text": "For (2), in the first column of Figure 3, these results demonstrate that TRACER using the entropybased uncertainty measure can effectively reduce the influence of corrupted data, thereby enhancing robustness and performance against all corruptions. We provide detailed information for this evaluation in Appendix C.4.2. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Robust RL. Robust RL can be categorized into two types: testing-time robust RL and training-time robust RL. Testing-time robust RL [19, 20] refers to training a policy on clean data and ensuring its robustness by testing in an environment with random noise or adversarial attacks. Training-time robust RL [16, 17] aims to learn a robust policy in the presence of random noise or adversarial attacks during training and evaluate the policy in a clean environment. In this paper, we focus on training-time robust RL under the offilne setting, where the offilne training data is subject to various data corruptions, also known as corruption-robust offline RL. ", "page_idx": 9}, {"type": "text", "text": "Corruption-Robust RL. Some theoretical work on corruption-robust online RL [60\u201363] aims to analyze the sub-optimal bounds of learned policies under data corruptions. However, these studies primarily address simple bandits or tabular MDPs and focus on the reward corruption. Some further work [64, 65] extends the modeling problem to more general MDPs and begins to investigate the corruption in transition dynamics. ", "page_idx": 9}, {"type": "text", "text": "It is worth noting that corruption-robust offline RL has not been widely studied. UWMSG [17] designs a value-based uncertainty-weighting technique, thus using the weight to mitigate the impact of corrupted data. RIQL [18] further extends the data corruptions to all four elements in the offline dataset, including states, actions, rewards, and next states (dynamics). It then introduces quantile estimators with an ensemble of action-value functions and employs a Huber regression based on IQL [26], alleviating the performance degradation caused by corrupted data. ", "page_idx": 9}, {"type": "text", "text": "Bayesian RL. Bayesian RL integrates the Bayesian inference with RL to create a framework for decision-making under uncertainty [28]. It is important to highlight that Bayesian RL is divided into two categories for different uncertainties: the parameter uncertainty in the learning of models [66, 67] and the inherent uncertainty from the data/environment in the distribution over returns [68, 69]. In this paper, we focus on capturing the latter. ", "page_idx": 9}, {"type": "text", "text": "For the latter uncertainty, in model-based Bayesian RL, many approaches [68, 70, 71] explicitly model the transition dynamics and using Bayesian inference to update the model. It is useful when dealing with complex environments for sample efficiency. In model-free Bayesian RL, value-based methods [69, 72] use the reward information to construct the posterior distribution of the action-value function. Besides, policy gradient methods [73, 74] use information of the return to construct the posterior distribution of the policy. They directly apply Bayesian inference to the value function or policy without explicitly modeling transition dynamics. ", "page_idx": 9}, {"type": "text", "text": "Offilne Bayesian RL. offilne Bayesian RL integrates Bayesian inference with offilne RL to tackle the challenges of learning robust policies from static datasets without further interactions with the environment. Many approaches [75\u201377] use Bayesian inference to model the transition dynamics or guide action selection for adaptive policy updates, thereby avoiding overly conservative estimates in the offilne setting. Furthermore, recent work [78] applies variational Bayesian inference to learn the model of transition dynamics, mitigating the distribution shift in offline RL. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigate and demonstrate the robustness and effectiveness of introducing Bayesian inference into offilne RL to address the challenges posed by data corruptions. By leveraging Bayesian techniques, our proposed approach TRACER captures the uncertainty caused by diverse corrupted data. Moreover, the use of entropy-based uncertainty measure in TRACER can distinguish corrupted data from clean data. Thus, TRACER can regulate the loss associated with corrupted data to reduce its influence, improving performance in clean environments. Our extensive experiments demonstrate the potential of Bayesian methods in developing reliable decision-making. ", "page_idx": 9}, {"type": "text", "text": "Regarding the limitations of TRACER, although it achieves significant performance improvement under diverse data corruptions, future work could explore more complex and realistic data corruption scenarios and related challenges, such as the noise in the preference data for RLHF and adversarial attacks on safety-critical driving decisions. Moreover, we look forward to the continued development and optimization of uncertainty-based corrupted-robust offilne RL, which could further enhance the effectiveness of TRACER and similar approaches for increasingly complex real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank all the anonymous reviewers for their insightful comments. This work was supported in part by National Key R&D Program of China under contract 2022ZD0119801, National Nature Science Foundations of China grants U23A20388 and 62021001, and DiDi GAIA Collaborative Research Funds. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2052\u20132062. PMLR, 2019.   \n[2] Rafael Figueiredo Prudencio, Marcos R. O. A. M\u00e1ximo, and Esther Luna Colombini. A survey on offilne reinforcement learning: Taxonomy, review, and open problems. CoRR, abs/2203.01387, 2022.   \n[3] Shengpu Tang and Jenna Wiens. Model selection for offilne reinforcement learning: Practical considerations for healthcare settings. In Ken Jung, Serena Yeung, Mark P. Sendak, Michael W. Sjoding, and Rajesh Ranganath, editors, Proceedings of the Machine Learning for Healthcare Conference, volume 149 of Proceedings of Machine Learning Research, pages 2\u201335. PMLR, 2021.   \n[4] Christopher Diehl, Timo Sievernich, Martin Kr\u00fcger, Frank Hoffmann, and Torsten Bertram. Uncertaintyaware model-based offline reinforcement learning for automated driving. IEEE Robotics Autom. Lett., 8(2):1167\u20131174, 2023.   \n[5] Tony Z. Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Nicolas Heess, Jon Scholz, Stefan Schaal, and Sergey Levine. Offline meta-reinforcement learning for industrial insertion. In 2022 International Conference on Robotics and Automation, pages 6386\u20136393. IEEE, 2022.   \n[6] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32, pages 11761\u201311771, 2019.   \n[7] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offilne reinforcement learning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022.   \n[8] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. MOPO: model-based offline policy optimization. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33, 2020.   \n[9] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33, 2020.   \n[10] Porter Jenkins, Hua Wei, J. Stockton Jenkins, and Zhenhui Li. Bayesian model-based offilne reinforcement learning for product allocation. In Thirty-Sixth AAAI Conference on Artificial Intelligence, pages 12531\u2013 12537. AAAI Press, 2022.   \n[11] Sen Lin, Jialin Wan, Tengyu Xu, Yingbin Liang, and Junshan Zhang. Model-based offline metareinforcement learning with regularization. In The Tenth International Conference on Learning Representations. OpenReview.net, 2022.   \n[12] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offilne reinforcement learning with diversified $\\mathbf{q}_{}$ -ensemble. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 7436\u20137447, 2021.   \n[13] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11319\u201311328. PMLR, 2021.   \n[14] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offilne reinforcement learning. In The Tenth International Conference on Learning Representations. OpenReview.net, 2022.   \n[15] Filippo Valdettaro and A. Aldo Faisal. Towards offilne reinforcement learning with pessimistic value priors. In Fabio Cuzzolin and Maryam Sultana, editors, Epistemic Uncertainty in Artificial Intelligence - First International Workshop, volume 14523 of Lecture Notes in Computer Science, pages 89\u2013100. Springer, 2024.   \n[16] Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun. Corruption-robust offilne reinforcement learning. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 5757\u20135773. PMLR, 2022.   \n[17] Chenlu Ye, Rui Yang, Quanquan Gu, and Tong Zhang. Corruption-robust offline reinforcement learning with general function approximation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36, 2023.   \n[18] Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han, and Tong Zhang. Towards robust offilne reinforcement learning under diverse data corruption. In The Eleventh International Conference on Learning Representations, 2023.   \n[19] Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. Robust reinforcement learning using offline data. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022.   \n[20] Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han. RORL: robust offilne reinforcement learning via conservative smoothing. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022.   \n[21] Jose H. Blanchet, Miao Lu, Tong Zhang, and Han Zhong. Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36, 2023.   \n[22] Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement learning and control. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32, pages 14543\u201314553, 2019.   \n[23] Fan Wu, Linyi Li, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding Zhao, and Bo Li. COPA: certifying robust policies for offilne reinforcement learning against poisoning attacks. In The Tenth International Conference on Learning Representations. OpenReview.net, 2022.   \n[24] Zhihe Yang and Yunjian Xu. Dmbp: Diffusion model based predictor for robust offline reinforcement learning against state observation perturbations. In The Twelfth International Conference on Learning Representations, 2023.   \n[25] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. CoRR, abs/1910.00177, 2019.   \n[26] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In The Tenth International Conference on Learning Representations. OpenReview.net, 2022.   \n[27] Rens van de Schoot, Sarah Depaoli, Ruth King, Bianca Kramer, Kaspar M\u00e4rtens, Mahlet G Tadesse, Marina Vannucci, Andrew Gelman, Duco Veen, Joukje Willemsen, et al. Bayesian statistics and modelling. Nature Reviews Methods Primers, 1(1):1, 2021.   \n[28] Box George EP and Tiao George C. Bayesian inference in statistical analysis. John Wiley & Sons, 2011.   \n[29] Esther Derman, Daniel J. Mankowitz, Timothy A. Mann, and Shie Mannor. A bayesian approach to robust reinforcement learning. In Amir Globerson and Ricardo Silva, editors, Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, volume 115 of Proceedings of Machine Learning Research, pages 648\u2013658. AUAI Press, 2019.   \n[30] Matthew Fellows, Anuj Mahajan, Tim G. J. Rudner, and Shimon Whiteson. VIREL: A variational inference framework for reinforcement learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32, pages 7120\u20137134, 2019.   \n[31] Mattie Fellows, Kristian Hartikainen, and Shimon Whiteson. Bayesian bellman operators. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 13641\u201313656, 2021.   \n[32] Brendan O\u2019Donoghue. Variational bayesian reinforcement learning with regret bounds. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 28208\u201328221, 2021.   \n[33] Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta reinforcement learning - identifiability challenges and effective data collection strategies. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 4607\u20134618, 2021.   \n[34] Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline RL policies should be trained to be adaptive. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7513\u20137530. PMLR, 2022.   \n[35] Yuhao Wang and Enlu Zhou. Bayesian risk-averse $\\mathbf{q}$ -learning with streaming observations. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36, 2023.   \n[36] Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement learning: A survey. Found. Trends Mach. Learn., 8(5-6):359\u2013483, 2015.   \n[37] Carl Doersch. Tutorial on variational autoencoders. CoRR, abs/1606.05908, 2016.   \n[38] Diederik P. Kingma and Max Welling. An introduction to variational autoencoders. Found. Trends Mach. Learn., 12(4):307\u2013392, 2019.   \n[39] Marc G. Bellemare, Will Dabney, and R\u00e9mi Munos. A distributional perspective on reinforcement learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 449\u2013458. PMLR, 2017.   \n[40] Kotz Samuel and Johnson Norman L. Breakthroughs in statistics: methodology and distribution. Springer Science & Business Media, 2012.   \n[41] Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. Generalized variational inference. CoRR, abs/1904.02063, 2019.   \n[42] Will Dabney, Mark Rowland, Marc G. Bellemare, and R\u00e9mi Munos. Distributional reinforcement learning with quantile regression. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 2892\u20132901. AAAI Press, 2018.   \n[43] Will Dabney, Georg Ostrovski, David Silver, and R\u00e9mi Munos. Implicit quantile networks for distributional reinforcement learning. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1104\u20131113. PMLR, 2018.   \n[44] Koenker Roger and Hallock Kevin F. Quantile regression. Journal of economic perspectives, 15(4):143\u2013156, 2001.   \n[45] M\u00fcller Alfred. Integral probability metrics and their generating classes of functions. Advances in applied probability, 29(2):429\u2013443, 1997.   \n[46] Abhishek Roy, Krishnakumar Balasubramanian, and Murat A. Erdogdu. On empirical risk minimization with dependent and heavy-tailed data. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 8913\u20138926, 2021.   \n[47] Michele Caprio, Souradeep Dutta, Kuk Jin Jang, Vivian Lin, Radoslav Ivanov, Oleg Sokolsky, and Insup Lee. Credal bayesian deep learning. arXiv e-prints, pages arXiv\u20132302, 2023.   \n[48] Michele Caprio, Yusuf Sale, Eyke H\u00fcllermeier, and Insup Lee. A novel bayes\u2019 theorem for upper probabilities. In Fabio Cuzzolin and Maryam Sultana, editors, Epistemic Uncertainty in Artificial Intelligence - First International Workshop, volume 14523 of Lecture Notes in Computer Science, pages 1\u201312. Springer, 2024.   \n[49] Souradeep Dutta, Michele Caprio, Vivian Lin, Matthew Cleaveland, Kuk Jin Jang, Ivan Ruchkin, Oleg Sokolsky, and Insup Lee. Distributionally robust statistical verification with imprecise neural networks. CoRR, abs/2308.14815, 2023.   \n[50] Jim W. Hall. Uncertainty-based sensitivity indices for imprecise probability distributions. Reliab. Eng. Syst. Saf., 91(10-11):1443\u20131451, 2006.   \n[51] L Lorne Campbell. Exponential entropy as a measure of extent of a distribution. Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und verwandte Gebiete, 5(3):217\u2013225, 1966.   \n[52] Sheri Edwards. Thomas m. cover and joy a. thomas, elements of information theory (2nd ed.), john wiley & sons, inc. (2006). Inf. Process. Manag., 44(1):400\u2013401, 2008.   \n[53] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, 2018.   \n[54] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane S. Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33, 2020.   \n[55] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep data-driven reinforcement learning. CoRR, 2020.   \n[56] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offilne reinforcement learning with diversified $\\mathbf{q}_{}$ -ensemble. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, 2021.   \n[57] Seyed Kamyar Seyed Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimating uncertainties for offline RL through ensembles, and why their independence matters. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022.   \n[58] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pages 5026\u20135033. IEEE, 2012.   \n[59] Alexey Dosovitskiy, Germ\u00e1n Ros, Felipe Codevilla, Antonio M. L\u00f3pez, and Vladlen Koltun. CARLA: an open urban driving simulator. In 1st Annual Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pages 1\u201316. PMLR, 2017.   \n[60] Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial markov decision processes with bandit feedback and unknown transition. In Proceedings of the 37th International Conference on Machine Learning, 2020.   \n[61] Thodoris Lykouris, Vahab S. Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors, Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, 2018.   \n[62] Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, 2019.   \n[63] Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with adversarial corruptions. In Alina Beygelzimer and Daniel Hsu, editors, Conference on Learning Theory, 2019.   \n[64] Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, 2023.   \n[65] Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration in episodic reinforcement learning. In Mikhail Belkin and Samory Kpotufe, editors, Conference on Learning Theory, 2021.   \n[66] Matthieu Geist and Olivier Pietquin. Kalman temporal differences. J. Artif. Intell. Res., 39:483\u2013532, 2010.   \n[67] Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. In Christopher J. C. Burges, L\u00e9on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3003\u20133011, 2013.   \n[68] Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: A model-based and data-efficient approach to policy search. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning, 2011.   \n[69] Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian q-learning. In Jack Mostow and Chuck Rich, editors, Proceedings of the Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference, 1998.   \n[70] Gal Yarin, McAllister Rowan, and Rasmussen Carl Edward. Improving pilco with bayesian neural network dynamics models. In Data-efficient machine learning workshop, ICML, volume 4, page 25, 2016.   \n[71] Zhihai Wang, Jie Wang, Qi Zhou, Bin Li, and Houqiang Li. Sample-efficient reinforcement learning via conservative model-based actor-critic. In Thirty-Sixth AAAI Conference on Artificial Intelligence, pages 8612\u20138620. AAAI Press, 2022.   \n[72] Yaakov Engel, Shie Mannor, and Ron Meir. Reinforcement learning with gaussian processes. In Luc De Raedt and Stefan Wrobel, editors, Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005), 2005.   \n[73] Mohammad Ghavamzadeh and Yaakov Engel. Bayesian policy gradient algorithms. In Bernhard Sch\u00f6lkopf, John C. Platt, and Thomas Hofmann, editors, Advances in Neural Information Processing Systems 19, 2006.   \n[74] Mohammad Ghavamzadeh, Yaakov Engel, and Michal Valko. Bayesian policy gradient and actor-critic algorithms. J. Mach. Learn. Res., 2016.   \n[75] Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline RL policies should be trained to be adaptive. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, 2022.   \n[76] Hao Hu, Yiqin Yang, Jianing Ye, Ziqing Mai, Yujing Hu, Tangjie Lv, Changjie Fan, Qianchuan Zhao, and Chongjie Zhang. Bayesian offline-to-online reinforcement learning : A realist approach, 2024.   \n[77] Yuhao Wang and Enlu Zhou. Bayesian risk-averse q-learning with streaming observations. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36, 2023.   \n[78] Toru Hishinuma and Kei Senda. Importance-weighted variational inference model estimation for offilne bayesian model-based reinforcement learning. IEEE Access, 2023.   \n[79] Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Claude Sammut and Achim G. Hoffmann, editors, Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002), pages 267\u2013274. Morgan Kaufmann, 2002.   \n[80] Vallender SS. Calculation of the wasserstein distance between probability distributions on the line. Theory of Probability & Its Applications, 18:784\u2013786, 1974.   \n[81] Anqi Li, Dipendra Misra, Andrey Kolobov, and Ching-An Cheng. Survival instinct in offilne reinforcement learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, 2023.   \n[82] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems 34, pages 15084\u201315097, 2021.   \n[83] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[84] Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. Controlling overestimation bias with truncated mixture of continuous distributional quantile critics. In International Conference on Machine Learning, pages 5556\u20135566. PMLR, 2020.   \n[85] Zhihai Wang, Taoxing Pan, Qi Zhou, and Jie Wang. Efficient exploration in resource-restricted reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10279\u201310287, 2023.   \n[86] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32, 2019.   \n[87] Daniel J Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature, 618(7964):257\u2013263, 2023.   \n[88] Chris Gamble and Jim Gao. Safety-first ai for autonomous data centre cooling and industrial control. DeepMind, August, 17, 2018.   \n[89] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey. ACM Computing Surveys (CSUR), 55(1):1\u201336, 2021.   \n[90] Zhihai Wang, Jie Wang, Dongsheng Zuo, Yunjie Ji, Xinli Xia, Yuzhe Ma, Jianye Hao, Mingxuan Yuan, Yongdong Zhang, and Feng Wu. A hierarchical adaptive multi-task reinforcement learning framework for multiplier circuit design. In Forty-first International Conference on Machine Learning. PMLR, 2024.   \n[91] Zhihai Wang, Xijun Li, Jie Wang, Yufei Kuang, Mingxuan Yuan, Jia Zeng, Yongdong Zhang, and Feng Wu. Learning cut selection for mixed-integer linear programming via hierarchical sequence model. In The Eleventh International Conference on Learning Representations, 2023.   \n[92] Jie Wang, Zhihai Wang, Xijun Li, Yufei Kuang, Zhihao Shi, Fangzhou Zhu, Mingxuan Yuan, Jia Zeng, Yongdong Zhang, and Feng Wu. Learning to cut via hierarchical sequence/set model for efficient mixedinteger programming. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u201317, 2024.   \n[93] Zhihai Wang, Zijie Geng, Zhaojie Tu, Jie Wang, Yuxi Qian, Zhexuan Xu, Ziyan Liu, Siyuan Xu, Zhentao Tang, Shixiong Kai, et al. Benchmarking end-to-end performance of ai-based chip placement algorithms. arXiv preprint arXiv:2407.15026, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix / supplemental material ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide a proof for the upper bound of value distributions and derivations of loss functions. See Table 4 for all notations. ", "page_idx": 16}, {"type": "text", "text": "A.1 Proof for Value Bound ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To prove an upper bound of the value distribution, we introduce an assumption from [18] below. ", "page_idx": 16}, {"type": "text", "text": "Assumption A.1. $\\begin{array}{r}{\\left[I\\boldsymbol{\\delta}J\\,L e t\\,\\boldsymbol{\\zeta}=\\sum_{i=1}^{N}\\left(2\\zeta_{i}+\\log\\zeta_{i}^{\\prime}\\right)\\right.}\\end{array}$ denote the cumulative corruption level, where $\\zeta_{i}$ and $\\zeta_{i}^{\\prime}$ are defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathcal{T}V\\left(s_{i}\\right)-\\tilde{\\mathcal{T}}V\\left(s_{i}\\right)\\right\\|_{\\infty}\\leq\\zeta_{i},\\quad\\operatorname*{max}\\left\\{\\frac{\\pi_{B}\\left(a\\mid s_{i}\\right)}{\\pi_{b}\\left(a\\mid s_{i}\\right)},\\frac{\\pi_{b}\\left(a\\mid s_{i}\\right)}{\\pi_{B}\\left(a\\mid s_{i}\\right)}\\right\\}\\leq\\zeta_{i}^{\\prime},\\quad\\forall a\\in\\mathcal{A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here $\\|\\cdot\\|_{\\infty}$ means taking supremum over $V:S\\mapsto[0,r_{\\operatorname*{max}}/(1-\\gamma)]$ . ", "page_idx": 16}, {"type": "text", "text": "Note that $\\{\\zeta_{i}^{\\prime}\\}_{i}=1^{N}$ can quantify the corruption level of states and actions, and $\\{\\zeta_{i}\\}_{i}=1^{N}$ can quantify the corruption level of rewards and next states (transition dynamics). ", "page_idx": 16}, {"type": "text", "text": "Then, we provide the following assumption. ", "page_idx": 16}, {"type": "text", "text": "Assumption A.2. There exists an $M>0$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\frac{d^{\\pi_{E}}(s,a)}{p_{b}(s,a)},\\frac{d^{\\pi_{E}}(s,a)}{p_{B}(s,a)},\\frac{d^{\\pi_{E}}(s)}{d^{\\pi_{E}}(s)},\\frac{d^{\\pi_{I\\theta L}}(s)}{d^{\\pi_{E}}(s)},\\frac{d^{\\widetilde{\\pi}_{I\\theta L}}(s)}{d^{\\widetilde{\\pi}_{E}}(s)}\\}\\leq M,\\quad\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\pi_{\\mathrm{E}}(a~\\mid~s)\\;\\propto\\;\\pi_{b}(a~\\mid~s)\\,\\cdot\\,\\exp\\left(\\beta\\cdot\\left[{\\mathcal{T}}Q^{*}-V^{*}\\right](s,a)\\right)$ is the policy of clean data, \u03c0IQL $i n$ Equation (17) is the policy following IQL\u2019s supervised policy learning scheme under clean data, $\\tilde{\\pi}_{\\mathrm{IQL}}=\\arg\\operatorname*{min}_{s\\sim\\mathcal{B}}\\big[\\mathrm{KL}\\left(\\tilde{\\pi}_{\\mathrm{E}}(\\cdot\\mid s),\\pi(\\cdot\\mid s)\\right)\\big]i$ s the policy under corrupted data, and $\\tilde{\\pi}_{\\mathrm{E}}(a\\mid s)\\propto$ \u03c0 is the ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{\\mathrm{IQL}}=\\underset{\\pi}{\\arg\\operatorname*{max}}\\ \\mathbb{E}_{(s,a)\\sim b}\\left[\\exp\\left(\\beta\\cdot\\left[{\\mathcal{T}}Q^{*}-V^{*}\\right](s,a)\\right)\\log\\pi(a\\mid s)\\right]}\\\\ &{\\qquad=\\underset{\\pi}{\\arg\\operatorname*{min}}\\ \\mathbb{E}_{s\\sim b}\\left[\\mathcal{D}_{\\mathrm{KL}}\\left(\\pi_{\\mathrm{E}}(\\cdot\\mid s),\\pi(\\cdot\\mid s)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As TRACER directly applies the weighted imitation learning technique from IQL to learn the policy, we can use $\\tilde{\\pi}_{\\mathrm{IQL}}$ as the policy learned by TRACER under data corruptions, akin to RIQL. Assumption A.2 requires that each pair, including the policy $\\pi_{E}$ and the clean data $b$ , the policy $\\tilde{\\pi}_{E}$ and the corrupted dataset $\\boldsymbol{\\beta}$ , $\\pi_{E}$ and $\\tilde{\\pi}_{E}$ , $\\pi_{E}$ and $\\pi_{\\mathrm{IQL}}$ , and $\\tilde{\\pi}_{E}$ and $\\tilde{\\pi}_{\\mathrm{IQL}}$ , has good coverage. It is similar to the coverage condition in [18]. Based on Assumptions A.1 and A.2, we can derive the following theorem to show the robustness of our approach using the supervised policy learning scheme and learning the action-value distribution. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.3. (Performance Difference) For any $\\tilde{\\pi}$ and $\\pi$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nZ^{\\tilde{\\pi}}(s)-Z^{\\pi}(s)=\\frac{1}{1-\\gamma}\\mathbb{E}_{(s,a)\\sim d^{\\tilde{\\pi}},\\tilde{\\pi}}\\left[D^{\\pi}(s,a,r)-Z^{\\pi}(s)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Based on [79], for any $\\tilde{\\pi}$ and $\\pi$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Z^{\\tilde{\\pi}}(s)=\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}_{(S_{t},A_{t})\\sim P,\\tilde{\\pi}}\\left[R(S_{t},A_{t})|S_{0}=s\\right]}\\\\ {=\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}_{(S_{t},A_{t})\\sim P,\\tilde{\\pi}}\\left[R(S_{t},A_{t})+Z^{\\pi}(S_{t})-Z^{\\pi}(S_{t})|S_{0}=s\\right]}\\\\ {=\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}_{(S_{t},A_{t},S_{t+1})\\sim P,\\tilde{\\pi}}\\left[R(S_{t},A_{t})+\\gamma Z^{\\pi}(S_{t+1})-Z^{\\pi}(S_{t})|S_{0}=s\\right]+Z^{\\pi}(s)}\\\\ {=\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}_{(S_{t},A_{t},S_{t+1})\\sim P,\\tilde{\\pi}}\\left[D^{\\pi}(S_{t},A_{t},R_{t})-Z^{\\pi}(S_{t})|S_{0}=s\\right]}\\\\ {=\\displaystyle Z^{\\pi}(s)+\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}_{(s,A_{t})\\sim P,\\tilde{\\pi}}\\left[D^{\\pi}(S_{t},A_{t},R_{t})-Z^{\\pi}(S_{t})|S_{0}=s\\right]}\\\\ {=Z^{\\pi}(s)+\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{(s,a)\\sim d^{\\pi}}\\nu\\left[D^{\\pi}(s,a,r)-Z^{\\pi}(s)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem A.4. (Robustness). Under Assumptions A.1 and A.2, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nW_{1}(q^{\\tilde{\\pi}},q^{\\pi})\\leq\\frac{2M r_{\\mathrm{max}}}{(1-\\gamma)^{2}}(\\sqrt{1-e^{-\\frac{M\\zeta}{N}}}+\\sqrt{1-e^{-\\epsilon_{1}}}+\\sqrt{1-e^{-\\epsilon_{2}}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $q^{\\pi}$ is the value distribution, $W_{1}(\\cdot,\\cdot)$ is the Wasserstein distance [80] for measuring the distribution difference, $\\epsilon_{1}=\\mathbb{E}_{s\\sim b}\\left[{\\mathcal{D}}_{K L}\\left(\\pi_{E}(\\cdot|s\\right)\\big||\\,\\pi_{I Q L}(\\cdot|s)\\right)\\right]$ , and $\\epsilon_{2}=\\mathbb{E}_{s\\sim\\mathcal{B}}\\left[\\mathcal{D}_{K L}\\left(\\tilde{\\pi}_{E}(\\cdot|s)\\left|\\right|\\tilde{\\pi}_{I Q L}(\\cdot|s)\\right)\\right]$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. The definition of Wasserstein metric is $\\begin{array}{r}{W_{p}(P,Q)=\\left(\\underset{\\gamma\\in\\Gamma(P,Q)}{\\operatorname*{inf}}\\int_{x}\\int_{y}||x-y||_{p}^{p}d\\gamma(x,y)\\right)^{\\frac{1}{p}}.}\\end{array}$ The value distribution $q^{\\pi}$ , also denoted by $p(\\cdot|s)$ in Section 3.2 of the main text, is an expactation of action value distribution $\\mathbb{E}_{a\\sim\\pi,r\\sim\\rho}[p_{\\theta}(\\cdot|\\dot{s},\\dot{a},\\dot{r})]$ . ", "page_idx": 17}, {"type": "text", "text": "Note that as TRACER directly applies the weighted imitation learning technique from IQL to learn the policy (see Section 3.2), we can use $\\tilde{\\pi}_{\\mathrm{IQL}}$ as the policy learned by TRACER under data corruptions, akin to RIQL. ", "page_idx": 17}, {"type": "text", "text": "Let $q^{\\tilde{\\pi}_{I Q L}}$ and $q^{\\pi_{I Q L}}$ be the value distributions of learned policies following IQL\u2019s supervised policy learning scheme under corrupted and clean data, respectively. $Z^{\\tilde{\\pi}_{I Q L}}\\sim q^{\\tilde{\\pi}_{I Q L}}$ and $Z^{\\pi_{I Q L}}\\sim q^{\\pi_{I Q L}}$ . Let $p^{\\pi_{E}}$ be the value distribution of the policy of clean data, $p^{\\tilde{\\pi}_{E}}$ be the value distribution of the policy of corrupted data, $Z^{\\pi_{E}}\\sim p^{\\pi_{E}}$ , and $Z^{\\tilde{\\pi}\\tilde{\\varepsilon}}\\sim p^{\\tilde{\\pi}_{E}}$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}(q^{\\tilde{\\pi}_{I Q L}},q^{\\pi_{I Q L}})\\leq W_{1}(q^{\\tilde{\\pi}_{I Q L}},p^{\\tilde{\\pi}_{E}})+W_{1}(p^{\\tilde{\\pi}_{E}},q^{\\pi_{I Q L}})}\\\\ &{\\qquad\\qquad\\qquad\\leq W_{1}(q^{\\tilde{\\pi}_{I Q L}},p^{\\tilde{\\pi}_{E}})+W_{1}(p^{\\tilde{\\pi}_{E}},p^{\\pi_{E}})+W_{1}(p^{\\pi_{E}},q^{\\pi_{I Q L}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, based on the Bretagnolle\u2013Huber inequality, we have $d_{T V}(P,Q)$ \u2264 $\\sqrt{1-\\exp(-{\\mathscr D}_{K L}(P||Q))}$ . For $W_{1}(p^{\\tilde{\\pi}_{E}},p^{\\pi_{E}})$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{W_{\\Phi}(\\mu^{k},\\rho^{\\prime},\\varepsilon^{\\prime\\prime})}&{=\\operatorname{curl}\\frac{\\mathrm{i}}{\\sqrt{\\pi}}\\varepsilon_{k}\\int_{\\varepsilon_{0}}^{\\varepsilon}\\int_{\\varepsilon_{0}}^{\\varepsilon}\\int_{\\varepsilon_{0}}^{\\varepsilon_{k}}\\int_{0}^{\\varepsilon_{k}}\\int_{0}^{\\varepsilon_{k}}\\mathcal{L}^{\\prime\\prime}\\,\\mathrm{d}\\hat{\\rho}^{\\prime}\\varepsilon^{\\prime\\prime}\\,\\frac{J^{\\prime}}{2}}\\\\ &{\\quad-\\nu_{k}\\pi\\frac{\\mathrm{i}}{\\sqrt{\\pi}}\\varepsilon_{k}^{\\prime}\\int_{0}^{\\varepsilon}\\Big[\\frac{\\mathrm{i}J^{\\prime}}{\\sqrt{\\pi}\\varepsilon_{k}}\\Big(\\hat{\\rho}^{\\prime}+\\hat{\\varepsilon}_{k}^{\\prime\\prime}(\\hat{\\rho}^{\\prime})\\Big)\\Big]\\Big\\langle\\Phi_{\\Phi}^{\\varepsilon_{k}}\\Big(\\hat{\\rho}^{\\prime}\\hat{\\rho}^{\\prime}+\\hat{\\varepsilon}_{k}^{\\prime\\prime}(\\hat{\\rho}^{\\prime})\\Big)\\Big\\rangle}\\\\ &{\\le\\operatorname{curl}\\frac{\\mathrm{i}}{\\sqrt{\\pi}}\\varepsilon_{k}\\int_{0}^{\\varepsilon}\\Big[\\int_{0}^{\\varepsilon}\\int_{\\varepsilon_{k}}^{\\varepsilon}\\Big(\\hat{\\rho}^{\\prime}+\\hat{\\varepsilon}_{k}^{\\prime\\prime}(\\hat{\\rho}^{\\prime})\\Big)d\\hat{\\rho}^{\\prime}}\\\\ &{\\quad+2\\varepsilon_{k}\\int_{-\\infty}^{\\varepsilon}\\int_{0}^{\\varepsilon}\\Big(\\hat{\\rho}^{\\prime}+\\hat{\\varepsilon}_{k}^{\\prime\\prime}(\\hat{\\rho}^{\\prime})-2\\hat{\\varepsilon}_{k}^{\\prime\\prime}(\\hat{\\rho}^{\\prime})\\Big)}\\\\ &{=M_{k}\\frac{1}{\\sqrt{\\pi}}\\varepsilon_{k}-\\frac{J^{\\prime}\\pi_{k}\\pi_{k}\\varepsilon}{\\sqrt{\\pi}}\\varepsilon_{k}\\int_{0}^{\\varepsilon}\\Big[\\hat{\\rho}^{\\prime}\\varepsilon_{k}^{\\prime\\prime}\\left(\\hat{\\rho}^{\\prime}-\\hat{\\rho}^{\\prime}\\right)^{k}(\\varepsilon_{k},\\rho_{k}^{\\prime})\\Big]}\\\\ &{\\overset{(a)}{\\le}M_{k}\\frac{1}{\\sqrt{\\pi}}\\varepsilon_{k} \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\tilde{\\pi}_{\\mathrm{E}}(a_{i}|s_{i})}{\\pi_{\\mathrm{E}}(a_{i}|s_{i})}=\\frac{\\tilde{\\pi}_{\\mathcal{B}}(a_{i}|s_{i})\\cdot\\exp(\\beta\\cdot[\\tilde{T}V^{*}-V^{*}](s,a))}{\\pi_{\\mathcal{B}}(a_{i}|s_{i})\\cdot\\exp(\\beta\\cdot[\\tilde{T}V^{*}-V^{*}](s,a))}}\\\\ &{\\qquad\\qquad\\qquad\\times\\,\\frac{\\sum_{a\\in\\mathcal{A}}\\tilde{\\pi}_{\\mathcal{B}}(a_{i}|s_{i})\\cdot\\exp(\\beta\\cdot[\\tilde{T}V^{*}-V^{*}](s,a))}{\\sum_{a\\in\\mathcal{A}}\\pi_{\\mathcal{B}}(a_{i}|s_{i})\\cdot\\exp(\\beta\\cdot[\\tilde{T}V^{*}-V^{*}](s,a))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the definition of corruption levels, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\tilde{\\pi}_{\\mathrm{E}}(a_{i}|s_{i})}{\\pi_{\\mathrm{E}}(a_{i}|s_{i})}\\le\\zeta_{i}^{'}\\cdot\\exp(\\beta\\zeta_{i})\\cdot\\frac{\\zeta_{i}^{'}\\cdot\\exp(\\beta\\zeta_{i})\\sum_{a\\in A}\\tilde{\\pi}_{B}\\cdot\\exp(\\beta\\cdot[\\tilde{\\mathcal{T}}V^{*}-V^{*}](s,a))}{\\tilde{\\pi}_{B}\\cdot\\exp(\\beta\\cdot[\\tilde{\\mathcal{T}}V^{*}-V^{*}](s,a))}}\\\\ &{\\qquad\\qquad=\\zeta_{i}^{'2}\\cdot\\exp(2\\beta\\zeta_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we can derive ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{1}(p^{\\tilde{\\pi}_{E}},p^{\\pi_{E}})\\leq\\frac{2M r_{\\operatorname*{max}}}{(1-\\gamma)^{2}}\\sqrt{1-e^{-\\frac{M\\zeta}{N}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, for $W_{1}(p^{\\pi_{E}},q^{\\pi_{I Q L}})$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{1}(p^{\\pi_{E}},q^{\\pi_{I Q L}})\\leq\\frac{M r_{\\operatorname*{max}}}{(1-\\gamma)^{2}}\\mathbb{E}_{s\\sim d^{\\pi_{E}}}||\\pi_{E}(\\cdot|s)-\\pi_{I Q L}(\\cdot|s)||_{1}\\leq\\frac{2M r_{\\operatorname*{max}}}{(1-\\gamma)^{2}}\\sqrt{1-e^{-\\epsilon_{1}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\epsilon_{1}=\\mathbb{E}_{s\\sim b}\\left[{\\mathcal{D}}_{\\mathrm{KL}}\\left(\\pi_{\\mathrm{E}}(\\cdot|s)\\;\\middle|\\;\\pi_{\\mathrm{IQL}}(\\cdot|s)\\right)\\right]$ . ", "page_idx": 18}, {"type": "text", "text": "Finally, for $W_{1}(q^{\\tilde{\\pi}_{I Q L}},p^{\\tilde{\\pi}_{E}})$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{1}(q^{\\tilde{\\pi}_{I Q L}},p^{\\tilde{\\pi}_{E}})\\leq\\frac{M r_{\\operatorname*{max}}}{(1-\\gamma)^{2}}\\mathbb{E}_{s\\sim d^{\\tilde{\\pi}_{E}}}||\\tilde{\\pi}_{E}(\\cdot|s)-\\tilde{\\pi}_{I Q L}(\\cdot|s)||_{1}\\leq\\frac{2M r_{\\operatorname*{max}}}{(1-\\gamma)^{2}}\\sqrt{1-e^{-\\epsilon_{2}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{1}(q^{\\tilde{\\pi}},q^{\\pi})\\leq\\frac{2M r_{\\mathrm{max}}}{(1-\\gamma)^{2}}(\\sqrt{1-e^{-\\frac{M\\zeta}{N}}}+\\sqrt{1-e^{-\\epsilon_{1}}}+\\sqrt{1-e^{-\\epsilon_{2}}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that in Theorem A.4, the major difference between TRACER and IQL/RIQL is that TRACER uses the action-value and value distributions rather than the action-value and value functions in IQL/RIQL. Therefore, we provide this theorem to prove an upper bound on the difference in value distributions of TRACER to show its robustness, which also provides a guarantee of how TRACER\u2019s performance degrades with increased data corruptions. ", "page_idx": 19}, {"type": "text", "text": "A.2 Derivation of Loss Functions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For Equations (7), (8), and (9), we provide the detailed derivations. Firstly, for Equation (7), we maximize the posterior and have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\log p(D)=\\operatorname*{max}\\log\\mathbb{E}_{S_{t},R_{t}}\\left[p(D|S_{t},R_{t})\\right]}\\\\ &{\\phantom{=\\operatorname*{max}}\\mathbb{E}_{S_{t},R_{t}}\\left[\\log p(D|S_{t},R_{t})\\right]}\\\\ &{\\phantom{=\\operatorname*{max}}=\\operatorname*{max}\\mathbb{E}_{S_{t},R_{t}}\\left[\\displaystyle\\int_{\\mathcal{P}(A)}p_{\\psi_{a}}(A_{t}|D,S_{t},S_{t+1})\\cdot\\log p(D|S_{t},R_{t})d A_{t}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{ogp}(D|S_{t},R_{t})=\\int_{\\mathcal{P}(\\mathcal{A})}p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})\\cdot\\log\\frac{p(D,A_{t}|S_{t},R_{t},S_{t+1})\\cdot p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})}{p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})\\cdot p(A_{t}|D,S_{t},S_{t+1})}d A_{t}}\\\\ {=\\int_{\\mathcal{P}(\\mathcal{A})}p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})\\cdot\\log\\frac{p(D,A_{t}|S_{t},R_{t})}{p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})\\cdot p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})}d A_{t}}\\\\ {\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\int_{\\mathcal{P}(\\mathcal{A})}p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})\\cdot\\log\\frac{p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})}{p(A_{t}|D,S_{t},S_{t+1})}d A_{t}}\\\\ {=\\int_{\\mathcal{P}(\\mathcal{A})}p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})\\cdot\\log\\frac{p(D,A_{t}|S_{t},R_{t})}{p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})\\cdot p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})}d A_{t}}\\\\ {\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\mathcal{P}\\mathrm{K}_{\\operatorname{ac}}\\left(p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})|p(A_{t}|D,S_{t},S_{t+1})\\right)}\\\\ {=L_{b}+\\mathcal{D}\\mathrm{K}_{\\operatorname{ac}}\\left(p_{\\psi_{\\alpha}}(A_{t}|D,S_{t},S_{t+1})||p(A_{t}|D,S_{t},S \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $L_{b}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{b}=\\displaystyle\\int_{\\mathcal{P}(\\boldsymbol{A})}p_{\\psi_{a}}(A_{t}|D,S_{t},S_{t+1})\\cdot\\log\\displaystyle\\frac{p(D,A_{t}|S_{t},R_{t})}{p_{\\psi_{a}}(A_{t}|D,S_{t},S_{t+1})}d A_{t}}\\\\ &{\\quad=\\displaystyle\\int_{\\mathcal{P}(\\boldsymbol{A})}p_{\\psi_{a}}(A_{t}|D,S_{t},S_{t+1})\\log\\displaystyle\\frac{\\pi(A_{t}|S_{t})\\cdot p_{\\theta}(D|S_{t},A_{t},R_{t})}{p_{\\psi_{a}}(A_{t}|D,S_{t},S_{t+1})}d A_{t}}\\\\ &{\\quad=-\\mathcal{D}_{\\mathrm{KL}}\\left(p_{\\psi_{a}}(A_{t}|D,S_{t},S_{t+1})\\|\\pi(A_{t}|S_{t})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\int_{\\mathcal{P}(\\boldsymbol{A})}p_{\\psi_{a}}(A_{t}|D,S_{t},S_{t+1})\\log p_{\\theta}(D|S_{t},A_{t},R_{t})d A_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Secondly, for Equation (8), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\log p(D)=\\operatorname*{max}\\log\\mathbb{E}_{S_{t},A_{t}}\\left[P(D|S_{t},A_{t})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\geq\\operatorname*{max}\\mathbb{E}_{S_{t},A_{t}}[\\log P(D|S_{t},A_{t})]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ =\\operatorname*{max}\\mathbb{E}_{S_{t},A_{t}}\\left[\\int_{\\mathcal{P}(\\mathcal{R})}p_{\\psi_{r}}(R_{t}|D,S_{t},A_{t})\\cdot\\log p(D|S_{t},A_{t})d R_{t}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\operatorname*{max}\\mathbb{E}_{S_{t},A_{t}}\\Bigg[-\\mathcal{D}_{\\mathrm{KL}}\\left(p_{\\psi_{r}}(R_{t}|D,S_{t},A_{t})||p(R_{t}|S_{t},A_{t})\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\int_{\\mathcal{P}(\\mathcal{R})}p_{\\psi_{r}}(R_{t}|D,S_{t},A_{t})\\log p(D|S_{t},A_{t},R_{t})d R_{t}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, for Equation (9), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\log p(D)=\\operatorname*{max}\\log\\mathbb{E}_{A_{t},R_{t}}\\left[p(D|A_{t},R_{t})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\geq\\operatorname*{max}\\mathbb{E}_{A_{t},R_{t}}[\\log p(D|A_{t},R_{t})]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ =\\operatorname*{max}\\mathbb{E}_{A_{t},R_{t}}\\Bigg[\\int_{\\mathcal{P}(S)}p_{\\psi_{s}}(S_{t}|D,A_{t},R_{t})\\cdot\\log p(D|A_{t},R_{t})d S_{t}\\Bigg]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{~max}\\mathbb{E}_{A_{t},R_{t}}\\Bigg[-\\mathcal{D}_{\\mathrm{KL}}\\left(p_{\\psi_{s}}(S_{t}|D,A_{t},R_{t})|p(S_{t}|A_{t},R_{t})\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\int_{\\mathcal{P}(S)}p_{\\psi_{s}}(S_{t}|D,A_{t},R_{t})\\log p(D|S_{t},A_{t},R_{t})d S_{t}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we have Equations (7), (8), and (9). ", "page_idx": 20}, {"type": "text", "text": "A.3 Estimation of Entropy ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We estimate differential entropy following [52]. We have differential entropy as follows. Note that we omit the condition in the following equations. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}(p_{\\theta}(D))=\\mathbb{E}[-p_{\\theta}(D)]=-\\int_{\\mathbb{R}}p_{\\theta}(D)\\log p(D)d x.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, we consider a continuous function $p_{\\theta}$ discretized into bins of size $\\Delta$ . By the mean-value theorem there exists a action value $D_{i}$ in each bin such that ", "page_idx": 20}, {"type": "equation", "text": "$$\np_{\\theta}(D_{i})\\Delta=\\int_{i\\Delta}^{(i+1)\\Delta}p_{\\theta}(D)d D.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can estimate the integral of $p_{\\theta}$ (in the Riemannian sense) by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{-\\infty}^{\\infty}p_{\\theta}(D)d D=\\operatorname*{lim}_{\\Delta\\rightarrow0}\\sum_{i=-\\infty}^{\\infty}p_{\\theta}(D_{i})\\Delta.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}(p_{\\theta}(D))=\\displaystyle\\int_{-\\infty}^{\\infty}p_{\\theta}(D)\\log p(D)d x=\\operatorname*{lim}_{\\Delta\\to0}\\left(\\mathcal{H}_{\\Delta}(p_{\\theta}(D))+\\log\\Delta\\right)}\\\\ &{\\quad\\quad\\quad=-\\displaystyle\\operatorname*{lim}_{\\Delta\\to0}\\sum_{i=-\\infty}^{\\infty}p_{\\theta}(D_{i})\\Delta\\log p_{\\theta}(D_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Based on Equation (56), we can derive Equation (15), where $\\hat{\\varsigma}_{n}$ denotes the midpoint of $p_{\\theta}(D_{i})$ , and $\\overline{{D}}_{\\theta_{i}}^{\\varsigma_{n}}$ denotes $\\Delta$ . ", "page_idx": 20}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/5439864763fcb824640f87f97aa3b32daeed1a87362706a4e707e5d421af5d5a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "rTxCIWsfsD/tmp/c35aacbab639d97fbc1eae07c9544db8377f24ae8f5ef5f7bbc19767032ccabc.jpg", "img_caption": ["Figure 4: Architecture of TRACER. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "B TRACER Approach ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.1 Architecture of TRACER ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "On the actor-critic architecture based on IQL, our approach TRACER adds just one ensemble model $(p_{\\varphi_{a}},p_{\\varphi_{r}},p_{\\varphi_{s}})$ to reconstruct the data distribution and replace the function approximation in the critic with a distribution estimation (see Figure 4). Based on our experiments, this structure significantly improves the ability to handle both simultaneous and individual corruptions. ", "page_idx": 21}, {"type": "text", "text": "B.2 Implementation Details for TRACER ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "TRACER consists of four components: the actor network, the critic network, the value network, and the observation model. We first implement the actor network for decision-making with a 3-layer MLP, where the dimension of the hidden layers are 256 and the activation functions are ReLUs. Then, we utilize quantile regression [44] to design the critic networks and the value network, approximating the distributions of the action-value function $D$ and value function $Z$ , respectively. ", "page_idx": 22}, {"type": "text", "text": "Specifically, for the action-value distribution, we use a function $h\\,:\\,[0,1]\\,\\,\\rightarrow\\,\\mathbb{R}^{d}$ to compute an embedding for the sampling point $\\tau$ . Then, we can obtain the corresponding approximation by $D^{\\tau}(s,a)\\;\\stackrel{*}{\\approx}\\;f(g(s)\\odot\\bar{h}(\\tau))_{a}$ . Here, $g\\,:\\,S\\,\\rightarrow\\,\\mathbb{R}^{d}$ is the function computed by MLPs and $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{|\\boldsymbol{A}|}$ is the subsequent fully-connected layers mapping $g(s)$ to the estimated action-values $D(s,a)\\approx f(g(s))_{a}$ . For the approximation of the value distribution $Z^{\\tau}(s)$ , we leverage the same architecture. Note that the function $h$ is implemented by a fully-connected layer, and the output dimension is set to 256. We then use an ensemble model with $K$ networks. Each network is a 3-layer MLP, consisting of 256 units and ReLU activations. By using the ensemble model, we can construct the critic network, using the quantile regression for optimization. Moreover, the value network is constructed by a 3-layer MLP. The dimension of the hidden layers are also 256 and activation functions are ReLUs. ", "page_idx": 22}, {"type": "text", "text": "The observation model uses an ensemble model with 3 neural networks. Each network is used to reconstruct different observations in the offline dataset. It consists of two fully connected layers, and the activation functions are ReLUs. We apply a masked matrix during the learning process for updating the observation model. Thus, each network in the observation model can use different input data to compute Equations (7), (8), and (9), respectively. ", "page_idx": 22}, {"type": "text", "text": "Following the setting of [18], we list the hyper-parameters of $N,K$ , and $\\alpha$ for the approxmiation of action-value functions, and $\\kappa$ in the Huber loss in TRACER under random and adversarial corruption in Table 5 and Table 6, respectively. Here, $N$ is the number of samples $\\tau$ , and $K$ is the number of ensemble models. ", "page_idx": 22}, {"type": "text", "text": "Moreover, we apply Adam optimizer using a learning rate $1\\times10^{-3}$ , $\\gamma=0.99$ , target smoothing coefficient $\\tau\\,=\\,0.05$ , batch size 256 and the update frequency for the target network is 2. The corruption rate is $c=0.3$ and the corruption scale is $\\epsilon=1.0$ for all experiments. For the loss function of observation models, we use a linear decay parameter $\\eta$ from 0.0001 to 0.01 to trade off the loss $\\mathcal{L}_{\\mathrm{first}}$ and $\\mathcal{L}_{\\mathrm{second}}$ . We update each algorithm for 3000 epochs. Each epoch uses 1000 update times following [56]. Then, we evaluate each algorithm in clean environments for 100 epochs and report the average normalized return (calculated by $\\begin{array}{r}{100\\times{\\frac{\\mathrm{score-random\\;score}}{\\mathrm{expert\\;score-random\\;score}}}}\\end{array}$ ) over four random seeds. ", "page_idx": 22}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/ad678a62f17a53e39e27c66d48792cd820b9644de565629b661c209e1843992c.jpg", "table_caption": ["Table 5: Hyper-parameters used for TRACER under the random corruption benchmark. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 6: Hyper-parameters used for TRACER under the adversarial corruption benchmark. ", "page_idx": 23}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/36a50e9af4d238a2019c89fd87dcc4d9bef633ca9234cffae5de97f549bc93df.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "C Detailed Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 Details of Data Corruptions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We follow the corruption settings proposed by [18], applying either random or adversarial corruptions to each element of the offline data, namely state, action, reward, and dynamics (or \u201cnext-state\u201d), to simulate potential attacks that may occur during the offline data collection and usage process in real-world scenarios. We begin with the details of random corruption below. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Random observation corruption. We randomly sample $c\\%$ transitions $(s,a,r,s^{\\prime})$ from the offilne dataset, and for each of these selected states $s$ , we add noise to form $\\hat{s}=s\\!+\\!\\lambda\\!\\cdot\\!s t d(s)$ , where $\\lambda\\sim U n i f o r m[-\\epsilon,\\epsilon]^{d_{s}}$ , $c$ is the corruption rate, $\\epsilon$ is the corruption scale, $d_{s}$ refers to the dimension of states and $s t d(s)$ represents the $d_{s}$ -dimensional standard deviation of all states in the offline dataset.   \n\u2022 Random action corruption. We randomly sample $c\\%$ transitions $(s,a,r,s^{\\prime})$ from the offline dataset, and for each of these selected actions $a$ , we add noise to form $\\hat{a}=a+\\lambda$ \u00b7 $s t d(a)$ , where $\\lambda\\sim U n i f o r m[-\\epsilon,\\epsilon]^{d_{a}}$ , $d_{a}$ refers to the dimension of actions and $s t d(a)$ represents the $d_{a}$ -dimensional standard deviation of all actions in the offline dataset.   \n\u2022 Random reward corruption. We randomly sample $c\\%$ transitions $(s,a,r,s^{\\prime})$ from the offilne dataset, and for each of these selected rewards $r$ , we modify it to $\\hat{r}=U n i f o r m[-30$ \u00b7 $\\epsilon,30\\cdot\\epsilon]$ . We adopt this harder reward corruption setting since [81] has found that offilne RL algorithms are often insensitive to small perturbations of rewards.   \n\u2022 Random dynamics corruption. We randomly sample $c\\%$ transitions $(s,a,r,s^{\\prime})$ from the offilne dataset, and for each of these selected next-step states $s^{\\prime}$ , we add noise to form $\\hat{s^{\\prime}}=s^{\\prime}\\!+\\!\\lambda\\!\\cdot\\!s t d(s^{\\prime})$ , where $\\lambda\\sim U n i f o r m[-\\epsilon,\\epsilon]^{d_{s^{\\prime}}}$ , $d_{s^{\\prime}}$ refers to the dimension of next-step states and $s t d(s^{\\prime})$ represents the $d_{s^{\\prime}}$ -dimensional standard deviation of all next states in the offline dataset. ", "page_idx": 23}, {"type": "text", "text": "The harder adversarial corruption settings are detailed as follows: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 Adversarial observation corruption. To impose adversarial attack on the offilne dataset, we first need to pretrain an EDAC agent with a set of $Q_{p}$ functions and a policy function $\\pi_{p}$ using clean dataset. Then, we randomly sample $c\\%$ transitions $(s,a,r,s^{\\prime})$ from the offilne dataset, and for each of these selected states $s$ , we attack it to form $\\hat{s}=m i\\dot{n}_{\\hat{s}\\in\\mathbb{B}_{d}\\left(s,\\epsilon\\right)}Q_{p}(\\hat{s},a)$ . Here, $\\mathbb{B}_{d}(s,\\epsilon)\\,=\\,\\{\\hat{s}||\\hat{s}-s|\\,\\le\\,\\epsilon\\cdot s t d(s)\\}$ regularizes the maximum difference for each state dimension. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Adversarial action corruption. We randomly sample $c\\%$ transitions $(s,a,r,s^{\\prime})$ from the offline dataset, and for each of these selected actions $a$ , we use the pretrained EDAC agent to attack it to form $\\hat{a}=m i n_{\\hat{a}\\in\\mathbb{B}_{d}(a,\\epsilon)}Q_{p}(s,\\hat{a})$ . Here, $\\mathbb{B}_{d}(a,\\epsilon)=\\{\\bar{a}||\\hat{a}-a|\\leq\\epsilon\\cdot s t d\\bar{(a)}\\}$ regularizes the maximum difference for each action dimension.   \n\u2022 Adversarial reward corruption. We randomly sample $c\\%$ transitions $(s,a,r,s^{\\prime})$ from the offline dataset, and for each of these selected rewards $r$ , we directly attack it to form $\\hat{r}=-\\epsilon\\times r$ without any adversarial model. This is because that the objective of adversarial reward corruption is $\\begin{array}{r}{\\hat{r}\\,=\\,\\mathrm{min}_{\\hat{r}\\in\\mathbb{B}(r,\\epsilon)}\\,\\hat{r}+\\gamma\\mathbb{E}[Q(s^{\\prime},a^{\\prime})]}\\end{array}$ . Here $\\mathbb{B}(r,\\epsilon)\\,=\\,\\{\\hat{r}\\;\\mid\\;\\vert\\hat{r}\\,-\\,r\\vert\\;\\le$ $(1\\!+\\!\\epsilon)\\!\\cdot\\!r_{\\mathrm{max}}\\big\\}$ regularizes the maximum distance for rewards. Therefore, we have $\\hat{r}=-{\\boldsymbol{\\epsilon}}\\times{\\boldsymbol{r}}$ .   \n\u2022 Adversarial dynamics corruption. We randomly sample $c\\%$ transitions $(s,a,r,s^{\\prime})$ from the offilne dataset, and for each of these selected next-step states $s^{\\prime}$ , we use the pretrained EDAC agent to attack it to form $\\hat{s^{\\prime}}\\,=\\,m i n_{\\hat{s^{\\prime}}\\in\\mathbb{B}_{d}(s^{\\prime},\\epsilon)}Q_{p}\\bigl(\\hat{s^{\\prime}},\\pi_{p}\\bigl(s^{\\prime}\\bigr)\\bigr)$ . Here, $\\mathbb{B}_{d}(s^{\\prime},\\epsilon)\\;=$ $\\{\\hat{s^{\\prime}}||\\hat{s^{\\prime}}-s^{\\prime}|\\leq\\epsilon\\cdot s t d(s^{\\prime})\\}$ regularizes the maximum difference for each dimension of the dynamics. ", "page_idx": 24}, {"type": "text", "text": "The optimization of the above adversarial corruptions are implemented through Projected Gradient Descent [53, 54]. Taking adversarial observation corruption for example, we first initialize a learnable vector $\\bar{z}\\in\\bar{[-\\epsilon,\\epsilon]}^{d_{s}}$ , and then conduct a 100-step gradient descent with a step size of 0.01 for $\\hat{s}=s+z\\cdot s t d(s)$ , and clip each dimension of $z$ within the range $[-\\epsilon,\\epsilon]$ after each update. For action and dynamics corruption, we conduct similar operation. ", "page_idx": 24}, {"type": "text", "text": "Building upon the corruption settings for individual elements as previously discussed, we further intensify the corruption to simulate the challenging conditions that might be encountered in real-world scenarios. We present the details of the simultaneous corruption below: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Random simultaneous corruptions. We sequentially conduct random observation corruption, random action corruption, random reward corruption, and random dynamics corruption to the offline dataset in order. That is to say, we randomly select $c\\%$ of the transitions each time and corrupt one element among them, repeating four times until states, actions, rewards, and dynamics of the offline dataset are all corrupted. \u2022 Adversarial simultaneous corruptions. We sequentially conduct adversarial observation corruption, adversarial action corruption, adversarial reward corruption, and adversarial dynamics corruption to the offline dataset in order. That is to say, we randomly select $c\\%$ of the transitions each time and attack one element among them, repeating four times until states, actions, rewards, and dynamics of the offline dataset are all corrupted. ", "page_idx": 24}, {"type": "text", "text": "C.2 Details for CARLA ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We conduct experiments in CARLA from D4RL benchmark. We use the clean environment \u2019CARLALane- $.\\mathbf{v}0^{\\circ}$ to evaluate IQL, RIQL, and TRACER (Ours). We report each mean result with the standard deviation in the left of Figure 2 over four random seeds for 3000 epochs. We apply the random simultaneous data corruptions, where each element in the offline dataset (including states, actions, rewards, and next states) may involve random noise. We follow the setting in Section 4.2, using the corruption rate $c=0.3$ and scale $\\epsilon=1.0$ in the CARLA task. The results in the left of Figure 2 show the superiority of TRACER in the random simultaneous corruptions. We provide the hyperparameters of TRACER in Table 5. ", "page_idx": 24}, {"type": "text", "text": "C.3 Additional Experiments and Analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.3.1 Results Comparison between TRACER and RIQL under Individual Corruptions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Tables 2 and 3 of the main text, we adhered to commonly used settings for individual corruptions in corruption-robust offline RL. We directly followed hyperparameters from RIQL (i.e., $\\kappa$ for huber loss, the ensemble number $K$ , and $\\alpha$ in action-value functions, see Tables 5 and 6). Results show that TRACER outperforms RIQL in 18 out of 24 settings, demonstrating its robustness even when aligned with RIQL\u2019s hyperparameters. ", "page_idx": 24}, {"type": "text", "text": "Further, we explore hyperparameter tuning, specifically of $\\kappa$ , on Hopper task to improve TRACER\u2019s performance. This results in TRACER outperforming RIQL in 7 out of 8 settings on Hopper, up from 5 settings (see Tables 7 and 8). The further improvement highlights TRACER\u2019s potential to achieve greater performance gains. ", "page_idx": 24}, {"type": "image", "img_path": "rTxCIWsfsD/tmp/d82d842ddd6639f3c2c6b32b33b404a6791e28e82c4f251c12ef22d0b6b9ef84.jpg", "img_caption": ["Figure 5: We report the smoothed curves of mean of entropy values for each batch in \u2019Walker2dmedium-replay-v2\u2019 and \u2019Halfcheetah-medium-replay-v2\u2019 under adversarial and random simultaneous data corruptions. "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/35fa1aa5094e71cd9f1eb5f6c0d6516d9f245b2130f42c00d5dd06c1315b698c.jpg", "table_caption": ["Table 7: Average results and standard errors with 2 seeds and 64 batch sizes in Hopper-mediumreplay-v2 task for hyperparameter tuning. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Based on Table 7, we find that TRACER requires low $\\kappa$ in Huber loss, using L1 loss for large errors.   \nThus, TRACER can linearly penalize corrupted data and reduce its influence on the overall model fti. ", "page_idx": 25}, {"type": "text", "text": "C.3.2 Additional Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We further conduct experiments on two AntMaze datasets and two additional Mujoco datasets, presenting results under random simultaneous corruptions in Table 9. ", "page_idx": 25}, {"type": "text", "text": "Each result represents the mean and standard error over four random seeds and 100 episodes in clean environments. For each experiment, the methods train agents using batch sizes of 64 for 3000 epochs. Building upon RIQL, we apply the experiment settings as follows. (1) For the two Mujoco datasets, we use a corruption rate of $c=0.3$ and scale of $\\epsilon=1.0$ . Note that simultaneous corruptions with $c=0.3$ implies that approximately $76.0\\%$ of the data is corrupted. (2) For the two AntMaze datasets, we use the corruption rate of 0.2, corruption scales for observation (0.3), action (1.0), reward (30.0), and dynamics (0.3). ", "page_idx": 25}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/11477d78df39ad51ccdfbb333814859fb563c635fac1e43c8392e969b03bb55f.jpg", "table_caption": ["Table 8: Average results and standard errors with 2 seeds and 64 batch sizes in Hopper-mediumreplay-v2 task under individual corruptions. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/785ec0651f80d5b4b3774c23131b70e8a7bf3fdca899d294184f9293b6edfdd1.jpg", "table_caption": ["Table 9: The average scores and standard errors under random simultaneous corruptions. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "The results in Table 9 show that TRACER significantly outperforms other methods in all these tasks with the aforementioned AntMaze and Mujoco datasets. ", "page_idx": 26}, {"type": "text", "text": "C.4 Details for Evaluation and Ablation Studies ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "C.4.1 Evaluation for Robustness of TRACER across Different Scales of Corrupted Data ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorem A.4 shows that the higher the scale of corrupted data, the greater the difference in actionvalue distributions and the lower the TRACER\u2019s performance. Thus, we further conduct experiments to evaluate TRACER across various corruption levels. Specifically, we apply different corruption rate $c$ in all four elements of the offline dataset. We randomly select $c\\%$ of transitions from the dataset and corrupt one element in each selected transition. Then, we repeat this step four times until all elements are corrupted. Therefore, approximately $100\\cdot(1-(1-\\bar{c})^{4})\\%$ of data in the offilne dataset is corrupted. ", "page_idx": 26}, {"type": "text", "text": "In Table 10, we evaluate TRACER using different $c\\%$ , including $10\\%$ , $20\\%$ , $30\\%$ , $40\\%$ , and $50\\%$ . These rates correspond to approximately $\\bar{3}4.4\\%$ , $59.0\\%$ , $76.0\\%$ , $87.0\\%$ , and $93.8\\%$ of the data being corrupted. The results in Table 10 demonstrate that while TRACER is robust to simultaneous corruptions, its scores depend on the extent of corrupted data it encounters. ", "page_idx": 26}, {"type": "text", "text": "C.4.2 Details for Evaluation of the Entropy-based Uncertainty Measure ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In Figure 5, we additionally report the entropy values of TRACER with and without using entropy-based uncertainty measure, corresponding to Figure 3. The curves illustrate that TRACER using entropy-based uncertainty measure can effectively regulate the loss associated with corrupted data, reducing the influence of corrupted samples. Therefore, the estimated entropy values of corrupted data can be higher than those of clean data. ", "page_idx": 26}, {"type": "text", "text": "C.4.3 Ablation Study for Bayesian Inference ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We conduct experiments of TRACER and TRACER without Bayesian inference, i.e., RIQL combined with distributional RL methods, namely DRIQL. The experiments are on \u2019Walker2d-medium-replay-v2\u2019 under random simultaneous data corruptions. We employ the same hyperparameters of Huber loss for these methods, using $\\alpha=0.25$ and $\\kappa=1.0$ . ", "page_idx": 26}, {"type": "image", "img_path": "rTxCIWsfsD/tmp/77ddaed21d56e3ded3a98428e0d9f860c1a849a7d2fce7d97da57f04c74bcd08.jpg", "img_caption": ["Figure 6: Results and standard errors under random simultaneous corruptions. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "We report each mean result with the standard deviation in Figure 6. Each result is averaged over four seeds for 3000 epochs. These results show the effectiveness of our proposed Bayesian inference. ", "page_idx": 26}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/02d71a732e16a45ae36f5438c49fc7ece679fcac15812ae7000ad98093c95636.jpg", "table_caption": ["Table 10: Results in Hopper-medium-replay-v2 under various random simultaneous corruption levels. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D Compute Resource ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this subsection, we provide the computational cost of our approach TRACER. ", "page_idx": 27}, {"type": "text", "text": "For the MuJoCo tasks, including Halfcheetah, Walker2d, and Hopper, the average training duration is 40.6 hours. For the CARLA task, training extends to 51 hours. We conduct all experiments on NVIDIA GeForce RTX 3090 GPUs. ", "page_idx": 27}, {"type": "text", "text": "To compare the computational cost, we report the average epoch time on Hopper in Table 11, where results of baselines (including DT [82]) are from [18]. The formula for computational cost is: ", "page_idx": 27}, {"type": "text", "text": "Note that TRACER requires a long epoch time due to two main reasons: ", "page_idx": 27}, {"type": "text", "text": "1. Unlike RIQL and IQL, which learn one-dimensional action-value functions, TRACER generates multiple samples for the estimation of action-value distributions. Following [43], we generate 32 samples of action values for each state-action pair.   \n2. TRACER uses states, actions, and rewards as observations to update models, estimating the posterior of action-value distributions. ", "page_idx": 27}, {"type": "table", "img_path": "rTxCIWsfsD/tmp/b33c917f0299e9d5b20dcccfc01da37c0398e1134b52de39b3e0b077b12edaea.jpg", "table_caption": ["Table 11: Average epoch time. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "E More Related Work ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Online RL. In general, standard online RL fall into two categories: model-free RL [83\u201385] and model-based RL [86, 71]. In recent years, RL has achieved great success in many important realworld decision-making tasks [87\u201393]. However, the online RL methods still typically rely on active data collection to succeed, hindering their application in scenarios where real-time data collection is expensive, risky, and/or impractical. Thus, we focus on the offline RL paradigm in this paper. ", "page_idx": 27}, {"type": "text", "text": "F Code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We implement our codes in Python version 3.8 and make the code available online 2. ", "page_idx": 27}, {"type": "text", "text": "G Limitations and Negative Societal Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "TRACER\u2019s performance is related to the extent of corrupted data within the offilne dataset. Although TRACER consistently outperforms RIQL even under conditions of extensive corrupted data (see Table 10), its performance does degrade as the corruption rate (i.e., the extent/scale of corrupted data) increases. To tackle this problem, we look forward to the continued development and optimization of corruption-robust offline RL with large language models, introducing the prior knowledge of clean data against large-scale or near-total corrupted data. Thus, this corruption-robust offline RL can perform well even when faced with large-scale or near-total corrupted data in the offline dataset. ", "page_idx": 27}, {"type": "text", "text": "This paper proposes a novel approach called TRACER to advance the field of robustness in offilne RL for diverse data corruptions, enhancing the potential of agents in real-world applications. Although our primary focus is on technical innovation, we recognize the potential societal consequences of our work, as robustness in offilne RL for data corruptions has the potential to influence various domains such as autonomous driving and the large language model field. We are committed to ethical research practices and attach great importance to the social implications and ethical considerations in the development of robustness research in offline RL. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide our main claims in the abstract and introduction. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide the limitations of this work in Appendix G and the second paragraph of Section 6. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We present all assumptions and proofs in Appendix A. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide detailed information of the hyperparameters and implementation of our approach (see Appendix B.2) to facilitate the reproduction of our main results. We also provide the detailed experiment settings in Section 4.1 and Appendix C.1. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide the data and code in Appendix F, which can also be found in the supplemental material. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We provide detailed experiment settings in Section 4.1, Appendices C.1 and B.2. Moreover, we provide details in Appendix C.4 for each result in Section 4.4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We report standard deviations and standard errors for all results in the main text, including results in Tables 1, 2, 3, Figures 2, and 3. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the detailed information on the computer resources in Appendix D Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: NA. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: All our experiment data is open source. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: NA. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA]   \nJustification: NA.   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}]