[{"figure_path": "PacBluO5m7/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison among baseline models and KnowGPT on three benchmark datasets.", "description": "This table presents a comprehensive comparison of KnowGPT's performance against various baseline models across three benchmark question-answering datasets: CommonsenseQA, OpenBookQA, and MedQA.  The baselines represent different approaches, including fine-tuned Language Models (LMs), KG-enhanced LMs, zero-shot LLMs, and other KG prompting methods.  The table shows the accuracy achieved by each model on the development and test sets of each dataset, allowing for a thorough evaluation of KnowGPT's effectiveness compared to state-of-the-art techniques.  The bottom rows highlight the percentage improvement of KnowGPT over GPT-3.5 and GPT-4, further emphasizing its superior performance.", "section": "4.2 Main Results (RQ1)"}, {"figure_path": "PacBluO5m7/tables/tables_8_1.jpg", "caption": "Table 2: OpenBookQA Official Leaderboard records of three groups of related models.", "description": "This table presents a comparison of different models' performance on the OpenBookQA benchmark, categorized into three groups: models without external knowledge graphs (KGs), KG-enhanced language models, and ensemble methods.  The table shows the accuracy achieved by each model, highlighting KnowGPT's competitive performance relative to other state-of-the-art approaches and the human performance baseline.", "section": "4.2.1 Leaderboard Ranking"}, {"figure_path": "PacBluO5m7/tables/tables_8_2.jpg", "caption": "Table 1: Performance comparison among baseline models and KnowGPT on three benchmark datasets.", "description": "This table compares the performance of KnowGPT against various baseline models across three question answering datasets: CommonsenseQA, OpenBookQA, and MedQA.  The baselines are categorized into several groups representing different approaches to question answering, including Language Models (LMs) with fine-tuning, KG-enhanced LMs, LLMs with zero-shot prompting, and LLMs using KG prompting.  The table shows the accuracy achieved by each model on the development and test sets of each dataset, demonstrating the superior performance of KnowGPT.", "section": "4.2 Main Results (RQ1)"}, {"figure_path": "PacBluO5m7/tables/tables_8_3.jpg", "caption": "Table 1: Performance comparison among baseline models and KnowGPT on three benchmark datasets.", "description": "This table presents a comprehensive comparison of KnowGPT's performance against various baseline models across three question answering datasets: CommonsenseQA, OpenBookQA, and MedQA.  The baselines are categorized into several groups: LLMs with fine-tuning, KG-enhanced LLMs, LLMs used in a zero-shot setting, and LLMs with KG prompting.  The table shows the accuracy (Dev-Acc and Test-Acc) achieved by each model on each dataset, allowing for a direct comparison of KnowGPT's performance relative to existing state-of-the-art methods.  The results highlight KnowGPT's significant performance improvements across all datasets.", "section": "4.2 Main Results (RQ1)"}, {"figure_path": "PacBluO5m7/tables/tables_18_1.jpg", "caption": "Table 1: Performance comparison among baseline models and KnowGPT on three benchmark datasets.", "description": "This table compares the performance of KnowGPT against various baseline models across three question answering datasets: CommonsenseQA, OpenBookQA, and MedQA.  The baselines represent different approaches, including fine-tuned language models, KG-enhanced language models, zero-shot LLMs, and other KG prompting methods.  The table shows the accuracy achieved by each model on the development and test sets of each dataset, allowing for a comprehensive comparison of KnowGPT's effectiveness.", "section": "4.2 Main Results (RQ1)"}, {"figure_path": "PacBluO5m7/tables/tables_19_1.jpg", "caption": "Table 6: Fine-tuned LLMs on three benchmark datasets.", "description": "This table compares the performance of several fine-tuned large language models (LLMs) against the KnowGPT model on three benchmark question answering datasets: CommonsenseQA, OpenBookQA, and MedQA.  The LLMs used were ChatGLM, ChatGLM2, LLaMA-7B, Baichuan-7B, Alpaca-7B, Vicuna-7B, and InternLM-7B.  The table shows the accuracy achieved by each model on each dataset.  It highlights that KnowGPT, despite not being fine-tuned, significantly outperforms all the fine-tuned LLMs across all three datasets.", "section": "C Supplementary Results"}, {"figure_path": "PacBluO5m7/tables/tables_20_1.jpg", "caption": "Table 7: Accuracy of different prompt formats on specific types of questions on CommonsenseQA.", "description": "This table presents the accuracy of three different prompt formats (Triples, Sentences, and Graph Description) on three categories of questions from the CommonsenseQA dataset: Simple, Multi-hop, and Graph reasoning questions.  The accuracy varies across the different prompt formats and question types, indicating the suitability of certain formats for specific reasoning complexities.", "section": "4.3 Ablation Studies (RQ2)"}, {"figure_path": "PacBluO5m7/tables/tables_20_2.jpg", "caption": "Table 8: API cost and model efficiency analysis on MedQA.", "description": "This table compares the API cost and model efficiency of KnowGPT against several baseline models on the MedQA dataset.  It shows the training time, inference time, average number of tokens used, cost in dollars, and performance (accuracy) for each model.  The baselines represent different approaches to question answering, including those that use fine-tuning, zero-shot methods, and other KG-prompting techniques. The table highlights KnowGPT's efficiency in terms of API cost while achieving competitive performance.", "section": "C.4 Efficiency Analysis: API Cost and Model Efficiency"}, {"figure_path": "PacBluO5m7/tables/tables_21_1.jpg", "caption": "Table 1: Performance comparison among baseline models and KnowGPT on three benchmark datasets.", "description": "This table presents a comprehensive comparison of KnowGPT's performance against various baseline models across three question answering datasets: CommonsenseQA, OpenBookQA, and MedQA.  The baseline models are categorized into four groups: LLMs with fine-tuning, KG-enhanced LLMs, LLMs with zero-shot prompting, and LLMs with KG prompting.  The table displays the development and test accuracy for each model on each dataset, allowing for a direct comparison of KnowGPT's effectiveness in improving the accuracy of large language models through knowledge graph prompting.", "section": "Main Results (RQ1)"}, {"figure_path": "PacBluO5m7/tables/tables_21_2.jpg", "caption": "Table 1: Performance comparison among baseline models and KnowGPT on three benchmark datasets.", "description": "This table compares the performance of KnowGPT against various baseline models across three question answering datasets: CommonsenseQA, OpenBookQA, and MedQA.  The baselines represent different approaches, including fine-tuned Language Models (LM), KG-enhanced LMs, zero-shot LLMs, and other KG prompting methods.  The table shows the accuracy achieved by each model on the development and test sets of each dataset.  This allows for a comprehensive evaluation of KnowGPT's performance relative to existing state-of-the-art techniques.", "section": "Main Results (RQ1)"}]