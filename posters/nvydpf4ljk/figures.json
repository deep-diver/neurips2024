[{"figure_path": "nvYDPF4LJK/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \u201csuper link\u201d technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.", "description": "This figure illustrates three different approaches for information transmission between a multimodal large language model (MLLM) and downstream tools or decoders.  (a) shows a text-based method where the MLLM communicates through text messages. This is simple, but inefficient and lacks scalability for multiple tasks. (b) shows an embedding-based method, which improves efficiency via learnable embeddings but still lacks robust multi-tasking support. (c) Finally, the paper's proposed 'super link' method is presented, enabling efficient information transfer and flexible integration with numerous (over 100) diverse tasks through a unified MLLM and multiple task-specific decoders.", "section": "1 Introduction"}, {"figure_path": "nvYDPF4LJK/figures/figures_4_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure presents the architecture of VisionLLM v2, a multimodal large language model. It shows how the model takes image and text/visual prompts as inputs. A central large language model (LLM) processes these inputs and generates text outputs. Importantly, the LLM can also produce special routing tokens ([DET], [SEG], [GEN]) that trigger the selection of specific downstream decoders (for detection, segmentation, generation, etc.).  Super-link queries are appended after these routing tokens, providing task-specific information to the decoders, allowing for end-to-end training across diverse visual tasks.  The detailed connection between the LLM and task-specific decoders is further explained in Figure A13.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_5_1.jpg", "caption": "Figure 1: Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \u201csuper link\u201d technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.", "description": "This figure compares three different methods for information transmission between a multimodal large language model (MLLM) and downstream tools. The text-based method uses text messages which is inefficient and limits multitasking.  The embedding-based method uses learnable embeddings for efficient transfer but still struggles with multiple tasks. The proposed 'super link' method, shown in (c), uses a unified MLLM with super links to connect to multiple task-specific decoders, enabling efficient information transfer and supporting over 100 diverse tasks.", "section": "1 Introduction"}, {"figure_path": "nvYDPF4LJK/figures/figures_8_1.jpg", "caption": "Figure 3: Qualitative results of image generation and image editing. The prompts for text-to-image generation are \u201cPirate ship trapped in a cosmic maelstrom nebula\u201d and \u201cA car in the style of van Gogh.\u201d", "description": "This figure displays qualitative results of VisionLLM v2's image generation and editing capabilities.  It shows three example outputs: (a) Text-to-image generation, showcasing two different images generated from two different text prompts (\"Pirate ship trapped in a cosmic maelstrom nebula\" and \"A car in the style of van Gogh\"),  and a comparison with results from Stable Diffusion v1.5; (b) Zero-shot bilingual image generation, demonstrating the model's ability to generate images from text prompts in different languages (Chinese and English); and (c) instruction-based image editing, illustrating how instructions can modify existing images (e.g., adding a hat to a person's head, turning a dog into a panda). The figure highlights the model's ability to generate high-quality, diverse images while following both textual and visual prompts.", "section": "Experiments"}, {"figure_path": "nvYDPF4LJK/figures/figures_8_2.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure shows the architecture of VisionLLM v2, a multimodal large language model.  It illustrates how the model processes image and text/visual prompts, using a large language model (LLM) as the core component to generate text and trigger task-specific decoders via routing tokens and super-link queries. The super-link queries are additional learned embeddings appended to the routing tokens to efficiently transfer information between the LLM and the various decoders, enabling VisionLLM v2 to handle a wide range of visual tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_8_3.jpg", "caption": "Figure 4: Shared vs. unshared super-link queries for different decoders. We report the box/keypoint AP on COCO.", "description": "This figure compares the performance of using shared versus separate super-link queries for different downstream decoders in the VisionLLM v2 model.  The experiment is conducted on the COCO dataset, measuring Average Precision (AP) for both bounding boxes (box AP) and keypoints (keypoint AP).  The results show that using separate queries for each decoder leads to better performance, likely because it avoids conflicts between tasks and allows for more specialized information transfer.  Using shared queries results in decreased performance, indicating that the naive method of sharing embeddings is not sufficient for effective multi-task learning.", "section": "3.2 Super Link Technique"}, {"figure_path": "nvYDPF4LJK/figures/figures_25_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure presents the architecture of VisionLLM v2, a multimodal large language model.  It shows how the model processes image and text/visual prompts using a large language model (LLM) as the core component. The LLM interacts with various task-specific decoders via a mechanism called \"super link.\" This enables the model to handle numerous visual tasks and generate various outputs beyond text, such as image generation, object detection, etc.  The super link consists of routing tokens that signal which decoder to use and super-link queries that transmit relevant task information to the decoder. The figure highlights the model's capacity to unify visual perception, understanding, and generation in an end-to-end fashion.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_26_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure shows the architecture of VisionLLM v2, a multimodal large language model.  The model takes image and text/visual prompts as input. A large language model (LLM) processes these inputs and generates text outputs. Importantly, the LLM can also produce special routing tokens that trigger the selection of different task-specific decoders (e.g., for detection, segmentation, generation).  Super-link queries are automatically appended to the routing tokens, further processed by the LLM, and act as a bridge between the LLM and the appropriate decoder, enabling efficient information transfer for a variety of visual tasks.  The figure highlights the end-to-end design and multimodal capabilities of the model.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_26_2.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "The figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  It shows how the model takes image and text/visual prompts as input, processes them using a large language model (LLM), and outputs text or triggers task-specific decoders via special routing tokens and super-link queries. The super-link queries act as a bridge between the LLM and the decoders, enabling the model to handle a wide variety of visual tasks.  The figure highlights the end-to-end nature of the model and its ability to adapt to different tasks through flexible information transmission.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_26_3.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure shows the architecture of VisionLLM v2, a multimodal large language model.  It takes images and text/visual prompts as input. A large language model (LLM) processes the input and generates text responses.  Crucially, the LLM can output special routing tokens which trigger the selection of different task-specific decoders (e.g., for object detection, segmentation, image generation). Super-link queries are appended to the routing tokens and further processed by the LLM to transmit task-specific information to the decoders, enabling end-to-end training across multiple tasks. This architecture allows the model to handle a wide range of vision and vision-language tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_26_4.jpg", "caption": "Figure A2: Object detection and instance segmentation. The model excels in various environments, supporting the detection of a large number of instances. Its flexibility is highlighted by its ability to detect only user-selected categories and identify novel classes.", "description": "This figure shows several examples of object detection and instance segmentation results on various images.  The key takeaway is that the VisionLLM v2 model can handle various scenarios, including crowded scenes and images with many objects. Importantly, it demonstrates the flexibility to detect only specific user-requested object categories (e.g., only 'bottles' and 'forks') and also identifies novel classes not explicitly seen during training.", "section": "A.3 Zero-shot Evaluation and In-Context Evaluation"}, {"figure_path": "nvYDPF4LJK/figures/figures_26_5.jpg", "caption": "Figure 3: Qualitative results of image generation and image editing. The prompts for text-to-image generation are \"Pirate ship trapped in a cosmic maelstrom nebula\" and \"A car in the style of van Gogh.\"", "description": "This figure shows three examples of VisionLLM v2's image generation and editing capabilities.  The top row demonstrates text-to-image generation with two different prompts.  The first generates a pirate ship in a cosmic setting, and the second generates a car in the style of Van Gogh, illustrating the model's ability to understand and generate images based on complex descriptions and artistic styles. The bottom row shows examples of instruction-based image editing. A user prompt instructs the model to make various edits to existing images, such as changing an image's color palette and adding or removing objects. This showcases VisionLLM v2's capability to perform complex image manipulations in response to natural language commands.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_26_6.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  It shows how the model processes image and text/visual prompts using a large language model (LLM) as the core. The LLM generates text responses and can output special routing tokens to select appropriate downstream decoders (e.g., for object detection, segmentation, image generation). Super-link queries are automatically added, enabling flexible information transmission and gradient feedback between the LLM and decoders. This unified architecture allows VisionLLM v2 to handle hundreds of diverse visual tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_27_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "The figure shows the architecture of VisionLLM v2, a multimodal large language model.  It takes image and text/visual prompts as input and uses a central Large Language Model (LLM) to process instructions and generate responses.  The LLM can output special routing tokens to indicate which task-specific decoder should be used. Super-link queries are added to routing tokens, processed by the LLM, and used to efficiently transfer information between the LLM and decoders, supporting hundreds of visual tasks. The detailed architecture of the connection between the LLM and task-specific decoders is shown in Figure A13.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_27_2.jpg", "caption": "Figure 3: Qualitative results of image generation and image editing. The prompts for text-to-image generation are \u201cPirate ship trapped in a cosmic maelstrom nebula\u201d and \u201cA car in the style of van Gogh.\u201d", "description": "This figure displays examples of the VisionLLM v2 model's image generation and editing capabilities.  The top row shows examples of text-to-image generation, where the model created images based on textual descriptions such as \"Pirate ship trapped in a cosmic maelstrom nebula\" and \"A car in the style of van Gogh.\" The bottom row showcases instruction-based image editing, demonstrating the model's ability to modify existing images according to user instructions.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/figures/figures_27_3.jpg", "caption": "Figure A5: Pose estimation. Our model is capable of detecting keypoints in humans and animals with flexibility. The model allows users to select specific instance categories for detection, as well as choose individual keypoints.", "description": "The figure shows the results of pose estimation on various images.  The model successfully detects keypoints on different subjects, including humans and animals.  Importantly, it demonstrates that the model can be prompted to focus on specific categories of subjects (e.g., only detecting humans) and even select specific keypoints, rather than all possible keypoints, for detection. This highlights the model's flexibility and its ability to adapt to user-defined constraints.", "section": "A.5 Qualitative Results"}, {"figure_path": "nvYDPF4LJK/figures/figures_27_4.jpg", "caption": "Figure 3: Qualitative results of image generation and image editing. The prompts for text-to-image generation are \u201cPirate ship trapped in a cosmic maelstrom nebula\u201d and \u201cA car in the style of van Gogh.\u201d", "description": "This figure shows three examples of VisionLLM v2's capabilities in image generation and editing.  The top row demonstrates text-to-image generation, where natural language prompts are used to create images. The two examples are a pirate ship in a nebula and a car in the style of Van Gogh's paintings.  The bottom row displays examples of image editing, where instructions specify changes to be made to an existing image. The edits include turning a dog into a panda and altering the style of another image.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/figures/figures_27_5.jpg", "caption": "Figure A5: Pose estimation. Our model is capable of detecting keypoints in humans and animals with flexibility. The model allows users to select specific instance categories for detection, as well as choose individual keypoints.", "description": "This figure shows examples of pose estimation results obtained using VisionLLM v2.  The model demonstrates its ability to perform pose estimation on both human and animal images. Notably, the model can be instructed to either detect keypoints for specific categories (e.g., humans or animals only) or to only focus on individual keypoints within a specific category. This highlights VisionLLM v2's flexibility and capacity to adapt to various user-specified instructions.", "section": "A.5 Qualitative Results"}, {"figure_path": "nvYDPF4LJK/figures/figures_27_6.jpg", "caption": "Figure A5: Pose estimation. Our model is capable of detecting keypoints in humans and animals with flexibility. The model allows users to select specific instance categories for detection, as well as choose individual keypoints.", "description": "This figure shows several examples of pose estimation results obtained using VisionLLM v2.  It highlights the model's ability to detect keypoints not only in humans but also animals.  Importantly, it demonstrates the model's flexibility in two ways:  First, the user can specify the type of object (human or animal) for which they want to obtain pose information. Second, users can specify which keypoints they wish to see (all keypoints or specific keypoints).  Each example within the figure illustrates different situations to demonstrate the generality of the approach.", "section": "A.5 Qualitative Results"}, {"figure_path": "nvYDPF4LJK/figures/figures_28_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "The figure illustrates the architecture of VisionLLM v2, a multimodal large language model. It shows how the model takes image and text/visual prompts as input.  A central Large Language Model (LLM) processes these inputs and generates textual responses.  Importantly, the LLM can generate special tokens to trigger task-specific decoders (e.g., for detection, segmentation, generation).  Super-link queries, appended after the routing token embeddings, act as a bridge connecting the LLM to these specialized decoders.  This design allows the single model to handle a wide variety of visual tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_29_1.jpg", "caption": "Figure 3: Qualitative results of image generation and image editing. The prompts for text-to-image generation are \"Pirate ship trapped in a cosmic maelstrom nebula\" and \"A car in the style of van Gogh.\"", "description": "This figure shows four examples of image generation and editing produced by the VisionLLM v2 model.  The top row displays examples of text-to-image generation, where different prompts are given to the model to generate unique images.  The bottom row shows examples of instruction-based image editing, where instructions are used to modify existing images. The images demonstrate the model's ability to generate high-quality, creative images and to perform a variety of image editing tasks.", "section": "4.4 Visual Generation Tasks"}, {"figure_path": "nvYDPF4LJK/figures/figures_29_2.jpg", "caption": "Figure 3: Qualitative results of image generation and image editing. The prompts for text-to-image generation are \"Pirate ship trapped in a cosmic maelstrom nebula\" and \"A car in the style of van Gogh.\"", "description": "This figure displays qualitative results of the VisionLLM v2 model on image generation and editing tasks.  The left column shows the results of text-to-image generation, where the model created images from textual descriptions (\"Pirate ship trapped in a cosmic maelstrom nebula\" and \"A car in the style of van Gogh\"). The right column demonstrates the model's image editing capabilities, modifying existing images based on given instructions. The results showcase the model's ability to generate and manipulate images based on varied and complex instructions.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/figures/figures_29_3.jpg", "caption": "Figure 3: Qualitative results of image generation and image editing. The prompts for text-to-image generation are \"Pirate ship trapped in a cosmic maelstrom nebula\" and \"A car in the style of van Gogh.\"", "description": "This figure shows qualitative results of VisionLLM v2 on image generation and image editing tasks.  The top row demonstrates text-to-image generation, showcasing the model's ability to create images from textual descriptions that include stylistic elements. The bottom row showcases instruction-based image editing, highlighting the model's ability to modify existing images according to specific instructions such as changing an object or applying a style. The examples illustrate VisionLLM v2's capability to perform both image generation and editing tasks with high fidelity and stylistic consistency.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_29_4.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  It shows how the model takes image and text/visual prompts as input. A central Large Language Model (LLM) processes these inputs and generates text outputs.  Crucially, the LLM can also output special routing tokens that trigger the use of different task-specific decoders (e.g., for object detection, image generation).  Super-link queries are appended to the routing tokens and are processed by the LLM to provide task-specific information to the appropriate decoder. This allows VisionLLM v2 to handle hundreds of visual tasks with a single unified architecture. ", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_30_1.jpg", "caption": "Figure 3: Qualitative results of image generation and image editing. The prompts for text-to-image generation are \"Pirate ship trapped in a cosmic maelstrom nebula\" and \"A car in the style of van Gogh.\"", "description": "This figure shows qualitative results of VisionLLM v2 on image generation and image editing tasks.  The left column shows the original image or text prompt, while the right column displays the results generated by VisionLLM v2.  The top row demonstrates text-to-image generation, showcasing the model's ability to create images from textual descriptions, such as a pirate ship in a nebula or a car in the style of Van Gogh. The bottom row illustrates the instruction-based image editing capabilities of the model. It highlights VisionLLM v2's ability to modify an existing image according to various instructions.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/figures/figures_30_2.jpg", "caption": "Figure 3: Qualitative results of image generation and image editing. The prompts for text-to-image generation are \"Pirate ship trapped in a cosmic maelstrom nebula\" and \"A car in the style of van Gogh.\"", "description": "This figure shows qualitative results of VisionLLM v2 on image generation and image editing tasks.  The top row displays examples of text-to-image generation, showcasing the model's ability to generate images from textual descriptions. The bottom row shows instruction-based image editing, demonstrating the model's capacity to manipulate existing images according to user instructions.  The figure highlights the model's versatility in handling various visual tasks and different artistic styles.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/figures/figures_30_3.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "The figure shows the architecture of VisionLLM v2, a multimodal large language model.  The model takes image and text/visual prompts as input. A large language model (LLM) processes these inputs and generates text-based responses.  Crucially, the LLM can also output special routing tokens, triggering the use of specific task-oriented decoders (e.g., for detection, segmentation, generation). Super-link queries are appended to these tokens, providing task-specific information to the appropriate decoder. This design allows VisionLLM v2 to handle a wide variety of visual tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_31_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  It shows how image and text/visual prompts are processed by an image encoder and text tokenizer respectively.  These are fed to a Large Language Model (LLM), which generates responses and can also output special routing tokens. These tokens trigger the selection of appropriate task-specific decoders via \"super links\", allowing for flexible task handling and efficient information transmission. The super links consist of Routing Tokens and Super-Link Queries.  The detailed decoder connections are shown in Figure A13. The system as a whole allows for the handling of hundreds of visual tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_31_2.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  It shows how the model processes image and text/visual prompts using a large language model (LLM) as the central component. The LLM generates text responses and can also output special routing tokens to select appropriate task-specific decoders.  Super-link queries are added to the routing tokens to enable efficient information transfer between the LLM and the decoders, ultimately enabling the model to handle many visual tasks.  Figure A13 provides further detail on the connections between the LLM and the various decoders.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_32_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "The figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  It shows how the model processes image and text/visual prompts using a central Large Language Model (LLM). The LLM generates text responses and can also produce special routing tokens that trigger the selection of task-specific decoders.  Super-link queries are appended to the routing tokens and processed by the LLM to transmit information to the appropriate decoder. This architecture allows the model to handle a wide range of visual tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_32_2.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "The figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  It shows how the model processes image and text/visual prompts. The core component is a large language model (LLM) that processes the inputs and generates text outputs.  Crucially, the LLM can generate special tokens (e.g., [DET]) that trigger task-specific decoders. These decoders handle various vision tasks like object detection or image generation.  Super-link queries are appended to routing tokens, acting as a bridge between the LLM and the decoders, enabling efficient information transfer.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_33_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure presents the architecture of VisionLLM v2, a multimodal large language model.  It shows how the model takes image and text/visual prompts as input. The core component, a large language model (LLM), processes this input to generate textual responses. Importantly, the LLM can generate special routing tokens (like [DET]), which trigger the selection of a specific task decoder (for tasks like detection or segmentation).  The super-link queries, added after routing tokens, enhance information transfer between the LLM and these specialized decoders. This design allows the single model to handle a large number of visual tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_35_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "The figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  It shows how image and text/visual prompts are processed.  A central Large Language Model (LLM) interprets user instructions and generates responses.  Crucially, the LLM can output special routing tokens ([DET], etc.), triggering the selection of task-specific decoders via 'super links'. These super links use learnable queries that are appended after the routing tokens and processed by the LLM to effectively transmit information between the LLM and the task decoders, allowing support for diverse visual tasks.  The detailed decoder connections are described in a supplementary figure.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_37_1.jpg", "caption": "Figure 1: Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \u201csuper link\u201d technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.", "description": "This figure compares three different approaches for information transmission between a multimodal large language model (MLLM) and downstream tools or decoders. The first approach uses text messages, which is simple but inefficient. The second approach uses learnable embeddings, which is more efficient but doesn't scale well to multiple tasks. The third approach, proposed by the authors, uses a 'super link' mechanism, which combines the efficiency of embeddings with the ability to handle multiple tasks through unified interfaces.", "section": "1 Introduction"}, {"figure_path": "nvYDPF4LJK/figures/figures_37_2.jpg", "caption": "Figure 1: Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \u201csuper link\u201d technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.", "description": "This figure compares three different approaches for transmitting information between a multimodal large language model (MLLM) and downstream tools or decoders.  (a) shows a text-based method, where the MLLM communicates with tools via text messages. This approach is simple but suffers from inefficient information transfer. (b) illustrates an embedding-based method, using learnable embeddings to connect the MLLM to task-specific decoders. While efficient, this method doesn't handle multiple tasks well. (c) presents the proposed \"super link\" method, a unified MLLM using super links to connect with multiple task decoders, enabling efficient information transfer and multitasking.", "section": "1 Introduction"}, {"figure_path": "nvYDPF4LJK/figures/figures_38_1.jpg", "caption": "Figure 1: Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \u201csuper link\u201d technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.", "description": "This figure illustrates three different approaches for transmitting information between a multimodal large language model (MLLM) and downstream tools or decoders.  (a) shows a traditional text-based method, where the MLLM communicates via text messages, which is slow and inefficient for complex tasks. (b) demonstrates an embedding-based approach that improves efficiency but still struggles with multiple tasks.  (c) presents the authors' proposed 'super link' method, which uses a unified MLLM and multiple task-specific decoders connected by super links, offering both efficiency and support for a large number of diverse tasks. This highlights the key advantage of the proposed approach.", "section": "1 Introduction"}, {"figure_path": "nvYDPF4LJK/figures/figures_38_2.jpg", "caption": "Figure 1: Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \u201csuper link\u201d technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.", "description": "This figure illustrates three different approaches for information transmission between a multimodal large language model (MLLM) and downstream tools or decoders.  (a) shows a text-based approach, where the MLLM communicates with tools via text messages. This is simple but inefficient. (b) shows an embedding-based approach, using learnable embeddings for communication, which is efficient but not suitable for multiple tasks. (c) presents the authors' proposed 'super link' method, a unified MLLM connected to multiple decoders via super links for efficient and flexible information transfer across numerous tasks.", "section": "1 Introduction"}, {"figure_path": "nvYDPF4LJK/figures/figures_39_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  The model takes image and text/visual prompts as input. A central Large Language Model (LLM) processes these inputs and generates text outputs.  Importantly, the LLM can output special routing tokens which trigger the use of specific task decoders (e.g., for object detection, pose estimation, image generation).  Super-link queries are appended to the routing tokens, acting as a bridge between the LLM and these decoders, enabling efficient information transfer and facilitating the handling of numerous visual tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_40_1.jpg", "caption": "Figure 1: Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \u201csuper link\u201d technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.", "description": "This figure compares three different approaches for information transmission between a Multimodal Large Language Model (MLLM) and downstream tools or decoders.  The text-based method (a) uses text messages, which is inefficient for complex information and lacks multi-tasking support. The embedding-based method (b) employs learnable embeddings, which is efficient for transferring information but still lacks multi-tasking support.  The proposed 'super link' method (c) uses a unified MLLM with multiple task-specific decoders connected via super links, efficiently handling over 100 tasks.", "section": "1 Introduction"}, {"figure_path": "nvYDPF4LJK/figures/figures_41_1.jpg", "caption": "Figure 1: Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \u201csuper link\u201d technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.", "description": "This figure illustrates three different approaches for information transmission between a Multimodal Large Language Model (MLLM) and downstream tools or decoders. The first approach (a) uses text messages, which are inefficient and hinder efficient information transfer. The second approach (b) leverages learnable embeddings, resulting in more efficient information transfer, but it still does not fully support multiple tasks effectively.  The authors' proposed method (c) uses a \"super link\" approach to achieve both efficiency in information transfer and the capability to effectively manage over 100 diverse tasks by connecting the unified MLLM with multiple task-specific decoders.", "section": "1 Introduction"}, {"figure_path": "nvYDPF4LJK/figures/figures_42_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure shows the architecture of VisionLLM v2, a multimodal large language model.  It details how the model processes image and text/visual prompts.  The core is a large language model (LLM) that generates text responses.  Importantly, the LLM can output special routing tokens that trigger the selection of specific downstream decoders (e.g., for object detection or image generation).  These decoders are connected to the LLM via \"super links\", which involve task-specific queries automatically appended to the routing token embedding. This design enables efficient information transmission and gradient backpropagation, allowing the model to handle a wide array of vision and vision-language tasks.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_42_2.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  It shows how the model takes image and text/visual prompts as input, processes them using a Large Language Model (LLM), and outputs text responses or triggers task-specific decoders via routing tokens and super-link queries.  The super-link mechanism allows flexible information transfer and gradient feedback between the LLM and the decoders for efficient multi-tasking. The figure highlights the model's ability to handle hundreds of visual tasks through a unified framework.", "section": "3 VisionLLM v2"}, {"figure_path": "nvYDPF4LJK/figures/figures_42_3.jpg", "caption": "Figure 1: Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \u201csuper link\u201d technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.", "description": "This figure compares three different methods of information transmission between a multimodal large language model (MLLM) and downstream tools or decoders.  Method (a) uses text messages, which are inefficient for complex information. Method (b) uses embeddings for efficient transfer but doesn't handle multiple tasks well.  Method (c), the authors' proposed approach, uses a 'super link' to connect the MLLM to multiple task-specific decoders efficiently and supports over 100 different tasks.", "section": "1 Introduction"}, {"figure_path": "nvYDPF4LJK/figures/figures_43_1.jpg", "caption": "Figure 2: Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.", "description": "This figure illustrates the architecture of VisionLLM v2, a multimodal large language model.  The model takes image and text/visual prompts as input. A large language model (LLM) processes the input and generates text responses. Importantly, the LLM can output special routing tokens (e.g., [DET]) that trigger the use of specific task decoders (e.g., for object detection, segmentation, generation) connected via \"super links\". These super links use learnable queries, appended after routing tokens, to efficiently transfer information and gradients between the LLM and the decoders, enabling efficient multi-tasking. The figure highlights the end-to-end nature of VisionLLM v2, capable of handling hundreds of vision-language tasks.", "section": "3 VisionLLM v2"}]