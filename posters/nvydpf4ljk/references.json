{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, a foundational vision-language model that VisionLLM v2 builds upon for image encoding."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-10-26", "reason": "This paper introduces a crucial training method (Visual Instruction Tuning) that VisionLLM v2 leverages for its multimodal training."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA-1.5-7B", "publication_date": "2023-10-26", "reason": "This paper introduces LLaVA, a strong baseline multimodal large language model that VisionLLM v2 is compared against and improves upon."}, {"fullname_first_author": "Shilong Liu", "paper_title": "Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection", "publication_date": "2023-03-15", "reason": "This paper introduces Grounding DINO, which VisionLLM v2 uses as one of its task-specific decoders, demonstrating the model's task-specific architecture."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-19", "reason": "This paper introduces Stable Diffusion, a critical component of VisionLLM v2's image generation capabilities."}]}