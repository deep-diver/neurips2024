[{"figure_path": "nvYDPF4LJK/tables/tables_5_1.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art (SoTA) models on several multimodal dialogue benchmarks.  It breaks down the results across two types of datasets: academic-oriented datasets (focused on question answering and visual reasoning) and instruction-following datasets (assessing the model's ability to follow instructions).  The asterisk (*) indicates that the training annotations for that dataset were visible to the model during its training, potentially influencing its performance. The table provides numerical scores for each model on various tasks within each dataset category.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of VisionLLM v2 and other state-of-the-art (SOTA) models on a range of multimodal dialogue benchmarks.  It shows performance across both academic-oriented datasets (requiring visual question answering and reasoning) and instruction-following datasets (measuring the model's ability to complete various tasks given instructions). The asterisk indicates when models had access to training annotations for a given dataset.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of region recognition and visual commonsense reasoning performance. (a) SS and S-IoU represent semantic similarity and semantic IoU, which originated from [207]. (b) Q, A, and R denote question, answer, and rationale, respectively. X\u2192Y means that the model needs to select option Y conditioned on X. *The model is finetuned on the dataset.", "description": "This table presents a comparison of the VisionLLM v2 model's performance against other state-of-the-art models on two tasks: region recognition and visual commonsense reasoning.  Region recognition evaluates the model's ability to correctly identify objects within a given region of an image. Visual commonsense reasoning assesses the model's capacity for complex reasoning and understanding of visual scenes, requiring it to select both the correct answer and rationale for a given question about an image.  The table provides multiple metrics for both tasks, indicating the model's performance across different datasets and conditions.", "section": "4.2 Mutimodal Benchmarks"}, {"figure_path": "nvYDPF4LJK/tables/tables_7_2.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art (SOTA) models on several multimodal dialogue benchmarks.  These benchmarks are categorized into academic-oriented datasets (focused on visual question answering) and instruction-following datasets (testing the model's ability to follow instructions).  The table shows various metrics for each dataset, allowing for a comprehensive comparison of the models' abilities across different types of multimodal tasks. The asterisk (*) indicates that the training annotations for the dataset were accessible during training.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_7_3.jpg", "caption": "Table 3: Comparison of object detection and instance segmentation performance. Instance seg. means instance segmentation. *The model is finetuned on the dataset.", "description": "This table compares the performance of VisionLLM v2 and other state-of-the-art models on object detection and instance segmentation tasks.  The metrics used are Average Precision (AP), AP at 50% IoU (AP50), AP at 75% IoU (AP75), and mean Average Precision (mAP) for object detection. For instance segmentation, the metrics are AP, AP50, and AP75.  The table shows that VisionLLM v2, while a generalist model, achieves comparable results to specialist models, particularly on the COCO dataset.  Note that some models listed were finetuned on the dataset, while VisionLLM v2 was not.", "section": "4.3 Visual Perception Tasks"}, {"figure_path": "nvYDPF4LJK/tables/tables_7_4.jpg", "caption": "Table 4: Comparison of pose estimation performance. * indicates that the results rely on ground-truth bounding boxes for top-down methods.", "description": "This table compares the performance of different pose estimation methods across various datasets.  The metrics used are Average Precision (AP) at different Intersection over Union (IoU) thresholds and Percentage of Correct Keypoints (PCK) at 0.2.  The methods are categorized as either specialist (designed for specific tasks like human pose estimation) or generalist (able to handle multiple tasks).  The backbone architecture used for each method is also specified. The asterisk (*) indicates that a method uses ground truth bounding boxes, a common approach in top-down pose estimation methods.", "section": "4.3 Visual Perception Tasks"}, {"figure_path": "nvYDPF4LJK/tables/tables_9_1.jpg", "caption": "Table 6: Ablation on the multi-task influence. The numbers denote the loss change when the model is fine-tuned on a single task.", "description": "This table presents an ablation study on the multi-task influence in the VisionLLM v2 model. It shows how fine-tuning the model on a single task (Image VQA, Instance Segmentation, or Image Generation) affects the loss on all three tasks. Positive values indicate that training on one task improves performance on another, while negative values suggest a detrimental effect, indicating potential task conflicts.", "section": "3.3 Training Strategy"}, {"figure_path": "nvYDPF4LJK/tables/tables_9_2.jpg", "caption": "Table 6: Ablation on the multi-task influence. The numbers denote the loss change when the model is fine-tuned on a single task.", "description": "This table shows the ablation study on the multi-task influence of VisionLLM v2. The model is fine-tuned on a single task (image VQA, instance segmentation, or image generation) for 1000 iterations. The table shows the loss change for all three tasks. A decrease in the loss value indicates beneficial training for the task, while an increase is detrimental.  The results indicate the mutual influence of multi-task joint training, highlighting the advantages and disadvantages of training on specific tasks and the impact on overall performance across tasks.", "section": "3.3 Training Strategy"}, {"figure_path": "nvYDPF4LJK/tables/tables_22_1.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art (SOTA) models on various multimodal dialogue benchmarks.  It breaks down the results into two categories of datasets: academic-oriented and instruction-following.  Academic-oriented datasets focus on visual question answering and reasoning, while instruction-following datasets evaluate the model's ability to follow instructions across different tasks.  The table highlights the VisionLLM v2's performance relative to other models and notes when training annotations were observed during training.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_23_1.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art (SOTA) models on several multimodal dialogue benchmarks.  These benchmarks are categorized into academic-oriented datasets (focused on visual question answering) and instruction-following datasets (testing the model's ability to follow instructions).  The table shows the performance of each model on various metrics for each dataset, highlighting VisionLLM v2's performance relative to others. The asterisk (*) indicates that the training annotations were observed during training, possibly influencing the results. ", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_23_2.jpg", "caption": "Table 3: Comparison of object detection and instance segmentation performance. Instance seg. means instance segmentation. *The model is finetuned on the dataset.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art models on object detection and instance segmentation tasks using the COCO and CrowdHuman datasets.  It shows the average precision (AP), AP at 50% IoU (AP50), AP at 75% IoU (AP75), and mMR (mean average precision) for different models and backbones (ResNet50, ViT-B, Swin-T, ViT-L, etc.).  The table highlights VisionLLM v2's performance against specialist models and demonstrates its capabilities in these visual perception tasks.", "section": "4.3 Visual Perception Tasks"}, {"figure_path": "nvYDPF4LJK/tables/tables_23_3.jpg", "caption": "Table A5: Comparison of interactive segmentation performance. The task is evaluated on the COCO-interactive dataset proposed by [218]. *The model is finetuned on the task.", "description": "This table compares the performance of different models on the COCO-interactive dataset for interactive segmentation.  The models are evaluated on metrics such as mIoU and cIoU (Intersection over Union).  The table highlights the performance of VisionLLM v2, both before and after fine-tuning on the specific task, demonstrating its improvement after fine-tuning. It also shows the results of various other state-of-the-art methods.", "section": "A.2 Evaluation on Various Domains"}, {"figure_path": "nvYDPF4LJK/tables/tables_23_4.jpg", "caption": "Table A5: Comparison of interactive segmentation performance. The task is evaluated on the COCO-interactive dataset proposed by [218]. *The model is finetuned on the task.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art models on interactive segmentation using different visual prompts (point, scribble, box, mask).  The mIoU (mean Intersection over Union) and cIoU (cumulative IoU) metrics are reported.  The * indicates that VisionLLM v2 was fine-tuned for this specific task, showing a significant improvement in performance.", "section": "A.1 More Experimental Results"}, {"figure_path": "nvYDPF4LJK/tables/tables_24_1.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of the VisionLLM v2 model with other state-of-the-art (SOTA) models on several multimodal dialogue benchmarks.  It breaks down the results across two types of datasets: academic-oriented datasets (designed for evaluating visual question answering and related tasks) and instruction-following datasets (focused on evaluating models' ability to follow complex instructions in multimodal contexts). The table shows that VisionLLM v2 achieves competitive or better performance than other models, especially on instruction-following tasks.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_24_2.jpg", "caption": "Table 3: Comparison of object detection and instance segmentation performance. Instance seg. means instance segmentation. *The model is finetuned on the dataset.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art models for object detection and instance segmentation tasks on the COCO and CrowdHuman datasets.  It shows the AP, AP50, AP75, and mMR metrics for both tasks and indicates whether the model was fine-tuned on the dataset. The table highlights VisionLLM v2's competitive performance, especially given its use of a lightweight Swin-T backbone.", "section": "4.3 Visual Perception Tasks"}, {"figure_path": "nvYDPF4LJK/tables/tables_24_3.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art (SoTA) multimodal large language models (MLLMs) on several benchmark datasets.  It's split into academic-oriented datasets (focused on visual question answering) and instruction-following datasets (assessing the model's ability to follow complex instructions). The asterisk (*) indicates that the training annotations of that dataset were observed during the training of the corresponding model.  The table helps demonstrate VisionLLM v2's performance against other models on a range of tasks.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_25_1.jpg", "caption": "Table 2: Comparison of region recognition and visual commonsense reasoning performance. (a) SS and S-IoU represent semantic similarity and semantic IoU, which originated from [207]. (b) Q, A, and R denote question, answer, and rationale, respectively. X\u2192Y means that the model needs to select option Y conditioned on X. *The model is finetuned on the dataset.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art models on two tasks: region recognition and visual commonsense reasoning.  Region recognition assesses the model's ability to identify objects within a given bounding box, while visual commonsense reasoning tests its ability to answer questions and provide rationales based on images and language.  The table shows performance metrics such as mAP (mean Average Precision), accuracy, semantic similarity (SS), semantic IoU (S-IoU), and question-answer-rationale (QAR) scores.  The results indicate VisionLLM v2's competitive performance compared to specialized models, especially on visual commonsense reasoning.", "section": "4.2 Mutimodal Benchmarks"}, {"figure_path": "nvYDPF4LJK/tables/tables_25_2.jpg", "caption": "Table 5: Ablation on the super-link queries number. We evaluate the results on the four crucial visual perception tasks: instance segmentation (COCO), visual grounding (RefCOCO), pose estimation (COCO), and interactive segmentation (COCO using scribble). Our default setting is marked in gray.", "description": "This table presents the ablation study on the number of super-link queries used in VisionLLM v2. It evaluates the impact of varying the number of queries on four key visual perception tasks: instance segmentation using the COCO dataset, visual grounding using the RefCOCO dataset, pose estimation using the COCO dataset, and interactive segmentation using scribbles on the COCO dataset. The results show that increasing the number of queries generally improves performance across all four tasks, indicating that richer representations lead to better results. The default setting (4 queries) is highlighted in gray.", "section": "4.5 Ablation Study"}, {"figure_path": "nvYDPF4LJK/tables/tables_34_1.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of VisionLLM v2 with state-of-the-art (SoTA) models on various multimodal dialogue benchmarks.  It breaks down the results across two types of datasets: academic-oriented datasets (VQA tasks) and instruction-following datasets.  The academic datasets assess the model's ability to answer questions about images, while the instruction-following datasets evaluate the model's capability to follow instructions and perform tasks based on them. The asterisk (*) indicates that the training annotations were visible to the model during training, highlighting a potential advantage for those models.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_34_2.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art (SoTA) models on several multimodal dialogue benchmarks.  These benchmarks are categorized into academic-oriented datasets (focused on visual question answering) and instruction-following datasets (assessing the model's ability to follow instructions). The table shows that VisionLLM v2 achieves competitive or superior performance compared to SoTA models across various metrics, particularly on instruction-following datasets, even though the training annotations for some datasets were visible during training.  The asterisk (*) indicates that the model's training data included the annotations from the dataset being evaluated.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_34_3.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of VisionLLM v2 with other state-of-the-art (SOTA) models on several multimodal dialogue benchmarks.  It breaks down the results across two types of datasets: academic-oriented datasets (VQAv2, GQA, VizWiz, ScienceQA, TextVQA) which focus on question-answering tasks, and instruction-following datasets (POPE, MME, MMBench-EN/CN, SEED-Bench) which evaluate the model's ability to follow instructions.  The asterisk (*) indicates that the training annotations for the dataset were observed during training, signifying a potential advantage for models that have seen that data during training.", "section": "4 Experiments"}, {"figure_path": "nvYDPF4LJK/tables/tables_36_1.jpg", "caption": "Table 1: Comparison with SoTA models on multimodal dialogue benchmarks. The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. *The training annotations of the dataset are observed during training.", "description": "This table compares the performance of the VisionLLM v2 model with other state-of-the-art (SOTA) models on several multimodal dialogue benchmarks.  These benchmarks are categorized into academic-oriented datasets (focused on visual question answering) and instruction-following datasets (testing the model's ability to follow instructions). The table shows various metrics for each model on each dataset, highlighting the VisionLLM v2's performance relative to the other models.  An asterisk (*) indicates that the training annotations of the dataset were observed during training, implying potential data leakage.", "section": "4 Experiments"}]