{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a highly influential model that serves as a foundational model for many vision-language tasks, and is frequently compared against in the current paper."}, {"fullname_first_author": "Samir Yitzhak Gadre", "paper_title": "Datacomp: In search of the next generation of multimodal datasets", "publication_date": "2023-00-00", "reason": "This paper introduces the Datacomp dataset, the primary dataset used for pretraining in the current paper, highlighting its significance in the experimental setup."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-00-00", "reason": "This paper introduces MAE, a significant self-supervised learning method for computer vision, which is compared against in the context of linear probing."}, {"fullname_first_author": "Mathilde Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-00-00", "reason": "This paper introduces DINO, another important self-supervised learning method, providing a basis for comparison with the proposed approach."}, {"fullname_first_author": "Xinlei Chen", "paper_title": "An empirical study of training self-supervised vision transformers", "publication_date": "2021-00-00", "reason": "This paper provides a comprehensive empirical study on training self-supervised vision transformers, offering valuable insights for comparison and analysis in the current paper."}]}