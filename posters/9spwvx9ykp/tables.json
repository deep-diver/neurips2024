[{"figure_path": "9SpWvX9ykp/tables/tables_6_1.jpg", "caption": "Table 1: APPS competition results: comparison of methods. We report the percentage of problems with all unit tests passed (Strict Accuracy). For our experiments, we also include the error of the mean on the percentage.", "description": "This table presents the results of different code generation methods on the APPS Competition dataset.  The \"Strict Accuracy\" column shows the percentage of problems for which all unit tests were passed.  The evaluation strategy shows how many program attempts were made per problem before deciding if the model succeeded. The table demonstrates GIF-MCTS' superior performance compared to several baselines.", "section": "5.2 APPS"}, {"figure_path": "9SpWvX9ykp/tables/tables_7_1.jpg", "caption": "Table 2: CWMB: main results. For each method, we report the CWM accuracy and the normalized return R, averaged separately across environments with discrete and continuous action spaces. Budget indicates the number of LLM calls. For each metric, we report the mean value across environments (and for the return, also across 10 episodes) with its error. For Llama 3, we report an average of three different random seeds for additional statistical significance.", "description": "This table presents the main results of the Code World Models Benchmark (CWMB).  It compares the performance of GIF-MCTS and WorldCoder across various RL environments, categorized by whether they have discrete or continuous action spaces.  The metrics used are CWM accuracy (how well the generated Code World Model predicts the environment) and normalized return (how well a model-based RL agent using the CWM performs compared to a random agent and an oracle planner). The budget refers to the number of LLM calls used in generating the CWM.  The results for Llama 3 are averaged across three random seeds.", "section": "5.3 Code World Models Benchmark"}, {"figure_path": "9SpWvX9ykp/tables/tables_8_1.jpg", "caption": "Table 3: RTFM results. For each method and computational budget (LLM calls), we report the CWM accuracy and the normalized return R (computed across 10 episodes), with their errors.", "description": "This table presents the results of the Read to Fight Monsters (RTFM) experiment.  It compares the performance of GIF-MCTS and WorldCoder using Llama 3 70B and GPT-4 Turbo language models. The table shows the CWM accuracy and normalized return for each method at different LLM call budgets (10 and 50). The normalized return R is a metric that represents the improvement in return obtained from using the CWM as a model compared to a random policy, relative to the true simulator.  It indicates how well the CWM enables planning relative to a random policy and to an optimal planner (oracle) with access to the true model.", "section": "5.4 Read to Fight Monsters"}, {"figure_path": "9SpWvX9ykp/tables/tables_15_1.jpg", "caption": "Table 1: APPS competition results: comparison of methods. We report the percentage of problems with all unit tests passed (Strict Accuracy). For our experiments, we also include the error of the mean on the percentage.", "description": "This table compares the performance of different methods on the APPS competition benchmark. It shows the percentage of problems (out of 1000) for which all unit tests were passed. It also includes the error of the mean for each method to indicate the level of statistical significance.", "section": "5.2 APPS"}, {"figure_path": "9SpWvX9ykp/tables/tables_15_2.jpg", "caption": "Table 5: Llama 3 hyperparameters. Note that for GPT-4 Turbo, the only parameter used was the number of maximum new tokens, set to the same value used for Llama.", "description": "This table lists the hyperparameters used for the Llama 3 language model in the GIF-MCTS experiments.  It shows the values used for parameters such as `max_new_tokens`, `temperature`, `top_k`, `top_p`, `num_return_sequences`, and `num_beams`.  Note that for the GPT-4 Turbo model, only the `max_new_tokens` parameter was used, and it was set to the same value as for Llama 3.", "section": "4 GIF-MCTS"}, {"figure_path": "9SpWvX9ykp/tables/tables_16_1.jpg", "caption": "Table 6: CWMB results: ablation study. We compare the full GIF-MCTS method against three ablated variants, each leaving out one of the three action types. For each method, we report the CWM accuracy and the normalized return R, averaged separately across environments with discrete and continuous action spaces. For each metric we report the mean value across environments (and for the return, also across 10 episodes) with its error.", "description": "This table presents the results of an ablation study on the GIF-MCTS algorithm.  The study evaluates the impact of each of the three action types (Generate, Improve, Fix) on the performance of the algorithm in generating Code World Models (CWMs) for the Code World Models Benchmark (CWMB).  The table shows the accuracy and normalized return for GIF-MCTS with all three action types and for variations where one action type is excluded. The results are separated for environments with discrete and continuous action spaces, reflecting that certain action types might be more or less helpful depending on environment type.", "section": "5 Experiments"}, {"figure_path": "9SpWvX9ykp/tables/tables_16_2.jpg", "caption": "Table 7: Qualitative Analysis. We report a qualitative study for the frequency with which GIF-MCTS chooses each type of action on average. The first section of the table is considering the whole tree, while the second section (path quantities) only consider the path from the root node to the node with the highest value (where the code used as the environment was generated).", "description": "This table presents a qualitative analysis of the GIF-MCTS algorithm's action selection. It compares the overall distribution of action types (generate, improve, fix) across the entire search tree with the distribution along the optimal path leading to the best solution. This analysis provides insights into the algorithm's exploration-exploitation behavior and its effectiveness in different scenarios, such as discrete vs. continuous action spaces.", "section": "D Qualitative Study"}, {"figure_path": "9SpWvX9ykp/tables/tables_19_1.jpg", "caption": "Table 8: Comparison: inference times between GPT-4 and CWM. Results are calculated from a sample of 10 transitions from the replay buffer used during GIF-MCTS.", "description": "This table compares the inference time of using GPT-4 directly as a world model versus using a Code World Model (CWM) generated by GIF-MCTS. The inference time is measured for 10 transitions from the replay buffer in three different environments: CartPole-v1, HalfCheetah-v4, and Humanoid-v4.  The results show that using the CWM is significantly faster than using GPT-4 directly (four to seven orders of magnitude faster).", "section": "5.3 Code World Models Benchmark"}, {"figure_path": "9SpWvX9ykp/tables/tables_19_2.jpg", "caption": "Table 9: CWMB details. Detailed statistics for each environment in the CWMB. An Action Space or Observation Space indicated between bars (|A|, |S| = n) indicate a discrete space with n different choices. The value intervals for each space are omitted for visual clarity.", "description": "This table provides detailed information about the 18 environments included in the Code World Models Benchmark (CWMB). For each environment, it lists the number of lines and tokens in its description, the dimensionality of its action and observation spaces, and the number of lines and tokens in its Python code implementation.  The table helps to characterize the complexity and diversity of the environments in the benchmark.", "section": "Code World Models Benchmark"}, {"figure_path": "9SpWvX9ykp/tables/tables_20_1.jpg", "caption": "Table 2: CWMB: main results. For each method, we report the CWM accuracy and the normalized return R, averaged separately across environments with discrete and continuous action spaces. Budget indicates the number of LLM calls. For each metric, we report the mean value across environments (and for the return, also across 10 episodes) with its error. For Llama 3, we report an average of three different random seeds for additional statistical significance.", "description": "This table presents the main results of the Code World Models Benchmark (CWMB). It compares the performance of GIF-MCTS and WorldCoder in terms of CWM accuracy and normalized return (R). The accuracy represents how well the generated code world model predicts the environment dynamics. The normalized return shows the relative performance of the model-based RL agent using the generated CWM, compared to a random policy and an oracle planner with access to the true environment.  The results are broken down for environments with discrete and continuous action spaces, and the number of LLM calls used is specified. The table also includes error bars, and an average of three random seeds is used for Llama 3 to ensure statistical significance.", "section": "5.3 Code World Models Benchmark"}, {"figure_path": "9SpWvX9ykp/tables/tables_20_2.jpg", "caption": "Table 2: CWMB: main results. For each method, we report the CWM accuracy and the normalized return R, averaged separately across environments with discrete and continuous action spaces. Budget indicates the number of LLM calls. For each metric, we report the mean value across environments (and for the return, also across 10 episodes) with its error. For Llama 3, we report an average of three different random seeds for additional statistical significance.", "description": "This table presents the main results of the Code World Models Benchmark (CWMB).  It compares the performance of GIF-MCTS and WorldCoder in generating Code World Models (CWMs) across 18 diverse reinforcement learning environments. The table is split into two sections, one for environments with discrete action spaces and one for environments with continuous action spaces.  For each method and each environment type, the table shows the average CWM accuracy and normalized return, along with standard error, computed across multiple random seeds and episodes. The budget (number of LLM calls) used for each method is also reported.", "section": "5.3 Code World Models Benchmark"}, {"figure_path": "9SpWvX9ykp/tables/tables_21_1.jpg", "caption": "Table 2: CWMB: main results. For each method, we report the CWM accuracy and the normalized return R, averaged separately across environments with discrete and continuous action spaces. Budget indicates the number of LLM calls. For each metric, we report the mean value across environments (and for the return, also across 10 episodes) with its error. For Llama 3, we report an average of three different random seeds for additional statistical significance.", "description": "This table presents the main results of the Code World Models Benchmark (CWMB).  It compares the performance of the proposed GIF-MCTS method against the WorldCoder baseline. The table shows the accuracy of the generated Code World Models (CWMs) and the normalized return achieved when using these models for planning.  The results are broken down by action space (discrete or continuous) and the number of LLM calls used.  Error bars are also included for statistical significance.", "section": "5.3 Code World Models Benchmark"}, {"figure_path": "9SpWvX9ykp/tables/tables_22_1.jpg", "caption": "Table 13: MCTS planner parameters.", "description": "This table lists the hyperparameters used in the Monte Carlo Tree Search (MCTS) algorithm for planning in environments with discrete action spaces.  It shows the parameter, its description, and the value used in the experiments.", "section": "L Planning algorithms details"}, {"figure_path": "9SpWvX9ykp/tables/tables_22_2.jpg", "caption": "Table 2: CWMB: main results. For each method, we report the CWM accuracy and the normalized return R, averaged separately across environments with discrete and continuous action spaces. Budget indicates the number of LLM calls. For each metric, we report the mean value across environments (and for the return, also across 10 episodes) with its error. For Llama 3, we report an average of three different random seeds for additional statistical significance.", "description": "This table presents the main results of the Code World Models Benchmark (CWMB).  It shows a comparison of the GIF-MCTS method against the WorldCoder baseline for different environment types (discrete and continuous action spaces).  Key metrics reported include the accuracy of the generated Code World Model (CWM) and its normalized return. The number of LLM calls (budget) used is also specified.  Results are averaged across multiple environments and episodes, and error margins are included for statistical significance.", "section": "5.3 Code World Models Benchmark"}, {"figure_path": "9SpWvX9ykp/tables/tables_30_1.jpg", "caption": "Table 1: APPS competition results: comparison of methods. We report the percentage of problems with all unit tests passed (Strict Accuracy). For our experiments, we also include the error of the mean on the percentage.", "description": "This table compares the performance of different methods on the APPS competition benchmark, specifically focusing on the \"Strict Accuracy\" metric, which represents the percentage of problems where all unit tests were passed.  The table includes both existing methods and the proposed GIF-MCTS approach, providing a quantitative comparison of their effectiveness in code generation tasks.", "section": "5.2 APPS"}, {"figure_path": "9SpWvX9ykp/tables/tables_31_1.jpg", "caption": "Table 1: APPS competition results: comparison of methods. We report the percentage of problems with all unit tests passed (Strict Accuracy). For our experiments, we also include the error of the mean on the percentage.", "description": "This table compares the performance of different methods on the APPS competition benchmark, specifically focusing on the \"Competition\" split which contains the hardest problems.  The metric used is \"Strict Accuracy\", representing the percentage of problems where all unit tests are passed.  The table includes the model size used for each method and the evaluation strategy (pass@k, where k is the number of attempts) to achieve this result.  Results for GIF-MCTS (the proposed method) are compared to the baselines.", "section": "5.2 APPS"}, {"figure_path": "9SpWvX9ykp/tables/tables_32_1.jpg", "caption": "Table 1: APPS competition results: comparison of methods. We report the percentage of problems with all unit tests passed (Strict Accuracy). For our experiments, we also include the error of the mean on the percentage.", "description": "This table presents a comparison of various methods for code generation on the APPS benchmark's Competition subset.  It shows the strict accuracy rate (percentage of problems where all unit tests passed) achieved by different models and methods, including the authors' GIF-MCTS. The error of the mean is also included to provide statistical significance.  It highlights GIF-MCTS's superior performance compared to existing techniques.", "section": "5.2 APPS"}, {"figure_path": "9SpWvX9ykp/tables/tables_32_2.jpg", "caption": "Table 2: CWMB: main results. For each method, we report the CWM accuracy and the normalized return R, averaged separately across environments with discrete and continuous action spaces. Budget indicates the number of LLM calls. For each metric, we report the mean value across environments (and for the return, also across 10 episodes) with its error. For Llama 3, we report an average of three different random seeds for additional statistical significance.", "description": "This table presents the main results of the Code World Models Benchmark (CWMB).  It compares the performance of GIF-MCTS and WorldCoder in synthesizing Code World Models (CWMs).  The table shows the accuracy of the generated CWMs and their normalized return (a measure of how well they perform in planning compared to a random policy and an oracle planner).  Results are broken down for environments with discrete and continuous action spaces, and error bars are provided to indicate statistical significance. The number of LLM calls (budget) used is also reported.", "section": "5.3 Code World Models Benchmark"}, {"figure_path": "9SpWvX9ykp/tables/tables_33_1.jpg", "caption": "Table 2: CWMB: main results. For each method, we report the CWM accuracy and the normalized return R, averaged separately across environments with discrete and continuous action spaces. Budget indicates the number of LLM calls. For each metric, we report the mean value across environments (and for the return, also across 10 episodes) with its error. For Llama 3, we report an average of three different random seeds for additional statistical significance.", "description": "This table presents the results of the Code World Models Benchmark (CWMB).  It compares two methods, GIF-MCTS and WorldCoder, across environments with both discrete and continuous action spaces. The metrics used are CWM accuracy (a measure of how well the generated Code World Model predicts the environment's dynamics) and normalized return R (a measure of the planning agent's performance using the generated CWM, compared to random and oracle planners).  The budget column shows the number of LLM calls used.  Error bars are included to show statistical significance.", "section": "5.3 Code World Models Benchmark"}]