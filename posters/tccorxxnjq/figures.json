[{"figure_path": "TcCorXxNJQ/figures/figures_1_1.jpg", "caption": "Figure 1: The overview of LoRA, FedIT, and our FLORA. The top row shows how LoRA updates the model in centralized fine-tuning. The middle and bottom rows show the global model updating strategies in FedIT and our FLORA respectively.", "description": "This figure illustrates the difference between the centralized LoRA, FedIT, and the proposed FLORA method for updating large language models (LLMs).  The top row shows how LoRA (Low-Rank Adaptation) works in a centralized setting, adding low-rank matrices A and B to the pre-trained parameters W to compute the update AW=BA. The middle row depicts FedIT's federated learning approach, which averages the local LoRA updates from multiple clients, leading to inaccurate aggregation and noise. The bottom row shows FLORA's novel stacking-based aggregation method, which avoids noise and supports heterogeneous LoRA adapters, enabling more efficient and accurate federated fine-tuning.", "section": "1 Introduction"}, {"figure_path": "TcCorXxNJQ/figures/figures_2_1.jpg", "caption": "Figure 2: Module stacking in FLORA is a noise-free aggregation for LoRA, while the module averaging in FedIT cannot accurately aggregate the local updates.", "description": "This figure compares the aggregation methods of FLORA and FedIT for Low-Rank Adaptation (LoRA) modules.  FedIT averages the local LoRA modules independently, leading to inaccurate aggregation and noise in the global model update. In contrast, FLORA stacks the local LoRA modules, resulting in a noise-free aggregation. The figure visually illustrates the difference between the two methods, showing how FedIT's averaging introduces an intermediate term that causes errors and how FLORA's stacking method avoids this error. The diagram uses color-coded blocks to represent the matrices of local LoRA modules and the aggregation process, making the comparison clear.", "section": "3 Proposed Method: FLORA"}, {"figure_path": "TcCorXxNJQ/figures/figures_4_1.jpg", "caption": "Figure 3: FLORA workflow. The local LoRA modules are initialized and optimized each round, and stacked by the server to obtain the global LoRA modules. The global modules are then sent back to clients to update local models.", "description": "This figure illustrates the workflow of the FLORA algorithm. Each client initializes and optimizes its own local LoRA modules. These modules are then sent to the server, where they are stacked together to form global LoRA modules using the stacking-based aggregation method. The server then sends the global modules back to each client, which uses them to update their local models. This process is repeated for each round of federated learning, leading to a noise-free and efficient fine-tuning of the LLM.", "section": "3 Proposed Method: FLoRA"}, {"figure_path": "TcCorXxNJQ/figures/figures_6_1.jpg", "caption": "Figure 4: Standalone experiment results. The red bars represent the global model performance and the blue bars represent the local model performance with varying LoRA ranks.", "description": "This figure shows the results of a standalone experiment, where each client trains the model locally without any federated learning. The red bars represent the performance of the global model, while the blue bars represent the performance of each client's local model using different LoRA ranks (64, 32, 16, 8, and 4). The results help to understand how different LoRA rank settings affect performance in a non-federated setting, and they also provide a benchmark against which the performance of federated learning methods can be compared.", "section": "4.2 Experiment Results"}, {"figure_path": "TcCorXxNJQ/figures/figures_7_1.jpg", "caption": "Figure 4: Standalone experiment results. The red bars represent the global model performance and the blue bars represent the local model performance with varying LoRA ranks.", "description": "This figure shows the results of standalone experiments, where each client trains the model locally without any federation. The red bars represent the global model performance, and the blue bars represent the local model performance with varying LORA ranks. The experiment is conducted to compare the performance of local and global models, and how varying LORA ranks affects the performance of the local model.", "section": "4.2 Experiment Results"}, {"figure_path": "TcCorXxNJQ/figures/figures_8_1.jpg", "caption": "Figure 6: The ratio of communicated parameter numbers to full fine-tuning.", "description": "This figure compares the communication overhead of three different fine-tuning methods: full fine-tuning, FedIT, and FLORA.  The ratio of communicated parameters to the total number of parameters in full fine-tuning is shown for each method across three communication rounds.  Full fine-tuning has a ratio of 1.000, indicating that all parameters are communicated.  FedIT has a much lower ratio (0.177) because it only communicates the updated LoRA parameters. FLORA has a slightly higher ratio than FedIT (0.215),  because it transmits both stacked LoRA parameters; however, it still maintains a significantly lower communication overhead compared to full fine-tuning.", "section": "5 Discussion"}, {"figure_path": "TcCorXxNJQ/figures/figures_13_1.jpg", "caption": "Figure 7: The impact of scaling factor on Llama2 model.", "description": "This figure shows the impact of varying scaling factors on the performance of the Llama2 model when fine-tuned using FLORA on the Wizard and ShareGPT datasets.  The x-axis represents the scaling factor (pk), and the y-axis represents the MT-bench score.  The results indicate that there is not a consistent optimal scaling factor across different datasets; the best scaling factor is data-dependent.", "section": "4.2 Experiment Results"}]